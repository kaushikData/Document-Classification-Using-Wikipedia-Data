{"id": "23216432", "url": "https://en.wikipedia.org/wiki?curid=23216432", "title": "2009 flu pandemic in France", "text": "2009 flu pandemic in France\n\nThe 2009 flu pandemic, involving an outbreak of a new strain of influenza commonly known as swine flu (usually referred as \"grippe A\" or \"grippe porcine\" in French), reached France in early May 2009.\n\nIn order to respond to flu epidemics in France, the government has a \"national plan\", which is also applied for this flu pandemic. In this plan, the different phases of the flu, which are slightly different from the World Health Organization phases, are detailed.\n\n\nA governmental web site has been created to inform the population of the dangers and good practices to avoid the spread of the disease.\n\nThe French government lags behind the decision of the World Health Organization to upgrade to phase 6. It is currently in phase 5A in France.\n\nA mandatory mass vaccination plan has been discussed. This vaccination plan is currently implemented, it is free and not mandatory.\n\nIn July 2010, France initially ordered 1,46 doses per inhabitant, the third largest amount among members of the Global Health Security Initiative (GHSI), behind the United Kingdom and Canada. In January, 50 million of the initial 94 million doses were cancelled.\n\nThe French government asks Europe to decide to cancel all flights to Mexico.\n\nStocks of Tamiflu and Relenza are 33 million.\n\nOn 1 May, the French Health Minister confirmed, during the 8 p.m. TF1 news, that 2 cases of A(H1N1) flu have been detected in France.\n\nThe government discusses the possibility of ordering 100 million vaccine doses against the flu from three companies (GlaxoSmithKline, Sanofi and Novartis).\n\nThe French government plans to vaccinate the entire French population (65 million inhabitants)\n\n\n\n\n\n\n\n\n\n\n\nThe detailed situation by region is updated by the National Institute of Health\n\n\n"}
{"id": "24451713", "url": "https://en.wikipedia.org/wiki?curid=24451713", "title": "2009 flu pandemic in Germany", "text": "2009 flu pandemic in Germany\n\nThe 2009 flu pandemic, involving an outbreak of a new strain of influenza commonly known as swine flu, reached Germany in April 2009.\n\nTwo men and a woman from Bielefeld who had been firstly suspected of having the virus tested negative on influenza type A.\n\nOn 29 April, the first case of swine flu in Germany was confirmed by the Robert Koch Institute in the area of Regensburg. A 22-year-old woman from Hamburg is also confirmed to have been infected by swine flu during a trip to Mexico. A 37-year-old woman from Kulmbach is also confirmed to have become infected during a similar trip.\n\nOn 1 May, the Robert Koch Institute confirmed the first case of human-to-human transmission of swine flu in Munich. A nurse who had contact with one of the infected people was infected with the virus. At approximately 10:00 she was claimed to be already healed. At 13:00 one additional infection in Bavaria was confirmed, but the patient was also claimed to be healthy again.\n\nOn 2 May, a new human-to-human infection, in the same hospital in Munich, was confirmed. The new patient, who was in the same room with the original infected German that came from Mexico, is currently being reported to show no signs of the new influenza strain anymore.\n\nOn 3 May, two further cases of swine flu in Brandenburg were reported. Two people from the same flight as patient in Hamburg were also infected.\n\nOn 5 May, one new case in Saxony-Anhalt has been confirmed bringing to 9 the total number of people infected.\n\nOn 7 May, another new case in Saxony-Anhalt is reported.\n\nOn 8 May, an adult male living in Bavaria who had recently been to United States.\n\nOn 11 May, the case of a 27-year-old Bavarian woman, who stayed for some weeks in Mexico and medicated patients in a hospital, is reported.\n\nOn 15 May, two more cases were reported, a female and her son from Saxony-Anhalt were obviously infected by her husband / his father, who returned from Mexico.\n\nOn 21 May, a case was found in a 43-year-old woman from Düsseldorf in North Rhine-Westphalia who returned from New York. One day later, Robert Koch Institute confirmed that her husband has been tested positive with swine influenza too. Furthermore, their six-year-old daughter, who did not stay in New York, has been infected by her parents, bringing the total to 17.\n\nUntil 5 June 2009, the total number of confirmed cases increased to 49. Most of them have been recent travellers to Mexico, the US or the UK. However, there was also a single-digit number of (isolated) in-country-transmissions.\n\nAs of 6 November 2009, the number of infections increased to 29,900 with nine deaths caused by the flu.\n"}
{"id": "3252", "url": "https://en.wikipedia.org/wiki?curid=3252", "title": "Amygdalin", "text": "Amygdalin\n\nAmygdalin (from Ancient Greek: \"\" \"almond\") is a naturally occurring chemical compound, famous for falsely being promoted as a cancer cure. It is found in many plants, but most notably in the seeds (kernels) of apricot, bitter almonds, apple, peach, and plum.\n\nAmygdalin is classified as a cyanogenic glycoside because each amygdalin molecule includes a nitrile group, which can be released as the toxic cyanide anion by the action of a beta-glucosidase: eating amygdalin will cause it to release cyanide in the human body, and may lead to cyanide poisoning. Neither amygdalin nor laetrile is a vitamin.\n\nSince the early 1950s, both amygdalin and a modified form named laetrile have been promoted as alternative cancer treatments, often using the misnomer vitamin B. But studies have found them to be clinically ineffective in the treatment of cancer, as well as potentially toxic or lethal when taken by mouth, due to cyanide poisoning. The promotion of laetrile to treat cancer has been described in the medical literature as a canonical example of quackery, and as \"the slickest, most sophisticated, and certainly the most remunerative cancer quack promotion in medical history\".\n\nAmygdalin is a cyanogenic glycoside derived from the aromatic amino acid phenylalanine. Amygdalin and prunasin are common among plants of the family Rosaceae, particularly the genus \"Prunus\", Poaceae (grasses), Fabaceae (legumes), and in other food plants, including flaxseed and manioc. Within these plants, amygdalin and the enzymes necessary to hydrolyze them are stored in separate locations so that they will mix in response to tissue damage. This provides a natural defense system.\n\nAmygdalin is contained in stone fruit kernels, such as apricot (1.4%), peach (0.68%), bitter almond (5%), and plum (0.04–1.7%), and also in the seeds of the apple (0.3%). The stones are taken out of the fruit and cracked to obtain the kernels, which are dried in the sun or in ovens. The kernels are boiled in ethanol; on evaporation of the solution and the addition of diethyl ether, amygdalin is precipitated as minute white crystals. Natural amygdalin has the (\"R\")-configuration at the chiral phenyl center. Under mild basic conditions, this stereogenic center isomerizes; the (\"S\")-epimer is called neoamygdalin. Although the synthesized version of amygdalin is the (\"R\")-epimer, the stereogenic center attached to the nitrile and phenyl groups easily epimerizes if the manufacturer does not store the compound correctly.\n\nAmygdalin is hydrolyzed by intestinal β-glucosidase (emulsin) and amygdalin beta-glucosidase (amygdalase) to give gentiobiose and -mandelonitrile. Gentiobiose is further hydrolyzed to give glucose, whereas mandelonitrile (the cyanohydrin of benzaldehyde) decomposes to give benzaldehyde and hydrogen cyanide. Hydrogen cyanide in sufficient quantities (allowable daily intake: ~0.6 mg) causes cyanide poisoning (fatal oral dose: 0.6–1.5 mg/kg).\n\nLaetrile (patented 1961) is a simpler semisynthetic version of amygdalin. Laetrile is synthesized from amygdalin by hydrolysis. The usual preferred commercial source is from apricot kernels (\"Prunus armeniaca\"). The name is derived from the separate words \"laevarotatory\" and \"mandelonitrile\". Laevarotatory describes the stereochemistry of the molecule, while mandelonitrile refers to the portion of the molecule from which cyanide is released by decomposition.\nA 500 mg laetrile tablet may contain between 5–51 mg of hydrogen cyanide per gram.\n\nLike amygdalin, laetrile is hydrolyzed in the duodenum (alkaline) and in the intestine (enzymatically) to -glucuronic acid and -mandelonitrile; the latter hydrolyzes to benzaldehyde and hydrogen cyanide, that in sufficient quantities causes cyanide poisoning.\n\nClaims for laetrile were based on three different hypotheses: The first hypothesis proposed that cancerous cells contained copious beta-glucosidases, which release HCN from laetrile via hydrolysis. Normal cells were reportedly unaffected, because they contained low concentrations of beta-glucosidases and high concentrations of rhodanese, which converts HCN to the less toxic thiocyanate. Later, however, it was shown that both cancerous and normal cells contain only trace amounts of beta-glucosidases and similar amounts of rhodanese.\n\nThe second proposed that, after ingestion, amygdalin was hydrolyzed to mandelonitrile, transported intact to the liver and converted to a beta-glucuronide complex, which was then carried to the cancerous cells, hydrolyzed by beta-glucuronidases to release mandelonitrile and then HCN. Mandelonitrile, however, dissociates to benzaldehyde and hydrogen cyanide, and cannot be stabilized by glycolsylation.\n\nFinally, the third asserted that laetrile is the discovered vitamin B-17, and further suggests that cancer is a result of \"B-17 deficiency\". It postulated that regular dietary administration of this form of laetrile would, therefore, actually prevent all incidence of cancer. There is no evidence supporting this conjecture in the form of a physiologic process, nutritional requirement, or identification of any deficiency syndrome. The term \"vitamin B-17\" is not recognized by Committee on Nomenclature of the American Institute of Nutrition Vitamins. Ernst T. Krebs branded laetrile as a vitamin in order to have it classified as a nutritional supplement rather than as a pharmaceutical.\n\nAmygdalin was first isolated in 1830 from bitter almond seeds (\"Prunus dulcis\") by Pierre-Jean Robiquet and Antoine Boutron-Charlard. Liebig and Wöhler found three hydrolysis products of amygdalin: sugar, benzaldehyde, and prussic acid (hydrogen cyanide, HCN). Later research showed that sulfuric acid hydrolyzes it into -glucose, benzaldehyde, and prussic acid; while hydrochloric acid gives mandelic acid, -glucose, and ammonia.\n\nIn 1845 amygdalin was used as a cancer treatment in Russia, and in the 1920s in the United States, but it was considered too poisonous. In the 1950s, a purportedly non-toxic, synthetic form was patented for use as a meat preservative, and later marketed as laetrile for cancer treatment.\n\nThe U.S. Food and Drug Administration prohibited the interstate shipment of amygdalin and laetrile in 1977. Thereafter, 27 U.S. states legalized the use of amygdalin within those states.\n\nIn a 1977 controlled, blinded trial, laetrile showed no more activity than placebo.\n\nSubsequently, laetrile was tested on 14 tumor systems without evidence of effectiveness. The Memorial Sloan–Kettering Cancer Center (MSKCC) concluded that \"laetrile showed no beneficial effects.\" Mistakes in an earlier MSKCC press release were highlighted by a group of laetrile proponents led by Ralph Moss, former public affairs official of MSKCC who had been fired following his appearance at a press conference accusing the hospital of covering up the benefits of laetrile. These mistakes were considered scientifically inconsequential, but Nicholas Wade in \"Science\" stated that \"even the appearance of a departure from strict objectivity is unfortunate.\" The results from these studies were published all together.\n\nA 2015 systematic review from the Cochrane Collaboration found: \n\nThe authors also recommended, on ethical and scientific grounds, that no further clinical research into laetrile or amygdalin be conducted.\n\nGiven the lack of evidence, laetrile has not been approved by the U.S. Food and Drug Administration or the European Commission.\n\nThe U.S. National Institutes of Health evaluated the evidence separately and concluded that clinical trials of amygdalin showed little or no effect against cancer. For example, a 1982 trial by the Mayo Clinic of 175 patients found that tumor size had increased in all but one patient. The authors reported that \"the hazards of amygdalin therapy were evidenced in several patients by symptoms of cyanide toxicity or by blood cyanide levels approaching the lethal range.\"\n\nThe study concluded \"Patients exposed to this agent should be instructed about the danger of cyanide poisoning, and their blood cyanide levels should be carefully monitored. Amygdalin (Laetrile) is a toxic drug that is not effective as a cancer treatment\".\n\nAdditionally, \"No controlled clinical trials (trials that compare groups of patients who receive the new treatment to groups who do not) of laetrile have been reported.\"\n\nThe side effects of laetrile treatment are the symptoms of cyanide poisoning. These symptoms include: nausea and vomiting, headache, dizziness, cherry red skin color, liver damage, abnormally low blood pressure, droopy upper eyelid, trouble walking due to damaged nerves, fever, mental confusion, coma, and death.\n\nThe European Food Safety Agency's Panel on Contaminants in the Food Chain has studied the potential toxicity of the amygdalin in apricot kernels. The Panel reported, \"If consumers follow the recommendations of websites that promote consumption of apricot kernels, their exposure to cyanide will greatly exceed\" the dose expected to be toxic. The Panel also reported that acute cyanide toxicity had occurred in adults who had consumed 20 or more kernels and that in children \"five or more kernels appear to be toxic\".\n\nAdvocates for laetrile assert that there is a conspiracy between the US Food and Drug Administration, the pharmaceutical industry and the medical community, including the American Medical Association and the American Cancer Society, to exploit the American people, and especially cancer patients.\n\nAdvocates of the use of laetrile have also changed the rationale for its use, first as a treatment of cancer, then as a vitamin, then as part of a \"holistic\" nutritional regimen, or as treatment for cancer pain, among others, none of which have any significant evidence supporting its use.\n\nDespite the lack of evidence for its use, laetrile developed a significant following due to its wide promotion as a \"pain-free\" treatment of cancer as an alternative to surgery and chemotherapy that have significant side effects. The use of laetrile led to a number of deaths.\nThe FDA and AMA crackdown, begun in the 1970s, effectively escalated prices on the black market, played into the conspiracy narrative and enabled unscrupulous profiteers to foster multimillion-dollar smuggling empires.\n\nSome North American cancer patients have traveled to Mexico for treatment with the substance, for example at the Oasis of Hope Hospital in Tijuana. The actor Steve McQueen died in Mexico following surgery to remove a stomach tumor having previously undergone extended treatment for pleural mesothelioma (a cancer associated with asbestos exposure) under the care of William D. Kelley, a de-licensed dentist and orthodontist who claimed to have devised a cancer treatment involving pancreatic enzymes, 50 daily vitamins and minerals, frequent body shampoos, enemas, and a specific diet as well as laetrile.\n\nLaetrile advocates in the United States include Dean Burk, a former chief chemist of the National Cancer Institute cytochemistry laboratory, and national arm wrestling champion Jason Vale, who claimed that his kidney and pancreatic cancers were cured by eating apricot seeds. Vale was convicted in 2004 for, among other things, fraudulently marketing laetrile as a cancer cure. The court also found that Vale had made at least $500,000 from his fraudulent sales of laetrile.\n\nIn the 1970s, court cases in several states challenged the FDA's authority to restrict access to what they claimed are potentially lifesaving drugs. More than twenty states passed laws making the use of Laetrile legal. After the unanimous Supreme Court ruling in \"Rutherford v. United States\" which established that interstate transport of the compound was illegal, usage fell off dramatically. The US Food and Drug Administration continues to seek jail sentences for vendors marketing laetrile for cancer treatment, calling it a \"highly toxic product that has not shown any effect on treating cancer.\"\n\n\n"}
{"id": "38362640", "url": "https://en.wikipedia.org/wiki?curid=38362640", "title": "Association for the Wellbeing of Children in Healthcare", "text": "Association for the Wellbeing of Children in Healthcare\n\nThe Association for the Wellbeing of Children in Healthcare is an Australian-based voluntary organisation that gives non-medical attention and support to hospitalised children and their parents. Formed in 1973, the group changed its name from Association for the Welfare of Children in Hospital to Association for the Welfare of Child Health in 1993 and then to Association for the Wellbeing of Children in Healthcare in 2007. In 2008, the association co-wrote \"Standards for the Care of Children and Adolescents in Health Services,\" a set of guidelines for treating children and teenagers in wards that are separate from adults. As of 2007, the association was based in Gladesville, New South Wales.\n\nIn 1959, the United Kingdom's Ministry of Health published the Platt Report 1959, a three-year investigative account of the welfare of children in United Kingdom hospital that required hospitals to implement major changes in the non-medical care of children in hospital. That led to the 1961 formation of Mother Care for Children in Hospital (MCCH), a group formed to increase the roles mothers had over the care of their child in hospital. By 1965, enough like-minded healthcare professionals joined the mother's group and the group was renamed the National Association for the Welfare of Children in Hospital (NAWCH). That led to the formation of similar groups in Canada (Canadian Association for Community Care) and Australia.\n\nThe Association for the Wellbeing of Children in Healthcare (AWCH) was founded as Association for the Welfare of Children in Hospital (AWCH) on 15 February 1973 in Australia as a voluntary organisation that gives non-medical attention and support to hospitalised children and their parents. The newly formed organisation wanted to make the public aware of conditions for children in NSW's hospitals, including deficiencies in standards of care. The Association was particularly concerned whether parents were allowed to visit freely in hospitals. Years of debate over the need to consider the emotional as well as physical needs of children in hospital lay behind the formation of the new body. The group changed its 1973 name from Association for the Welfare of Children in Hospital to Association for the Welfare of Child Health in 1993, and then to Association for the Wellbeing of Children in Healthcare in 2007.\n\nIn 2008, the Association for the Wellbeing of Children in Healthcare and the Royal Australian College of Physicians co-wrote \"Standards for the Care of Children and Adolescents in Health Services,\" a set of guidelines for treating children and teenagers in wards that are separate from adults. The standard concluded that treating children and teenagers on mixed wards alongside adults exposed the minors to poor care and mental trauma. To address this, the Association for the Wellbeing of Children in Healthcare proposed in the standard to treat children in separate areas of adult wards where staff are trained in children's needs or in wards that are completely separate from adults.\n\n"}
{"id": "31511965", "url": "https://en.wikipedia.org/wiki?curid=31511965", "title": "Bernice Ackerman", "text": "Bernice Ackerman\n\nBernice Ackerman (1925-1995) was an American meteorologist, known for being the first woman weathercaster in the U.S. and the first woman meteorologist at Argonne National Laboratory.\n\nPrior to attending college, Ackerman was a weather observer and flight briefer for the Women Accepted for Volunteer Emergency Service (WAVES) in World War II. Ackerman attended the University of Chicago throughout her education, where she received her a bachelor's degree in meteorology and Phi Beta Kappa in 1948, her master's degree in meteorology in 1955, and her Ph.D. in geophysical science in 1965.\n\nAfter earning her bachelor's degree, Ackerman took up a position as a meteorologist and hydrologist at the U.S. Weather Bureau, where she stayed until 1953. She then moved to Argonne National Laboratory, where she was the only woman to research in its Cloud Physics Laboratory, a joint project with the University of Chicago. After earning her Ph.D., Ackerman became an assistant professor at Texas A&M University; she was promoted to associate professor in 1967. While there, she taught cloud physics and boundary layer meteorology. She left Texas A&M in 1970 and returned to Argonne for two years, then moved to the Illinois State Water Survey at the University of Illinois, where she stayed until 1989, eventually becoming the head of the meteorology section.\n\n"}
{"id": "1277062", "url": "https://en.wikipedia.org/wiki?curid=1277062", "title": "Bread crumbs", "text": "Bread crumbs\n\nBread crumbs or breadcrumbs (regional variants: breading, crispies) are sliced residue of dry bread, used for breading or crumbing foods, topping casseroles, stuffing poultry, thickening stews, adding inexpensive bulk to soups, meatloaves and similar foods, and making a crisp and crunchy covering for fried foods, especially breaded cutlets like tonkatsu and schnitzel. The Japanese variety of bread crumbs is called \"panko\".\n\nDry breadcrumbs are made from dry breads which have been baked or toasted to remove most remaining moisture, and may have a sandy or even powdery texture. Bread crumbs are most easily produced by pulverizing slices of bread in a food processor, using a steel blade to make coarse crumbs, or a grating blade to make fine crumbs. A grater or similar tool will also do.\n\nThe breads used to make soft or fresh bread crumbs are not quite as dry, so the crumbs are larger and produce a softer coating, crust, or stuffing. The \"crumb\" of \"bread crumb\" also refers to the texture of the soft, inner part of a bread loaf, as distinguished from the crust, or \"skin\".\n\n is a variety of flaky bread crumb used in Japanese cuisine as a crunchy coating for fried foods, such as \"tonkatsu\". \"Panko\" is made from bread baked by electrical current, which yields a bread without a crust, and then grinding the bread to create fine slivers of crumb. It has a crisper, airier texture than most types of breading found in Western cuisine and resists absorbing oil or grease when fried, resulting in a lighter coating. Outside Japan, its use is becoming more popular in both Asian and non-Asian dishes: It is often used on seafood and is often available in Asian markets, speciality stores, and, increasingly, in many large supermarkets.\n\n\"Panko\" is produced worldwide, particularly in Asian countries, including Japan, Korea, Thailand, China, and Vietnam.\n\nThe Japanese first learned to make bread from the Europeans. The word \"panko\" is derived from \"pan\", the Portuguese/Spanish/Japanese word for bread, and \"-ko\", a Japanese suffix indicating \"flour\", \"coating\", \"crumb\", or \"powder\" (as in \"komeko\", \"rice powder\", \"sobako\", \"buckwheat flour\", and \"komugiko\", \"wheat flour\").\n\nBreading (also known as crumbing) is a dry grain-derived food coating for a piece of food made from bread crumbs or a breading mixture with seasonings. Breading can also refer to the process of applying a bread-like coating to a food. Breading is well suited for frying as it lends itself to creating a crisp coating around the food. Breading mixtures can be made of breadcrumb, flour, cornmeal, and seasoning that the item to be breaded is dredged in before cooking. If the item to be breaded is too dry for the coating to stick, the item may first be moistened with buttermilk, raw egg, egg wash or other liquid.\n\nBreading contrasts with batter, which is a grain-based liquid coating for food that produces a smoother and finer texture, but which can be softer overall.\n\nIn the fairy tale Hansel and Gretel, breadcrumbs were used by Hansel and Gretel to track their footpath. However, the bread crumbs were eventually eaten by birds, subsequently leading them to become lost in the woods. The popularity of breadcrumbs in the fairy tale lead to the inspiration the name \"breadcrumb\" as a navigation element that allows users to keep track of their locations within programs or documents.\n\nFried Shrimp, Fried Chicken Fillet, Fried Fish, Fried Mushroom... List of recipes with Panko in countless and its range is your imagination.Some recipes with Panko Tempura Powder are: Nanban Chicken (Tempura + Chicken Fillet), Puff Onion (Tempura + Onion), Puff Shrimp (Tempura + Shrimp)\n"}
{"id": "5218858", "url": "https://en.wikipedia.org/wiki?curid=5218858", "title": "Canadian Academy of Endodontics", "text": "Canadian Academy of Endodontics\n\nAn association of endodontists located in Canada, whose goal is to advance the state of endodontics.\n"}
{"id": "40859735", "url": "https://en.wikipedia.org/wiki?curid=40859735", "title": "Celibacy syndrome", "text": "Celibacy syndrome\n\nCelibacy syndrome (, \"sekkusu shinai shōkōgun\") is a media hypothesis proposing that a growing number of Japanese adults have lost interest in sexual activity and have also lost interest in romantic love, dating and marriage. The theory has been reported by unknown members of \"Japan's media\" according to journalist Abigail Haworth of \"the Guardian\". Following the report, the theory gained widespread attention in English media outlets in 2013, and was subsequently refuted by several journalists and bloggers.\n\nIn addition to celibacy, the theory cites declining numbers of marriages and declining birthrates in Japan. According to surveys conducted by the Japan Association for Sex Education, between 2011 and 2013, the number of female college students reporting to be virgins increased. Additionally, surveys conducted by the Japanese Family Planning Association (JFPA) indicated a high number of Japanese women who reported that they \"were not interested in or despised sexual contact\". Meanwhile, surveys conducted by the National Institute of Population and Social Security Research in Japan in 2008 and 2013, revealed that the number of Japanese men and women reporting to not be in any kind of romantic relationship grew by 10%.\n\nThe theory attributes two possible causes for these reports: the past two decades of economic stagnation as well as high gender inequality in Japan.\n\nOne critic accused \"The Guardian\" and other media outlets of using \"cherry-picked\" data in order to make a sensational claim that appeals to Western notions of a \"weird Japan\". Another criticism points to contrary statistics that indicate that Japanese youth are having sex more frequently than ever. Additionally, one of the surveys on which the theory is based has been criticized as having a statistically invalid sample size. In that survey, only 60 respondents (aged 16 to 19) claimed an aversion to sex, and merely 126 respondents were used to represent the young Japanese population (aged 16 to 19), which was nearly 6 million in 2014. Another criticism points out that, while the theory conflates sexlessness with low birthrate, these have been demonstrated by others to be uncorrelated.\n\n"}
{"id": "11025638", "url": "https://en.wikipedia.org/wiki?curid=11025638", "title": "Children's Cancer Center of Lebanon", "text": "Children's Cancer Center of Lebanon\n\nThe Children's Cancer Center of Lebanon is a non-profit medical institution dedicated to the comprehensive treatment of pediatric cancer. The center is affiliated with St. Jude Children's Research Hospital in Memphis, Tennessee, founded by Lebanese U.S. comedian and actor Danny Thomas. He had a dream to open a similar medical institute in his ancestral home of Lebanon.\n\nDanny Thomas's children Marlo and Tony Thomas pursued his dream until the center opened in Beirut on April 12, 2002. The center, which is located on 56 Rue Clémenceau, works in association with the nearby American University of Beirut Medical Center (AUBMC). The mission of the center is to save children's lives using the protocols of St. Jude. The center depends completely on donations and fundraising programs. \n\nOn April 12, 2002, the late Prime Minister of Lebanon, Rafik Hariri, several ministers of the Lebanese government and members of the Lebanese Parliament, as well as major officials from the American University of Beirut, joined a delegation from the American Lebanese Syrian Associated Charities and St. Jude Children's Research Hospital and the board of the \"Children's Cancer Center of Lebanon\" for the inauguration ceremony of the center. Additionally, parents and families of Lebanese children treated at St. Jude's in Memphis also attended the opening ceremony.\n"}
{"id": "16429740", "url": "https://en.wikipedia.org/wiki?curid=16429740", "title": "Chua Jui Meng", "text": "Chua Jui Meng\n\nDato' Chua Jui Meng (; born 22 October 1943) is a Malaysian politician. He was the Member of the Parliament of Malaysia for the Bakri constituency in the State of Johor for five terms from 1986 to 2008. He was the country's longest serving Minister of Health, holding that position from 1995 to 2004.\n\nChua is a lawyer called to the British Bar as a Barrister-at-law at the Inner Temple before entering politics through his involvement with the Malaysian Chinese Association (MCA). After 35 years with the party, he quit to join Parti Keadilan Rakyat (PKR) in 2009.\n\nPrior to entering full-time politics, Chua was a student activist in the 1970s. He was president of the Malaysian and Singaporean Law Society in the United Kingdom and Ireland as well as Editor in Chief of the Federation of UK and Ireland Malaysian and Singaporean Student Associations.\n\nChua began his political career in 1976 when he became a member of the Malaysian Chinese Association, a component of the ruling Barisan Nasional coalition. In the 1986 general election, he was elected as Member of Parliament for Bakri, a seat he retained for five consecutive terms.\n\nHis speech in Parliament in 1988 on the \"Malaysian Chinese dilemma\" as a result of the \"deviations and misimplementation\" of the New Economic Policy (NEP) sparked the formation of the National Economic Consultative Council (NECC). The NECC formulated the National Development Policy (1990–2000), which Chua credited as key to liberalising the economy, education and culture and turning Chinese voter in favour of Barisan Nasional from the 1990s to 2004.\n\nIn 1989, Chua was appointed parliamentary secretary for the Ministry of Health. He was elected as a vice-president of the MCA in 1990 and became the Deputy Minister of International Trade and Industry following the 1990 general election. During this period, he worked on developing small and medium enterprises and promoting trade with China.\n\nAfter the 1995 general election, Chua was appointed Minister of Health. During his tenure, Chua led the government's fight against the Coxsackievirus outbreak in 1997, the Nipah virus outbreak in 1999, the Japanese encephalitis outbreak in 2000 and the global SARS epidemic in 2003.\n\nMCA fell into crisis in 2001, when factional infighting between \"Teams A\" and \"B\" became public. Chua aligned himself to the Lim Ah Lek-led Team B during the \"Nanyang Siang Pau\" takeover crisis. The 2002 party elections were cancelled, and Chua retained his vice presidency under the MCA \"peace plan\" of 2003, which saw Ong Ka Ting assume the presidency. However, he was dropped from the Cabinet after the 2004 general election as he was not recommended to the prime minister by the new party leader.\n\nIn the following year's party elections, Chua challenged Ong for the presidency. He performed above expectations, garnering more than a third of the delegates' votes, but was unable to topple the heavily favoured Ong. He made another long-shot attempt at the presidency in 2008, but lost out to Ong Tee Keat. He did not contest the 2008 general election and 2013 as his former seat of Bakri fell to the opposition.\n\nIn July 2009, Chua quit the MCA to join the opposition Parti Keadilan Rakyat (PKR), citing the need to preserve the two-party system that emerged after the 2008 elections. In June 2010, the Sultan of Johor, Sultan Ibrahim Ismail revoked the state awards (carrying the titles Dato' and Dato' Seri) conferred to Chua by the previous Sultan. Chua accused Barisan Nasional of instigating the move as payback for joining the opposition. He still retains the Dato' title since he was conferred other state titles by the Sultans of Selangor and Pahang.\n\nHe was PKR's chief in Johor, which is regarded as a Barisan Nasional stronghold.\n\nIn February 2013, an open verbal conflict erupted between Johor DAP chairman Boo Cheng Hau and Chua Jui Meng, prompting calls from both DAP and PKR party heavyweights to weigh in for their party stalwarts. This as a result of accusations from either side accusing each other of splitting Pakatan Rakyat in the state of Johor. The conflict was resolved after talks and mediation by the central leadership of both parties. One of the sign of reconciliation between Chua and Johor DAP was demonstrated when Chua received unconditional support from the DAP chief in the Segamat constituency which he was chosen to contest at the 2013 election. The seat was traditionally contested by DAP. Chua was defeated in the election by the incumbent member Dr. S. Subramaniam, a federal minister.\n\n\n"}
{"id": "6324", "url": "https://en.wikipedia.org/wiki?curid=6324", "title": "Collective trauma", "text": "Collective trauma\n\nA collective trauma is a traumatic psychological effect shared by a group of people of any size, up to and including an entire society. Traumatic events witnessed by an entire society can stir up collective sentiment, often resulting in a shift in that society's culture and mass actions.\n\nWell known collective traumas include: The Holocaust, the Armenian Genocide, Slavery in the United States, the Atomic bombings of Hiroshima and Nagasaki, the Trail of Tears, the MS Estonia in Sweden, the September 11, 2001 attacks in the United States, and various others.\n\nCollective traumas have been shown to play a key role in group identity formation (see: Law of Common Fate). During World War II, a US submarine, the USS Puffer (SS-268), came under several hours of depth charge attack by a Japanese surface vessel until the ship became convinced the submarine had somehow escaped. Psychological studies later showed that crewmen transferred to the submarine after the event were never accepted as part of the team. Later, US naval policy was changed so that after events of such psychological trauma, the crew would be dispersed to new assignments.\n\nRehabilitation of survivors becomes extremely difficult when entire nation has experienced such severe traumas as war, genocide, torture, massacre, etc. Treatment is hardly effective when everybody is traumatized. Trauma remains chronic and would reproduce itself as long as social causes are not addressed and perpetrators continue to enjoy impunity. The whole society may suffer from an everlasting culture of pain. (1)\n\nDuring the Algerian War, Frantz Omar Fanon found his practice of treatment of native Algerians ineffective due to the continuation of the horror of a colonial war. He emphasized about the social origin of traumas, joined the liberation movement and urged oppressed people to purge themselves of their degrading traumas through their collective liberation struggle. He made the following remarks in his letter of resignation, as the Head of the Psychiatry Department at the Blida-Joinville Hospital in Algeria:\"If psychiatry is the medical technique that aims to enable man no longer to be a stranger to his environment, I owe it to myself to affirm that the Arab, permanently an alien in his own country, lives in a state of absolute depersonalization.\" (2) Inculcation of horror and anxiety, through widespread torture, massacre, genocide and similar coercive measures has happened frequently in human history. There are plenty of examples in our modern history. Tyrants have always used their technique of \"psychological artillery\" in an attempt to cause havoc and confusion in the minds of people and hypnotize them with intimidation and cynicism. The result is a collective trauma that will pass through generations. There is no magic formula of rehabilitation. Collective trauma can be alleviated through cohesive and collective efforts such as recognition, remembrance, solidarity, communal therapy and massive cooperation.\n\n\n"}
{"id": "42863112", "url": "https://en.wikipedia.org/wiki?curid=42863112", "title": "Commissioning support unit", "text": "Commissioning support unit\n\nCommissioning support units were established in April 2013 from the remains of the Primary Care Trusts and Strategic Health Authorities as part of the reorganisation of the National Health Service in England following the Health and Social Care Act 2012.\n\nTwenty five regional commissioning support units submitted outline business plans in 2012 to NHS England which hosted them. The commissioning support units were largely staffed by former employees of primary care trusts. They were intended to provide support to clinical commissioning groups by providing business intelligence, health and clinical procurement services and other back-office administrative functions, including contract management. The plan was to introduce competition into this market by making them independent businesses from 2016. By May 2014 there had been a number of amalgamations as some clinical commissioning groups brought their commissioning support services in-house.\n\nGreater East Midlands Commissioning Support Unit was criticised by Liz Kendall MP who said it should no longer run NHS continuing healthcare services for patients with complex needs and that it only had one part-time person monitoring the quality of all its home care providers. It announced plans for a merger with NHS Arden CSU in November 2014. Between them they provide specialist support to 37 Clinical commissioning groups. \n\nNorth of England Commissioning Support Unit has expanded some of their services to other areas of the country such as their business intelligence tool RAIDR, which is now deployed in over 20% of GP Practices In England. \n\nThe South, South West and Central Southern Commissioning Support Units announced in January 2015 that they planned to merge into a new organisation to be called South, Central and West Commissioning Support which will span Sussex, Cornwall, Gloucestershire and Oxfordshire. The CSUs in the North West and in Yorkshire were not admitted to the NHS procurement framework in February 2015 and NHS England announced that they had no future. Services in Greater Manchester were transferred to Greater Manchester Shared Services.\n\nOn 1 April 2017, NHS South East CSU formed a partnership with NHS North and East London CSU (NEL CSU). \n\nEight organisations were accredited to provide CCGs with support:\n\nin 2013/4 NHS England paid £125m for services from the 19 CSUs operational in that year. £602m of their income came from clinical commissioning groups, and £80m from “other sources” such as provider trusts and local councils.\n\nA tendering exercise for the services supplied by the Yorkshire and Humber commissioning support unit was delayed in September 2015 because of lack of interest by potential suppliers in the contract, valued at £20 million pa. It appeared that the CCGs were planning to take a lot of the work in house, as the existing contract amounted to £85 million pa.\n\nThe North of England Commissioning Support Unit is to be converted into a community interest company owned by the 11 clinical commissioning groups covering the North East and Cumbria.\n"}
{"id": "47677297", "url": "https://en.wikipedia.org/wiki?curid=47677297", "title": "Cosmetic surgery in Australia", "text": "Cosmetic surgery in Australia\n\nCosmetic surgery, also referred to as aesthetic surgery, is a surgical procedure which endeavours to improve the physical aspects of one's appearance to become more aesthetically pleasing. The continuously growing field of cosmetic surgery is closely linked with plastic surgery, the difference being, cosmetic surgery is an elective surgery with the sole purpose to enhance the physical features of one's appearance. Plastic surgery is performed in order to rectify defects to reinstate normality to function and appearance. Cosmetic surgical procedures are generally performed on healthy functioning body parts, with the procedure being optional not medically necessary. The inevitable aim of cosmetic surgery is to enhance one's image, encompassing reducing the signs of aging and/or correction of a believed deviation on one's body in turn it is surrounded by controversy. Although the implementation of cosmetic surgery within Australian society is growing, the trade has struggled to find its place within the Australian culture.\n\nThe word \"cosmetic\", originates from the Greek term Kosmetike, meaning the \"art of beautifying\".\n\nThe history of cosmetic surgery can be linked back to that of plastic surgery, as the debate persists, around the blurred lines of the two. Plastic surgery originated in 600 BC when Hindu surgeons performed rhinoplasty with the use of segments of cheek tissue.\n\nAt the end of the fifteenth century when syphilis was prevalent, came the introduction of debatable reconstructive surgery to rectify the ill shaped nose, a prominent feature of Syphilis sufferers. The sixteenth century saw an Italian by the name of Gaspare Tagliacozzi adopt the method of using upper arm tissue to reconstruct the nose during rhinoplasty, granting him the nickname 'the father of plastic surgery'. Although Tagliacozzi's approach left patients required to have their arm raised to their nose for several months, requiring numerous surgeries, with excessive Scaring.\n\nEngland was exposed to the Hindu techniques of rhinoplasty by a practitioner in 1815, who clearly defined the use for the surgery, limited to those who were physically affected by the horrors of Napoleonic Wars. Towards the end of the century in the 1880s John Orlando Roe, a New York surgeon, developed a technique which prevented scarring by operating from inside the nostrils.\n\nWorld War I was the most costly war to Australia in regards to fatality. The brutality sparked the generation of plastic surgery within Australia introduced by a man by the name of Harold Gillies. Gillies oversaw the development of the first unit to treat the returned battle scared veterans of the war. This led to the relocation of the Red Cross to the Queen Mary Hospital in Sidcup, England. The Queen Mary Hospital opened in 1917 was a six hundred bed hospital which focused solely on plastic surgery. It was here that Gillies trained not only Australian plastic surgeons but surgeons from all over the globe. The return of these surgeons to their home countries such as Australia, saw the spread of the plastic surgery trade across the globe.\n\nThe war gave the dishonoured trade a respected name through the treatment and resurrection of returned war veterans, with shattered physical traits. The century gave rise to anaesthetics and Antiseptics prompting an increase in the number of surgeries being performed. But with this heroic status and development of techniques came the taboo ideology cast upon cosmetic surgery as the trade filtered into the population, with civilians un-pleased with their aesthetic appearance undergoing surgery. In turn the need for secrecy arose as people felt the need to hide the truth about their surgical endeavours. From here a surgeon by the name of Henry Junius Schireson, acquired his license to practice throughout multiple states of America, who became known in 1923 when he performed rhinoplasty on a Jewish actress Fanny Brice in her New York apartment, giving birth to the booming trade of cosmetic surgery for everyday civilians.\n\nBenjamin Rank was an Australian trained by Gillies himself who in the 1940s governed the Royal Melbourne Hospital which was the first plastic surgery unit within Australia. It was in 1956 that plastic surgery was acknowledged by the Royal Australia College of Surgeons as a separate specialty trade of plastic surgery. Today, the Australian Society of Plastic Surgeons Inc. founded in 1917, commonly known as ASPS, was founded in 1970 with the aim to uphold the integrity of the plastic surgery field (inclusive of cosmetic and reconstructive surgery) within Australia. Today, the deliverance of the highest quality surgeries is at the forefront of their work. They govern the AMC accredited Surgical Education and Training (SET) Program within Australia.\n\nThe development of different techniques within the field of cosmetic surgery has led to the innovation of non-invasive methods. Nine percent of the Australian population have undergone a non-invasive form of cosmetic surgery. Numerous surgeries are now performed using these techniques as opposed to open surgery methods which have been used in the past. This adaptation has led to a reduction in cost, time, scarring and pain involved with these procedures. Through the development of aiding surgical instruments such as a viewing scope or Lasers (see below), this shift has been made possible, reducing the incision site resulting in a faster recovery time for patients. Examples of non-invasive surgeries are as muscle relaxants, such as Botox or Dysport.\n\nAnother form of laser treatment is intense pulsed light (IPL). IPL differs from laser treatment as unlike laser treatments, IPL will perform multiple treatments at once but only a few are capable of doing so with the same potency as a laser. IPL is predominantly used for mild skin issues in comparison to laser being used for the more extreme cases.\n\nAlso known as breast augmentation, Augmentation Mammaplasty involves the use of breast implants or the transfer of fat tissue to restore and create volume or improve symmetry of the breast. The implants used are most commonly composed of silicone which are inserted underneath the skin/muscle through a small incision. Breast Augmentation frequently is performed in conjunction with mastopexy.\n\nMastopexy common known as a breast lift, involves the reshaping of the surrounding skin and tissue of the breast to produce a more youthful looking breast in appearance. The removal of excess skin along with tightening of tissue results in enhancing the shape of the breast, raising the breast to sit higher. The causes of loss of skin elasticity are vast being a result of breast feeding, pregnancy, aging, gravity, genetics and fluctuation in weight. The areola of the breast, (pigmented ring of skin which sits around the nipple), may stretch over a life span, which can be reduced during mastopexy.\n\nAlso known as a breast reduction, is the removal of breast fat, glandular tissue and skin, resulting in a reduction in the size and weight of the breasts. This type of surgery is frequently used by women who experience discomfort such as back pain due to the in-proportion between body size and breast size and shape.\n\nA common term for brachioplasty is an arm lift. This is the reshaping of the dorsal portion of the arm spanning from the upper arm to the elbow. This procedure will reduce the downward sagging of the skin around the area of the triceps, tightening the skin. The supportive tissue which emphasises the shape of the arm is remodelled, tightening and smoothing its appearance via the removal of pockets of fat.\n\nLipoplasty also referred to as liposuction is the removal of deposits of fat around specific areas of the body with the goal to enhance the physical contours of the body. Fat can be removed from selected area reducing prominent sections in which the patient finds unpleasing. Areas which are commonly focused on by patients are the Thighs, Hips, arms, abdomen, buttocks, inner knee, back, calves, face and chest. This form of surgery is commonly performed alongside other surgical procedures such as abdominoplasty.\n\nBody contouring is a common procedure in both men and women. The surgery is prominent in those who have undergone significant weight loss resulting in excess sagging skin being present around areas of the body. Patients who have lost a large portion of excess weight may experience issues with elasticity of the skin once the fat has been lost. The skin losses elasticity (a condition called elastosis) once it has been stretched past capacity and is unable to recoil back to its standard position against the body and also with age. Body contouring is the removal of this excess skin and fat from numerous areas of the body, restoring the appearance of skin elasticity of the remaining skin, enhancing the contouring an tone of the body.\n\nAlso known as a tummy tuck, this form of surgery removes the excess skin and fat from the abdominal area. Commonly the surgery will also restore the muscles which have been weakened and/or separated by carrying excess weight for long periods of time.\n\nCommon in women due to the hairless trend within society, numerous women seek medical treatment to remove visible 'spider veins' (Telangiectasia), which appear on the surface of the skin. The small blood vessels spread across the skin which are cause by stress to the vessel itself. They can vary in colour, red, blue or purple and also vary in size. They are common on the legs, thighs and ankles. These veins can be caused by prolonged sitting, pregnancy, hereditary, changes in weigh and also due to hormone fluctuation. The vessel is injected with a solution via a needle which causes the vein to collapse and begin to fade, becoming less prominent in colour on the skin.\n\nButtock augmentation increases the size and prominence of the buttock muscles. This look it created using either the grafting of fat, implants or both, to boost the appearance of the muscles. This fills the buttock giving it a rounded smooth appearance and is also said to help improve posture and balance.\n\nGynecomastia surgery is a breast reduction surgery for men. The condition Gynecomastia has numerous causes to the growth of breasts in men including obesity, a hormone imbalance, genetic dispositions or the side effect of a medication. This condition is not painful although some suggest there are mental side effects of the condition, hence the availability of surgical correction.\n\nDermal fillers are injected below the skin to give a more fuller, youthful appearance of a feature or section of the face. These fillers deplete signs of aging such as wrinkles by filling and stretching the skin. One type of dermal filler is Hyaluronic acid. Hyaluronic acid is naturally found throughout the human body. It plays a vital role in moving Nutrients to the cells of the skin from the blood. It is also commonly used in patients suffering from Arthritis as it acts like a cushion to the bones which have depleted the articular cartilage casing. Development within this field has occurred over time with synthetic forms of hyaluronic acid is being created, playing roles in other forms of cosmetic surgery such as facial augmentation.\n\nRhinoplasty is surgery of the nose, reshaping the structure of the facial feature. This surgery can be used by people for improving the look of their nose or by people who suffer from breathing problems as a result of the structure of their nose obscuring the airways. Frequent changes people desire are to the shape of the Nostrils, size of the bridge, correction of a deviated septum, width or length of the nose, facial symmetry, depression or prominent bumps of the nose.\n\nImplants can be used in order to enhance features of the face. They are used to perfect symmetry and make the appearance of features more prominent such as the cheek, chin and jaw.\n\nA common form of cosmetic surgery is Rhytidectomy which is known as a face lift. Patients undergo a face lift to reduce the signs of aging. Sagging skin, wrinkles and loss of skin tones are a few main signs of aging which a face lift can rectify. Numerous factors can enhance the speed of aging such as exposure to the sun, hereditary, gravitational pull and medical conditions.\n\nMicropigmentation is the creation of permanent makeup using natural pigments to places such as the eyes to create the effect of eye shadow, lips creating lipstick and cheek bones to create a blush like look. The pigment is inserted beneath the skin using a machine which injects a small needle at a very fast rate carrying pigment into the skin, creating a lasting colouration of the desired area.\n\nEar Surgery is performed in order to improve the look of the ear. Common reasons one feels they require the surgery can be due to dis-figuration caused by injury, birth defects, irregular shape, prominent position of the ear, irregular size being either too small or too large (oversized ears is a condition known as macrotia) in proportion with the head.\n\nMentolasty is surgery to the chin. This can involve either enhancing or reducing the size of the chin. Enhancements are achieved with the use of facial implants. Reduction of the chin involved reducing the size of the chin bone.\n\nDue to cosmetic surgery being an elective surgical procedure it is of common assumption that the risk factors around these surgeries are lower than that of other surgeries. The invasive procedures have numerous risk factors as it is still a medical procedure, all of which come with a level of risk. The non-invasive treatments also come with a level of risk although lower than invasive methods. This common impression can be linked to the controversies surrounding the trade as critics struggle to see the link between the benefits and the risk facts. Surgeries are generally linked to, although not restricted to, risk factors effecting the area in which the surgery is performed. Some of the common risks are the development of a hematoma, organ damage, deep vein thrombosis, seroma, excessive bleeding, swelling, bruising, ectropion (optical), blindness (optical), obstruction of airways (nasal), loss of sensation, excessive scarring (including of keloid scars), a shift in position of hair line effecting symmetry and nerve damage.\n\nMost surgeons will suggest to patients electing to undergo cosmetic surgery to cease smoking for a period before and after their alterations. Generally a period of four weeks pre-operative and post operative, to aid in the recovery time and the healing of the wound. Just like other surgeries, cosmetic surgery may require incisions to be made to the skin, in one or more places of the body. These wounds will be required to heal post operation, therefore leaving the patient at risk of poor wound healing which may be due to numerous causes such as infection requiring antibiotics.\n\nMedical conditions can impact on the level of risk involved with cosmetic surgery as there can be underlying effects caused by different medications. For example, blood thinning medication can cause excessive bleeding due to the bloods ability to clot being lowered by the medication. All medications prescribed to a patient are noted and discussed by the surgeons to reduce the chance of issues arising.\n\nThe most common reason behind one's choice to undergo cosmetic surgery is due to dissatisfaction with their body image. Body image issues are commonly allied with lower levels of self esteem and psychological well being. These issues are the cause of many women turning to cosmetic surgery. In today's world viewers are flooded with images and advertisements, showing generally, naturally unobtainable faces and bodies. There is a growing trend of reality television shows broadcasting makeovers of ordinary civilians undergoing cosmetic surgery to enhance their aesthetic image. Viewpoints grow around the link between the climbing figures of cosmetic surgery and the constantly changing world of media. There is a constant stream of connection developed via social media outlets such as Facebook and Instagram which hold a high level of importance within people lives. Other viewpoints circle the growth of public awareness of the topic is increasing becoming a direct link to this growth spurt in popularity of surgery. Many of the patients who undergo cosmetic surgery have been found to have low levels of self-esteem and use cosmetic surgery to rectify the issues they have with their body image. Numerous studies have focused on the final outcome of cosmetic procedures and the level of satisfaction which patients have with their results showing a large portion of patients look for additional surgeries to rectify the issue. Cosmetic surgery in a lot of cases will enhances the problems patients have with their self-esteem issues instead of depressing them as first desired. Studies show that patients who undergo cosmetic surgery who have been made aware of the risks involved along with the technical side of the procedure, have higher levels of satisfaction with their outcome post-surgery.\n\nBody dysmorphic disorder (BDD) is a condition of which people find their own image immensely flawed. It effects both men and women equally, with the average onset age of thirteen years of age. The condition is treatable although there is an alarmingly high rate of suicide within BDD sufferers, one in every three hundred and thirty diagnosed will end their own lives. Patients of BDD will commonly turn to cosmetic surgery to rectify these flaws resulting in unsatisfactory results due to their condition.\n\nHistrionic personality disorder can be categorised as a person who thrives on being the center of attention in all social settings. They show signs of attention seeking behaviour along with instability emotionally. Immense discomfort is felt by the individual when they do not feel they are the epicenter of a group or one on one environment and often struggle when it comes to relationships with others, being both on a level of friendship and sexual relationships. Suggestions have been made in regards to the need for screening before cosmetic surgeries are performed on patients who suffer from disorders such as histrionic personality disorder as the level of satisfaction which is felt by individuals post-surgery is low, triggering the desire for additional treatments. The surgery impacts on their self-esteem leaving them higher levels of distressed post-op.\n\nThe controversies surrounding cosmetic surgery are plentiful. The stigma that exists around the practice has been evident since its introduction into the modern world. The taboo stigma around these types of surgery is beginning to fade as we see the trend of cosmetic surgery growing. \n\nThe age limitations around cosmetic surgery within Australia is eighteen years of age until one is eligible to opt to undergo cosmetic surgery. There is numerous discussions occurring around the use of cosmetic surgery within children which have undergone extensive trauma due to a catastrophic life event or birth defects, in turn patients seek cosmetic surgery to rectify the issue. The debate around whether this is ethically correct exists with the viewpoint that this is unethical and the surgeries are for the purpose of vanity. Other sides of the debate argue for the lasting impacts on the children during adolescent years due to the dis-figuration causing prejudice and view the surgical procedures will attribute to the child's mental health in later years.\n\nIn 2011, concerns were expressed as information arose in regards to the quality of the material being used in silicone breast implants. Industrial grade silicone was being used in the replacement of medical grade supplies by a French firm by the name of Poly Implant Prothese (PIP). The silicone being used was found to be suitable for uses within the production of mattresses. In 2011, it was reported that in twelve years of production, more than 300,000 PIP implants were sold globally. In the case of rupture the dispute around safety risks arose with parties debating the increase cancer risks due to the poor grade materials. To date there has been no scientific evidence proving the implants should cause safety concerns linking to toxicity or cancer. The debate surrounds whether the removal of the implants is required due to safety concerns around rupture and toxicity.\nConcerns are sparked within Australia around the level of regulation which exists around the implementation of laser and IPL treatments. The practice has little federal regulated with inconsistencies existing between the states within Australia. It is largely debated around the level of regulation in comparison to the heavy controls placed around the use of schedule 4 drugs such as Botox or Dysport. Many rally for the investigation into the uses of these techniques, claiming long term damage can be a result of mistreatment. The authority involved in the monitoring these regulations within Australia is the Department of Health, the Therapeutic Goods Administration.\n\nA growing trend all over the world is the implementation of cosmetic surgery within the field of veterinary science. Pet owners are spending thousands on rectifying issues which are present in their pets appearance. Some of the surgical procedures being performed are teeth straightening methods, face lifts to reduce wrinkles, eyebrow corrections, botox, implants (facial and testicular) and excess skin removal from specific parts of the body. Many of these treatments although not life-threatening are seen to some as enhancing the animals quality of life, therefore warrant surgical correction. In the realms of animal showing such as Show dogs, cosmetic correction is common to ensure the animal conform to the strict guidelines of breed specific characteristics. Those who support the use of cosmetic surgery within pets argue the corrections enhance the pets self-esteem thus quality of life. Those that oppose the trade argue the surgical correction for aesthetics is in line with animal cruelty and it is for the benefit of the owners vanity in opposition to the animals welfare. Only a select few veterinary surgeons will perform these types of procedures if a direct enhancement to the pets quality of life is not evident.\n\n\n"}
{"id": "18964621", "url": "https://en.wikipedia.org/wiki?curid=18964621", "title": "Cultural competency training", "text": "Cultural competency training\n\nCultural Competency Training is an instruction to achieve cultural competence and the ability to appreciate and interpret accurately other cultures.\n\nCultural competence refers to an ability to interact effectively with people of different cultures. Cultural competence comprises four components: (a) awareness of one's own cultural worldview, (b) attitude towards cultural differences, (c) knowledge of different cultural practices and worldviews, and (d) cross-cultural skills. Developing cultural competence results in an ability to understand, communicate with, and effectively interact with people across cultures and leads to a 15% decrease in miscommunication. Cultural Competency has a fundamental importance in every aspect of a work field and that includes school and government setting. With the amalgamation of different cultures in American society, it has become imperative for teachers and government employees to have some form of cultural competency training.\n\nTo cater to an increasingly globalized society, many hospitals, organizations, and employers may choose to implement forms of cultural competency training methods to enhance transparency between language, values, beliefs, and cultural differences. Training in cultural competence often includes careful consideration of how best to approach people's various forms of diversity. This new found awareness oftentimes allows military members, educators, medical practitioners, workers, and common citizens to establish equity in their environments and enhances interrelationships between one another for increased productivity levels. There have been numerous developed theories as to how best to conduct cultural competency training, which oftentimes is dependent on the specific environment and type of work.\n\nWhen defining the ideas that surround cultural competence training, defining what culture is can help to understand the ideas that shape the concept. Culture is defined as the set of shared attitudes, values, goals, and practices that characterizes an institution or organization. When looking at culture in terms of cultural competence training, certain groups of individuals should be focused on because of their relevance to society. There are many groups that are marginalized and underrepresented; however, four specific areas to look at are:\n\nLGBTQIAP community, race, and religion. These areas, along with others, represent concepts that make up one’s identity. The approach to identity helps to shape the ideas and themes that go into cultural competence training.\nThe acronym LGBTQIAP stands for Lesbian, Gay, Bisexual, Transsexual, Transgender, Queer, Questioning, Intersex, Asexual, Ally, and Pansexual. This particular group of individuals has faced numerous obstacles and has historical events to highlight the inequalities they face such as the Stonewall riots. The Stonewall riots became a symbol for the gay liberation movement when police attempted a raid at the Stonewall Inn bar to arrest the gay and lesbian patrons and the gay community fought back. Numerous systemic oppressions historically and currently target LGBT individuals. Cultural competence training helps professionals develop knowledge and skills on how to address issues and be more aware on the type of language that is politically correct.\n\nRace is a sensitive aspect of cultural competency training that requires professionals to be able to identify, acknowledge and value cultural differences. Training on this aspect of cultural competence teaches professionals that to ignore racial differences, is a form of microaggression that can help exacerbate racial inequalities. In order to begin to understand intercultural communications one must understand the historical and social context under which different cultural groups operate. For example, the history related to the cultural genocide of indigenous peoples in North America, understanding the said group’s value system, their ways of learning, and logic is essential in being able to understand how certain aspects of their culture may be similar or different from our own. Such distinction must be approached with respect and without ascribing superiority or inferiority to the difference.\n\nReligious differences can play a role in how professionals interact and communicate with others. Religiosity refers to the nature and extent of public and private religious activity, including belief in God, prayer, and place of worship attendance. Religiosity is usually linked to formal religious traditions (such as Christianity), institutions (such as mosques), sacred texts (such as The Book of Mormon), and a definitive moral code (such as the Decalogue). Spirituality can be an important part of religion but can also exist independent of extant faith traditions, involving a variety of more individual subjective beliefs and activities related to the sacred. In this aspect of cultural competence training professionals should learn how to have religious competence. Religious competence refers to skills, practices, and orientations that recognize, explore, and harness patient religiosity to facilitate diagnosis, recovery, and healing. Religious competence involves the learning and deployment of generic competencies, including active listening and a nonjudgmental stance. It is also an overarching orientation, providing a safe place for discussion of religious issues and identities received in a humble, respectful, and empathetic manner.\n\nin terms of nationality, particularly for people who are immigrants, the recent increase in global migration make them an increasingly common demographic everywhere. Though they will have varying cultures as well. In this aspect, it is important for those who are trained to understand both similarities and differences between them, and the individual they are helping. With this knowledge, it makes the process of aiding the individual more efficient, and successful. Both the past Nation the individual has come from, and their journey of immigration as an experience, can shape their mentality. To have specialists with specific nationalities help explain some differences is a helpful strategy. \n\nSchool is considered to be the second learning home for kids. Every year a large number of people come to the United States. These groups of people are often families, including small children. In today’s world, cultural competency plays a very vital role in shaping the kids future. In the United States, there is an underlying difference among parents as to how a kid should be raised, but it is clear that cultural competency should be taught at a young age. The United States is not the front runner in cultural competency training amongst children, as Canada and Australia are seemingly far more progressive in this sector. Cultural competency training can be a huge help for the families who are thinking of adopting a foster child, specifically, if that child was born outside of United States. A school is a mixture of different races and cultures and as an educator, one must be sensitive to everyone’s needs. Different cultures act uniquely to the different situations, and as an educator, one has to not only value diversity, but also have a strategy for everyone to feel welcomed.\n\nOver the years, there have been new developed ways of practicing cultural competency in the workforce. There are many different methods that would allow assistance in cultural competency such as: Global leadership programs, international team building exercises and specific cross-cultural skills training for special executive positions. Having a good grasp on the many different cultures that exist is increasingly becoming a major principle in the workforce. The techniques for cultural competency training must be practiced more than just in class room lecture. Trainers must be extremely educated in this matter to be able to sufficiently train people. They must take notice of their own biases perspective and about the different types cultures that receive discrimination.\n\nIn the medical setting, effective communication between clinicians, patients, families and other health care providers is fundamental.\n\nHealth disparities refer to gaps in the quality of health and health care across racial, ethnic, and socioeconomic groups. Studies have demonstrated the multiple factors that contribute to health disparities.\n\nCultural Competence Online for Medical Practice (CCOMP) is an attempt in the United States to address one of the factors - the patient-doctor interaction. The CCOMP project is funded by a grant from the National Institutes of Health (NIH) through the National Heart Lung and Blood Institute (NHLBI). CCOMP offers a clinician's guide to reduce cardiovascular disparities, intended to create effective cross-cultural approaches to care for African-American patients with cardiovascular disease, especially hypertension. Videos with real patient scenarios and case-based modules are aimed at developing this increased awareness.\n\n\n"}
{"id": "46821116", "url": "https://en.wikipedia.org/wiki?curid=46821116", "title": "Dipak Kalra", "text": "Dipak Kalra\n\nProfessor Dipak Kalra (born 18 July 1959, London, UK) is President of the European Institute for Health Records and of the European Institute for Innovation through Health Data. He undertakes international research and standards development, and advises on adoption strategies, relating to Electronic Health Records.\n\nDipak Kalra studied medicine at Guy’s Hospital in London, and specialized in General Practice. He is a Fellow of the Royal College of General Practitioners. He worked as a London GP for a decade before specializing in Health Informatics. He obtained a PhD in Health Informatics in 2003 and is a Fellow of the British Computer Society.\n\nProfessor Dipak Kalra, PhD, FRCGP, FBCS, plays a leading international role in research and development of Electronic Health Record architectures and systems, including the requirements and models needed to ensure the robust long-term preservation of clinical meaning and protection of privacy. He leads the development of CEN (European Committee for Standardization) and ISO (International Organization for Standardization) standards on EHR interoperability, personal health records, EHR requirements, and has contributed to several EHR security and confidentiality standards.\n\nHe has led multiple European projects in these areas, including Horizon 2020 and the IMI programme alongside pharma companies, hospitals and ICT companies. He recently co-led a €16m project on the re-use of EHR information for clinical research, EHR4CR, alongside ten global pharma. He is a partner in another IMI project, EMIF, on the development of a European clinical research platform federating multiple population health and cohort studies. Dipak also led an EU Network of Excellence on semantic interoperability, and is a partner in other EU projects on the sustainability of interoperability assets, the transatlantic sharing of patient summaries and quality labelling.\n\nDipak is President of the European Institute for Innovation through Health Data (www.i-HD.eu), which seeks to drive best practices in the trustworthy use of high quality and interoperable health data by all stakeholders, for optimising health and knowledge discovery. He is also President of the European Institute for Health Records (EuroRec), which is the coordinator or a partner in many EC projects on electronic health record quality and systems accreditation, interoperability and the uses of health data for research. EuroRec leads a network of national ProRec Centres which promote good quality EHR system adoption across Europe.\n\nDipak Kalra is Professor of Health Informatics at University College London and Visiting Professor of Health Informatics at the University of Gent.\n\nDipak is a member of multiple standards bodies including BSI Group, CEN, ISO and HL7-UK (International HL7 Implementations).\n\nDipak was a founding Director of the openEHR Foundation, a not-for-profit company which exists to promote and publish, via the Web, the formal specification of requirements for electronic health record information, supporting development of open specifications for health information systems.\n\nDipak’s innovations in EHR architectures have been spun out into a company: Helicon Health, providing cardiovascular chronic disease management services across London.\n\n\nThis project, sponsored through the Innovative Medicines Initiative, comprises almost 50 academic partners and a dozen Pharma partners, developing a generic platform to provide harmonised views across multiple population health (cohort) data sets across Europe and geographically proximal EHR systems. The two initial clinical research areas are dementia and metabolic disorders. Dipak leads workpackages on semantic interoperability and ethics, and is a member of the business modeling task force.\n\nThis public-private research project, involving 35 partners from academia and 10 Pharma companies. EHR4CR is developing a platform to support remote querying of hospital electronic health records in order to enable more efficient feasibility assessment, recruitment and conduct of clinical trials. Dipak leads the Managing Entity and co-leads two work packages on requirements and on sustainable business models. The project is now spinning out a commercial platform for European scale-up to be run by Custodix, and a not-for-profit institute that Dipak will lead: the European Institute for Innovation through Health Data.\n\nVALUeHEALTH is establishing how eHealth interoperability can create, deliver, and capture value for all stakeholders. It will develop an evidence-based business plan for self-funding priority pan-European eHealth Services beyond 2020. It will examine the maturity of existing standards and infrastructures, propose organisational changes and incentives, and perform state-of-the-art Cost Benefit Assessments, and from this produce a definitive Business Plan and Strategy for taking forward public-private investment in digital eHealth services. Dipak is the co-ordinator of VALUeHEALTH.\n\neStandards brings together the leading Standard Developing Organizations in Europe, supported by the eHealth Network and EuroRec. It will develop an evidence-based roadmap for eHealth standards alignment that is endorsed by SDOs, the eHealth Network, and key stakeholders. It will contribute to the European eHealth Interoperability Framework, focusing on clinical content modelling for different paradigms and embedding a quality management system for interoperability testing and certification of eHealth systems. Dipak leads tasks on multi-stakeholder engagement and the development of good practice in clinical information modelling.\n\nASSESS CT will contribute to better semantic interoperability of eHealth services in Europe, in order to optimise care and to minimise harm in delivery of care. The ASSESS CT project, integrating a broad range of stakeholders, will investigate the fitness of the international clinical terminology SNOMED CT as a potential standard for EU-wide eHealth deployments. It will investigate Member State reasons for adoption/non adoption of SNOMED CT, lessons learned, success factors and the impact of SNOMED CT adoption from a socio-economic viewpoint. Dipak leads the workpackage to define policy guidance and make the final recommendations of the project.\n\nSemantic interoperability of EHR systems is a vital prerequisite for enabling patient-centred care and advanced clinical and biomedical research. SemanticHealthNet will develop a scalable and sustainable pan-European organisational and governance process to achieve this objective across healthcare systems and institutions. The consortium comprises 17 Partners and more than 40 internationally recognised experts, including from United States and Canada, ensuring a global impact. Dipak is the project lead.\n\nThis project will design a development and adoption roadmap for sharing patient summaries between the US and the EU, with partners from both sides of the Atlantic. Trillium Bridge supports the Transatlantic eHealth/health IT Cooperation Memorandum of Understanding and Roadmap and the Digital Agenda for Europe in achieving a triple win for eHealth by establishing the foundations of an interoperability bridge to meaningfully exchange patient summaries and electronic health records among the EU and US.\n\nEXPAND - Expanding Health Data Interoperability Services – is a Thematic Network (TN) EC project to progress towards an environment of sustainable cross border eHealth services established at EU level by the Connecting Europe Facility (CEF) and at national level through the deployment of suitable national infrastructures and services.\n\n\nISO International standards (development led by Dipak)\n\n"}
{"id": "9828265", "url": "https://en.wikipedia.org/wiki?curid=9828265", "title": "Doctor–patient relationship", "text": "Doctor–patient relationship\n\nThe doctor–patient relationship is a central part of health care and the practice of medicine. The doctor–patient relationship forms one of the foundations of contemporary medical ethics.\n\nA patient must have confidence in the competence of their physician and must feel that they can confide in him or her. For most physicians, the establishment of good rapport with a patient is important. Some medical specialties, such as psychiatry and family medicine, emphasize the physician–patient relationship more than others, such as pathology or radiology, which have very little contact with patients.\n\nThe quality of the patient–physician relationship is important to both parties. The doctor and patient's values and perspectives about disease, life, and time available play a role in building up this relationship. A strong relationship between the doctor and patient will lead to frequent, quality information about the patient's disease and better health care for the patient and their family. Enhancing the accuracy of the diagnosis and increasing the patient's knowledge about the disease all come with a good relationship between the doctor and the patient. Where such a relationship is poor the physician's ability to make a full assessment is compromised and the patient is more likely to distrust the diagnosis and proposed treatment, causing decreased compliance to actually follow the medical advice which results in bad health outcomes. In these circumstances and also in cases where there is genuine divergence of medical opinions, a \"second opinion\" from another physician may be sought or the patient may choose to go to another physician that they trust more. Additionally, the benefits of any placebo effect are also based upon the patient's subjective assessment (conscious or unconscious) of the physician's credibility and skills.\n\nMichael and Enid Balint together pioneered the study of the physician patient relationship in the UK. Michael Balint's \"The Doctor, His Patient and the Illness\" (1957) outlined several case histories in detail and became a seminal text. Their work is continued by the Balint Society, The International Balint Federation and other national Balint societies in other countries. It is one of the most influential works on the topic of doctor-patient relationships. In addition, a Canadian physician known as Sir William Osler was known as one of the \"Big Four\" professors at the time that the Johns Hopkins Hospital was first founded. At the Johns Hopkins Hospital, Osler had invented the world's first medical residency system. In terms of efficacy (i.e. the outcome of treatment), the doctor–patient relationship seems to have a \"small, but statistically significant impact on healthcare outcomes\". However, due to a relatively small sample size and a minimally effective test, researchers concluded additional research on this topic is necessary. Recognizing that patients receive the best care when they work in partnership with doctors, the UK General Medical Council issued guidance for patients \"What to expect from your doctor\" in April 2013.\n\nThe following aspects of the doctor–patient relationship are the subject of commentary and discussion.\n\nThe default medical practice for showing respect to patients and their families is for the doctor to be truthful in informing the patient of their health and to be direct in asking for the patient's consent before giving treatment. Historically in many cultures there has been a shift from paternalism, the view that the \"doctor always knows best,\" to the idea that patients must have a choice in the provision of their care and be given the right to provide informed consent to medical procedures. There can be issues with how to handle informed consent in a doctor–patient relationship; for instance, with patients who do not want to know the truth about their condition. Furthermore, there are ethical concerns regarding the use of placebo. Does giving a sugar pill lead to an undermining of trust between doctor and patient? Is deceiving a patient for his or her own good compatible with a respectful and consent-based doctor–patient relationship? These types of questions come up frequently in the healthcare system and the answers to all of these questions are usually far from clear but should be informed by medical ethics.\n\nShared decision making is the idea that as a patient gives informed consent to treatment, that patient also is given an opportunity to choose among the treatment options provided by the physician that is responsible for their healthcare. This means the doctor does not recommend what the patient should do, rather the patient's autonomy is respected and they choose what medical treatment they want to have done. A practice which is an alternative to this is for the doctor to make a person's health decisions without considering that person's treatment goals or having that person's input into the decision-making process is grossly unethical and against the idea of personal autonomy and freedom.\n\nThe spectrum of a physician’s inclusion of a patient into treatment decisions is well represented in Ulrich Beck’s \"World at Risk\". At one end of this spectrum is Beck’s Negotiated Approach to risk communication, in which the communicator maintains an open dialogue with the patient and settles on a compromise on which both patient and physician agree. A majority of physicians employ a variation of this communication model to some degree, as it is only with this technique that a doctor can maintain the open cooperation of his or her patient. At the opposite end of this spectrum is the Technocratic Approach to risk communication, in which the physician exerts authoritarian control over the patient’s treatment and pushes the patient to accept the treatment plan with which they are presented in a paternalistic manner. This communication model places the physician in a position of omniscience and omnipotence over the patient and leaves little room for patient contribution to a treatment plan.\n\nThe physician may be viewed as superior to the patient simply because physicians tend to use big words and concepts to put him or herself in a position above the patient. The physician–patient relationship is also complicated by the patient's suffering (\"patient\" derives from the Latin \"patior\", \"suffer\") and limited ability to relieve it on his or her own, potentially resulting in a state of desperation and dependency on the physician. A physician should be aware of these disparities in order to establish a good rapport and optimize communication with the patient. Additionally, having a clear perception of these disparities can go a long way to helping the patient in the future treatment. It may be further beneficial for the doctor–patient relationship to have a form of shared care with patient empowerment to take a major degree of responsibility for her or his care.\n\nThose who go to a doctor typically do not know exact medical reasons of why they are there, which is why they go to a doctor in the first place. For a patient to not be able to understand what is going on with their body, because they can’t understand lab results or their doctor isn’t sharing or explaining them, can be a frightening and frustrating situation to be in. An in depth discussion of lab results and the certainty that the patient can understand them may lead to the patient feeling reassured, and with that may bring positive outcomes in the physician-patient relationship.\n\nA dilemma may arise in situations where determining the most efficient treatment, or encountering avoidance of treatment, creates a disagreement between the physician and the patient, for any number of reasons. In such cases, the physician needs strategies for presenting unfavorable treatment options or unwelcome information in a way that minimizes strain on the doctor–patient relationship while benefiting the patient's overall physical health and best interests. When the patient either can not or will not do what the physician knows is the correct course of treatment, the patient becomes non-adherent. Adherence management coaching becomes necessary to provide positive reinforcement of unpleasant options.\n\nFor example, according to a Scottish study, patients want to be addressed by their first name more often than is currently the case. In this study, most of the patients either liked (223) or did not mind (175) being called by their first names. Only 77 individuals disliked being called by their first name, most of whom were aged over 65. On the other hand, most patients do not want to call the doctor by his or her first name.\n\nSome familiarity with the doctor generally makes it easier for patients to talk about intimate issues such as sexual subjects, but for some patients, a very high degree of familiarity may make the patient reluctant to reveal such intimate issues.\n\nTransitions of patients between health care practitioners may decrease the quality of care in the time it takes to reestablish proper doctor–patient relationships. Generally, the doctor–patient relationship is facilitated by continuity of care in regard to attending personnel. Special strategies of integrated care may be required where multiple health care providers are involved, including \"horizontal integration\" (linking similar levels of care, e.g. multiprofessional teams) and \"vertical integration\" (linking different levels of care, e.g. primary, secondary and tertiary care).\n\nThe process of turn-taking between health care professionals and the patients has a profound impact on the relationship between them. In most scenarios, a doctor will walk into the room in which the patient is being held and will ask a variety of questions involving the patient's history, examination, and diagnosis. These are often the foundation of the relationship between the doctor and the patient as this interaction tends to be the first they have together. This can go a long way into impacting the future of the relationship throughout the patient's care. All speech acts between individuals seek to accomplish the same goal, sharing and exchanging information and meeting each participants conversational goals.\n\nResearch carried out in medical scenarios analyzed 188 situations in which an interruption occurred between a physician and a patient. Of these 188 analyzed situations, research found that the doctor is much more likely (67% of the time, 126 occasions) as compared to the patient (33% of the time, 62 occasions). This shows that physicians are practicing a form of conversational dominance in which they see themselves as far superior to the patient in terms of importance and knowledge and therefore dominate all aspects of the conversation. A question that comes to mind considering this is if interruptions hinder or improve the condition of the patient. Constant interruptions from the patient whilst the doctor is discussing treatment options and diagnoses can be detrimental or lead to less effective efforts in patient treatment. This is extremely important to take note of as it is something that can be addressed in quite a simple manner. This research conducted on doctor-patient interruptions also indicates that males are much more likely to interject out of turn in a conversation then women. This can also lead to problems if a female physician is trying to convey messages as females statistically tend to interrupt in conversations less often than their male counterparts.\n\nAn example of where other people present in a doctor–patient encounter may influence their communication is one or more parents present at a minor's visit to a doctor. These may provide psychological support for the patient, but in some cases it may compromise the doctor–patient confidentiality and inhibit the patient from disclosing uncomfortable or intimate subjects.\n\nWhen visiting a health provider about sexual issues, having both partners of a couple present is often necessary, and is typically a good thing, but may also prevent the disclosure of certain subjects, and, according to one report, increases the stress level.\n\nHaving family around when dealing with difficult medical circumstances or treatments can also lead to complications. Family members, in addition to the patient needing treatment may disagree on the treatment needing to be done. This can lead to tension and discomfort for the patient and the doctor, putting further strain on the relationship.\n\nA good bedside manner is typically one that reassures and comforts the patient while remaining honest about a diagnosis. Vocal tones, body language, openness, presence, honesty, and concealment of attitude may all affect bedside manner. Poor bedside manner leaves the patient feeling unsatisfied, worried, frightened, or alone. Bedside manner becomes difficult when a healthcare professional must explain an unfavorable diagnosis to the patient, while keeping the patient from being alarmed.\n\nDr. Rita Charon launched the narrative medicine movement in 2001 with an article in the Journal of the American Medical Association. In the article she claimed that better understanding the patient's narrative could lead to better medical care.\n\nResearchers and Ph.D.s in a \"BMC Medical Education\" journal conducted a recent study that resulted in five key conclusions about the needs of patients from their health care providers. First, patients want their providers to provide reassurance. Second, patients feel anxious asking their providers questions; they want their providers to tell them it’s ok to ask questions. Third, patients want to see their lab results and for the doctor to explain what they mean. Fourth, patients simply do not want to feel judged by their providers. And fifth, patients want to be participants in medical decision-making; they want providers to ask them what they want.\n\nAn example of how body language affects patient perception of care is that the time spent with the patient in the emergency department is perceived as longer if the doctor sits down during the encounter.\n\n\nThe behavior of the patient affects the doctor–patient relationship. Rude or aggressive behavior from patients or their family members can also distract healthcare professionals and cause them to be less effective or to make mistakes during a medical procedure. When dealing with situations in any healthcare setting, there is stress on the medical staff to do their job effectively. Whilst many factors can affect how their job gets done, rude patients and unappealing attitudes can play a big role. Research carried out by Dr. Pete Hamburger, associate dean for research at Tel Aviv University, evidences this fact. His research showed that rude and harsh attitudes shown toward the medical staff reduced their ability to effectively carry out some of their simpler and more procedural tasks. This is important because if the medical staff are not performing sufficiently in what should be simple tasks, their ability to work effectively in critical conditions will also be impaired.While it is completely understandable that patients are going through an extremely tough time compounded by stress from other external and internal factors, it is important for the doctors and medical staff to be wary of the rude attitudes that may come their way.\n\n\n"}
{"id": "23742969", "url": "https://en.wikipedia.org/wiki?curid=23742969", "title": "Eric Goosby", "text": "Eric Goosby\n\nEric Goosby (born 1952) serves as the UN Special Envoy on Tuberculosis, and has previously served as the United States Global AIDS Coordinator from 2009 until mid-November 2013. As US Global AIDS coordinator, Goosby directed the U.S. strategy for addressing HIV around the world and led President Obama's implementation of the President's Emergency Plan for AIDS Relief (PEPFAR). Goosby was sworn in in June 2009 and resigned in November 2013, taking a position as a professor at UCSF, where he directs the UCSF-UC Berkeley Institute of Global Health Delivery & Diplomacy.\n\nGoosby received his MD (1978) from the University of California, San Francisco, where he also completed his residency (1981). Goosby then completed a two-year Kaiser Fellowship at UCSF in General Internal Medicine with a subspecialty in Infectious Diseases.(1983).\n\nGoosby has over 25 years experience working in the field of HIV/AIDS. Goosby treated patients at San Francisco General Hospital when HIV/AIDS first began to emerge and take its toll in the early 1980s. In 1986, he served as the AIDS activity division attending physician, and in 1987 was appointed associate medical director of San Francisco General Hospital's AIDS Clinic. During his time at San Francisco General, he helped develop effective models for HIV/AIDS clinical care for intravenous drug users, establishing three medical facilities located in methadone treatment centers.\n\nIn 1991, Goosby began his government career as director of HIV Services at the Health Resources and Services Administration, in the United States Department of Health and Human Services. In this position, he administered the newly authorized Ryan White Care Act, overseeing the distribution of federal funds and the planning of services in 25 AIDS epicenters, as well as in all 50 states and U.S. territories. In 1994, Goosby became director of the Office of HIV/AIDS Policy in the Department of Health and Human Services where he advised on the federal HIV/AIDS budget and worked with Congress on all AIDS-related issues.\n\nIn 1995, Goosby created and convened the DHHS Panel on Clinical Practices for the Treatment of HIV Infections. This panel defined how to use protease inhibitors in conjunction with already existing antiretrovirals, later expanding its work to address standards of care for antiretroviral use for pediatric patients and pregnant women. Dr. Goosby has remained actively involved in this panel, now known the DHHS Panel on Antiretroviral Guidelines for Adults and Adolescents, which is widely recognized as defining the standard of care for HIV/AIDS treatment in the United States.\n\nIn 1997, Goosby also served as interim director of the National AIDS Policy Office at the White House, reporting directly to the President as his senior advisor on HIV-related issues. In 1998, he helped to foster and orchestrate the dialogue on racial disparities in HIV/AIDS that led to the Minority AIDS Initiative. Dr. Goosby's office was responsible for guiding the implementation of this initiative at HHS over the next three years. Dr. Goosby's office also coordinated scientific reviews of needle exchange as a public health intervention. In 2000, Goosby served as acting deputy director of the National AIDS Policy Office in the White House, while continuing to work as director of HIV/AIDS policy at the Department of Health and Human Services.\n\nAfter leaving government service, Goosby was CEO and Chief Medical Officer of Pangaea Global AIDS Foundation from 2001 to 2009. He was also a Professor of Clinical Medicine at the University of California, San Francisco and a clinical provider for the 360 Men of Color Program.\n\nWhile with Pangaea, Dr. Goosby played a key role in the development and/or implementation of HIV/AIDS national treatment scale-up plans in Rwanda, South Africa, China, and Ukraine. During this time, Dr. Goosby developed extensive international experience in the development of treatment guidelines for use of antiretroviral therapies, clinical mentoring and training of health professionals, and the design and implementation of local models of care for HIV/AIDS, focusing his expertise on the scale-up of sustainable HIV/AIDS treatment capacity, including the delivery of HIV antiretroviral drugs, within existing healthcare systems.\n\nOn January 20, 2015, United Nations Secretary-General Ban Ki-moon appointed Eric Goosby of the United States as his new Special Envoy on Tuberculosis (TB). In this capacity, Dr. Goosby will work towards boosting the profile of the fight against TB and promoting the adoption, financing and implementation of the UN World Health Organization’s (WHO) global End TB Strategy after 2015.\n\nIn addition, he will push for the achievement of the Strategy’s “ambitious international targets” for tuberculosis prevention, care and control while also pursuing the TB 2015 targets described in the Millennium Development Goals.\n\nGoosby has served on the board of directors of the Clinton Foundation since 2013.\n\nOn June 23, 2009, Goosby was sworn in as the United States Global AIDS Coordinator, heading up the President's Emergency Plan for AIDS Relief (PEPFAR). At the time of his swearing in, Goosby stated that his top priorities included:\n"}
{"id": "24143236", "url": "https://en.wikipedia.org/wiki?curid=24143236", "title": "European Society for Medical Oncology", "text": "European Society for Medical Oncology\n\nThe European Society for Medical Oncology (ESMO) is the leading professional organisation for medical oncology. With 20,000 members representing oncology professionals from over 150 countries worldwide, ESMO is the society of reference for oncology education and information. ESMO is committed to offer the best care to people with cancer, through fostering integrated cancer care, supporting oncologists in their professional development, and advocating for sustainable cancer care worldwide.\n\nFounded in 1975, ESMO has European roots with a global reach. Home for all oncology stakeholders, ESMO connects professionals with diverse expertise and experience. Its education and information resources support an integrated multi-professional approach to cancer care, from a medical oncology perspective. ESMO seeks to erase boundaries in cancer care, whether between countries or specialities, and pursue its mission across oncology, worldwide.\n\nThe mission of the Society is to:\n\nFounded in 1990, ESMO’s flagship scientific journal, Annals of Oncology, publishes articles addressing medical oncology, surgery, radiotherapy, pediatric oncology, basic research and the comprehensive management of patients with malignant diseases. Annals of Oncology is the official journal of ESMO and from 2008 of the Japanese Society for Medical Oncology (JSMO)\n\nThe ESMO Clinical Practice Guidelines (CPG) are intended to provide oncology professionals with a set of recommendations for the best standards of cancer care, based on the findings of evidence-based medicine. Each Clinical Practice Guideline includes information on the incidence of the malignancy, diagnostic criteria, staging of disease and risk assessment, treatment plans and follow-up designed to help oncologists deliver an appropriate quality of care to their patients.\n\n\nThe annual ESMO Congress, held every year is attended by 25,000 participants. The congress presents the latest scientific developments in basic, translational and clinical cancer research and contextualises new findings for practical implementation in every day patient care.\n\nESMO publishes handbooks, scientific meeting reports, and medical oncology training guidelines. The Society provides fellowships for research training for young oncologists, an Exam in Medical Oncology and an accreditation program for institutes providing patients with integrated supportive and palliative care. Through an online professional networking platform ESMO members collaborate, interact and share knowledge on topics of research and clinical practice.\n\n"}
{"id": "57996542", "url": "https://en.wikipedia.org/wiki?curid=57996542", "title": "Federal Agency for the Safety of the Food Chain", "text": "Federal Agency for the Safety of the Food Chain\n\nThe Federal Agency for the Safety of the Food Chain (FASFC) is an authority tasked with ensuring the quality and safety of foodstuffs in Belgium, and safeguarding plant, animal and human health this way. It controls and inspects all processes in the food industry \"from farm to fork\", meaning all food production, food processing, food distribution and food service. With food safety in mind, the agency is also responsible for combating animal and plant diseases.\n\nThe agency is an 'organism of public interest' (parastatal body) type A according to Belgian public law. It was founded after the dioxin affair rocked Belgium in 1999, which was caused by the use of animal feed contaminated with polychlorinated biphenyls. Through the food chain, the toxic contamination made its way to supermarkets and consumers, inciting a nationwide scandal when it was discovered. At its foundation, the agency inherited all inspection tasks related to food safety from the relevant services of the federal Ministry of Social Affairs, Public Health and Environment, and of the federal Ministry of Agriculture.\n\nAnno 2018, the agency operates under the authority of the Belgian federal minister of Agriculture. As of 2017, the agency has a budget of 166 million euros and is served by 1,260 employees.\n\n"}
{"id": "11798302", "url": "https://en.wikipedia.org/wiki?curid=11798302", "title": "Ganoderma tsugae", "text": "Ganoderma tsugae\n\nGanoderma tsugae, also known as Hemlock varnish shelf, is a flat polypore mushroom of the genus \"Ganoderma\". \n\nIn contrast to \"Ganoderma lucidum\", to which it is closely related and which it closely resembles, \"G. tsugae\" tends to grow on conifers, especially hemlocks. \n\nLike \"G. lucidum\", \"G. tsugae\" is said to have medicinal properties including use for dressing a skin wound. Phylogenetic analysis has begun to better differentiate between many closely related species of \"Ganoderma\"; however, there is still disagreement as to which have the most medicinal properties. In addition, variations within the same species as well as the growth substrate and environmental conditions all the way through to preparation can have a substantial effect on the medicinal value of the product.\n\nLike \"G. lucidum\", \"G. tsugae\" is non-poisonous but generally considered inedible, because of its solid woody nature; however, teas and extracts made from its fruiting bodies allow medicinal use of the compounds it contains. A hot water extraction or tea can be very effective for extracting the polysaccharides; however, an alcohol or alcohol/glycerin extraction method is more effective for the triterpenoids.\n\nStudies in mice has shown that \"G. tsugae\" shows several potential medicinal benefits including anti-tumor activity through some of the active polysaccharides found in \"G. tsugae\"\n\"G. tsugae\" has also been shown to significantly promote wound healing in mice as well as significantly increase the proliferation and migration of fibroblast cells in culture.\n"}
{"id": "56226684", "url": "https://en.wikipedia.org/wiki?curid=56226684", "title": "HeRAMS", "text": "HeRAMS\n\nHeRAMS (Health Resources Availability Mapping System) is an electronic system for monitoring medical resources. It is a WHO tool for standardizing and assessing the availability of medical services in different countries. It is mostly used for emergency response.\n\nWhen conducting programs using HeRAMS, information is collected from health facilities in a specific area. Then the analysis of the received data is carried out.\n\nThe system collects information on four major contingencies related to emergencies:\n\nBased on the findings, WHO, in collaboration with local health ministries, makes analytical reports and develops possible measures to improve the situation.\n\nWHO pays the greatest attention to countries with destroyed infrastructure, including medical ones.The evaluation was conducted in Sudan, Mali, Philippines, Central African Republic, Syria, Fiji, Nigeria, Yemen.\n\nIn autumn 2017 World Health Organization, in cooperation with the Ministry of Healthcare of Ukraine, launched the \"HeRAMS Ukraine\" project.\n\n"}
{"id": "12999450", "url": "https://en.wikipedia.org/wiki?curid=12999450", "title": "History of emerging infectious diseases", "text": "History of emerging infectious diseases\n\nThe discovery of disease-causing pathogens is an important activity in the field of medical science. Many viruses, bacteria, protozoa, fungi, helminthes and prions are identified as a confirmed or potential pathogen. In the United States, a Centers for Disease Control program, begun in 1995, identified over a hundred patients with life-threatening illnesses that were considered to be of an infectious cause, but that could not be linked to a known pathogen. The association of pathogens with disease can be a complex and controversial process, in some cases requiring decades or even centuries to achieve.\n\nFactors which have been identified as impeding the identification of pathogens include the following:\n\nVibrio cholera bacteria are transmitted through contaminated water. Once ingested, the bacteria colonizes the intestinal tract of the host and produces a toxin which causes body fluids to flow across the lining of the intestine. Death can result in 2–3 hours from dehydration if no treatment is provided.\n\nBefore the discovery of an infectious cause, the symptoms of cholera were thought to be caused by an excess of bile in the patient; the disease cholera gets its name from the Greek word \"choler\" meaning bile. This theory was consistent with humorism, and led to such medical practices as bloodletting. The bacterium was first reported in 1849 by Gabriel Pouchet, who discovered it in stools from patients with cholera, but he did not appreciate the significance of this presence. The first scientist to understand the significance of \"Vibrio cholerae\" was the Italian anatomist Filippo Pacini, who published detailed drawings of the organism in \"Microscopical observations and pathological deductions on cholera\" in 1854. He published further papers in 1866, 1871, 1876 and 1880, which were ignored by the scientific community. He correctly described how the bacteria caused diarrhea, and developed treatments that were found to be effective. Whilst John Snow's edpidemiology maps were well recognised, and led to the removal of the Broad Street pump handle, e.g. 1854 Broad Street cholera outbreak. In 1874, scientific representatives from 21 countries voted unanimously to resolve that cholera was caused by environmental toxins from \"miasmatas\", or clouds of unhealthy substances which float in the air. In 1884, Robert Koch re-discovered \"Vibrio cholerae\" as a causal element in cholera. Some scientists opposed the new theory, and even drank cholera cultures to disprove it:\n\nVon Pettenkofer considered his experience proof that \"Vibrio cholerae\" was harmless, as he did not develop cholera from consuming the culture. Between 1849 when Pouchet discovered \"Vibrio cholerae\" and 1891, over a million people died in cholera epidemics in Europe and Russia. In 1995, researchers published a study in \"Science\" explaining why some persons are able to be infected with cholera without symptoms, possibly explaining why Pettenkofer did not get sick. The study showed that a series of genetic mutations in some people provide resistance to cholera toxin; but these mutations come at a price. If too many of them occur in a person, they will develop cystic fibrosis, an incurable and often fatal genetic disorder.\n\nGiardiasis is a disease caused by infection with the protozoan \"Giardia lamblia\". Infection with \"Giardia\" can produce diarrhea, gas, and abdominal pain in some people. If untreated, infection can be chronic. In children, chronic \"Giardia\" infection can cause stunting (stunted growth) and lowered intelligence, Infection with \"Giardia\" is now universally recognized as a disease, and treated by physicians with anti-protozoal drugs. Since 2002, \"Giardia\" cases must be reported to the Center for Disease Control, according to the CDC’s Reportable Disease Spreadsheet. The United States National Institutes of Health \nGastrointestinal Parasites Lab studies \"Giardia\" almost exclusively.\n\nHowever, \"Giardia\" experienced an extraordinarily long term of emergence, from its discovery in 1681, until the 1970s when it was fully accepted that infection with \"Giardia\" was a treatable cause of chronic diarrhea:\n\nSome of the first evidence in modern times of \"Giardia's\" pathogenicity came during World War II when soldiers were treated for malaria with the antiprotozoal Quinacrine, and their diarrhea disappeared, as did the \"Giardia\" from their stool samples. In 1954, Dr. R.C. Rendtorff performed experiments on prisoner volunteers, infecting them with \"Giardia\". In the experiment, although some prisoners experienced changes in stool habits, he concluded that these could not be conclusively linked to \"Giardia\" infection, and also indicated that all prisoners experienced spontaneous clearance of \"Giardia\". His experiments were described in the EPA Symposium on Waterborne Transmission of Giardiasis in 1979:\n\nIn 1954-55, an outbreak of \"Giardia\" infection occurred in Oregon (US), sickening 50,000 people. This was documented in a communication by Dr. Lyle Veazie, which wasn't published until 15 years later in the \"New England Journal of Medicine.\" In the communication, Veazie notes that he was unable to find a publisher for his account of the epidemic. The communication was re-published in the \"EPA Symposium on Waterborne Transmission of Giardiasis\" in 1979, and that version included the following quote from the Director of the Oregon State Board of Health, suggesting that diarrhea from \"Giardia\" was still being attributed to other causes by health authorities in 1954:\n\nInfection with the bacteria \"Helicobacter pylori\" is the cause of most stomach ulcers. The discovery is generally credited to Australian gastroenterologists Dr. Barry Marshall and Dr. J Robin Warren, who published their findings in 1983. The pair received the Nobel Prize in 2005 for their work. Before this, nobody really knew what caused stomach ulcers, though a popular belief was that the \"stress\" played a role. Some researchers suggested that ulcers were a psychosomatic illness.\n\nIn \"H Pylori Pioneers\", Dr. Marshall noted that other physicians had produced evidence of \"H. pylori\" infection as early as 1892. Marshall writes that earlier reports were disregarded because they conflicted with existing belief. The first description of \"H. Pylori\" came in 1892 from Giulio Bizzozero, who identified acid-tolerant bacteria living in a dog’s stomach. Later, a theory would be developed that no bacteria could live in the stomach. Although the theory has no scientific basis, it would become a stumbling block for scientists, discouraging them for searching for infective causes of stomach ulcers. In 1940, two physicians, Dr. A. Stone Freeberg and Dr. Louis E. Barron published a paper describing a spiral bacteria found in about half of their gastroenterology patients who had stomach ulcers. Dr. John Lykoudis, a Greek physician, was one of the first physicians to treat stomach ulcers as an infectious disease. Between 1960 and 1970, he treated over 10,000 ulcer patients in Athens with antibiotics. Lykoudis tried to publish a paper on his findings, but they conflicted with traditional theory, and his work was never published. Lykoudis' experience was followed in 1975 by a further publication in Gut magazine that included spiral bacteria living on the borders of duodonal ulcers. The medical significance of Steer’s findings was disregarded, but he “continued to publish papers on \"H. Pylori\", mostly as a hobby.\"\n\n\"H. pylori\" can infect the stomach of some people without causing stomach ulcers. In investigating asymptomatic carriers of \"H. pylori\", researchers identified a genetic trait called Interleuik-1 beta-31 which causes increased production of stomach acid, resulting in ulcers if patients become infected with \"H. pylori\". Patients without the trait do not develop stomach ulcers in response to \"H. pylori\" infection, but instead have increased risk from stomach cancer if they become infected. Investigation into other gastrointestinal infections has also shown that the symptoms are the result of interaction between the infection and specific genetic mutations in the host.\n\nThere are different types of \"E. coli\", some of which are found in humans and are harmless. Enterotoxigenic Escherichia coli (ETEC) is a type found to cause illness in humans, possessing gene that allows it to manufacture a substance toxic to humans. Cattle are immune to its effects but when people eat food contaminated with cattle feces, the organism can cause disease. Reports of pathogenic \"E. coli\" appear in medical literature as early as 1947. Publications regarding variants of \"E. coli\" which cause disease appeared regularly in medical journals throughout the 1950s, 60s, and 70s, with fatalities being reported in humans and infants starting in the 1970s. Despite the earlier reports, pathogenic \"E. coli\" did not rise to public prominence until 1983, when a Center for Disease Control researcher published a paper identifying ETEC as the cause of a series of outbreaks of unexplained hemorrhagic gastrointestinal illness. Despite the earlier publication of pathogenic variants of E. coli, researchers encountered significant difficulties in establishing ETEC as a pathogen.\n\nAIDS was first reported June 5, 1981, when the U.S. Centers for Disease Control and Prevention recorded a cluster of \"Pneumocystis carinii\" pneumonia (now still classified as PCP but known to be caused by \"Pneumocystis jirovecii\") in five homosexual men in Los Angeles. The discovery of the virus took several years of research, and was announced in 1984 by Dr. Gallo of the National Cancer Institute, Dr. Luc Montagnier at the Pasteur Institute in Paris, and Dr. Jay Levy at the University of California, San Francisco.\n\nHowever, HIV existed long before the 1981 CDC report. Three of the earliest known instances of HIV infection are as follows:\n\nTwo species of HIV infect humans: HIV-1 and HIV-2. HIV-1 is more virulent and more easily transmitted. HIV-1 is the source of the majority of HIV infections throughout the world, while HIV-2 is not as easily transmitted and is largely confined to West Africa. Both HIV-1 and HIV-2 are of primate origin. The origin of HIV-1 is the central common chimpanzee (\"Pan troglodytes troglodytes\") found in southern Cameroon. It is established that HIV-2 originated from the sooty mangabey (\"Cercocebus atys\"), an Old World monkey of Guinea Bissau, Gabon, and Cameroon.\n\nIt is hypothesized that HIV probably transferred to humans as a result of direct contact with primates, for instance during hunting, butchery, or inter-species sexual contact.\n\nCyclospora is a gastrointestinal pathogen that causes fever, diarrhea, vomiting, and severe weight loss. Outbreaks of the disease occurred in Chicago in 1989 and other areas in the United States. But investigation by the Center for Disease Control could not identify an infectious cause. The discovery of the cause was made by Mr. Ramachandran Rajah, the head of a medical clinic's laboratory in Kathmandu, Nepal. Mr. Rajah was trying to discover why local residents and visitors were becoming ill every summer. He identified an unusual looking organism in stool samples from patients who were sick. But when the clinic sent slides of the organism to the Center for Disease Control, it was identified as blue-green algae, which is harmless. Many pathologists had seen the same thing before, but dismissed it as irrelevant to the patient's disease. Later, the organism would be identified as a special kind of parasite, and treatment would be developed to help patients with the infection. In the United States, Cyclospora infection must be reported to the Center for Disease Control according to the CDC's Reportable Disease Chart\n\nThe process of identifying new infectious agents continues. One study has suggested there are a large number of pathogens already causing illness in the population, but they have not yet been properly identified.\n\nMany recently emerged pathogens infect the gastrointestinal tract. For example, there are three gastrointestinal protozoal infections which must be reported to the Center for Disease Control.\nThey are Giardia, Cyclospora and Cryptosporidium, and none of them were known to be significant pathogens in the 1970s.\n\nFigure 1 shows the prevalence of gastrointestinal protozoa in studies from the United States and Canada. The most prevalent protozoa in these studies are considered emerging infectious diseases by some researchers, because a consensus does not yet exist in the medical and public health spheres concerning their importance in the role of human disease. Researchers have suggested that their treatment may be complicated by differing opinions regarding pathogenicity, lack of reliable testing procedures, and lack of reliable treatments. As with newly discovered pathogens before them, researchers are reporting that these organisms may be responsible for illnesses for which no clear cause has been found, such as Irritable Bowel Syndrome.\n\n\"Dientamoeba fragilis\" is a single-celled parasite which infects the large intestine causing diarrhea, gas, and abdominal pain. An Australian study identified patients with symptoms of IBS who were actually infected with \"Dientamoeba fragilis\". Their symptoms resolved following treatment. A study in Denmark identified a high incidence \"Dientamoeba fragilis\" infection in a group of patients suspected of having gastrointestinal illness of an infectious nature. The study also suggested special methods may be required to identify infection.\n\n\"Blastocystis\" is a single-celled protozoan which infects the large intestine. Physicians report that patients with infection show symptoms of abdominal pain, constipation, and diarrhea. One study found that 43% of IBS patients were infected with \"Blastocystis\" versus 7% of controls. An additional study found that many IBS patients from whom \"Blastocystis\" could not be identified showed a strong antibody reaction to the organism, which is a type of test used to diagnose certain difficult-to-detect infections. Other researchers have also reported that special testing techniques may be necessary to identify the infection in some people. While some scientists believe the finding that IBS patients carry a protozoal infection to be significant, other researchers have reported their belief that the presence of the infection is not medically significant. Researchers report that the infection can be resistant to common protozoal treatments in laboratory culture study, and in experience with patients,; therefore, identifying \"Blastocystis\" infection may not be of immediate help to a patient. A 2006 study of gastrointestinal infections in the United States suggested that \"Blastocystis\" infection has become the leading cause of protozoal diarrhea in that country. \"Blastocystis\" was the most frequently identified protozoal infection found in patients in a 2006 Canadian study.\n\n"}
{"id": "37915503", "url": "https://en.wikipedia.org/wiki?curid=37915503", "title": "Hospital Insurance and Diagnostic Services Act", "text": "Hospital Insurance and Diagnostic Services Act\n\nThe Hospital Insurance and Diagnostic Services Act (HIDS) is a statute passed by the Parliament of Canada in 1957 that reimbursed one-half of provincial and territorial costs for hospital and diagnostic services administered under provincial and territorial health insurance programs. Originally implemented on July 1, 1958, with five participating provinces, by January 1, 1961, all 10 provinces were enlisted. The federal funding was coupled with terms and conditions borrowed from the Saskatchewan Hospital Services Plan, introduced in 1947 as the first universal hospital insurance program in North America. In order to receive funding, services had to be universal, comprehensive, accessible and portable. This stipulation was dropped in 1977 with the Established Programs Financing Act and then reinstated in 1984 in the Canada Health Act. Widely acknowledged as the foundation for future developments in the Canadian health care system, the HIDS Act was a landmark example of federal-provincial cooperation in post-war Canada.\n\nPrior to World War II, health care in Canada was privately funded and delivered, with the exception of services provided to the sick poor that were financed by local governments. The experience of the 1930s left many Canadians in challenging financial situations. As personal financial situations deteriorated, the municipal governments were overwhelmed. Though the provinces provided relief payments for food, clothing, and shelter, additional medical costs were beyond the capacity of most of the provincial budgets. Many Canadians were not receiving adequate medical care, and those that did were overwhelmed with the associated costs. As such, preventable diseases and deaths were still common occurrences.\n\nTen years of depression, followed by six years of war, formed the social context of the ambitious federal Green Book Proposals. In a bid for unprecedented cooperation between the federal and provincial governments, these initiatives formed the foundations of a national program for social security, including provisions for health insurance. However, the failure to come to a consensus on the required allocation of tax resources at the Dominion-Provincial Conference in August 1945 precluded adoption and delayed subsequent action. Although the Green Book Proposals were not adopted, they effectively created an appetite for government-funded health services.\nDespite a lack of commitment for federal funding, Saskatchewan proceeded with a plan for provincial hospital insurance. From the collective efforts of the \"wheat economy\" came a cooperative movement towards efficient agencies to deliver services to Saskatchewan’s sparse population. Strong local engagement contributed to creation of the union hospital system and municipal hospital care plans. However, a solution to the problem of providing medical and hospital services to a population reeling from the devastating effects of the depression required greater provincial contribution. The Co-operative Commonwealth Federation won their first majority government in 1944. Continuing the Liberal health insurance platform that introduced \"A Bill Respecting Health Insurance,\" Tommy Douglas, as the new premier, signaled his commitment to the provision of health services by assuming the role of Health Minister as well. By 1947, Saskatchewan introduced the first universal hospital insurance program in North America.\n\nSaskatchewan’s decision to launch the Saskatchewan Hospital Services Plan accelerated and influenced the development of other provincial insurance plans. The British Columbia Hospital Insurance Service was passed in early 1948, and followed soon after by the Alberta insurance system. The success of these provincial plans combined with the volume of illness and associated costs, in addition to provincial disparities in health coverage, fuelled debate on the topic of a federally funded health service. There was much disagreement as to the appropriate scope, funding allocation, and administration of such a plan.\n\nAfter several years of debate between the stakeholders, including Canadian medical professional associations and the provincial governments, the federal government made an offer to fund approximately one half of the national cost of diagnostic services and in-patient hospital care for provinces that implemented insurance plans. Five provinces, namely British Columbia, Alberta, Saskatchewan, Ontario, and Newfoundland, accepted the proposal, laying the groundwork for a Canadian health insurance plan.\n\nOn May 1, 1957, the HIDS Act was formally legislated in Canada in response to the increasing pressures for national comprehensive health insurance. Under the Act, the federal government agreed to fund approximately 50% of the costs of provincial or territorial insurance plans for hospital and diagnostic services. Formally, federal funding comprised 25% of the per capita costs for hospital services in Canada plus 25% of the per capita costs for hospital services in the province or territory multiplied by the number of insured persons in that jurisdiction. Funding was made available to any province or territory that agreed to make insured hospital services available to the region under uniform provisions.\n\nParticipating provinces and territories were obligated to satisfy four funding conditions as follows:\n\n\nProvinces and Territories were also obligated to limit co-payments and other \"deterrent\" fees to ensure that patients were not placed under financial burden at the point of care. Though there was no other explicit provision preventing provinces and territories from demanding financial contribution for services from patients, such charges would have reduced the federal contribution since, under the cost-sharing arrangement, federal funding was proportional to provincial and territorial contributions. Therefore, the Act intrinsically deterred provinces and territories from charging patients for services.\n\nProvincial and territorial insurance plans were to cover acute, convalescent, and chronic care of patients, including diagnostic services and in-patient drug administration in hospital facilities. However, coverage was not provided for hospitals for tuberculosis, mental hospitals, nursing homes, capital expenditures, or administrative costs.\n\nEach province and territory was to be responsible for administering its own plan; therefore, each had the right to decide how to raise its proportion of funding for the insurance program, either through insurance premiums or taxation.\n\nUnder the Act, insured services could only be delivered by hospitals, most of which were private entities. Hospital employees including physicians, laboratory technicians, and radiologists were to be paid via a fee-for-service model negotiated with the provincial or territorial administrative body.\n\nThe passage of the HIDS Act was the first milestone in the evolution of national health insurance in Canada and provided the foundation for all future Canadian Health legislation.\n\nBy 1961, almost all Canadians were entitled to comprehensive hospital care benefits, protecting them from large hospital bills. The HIDS act enabled hospital operations that were not previously feasible and facilitated access to care for who could not otherwise afford it.\nThe HIDS Act laid the foundation for other notable developments in the Canadian health care system. One of the criticisms of the Act was that it did not cover medical services, which in 1955 comprised approximately 40% of national healthcare costs. Following the adoption of the HIDS Act, then, extending health insurance to cover additional medical services was next on the federal agenda. At the Federal-Provincial Conference in July, 1965, the decision for Medicare was made. Then, on July 1, 1967, the governing Liberals under Lester B. Pearson introduced the Medical Care Act, covering 50% of physician costs outside of a hospital.\nTogether, the HIDS Act and Medical Care Act brought hospital and physician services to all Canadians, regardless of their ability to pay. Though criticized for imposing federal priorities on provincial jurisdiction, provincial governments were left with no option other than to meet the federal provisions or forgo supplementary funding altogether. To address these concerns, the Established Programs Financing Act was passed in 1977, transferring the responsibility of the program to the provinces by decoupling the amount of the federal transfer from the provisions. Some provinces levied user charges and authorized extra-billing, which threatened universal and free access to healthcare. The federal government, then, enacted the Canada Health Act in 1984 to re-instate the provisions of the HIDS Act and the Medical Care Act.\n\n"}
{"id": "24261639", "url": "https://en.wikipedia.org/wiki?curid=24261639", "title": "Hugo Sigman", "text": "Hugo Sigman\n\nHugo Sigman is the founder, CEO and –jointly with his wife, Biochemist Silvia Gold— the only shareholder of Grupo Insud, a business conglomerate with an active presence in the fields of pharmaceuticals, agroforestry, culture, nature and design.\n\nHugo Sigman was born in Buenos Aires, Argentina, in 1944. He completed his primary and secondary schooling in public schools.\nIn 1969, he earned his Medical Doctor degree from Universidad de Buenos Aires, and in the same year, he graduated from the Social Psychology School led by Dr. Pichon Riviére.\n\nIn 1970, he joined as Resident the Psychiatry Service of Policlínico Lanús, directed by Professor Mauricio Goldenberg. He continued his career, first as Chief of Residents, and then as founder and director of the Psychiatric Emergency Unit at the same hospital. In 1976, he moved to Spain, where he worked at the Psychiatry Service of Hospital Clínico de Barcelona.\n\nAt that time, encouraged by his father-in-law Roberto Gold, Hugo Sigman started his entrepreneurial career and got involved in the pharmaceutical industry.\nIn 1978, with his wife, Silvia Gold –his partner in love and work for 45 years-, he founded Chemo, a chemical-pharmaceutical company, and the first company of Grupo Insud.\nTheir goal was to create a company to produce high-quality medicines at affordable prices.\nIn the 1980s, Hugo and Silvia returned to Argentina to continue the path they had started in Europe.\n\nAt present, Grupo Insud comprises three companies in the life sciences industry: Chemo, a company that sells active pharmaceutical ingredients (APIs) and finished dosage forms (FDFs) to 1,200 pharmaceutical companies around the world, has 15 manufacturing sites and 9 R&D centers; Exeltis, a company that sells branded products in more than 40 countries, and mAbxience, a biotech company created in 2008, with 3 manufacturing sites, the first monoclonal antibody plant in Latin America –pharmADN- and the first vaccine and biotech production plant in Argentina, Sinergium Biotech.\n\nChemo signed an agreement with the World Health Organization and Utrecht University to produce the Palivizumab monoclonal antibody and facilitate access to this medicine for premature children, as it is estimated that 250,000 premature children die every year around the world without treatment due to its high cost.\n\nJointly with renowned Argentine companies, in 2011 Hugo Sigman founded the Argentine Chamber of Biotechnology, Cámara Argentina de Biotecnología, which he currently chairs. The goal of CAB is to strengthen the public-private collaboration policy in biotechnology and encourage its development in the region.\nIn Argentina’s veterinary industry, he is a partner of Biogénesis Bagó, which runs a world-class facility for foot-and-mouth disease vaccine production, the only one that has been authorized by the government of China to build a plant in that country.\n\nFor 20 years, Hugo and Silvia have led the Public-Private Consortium for Research and Development of Innovative Oncology Therapies, which developed the first therapeutic vaccine against lung cancer, Racotumomab (Vaxira), introduced in 2013, and which currently has five new products in research.\n\nIn this framework, Hugo Sigman strongly supports public-private partnerships, which he views as a positive paradigm to be promoted with ethics and transparency. He understands that public research centers should give back to society by contributing their knowledge and developments, and the private sector should recognize that contribution by compensating them accordingly. Inspired in that model, he has built partnerships with prestigious institutions including Universidad de Buenos Aires, Universidad Nacional de Quilmes, Universidad Nacional de San Martín, Roffo and Garrahan Hospitals, , and several foreign universities.\n\nApproximately 250 researchers work at the partnerships supported by the Group, doing basic and applied research; 100 of them are dedicated to the development of products for treating cancer.\n\nHis entrepreneurial spirit and interest in biology have led Hugo Sigman beyond the frontiers of the pharmaceutical industry. In 1998, he started agricultural and forestry activities in different locations of Argentina, focused on genetic improvement and sustainable production, with companies Garruchos, devoted to farming and cattle-raising; Pomera, ranked as the top Argentine-capital forestry company; and Cabaña Los Murmullos, a leading cattle breeding ranch.\nSigman is also a shareholder of Bioceres, an Argentine biotechnology company focused on agricultural production. This young company developed a gene that enables the production of wheat, corn and soybean resistant to draughts and soil salinity, and has licensed its products to the USA, France and India.\n\nHugo Sigman encourages the exchange of ideas through publishing company Capital intellectual, which includes a line of publications in Spain under the publishing brand Clave Intellectual, the Southern Cone edition of Le Monde Diplomatique, and the Latin American version of prestigious The New York Review of Books.\nIn 2005, Hugo Sigman partnered with Oscar Kramer and established filmmaking company Kramer&Sigman Films (K&S), which has grown to be a thriving production company, with several box-office successes, including \"Kamchatka\", \"El perro\", \"The Last Elvis\", \"Seventh Floor\", \"Wild Tales\".\n\nProduced by Hugo Sigman for Kramer& Sigman Films and by El Deseo (Pedro Almodóvar and Agustín Almodóvar ’s production company), \"Wild Tales\" (Relatos Salvajes) was directed by Damián Szifron, starring Ricardo Darín, Julieta Zylberberg, Rita Cortese; Darío Grandinetti; Érica Rivas; Oscar Martínez and Leonardo Sbaraglia. Co-produced by Telefé and Corner, and distributed by Warner Bros. Pictures, Wild Tales exceeded 1 million tickets sold in its first ten days of screening.\n\nAn art and culture enthusiast, between 2010 and 2013, Hugo Sigman was a member of the Advisory Council of the National Fine Arts Museum of\nArgentina, and is currently a member of the Board of Patronage of Museo Reina Sofía, in Madrid, Spain. Additionally, the venue of Grupo INSUD in Buenos Aires (the traditional Díaz Vélez Palace), built in 1907 and recently restored, has won several architectural awards and has been declared a protected heritage site of Buenos Aires City.\n\nHugo and Silvia have always had a strong commitment to give back to society. This social responsibility is infused in their work with the Mundo Sano Foundation. in Argentina, Spain and Ethiopia, with the vision to transform the reality of populations affected by neglected tropical diseases (NTDs) including Chagas, dengue and soil-transmitted helminths.\n\nIn 2015, Hugo Sigman received the Entrepreneur of the Year Award, from EY consulting firm (Ernst & Young) in Argentina.\n\nIn 2013, the Endeavor Foundation named him the \"Model Entrepreneur\" for his values and track record. In 2008, he received the merit diploma at the Konex Awards, in the \"Innovative Entrepreneurs\" category. In 2011, jointly with Silvia Gold, he received an award from Asociación de Dirigentes de Empresa (ADE) in the \"Agroindustry\" category.\n\nSigman presented the achievements of the Research, Development and Innovation Consortium at Experience Endeavor in August 2013, and at the 11th International Symposium on AIDS, organized by Fundación Huésped, in 2012.\n\nIn April 2012, he was a speaker at the Global Investment Forum, organized by the United Nations in Qatar. He presented the state-of-the-art productive investment model based on partnerships with other pharmaceutical companies and public entities.\n\nIn September 2011, Sigman was guest speaker at the United Nations Conference on Trade and Development held in Geneva, Switzerland. His lecture was about the manufacturing of vaccines and biotechnologicals in Argentina. \"The Sinergium Biotech model of integration for the production of vaccines and biotech products involves technology transfer, export potential and the creation of 320 jobs,\" said Sigman. Also in 2011, Sigman delivered a presentation on global entrepreneurship at the 47th IDEA Colloquium.\n\nIn 2009, Sigman took part in the 15th Conference of the Argentine Industrial Union, and was guest speaker representing Argentina at Harvard and Columbia Universities.\n\n"}
{"id": "13328566", "url": "https://en.wikipedia.org/wiki?curid=13328566", "title": "Insulated shipping container", "text": "Insulated shipping container\n\nInsulated shipping containers are a type of packaging used to ship temperature sensitive products such as foods, pharmaceuticals, organs, blood, biologic materials, and chemicals. They are used as part of a cold chain to help maintain product freshness and efficacy. The term can also refer to insulated intermodal containers or insulated swap bodies.\n\nA variety of constructions have been developed. \n\n\nSome are designed for single use while others are returnable for reuse. Some insulated containers are decommissioned refrigeration units. Some empty containers are sent to the shipper disassembled or “knocked down”, assembled and used, then knocked down again for easier return shipment.\n\nShipping containers are available for maintaining cryogenic temperatures, with the use of liquid nitrogen. Some carriers have these as a specialized service\n\nInsulated shipping containers are part of a comprehensive cold chain which controls and documents the temperature of a product through its entire distribution cycle. The containers may be used with a refrigerant or coolant such as:\n\nA digital Temperature data logger or a time temperature indicator is often enclosed to monitor the temperature inside the container for its entire shipment.\n\nLabels and appropriate documentation (internal and external) are usually required.\n\nPersonnel throughout the cold chain need to be aware of the special handling and documentation required for some controlled shipments. With some regulated products, complete documentation is required.\n\nThe use of “off the shelf” insulated shipping containers does not necessarily guarantee proper performance. Several factors need to be considered:\n\n\nIn specifying an insulated shipping container, the two primary characteristics of the material are its thermal conductivity or R-value, and its thickness. These two attributes will help determine the resistance to heat transfer from the ambient environment into the payload space. The coolant material load temperature, quantity, latent heat, and sensible heat will help determine the amount of heat the parcel can absorb while maintaining the desired control temperature. Combining the attributes from the insulator and coolant will allow analysis of expected duration of the insulated shipping container system.\n\nIt is wise (and sometimes mandatory) to have formal verification of the performance of the insulated shipping container. Laboratory package testing might include ASTM D3103-07, Standard Test Method for Thermal Insulation Performance of Packages, ISTA Guide 5B: Focused Simulation Guide for Thermal Performance Testing of Temperature Controlled Transport Packaging, and others. In addition, validation of field performance (performance qualification) is extremely useful.\n\nSpecialists in design and testing of packaging for temperature sensitive products are often needed. These may be consultants, independent laboratories, universities, or reputable vendors. Many laboratories have certifications and accreditations: ISO 9000s, ISO/IEC 17025, etc.\n\nParcel to pallet sized insulated shipping containers have historically been single-use products due to the low-cost material composition of EPS and water-based gel packs. The insulation material typically finds its way into landfill streams as it is not readily recyclable in the United States.\n\nThe development of reusable high-performance shipping containers have been shown to reduce packing waste by 95% while also contributing significant savings to other environmental pollutants.\n\n"}
{"id": "35535612", "url": "https://en.wikipedia.org/wiki?curid=35535612", "title": "Irish Water", "text": "Irish Water\n\nIrish Water Ltd. (Irish: \"Uisce Éireann\") is a water utility company in Ireland. The company was created by the Irish Government through the \"Water Services Act (2013)\", which formally created Irish Water as a subsidiary of Bord Gáis, to provide \"safe, clean and affordable water and waste water services\" to water users in Ireland. Water and wastewater services were previously provided by local authorities in Ireland.\n\nPublic concerns on operational, documentation, company responsiveness, data security, financial and perceived wasteful spending issues were highlighted throughout the initial months of the subsidiary's operations. Together with privatisation fears, these public concerns resulted in a significant volume of unreturned application forms, large nationwide protests and pressure on company management and the government during 2014. In 2015, the scale of non-payment issues, and an unfavourable assessment of the viability of the organisation as an independent entity further increased attention and calls by some to dissolve the organisation. The viability of the utility was also a feature ahead of the 2016 general election, and post-election discussions on government formation.\n\nIrish Water is accountable to two regulatory bodies, the Commission for Energy Regulation (CER) which is the economic regulator for the water industry, and the Environmental Protection Agency (EPA) which is the environmental regulator.\n\nWater supplies in Ireland are governed by the Water Services Acts of 2007 to 2014. Until 2015, this legislation provided for the provision of water and wastewater services by local authorities, with domestic usage funded through central taxation, and non-domestic usage funded via local authority rates. Under terms of a 2010 Economic Adjustment (Bailout) Programme, the then government agreed to change this arrangement. From 2015, legislation came into force such that a new utility company, Irish Water, became responsible for providing water and wastewater services, with the intention that the company would be funded through direct billing. The new company was set up as a subsidiary of an existing semi-state corporation, Bord Gáis (Ervia). The newly created company effectively took on the existing local authority employees and water management facilities, pipes and infrastructure. Operationally, Irish Water delegates some work, for example water meter installation and customer support, to sub-contractors.\n\nIrish Water has been the subject of several civil cases, including one taken by Crohn's Disease sufferer Elizabeth Hourihane, and one taken by the Environmental Protection Agency which initiated proceedings over the standard of water in County Donegal.\n\nIn 2014 and 2015, local protests were encouraged by residents across the country, and supported by Sinn Féin, Socialist Party, Socialist Workers Party, Eirigi, Republican Sinn Féin, 32 County Sovereignty Movement, Communist Party of Ireland, Workers' Party, Workers Solidarity Movement, Direct Democracy Ireland along with trade unionists and other civil society organisations. Those opposed to the plans physically blocked the installation of water meters and demonstrated against the introduction of water charges. A demonstration that took place in Dublin on 11 October 2014 involved approximately 130,000 (4% of Ireland's population). The \"Irish Times\" newspaper conducted a poll the week before which found that 33% of people intended to boycott water charges. Also on 11 October, Paul Murphy, an anti-austerity candidate, won the Dublin-West by-election. This resulted in journalist Fintan O'Toole to describe 11 October 2014 as the 'Water Rebellion'. Further demonstrations took place in key provincial towns and cities in November 2014, and on 10 December 2014 approximately 100,000 people protested in Dublin against water charges, with the Gardaí (police) setting up barricades to establish a seclusion zone around government buildings. In response protestors blockaded roads and bridges in the city centre, postponing bus services, until the early hours of the next morning. Community groups set up to oppose water meters also reportedly physically removed water meters in the days after the protest.\n\nThe utility and associated charges were also a feature in the 2016 general election, with a number of parties and candidates campaigning specifically on the issue.\n\nWater charges were suspended in the months following the election, and an 'expert commission on the funding of water services' established to assess the issue. The commission published a report on 29 November 2016 which recommended that normal household water usage should be paid for by the State, with excessive usage paid for by the consumer in a \"polluter pays\" model.\n"}
{"id": "17330528", "url": "https://en.wikipedia.org/wiki?curid=17330528", "title": "Jean-Antoine Carrel", "text": "Jean-Antoine Carrel\n\nJean-Antoine Carrel (1829 – August 1891) was an Italian mountain climber and guide. He had made climbs with Edward Whymper and was his rival when he attempted to climb the Matterhorn for the first time. Whymper ultimately succeeded in making the mountain's first ascent in July 1865 while Carrel led the party that achieved the second ascent three days later. Carrel was in the group that became the first Europeans to reach the summit of Chimborazo in 1880. He died from exhaustion when guiding a party on the south side of the Matterhorn.\n\nCarrel was born on 16 January 1829 in Valtournenche, in the Aosta Valley, an Arpitan-speaking village of Kingdom of Sardinia (now Italy) which lies at the foot of the Matterhorn. He served in the Bersaglieri, a light infantry unit of the Piedmontese army. He resigned from the Bersaglieri to work as a hunter and mountain guide, but was recalled to duty in 1859 to defend Italy against Austria in the Second Italian War of Independence, for which he won a French medal for the Italian campaign.\n\nCarrel first attempted to climb the Matterhorn's Lion Ridge in 1857—by which time the mountain was the tallest unclimbed peak in the Alps—with his uncle and Amé Gorret. In the early 1860s, Carrel made numerous attempts to climb the Matterhorn, often in the same party as Edward Whymper and John Tyndall, and at other times competing against them to reach the summit first. Carrel had agreed to accompany Whymper on his ascent of the Swiss side in 1865, but withdrew at the last minute when he was recruited by Felice Giordano on behalf of the Italian Alpine Club to lead an Italian party up the Italian side at the same time. Ultimately, Whymper's party outclimbed the Italians and reached the summit on 14 July 1865, marking the first ascent of the Matterhorn. Carrel and his Italian party successfully summited the Matterhorn three days later.\n\nCarrel died in August 1891 while guiding a party on the south side of the Matterhorn. After ensuring that his clients descended the mountain safely and easily in a severe storm, he collapsed from exhaustion and died on a rock at the mountain's base.\n\nAfter Carrel's death, Whymper wrote that Carrel was \"a man who was possessed with a pure and genuine love of mountains; a man of originality and resource, courage and determination, who delighted in exploration ... The manner of his death strikes a chord in hearts he never knew.\"\n"}
{"id": "521571", "url": "https://en.wikipedia.org/wiki?curid=521571", "title": "Laryngitis", "text": "Laryngitis\n\nLaryngitis is inflammation of the larynx (voice box). Symptoms often include a hoarse voice and may include fever, cough, pain in the front of the neck, and trouble swallowing. Typically, these last under two weeks.\nLaryngitis is categorised as acute if it lasts less than three weeks and chronic if symptoms last more than three weeks. Acute cases usually occur as part of a viral upper respiratory tract infection. Other infections and trauma such as from coughing are other causes. Chronic cases may occur due to smoking, tuberculosis, allergies, acid reflux, rheumatoid arthritis, or sarcoidosis. The underlying mechanism involves irritation of the vocal cords.\nConcerning signs that may require further investigation include stridor, history of radiation therapy to the neck, trouble swallowing, duration of more than three weeks, and a history of smoking. If concerning signs are present the vocal cords should be examined via laryngoscopy. Other conditions that can produce similar symptoms include epiglottitis, croup, inhaling a foreign body, and laryngeal cancer.\nThe acute form generally resolves without specific treatment. Resting the voice and sufficient fluids may help. Antibiotics generally do not appear to be useful in the acute form. The acute form is common while the chronic form is not. The chronic form occurs most often in middle age and is more common in men than women.\nThe primary symptom of laryngitis is a hoarse voice. Because laryngitis can have various causes, other signs and symptoms may vary. They can include\n\nAside from a hoarse-sounding voice, changes to pitch and volume may occur with laryngitis. Speakers may experience a lower or higher pitch than normal, depending on whether their vocal folds are swollen or stiff. They may also have breathier voices, as more air flows through the space between the vocal folds (the glottis), quieter volume and a reduced range.\n\nLaryngitis can be infectious as well as noninfectious in origin. The resulting inflammation of the vocal folds results in a distortion of the sound produced there. It normally develops in response to either an infection, trauma to the vocal folds, or allergies. Chronic laryngitis may also be caused by more severe problems, such as nerve damage, sores, polyps, or hard and thick lumps (nodules) on the vocal cords.\n\n\n\n\n\n\n\n\nDiagnosis of different forms of acute laryngitis\n\n\nThe larynx itself will often show erythema (reddening) and edema (swelling). This can be seen with laryngoscopy or stroboscopy (method depends on the type of laryngitis). Other features of the laryngeal tissues may include\n\nSome signs and symptoms indicate the need for early referral. These include\n\n\nTreatment is often supportive in nature, and depends on the severity and type of laryngitis (acute or chronic). General measures to relieve symptoms of laryngitis include behaviour modification, hydration and humidification.\n\nVocal hygiene (care of the voice) is very important to relieve symptoms of laryngitis. Vocal hygiene involves measures such as: resting the voice, drinking sufficient water, reducing caffeine and alcohol intake, stopping smoking and limiting throat clearing.\n\nIn general, acute laryngitis treatment involves vocal hygiene, painkillers (analgesics), humidification, and antibiotics.\n\nThe suggested treatment for viral laryngitis involves vocal rest, pain medication, and mucolytics for frequent coughing. Home remedies such as tea and honey may also be helpful. Antibiotics are not used for treatment of viral laryngitis.\n\nAntibiotics may be prescribed for bacterial laryngitis, especially when symptoms of upper respiratory infection are present. However, the use of antibiotics is highly debated for acute laryngitis. This relates to issues of effectiveness, side effects, cost, and possibility of antibiotic resistance patterns. Overall, antibiotics do not appear to be very effective in the treatment of acute laryngitis.\n\nIn severe cases of bacterial laryngitis, such as supraglottitis or epiglottitis, there is a higher risk of the airway becoming blocked. An urgent referral should be made to manage the airway. Treatment may involve humidification, corticosteroids, intravenous antibiotics, and nebulised adrenaline.\n\nFungal laryngitis can be treated with oral antifungal tablets and antifungal solutions. These are typically used for up to three weeks and treatment may need to be repeated if the fungal infection returns.\n\nLaryngitis caused by excessive use or misuse of the voice can be managed though vocal hygiene measures.\n\nLaryngopharyngeal reflux treatment primarily involves behavioural management and medication. Behavioural management involves aspects such as\nAnti-reflux medications may be prescribed for patients with signs of chronic laryngitis and hoarse voice. If anti-reflux treatment does not result in a decrease of symptoms, other possible causes should be examined. Over-the-counter medications for neutralizing acids (antacids) and acid suppressants (H-2 blockers) may be used. Antacids are often short-acting and may not be sufficient for treatment. Proton pump inhibitors are an effective type of medication. These should only be prescribed for a set period of time, after which the symptoms should be reviewed. Proton pump inhibitors do not work for everyone. A physical reflux barrier (e.g. Gaviscon Liquid) may be more appropriate for some. Antisecretory medications can have several side-effects.\n\nWhen appropriate, anti-reflux surgery may benefit some individuals.\n\nWhen treating allergic laryngitis, topical nasal steroids and immunotherapy have been found to be effective for allergic rhinitis. Antihistamines may also be helpful, but can create a dryness in the larynx. Inhaled steroids that are used for a long period can lead to problems with the larynx and voice.\n\nMucous membrane pemphigoid may be managed with medication (cyclophosphamide and prednisolone).\n\nSarcoidosis is typically treated with systemic corticosteroids. Less frequently used treatments include intralesional injections or laser resection.\n\nAcute laryngitis may persist, but will typically resolve on its own within two weeks. Recovery is likely to be quick if the patient follows the treatment plan. In viral laryngitis, symptoms can persist for an extended amount of time, even when upper respiratory tract inflammation has been resolved.\n\nLaryngitis that continues for more than three weeks is considered chronic. If laryngeal symptoms last for more than three weeks, a referral should be made for further examination, including direct laryngoscopy. The prognosis for chronic laryngitis varies depending on the cause of the laryngitis.\n\n"}
{"id": "4319477", "url": "https://en.wikipedia.org/wiki?curid=4319477", "title": "List of cigarette smoke carcinogens", "text": "List of cigarette smoke carcinogens\n\nCommercial tobacco smoke is a mixture of more than 5,000 chemicals. According to the U.S. Department of Health and Human Services, the following are known human carcinogens found in cigarette smoke:\n"}
{"id": "4723694", "url": "https://en.wikipedia.org/wiki?curid=4723694", "title": "List of fatal alligator attacks in the United States", "text": "List of fatal alligator attacks in the United States\n\nThis is a list of fatal alligator attacks in the United States in reverse chronological order by decade. All occurred in the Southeast, where alligators are endemic to wetlands and tidal marshes.\n\nSpecies:\n\n\n"}
{"id": "47747598", "url": "https://en.wikipedia.org/wiki?curid=47747598", "title": "List of microbiologists", "text": "List of microbiologists\n\nMajor contributions to the science of microbiology (as a discipline in its modern sense) have spanned the time from the mid-17th century to the present day. The following is a list of prominent microbiologists who have made significant contributions to the study of microorganisms. Many of those listed have received a Nobel prize for their contributions to the field of microbiology. The others are typically considered historical figures whose work in microbiology had a notable impact in the field. Those microbiologists who currently work in the field have been excluded unless they have received recognition beyond that of being on the faculty in a college or university.\n\n"}
{"id": "1682927", "url": "https://en.wikipedia.org/wiki?curid=1682927", "title": "List of premature obituaries", "text": "List of premature obituaries\n\n<onlyinclude>\nA premature obituary is an obituary published whose subject is not actually deceased at the time of publication. Examples of premature obituaries include that of inventor, businessman, and chemist Alfred Nobel, whose premature obituary condemning him as a \"merchant of death\" may have caused him to create the Nobel Prize; black nationalist Marcus Garvey, whose actual death was apparently caused by reading his own obituary; and actor Abe Vigoda, who was the subject of so many premature obituaries that a website was created to state whether he was alive or dead.\n\nThis article lists the recipients of incorrect death reports (not just formal obituaries) from publications, media organisations, official bodies, and widely used information sources such as the Internet Movie Database; but not mere rumours of deaths, nor reports from sites which feature automated death hoax stories designed to draw in page clicks from specific web searches. People who were presumed (though not categorically declared) to be dead, and joke death reports that were widely believed, are also included.</onlyinclude>\n\nPremature obituaries may be published for reasons such as the following:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMultiple premature obituaries came to light on April 16, 2003, when it was discovered that pre-written draft memorials to several world figures were available on the development area of the CNN website without requiring a password (and may have been accessible for some time before). The pages included tributes to Fidel Castro (d. 2016), Dick Cheney, Nelson Mandela (d. 2013), Bob Hope (d. 2003), Gerald Ford (d. 2006), Pope John Paul II (d. 2005), and Ronald Reagan (d. 2004) (all of which claim they died in 2001).\n\nSome of these obituaries contained fragments taken from others, particularly from Queen Elizabeth the Queen Mother's obituary, which had apparently been used as a template. Dick Cheney for example was described as the \"UK's favorite grandmother\", the site noted the Pope's 'love of racing', and described Castro as \"lifeguard, athlete, movie star\" (a reference to Ronald Reagan). Although the Queen Mother was already dead, in an unrelated incident she had previously received a premature obituary of her own.\n\n\n\n"}
{"id": "19051", "url": "https://en.wikipedia.org/wiki?curid=19051", "title": "Manganese", "text": "Manganese\n\nManganese is a chemical element with symbol Mn and atomic number 25. It is not found as a free element in nature; it is often found in minerals in combination with iron. Manganese is a metal with important industrial metal alloy uses, particularly in stainless steels.\n\nHistorically, manganese is named for pyrolusite and other black minerals from the region of Magnesia in Greece, which also gave its name to magnesium and the iron ore magnetite. By the mid-18th century, Swedish-German chemist Carl Wilhelm Scheele had used pyrolusite to produce chlorine. Scheele and others were aware that pyrolusite (now known to be manganese dioxide) contained a new element, but they were unable to isolate it. Johan Gottlieb Gahn was the first to isolate an impure sample of manganese metal in 1774, which he did by reducing the dioxide with carbon.\n\nManganese phosphating is used for rust and corrosion prevention on steel. Ionized manganese is used industrially as pigments of various colors, which depend on the oxidation state of the ions. The permanganates of alkali and alkaline earth metals are powerful oxidizers. Manganese dioxide is used as the cathode (electron acceptor) material in zinc-carbon and alkaline batteries.\n\nIn biology, manganese(II) ions function as cofactors for a large variety of enzymes with many functions. Manganese enzymes are particularly essential in detoxification of superoxide free radicals in organisms that must deal with elemental oxygen. Manganese also functions in the oxygen-evolving complex of photosynthetic plants. While the element is a required trace mineral for all known living organisms, it also acts as a neurotoxin in larger amounts. Especially through inhalation, it can cause manganism, a condition in mammals leading to neurological damage that is sometimes irreversible.\n\nManganese is a silvery-gray metal that resembles iron. It is hard and very brittle, difficult to fuse, but easy to oxidize. Manganese metal and its common ions are paramagnetic. Manganese tarnishes slowly in air and oxidizes (\"rusts\") like iron in water containing dissolved oxygen.\n\nNaturally occurring manganese is composed of one stable isotope, Mn. Eighteen radioisotopes have been isolated and described, ranging in atomic weight from 46 u (Mn) to 65 u (Mn). The most stable are Mn with a half-life of 3.7 million years, Mn with a half-life of 312.3 days, and Mn with a half-life of 5.591 days. All of the remaining radioactive isotopes have half-lives of less than three hours, and the majority of less than one minute. The primary decay mode before the most abundant stable isotope, Mn, is electron capture and the primary mode after is beta decay. Manganese also has three meta states.\n\nManganese is part of the iron group of elements, which are thought to be synthesized in large stars shortly before the supernova explosion. Mn decays to Cr with a half-life of 3.7 million years. Because of its relatively short half-life, Mn is relatively rare, produced by cosmic rays impact on iron. Manganese isotopic contents are typically combined with chromium isotopic contents and have found application in isotope geology and radiometric dating. Mn–Cr isotopic ratios reinforce the evidence from Al and Pd for the early history of the solar system. Variations in Cr/Cr and Mn/Cr ratios from several meteorites suggest an initial Mn/Mn ratio, which indicates that Mn–Cr isotopic composition must result from \"in situ\" decay of Mn in differentiated planetary bodies. Hence, Mn provides additional evidence for nucleosynthetic processes immediately before coalescence of the solar system.\n\nThe most common oxidation states of manganese are +2, +3, +4, +6, and +7, though all oxidation states from −3 to +7 have been observed. Mn often competes with Mg in biological systems. Manganese compounds where manganese is in oxidation state +7, which are mostly restricted to the unstable oxide MnO, compounds of the intensely purple permanganate anion MnO, and a few oxyhalides (MnOF and MnOCl), are powerful oxidizing agents. Compounds with oxidation states +5 (blue) and +6 (green) are strong oxidizing agents and are vulnerable to disproportionation.\nThe most stable oxidation state for manganese is +2, which has a pale pink color, and many manganese(II) compounds are known, such as manganese(II) sulfate (MnSO) and manganese(II) chloride (MnCl). This oxidation state is also seen in the mineral rhodochrosite (manganese(II) carbonate). Manganese(II) most commonly exists with a high spin, S = 5/2 ground state because of the high pairing energy for manganese(II). However, there are a few examples of low-spin, S =1/2 manganese(II). There are no spin-allowed d–d transitions in manganese(II), explaining why manganese(II) compounds are typically pale to colorless.\n\nThe +3 oxidation state is known in compounds like manganese(III) acetate, but these are quite powerful oxidizing agents and also prone to disproportionation in solution, forming manganese(II) and manganese(IV). Solid compounds of manganese(III) are characterized by its strong purple-red color and a preference for distorted octahedral coordination resulting from the Jahn-Teller effect.\n\nThe oxidation state +5 can be produced by dissolving manganese dioxide in molten sodium nitrite. Manganate (VI) salts can be produced by dissolving Mn compounds, such as manganese dioxide, in molten alkali while exposed to air. Permanganate (+7 oxidation state) compounds are purple, and can give glass a violet color. Potassium permanganate, sodium permanganate, and barium permanganate are all potent oxidizers. Potassium permanganate, also called Condy's crystals, is a commonly used laboratory reagent because of its oxidizing properties; it is used as a topical medicine (for example, in the treatment of fish diseases). Solutions of potassium permanganate were among the first stains and fixatives to be used in the preparation of biological cells and tissues for electron microscopy.\n\nThe origin of the name manganese is complex. In ancient times, two black minerals from Magnesia (located within modern Greece) were both called \"magnes\" from their place of origin, but were thought to differ in gender. The male \"magnes\" attracted iron, and was the iron ore now known as lodestone or magnetite, and which probably gave us the term magnet. The female \"magnes\" ore did not attract iron, but was used to decolorize glass. This feminine \"magnes\" was later called \"magnesia\", known now in modern times as pyrolusite or manganese dioxide. Neither this mineral nor elemental manganese is magnetic. In the 16th century, manganese dioxide was called \"manganesum\" (note the two Ns instead of one) by glassmakers, possibly as a corruption and concatenation of two words, since alchemists and glassmakers eventually had to differentiate a \"magnesia negra\" (the black ore) from \"magnesia alba\" (a white ore, also from Magnesia, also useful in glassmaking). Michele Mercati called magnesia negra \"manganesa\", and finally the metal isolated from it became known as \"manganese\" (German: \"Mangan\"). The name \"magnesia\" eventually was then used to refer only to the white magnesia alba (magnesium oxide), which provided the name magnesium for the free element when it was isolated much later.\n\nSeveral colorful oxides of manganese, for example manganese dioxide, are abundant in nature and have been used as pigments since the Stone Age. The cave paintings in Gargas that are 30,000 to 24,000 years old contain manganese pigments.\n\nManganese compounds were used by Egyptian and Roman glassmakers, either to add to, or remove color from glass. Use as \"glassmakers soap\" continued through the Middle Ages until modern times and is evident in 14th-century glass from Venice.\n\nBecause it was used in glassmaking, manganese dioxide was available for experiments by alchemists, the first chemists. Ignatius Gottfried Kaim (1770) and Johann Glauber (17th century) discovered that manganese dioxide could be converted to permanganate, a useful laboratory reagent. By the mid-18th century, the Swedish chemist Carl Wilhelm Scheele used manganese dioxide to produce chlorine. First, hydrochloric acid, or a mixture of dilute sulfuric acid and sodium chloride was made to react with manganese dioxide, later hydrochloric acid from the Leblanc process was used and the manganese dioxide was recycled by the Weldon process. The production of chlorine and hypochlorite bleaching agents was a large consumer of manganese ores.\n\nScheele and other chemists were aware that manganese dioxide contained a new element, but they were not able to isolate it. Johan Gottlieb Gahn was the first to isolate an impure sample of manganese metal in 1774, by reducing the dioxide with carbon.\n\nThe manganese content of some iron ores used in Greece led to speculations that steel produced from that ore contains additional manganese, making the Spartan steel exceptionally hard. Around the beginning of the 19th century, manganese was used in steelmaking and several patents were granted. In 1816, it was documented that iron alloyed with manganese was harder but not more brittle. In 1837, British academic James Couper noted an association between miners' heavy exposure to manganese with a form of Parkinson's disease. In 1912, United States patents were granted for protecting firearms against rust and corrosion with manganese phosphate electrochemical conversion coatings, and the process has seen widespread use ever since.\n\nThe invention of the Leclanché cell in 1866 and the subsequent improvement of batteries containing manganese dioxide as cathodic depolarizer increased the demand for manganese dioxide. Until the development of batteries with nickel-cadmium and lithium, most batteries contained manganese. The zinc-carbon battery and the alkaline battery normally use industrially produced manganese dioxide because naturally occurring manganese dioxide contains impurities. In the 20th century, manganese dioxide was widely used as the cathodic for commercial disposable dry batteries of both the standard (zinc-carbon) and alkaline types.\n\nManganese comprises about 1000 ppm (0.1%) of the Earth's crust, the 12th most abundant of the crust's elements. Soil contains 7–9000 ppm of manganese with an average of 440 ppm. Seawater has only 10 ppm manganese and the atmosphere contains 0.01 µg/m. Manganese occurs principally as pyrolusite (MnO), braunite, (MnMn)(SiO), psilomelane (Ba,HO)MnO, and to a lesser extent as rhodochrosite (MnCO).\n\nThe most important manganese ore is pyrolusite (MnO). Other economically important manganese ores usually show a close spatial relation to the iron ores. Land-based resources are large but irregularly distributed. About 80% of the known world manganese resources are in South Africa; other important manganese deposits are in Ukraine, Australia, India, China, Gabon and Brazil. According to 1978 estimate, the ocean floor has 500 billion tons of manganese nodules. Attempts to find economically viable methods of harvesting manganese nodules were abandoned in the 1970s. The CIA once used mining manganese modules on the ocean floor as a cover story for recovering a sunken Soviet submarine.\n\nIn South Africa, most identified deposits are located near Hotazel in the Northern Cape Province, with a 2011 estimate of 15 billion tons. In 2011 South Africa produced 3.4 million tons, topping all other nations.\n\nManganese is mainly mined in South Africa, Australia, China, Gabon, Brazil, India, Kazakhstan, Ghana, Ukraine and Malaysia. US Import Sources (1998–2001): Manganese ore: Gabon, 70%; South Africa, 10%; Australia, 9%; Mexico, 5%; and other, 6%. Ferromanganese: South Africa, 47%; France, 22%; Mexico, 8%; Australia, 8%; and other, 15%. Manganese contained in all manganese imports: South Africa, 31%; Gabon, 21%; Australia, 13%; Mexico, 8%; and other, 27%.\n\nFor the production of ferromanganese, the manganese ore is mixed with iron ore and carbon, and then reduced either in a blast furnace or in an electric arc furnace. The resulting ferromanganese has a manganese content of 30 to 80%. Pure manganese used for the production of iron-free alloys is produced by leaching manganese ore with sulfuric acid and a subsequent electrowinning process.\nA more progressive extraction process involves directly reducing manganese ore in a heap leach. This is done by percolating natural gas through the bottom of the heap; the natural gas provides the heat (needs to be at least 850 °C) and the reducing agent (carbon monoxide). This reduces all of the manganese ore to manganese oxide (MnO), which is a leachable form. The ore then travels through a grinding circuit to reduce the particle size of the ore to between 150–250 μm, increasing the surface area to aid leaching. The ore is then added to a leach tank of sulfuric acid and ferrous iron (Fe) in a 1.6:1 ratio. The iron reacts with the manganese dioxide to form iron hydroxide and elemental manganese. This process yields approximately 92% recovery of the manganese. For further purification, the manganese can then be sent to an electrowinning facility.\n\nIn 1972 the CIA's Project Azorian, through billionaire Howard Hughes, commissioned the ship \"Hughes Glomar Explorer\" with the cover story of harvesting manganese nodules from the sea floor. That triggered a rush of activity to collect manganese nodules, which was not actually practical. The real mission of \"Hughes Glomar Explorer\" was to raise a sunken Soviet submarine, the K-129, with the goal of retrieving Soviet code books.\n\nManganese has no satisfactory substitute in its major applications in metallurgy. In minor applications, (e.g., manganese phosphating), zinc and sometimes vanadium are viable substitutes.\n\nManganese is essential to iron and steel production by virtue of its sulfur-fixing, deoxidizing, and alloying properties, as first recognized by the British metallurgist Robert Forester Mushet (1811–1891) who, in 1856, introduced the element, in the form of Spiegeleisen, into steel for the specific purpose of removing excess dissolved oxygen, sulfur, and phosphorus in order to improve its malleability. Steelmaking, including its ironmaking component, has accounted for most manganese demand, presently in the range of 85% to 90% of the total demand. Manganese is a key component of low-cost stainless steel. Often ferromanganese (usually about 80% manganese) is the intermediate in modern processes.\n\nSmall amounts of manganese improve the workability of steel at high temperatures by forming a high-melting sulfide and preventing the formation of a liquid iron sulfide at the grain boundaries. If the manganese content reaches 4%, the embrittlement of the steel becomes a dominant feature. The embrittlement decreases at higher manganese concentrations and reaches an acceptable level at 8%. Steel containing 8 to 15% of manganese has a high tensile strength of up to 863 MPa. Steel with 12% manganese was discovered in 1882 by Robert Hadfield and is still known as Hadfield steel (mangalloy). It was used for British military steel helmets and later by the U.S. military.\n\nThe second largest application for manganese is in aluminium alloys. Aluminium with roughly 1.5% manganese has increased resistance to corrosion through grains that absorb impurities which would lead to galvanic corrosion. The corrosion-resistant aluminium alloys 3004 and 3104 (0.8 to 1.5% manganese) are used for most beverage cans. Before 2000, more than 1.6 million tonnes of those alloys were used; at 1% manganese, this consumed 16,000 tonnes of manganese.\n\nMethylcyclopentadienyl manganese tricarbonyl is used as an additive in unleaded gasoline to boost octane rating and reduce engine knocking. The manganese in this unusual organometallic compound is in the +1 oxidation state.\n\nManganese(IV) oxide (manganese dioxide, MnO) is used as a reagent in organic chemistry for the oxidation of benzylic alcohols (where the hydroxyl group is adjacent to an aromatic ring). Manganese dioxide has been used since antiquity to oxidize and neutralize the greenish tinge in glass from trace amounts of iron contamination. MnO is also used in the manufacture of oxygen and chlorine and in drying black paints. In some preparations, it is a brown pigment for paint and is a constituent of natural umber.\n\nManganese(IV) oxide was used in the original type of dry cell battery as an electron acceptor from zinc, and is the blackish material in carbon–zinc type flashlight cells. The manganese dioxide is reduced to the manganese oxide-hydroxide MnO(OH) during discharging, preventing the formation of hydrogen at the anode of the battery.\n\nThe same material also functions in newer alkaline batteries (usually battery cells), which use the same basic reaction, but a different electrolyte mixture. In 2002, more than 230,000 tons of manganese dioxide was used for this purpose.\n\nThe metal is occasionally used in coins; until 2000, the only United States coin to use manganese was the from 1942 to 1945. An alloy of 75% copper and 25% nickel was traditionally used for the production of nickel coins. However, because of shortage of nickel metal during the war, it was substituted by more available silver and manganese, thus resulting in an alloy of 56% copper, 35% silver and 9% manganese. Since 2000, dollar coins, for example the Sacagawea dollar and the Presidential $1 coins, are made from a brass containing 7% of manganese with a pure copper core. In both cases of nickel and dollar, the use of manganese in the coin was to duplicate the electromagnetic properties of a previous identically sized and valued coin in the mechanisms of vending machines. In the case of the later U.S. dollar coins, the manganese alloy was intended to duplicate the properties of the copper/nickel alloy used in the previous Susan B. Anthony dollar. \n\nManganese compounds have been used as pigments and for the coloring of ceramics and glass. The brown color of ceramic is sometimes the result of manganese compounds. In the glass industry, manganese compounds are used for two effects. Manganese(III) reacts with iron(II) to induce a strong green color in glass by forming less-colored iron(III) and slightly pink manganese(II), compensating for the residual color of the iron(III). Larger quantities of manganese are used to produce pink colored glass.\n\nTetravalent manganese is used as an activator in red-emitting phosphors. While many compounds are known which show luminescence the majority is not used in commercial application due to low efficiency or deep red emission. However, several Mn aktivated fluorides were reported as potential red emitting phorshors for warm-white LEDs. But to this day, only KSiF:Mn is commercially available for use in warm-white LEDs.\n\nThe manganese oxide is also used in portland cement mixtures.\n\nManganese is an important element for human health, essential for development, metabolism, and the antioxidant system. Nevertheless, excessive exposure or intake may lead to a condition known as manganism, a neurodegenerative disorder that causes dopaminergic neuronal death and symptoms similar to Parkinson's disease. The classes of enzymes that have manganese cofactors is large and includes oxidoreductases, transferases, hydrolases, lyases, isomerases, ligases, lectins, and integrins. The reverse transcriptases of many retroviruses (though not lentiviruses such as HIV) contain manganese. The best-known manganese-containing polypeptides may be arginase, the diphtheria toxin, and Mn-containing superoxide dismutase (Mn-SOD).\n\nMn-SOD is the type of SOD present in eukaryotic mitochondria, and also in most bacteria (this fact is in keeping with the bacterial-origin theory of mitochondria). The Mn-SOD enzyme is probably one of the most ancient, for nearly all organisms living in the presence of oxygen use it to deal with the toxic effects of superoxide (), formed from the 1-electron reduction of dioxygen. The exceptions, which are all bacteria, include \"Lactobacillus plantarum\" and related lactobacilli, which use a different nonenzymatic mechanism with manganese (Mn) ions complexed with polyphosphate, suggesting a path of evolution for this function in aerobic life.\n\nThe human body contains about 12 mg of manganese, mostly in the bones. The soft tissue remainder is concentrated in the liver and kidneys. In the human brain, the manganese is bound to manganese metalloproteins, most notably glutamine synthetase in astrocytes.\n\nManganese is also important in photosynthetic oxygen evolution in chloroplasts in plants. The oxygen-evolving complex (OEC) is a part of photosystem II contained in the thylakoid membranes of chloroplasts; it is responsible for the terminal photooxidation of water during the light reactions of photosynthesis, and has a metalloenzyme core containing four atoms of manganese. To fulfill this requirement, most broad-spectrum plant fertilizers contain manganese.\n\nThe U.S. Institute of Medicine (IOM) updated Estimated Average Requirements (EARs) and Recommended Dietary Allowances (RDAs) for minerals in 2001. For manganese there was not sufficient information to set EARs and RDAs, so needs are described as estimates for Adequate Intakes (AIs). As for safety, the IOM sets Tolerable upper intake levels (ULs) for vitamins and minerals when evidence is sufficient. In the case of manganese the adult UL is set at 11 mg/day. Collectively the EARs, RDAs, AIs and ULs are referred to as Dietary Reference Intakes (DRIs). Manganese deficiency is rare.\n\nThe European Food Safety Authority (EFSA) refers to the collective set of information as Dietary Reference Values, with Population Reference Intake (PRI) instead of RDA, and Average Requirement instead of EAR. AI and UL defined the same as in United States. For people ages 15 and older the AI is set at 3.0 mg/day. AIs for pregnancy and lactation is 3.0 mg/day. For children ages 1–14 years the AIs increase with age from 0.5 to 2.0 mg/day. The adult AIs are higher than the U.S. RDAs. The EFSA reviewed the same safety question and decided that there was insufficient information to set a UL.\n\nFor U.S. food and dietary supplement labeling purposes the amount in a serving is expressed as a percent of Daily Value (%DV). For manganese labeling purposes 100% of the Daily Value was 2.0 mg, but as of May 27, 2016 it was revised to 2.3 mg to bring it into agreement with the RDA. A table of the old and new adult Daily Values is provided at Reference Daily Intake. Food and supplement companies have until January 1, 2020 to comply with the change.\n\nManganese compounds are less toxic than those of other widespread metals, such as nickel and copper. However, exposure to manganese dusts and fumes should not exceed the ceiling value of 5 mg/m even for short periods because of its toxicity level. Manganese poisoning has been linked to impaired motor skills and cognitive disorders.\n\nThe permanganate exhibits a higher toxicity than the manganese(II) compounds. The fatal dose is about 10 g, and several fatal intoxications have occurred. The strong oxidative effect leads to necrosis of the mucous membrane. For example, the esophagus is affected if the permanganate is swallowed. Only a limited amount is absorbed by the intestines, but this small amount shows severe effects on the kidneys and on the liver.\n\nManganese exposure in United States is regulated by the Occupational Safety and Health Administration (OSHA). People can be exposed to manganese in the workplace by breathing it in or swallowing it. OSHA has set the legal limit (permissible exposure limit) for manganese exposure in the workplace as 5 mg/m over an 8-hour workday. The National Institute for Occupational Safety and Health (NIOSH) has set a recommended exposure limit (REL) of 1 mg/m over an 8-hour workday and a short term limit of 3 mg/m. At levels of 500 mg/m, manganese is immediately dangerous to life and health.\n\nGenerally, exposure to ambient Mn air concentrations in excess of 5 μg Mn/m3 can lead to Mn-induced symptoms. Increased ferroportin protein expression in human embryonic kidney (HEK293) cells is associated with decreased intracellular Mn concentration and attenuated cytotoxicity, characterized by the reversal of Mn-reduced glutamate uptake and diminished lactate dehydrogenase leakage.\n\nWaterborne manganese has a greater bioavailability than dietary manganese. According to results from a 2010 study, higher levels of exposure to manganese in drinking water are associated with increased intellectual impairment and reduced intelligence quotients in school-age children. It is hypothesized that long-term exposure due to inhaling the naturally occurring manganese in shower water puts up to 8.7 million Americans at risk. However, data indicates that the human body can recover from certain adverse effects of overexposure to manganese if the exposure is stopped and the body can clear the excess.\n\nMethylcyclopentadienyl manganese tricarbonyl (MMT) is a gasoline additive used to replace lead compounds for unleaded gasolines to improve the octane rating of low octane petroleum distillates. It reduces engine knock agent through the action of the carbonyl groups. Fuels containing manganese tend to form manganese carbides, which damage exhaust valves. Compared to 1953, levels of manganese in air have dropped. Many racing competitions specifically ban manganese compounds in racing fuel for carts and minibikes. MMT contains 24.4–25.2% manganese. Elevated atmospheric manganese concentrations are strongly correlated with automobile traffic density. The Level of manganese emitted by MMT fuels has been found to be safe for the general population and vulnerable groups such as infants and the elderly by the EPA and European and Canadian environmental agencies.\n\nThe tobacco plant readily absorbs and accumulates heavy metals such as manganese from the surrounding soil into its leaves. These are subsequently inhaled during tobacco smoking. While manganese is a constituent of tobacco smoke, studies have largely concluded that concentrations are not hazardous for human health.\n\nManganese overexposure is most frequently associated with manganism, a rare neurological disorder associated with excessive manganese ingestion or inhalation. Historically, persons employed in the production or processing of manganese alloys have been at risk for developing manganism; however, current health and safety regulations protect workers in developed nations. The disorder was first described in 1837 by British academic John Couper, who studied two patients who were manganese grinders.\n\nManganism is a biphasic disorder. In its early stages, an intoxicated person may experience depression, mood swings, compulsive behaviors, and psychosis. Early neurological symptoms give way to late-stage manganism, which resembles Parkinson's disease. Symptoms include weakness, monotone and slowed speech, an expressionless face, tremor, forward-leaning gait, inability to walk backwards without falling, rigidity, and general problems with dexterity, gait and balance. Unlike Parkinson's disease, manganism is not associated with loss of the sense of smell and patients are typically unresponsive to treatment with L-DOPA. Symptoms of late-stage manganism become more severe over time even if the source of exposure is removed and brain manganese levels return to normal.\n\nChronic manganese exposure has been shown to produce a parkinsonism-like illness characterized by movement abnormalities. This condition is not responsive to typical therapies used in the treatment of PD, suggesting an alternative pathway than the typical dopaminergic loss within the substantia nigra. Manganese may accumulate in the basal ganglia, leading to the abnormal movements. A mutation of the SLC30A10 gene, a manganese efflux transporter necessary for decreasing intracellular Mn, has been linked with the development of this Parkinsonism-like disease. The Lewy bodies typical to PD are not seen in Mn-induced parkinsonism.\n\nAnimal experiments have given the opportunity to examine the consequences of manganese overexposure under controlled conditions. In (non-aggressive) rats, manganese induces mouse-killing behavior.\n\nSeveral recent studies attempt to examine the effects of chronic low-dose manganese overexposure on child development. The earliest study was conducted in the Chinese province of Shanxi. Drinking water there had been contaminated through improper sewage irrigation and contained 240–350 µg Mn/L. Although Mn concentrations at or below 300 µg Mn/L were considered safe at the time of the study by the US EPA and 400 µg Mn/L by the World Health Organization, the 92 children sampled (between 11 and 13 years of age) from this province displayed lower performance on tests of manual dexterity and rapidity, short-term memory, and visual identification, compared to children from an uncontaminated area. More recently, a study of 10-year-old children in Bangladesh showed a relationship between Mn concentration in well water and diminished IQ scores. A third study conducted in Quebec examined school children between the ages of 6 and 15 living in homes that received water from a well containing 610 µg Mn/L; controls lived in homes that received water from a 160 µg Mn/L well. Children in the experimental group showed increased hyperactive and oppositional behavior.\n\nThe current maximum safe concentration under EPA rules is 50 µg Mn/L.\n\nA protein called DMT1 is the major transporter in manganese absorption from the intestine, and may be the major transporter of manganese across the blood–brain barrier. DMT1 also transports inhaled manganese across the nasal epithelium. The proposed mechanism for manganese toxicity is that dysregulation leads to oxidative stress, mitochondrial dysfunction, glutamate-mediated excitoxicity, and aggregation of proteins.\n\n\n"}
{"id": "55510040", "url": "https://en.wikipedia.org/wiki?curid=55510040", "title": "Maude C. Davison", "text": "Maude C. Davison\n\nMaude Campbell was born on 27 March 1885 in Cannington, Ontario, Canada to Janet (or Jeannette) Campbell. In 1894, her mother, who had immigrated from Scotland, married Abraham Sidders. She graduated in 1909 from the Ontario Agricultural College with a certificate from the MacDonald School of Home Economics. \n\nCampbell began her career as a dietitian at the Baptist College in Brandon, Manitoba. Immigrating to the United States in 1909, she took employment in South Bend, Indiana at the Epworth Hospital as a dietitian and instructor in domestic science and remained until 1911. She returned from Canada in 1914 and entered the Pasadena Hospital Training School for Nurses. In 1917, she graduated having earned her RN designation. The following year, she joined the Nurse Reserves of the United States Army Nurse Corps and began working as a staff nurse at the base hospital of Camp Fremont in Palo Alto, California. After serving at Letterman General Hospital, in San Francisco, she was sent to Fort Leavenworth, Kansas in 1920, to take up a post at the hospital for the United States Disciplinary Barracks. With this move, she became an American citizen and was transferred as a second lieutenant to the Regular Army of the Nurse Corps. Between 1921 and 1922, she was deployed to Coblenz, Germany, serving with the Allied Occupation Forces assisting with Russian famine refugees, influenza victims and war casualties. Returning to the United States, in 1924, she was promoted to first lieutenant after passing the Chief Nursing Examination. \n\nDavison entered Columbia University in 1926 and earned a bachelor’s degree in home economics in 1928. Upon completion of her education, she returned to service as a nurse and dietician at several Army hospitals throughout the US. In 1939, she was deployed during World War II to Fort Mills Station Hospital on Corregidor Island in the Philippines. She was promoted to captain in 1941 and placed as chief nurse of the nursing corps of the Philippine Department. Most of the nurses in the Far East Command were serving under Davison with her second-in-command, Josephine Nesbit, at Sternberg Hospital on the south side of Manila Bay. When the Japanese invaded the Philippines, on 8 December 1941, the day after the bombing of Pearl Harbor, Davison organized civilian nurses to help with the casualties, sending five Army nurses and fifteen local Filipino nurses to the facility at Fort Stotsenburg. Within a week the Fort, along with other military facilities, was in ruins and the nurses were prepared for evacuation back to Sternberg.\n\nBefore Christmas, Davison was injured in a bombing raid and turned command over to Nesbit. Between Christmas and New Year's Eve 1941, all the army nurses were evacuated from Manila and sent to Bataan. Davison left with the last of the American troops for Corregidor to coordinate the nursing activities in establishing two jungle hospitals, known simply as Hospital #1 and Hospital #2. From these field hospitals, the nurses carried out battlefield nursing. Simultaneously, she directed nurses in setting up the hospital where the troops on Corregidor had been sent in the Malinta Tunnel. The underground hospital had one central hallway that was one-hundred-yards long and eight wards established in lateral corridors. In April 1942, as Bataan fell, the nurses, including the Filipino civilians, were evacuated to Corregidor and the tunnel hospital. At the end of the month, when it became evident that Corregidor would also fall, an attempt was made to evacuate some of the nurses. Davison and Colonel Wibb Cooper, the ranking medical officer, made the selections of who would be evacuated. Though Davison later said the twenty evacuees were chosen randomly, the nurses saw through her ruse, noting that those who were ill, wounded or fatigued or might not withstand the pressure of imprisonment were chosen. \n\nUpon the Allied surrender in May 1942, Davison led her 66 remaining nurses to their captivity at Santo Tomas Internment Camp in Manila. They joined 11 United States Navy Nurse Corps personnel under the command of Lieutenant Commander Laura Cobb, who had surrendered to the Japanese the previous January. In September, ten of the nurses who had been part of the April evacuation joined them as their aircraft was damaged while refueling enroute to Australia and they were captured. The nurses came to be known as the Angels of Bataan and were the first and largest group of American military women taken as Prisoners of War (POWs). Known as a strict disciplinarian, she required her nurses to follow her rules and army regulations to the letter, despite the fact that they were in a Japanese-run camp. Interned as POWs, she organized the prison camp hospital and continued managing her staff. Conditions in the camp caused the death of 390 of the 3,785 inmates, but none of the nurses were among the dead. \n\nAfter three years, on 3 February 1945, the camp was liberated and Davison was hospitalized because of her poor health. When the nurses arrived in the United States at the end of the month, Davison, who normally weighed weighed only . Her nurses credited Davison with their survival and though she was nominated for the Distinguished Service Medal, the War Decorations Board denied the honor, based upon a determination that she did not act independently but under the advice of the physicians and military commanders with whom she served. She was awarded the Legion of Merit and medically retired on 31 January 1946. In 1947, Davison married the Reverend Charles W. Jackson, who had served as dean of Long Beach City College. The two had met many years earlier when she was working at the Baptist College and she had rented a room from his family, which had immigrated to the United States, during her nursing studies in Pasadena, California. Jackson, a widower, had two grown sons from a prior marriage who found \"Davy\", as they called Davison, distant and formal. After her marriage, she rarely had contact with her former staff, but in 1955 she participated in a Veterans Day parade in Los Angeles, where she received a special citation of merit.\n\nJackson died on 11 June 1956 at the Veterans Hospital in Long Beach, California following a lengthy illness. She was buried near her mother in the Cedar Vale Cemetery, Cannington, Ontario, Canada. In 2001, she was posthumously recognized with the Distinguished Service Medal due to the efforts of the surviving \"Angels\" such as Brigadier General Connie L. Slewitzke, Senator Daniel Inouye, and many others.\n\nWorld War I:\n\nWorld War II:\n\n"}
{"id": "10188365", "url": "https://en.wikipedia.org/wiki?curid=10188365", "title": "Maximum intercuspation", "text": "Maximum intercuspation\n\nIn dentistry, maximum intercuspation refers to the occlusal position of the mandible in which the cusps of the teeth of both arches fully interpose themselves with the cusps of the teeth of the opposing arch. This position used to be referred to as \"centric occlusion\".\n\nThis is an important jaw position, as it defines both the anterior-posterior and lateral relationships of the mandible and the maxilla, as well as the superior-inferior relationship known as the vertical dimension of occlusion. These are important considerations when evaluating a patient orthodontically, as well as restoring them prosthodontically.\n\n\n"}
{"id": "12641570", "url": "https://en.wikipedia.org/wiki?curid=12641570", "title": "Minister of Health (New Zealand)", "text": "Minister of Health (New Zealand)\n\nThe Minister of Health, formerly styled Minister of Public Health, is a minister in the government of New Zealand with responsibility for the New Zealand Ministry of Health and the District Health Boards.\n\nThe present Minister is David Clark, a member of the Labour Party.\n\nThe first Minister of Public Health was appointed in 1900, during the premiership of Richard Seddon. The word \"Public\" was dropped from the title when Sir Māui Pōmare took over the portfolio from 27 June 1923, as simply \"Minister of Health\".\n\nIn the health system reforms of the 1980s, the Department of Health lost responsibility for both the provision and funding of healthcare — these roles were transferred to separate Crown Health Enterprises (the precursors to today's District Health Boards) and the Health Funding Authority, respectively. The only function remaining was policy-making (resulting in the Department being renamed a Ministry). For a time, there was a separate Minister in Charge of Crown Health Enterprises, who was not necessarily the same as the Minister of Health. Further reforms have changed this, however — the Health Funding Authority has been re-absorbed into the Ministry of Health, and the modern District Health Boards, while not part of the Ministry, are considered a responsibility of the Minister of Health.\n\n"}
{"id": "10179639", "url": "https://en.wikipedia.org/wiki?curid=10179639", "title": "Occlusion (dentistry)", "text": "Occlusion (dentistry)\n\nOcclusion, in a dental context, means simply the contact between teeth. More technically, it is the relationship between the maxillary (upper) and mandibular (lower) teeth when they approach each other, as occurs during chewing or at rest.\n\nMalocclusion is the misalignment of teeth and jaws, or more simply, a \"bad bite\". Malocclusion can cause a number of health and dental problems. Malocclusion occurs as a result of disturbances in normal occlusal development.\n\nStatic occlusion refers to contact between teeth when the jaw is closed and stationary, while dynamic occlusion refers to occlusal contacts made when the jaw is moving. Dynamic occlusion is also termed as articulation. During chewing, there is no tooth contact between the teeth on the chewing side of the mouth.\n\nCentric occlusion is a relationship between upper and lower teeth when they come together, teeth do not need to be in centric relation in order to be in centric occlusion, i.e. the condyle may be anywhere within the glenoid fossa when the teeth are in centric occlusion. Centric occlusion is the first tooth contact and may or may not coincide with maximum intercuspation. It is also referred to as a person's habitual bite, bite of convenience, or intercuspation position (ICP). \"Centric relation\", not to be confused with \"centric occlusion\", is a relationship between the maxilla and mandible.\n\nOcclusion is defined as the act of opening and closing, while in dentistry its definition is broader and includes the contact of the teeth in the functional and parafunctional movements. In addition, it includes the development and function of the masticatory system. Over the course of history there have been several occlusion concepts, which tend to vary based on the specialty of dentistry. Initially the concepts were based on complete dentures.\n\nCentric occlusion (CO) is the occlusion, or position, of opposing teeth when the mandible is in centric relation. Centric relation (CR) is the relationship between the maxilla and the mandible with regards to the position of the mandibular condyle articulating with the thinnest avascular portion of the respective disks in the anterior-superior position. This position does not require the teeth to be in contact, but it is possible. Maximal intercuspation (maximal intercuspal position or MIP) is the full intercuspation of the opposing teeth in the dental arches regardless of condylar position.\n\nThe concept of ideal occlusion varies based on a case by case basis, the goal of the dentist is to choose a model that reduces vertical and horizontal stresses, provides MIP during CR.\n\nBalanced Occlusion is defined as the bilateral, simultaneous, anterior, and posterior occlusal contact of teeth in centric and eccentric positions. Balanced occlusion in complete dentures is unique, as it does not occur with natural teeth.\n\nThis concept of occlusion is based on the efforts of Curve of Spee and Monson's spherical theory and is known also as \"fully balanced occlusion\" or \"bilateral balanced occlusion.\" This occlusion concept requires that all the teeth be in contact during both maximum intercuspation and eccentric mandibular movements. During masticatory movements, there are no vertical forces, just horizontal ones. The lateral forces that are produced get directed upon the periodontal ligaments and are distributed through the contact area created through the occlusion scheme.\n\nThe concept is ideal for those receiving dentures, but is difficult to obtain on those with normal dentition. If this occlusal scheme is found in a patient it typically indicates that there is advanced wear on the dentition. An “ideal” dentition is based on the end goals of restorations involved with orthodontics, dentures and for some, full mouth rehabilitation. The occlusion is therefore not static. It is a continuous functional relationship between the maxillary (upper) and mandibular (lower) teeth. Any disturbances to the masticatory (chewing) system can be due to malocclusion, occlusal dysfunction and conditions such as bruxism (teeth grinding). Patients with bruxism may be treated with preventative therapy including stabilisation with an occlusal bite plane splint. At present, there is not enough evidence to suggest that non-surgical therapy is effective in patients experiencing teeth during sleep bruxism.\n\nUnilateral balanced occlusion is a type of occlusion seen on occlusal surfaces of teeth on one side when they occlude simultaneously with a smooth, uninterrupted glide. This is not followed during complete denture preparation. It is more pertained to fixed partial dentures.\n\nBilateral balanced occlusion is a type of occlusion that is seen when a simultaneous contract occurs on both sides in centric and eccentric positions. Bilateral balanced occlusion helps to distribute the occlusal load evenly across the arch and therefore helps to improve the stability of the denture \n\nThe occlusal scheme was established in 1974 by Dawson that used data study in 1960 revealed a set of patients whose molars did not contact in eccentric movements, while the anterior teeth did not contact in maximum intercuspation. In order to classify as mutually protected occlusion the following criteria must be met:\nIn protrusive movements, the canine and the posterior teeth are protected by the incisors, while in a lateral movement the incisors and posteriors are protected by the canines. The posterior teeth protect the anteriors in a centric position , which reduces the load onto the temporomandibular joint.\n\nMutually protected occlusion is thought to be the best scheme for natural dentition as the cusp-to-fossa relationship provides maximum support for centric relation. Another reason is that the forces are directed almost completely along the long axis of the tooth. The scheme is not recommended in patients with compromised periodontium or or if the patient has a horizontal masitcatory cycle.\n\nAs the teeth start to erupt at six months, the maxillary (upper) and mandibular (lower) primary (baby) teeth aim to occlude with two teeth of the opposing jaw. The two exceptions to this are the upper left and right central incisor and the lower left and right second molar. Normal alignment and occlusion of the two arches should be achieved after 2 years of age, with the full formation of the roots by the time the child has reached 3 years. The jaws develop in such a way that after one year, a diastema (interdental space) has formed between some teeth. This is more prominent in the anterior teeth as the jaws and permanent teeth grow.\n\nExtra-oral Assessment\nLoss of teeth and occlusal stops can result in over-closure causing a reduced face height. Over-closure is unlikely for patients with tooth wear due to dento-alveolar compensation. Over-eruption may occur for patients due to dento-alveolar development in absence of tooth wear which may result in increased face height.\nThe maximum extent the patient can open is measured between the incisal edges of the upper and lower incisors. Deviation of mandible on opening or closing should be described. Clicking, crepitus and tenderness of the jaw should be noted as well.\n\nIntra-Oral Assessment\nICP is defined as position of the jaws when there is maximum intercuspation of the maxillary and mandibular teeth. Stability of occlusion in ICP is essential or further dental work will be complicated.\nRCP refers to the most comfortable posterior location of the mandible when it is bilaterally manipulated backwards and upwards into a retrusive position. Terminal hinge axis refers to an imaginary axis drawn through the center of the head of both condyles when the mandible opens and closes on an arc of curvature. When the mandible closes in the Terminal Hinge Axis, the first tooth contact refers to RCP. \nProtrusion\n\nCondyles move from the glenoid fossa in a forward and downward movement onto the articular eminence when the mandible moves into protrusion. Condylar inclination refers to the angle protrusion makes when the horizontal when the patient is sitting upright\n\nIn protrusion, contact between the teeth is governed by incisor relationship and guidance. For instance, the mandibular movement of patients with Class I incisors relationships would be inferiorly resulting in separation of the posterior teeth. This is to overcome the natural overbite of the Class I relationship for the mandible to make a protrusive movement.\n\nLateral Excursion\n\nThe side to which the mandible moves is called the working side and the opposite side is the non-working side. Bennett Movement refers to the lateral movement of the working side condyle when the mandible moves laterally. Bennett’s Angle is measured at the non-working condyle when it moves forward and medially during lateral excursion\n\nLateral excursion of the mandible is usually governed by Canine guidance or Group Function at the working side. In some cases, teeth at the non-working side can also be in contact when the condylar inclination is shallow or if the tooth guidance on the working side is shallow.\n\nMalocclusion is the result of the body trying to optimize its function in a dysfunctional environment. For example, the maxilla (upper jaw) can be placed too far anteriorly compared to the mandible (lower jaw). This would be called a Class II Malocclusion. If the mandible is placed too far posterior compared to the maxilla, it would be a Class III malocclusion. Malocclusion can can also be associated with a number of problems:\nMalocclusion can cause teeth, fillings, and crowns to wear, break, or loosen, and teeth may be tender or ache. \nReceding gums can be exacerbated by a faulty bite.\nIf the jaw is mispositioned, jaw muscles may have to work harder, which can lead to fatigue and or muscle spasms. This in turn can lead to headaches or migraines, eye or sinus pain, and pain in the neck, shoulder, or even back. \nMalocclusion can be a contributing factor to sleep disordered breathing which may include snoring, upper airway resistance syndrome, and / or sleep apnea (apnea means without breath). Untreated damaging malocclusion can lead to occlusal trauma.\n\nTreatment for occlusal problems\nSome of the treatments for different occlusal problems include protecting the teeth with dental splints (orthotics), tooth adjustments, replacement of teeth, medication (usually temporary), a diet of softer foods, TENS to relax tensed muscles, and relaxation therapy for stress-related clenching. Removable dental appliances may be used to alter the development of the jaws. Fixed appliances such as braces may be used to move the teeth in the jaws. Jaw surgery is also used to correct malocclusion.\n</div>\n\nCitations\nBibliography\n"}
{"id": "5301306", "url": "https://en.wikipedia.org/wiki?curid=5301306", "title": "Portable water purification", "text": "Portable water purification\n\nPortable water purification devices are self-contained, easily transported units used to purify water from untreated sources (such as rivers, lakes, and wells) for drinking purposes. Their main function is to eliminate pathogens, and often also of suspended solids and some unpalatable or toxic compounds.\n\nThese units provide an autonomous supply of drinking water to people without access to clean water supply services, including inhabitants of developing countries and disaster areas, military personnel, campers, hikers, and workers in wilderness, and survivalists. They are also called point-of-use (POU) water treatment systems and field water disinfection techniques.\n\nTechniques include heat (including boiling), filtration, activated charcoal adsorption, chemical disinfection (e.g. chlorination, iodine, ozonation, etc.), ultraviolet purification (including SODIS), distillation (including solar distillation), and flocculation. Often these are used in combination.\n\nUntreated water may contain potentially pathogenic agents, including protozoa, bacteria, viruses, and some larvae of higher-order parasites such as liver flukes and roundworms. Chemical pollutants such as pesticides, heavy metals and synthetic organics may be present. Other components may affect taste, odour and general aesthetic qualities, including turbidity from soil or clay, colour from humic acid or microscopic algae, odours from certain type of bacteria, particularly Actinomycetes which produce geosmin, and saltiness from brackish or sea water.\n\nCommon metallic contaminants such as copper and lead can be treated by increasing the pH using soda ash or lime, which precipitates such metals. Careful decanting of the clear water after settlement or the use of filtration provides acceptably low levels of metals. Water contaminated by aluminium or zinc cannot be treated in this way using a strong alkali as higher pHs re-dissolve the metal salts. Salt is difficult to remove except by reverse osmosis or distillation.\n\nMost portable treatment processes focus on mitigating human pathogens for safety and removing particulates matter, tastes and odours. Significant pathogens commonly present in the developed world include \"Giardia\", \"Cryptosporidium\", \"Shigella\", hepatitis A virus, \"Escherichia coli\", and enterovirus. In less developed countries there may be risks from cholera and dysentery organisms and a range of tropical enteroparasites.\n\n\"Giardia lamblia\" and \"Cryptosporidium spp.\", both of which cause diarrhea (see giardiasis and cryptosporidiosis) are common pathogens. In backcountry areas of the United States and Canada they are sometimes present in sufficient quantity that water treatment is justified for backpackers, although this has created some controversy. (See wilderness acquired diarrhea.) In Hawaii and other tropical areas, \"Leptospira spp.\" are another possible problem.\n\nLess commonly seen in developed countries are organisms such as \"Vibrio cholerae\" which causes cholera and various strains of \"Salmonella\" which cause typhoid and para-typhoid diseases. Pathogenic viruses may also be found in water. The larvae of flukes are particularly dangerous in area frequented by sheep, deer, or cattle. If such microscopic larvae are ingested, they can form potentially life-threatening cysts in the brain or liver. This risk extends to plants grown in or near water including the commonly eaten watercress.\n\nIn general, more human activity up stream (i.e. the larger the stream/river) the greater the potential for contamination from sewage effluent, surface runoff, or industrial pollutants. Groundwater pollution may occur from human activity (e.g. on-site sanitation systems or mining) or might be naturally occurring (e.g. from arsenic in some regions of India and Bangladesh). Water collected as far upstream as possible above all known or anticipated risks of pollution poses the lowest risk of contamination and is best suited to portable treatment methods.\n\nNot all techniques by themselves will mitigate all hazards. Although flocculation followed by filtration has been suggested as best practice this is rarely practicable without the ability to carefully control pH and settling conditions. Ill-advised used of alum as a flocculant can lead to unacceptable levels of aluminium in the water so treated. If water is to be stored, halogens offer extended protection.\n\nHeat kills disease-causing micro-organisms, with higher temperatures and/or duration required for some pathogens. Sterilization of water (killing all living contaminants) is not necessary to make water safe to drink; one only needs to render enteric (intestinal) pathogens harmless. Boiling does not remove most pollutants and does not leave any residual protection.\n\nThe WHO states bringing water to rolling boil then naturally cooling is sufficient to inactivate pathogenic bacteria, viruses and protozoa.\n\nThe CDC recommends a rolling boil for 1 minute. At high elevations, though, the boiling point of water drops. At altitudes greater than 6,562 feet (2000 meters) boiling should continue for 3 minutes.\n\nAll bacterial pathogens are quickly killed above 60 °C (140 °F), therefore, although boiling is not necessary to make the water safe to drink, the time taken to heat the water to boiling is usually sufficient to reduce bacterial concentrations to safe levels. Encysted protozoan pathogens may require higher temperatures to remove any risk.\n\nBoiling is not always necessary nor sometimes enough. Pasteurization where \"enough\" pathogens are killed typically occurs at 63 °C for 30 minutes or 72 °C for 15 seconds. Certain pathogens must be heated above boiling (e.g. botulism\"Clostridium botulinum\" requires , most endospores require , and prions even higher). Higher temperatures may be achieved with a pressure cooker. Heat combined with ultraviolet light (UV), such as SODIS method, reduces the necessary temperature + duration.\n\nPortable pump filters are commercially available with ceramic filters that filter 5,000 to 50,000 litres per cartridge, removing pathogens down to the 0.2–0.3 micrometer (µm) range. Some also utilize activated charcoal filtering. Most filters of this kind remove most bacteria and protozoa, such as \"Cryptosporidium\" and \"Giardia lamblia,\" but not viruses except for the very largest of 0.3 µm and larger diameters, so disinfection by chemicals or ultraviolet light is still required after filtration. It is worth noting that not all bacteria are removed by 0.2 µm pump filters; for example, strands of thread-like \"Leptospira spp.\" (which can cause leptospirosis) are thin enough to pass through a 0.2 µm filter. Effective chemical additives to address shortcomings in pump filters include chlorine, chlorine dioxide, iodine, and sodium hypochlorite (bleach). There have been polymer and ceramic filters on the market that incorporated iodine post-treatment in their filter elements to kill viruses and the smaller bacteria that cannot be filtered out, but most have disappeared due to the unpleasant taste imparted to the water, as well as possible adverse health effects when iodine is ingested over protracted periods.\n\nWhile the filtration elements may do an excellent job of removing most bacteria and fungi contaminants from drinking water when new, the elements themselves can become colonization sites. In recent years some filters have been enhanced by bonding silver metal nanoparticles to the ceramic element and/or to the activated charcoal to suppress growth of pathogens.\n\nSmall, hand-pumped reverse osmosis filters were originally developed for the military in the late 1980s for use as survival equipment, for example, to be included with inflatable rafts on aircraft. Civilian versions are available. Instead of using the static pressure of a water supply line to force the water through the filter, pressure is provided by a hand-operated pump, similar in function and appearance to a mechanic's grease gun. These devices can generate drinkable water from seawater.\n\nThe Portable Aqua Unit for Lifesaving (short PAUL) is a portable ultrafiltration-based membrane water filter for humanitarian aid. It allows the decentralized supply of clean water in emergency and disaster situations for about 400 persons per unit per day. The filter is designed to function with neither chemicals nor energy nor trained personnel.\n\nGranular activated carbon filtering utilizes a form of activated carbon with a high surface area, and adsorbs many compounds, including many toxic compounds. Water passing through activated carbon is commonly used in concert with hand pumped filters to address organic contamination, taste, or objectionable odors. Activated carbon filters aren't usually used as the primary purification techniques of portable water purification devices, but rather as secondary means to complement another purification technique. It is most commonly implemented for pre- or post-filtering, in a separate step than ceramic filtering, in either case being implemented prior to the addition of chemical disinfectants used to control bacteria or viruses that filters cannot remove. Activated charcoal can remove chlorine from treated water, removing any residual protection remaining in the water protecting against pathogens, and should not, in general, be used without careful thought after chemical disinfection treatments in portable water purification processing. Ceramic/Carbon Core filters with a 0.5 µm or smaller pore size are excellent for removing bacteria and cysts while also removing chemicals.\n\nChemical disinfection with halogens, chiefly chlorine and iodine, results from oxidation of essential cellular structures and enzymes. The primary factors that determine the rate and proportion of microorganisms killed are the residual or available halogen concentration and the exposure time. Secondary factors are pathogen species, water temperature, pH, and organic contaminants. In field-water disinfection, use of concentrations of 1–16 mg/L for 10–60 min is generally effective. Of note, Cryptosporidium oocysts, likely Cyclospora species, Ascaris eggs are extremely resistant to halogens and field inactivation may not be practical with bleach and iodine.\n\nIodine used for water purification is commonly added to water as a solution, in crystallized form, or in tablets containing tetraglycine hydroperiodide that release 8 mg of iodine per tablet adaptation to chronic tetraglycine hydroperiodide. The iodine kills many, but not all, of the most common pathogens present in natural fresh water sources. Carrying iodine for water purification is an imperfect but lightweight solution for those in need of field purification of drinking water. Kits are available in camping stores that include an iodine pill and a second pill (vitamin C or ascorbic acid) that will remove the iodine taste from the water after it has been disinfected. The addition of vitamin C, in the form of a pill or in flavored drink powders, precipitates much of the iodine out of the solution, so it should not be added until the iodine has had sufficient time to work. This time is 30 minutes in relatively clear, warm water, but is considerably longer if the water is turbid or cold. Iodine treated drinking water, treated with tablets containing tetraglycine hydroperiodide, also reduces the uptake of radioactive iodine in human subjects to only 2% of the value it would otherwise be. This could be an important factor worthy of consideration for treating water in a recent post nuclear event survival situation, where radioactive iodine ingestion is a concern for internal radiotoxicity. If the iodine has precipitated out of the solution, then the drinking water has less available iodine in the solution. Also the amount of iodine in one tablet is not sufficient to block uptake. Tetraglycine hydroperiodide maintains its effectiveness indefinitely before the container is opened; although some manufacturers suggest not using the tablets more than three months after the container has initially been opened, the shelf life is in fact very long provided that the container is resealed immediately after each time it is opened.\n\nIodine should be allowed at least 30 minutes to kill Giardia.\n\nA potentially lower cost alternative to using iodine-based water purification tablets is the use of iodine crystals although there are serious risks of acute iodine toxicity if preparation and dilution and not measured with some accuracy. This method may not be adequate in killing Giardia cysts in cold water. An advantage of using iodine crystals is that only a small amount of iodine is dissolved from the iodine crystals at each use, giving this method of treating water a capability for treating very large volumes of water. Unlike tetraglycine hydroperiodide tablets, iodine crystals have an unlimited shelf life as long as they are not exposed to air for long periods of time or are kept under water. Iodine crystals will sublimate if exposed to air for long periods of time. The large quantity of water that can be purified with iodine crystals at low cost makes this technique especially cost effective for point of use or emergency water purification methods intended for use longer than the shelf life of tetraglycine hydroperiodide.\n\nChlorine-based halazone tablets were formerly popularly used for portable water purification. Chlorine in water is more than three times more effective as a disinfectant against \"Escherichia coli\" than iodine. Halazone tablets were thus commonly used during World War II by U.S. soldiers for portable water purification, even being included in accessory packs for C-rations until 1945.\n\nSodium dichloroisocyanurate (NaDCC) has largely displaced halazone tablets for the few remaining chlorine-based water purification tablets available today.It is compressed with effervescent salts, usually adipic acid and sodium bicarbonate, to form rapidly dissolving tablets, diluted to 10 parts per million available chlorine (ppm av.cl) when drinking water is mildly contaminated and 20ppm when visibly contaminated.\n\nChlorine bleach tablets give a more stable platform for disinfecting the water than liquid bleach (sodium hypochlorite) as the liquid version tends to degrade with age and give unregulated results unless assays are carried outnot practical on the spot. Still, despite chlorine-based halazone tablets falling from favor for portable water purification, chlorine-based bleach may nonetheless safely be used for short-term emergency water disinfection. Two drops of unscented 5% bleach can be added per liter or quart of clear water, then allowed to stand covered for 30 to 60 minutes. After this treatment, the water may be left open to reduce the chlorine smell and taste. Guidelines are available online for effective emergency use of bleach to render unsafe water potable.\n\nThe Centers for Disease Control & Prevention (CDC) and Population Services International (PSI) promote a similar product (a 0.5% - 1.5% sodium hypochlorite solution) as part of their Safe Water System (SWS) strategy. The product is sold in developing countries under local brand names specifically for the purpose of disinfecting drinking water.\n\nCommon bleach including calcium hypochlorite (Ca[OCl]) and sodium hypochlorite (NaOCl) are common, well-researched, low-cost oxidizers.\n\nThe EPA recommends two drops of 8.25% sodium hypochlorite solution (regular, unscented chlorine bleach) mixed per one quart/liter of water and let stand 30 minutes. Two drops of 5% solution also suffices. Double the amount of bleach if the water is cloudy, colored, or very cold. Afterwards, the water should have a slight chlorine odor. If not repeat the dosage and let stand for another 15 minutes before use.\n\nNeither chlorine (e.g., bleach) nor iodine alone is considered completely effective against \"Cryptosporidium\", although they are partially effective against \"Giardia\". Chlorine is considered slightly better against the latter. A more complete field solution that includes chemical disinfectants is to first filter the water, using a 0.2 µm ceramic cartridge pumped filter, followed by treatment with iodine or chlorine, thereby filtering out cryptosporidium, Giardia, and most bacteria, along with the larger viruses, while also using chemical disinfectant to address smaller viruses and bacteria that the filter cannot remove. This combination is also potentially more effective in some cases than even using portable electronic disinfection based on UV treatment.\n\nChlorine dioxide can come from tablets or be created by mixing two chemicals together. It is more effective than iodine or chlorine against giardia, and although it has only low to moderate effectiveness against cryptosporidium, iodine and chlorine are ineffective against this protozoan. The cost of chlorine dioxide treatment is higher than the cost of iodine treatment. \n\nA simple brine {salt + water} solution in an electrolytic reaction produces a powerful mixed oxidant disinfectant (mostly chlorine in the form of hypochlorous acid (HOCl) and some peroxide, ozone, chlorine dioxide) to inactivate viruses, bacteria, Giardia and Cryptosporidium killing 99.9% of all organisms without the need to filter. MiOx is considered stronger than chlorine.\n\nSodium dichloroisocyanurate or Troclosene Sodium more commonly shortened as NaDCC, is a form of chlorine used for disinfection. It is used by all major NGO's such as UNICEF to treat water in emergencies, and widely by social marketing organisations for household water treatment where household sources of water may not be safe.\n\nNaDCC tablets are available in a range of concentrations to treat differing volumes of water to give the World Health Organization's recommended 5ppm available chlorine. They are effervescent tablets allowing the tablet to dissolve in a matter of minutes.\n\nAn alternative to iodine-based preparations in some usage scenarios are silver ion/chlorine dioxide-based tablets or droplets. These solutions may disinfect water more effectively than iodine-based techniques while leaving hardly any noticeable taste in the water in some usage scenarios. Silver ion/chlorine dioxide-based disinfecting agents will kill \"Cryptosporidium\" and \"Giardia\", if utilized correctly. The primary disadvantage of silver ion/chlorine dioxide-based techniques is the long purification times (generally 30 minutes to 4 hours, depending on the formulation used). Another concern is the possible deposition and accumulation of silver compounds in various body tissues leading to a rare condition called argyria that results in a permanent, disfiguring, bluish-gray pigmentation of the skin, eyes, and mucous membranes.\n\nOne recent study has found that the wild Salmonella which would reproduce quickly during subsequent dark storage of solar-disinfected water could be controlled by the addition of just 10 parts per million of hydrogen peroxide.\n\nUltraviolet (UV) light induces the formation of covalent linkages on DNA and thereby prevents microbes from reproducing. Without reproduction, the microbes become far less dangerous. Germicidal UV-C light in the short wavelength range of 100–280 nm acts on thymine, one of the four base nucleotides in DNA. When a germicidal UV photon is absorbed by a thymine molecule that is adjacent to another thymine within the DNA strand, a covalent bond or dimer between the molecules is created. This thymine dimer prevents enzymes from \"reading\" the DNA and copying it, thus neutering the microbe. Prolonged exposure to ionizing radiation can cause single and double-stranded breaks in DNA, oxidation of membrane lipids, and denaturation of proteins, all of which are toxic to cells. Still, there are limits to this technology. Water turbidity (i.e., the amount of suspended & colloidal solids contained in the water to be treated) must be low, such that the water is clear, for UV purification to work well - thus a pre-filter step might be necessary.\n\nA concern with UV portable water purification is that some pathogens are hundreds of times less sensitive to UV light than others. Protozoan cysts were once believed to be among the least sensitive, however recent studies have proved otherwise, demonstrating that both Cryptosporidium and Giardia are deactivated by a UV dose of just 6 mJ/cm However, EPA regulations and other studies show that it is viruses that are the limiting factor of UV treatment, requiring a 10-30 times greater dose of UV light than \"Giardia\" or \"Cryptosporidium\".\nStudies have shown that UV doses at the levels provided by common portable UV units are effective at killing \"Giardia\" and that there was no evidence of repair and reactivation of the cysts.\n\nWater treated with UV still has the microbes present in the water, only with their means for reproduction turned \"off\". In the event that such UV-treated water containing neutered microbes is exposed to visible light (specifically, wavelengths of light over 330-500 nm) for any significant period of time, a process known as photo reactivation can take place, where the possibility for repairing the damage in the bacteria's reproduction DNA arises, potentially rendering them once more capable of reproducing and causing disease. UV-treated water must therefore not be exposed to visible light for any significant period of time after UV treatment, before consumption, to avoid ingesting reactivated and dangerous microbes.\n\nRecent developments in semiconductor technology allows for the development of UV-C Light Emitting Diodes (LEDs). UV-C LED systems address disadvantages of mercury-based technology, namely: power-cycling penalties, high power needs, fragility, warm-up time, and mercury content.\n\nIn ozone water disinfection, microbes are destroyed by ozone gas (O) provided by an ozone generator. Common in Europe, ozone gas is now becoming widely adopted in the United States. It is emerging across a wide array of industries; from municipal water treatment plants, to food processing plants, to healthcare organizations. It is being adopted due to its ability to sanitize water and surfaces without wasting water, and because there are no by-products. When its job is done, ozone gas quickly degrades into oxygen. Ozone is more effective than chlorine in destroying viruses and bacteria.\n\nIn 1990, the Organic Foods Production Act (OFPA) identified aqueous ozone as a substance that is allowed for use in organic crop and livestock production. In 1997, it was approved by the FDA as an antimicrobial agent for use on food. In 2002, the FDA approved ozone for use on food contact areas and directly on food with its Generally Regarded as Safe (“GRAS”) designation.\n\nOzone is most commonly created by a process called “corona discharge”, which causes oxygen molecules (O) to temporarily re-combine into ozone (O). This gas is very unstable, and the 3rd oxygen molecule reacts with pathogens by penetrating the cell walls of bacteria and viruses. This destroys the organisms.\n\nOzone is effective against pollutants for the same reason; it will react with long-chain carbon (organic) molecules, and break them down into less complex (and typically less harmful) molecules through oxidation.\n\nAdvances in ozone generation techniques, coupled with filtration, make this a viable new portable water purification method.\n\nIn solar water disinfection (SODIS), microbes are destroyed by temperature and UVA radiation provided by the sun. Water is placed in a transparent plastic PET bottle or plastic bag, oxygenated by shaking partially filled capped bottles prior to filling the bottles all the way, and left in the sun for 6–24 hours atop a reflective surface.\n\nSolar distillation relies on sunlight to warm and evaporate the water to be purified which then condenses and trickles into a container. In theory, a solar (condensation) still removes all pathogens, salts, metals, and most chemicals but in field practice the lack of clean components, easy contact with dirt, improvised construction, and disturbances result in cleaner, yet contaminated water.\n\nWater filters can be made on-site using local materials such as sand and charcoal (e.g. from firewood burned in a special way). These filters are sometimes used by soldiers and outdoor enthusiasts. Due to their low cost they can be made and used by anyone. The reliability of such systems is highly variable. Such filters can do little, if anything, to mitigate germs and other harmful constituents and can give a false sense of security that the water so produced is potable. Water processed through an improvised filter should undergo secondary processing such as boiling to render it safe for consumption.\n\nHuman water-borne diseases usually come from other humans, thus human-derived materials (feces, medical waste, wash water, lawn chemicals, gasoline engines, garbage, etc.) should be kept far away from water sources. For example, human excreta should be buried well away (>60 meters/200 feet) from water sources to reduce contamination. In some wilderness areas it is recommended that all waste be packed up and carted out to a properly designated disposal point.\n\n\n"}
{"id": "12721713", "url": "https://en.wikipedia.org/wiki?curid=12721713", "title": "Proteolix", "text": "Proteolix\n\nProteolix, Inc., was a private biotechnology company with headquarters in South San Francisco, California. Proteolix was founded in 2003 based on technology developed by co-founders Dr. Craig Crews (Yale University) and Dr. Raymond J. Deshaies (California Institute of Technology). Drs. Susan Molineaux and Phil Whitcome (deceased) joined Drs. Crews and Deshaies as co-founders. Proteolix was launched based on an $18.2 million A round comprising investments by Latterell Venture Partners, US Venture Partners, Advanced Technology Ventures, and The Vertical Group. Proteolix focused primarily on the proteasome as a therapeutic target. Its lead product candidate, carfilzomib (PR-171), is a tetrapeptide epoxyketone. At the time of its sale (see below), the company had two earlier-stage programs, an orally-bioavailable proteasome inhibitor for oncology (PR-047), and an agent preferentially targeting the immuno form of the proteasome (PR-957), with potential utility in areas such as rheumatoid arthritis. At the time of sale, Carfilzomib's route of administration was intravenous, and the company was exploring its potential utility in multiple myeloma, Non-Hodgkin lymphoma (NHL) and other cancers. \n\nProteolix was acquired by Onyx Pharmaceuticals in 2009 for $810 million (nominal value). Onyx renamed PR-047 to \"ONX 0912\" and PR-957 to \"ONX 0914\".\n\n"}
{"id": "29514096", "url": "https://en.wikipedia.org/wiki?curid=29514096", "title": "Registered Nurse Certified in Neonatal Intensive Care", "text": "Registered Nurse Certified in Neonatal Intensive Care\n\nRegistered Nurse, Certified in Neonatal Intensive Care (RNC-NIC) is the US designation for a neonatal intensive care nurse who has earned nursing board certification. This exam is one of the core certification exams offered by the National Certification Corporation (NCC).\n\nThe organization's other core certifications include Low Risk Neonatal (RNC-LRN), Maternal Newborn Nursing (RNC-MNN) and Inpatient Obstetrics (RNC-OB) for nurses in those related specialties.\n\nNeonatal nursing is a specialty where the nurses care for newborn babies who need critical care. This may include newborns who are very sick, need immediate surgery, or have birth defects. Neonatal nurses will provide care around the clock to these infants. \n\n\n"}
{"id": "34597731", "url": "https://en.wikipedia.org/wiki?curid=34597731", "title": "Sardeh Band Dam", "text": "Sardeh Band Dam\n\nThe Sardeh Band Dam () is located near Sardeh Band town, in the eastern part of Andar District of Ghazni Province of Afghanistan. It was constructed in 1967 (1346 in the Islamic calendar) by the Soviet Union and Afghanistan during the reign of Mohammed Zahir Shah prior to the Soviet–Afghan War. The dam provided irrigation water for more than 67,000 jeribs of land after completion. Currently only 2,000 jeribs of land is under cultivation. Maximum capacity of the reservoir is 259 million cubic meters of water, and the reservoir holds about 164 million gallons at present.\n\nThe dam system contains an earth dam, intake, spillway, one main canal (which is divides into two branches (the 21.5-kilometer Right Canal with 6 sub-canals and the 30-kilometer Left Canal with 16 sub-canals) and administration buildings.\n\nThe Sardeh Dam Reservoir is fed by the Jilga River which flows north–south from Paktika Province and Paktiya Province. The river is also labeled as the Gardez River north of the dam and the Sardeh River south of the dam (by the National Atlas of the Democratic Republic of Afghanistan, page 13-14).\n\nThe dam is an indication of the presence of aid from the USSR in Afghanistan prior to the invasion and subsequent occupation from 1979–1989. According to C.J. Chivers of the \"New York Times\": \"Moscow built schools, roads, airports and dams, and sponsored ministries, too. Soviet officials recruited students and bureaucrats for all manner of training, and invited the country’s elite and its officer-, civil-service and intellectual classes to long periods of study in the Soviet Union. Education, development and modernization—like this dam, which still influences both flood control and irrigation downstream—played no small part in the Kremlin’s Afghan policy, which ultimately failed.\"\n\nThe Band E Sardeh Dam was built during the same period as the High Dam in Aswan, Egypt, indicating a larger Soviet strategy for the development of third world nations it sought to bring under its sphere of influence. Large lettered Russian writing is still present on the down-stream face of the dam (visible from over 500 meters away) commemorating its completion in Russian and Pashto script. On the left it reads: \"сарде 1967\" and on the right: دسرده بند۱۳۴۶\n\nAccording to USAID's Ghazni Infrastructure Needs Assessment: September 14–18, 2003 The Deputy Governor and other provincial officials agreed that the top priority for irrigation infrastructure rehabilitation in Ghazni province is the Sardeh Band Dam. The Director of the dam, as well as the local farmers, is extremely disappointed in the progress of the donor community on the dam. The Director reported that 14 NGOs, both national and international, have been asked to conduct assessments on the dam and surveys for funding the rehabilitation of the dam. Of these 14 NGOs, only four have conducted the assessments, and none have received funding for work to date. The NGOs who have conducted assessments on the dam are: International Engineering Consultancy Company, ACLU, LERCC and ARA (local NGO).\n\nOn 2 February 2010 members of Ghazni Provincial Reconstruction Team and Central Asia Development Group met with key leaders from Andar district to enlist support for a massive irrigation project concerning the repair of the irrigation canals fed by the Band E Sardeh Dam. \"The Chardewal irrigation system was built to distribute water from the Bandee Sardi Dam throughout much of Andar, but due to neglect the canals have fallen into disrepair. Currently, water flows at only 50 percent capacity, limiting the ability of local farmers of the Kahnjoor farming zone to make a living. The reconstructed canal would be a one-kilometer cobblestone irrigation channel along the village with a curb separating the roadway from the canal. Chardewal, a village located in the middle of Andar, has suffered from drought and poor water management. With mentoring from the PRT, cooperation from the district sub-governor and village elders, the CADG will rebuild the village canal. It is the first stage of a long term cash-for-work program aimed at restoring the irrigation system and rehabilitating the Kahnjoor farming zone. Capacity building in this unstable district is considered the most critical mission for the development, reconstruction and stabilization efforts being made in Andar. 'Our problems are our water (supply) and our roads,' said Niaz Mohammad, an elder from Chardewal. 'We do not have enough water, wells or roads.' The project hopes to provide cash-for-work for deprived households in a Pashtun dominated area. The project could improve the agricultural potential for more than 2,000 people.\"\n\nDuring the 7 August 2011 plenary session of the Meshrano Jirga (MJ) Senators invited the Emergency Response Committee to hear about their preparations for emergencies. When discussing current drought conditions, Alhaj Allah Dad (Ghazni Senator)said: \"The Bandi Sardi dam is full of mud. If cleared, it could irrigate thousands of acres of agricultural land.\"\n\nThe Band E Sardeh Dam was the home of a Soviet garrison during their occupation of Afghanistan during the 1980s and currently garrisons an Afghan National Army Battalion. Nearby the Band E Sardeh Dam is the abandoned Sardeh Band Airport which was used during the Soviet occupation but now no longer exists. The wrecked hulls of Soviet tanks, armored personnel carriers, and other heavy equipment are still present as of 2012. Both the Afghan and U.S. militaries at Bande Sardeh have used the base as a gathering place for locals and elders to discuss local security and military developments.\n\nOn 12 June 2002 a Lockheed MC-130H Hercules was participating in a night exfiltration mission to remove U.S. Army Special Forces troops from the area when it tried to take off from an improved airstrip at Sardeh Band. The plane impacted the ground and crashed in a barren area, 2.5 nautical miles from the airstrip.\n\nThe Soviet Union built many other dams in other countries as it sought to extend its sphere of influence. Many of these projects were undertaken by a Russian firm called Hydroproject (Russian: Институт «Гидропроект», Gidroproekt). It is unclear if Hydroproject was involved in the design or construction of Band E Sardeh Dam. Other expeditionary projects undertaken by Hydroproject include:\n\n\n"}
{"id": "9999924", "url": "https://en.wikipedia.org/wiki?curid=9999924", "title": "Semen analysis", "text": "Semen analysis\n\nA semen analysis (plural: semen analyses), also called \"seminogram\" evaluates certain characteristics of a male's semen and the sperm contained therein. It is done to help evaluate male fertility, whether for those seeking pregnancy or verifying the success of vasectomy. Depending on the measurement method, just a few characteristics may be evaluated (such as with a home kit) or many characteristics may be evaluated (generally by a diagnostic laboratory). Collection techniques and precise measurement method may influence results.\n\nThe most common reasons for laboratory semen analysis in humans are as part of a couple's infertility investigation and after a vasectomy to verify that the procedure was successful. It is also commonly used for testing human donors for sperm donation, and for animals semen analysis is commonly used in stud farming and farm animal breeding.\n\nOccasionally a man will have a semen analysis done as part of routine pre-pregnancy testing. At the laboratory level this is rare, as most healthcare providers will not test the semen and sperm unless specifically requested or there is a strong suspicion of a pathology in one of these areas discovered during the medical history or during the physical examination. Such testing is very expensive and time-consuming, and in the U.S. is unlikely to be covered by insurance. In other countries, such as Germany, the testing is covered by all insurances.\n\nThe characteristics measured by semen analysis are only some of the factors in semen quality. One source states that 30% of men with a normal semen analysis actually have abnormal sperm function. Conversely, men with poor semen analysis results may go on to father children. In NICE guidelines, \"mild male factor infertility\" is defined as when 2 or more semen analyses have 1 or more variables below the 5th percentile, and confers a chance of pregnancy occurring\nnaturally through vaginal intercourse within 2 years similar to people with mild endometriosis.\n\nDifferent methods used for semen collection are masturbation, condom collection and epididymal extraction, etc. The sample should never be obtained through coitus interruptus for several reasons: Some part of ejaculation could be lost, bacterial contamination could happen and the acid vaginal pH could be detrimental for sperm motility. \nThe optimal sexual abstinence for semen sample obtaining is from 2–7 days. The most common way to obtain a semen sample is through masturbation and the best place to obtain it is in the clinic where the analysis will take place in order to avoid temperature changes during the transport that can be lethal for some spermatozoa.\nOnce the sample is obtained, it must be put directly in a sterile plastic recipient (never in a conventional preservative since they have chemical substances as lubricants or spermicides that could damage the sample) and be handed in the clinic for it to be studied within the following hour.\n\nExamples of parameters measured in a semen analysis are: sperm count, motility, morphology, volume, fructose level and pH.\n\nSperm count, or \"sperm concentration\" to avoid confusion with \"total sperm count\", measures the concentration of sperm in a man's ejaculate, distinguished from \"total sperm count\", which is the sperm count multiplied with volume. Over 15 million sperm per milliliter is considered normal, according to the WHO in 2010. Older definitions state 20 million. A lower sperm count is considered oligozoospermia. A vasectomy is considered successful if the sample is azoospermic (zero sperm of any kind found). Some define success as when rare/occasional non-motile sperm are observed (fewer than 100,000 per millilitre). Others advocate obtaining a second semen analysis to verify the counts are not increasing (as can happen with re-canalization) and others still may perform a repeat vasectomy for this situation.\n\nChips for home use are emerging that can give an accurate estimation of sperm count after three samples taken on different days. Such a chip may measure the concentration of sperm in a semen sample against a control liquid filled with polystyrene beads.\n\nThe World Health Organization has a value of 50% and this must be measured within 60 minutes of collection. WHO also has a parameter of \"vitality\", with a lower reference limit of 60% live spermatozoa. A man can have a total number of sperm far over the limit of 20 million sperm cells per milliliter, but still have bad quality because too few of them are motile. However, if the sperm count is very high, then a low motility (for example, less than 60%) might not matter, because the fraction might still be more than 8 million per millilitre. The other way around, a man can have a sperm count far less than 20 million sperm cells per millilitre and still have good motility, if more than 60% of those observed sperm cells show good forward movement - which is beneficial because nature favours quality over quantity.\n\nA more specified measure is \"motility grade\", where the motility of sperm are divided into four different grades:\n\n\nRegarding sperm morphology, the WHO criteria as described in 2010 state that a sample is normal (samples from men whose partners had a pregnancy in the last 12 months) if 4% (or 5th centile) or more of the observed sperm have normal morphology.\n\nMorphology is a predictor of success in fertilizing oocytes during in vitro fertilization.\n\nUp to 10% of all spermatozoa have observable defects and as such are disadvantaged in terms of fertilising an oocyte.\n\nAlso, sperm cells with tail-tip swelling patterns generally have lower frequency of aneuploidy.\nA \"motile sperm organelle morphology examination\" (MSOME) is a particular morphologic investigation wherein an inverted light microscope equipped with high-power optics and enhanced by digital imaging is used to achieve a magnification above x6000, which is much higher than the magnification used habitually by embryologists in spermatozoa selection for intracytoplasmic sperm injection (x200 to x400). A potential finding on MSOME is the presence of sperm vacuoles, which are associated with sperm chromatin immaturity, particularly in the case of large vacuoles.\n\nAccording to one lab test manual semen volumes between 2.0 mL and 5 mL are normal; WHO regards 1.5 ml as the lower reference limit. Low volume may indicate partial or complete blockage of the seminal vesicles, or that the man was born without seminal vesicles. In clinical practice, a volume of less than 2 mL in the setting of infertility and absent sperm should prompt an evaluation for obstructive azoospermia. A caveat to this is be sure it has been at least 48 hours since the last ejaculation to time of sample collection.\n\nThe human ejaculate is mostly composed of water. 96 to 98% of semen is water. One way of ensuring that a man produces more ejaculate is to drink more liquids. Men also produce more seminal fluid after lengthy sexual stimulation and arousal. Reducing the frequency of sex and masturbation helps increase semen volume. Sexually transmitted diseases also affect the production of semen. Men who are infected with the human immunodeficiency virus (HIV) produce lower semen volume.\n\nSemen normally has a whitish-gray color. It tends to get a yellowish tint as a man ages. Semen color is also influenced by the food we eat: foods that are high in sulfur, such as garlic, may result in a man producing yellow semen. Presence of blood in semen (hematospermia) leads to a brownish or red colored ejaculate. Hematospermia is a rare condition.\n\nSemen that has a deep yellow color or is greenish in appearance may be due to medication. Brown semen is mainly a result of infection and inflammation of the prostate gland, urethra, epididymis and seminal vesicles. Other causes of unusual semen color include sexually transmitted infections such as gonorrhea and chlamydia, genital surgery and injury to the male sex organs.\n\nFructose level in the semen may be analysed to determine the amount of energy available to the semen for moving. WHO specifies a normal level of 13 μmol per sample. Absence of fructose may indicate a problem with the seminal vesicles.\n\nAccording to one lab test manual normal pH range is 7.1-8.0; WHO criteria specify normal as 7.2-7.8. Acidic ejaculate (lower pH value) may indicate one or both of the seminal vesicles are blocked. A basic ejaculate (higher pH value) may indicate an infection. A pH value outside of the normal range is harmful to sperm and affect their ability to penetrate the egg.\n\nThe liquefaction is the process when the gel formed by proteins from the seminal vesicles is broken up and the semen becomes more liquid. It normally takes less than 20 minutes for the sample to change from a thick gel into a liquid. In the NICE guidelines, a liquefaction time within 60 minutes is regarded as within normal ranges.\n\nMOT is a measure of how many million sperm cells per ml are highly motile, that is, approximately of grade a (>25 micrometer per 5 sek. at room temperature) and grade b (>25 micrometer per 25 sek. at room temperature). Thus, it is a combination of sperm count and motility.\n\nWith a straw or a vial volume of 0.5 milliliter, the general guideline is that, for intracervical insemination (ICI), straws or vials making a total of 20 million motile spermatozoa in total is recommended. This is equal to 8 straws or vials 0.5 ml with MOT5, or 2 straws or vials of MOT20. For intrauterine insemination (IUI), 1-2 MOT5 straws or vials is regarded sufficient. In WHO terms, it is thus recommended to use approximately 20 million grade a+b sperm in ICI, and 2 million grade a+b in IUI.\n\nDNA damage in sperm cells that is related to infertility can be probed by analysis of DNA susceptibility to denaturation in response to heat or acid treatment and/or by detection of DNA fragmentation revealed by the presence of double-strand breaks detected by the TUNEL assay.\n\n\"Total motile spermatozoa\" (TMS) or \"total motile sperm count\" (TMSC) is a combination of sperm count, motility and volume, measuring how many million sperm cells in an entire ejaculate are motile.\n\nUse of approximately 20 million sperm of motility grade c or d in ICI, and 5 million ones in IUI may be an approximate recommendation.\n\nThe NICE guidelines also include testing \"vitality\", with normal ranges defined as more than 75% of sperm cells alive.\n\nThe sample may also be tested for white blood cells. A high level of white blood cells in semen is called and may indicate an infection. Cutoffs may vary, but an example cutoff is over 1 million white blood cells per milliliter of semen.\n\n\nApart from the semen quality itself, there are various methodological factors that may influence the results, giving rise to inter-method variation.\n\nCompared to samples obtained from masturbation, semen samples from collection condoms have higher total sperm counts, sperm motility, and percentage of sperm with normal morphology . For this reason, they are believed to give more accurate results when used for semen analysis.\n\nIf the results from a man's first sample are subfertile, they must be verified with at least two more analyses. At least 2 to 4 weeks must be allowed between each analysis. Results for a single man may have a large amount of natural variation over time, meaning a single sample may not be representative of a man's average semen characteristics. In addition, sperm physiologist Joanna Ellington believes that the stress of producing an ejaculate sample for examination, often in an unfamiliar setting and without any lubrication (most lubricants are somewhat harmful to sperm), may explain why men's first samples often show poor results while later samples show normal results.\nA man may prefer to produce his sample at home rather than at the clinic. The site of semen collection does \"not\" affect the results of a semen analysis.\n\nVolume can be determined by measuring the weight of the sample container, knowing the mass of the empty container. Sperm count and morphology can be calculated by microscopy. Sperm count can also be estimated by kits that measure the amount of a sperm-associated protein, and are suitable for home use.\n\nComputer Assisted Semen Analysis (CASA) is a catch-all phrase for automatic or semi-automatic semen analysis techniques. Most systems are based on image analysis, but alternative methods exist such as tracking cell movement on a digitizing tablet. Computer-assisted techniques are most-often used for the assessment of sperm concentration and mobility characteristics, such as velocity and linear velocity. Nowadays, there are CASA systems, based on image analysis and using new techniques, with near perfect results, and doing full analysis in a few seconds. With some techniques, sperm concentration and motility measurements are at least as reliable as current manual methods.\n\nRaman spectroscopy has made progress in its ability to perform characterization, identification and localization of sperm nuclear DNA damage.\n\n\n"}
{"id": "28273434", "url": "https://en.wikipedia.org/wiki?curid=28273434", "title": "Smoking in Ireland", "text": "Smoking in Ireland\n\nSmoking in Ireland is banned fully in the general workplace, enclosed public places, restaurants, bars, education facilities, healthcare facilities and public transport. However, it is permitted in designated hotel rooms and there is no ban in residential care, prisons and in outdoor areas. Public opinion is in favour of the bans on smoking imposed in Ireland.\n\nCurrently, the Irish law prohibits smoking fully in the general workplace, enclosed public places, restaurants, bars, education facilities, healthcare facilities and public transport. Smoking rooms are permitted in hotels.\n\nAs of July 2009, it is prohibited to advertise cigarettes and sell 10-packs of cigarettes in retail outlets. Additionally, as of February 2013, any tobacco product placed on the market must have graphic warnings. There is legislation being made to introduce plain tobacco packaging and make Ireland the second country to do so, after Australia.\n\nSmoking in workplaces in Ireland was banned on 29 March 2004, making Ireland the first country in the world to institute an outright ban on smoking in workplaces. From that date onwards, under the \"Public Health (Tobacco) Acts\", it has been illegal to smoke in all enclosed workplaces. The ban is strictly enforced and includes bars, restaurants, clubs, offices, public buildings, company cars, trucks, taxis and vans. A private residence is considered a workplace when tradespeople, such as plumbers or electricians, are working there.\n€3,000 is the maximum fine on the spot, while a prison sentence can also be given at a later time for violators. The law exempts dwellings, prisons, nursing homes, psychiatric wards, hotel rooms charitable accommodation and college dorm rooms. Certain buildings such as some hospitals forbid smoking anywhere in the grounds.\n\nBefore the 2004 law, smoking was already outlawed in public buildings, hospitals, schools, restaurant kitchens, cinemas, public pharmacies, public hairdressing premises, public banking halls, and on public transport aircraft and buses and some trains (Intercity trains provided smokers' carriages).\n\nPremises must display a sign to inform patrons of the ban (in Irish or English), and the contact person for any complaints. A workplace can be given a fine of €3,000 for each person that is found smoking (this means €15,000 for 5 people in violation). Smoking rooms are not allowed. Any shelter can not have more than 50 percent coverage of walls. There is also a \"Compliance Line\" set up by the Office of Tobacco Control, that people can call to report people smoking in a workplace or retail outlets selling tobacco to under-18s. \n\nOn 18 July 2008, Irish Fine Gael MEP Avril Doyle proposed in a committee in the European Parliament, that she would like to see an EU-wide ban on cigarettes and cigars by 2025.\n\nOn 1 July 2009, Ireland banned in-store tobacco advertising and displays of tobacco products at retail outlets and new controls on tobacco vending machines (limiting them to being token-operated in registered bars and clubs only) were also introduced. At the same time a ban on the sale of packets of 10 cigarettes was introduced. Tobacco advertising had already been banned from radio, television and on billboards beforehand. The changes now mean that tobacco products must now be stored out of sight in closed containers behind the counter (accessible by retail staff only) and customers can be shown a card showing all available brands in a pictorial list if they wish to purchase cigarettes. Signs must also be shown informing customers that tobacco is sold at the premises. Ireland was the first country in the EU and third in the world (after Canada and Iceland) to introduce such measures, which are punishable with a fine of €3,000 and/or a six-month prison sentence. However, specialist tobacco shops (of which there are fewer than six) are exempt from the new rules; all retailers selling tobacco must register with the Health Service Executive and the new laws will be enforced by environmental health officers.\n\nAs of 1 February 2013, any tobacco product placed on the market must have graphic warnings. The Minister for Health, Dr. James Reilly's proposal to introduce plain tobacco packaging was approved by the Cabinet and expected legislation will be in place by early 2014. As of September 2017 all cigarettes must remove all logos and feature plain packaging.\n\nThe public, according to a Flash Eurobarometer poll conducted by the Gallup Organisation in 2008 for the European Commission, is in favour of a full ban on smoking in restaurants, bars, pubs, clubs, offices and other indoor workplaces.\n\nAccording to the above poll there is a majority of people who support the ban on smoking introduced in 2004.\n\nSince 2009, Irish anti-smoking campaigners and scientists had been urging the government to introduce such a ban. In July 2011 the Minister for Health said that he was considering a ban where children are present in the car. On January 1st 2016, regulations make it an offence for a person to smoke in a private vehicle when there is more than one person present and there is a person under the age of 18 present. The offence would fall on the person smoking regardless of their age.\n\nThe senator and oncologist John Crown put a bill before parliament in 2012 to ban smoking in a car with children. This bill was passed into law in December 2014 with an expectation that enforcement by Gardaí would commence in 2015.\n"}
{"id": "34104355", "url": "https://en.wikipedia.org/wiki?curid=34104355", "title": "Subjective well-being", "text": "Subjective well-being\n\nSubjective well-being (SWB) is a self-reported measure of well-being, typically obtained by questionnaire.\n\nEd Diener developed a tripartite model of subjective well-being in 1984, which describes how people experience the quality of their lives and includes both emotional reactions and cognitive judgments. It posits \"three distinct but often related components of wellbeing: frequent positive affect, infrequent negative affect, and cognitive evaluations such as life satisfaction.\"\n\nSWB therefore encompasses moods and emotions as well as evaluations of one's satisfaction with general and specific areas of one's life. Concepts encompassed by SWB include happiness. \n\nSWB tends to be stable over time and is strongly related to personality traits. There is evidence that health and SWB may mutually influence each other, as good health tends to be associated with greater happiness, and a number of studies have found that positive emotions and optimism can have a beneficial influence on health.\n\nDiener et al. argued that the various components of SWB represent distinct constructs that need to be understood separately, even though they are closely related. Hence, SWB may be considered \"a general area of scientific interest rather than a single specific construct\". Due to the specific focus on the \"subjective\" aspects of well-being, definitions of SWB typically exclude \"objective\" conditions such as material conditions or health, although these can influence ratings of SWB. Definitions of SWB therefore focus on how a person evaluates his/her own life, including emotional experiences of pleasure versus pain in response to specific events and cognitive evaluations of what a person considers a good life. Components of SWB relating to affect include positive affect (experiencing pleasant emotions and moods) and low negative affect (experiencing unpleasant, distressing emotions and moods), as well as \"overall affect\" or \"hedonic balance\", defined as the overall equilibrium between positive and negative affect, and usually measured as the difference between the two. High positive affect and low negative affect are often highly correlated, but not always.\n\nThere are two components of SWB. One is Affective Balance and the other is Life Satisfaction. An individual's scores on the two measures are summed to produce a total SWB score. In some cases, these scores are kept separate.\nAffective balance refers to the emotions, moods, and feelings a person has. These can be all positive, all negative, or a combination of both positive and negative. Some research shows also that feelings of reward are separate from positive and negative affect.\nLife satisfaction (global judgments of one's life) and satisfaction with specific life domains (e.g. work satisfaction) are considered cognitive components of SWB. The term \"happiness\" is also commonly used in regards to SWB and has been defined variously as \"satisfaction of desires and goals\" (therefore related to life satisfaction), as a \"preponderance of positive over negative affect\" (therefore related to emotional components of SWB), as \"contentment\", and as a \"consistent, optimistic mood state\" and may imply an affective evaluation of one's life as a whole. Life satisfaction can also be known as the \"stable\" component in one's life. Affective concepts of SWB can be considered in terms of momentary emotional states as well as in terms of longer-term moods and tendencies (i.e. how much positive and/or negative affect a person generally experiences over any given period of time). Life satisfaction and in some research happiness are typically considered over long durations, up to one's lifetime. \"Quality of life\" has also been studied as a conceptualization of SWB. Although its exact definition varies, it is usually measured as an aggregation of well-being across several life domains and may include both subjective and objective components.\n\nLife satisfaction and Affect balance are generally measured separately and independently. \nSometimes a single SWB question attempts to capture an overall picture.\n\nThe issue with the such measurements of life satisfaction and affective balance is that they are self-reports. The problem with self-reports is that the participants may be lying or at least not telling the whole truth on the questionnaires. Participants may be lying or holding back from revealing certain things because they are either embarrassed or they may be filling in what they believe the researcher wants to see in the results. To gain more accurate results, other methods of measurement have been used to determine one’s SWB. \n\nAnother way to corroborate or confirm that the self-report results are accurate is through informant reports. Informant reports are given to the participant’s closest friends and family and they are asked to fill out either a survey or a form asking about the participants mood, emotions, and overall lifestyle. The participant may write in the self-report that they are very happy, however that participant’s friends and family record that he/she is always depressed. This would obviously be a contradiction in results which would ultimately lead to inaccurate results. \n\nAnother method of gaining a better understanding of the true results is through ESM, or the Experience Sampling Method. In this measure, participants are given a beeper/pager that will randomly ring throughout the day. Whenever the beeper/pager sounds, the participant will stop what he/she is doing and record the activity they are currently engaged in and their current mood and feelings. Tracking this over a period of a week or a month will give researchers a better understanding of the true emotions, moods, and feelings the participant is experiencing, and how these factors interact with other thoughts and behaviors. A third measurement to ensure validity is the Day Reconstruction Method. In this measure, participants fill out a diary of the previous days’ activities. The participant is then asked to describe each activity and provide a report of how they were feeling, what mood they were experiencing, and any emotions that surfaced. Thus to ensure valid results, a researcher may tend to use self-reports along with another form of measurement mentioned above. Someone with a high level of life satisfaction and a positive affective balance is said to have a high level of SWB.\n\nTheories of the causes of SWB tend to emphasise either top-down or bottom-up influences.\n\nIn the top-down view, global features of personality influence the way a person perceives events. Individuals may therefore have a global tendency to perceive life in a consistently positive or negative manner, depending on their stable personality traits. Top-down theories of SWB suggest that people have a genetic predisposition to be happy or unhappy and this predisposition determines their SWB \"setpoint\". Set Point theory implies that a person's baseline or equilibrium level of SWB is a consequence of hereditary characteristics and therefore, almost entirely predetermined at birth. Evidence for this genetic predisposition derives from behavior-genetic studies that have found that positive and negative affectivity each have high heritability (40% and 55% respectively in one study). Numerous twin studies confirm the notion of set point theory, however, they do not rule out the possibility that is it possible for individuals to experience long term changes in SWB.\n\nDiener et al. note that heritability studies are limited in that they describe long-term SWB in a sample of people in a modern western society but may not be applicable to more extreme environments that might influence SWB and do not provide absolute indicators of genetic effects. Additionally, heritability estimates are inconsistent across studies.\n\nFurther evidence for a genetically influenced predisposition to SWB comes from findings that personality has a large influence on long-term SWB. This has led to the \"dynamic equilibrium model\" of SWB. This model proposes that personality provides a baseline for emotional responses. External events may move people away from the baseline, sometimes dramatically, but these movements tend to be of limited duration, with most people returning to their baseline eventually.\n\nFrom a bottom-up perspective, happiness represents an accumulation of happy experiences. Bottom-up influences include external events, and broad situational and demographic factors, including health and marital status. Bottom-up approaches are based on the idea that there are universal basic human needs and that happiness results from their fulfilment. In support of this view, there is evidence that daily pleasurable events are associated with increased positive affect, and daily unpleasant events or hassles are associated with increased negative affect.\n\nHowever, research suggests that external events account for a much smaller proportion of the variance in self-reports of SWB than top-down factors, such as personality. A theory proposed to explain the limited impact of external events on SWB is hedonic adaptation. Based originally on the concept of a \"hedonic treadmill\", this theory proposes that positive or negative external events temporarily increase or decrease feelings of SWB, but as time passes people tend to become habituated to their circumstances and have a tendency to return to a personal SWB \"setpoint\" or baseline level.\n\nThe hedonic treadmill theory originally proposed that most people return to a neutral level of SWB (i.e. neither happy nor unhappy) as they habituate to events. However, subsequent research has shown that for most people, the baseline level of SWB is at least mildly positive, as most people tend to report being at least somewhat happy in general and tend to experience positive mood when no adverse events are occurring. Additional refinements to this theory have shown that people do not adapt to all life events equally, as people tend to adapt rapidly to some events (e.g. imprisonment), slowly to others (e.g. the death of a loved one), and not at all to others (e.g. noise and sex).\n\nA number of studies have found that SWB constructs are strongly associated with a range of personality traits, including those in the five factor model. Findings from numerous personality studies show that genetics account for 20-48% of the variance in Five-Factor Model and the variance in subjective well-being is also heritable. Specifically, neuroticism predicts poorer subjective well-being whilst extraversion, agreeableness, conscientiousness and openness to experience tend to predict higher subjective well-being. A meta-analysis found that neuroticism, extraversion, agreeableness, and conscientiousness were significantly related to all facets of SWB examined (positive, negative, and overall affect; happiness; life satisfaction; and quality of life). Neuroticism was the strongest predictor of overall SWB and is the strongest predictor of negative affect.\n\nA large number of personality traits are related to SWB constructs, although intelligence has negligible relationships. Positive affect is most strongly predicted by extraversion, to a lesser extent agreeableness, and more weakly by openness to experience. Happiness was most strongly predicted by extraversion, and also strongly predicted by neuroticism, and to a lesser extent by the other three factors. Life satisfaction was significantly predicted by neuroticism, extraversion, agreeableness, and conscientiousness. Quality of life was very strongly predicted by neuroticism, and also strongly predicted by extraversion and conscientiousness, and to a modest extent by agreeableness and openness to experience. One study found that subjective well-being was genetically indistinct from personality traits, especially those that reflected emotional stability (low Neuroticism), and social and physical activity (high Extraversion), and constraint (high Conscientiousness).\n\nDeNeve (1999) argued that there are three trends in the relationship between personality and SWB. Firstly, SWB is closely tied to traits associated with emotional tendencies (emotional stability, positive affectivity, and tension). Secondly, relationship enhancing traits (e.g. trust, affiliation) are important for subjective well-being. Happy people tend to have strong relationships and be good at fostering them. Thirdly, the way people think about and explain events is important for subjective well-being. Appraising events in an optimistic fashion, having a sense of control, and making active coping efforts facilitates subjective well-being. Trust, a trait substantially related to SWB, as opposed to cynicism involves making positive rather than negative attributions about others. Making positive, optimistic attributions rather than negative pessimistic ones facilitates subjective well-being.\n\nThe related trait of eudaimonia or psychological well-being, is also heritable. Evidence from one study supports 5 independent genetic mechanisms underlying the Ryff facets of psychological well-being, leading to a genetic construct of eudaimonia in terms of general self-control, and four subsidiary biological mechanisms enabling the psychological capabilities of purpose, agency, growth, and positive social relations\n\nA person's level of subjective well-being is determined by many different factors and social influences prove to be a strong one. Results from the famous Framingham Heart Study indicate that friends three degrees of separation away (that is, friends of friends of friends) can affect a person's happiness. From abstract: \"A friend who lives within a mile (about 1.6 km) and who becomes happy increases the probability that a person is happy by 25%.\"\n\nResearch indicates that wealth is related to many positive outcomes in life. Such outcomes include: improved health and mental health, greater longevity, lower rates of infant mortality, experience fewer stressful life events, and less frequently the victims of violent crimes However, research suggests that wealth has a smaller impact on SWB than people generally think, even though higher incomes do correlate substantially with life satisfaction reports.\n\nThe relative influence of wealth together with other material components on overall subjective well-being of a person is being studied through new researches. The Well-being Project at Human Science Lab investigates how material well-being and perceptual well-being works as relative determinants in conditioning our mind for positive emotions.\n\nIn a study done by Aknin, Norton, & Dunn (2009), researchers asked participants from across the income spectrum to report their own happiness and to predict the happiness of others and themselves at different income levels. In study 1, predicted happiness ranged between 2.4-7.9 and actual happiness ranged between 5.2-7.7. In study 2, predicted happiness ranged between 15-80 and actual happiness ranged between 50-80. These findings show that people believe that money does more for happiness than it really does. However, some research indicates that while socioeconomic measures of status do not correspond to greater happiness, measures of sociometric status (status compared to people encountered face-to-face on a daily basis) do correlate to increased subjective well-being, above and beyond the effects of extroversion and other factors.\n\nThe Easterlin Paradox also suggests that there is no connection between a society's economic development and its average level of happiness. Through time, the Easterlin has looked at the relationship between happiness and Gross Domestic Product (GDP) across countries and within countries. There are three different phenomena to look at when examining the connection between money and Subjective well-being; rising GDP within a country, relative income within a country, and differences in GDP between countries.\n\nMore specifically, when making comparisons between countries, a principle called the Diminishing Marginal Utility of Income (DMUI) stands strong. Veenhoven (1991) said, \"[W]e not only see a clear positive relationship [between happiness and GNP per capita], but also a curvilinear pattern; which suggest that wealth is subject to a law of diminishing happiness returns.\" Meaning a $1,000 increase in real income, becomes progressively smaller the higher the initial level of income, having less of an impact on subjective well-being. Easterlin (1995) proved that the DMUI is true when comparing countries, but not when looking at rising gross domestic product within countries.\n\nThere are substantial positive associations between health and SWB so that people who rate their general health as \"good\" or \"excellent\" tend to experience better SWB compared to those who rate their health as \"fair\" or \"poor\". A meta-analysis found that self-ratings of general health were more strongly related to SWB than physician ratings of health. The relationship between health and SWB may be bidirectional. There is evidence that good subjective well-being contributes to better health.\nA review of longitudinal studies found that measures of baseline subjective well-being constructs such as optimism and positive affect predicted longer-term health status and mortality. Conversely, a number of studies found that baseline depression predicted poorer longer-term health status and mortality. Baseline health may well have a causal influence on subjective well-being so causality is difficult to establish.\nA number of studies found that positive emotions and optimism had a beneficial impact on cardiovascular health and on immune functioning. Changes in mood are also known to be associated with changes in immune and cardiovascular response.\nThere is evidence that interventions that are successful in improving subjective well-being can have beneficial effects on aspects of health. For example, meditation and relaxation training have been found to increase positive affect and to reduce blood pressure. The effect of specific types of subjective well-being is not entirely clear. For example, how durable the effects of mood and emotions on health are remains unclear. Whether some types of subjective well-being predict health independently of others is also unclear. Meditation has the power to increase happiness because it can improve self-confidence and reduces anxiety, which increases your well-being. Cultivating personal strengths and resources, like humour, social/animal company, and daily occupations, also appears to help people preserve acceptable levels of SWB despite the presence of symptoms of depression, anxiety, and stress.\n\nResearch suggests that probing a patient's happiness is one of the most important things a doctor can do to predict that patient's health and longevity. In health-conscious modern societies, most people overlook the emotions as a vital component of one's health, while over focusing on diet and exercise. According to Diener & Biswas-Diener, people who are happy become less sick than people who are unhappy. There are three types of health: morbidity, survival, and longevity. Evidence suggests that all three can be improved through happiness:\n\n\nA positive relationship has been found between the volume of gray matter in the right precuneus area of the brain, and the subject's subjective happiness score. A 6 week mindfulness based intervention was found to correlate with a significant gray matter increase within the precuneus.\n\nThere are a number of domains that are thought to contribute to subjective well-being. In a study by Hribernik and Mussap (2010), leisure satisfaction was found to predict unique variance in life satisfaction, supporting its inclusion as a distinct life domain contributing to subjective well-being. Additionally, relationship status interacted with age group and gender on differences in leisure satisfaction. The relationship between leisure satisfaction and life satisfaction, however, was reduced when considering the impact of core affect (underlying mood state). This suggests that leisure satisfaction may primarily be influenced by an individual's subjective well-being level as represented by core affect. This has implications for possible limitations in the extent to which leisure satisfaction may be improved beyond pre-existing levels of well-being and mood in individuals.\n\nAlthough all cultures seem to value happiness, cultures vary in how they define happiness. There is also evidence that people in more individualistic cultures tend to rate themselves as higher in subjective well-being compared to people in more collectivistic cultures.\n\nIn Western cultures, predictors of happiness include elements that support personal independence, a sense of personal agency, and self-expression. In Eastern cultures, predictors of happiness focus on an interdependent self that is inseparable from significant others. Compared to people in individualistic cultures, people in collectivistic cultures are more likely to base their judgments of life satisfaction on how significant others appraise their life than on the balance of inner emotions experienced as pleasant versus unpleasant. Pleasant emotional experiences have a stronger social component in East Asian cultures compared to Western ones. For example, people in Japan are more likely to associate happiness with interpersonally engaging emotions (such as friendly feelings), whereas people in the United States are more likely to associate happiness with interpersonally disengaging emotions (pride, for example). There are also cultural differences in motives and goals associated with happiness. For example, Asian Americans tend to experience greater happiness after achieving goals that are pleasing to or approved of by significant others compared to European Americans. There is also evidence that high self-esteem, a sense of personal control and a consistent sense of identity relate more strongly to SWB in Western cultures than they do in Eastern ones. However, this is not to say that these things are unimportant to SWB in Eastern cultures. Research has found that even within Eastern cultures, people with high self-esteem and a more consistent sense of identity are somewhat happier than those who are low in these characteristics. There is no evidence that low self-esteem and so on are actually beneficial to SWB in any known culture.\n\nA large body of research evidence has confirmed that people in individualistic societies report higher levels of happiness than people in collectivistic ones and that socioeconomic factors alone are insufficient to explain this difference. In addition to political and economic differences, individualistic versus collectivistic nations reliably differ in a variety of psychological characteristics that are related to SWB, such as emotion norms and attitudes to the expression of individual needs. Collectivistic cultures are based around the belief that the individual exists for the benefit of the larger social unit, whereas more individualistic cultures assume the opposite. Collectivistic cultures emphasise maintaining social order and harmony and therefore expect members to suppress their personal desires when necessary in order to promote collective interests. Such cultures therefore consider self-regulation more important than self-expression or than individual rights. Individualistic cultures by contrast emphasise the inalienable value of each person and expect individuals to become self-directive and self-sufficient. Although people in collectivistic cultures may gain happiness from the social approval they receive from suppressing self-interest, research seems to suggest that self-expression produces a greater happiness \"payoff\" compared to seeking approval outside oneself.\n\nPositive psychology is particularly concerned with the study of SWB. Positive psychology was founded by Seligman and Csikszentmihalyi (2000) who identified that psychology is not just the study of pathology, weakness, and damage; but it is also the study of strength and virtue. Researchers in positive psychology have pointed out that in almost every culture studied the pursuit of happiness is regarded as one of the most valued goals in life. Understanding individual differences in SWB is of key interest in positive psychology, particularly the issue of why some people are happier than others. Some people continue to be happy in the face of adversity whereas others are chronically unhappy at the best of times. \n\nPositive psychology has investigated how people might improve their level of SWB and maintain these improvements over the longer term, rather than returning to baseline. Lyubomirsky (2001) argued that SWB is influenced by a combination of personality/genetics (studies have found that genetic influences usually account for 35-50% of the variance in happiness measures), external circumstances, and activities that affect SWB. She argued that changing one's external circumstances tends to have only a temporary effect on SWB, whereas engaging in activities (mental and/or physical) that enhance SWB can lead to more lasting improvements in SWB.\nSWB is often used in appraising the wellbeing of populations.\n\n\n"}
{"id": "33242852", "url": "https://en.wikipedia.org/wiki?curid=33242852", "title": "The Health Lottery", "text": "The Health Lottery\n\nThe Health Lottery is a lottery that operates on behalf of 12 local society lotteries across Great Britain. It was launched in October 2011 and runs 5 weekly draws on Tuesdays, Wednesdays, Thursdays, Fridays, and Saturdays. Each week a different Community Interest Company raises the funds for their respective area, with 20% of ticket sales being awarded to local health-related causes. It is operated by Northern and Shell.\n\nThe origins of the Health Lottery predate the National Lottery, to the failed National Health Service Lottery which had an abortive launch in 1988, before being declared illegal. The assets of the NHS Lottery were purchased in 2007 by Altala Group Ltd, a company run by Ian Milligan, a former employee of Camelot Group, the operators of the UK National Lottery. Altala went into administration in 2009, shortly before it was due to be granted its gambling licence, and was purchased by Health Lottery Ltd. After winning the licence, the Health Lottery was sold in February 2011 to Richard Desmond's Northern & Shell group. The Health Lottery launched in September 2011.\n\nFrom launch, draws for the Health Lottery were broadcast live on the ITV network during an advertisement break of \"The X Factor\", paid for by Northern and Shell, and shortly after on Channel 5. Draws are currently broadcast at around 9.55pm on Channel 5. It was initially planned to be hosted by Eamonn Holmes but as Holmes was employed by Sky News, he was dropped due to a possible conflict of interest. Melinda Messenger was confirmed as the host on 5 October 2011.\n\nTo celebrate the first anniversary of the Health Lottery, a new Wednesday night draw called \"Win Wednesday\" was launched.\n\nOn 5 August 2015, it was announced that the Health Lottery would be extended to five nights a week, with previous National Lottery presenter Anthea Turner taking over as host of the new-look draws from 13 August.\n\nThe game has been criticised for donating only 20.3% of the ticket price to causes, compared with 28% donated by the National Lottery. Spokespersons for The Health Lottery argue that the game will grow the market. Some attention was also given to the possibility that favourable coverage of the lottery in Desmond's \"Daily Express\" and the \"Daily Star\" amounted to disguised advertising, which was reported to the Advertising Standards Authority, who found the issue to be outside their remit. The Culture Secretary Jeremy Hunt announced in October 2011 that the Gambling Commission would investigate the legality of the Health Lottery, after questions were raised about the way the lottery was being run and the cost to smaller lotteries nationwide.\n\nCamelot Group attempted to take high court legal action to have The Health Lottery’s gambling licence revoked in 2012, claiming that they had not received adequate protection from The Gambling Commission. Camelot lost the case, a verdict described by Richard Desmond as “a complete vindication of all the hard work that has gone into launching The Health Lottery…if Camelot had succeeded in this case, they would have shut down lifeline funding to hundreds of health projects and charities. The charity sector would have lost out badly.\"\n\nTo circumvent the requirements of the Gambling Act, the Health Lottery is not structured as a national lottery, but rather as 12 different society lotteries, each one representing at least one local authority area within England, Scotland and Wales. Each society lottery is licensed by the Gambling Commission and operates as an individual Community Interest Company, or CIC. Different CICs take turns in participating in a weekly draw so each week different CICs are represented and every region gets a share of the pot. Money is then donated to support health-related causes within their respective local areas. The Health Lottery ELM Ltd operates as an external lottery manager to oversee the society lotteries. The ELM charges a management fee of 0.5p in every pound, which provides the profit for the lottery owners. The allocation of funds to causes raised by the Health Lottery is determined by the relevant CICs and their partner charity, the People’s Health Trust, and not by The Health Lottery ELM Ltd.\n\nIn 2010 this complicated structure was judged by the Gambling Commission regulatory panel to be \"close to the line in respect of section 99 of the [Gambling] Act\" and that while the structure was \"capable of being compliant with the Act\" it was \"finely balanced\". Concern was expressed that the individual lotteries were not to be combined into one de facto national lottery, as to do so would be a breach of the Act.\n\n"}
{"id": "13086027", "url": "https://en.wikipedia.org/wiki?curid=13086027", "title": "The Natural History of Alcoholism Revisited", "text": "The Natural History of Alcoholism Revisited\n\nThe Natural History of Alcoholism Revisited (1995) is a book by psychiatrist George E. Vaillant that describes two multi-decade studies of the lives of 600 American males, non-alcoholics at the outset, focusing on their lifelong drinking behaviours. By following the men from youth to old age it was possible to chart their drinking patterns and what factors may have contributed to alcoholism. Another study followed 100 severe alcoholics from a clinic eight years after their detoxification. \"The National Review\" hailed the first edition (1983) as \"a genuine revolution in the field of alcoholism research\" and said that \"Vaillant has combined clinical experience with an unprecedented amount of empirical data to produce what may ultimately come to be viewed as the single most important contribution to the literature of alcoholism since the first edition of AA's Big Book.\" Some of the main conclusions of Vaillant’s book are:\n\nCore City: In 1940, Sheldon and Eleanour Glueck of Harvard began a major study of juvenile delinquency in teens from Boston — mostly poor kids in tenements, half without a bathtub in their homes. The control group for the study comprised 456 boys who were assessed as non-delinquent. In 1974 this control group, which Vaillant referred to as the Core City sample, was turned over to him to continue research. The Core City group had a mean IQ of 95, and 48% graduated from high school.\n\nCollege: In 1976 Vaillant inherited another study of more than 200 Harvard sophomores started in 1938—the College sample. The sophomores were white males, selected because they were high achievers and had no known medical or psychological problems. The College sample had a mean IQ of 130, and 76% attended graduate school. Their 1976 mean income was three times that of the Core City group.\n\nThe research eventually showed that for the Core City sample at age 60, 36% had abused alcohol at some time in their lives; for the College sample at age 70, the figure was 22%.\n\nThe samples were narrow (\"male, white, American, and born between 1919 and 1932.\") but were followed for a long period. As critics and Vaillant himself pointed out, the samples did not include important segments of the population such as African-Americans and women. Both samples likely excluded those who began abusing alcohol in early adolescence.\n\nThe Clinic sample was a group of 100 severe alcoholics who were detoxified at clinic in an urban, municipal hospital (Cambridge Hospital) in Massachusetts during the winter of 1971-1972. The treatment was carried out under what was known as the CASPAR program—Cambridge-Somerville Program for Alcohol Rehabilitation. This group was followed for 8 years to measure the effectiveness of the treatment.\n\n\nIn the 1983 edition of his book, Vaillant required four positive answers to questions on his Problem Drinking Scale (PDS) to indicate \"alcohol abuse\". To diagnose full-blown alcoholism—i.e. \"alcohol dependence\"—he used DSM III, which requires either physical tolerance or physiological withdrawal. For the 1995 edition he abandoned the PDS and used the DSM definitions of both \"abuse\" and \"alcoholism\".\n\nA longitudinal study is one that follows test subjects over a long period of time — as opposed to a cross-sectional study, which gives a 'snap-shot' of a group at one point in time. Longitudinal studies tend to examine smaller groups in greater detail, whereas cross-sectional studies are often based on a more representative segment of the population over a short time. The longitudinal method was useful in identifying factors in alcoholism, for instance by investigating whether delinquent behaviour started before or after drinking.\n\nA prospective study is forward-looking and includes subjects who did not originally have symptoms of the disease being studied. Many retrospective, or backward-looking, studies might take a group of alcoholics and try to determine what common traits might have caused their alcoholism. A prospective study takes a group of healthy individuals and tries to predict which ones would become alcoholic based on their histories—a much broader technique that often yields surprising results.\n\nAlcoholics present special challenges for researchers because they are good at concealing their drunkenness. Vaillant asserts that \"Alcoholics are expert forgetters,\" have inaccurate memories, and give persuasive denials that manifest \"an extraordinary ability to deny the consequences of their drinking.\" For effective interviews, the subject should first be relatively sober. The interviewer should ask non-threatening, non-judgmental questions that do not challenge the alcoholic's right to drink and that minimise guilt. (Failure to observe these guidelines by medical professionals has likely contributed to alcoholics’ reputation for denial.) Interviewers should ask for objective evidence. For example, if an alcoholic claims that his divorce led to him drinking, the interviewer could ask if the alcoholic's spouse ever complained of his drinking \"before\" they split. Interviews and questionnaires should always be backed up with interviews with the subject's family, with consultation of medical records, and with searches in public records for evidence of legal problems associated with drinking.\n\nA major focus in the book was a comparison of the various definitions of alcoholism:\n\nThe first interesting observation was that there is no sharp dividing line between alcoholics and non-alcoholics. The number of drinking problems is spread out along a scale, just like IQ and blood pressure; there is not a cluster of alcoholics at the top end of the scale.\n\nVaillant compiled indicators of alcoholism from many sources, medical and sociological, and applied them to the Core City drinkers. Possible criteria included\nStatistical techniques were used to determine which, if any, of the criteria were the best indicators of alcoholism. Surprisingly, the answer was that all criteria were of roughly equal importance. No particular indicator or cluster of indicators predominated: it was the \"number\" and \"frequency\" of problems that best defined alcoholism. More importantly, the medical, sociological, and behavioural criteria were equally reliable (i.e. highly correlated). In other words, it was equally valid to call alcoholism a medical or a behavioural disorder—evidence that doctors and sociologists are indeed talking about the same “unitary disorder”.\n\nThere are some grounds, Vaillant argues, for considering alcoholism a medical disease in the most severe cases. As the disorder worsens, conscious choice becomes less and less important and the alcoholic needs medical assistance to detoxify without risk to life (unlike, for example, heroin, which poses less physical danger to addicts going ‘cold turkey’). In this respect, alcoholism resembles coronary heart disease, which starts as ‘voluntary’, unhealthy behaviours such as poor diet and lack of exercise, but ends in a life-threatening medical condition.\n\nPublic opinion has it that alcoholics drink because they have underlying anxiety, unhappy childhoods, and lack of self-control. However Vaillant’s results indicated that some \"obvious\" causes of alcoholism such as anxiety or unhappy childhoods, were not significant and that the alcoholic personality—self-centered, immature, dependent, resentful, and irresponsible—was not evident until \"after\" the subjects had started to abuse alcohol The type of personality found most likely to become alcoholic was antisocial and extroverted, although most antisocial behaviour observed was a result of alcoholism. The presence or absence of childhood environmental strengths predicted which of the College men would take tranquilizers or require medicine for physical complaints, but did not predict alcoholism. Unhappy family environments caused alcoholism only if the unhappy environments were the \"result\" of alcoholism in the first place.\n\nThe ethnic culture of each man was important. Among the Core City subjects, 61% of whose parents were born in foreign countries, alcoholics were seven times more likely to have Irish than Italian backgrounds. In general, more alcoholics came from countries such as Ireland that prohibited drinking among children but condoned adult drunkenness. Fewer alcoholics came from countries such as Italy that allowed children to drink, especially at meals, and looked down on adult drunkenness.\n\nAlcoholism in ancestors was a factor. Men with several alcohol-abusing ancestors (i.e. not members of immediate family) were twice as likely to become alcoholics as those with none. The presence of an alcoholic parent increased the risk of alcoholism by three times, although it was not clear from the data if the factors were genetic or environmental.\n\nOther miscellaneous factors leading to alcohol dependence included the rapidity with which the alcohol reaches the brain (\"gives a high\"); jobs such as journalism that encourage drinking because they have no daily structure; drinking behaviour in one's social group; legal availability of alcohol; cost of alcohol; and social stability—in other words medical, environmental, social, and economic factors.\n\nDepression, clinically so often found to occur with alcoholism, was likewise found to be a \"result\" of alcoholism. Evidence such as this indicated that alcoholism is not merely a symptom of an underlying disorder, but is an independent disorder in itself.\n\nContrary to popular conception, alcoholism does not start with the first drink, but usually has a gradual onset over 5 to 15 years of continuous alcohol abuse. One surprise to Vaillant was the number of men who were able to abuse alcohol for decades without becoming dependent. Of 29 alcohol abusers in the College sample, seven men were able to drink heavily for a mean of three decades without showing symptoms of dependence.\n\nThe average age of onset of alcohol abuse was 29 years for the Core City men and 41 years for the College men. Full blown alcoholism, where it appeared, usually lasted a decade or two before sobriety was attained. The number of alcoholics increased steadily until age 40 and then began to decline at a rate of stable remission of 2 to 3% per year. Older alcoholics are relatively rare because of the rate of remission and a higher mortality rate.\n\nSeventy-two alcoholics in the Core City sample were successfully followed to age 70. By this age 54% had died, 32% were abstinent, 1% were controlled drinkers, and 12% were still abusing alcohol.\n\nBy comparison 19 alcoholics in the College sample were successfully followed to age 70. By this age 58% had died, 21% were abstinent, 10.5% were controlled drinkers, and 10.5% were still abusing alcohol.\n\nOn the topic of whether controlled drinking is advisable as a therapeutic goal, Vaillant concluded that “training alcohol-dependent individuals to achieve stable return to controlled drinking is a mirage.” Successful return to controlled drinking is possible, just a rare and unstable outcome that in the long term usually ends in relapse or abstinence, especially for the more severe cases. Vaillant tracked two samples within his study group: 21 alcohol abusers who had attained stable abstinence, and 22 who had returned to a stable pattern of controlled drinking. At the end of 15 years of follow-up, in 1995, one of the 21 abstainers had returned to controlled drinking, and one had relapsed. In contrast, of the 22 controlled drinkers 3 became abstinent and 7 relapsed. For the less severe cases, Vaillant concluded that controlled drinking \"is\" a worthwhile and valid goal, but “by the time an alcoholic is ill enough to require clinic treatment, return to asymptomatic drinking is the exception not the rule.”\n\nIn the Clinic sample, 100 severe alcoholics treated at the clinic were followed for 8 years. The clinic’s methods were multi-modal: detoxification and hospital treatment followed by referral to AA. At the end of the 8 years, 34% of subjects had achieved stable abstinence, 29% had died, and 26% were still abusing alcohol, and the evidence was that other clinical studies had reported similar lack of success. Subjects who had a stable social environment or who frequently went to AA meetings had the highest rates of abstinence. Overall, however, treatment other than AA did not significantly improve the subjects’ outcome. In fact Vaillant reports the dismal fact that fully 95% of the Clinic sample had relapsed at some time during the 8-year study period. Vaillant noted that clinical treatment helped only in the short term, as crisis intervention and detox. There was one indicator, a financial one, of short-term success: clinical intervention had significantly reduced the cost of future health care for the alcoholics.\n\nVaillant’s conclusion was that “There is compelling evidence that the results of our treatment were no better than the natural history of the disease.” If clinical treatment had failed to improve on the long-term recovery rates of alcoholics, then what would be the most hopeful route to sobriety?\n\nResearch by Vaillant and others found that there were no obvious factors or personality differences to distinguish alcoholics from abstainers; “To a large extent, relapse to and remission from alcoholism remains a mystery.” As was observed in the 1940s in patients with tuberculosis—at that time incurable—recovery depended largely on the patient’s own resistance and morale. The same applies to alcoholism, which at present still has no known ‘cure’. As with diabetes, professional help is in training to prevent a relapse and in crisis intervention until patients are strong enough to heal themselves. If natural forces are dominant in the healing process, then treatment should aim to strengthen and support these natural forces, Vaillant argued. The alcoholic needs support in making the required personality change. Thus, achieving long-term sobriety usually involves \n\nVaillant argues that an important contribution health professionals can make is to explain alcoholism to patients as a \"disease\", which encourages the patient to take responsibility for their problem without debilitating guilt, in the same way that a diabetic becomes responsible for proper self care when they become aware of their condition.\n\nVaillant, who is a non-alcoholic Trustee of AA, made the effectiveness of AA one of the key questions to be investigated in his research. Vaillant argues that AA and other similar groups effectively harness the above four factors of healing and that many alcoholics achieve sobriety through AA attendance. However, he also notes that the “effectiveness of AA has not been adequately assessed” and that “direct evidence for the efficacy of AA… remains as elusive as ever. For example, if an alcoholic achieves sobriety during AA attendance, who is to say if AA helped or if he merely went to AA when he was ready to heal?\n\nIn the Clinic sample, 48% of the 29 alcoholics who achieved sobriety eventually attended 300 or more AA meetings, and AA attendance was associated with good outcomes in patients who otherwise would have been predicted not to remit. In the Core City sample the more severe alcoholics attended AA, possibly because all other avenues had failed—after all, AA meetings are rarely attended for hedonistic reasons. The implication from all three samples was simply that many alcoholics find help through AA.\n\nVaillant’s academic peers saw \"The Natural History of Alcoholism\" as “objective, scholarly, and factual,” “wise” and “comprehensive”, an “outstanding and highly recommended text”, and “one of the few [longitudinal studies] and by far the most thorough and scientific.” James Royce wrote that Vaillant \"cites innumerable studies and examines opposing viewpoints on every issue,\" but that this objectivity made the book harder to read for the general reader since the conclusions were difficult to extract.\n\nThere were varying opinions on the book’s readability. According to David N. Saunders “The book is hard to follow because so much research material is included.” \"The New York Times\" advised that the casual reader should skip over most of the technical discussion, whereas \"The National Review\" noted only an “occasional thicket of psycho-statistical jargon.”\n\nRoyce wrote that Vaillant failed to summarize new (in 1983) research findings on alcohol’s interaction with the brain, and that Vaillant had not quoted some notable researchers who have argued for the disease model of alcoholism. Saunders held that more discussion of the treatment issues was needed and noted that many of the measurements made before Vaillant took over the studies were very crude.\n\nPerhaps the sharpest critic of Vaillant’s work was controlled drinking proponent Stanton Peele. In a 1983 review in \"The New York Times\", Peele wrote that \"The results of this research do not provide ready support for the disease theory of alcoholism. ... [For example, Vaillant] finds strong evidence in the inner city group for sociocultural causality in alcoholism.\" In his book \"Diseasing of America\" Peele claimed that \"Vaillant emphatically endorses the disease model... He sees alcoholism as a primary disease... However, Vaillant's claims are not supported by his own data.\" Other reviewers held the opposite, that Vaillant did \"not\" see alcoholism as a disease. Addiction researcher James E. Royce wrote that \"Vaillant avoids a simplistic medical model of alcoholism, pointing up instead its complexity as a socio-psycho-biological illness.\" David N. Saunders of the School of Social Work, Virginia Commonwealth University, wrote that Vaillant \"maintains that alcoholism is both a disease and a behaviour disorder.\" In his summary at the end of the book, Vaillant in fact wrote that \"Alcoholism can simultaneously reflect both a conditioned habit and a disease; and the disease of alcoholism can be as well defined by a sociological model as by a medical model.\"\n\n\n"}
{"id": "52484031", "url": "https://en.wikipedia.org/wiki?curid=52484031", "title": "Timeline of healthcare in the United Kingdom", "text": "Timeline of healthcare in the United Kingdom\n\nThis is a timeline of healthcare in the United Kingdom. Major events such as crisis, policies and organizations are described.\n"}
{"id": "52289259", "url": "https://en.wikipedia.org/wiki?curid=52289259", "title": "Traditional Cambodian medicine", "text": "Traditional Cambodian medicine\n\nTraditional Cambodian medicine (Khmer:វេជ្ជសាស្រ្តបូរាណខ្មែរ) comprise several traditional medicine systems in Cambodia.\n\nHealers and herbalists of Cambodian traditional medicine are collectively referred to as \"Kru Khmer\" (Khmer:គ្រូខ្មែរ). There are many regional variations of the practice and herbal knowledge of traditional medicine within Cambodia. Traditional Cambodian medical practices are widely used in Cambodia.\n\nEven though health is among the Cambodian governments five most important issues, the healthcare system in the country is inadequate and people in more remote villages in the provinces have difficulty obtaining health care. This situation is reflected in many developing countries and, in part for this reason, the World Health Organization (WHO) is promoting the use of and preservation of knowledge of several traditional medicines in many of these areas across the globe, including Cambodia.\n\nBecause of the ethnic Chinese and ethnic Vietnamese populations of Cambodia, traditional Chinese medicine and traditional Vietnamese medicine are also practiced and represented in the country.\n\nPractitioners of traditional Cambodian medicine are called \"Kru Khmer\" (or alternately \"kru khmae\") (គ្រូខ្មែរ) meaning \"Khmer teachers\", and the teacher-student aspect between practitioner and patient is of central importance to the consultation. Kru Khmers specialise in several categories, such as bone setting, herbalism or divining. Various animal parts, minerals and tattoos are sometimes involved. In the framework of traditional Cambodian medicine, the supernatural world can both cure and cause illness and therefore the definitions between what is medicinal and what is spiritual is often blurred.\n\nKhmer traditional medicine share with Chinese traditional medicine three explanatory models of disease: supernaturalistic theory, naturalistic theory, and maintenance of a hot-cold (yin-yang) balance. Four forms of therapy are delivered by medical and para-medical personnel: spirit offerings, dermabrasion, maintaining hot-cold balance, and herbal medicines.\n\nThe knowledge of Khmer traditional medicine is mediated from teacher to teacher. Each \"Kru Khmer\" answers to his individual teacher, called \"Kru Khom\", through a spiritual connection, even after the death of his \"Kru Khom\". There are intricate rules and rituals involved in this relationship.\n\nKhmer traditional medicine used to rely on written texts, Khmer sastras or palm-leaf manuscripts, since the 9th century, stored and studied at the many temples across the former Khmer Empire. However, during the Cambodian civil war and the following Khmer rouge regime, virtually all historic scholarly texts and philosophical literature in Cambodia were destroyed, including many Khmer medical manuscripts.\n\nTo understand the profession and practice of the \"Kru Khmer\" in more detail, the profession may be divided into a number of sub-classifications, each tied to a specific method, affliction of attention and/or service provided.\n\n\nIndividual \"Kru Khmer\" healers may perform several roles altogether.\n\nThe exact origins of traditional Khmer medicine (TKM) remains unclear, but it is believed to have been founded and formalised from the Nokor Phnom period (Funan era) to the 9th century, during the Angkorian period. It is influenced by Ayurveda (traditional Indian medicine) and traditional Chinese medicine. These foreign frameworks and practices were mixed with local beliefs and superstitions to create the foundations of TKM. The temple of Neak Poan is believed to have been the central temple for Khmer medicine during the Angkorian era. Jayavarman VII, who reigned c.1181–1218, ordered the construction of 102 hospitals (‘halls devoid of disease’ or \"arogyasala\") throughout his realm.\n\nKhmer inscriptions investigated by French archeological researcher George Cœdès in the early 20th century, confirmed the existence of 15 hospitals (out of Jayavarman VII's 102 hospitals) across the kingdom. Those 15 hospitals are :\n\nInscriptions in these hospitals describe the number of medical staff and their different roles such as hospital managers, drug combiner staff, water boiler staff, drug grinders and drug distributors. An inscription in Sai Fong has become the most renowned quote of King Jayavarman VII: \"Diseases of the people make him more painful than his own illness.\"\n\nPrasat Chrey (Khmer: ប្រាសាទជ្រៃ) and Prasat Lek 8 (Number 8 temple) (Khmer: ប្រាសាទលេខ៨) are temples in Sambor Prei Kuk that were previous hospital chapels during the Chenla era. Inside the hospital chapels, ground or smashed traditional herbal medicines extracted from plant trunks, roots and leaves were mixed with purified water. The drug liquid was then poured over a Shiva Linga to enhance the effectiveness, after which it flowed from the Linga base through the outside part of the chapels via a drainpipe connected to the north side of the buildings. Here, patients, who were waiting outside the hospital, finally received the drug liquid.\nThe knowledge and practice of TKM was written down on palm-leaf manuscripts, written in the pali language, and stored in temples all over the empire. Most of these original Khmer medicinal manuscripts are thought to have been destroyed in the Cambodian civil war, but some still exists and they represent some of the most reliable sources to the origins of TKM.\n\nSurviving ancient Khmer medical texts shows a considerable systematization of medical knowledge, but an institutionalized Khmer medical system with associated doctrine-based practices, did not survive into the modern age in Cambodia. Scholars and historians have long wondered what happened to this grand medical tradition and the 13th century is considered a crucial tipping point in the history of traditional Khmer medicine. The gradual decline of the Angkorian Empire and the religious shifts to Theravada Buddhism appears to have affected the original medical culture greatly. The Siamese conquest of Angkor is not thought to have destroyed the medical traditions, but rather appropriated the medical knowledge and preserved it as Thai instead of Khmer. The French colonial era is also thought to have affected and prevented the rise of the ancient Khmer medical tradition. Unlike India for example, where dialogue and knowledge exchange between initial colonialists and Indian doctors took place, the colonial presence beginning with the 18th century in Cambodia was almost instantaneous and the French demanded and relied upon Western medicine from the very start, abolishing the local Cambodian medical traditions altogether. The following independence and warring upheaval of Cambodia during the civil war, the Khmer Rouge regime and the Vietnamese occupation, all continued this suppression of medical traditions, in particular the spiritual aspects. This centuries long pressure have resulted in fragmenting the Khmer medical tradition, sometimes with local reinventions emerging.\n\nHistorically, three sub-categorizations of the \"Kru Khmer\" profession have been noted: \"kru pet\" (Khmer:គ្រូពេទ្យ), \"kru thnam\" and \"tmup\". \"Kru pet\" were the most revered and theoretically educated. They studied the palm-leaf manuscripts in temples and were mostly found around the royalty's residences and temples, less so among the general population. \"Kru thnam\", who were herbalists without much interest in religious aspects, were much more numerous and to be found throughout the country. \"Tmup\" were sorcerers. These general categorizations are still in use in Cambodia today, but the \"kru pet\" class is nonexistent.\n\n\"Kru pet\" hermits have always been glorified and venerated in the Khmer culture for their kindness of saving human and animal lives. Statues were made to pay respect to \"kru pet\", like the hermit sculpture on Phnom Santuk mountain, and hermitages or temples dedicated to \"kru pet\" were built such as Maha Rusey (Khmer: មហាឫសី) hermitage on Phnom Da mountain.\n\nSemahatata (Khmer: \"សឹម្ហទត្ត\") was a royal doctor of King Rodravarman and Jayavarman I during Chenla period. In addition, he was the royal official of King Bhavavarman I and Mahendravarman. At that time, he was the mayor of Vyadhapura also.\n\nYajnavaraha was a religious \"Kru Khmer\" and royal physician of the Khmer Empire in the 10th century. Along with Khmer traditional medicine he also practiced Ayurveda, an Indian traditional medicine system.\n\nA Traditional Medicine Research Center was opened in Cambodia in 1982. In 1983, production of selected medicines began at the research center and a number of books on healing plants have been published. Specialists in Cambodian traditional medicine have been brought to the Faculty of Medicine for conferences and one of the faculty's goals is to introduce traditional medicine in all curricula at primary, secondary, and university levels.\n\nIn 2009, the National School of Traditional Medicine opened in Phnom Penh. The school teaches and collects knowledge of traditional medicine in Cambodia, usually referred to as Khmer traditional medicine. The school has a special educational focus on hygiene and anatomy.\n\nRegulation of herbal medicines in Cambodia was introduced in 1998. Herbal medicines are regulated as over the counter medicines and for self medication only. By law, no claims may be made about herbal medicines.\n\nThe regulatory requirements for manufacturing or safety assessment, and the control mechanisms established to ensure compliance, come under the Department of Drugs and Food. As of 2005, there are 48 registered herbal medicines in Cambodia; however, none of them are included on a national essential drug list.\n\nHerbal medicines in Cambodia are sold in pharmacies as over the counter medicines, in special outlets, by licensed practitioners and without restriction.\n\n\n"}
{"id": "16264963", "url": "https://en.wikipedia.org/wiki?curid=16264963", "title": "USMLE Step 3", "text": "USMLE Step 3\n\nStep 3 is the final exam in the USMLE series of examinations. It is part of the licensing requirements for Doctors of Medicine (M.D.) and international medical graduates to practice medicine in the United States. The USMLE Step 3 exam is considered the final step in the series of medical licensure examinations. Generally, it is a pre-requisite of the majority of the state licensing boards.\n\nUSMLE Step 3 tests several concepts that are often required to provide general health care to a patient. USMLE Step 3 is a mandatory exam that must be passed in order to obtain license as a practicing physician. Some International Medical Graduates are required to pass USMLE Step 3 in order to obtain an H1 Visa.\n\nMost of the USMLE Step 3 exam (75 percent) consists of multiple choice questions, while the remaining 25 percent are clinical case simulations. A full description of the content of the exam can be found on the USMLE website. USMLE Step 3 exams are delivered online and are available throughout the year to the examinees. The examinee needs to register via a state licensing board for this exam.\n\nSince 2014 USMLE Step 3 can be taken on two non-consecutive days, instead of two consecutive days.\n\nUSMLE Step 3 examination tests on general topics that are required to understand and practice concepts of general medicine/ family medicine.\n\nThe following components are tested:\n\nNormal conditions and disease categories (normal growth and development, basic concepts, and general principles)\n\nClinical encounter frame (initial work up, continuing care, urgent intervention)\n\nPhysician task (applying scientific concepts, formulating a diagnosis based on history, physical exam, and lab findings, and managing the patient).\n\nClinical encounter frames are common clinical scenarios physicians may encounter. They range from nonemergency problems, to the continuity of care, to life-threatening emergency situations encountered in emergency departments, clinics, offices, care facilities, inpatient settings, and on the telephone. Each test item, in an encounter frame, represents one of the six physician tasks. For example, initial care encounters emphasize taking a history and performing a physical examination. In contrast, continued care encounters emphasize decisions regarding prognosis and management.\n\nDay 1 (Foundations of Independent Practice [FIP]) will continue to be divided into six 60-minute blocks. Each FIP block will have 38 to 40 multiple-choice questions (MCQs). The total number of MCQs on the FIP portion of the examination will be 233. The total testing day will be approximately 7 hours.\n\nDay 2 (Advanced Clinical Medicine [ACM]) will continue to be divided into six 45-minute blocks of MCQs, and 13 computer-based case simulations (CCS). Each ACM MCQ block will have 30 items. The total number of MCQ items on the ACM portion of the examination will be 180. The second half of day 2 will contain the 13 CCS cases. \nTo be eligible to take the USMLE Step 3 exam, the physician must hold a medical degree, and pass the USMLE Step 1 and Step 2 Clinical Knowledge exams. International medical graduates (IMGs) must obtain certification by the Educational Commission for Foreign Medical Graduates (ECFMG). (Prior to December 2009, one could alternatively complete a \"Fifth Pathway\" program.) The Step 2 CS may also be required. Canadian M.D. graduates are not considered IMGs.\n\nStarting November 2014, fulfillment of specific requirements from individual medical licensing authorities will not be needed.\n\nTypically, worldwide examinees require two to three months(citation needed) to prepare for this exam, although in the US, examinees who are American medical school graduates commonly prepare for only a few days to a few weeks. Physicians in post-graduate training that plan for fellowships or additional training often are advised to consider more detailed preparation. An examinee is tested on clinical skills, diagnostic acumen, decision making, treatment guidelines and follow up care. Most recently, some changes have been made to USMLE Step 3 multiple choice questions including increased emphasis placed on biostatistics, epidemiology, and population health, literature interpretation, medical ethics, and patient safety.\n\nSince the USMLE Step 3 exam is typically taken after matching into a residency, it does not require a physician to obtain a competitive score and only need to pass to obtain their medical license. However, a competitive score may be needed if they want to apply for a fellowship.\n\nOn January 1, 2016 the recommended Step 3 minimum passing score changed to 196. In 2016, there was also a reduction in the number of items on the examination.\n\nFirst-time USMLE pass rates for D.O. and M.D. students in 2015 were 91 percent and 98 percent, respectively. Pass rates for students from schools outside of the United States and Canada were 89 percent. Trainees in fields which encompass multiple specialties, such as emergency medicine or internal medicine, tend to perform well on Step 3 regardless of when they take the exam; trainees in other fields tend to do better if they take the exam shortly after medical school.\n"}
{"id": "25640565", "url": "https://en.wikipedia.org/wiki?curid=25640565", "title": "Women's Health (magazine)", "text": "Women's Health (magazine)\n\nWomen's Health, published by Rodale in Emmaus, Pennsylvania, is a magazine focusing on health, nutrition, fitness, sex, and lifestyle. It is published 10 times a year in the United States and has a circulation of 1.5 million readers. The magazine has 13 international editions spanning 25 countries and reaching more than 8 million readers globally.\n\nThe magazine features multiple different sections, such as fitness, sex & love, food, weight loss, Eat This!, health, beauty and style.\n\nThe magazine allows women an organized approach to keeping their lives on track and staying healthy. The \"Fitness\" section showcases how-to workouts with detailed moves so the reader can replicate the activities effectively. The fitness section also covers topics such as injury prevention, getting started with fitness, cardio, toning, and seasonal tips for working-out in certain conditions. The \"Sex & Love\" section covers topics including relationship advice, tips for better sex and an array of topics on women’s health. The \"Food\" section focuses on topics such as metabolism, organic foods, recipes, ways to boost energy and more. \"Weight Loss\" looks at a wide variety of topics that focus on anything from food to work-outs to sleep, in relation to losing weight. The \"Eat This!\" section, which teaches readers ways to substitute their current meals for healthy alternatives, is based on Rodale Inc.’s \"Eat This, Not That\" book series. This topic looks at the ingredients and nutritional values, so consumers can make healthy eating choices while. The \"Health\" section covers topics such as seasonal colds, women’s heart health, foods that benefit your health, energy boosting tips, eating disorders and more. The \"Beauty\" section covers topics such as skin health, hairstyles, make-up tips and affordable beauty products.\n\n\"Women’s Health\" features a celebrity each month that exudes the lifestyle of a healthy, active woman. Past \"Women’s Health\" cover models include Elisha Cuthbert, Ashley Greene, Anna Kournikova, Michelle Monaghan, Zoe Saldana and Elizabeth Banks.\n\n\"Women's Health\" was created in 2005 as the sister publication of \"Men's Health\" magazine. The test-issue team was headed by Bill Stump, a former \"Men’s Health\" editor who was then the head of Rodale Inc.’s \"New Product Development\" department. The magazine's founding editor-in-chief was Kristina Johnson, previously the original executive editor of \"Teen People\". Within a year, the circulation reached 750,000 readers. David Zinczenko, editor-in-chief of \"Men's Health\" magazine, was named editorial director. Jack Essig, SVP/Publisher of \"Men's Health\", was named SVP/Publisher of \"Women's Health\" in March 2009. \n\nSince its creation, the magazine has been building its brand to encompass a digitally advanced website including a Facebook and Twitter page, and downloadable apps and workouts for its viewers. \"Women’s Health\" is published internationally in the following 14 countries: United States, Argentina, Australia, Brazil, Latin America, China, Germany, Malaysia, New Zealand, Philippines, Poland, South Africa, Thailand and Turkey. The German edition was launched in April 2011.\n\nOn the \"Women's Health\" website viewers can access videos, print-outs, MP3s and personal trainers to use for working out and living a healthy lifestyle. \"Women’s Health\" is connected with nearly 500,000 viewers on \"Facebook\" and \"Twitter\". The website also offers almost two million viewers with monthly newsletters. Women’s Health uses their digital resources to connect with their consumers and provide them with the tools they need to succeed in their active lives.\n\nThe editors at \"Women’s Health\" have teamed together to produce helpful \"DVDs\" for their consumers to aid in weight loss, toning or just getting in better physical condition. Some of their DVDs include: Look Better Naked, Ultimate Abs Workout, The Tone-up Workout, The Wedding Workout, and Train for Your Body Type.\n\nIn conjunction to DVDs, the \"Women's Health\" editors have also published health and fitness books. \"Women’s Health\" has pushed its brand with books such as: \"The Women’s Health Diet\", \"The Big Book of Exercises\", \"The Big Book of Abs\", \"Look Better Naked\", and \"Six Weeks to Skinny Jeans\".\n\n\"Women’s Health\" Are you game? is an event that focuses on women’s fitness, nutrition, beauty and style. This all-day event takes place in Chicago and New York City, and features work-out classes, a fashion show, cooking demos, chair massages and more.\n\n\"Women’s Health\" is a media sponsor for the ZOOMA race. This race features a half marathon, a 5K and 10k race, and a post-race celebration. ZOOMZ 2012 took place in Annapolis, Atlanta, Texas, Cape Cod and The Great Lakes. ZOOMA is an event that celebrates the personal triumphs of women and provides some well-deserved pampering and partying after the race. \nSummer Streets NYC is another event that \"Women’s Health\" helps sponsor, this event shuts down some of the streets in New York City to traffic. This gives people the opportunity to explore more of the city via bike or foot. This event was designed to encourage people to explore alternate modes of transportation in the city.\n\nThe launch of \"Women’s Health\" magazine was one of the most successful today. Currently, WH is ranked #4 on Adweek Media’s “Hot List,” as well as #2 on Advertising Age’s annual “A-List.” Both recognize magazines for their superior performance in advertising and circulation. In March 2008, \"Women’s Health\" finished #1 on \"Adweek\"’s “10 under 50” Hot List. The magazine was named #2 on \"Advertising Ages\" 2008 A List. In 2009, the magazine was named \"Magazine of the Year\" by \"Advertising Age\". In 2011, \"Women's Health\" took home a National Magazine Award for \"General Excellence.\"\n\nFounding editor-in-chief, Kristina Johnson, guided \"Women’s Health\" from its inception in 2003 until her departure in 2008. Under Johnson, the magazine was named the #1 hottest publication in its circulation category by \"Ad Week\" and #2 by \"Advertising Age\" (both in 2008). In January 2009, Michelle Promalayko took over the title.\n\n"}
