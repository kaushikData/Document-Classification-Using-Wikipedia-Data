{"id": "42091770", "url": "https://en.wikipedia.org/wiki?curid=42091770", "title": "Alternative formats", "text": "Alternative formats\n\nAlternative formats include audio, braille, electronic or large print versions of standard print such as educational material, textbooks, information leaflets, and even people's personal bills and letters. Alternative formats are created to help people who are blind or visual impaired to gain assess to their personal and leisure information either by sight (large print), by ear (audio) or by touch (braille).\n\nAudio information is beneficial for many people and can be used by anyone who owns a CD player, a DAISY player or a computer. Audio enables people who are blind or visual impaired to access information through hearing, in the sense that print readers would understand it. \n\nDifferent people are likely to have their own preferences about the way they access audio, depending on their experiences, how comfortable are they with technology, and the equipment they to access the audio content.\n\nCompact Discs (CDs) were first produced in the 1980s to store and playback sound recordings exclusively, and became superior to cassette tapes (AC) offering better sound and size. Standard CDs have a diameter of 120 millimetres (4.7 in) and can hold up to 80 minutes of uncompressed audio (or 700 Megabytes of data). CDs offer some navigation from the beginning of one track to another, rather than having to fast forward or rewind and guess where the tracks started and finished.\n\nOver time, CDs have progressed from being solely for music, to being a format for all kinds of data storage, such as text, images, photos and videos. Educators can use CDs to store educational materials including taped lectures, presentations, and handouts into one compact disc for the student to access on CD players or computers.\n\nBraille is a tactile system of raised dots that enables people who are visual impaired or blind to access information by touch. The pattern of raised dots is arranged in cells of up to six dots, creating a total of 63 different combinations possible. Each cell represents an alphabet letter, numeral or punctuation mark. Some frequently used words and letter combinations also have their own single cell patterns.\nBraille can be the building block for language skills and a way to teach spelling, grammar, and punctuations to people with vision loss or who are deaf-blind. Not only do Braille codes represent alphabets, they also denote numbers, symbols, music and mathematical notations, enabling people to learn about different subjects in a different way from most of us. Braille books are available in all subject areas, ranging from modern fiction to mathematics, music and law. As with printed text, Braille makes it possible for people to access information in this format.\n\nFor many of us, having access to computer equipment and appropriate software makes written information accessible. Today’s technology allows people to do majority of their research online, share documents via email, and download lecture notes from school websites. With written text converted into a format that is readable on the computer, it can be accessed visually with screen magnification software, or through auditory means with text-to-speech technology.\n\nLarge print is essential for people who have visual or learning difficulties and have trouble reading fine print or deciphering crowded text at one time. Large print usually ranges from 16 to 22 point, while giant print uses fonts that are bigger than 24 point.\n\nHaving one single size will not meet everyone’s needs; therefore, it is important to find out which is a comfortable reading size for each individual. Research has demonstrated the positive impacts of providing enlarged font size for people with mild to moderate visual impairments, resulting in an increased reading fluency and speed.\n"}
{"id": "51020703", "url": "https://en.wikipedia.org/wiki?curid=51020703", "title": "Andrew Rae Gilchrist", "text": "Andrew Rae Gilchrist\n\nProf Andrew Rae Gilchrist CBE PRCPE FRSE FRCP (1899–1995) was an eminent Scottish cardiologist who served as President of the Royal College of Physicians of Edinburgh 1957 to 1960. He was their longest serving Fellow: 1929 to 1995 (66 years). He did extensive work on anti-coagulants. In 1959 he was a co-founder of the British Heart Foundation, a charity raising funds for heart research.\n\nHe was born on 7 July 1899 the only son of Rev Andrew Gilchrist (1871–1954) and his wife Catherine Hill. He was born in Holywood, County Down in Northern Ireland. His parents were Scots and he was sent to George Watson's College in Edinburgh for schooling.\n\nAt the age of 18 he was conscripted into the Royal Field Artillery and served for the final year of the First World War.\nHe graduated from Edinburgh University in Medicine in 1921. His first job was at Addenbrooke's Hospital in Cambridge then he moved to the Princess Elizabeth Hospital for Children before spending a year at the Rockefeller Hospital in New York.\nIn 1928 he made the first clinical diagnosis of (and description of) Myocardial infarction and in 1930 he made the first seven recordings of Coronary thrombosis in Europe (published in the Edinburgh Medical Journal). He expressed the opinion that heart attacks were a new disease of the 20th century.\n\nFrom 1931 he worked as a Consultant at the Edinburgh Royal Infirmary alongside Sir Stanley Davidson and Derrick Dunlop. He was an examiner in all Scottish universities plus Makerere University in East Africa and the University of Baghdad.\nIn 1937 he was elected a Fellow of the Royal Society of Edinburgh. His proposers were Edwin Bramwell, Sir Robert William Philip, Arthur Logan Turner and Sir Sydney Smith.\n\nIn 1961 he was made a Commander of the Order of the British Empire (CBE).\n\nIn a typically ironic twist of fate he suffered a Myocardial infarction in 1965 forcing hime to retire.\n\nHe died on 1 March 1995.\n\nIn 1931 he married Emily Faulds (died 1967). They had one son (Andrew Kirkwood Rae Gilchrist) and one daughter (Dr. Jennifer Mary Rae Gilchrist). Following Emily's death he married Elspeth Wrightman in 1975.\n"}
{"id": "569641", "url": "https://en.wikipedia.org/wiki?curid=569641", "title": "Attachment parenting", "text": "Attachment parenting\n\nAttachment parenting (AP) is a parenting philosophy that proposes methods which aim to promote the attachment of parent and infant not only by maximal parental empathy and responsiveness but also by continuous bodily closeness and touch. The term \"attachment parenting\" was coined by the American pediatrician William Sears.\n\nIn family sociology, attachment parenting is considered to be the most striking manifestation of \"intensive mothering\" or \"New momism\". The doctrine has hence been targeted by criticism from a host of objectors.\n\nAttachment parenting is only one of many responsiveness and love-oriented parenting philosophies that entered the pedagogical mainstream after World War II, and it owes many of its ideas to older teachings, such as Benjamin Spock's influential handbook \"Baby and Child Care\" (1946). Spock had mothers advised to raise their infants according to their own common sense and with plenty of physical contact – a guideline that radically broke with the preceding doctrines of L. Emmett Holt and John B. Watson; the book became a bestseller, and Spock's new child rearing concept greatly influenced the upbringing of the post-war generations.\n\nThirty years later, Jean Liedloff caused a stir by a \"continuum concept\" that she presented to the public in a book of the same title (1975). In Venezuela, Liedhoff had studied Ye'kuana people, and later she recommended to Western mothers to nurse and to wear their infants and to share their bed with them. She argued that infants, speaking in terms of evolution, have not arrived in the modernity yet, so that today's way of child care – with bottle feeding, use of cribs and baby carriages, etc. – does not meet their needs. Later, authors such as Sharon Heller and Meredith Small contributed further ethnopediatric insights.\n\nIn 1984, developmental psychologist Aletha Solter published her book \"The Aware Baby\" about a parenting philosophy that advocates attachment, extended breastfeeding and abstinence from punishment, similarly to what William Sears later wrote; however, the point that Solter stressed most was an encouragement of the child's emotional expression in order to heal stress and trauma.\n\nIn the 1990s, T. Berry Brazelton invigorated the discussion. He contributed new research about the capacity of even newborn infants to express themselves and their emotions, sensitized parents for these signals, and encouraged them – just like Spock – to follow their own judgment.\n\nWilliam Sears came to the term \"attachment parenting\" in 1982 by reading Liedloff. Initially, he referred to his new philosophy as \"the new continuum concept\" and \"immersion mothering\". When he published his book \"Creative Parenting\" in 1982, the concept was largely elaborate already. The \"7 Baby-Bs\" were not explicitly presented as a canon yet, but as basic elements of a new parenting philosophy they were distinctly clear even at that early point. In 1985, William Sears and his wife Martha Sears began to link the concept – ex post – with attachment theory which they had begun to recognize at that time. From then on, they used the term \"attachment parenting\".\n\nIn 1993, William Sears and Martha Sears published \"The Baby Book\" which became the first comprehensive manual for AP-parents and which was occasionally dubbed \"the attachment parenting bible\". The first attachment parenting organization, \"Attachment Parenting International\", formed in 1994 in Alpharetta, Georgia and was founded by Lysa Parker and Barbara Nicholson. The first book that carried the term \"attachment parenting\" in the title was written by Tammy Frissell-Deppe, a mother who gave an account of her personal experiences and of those of her friends and acquaintances. In 1999, blogger Katie Allison Granju followed with another book, to which William Sears contributed a foreword, before he, together with Martha Sears, published his own work, \"The Attachment Parenting Book\" in 2001. All three books stood – with their opposition against a crude behavioristic infant anthropology – in the tradition of Spock, but radicalized the concept of a contingency-oriented parenting on the one hand, and incorporated Liedloff's idea of an instinct-guided resp. \"natural\" childrearing on the other hand.\n\nIn the same year as Sears and Sears' \"Attachment Parenting Book\", Jan Hunt published her essay collection \"The Natural Child. Parenting from the Heart\". Hunt who sees herself as a child advocate, campaigned in this book not only for attachment parenting, but also for unschooling. A more recent AP proponent is parenting advisor Naomi Aldort, who published her book \"Raising Our Children, Raising Ourselves\" in 2006.\n\nLike before him the founders of attachment theory, Mary Ainsworth in particular, William Sears teaches that a strong mother-child-attachment emerges from contingency, that is of emotional attunement of mother and child, which again is based on the mother's sensitivity. Since the mother \"reads\" the signals of her infant, Sears speaks in this context of \"babyreading\". Another metaphor that he uses is \"to be in the groove\".\n\nWilliam Sears strongly believes in the existence of child rearing practices that support \"babyreading\" and that augment the maternal sensitivity. The methods of attachment parenting include seven practices resp. principles that according to Sears form a \"synergetic\" ensemble and that are based on the child's \"biological needs\". Sears refers to those principles as \"7 Baby Bs\":\n\nUntil 1999, Sears named only five Baby Bs. The last two were only added in 2001 with the publication of the \"Attachment Parenting Book\".\n\nWilliam Sears postulates the existence of a brief time slot immediately after birth during which the newborn is in a \"quiet alert state\" and particularly accessible for bonding. He refers to this birth bonding as \"imprinting\" and bases himself on a study by Marshall Klaus and John Kennell from 1967; however, Klaus and Kennell later modified their original assumptions, including the one cited by Sears. Sears advises women to abstain from analgesics during childbirth, since those drug the child, too, and according to Sears interfere with the birth bonding.\n\nWilliam Sears argues that breastfeeding greatly accommodates mother-child-attachment because it triggers the release of oxytocin in the mother which supports her emotional bonding with the child, notably in the first ten days after childbirth. In opposition to bottle feeding which tends to being done in 3 to 4 hour intervals, breastfeeding enables the mother, too, to perceive the child's moods and needs exactly. Since the half-life period of the hormones prolactin and oxytocin (which promote bonding) are very short, Sears recommends to breastfeed very frequently, newborns in particular (8 to 12 times a day). He claims that the hours between 1 and 6 am are the most beneficial for breastfeeding. In general, Sears argues that breastfeeding is beneficial for the health of both child and mother. He claims that infants up to 6 months should be exclusively fed with breast milk, since he believes that, at that age, children are allergic to all other foods.\n\nWilliam and Martha Sears advise mothers to breastfeed every child for 1–4 years:\n\nWilliam Sears advocates extended breastfeeding, since he is convinced that breastfeeding supports attachment even of older children and that it is a valid instrument to comfort older children or to bring mother and child together on turbulent days. Neither does he object nighttime breastfeeding of toddlers. As early as in 1992, Norma Jane Bumgarner had campaigned for extended breastfeeding.\n\nSears’ recommendations are in accordance with the WHO guidelines on breastfeeding, which recommend exclusive breastfeeding in the first 6 months and complementary breastfeeding in the first 2 years for all countries.\n\nSince breastfeeding studies are, for ethical reasons, never conducted as randomized controlled trials, critics have repeatedly suspected that studies may have produced the superiority of breastfeeding as an artifact. Both the physical, emotional and mental development of children and the preferences of women for a feeding method are strongly determined by socioeconomical factors such as the mother's ethnicity, social class, and education. If researchers go without randomization and turn a blind eye to those possible alternative factors, they fundamentally run a risk to falsely credit the feeding method for effects of socioeconomical factors. A loophole from this problem was first presented by Cynthia G. Colen (Ohio State University), who successfully factored out socioeconomical determinants by comparing siblings only; her study demonstrated that formula fed children showed only minimal differences to their breastfed siblings, insofar as their physical, emotional and mental thriving was concerned.\n\nWilliam Sears' assumptions about the benefit of breastfeeding for the attachment have been studied. In 2006, John R. Britton and a research team (Kaiser Permanente) found that highly sensitive mothers are more likely than less sensitive mothers to breastfeed and to breastfeed over a long time period. However, the study showed no effect of the feeding method on the attachment quality.\n\nSears advises mothers to wear infants on the body as many hours during the day as possible, for example in a sling. He argues that this practice makes the child happy and allows the mother to involve the child into everything she does and never to lose sight of the child. He advises working mothers to wear the child at least 4–5 hours every night in order to make good for her absence during the day.\n\nIn 1990, a research team from New York revealed in a randomized study that children of lower class mothers who to the age of 13 months spent a lot of time in a child carrier on their mother's body showed significantly more frequently a secure attachment as defined by Ainsworth than the control group children, who spend more time in an infant seat. For middle-class families, an equivalent study doesn't exist yet.\n\nSears argues furthermore that babywearing exercises the child's sense of balance; since a child who is worn on the mother's experiences more of her conversations, he believes that babywearing is also beneficial for the child's language acquisition. However, there are not studies that confirm such effects.\n\nIt is undisputed that babywearing can calm children down. Infants cry the most in the age of 6 weeks; in 1986, a research team at McGill University showed in a randomized study that infants of that age cried significantly less if their parents wore them a lot on the body during the day. Sears recommends babywearing for the purpose of settling a baby to sleep, too. He approves on the use of a sling up to the age of 3, since childwearing can also be used to calm a misbehaving toddler down. Other pediatricians find it disputable to wear children beyond the age of 9 months permanently on the body, arguing that this is against the child's natural desire for autonomy.\n\nWilliam Sears states that any sleeping arrangement that a family practices is acceptable as long as it works; but he advises mother to sleep close to the child. He thinks of co-sleeping as the ideal arrangement and refers to it as the nighttime equivalent of babywearing: co-sleeping supports, in his opinion, the mother-child-attachment, makes breastfeeding more convenient, and prevents not only separation anxiety, but SIDS, too. Sears is convinced that mother and child, in spite of frequent nighttime breastfeeding, have the best sleep when they sleep close together. He is also convinced that due to the extra nighttime feedings, a child that sleeps close to the mother thrives better than a child \"crying, alone, behind bars\". Moreover, Katie Allison Granju argued that co-sleeping is beneficial for children, too, because it gives children a vivid notion of the concept of bedtime.\n\nThe idea of co-sleeping was not new in modern Western societies; as early as in 1976, Tine Thevenin had campaigned for the \"family bed\". Sears doesn't see a problem when a 3 year old still shares his mother's bed every night. He doesn't even object if a child is in the habit of literally spending the whole night with her mother's nipple in her mouth, except when the mother really feels uncomfortable. Sears advises working mothers to co-sleep on all accounts in order to compensate the child for her daytime absence.\n\nSudden infant death syndrome (SIDS) is a very rare incident; it occurs in less than ½ per mill of all infants. James J. McKenna (University of Notre Dame) has discovered that co-sleeping mothers and infants not only synchronize their sleep-wake-rhythm, but their breathing, too; he therefore reasons that co-sleeping lowers the SIDS risk. Nonetheless, studies that investigate SIDS \"directly\" have shown that permanent bed sharing rather raises the SIDS risk than lowering it; it is worth noting that in the study, the increased risk of SIDS occurred in infants younger than four months when the parents were especially tired, had consumed alcohol, were smokers, slept on a sofa, or the baby was in a duvet. Otherwise, no increased risk was associated with bed sharing. The U.S. Consumer Product Safety Commission also warns against co-sleeping. Attachment Parenting International issued a response which stated that the data referenced in the Consumer Product Safety Commission statement were unreliable, and that co-sponsors of the campaign had created a conflict of interest. The American Academy of Pediatrics' policy on SIDS prevention opposes bed-sharing with infants, although room-sharing is encouraged.\n\nIn general, research doesn't confirm an advantage of co-sleeping over separate beds. A meta study from Israel has pointed out in 2000 that sleeping aids such as pacifiers and teddy bears significantly improve the child's sleep, while co-sleeping and frequent nighttime breastfeeding if anything hinder the formation of wholesome sleeping patterns. Co-sleeping mothers breastfeed 3 times as frequently during the night as mothers who have their bed for themselves. Most important factor for a good sleep of the child proved to be the mother's emotional accessibility, not her permanent physical closeness.\n\nWilliam Sears determines crying as the child's pivotal mean of self-expression. Parents are challenged to \"read\" the crying – which is initially generalized – and to provide the child with empathic feedback in order help him to differentiate and elaborate the repertoire of his signals gradually. Furthermore, he recommends \"prevention\" of crying: parents are advised not only to practice breastfeeding, babywearing and co-sleeping as much as possible, but also to get into the habit of properly responding to the early warning signals so that crying doesn't happen in the first place. Likewise, parents must teach their child that some trivial occasions are no cause for alarm at all.\n\nIn general, Sears argues that infants should never been left crying because this would harm them. But as early as in 1962, T. Berry Brazelton had shown in a study that a certain amount of crying in young infants does not indicate emotional or physical problems, but is to be considered normal and harmless.\n\nWilliam Sears names two reasons why infants should not undergo sleep training: he believes that infant training hardens the mother emotionally and that children who underwent such training don't sleep better but merely resign and become apathic, a state that he refers to as \"shutdown syndrome\", although a condition of this name doesn't exist in DSM or ICD. Frissell-Deppe and Granju believe that sleep training is traumatic for children.\n\nSears argues that advocates of sleep training are professionally incompetent and merely business oriented, and that there is no scientific proof that sleep training is beneficial for children.\n\nFor parents and particularly for mothers, attachment parenting is more strenuous and demanding \nthan most other present-day ways of parenting, placing high responsibility on them without allowing for a support network of helpful friends or family. William Sears is fully aware of the arduousness of the methods. He suggests a whole package of measures that aim to prevent an emotional burnout of the mother, like the prioritization and delegation of duties and responsibilities, streamlining of daily routines, and collaboration between both parents. Sears advises mothers to turn to a psychotherapist if necessary, but to stick to attachment parenting at all costs.\n\nSears finds the burden of attachment parenting just and reasonable, and describes the opponents of this philosophy as \"autoritarian males […] caught up in their role of advice giver\". Granju, too, takes a swipe at \"the male dominated ′scientific′ childcare guidance\". She argues that the low reputation that breastfeeding, namely extended breastfeeding in the Western world has, arises from a sexualization of the female breast: from the perspective of a sexistic society, the breast \"belongs\" to men, not to children. Mayim Bialik, too, considers attachment a feminist option, since it constitutes an alternative to the – male dominated – superiority of physicians who traditionally shaped the spheres of pregnancy, childbirth, and motherhood.\n\nSince attachment parenting poses a considerable challenge to the reconcilability of motherhood and female career, the philosophy has been greatly criticized, most notably in the context of the attachment parenting controversy from 2012.\n\nSears states that in attachment families, parents and children practice a highly developed and sophisticated type of communication that makes it unnecessary for parents to use practices such as scolding; often, all it takes is a mere frown. He is convinced that children who trust their parents are cooperative and don't resist parental guidance. He therefore recommends positive discipline. But in contrast to many AP parents, he isn’t fundamentally opposed to confrontative methods (\"firm, corrective response\"), and he gives high significance to child obedience and conscience. Sears is a decided advocate for authoritative parenting.\n\nAs studies have shown, it is indeed possible to use discipline strategies that are sensitive and, therefore, one should not equate discipline and insensitive caregiving.\n\nLike Benjamin Spock before them, William and Martha Sears consider their parenting philosophy as a common sense and instinct-guided ad hoc way of parenting. In contrast to Spock who derived his ideas in a straight line from Freud’s psychoanalysis, the Searses in fact didn’t start out from a theory; even the tie to attachment theory was only engineered ex post, when the philosophy was already largely complete. Apart from Liedloff’s rather eclectic thoughts, they came to their ideas mainly from their own personal impressions:\n\nDespite the lack of a consistent theory, William and Martha Sears consider attachment parenting scientifically proven:\n\nTheir belief in such scientific proof doesn’t hinder the Searses to advise AP parents not to engage in discussions with AP critics. They also favor some science while they refuse other:\n\nCritics consider a lack of a consistent theoretical foundation – notably the lack of precise definitions of the fundamental terms – a shortcoming of the attachment parenting concept.\n\nThe concept of mutual emotional fine-tuning has been known in psychology since Franz Mesmer, who introduced it under the term \"rapport\", before Freud adopted it for psychoanalysis. In relation to the mother-child-tie, behaviorists and developmental psychologists rather speak of \"contingency\" today; Daniel Stern coined the term \"attunement\", too.\n\nFor Williams Sears, attachment parenting is a kind of parenting that is radically characterized by maternal responsivity. For that, he adopted Mary Ainsworth’s term of \"maternal sensitivity\": The woman directs her attention completely on the child (\"babyreading\") and responds continuously to every signal that the child sends; the result is a state of harmony between mother and child that leads to mutual attachment. Sears believes that the maternal \"tuning-in\" begins during pregnancy already.\n\nWithin the framework of infant cognitive development studies, the child's attachment to the parents has been well researched. As early as in the late 1940s, Donald Winnicott gave a detailed account of the development of the child's attachment; at the latest after the sixth month, healthy children begin to disengage from the mother-child symbiosis quite normally. However, it was Margaret Mahler who gave the most accurate description of the attachment development during the first three years. William Sears’ publications reveal no knowledge of this relevant literature.\n\nSears’ use of the term \"attachment\" is merely colloquial. He applies it synonymously with terms like trust, harmony, closeness, bonding, love bonds, and connection: \"Attachment describes the whole caregiving relationship between mother or father and baby.\" He mentions that attachment emerges from contingency, but in his further accounts, he never differentiates between attachment and contingency. The readers must therefore assume that attachment is a deeply vulnerable state that never stabilizes and that requires constant reestablishment through incessant sensitivity.\n\nLater in the book, in contradiction to his own preceding statements, Sears reassures adoptive parents: \"Don't worry about the attachment your child may have ’missed’ in foster care. Infants are extremely resilient.\"\n\nThe establishment of a secure mother-child attachment is the declared and pivotal goal of attachment parenting.\n\nIn numerous scientific studies, the normal development of attachment has been well documented. The same applies for deviant or pathological developments. Problematic or disturbed attachment has been described in three contexts: \n\n\nWilliam Sears uses the terms \"lesser quality of attachment\", \"insecure attachment\", and \"non-attachment\" synonymously. His formulations don't reveal which kind of problematic attachment is meant: reactive attachment disorder (ICD), desorganized attachment (Ainsworth) or the two forms of insecure attachment (Ainsworth). Still in 1982, he mentioned \"diseases of non-attachment\" not referring to the attachment theorists Bowlby and Ainsworth, but to Selma Fraiberg, a psychoanalyst who studied blindly born children in the 1970s. Due to the vague description of problematic attachment, Sears and AP organisations who use his criteria have been reproached to produce a high rate of \"false positives\". The same applies to definitions of attachment therapy, a concept that frequently appears to be partially overlapping with attachment parenting. Attachment parenting supporters have distanced themselves from attachment therapy, notably from its methods, but not from its diagnostic criteria.\n\nSears offers a discrimation between (good) attachment and (bad) enmeshment, but again without explaining to his readers how exactly they can identify the difference.\n\nThere is no conclusive body of research that shows Sears’ approach to be superior to \"mainstream parenting\". In field studies in Uganda, Ainsworth has observed that sometimes even children who spend plenty of time with their mothers and who were breastfed on cue, developed signs of insecure attachment; she concluded that it is not the quantity of mother-child interaction that determines the attachment type, but the quality. It is, therefore, not practices like co-sleeping, babywearing or feeding on cue that Ainsworth identifies as the crucial determinant for a secure attachment, but the \"maternal sensitivity\".\n\nThe theoretical starting point of attachment parenting – the idea of contingency – would suggest a concept of the infant as a creature who is essentially defined by his \"feelings\" and his \"communication\". William Sears, though, defines infants even more essentially by their \"needs\". Need is therefore another basic term; attachment parenting means quintessentially to attend to the child's needs.\n\nAs early as in the 1940s, psychologists such as Abraham Maslow shaped detailed models of the human needs; ever since, scientists have made a clear distinction between needs on the one hand and desires on the other hand. In 2000, T. Berry Brazelton, a pioneer in the field of newborn psychology, and child psychiatrist Stanley Greenspan published their book \"The Irreducible Needs of Children\", in which they re-assessed the term for pediatrics. When the Searses published their \"Attachment Parenting Book\" one year later, they responded neither to Maslow nor to Brazelton and Greenspan, but used the word \"need\" merely in a colloquial sense. Although they stressed that parents must distinguish between \"needs\" and \"desires\" of children, in particular of older children, they denied their readers a guideline of how to tell needs and desires apart. With a view to infants, they believe that needs and desires are plainly identical. In general, they use both terms synonymously. With a view to toddlers, they often phrase it: a child is \"not ready yet\" (to do without breastfeeding, without co-sleeping, etc.); but even in contexts like these, they speak of \"needs\", too.\n\nOpponents of attachment parenting have questioned that the behavior of a 3½ year old who still demands to nurse can actually be classified as a \"need\". Most likely the child is seeking consolation. To give a child comfort is an important parental responsibility; but parents are just as well liable to teach their child to take heart by his own power.\n\nStress has been surveyed and documented in many studies. The theoretical foundation was created in the 1960s by Richard Lazarus. In 1974, Hans Selye introduced the differentiation between distress and eustress, and in 1984, psychoanalyst Heinz Kohut proposed the concept of \"optimal frustration\"; Kohut postulated that the harmony between parents and child needs some well allotted disruption in order to empower the child to develop a healthy personality. In resilience psychology, too, there is broad agreement today that it harms children if their parents keep any stress away from them indiscriminately; by doing so, they suggest to the child that everyday problems are painful and overall to be avoided.\n\nEven though stress is one of the fundamental terms of attachment parenting, William Sears’ publications don't reveal acquaintance with pertinent literature about this topic. Sears links stress and distress with the release of cortisol, but uses both terms synonymously and in a purely colloquial sense. He refers the term to any uncomfortable or frustrating state which makes the child cry – a signal which AP mothers are supposed to carefully attend to since stress sickens the child. On the other hand, Sears advises mothers not to overreact and to teach the child imperturbation (\"Caribbean approach\"). He leaves it up to the parents to decide which type of response individual situations ask for.\n\nFor parenting, any fuzziness of the term \"stress\" as well as of the term \"need\" have far-reaching consequences. If it is assumed that any crying of the child indicates harmful stress and that any of his demands indicate a true need, parents are bound to confuse rapport, sensitivity, responsivity, emotional availability, and wise protection with behaviors that, from an educational standpoint, are highly dysfunctional and that William Sears mostly wouldn't agree with himself:\n\nInstinct is another basic term of attachment parenting. The Searses describe attachment parenting as the natural, biological, intuitive and spontaneous behavior of mothers who rely on their instincts, sixth sense, inner wisdom or common sense. They attribute even motherliness itself to instincts, whereas they attest men a reduced instinct for children's needs.\n\nInstinct theory developed in the 1930s within the framework of ethology. It owes its basic ideas to William McDougall among others, and its elaboration mainly to Konrad Lorenz and Nikolaas Tinbergen. Lorenz believed that instincts are physiological processes, and assumed they could be described as neuronal circuitry in the brain. But already Arnold Gehlen had disputed that humans still have much instinct at their disposal; for him, plasticity and learning aptitude outranked instinct. In today's research, the term instinct is regarded as absolete. Recent studies have demonstrated that motherly behavior is not inbred but biologically and socially determined. It is partly triggered by oxytocin, partly learned.\n\nWilliam Sears' writings show no knowledge of this current state of research. The Searses use the word instinct in a purely colloquial sense and synonymous with terms like \"hormonal\" and \"natural\"; as an antipole of instinct and nature, they identify the things that \"childcare advisors\" say.\n\nWilliam Sears, who owes his formative impressions to Jean Liefloff, points to mammals, primates, \"other\", \"primitive\", and \"traditional cultures\", namely on Bali and in Zambia. Developmental psychologist Heidi Keller who comparatively researched the mother-child relationship in a large bandwidth of cultures, disputes that attachment parenting can be described as a return to a \"natural motherliness\", like many supporters advertise it. Keller doesn't rank attachment parenting as a counteragent to the high-tech world, but asserts that it \"paradoxically fits optimally into a society of individualists and lone warriors how we experience it in the Western world\". Many of the methods that the representatives of attachment parenting attribute to the evolutionary history of life don't actually play the major role in non-western cultures that is attributed to them. In Cameroon for example, children are actually carried in a sling initially, but then have to learn to sit and to walk much earlier than European and North American children; rather than to cultivate affectionate eye contact, mothers blow into their children's face in order to get them out of the habit of making eye contact.\nEven in the United States, there are minority groups which can be classified as highly \"traditional\", none of them practicing attachment parenting. Amish mothers for example co-sleep with their infants, but only for the first several months; they never let their infants and toddlers out of view, but they don't wear them while they are working. From very early on, Amish children are raised to serve God, family, and community rather than to express their own needs. The infants of orthodox Jews traditionally sleep in cradles. In communities where there is no \"eruv\", Jewish parents are not allowed to carry their children about on Shabbat. Native Americans traditionally used cradleboards which could be worn, but which involved minimal physical touch of mother and child.\n\nAs Suzanne M. Cox (Northwestern University) has pointed out, neither attachment theory nor attachment parenting offer a general outline of the optimal development of the child, which could be used to empirically measure the efficacy of attachment parenting. The Searses promise parenting results such as increased independence, confidence, health, physical growth, improved development of the motor and language skills, good manners, conscientiousness, social competence, sense of justice, altruism, sensitivity, empathy, concentration, self-control, and intelligence. However, there is no conclusive evidence from empirical research that supports such claims.\n\nThe ultimate target of child rearing is, according to Sears, happiness. Similar to the German catholic Albert Wunsch, Sears therefore ranks among those parenting advisors whose philosophies reflect stray aspects of their religious beliefs, but result in a purely worldly target. In the year of the publication of the \"Attachment Parenting Book\", Wendy Mogel, by contrast, suggested her own very influential concept of character education that was straightforwardly based on her Jewish faith (\"The Blessings of a Skinned Knee\", 2001).\n\nAttachment parenting is particularly popular among educated urban women in Western countries, who are interested in ecological and social issues.\n\nIn the United States, parenting tips of well-known people like the actresses Mayim Bialik and Alicia Silverstone contributed to the popularity of the philosophy. Many North American Women are organized in support groups of \"Attachment Parenting International\" (API), the movement's umbrella organization, in which Martha Sears serves as a board member. In Canada, there are further AP organizations such as the \"Attachment Parenting Canada Association\" (Calgary); even some public health organizations promote attachment parenting. William Sears has close ties to the international La Leche League (LLL) which feature him as a conference speaker and published several of his books. In LLL groups, many mothers get in touch with attachment parenting for the first time. There are also attachment parenting organizations in Australia and in New Zealand.\n\nIn Europe, \"Attachment Parenting Europe\" (APEU, in Lelystad, Netherlands) campaigns for attachment parenting; in the Dutch language the philosophy is referred to as \"natuurlijk ouderschap\" (\"natural parenthood\"). This organization keeps liaisons to representatives in Belgium, Denmark, Germany, Ireland, Italy, Norway, the United Kingdom, and Switzerland. In 2012, there were 30 AP groups in England and Wales.\n\nIn Germany, there are independent AP institutions in several cities. Hamburg, the movement's central point in Germany, hosted a first \"Attachment Parenting Congress\" in 2014, under the patronage of Federal Minister of Family Affairs, Manuela Schwesig. A second one has been announced for 2016.\n\nIn Austria and Switzerland there exist a small number of AP institutions, too. In Sweden, fantasy and science fiction writer Jorun Modén solicits attachment parenting, which she refers to as \"nära föräldraskap\" (\"proximal parenthood\"). In France where the philosophy is dubbed as \"maternage intensif\" or \"maternage proximal\", the movement has virtually no followers; due to the success of the Napoleonic education reforms, the French traditionally have a deeply rooted belief that educated child care specialists educate children at least as well as mothers do.\n\nSince 2012, there has been a controversy about Sears' positions which has been mostly carried out in the English-speaking world.\n\nIt began in 2012 with a cover picture on the \"Time\" magazine that showed a Californian mother breastfeeding her almost 4-year-old. In the accompanying article \"The Man Who Remade Motherhood\", journalist Kate Pickert argued that even if William Sears' positions are much less radical than those of his followers, they are misogynic and give mothers a chronically guilty conscience, and that they frequently disagree with relevant research results. The cover picture and article became the starting point of agitated disputes in many media.\n\nAt the same time, attachment parenting attracted attention of sociologists like Ellie Lee, Charlotte Faircloth, Jan Macvarish, and Frank Furedi who described the phenomenon an example of 21st century \"Parental Determinism\". As early as in 1996, sociologist Sharon Hays had described the sociocultural phenomenon of an \"Intensive Mothering\"; with attachment parenting, this phenomenon finally became tangible and recognizable. In 2004, media critic Susan J. Douglas and philosopher Meredith W. Michaels followed with their account of a \"New Momism\".\n\n\"Time\" cover picture and article were published May 21, 2012. Pickert described how parents who follow Sears tend to take opinions that are much more radical than Sears himself. Nevertheless, many parents catch from Sears' books an outlook that Pickert jestingly describes as a \"post-traumatic Sears disorder\": a severe sense of insufficiency that seems to appear in particular in such mothers who \"want\" to follow Sears' advice, for the sake of their children's mental health, but \"cannot\", e.g. because they can't afford to be stay-at-home-moms.\n\nKatha Pollitt referred to attachment parenting as a fad. Parents who follow the philosophy have been reproached as acting according to their own helplessness and unsatisfied emotional neediness which may be the true reasons for their decision to incessantly pacify their child by breastfeeding and babywearing even into toddlerhood, as the belief that the child actually needs all that permanent intimacy for his healthy development is only a subterfuge. Emma Jenner argued that parents who are in the habit of stereotypically attending to each of the child's signals with physical proximity will not learn to perceive the child's needs in the full extend of their bandwidth and complexity.\n\nKatie Allison Granju, who advocates attachment parenting and who published comprehensive guidelines for AP parents, offers a different perspective. She characterizes attachment parenting as not just a parenting style, but \"a completely fulfilling way of life\".\n\nSociologist Jan Macvarish (University of Kent), a pioneer in the recent field of parenting culture study, described how AP parents utilize their parenting philosophy as a strategy of individualization, as a way to find personal identity and to join a group of congenial adults. Macvarish even speaks of \"parental tribalism\". According to Macvarish, it is characteristic for such choices that they are much more angled towards the parents' self-perception than towards the child's needs. Sociologist Charlotte Faircloth, too, considers attachment parenting a strategy that women pursue in order to gain and to express personal identity.\n\nMultiple authors have stated that many parents choose attachment parenting as part of an individualization strategy and as a statement of personal identity and of social affiliation. This assumption is supported by the observation that most AP parents show further distinctive parenting and life style preferences that are based on a particular set of attitudes (notably: a striving for naturalness), which, however, are mostly not directly tied to the declared goal of attachment parenting:\n\nSome practices and preferences of AP parents are prevalent only in North America:\n\nThe Sears encourage some of these practices explicitly, for example non-smoking, healthy and home-prepared food, no circumcision, but don't comment on how they are supposed to be linked to the core ideas of attachment parenting. Only in the case of positive discipline, the link is quite obvious.\n\nIn his \"Complete Book of Christian Parenting and Child Care\" (1997), William Sears opposes maternal occupation, because he is convinced that it harms the child:\n\nAny form of intensive, obsessive mothering has, as Katha Pollitt stated, devastating consequences for the equality of treatment of women in the society. In France, Élisabeth Badinter argued that over-parenting, obsession with washable diapers and organic, home made infant food, and parenting practices as the ones recommended by Sears, with breastfeeding into toddlerhood, bring women inevitably back into outdated patterns of gender role. In the United States, Badinter's book \"The Conflict: How Modern Motherhood Undermines the Status of Women\" (2010) had a partially critical reception, because there is no publicly paid childcare leave in this country, and many women consider it a luxury to be able to be a stay-at-home-mom during the child's first years. Still, gynecologist Amy Tuteur (formerly Harvard Medical School) stated that attachment parenting amounts to a new subjection of the woman's body under social control – a trend that is more than questionable in the face to the hard-fought achievements of women's movement.\n\nAs Erica Jong observed, the rise of attachment parenting followed a surge of glamourized motherhood of popular stars (Angelina Jolie, Madonna, Gisele Bündchen) in the mass media. She stated that the effort to model exceptional children under sacrifice of the parent's own well-being transformed motherhood into a \"highly competitive race\"; all attempts of women to radically monopolize their parental responsibilities very much accommodate right-wing politics.\n\nIn her 2005 book \"Perfect Madness. Motherhood in the Age of Anxiety\", Judith Warner, too, described how attachment parenting has taken a strong influence on mainstream parenting and how it has established a \"culture of total motherhood\"; due to these cultural changes, mothers are convinced today that they have to instantly attend to every need of their children in order to protect them from the risk of livelong abandonment issues. As early as in 1996, sociologist Sharon Hays wrote about a newly formed \"ideology of intensive mothering\". Characteristic of this ideology is the tendency to impose parenting responsibility primarily on \"mothers\" and to favor a kind of parenting that is child-centered, expert-guided, emotionally absorbing, labour- and financially intensive. Hays saw the motives for the overloading of motherhood in the idealistic endeavor to cure an overly egoistical and competitive society through a counterbalancing principle of altruistic motherliness. But according to Hays, any kind of \"intensive motherhood\" that systematically privileges children's needs over mothers' needs happens without fail to the economical and personal disadvantage of mothers.\n\nIn 2014, a team of researchers at the University of Mary Washington showed in a study that 23% of mothers who believe in \"intensive motherhood\" suffer from signs of depression; among average mothers, this applies only to 6.7%.\n\n\n"}
{"id": "17405625", "url": "https://en.wikipedia.org/wiki?curid=17405625", "title": "Bed management", "text": "Bed management\n\nBed management is the allocation and provision of beds, especially in a hospital where beds in specialist wards are a scarce resource. The \"bed\" in this context represents not simply a place for the patient to sleep, but the services that go with being cared for by the medical facility: admission processing, physician time, nursing care, necessary diagnostic work, appropriate treatment, and so forth.\n\nIn the UK, acute hospital bed management is usually performed by a dedicated team and may form part of a larger process of patient flow management.\n\nBecause hospital beds are economically scarce resources, there is naturally pressure to ensure high occupancy rates and therefore a minimal buffer of empty beds. However, because the volume of emergency admissions is unpredictable, hospitals with average occupancy levels above 85 per cent \"can expect to have regular bed shortages and periodic bed crises.\" In the first quarter of 2017 average overnight occupancy in English hospitals was 91.4%.\n\nShortage of beds can result in cancellations of admissions for planned (elective) surgery, admission to inappropriate wards (medical vs. surgical, male vs. female etc.), delay admitting emergency patients, and transfers of existing inpatients between wards, which \"will add a day to a patient’s length of stay\".\n\nThese can be politically sensitive issues in publicly funded healthcare systems. In the UK there has been concern over inaccurate and sometimes fraudulently manipulated waiting list statistics, and claims that \"the current A&E target is simply not achievable without the employment of dubious management tactics.\" In 2013 two Stafford Hospital nurses were struck off the nursing register for falsifying A&E discharge times between 2000 and 2010 to avoid breaches of four-hour waiting targets.\n\nIn 2018 NHS England started a new initiative to reduce the number of what it now called \"stranded\" or \"super stranded\" patients, super stranded being people in hospital for more 20 days. About 18,000 of the 101,259 acute and general beds in English NHS hospitals were occupied by “super stranded” patients in May 2018. By August 2018 2,338 of these beds had been freed. Some of the delays were related to social care, but more related to the management of inpatient stays.\n\n\n"}
{"id": "12496657", "url": "https://en.wikipedia.org/wiki?curid=12496657", "title": "Belize Red Cross Society", "text": "Belize Red Cross Society\n\nBelize Red Cross Society was founded in 1950 by Dr. Daniel Tenenbaum of Belmopan City Hospital. Previously it was part of the British Red Cross. It has its headquarters in Belize City.\n\n\n"}
{"id": "47551794", "url": "https://en.wikipedia.org/wiki?curid=47551794", "title": "Breast crawl", "text": "Breast crawl\n\nBreast crawl is the instinct of mammal (including human) newborns to move towards the nipple and attach to it for breastfeeding all by themselves. In humans, if the newborn baby is put on the mother's abdomen, the movements start 12 to 44 minutes after birth, followed by spontaneous suckling at 27 to 71 minutes after birth. \n\nThe infants use their sense of smell in finding the nipple. The areola smells similar to amniotic fluid, the baby recognizes this smell on its hands and begins to move towards the breast. As shown in a 1994 study in which one of the breasts was washed with unscented soap and the baby preferred the other one. They also use visual stimuli (such as the mother's face and the breast's areola) and auditory (the mother's voice).\n"}
{"id": "33630994", "url": "https://en.wikipedia.org/wiki?curid=33630994", "title": "British Society for the Study of Sex Psychology", "text": "British Society for the Study of Sex Psychology\n\nThe British Society for the Study of Sex Psychology (BSSSP) was founded in 1913, \"to advance a particularly radical agenda in the field of sex reform, based on the writings of gurus such as [Edward] Carpenter and [Havelock] Ellis.\" In 1931 the Society was renamed the British Sexological Society.\nIt seems to have continued until some point in the 1940s.\n\nThe society was particularly concerned with homosexuality, aiming to combat legal discrimination against homosexuality with scientific understanding. Members included George Cecil Ives, Edward Carpenter, Montague Summers, Stella Browne, Laurence Housman, Havelock Ellis, Bernard Shaw, and Ernest Jones.\n\nThe Society published a series of pamphlets:\n\n"}
{"id": "5932", "url": "https://en.wikipedia.org/wiki?curid=5932", "title": "Carbohydrate", "text": "Carbohydrate\n\nA carbohydrate () is a biomolecule consisting of carbon (C), hydrogen (H) and oxygen (O) atoms, usually with a hydrogen–oxygen atom ratio of 2:1 (as in water) and thus with the empirical formula (where \"m\" may be different from \"n\"). This formula holds true for monosaccharides. Some exceptions exist; for example, deoxyribose, a sugar component of DNA, has the empirical formula CHO. The carbohydrates are technically hydrates of carbon; structurally it is more accurate to view them as aldoses and ketoses.\n\nThe term is most common in biochemistry, where it is a synonym of 'saccharide', a group that includes sugars, starch, and cellulose. The saccharides are divided into four chemical groups: monosaccharides, disaccharides, oligosaccharides, and polysaccharides. Monosaccharides and disaccharides, the smallest (lower molecular weight) carbohydrates, are commonly referred to as sugars. The word \"saccharide\" comes from the Greek word \"σάκχαρον\" (\"sákkharon\"), meaning \"sugar\". While the scientific nomenclature of carbohydrates is complex, the names of the monosaccharides and disaccharides very often end in the suffix -ose, as in the monosaccharides fructose (fruit sugar) and glucose (starch sugar) and the disaccharides sucrose (cane or beet sugar) and lactose (milk sugar).\n\nCarbohydrates perform numerous roles in living organisms. Polysaccharides serve for the storage of energy (e.g. starch and glycogen) and as structural components (e.g. cellulose in plants and chitin in arthropods). The 5-carbon monosaccharide ribose is an important component of coenzymes (e.g. ATP, FAD and NAD) and the backbone of the genetic molecule known as RNA. The related deoxyribose is a component of DNA. Saccharides and their derivatives include many other important biomolecules that play key roles in the immune system, fertilization, preventing pathogenesis, blood clotting, and development.\n\nStarch and sugars are the most important carbohydrates in human diet. They are found in a wide variety of natural and processed foods. Starch is a polysaccharide. It is abundant in cereals (wheat, maize, rice), potatoes, and processed food based on cereal flour, such as bread, pizza or pasta. Sugars appear in human diet mainly as table sugar (sucrose, extracted from sugarcane or sugar beets), lactose (abundant in milk), glucose and fructose, both of which occur naturally in honey, many fruits, and some vegetables. Table sugar, milk, or honey are often added to drinks and many prepared foods such as jam, biscuits and cakes.\n\nCellulose, a polysaccharide found in the cell walls of all plants, is one of the main components of insoluble dietary fiber. Although it is not digestible, insoluble dietary fiber helps to maintain a healthy digestive system by easing defecation. Other polysaccharides contained in dietary fiber include resistant starch and inulin, which feed some bacteria in the microbiota of the large intestine, and are metabolized by these bacteria to yield short-chain fatty acids.\n\nIn scientific literature, the term \"carbohydrate\" has many synonyms, like \"sugar\" (in the broad sense), \"saccharide\", \"ose\", \"glucide\", \"hydrate of carbon\" or \"polyhydroxy compounds with aldehyde or ketone\". Some of these terms, specially \"carbohydrate\" and \"sugar\", are also used with other meanings.\n\nIn food science and in many informal contexts, the term \"carbohydrate\" often means any food that is particularly rich in the complex carbohydrate starch (such as cereals, bread and pasta) or simple carbohydrates, such as sugar (found in candy, jams, and desserts).\n\nOften in lists of nutritional information, such as the USDA National Nutrient Database, the term \"carbohydrate\" (or \"carbohydrate by difference\") is used for everything other than water, protein, fat, ash, and ethanol. This includes chemical compounds such as acetic or lactic acid, which are not normally considered carbohydrates. It also includes dietary fiber which is a carbohydrate but which does not contribute much in the way of food energy (kilocalories), even though it is often included in the calculation of total food energy just as though it were a sugar.\n\nIn the strict sense, \"sugar\" is applied for sweet, soluble carbohydrates, many of which are used in food.\n\nFormerly the name \"carbohydrate\" was used in chemistry for any compound with the formula C (HO). Following this definition, some chemists considered formaldehyde (CHO) to be the simplest carbohydrate, while others claimed that title for glycolaldehyde. Today, the term is generally understood in the biochemistry sense, which excludes compounds with only one or two carbons and includes many biological carbohydrates which deviate from this formula. For example, while the above representative formulas would seem to capture the commonly known carbohydrates, ubiquitous and abundant carbohydrates often deviate from this. For example, carbohydrates often display chemical groups such as: \"N\"-acetyl (e.g. chitin), sulphate (e.g. glycosaminoglycans), carboxylic acid (e.g. sialic acid) and deoxy modifications (e.g. fucose and sialic acid).\n\nNatural saccharides are generally built of simple carbohydrates called monosaccharides with general formula (CHO) where \"n\" is three or more. A typical monosaccharide has the structure H–(CHOH)(C=O)–(CHOH)–H, that is, an aldehyde or ketone with many hydroxyl groups added, usually one on each carbon atom that is not part of the aldehyde or ketone functional group. Examples of monosaccharides are glucose, fructose, and glyceraldehydes. However, some biological substances commonly called \"monosaccharides\" do not conform to this formula (e.g. uronic acids and deoxy-sugars such as fucose) and there are many chemicals that do conform to this formula but are not considered to be monosaccharides (e.g. formaldehyde CHO and inositol (CHO)).\n\nThe open-chain form of a monosaccharide often coexists with a closed ring form where the aldehyde/ketone carbonyl group carbon (C=O) and hydroxyl group (–OH) react forming a hemiacetal with a new C–O–C bridge.\n\nMonosaccharides can be linked together into what are called polysaccharides (or oligosaccharides) in a large variety of ways. Many carbohydrates contain one or more modified monosaccharide units that have had one or more groups replaced or removed. For example, deoxyribose, a component of DNA, is a modified version of ribose; chitin is composed of repeating units of N-acetyl glucosamine, a nitrogen-containing form of glucose.\n\nCarbohydrates are polyhydroxy aldehydes, ketones, alcohols, acids, their simple derivatives and their polymers having linkages of the acetal type. They may be classified according to their degree of polymerization, and may be divided initially into three principal groups, namely sugars, oligosaccharides and polysaccharides\n\nDP * = Degree of polymerization\n\nMonosaccharides are the simplest carbohydrates in that they cannot be hydrolyzed to smaller carbohydrates. They are aldehydes or ketones with two or more hydroxyl groups. The general chemical formula of an unmodified monosaccharide is (C•HO), literally a \"carbon hydrate\". Monosaccharides are important fuel molecules as well as building blocks for nucleic acids. The smallest monosaccharides, for which n=3, are dihydroxyacetone and D- and L-glyceraldehydes.\n\nMonosaccharides are classified according to three different characteristics: the placement of its carbonyl group, the number of carbon atoms it contains, and its chiral handedness. If the carbonyl group is an aldehyde, the monosaccharide is an aldose; if the carbonyl group is a ketone, the monosaccharide is a ketose. Monosaccharides with three carbon atoms are called trioses, those with four are called tetroses, five are called pentoses, six are hexoses, and so on. These two systems of classification are often combined. For example, glucose is an aldohexose (a six-carbon aldehyde), ribose is an aldopentose (a five-carbon aldehyde), and fructose is a ketohexose (a six-carbon ketone).\n\nEach carbon atom bearing a hydroxyl group (-OH), with the exception of the first and last carbons, are asymmetric, making them stereo centers with two possible configurations each (R or S). Because of this asymmetry, a number of isomers may exist for any given monosaccharide formula. Using Le Bel-van't Hoff rule, the aldohexose D-glucose, for example, has the formula (C·HO), of which four of its six carbons atoms are stereogenic, making D-glucose one of 2=16 possible stereoisomers. In the case of glyceraldehydes, an aldotriose, there is one pair of possible stereoisomers, which are enantiomers and epimers. 1, 3-dihydroxyacetone, the ketose corresponding to the aldose glyceraldehydes, is a symmetric molecule with no stereo centers. The assignment of D or L is made according to the orientation of the asymmetric carbon furthest from the carbonyl group: in a standard Fischer projection if the hydroxyl group is on the right the molecule is a D sugar, otherwise it is an L sugar. The \"D-\" and \"L-\" prefixes should not be confused with \"d-\" or \"l-\", which indicate the direction that the sugar rotates plane polarized light. This usage of \"d-\" and \"l-\" is no longer followed in carbohydrate chemistry.\n\nThe aldehyde or ketone group of a straight-chain monosaccharide will react reversibly with a hydroxyl group on a different carbon atom to form a hemiacetal or hemiketal, forming a heterocyclic ring with an oxygen bridge between two carbon atoms. Rings with five and six atoms are called furanose and pyranose forms, respectively, and exist in equilibrium with the straight-chain form.\n\nDuring the conversion from straight-chain form to the cyclic form, the carbon atom containing the carbonyl oxygen, called the anomeric carbon, becomes a stereogenic center with two possible configurations: The oxygen atom may take a position either above or below the plane of the ring. The resulting possible pair of stereoisomers is called anomers. In the \"α anomer\", the -OH substituent on the anomeric carbon rests on the opposite side (trans) of the ring from the CHOH side branch. The alternative form, in which the CHOH substituent and the anomeric hydroxyl are on the same side (cis) of the plane of the ring, is called the \"β anomer\".\n\nMonosaccharides are the major source of fuel for metabolism, being used both as an energy source (glucose being the most important in nature) and in biosynthesis. When monosaccharides are not immediately needed by many cells they are often converted to more space-efficient forms, often polysaccharides. In many animals, including humans, this storage form is glycogen, especially in liver and muscle cells. In plants, starch is used for the same purpose. The most abundant carbohydrate, cellulose, is a structural component of the cell wall of plants and many forms of algae. Ribose is a component of RNA. Deoxyribose is a component of DNA. Lyxose is a component of lyxoflavin found in the human heart. Ribulose and xylulose occur in the pentose phosphate pathway. Galactose, a component of milk sugar lactose, is found in galactolipids in plant cell membranes and in glycoproteins in many tissues. Mannose occurs in human metabolism, especially in the glycosylation of certain proteins. Fructose, or fruit sugar, is found in many plants and in humans, it is metabolized in the liver, absorbed directly into the intestines during digestion, and found in semen. Trehalose, a major sugar of insects, is rapidly hydrolyzed into two glucose molecules to support continuous flight.\n\nTwo joined monosaccharides are called a disaccharide and these are the simplest polysaccharides. Examples include sucrose and lactose. They are composed of two monosaccharide units bound together by a covalent bond known as a glycosidic linkage formed via a dehydration reaction, resulting in the loss of a hydrogen atom from one monosaccharide and a hydroxyl group from the other. The formula of unmodified disaccharides is CHO. Although there are numerous kinds of disaccharides, a handful of disaccharides are particularly notable.\n\nSucrose, pictured to the right, is the most abundant disaccharide, and the main form in which carbohydrates are transported in plants. It is composed of one D-glucose molecule and one D-fructose molecule. The systematic name for sucrose, \"O\"-α-D-glucopyranosyl-(1→2)-D-fructofuranoside, indicates four things:\n\nLactose, a disaccharide composed of one D-galactose molecule and one D-glucose molecule, occurs naturally in mammalian milk. The systematic name for lactose is \"O\"-β-D-galactopyranosyl-(1→4)-D-glucopyranose. Other notable disaccharides include maltose (two D-glucoses linked α-1,4) and cellulobiose (two D-glucoses linked β-1,4). Disaccharides can be classified into two types: reducing and non-reducing disaccharides. If the functional group is present in bonding with another sugar unit, it is called a reducing disaccharide or biose.\n\nCarbohydrate consumed in food yields 3.87 kilocalories of energy per gram for simple sugars, and 3.57 to 4.12 kilocalories per gram for complex carbohydrate in most other foods. Relatively high levels of carbohydrate are associated with processed foods or refined foods made from plants, including sweets, cookies and candy, table sugar, honey, soft drinks, breads and crackers, jams and fruit products, pastas and breakfast cereals. Lower amounts of carbohydrate are usually associated with unrefined foods, including beans, tubers, rice, and unrefined fruit. Animal-based foods generally have the lowest carbohydrate levels, although milk does contain a high proportion of lactose.\n\nOrganisms typically cannot metabolize all types of carbohydrate to yield energy. Glucose is a nearly universal and accessible source of energy. Many organisms also have the ability to metabolize other monosaccharides and disaccharides but glucose is often metabolized first. In \"Escherichia coli\", for example, the lac operon will express enzymes for the digestion of lactose when it is present, but if both lactose and glucose are present the \"lac\" operon is repressed, resulting in the glucose being used first (see: Diauxie). Polysaccharides are also common sources of energy. Many organisms can easily break down starches into glucose; most organisms, however, cannot metabolize cellulose or other polysaccharides like chitin and arabinoxylans. These carbohydrate types can be metabolized by some bacteria and protists. Ruminants and termites, for example, use microorganisms to process cellulose. Even though these complex carbohydrates are not very digestible, they represent an important dietary element for humans, called dietary fiber. Fiber enhances digestion, among other benefits.\n\nBased on the effects on risk of heart disease and obesity in otherwise healthy middle-aged adults, the Institute of Medicine recommends that American and Canadian adults get between 45–65% of dietary energy from whole-grain carbohydrates. The Food and Agriculture Organization and World Health Organization jointly recommend that national dietary guidelines set a goal of 55–75% of total energy from carbohydrates, but only 10% directly from sugars (their term for simple carbohydrates). A 2017 Cochrane Systematic Review concluded that there was insufficient evidence to support the claim that whole grain diets can affect cardiovascular disease.\n\nNutritionists often refer to carbohydrates as either simple or complex. However, the exact distinction between these groups can be ambiguous. The term \"complex carbohydrate\" was first used in the U.S. Senate Select Committee on Nutrition and Human Needs publication \"Dietary Goals for the United States\" (1977) where it was intended to distinguish sugars from other carbohydrates (which were perceived to be nutritionally superior). However, the report put \"fruit, vegetables and whole-grains\" in the complex carbohydrate column, despite the fact that these may contain sugars as well as polysaccharides. This confusion persists as today some nutritionists use the term complex carbohydrate to refer to any sort of digestible saccharide present in a whole food, where fiber, vitamins and minerals are also found (as opposed to processed carbohydrates, which provide energy but few other nutrients). The standard usage, however, is to classify carbohydrates chemically: simple if they are sugars (monosaccharides and disaccharides) and complex if they are polysaccharides (or oligosaccharides).\n\nIn any case, the simple vs. complex chemical distinction has little value for determining the nutritional quality of carbohydrates. Some simple carbohydrates (e.g. fructose) raise blood glucose slowly, while some complex carbohydrates (starches), especially if processed, raise blood sugar rapidly. The speed of digestion is determined by a variety of factors including which other nutrients are consumed with the carbohydrate, how the food is prepared, individual differences in metabolism, and the chemistry of the carbohydrate.\n\nThe USDA's \"Dietary Guidelines for Americans 2010\" call for moderate- to high-carbohydrate consumption from a balanced diet that includes six one-ounce servings of grain foods each day, at least half from whole grain sources and the rest from enriched.\n\nThe glycemic index (GI) and glycemic load concepts have been developed to characterize food behavior during human digestion. They rank carbohydrate-rich foods based on the rapidity and magnitude of their effect on blood glucose levels. Glycemic index is a measure of how quickly food glucose is absorbed, while glycemic load is a measure of the total absorbable glucose in foods. The insulin index is a similar, more recent classification method that ranks foods based on their effects on blood insulin levels, which are caused by glucose (or starch) and some amino acids in food.\n\nCarbohydrates are a common source of energy in living organisms; however, no single carbohydrate is an essential nutrient in humans. Humans are able to obtain all of their energy requirement from protein and fats, though the potential for some negative health effects of extreme carbohydrate restriction remains, as the issue has not been studied extensively yet. However, in the case of dietary fiber – indigestible carbohydrates which are not a source of energy – inadequate intake can lead to significant increases in mortality.\n\nFollowing a diet consisting of very low amounts of daily carbohydrate for several days will usually result in higher levels of blood ketone bodies than an isocaloric diet with similar protein content. This relatively high level of ketone bodies is commonly known as ketosis and is very often confused with the potentially fatal condition often seen in type 1 diabetics known as diabetic ketoacidosis. Somebody suffering ketoacidosis will have much higher levels of blood ketone bodies along with high blood sugar, dehydration and electrolyte imbalance.\n\nLong-chain fatty acids cannot cross the blood–brain barrier, but the liver can break these down to produce ketones. However, the medium-chain fatty acids octanoic and heptanoic acids can cross the barrier and be used by the brain, which normally relies upon glucose for its energy.\nGluconeogenesis allows humans to synthesize some glucose from specific amino acids: from the glycerol backbone in triglycerides and in some cases from fatty acids.\n\nCarbohydrate metabolism denotes the various biochemical processes responsible for the formation, breakdown and interconversion of carbohydrates in living organisms.\n\nThe most important carbohydrate is glucose, a simple sugar (monosaccharide) that is metabolized by nearly all known organisms. Glucose and other carbohydrates are part of a wide variety of metabolic pathways across species: plants synthesize carbohydrates from carbon dioxide and water by photosynthesis storing the absorbed energy internally, often in the form of starch or lipids. Plant components are consumed by animals and fungi, and used as fuel for cellular respiration. Oxidation of one gram of carbohydrate yields approximately 9 kJ (4 kcal) of energy, while the oxidation of one gram of lipids yields about 38 kJ (9 kcal). The human body stores between 300 to 500 g of carbohydrates depending on body weight, with the skeletal muscle contributing to a large portion of the storage. Energy obtained from metabolism (e.g., oxidation of glucose) is usually stored temporarily within cells in the form of ATP. Organisms capable of anaerobic and aerobic respiration that metabolizes glucose and oxygen (aerobic) to release energy with carbon dioxide and water as byproducts.\n\nCatabolism is the metabolic reaction which cells undergo to break down larger molecules, extracting energy. There are two major metabolic pathways of monosaccharide catabolism: glycolysis and the citric acid cycle.\n\nIn glycolysis, oligo- and polysaccharides are cleaved first to smaller monosaccharides by enzymes called glycoside hydrolases. The monosaccharide units can then enter into monosaccharide catabolism. A 2 ATP investment is required in the early steps of glycolysis to phosphorylate Glucose to Glucose 6-Phosphate (G6P) and Fructose 6-Phosphate (F6P) to Fructose 1,6-biphosphate (FBP), thereby pushing the reaction forward irreversibly. In some cases, as with humans, not all carbohydrate types are usable as the digestive and metabolic enzymes necessary are not present.\n\nCarbohydrate chemistry is a large and economically important branch of organic chemistry. Some of the main organic reactions that involve carbohydrates are:\n\n"}
{"id": "37762190", "url": "https://en.wikipedia.org/wiki?curid=37762190", "title": "Community Health Partnerships", "text": "Community Health Partnerships\n\nCommunity Health Partnerships (formerly Partnerships for Health) is a Department of Health (DoH) owned company in the United Kingdom. Its role is to set up public-private partnerships to invest in new healthcare facilities in England via the NHS Local Improvement Finance Trust (LIFT) programme.\n\nThe company was established in 2001 as Partnerships for Health (PfH), a joint venture between the DoH and Partnerships UK. In December 2006, the DoH acquired the full shareholding of PfH and it was renamed Community Health Partnerships in November 2007.\n\nIts current portfolio is over 300 buildings which directly benefit the public by offering modern,well designed health care facilities. These range from NHS Foundation Trusts, GPs, Dentists, Pharmacies amongst others. \n\n"}
{"id": "2698482", "url": "https://en.wikipedia.org/wiki?curid=2698482", "title": "Continuity of Care Record", "text": "Continuity of Care Record\n\nContinuity of Care Record (CCR) is a health record standard specification developed jointly by ASTM International, the Massachusetts Medical Society (MMS), the Healthcare Information and Management Systems Society (HIMSS), the American Academy of Family Physicians (AAFP), the American Academy of Pediatrics (AAP), and other health informatics vendors.\n\nThe CCR was generated by health care practitioners based on their views of the data they may want to share in any given situation. The CCR document is used to allow timely and focused transmission of information to other health professionals involved in the patient's care. The CCR aims to increase the role of the patient in managing their health and reduce error while improving continuity of patient care. The CCR standard is a patient health summary standard. It is a way to create flexible documents that contain the most relevant and timely core health information about a patient, and to send these electronically from one caregiver to another. The CCR's intent is also to create a standard of health information transportability when a patient is transferred or referred, or is seen by another healthcare professional.\n\nThe CCR is a unique development effort via a syndicate of the following sponsors:\n\n\nThe CCR data set contains a summary of the patient's health status including problems, medications, allergies, and basic information about health insurance, care documentation, and the patient's care plan. These represent a \"snapshot\" of a patient's health data that can be useful or possibly lifesaving, if available at the time of clinical encounter. The ASTM CCR standard's purpose is to permit easy creation by a physician using an electronic health record (EHR) system at the end of an encounter.\nMore specifically within the CCR, there are mandated core elements in 6 sections.\nThese 6 sections are:\n\n\nBecause it is expressed in the standard data interchange language known as XML, a CCR can potentially be created, read, and interpreted by any EHR or EMR software application. A CCR can also be exported to other formats, such as PDF or Office Open XML (Microsoft Word 2007 format).\n\nThe Continuity of Care Document (CCD) is an HL7 CDA implementation of the Continuity of Care Record (CCR). A CCR document can generally be converted into CCD using Extensible Stylesheet Language Transformations (XSLT), but it is not always possible to perform the inverse transformation, since some CCD features are not supported in CCR. HITSP provides reference information that demonstrates how CCD and CCR (as HITSP C32) are embedded in CDA. \n\nAlthough the CCR and CCD standards could continue to coexist, with CCR providing for basic information requests and CCD servicing more detailed requests, the newer CCD standard might eventually completely supplant CCR.\n\nAs mentioned, the CCR standard uses eXtensible Markup Language (XML) as it is aimed at being technology neutral to allow for maximum applicability. This specified XML coding provides flexibility that will allow users to formulate, transfer, and view the CCR in a number of ways, for example, in a browser, in a Health Level 7 (HL7) message, in a secure email, as a PDF file, as an HTML file, or as a word document. This is aimed at producing flexible expression of structured data in avenues such as electronic health record (EHR) systems. In terms of the CCR's transportability, secure carriage and transmission of the electronic file can occur via physical\ntransport media, for example on a USB thumb drive, tablet or phone, CD ROM, or smart card, and in an electronic sense, secure transmission can occur via a network line, or the Internet.\n\n\n"}
{"id": "11750606", "url": "https://en.wikipedia.org/wiki?curid=11750606", "title": "Duane Alexander", "text": "Duane Alexander\n\nDuane Alexander (born August 11, 1940 in Baltimore, Maryland) is an American physician who was the director of the National Institute of Child Health and Human Development from 1986 through 2009. In 2009 he moved to the position of senior scientific advisor to the Director of the NIH's Fogarty International Center.\n\nAlexander married Marianne Ellis in 1963. They have two children, Keith and Kristin.\n\nHe received his MD from Johns Hopkins.\n\n"}
{"id": "26353918", "url": "https://en.wikipedia.org/wiki?curid=26353918", "title": "Earl Bradley", "text": "Earl Bradley\n\nEarl Brian Bradley (born May 10, 1953) is a former pediatrician from Lewes, Delaware and convicted serial child molester. He was indicted in 2010 on 471 charges of molesting, raping and exploiting 103 child patients (102 girls and 1 boy). Some of the victims were as young as three months old. He was charged in April 2010 with an additional 58 offenses in relation to the abuse of 24 additional victims. He has been described by a number of reputable news outlets and commentators as \"the worst pedophile in American history.\" Dr. Eli Newberger, a professor at Harvard Medical School and a pediatrician who has studied child abuse cases for almost 40 years, said Bradley's was \"the worst pediatrician abuse case I've ever heard of.\" Bradley had access to an estimated 7,000 pediatric patients. According to a personal injury law firm in Baltimore, MD, one of many representing class action plaintiffs, 1,400 families in the class action alleged abuse. Bradley was ultimately found guilty on all charges and was sentenced to 14 consecutive terms of life plus 165 years in prison without parole on June 26, 2011. His conviction was affirmed by the Delaware Supreme Court on September 6, 2012.\n\nIn the wake of his arrest, it emerged that he had faced accusations of child abuse as early as 1995 in both Delaware and Pennsylvania.\n\nBradley was born and raised in Philadelphia. He graduated from the Temple University School of Medicine in 1983 and completed his pediatrics residency at Thomas Jefferson University Hospital in 1986. Around 1984, he began working at Frankford-Torresdale hospital on Knights Road in Northeast Philadelphia. He opened his own practice in a small complex just a few blocks away, at Academy and Red Lion Roads in Morrell Park. He continued to work at Jefferson until a sudden move to Lewes on the Delmarva Peninsula in 1995. The move was abrupt and poorly planned, with many patients complaining that they were not even notified that appointments had been cancelled.\n\nIn Lewes, Bradley was widely regarded as eccentric. His practice, BayBees Pediatrics, had patients from nearby resort communities and area farming communities. The medical offices, located near Lewes in unincorporated Sussex County, were ostentatiously decorated with carnival rides and other child-friendly decorations, such as a giant statue of Buzz Lightyear from Disney's \"Toy Story\" and a small movie theatre showing Disney movies. He owned several vehicles which were painted yellow and black with eyes and a tail to resemble bumblebees. The oddities extended to his home, where he prominently displayed a full suit of medieval armor on his porch.\n\nThe first known allegations of inappropriate conduct by Bradley came in 1994, when Thomas Jefferson University Hospital investigated a patient's complaint of sexual misconduct. There was a second allegation in 1995. The hospital could not verify the claim, and records remain sealed. Bradley promptly closed his fledgling private practice and relocated with his children to Lewes, where he took a job with Beebe Medical Center.\n\nIn 2004, Bradley's sister Lynda Barnes, who had served as an office manager at his medical office, alerted the state medical society that parents had complained to her about inappropriate touching by Bradley. Barnes also reported that Bradley physically and emotionally abused his own son, and stole prescription antidepressants from the office.\n\nAllegations were made again in 2005. Police records show that a nurse reported that he videotaped kids playing and other doctors reported complaints about long and unnecessary vaginal exams. When police in Milford, Delaware, sought a warrant to arrest him for inappropriately touching a child patient, the Attorney General's office concluded at the time that there was insufficient evidence to warrant prosecution.\n\nOn December 16, 2009, following a year-long investigation and complaints of inappropriate touching by a two-year-old patient, Bradley was arrested and charged with nine counts, including a felony charge for a fourth degree rape of a two-year-old patient. Soon after, relying on more than 13 hours of videotaped rapes and molestations discovered by police in Bradley's home and office, additional warrants were issued. These included felony warrants for several counts of child exploitation and first-degree rape.\n\nBradley surrendered to authorities on December 18, 2009. His bail was set at $2.9 million cash, which was not posted. An initial preliminary hearing was delayed after prison officials placed Bradley on a suicide watch. His attorney, Eugene Maurer, denied that Bradley was suicidal but complained that prison officials had deprived him of his prescription glasses.\n\nIn February 2010, a grand jury sitting for the Delaware Superior Court indicted Bradley on 471 charges, including counts of rape and exploitation. 103 victims were identified in the indictment, though the Attorney General indicated that they expected to identify even more victims. The indictment included allegations that Bradley had forced children as young as three months old to engage in intercourse and oral sex. It also revealed that Bradley had videotaped sexual assaults during which his victims \"appeared to lose consciousness\" from Bradley choking them by forcing oral sex on his child victims. The videos also show children in diapers screaming as they attempted to escape from Bradley before he raped them in an outbuilding on the property.\n\nThough his private lawyers quit after Delaware took steps to freeze his assets, Bradley was afforded a public defender and arraigned on March 24, 2010. He pleaded not guilty to all charges. A follow-up hearing was scheduled for May 17.\n\nThe Delaware Board of Medical Practice suspended Bradley's license permanently on February 19, 2010.\n\nDelaware governor Jack Markell, concerned about failures in the medical, police and legal communities that allowed Bradley's crimes to continue for more than a decade, has called for an independent review. Widener University Law School Dean Linda Ammons was appointed to head up that review.\n\nDelaware Attorney General Beau Biden, son of U.S. Vice President Joe Biden, announced in a January 2010 speech that he would not seek election to his father's former seat in the United States Senate because he felt it was more important to fully pursue Bradley's prosecution.\n\nBradley's case was moved to New Castle County from Sussex County because of concerns about getting an impartial jury in Sussex County, as many families of his 127 alleged victims lived there. However, Bradley then waived his right to a jury trial, opting instead for a bench trial. The case was then moved back to Sussex County. After hearing evidence on June 7, 2011, Judge William C. Carpenter, the presiding Judge in Bradley's bench trial, stated he would issue his verdict at a later date.\n\nOn June 23, 2011, Bradley was convicted on all 24 counts on a consolidated indictment (which originally contained 529 counts): 14 counts of rape, seven counts of assault, and three counts of sexual exploitation of a child.\n\nOn August 26, 2011, Judge Carpenter sentenced Bradley to the maximum sentence of 14 consecutive terms of life in prison plus a further 165 years in prison without parole. Under Delaware law, anyone convicted of raping three separate persons automatically receives life without parole. Judge Carpenter said that Bradley \"betrayed his patients' trust and disgraced the medical profession\", and that \"you will never be in a position to harm a child again\".\n\nBradley appealed to the Delaware Supreme Court, claiming that the original search warrant was not specific enough about where the evidence would be located, and that the police exceeded the limits of the warrant without probable cause. The Delaware Supreme Court unanimously affirmed Bradley's convictions on September 6, 2012.\n\nThe office complex housing his former practice was demolished on October 10, 2011. Earlier, state police confiscated the contents of Bradley's storage locker in Rehoboth Beach and destroyed them; the items were to be auctioned off to satisfy unpaid rent, but Biden intervened on behalf of the victims to buy them for a symbolic $1 so as not to take the chance of them ever being used again.\n\nBradley was held in the Special Housing Unit (SHU) of the James T. Vaughn Correctional Center in New Castle County until 2016, when Delaware authorities announced they would move him to an out-of-state prison because many of his victims and/or state residents otherwise affected by his actions either worked in or were incarcerated in Delaware prisons. Connecticut authorities revealed that Bradley was moved to the Cheshire Correctional Institution in Cheshire, Connecticut.\n"}
{"id": "11022353", "url": "https://en.wikipedia.org/wiki?curid=11022353", "title": "Form (exercise)", "text": "Form (exercise)\n\nForm is a specific way of performing a movement, often a strength training exercise, to avoid injury, prevent cheating and increase strength.\n\nExercises or drills in sport have a recognized way of performing the movements that have two purposes:\nBy using proper or 'good and' form, the risk of injury is lowered. A lack of proper form commonly results in injury or a lack of effect from the exercise being performed\n\nGood form ensures that the movement only uses the main muscles, and avoids recruiting secondary muscles. As a muscle fatigues, the body attempts to compensate by recruiting other muscle groups and transferring force generation to non-fatigued units. This reduces the benefits in strength or size gain experienced by the muscles as they are not worked to failure.\n"}
{"id": "38644815", "url": "https://en.wikipedia.org/wiki?curid=38644815", "title": "Gartner's duct cyst", "text": "Gartner's duct cyst\n\nA Gartner's duct cyst (sometimes incorrectly referred to as \"vaginal inclusion cyst\") is a benign vaginal cyst that originates from the Gartner's duct, which is a vestigial remnant of the mesonephric duct (wolffian duct) in females. They are typically small asymptomatic cysts that occur along the lateral walls of the vagina, following the course of the duct. They can present in adolescence with painful menstruation (Dysmenorrhea) or difficulty inserting a tampon. They can also enlarge to substantial proportions and be mistaken for urethral diverticulum or cystocele. In some rare instances, they can be congenital. \n\nThere is a small association between Gartner's duct cysts and metanephric urinary anomalies, such as ectopic ureter & ipsilateral renal hypoplasia. Symptoms of a Gartner's duct cyst include: infections, bladder dysfunction, abdominal pain, vaginal discharge, and urinary incontinence. \n"}
{"id": "4672868", "url": "https://en.wikipedia.org/wiki?curid=4672868", "title": "Hodbarrow RSPB reserve", "text": "Hodbarrow RSPB reserve\n\nHodbarrow RSPB Reserve is a nature reserve run by the Royal Society for the Protection of Birds on the edge of the Lake District National Park in Cumbria, England. It is on the Duddon Estuary near the town of Millom. \n\nThe nature reserve occupies a site where iron ore was mined until the 1960s.\nThe mining caused subsidence, leading to flooding since the closure of the mine, as the site is no longer dewatered. The reserve continues to be protected from the sea by a seawall completed in 1905. Most of the area of the reserve is taken up by Hodbarrow Lagoon, a flooded part of the former mine, which is described as a \"coastal lagoon\".\n\nThere is a car park on the Millom side of the reserve from which you can walk to the seawall. (Alternatively, there is more direct pedestrian access to the seawall from Haverigg.)\n\nThere is a bird hide on the seawall which gives views of the lagoon.\n\nThe lagoon was originally a separate Site of Special Scientific Interest, notified in 1983. Following an amalgamation of SSSIs, it is part of the Duddon Estuary Site of Special Scientific Interest.\nThe Duddon estuary is also an Important Bird Area, and a Special Protection Area.\n\nHodbarrow has breeding populations of terns. It is renowned for large numbers of wildfowl during the winter, especially teal, wigeon, coot, mallard, tufted duck, common pochard, goldeneye, red-breasted merganser, and occasionally long-tailed duck, eider, goosander, pintail and shoveler.\n\n"}
{"id": "53003952", "url": "https://en.wikipedia.org/wiki?curid=53003952", "title": "Institute of Human Virology Nigeria", "text": "Institute of Human Virology Nigeria\n\nThe Institute of Human Virology Nigeria (IHVN) is a non-governmental organization that focuses on HIV/AIDS related problems in Nigeria. It was established as an affiliate to the Institute of Human Virology, University of Maryland School of Medicine, Baltimore in 2004. In 2016, IHVN claimed that it reaches 2.3 million Nigerians with HIV testing services, including about 25,000 who tested positive for the disease.\n\n\nThe activities of the Institute are funded by international organizations which include:\n\nCenters for Disease Control and Prevention (CDC)\n"}
{"id": "4937465", "url": "https://en.wikipedia.org/wiki?curid=4937465", "title": "Janssen-Cilag", "text": "Janssen-Cilag\n\nJanssen-Cilag is a subsidiary of the Johnson & Johnson pharmaceutical company. The company was founded in the early 1990s by a merger between Janssen Pharmaceutica and Cilag.\n\n"}
{"id": "216908", "url": "https://en.wikipedia.org/wiki?curid=216908", "title": "Karolinska Institute", "text": "Karolinska Institute\n\nThe Karolinska Institute (KI; ; sometimes known as the (Royal) Caroline Institute in English) is a medical university in Solna within the Stockholm urban area of Sweden. It is recognised as Sweden's best university and one of the largest, most prestigious medical universities in the world. The Nobel Assembly at the Karolinska Institute awards the Nobel Prize in Physiology or Medicine. The assembly consists of fifty professors from various medical disciplines at the university. The current rector of Karolinska Institute is Ole Petter Ottersen, who took office in August 2017.\n\nThe Karolinska Institute was founded in 1810 on the island of Kungsholmen on the west side of Stockholm; the main campus was relocated decades later to Solna, just outside Stockholm. A second campus was established more recently in Flemingsberg, Huddinge, south of Stockholm. The Karolinska Institute is consistently ranked among the top medical universities internationally in a number of ranking tables.\n\nThe Karolinska Institute is Sweden's third oldest medical school, after Uppsala University (founded in 1477) and Lund University (founded in 1666). It is one of Sweden's largest centres for training and research, accounting for 30% of the medical training and more than 40% of all academic medical and life science research conducted in Sweden.\n\nThe Karolinska University Hospital, located in Solna and Huddinge, is associated with the university as a research and teaching hospital. Together they form an academic health science centre. While most of the medical programs are taught in Swedish, the bulk of the Ph.D. projects are conducted in English. The institute's name is a reference to the Caroleans.\n\nThe Karolinska Institute was founded by King Karl XIII on 13 December 1810 as an \"academy for the training of skilled army surgeons\" after one in three soldiers wounded in the Finnish War against Russia died in field hospitals. Indeed, a report of the time came to the conclusion that \"the medical skills of the army barber-surgeons are manifestly inadequate, so Sweden needs to train surgeons in order to better prepare the country for future wars.\" Just one year later, in 1811, the Karolinska Institute was granted license to train not only surgeons but medical practitioners in general.\nAs one of KI's first professors, Jöns Jacob Berzelius laid the foundations of the newly inaugurated institute's scientific orientation, which in 1816 is granted the name \"Carolinska Institutet\" (in reference to the Caroleans). This name, however, didn't really make an impact at the time and so was expanded to \"Carolinska Medico Chirurgiska institutet\", which proved more popular, especially when preceded by the epithet \"Kongliga\" (Royal), as introduced in 1822. This original institute was situated in the Royal Bakery on Riddarholmen (a small but central island in Stockholm) and within a just a couple of years had grown to encompass four professorships in anatomy, natural history and pharmacy, theoretical medicine and practical medicine (internal medicine and surgery).\n\nAt around the same time Anders Johan Hagströmer, a professor of anatomy and surgery from the Collegium Medicum (the National Board of Health and Welfare of its day), was appointed the institute's first inspector, a post equivalent to today's president. In the same year, the institute moved to the old Glasbruk quarter on Norr Mälarstrand, beside what is now the City Hall. The move across the waters of Riddarfjärden was accomplished with the help of barges, one of which is said to have capsized, consigning parts of Hagströmer's collection of preparations to the lake bed. Despite this his library survives intact and today forms part of the KI-Swedish Society of Medicine museum at the institute's Hagströmer Library.\nIn 1861 the institute reached a significant milestone in being awarded the right to confer its own degrees; as such it was granted a status equal to that of a university. This, in turn, led to an increase in the size of the student body, necessitating the demolition of the old building on the Glasbruk plot and its replacement with a new, larger one. This new institute building was built in stages, mostly during the 1880s and into the first decade of the 20th century; it stands to this day, and has remained largely unchanged since its opening.\nAlthough it had already gained the right to confer general degrees, KI wasn't licensed to confer medical degrees until 1874. Previously, even though the institute could run courses in medicine, the right to confer medical degrees was almost exclusively that of Uppsala University. Following on from this change in the institute's status the first doctoral thesis was defended at KI by Alfred Levertin, on the subject of \"Om Torpa Källa\". Just shortly thereafter the Medical Students' Union was formed.\n\nThe next decade was one of firsts. By 1880 the Karolinska Institute had started to accept women and so it was in 1884 that Karolina Widerström became the first woman to obtain a bachelor's degree in medicine from the institute; she later went on to obtain a Licentiate degree in medicine and chose to specialise in women's medicine and gynaecology. Anna Stecksén later became the first woman to obtain a doctorate from the university.\n\nJust five years later, following the death of Alfred Nobel in 1895, the Karolinska Institute received the right to select the recipient of the Nobel Prize in Physiology or Medicine. Since then, this assignment has given the Karolinska Institute a broad contact network in the field of medical science. Indeed, over the years, five of the institute's own researchers have been awarded the Nobel Prize in Physiology or Medicine.\n\nBy 1930 the Swedish parliament had decided that a new teaching hospital was needed and resolved to build the new facility at Norrbacka in the Solna area of Stockholm. The hospital was, from the start, planned to have its theoretical and practical functions side by side, essentially signalling the start of the KI's move to the new site. The chief architect appointed to design the building, later to be named the Karolinska Hospital after a proposal by the Karolinska Institute, was Carl Westman. Westman worked tirelessly on this project, allowing for completion of the main hospital building by 1940. The hospital was officially opened in the same year along with the new Department of Public Health, KI's first building co-located with the hospital complex, and by 1945 KI moved the last of its departments from Kungsholmen to the Norrbacka site, known to current students and staff as the institute's Solna Campus.\n\nThe Karolinska Institute was initially slow to adopt social change. Nanna Svartz became the Karolinska Institute and Sweden's first state-employed female professor in 1937; she was the only female academic of this rank until Astrid Fagréus became the institute's second female professor almost thirty years later in 1965. However, by the mid-twentieth century student revolts were taking place regularly in the pursuit of greater social equality. It was in response to the largest of these revolts in 1968 that the name of the institute was shortened to the current form \"Karolinska Institutet\", or \"KI\".\nBy the early 1970s the Huddinge Hospital became increasingly affiliated with the institute and, as more and more of the Karolinska Institute's departments start to move into the hospital buildings, the area developed to become what is KI's Huddinge campus today. It was here that the medical information centre for computer-based citation research (MIC) was set up, making the Karolinska Institute the first MEDLARS (Medical Literature Analysis and Retrieval System) Centre outside the United States. Similarly innovative work continued at the institute throughout the 1970s, with Sweden's first toxicology programme commencing at KI in 1976 and the establishment of a psychotherapy course in 1979. Furthermore, in 1977 the Stockholm Institute of Physiotherapy closed, with the study programme transferring to a new physiotherapy department at KI.\nIn 1982 the Huddinge campus was expanded with the addition of the Novum Research Centre for the study of biotechnology, oral biology, nutrition and toxicology, and structural biochemistry. The number of departments was reduced from 150 to 30 in 1993 and programmes in optometry, biomedicine, and dental technology commenced. The other key feature of the 1990s was the institute's increasingly ambitious plans for commercialisation, as a result of which 'Karolinska Institutet Holding AB' and 'Karolinska Innovations AB' were formed to help the institute intensify its relations with the business community and facilitate scientists' attempts to commercialise their discoveries.\n\nThe year 1997 marked a major point in the history of the Karolinska Institute as it was finally granted official university status with a stated mission to \"contribute to the improvement of human health through research, education and information\". This newly acquired status later led to the incorporation of the Stockholm University of Health Sciences into KI, as a result of which seven new study programmes in occupational therapy, audionomy, midwifery, biomedical laboratory science, nursing, radiology nursing and dental hygiene, were added to KI's teaching portfolio. Around the same time the institute's public health science programme commenced bringing the total number of study programmes to nineteen. Similarly, by this point the institute possessed three student unions: The Medical Students' Union, the Dental Students' Union, and the Physiotherapy Students' Union.\n\nWith the turn of the millennium the Karolinska Institute sought to strengthen its activities in the field of external education and research, forming dedicated companies to help it do so and later establishing a fundraising campaign with a target of raising an extra 1 billion kronor for research between 2007 and 2010. During the same period new programmes were established in medical informatics, podiatry, and psychology as well as master's programmes of one and two years duration, and in 2004, the Karolinska Institute appointed its first female president – Harriet Wallberg-Henriksson, professor of integrative physiology.\n\nThe year 2010 marked yet another significant event in the history of the KI as the institute celebrated its 200th anniversary under the patronage of King Carl XVI Gustaf. Shortly thereafter, in 2013, Anders Hamsten assumed his new position as vice-chancellor of the Karolinska Institute, a position he held until when he resigned as in the wake of the Macchiarini affair.\n\n\nThe rod of Asclepius is named after the god of medicine, Aesculapius or Asclepius. This ancient god was the son of Apollo and was generally accompanied by a snake. Over time, the snake became coiled around the staff borne by the god.\n\nThe snake bowl was originally depicted together with Asclepius' daughter, the virgin goddess of health Hygieia or Hygiea. The snake ate from her bowl, which was considered to bring good fortune. There is nothing to support the notion that the snake would secrete its venom into the bowl.\n\nThe cockerel symbolises new life and was sacrificed to Asclepius by those who had recovered from illness. This is the meaning behind the Greek philosopher Socrates' last words after he drank the poisoned cup: \"Crito, we owe a cock to Asclepius. Do pay it. Don't forget.\"\n\nThe Karolinska Institute offers the widest range of medical education under one roof in Sweden. Several of the programmes include clinical training or other training within the healthcare system. The close proximity of the Karolinska University Hospital and other teaching hospitals in the Stockholm area thus plays an important role during the education.\nApproximately 6,000 full-time students are taking educational and single subject courses at Bachelor and Master levels at the Karolinska Institute. Annually, 20 upper high school students from all over Sweden get selected to attend Karolinska's 7-week long biomedical summer research school, informally named \"SoFo\".\n\nAs of 2018, Times Higher Education ranks the Karolinska Institute at number 38 overall in the world. In the field of Clinical Medicine and Pharmacy, it is ranked twelfth worldwide and third in Europe for 2016. According to the 2018 Times Higher Education World University Rankings, the Karolinska Institute is ranked 15th worldwide and fifth in Europe in the subject Clinical, pre-clinical and health. The Karolinska Institute is not listed in the overall QS World University Rankings since it only ranks multi-faculty universities. However, QS does rank the Karolinska Institute in the category of Life Sciences and Medicine, placing it as the best in Sweden, 3rd in Europe and 6th worldwide in 2017. In 2015, the QS ranked the Department of Dental Medicine 1st in the world.\n\nThe university was a founding member of the League of European Research Universities. \n\nIn February 2015, the KI announced it had received a record $50 million donation from Lau Ming-wai, who chairs Hong Kong property developer Chinese Estates Holdings, and would establish a research centre in the city. Within a few days, \"Next Magazine\" revealed that Chuen-yan – son of Hong Kong Chief Executive CY Leung – had recently been awarded a fellowship to research heart disease therapeutics at the institute in Stockholm beginning that year, and raised questions about the \"intricate relationship between the chief executive and powerful individuals\". CY Leung had visited KI when in Sweden in 2014, and subsequently introduced KI president, Anders Hamsten, to Lau. The Democratic Party urged the ICAC to investigate the donation, suggesting that Leung may have abused his public position to further his son's career. The Chief Executive's Office strenuously denied suggestions of any quid pro quo, saying that \"the admission of the [Chief Executive's] son to post-doctoral research at KI is an independent decision by KI having regard to his professional standards. He [the son] plays no role and does not hold any position at the [proposed] Ming Wai Lau Center for Regenerative Medicine.\" This accusation has also been questioned by the \"South China Morning Post\"'s Canadian-based pro-Beijing and pro-government opinion columnist, Alex Lo: \"The insinuation is that Leung Chuen-yan with a doctorate from Cambridge doesn't deserve his job at the Karolinska Institute...Leung the son probably could get similar junior posts in many other prestigious-sounding – at least to brand-obsessed Hongkongers – research institutes; it's not that big a deal.\"\n\nThe Nobel Assembly at the Karolinska Institute is a body at the Karolinska Institute which awards the Nobel Prize in Physiology or Medicine. The Nobel Assembly consists of fifty professors in medical subjects at the Karolinska Institute, appointed by the faculty of the Institute, and is a private organisation which is formally not part of the Karolinska Institute. The main work involved in collecting nominations and screening nominees is performed by the Nobel Committee at the Karolinska Institute, which has five members. The Nobel Committee, which is appointed by the Nobel Assembly, is only empowered to recommend laureates, while the final decision rests with the Nobel Assembly.\n\nIn the early history of the Nobel Prize in Physiology or Medicine, which was first awarded in 1901, the laurates were decided upon by the entire faculty of the Karolinska Institute. The reason for creating a special body for the decisions concerning the Nobel Prize was the fact that the Karolinska Institute is a state-run university, which in turn means that it is subject to various laws that apply to government agencies in Sweden and similar Swedish public sector organisations, such as freedom of information legislation. By moving the actual decision making to a private body at Karolinska Institute (but not part of it), it is possible to follow the regulations for the Nobel Prize set down by the Nobel Foundation, including keeping the confidentiality of all documents and proceedings for a minimum of 50 years. Also, the legal possibility of contesting the decisions in e.g. administrative courts is removed.\n\nThe other two Nobel Prize-awarding bodies in Sweden, the Royal Swedish Academy of Sciences and the Swedish Academy, are legally private organisations (although enjoying royal patronage), and have therefore not had to make any special arrangements to be able to follow the Nobel Foundation's regulations.\n\n\n\n"}
{"id": "49404", "url": "https://en.wikipedia.org/wiki?curid=49404", "title": "Kitchen", "text": "Kitchen\n\nA kitchen is a room or part of a room used for cooking and food preparation in a dwelling or in a commercial establishment. A modern middle-class residential kitchen is typically equipped with a stove, a sink with hot and cold running water, a refrigerator, and worktops and kitchen cabinets arranged according to a modular design. Many households have a microwave oven, a dishwasher, and other electric appliances. The main functions of a kitchen are to store, prepare and cook food (and to complete related tasks such as dishwashing). The room or area may also be used for dining (or small meals such as breakfast), entertaining and laundry. The design and construction of kitchens is a huge market all over the world. The United States are expected to generate $47,730m in the kitchen furniture industry for 2018 alone.\n\nCommercial kitchens are found in restaurants, cafeterias, hotels, hospitals, educational and workplace facilities, army barracks, and similar establishments. These kitchens are generally larger and equipped with bigger and more heavy-duty equipment than a residential kitchen. For example, a large restaurant may have a huge walk-in refrigerator and a large commercial dishwasher machine. In some instances commercial kitchen equipment such as commercial sinks are used in household settings as it offers ease of use for food preparation and high durability.\n\nIn developed countries, commercial kitchens are generally subject to public health laws. They are inspected periodically by public-health officials, and forced to close if they do not meet hygienic requirements mandated by law.\n\nThe evolution of the kitchen is linked to the invention of the cooking range or stove and the development of water infrastructure capable of supplying running water to private homes. Food was cooked over an open fire. Technical advances in heating food in the 18th and 19th centuries changed the architecture of the kitchen. Before the advent of modern pipes, water was brought from an outdoor source such as wells, pumps or springs.\n\nThe houses in Ancient Greece were commonly of the atrium-type: the rooms were arranged around a central courtyard for women. In many such homes, a covered but otherwise open patio served as the kitchen. Homes of the wealthy had the kitchen as a separate room, usually next to a bathroom (so that both rooms could be heated by the kitchen fire), both rooms being accessible from the court. In such houses, there was often a separate small storage room in the back of the kitchen used for storing food and kitchen utensils.\nIn the Roman Empire, common folk in cities often had no kitchen of their own; they did their cooking in large public kitchens. Some had small mobile bronze stoves, on which a fire could be lit for cooking. Wealthy Romans had relatively well-equipped kitchens. In a Roman villa, the kitchen was typically integrated into the main building as a separate room, set apart for practical reasons of smoke and sociological reasons of the kitchen being operated by slaves. The fireplace was typically on the floor, placed at a wall—sometimes raised a little bit—such that one had to kneel to cook. There were no chimneys.\n\nEarly medieval European longhouses had an open fire under the highest point of the building. The \"kitchen area\" was between the entrance and the fireplace. In wealthy homes there was typically more than one kitchen. In some homes there were upwards of three kitchens. The kitchens were divided based on the types of food prepared in them. In place of a chimney, these early buildings had a hole in the roof through which some of the smoke could escape. Besides cooking, the fire also served as a source of heat and light to the single-room building. A similar design can be found in the Iroquois longhouses of North America.\n\nIn the larger homesteads of European nobles, the kitchen was sometimes in a separate sunken floor building to keep the main building, which served social and official purposes, free from indoor smoke.\n\nThe first known stoves in Japan date from about the same time. The earliest findings are from the Kofun period (3rd to 6th century). These stoves, called \"kamado\", were typically made of clay and mortar; they were fired with wood or charcoal through a hole in the front and had a hole in the top, into which a pot could be hanged by its rim. This type of stove remained in use for centuries to come, with only minor modifications. Like in Europe, the wealthier homes had a separate building which served for cooking. A kind of open fire pit fired with charcoal, called \"irori\", remained in use as the secondary stove in most homes until the Edo period (17th to 19th century). A \"kamado\" was used to cook the staple food, for instance rice, while \"irori\" served both to cook side dishes and as a heat source.\n\nThe kitchen remained largely unaffected by architectural advances throughout the Middle Ages; open fire remained the only method of heating food. European medieval kitchens were dark, smoky, and sooty places, whence their name \"smoke kitchen\". In European medieval cities around the 10th to 12th centuries, the kitchen still used an open fire hearth in the middle of the room. In wealthy homes, the ground floor was often used as a stable while the kitchen was located on the floor above, like the bedroom and the hall. In castles and monasteries, the living and working areas were separated; the kitchen was sometimes moved to a separate building, and thus could not serve anymore to heat the living rooms. In some castles the kitchen was retained in the same structure, but servants were strictly separated from nobles, by constructing separate spiral stone staircases for use of servants to bring food to upper levels. The kitchen might be separate from the great hall due to the smoke from cooking fires and the chance the fires may get out of control. Few medieval kitchens survive as they were \"notoriously ephemeral structures\". An extant example of such a medieval kitchen with servants' staircase is at Muchalls Castle in Scotland. In Japanese homes, the kitchen started to become a separate room within the main building at that time.\n\nWith the advent of the chimney, the hearth moved from the center of the room to one wall, and the first brick-and-mortar hearths were built. The fire was lit on top of the construction; a vault underneath served to store wood. Pots made of iron, bronze, or copper started to replace the pottery used earlier. The temperature was controlled by hanging the pot higher or lower over the fire, or placing it on a trivet or directly on the hot ashes. Using open fire for cooking (and heating) was risky; fires devastating whole cities occurred frequently.\n\nLeonardo da Vinci invented an automated system for a rotating spit for spit-roasting: a propeller in the chimney made the spit turn all by itself. This kind of system was widely used in wealthier homes. Beginning in the late Middle Ages, kitchens in Europe lost their home-heating function even more and were increasingly moved from the living area into a separate room. The living room was now heated by tiled stoves, operated from the kitchen, which offered the huge advantage of not filling the room with smoke.\n\nFreed from smoke and dirt, the living room thus began to serve as an area for social functions and increasingly became a showcase for the owner's wealth. In the upper classes, cooking and the kitchen were the domain of the servants, and the kitchen was set apart from the living rooms, sometimes even far from the dining room. Poorer homes often did not yet have a separate kitchen; they kept the one-room arrangement where all activities took place, or at the most had the kitchen in the entrance hall.\n\nThe medieval smoke kitchen (or Farmhouse kitchen) remained common, especially in rural farmhouses and generally in poorer homes, until much later. In a few European farmhouses, the smoke kitchen was in regular use until the middle of the 20th century. These houses often had no chimney, but only a smoke hood above the fireplace, made of wood and covered with clay, used to smoke meat. The smoke rose more or less freely, warming the upstairs rooms and protecting the woodwork from vermin.\n\nIn Connecticut, as in other colonies of New England during Colonial America, kitchens were often built as separate rooms and were located behind the parlor and keeping room or dining room. One early record of a kitchen is found in the 1648 inventory of the estate of a John Porter of Windsor, Connecticut. The inventory lists goods in the house \"over the kittchin\" and \"in the kittchin\". The items listed in the kitchen were: silver spoons, pewter, brass, iron, arms, ammunition, hemp, flax and \"other implements about the room\". Separate summer kitchens were also common on large farms in the north; these were used to prepare meals for harvest workers and tasks such as canning during the warm summer months, to keep the heat out of the main house.\n\nIn the southern states, where the climate and sociological conditions differed from the north, the kitchen was often relegated to an outbuilding. On plantations, it was separate from the big house or mansion in much the same way as the feudal kitchen in medieval Europe: the kitchen was operated by slaves in the antebellum years. Their working place was separated from the living area of the masters by the social standards, but more importantly, it was a means to reduce the chance of fire in the main house from kitchen operations.\n\nTechnological advances during industrialization brought major changes to the kitchen. Iron stoves, which enclosed the fire completely and were more efficient, appeared. Early models included the Franklin stove around 1740, which was a furnace stove intended for heating, not for cooking. Benjamin Thompson in England designed his \"Rumford stove\" around 1800. This stove was much more energy efficient than earlier stoves; it used one fire to heat several pots, which were hung into holes on top of the stove and were thus heated from all sides instead of just from the bottom. However, his stove was designed for large kitchens; it was too big for domestic use. The \"Oberlin stove\" was a refinement of the technique that resulted in a size reduction; it was patented in the U.S. in 1834 and became a commercial success with some 90,000 units sold over the next 30 years. These stoves were still fired with wood or coal. Although the first gas street lamps were installed in Paris, London, and Berlin at the beginning of the 1820s and the first U.S. patent on a gas stove was granted in 1825, it was not until the late 19th century that using gas for lighting and cooking became commonplace in urban areas.\n\nBefore and after the beginning of the 20th century, kitchens were frequently not equipped with built-in cabinetry, and the lack of storage space in the kitchen became a real problem. The Hoosier Manufacturing Co. of Indiana adapted an existing furniture piece, the baker's cabinet, which had a similar structure of a table top with some cabinets above it (and frequently flour bins beneath) to solve the storage problem. By rearranging the parts and taking advantage of (then) modern metal working, they were able to produce a well-organized, compact cabinet which answered the home cook's needs for storage and working space. A distinctive feature of the Hoosier cabinet is its accessories. As originally supplied, they were equipped with various racks and other hardware to hold and organize spices and various staples. One useful feature was the combination flour-bin/sifter, a tin hopper that could be used without having to remove it from the cabinet. A similar sugar bin was also common.\n\nThe urbanization in the second half of the 19th century induced other significant changes that would ultimately change the kitchen. Out of sheer necessity, cities began planning and building water distribution pipes into homes, and built sewers to deal with the waste water. Gas pipes were laid; gas was used first for lighting purposes, but once the network had grown sufficiently, it also became available for heating and cooking on gas stoves. At the turn of the 20th century, electricity had been mastered well enough to become a commercially viable alternative to gas and slowly started replacing the latter. But like the gas stove, the electric stove had a slow start. The first electrical stove had been presented in 1893 at the World's Columbian Exposition in Chicago, but it was not until the 1930s that the technology was stable enough and began to take off.\nIndustrialization also caused social changes. The new factory working class in the cities was housed under generally poor conditions. Whole families lived in small one or two-room apartments in tenement buildings up to six stories high, badly aired and with insufficient lighting. Sometimes, they shared apartments with \"night sleepers\", unmarried men who paid for a bed at night. The kitchen in such an apartment was often used as a living and sleeping room, and even as a bathroom. Water had to be fetched from wells and heated on the stove. Water pipes were laid only towards the end of the 19th century, and then often only with one tap per building or per story. Brick-and-mortar stoves fired with coal remained the norm until well into the second half of the century. Pots and kitchenware were typically stored on open shelves, and parts of the room could be separated from the rest using simple curtains.\n\nIn contrast, there were no dramatic changes for the upper classes. The kitchen, located in the basement or the ground floor, continued to be operated by servants. In some houses, water pumps were installed, and some even had kitchen sinks and drains (but no water on tap yet, except for some feudal kitchens in castles). The kitchen became a much cleaner space with the advent of \"cooking machines\", closed stoves made of iron plates and fired by wood and increasingly charcoal or coal, and that had flue pipes connected to the chimney. For the servants the kitchen continued to also serve as a sleeping room; they slept either on the floor, or later in narrow spaces above a lowered ceiling, for the new stoves with their smoke outlet no longer required a high ceiling in the kitchen. The kitchen floors were tiled; kitchenware was neatly stored in cupboards to protect them from dust and steam. A large table served as a workbench; there were at least as many chairs as there were servants, for the table in the kitchen also doubled as the eating place for the servants.\n\nThe urban middle class imitated the luxurious dining styles of the upper class as best as they could. Living in smaller apartments, the kitchen was the main room—here, the family lived. The study or living room was saved for special occasions such as an occasional dinner invitation. Because of this, these middle-class kitchens were often more homely than those of the upper class, where the kitchen was a work-only room occupied only by the servants. Besides a cupboard to store the kitchenware, there were a table and chairs, where the family would dine, and sometimes—if space allowed—even a fauteuil or a couch.\nGas pipes were first laid in the late 19th century, and gas stoves started to replace the older coal-fired stoves. Gas was more expensive than coal, though, and thus the new technology was first installed in the wealthier homes. Where workers' apartments were equipped with a gas stove, gas distribution would go through a coin meter.\n\nIn rural areas, the older technology using coal or wood stoves or even brick-and-mortar open fireplaces remained common throughout. Gas and water pipes were first installed in the big cities; small villages were connected only much later.\n\nThe trend to increasing gasification and electrification continued at the turn of the 20th century. In industry, it was the phase of work process optimization. Taylorism was born, and time-motion studies were used to optimize processes. These ideas also spilled over into domestic kitchen architecture because of a growing trend that called for a professionalization of household work, started in the mid-19th century by Catharine Beecher and amplified by Christine Frederick's publications in the 1910s.\n\nA stepstone was the kitchen designed in Frankfurt by Margarethe Schütte-Lihotzky. Working class women frequently worked in factories to ensure the family's survival, as the men's wages often did not suffice. Social housing projects led to the next milestone: the Frankfurt Kitchen. Developed in 1926, this kitchen measured 1.9 m by 3.4 m (approximately 6 ft 2 in by 11 ft 2 in, with a standard layout). It was built for two purposes: to optimize kitchen work to reduce cooking time and lower the cost of building decently equipped kitchens. The design, created by Margarete Schütte-Lihotzky, was the result of detailed time-motion studies and interviews with future tenants to identify what they needed from their kitchens. Schütte-Lihotzky's fitted kitchen was built in some 10,000 apartments in the housing projects erected in Frankfurt in the 1930s.\n\nThe initial reception was critical: it was so small that only one person could work in it; some storage spaces intended for raw loose food ingredients such as flour were reachable by children. But the Frankfurt kitchen embodied a standard for the rest of the 20th century in rental apartments: the \"work kitchen\". It was criticized as \"exiling the women in the kitchen\", but post-World War II economic reasons prevailed. The kitchen once more was seen as a work place that needed to be separated from the living areas. Practical reasons also played a role in this development: just as in the bourgeois homes of the past, one reason for separating the kitchen was to keep the steam and smells of cooking out of the living room.\n\nThe idea of standardized was first introduced locally with the Frankfurt kitchen, but later defined new in the \"Swedish kitchen\" (\"Svensk köksstandard\", Swedish kitchen standard). The equipment used remained a standard for years to come: hot and cold water on tap and a kitchen sink and an electrical or gas stove and oven. Not much later, the refrigerator was added as a standard item. The concept was refined in the \"Swedish kitchen\" using unit furniture with wooden fronts for the kitchen cabinets. Soon, the concept was amended by the use of smooth synthetic door and drawer fronts, first in white, recalling a sense of cleanliness and alluding to sterile lab or hospital settings, but soon after in more lively colors, too. Some years after the Frankfurt Kitchen, Poggenpohl presented the \"reform kitchen\" in 1928 with interconnecting cabinets and functional interiors. The reform kitchen was a forerunner to the later unit kitchen and fitted kitchen.\n\nUnit construction since its introduction has defined the development of the modern kitchen. Pre-manufactured modules, using mass manufacturing techniques developed during World War II, greatly brought down the cost of a kitchen. Units which are kept on the floor are called \"floor units\", \"floor cabinets\", or \"base cabinets\" on which a kitchen worktop – originally often formica and often now made of granite, marble, tile or wood – is placed. The units which are held on the wall for storage purposes are termed as \"wall units\" or \"wall cabinets\". In small areas of kitchen in an apartment, even a \"tall storage unit\" is available for effective storage. In cheaper brands, all cabinets are kept a uniform color, normally white, with interchangeable doors and accessories chosen by the customer to give a varied look. In more expensive brands, the cabinets are produced matching the doors' colors and finishes, for an older more bespoke look.\n\nStarting in the 1980s, the perfection of the extractor hood allowed an open kitchen again, integrated more or less with the living room without causing the whole apartment or house to smell. Before that, only a few earlier experiments, typically in newly built upper-middle-class family homes, had open kitchens. Examples are Frank Lloyd Wright's \"House Willey\" (1934) and \"House Jacobs\" (1936). Both had open kitchens, with high ceilings (up to the roof) and were aired by skylights. The extractor hood made it possible to build open kitchens in apartments, too, where both high ceilings and skylights were not possible.\n\nThe re-integration of the kitchen and the living area went hand in hand with a change in the perception of cooking: increasingly, cooking was seen as a creative and sometimes social act instead of work. And there was a rejection by younger home-owners of the standard suburban model of separate kitchens and dining rooms found in most 1900-1950 houses. Many families also appreciated the trend towards open kitchens, as it made it easier for the parents to supervise the children while cooking and to clean up spills. The enhanced status of cooking also made the kitchen a prestige object for showing off one's wealth or cooking professionalism. Some architects have capitalized on this \"object\" aspect of the kitchen by designing freestanding \"kitchen objects\". However, like their precursor, Colani's \"kitchen satellite\", such futuristic designs are exceptions.\n\nAnother reason for the trend back to open kitchens (and a foundation of the \"kitchen object\" philosophy) is changes in how food is prepared. Whereas prior to the 1950s most cooking started out with raw ingredients and a meal had to be prepared from scratch, the advent of frozen meals and pre-prepared convenience food changed the cooking habits of many people, who consequently used the kitchen less and less. For others, who followed the \"cooking as a social act\" trend, the open kitchen had the advantage that they could be with their guests while cooking, and for the \"creative cooks\" it might even become a stage for their cooking performance.\n\nThe \"Trophy Kitchen\" is equipped with very expensive and sophisticated appliances which are used primarily to impress visitors and to project social status, rather than for actual cooking.\n\nThe ventilation of a kitchen, in particular a large restaurant kitchen, poses certain difficulties that are not present in the ventilation of other kinds of spaces. In particular, the air in a kitchen differs from that of other rooms in that it typically contains grease, smoke and odours.\n\nThe Frankfurt Kitchen of 1926 was made of several materials depending on the application. The modern built-in kitchens of today use particle boards or MDF, decorated with a variety of materials and finishes including wood veneers, lacquer, glass, melamine, laminate, ceramic and eco gloss. Very few manufacturers produce home built-in kitchens from stainless-steel. Until the 1950s, steel kitchens were used by architects, but this material was displaced by the cheaper particle board panels sometimes decorated with a steel surface.\n\nDomestic (or residential) kitchen design is a relatively recent discipline. The first ideas to optimize the work in the kitchen go back to Catharine Beecher's \"A Treatise on Domestic Economy\" (1843, revised and republished together with her sister Harriet Beecher Stowe as \"The American Woman's Home\" in 1869). Beecher's \"model kitchen\" propagated for the first time a systematic design based on early ergonomics. The design included regular shelves on the walls, ample work space, and dedicated storage areas for various food items. Beecher even separated the functions of preparing food and cooking it altogether by moving the stove into a compartment adjacent to the kitchen.\n\nChristine Frederick published from 1913 a series of articles on \"New Household Management\" in which she analyzed the kitchen following Taylorist principles of efficiency, presented detailed time-motion studies, and derived a kitchen design from them. Her ideas were taken up in the 1920s by architects in Germany and Austria, most notably Bruno Taut, Erna Meyer, and Margarete Schütte-Lihotzky. A social housing project in Frankfurt (the \"Römerstadt\" of architect Ernst May) realized in 1927/38 was the breakthrough for her Frankfurt kitchen, which embodied this new notion of efficiency in the kitchen.\n\nWhile this \"work kitchen\" and variants derived from it were a great success for tenement buildings, home owners had different demands and did not want to be constrained by a 6.4 m² kitchen. Nevertheless, kitchen design was mostly ad-hoc following the whims of the architect. In the U.S., the \"Small Homes Council\", since 1993 the \"Building Research Council\", of the School of Architecture of the University of Illinois at Urbana–Champaign was founded in 1944 with the goal to improve the state of the art in home building, originally with an emphasis on standardization for cost reduction. It was there that the notion of the \"kitchen work triangle\" was formalized: the three main functions in a kitchen are storage, preparation, and cooking (which Catharine Beecher had already recognized), and the places for these functions should be arranged in the kitchen in such a way that work at one place does not interfere with work at another place, the distance between these places is not unnecessarily large, and no obstacles are in the way. A natural arrangement is a triangle, with the refrigerator, the sink, and the stove at a vertex each.\n\nThis observation led to a few common kitchen forms, commonly characterized by the arrangement of the kitchen cabinets and sink, stove, and refrigerator:\n\nIn the 1980s, there was a backlash against industrial kitchen planning and cabinets with people installing a mix of work surfaces and free standing furniture, led by kitchen designer Johnny Grey and his concept of the \"unfitted kitchen\". Modern kitchens often have enough informal space to allow for people to eat in it without having to use the formal dining room. Such areas are called \"breakfast areas\", \"breakfast nooks\" or \"breakfast bars\" if the space is integrated into a kitchen counter. Kitchens with enough space to eat in are sometimes called \"eat-in kitchens\". During the 2000s, flat pack kitchens were popular for people doing DIY renovating on a budget. The flat pack kitchens industry makes it easy to put together and mix and matching doors, bench tops and cabinets. In flat pack systems, many components can be interchanged.\n\nRestaurant and canteen kitchens found in hotels, hospitals, educational and work place facilities, army barracks, and similar institutions are generally (in developed countries) subject to public health laws. They are inspected periodically by public health officials, and forced to close if they do not meet hygienic requirements mandated by law.\n\nCanteen kitchens (and castle kitchens) were often the places where new technology was used first. For instance, Benjamin Thompson's \"energy saving stove\", an early 19th-century fully closed iron stove using one fire to heat several pots, was designed for large kitchens; another thirty years passed before they were adapted for domestic use.\n\nAs of 2017, restaurant kitchens usually have tiled walls and floors and use stainless steel for other surfaces (workbench, but also door and drawer fronts) because these materials are durable and easy to clean. Professional kitchens are often equipped with gas stoves, as these allow cooks to regulate the heat more quickly and more finely than electrical stoves. Some special appliances are typical for professional kitchens, such as large installed deep fryers, steamers, or a bain-marie. \n\nThe fast food and convenience food trends have changed the manner in which restaurant kitchens operate. Some of these type restaurants may only \"finish\" convenience food that is delivered to them, or just reheat completely prepared meals. At the most they may grill a hamburger or a steak. But in the early 21st century, c-stores (convenience stores) are attracting greater market share by performing more food preparation on-site and better customer service than some fast food outlets.\n\nThe kitchens in railway dining cars have presented special challenges: space is limited, and, personnel must be able to serve a great number of meals quickly. Especially in the early history of railways, this required flawless organization of processes; in modern times, the microwave oven and prepared meals have made this task much easier. Kitchens aboard ships, aircraft and sometimes railcars are often referred to as galleys. On yachts, galleys are often cramped, with one or two burners fueled by an LP gas bottle. Kitchens on cruise ships or large warships, by contrast, are comparable in every respect with restaurants or canteen kitchens. \n\nOn passenger airliners, the kitchen is reduced to a pantry. The crew's role is to heat and serve in-flight meals delivered by a catering company. An extreme form of the kitchen occurs in space, \"e.g.\", aboard a Space Shuttle (where it is also called the \"galley\") or the International Space Station. The astronauts' food is generally completely prepared, dehydrated, and sealed in plastic pouches before the flight. The kitchen is reduced to a rehydration and heating module.\n\nOutdoor areas where food is prepared are generally not considered kitchens, even though an outdoor area set up for regular food preparation, for instance when camping, might be referred to as an \"outdoor kitchen\". An outdoor kitchen at a campsite might be placed near a well, water pump, or water tap, and it might provide tables for food preparation and cooking (using portable campstoves). Some campsite kitchen areas have a large tank of propane connected to burners, so that campers can cook their meals. Military camps and similar temporary settlements of nomads may have dedicated kitchen tents, which have a vent to enable cooking smoke to escape.\n\nIn schools where home economics, food technology (previously known as \"domestic science\"), or culinary arts are taught, there are typically a series of kitchens with multiple equipment (similar in some respects to laboratories) solely for the purpose of teaching. These consist of multiple workstations, each with its own oven, sink, and kitchen utensils, where the teacher can show students how to prepare food and cook it.\n\nKitchens in China are called . More than 3000 years ago, the ancient Chinese used the ding for cooking food. The ding was developed into the wok and pot used today. Many Chinese people believe that there is a Kitchen God who watches over the kitchen for the family. According to this belief, the god returns to heaven to give a report to the Jade Emperor annually about this family behavior. Every Chinese New Year Eve, families will gather together to pray for the kitchen god to give a good report to heaven and wish him to bring back good news on the fifth day of the New Year. \nThe most common cooking equipment in Chinese family kitchens and restaurant kitchens are woks, steamer baskets and pots. The fuel or heating resource was also important technique to practice the cooking skills. Traditionally Chinese were using wood or straw as the fuel to cook food. A Chinese chef had to master flaming and heat radiation to reliably prepare traditional recipes. Chinese cooking will use a pot or wok for pan frying, stir frying, deep frying or boiling.\n\nKitchens in Japan are called Daidokoro (台所; lit. \"kitchen\"). Daidokoro is the place where food is prepared in a Japanese house. Until the Meiji era, a kitchen was also called \"kamado\" (かまど; lit. stove) and there are many sayings in the Japanese language that involve kamado as it was considered the symbol of a house and the term could even be used to mean \"family\" or \"household\" (similar to the English word \"hearth\"). When separating a family, it was called \"Kamado wo wakeru\", which means \"divide the stove\". \"Kamado wo yaburu\" (lit. \"break the stove\") means that the family was bankrupt.\n\nIn India, a kitchen is called a “Rasoi” (in hindi\\Sanskrit) or a “Swayampak ghar” in Marathi , and there exist many other names for it in the various regional languages. Many different methods of cooking exist across the country, and the structure and the materials used in constructing kitchens have varied depending on the region. For example, in north and central India, cooking used to be carried out in clay ovens called “Chulha”s, fired by wood, coal or dried cowdung. In households where members observed vegetarianism, separate kitchens were maintained to cook and store vegetarian and non-vegetarian food. Religious families often treat the kitchen as a sacred space. Indian kitchens are built on an Indian architectural science called vastushastra. The Indian kitchen vastu is of utmost importance while designing a kitchens in India. Modern-day architects also follow the norms of vastushastra while designing Indian kitchens across the world.\n\nWhile many kitchens belonging to poor families continue to use clay stoves and the older forms of fuel, the urban middle and upper classes usually have gas stoves with cylinders or piped gas attached. Electric cooktops are rarer since they consume a great deal of electricity, but microwave ovens are gaining popularity in urban households and commercial enterprises. Indian kitchens are also supported by biogas and solar energy as fuel. World's largest solar energy kitchen is built in India. In association with government bodies, India is encouraging domestic biogas plants to support the kitchen system.\n\n\n\n"}
{"id": "12967948", "url": "https://en.wikipedia.org/wiki?curid=12967948", "title": "Linea alba (cheek)", "text": "Linea alba (cheek)\n\nThe linea alba (Latin for \"white line\"), in dentistry, is a horizontal streak on the inner surface of the cheek, level with the biting plane. It usually extends from the commissure to the posterior teeth and can extend to the inner lip mucosa and corners of the mouth.\n\nIt is a common finding and most likely associated with pressure, frictional irritation, or sucking trauma from the facial surfaces of the teeth. It may be found in individuals who chew tobacco, and may be mistaken for a lesion requiring treatment.\n\n\nTreatment is not required.\n\n\n"}
{"id": "3444271", "url": "https://en.wikipedia.org/wiki?curid=3444271", "title": "List of United Nations resolutions relating to Lebanon", "text": "List of United Nations resolutions relating to Lebanon\n"}
{"id": "33846430", "url": "https://en.wikipedia.org/wiki?curid=33846430", "title": "List of biobanks", "text": "List of biobanks\n\nA biobank is a physical place which stores biobank specimens. In some cases, participant data is also collected and stored. Access policies details may vary across biobanks but generally involve obtaining ethics approval from institutional review boards and scientific review or peer review approval from the institutions under which the biobanks operate as well as Ethics approval from the institutions where the research projects will be undertaken. The samples and data are safeguarded so that researchers can use them in experiments deemed adequate. This page contains a list of biobanks.\n\nBiobanks can be classified in several ways. Some examples of how they can be classified is by their controlling entity (government, commercial enterprise, or private research institution), by their geographical location, or by what sorts of samples they collect.\n\nBiobanks may be classified by purpose or design. Disease-oriented biobanks usually have a hospital affiliation through which they collect samples representing a variety of diseases, perhaps to look for biomarkers affiliated with disease. Population-based biobanks need no particular hospital affiliation because they sample from large numbers of all kinds of people, perhaps to look for biomarkers for disease susceptibility in a general population.\n\n"}
{"id": "58948", "url": "https://en.wikipedia.org/wiki?curid=58948", "title": "List of notifiable diseases", "text": "List of notifiable diseases\n\nThe following is a list of notifiable diseases arranged by country.\n"}
{"id": "57732274", "url": "https://en.wikipedia.org/wiki?curid=57732274", "title": "Maurice Collis (surgeon)", "text": "Maurice Collis (surgeon)\n\nMaurice Collis (1791 - March 1852) was the president of the Royal College of Surgeons in Ireland (RCSI) in 1839.\n\n"}
{"id": "53716092", "url": "https://en.wikipedia.org/wiki?curid=53716092", "title": "Mental health inequality", "text": "Mental health inequality\n\nMental health inequality refers to the differences in quality of mental health and mental health care for different identities and populations. Mental health can be defined as well-being and/or the absence of clinically defined mental illness. There are social economic factors that influence individuals or groups of people of a certain demographic. This can be a factor to mental health care access. Inequalities may include presence of mental health, access to mental health care, quality of mental health care, and mental health outcomes between populations with different race, ethnicity, sexual orientation, sex, gender, socioeconomic statuses, education level, and geographic location.\n\nSocial determinants of health are factors such as economic status, education level, demographics, where you live and genetics that can influence one's health. Due to experiences certain populations are much more susceptible to mental disorders or illnesses. Traumatic experiences can develop by the lack of resources of the determinants of health. Not having employment or being from a certain area where those resources are limited trigger some of the most common mental disorders. ( Depression, anxiety, bipolar, psychological stress) Mental health within itself leads to the inequality among the social determinants of health. The economic level of one's life is a factor to their life expectancy and mental health. For example lung cancer is by far the most preventable disease yet the rates of consumption of tobacco just increase. Ironically the consumers are those with mental disorders.\n\nWhile education and mental health seem mutually exclusive, there is a significant parallel between the inequities. Educational disparities can be defined as unjust or unfair differences in educational outcomes that can be a result of difference in treatment of certain minority groups in schools, varying socioeconomic statuses, and varying educational needs. These disparities in education can ultimately lead to issues of mental health. When this happens, less privileged groups get looped into the cascading effects of inequality.\n\nThe disparities that arise in education do not happen by chance. There are definitely predictors that factor into these inequalities. Some common ones are socioeconomic status, immigrant status, and ethnic/racial status. Socioeconomic status plays a large role in the difference in access to educational resources. School districts are split geographically. Because the current funding for public schools comes from local property taxes, there is more incentive for high-status individuals to narrow the boundaries to not include lower income families from their school districts. Because each school district is then only encompassing one socioeconomic group, the programs and quality are affected. This is where we begin to see the dramatic differences between school districts. While some schools offer amazing guidance departments, advanced classes, and phenomenal facilities, other areas struggle to find qualified and motivated teachers to teach basic classes. Although public education is something that is supposed to be a right for all, an individual's socioeconomic status can greatly affect the quality of that education.\n\nAn individual's immigration status also affects the quality of education received. While there are some immigrant groups which do well after immigrating to the United States, many do not have the same level of success. There are many barriers that prevent the academic success of immigrant children. These barriers include but are not limited to the fact that most parents of immigrant children do not understand the United States educational system, inadequate English as a Second Language programs, and segregation. There are also differences in outcomes across immigrant generation, with first-generation immigrants performing better than subsequent generations. This is termed the immigrant paradox. These issues along with the psychological effects of acculturation (e.g., adapting to a whole new country, language, and culture) amplify educational inequality.\n\nDisparities in education are the insufficiency of resources that are included but limited. These disparities usually targets socially excluded communities with low income. Statistics are used when measuring grades, GPA, test scores, and dropout rates to determine the success of students. Disparities as such affect somones mental health by creating a system in which a person could never succeed.\n\nFurther research observes that those minority races living in areas of low-poverty have more and easier access to mental health services than those in high-poverty neighborhoods. There is a pattern of people in high-poverty areas being unable escape this cycle. Due to these conditions, inequality remains and they are unable to gain access to mental health care which can be very beneficial to those who may be suffering from stress due to lack of resources and money.\n\nMany minorities including African Americans, Hispanics, and Asian Americans inhabit these poverty filled neighborhoods due to factors being not in their favor in certain aspects of society. These neighborhoods lack resources such as offices with psychiatrists or health clinics with good doctors who are trained to help those in need of mental health care. It would also be beneficial to make specific services just for those in high-poverty neighborhoods who lack the resources so we can encourage those in need to get the help that they deserve. With adjustments made to meet these circumstances, the spatial disparities can be lowered and allow those who need the help to get it.\n\nThere is inequality in mental health care access for different races and ethnicities. It is known through much research that even poor minorities have less access to mental health care than poor non-Latino whites. In addition, it is also known that blacks have even less of a chance to access to mental health services and care than those who are white. Many of these minorities may confront an issue directly resulting in the search for mental health care support, yet they don't have the same access as other people.\n\nAfter surveying people of different races over years we observe that African Americans, Hispanics, and Asian Americans gain less access to the same type of mental services that non-minority whites get access to. With this research there was a piece that stated:\n\n\"This theory postulates that Whites have a greater propensity to avoid living in poverty communities because they are more likely to enjoy social and economic advantages. Only seriously mentally ill Whites suffer from steep downward mobility and come to reside in high-poverty neighborhoods\" (Julian Chun-Chung Chow, Kim Jaffee, and Lonnie Snowden).\n\nThis has been a problem for minority races that need the same services. It is an issue because African Americans, Hispanics, and Asian Americans need the services more in certain areas due to how biologically certain minority races are more likely to be diagnosed with a mental illness than whites.\n\nSexuality plays a large role in the prediction of mental illnesses and overall mental health. Those who identify as lesbian, gay, bisexual, transgender, and/or queer have a higher risk of having mental health issues, most likely as a result of the continued discrimination and victimization they receive at the hands of others. Members of this population are confronted with derogatory and hateful comments, whether through face-to-face communication or through social media, which affects their self-worth and confidence, leading to anxiety, depression, thoughts of suicide, suicide attempts, and suicide. These mental health effects are most commonly seen among adolescents, however, they are also prevalent among adults of all ages. The sources of discrimination and victimization that the LGBTQ population suffers from can be both external and internal. While parts of society today are not accepting of the LGBTQ community and make public statements to advertise their discontent, an identifying LGBTQ can also have low confidence and a lack of self-worth that furthers these negative mental health effects.\n\nThe most notable predictor of mental health illnesses among the LGBTQ population is family acceptance. Those of the LGBTQ population who receive little or no family support and acceptance are three times more likely to have thoughts of suicide than those who do have a strong family support system behind them. Oftentimes, the lack of familial support is more conducive of detrimental behaviors, such as drug and illegal substance abuse, which can cause further harm to the individual. Multiple aspects of lifestyles, including religion, can affect family support. Those who have strong family ties to religion may be less likely to seek support and help from family members due to fear of a lack of acceptance within the family, as well as within the religious community.\n\nWhile gender differences among those with mental health disorders are an underdeveloped field of study, there are gender specific aspects to life that cause disparities. Gender is often a determinant of the amount of power one has over factors in their life, such as socioeconomic status and social position, and the stressors that go along with these factors. The location of genders and sex within the social construct can be a great determinant of risks and predictors of mental health disorders. These disparities in gender can correlate to the disparities in the types of mental health disorders that individuals have. While all genders and sexes are at risk of a large variety of mental health illnesses, some illnesses and disorders are more common among one sex than another. Women are twice as likely as men to be diagnosed with forms of depression, whereas men are three times more likely to be given a diagnosis of a social anxiety disorder than women.\n\nSex can also be a determinant of other aspects of mental health as well. The time of onset of symptoms can be different dependent on one's sex. Women are more likely to show signs of mental illnesses, such as depression, earlier and at a younger age than men. Many believe this to be a correlation with the onset time of puberty. As a result of social stigmatisms and stereotypes within society, women are also more likely to be prescribed mood-altering medications, whereas men are more likely to be prescribed medications for addictions. Further research on the mental health disparities among sex and gender is needed in order to gain a deeper knowledge of the predictors of mental health and the possible differences in treatments.\n\nOver the past decade, many of the disparities regarding mental health stem from differences in racial/ethnic groups. Mental illness is one of the highest health burdens for minority groups.\n\nIn many minority groups, certain mental illnesses are under and over diagnosed. For example, schizophrenia is over diagnosed in African Americans while mood disorders, depression, and anxiety are under diagnosed. This is due to a variety of reasons. The recognition of mental illness is often thought of in conjunction with stereotypes regarding African Americans. Those who present symptoms exhibiting depression and mood swings may be mistaken as stereotypically violent.\n\nWhile the diagnosis of these minority groups is vastly different, the care they receive also varies between whites and minority groups. This is not only due to external factors and facilities but also the way in which these minority groups choose to proceed with treatment. African Americans generally do receive care from lower quality facilities, but they are also more likely than whites to terminate treatment prematurely.\n\nFinally, health care plans are also a major contributor to the inequities in mental health care access. Provider discrimination involves health care providers unfairly using stereotypes to decide how to distribute diagnoses. Physicians often rely on common stereotypes of individuals in deciding treatment, which ultimately leads to minority groups not getting the specialized treatment required to diagnose and treat mental illness.\n\nThere is a growing unmet need for mental health services and equity in the quality of these services. While these services often advertise themselves as being a support system and caregiver for any and all who need treatment or support, oftentimes certain aspects of an individual's life, such as race, ethnicity, and sexual orientation, will determine the type of care that they are given.\n\nDue to a growing level of socioeconomic inequality among races, African Americans are less likely to have access to mental health care and are more likely to have lesser quality care when they do find it. African Americans and Hispanics are more likely to be uninsured or have Medicaid, limiting the amount and type of access that they have mental health outpatient sources. In one study, of all those who received mental health care, minority populations reported a higher degree of unmet needs and dissatisfaction with the services they were given (12.5% of whites, 25.4% of African Americans, and 22.6% of Hispanics reported poor care).\n\nThe LGBTQ population, while still open to the same disparities as racial minority groups, is often confronted with the problem of being denied mental health treatment because of the gender they identify as or their sexual orientation. In a study conducted by The National Center for Transgender Equality and the National Gay and Lesbian Task Force, 19% of the LGBTQ sample reported being denied the healthcare they needed. In addition, 28% of the sample reported being harassed or even physically assaulted during the health visit. While denial of treatment and harassment during treatment are large causes of the disparities among mental health care quality, the lack of knowledge is also of concern among the LGBTQ population. As it is such a newly developing field of study, there is very little knowledge or research conducted that relate specifically to LGBTQ health and healthcare. Because of this, about 50% of the LGBTQ population report having to teach aspects of their health and treatment to the health care providers.\n\nBecause mental health inequality is largely due to disparities in health insurance, ways to improve mental health equity must come from changes in healthcare policies. While much of mental health disparities comes from improving access to healthcare for the underprivileged. The biggest reason why people are not obtaining mental health care is because they cannot afford it. Also, changing the content of healthcare literature to include mental health is equally important. The United States has made massive strides to break down the stigmas surrounding mental health. Given that, mental health is still not considered to be a main part of basic health care plans. In order for individuals to receive the treatment necessary for mental illness, it must be acknowledged as an illness.\n\n"}
{"id": "20517237", "url": "https://en.wikipedia.org/wiki?curid=20517237", "title": "Methodic school", "text": "Methodic school\n\nThe Methodic school of medicine (\"Methodics\", \"Methodists\", or \"Methodici\", ) was an ancient school of medicine in ancient Greece and Rome. The Methodic school arose in reaction to both the Empiric school and the Dogmatic school (sometimes referred to as the Rationalist school). While the exact origins of the Methodic school are shrouded in some controversy, its doctrines are fairly well documented. Sextus Empiricus points to the school's common ground with Pyrrhonism, in that it “follow[s] the appearances and take[s] from these whatever seems expedient.”\n\nThere is no clear consensus on who founded the Methodic school and when it was founded. It has been purported that the Methodic school was founded by the students of Asclepiades. In particular, Themison of Laodicea, Asclepiades’ most distinguished student, is often credited with founding the Methodic school in the first century BC. However, some historians claim that the Methodic school was founded by Asclepiades himself in 50 BC. It has also been claimed that the Methodism does not truly arise until the first century AD. In any case, it is widely accepted that Methodism arose as a reaction to the Empiric and Rationalist (or Dogmatic) schools, bearing some similarities to both schools but fundamentally different.\n\nThe Methodic school emphasized the treatment of diseases rather than the history of the individual patient. According to the Methodists, medicine is no more than a “knowledge of manifest generalities” (\"gnōsis phainomenōn koinotēnōn\"). In other words, medicine was no more than the awareness of general, recurring features that manifest in a tangible way. While Methodist views on medicine are slightly more complex than this, the above generalization was meant to apply to not only medicine, but to any art. Methodists conceive of medicine as a true art, in contrast to Empiricists or Dogmatists.\n\nThey asserted that the knowledge of the cause of the disease bears no relation to the method of cure, and that it is sufficient to observe some general symptoms of illnesses. All a doctor really needs to know is the disease itself, and from that knowledge alone will he know the treatment. To claim that knowledge of the disease alone will provide knowledge of the treatment, the Methodists first claim that diseases are indicative of their own treatments. Just as how hunger leads a person naturally to food and how thirst leads a person naturally to water, so too does the disease naturally indicate the cure. As Sextus Empiricus points out, when a dog is pricked by a thorn, it naturally removes the foreign object ailing its body.\n\nThe core theory was disruption of the normal circulation of 'atoms' through the body's 'pores' caused disease. To cure a disease it is sufficient to observe some general symptoms of illnesses; and that there are three kinds of diseases, one bound, another loose (\"fluens\", a disorder attended with some discharge), and the third a mixture of these. Sometimes the excretions of sick people are too small or too large, or a particular excretion might be deficient or excessive. These kinds of illnesses are sometimes severe, sometimes chronic, sometimes increasing, sometimes stable, and sometimes abating. As soon as it is known to which of these diseases an illness belongs, if the body is bound, then it must be opened; if it is loose, then it must be restrained; if it is complicated, then the most urgent malady must be fought first. One type of treatment is required in acute, another in inveterate illnesses; another when diseases are increasing, another when stable, and another when decreasing. The observation of these things constitute the art of medicine, called \"method\" ().\n\nAs the seeking after the causes of diseases seemed to Themison to rest on too uncertain a foundation, he thus wished to establish his system upon the analogies and indications common to many diseases (), no matter that these analogies were as obscure as the causes of the Dogmatic school. Themison wrote several works which are now lost.\n\nThe Methodic school takes it to be that once a doctor has recognized the disease a patient has for what it is, the treatment that should follow is inherently obvious. It is not a matter of inference or observation, but of an immediate knowledge. To a Dogmatist, the symptoms a disease manifest is indicative of a hidden state that causes the disease. Only by knowing the hidden state can a doctor understand how to treat a patient. The symptoms manifested by a patient are indicative of the underlying state causing the disease, and the hidden state is indicative of the treatment that follows. Like the Empiricists, the Methodists refuse the notion of hidden states, claiming that there is no need to take a detour into inferences of hidden states. The symptoms manifested make it immediately obvious what needs to be done.\n\nOn the other hand, Methodists also reject the Empiricist notion that the connection between a disease and its treatment is a matter of experience. Methodists purport that experience is not necessary to understand that a state of depletion implies a need for replenishment, that a state of restraint must be loosened. To a Methodist, treatments to diseases are immediately obvious; it is a matter of common sense, of reason. There is no need for justification by experience; to Methodists, there are no conceivable alternatives to their innate knowledge of proper treatments.\n\nBecause Methodists do not take their knowledge of proper treatment as an issue of observation or experience, they are willing to concede that their knowledge is a matter of reason. On this point, the Methodists bear a similarity to Dogmatists, taking reason as a constructive approach to appropriating the proper treatment for an ailment. However, Methodists do not support the Dogmatic concept of employing reason to find hidden causes that belie the disease manifested. The causes of diseases can not be fantastic or obscure forces that would not occur in ordinary life. The key difference between Methodist doctors and Empiricist or Dogmatic doctors is that a Methodist's knowledge is \"firm and certain,\" and that it leaves no room for future revision. Rather than rely upon reason and experience, the Methodist does what is inherently obvious; there is no room for error.\n\n\n\n"}
{"id": "5795298", "url": "https://en.wikipedia.org/wiki?curid=5795298", "title": "Molecular mimicry", "text": "Molecular mimicry\n\nMolecular mimicry is defined as the theoretical possibility that sequence similarities between foreign and self-peptides are sufficient to result in the cross-activation of autoreactive T or B cells by pathogen-derived peptides. Despite the prevalence of several peptide sequences which can be both foreign and self in nature, a single antibody or TCR (T cell receptor) can be activated by just a few crucial residues which stresses the importance of structural homology in the theory of molecular mimicry. Upon the activation of B or T cells, it is believed that these \"peptide mimic\" specific T or B cells can cross-react with self-epitopes, thus leading to tissue pathology (autoimmunity). Molecular mimicry is a phenomenon that has been just recently discovered as one of several ways in which autoimmunity can be evoked. A molecular mimicking event is, however, more than an epiphmenon despite its low statistical probability of occurring and these events have serious implications in the onset of many human autoimmune disorders.\n\nIn the past decade the study of autoimmunity System, the failure to recognize self antigens as \"self\", has grown immensely. Autoimmunity is thought by many researchers to be a result of a loss of immunological tolerance, the ability for an individual to discriminate between self and non-self, though others are beginning to think that many autoimmune diseases are due to mutations governing programmed cell death, or to environmental products that injure target tissues, thus causing a release of immunostimulatory alarm signals. Growth in the field of autoimmunity has resulted in more and more frequent diagnosis of autoimmune diseases. Consequently, recent data show that autoimmune diseases affect approximately 1 in 31 people within the general population. Growth has also led to a greater characterization of what autoimmunity is and how it can be studied and treated. With an increased amount of research, there has been tremendous growth in the study of the several different ways in which autoimmunity can occur, one of which is molecular mimicry. The mechanism by which pathogens have evolved, or obtained by chance, similar amino acid sequences or the homologous three-dimensional crystal structure of immunodominant epitopes remains a mystery.\n\n\nTolerance is a fundamental property of the immune system. Tolerance involves non-self discrimination which is the ability of the normal immune system to recognize and respond to foreign antigens, but not self antigens. Autoimmunity is evoked when this tolerance to self antigen is broken. Tolerance within an individual is normally evoked as a fetus. This is known as maternal-fetal tolerance where B cells expressing receptors specific for a particular antigen enter the circulation of the developing fetus via the placenta.\n\nAfter pre-B cells leave the bone marrow where they are synthesized, they are moved to the bone marrow where the maturation of B cells occurs. It is here where the first wave of B cell tolerance arises. Within the bone marrow, pre-B cells will encounter various self and foreign antigens present in the thymus that enter the thymus from peripheral sites via the circulatory system. Within the thymus, pre-T cells undergo a selection process where they must be positively selected and should avoid negative selection. B cells that bind with low avidity to self-MHC receptors are positively selected for maturation, those that do not die by apoptosis. Cells that survive positive selection, but bind strongly to self-antigens are negatively selected also by active induction of apoptosis. This negative selection is known as clonal deletion, one of the mechanisms for B cell tolerance. Approximately 99 percent of pre-B cells within the thymus are negatively selected. Only approximately 1 percent are positively selected for maturity. \n\nHowever, there is only a limited repertoire of antigen that B cells can encounter within the thymus. B cell tolerance then must occur within the periphery after the induction of B cell tolerance within the thymus as a more diverse group of antigens can be encountered in peripheral tissues. This same positive and negative selection mechanism, but in peripheral tissues, is known as clonal anergy. The mechanism of clonal anergy is important to maintain tolerance to many autologous antigens. Active suppression is the other known mechanism of T cell tolerance. Active suppression involves the injection of large amounts of foreign antigen in the absence of an adjuvant which leads to a state of unresponsiveness. This unresponsive state is then transferred to a naïve recipient from the injected donor to induce a state of tolerance within the recipient.\n\nTolerance is also produced in T cells. There are also various processes which lead to B cell tolerance. Just as in T cells, clonal deletion and clonal anergy can physically eliminate autoreactive B cell clones. Receptor editing is another mechanism for B cell tolerance. This involves the reactivation or maintenance of V(D)J recombination in the cell which leads to the expression of novel receptor specificity through V region gene rearrangements which will create variation in the heavy and light immunoglobulin (Ig) chains.\n\nAutoimmunity can thus be defined simply as exceptions to the tolerance \"rules.\" By doing this, an immune response is generated against self-tissue and cells. These mechanisms are known by many to be intrinsic. However, there are pathogenic mechanisms for the generation of autoimmune disease. Pathogens can induce autoimmunity by polyclonal activation of B or T cells, or increased expression of major histocompatibility complex (MHC) class I or II molecules. There are several ways in which a pathogen can cause an autoimmune response. A pathogen may contain a protein that acts as a mitogen to encourage cell division, thus causing more B or T cell clones to be produced. Similarly, a pathogenic protein may act as a superantigen which causes rapid polyclonal activation of B or T cells. Pathogens can also cause the release of cytokines resulting in the activation of B or T cells, or they can alter macrophage function. Finally, pathogens may also expose B or T cells to cryptic determinants, which are self antigen determinants that have not been processed and presented sufficiently to tolerize the developing T cells in the thymus and are presented at the periphery where the infection occurs. \nMolecular mimicry has been characterized as recently as the 1970s as another mechanism by which a pathogen can generate autoimmunity. Molecular mimicry is defined as similar structures shared by molecules from dissimilar genes or by their protein products. Either the linear amino acid sequence or the conformational fit of the immunodominant epitope may be shared between the pathogen and host. This is also known as \"cross-reactivity\" between self antigen of the host and immunodominant epitopes of the pathogen. An autoimmune response is then generated against the epitope. Due to similar sequence homology in the epitope between the pathogen and the host, cells and tissues of the host associated with the protein are destroyed as a result of the autoimmune response.\n\nThe prerequisite for molecular mimicry to occur is thus the sharing of the immunodominant epitope between the pathogen and the immunodominant self sequence that is generated by a cell or tissue. However, due to the amino acid variation between different proteins, molecular mimicry should not happen from a probability standpoint. Assuming five to six amino acid residues are used to induce a monoclonal antibody response, the probability of 20 amino acids occurring in six identical residues between two proteins is 1 in 20 or 1 in 64,000,000. However, there has been evidence shown and documented of many molecular mimicry events.\n\nTo determine which epitopes are shared between pathogen and self, large protein databases are used. The largest protein database in the world, known as the UniProt database (formerly SwissProt), has shown reports of molecular mimicry becoming more common with expansion of the database. The database currently contains 1.5 X 10 residues. The probability of finding a perfect match with a motif of 5 amino acids in length is 1 in 3.7 X 10 (0.055). Therefore, within the SwissProt database, one would expect to find 1.5 X 10 X 3.7 X 10 = 5 matches. However, there are sequence motifs within the database that are overrepresented and are found more than 5 times. For example, the QKRAA sequence is an amino acid motif in the third hypervariable region of HLA-DRB1*0401. This motif is also expressed on numerous other proteins, such as on gp110 of the Epstein-Barr virus and in \"E. coli\". This motif occurs 37 times in the database. This would suggest that the linear amino acid sequence may not be an underlying cause of molecular mimicry since it can be found numerous times within the database. The possibility exists, then, for variability within amino acid sequence, but similarity in three-dimensional structure between two peptides can be recognized by T cell clones. This, therefore, uncovers a flaw of such large databases. They may be able to give a hint to relationships between epitopes, but the important three-dimensional structure cannot yet be searched for in such a database.\n\nDespite no obvious amino acid sequence similarity from pathogen to host factors, structural studies have revealed that mimicry can still occur at the host level. In some cases, pathogenic mimics can possess a structural architecture that differs markedly from that of the functional homologues. Therefore, proteins of dissimilar sequence may have a common structure which elicits an autoimmune response. It has been hypothesized that these virulent proteins display their mimicry through molecular surfaces that mimic host protein surfaces (protein fold or three-dimensional conformation), which have been obtained by convergent evolution. It has also been theorized that these similar protein folds have been obtained by horizontal gene transfer, most likely from a eukaryotic host. This further supports the theory that microbial organisms have evolved a mechanism of concealment similar to that of higher organisms such as the African praying mantis or chameleon who camouflage themselves so that they can mimic their background as not to be recognized by others.\n\nDespite dissimilar sequence homology between self and foreign peptide, weak electrostatic interactions between foreign peptide and the MHC can also mimic self peptide to elicit an autoimmune response within the host. For example, charged residues can explain the enhanced on-rate and reduced off-rate of a particular antigen or can contribute to a higher affinity and activity for a particular antigen that can perhaps mimic that of the host. Similarly, prominent ridges on the floor of peptide-binding grooves can do such things as create C-terminal bulges in particular peptides that can greatly increase the interaction between foreign and self peptide on the MHC. Similarly, there has been evidence that even gross features such as acidic/basic and hydrophobic/hydrophilic interactions have allowed foreign peptides to interact with an antibody or MHC and TCR. It is now apparent that sequence similarity considerations are not sufficient when evaluating potential mimic epitopes and the underlying mechanisms of molecular mimicry. Molecular mimicry, from these examples, has therefore been shown to occur in the absence of any true sequence homology.\n\nThere has been increasing evidence for mimicking events caused not only by amino acid similarities but also in similarities in binding motifs to the MHC. Molecular mimicry is thus occurring between two recognized peptides that have similar antigenic surfaces in the absence of primary sequence homology. For example, specific single amino acid residues such as cysteine (creates di-sulfide bonds), arginine or lysine (form multiple hydrogen bonds), could be essential for T cell cross-reactivity. These single residues may be the only residues conserved between self and foreign antigen that allow the structurally similar but sequence non-specific peptides to bind to the MHC.\n\nEpitope spreading, also known as determinant spreading, is another common way in which autoimmunity can occur which uses the molecular mimicry mechanism. Autoreactive T cells are activated \"de novo\" by self epitopes released secondary to pathogen-specific T cell-mediated bystander damage. T cell responses to progressively less dominant epitopes are activated as a consequence of the release of other antigens secondary to the destruction of the pathogen with a homologous immunodominant sequence. Thus, inflammatory responses induced by specific pathogens that trigger pro-inflammatory T1 responses have the ability to persist in genetically susceptible hosts. This may lead to organ-specific autoimmune disease. Conversely, epitope spreading could be due to target antigens being physically linked intracellularly as members of a complex to self antigen. The result of this is an autoimmune response that is triggered by exogenous antigen that progresses to a truly autoimmune response against mimicked self antigen and other antigens. From these examples, it is clear that the search for candidate mimic epitopes must extend beyond the immunodominant epitopes of a given autoimmune response.\n\nThe HIV-1 virus has been shown to cause diseases of the central nervous system (CNS) in humans through a molecular mimicry apparatus. HIV-1 gp41 is used to bind chemokines on the cell surface of the host so that the virion may gain entrance into the host. Astrocytes are cells of the CNS which are used to regulate the concentrations of K and neurotransmitter which enter the cerebrospinal fluid (CSF) to contribute to the blood brain barrier. A twelve amino acid sequence (Leu-Gly-Ile-Trp-Gly-Cys-Ser-Gly-Lys-Leu-Ile-Cys) on gp41 of the HIV-1 virus (immunodominant region) shows sequence homology with a twelve amino acid protein on the surface of human astrocytes. Antibodies are produced for the HIV-1 gp41 protein. These antibodies can cross-react with astrocytes within human CNS tissue and act as autoantibodies. This contributes to many CNS complications found in AIDS patients.\n\nTheiler's murine encephalomyelitis virus (TMEV) leads to the development in mice of a progressive CD4 T cell-mediated response after these cells have infiltrated the CNS. This virus has been shown to cause CNS disease in mice that resembles multiple sclerosis, an autoimmune disease in humans that results in the gradual destruction of the myelin sheath coating axons of the CNS. The TMEV mouse virus shares a thirteen amino acid sequence (His-Cys-Leu-Gly-Lys-Trp-Leu-Gly-His-Pro-Asp-Lys-Phe) (PLP (proteolipid protein) 139-151 epitope) with that of a human myelin-specific epitope. Bystander myelin damage is caused by virus specific T1 cells that cross react with this self epitope. To test the efficacy in which TMEV uses molecular mimicry to its advantage, a sequence of the human myelin-specific epitope was inserted into a non-pathogenic TMEV variant. As a result, there was a CD4 T cell response and autoimmune demyelination was initiated by infection with a TMEV peptide ligand. In humans, it has recently been shown that there are other possible targets for molecular mimicry in patients with multiple sclerosis. These involve the hepatitis B virus mimicking the human proteolipid protein (myelin protein) and the Epstein-Barr virus mimicking anti-myelin oligodendrocyte glycoprotein (contributes to a ring of myelin around blood vessels).\n\nMyasthenia gravis is another common autoimmune disease. This disease causes fluctuating muscle weakness and fatigue. The disease occurs due to detectable antibodies produced against the human acetylcholine receptor. The receptor contains a seven amino acid sequence (Trp-Thr-Tyr-Asp-Gly-Thr-Lys) in the α-subunit that demonstrates immunological cross-reactivity with a shared immunodominant domain of gpD of the herpes simplex virus (HSV). Similar to HIV-1, gpD also aids in binding to chemokines on the cell surface of the host to gain entry into the host. Cross-reactivity of the self epitope (α-subunit of the receptor) with antibodies produced against HSV suggests that the virus is associated with the initiation of myasthenia gravis. Not only does HSV cause immunologic cross-reactivity, but the gpD peptide also competitively inhibits the binding of antibody made against the α-subunit to its corresponding peptide on the α-subunit. Despite this, an autoimmune response still occurs. This further shows an immunologically significant sequence homology to the biologically active site of the human acetylcholine receptor.\n\nThere are ways in which autoimmunity caused by molecular mimicry can be avoided. Control of the initiating factor (pathogen) via vaccination seems to be the most common method to avoid autoimmunity. Inducing tolerance to the host autoantigen in this way may also be the most stable factor. The development of a downregulating immune response to the shared epitope between pathogen and host may be the best way of treating an autoimmune disease caused by molecular mimicry. Alternatively, treatment with immunosuppressive drugs such as ciclosporin and azathioprine has also been used as a possible solution. However, in many cases this has been shown to be ineffective because cells and tissues have already been destroyed at the onset of the infection.\n\nThe concept of molecular mimicry is a useful tool in understanding the etiology, pathogenesis, treatment, and prevention of autoimmune disorders. Molecular mimicry is, however, only one mechanism by which an autoimmune disease can occur in association with a pathogen. Understanding the mechanisms of molecular mimicry may allow future research to be directed toward uncovering the initiating infectious agent as well as recognizing the self determinant. This way, future research may be able to design strategies for treatment and prevention of autoimmune disorders. The use of transgenic models such as those used for discovery of the mimicry events leading to diseases of the CNS and muscle disorders has helped evaluate the sequence of events leading to molecular mimicry.\n"}
{"id": "49546501", "url": "https://en.wikipedia.org/wiki?curid=49546501", "title": "Mr. Ouch", "text": "Mr. Ouch\n\nMr. Ouch is a hazard symbol developed by the National Electrical Manufacturers Association (NEMA) to represent electrical hazards. Unlike other high-voltage warning symbols, Mr. Ouch was specifically designed with young children in mind.\n\nMr. Ouch is similar in name, purpose, and appearance to the UPMC Children's Hospital of Pittsburgh's \"Mr. Yuk\" design used to label poisonous substances, although the two symbols were developed independently.\n\nMr. Ouch is anthropomorphized electricity. The design shows a snarling, spiderlike creature with jagged, lightning-bolt arms throwing a child backwards.\n\nIn early 1981, several member companies of NEMA began studying how to prevent young children from being electrocuted by electrical transformers. This followed incidents where transformer cabinets were vandalized or left unlocked, allowing access to the high-voltage equipment inside and resulting in disfigurement and death. NEMA realized that existing signage did not adequately convey the danger, either because it required literacy (text-only warnings) or because it was too abstract to register on a child (bolts of electricity).\n\nConcerned about \"failure to warn\" lawsuits, NEMA began exploring ways of warning young children about the dangers of exposed high-voltage equipment. Member companies within NEMA's Transformer Section formed a task force—the Task Force on Safety Labels for Pad-Mounted Switchgear and Transformers Sited in Public Areas—to design a safety label that very young children would understand, as well as standards on how that label was to be used.\n\nAt the beginning of the project, the task force reached out to transformer manufacturers in an effort to build consensus for standardizing a design. NEMA hired the Agnew Moyer Smith company of Pittsburgh, PA to design the label, and the George R. Fraich Associates Testing Organization of Chicago to test children's reactions to it. NEMA began testing the first iterations of the design in late 1981, initially choosing sixteen different illustrations to test on children. The test groups were located in Chicago, IL and San Antonio, Texas, and consisted of an equal number of English and non-English-speaking children, aged 2.5 to 6 years old. From that group of sixteen illustrations, NEMA selected the four most successful designs and conducted a second round of user tests in early 1982, using a group of children located in Chicago and identical in age range and language. Of those four illustrations, the one children most strongly associated with danger was Mr. Ouch. A 1995 study presented at the annual meeting of the Human Factors and Ergonomics Society confirmed that Mr. Ouch most effectively suggested an electrical hazard, out of four other symbols. \n\nMr. Ouch was the first warning label system developed and standardized by an industry.\n\nThe Mr. Ouch design is owned by NEMA. Usage guidelines for the signage are specified in NEMA Standards Publication 260-1996, \"Safety Labels for Pad-Mounted Switchgear and Transformers Sited in Public Areas\". Mr. Ouch labels are available from online suppliers, and are typically applied to pad-mounted transformers. Label designs frequently include the signal words \"Warning\" or \"Danger\" in accordance with ANSI-Z535.\n\nWhen first introduced in 1983, the Mr. Ouch design was not considered a replacement for OSHA warning signage, and had to accompany signage compliant with OSHA's existing ANSI-based standards in order to satisfy OSHA safety regulations. However, the Mr. Ouch label designs shown in the 1996 revision of NEMA-260 share many characteristics of OSHA's current ANSI-Z535-based safety standards—clear pictogram, caution symbol, and direct explanatory language—and NEMA-260-1996 does not specify that Mr. Ouch labels must be used in accordance with other signage.\n\nWisconsin Energies created an educational video for schools, to help children recognize Mr. Ouch.\n\n"}
{"id": "45100274", "url": "https://en.wikipedia.org/wiki?curid=45100274", "title": "Non-aqueous phase liquid", "text": "Non-aqueous phase liquid\n\nNon-aqueous phase liquids or NAPLs are liquid solution contaminants that do not dissolve in or easily mix with water (hydrophobic), like oil, gasoline and petroleum products.\n\nNAPLs tend to contaminate soil and groundwaters. Many common groundwater contaminants such as chlorinated solvents and many petroleum products enter the subsurface in nonaqueous-phase solutions. They do not mix readily with water and therefore flow separately from ground water.\n\nIf the NAPL is denser than water, like trichloroethylene, it is called DNAPL and will tend to sink once it reaches the water table. If it is lighter, like gasoline, it is called LNAPL and will tend to float on the water table. DNAPL penetrates below the water table like ice cubes in a glass of water. \n\n\n\n"}
{"id": "18561586", "url": "https://en.wikipedia.org/wiki?curid=18561586", "title": "Ovarian disease", "text": "Ovarian disease\n\nOvarian diseases are conditions that happen to young women and can affect their reproductive system and general health. \n\nThese can be classified as endocrine disorders or as a disorders of the reproductive system.\n\nIf the egg fails to release from the follicle in the ovary an ovarian cyst may form. Small ovarian cysts are common in healthy women. Some women have more follicles than usual (polycystic ovary syndrome), which inhibits the follicles to grow normally and this will cause cycle irregularities. \n\nThere are various types of ovarian diseases. Some of the ovarian diseases or disorders are:\n\n\nEndometriosis is a condition in which tissues lining the uterus grows abnormally beyond the uterus that may become quite painful. In simpler terms, it means that the tissue lining the uterus develops in different parts outside of it. \n\nCauses:\n\nThere is no exact cause of endometriosis. \n\nSymptoms: \n\nMenstrual cramps, heavy menstrual bleeding, bowel or urinary problems, nausea, vomiting, blood with stools, painful intercourse, fatigues, spotting or bleeding between periods. \n\nTreatment:\n\n\nIt is common for many women to develop one cyst in their lifetime. At times, these can go unnoticed without pain or visible symptoms. A cyst may develop in either of the ovaries that are responsible for producing hormones and carrying eggs in the bodies of women. Ovarian cysts can be of various types like dermoid cysts, endometrioma cysts and the functional cyst being the most common one \n\nSymptoms:\n\n\nTreatment:\n\n\nIt is one of the common ovarian cancers that affect women worldwide. It develops outside the ovaries and ultimately spreads outside and can affect other organs. \n\nCauses:\n\nIt may happen if there is a family medical history of breast cancer, colon cancer, rectal cancer or uterine cancer, Lynch syndrome. If someone is under the Estrogen Replacement Therapy for a long time. Smoking habits may also lead to the same. \n\nSymptoms:\n\nSymptoms are not very clear and for most of them, it comes to notice when it spreads throughout the abdomen. \n\nTreatment:\n\n\nOvarian germ cell tumors are common among teenagers and young women. It is a growth in the ovaries. \n\nCauses:\n\nThough the exact causes are not known, it may happen owing to certain birth defects affecting the genitals, nervous system or the urinary tract. There may be genetic conditions affecting the sex chromosomes that result in these kind of tumors as well.\n\nSymptoms\n\n\nTreatment:\n\n\nThe tumor forms in the ovaries and gradually spreads to the outside of ovary. This mostly affects younger women and also hinders the reproductive system. \n\nCauses:\n\nCauses are debatable and these may occur to both pregnant women and women who do not opt for pregnancy \n\nSymptoms:\n\n\nTreatment:\n\nDepending on the size of the tumor, choice of pregnancy, the spreading of the tumor, age and choices, removing the affected ovary is the most common treatment. In rare situations, the tumor is taken out of the ovary. Also, hysterectomy can be an option. \n\nThis is a condition where the hormone levels in a woman’s body get affected leading to more amounts of male hormones than needed. \n\nCauses:\n\nThe increased level of hormones may result in irregular menstrual cycle and diabetes and heart problem in the long run. It also affects the body in various ways like problem getting pregnant, sleep apnea, depression and anxiety, can enhance the risk of endometrial cancer. \n\nSymptoms:\n\n\nTreatment: \n\n\nOther conditions include:\n\n"}
{"id": "45366563", "url": "https://en.wikipedia.org/wiki?curid=45366563", "title": "Polyclinic", "text": "Polyclinic\n\nA polyclinic is a clinic that provides both general and specialist examinations and treatments to outpatients and is usually independent of a hospital. The term is not clearly defined and includes:\n\n\n"}
{"id": "17968101", "url": "https://en.wikipedia.org/wiki?curid=17968101", "title": "Position (obstetrics)", "text": "Position (obstetrics)\n\nIn obstetrics, position is the orientation of the fetus in the womb, identified by the location of the presenting part of the fetus relative to the pelvis of the mother. Conventionally, it is the position assumed by the fetus before the process of birth, as the fetus assumes various positions and postures during the course of childbirth.\n\nDepending upon which part of the fetus is expected to be delivered first (fetal presentation), there are many possible positions:\n\n\n\n\n"}
{"id": "51494341", "url": "https://en.wikipedia.org/wiki?curid=51494341", "title": "Premature thelarche", "text": "Premature thelarche\n\nPremature thelarche (PT) is a medical condition, characterised by isolated breast development in female infants. It occurs in females younger than 8 years, with the highest occurrence before the age of 2. PT is rare, occurring in 2.2-4.7% of females aged 0 to 2 years old. The exact cause of the condition is still unknown, but it has been linked to a variety of genetic, dietary and physiological factors.\n\nPT is a form of Incomplete Precocious Puberty (IPP). IPP is the presence of a secondary sex characteristic in an infant, without a change in their sex hormone levels. Central Precocious Puberty (CPP) is a more severe condition than IPP. CPP is the presentation of secondary sex characteristics, with a change in sex hormones due to alteration of the hypothalamic-pituitary-gonadal (HPG) axis.CPP is an aggressive endocrine disorder with harmful developmental consequences for the patient. At the presentation of PT, diagnostics are used to ensure it isn’t early stage CPP. CPP can be differentiated from PT through biochemical testing, ultrasounds and ongoing observation. There is no treatment for PT but regular observation is important to ensure it doesn’t progress to CPP. CPP diagnosis is important as treatment is necessary.\n\nPremature thelarche is breast hypertrophy before puberty. This form of hypertrophy is an increase in breast tissue. PT occurs in pre-pubecent females, under the age of 8, having a peak occurrence in the first two years of life. The breast development is usually bi-lateral: both breasts show development. In some cases development may be unilateral: one breast develops.\n\nThere are four patterns of PT development. Most patients have hypertrophy followed by complete loss of the excess breast tissue (51% of cases) or loss of most excess tissue, but some remains until puberty (36% of cases). Less commonly patients have ongoing patterns of thelarche: 9.7% suffer from a cyclic pattern where the size of the breast tissue varies over time, and 3.2% experience continual increase in tissue size.\n\nThe main symptom of PT is enlarged breast tissue in infants. Estrogen’s role in PT, also leads to increased bone age and growth in some cases. In PT these secondary symptoms are minimal: bone age only varies from actual age by a few months and growth velocity only slightly varies from the norm. Diagnostic tests will distinguish these PT secondary symptoms from the more severe bone aging and growth occurring in early CPP.\n\nThe direct pathophysiology behind PT is still unknown, but there are many postulated causes.\n\nPT is linked to increased sensitivity of the breast tissue to estradiol, an estrogen derivative, in certain prepubertal individuals. Sporadic estrogen or estradiol production in the adrenal glands, follicles or ovarian cysts is also linked to the condition.\n\nFollicle Stimulating Hormone (FSH) is secreted from the anterior pituitary. FSH plays a key role in development, growth and puberty, thus it is suspected to play a role in PT. Gondotropin-releasing hormone (GnRH) stimulation testing in some patients with PT has shown a dominant response from FSH. This response is linked to active mutations in the FSH receptor and Gs-a subunit in PT. Genetic investigation indicated these mutations only account for few cases of premature PT. PT may also be caused by transient partial activation of the HPG axis. Partial activation would release a surplus of FSH from the anterior pituitary without further disruption of the HPG axis.\n\nThe consumption or exposure to certain endocrine disrupters have also been linked to PT.\n\nPT is the benign growth of breasts in infants, while CPP is a condition that involves the frequent activation of the HPG axis in patients. PT does not require treatment, as the condition is limited to enlarged breast tissue that usually subsides with time. CPP is associated with a wider range of symptoms including thelarche, pubic hair growth, accelerated bone aging, increased growth velocity and early epiphyseal growth. If an individual is affected with CPP they will need to begin treatment immediately. CPP is treated with lutenizing hormone (LH) releasing hormone agonists. PT can impact growth velocity and bone age slightly, but CPP affects these characteristics to the point of detriment to the adult stature. Patients with suspected PT must undergo diagnostic testing to ensure it isn’t CPP or exaggerated thelarche, the intermediate stage before CPP.\n\nNotable hormone differences occur between CPP and PT patients, so studying these hormone levels is the main biochemical diagnostic used in CPP. Individuals with CPP usually have a higher basal LH levels and LH:FSH ratios.\n\nFew PT patients, 9 to 14%, are predicted to develop CPP. Observation allows clinicians to identify the presentation of CPP indicative symptoms in PT patients. No diagnostics tests can indicate if a PT patient is at risk of developing CPP.\n\nPremature thelarche does not require treatment. In PT, breast hypertrophy will usually stop completely and patients will experience regression of the breast tissue over 3 to 60 months. Less commonly, patients may remain with residual breast tissue or continue through cycles of breast hypertrophy and regression until puberty.\n\nDiagnostics are utilised in individuals with PT, especially at the presentation of other secondary sex characteristics. Diagnostics aim to ensure PT patients are not suffering from CPP.\n\nPelvic ultrasounds are important in diagnosing CPP. Patients with CPP have an increased ovary and uterus size. The ovary and uterus volume of CPP patients is similar to that of females undergoing puberty. The pelvis ultrasound is problematic as a diagnostic, as there is not a specific cut-off for the uterine and ovary volumes that indicate the patient has CPP. Patients with PT should have a uterine and ovarian volume within the normal range for their age. Pelvic ultrasounds are a desirable diagnostic as they are non-invasive and easy to continually review. The pelvic ultrasound should be paired with biochemical tests to determine the presence of CPP.\n\nBiochemical tests study the hormone levels in patients. CPP patients have elevated LH levels and peak LH:FSH ratios when compared to PT patients. It is hard to use LH as a diagnostic for CPP, as the LH assay has varying sensitivity and specificity. The GnRH stimulation test is the main diagnostic biochemical test used to distinguish PT from CPP. The GnRH test demonstrates the pituitary responsiveness to GnRH. GnRH stimulates the release of LH and FSH from the anterior pituitary. The peak LH:FSH ratio in CPP patients is similar to the ratio of pubertal females. Females with PT demonstrated a LH:FSH ratio lower than pubertal females. The disadvantages of the GnRH stimulation test is it takes a long time to perform and requires multiple collections from the patient, making the process time consuming and inconvenient. The test is highly specific but has low sensitivity as the LH hormone response is usually observed in later stages of CPP. There are also overlaps in the expected value in the GnRH test results of individuals with CPP and PT.\n\nThe diagnostic inconsistency in CPP means that a combination of all of pelvic ultrasounds and biochemical tests should be paired with observation, to ensure PT doesn’t progress to CPP.\n\nNatural commodities like fennel, lavender and tea tree oils have been linked to PT. Lavender and tea tree oil have weak estrogenic activities. These estrogenic properties may cause an imbalance in endocrine signalling pathways, leading to PT in regular users of these products. Fennel tea has been studied as an endocrine disrupter linked to PT. Fennel seed oil contains anethole a compound with estrogenic effects. The tea contains fennel seed oil and regular use results in increased estradiol levels in the infant. Infants with fennel tea related PT, were given the tea as a homeopathic remedy for restlessness. The tea was consumed for at least four months before the presentation of PT symptoms. PT resulting from fennel tea subsides approxiamately six months after stopping the use of fennel tea.\n\nLeptin is an adipocyte hormone that has important implications of puberty and sex hormone secretion. Increased leptin has been linked to estrogen and estradiol secretion. Leptin has key roles in maintaining age appropriate body composition and desired weight. Leptin receptors are also found in mammary epithelial cells and leptin has been observed as a growth factor in breast tissue. Increased leptin levels have been observed in some cases of PT. The increase in leptin levels cause increased estradiol levels and development of breast tissue.\n\nThe form of PT with fluctuating hypertrophy in patients has been linked to activating mutations in the GNAS1 gene. This mutation accounts for a small number of cases of PT.\n\n"}
{"id": "4259042", "url": "https://en.wikipedia.org/wiki?curid=4259042", "title": "Saint Louis University School of Medicine", "text": "Saint Louis University School of Medicine\n\nSaint Louis University School of Medicine (SLUSOM) is a private, American Medical School within Saint Louis University. Located in the city of St. Louis, Missouri, Saint Louis University School of Medicine was established in 1836 and has the distinction of awarding the first M.D. degree west of the Mississippi River.\n\nThe school comprises about 700 medical students, 550 faculty members and 550 residents in 48 graduate medical education programs including residencies, subspecialty residencies and fellowships. The School is a pioneer in geriatric medicine, chronic disease prevention, cardiovascular disease, organ transplantation, neurosciences and vaccine research among others. It is a leading center of research in five key areas: cancer, infectious disease, liver disease, aging and brain disorders, and heart/lung disease.\n\nIt provides health services on a local, national, and international level while conducting medical research and training physicians and biomedical scientists of the future. Cardinal Glennon Children's Hospital and Saint Louis University Hospital are the two main affiliated teaching hospitals of the school.\n\nSaint Louis University School of Medicine was established in 1836 as the Medical Department of the University and had the distinction of awarding the first M.D. degree west of the Mississippi River in 1839. Several affiliated doctors of national importance include William Beaumont, whose pioneering studies of the human digestive system opened a new world of research, and Daniel Brainerd, who later founded Rush Medical College (then part of the University of Chicago). Cardinal Glennon Children's Hospital and Saint Louis University Hospital are the two main affiliated teaching hospitals of the school.\n\nThe Know-Nothing movement, an anti-immigrant and subsequently anti Catholic movement that surged through the United States in the 1840s and 1850s eventually led to the separation of the University's Medical Department from the University in 1854. As a result, the University was without a medical school for 59 years until the presidency of Father William Banks Rogers (1900 to 1908), during which plans were initiated for the integration of a new medical school into the University.\n\nIn 1903, the Marion Sims-Beaumont College of Medicine was incorporated into the University. At this time, Marion Sims-Beaumont College was a medical school owned and operated by a group of St. Louis physicians. The college's decision to merge with the University was greatly influenced and reinforced by the recommendations of the Council on Medical Education and Hospitals of the American Medical Association, which insisted that all schools of medicine be affiliated with a University. Assured of financial support from the St. Louis civic leader Festus J. Wade, President Rogers successfully secured the needed funds for the purchase of the Marion Sims-Beaumont College.\n\nSince 1994, the students and physicians of the School of Medicine have been committed to providing free primary healthcare services in an academic environment through their Health Resource Center. They currently operate 8 free clinics throughout the Saint Louis area including an adult clinic, pediatric clinic, well woman clinic, cardiology clinic, diabetes clinic, homeless health clinic, extended services clinic, and their most recent addition, the asthma and allergy clinic.\n\nIn 2007, a new research center was added to the university. Made of glass and steel in just 522 working days, the 10-story tower now stands at the northern gateway to the University's Medical Center. This 206,000 square-foot building is named after the late Dr. Edward Adelbert Doisy, who was not only a Nobel Laureate but also a professor at Saint Louis University for five decades. \"The University's $82 million Edward A. Doisy Research Center is now the new home of Saint Louis University researchers working in five key areas of scientific discovery: cancer and molecular biology; liver disease; cardiovascular disease; neurosciences and aging; and vaccine development\".\n\nEffective April 1, 2008, Philip O. Alderson, former chairman of the radiology department at Columbia University, became the 12th dean of the Medical School.\n\nThe medical school continues to be a leader in research in many fields, especially in emerging diseases, neuroscience, organ transplantation, vaccine development, cardiac health, and afflictions of the liver. Currently, numerous cutting edge research projects are underway including those examining aging and brain disorders, Alzheimer's disease, multiple sclerosis, biodefense, influenza, respiratory syncytial virus (RSV) and tuberculosis vaccines. Actually, the Saint Louis School of Medicine is one of only eight NIH-funded vaccine research institutions, and made significant contributions to the research and development of the H1N1 influenza vaccine. Furthermore, the Saint Louis University Liver Center is a national leader in the field of hepatology and is also considered one of the largest hepatitis C practices in the world.\n\nAdmissions to Saint Louis University School of Medicine is highly selective. Matriculates had an average GPA of 3.84 and an average MCAT score of 33 (Medical School Admission Requirements 2014 edition). Apart from these academic characteristics, the admissions committee recognizes a responsibility to consider applicants as individuals, particularly in the evaluation of the breadth of their educational experience, their personality traits, maturity level, and appropriate motivation and commitment to a career in medicine. For the most recent class to matriculate at Saint Louis University School of Medicine, 8,518 applicants competed for 181 seats. \n\nSaint Louis University School of Medicine is currently ranked 70th for research by the 2019 edition of \"U.S. News and World Report\" of the 149 fully accredited U.S. medical schools (by the Liaison Committee on Medical Education and the American Osteopathic Association). Additionally, Saint Louis University School of Medicine was recently ranked 54 among the nation's 130 medical schools surveyed and the school's geriatrics program was ranked number 13 in the nation.\n\n"}
{"id": "26524912", "url": "https://en.wikipedia.org/wiki?curid=26524912", "title": "Smoking in France", "text": "Smoking in France\n\nSmoking in France was first restricted on public transport by the 1976 Veil law. Further restrictions were established in the 1991 Évin law, which contains a variety of measures against alcoholism and tobacco consumption. A much stronger smoking ban was introduced on 1 February 2007. Smoking in enclosed public places such as offices, schools, government buildings and restaurants is strictly prohibited. Law officials may enforce the laws with minimum fines set at €500.\n\nThe Veil law is named after Simone Veil, the French activist health minister, who took an initiative to fight against tobacco smoking in France in 1976. Veil banned advertising for tobacco or tobacco products and required tobacco companies to print severe warnings on their cigarette packages, such as \"\"Abus Dangereux\" – [Overuse is Hazardous].\" Another significant aspect of the Veil Law was to place limitations on smoking places \"affectés à un usage collectif\" (open to the public).\n\nThe Évin law is named after Claude Évin, the minister who pushed for it. The law leaves certain important criteria on what is allowed or not with respect to smoking sections to executive-issued regulations, and it is those regulations that were altered in 2007.\nA legal challenge against the new regulations was filed before the Conseil d'État in 2007, but was rejected. Under the initial implementation rules of the 1991 Évin law, restaurants, cafés etc. just had to provide smoking and non-smoking sections, which in practice were often not well separated. In larger establishments, smoking and non-smoking sections could be separate rooms, but often they were just areas within the same room.\n\nSmoking and vaping are banned in all indoor public places (government buildings, offices, public transport, universities, museums, restaurants, cafés, nightclubs, etc.). Cafés and shops selling tobacco-related products are submitted to the same regulations. No exceptions exist for special smoking rooms fulfilling strict conditions.\nAdditionally, some outdoor public places also ban smoking and vaping (railway stations).\n\nAs of 2015, 32% of French adults declare themselves to be regular smokers.\n\nIn case of violation of tobacco laws, the smoker can face a fine up to €450 and the owner of the venue up to €750.\n\nPlain packaging was introduced in 2017.\n\n"}
{"id": "2601204", "url": "https://en.wikipedia.org/wiki?curid=2601204", "title": "Spinalonga", "text": "Spinalonga\n\nThe island of Spinalonga (), officially known as Kalydon (Καλυδών), is located in the Gulf of Elounda in north-eastern Crete, in Lasithi, next to the town of Plaka. The island is further assigned to the area of Kalydon.\nIt is near the Spinalonga peninsula (\"large Spinalonga\") – which often causes confusion as the same name is used for both. The official Greek name of the island today is Kalydon.\n\nOriginally, Spinalonga was not an island – it was part of the island of Crete. During Venetian occupation the island was carved out of the coast for defence purposes and a fort was built there. \n\nDuring Venetian rule, salt was harvested from salt pans around the island. The island has also been used as a leper colony. Spinalonga has appeared in novels, television series, and a short film.\n\nAccording to Venetian documents, the name of the island originated in the Greek expression στην Ελούντα \"stin Elounda\" (meaning \"to Elounda\"). The Venetians could not understand the expression so they familiarized it using their own language, and called it \"spina\" \"thorn\" \"longa\" \"long\", an expression that was also maintained by the locals. The Venetians were inspired for this expression by the name of an island near Venice called by the same name and which is known today as the island of Giudecca.\n\nThe Venetian cartographer Vincenzo Coronelli reports that Spinalonga was not always an island, but was once linked with the adjacent Peninsula Spinalonga. He mentions that in 1526, the Venetians cut down a portion of the peninsula and thus created the island. Because of its position the island was fortified from its earliest years in order to protect the entranceway of the port of Ancient Olous.\n\nOlous, and accordingly the wider region, were depopulated at the middle of the 7th century because of the raids of the Arab pirates in the Mediterranean. Olous remained deserted until the mid-15th century when the Venetians began to construct salt-pans in the shallow and salty waters of the gulf. Subsequently, the region acquired commercial value and became inhabited. This, in combination with the emergent Turkish threat, particularly after the Fall of Constantinople in 1453, and the continuous pirate raids, forced the Venetians to fortify the island.\n\nIn 1578 the Venetians charged the engineer Genese Bressani to plan the island's fortifications. He created blockhouses at the highest points of the northern and southern side of the island, as well as a fortification ring along the coast that closed out any hostile disembarkation. In 1579, the \"Provveditore generale di Candia\", Luca Michiel, put the foundation stone of the fortifications, built over the ruins of an acropolis. There are two inscriptions that cite this event, one on the transom of the main gate of the castle and the other on the base of the rampart at the north side of the castle. In 1584, the Venetians, realising that the coastal fortifications were easy to conquer by the enemies attacking from the nearby hills, decided to strengthen their defense by constructing new fortifications at the top of the hill. The Venetian fire would thus have bigger range, rendering Spinalonga an impregnable sea fortress, one of the most important in the Mediterranean basin.\n\nSpinalonga, along with Gramvousa and Souda, remained in Venetian hands even after the rest of Crete fell to the Ottomans in the Cretan War (1645–1669) and until 1715, when they fell to the Ottomans during the last Ottoman–Venetian War. These three forts defended Venetian trade routes and were also useful bases in the event of a new Venetian-Turkish war for Crete. Many Christians found refuge in these fortresses to escape persecution from the Ottoman Turks.\n\nIn 1715, the Ottoman Turks captured Spinalonga taking over the last remaining Venetian fortress and removing the last trace of Venetian military presence from the island of Crete.\n\nAt the end of the Ottoman occupation the island, together with the fort at Ierapetra, was the refuge of many Ottoman families that feared Christian reprisals. After the revolution of 1866 other Ottoman families came to the island from all the region of Mirabello. During the Cretan revolt of 1878, only Spinalonga and the fortress at Ierapetra were not taken by the Christian Cretan insurgents. In 1881 the 1112 Ottomans formed their own community and later, in 1903, the last Turks left the island.\n\nThe island was subsequently used as a leper colony from 1903 to 1957. It is notable for being one of the last active leper colonies in Europe. The last inhabitant, a priest, left the island in 1962. This was to maintain the religious tradition of the Greek Orthodox church, in which a buried person has to be commemorated at following intervals of 40 days; 6 months; 1 year; 3 years; and 5 years, after their death. Other leper colonies that have survived Spinalonga include Tichileşti in Eastern Romania, Fontilles in Spain and Talsi in Latvia. As of 2002, few lazarettos remain in Europe.\n\nThere were two entrances to Spinalonga, one being the \"lepers' entrance\", a tunnel known as \"Dante's Gate\". This was so named because the patients did not know what was going to happen to them once they arrived. However, once on the island they received food, water, medical attention and social security payments. Previously, such amenities had been unavailable to Crete's leprosy patients, as they mostly lived in the area's caves, away from civilization.\n\nToday, the uninhabited island is a popular tourist attraction in Crete. In addition to the abandoned leper colony and the fortress, Spinalonga is known for its small pebble beaches and shallow waters. The island can easily be accessed from Plaka, Elounda and Agios Nikolaos. Tourist boats depart from all three towns on a daily basis (every 30 minutes from Elounda). There is no accommodation on Spinalonga, meaning all tours last only a few hours. Boat trips from Elounda take approximately 25 minutes while trips departing Agios Nikolaos can take almost an hour. Round trip fare from Plaka is 8 Euro, from Elounda 10 Euro. Entrance to the island costs 8 Euro/adult.\n\nSpinalonga featured in the British television series \"Who Pays the Ferryman?\" and Werner Herzog's experimental short film \"Last Words\". It is the (unnamed) setting of Ali Smith's short story \"The Touching of Wood\" (in \"Free Love and Other Stories\", 1995). It is also the setting for the 2005 novel \"The Island\" by Victoria Hislop, the story of a family's ties to the leper colony; the book was adapted for television in the television series \"To Nisi\" by Mega Channel Greece.\n\nThe short story \"Spinalonga\" by John Ware, about a tourist group that visits the island, was included in the 13th Pan Book of Horror.\n\n\n"}
{"id": "54439024", "url": "https://en.wikipedia.org/wiki?curid=54439024", "title": "Squamous cell carcinoma", "text": "Squamous cell carcinoma\n\nSquamous cell carcinomas (SCCs), also known as epidermoid carcinomas, comprise a number of different types of cancer that result from squamous cells. These cells form the surface of the skin and lining of hollow organs in the body and line the respiratory and digestive tracts.\n\nCommon types include:\n\nDespite sharing the name \"squamous cell carcinoma\", the SCCs of different body sites can show differences in their presented symptoms, natural history, prognosis, and response to treatment.\n\nHuman papillomavirus infection has been associated with SCCs of the oropharynx, lung, fingers, and anogenital region.\n\nAbout 90%t of cases of head and neck cancer (cancer of the mouth, nasal cavity, nasopharynx, throat and associated structures) are due to SCC.\n\nPrimary squamous cell thyroid carcinoma shows an aggressive biological phenotype resulting in poor prognosis for patients.\n\nEsophageal cancer may be due to either esophageal squamous cell carcinoma (ESCC) or adenocarcinoma (EAC). SCCs tend to occur closer to the mouth, while adenocarcinomas occur closer to the stomach. Dysphagia (difficulty swallowing, solids worse than liquids) and painful swallowing are common initial symptoms. If the disease is localized, surgical removal of the affected esophagus may offer the possibility of a cure. If the disease has spread, chemotherapy and radiotherapy are commonly used.\n\nWhen associated with the lung, it is typically a centrally located large-cell cancer (nonsmall-cell lung cancer). It often has a paraneoplastic syndrome causing ectopic production of parathyroid hormone-related protein, resulting in hypercalcemia, but paraneoplastic syndrome is more commonly associated with small-cell lung cancer. It is primarily due to smoking.\n\nHuman papillomavirus (HPV), primarily HPV 16 and 18, are strongly implicated in the development of SCC of the penis.\nThree carcinomas \"in situ\" are associated with SCCs of the penis:\n\n\nWhen associated with the prostate, squamous cell carcinoma is very aggressive in nature. It is difficult to detect as no increase in prostate-specific antigen levels is seen, meaning that the cancer is often diagnosed at an advanced stage.\n\nVaginal SCC spreads slowly and usually stays near the vagina, but may spread to the lungs and liver. This is the most common type of vaginal cancer.\n\nMost bladder cancer is transitional cell, but bladder cancer associated with schistosomiasis is often SCC.\n\nCancer can be considered a very large and exceptionally heterogeneous family of malignant diseases, with squamous cell carcinomas comprising one of the largest subsets. All SCC lesions are thought to begin via the repeated, uncontrolled division of cancer stem cells of epithelial lineage or characteristics. SCCs arise from squamous cells, which are flat cells that line many areas of the body. Accumulation of these cancer cells causes a microscopic focus of abnormal cells that are, at least initially, locally confined within the specific tissue in which the progenitor cell resided. This condition is called squamous cell carcinoma \"in situ\", and it is diagnosed when the tumor has not yet penetrated the basement membrane or other delimiting structure to invade adjacent tissues. Once the lesion has grown and progressed to the point where it has breached, penetrated, and infiltrated adjacent structures, it is referred to as \"invasive\" squamous cell carcinoma. Once a carcinoma becomes invasive, it is able to spread to other organs and cause the formation of a metastasis, or \"secondary tumor\".\n\nThe International Classification of Diseases for Oncology (ICD-O) system lists a number of morphological subtypes and variants of malignant squamous cell neoplasms, including:\n\nOther variants of SCCs are recognized under other systems, such as keratoacanthoma.\n\n\nOne method of classifying squamous cell carcinomas is by their appearance under microscope. Subtypes may include:\n\nSCC is a histologically distinct form of cancer. It arises from the uncontrolled multiplication of cells of epithelium, or cells showing particular cytological or tissue architectural characteristics of squamous cell differentiation, such as the presence of keratin, tonofilament bundles, or desmosomes, structures involved in cell-to-cell adhesion.\n\nStudies have found evidences for an association between diet and skin cancers, including SCC. The consumption of high-fat dairy foods increases SCC tumor risk in people with previous skin cancer. Green leafy vegetables may help prevent development of subsequent SCC and multiple studies found that raw vegetables, citrus fruits and noncitrus fruits are significantly protective against SSC risk. On the other hand, consumption of whole milk, yogurt, and cheese may increase SCC risk in susceptible people. In addition, meat and fat dietary pattern can increase the risk of SCC in people without a history of SCC, but the association is again more prominent in people with a history of skin cancer. Tobacco smoking and a dietary pattern characterized by high beer and liquor intake also increase the risk of SCC significantly.\n"}
{"id": "2211873", "url": "https://en.wikipedia.org/wiki?curid=2211873", "title": "St John Ambulance Australia", "text": "St John Ambulance Australia\n\nSt John Ambulance Australia (also known as St John ) (SJAA) is a self-funding charitable organisation dedicated to helping people in sickness, distress, suffering or danger. It is part of an international organisation that consists of eight Priories that form the Order of St John. The organisation is sometimes incorrectly referred to as \"St John's Ambulance\" instead of \"St John Ambulance\".\n\nSt John First Aid training centres were established in Australia in the late 19th century. On 13 June 1883 a public meeting was held in the Melbourne Town Hall to form a local branch of the association. By the end of June 1883, a centre had been established under the leadership of Edward Neild.\n\nThe first division of the St John Ambulance Brigade (now known as St John Ambulance Event Health Services) was established in Glebe, New South Wales in 1903. A division of this organisation is still in operation today and is known as St John Ambulance Glebe Division. After this initial division was established other states followed suit, with divisions being set up in other states soon after. In 1987, the organisation adopted a single public title, \"St John Ambulance Australia\". The cadet movement was established in Australia in 1925 with a division in Glebe, NSW. The first Grand Prior's Badge issued outside the UK went to a cadet from Marrickville Cadet Division in 1933 named Marion Higgins.\n\nThe organisation is divided into the states/territories who have their own boards and oversee the day-to-day running of St John. Some states are also divided into regions, who oversee all branches and report to the state boards. St John Ambulance Australia has three main branches, with each one having its own specific area of operation.\n\nSt John provides volunteer event health & first aid services at events and emergencies. First Aid Services is divided into states, regions and divisions.\n\nEvents covered by St. John Australia include sports, such as the 2006 Melbourne Commonwealth Games, where a team of 500 members treated over 3000 casualties. Other events covered include sporting events, such as the Australian Open, music concerts and community fetes.\n\nSt John EHS volunteers support state emergency management and disaster plans in some states, in conjunction with other organisations like the State Emergency Service.\n\nSt John is the largest first aid training organisation in Australia. Apart from its flagship Provide First aid (a workplace first aid course), St John also offer instruction in topics such as Advanced Resuscitation, Advanced First Aid, AED (Automated External Defibrillation) and analgesic administration, remote first aid and occupational first aid.\n\nSt John Ambulance in Australia raises funds and recruits staff for the St John Ophthalmic Eye Hospital in Jerusalem. Funds are raised through public donations, and income received from conducting first aid courses and selling first aid kits and merchandise.\n\nSt John Community Care conducts programs that are specific to each state. These activities range from assisting disadvantaged youth, to providing voluntary transport and support programs for the frail and elderly.\n\nIn Western Australia St John Ambulance provides the statutory ambulance service. This service is provided through a combination of paid and volunteer staff. Paid ambulance officers and paramedics are used in the metropolitan areas and larger regional centres. Volunteer ambulance officers are used in regional areas and some outer metropolitan areas.\n\nIn the Northern Territory St John Ambulance provides the statutory ambulance service. This service is provided through a combination of paid and volunteer staff. Paid ambulance officers and paramedics are used in the metropolitan areas and larger regional centres. Volunteer ambulance officers are used in regional areas. Aeromedical role is offered by CareFlight operating an AW 139 out of Darwin.\n\n^ Youth Leaders are adult members who have undertaken a specific leadership course and are ranked above youth but below all adult ranks\n\n\nSt John runs Cadet Divisions for children aged 8–17, this includes Juniors (8-11) and Cadets (11-17). These can be found in most towns or suburbs of major cities in Australia. Examples are Glebe Division and Bathurst Division in New South Wales, Greater Dandenong Division in Victoria and Playford Cadet Division in South Australia.\n\nThe youth program in Australia, focuses on developing young people in a variety of aspects. Young members are taught first aid and participate in youth development and social activities. For cadets, it is also possible to study for various 'badges'. Some of the topics available include counter-disaster, animal care and cookery.\n\nAcross most divisions, youth and cadet divisions meet once a week, in a designated place, to conduct a training night. There is a designated training program for youth and cadet divisions\n\nYouth members within Event and Emergency First Aid Service programs attend public duties to provide first aid at various events to members of the public. These duties include things such as: Big Day Out, Royal Easter Show (NSW), National Folk Festival (ACT), AFL Games (All AFL states), NRL, Super 12, and Rugby Union games, amongst other popular events. There are also many more lower profile events, like local fetes and markets. At these duties, St John members use treatment tools such as oxygen therapy equipment, defibrillators and analgesic gases on top of the standard first aid equipment.\n\nIn most states, new youth members (minimum age 14) will be put through a Senior First Aid Course (SFA) free of charge.\n\nSt John youth also provides leadership opportunities for people of all ages. The program possesses a leadership program and a ranking system similar to the military.\n\nFirst aid competitions are also held each year. In these competitions, cadets (in teams of up to 3, or on their own) are tested on their first aid skill, practical thinking and problem solving ability, and scene management skills. A national competition is held every year, at the National Cadet Camp.\n\nEach State and Territory is encouraged to facilitate provision of a Youth Council, the National Office also facilitates the Australian Youth Council. Broadly, Youth Councils provide guidance to St John on issues affecting the organisation and its future development, particularly concerning the opinion and interests of young people.\n\nIn 2006, the Australian Youth Council (AYC) restructured to be made up of 16 State/Territory representatives (nominated by their State/Territory, including the State/Territory Chairperson and another representative) and 5 National portfolio holders, including a Communications Coordinator, Training and Leadership Coordinator, Research Development Coordinator, Policy Coordinator and Australian Chair. The AYC Chair also sits as a full member of the National Board of Directors for St John Ambulance Australia. The aims of the Australian Youth Council include:\n\n\nThe AYC usually meets in person once or twice a year, usually including the National Conference or Priory in June, and another meeting (Youth Stakeholders Weekend) later in the year, as well as teleconferences during the year which may include either the entire Council or the National Team.\n\nYouth councils consist of young people in the organisation who are aged 12 to 25 years.\n\n\n\n"}
{"id": "12528903", "url": "https://en.wikipedia.org/wiki?curid=12528903", "title": "Sustainable Sanitation Alliance", "text": "Sustainable Sanitation Alliance\n\nThe Sustainable Sanitation Alliance (SuSanA) is a loose network of organizations who are \"working along the same lines towards achieving sustainable sanitation\". It began its work in 2007, one year before the United Nation's International Year of Sanitation in 2008. The intention of creating SuSanA was to have a joint label for the planned activities for 2008 and to align the various organizations for further initiatives.\n\nSuSanA has over 340 partner organizations and over 10,000 individual members (as of November 2018). SuSanA's vision document contains a definition of sustainable sanitation which was developed by SuSanA partners in 2007.\n\nSuSanA is not an NGO (non-governmental organization). It has no legal structure and takes no membership fees. It encourages other organizations to join the network and to become active members in the thematic working groups.\n\nThe SuSanA secretariat is funded by the German Ministry for Economic Cooperation and Development which has commissioned the Deutsche Gesellschaft für Internationale Zusammenarbeit (GIZ) for this task. Other SuSanA partners make contributions for example by paid time of their staff members.\n\nSuSanA is dedicated to achieving the Sustainable Development Goals, and in particular SDG6 (Goal Number 6) which is \"water and sanitation for all\". This is done by promoting sustainable sanitation systems. These systems should be \"economically viable, socially acceptable, technically and institutionally appropriate, and protect health, the environment and natural resources\".\n\nSuSanA is one of several knowledge management platform in the WASH sector such as the LinkedIn Discussion Group \"Community of Practice on Sanitation and Hygiene in Developing Countries\" by WSSCC, Blue Planet, International Water Association (IWA), Akvopedia and others.\n\nSince 2007, SuSanA has held 23 meetings in different locations around the world. Each year one meeting takes place before or after the World Water Week in Stockholm, and a further meeting usually takes place in the Global South, connected to another WASH event. SuSanA also organises side events, seminars and working group meetings in conjunction with other major WASH conferences.\n\nSuSanA members are contributing to Wikipedia articles on WASH-related topics. They are particularly active just before two international observance days: World Water Day on 22 March and World Toilet Day on 19 November. They have also set up a list of \"List of abbreviations used in sanitation\". On their Twitter account they promote these activities by using the hashtag: #edit4toilets.\n\nSuSanA has no legal structure, budget nor income. Partners contribute time and resources from their own budgets. The SuSanA secretariat is funded by the German Ministry for Economic Cooperation and Development (BMZ) who has commissioned the Deutsche Gesellschaft für Internationale Zusammenarbeit (GIZ) GmbH. Between 2012 and 2018, co-funding for the online Discussion Forum and other improvements to the SuSanA platform was provided by the Bill and Melinda Gates Foundation.\n\nSeveral active core group partners, for example SEI, seecon, BORDA, Swiss Federal Institute of Aquatic Science and Technology, IWA, WASTE have also funded various travel costs of SuSanA members, seminars, the printing of SuSanA publications and so forth.\n\nThe activities of the SuSanA network have contributed to increasing awareness about sustainability in the sanitation sector. SuSanA members were active in the Post-2015 Development Agenda and helped to shape the Sustainable Development Goals where Goal Number 6 now includes a goal of universal use of sustainable sanitation services that protect public health and dignity.\n\nOther actors have picked up on the theme of innovative sanitation (often with reuse of excreta in some form), most notably the Bill and Melinda Gates Foundation. Sustainable sanitation has become a topic in the nexus (water, energy, food) dialogue as well as in the WASH and nutrition theme.\n\nSuSanA has 13 thematic working groups covering areas of sustainable sanitation where conceptual and knowledge management work is required:\n\nSuSanA has over 340 partner organizations (as of November 2018). The partners are of the following types: Local NGO, International NGO, private sector, research and education, governmental / state-owned organization, multi-lateral organizations, associations and networks and others. All prospective new SuSanA partner organizations have to agree to the vision document when they join.\n\nA network analysis study conducted in 2014 assessed the SuSanA network by examining the communication channels used and the quality of relationships among partners. It found that \"SuSanA partners have strong levels of trust, cooperation and information exchange with one another\". However, partners seem to have low diversity of relationships with partners in different economic zones, such as developing countries versus developed countries. Many of the partners use their membership primarily to receive information from the discussion forum.\n\nIndividuals can join as members. There are nearly 9000 members (as of November 2017).\n\nSuSanA has been criticized by some in the WASH (Water, Sanitation, Hygiene) sector for a perceived dominance of the ecosan theme in SuSanA. This is due to the strong focus of two of its founding organizations on ecosan: Stockholm Environment Institute and Deutsche Gesellschaft für Internationale Zusammenarbeit (GIZ) GmbH. Others have criticised SuSanA for being too focussed on technologies and sanitation systems (rather than on non-technical issues); that it is too dominated by people from the Global North; too dominated by GIZ who leads the secretariat; and too theoretical and far removed from the realities on the ground.\n\nSuSanA has no regional nodes, offices or secretariats. It also has limited impact so far in the non-English speaking parts of the world, notably Latin America, Russia or Central Asia.\n\nThe SuSanA core group has reacted to these criticisms by defining a mission statement in 2014, a roadmap for 2013 onwards and by hosting an open discussion forum where such issues can be discussed. Recommendations made in 2014 to the SuSanA network for its future development and to further develop relationships among partners include: Continue to hold meetings in different locations around the world, establish regional nodes, re-activate the working groups, and create more active members through engagement.\n\nSuSanA started in January 2007 with a first meeting in Eschborn, Germany at Deutsche Gesellschaft für Internationale Zusammenarbeit (GIZ) GmbH, an international enterprise owned by the German Federal Government. GIZ agreed to host SuSanA's secretariat and has been doing so since 2007. The reason why SuSanA was started in 2007 was to prepare for the International Year of Sanitation in 2008, and to align the organizations active in sustainable sanitation.\n\n\n"}
{"id": "47176071", "url": "https://en.wikipedia.org/wiki?curid=47176071", "title": "T-3000", "text": "T-3000\n\nThe T-3000 is a fictional cyborg assassin, serving as the primary antagonist in \"Terminator Genisys\", the fifth installment in the \"Terminator\" series, portrayed by Jason Clarke. In the film, the T-3000 is an alternate timeline counterpart of Skynet's (portrayed by Matt Smith) nemesis John Connor (also portrayed by Clarke), created after Skynet infects a variant of Connor with nanotechnology and fractures the timeline. T-3000 also serves as a foil personality to \"Guardian\" (a reprogrammed T-800 portrayed by Arnold Schwarzenegger), a protagonist who is somewhat similar to T-3000 but also opposite in many ways, of their relationship dynamics with Sarah Connor (portrayed by Emilia Clarke) and Kyle Reese (portrayed by Jai Courtney).\n\nThe T-3000's sole mission is to protect and ensure the ultimate survival of Skynet, which seeks to eliminate the human race with its global machine network. The T-3000 describes itself as neither machine nor human; rather, it is a hybrid nanotechnological cyborg. Producer David Ellison explains that the title \"Terminator Genisys\" \"[is] in reference to genesis, which is in reference to the singularity and the man-machine hybrid that John Connor ends up being.\"\n\nIn a desperate effort to ensure its survival, the rogue artificial intelligence Skynet creates an avatar for itself in the form of a T-5000 (Matt Smith). This Terminator travels through many timelines searching for a way to defeat the Human Resistance and ultimately infiltrates it under the guise of a fighter named Alex. \"Alex\" is present as a soldier when John Connor and Kyle Reese (Jai Courtney) discover Skynet's time machine at the end of the war with the machines. As Kyle is being sent back in time to protect John's mother Sarah Connor (Emilia Clarke) from the T-800 Terminator sent to kill her in 1984, \"Alex\" carries out his attack. He kills all of the other soldiers nearby and infects John Connor with nanomachines, transforming him into a new hybrid Terminator designated as the T-3000. John Connor's conversion into a Terminator as Kyle travels back in time turns out to be such a major event that it leads to a fracturing of the timeline, completely rewriting the past and the future.\n\nThe T-3000 is sent back in time to 2014, and given the mission to assist Cyberdyne Systems as the company's architect in developing a new operating system named \"Genisys\", in addition to revolutionary robotic and time traveling technology, which is in reality Skynet and the recreation of its machines. When Sarah and Kyle arrive in 2017, he meets them at a hospital and convinces them that he really is John Connor, but his disguise is exposed by the Guardian (a reprogrammed T-800 portrayed by Schwarzenegger). The T-3000 engages the trio in multiple destructive battles before finally being destroyed by the Guardian by having his body disintegrated by a prototype time machine's magnetic field. Before his demise, the T-3000 furiously throws what remains of the Guardian's endoskeleton into a vat of mimetic polyalloy, inadvertently converting the Guardian into a nano-android with abilities similar to those of the T-1000, and saving the Guardian's life.\n\nThe Guardian explains during the film that before the final battle between Skynet and the Resistance, the former attempted to create a number of T-3000s, but the human subjects went insane and died; John Connor was the only known subject to survive the transformation reasonably mentally intact, albeit now loyal to Skynet as opposed to his previous loyalty to humanity.\n\nThe on-screen representation of the T-3000 was made by British effects company Double Negative. Footage would combine Jason Clarke filmed on set, keyframed animation, and motion capture. Supervisor Peter Bebb said that the company tried to design the T-3000 like a computer would do it, focusing on design and battle efficiency, \"form follows function\". Given it is a Terminator built out of a human, the result is \"a pure robot that sits under flesh structure\", still retaining an overall human shape. The mechanical cells tried to resemble the material on stealth aircraft, with a result described as \"more matte than metal\", resembling a slightly iridescent ceramic carbon.\n\nThe Guardian identifies the T-3000 with John's form and memories as being made of \"machine-phase matter\" (essentially, programmable matter) held together by a magnetic field. Because of this, the T-3000's abilities far exceed those of the T-1000 and other Terminator models. It is capable of shapeshifting at much more rapid speed than the mimetic polyalloy Terminators, though it is still limited by complexity or mass; its transformation abilities are such that it can dissolve into its most basic form while falling head-first through the air, reorient itself to land on its feet and be fully transformed back to normal by the time it stands up. Likewise, it is capable of regenerating from almost any injury in mere moments. Unlike previous Terminators, the T-3000 transforms and regenerates in layers, beginning with its bone structure, then muscle tissue, skin and clothing. These layers can be adjusted independently of each other, as shown when John Connor adds and removes his facial scar at will in the film. These aspects, combined with the access the cyborg has to its original human host's memories in addition retaining his or her personality, behavioral traits, and some levels of emotions, the T-3000 can easily convince even those well-versed in tactics used by Terminators that it is actually a human, essentially a machine that thinks like them (see Turing test). This makes it more effective than other Terminators at infiltration. It appears to retain the ability to infect others and create more T-3000s, as it offers this to Sarah Connor and Kyle Reese in \"Terminator Genisys\". The transformation, which is described as replacing its victim's body on the cellular level, cannot be reversed.\n\nThough a formidable enemy, the T-3000 is not invulnerable. Because its abilities are primarily tied to its magnetic field, it is susceptible to magnetic attacks that disrupt said field. Weaker magnetism is capable of disrupting or disabling the T-3000's shapeshifting and regeneration abilities, due to affecting its ability to manipulate its particles. A sufficiently strong magnetic field can pull the T-3000 apart briefly to restrain it, and, if its body is exposed to a powerful magnetic field for a sustained amount of time, the cyborg can be torn apart and terminated. The machine-phase matter is also vulnerable to the laser used in shaping mimetic polyalloy for use in the construction of T-1000s. If struck by this laser, the T-3000 can suffer enough external damage to render its outermost layer irreparably compromised, forcing it to discard its human appearance and use its true machine form. The T-3000 is still susceptible to some degrees of physical injuries and pains, and its agonies can escalate when it is under a magnetic attack. It is also shown to experience pain and have its movements impaired when trapped in an electrical current. However, these effects are only temporary due to its regeneration capabilities, which heal it once it manages to escape the current.\n\n"}
{"id": "22722838", "url": "https://en.wikipedia.org/wiki?curid=22722838", "title": "Teaching grandmother to suck eggs", "text": "Teaching grandmother to suck eggs\n\nTeaching grandmother to suck eggs is an English language saying meaning that a person is giving advice to someone else about a subject of which they are already familiar (and probably more so than the first person).\n\nThe origins of the phrase are not clear. The OED and others suggest that it comes from a translation in 1707, by J. Stevens, of Francisco de Quevedo (Spanish author):\n\n\"You would have me teach my Grandame to suck Eggs\"\n\nThe use of the phrase \"Suck-egg\" for \"a silly person\" dates back to 1609, in the OED.\n"}
{"id": "22801248", "url": "https://en.wikipedia.org/wiki?curid=22801248", "title": "Video game-related health problems", "text": "Video game-related health problems\n\nVideo game-related health problems can induce repetitive strain injuries, skin disorders or other health issues. Other problems include video game-provoked seizures in patients with epilepsy. In rare and extreme cases, deaths have resulted from excessive video game playing (see Deaths due to video game addiction).\n\nStudies look at students with powerful behavior records in the past and who finished a measure of characteristic aggressive. They found that students, who reported playing more violent games in their middle and high school time, occupied with more forceful behavior, Moreover, the time spent playing violent games in the past related to lower grades in school, which is a cause of dissatisfaction for some students. \n\nThere may or may not be an accompanying video game addiction. Video games are also been linked in some studies to aggressive behaviour and violence or fearful behaviour by its players in the short term although other studies have not supported this link.\n\nStudies have mainly reported health problems in children, mainly boys. Several specific names have been given to video-game related health problems, for example PlayStation thumb, Nintendinitis and acute Wiiitis; however, the literature does not seem to support these as truly separate disease entities. Video game consoles linked to medical problems include the PlayStation and the Nintendo Wii, although it is unknown whether certain types are more connected to these problems than others.\n\nPhysical signs linked to excessive video game playing include black rings in the skin under the eyes and muscular stiffness in the shoulders, possibly caused by a tense posture or sleep deprivation.\n\nExisting literature on gaming is inconsistent, and studies occasionally produce contradictory results. Some studies show strong correlations between gaming and psychological issues like increased aggression in males and increased depression in females. Another study claims that girls who gamed were less likely to experience depression but were more likely to get into fights.\n\nIn 2009, during a speech to the American Medical Association, US President Barack Obama identified video games as a health concern, stating that they are a key factor in unhealthy sedentary lifestyles.\n\nWhen questioned, children often admit to having physical complaints during video game playing, for example pain in the hands and wrists, back and neck.\nErgonomic measures could improve postural problems associated with video game playing.\n\nA 2004 case report in \"The Lancet\", authored by a 9-year-old boy, mentions the Playstation thumb, which features numbness and a blister caused by friction between the thumb and the controller from rapid game play. Using dermoscopy, dermatologists found point-like hemorrhages and onycholysis (letting go of the nail) in a patient who presented with hyperkeratosis.\n\nNintendonitis has been used to describe tendon injuries (tendinosis) of the hands and wrists in Nintendo players.\n\nA 2010 case report in the \"New England Journal of Medicine\" reported a fracture of the base of the fifth metatarsal after using a Wii balanceboard; this was dubbed a Wii fracture.\n\nA further study involving musculoskeletal symptoms and computer use among Finnish adolescents affirmed the association between musculoskeletal symptoms and computer usage. The study claims that daily computer use of 2 hours or more increases the risk for pain at most anatomic sites.\n\nConsistently long sessions of video game play also leads to an increased likelihood of lower back pain, according to a study conducted in a population of school children. Children who played video games for more than 2 hours a day were more inclined to have lower back pain, however the same could not be said for those who watched television instead.\n\nVideo game playing may be associated with vision problems. Extensive viewing of the screen can cause eye strain, as the cornea, pupil, and iris are not intended for mass viewing sessions of electronic devices. Using video games for too long may also cause headaches, dizziness, and chances of vomiting from focusing on a screen. \n\nHowever, certain studies have shown that video games can be used to improve various eye conditions. An investigation into the effect of action gaming on spatial distribution of attention was conducted and revealed that gamers exhibited an enhancement with attention resources compared to non-gamers, not only in the periphery but also in central vision. Further studies in 2011, concluded that a combination of video game therapy alongside occlusion therapy, would greatly improve the recovery of visual acuity in those experiencing amblyopia.\n\nConcerns that video games can trigger epileptic seizures began in the early 1980s, with the first medically documented case of a video game-induced seizure occurring in 1981. In early 1993 \"The Sun\" reported a boy choked to death on his own vomit during a seizure triggered by playing a video game; similar though less serious incidents were subsequently reported by news media around the world, and within a year all video game console manufacturers required that epilepsy warnings be included in the instruction manuals for all games published for their consoles.\n\nStudies published in 1994 in \"Pediatrics\" and \"The Lancet\" found that video games only cause seizures in people already predisposed to epilepsy, and that people with a predisposition to epilepsy can greatly reduce the risk of a seizure by staying 10 feet or more away from the TV set and wearing sunglasses while playing.\n\nVideo game play has been constantly associated with obesity. Many studies have been conducted on the link between television & video games and increased BMI (Body Mass Index). Due to video games replacing physical activities, there appears to be a clear association between time spent playing video games and increased BMI in young children. One such study produced data that indicated that boys who spend less than 1.5 hours on the television and playing video games, were 75.4% less likely to be overweight than those who spend more than 1.5 hours.\n\nA study conducted in 2011 formalized the association of video game play and an increase in food intake in teens. A single session of video game play resulted in an increase in food intake, regardless of appetite. The recent trend of \"active video games\" revolving around the Wii and Xbox Kinect might be a way to help combat the aforementioned problem however this finding still needs confirmation from other studies. Furthermore, a study conducted in Baylor College of Medicine revolving around children claims that there is no evidence which supports the belief that acquiring an active video game under naturalistic circumstance would result in a beneficial outcome toward children. The study produced no results showing an increased amount of physical activity within the children receiving the active video games. It has been estimated that children in the United States are spending 25 percent of their waking hours watching television and playing video games. Statistically the children who watch the most hours of television or play video games have the highest incidence of obesity we can see.\n"}
{"id": "2353519", "url": "https://en.wikipedia.org/wiki?curid=2353519", "title": "Wake therapy", "text": "Wake therapy\n\nWake therapy is a form of sleep deprivation used as a treatment for depression. The subject stays awake all night, or is woken at 1AM and stays awake all morning, and the next full day. While sleepy, patients find that their depression vanishes, until they sleep again. Combining this with bright light therapy make the beneficial effects last longer than one day. Partial sleep deprivation in the second half of the night may be as effective as an all-night sleep deprivation session.\n\nWake Therapy is a therapy that falls under chronotherapeutics. Chronotherapy (treatment scheduling) is a process to manipulate biological rhythms and sleep that can help to improve affective disorders quickly.\n\nWake therapy is beneficial for those experiencing major depression along with unipolar, bipolar, and melancholic types of depression. Wake therapy is best used to jump start the effects of the use of an antidepressant. Wake therapy is the use of prolonged times of wakefulness, along with periods of recovering sleep. It is a fast way to improve symptoms of depression. This therapy is best used with other chronotherapeutic techniques. Months of use of this therapy and other therapies can be quite effective to help prevent relapse of depression.\n\nWake therapy involves one to three rounds which consist of complete sleep avoidance for the night and entire day after, then the cycles are separated with nights of recovery sleep (a full night's sleep). The treatment lasts between two and five days, depending on the number of rounds done. Relapse into depression typically occurs immediately after any amount of sleep. The intervention is done in a hospital or sleep center in order to make sure the program is done properly and full effects are established.\n\n"}
{"id": "1701654", "url": "https://en.wikipedia.org/wiki?curid=1701654", "title": "Walter Mittelholzer", "text": "Walter Mittelholzer\n\nWalter Mittelholzer (April 2, 1894 – May 9, 1937) was a Swiss aviation pioneer. He was active as a pilot, photographer, travel writer, and also as one of the first aviation entrepreneurs.\n\nBorn on April 2, 1894 in St. Gallen in Mittelholzer earned his private pilot's license in 1917, and in 1918 he completed his instruction as a military pilot.\n\nOn November 5, 1919 he co-founded an air-photo and passenger flight business, \"Comte, Mittelholzer, and Co.\" In 1920 this firm merged with the financially stronger Ad Astra Aero. Mittelholzer was the director and head pilot of Ad Astra Aero which later became Swissair.\n\nHe made the first North-South flight across Africa. It took him 77 days. Mittelholzer started in Zürich on December 7, 1926, flying via Alexandria and landing in Cape Town on February 21, 1927. Earlier, he had been the first to do serious aerial reconnaissance of Spitsbergen, in a Junkers monoplane, in 1923. On December 15, 1929 he became the first person to fly over Mt. Kilimanjaro, and planned to fly over Mount Everest in 1930. In 1931, Mittelholzer was appointed technical director of the new airline called Swissair, formed from the merger of Ad Astra Aero and Balair. Throughout his life he published many books of aerial photographs and marketed his expeditions through films and the media as well. He died in 1937 in a climbing accident on an expedition in the Hochschwab massif in Styria, Austria.\n\nAmong other Swiss air pioneers, he is commemorated in a Swiss postage stamp issued in January 1977. His legacy of some 18,500 photographs is kept at ETH Library's image archive in Zurich, Switzerland.\n\n\n"}
{"id": "35427527", "url": "https://en.wikipedia.org/wiki?curid=35427527", "title": "Water conflict in the Middle East and North Africa", "text": "Water conflict in the Middle East and North Africa\n\nWater conflict in the Middle East and North Africa (MENA) primarily deals with three major river basins: the Jordan River Basin, the Tigris-Euphrates River Basin, and the Nile River Basin. The MENA region covers roughly 11.1 million square km. There are three major deserts in the MENA region:\nAdditionally, much of Iran is covered in desert. Average annual rainfall is less than 100 mm in 65% of the region, between 100 and 300 mm in 15% of the region, and more than 300mm in the remaining region.\n\nThe three headwaters of the Jordan River – the Hasbani River (annual stable flow of 250 Mm^3), the Banias River (annual stable flow of 125 Mm^3), and the Dan River (annual stable flow of 250 Mm^3) originate in Lebanon, Syria, and Israel, respectively. They merge in Lake Huleh and then flow south as the Jordan River. Just south of Tiberias, the Yarmuk River joins the Jordan River. In total, the Jordan River flows for roughly 350 km, starting in the foothills of Mount Hermon in the north and ending in the Dead Sea to the south. The River has an estimated flow of 1880 MCM/Y, 73 percent of which originates in Arab countries and 27 percent of which originates in Israel. According to a September 2000 USAF study, over 90% of Syria’s water is shared with neighboring Iraq, Turkey, Israel, Lebanon, and Jordan. Approximately 36 percent of Jordan’s water sources are shared with Syria, the West Bank, and Israel, and more than half of Israel’s waters is shared with Syria, Lebanon, Jordan, and the West Bank.\n\nFurther north, the Litani River and Orontes River comprise the northern Levant watershed. Although their headwaters are geographically close, the Litani flows south then west to the Mediterranean near Tyre, crossing through Lebanon alone, while the Orontes flows north into Syria, entering the Mediterranean around the city of Antioch. The Litani's estimated annual flow of 410 MCM/Y has been the target of proposed Syrian, Jordanian, and Israeli water solutions.\n\nSyria, Lebanon, and Israel all have indigenous underground fresh water aquifers. There are two main underground aquifers that provide fresh water to modern day Israel. The Mountain Aquifer is underneath the West Bank. It recharges 679 Mm^3 of water annually, of which 78 Mm^3 are brackish. The Mountain Aquifer’s three main basins are the Eastern Basin, the Western Basin, and the Northeastern Basin. The Eastern Basin is the smallest in terms of fresh water quantities, but largest in terms of area. It covers roughly 3260 km^2. The Western Basin is the largest of the three in terms of fresh water content. It covers roughly 1780 km^2. Finally, the Northeastern Basin has an area of roughly 610 km^2. The Coastal Aquifer located along the Mediterranean Coast has a total annual recharge of 330 Mm^3.\n\nThe area around the Jordan River Basin is one of the driest in the world. Israel and northwestern Jordan receive an average of 110 cm of rainfall per year. Little of this water can be exploited. Only Kuwait, Libya, Oman, and Singapore receive less rain annually than Jordan or Israel. Syria and Lebanon, in contrast, receive enough rainfall to support agriculture and recharge underground aquifers. As stated earlier, while northern parts of Jordan and Israel receive 110 cm of annual rainfall, central Jordan and the West Bank receive 20 mcm/y. Only 3 mcm/y of this water is usable, however.\n\nIsrael uses roughly 850 MCM/Y of groundwater, 400 MCM/Y of which come from the Mountain Aquifer. In 1964, Israel began to withdraw 320 MCM/Y from the Jordan River for the National Water Carrier. By 1967, Israel’s National Water Carrier was extracting almost 70 percent of the Jordan River before it reached the Palestinians in the West Bank. In the 1980s, Israeli settlers consumed seven times more water than Arab farmers. Other sources show a smaller, yet still sizeable difference. Per a September 2000 study, Israel uses 1180 MCM/Y, or 62 percent of its total annual water supply on agriculture. In comparison, Jordan uses 67 MCM/Y, or 74 percent of its total water supply on agriculture. Roughly 48 percent of Israel’s average annual water usage of 1,950 MCM/Y comes from territory captured in 1967. 400 MCM/Y of this comes from ground water in the West Bank, while 450 MCM/Y comes from the upper Jordan River and the Golan Heights. As early as 1981, however, Israel was exploiting 99 percent of its available water resources.\n\nAt the time of the British Mandate of Palestine (1913–148), three irrigation systems existed along the eastern slopes of the Judaean Mountains. The Wadi Qelt aqueduct provided 3 MCM/Y of fresh water to Jericho from the Ein Fara, Ein Fawar, and Ein Qelt springs. Additionally, the Wadi Uja aqueduct supplied the Uja Valley with 7 MCM/Y of fresh water from the Ein Uja spring and the Wadi Faria aqueduct transported 5 MCM/Y of water from the Ein Shibli, Ein Isca, and Ein Baidan springs to the Giftlik. Under Roman rule, the Nablus and Jerusalem aqueducts were also active in the Judea and Samaria Mountains. These aqueducts brought a combined additional 3 MCM/Y to Sabastia and Jerusalem. During the Mandate, another 200 small groundwater springs were used and rainwater was collected from cisterns, yielding an additional 5 MCM/Y of fresh water during rainy years. With the addition of two electric power plants supplying an addition 2 MCM/Y to Jerusalem and Ramallah, the maximum water capacity in the Judea and Samaria Mountains was 25 MCM/Y. \n\nSeveral plans attempted to allocate the water resources of the region during the British Mandate. The Ionides Plan of 1939 made three recommendations. First, it proposed to store the floodwaters of the Yarmouk River in Lake Tiberias (Sea of Galilea). Second, it set and amount of the water in the lake to be diverted through the East Ghor Canal and used for irrigation east of the Jordan River. Third, it limited the use of Jordan River Basin water to the Jordan River Valley. This was rejected by Zionist advocates who envisioned using waters from the Jordan River Basin to settle the Negev Desert in the south of the country. In contrast, the Lowdermilk Plan, commissioned by the United States Department of Agriculture, proposed irrigating the Jordan River Valley and diverting the Jordan and Yarmouk Rivers for hydroelectric power. In 1948, James B. Hays used the findings in Lowdermilk as a basis for the Hays Plan in which the Yarmouk River was to be diverted into Lake Tiberias to compensate for the waters lost in the diversion of the Jordan River for agricultural purposes.\n\nIn 1948 Israel officially became a state. The Kingdom of Jordan assumed control of the water resources of the Judea and Samaria Mountains. They retained control over the region until the Six-Day War in 1967. Additional wells drilled during this period and the maximum water supply in the region reached 66 MCM/Y. Only four of 708 Palestinian towns, however, were connected to the water supply at the time. In 1965 an additional 350 wells were drilled, supplying a total of 41 MCM/Y.\nIn the Israeli War of Independence (1948) the Jewish Army destroyed the Rutenburg electric plant in trying to prevent the exclusive Arab control of both the Jordan and Yarmouk Rivers. As a direct result the Jordanian Government and the United Nations Relief and Works Agency commission the MacDonald Plan, which resembled the Ionides Plan (1939) in limiting the waters of the Jordan River Basin to the Jordan River Valley. It also called for a diversion of the Yarmouk River for storage in Lake Tiberias to be used to support agricultural developments on both sides of the Jordan River. Jordan and Syria backed the Bunger Plan, which proposed a dam of the Yarmouk River and the creation of two hydroelectric power plants to supply electricity to Jordan and Syria. In 1953, shortly after construction had started, increased pressure from the U.S. forced Jordan and Syria to abandon their plans. The Eisenhower Administration sent Eric Johnston as a special envoy. The Johnston Plan implemented the policies of a Tennessee Valley Authority study conducted by Chales Main – the Main Plan – that included water distribution quotas of the Jordan River Basin. Although neither side accepted the plan, it has been used as the basis for many subsequent water conflict negotiations.\n\nWhen Israel occupied the West Bank in the Six-Day-War 1967, the water resources of the region were declared as state property. The occupation has huge impacts on the daily lives of the Palestinian people and their water supply: In many aspects the occupation implicates breaches of international law, such as the international human rights law or the international humanitarian law. Especially the land confiscations and property demolitions by the Israeli military as well as the restrictions on the movement of people and goods are criticized. Since 1967, most water sources are controlled by the Israeli military. Moreover, water resources are diverted and the Palestinian population is denied access to most water resources. In the same time, Israel claims that the Palestinians drill unauthorized wells. Large parts of the Palestinian water infrastructure, including cisterns, wells, and irrigation systems, are declared as illegal and systematically demolished. As the development of new Palestinian water infrastructures is mostly prohibited, the Palestinian water supply systems are in a very poor condition. In that way, they are reflecting the weak and ambiguous political status of Palestine. As most people in the West Bank depend on agriculture, the Palestinian water deficit is also weakening the local economy. Thereby it is strongly contributing to poverty and unemployment. The unbalanced water allocation in the Palestinian territories shaped the term of \"water apartheid\".\n\nEven though it is difficult to get accurate data, the average Israeli water consumption in the West Bank is about 6 times higher than the average Palestinian water consumption of approximately 50 liters/person/day, what is about the half of the World Health Organization minimum recommended level. However, there are other data showing a different picture. By excluding treated sewage and\ndesalinated water and using a different type of demographic data, Gvirtzman (2012) for instance argues that the average Israeli and the average Palestinian water consumption is nowadays almost the same. The huge differences in the Israeli and Palestinian water data illustrate how contested and urgent the question of water allocation still is.\n\nUntil today, there is no final agreement granting water rights to the Palestinian population. Although water questions were a topic in the Oslo Peace Process, the water dispute was not solved through the negotiations. Indeed, Article 40 of the \"Interim Agreement on the West Bank and the Gaza Strip\" (1995) deals with the Palestinian water rights, but it states that \"these will be negotiated in the permanent status negotiations and settled in the Permanent Status Agreement relating to the various water resources\". In the meantime, \"during the interim period a total quantity of 28.6 mcm/year\" will be allocated to the Palestinian population \"in order to meet the immediate needs [..] in fresh water for domestic use\". In reality, due to the ongoing Israeli-Palestinian tensions, a Permanent Status Agreement was never negotiated. Therefore, the agreement indirectly maintained Israeli domination over water resources and handled the issue of water deficit in the occupied Palestinian territories as a matter of humanitarian need rather than a right.\n\nMoreover, the Israeli-Palestinian Joint Water Commission (JWC) was established to implement the agreement in a collaborative way. The Hydrological, Engineering, Sewage, and Pricing Committees of the JWC meet on a regular basis to approve the construction of water supply systems or sewage installations. As a part of that agreement, Israel transferred the control of the water supply in the Palestinian territories to the Palestinian Authority. Nevertheless, the Palestinian Authority is hardly able to fulfill its responsibilities, as Israeli members of the Joint Water Commission are able to veto any Palestinian project. Furthermore, the members of the Palestinian Authority are unable to access most areas as they are denied access by the military. The result is, that more than 33% of the water supplied to the Palestinian territories \"leaks from internal pipelines\". Both sides contest that the laws established in Oslo are no longer being followed. As the Palestinian population is not allocated enough water, the work of the Joint Water Commission can be considered as insufficient.\n\nThe Nile River is an international water basin, running through 10 different sovereign nations. The Nile runs through Sudan, South Sudan, Burundi, Rwanda, Democratic Republic of the Congo, Tanzania, Kenya, Uganda, Ethiopia, and Egypt and is considered to be the longest river in the world. The Nile is the only significant source of water in North Africa and 40% of Africa’s population lives in the Nile River Basin. The Nile has two major tributaries: the White Nile and the Blue Nile. The White Nile is the longer of the two, rising in the Great Lakes Region of central Africa. Its most distant source is unknown, located in either Burundi or Rwanda. From there, it flows through Tanzania, Lake Victoria, Uganda, and South Sudan. The Blue Nile originates from Ethiopia’s Lake Tana, flowing into Sudan from the southeast. In total, Ethiopian headwaters provides 86% of Nile River waters. The two major tributaries meet near the Sudanese capital of Khartoum and flow north through Egypt and into the Mediterranean Sea.\n\nAccess to key resources, mainly water, is a primary source of the Civil War in Sudan. Ecologically, Sudan is divided into a northern and southern region. The North is densely populated with Arab Muslims. Although the Nile flows through the northern part of Sudan, it is largely beyond the drainage basins of the feeder streams and rivers that consolidate themselves into the White and Blue Niles further to the south. The South is lightly populated, but much more fertile. It receives adequate rainfall and benefits from many feeder streams and springs scattered throughout the area. Persistent drought and desertification, population expansion, and the need to increase food output have forced many northern Arabs to look south for lands and resources. The introduction by the North of mechanized farming equipment continues to threaten southern subsistence farming. Organized as the Sudan People’s Liberation Army (SPLA), southerners have fought back against northern intrusion for the better part of three decades.\n\nWater conflict in Sudan typically concerns canal building or farming projects. One of the more notable sources of conflict has been the Jonglei Canal project, begun in 1978. The project was started for two primary reasons: to drain the Sudd Swamps to create additional farmland and to conserve the water being evaporated as it sat idle in the swamps. The amount lost through evaporation was estimated at 4000 mcm/y. Both the Sudanese and Egyptian governments supported the project as both would share in the benefits of additional water. The Jonglei Canal is massive enough to be seen from space. It averages 210 feet in width and 16 feet in depth.\nDrainage of the wetlands threatened the roughly 1.7 million local tribesmen that depended on the swamps for survival and in November 1974, locals rioted in the southern city of Juba. Southerners began to flock to the SPLA, which led violent attacks on construction sites along the Jonglei Canal. They eventually forced the suspension of the project in 1984. 250 of 360 km of canal had been built.\n\nThe incursion of northern mechanized farming in southern Sudan has also caused conflict. The Arab-dominated government envisioned agricultural development in the South and northern farmers continually encroached upon the fertile southern plains. Such encroachment threatened the Nilotic tribes, who ran the cattle economies of the south. Southerners responded to northern incursion with hostility and violence.\n\nConflict again broke out between the northern government and the Sudanese people concerning the government’s decision to build the Kajbar Dam on the Nile. The resulting reservoir would have forced the relocation of the last remaining Nubian tribes in the Sudan. Much of the Nubian homeland had already been flooded after the Aswan High Dam was built by Egypt in the 1950s. Many Sudanese protested the building of the Kajbar, with members of the Nubain Alliance even threatening mass suicide in protest. They have declined offers of compensation from the Sudanese government.\n\nAside from a source of internal conflict within nations, water has caused external tension between sovereign states. While Egypt consumes 99% of the Nile’s water supply, little water originates within Egypt’s sovereign borders. High water demands of a lower riparian have often fueled regional conflict. Such is the case in North Africa.\nIn the early 1900s, a world cotton shortage put pressure on Egypt and Sudan to use arable lands to increase cotton production. Cotton, however, requires extensive irrigation and is less suitable to Egyptian and Sudanese climate than are other more traditional crops. The need for water and flood control precipitated water development projects along the Nile, which often led to clashes between the British foreign office – Sudan was a British-Egyptian condominium at the time – and local Egyptians and Sudans. After World War I it became apparent that a formal allocations treaty would need to be signed to regulate water usage of the Nile. In 1920, the Nile Projects Commission was formed. Led by representatives from India, the United Kingdom, and the United States, the commission estimated water usage based on the water usage needs of each country. Based on an average flow rate of 84 bcm/y the commission estimated necessary Egyptian consumption to be 58 bcm/y, while Sudan was believed to be able to meet its irrigation needs using only the waters of the Blue Nile. The commission also proposed dividing any deviations from the annual average water flow equally between Egypt and Sudan. The findings of the commission, however, were never acted upon. Also in 1920, the British proposed the Century Storage Scheme, the most comprehensive plan for water development along the Nile. The plan called for a water storage facility to be built on the Uganda-Sudan border, a dam to be built at Sennar, Sudan, to irrigate the regions south of Sudan, and a dam along the White Nile to hold summer floodwaters for Egyptian dry-season consumption. The British scheme worried many Egyptians, as most of the major water structures were to be built outside of Egyptian territory. On May 7, 1929, Egypt and Sudan signed the Nile Waters Agreement. Based largely on findings from the 1920 commission, the treaty allocated 4 bcm/y to Sudan, and 48 bcm/y to Egypt. In between January 20 and July 15, however, the entire flow of the Nile was allocated for Egyptian consumption.\n\nIn the 1950s, Egypt unilaterally pursued construction of the Aswan High Dam until 1954, when the Sudanese was brought in on the negotiations. The first round of negotiations between Egypt and Sudan took place in December 1954, but was called off before a consensus could be reached. The two governments met again in April 1955, again failing to reach a compromise. Tensions mounted in 1958 after a failed Egyptian expedition into disputed territory along the Sudanese border. In response, Sudan raised the Sennar Dam in the summer of 1959, effectively repudiating the 1929 agreement signed with Egypt. The two countries signed the Agreement for the Full Utilization of the Nile Waters (Nile Waters Treaty) on November 8, 1959. The Nile Waters Treaty, based on an average annual water flow of 84 bcm/y provided the following:\nAdditionally, the 1959 treaty established a Permanent Joint Technical Committee to resolve any future conflicts. Both Sudan and Egypt agreed that the water needs of the other eight riparians would not exceed 1 to 2 bcm/y and further agreed to meet any threat to Nile resources with a unified Egyptian-Sudanese response. The building of the Aswan High Dam in the 1960s was contentious as it flooded areas of southern Sudan near Wadi Halfa and displaced Sudanese residents living along the Egyptian-Sudanese border.\n\nIn the mid 1990s Egypt began a massive project just south of the Aswan. The New Delta Project in southern Egypt and the Salaam Canal in the Sinai Peninsula were both initiatives undertaken by former Egyptian president Hosni Mubarak to increase water supply for the expanding Egyptian population, which has dispersed further and further from the Nile. Increasing Egyptian water consumption is a source of conflict in the region as upstream riparians – Sudan and Ethiopia – have begun to assert their own rights to dam the river for hydroelectric development. Sudanese plans to build the Kajbar Dam near Khartoum at the confluence of the White and Blue Niles has caused much international tension. Sudan is also planning on building the Merowe Dam south of the Kajbar and enlarging the Roseires Dam, located 300 miles southeast of Khartoum on the Blue Nile. It is estimated that the building of these projects would likely lead to Sudan exceeding its water allotments from the 1959 treaty.\n\nEthiopia has also emerged as a major player in Nile River development. In 1957 they announced their intentions to pursue unilaterally the development of Nile River headwaters within Ethiopian territory. By the year 2000, more than 200 small dams had been constructed along Nile headwaters. Collectively, the dams will use nearly 500 million mcm/y of the Nile’s annual flow. Ethiopia is the only Nile River riparian to make a legal claim to Nile waters other than Egypt or Sudan since the Nile Waters Treaty was signed in 1959. Like in Egypt, population growth in Ethiopia has led to an increase in water consumption. Ethiopian population growth exceeds that of Egypt, and with populations of roughly equal size, Ethiopia’s water demands may soon exceed Egypt’s. Irrigating only half of Ethiopia’s arable lands would reduce water flow to downstream Sudan and Egypt by 15%.\n\nThe Tigris-Euphrates River Basin consists of its two primary rivers – The Tigris River and the Euphrates River – and their minor tributaries. Both the Tigris and the Euphrates originate in southeastern Turkey. The Tigris flows from Turkey, tracing the border between Turkey and Syria for 32 km before flowing south through Iraq. From its origin in Turkey, the Euphrates flows into Syria from the north, before continuing on through Iraq. The two rivers meet in Iraq to form the Shatt Al-Arab, which flows south into the Persian Gulf.\n\nAlthough hydropolitical tensions in the region have existed for centuries, population pressures in the 1960s led each country to pursue unilateral development of water resources. The Keban Dam was built in southern Anatolia from 1965 to 1973 and Syria built its Tabqa Dam between 1965 and 1973. Despite bilateral and sometimes tripartite (including the Soviet Union) negotiations, no formal agreements were in place when the two dams became operational in 1973. As the dams began to fill, downstream water flow decreased significantly. In 1974, Syria granted an Iraqi request and allowed an additional 200 mcm/year to flow from the Tabqa Dam. The following year, however, Iraq asked for Arab League intervention, citing that the flow of water reaching Iraq had fallen from the normal 920 m³/s to an \"intolerable\" 197 m³/s. The Syrian government responded by claiming that water flow into Syria had been reduced by more than 50%. After trading mutually hostile statements, Syria withdrew from an Arab League committee tasked with resolving the dispute. Tension increased in May 1975 as Syria closed its airspace to Iraqi flights and both countries amassed troops on their borders. Saudi Arabian intervention, however, defused the situation with both sides arriving at an agreement that averted the impending violence. The terms of the agreement were never made public, but Iraqi sources have privately said that Syria was permitted to keep 40% of the water in the Euphrates flowing through its country, while 60% of the water was to be allowed to flow south through Iraq.\n\nThe Southeastern Anatolia Project (Turkish acronym: GAP) continues to be a source of tension in the region. GAP is a massive hydroelectric project, consisting of 21 dams and 19 hydroelectric facilities. The water will be used to generate 26 billion kilowatt-hours of electricity with an installed capacity of 7,500 megawatts and irrigate 1.65 million hectares of land. In 1987, Turkish Prime Minister Turgut Ozal reportedly signed an agreement guaranteeing a minimum flow of 500 mcm/y into Syria. Talks between Turkey, Syria, and Iraq were held again in January 1990 when Turkey shut off the flow of the Euphrates for 30 days by closing the gates of the Ataturk Dam. Iraq insisted that Turkey allow a minimum of 500 mcm/y to pass into Syria, but negotiations were suspended due to the outbreak of the first Gulf War. Negotiations after the war in September 1992 failed to reach an agreement.\n\nThe Blue Peace method which seeks to transforms trans-boundary water issues into instruments for cooperation has proven to be effective in cases like the Middle East and the Nile basin. This unique approach to turn tensions around water into opportunities for socio-economic development was developed by Strategic Foresight Group in partnership with the Governments of Switzerland and Sweden. A recent report \"Water Cooperation for a Secure World\" published by Strategic Foresight Group shows that active water cooperation between countries reduces the risk of war. This conclusion is reached after examining trans-boundary water relations in over 200 shared river basins in 148 countries.Countries in the Middle East face the risk of war as they have avoided regional cooperation for too long. The report provides examples of successful cooperation, which can be used by countries in the Middle East.\n\nThe challenges of water security have a great human costs and a much larger impact on the Middle East. Water insecurity is always accompanied by one or more issues such as poverty, war and conflict, low women’s development and environmental degradation. These devastating effects have been documented in two reports release in 2015: ‘The Hydro Insecure: Crisis of Survival in the Middle East’ and ' Water and Violence: Crisis of Survival in the Middle East'.\n\n\n"}
{"id": "12286514", "url": "https://en.wikipedia.org/wiki?curid=12286514", "title": "Water supply and sanitation in Jamaica", "text": "Water supply and sanitation in Jamaica\n\n\"This article has been written in 2007/08 with only minor updates since. Please feel free to update it, if need be.\"\n\nWater supply and sanitation in Jamaica is characterized by high levels of access to an improved water source, while access to adequate sanitation stands at only 80%. This situation affects especially the poor, including the urban poor many of which live in the country's over 595 unplanned squatter settlements in unhealthy and unsanitary environments with a high risk of waterborne disease. Despite a number of policy papers that were mainly focused on water supply and despite various projects funded by external donors, increases in access have remained limited (1% for water and 5% for sanitation between 1990 and 2004).\n\nThe responsibility for water and sanitation policies within the government rests with the Ministry of Water and Housing, and the main service provider is the National Water Commission. An autonomous regulatory agency, the Office of Utilities Regulation, approves tariffs and establishes targets for efficiency increases.\n\n\"Source\": Joint Monitoring Program WHO/UNICEF(JMP/2006). Data for water and sanitation based on the Survey of Living Conditions (2002).\n\nIn urban areas, where 52% of Jamaica’s population lives, access to improved water supply is 98% access to improved sanitation is 91%. Only 31% of the urban population is connected to sewers. In rural areas access stands at 88% for improved water supply and 69% for improved sanitation. Overall, access to improved water supply in Jamaica stands at 93% and access to improved sanitation is currently at 80%.\n\nBetween 1990 and 2004 access only increased slightly by 1% for water by 5% for sanitation.\n\nMany homes receive water only at low pressure. Many rural communities receive water that is not or only irregularly chlorinated. According to the 2011 Population and Housing Census, 69 percent of Jamaica's households receive untreated drinking water.\n\nThe following table summarizes the results of the water tested from the 11 major surface sources and 6 wells of the National Water Commission (NWC) as compared to the standards set by the Ministry of Health (MOH). \n\n\"Source\": Office of Utilities Regulation Annual Report 2005-2006, p. 48\n\nMost of the country's over 595 unplanned squatter settlements or approximately 10% of the population is located nearby unhealthy and unsanitary environments without piped water or sanitation where there is a high risk of waterborne diseases. The vulnerability to natural disasters threatens the existing water and sanitation systems. Furthermore, there is a heightened probability of contamination of systems following hurricanes due to service interruption as well as the incidence of health-related diseases.\n\nThe National Water Commission (NWC), which produces more than 90% of Jamaica's total potable water supply, operates a network of more than 160 wells, over 116 river sources (via water treatment plants) and 147 springs. The various Parish Councils and a small number of private water companies supply the rest of the potable water. The NWC operates more than 1,000 water supply and over 100 sewerage facilities islandwide. These vary from large raw water storage reservoirs at Hermitage and Mona in St. Andrew and the Great River treatment plant in St. James, to medium-sized and small diesel-driven pumping installations serving rural towns and villages across Jamaica. The NWC facilities also include over 4,000 kilometres of pipelines and more than 500 kilometres of sewer mains across the island. NWC supplies some 190 million gallons of potable water each day.\n\nPolicy institutions The Ministry of Water and Housing (MWH) is responsible for setting water and sanitation policies. It is also in charge of ensuring that all housing developments meet required standards for sanitation. The Ministry of Health develops and implements health policies and legislation to promote appropriate sanitation practices; establishes and monitors health indicators for sanitation, enforces public health laws, provides public education on sanitation and hygiene. The National Environment and Planning Agency (NEPA) determines and monitors environmental standards for water supply and sanitation. Local Authorities have only a limited role in the sector.\n\nThe following water agencies operate under the supervision of the Ministry of Water and Housing\n\nThe Water Resources Authority (WRA) is responsible for the management, protection, and controlled allocation and use of Jamaica's water resources. The WRA maintains a hydrological database and provides data, information, and technical assistance to government and non-government institutions.\n\nPolicies Some of the existing policies related to water supply and sanitation are:\n\nRegulation The Office of Utilities Regulation (OUR), established by an Act of Parliament in 1995, regulates, among others, water and sanitation service provision. The Office approves tariffs, sets targets for efficiency improvements, processes all license applications for utility services and makes recommendations to the Minister.\n\nWater service on the island is provided by one large public entity and three small private companies:\nSanitation services are provided by NWC, Can Cara Development Company and Rosehall Utilities Company.\n\nThe Jamaica Social Investment Fund finances community-based water and sanitation projects.\n\nThe water and sanitation infrastructure is inadequate and inefficiently operated, as the level of Non-revenue water for the NWC was 66% in 2005. In 2004, service providers produced 277 million cubic meters of water, but only 103 million cubic meters were consumed.\n\nCost recovery Despite a tariff increase granted to NWC in 2004, the utility continued to register an operating loss for the fourth consecutive year. There thus is no cost recovery for capital costs. Poor enforcement of tariff payments contributes to the utility's low revenues. Whenever tariffs are adjusted, the NWC and the OUR shall implement a public awareness campaign on how to reduce bills through water conservation.\n\nAffordability The average Jamaican household spends 2.1% of its income on water services. This is approximately half of what the average household spend on electricity, and a bit less than half of what the average household spends on telephone services. The poorest 20% of household in Jamaica spend 3.2% of their income on water, while the richest 20% send 1.8% of their income on water.\n\nMost financing is provided through government grants to NWC, which are in turn financed by external loans, internal debt or general tax revenue. The government intends to tap other sources of financing, such as property taxes (called millage) and private sector financing.\n\nThe Water Sector Policy Paper of 1999 estimated investment needs until 2015 at US$2.2 billion, including US$1.3 billion for the \"non-agricultural sector\", US$0.3 billion for the agricultural sector and US$0.6 billion for sewerage systems.\n\nThe main external cooperation partners that provide support to the Jamaican government for water supply and sanitation are the Inter-American Development Bank, the World Bank, United States and Japan.\n\nApproved on 23 June 2004, a US$40 million loan aims to improve water and sanitation as well as to modernize the management in Kingston, Jamaica. The project is being implemented by the Ministry of Finance and Planning, as well as by the National Water Commission.\n\nApproved on 29 March 2006, 38% of the US$32.8 million loan is allocated to improving basic urban water, sanitation, and flood protection services. The project is being implemented by the Jamaica Social Investment Fund (JSIF).\nApproved in 2002 and closed in 2008, 20% of the US$29.65 million loan was aimed at improving the water, sanitation, and flood protection sector.\n\nKingston Metropolitan Area Water Supply and Rehabilitation Project (US$6m): Approved on 1 June 2007 and aims to assist USAID in expanding water supply to the Kingston Metropolitan Area. It follows in the footsteps of an earlier Kingston Metropolitan Area Water Supply project approved in 1996.\n\nThe Ridge to Reef Program (1997-2005) aimed at improving the environmental quality of coastal waters through influencing activities in upland watersheds and coastal areas through three projects:\n\n\n\n\n"}
{"id": "3229544", "url": "https://en.wikipedia.org/wiki?curid=3229544", "title": "WellChoice", "text": "WellChoice\n\nWellChoice, Incorporated is the parent company of Empire Blue Cross-Blue Shield of New York, created when Empire became a publicly traded company in 2003. WellChoice is listed on the Fortune 500 with revenues of over US$5,000,000,000.\n\nEmpire changed the named of its parent company to WellChoice in November 2002 after its initial public offering.\n\nIn late 2005 it was announced that Wellchoice would begin a merger with WellPoint, also a Fortune 500 company, in January 2006.\n"}
{"id": "6990092", "url": "https://en.wikipedia.org/wiki?curid=6990092", "title": "Willi A. Kalender", "text": "Willi A. Kalender\n\nWilli A. Kalender (born August 1, 1949) is a German Medical Physicist and Professor and Chairman of the Institute of Medical Physics of the University of Erlangen-Nuremberg. He is a Fellow of the American Association of Physicists in Medicine (AAPM) and Honorary Fellow of the British Institute of Radiology (BIR) and of the Institute of Physics and Engineering in Medicine (IPEM).\n\nWilli Kalender started his studies in physics and mathematics at the University of Bonn, Germany. He completed his Master’s and Ph.D. degree in Medical Physics at the University of Wisconsin in 1974 and 1979, respectively. In 1988 he completed all postdoctoral lecturing qualifications (Habilitation) at the University of Tübingen, Germany. In order to get a better grasp of the subject, he took and successfully completed all courses in the pre-clinical medicine curriculum.\n\nFrom 1979 to 1995 Dr. Kalender worked in the research laboratories of Siemens Medical Systems in Erlangen, Germany; he was appointed head of the Medical Physics group in 1988. In 1995 he was appointed full Professor and Chairman of the newly established Institute of Medical Physics at the Friedrich-Alexander-University Erlangen-Nuremberg, Germany. In 1999 he became Distinguished Visiting Professor to the Department of Radiology at Stanford University, Stanford, CA, USA.\n\nWilli Kalender has conducted research mainly in the area of diagnostic radiology imaging with a clear focus on special CT applications. The goals of and motivation for his projects were mostly derived from observations during patient exams in clinical radiology.\nOutstanding results were the development of the world’s first product options for dual-energy CT [1] in 1983 and for metal artefact reduction (MAR) [2] in 1987.\n\nThe development and introduction of volumetric spiral computed tomography was a highlight of his work; the world’s first clinical spiral CT studies were presented at RSNA 1989 [3] and were highly welcomed. The combination of continuous data acquisition with slip-ring-based data transmission and continuous table translation led to considerable reduction of examination times, to an essential reduction of motion artefacts and image quality significantly improved by providing isotropic spatial resolution as a decisive feature.\nThe development of a number of new and advanced applications such as angio-CT [4] and heart phase-specific cardiac imaging [6] followed and became a big success.\nOther highly important fields of W. Kalender’s research have been radiation protection and the development of intelligent and very efficient dose reduction approaches such as tube current modulation [7] and the optimization of the choice of x-ray spectra [8]. They allow reducing the patient dose by an order of magnitude in many cases without impairing image quality. An overview of all measures he introduced is given in [9].\n\nPresently, Willi Kalender’s research focusses on the development of an efficient breast-CT system to improve the early detection of breast cancer [10]. The scanner and in particular the development of its high-resolution photon-counting detector design allow scanning the breast with a dose similar to that of digital mammography, but with much higher resolution and clarity than mammography due to superposition-free fully 3D-resolution of breast-CT [11]. The introduction into clinical practice is expected for 2018 and should become another success story.\n\nIn the course of his prolific career Willi Kalender has founded a number of university spinoff companies to transfer scientific results into products and small business. In recognition of his contributions to the field of medical imaging Willi Kalender has received a number of prestigious awards.\n\n\nWilli Kalender so far authored a total of 963 publications, including 286 original scientific papers;\nhis Hirsch-index is cited with a remarkable 69 (ISI-Web of Knowledge, 2018)\n\nTwelve out of 286 original scientific papers:\n\n[1] Kalender WA, Klotz E, Süß C. An integral approach to vertebral bone mineral analysis by X-ray computed tomography. Radiology 1987; 164:419-423\n\n[2] Kalender WA, Hebel R. Fast routine for the reduction of artifacts caused by metallic implants in CT images. Radiology 1986; 161(P):345\n\n[3] Kalender WA, Seissler W, Klotz E, Vock P. Spiral volumetric CT with single-breath-hold technique, continuous transport, and continuous scanner rotation. Radiology 1990; 176:181-183\n\n[4] Kalender WA. Thin-section three-dimensional spiral CT: Is isotropic imaging possible?\nRadiology 1995; 197: 578-580\n\n[5] Bautz W, Strotzer M, Lenz M, Dittler HJ, Kalender WA. Preoperative evaluation of the vessels of the upper abdomen with spiral CT: Comparison with conventional CT and arterial DXA. Radiology 1991; 181(P):261\n\n[6] Kachelrieß M, Kalender WA: ECG-correlated image reconstruction from sub second\nmulti-slice spiral CT scans of the heart. Med. Phys. 2000; 27 (8): 1881-1902\n\n[7] Kalender WA, Wolf H, Suess C, Gies M, Greess H, Bautz WA: Dose reduction in CT \nby online tube current control: principles and validation on phantoms and cadavers.\nEur. Radiol. 1999; 9: 323-328\n\n[8] Kalender WA, Deak P, van Straten M, Vollmar SV. Application- and patient size-dependent optimization of x-ray spectra for CT. Med. Phys. 2009; 36:993-1007\n\n[9] Kalender WA. Dose in x-ray computed tomography. Phys Med Biol 2014; 59 R129-R150\n\n[10] Kalender WA, Beister M, Boone JM, Kolditz D, Vollmar SV, Weigel MCC . High-resolution spiral CT of the breast at very low dose: concept and feasibility considerations Eur Radiol 2012; 22: 1-8\n\n[11] Kalender WA, Kolditz D, Steiding C, Ruth V, Lück F, Rößler AC, Wenkel E. Technical feasibility proof for high-resolution low-dose CT of the breast. Eur Radiol 2017; 27:1081-1087\n\n[12] The 4th edition of the textbook “Computed Tomography” by WA Kalender, published \nby Publicis Publishing Erlangen is expected for late 2018.\n\n"}
{"id": "795651", "url": "https://en.wikipedia.org/wiki?curid=795651", "title": "Windbreak", "text": "Windbreak\n\nA windbreak (shelterbelt) is a planting usually made up of one or more rows of trees or shrubs planted in such a manner as to provide shelter from the wind and to protect soil from erosion. They are commonly planted in hedgerows around the edges of fields on farms. If designed properly, windbreaks around a home can reduce the cost of heating and cooling and save energy. Windbreaks are also planted to help keep snow from drifting onto roadways or yards. Farmers sometimes use windbreaks to keep snow drifts on farm land that will provide water when the snow melts in the spring. Other benefits include contributing to a microclimate around crops (with slightly less drying and chilling at night), providing habitat for wildlife, and, in some regions, providing wood if the trees are harvested.\n\nWindbreaks and intercropping can be combined in a farming practice referred to as alleycropping. Fields are planted in rows of different crops surrounded by rows of trees. These trees provide fruit, wood, or protect the crops from the wind. Alley cropping has been particularly successful in India, Africa, and Brazil, where coffee growers have combined farming and forestry.\n\nA further use for a shelterbelt is to screen a farm from a main road or motorway. This improves the farm landscape by reducing the visual incursion of the motorway, mitigating noise from the traffic and providing a safe barrier between farm animals and the road.\n\nThe term \"windbreak\" is also used to describe an article of clothing worn to prevent wind chill; this term is favored by Europeans whereas Americans tend to use the term \"windbreaker\".\n\nFences called \"windbreaks\" are also used. Normally made from cotton, nylon, canvas, and recycled sails, windbreaks tend to have three or more panels held in place with poles that slide into pockets sewn into the panel. The poles are then hammered into the ground and a windbreak is formed. Windbreaks or \"wind fences\" are used to reduce wind speeds over erodible areas such as open fields, industrial stockpiles, and dusty industrial operations. As erosion is proportional to wind speed cubed, a reduction of wind speed of 1/2 (for example) will reduce erosion by over 80%.\n\nIn essence, when the wind encounters a porous obstacle such as a windbreak or shelterbelt, air pressure increases (loosely speaking, air \"piles up\") on the windward side and (conversely) air pressure decreases on the leeward side. As a result, the airstream approaching the barrier is retarded, and a proportion of it is displaced up and over the barrier, resulting in a \"jet\" of higher wind speed aloft. The remainder of the impinging airstream, having been retarded in its approach, now circulates through the barrier to its downstream edge, pushed along by the decrease in pressure across the shelterbelt's width; emerging on the downwind side, that airstream is now further retarded by an adverse pressure gradient, because in the lee of the barrier, with increasing downwind distance air pressure \"recovers\" again to the ambient level. The result is that minimum wind speed occurs not at or within the windbreak, nor at its downwind edge, but further downwind - nominally, at a distance of about 3 to 5 times the windbreak height \"H\". Beyond that point wind speed recovers, aided by downward momentum transport from the overlying, faster-moving stream. From the perspective of the Reynolds-averaged Navier–Stokes equations these effects can be understood as resulting from the loss of momentum caused by the drag of leaves and branches and would be represented by the body force \"f\" (a distributed momentum sink).\n\nNot only is the mean (average) wind speed reduced in the lee of the shelter, the wind is also less gusty, for turbulent wind fluctuations are also damped. As a result, turbulent vertical mixing is weaker in the lee of the barrier than it is upwind, and interesting secondary microclimatic effects result. For instance, by day sensible heat rising from the ground due to the absorption of sunlight (see \"surface energy budget\") is mixed upward less efficiently in the lee of a windbreak, with the result that air temperature near ground is somewhat higher in the lee than on the windward side. Of course this effect is attenuated with increasing downwind distance and indeed, beyond about 8\"H\" downstream a zone may exist that is actually \"cooler\" than upwind.\n\n\n"}
{"id": "24535320", "url": "https://en.wikipedia.org/wiki?curid=24535320", "title": "Yehuda Hiss", "text": "Yehuda Hiss\n\nYehuda Hiss (born c. 1946) is a retired Israeli pathologist. He served as the Chief Pathologist at the Abu Kabir Institute of Forensic Medicine between 1988 and possibly as late as 2005. Hiss has also served as part of the faculty for the Terrorism and Medicine Program at the Institute for Counter-Terrorism (ICT) at IDC Herzliya and in the Department of Pathology for the Sackler Faculty of Medicine at Tel Aviv University.\n\nAs director of the Institute at Abu Kabir, the only place in Israel authorized to conduct autopsies in cases of unnatural death, Hiss conducted the autopsies of and authored the pathology reports for notable figures, including Yitzhak Rabin and Rachel Corrie, among others. His position as director was a subject of controversy. He was dismissed from this position after the legal system took up some of the charges against him. Investigations of the newspaper Al-Ahram, revealed that Hiss had removed organs, bones and other tissues from corpses, against the expressed wishes of family, and had sold many of the organs he removed to medical institutions and universities. He remained the chief pathologist of the Institute and regained his position as director before being dismissed by the Deputy Health Minister Yaakov Litzman on Oct 15 2012.\n\nYehuda Hiss was born in Poland shortly after the end of World War II. His family immigrated to Israel when he was ten years old. He studied medicine in Israel, Italy, Austria, Britain and the United States.\n\nHiss became the director and chief pathologist at Abu Kabir in 1988. As director of the Institute, Hiss was involved with high-profile cases. Hiss performed the autopsy on Yitzhak Rabin after his 1995 assassination; his report was challenged in March 1999 by a group of Israeli academics. In May 2003, he confirmed the identification of the body that washed ashore the beaches of Tel Aviv to be that of Omar Sharif, the accomplice of Asif Hanif, who carried out the Mike's Place suicide bombing the month previous. Hiss told Haaretz that Sharif's DNA was matched to samples provided by two British detectives and the cause of death was determined to be drowning. He also performed the autopsy on Rachel Corrie.\n\nA piece featuring Hiss that appeared in \"The New York Times\" in February 2004 noted Hiss played a \"unique role\" in the response network that Israel had to developed to deal with the more than 100 suicide bombings it had experienced over the three years previous. All the dead from such attacks are brought to the Forensic Institute, and Hiss had been present to attend to, \"the dismembered victims and shattered victims,\" in all but one attack. The same article mentions the controversy surrounding Hiss as follows: \"While the forensic center is praised for its work after bombings, Dr. Hiss has been involved in controversies related to other cases, including allegations that the institute removed organs from corpses without permission from the families. The issue is enormously sensitive because of Judaism's emphasis on burying the whole body. Government inquiries have not resulted in any charges against Dr. Hiss. But in December the Israeli attorney general recommended disciplinary action. The issue is still under consideration, and no sanctions have been imposed so far.\"\n\nIn January 2006, following the carrying out of an autopsy that was ordered by the Israeli courts for a Haredi woman found murdered in her apartment, a riot by dozens of Haredim took place inside the institute where Hiss worked. According to Dr. Benny Davidson, the manager of the institute, \"Even in the institute's darkest days, there wasn't an event this big. They wrecked the entire hall, broke expensive equipment, and destroyed (institute director) Yehuda Hiss' room. The public reaction to the articles on the pathological institute was an abandonment of Hiss.\"\n\nAccording to Rebecca Dube in the \"Forward\", allegations that Hiss and his lab were taking organs from corpses without permission and using them for research or selling them to medical schools were substantiated by the Israeli government. In 1998, after Alistair Sinclair, a Scottish tourist apprehended on suspicion of dealing drugs, died in a holding cell at Ben Gurion International Airport, apparently having hanged himself. After an autopsy overseen by Hiss, the body was returned to his family; a second autopsy performed by pathologists at the University of Glasgow found that the heart and a small bone at the base of his tongue were missing. Jonathan Rosenblum reported that the missing bone from his neck was considered necessary to confirming that he had hanged himself as stated, and that the Sinclair family believed his heart had been used for transplant. The scandal was publicized by media in Israel and Scotland after the family sued.\n\nRosenblum, writing in \"The Jerusalem Post\" in October 2000 cited a 12-page investigative report in November 1999 by the local Tel Aviv newspaper \"Ha'ir\" claiming that medical students at Abu Kabir under Hiss' direction were allowed to practice on bodies sent there for autopsy, and body parts were transferred for transplant without permission from the families. In January 2001, \"Yediot Aharonot\" claimed the institute headed by Hiss had been involved in \"organ sales\" of body parts to universities and medical schools for research and training, citing evidence including the \"price listings\" for body parts.\n\nIsrael's Ministry of Health convened a committee to investigate the claims which found that Hiss had been involved for years in the sale of illegally removed body parts to medical schools. Hiss was never charged with any crime, but was forced to step down from running the state morgue in 2004. According to Rebecca Dube, \"the final straw, apparently, was when the body of a youth killed in a road accident was gnawed upon by a rat in Hiss's lab.\" Every body that ended up in Hiss' morgue, whether Israeli or Palestinian, was fair game for organ harvesting.\n\nHiss ceased being the director of the institute in 2005 when allegations of a trade in organs resurfaced. After Hiss admitted to having removed parts from 125 bodies without authorization, and following a plea bargain with the State of Israel, Israel's attorney-general, decided not to press criminal charges. Hiss was given only a reprimand and continued to hold his position as chief pathologist at Abu Kabir and eventually regained his position as director.\n\nJonathan Cook writing in \"Al-Ahram Weekly\" in 2009 recalls Hiss' history in connection with the \"Aftonbladet\" allegations.\n\n\n"}
