{"id": "21902856", "url": "https://en.wikipedia.org/wiki?curid=21902856", "title": "Air well (condenser)", "text": "Air well (condenser)\n\nAn air well or aerial well is a structure or device that collects water by promoting the condensation of moisture from air. Designs for air wells are many and varied, but the simplest designs are completely passive, require no external energy source and have few, if any, moving parts.\n\nThree principal designs are used for air wells, designated as high mass, radiative, and active:\n\nAll air well designs incorporate a substrate with a temperature sufficiently low so that dew forms. Dew is a form of precipitation that occurs naturally when atmospheric water vapour condenses onto a substrate. It is distinct from fog, in that fog is made of droplets of water that condense around particles in the air. Condensation releases latent heat which must be dissipated in order for water collection to continue.\n\nAn air well requires moisture from the air. Everywhere on Earth, even in deserts, the surrounding atmosphere contains at least some water. According to Beysens and Milimouk: \"The atmosphere contains of fresh water, composed of 98 percent water vapour and 2 percent condensed water (clouds): a figure comparable to the renewable liquid water resources of inhabited lands .\" The quantity of water vapour contained within the air is commonly reported as a relative humidity, and this depends on temperature—warmer air can contain more water vapour than cooler air. When air is cooled to the dew point, it becomes saturated, and moisture will condense on a suitable surface. For instance, the dew temperature of air at and 80 percent relative humidity is . The dew temperature falls to if the relative humidity is 50 percent.\n\nA related, but quite distinct, technique of obtaining atmospheric moisture is the fog fence.\n\nAn air well should not be confused with a dew pond. A dew pond is an artificial pond intended for watering livestock. The name \"dew pond\" (sometimes \"cloud pond\" or \"mist pond\") derives from the widely held belief that the pond was filled by moisture from the air. In fact, dew ponds are primarily filled by rainwater.\n\nA stone mulch can significantly increase crop yields in arid areas. This is most notably the case in the Canary Islands: on the island of Lanzarote there is about of rain each year and there are no permanent rivers. Despite this, substantial crops can be grown by using a mulch of volcanic stones, a trick discovered after volcanic eruptions in 1730. Some credit the stone mulch with promoting dew; although the idea has inspired some thinkers, it seems unlikely that the effect is significant. Rather, plants are able to absorb dew directly from their leaves, and the main benefit of a stone mulch is to reduce water loss from the soil and to eliminate competition from weeds.\n\nBeginning in the early 20th century, a number of inventors experimented with high-mass collectors. Notable investigators were the Russian engineer Friedrich Zibold (sometimes given as Friedrich Siebold), the French bioclimatologist Leon Chaptal, the German-Australian researcher Wolf Klaphake and the Belgian inventor Achille Knapen.\n\nIn 1900, near the site of the ancient Byzantine city of Theodosia, thirteen large piles of stones were discovered by Zibold who was a forester and engineer in charge of this area. Each stone pile covered just over and was about tall. The finds were associated with the remains of terracotta pipes that apparently led to wells and fountains in the city. Zibold concluded that the stacks of stone were condensers that supplied Theodosia with water; and calculated that each air well produced more than each day.\n\nTo verify his hypothesis Zibold constructed a stone-pile condenser at an altitude of on mount Tepe-Oba near the ancient site of Theodosia. Zibold's condenser was surrounded by a wall high, wide, around a bowl-shaped collection area with drainage. He used sea stones in diameter piled high in a truncated cone that was in diameter across the top. The shape of the stone pile allowed a good air flow with only minimal thermal contact between the stones.\n\nZibold's condenser began to operate in 1912 with a maximum daily production that was later estimated to have been – Zibold made no public record of his results at the time. The base developed leaks that forced the experiment to end in 1915 and the site was partially dismantled before being abandoned. (The site was rediscovered in 1993 and cleaned up.) Zibold's condenser was approximately the same size as the ancient stone piles that had been found, and although the yield was very much less than the yield Zibold had calculated for the original structures, the experiment was an inspiration for later developers.\n\nInspired by Zibold's work, Chaptal built a small air well near Montpellier in 1929. Chaptal's condenser was a pyramidal concrete structure square and high, it was filled with of limestone pieces being about in diameter. Small vent holes ringed the top and bottom of the pyramid. These holes could be closed or opened as required to control the flow of air. The structure was allowed to cool during the night, and then warm moist air was let in during the day. Dew formed on the limestone pieces and collected in a reservoir below ground level. The amount of water obtained varied from to per day depending on the atmospheric conditions.\n\nChaptal did not consider his experiment a success. When he retired in 1946, he put the condenser out of order, possibly because he did not want to leave an improper installation to mislead those who might later continue studies on air wells.\n\nWolf Klaphake was a successful chemist working in Berlin during the 1920s and 1930s. During that time, he tested several forms of air wells in Yugoslavia and on Vis Island in the Adriatic Sea. Klaphake's work was inspired by Zibold and by the works of Maimonides, a known Jewish scholar who wrote in Arabic about 1,000 years ago and who mentioned the use of water condensers in Palestine.\n\nKlaphake experimented with a very simple design: an area of mountain slope was cleared and smoothed with a watertight surface. It was shaded by a simple canopy supported by pillars or ridges. The sides of the structure were closed, but the top and bottom edges were left open. At night the mountain slope would cool, and in the day moisture would collect on and run down the smoothed surface. Although the system apparently worked, it was expensive, and Klaphake finally adopted a more compact design based on a masonry structure. This design was a sugarloaf-shaped building, about high, with walls at least thick, with holes on the top and at the bottom. The outer wall was made of concrete to give a high thermal capacity, and the inner surface was made of a porous material such as sandstone. According to Klaphake:\n\nTraces of Klaphake's condensers have been tentatively identified.\n\nIn 1935, Wolf Klaphake and his wife Maria emigrated to Australia. The Klaphakes' decision to emigrate was probably primarily the result of Maria's encounters with Nazi authorities; their decision to settle in Australia (rather than, say, in Britain) was influenced by Wolf's desire to develop a dew condenser. As a dry continent, Australia was likely to need alternative sources of fresh water, and the Premier of South Australia, whom he had met in London, had expressed an interest. Klaphake made a specific proposal for a condenser at the small town of Cook, where there was no supply of potable water. At Cook, the railway company had previously installed a large coal-powered active condenser, but it was prohibitively expensive to run, and it was cheaper to simply transport water. However, the Australian government turned down Klaphake's proposal, and he lost interest in the project.\n\nKnapen, who had previously worked on systems for removing moisture from buildings, was in turn inspired by Chaptal's work and he set about building an ambitiously large \"puits aerien\" (aerial well) on a high hill at Trans-en-Provence in France. Beginning in 1930, Knapen's dew tower took 18 months to build; it still stands today, albeit in dilapidated condition. At the time of its construction, the condenser excited some public interest.\n\nThe tower is high and has massive masonry walls about thick with a number of apertures to let in air. Inside there is a massive column made of concrete. At night, the whole structure is allowed to cool, and during the day warm moist air enters the structure via the high apertures, cools, descends, and leaves the building by the lower apertures. Knapen's intention was that water should condense on the cool inner column. In keeping with Chaptal's finding that the condensing surface must be rough and the surface tension must be sufficiently low that the condensed water can drip, the central column's outer surface was studded with projecting plates of slate. The slates were placed nearly vertically to encourage dripping down to a collecting basin at the bottom of the structure. Unfortunately, the aerial well never achieved anything like its hoped-for performance and produced no more than a few litres of water each day.\n\nBy the end of the twentieth century, the mechanics of how dew condenses were much better understood. The key insight was that low-mass collectors which rapidly lose heat by radiation perform best. A number of researchers worked on this method. In the early 1960s, dew condensers made from sheets of polyethylene supported on a simple frame resembling a ridge tent were used in Israel to irrigate plants. Saplings supplied with dew and very slight rainfall from these collectors survived much better than the control group planted without such aids – they all dried up over the summer. In 1986 in New Mexico condensers made of a special foil produced sufficient water to supply young saplings.\n\nIn 1992 a party of French academics attended a condensed matter conference in Ukraine where physicist Daniel Beysens introduced them to the story of how ancient Theodosia was supplied with water from dew condensers. They were sufficiently intrigued that in 1993 they went to see for themselves. They concluded that the mounds that Zibold identified as dew condensers were in fact ancient burial mounds (a part of the necropolis of ancient Theodosia) and that the pipes were medieval in origin and not associated with the construction of the mounds. They found the remains of Zibold's condenser, which they tidied up and examined closely. Zibold's condenser had apparently performed reasonably well, but in fact his exact results are not at all clear, and it is possible that the collector was intercepting fog, which added significantly to the yield. If Zibold's condenser worked at all, this was probably due to fact that a few stones near the surface of the mound were able to lose heat at night while being thermally isolated from the ground; however, it could never have produced the yield that Zibold envisaged.\n\nFired with enthusiasm, the party returned to France and set up the \"International Organisation for Dew Utilization\" (OPUR), with the specific objective of making dew available as an alternative source of water.\n\nOPUR began a study of dew condensation under laboratory conditions; they developed a special hydrophobic film and experimented with trial installations, including a collector in Corsica. Vital insights included the idea that the mass of the condensing surface should be as low as possible so that it cannot easily retain heat, that it should be protected from unwanted thermal radiation by a layer of insulation, and that it should be hydrophobic, so as to shed condensed moisture readily.\n\nBy the time they were ready for their first practical installation, they heard that one of their members, Girja Sharan, had obtained a grant to construct a dew condenser in Kothara, India. In April 2001, Sharan had incidentally noticed substantial condensation on the roof of a cottage at Toran Beach Resort in the arid coastal region of Kutch, where he was briefly staying. The following year, he investigated the phenomenon more closely and interviewed local people. Financed by the Gujarat Energy Development Agency and the World Bank, Sharan and his team went on to develop passive, radiative condensers for use in the arid coastal region of Kutch. Active commercialisation began in 2006.\n\nSharan tested a wide range of materials and got good results from galvanised iron and aluminium sheets, but found that sheets of the special plastic developed by the OPUR just thick generally worked even better than the metal sheets and were less expensive. The plastic film, known as OPUR foil, is hydrophilic and is made from polyethylene mixed with titanium dioxide and barium sulphate.\n\nThere are three principal approaches to the design of the heat sinks that collect the moisture in air wells: high mass, radiative and active. Early in the twentieth century, there was interest in high-mass air wells, but despite much experimentation including the construction of massive structures, this approach proved to be a failure.\n\nFrom the late twentieth century onwards, there has been much investigation of low-mass, radiative collectors; these have proved to be much more successful.\n\nThe high-mass air well design attempts to cool a large mass of masonry with cool nighttime air entering the structure due to breezes or natural convection. In the day, the warmth of the sun results in increased atmospheric humidity. When moist daytime air enters the air well, it condenses on the presumably cool masonry. None of the high-mass collectors performed well, Knapen's aerial well being a particularly conspicuous example.\n\nThe problem with the high-mass collectors was that they could not get rid of sufficient heat during the night – despite design features intended to ensure that this would happen. While some thinkers have believed that Zibold might have been correct after all, an article in \"Journal of Arid Environments\" discusses why high-mass condenser designs of this type cannot yield useful amounts of water:\n\nAlthough ancient air wells are mentioned in some sources, there is scant evidence for them, and persistent belief in their existence has the character of a modern myth.\n\nA radiative air well is designed to cool a substrate by radiating heat to the night sky. The substrate has a low mass so that it cannot hold onto heat, and it is thermally isolated from any mass, including the ground. A typical radiative collector presents a condensing surface at an angle of 30° from the horizontal. The condensing surface is backed by a thick layer of insulating material such as polystyrene foam and supported above ground level. Such condensers may be conveniently installed on the ridge roofs of low buildings or supported by a simple frame. Although other heights do not typically work quite so well, it may be less expensive or more convenient to mount a collector near to ground level or on a two-story building.\n\nThe radiative condenser illustrated near the start of this article is built near the ground. In the area of north-west India where it is installed dew occurs for 8 months a year, and the installation collects about of dew water over the season with nearly 100 dew-nights. In a year it provides a total of about of potable water for the school which owns and operates the site.\n\nAlthough flat designs have the benefit of simplicity, other designs such as inverted pyramids and cones can be significantly more effective. This is probably because the designs shield the condensing surfaces from unwanted heat radiated by the lower atmosphere, and, being symmetrical, they are not sensitive to wind direction.\n\nNew materials may make even better collectors. One such material is inspired by the Namib Desert beetle, which survives only on the moisture it extracts from the atmosphere. It has been found that its back is coated with microscopic projections: the peaks are hydrophilic and the troughs are hydrophobic. Researchers at the Massachusetts Institute of Technology have emulated this capability by creating a textured surface that combines alternating hydrophobic and hydrophilic materials.\n\nActive atmospheric water collectors have been in use since the commercialisation of mechanical refrigeration. Essentially, all that is required is to cool a heat exchanger below the dew point, and water will be produced. Such water production may take place as a by-product, possibly unwanted, of dehumidification. The air conditioning system of the Burj Khalifa in Dubai, for example, produces an estimated of water each year that is used for irrigating the tower's landscape plantings.\n\nBecause mechanical refrigeration is energy intensive, active collectors are typically restricted to places where there is no supply of water that can be desalinated or purified at a lower cost and that are sufficiently far from a supply of fresh water to make transport uneconomical. Such circumstances are uncommon, and even then large installations such as that tried in the 1930s at Cook in South Australia failed because of the cost of running the installation – it was cheaper to transport water over large distances.\n\nIn the case of small installations, convenience may outweigh cost. There is a wide range of small machines designed to be used in offices that produce a few litres of drinking water from the atmosphere. However, there are circumstances where there really is no source of water other than the atmosphere. For example, in the 1930s, American designers added condenser systems to airships – in this case the air was that emitted by the exhaust of the engines, and so it contained additional water as a product of combustion. The moisture was collected and used as additional ballast to compensate for the loss of weight as fuel was consumed. By collecting ballast in this way, the airship's buoyancy could be kept relatively constant without having to release helium gas, which was both expensive and in limited supply.\n\nMore recently, on the International Space Station, the Zvezda module includes a humidity control system. The water it collects is usually used to supply the Elektron system that electrolyses water into hydrogen and oxygen, but it can be used for drinking in an emergency.\n\nThere are a number of designs that minimise the energy requirements of active condensers:\n\n\n\n\n\n\n"}
{"id": "20115929", "url": "https://en.wikipedia.org/wiki?curid=20115929", "title": "Alcohol Advisory Council of New Zealand", "text": "Alcohol Advisory Council of New Zealand\n\nThe Alcohol Advisory Council of New Zealand (\"ALAC\") was established in 1976, by the government of New Zealand, under the Alcohol Advisory Council Act of 1976, following a report by the Royal Commission of Inquiry into the Sale of Liquor. Its purpose is \"the encouragement and promotion of moderation in the use of liquor, the discouragement and reduction of the misuse of liquor, and the minimisation of the personal, social, and economic harm resulting from the misuse of liquor.\"\n\nALAC is funded by a levy on alcohol produced and imported for sale in New Zealand. The levy is collected by the Customs Service.\n\nALAC has a 2008/09 budget of $12.7m and spent $13m in the 2007 year. \n\nMost of its work revolves around providing information (pamphlets, on-line resources), advertising and education programs.\n\n"}
{"id": "42673840", "url": "https://en.wikipedia.org/wiki?curid=42673840", "title": "Autosomal dominant retinal vasculopathy with cerebral leukodystrophy", "text": "Autosomal dominant retinal vasculopathy with cerebral leukodystrophy\n\nAutosomal Dominant Retinal Vasculopathy with Cerebral Leukodystrophy (AD-RVCL) (previously known also as Cerebroretinal Vasculopathy, CRV, or Hereditary Vascular Retinopathy, HVR or Hereditary Endotheliopathy, Retinopathy, Nephropathy, and Stroke, HERNS) is an inherited condition resulting from a frameshift mutation to the TREX1 gene. This genetically inherited condition affects the retina and the white matter of the central nervous system, resulting in vision loss, lacunar strokes and ultimately dementia. Symptoms commonly begin in the early to mid-forties, and treatments currently aim to manage or alleviate the symptoms rather than treating the underlying cause. The overall prognosis is poor, and death can sometimes occur within 10 years of the first symptoms appearing.\n\n\nThe official name of the TREX1 gene is “three prime repair exonuclease 1.” The normal function of the TREX1 gene is to provide instructions for making the 3-prime repair exonuclease 1 enzyme. This enzyme is a DNA exonuclease, which means it trims molecules of DNA by removing DNA building blocks (nucleotides) from the ends of the molecules. In this way, it breaks down unneeded DNA molecules or fragments that may be generated during genetic material in preparation for cell division, DNA repair, cell death, and other processes.\nChanges (mutations) to the TREX1 gene can result in a range of conditions one of which is AD-RVCL. The mutations to the TREX1 gene are believed to prevent the production of the 3-prime repair exonuclease 1 enzyme. Researchers suggest that the absence of this enzyme may result in an accumulation of unneeded DNA and RNA in cells. These DNA and RNA molecules may be mistaken by cells for those of viral invaders, triggering immune system reactions that result in the symptoms of AD-RVCL.\n\nMutations in the TREX1 gene have also been identified in people with other disorders involving the immune system. These disorders include a chronic inflammatory disease called systemic lupus erythematosus (SLE), including a rare form of SLE called chilblain lupus that mainly affects the skin.\n\nThe TREX1 gene is located on chromosome 3: base pairs 48,465,519 to 48,467,644\n\n\nDuring mitosis, tiny fragments of “scrap” single strand DNA naturally occur inside the cell. Enzymes find and destroy the “scrap” DNA. The TREX1 gene provides the information necessary to create the enzyme that destroys this single strand “scrap” DNA. A mutation in the TREX1 gene causes the enzyme that would destroy the single strand DNA to be less than completely effective. The less than completely effective nature of the enzyme allows “scrap” single strand DNA to build up in the cell. The buildup of “scrap” single strand DNA alerts the immune system that the cell is abnormal.\n\nThe abnormality of the cells with the high concentration of “scrap” DNA triggers a T-cell response and the abnormal cells are destroyed. Because the TREX1 gene is identical in all of the cells in the body the ineffective enzyme allows the accumulation of “scrap” single strand DNA in all of the cells in the body. Eventually, the immune system has destroyed enough of the cells in the walls of the blood vessels that the capillaries burst open. The capillary bursting happens throughout the body but is most recognizable when it happens in the eyes and brain because these are the two places where capillary bursting has the most pronounced effect.\n\nThe main pathologic process centers on small blood vessels that prematurely “drop out” and disappear. The retina of the eye and white matter of the brain are the most sensitive to this pathologic process. Over a five to ten-year period, this vasculopathy (blood vessel pathology) results in vision loss and destructive brain lesions with neurologic deficits and death.\n\nMost recently, AD-RVCL (CRV) has been renamed. The new name is CHARIOT which stands for Cerebral Hereditary Angiopathy with vascular Retinopathy and Impaired Organ function caused by TREX1 mutations. \n\nCurrently, there is no therapy to prevent the blood vessel deterioration.\n\n"}
{"id": "22418887", "url": "https://en.wikipedia.org/wiki?curid=22418887", "title": "Bar code medication administration", "text": "Bar code medication administration\n\nBar code medication administration (BCMA) is a bar code system designed by Glenna Sue Kennick to prevent medication errors in healthcare settings and to improve the quality and safety of medication administration. The overall goals of BCMA are to improve accuracy, prevent errors, and generate online records of medication administration. \n\nBCMA was first implemented in 1995 at the Colmery-O'Neil Veteran Medical Center in Topeka, Kansas, US. It was created by a nurse who was inspired by a car rental service using bar code technology. From 1999 to 2001, the Department of Veterans Affairs promoted the system to 161 facilities. Cummings and others recommend the BCMA system for its reduction of errors. They suggest healthcare settings to consider the system first while they are waiting for radiofrequency identification (RFID). They also pointed out that adopting the system takes a careful plan and a deep change in work patterns. As of the year 2004, hospitals were mandated by the federal government to start using BCMA for all prescription drugs.\n\nIt consists of a bar code reader, a portable or desktop computer with wireless connection, a computer server, and some software. When a nurse gives medication to a patient in a healthcare setting, the nurse can scan the barcode on the patient's wristband on the patient to verify the patient's identity. The nurse can then scan the bar code on medication and use software to verify that he/she is administering the right medication to the right patient at the right dose, through the right route, and at the right time (\"five rights of medication administration\"). Bar code medication administration was designed as an additional check to aid the nurse in administering medications; however, it cannot replace the expertise and professional judgment of the nurse. The implementation of BCMA has shown a decrease in medication administration errors in the healthcare setting.\nBar codes on medication have federal government guidelines that are reflected within the bar code packaging. The first few digits are used to identify the labeler, this code is issued by the Food and Drug Administration. The next section of the label contains the product code, known as the medication, and the last section of the bar code label lists the packager's code for the medication.\n\n"}
{"id": "36116358", "url": "https://en.wikipedia.org/wiki?curid=36116358", "title": "Bioburden", "text": "Bioburden\n\nBioburden is normally defined as the number of bacteria living on a surface that has not been sterilized.\n\nThe term is most often used in the context of bioburden testing, also known as microbial limit testing, which is performed on pharmaceutical products and medical products for quality control purposes. Products or components used in the pharmaceutical or medical field require control of microbial levels during processing and handling. Bioburden or microbial limit testing on these products proves that these requirements have been met. Bioburden testing for medical devices made or used in the USA is governed by Title 21 of the Code of Federal Regulations and worldwide by ISO 11737.\n\nThe aim of bioburden testing is to measure the total number of viable micro-organisms (total microbial count) on a medical device prior to its final sterilization before implantation or use.\n\n21 C.F.R. 211.110 (a)(6) states that bioburden in-process testing must be conducted pursuant to written procedures during the manufacturing process of drug products. The United States Pharmacopeia (USP) outlines several tests that can be done to quantitatively determine the bioburden of non-sterile drug products.\n\nIt is important when conducting these tests to ensure that the testing method does not either introduce bacteria into the test sample or kill bacteria in the test sample. To prepare drug products for testing, they must be dissolved in certain substances based on their \"physical characteristics.\" For example, a water-soluble drug product should be dissolved in \"Buffered Sodium Chloride-Peptone Solution pH 7.0, Phosphate Buffer Solution pH 7.2, or Soybean-Casein Digest Broth.\"\n\nThe Membrane-Filtration Method and Plate Count Method can be used to measure the number of microbes in a sample. In the Membrane-Filtration Method, the sample is passed through a membrane filter with a pore size of 0.45 micrometers or less. The membrane filter is then placed onto Soybean-Casein Digest Agar and incubated in order to be able to determine the total aerobic microbial count (TAMC).\n\nIn the Plate Count Method, the sample of drug product to be tested and Soybean-Casein Digest Broth is poured into a Petri dish. The Petri dish is then incubated. The most probable number method (MPN) can also be performed for products considered to have a low bioburden. The MPN is considered to be one of the least accurate tests.\n\nThe bioburden quantification is expressed in colony forming unit (CFU). There are generally established guidelines for the maximum CFU that a drug product can contain. Contact plates or sterile swabs can also be used to test for microbes on a surface when compounding sterile products to ensure compliance with USP 797.\n\nBioburden is also associated with biofouling, where microbes collect on the surface of a device or inside of fan cooled equipment. In healthcare settings, this increases the risk of Healthcare-associated infections (HAIs) or Hospital-acquired infections as pathogens can be spread through contact or through the air to new patients and hospital staff. Fan cooled system are generally avoided in critical care and operating rooms, thus relying on natural convection or liquid cooling to cool devices and equipment. Clean rooms (surgical operating rooms, for example) are also required to maintain positive air pressure so that air may leave those rooms, but contaminated air cannot enter from adjacent spaces. HEPA filters are also used to collect airborne pathogens larger than 0.3 microns.\n"}
{"id": "18978770", "url": "https://en.wikipedia.org/wiki?curid=18978770", "title": "Birth control", "text": "Birth control\n\nBirth control, also known as contraception and fertility control, is a method or device used to prevent pregnancy. Birth control has been used since ancient times, but effective and safe methods of birth control only became available in the 20th century. Planning, making available, and using birth control is called family planning. Some cultures limit or discourage access to birth control because they consider it to be morally, religiously, or politically undesirable.\nThe most effective methods of birth control are sterilization by means of vasectomy in males and tubal ligation in females, intrauterine devices (IUDs), and implantable birth control. This is followed by a number of hormone-based methods including oral pills, patches, vaginal rings, and injections. Less effective methods include physical barriers such as condoms, diaphragms and birth control sponges and fertility awareness methods. The least effective methods are spermicides and withdrawal by the male before ejaculation. Sterilization, while highly effective, is not usually reversible; all other methods are reversible, most immediately upon stopping them. Safe sex practices, such as with the use of male or female condoms, can also help prevent sexually transmitted infections. Other methods of birth control do not protect against sexually transmitted diseases. Emergency birth control can prevent pregnancy if taken within the 72 to 120 hours after unprotected sex. Some argue not having sex as a form of birth control, but abstinence-only sex education may increase teenage pregnancies if offered without birth control education, due to non-compliance.\nIn teenagers, pregnancies are at greater risk of poor outcomes. Comprehensive sex education and access to birth control decreases the rate of unwanted pregnancies in this age group. While all forms of birth control can generally be used by young people, long-acting reversible birth control such as implants, IUDs, or vaginal rings are more successful in reducing rates of teenage pregnancy. After the delivery of a child, a woman who is not exclusively breastfeeding may become pregnant again after as few as four to six weeks. Some methods of birth control can be started immediately following the birth, while others require a delay of up to six months. In women who are breastfeeding, progestin-only methods are preferred over combined oral birth control pills. In women who have reached menopause, it is recommended that birth control be continued for one year after the last period.\nAbout 222 million women who want to avoid pregnancy in developing countries are not using a modern birth control method. Birth control use in developing countries has decreased the number of deaths during or around the time of pregnancy by 40% (about 270,000 deaths prevented in 2008) and could prevent 70% if the full demand for birth control were met. By lengthening the time between pregnancies, birth control can improve adult women's delivery outcomes and the survival of their children. In the developing world women's earnings, assets, weight, and their children's schooling and health all improve with greater access to birth control. Birth control increases economic growth because of fewer dependent children, more women participating in the workforce, and less use of scarce resources.\n\nBirth control methods include barrier methods, hormonal birth control, intrauterine devices (IUDs), sterilization, and behavioral methods. They are used before or during sex while emergency contraceptives are effective for up to five days after sex. Effectiveness is generally expressed as the percentage of women who become pregnant using a given method during the first year, and sometimes as a lifetime failure rate among methods with high effectiveness, such as tubal ligation.\n\nThe most effective methods are those that are long acting and do not require ongoing health care visits. Surgical sterilization, implantable hormones, and intrauterine devices all have first-year failure rates of less than 1%. Hormonal contraceptive pills, patches or vaginal rings, and the lactational amenorrhea method (LAM), if adhered to strictly, can also have first-year (or for LAM, first-6-month) failure rates of less than 1%. With typical use, first-year failure rates are considerably high, at 9%, due to inconsistent use. Other methods such as condoms, diaphragms, and spermicides have higher first-year failure rates even with perfect usage. The American Academy of Pediatrics recommends long acting reversible birth control as first line for young individuals.\n\nWhile all methods of birth control have some potential adverse effects, the risk is less than that of pregnancy. After stopping or removing many methods of birth control, including oral contraceptives, IUDs, implants and injections, the rate of pregnancy during the subsequent year is the same as for those who used no birth control.\n\nFor individuals with specific health problems, certain forms of birth control may require further investigations. For women who are otherwise healthy, many methods of birth control should not require a medical exam—including birth control pills, injectable or implantable birth control, and condoms. For example, a pelvic exam, breast exam, or blood test before starting birth control pills does not appear to affect outcomes. In 2009, the World Health Organization (WHO) published a detailed list of medical eligibility criteria for each type of birth control.\n\nHormonal contraception is available in a number of different forms, including oral pills, implants under the skin, injections, patches, IUDs and a vaginal ring. They are currently available only for women, although hormonal contraceptives for men have been and are being clinically tested. There are two types of oral birth control pills, the combined oral contraceptive pills (which contain both estrogen and a progestin) and the progestogen-only pills (sometimes called minipills). If either is taken during pregnancy, they do not increase the risk of miscarriage nor cause birth defects. Both types of birth control pills prevent fertilization mainly by inhibiting ovulation and thickening cervical mucus. They may also change the lining of the uterus and thus decrease implantation. Their effectiveness depends on the user's adherence to taking the pills.\nCombined hormonal contraceptives are associated with a slightly increased risk of venous and arterial blood clots. Venous clots, on average, increase from 2.8 to 9.8 per 10,000 women years which is still less than that associated with pregnancy. Due to this risk, they are not recommended in women over 35 years of age who continue to smoke. Due to the increased risk, they are included in decision tools such as the DASH score and PERC rule used to predict the risk of blood clots.\n\nThe effect on sexual desire is varied, with increase or decrease in some but with no effect in most. Combined oral contraceptives reduce the risk of ovarian cancer and endometrial cancer and do not change the risk of breast cancer. They often reduce menstrual bleeding and painful menstruation cramps. The lower doses of estrogen released from the vaginal ring may reduce the risk of breast tenderness, nausea, and headache associated with higher dose estrogen products.\nProgestin-only pills, injections and intrauterine devices are not associated with an increased risk of blood clots and may be used by women with a history of blood clots in their veins. In those with a history of arterial blood clots, non-hormonal birth control or a progestin-only method other than the injectable version should be used. Progestin-only pills may improve menstrual symptoms and can be used by breastfeeding women as they do not affect milk production. Irregular bleeding may occur with progestin-only methods, with some users reporting no periods. The progestins drospirenone and desogestrel minimize the androgenic side effects but increase the risks of blood clots and are thus not first line. The perfect use first-year failure rate of injectable progestin is 0.2%; the typical use first failure rate is 6%.\n\nBarrier contraceptives are devices that attempt to prevent pregnancy by physically preventing sperm from entering the uterus. They include male condoms, female condoms, cervical caps, diaphragms, and contraceptive sponges with spermicide.\n\nGlobally, condoms are the most common method of birth control. Male condoms are put on a man's erect penis and physically block ejaculated sperm from entering the body of a sexual partner. Modern condoms are most often made from latex, but some are made from other materials such as polyurethane, or lamb's intestine. Female condoms are also available, most often made of nitrile, latex or polyurethane. Male condoms have the advantage of being inexpensive, easy to use, and have few adverse effects. Making condoms available to teenagers does not appear to affect the age of onset of sexual activity or its frequency. In Japan, about 80% of couples who are using birth control use condoms, while in Germany this number is about 25%, and in the United States it is 18%.\n\nMale condoms and the diaphragm with spermicide have typical use first-year failure rates of 18% and 12%, respectively. With perfect use condoms are more effective with a 2% first-year failure rate versus a 6% first-year rate with the diaphragm. Condoms have the additional benefit of helping to prevent the spread of some sexually transmitted infections such as HIV/AIDS, however, condoms made from animal intestine do not.\n\nContraceptive sponges combine a barrier with a spermicide. Like diaphragms, they are inserted vaginally before intercourse and must be placed over the cervix to be effective. Typical failure rates during the first year depend on whether or not a woman has previously given birth, being 24% in those who have and 12% in those who have not. The sponge can be inserted up to 24 hours before intercourse and must be left in place for at least six hours afterward. Allergic reactions and more severe adverse effects such as toxic shock syndrome have been reported.\n\nThe current intrauterine devices (IUD) are small devices, often 'T'-shaped, containing either copper or levonorgestrel, which are inserted into the uterus. They are one form of long-acting reversible contraception which are the most effective types of reversible birth control. Failure rates with the copper IUD is about 0.8% while the levonorgestrel IUD has a failure rates of 0.2% in the first year of use. Among types of birth control, they, along with birth control implants, result in the greatest satisfaction among users. As of 2007, IUDs are the most widely used form of reversible contraception, with more than 180 million users worldwide.\n\nEvidence supports effectiveness and safety in adolescents and those who have and have not previously had children. IUDs do not affect breastfeeding and can be inserted immediately after delivery. They may also be used immediately after an abortion. Once removed, even after long term use, fertility returns to normal immediately.\n\nWhile copper IUDs may increase menstrual bleeding and result in more painful cramps, hormonal IUDs may reduce menstrual bleeding or stop menstruation altogether. Cramping can be treated with painkillers like non-steroidal anti-inflammatory drugs. Other potential complications include expulsion (2–5%) and rarely perforation of the uterus (less than 0.7%). A previous model of the intrauterine device (the Dalkon shield) was associated with an increased risk of pelvic inflammatory disease, however the risk is not affected with current models in those without sexually transmitted infections around the time of insertion. IUDs appear to decrease the risk of ovarian cancer.\n\nSurgical sterilization is available in the form of tubal ligation for women and vasectomy for men. There are no significant long term side effects, and tubal ligation decreases the risk of ovarian cancer. Short term complications are twenty times less likely from a vasectomy than a tubal ligation. After a vasectomy, there may be swelling and pain of the scrotum which usually resolves in one or two weeks. With tubal ligation, complications occur in 1 to 2 percent of procedures with serious complications usually due to the anesthesia. Neither method offers protection from sexually transmitted infections.\n\nThis decision may cause regret in some men and women. Of women aged over 30 who have undergone tubal ligation, about 5% regret their decision, as compared with 20% of women aged under 30. By contrast, less than 5% of men are likely to regret sterilization. Men who are more likely to regret sterilization are younger, have young or no children, or have an unstable marriage. In a survey of biological parents, 9% stated they would not have had children if they were able to do it over again.\n\nAlthough sterilization is considered a permanent procedure, it is possible to attempt a tubal reversal to reconnect the fallopian tubes or a vasectomy reversal to reconnect the vasa deferentia. In women, the desire for a reversal is often associated with a change in spouse. Pregnancy success rates after tubal reversal are between 31 and 88 percent, with complications including an increased risk of ectopic pregnancy. The number of males who request reversal is between 2 and 6 percent. Rates of success in fathering another child after reversal are between 38 and 84 percent; with success being lower the longer the time period between the vasectomy and the reversal. Sperm extraction followed by in vitro fertilization may also be an option in men.\n\nBehavioral methods involve regulating the timing or method of intercourse to prevent introduction of sperm into the female reproductive tract, either altogether or when an egg may be present. If used perfectly the first-year failure rate may be around 3.4%, however if used poorly first-year failure rates may approach 85%.\n\nFertility awareness methods involve determining the most fertile days of the menstrual cycle and avoiding unprotected intercourse. Techniques for determining fertility include monitoring basal body temperature, cervical secretions, or the day of the cycle. They have typical first-year failure rates of 24%; perfect use first-year failure rates depend on which method is used and range from 0.4% to 5%. The evidence on which these estimates are based, however, is poor as the majority of people in trials stop their use early. Globally, they are used by about 3.6% of couples. If based on both basal body temperature and another primary sign, the method is referred to as symptothermal. First-year failure rates of 20% overall and 0.4% for perfect use have been reported in clinical studies of the symptothermal method. A number of fertility tracking apps are available, as of 2016, but they are more commonly designed to assist those trying to get pregnant rather than prevent pregnancy.\n\nThe withdrawal method (also known as coitus interruptus) is the practice of ending intercourse (\"pulling out\") before ejaculation. The main risk of the withdrawal method is that the man may not perform the maneuver correctly or in a timely manner. First-year failure rates vary from 4% with perfect usage to 22% with typical usage. It is not considered birth control by some medical professionals.\n\nThere is little data regarding the sperm content of pre-ejaculatory fluid. While some tentative research did not find sperm, one trial found sperm present in 10 out of 27 volunteers. The withdrawal method is used as birth control by about 3% of couples.\n\nSexual abstinence may be used as a form of birth control, meaning either not engaging in any type of sexual activity, or specifically not engaging in vaginal intercourse, while engaging in other forms of non-vaginal sex. Complete sexual abstinence is 100% effective in preventing pregnancy. However, among those who take a pledge to abstain from premarital sex, as many as 88% who engage in sex, do so prior to marriage. The choice to abstain from sex cannot protect against pregnancy as a result of rape, and public health efforts emphasizing abstinence to reduce unwanted pregnancy may have limited effectiveness, especially in developing countries and among disadvantaged groups.\n\nDeliberate non-penetrative sex without vaginal sex or deliberate oral sex without vaginal sex are also sometimes considered birth control. While this generally avoids pregnancy, pregnancy can still occur with intercrural sex and other forms of penis-near-vagina sex (genital rubbing, and the penis exiting from anal intercourse) where sperm can be deposited near the entrance to the vagina and can travel along the vagina's lubricating fluids.\n\nAbstinence-only sex education does not reduce teenage pregnancy. Teen pregnancy rates and STI rates are generally the same or higher in states where students are given abstinence-only education, as compared with comprehensive sex education. Some authorities recommend that those using abstinence as a primary method have backup methods available (such as condoms or emergency contraceptive pills).\n\nThe lactational amenorrhea method involves the use of a woman's natural postpartum infertility which occurs after delivery and may be extended by breastfeeding. This usually requires the presence of no periods, exclusively breastfeeding the infant, and a child younger than six months. The World Health Organization states that if breastfeeding is the infant's only source of nutrition, the failure rate is 2% in the six months following delivery. Six uncontrolled studies of lactational amenorrhea method users found failure rates at 6 months postpartum between 0% and 7.5%. Failure rates increase to 4–7% at one year and 13% at two years. Feeding formula, pumping instead of nursing, the use of a pacifier, and feeding solids all increase its failure rate. In those who are exclusively breastfeeding, about 10% begin having periods before three months and 20% before six months. In those who are not breastfeeding, fertility may return four weeks after delivery.\n\nEmergency contraceptive methods are medications (sometimes misleadingly referred to as \"morning-after pills\") or devices used after unprotected sexual intercourse with the hope of preventing pregnancy. They work primarily by preventing ovulation or fertilization. They are unlikely to affect implantation, but this has not been completely excluded. A number of options exist, including high dose birth control pills, levonorgestrel, mifepristone, ulipristal and IUDs. Levonorgestrel pills, when used within 3 days, decrease the chance of pregnancy after a single episode of unprotected sex or condom failure by 70% (resulting in a pregnancy rate of 2.2%). Ulipristal, when used within 5 days, decreases the chance of pregnancy by about 85% (pregnancy rate 1.4%) and is more effective than levonorgestrel. Mifepristone is also more effective than levonorgestrel, while copper IUDs are the most effective method. IUDs can be inserted up to five days after intercourse and prevent about 99% of pregnancies after an episode of unprotected sex (pregnancy rate of 0.1 to 0.2%). This makes them the most effective form of emergency contraceptive. In those who are overweight or obese, levonorgestrel is less effective and an IUD or ulipristal is recommended.\n\nProviding emergency contraceptive pills to women in advance does not affect rates of sexually transmitted infections, condom use, pregnancy rates, or sexual risk-taking behavior. All methods have minimal side effects.\n\nDual protection is the use of methods that prevent both sexually transmitted infections and pregnancy. This can be with condoms either alone or along with another birth control method or by the avoidance of penetrative sex.\n\nIf pregnancy is a high concern, using two methods at the same time is reasonable. For example, two forms of birth control are recommended in those taking the anti-acne drug isotretinoin or anti-epileptic drugs like carbamazepine, due to the high risk of birth defects if taken during pregnancy.\n\nContraceptive use in developing countries is estimated to have decreased the number of maternal deaths by 40% (about 270,000 deaths prevented in 2008) and could prevent 70% of deaths if the full demand for birth control were met. These benefits are achieved by reducing the number of unplanned pregnancies that subsequently result in unsafe abortions and by preventing pregnancies in those at high risk.\n\nBirth control also improves child survival in the developing world by lengthening the time between pregnancies. In this population, outcomes are worse when a mother gets pregnant within eighteen months of a previous delivery. Delaying another pregnancy after a miscarriage however does not appear to alter risk and women are advised to attempt pregnancy in this situation whenever they are ready.\n\nTeenage pregnancies, especially among younger teens, are at greater risk of adverse outcomes including early birth, low birth weight, and death of the infant. In the United States 82% of pregnancies in those between 15 and 19 are unplanned. Comprehensive sex education and access to birth control are effective in decreasing pregnancy rates in this age group.\n\nIn the developing world, birth control increases economic growth due to there being fewer dependent children and thus more women participating in or increased contribution to the workforce. Women's earnings, assets, body mass index, and their children's schooling and body mass index all improve with greater access to birth control. Family planning, via the use of modern birth control, is one of the most cost-effective health interventions. For every dollar spent, the United Nations estimates that two to six dollars are saved. These cost savings are related to preventing unplanned pregnancies and decreasing the spread of sexually transmitted illnesses. While all methods are beneficial financially, the use of copper IUDs resulted in the greatest savings.\n\nThe total medical cost for a pregnancy, delivery and care of a newborn in the United States is on average $21,000 for a vaginal delivery and $31,000 for a caesarean delivery as of 2012. In most other countries, the cost is less than half. For a child born in 2011, an average US family will spend $235,000 over 17 years to raise them.\n\nGlobally, as of 2009, approximately 60% of those who are married and able to have children use birth control. How frequently different methods are used varies widely between countries. The most common method in the developed world is condoms and oral contraceptives, while in Africa it is oral contraceptives and in Latin America and Asia it is sterilization. In the developing world overall, 35% of birth control is via female sterilization, 30% is via IUDs, 12% is via oral contraceptives, 11% is via condoms, and 4% is via male sterilization.\n\nWhile less used in the developed countries than the developing world, the number of women using IUDs as of 2007 was more than 180 million. Avoiding sex when fertile is used by about 3.6% of women of childbearing age, with usage as high as 20% in areas of South America. As of 2005, 12% of couples are using a male form of birth control (either condoms or a vasectomy) with higher rates in the developed world. Usage of male forms of birth control has decreased between 1985 and 2009. Contraceptive use among women in Sub-Saharan Africa has risen from about 5% in 1991 to about 30% in 2006.\n\nAs of 2012, 57% of women of childbearing age want to avoid pregnancy (867 of 1,520 million). About 222 million women however were not able to access birth control, 53 million of whom were in sub-Saharan Africa and 97 million of whom were in Asia. This results in 54 million unplanned pregnancies and nearly 80,000 maternal deaths a year. Part of the reason that many women are without birth control is that many countries limit access due to religious or political reasons, while another contributor is poverty. Due to restrictive abortion laws in Sub-Saharan Africa, many women turn to unlicensed abortion providers for unintended pregnancy, resulting in about 2–4% obtaining unsafe abortions each year.\n\nThe Egyptian Ebers Papyrus from 1550 BC and the Kahun Papyrus from 1850 BC have within them some of the earliest documented descriptions of birth control: the use of honey, acacia leaves and lint to be placed in the vagina to block sperm. Silphium, a species of giant fennel native to north Africa, may have been used as birth control in ancient Greece and the ancient Near East. Due to its supposed desirability, by the first century AD, it had become so rare that it was worth more than its weight in silver and, by late antiquity, it was fully extinct. Most methods of birth control used in antiquity were probably ineffective.\n\nThe ancient Greek philosopher Aristotle ( 384–322 BC) recommended applying cedar oil to the womb before intercourse, a method which was probably only effective on occasion. A Hippocratic text \"On the Nature of Women\" recommended that a woman drink a copper salt dissolved in water, which it claimed would prevent pregnancy for a year. This method was not only ineffective, but also dangerous, as the later medical writer Soranus of Ephesus ( 98–138 AD) pointed out. Soranus attempted to list reliable methods of birth control based on rational principles. He rejected the use of superstition and amulets and instead prescribed mechanical methods such as vaginal plugs and pessaries using wool as a base covered in oils or other gummy substances. Many of Soranus's methods were probably also ineffective.\n\nIn medieval Europe, any effort to halt pregnancy was deemed immoral by the Catholic Church, although it is believed that women of the time still used a number of birth control measures, such as coitus interruptus and inserting lily root and rue into the vagina. Women in the Middle Ages were also encouraged to tie weasel testicles around their thighs during sex to prevent pregnancy. The oldest condoms discovered to date were recovered in the ruins of Dudley Castle in England, and are dated back to 1640. They were made of animal gut, and were most likely used to prevent the spread of sexually transmitted diseases during the English Civil War. Casanova, living in 18th century Italy, described the use of a lambskin covering to prevent pregnancy; however, condoms only became widely available in the 20th century.\n\nThe birth control movement developed during the 19th and early 20th centuries. The Malthusian League, based on the ideas of Thomas Malthus, was established in 1877 in the United Kingdom to educate the public about the importance of family planning and to advocate for getting rid of penalties for promoting birth control. It was founded during the \"Knowlton trial\" of Annie Besant and Charles Bradlaugh, who were prosecuted for publishing on various methods of birth control.\n\nIn the United States, Margaret Sanger and Otto Bobsein popularized the phrase \"birth control\" in 1914. Sanger primarily advocated for birth control on the idea that it would prevent women from seeking unsafe abortions, but during her lifetime, she began to campaign for it on the grounds that it would reduce mental and physical defects. She was mainly active in the United States but had gained an international reputation by the 1930s. At the time, under the Comstock Law, distribution of birth control information was illegal. She jumped bail in 1914 after her arrest for distributing birth control information and left the United States for the United Kingdom. In the U.K., Sanger, influenced by Havelock Ellis, further developed her arguments for birth control. She believed women needed to enjoy sex without fearing a pregnancy. During her time abroad, Sanger also saw a more flexible diaphragm in a Dutch clinic, which she thought was a better form of contraceptive. Once Sanger returned to the United States, she established a short-lived birth-control clinic with the help of her sister, Ethel Bryne, based in the Brownville section of Brooklyn, New York in 1916. It was shut down after eleven days and resulted in her arrest. The publicity surrounding the arrest, trial, and appeal sparked birth control activism across the United States. Besides her sister, Sanger was helped in the movement by her first husband, William Sanger, who distributed copies of “Family Limitation.” Sanger’s second husband, James Noah H. Slee, would also later become involved in the movement, acting as its main funder.\n\nThe first permanent birth-control clinic was established in Britain in 1921 by Marie Stopes working with the Malthusian League. The clinic, run by midwives and supported by visiting doctors, offered women's birth-control advice and taught them the use of a cervical cap. Her clinic made contraception acceptable during the 1920s by presenting it in scientific terms. In 1921, Sanger founded the American Birth Control League, which later became the Planned Parenthood Federation of America. In 1924 the Society for the Provision of Birth Control Clinics was founded to campaign for municipal clinics; this led to the opening of a second clinic in Greengate, Salford in 1926. Throughout the 1920s, Stopes and other feminist pioneers, including Dora Russell and Stella Browne, played a major role in breaking down taboos about sex. In April 1930 the Birth Control Conference assembled 700 delegates and was successful in bringing birth control and abortion into the political sphere – three months later, the Ministry of Health, in the United Kingdom, allowed local authorities to give birth-control advice in welfare centres.\n\nThe National Birth Control Association was founded in Britain in 1931, and became the Family Planning Association eight years later. The Association amalgamated several British birth control-focused groups into 'a central organisation' for administering and overseeing birth control in Britain. The group incorporated the Birth Control Investigation Committee, a collective of physicians and scientists that was founded to investigate scientific and medical aspects of contraception with 'neutrality and impartiality'. Subsequently, the Association effected a series of 'pure' and 'applied' product and safety standards that manufacturers must meet to ensure their contraceptives could be prescribed as part of the Association's standard two-part-technique combining ‘a rubber appliance to protect the mouth of the womb’ with a ‘chemical preparation capable of destroying... sperm’. Between 1931 and 1959, the Association founded and funded a series of tests to assess chemical efficacy and safety and rubber quality. These tests became the basis for the Association's Approved List of contraceptives, which was launched in 1937, and went on to become an annual publication that the expanding network of FPA clinics relied upon as a means to 'establish facts [about contraceptives] and to publish these facts as a basis on which a sound public and scientific opinion can be built'.\n\nIn 1936 the U.S. court ruled in U.S. v. One Package that medically prescribing contraception to save a person's life or well-being was not illegal under the Comstock Law; following this decision, the American Medical Association Committee on Contraception revoked its 1936 statement condemning birth control. A national survey in 1937 showed 71 percent of the adult population supported the use of contraception. By 1938 347 birth control clinics were running in the United States despite their advertisement still being illegal. First Lady Eleanor Roosevelt publicly supported birth control and family planning. In 1966, President Lyndon B. Johnson started endorsing public funding for family planning services, and the Federal Government began subsidizing birth control services for low-income families. The Affordable Care Act, passed into law on March 23, 2010 under President Barack Obama, requires all plans in the Health Insurance Marketplace to cover contraceptive methods. These include barrier methods, hormonal methods, implanted devices, emergency contraceptives, and sterilization procedures.\n\nIn 1909, Richard Richter developed the first intrauterine device made from silkworm gut, which was further developed and marketed in Germany by Ernst Gräfenberg in the late 1920s. In 1951, a chemist, named Carl Djerassi from Mexico City made the hormones in progesterone pills using Mexican yams. Djerassi had chemically created the pill but was not equipped to distribute it to patients. Meanwhile, Gregory Pincus and John Rock with help from the Planned Parenthood Federation of America developed the first birth control pills in the 1950s, such as mestranol/noretynodrel, which became publicly available in the 1960s through the Food and Drug Administration under the name \"Enovid\". Medical abortion became an alternative to surgical abortion with the availability of prostaglandin analogs in the 1970s and mifepristone in the 1980s.\n\nHuman rights agreements require most governments to provide family planning and contraceptive information and services. These include the requirement to create a national plan for family planning services, remove laws that limit access to family planning, ensure that a wide variety of safe and effective birth control methods are available including emergency contraceptives, make sure there are appropriately trained healthcare providers and facilities at an affordable price, and create a process to review the programs implemented. If governments fail to do the above it may put them in breach of binding international treaty obligations.\n\nIn the United States, the 1965 Supreme Court decision \"Griswold v. Connecticut\" overturned a state law prohibiting dissemination of contraception information based on a constitutional right to privacy for marital relationships. In 1971, \"Eisenstadt v. Baird\" extended this right to privacy to single people.\n\nIn 2010, the United Nations launched the \"Every Woman Every Child\" movement to assess the progress toward meeting women's contraceptive needs. The initiative has set a goal of increasing the number of users of modern birth control by 120 million women in the world's 69 poorest countries by the year 2020. Additionally, they aim to eradicate discrimination against girls and young women who seek contraceptives. The American Congress of Obstetricians and Gynecologists (ACOG) recommended in 2014 that oral birth control pills should be over the counter medications.\n\nSince at least the 1870s, American religious, medical, legislative, and legal commentators have debated contraception laws. Ana Garner and Angela Michel have found that in these discussions men often attach reproductive rights to moral and political matters, as part of an ongoing attempt to regulate human bodies. In press coverage between 1873–2013 they found a divide between institutional ideology and real-life experiences of women.\n\nReligions vary widely in their views of the ethics of birth control. The Roman Catholic Church officially only accepts natural family planning, although large numbers of Catholics in developed countries accept and use modern methods of birth control. Among Protestants, there is a wide range of views from supporting none, such as in the Quiverfull movement, to allowing all methods of birth control. Views in Judaism range from the stricter Orthodox sect, which prohibits all methods of birth control, to the more relaxed Reform sect, which allows most. Hindus may use both natural and modern contraceptives. A common Buddhist view is that preventing conception is acceptable, while intervening after conception has occurred is not. In Islam, contraceptives are allowed if they do not threaten health, although their use is discouraged by some.\n\nSeptember 26 is World Contraception Day, devoted to raising awareness and improving education about sexual and reproductive health, with a vision of \"a world where every pregnancy is wanted.\" It is supported by a group of governments and international NGOs, including the Office of Population Affairs, the Asian Pacific Council on Contraception, Centro Latinamericano Salud y Mujer, the European Society of Contraception and Reproductive Health, the German Foundation for World Population, the International Federation of Pediatric and Adolescent Gynecology, International Planned Parenthood Federation, the Marie Stopes International, Population Services International, the Population Council, the United States Agency for International Development (USAID), and Women Deliver.\n\nThere are a number of common misconceptions regarding sex and pregnancy. Douching after sexual intercourse is not an effective form of birth control. Additionally, it is associated with a number of health problems and thus is not recommended. Women can become pregnant the first time they have sexual intercourse and in any sexual position. It is possible, although not very likely, to become pregnant during menstruation.\n\nImprovements of existing birth control methods are needed, as around half of those who get pregnant unintentionally are using birth control at the time. A number of alterations of existing contraceptive methods are being studied, including a better female condom, an improved diaphragm, a patch containing only progestin, and a vaginal ring containing long-acting progesterone. This vaginal ring appears to be effective for three or four months and is currently available in some areas of the world. For women who rarely have sex, the taking of the hormonal birth control levonorgestrel around the time of sex looks promising.\n\nA number of methods to perform sterilization via the cervix are being studied. One involves putting quinacrine in the uterus which causes scarring and infertility. While the procedure is inexpensive and does not require surgical skills, there are concerns regarding long-term side effects. Another substance, polidocanol, which functions in the same manner is being looked at. A device called Essure, which expands when placed in the fallopian tubes and blocks them, was approved in the United States in 2002.\n\nMethods of male birth control include condoms, vasectomies and withdrawal. Between 25 and 75% of males who are sexually active would use hormonal birth control if it was available for them. A number of hormonal and non-hormonal methods are in trials, and there is some research looking at the possibility of contraceptive vaccines.\n\nA reversible surgical method under investigation is reversible inhibition of sperm under guidance (RISUG) which consists of injecting a polymer gel, styrene maleic anhydride in dimethyl sulfoxide, into the vas deferens. An injection with sodium bicarbonate washes out the substance and restores fertility. Another is an intravas device which involves putting a urethane plug into the vas deferens to block it. A combination of an androgen and a progestin seems promising, as do selective androgen receptor modulators. Ultrasound and methods to heat the testicles have undergone preliminary studies.\n\nNeutering or spaying, which involves removing some of the reproductive organs, is often carried out as a method of birth control in household pets. Many animal shelters require these procedures as part of adoption agreements. In large animals the surgery is known as castration.\n\nBirth control is also being considered as an alternative to hunting as a means of controlling overpopulation in wild animals. Contraceptive vaccines have been found to be effective in a number of different animal populations. Kenyan goat herders fix a skirt, called an olor, to male goats to prevent them from impregnating female goats.\n\n\n"}
{"id": "25253589", "url": "https://en.wikipedia.org/wiki?curid=25253589", "title": "British birth cohort studies", "text": "British birth cohort studies\n\nBirth cohort studies in Britain include four long-term medical and social studies, carried out over the lives of a group of participants, from birth. Two of these studies have continued for over 50 years.\n\n\nThe studies involve repeated surveys of large numbers of individuals (typically around 17,000) from birth and throughout their lives. They have collected information on education and employment, family and parenting, physical and mental health, and social attitudes, as well as applying cognitive tests at various ages.\n\nThey are longitudinal studies that follow the same groups of people throughout their lives. As such, they enable research exploring how histories of health, wealth, education, family and employment are interwoven for individuals, vary between them and affect outcomes and achievements in later life. There have been approximately 2,500 published pieces of research worldwide using the four studies according to one source and over 6,000 papers and forty books by another source. \n\nComparisons between the different generations in the four cohorts enable academics to chart social change and start to untangle the reasons behind it. Findings from the studies have contributed to debates and enquiries in a number of policy areas over the last half-century including: education and equality of opportunity; poverty and social exclusion; gender differences in pay and employment; social class differences in health; changing family structures; and anti-social behaviour.\n\nThe studies were key sources of evidence for a number of UK Government inquiries, such as the Plowden Committee on Primary Education (1967), the Warnock Committee on Children with Special Educational Needs (1978), the Finer Committee on One Parent Families (1966–74), the Acheson Independent Inquiry into Inequalities in Health (1998) and the Moser Committee on Adult Basic Skills (1997–99). A study of working mothers and early child development was influential in making the argument for increased maternity leave. Another study on the impact of assets, such as savings and investments on future life chances, played a major part in the development of assets-based welfare policy, including the much-debated Child Trust Fund.\n\n\n"}
{"id": "13932188", "url": "https://en.wikipedia.org/wiki?curid=13932188", "title": "Capital punishment in Greece", "text": "Capital punishment in Greece\n\nCapital punishment in modern Greece was carried out using the guillotine (until 1913) or by firing squad. It was last applied in 1972, and the death penalty was abolished in stages between 1975 and 2005.\n\nExecutions during the Greek War of Independence were carried out by firing squad, although when the monarchy introduced the Penal Code in 1834, beheading by guillotine became the only mode of execution. In 1847, difficulties in making the guillotine available for every execution made the government establish the firing squad as an alternative mode of execution. Both would be used until the firing squad was established as the only means of execution in 1929 (the last execution by guillotine took place in 1913). Over 3,000 executions took place between 1946 and 1949 during the Greek Civil War. The last execution took place on 25 August 1972, when the 27-year-old Vassilis Lymberis was shot by firing squad for the murder of his wife, mother-in-law and two children (he burned them alive inside their house) on the island of Crete.\n\nCapital punishment was abolished for peacetime crimes other than high treason during wartime by article 7 of the Constitution of 1975. Previously, three officers were sentenced to death during the Greek Junta Trials, but these sentences were commuted to life imprisonment by the Karamanlis government.\n\nIn 1997 Greece ratified the Second Optional Protocol to the International Covenant on Civil and Political Rights, aiming at the abolition of the death penalty; however, a reservation was made allowing for death penalty use for the most serious crimes, i.e. high treason, committed during wartime. Protocol No. 6 to the European Convention on Human Rights (ECHR), providing for the abolition of the death penalty in peacetime, was ratified in 1998.\n\nGreece abolished the death penalty for all crimes in 2004. In 2005, Greece ratified the Protocol No. 13 to the ECHR, concerning the abolition of the death penalty under all circumstances.\n\nThe Golden Dawn party called in 2013 for the restoration of the death penalty for immigrants convicted of violent crimes.\n\n"}
{"id": "271056", "url": "https://en.wikipedia.org/wiki?curid=271056", "title": "Carnitine", "text": "Carnitine\n\nCarnitine (β-hydroxy-γ-\"N\"-trimethylaminobutyric acid, 3-hydroxy-4-\"N\",\"N\",\"N\"-trimethylaminobutyrate) is a quaternary ammonium compound involved in metabolism in most mammals, plants and some bacteria. Carnitine may exist in two isomers, labeled -carnitine and -carnitine, as they are optically active. At room temperature, pure carnitine is a white powder, and a water-soluble zwitterion with low toxicity. Carnitine only exists in animals as the -enantiomer, and -carnitine is toxic because it inhibits the activity of -carnitine. Carnitine, derived from an amino acid, is found in nearly all organisms and animal tissue. Carnitine is the generic expression for a number of compounds that include L-carnitine, acetyl-L-carnitine, and propionyl-L-carnitine. It is most accumulated in cardiac and skeletal muscles as it accounts for 0.1% of its dry matter. It was first derived from meat extracts in 1905, therefore the name carnitine is derived from Latin \"carnus\" or flesh. The body synthesizes enough carnitine from lysine side chains to keep up with the needs of energy production in the body as carnitine acts as a transporter of long-chain fatty acids into the mitochondria to be oxidized and produce energy. Some individuals with genetic or medical disorders (like preterm infants) cannot make enough, so this makes carnitine a conditionally essential nutrient for them.\n\nMany eukaryotes have the ability to synthesize carnitine, including humans. Humans synthesize carnitine from the substrate TML (6-\"N\"-trimethyllysine), which is in turn derived from the methylation of the amino acid lysine. TML is then hydroxylated into hydroxytrimethyllysine (HTML) by trimethyllysine dioxygenase, requiring the presence of ascorbic acid and iron. HTML is then cleaved by HTML aldolase (a pyridoxal phosphate requiring enzyme), yielding 4-trimethylaminobutyraldehyde (TMABA) and glycine. TMABA is then dehydrogenated into gamma-butyrobetaine in an NAD-dependent reaction, catalyzed by TMABA dehydrogenase. Gamma-butyrobetaine is then hydroxylated by gamma butyrobetaine hydroxylase (a zinc binding enzyme) into -carnitine, requiring iron in the form of Fe.\n\nCarnitine is involved in transporting fatty acids across the mitochondrial membrane, by forming a long chain acetylcarnitine ester and being transported by carnitine palmitoyltransferase I and carnitine palmitoyltransferase II. Carnitine also plays a role in stabilizing Acetyl-CoA and coenzyme A levels through the ability to receive or give an acetyl group.\n\nRebouche and Engel had investigated the tissue distribution of carnitine-biosynthetic enzymes in humans. They found TMLD to be active in the liver, heart, muscle, brain and highest in kidney. HTMLA activity is found primarily in the liver. The rate of TMABA oxidation is greatest in the liver, with considerable activity also found in the kidney, however is low in brain, heart and muscle. These results indicate that all the investigated tissues have the ability to convert TML into butyrobetaine through containing the required enzymes for it but not all of them can convert butyrobetaine into carnitine, only the kidney, liver and brain are capable of that.\n\nThe free-floating fatty acids, released from adipose tissues to the blood, bind to carrier protein molecule known as serum albumin that carry the fatty acids to the cytoplasm of target cells such as the heart, skeletal muscle, and other tissue cells, where they are used for fuel. But before the target cells can use the fatty acids for ATP production and β oxidation, the fatty acid with chain lengths of 14 or more carbons must be activated and subsequently transported into mitochondrial matrix of the cells in three enzymatic reactions of the carnitine shuttle.\n\nThe first reaction of the carnitine shuttle is a two-step process catalyzed by a family of isozymes of acyl-CoA synthetase that are found in the outer mitochondrial membrane, where they promote the activation of fatty acids by forming a thioester bond between the fatty acid carboxyl group and the thiol group of coenzyme A to yield a fatty acyl–CoA.\n\nIn the first step of the reaction, acyl-CoA synthetase catalyzes the transfer of adenosine monophosphate group (AMP) from an ATP molecule onto the fatty acid generating a fatty acyl–adenylate intermediate and a pyrophosphate group (PP). The pyrophosphate, formed from the hydrolysis of the two high-energy bonds in ATP, is immediately hydrolyzed to two molecule of P by inorganic pyro phosphatase. This reaction is highly exergonic which drives the activation reaction forward and makes it more favorable. In the second step, the thiol group of a cytosolic coenzyme A attacks the acyl-adenylate, displacing AMP to form thioester fatty acyl-CoA.\n\nIn the second reaction, the activated fatty acids that are intended for mitochondrial oxidation are transported into the matrix by a carrier protein, but first the acyl-CoA must be transiently attached to the hydroxyl group of carnitine to form fatty acyl–carnitine. This transesterification is catalyzed by an enzyme found in the outer membrane of the mitochondria known as carnitine acyltransferase 1 (also called carnitine palmitoyltransferase 1, CPT1).\n\nThe fatty acyl–carnitine ester formed then diffuses across the intermembrane space of the mitochondria and enters the matrix by passive transport through the acyl-carnitine/carnitine cotransporter that is found in inner mitochondrial membrane. This cotransporter return one molecule of carnitine from the matrix to the intermembrane space as one molecule of fatty acyl– carnitine moves into the matrix.\n\nIn the third and final reaction of the carnitine shuttle, the fatty acyl group is transferred back from fatty acyl-carnitine in the matrix to intramitochondrial coenzyme A regenerating fatty acyl–CoA and a free carnitine molecule. This reaction is catalyzed by carnitine acyltransferase 2 (also called CPT2), which is placed on the inner face of the inner mitochondrial membrane. The carnitine molecule formed is then shuttled back into the intermembrane space by the same cotransporter while the fatty acyl-CoA is oxidized and used for ATP production.\n\nThe carnitine-mediated entry process is a rate-limiting factor for fatty acid oxidation and is an important point of regulation.\n\nThe liver starts actively making triglycerides from excess glucose when it is supplied with glucose that cannot be oxidized or stored as glycogen. This increases the concentration of malonyl-CoA, the first intermediate in fatty acid synthesis, leading to the inhibition of carnitine acyltransferase 1, thereby preventing fatty acid entry into the mitochondrial matrix for β oxidation. This inhibition prevents fatty acid breakdown while synthesis is happening.\n\nCarnitine activation occurs due to a need for fatty acid oxidation which is required for energy production. During vigorous muscle contraction or during fasting, ATP concentration decrease and AMP concentration increase which leads to the activation of AMP-activated protein kinase (AMPK). AMPK phosphorylates acetyl-CoA carboxylase which catalyzes malonyl-CoA synthesis. This phosphorylation inhibits the acetyl-CoA carboxylase which in turn lowers the concentration of malonyl-CoA and as a result it relieves the inhibition of fatty acyl–carnitine transport into mitochondria, thus allowing β oxidation to replenish the supply of ATP.\n\nPeroxisome proliferator-activated receptor alpha (PPAR\"α\") is a nuclear receptor that functions as a transcription factor. It acts in muscle, adipose tissue, and liver to turn on a set of genes essential for fatty acid oxidation, including the fatty acid transporters carnitine acyltransferases 1 and 2, the fatty acyl–CoA dehydrogenases for short, medium, long, and very long acyl chains, and related enzymes.\n\nPPAR\"α\" functions as a transcription factor in two cases; as mentioned before when there is an increased demand for energy from fat catabolism, such as during a fast between meals or long-term starvation. Besides that, the transition from fetal to neonatal metabolism in the heart. In the fetus, fuel sources in heart muscle are glucose and lactate, but in the neonatal heart, fatty acids are the main fuel which require the PPAR\"α\" to be activated so it is able in turn to activate the genes essential for fatty acid metabolism in this stage.\n\nMore than 20 human genetic defects in fatty acid transport or oxidation have been approved. In case of Fatty acid oxidation defects, acyl-carnitines accumulate in mitochondria and are transferred into the cytosol, and then into the blood. Plasma levels of acyl-carnitine in new born infants can be detected in a small blood sample by tandem mass spectrometry.\n\nWhen \"β\" oxidation is defective because of either mutation or deficiency in carnitine, the ω Oxidation of Fatty Acids becomes more important in mammals. Actually, the ω Oxidation of Fatty Acids is another pathway for F-A degradation in some species of vertebrates and mammals that occurs in the endoplasmic reticulum of liver and kidney, it is the oxidation of the ω (omega) carbon—the carbon most far from the carboxyl group (in contrast to formula_1 oxidation which occurs at the carboxyl end of fatty acid, in the mitochondria).\n\nThere are two types of carnitine deficiency, primary and secondary carnitine deficiency. Under these circumstances there is a specific and scientific value of carnitine intake. Primary carnitine deficiency is a genetic disorder of the cellular carnitine-transporter system that typically appears by the age of five with symptoms of cardiomyopathy, skeletal-muscle weakness, and hypoglycemia. Secondary carnitine deficiencies may happen as the result of certain disorders such as chronic renal failure, or under conditions that reduce carnitine absorption or increase its excretion, for example taking antibiotics, malnutrition, and poor absorption.\n\nSome research has been carried out on carnitine supplementation in athletes, given its role in fatty acid metabolism; however, individual responses varied significantly in the 300 people involved in one study. Carnitine has been studied in various cardiometabolic conditions, with a bit of evidence pointing towards efficacy as an adjunct in heart disease and diabetes. However, there are insufficient trials to determine its efficacy. Carnitine has no effect on preventing mortality associated with cardiovascular conditions. Carnitine has no effect on serum lipids, except a possible lowering of LDL. Carnitine has no effect on most parameters in end stage kidney disease, however it possibly has an effect on c-reactive protein. The effects on mortality and disease outcome are unknown.\n\nThe carnitine content of seminal fluid is directly related to sperm count and motility, suggesting that the compound might be of value in treating male infertility. One study concluded that carnitine supplementation may improve sperm quality, and the reported benefits may relate to increased mitochondrial fatty-acid oxidation (providing more energy for sperm) and reduced cell death in the testes of mice subjected to physical stress to the testes.\n\nSeveral studies have approved the effectiveness of supplemental carnitine in the management of cardiac ischemia (restriction of blood flow to the heart) and peripheral arterial disease. In fact, levels of carnitine are low in the failing heart muscle, supplemental amounts might counteract the toxic effects of free fatty acids and improve carbohydrate metabolism. Carnitine has had anti-ischemic properties when given orally and by injection.\n\nAn important interaction between diet and the intestinal microbiome brings into play additional metabolic factors that aggravate atherosclerosis beyond dietary cholesterol. This may help to explain some benefits of the Mediterranean diet. Hazen’s group from the Cleveland Clinic reported that carnitine from animal flesh (four times as much in red meat as in fish or chicken), as well as phosphatidylcholine from egg yolk, are converted by intestinal bacteria to trimethylamine (the compound that causes uremic breath to smell fishy). Trimethylamine is oxidized in the liver to trimethylamine \"N\"-oxide (TMAO), which causes atherosclerosis in animal models. Patients in the top quartile of TMAO had a 2.5-fold increase in the 3-year risk of stroke, death, or myocardial infarction.\n\nA key issue is that vegans who consumed -carnitine did not produce TMAO because they did not have the intestinal bacteria that produce TMA from carnitine.\n\nChemotherapy and radiation treatment adverse effects such as fatigue, mood and sleep disturbances, are common in cancer patients. A deficiency in carnitine may also occur, some studies were done on cancer patients with deficiency of carnitine. Carnitine supplementation have showed normal blood levels of carnitine, enhancement in mood and sleep, and relieving of the fatigue caused by chemotherapy. However, a 2018 meta-analysis found little evidence to support the use of carnitine for cancer-related fatigue. Carnitine may be depleted in some cancer chemotherapy treatments. There is weak evidence from clinical trials that carnitine supplementation may be beneficial in treating cancer anorexia-cachexia syndrome.\n\nType 2 diabetes which is marked by insulin resistance may be associated with a defect in fatty acid oxidation in muscle. Several studies suggest that carnitine supplementation may have a beneficial effect on glucose utilization and reduce diabetic neuropathy. However carnitine may also increase overall cardio-metabolic risk.\n\nGenerally HIV infected patients accumulate fat in some areas of the body and lose fat in other areas, besides having high blood levels of fats (hyperlipidemia) and insulin resistance which is known as the lipdystrophy syndrome. This syndrome causes a deficiency in L- carnitine which causes defects in fat metabolism in mitochondria. Supplementation with carnitine in HIV-infected individuals may slow the death of lymphocytes, reduce neuropathy and favorably affect blood lipid levels.\n\nThe kidneys contribute to overall homeostasis in the body, including carnitine levels. In the case of renal impairment, urinary elimination of carnitine increasing, endogenous synthesis decreasing, and poor nutrition as a result of disease-induced anorexia can result in carnitine deficiency. Carnitine blood levels and muscle stores can become very low, which may contribute to anemia, muscle weakness, fatigue, altered levels of blood fats, and heart disorders. Some studies have shown that supplementation of high doses of L-carnitine (often injected) may aid in anemia management.\n\nCartinine is a chiral molecule, meaning that it exists as two isomers (L-carnitine and D-carnitine), each of which is a mirror image of the other. The form present in the body is L-carnitine, which is also the form present in food. Food sources rich in L-carnitine are animal products such as meat, poultry, fish, and milk. Redder meats tend to have higher levels of L-carnitine. Adults eating diverse diets that contain animal products attain about 60–180 milligrams of carnitine per day. Vegans get noticeably less (about 10–12 milligrams) since their diets lack these carnitine-rich animal-derived foods. Approximately 54% to 86% of dietary carnitine is absorbed in the small intestine and then enters the bloodstream. Even carnitine-poor diets have little effect on the body’s total carnitine content as the kidneys conserve carnitine very efficiently. The carnitine content of several foods is listed in Table 1.\n\nIn general omnivorous humans consume 2–12 µmol of carnitine per day per kg of body weight that forms 75% of body carnitine. Humans produce 1.2 µmol per day per kg of body weight of carnitine endogenously which is 25% of body carnitine. Strict vegetarians obtain very little of carnitine from diet (0.1 µmol per day per kg of body weight) as carnitine is mainly found in foods coming from animals. This means that 90% of their body carnitine is obtained through biosynthesis. However this difference of plasma levels of carnitine between omnivorous humans and strict vegetarians is possibly not of any clinical significance.\n\nIn 1989, the Food and Nutrition Board (FNB) concluded that carnitine wasn't an essential nutrient as healthy human liver and kidneys synthesize sufficient quantities of carnitine from lysine and methionine to meet up with daily body requirements without the need of consuming it from supplements or food. Also, the FNB has not established Dietary Reference Intakes (DRIs) for carnitine.\n\nL-carnitine, acetyl-L-carnitine, and propionyl-L-carnitine are available in nutraceutical dietary supplement pills or powders. It is also a drug approved by the Food and Drug Administration to treat primary and certain secondary carnitine-deficiency syndromes.\n\n1. Carnitine interacts with pivalate-conjugated antibiotics such as pivampicillin. Chronic administration of these antibiotics increases the excretion of pivaloyl-carnitine, which can lead to carnitine depletion.\n\n2. Treatment with the anticonvulsants valproic acid, phenobarbital, phenytoin, or carbamazepine significantly reduces blood levels of carnitine. In addition, the use of valproic acid may cause hepatotoxicity. (L-carnitine administration may help treat valproic acid toxicity in children and adults)\n\nLevocarnitine was approved by the U.S. Food and Drug Administration as a new molecular entity under the brand name Carnitor on December 27, 1985.\n\n\n"}
{"id": "3340717", "url": "https://en.wikipedia.org/wiki?curid=3340717", "title": "Certified nurse midwife", "text": "Certified nurse midwife\n\nIn the United States, a certified nurse-midwife (CNM) is a midwife who exceeds the International Confederation of Midwives essential competencies for a midwife and is also an advanced practice registered nurse having completed registered nursing and midwifery education. CNMs provide care of women across their lifespan, including pregnancy and the postpartum period, and well woman care and birth control. Certified nurse midwives are exceptionally recognized by the International Confederation of Midwives as a type of midwives in the United States.\n\nThe American College of Nurse-Midwives accredits midwifery education programs and serves as the national specialty society for the nation's CNMs and Certified Midwives (CMs). CNMs in most states are required to \n\nCNMs function as primary healthcare providers for women and most often provide medical care for relatively healthy women, whose health and births are considered uncomplicated and not \"high risk,\" as well as their neonates. Often, women with high risk pregnancies can receive the benefits of midwifery care from a CNM in collaboration with a physician. CNMs may work closely or in collaboration with an obstetrician & gynecologist, who provides consultation and/or assistance to patients who develop complications or have complex medical histories or disease(s). CNMs provide health care for sexual health, as they also see women for routine exams and are able to initiate all types of contraception.\n\nCNMs practice in hospitals and private practice medical clinics and may also deliver babies in birthing centers and attend at-home births. Some work with academic institutions as professors. They are able to prescribe medications, treatments, medical devices, therapeutic and diagnostic measures. CNMs are able to provide medical care to women from puberty through menopause, including care for their newborn (neonatology), antepartum, intrapartum, postpartum and nonsurgical gynecological care. In some cases, CNMs may also provide care to the male partner, in areas of sexually transmitted diseases and reproductive health, of their female patients. In the United States, fewer than 1% of nurse midwives are men..\n\n"}
{"id": "46463468", "url": "https://en.wikipedia.org/wiki?curid=46463468", "title": "Childbirth in Ghana", "text": "Childbirth in Ghana\n\nChildbirth in Ghana is often seen as a joyous occasion in Ghanaian society, as children represent wealth, status, and the continuation of a lineage. Pregnant women are often given special privileges and are considered to be beautiful, fragile, and vulnerable to evil spirits. Therefore, women may seek guidance from a religious or spiritual diviner to protect their fetus or to increase their chances of conceiving. For example, the Akan may carry \"akuaba\" dolls, a fertility symbol, during pregnancy to ensure that they will birth a healthy and beautiful baby that resembles the doll's exaggerated features.\n\nDue to the cultural implications and the importance of bearing children, infertility in Ghana can be devastating. Throughout the years, Ghanaians have believed that both physical and spiritual ailments are the cause of infertility. Some people believe that a womb could be too hot or too cold to support a developing baby. Others believe that the cause of their infertility is witchcraft, and this belief is often supported by traditional priests who assert that the woman can be rid of a witch's curse and be allowed to conceive if the priest performs a rite in which he asks fertility gods how they can be appeased. \n\nOther spiritual beliefs involving infertility include the belief that infertility is the result of a woman's disobedience to God, in which the remedy is prayer and repentance. Women who consult a tradition birth attendant are likely to be told that prayer is the medicine for infertility, although they acknowledge that infertility is sometimes caused by a physical ailment. In these cases, the traditional birth attendant may blame the infertility on stomach aches and advise the woman to visit a hospital and take the medicine given by the doctor.\n\nCultural influences and sociodemographic characteristics play an important role in a woman's decision to seek maternal-child health services. These influences and characteristics include level of education, religious affiliation, region of residence, ethnicity, and occupation. In most communities, maternal-child health services coexist with traditional indigenous health care, and pregnant women in these rural areas may choose between modern medicine, herbalists, diviners, and spiritualists for care. The use of a doctor for prenatal care is low among women living in rural areas of Greater Accra and the Northern and Upper regions of Ghana. 23.1% of Protestant and Catholic women, and 10.3% of traditional women consult a doctor for prenatal care.\n\nThe majority of women believe that antenatal care from a health professional is necessary to determine that their pregnancy is normal, to see that the fetus is well-positioned, to learn about when they were expected to deliver, to obtain a tetanus immunization, for the diagnosis and treatment of illnesses during pregnancy, and nutritional advice. The barriers to seeking antenatal care include travel time and distance from health care facilities, the high cost, and the inconvenient hours of operations of the clinics. Some women also did not want people to know they were pregnant until their third month of pregnancy, and would wait until then to seek antenatal care. Inexpensive care from traditional birth attendants included routine antenatal care, however untrained attendants and traditional healers did not provide antenatal care.\n\nKnowledge about nutrition in Ghana is obtained through formal education, community health services, friends and families, cultural practices, traditions, and beliefs of the community. The majority of foods consumed by Ghanaian women during pregnancy were foods indigenous to Ghanaian diets in general such as green leafy vegetables, meats, fish, fruits, and legumes. They also consume foreign foods such as dairy products, biscuits, and beverages such as tea and powdered mixes. \n\nRecommended raw foods during pregnancy include: bananas, oranges, papaya, pineapples, carrots, cabbage, lettuce, honey, and kwawu nsua/nsaman troba. Recommended cooked foods include bean stew with palm oil and boiled white rice, bread, fried anchovies, smoked mackerel, snails in soup, stews with eggs and vegetables, palm nut soup with fufu and rice, and Nkontomire stew which is made with palm oil and boiled sweet cassava, or plantain, or yam. The beliefs about beneficial foods during pregnancy are based on their understanding of how certain foods can prevent anemia, enhance physical strength, support the development of the fetus, and minimize physiological disruption.\n\nWomen in Ghana also believe there are foods one must avoid during pregnancy. Most women agree that excess fat and sugar are harmful during pregnancy, and Fante and Akwapim women have beliefs about specific foods to avoid which include sugarcane, coconut, oranges, pineapple, chilli pepper, ground nut soup, eggs, high fat and high sugar foods, cooking oils, salt, and clay.\n\nIn 1985, the abortion law passed in Ghana allowed for legal and safe abortion by a qualified medical practitioner for pregnancies that resulted from rape or incest, if the pregnancy threatens the life of the woman or her physical or mental health, or if there is substantial risk that the fetus would suffer from a serious physical anomaly or devastating disease. Many women in Ghana are not aware that abortion is legal in their country and tend to seek unsafe abortion providers and receive unsafe care afterwards. As a result, more than 11% of maternal deaths are due to unsafe abortions, making it the second most common death in women in Ghana. \n\nAccording to the 2007 Ghana Maternal Health Survey (GMHS), 7% of all pregnancies end in abortion. The incidences of abortion are higher in women who are 20–24 years old, educated and wealthy women, and women who live in urban areas. Contraceptive use is low in Ghana with about 24% of women using contraceptives in 2008. 35% Of married women in Ghana are in need of contraceptives, but are not using any\n\nPeople believed that women who were not faithful to their husbands would experience prolonged labor, and male traditional healers would remark that a promiscuous woman would have to tell every person in the room the number of men she has slept with apart from her husband before she would be able to give birth. Other traditional beliefs and practices related to laboring women included special herbs being used when the umbilical cord was wrapped around the neck of the fetus and for breech presentations, hot water poured on the abdomen and okra smeared on the vagina to expedite delivery, women who could not birth the placenta were given a bottle to blow into in order to force the placenta out, and a calabash of hot water was placed on the abdomen to stop postpartum hemorrhage and bleeding. Although many people have extensive knowledge of these traditional beliefs and practices, many indicate that most people no longer use them.\n\nMost deliveries in Ghana are attended by untrained personnel, including traditional birth attendants, and most traditional birth attendants in rural areas are illiterate elderly farmers.\nMany women choose traditional birth attendants because of the lower cost, and because they live in the community and are able to assist quickly. They are known to assist with bathing the newborn, and giving advice on breastfeeding and newborn care. Traditional birth attendants deliver in cases on uncomplicated labor and are able to refer women to health care facilities when complications arise.\n\nThe majority of Ghanaian women deliver at home with a traditional birth attendant and are referred to the hospital in cases of complicated labor. Although most deliveries are attended by a traditional birth attendant, most women prefer to deliver with a health professional. Barriers to obtaining professional health care include high costs, inadequate transportation, long distances to health care facilities, and poor road conditions. Some women were also too embarrassed to go to a hospital because they did not have nice clothes and preferred to deliver at home where no one could see that they were poor.\n\nIn general, men have most of the decision-making power in Ghanaian society. In labor and delivery, husbands and heads of the households also make most of the decisions in collaboration with the birth attendant, and sometimes a soothsayer. During labor, the husband will usually make the decision about where to deliver because a woman in labor is considered unable to think clearly due to the pain, although the head of the household will have the final say. In some cases, women, especially those who can afford to go to a hospital, will make their own decision about where to give birth without waiting for the approval of their husband or household heads.\n\nTraditionally, the placenta has been buried outside of the house to show that the ancestors of the family has accepted the baby.\n\nThe older women in the compound or community look after women in their postpartum period and assist with cleaning the baby and massage. Mothers receive postpartum teaching from the older women in the community. In the early postpartum period, there is great emphasis placed on the health and well-being of the mother and infant. Breastfeeding and postpartum abstinence is mandatory. Both mother and baby receive massages using shea-butter, palm kernel oil, and other oils. During this time, different colored and sized beads are tied around the wrist, waist, and ankles to monitor the growth of the infant. Older women will also keep a brass bowl full of herbs, mixtures, white clay, soft sponge, and charcoal. These items are believed to prevent health problems for the infant and mother.\n\nWhen babies are born, they are kept indoors for seven days because Ghanaians believe that this is the period in which they are most vulnerable to both physical and spiritual harm. During this period, it is believed that the child is wandering between the spiritual and physical world and may decide to go back to the spiritual world at any point. They are known as strangers until the end of the seven days when they are acknowledged and welcomed into society with an elaborate naming ceremony, or outdooring ceremony.\n\nOutdooring ceremonies can vary in practice among the different Ghanaian ethnic groups. The Ga place drops of water and alcohol onto the child's tongue to symbolize teaching them the difference between good and evil, and they may also place a piece of food such as kenkey so the child will appreciate a staple food of the Ga. Some groups practice raising the baby toward the sky three times to introduce them to heaven and Earth. \n\nLibations are also poured and live sacrifices of chickens or goats are made to ensure good health and protect the child from evil. The father or an elder will call out the child's name for the guests to repeat aloud to formally welcome the child into the community. After the naming concludes, friends and relatives give gifts to the family, and feasting and dancing follows.\n\nTraditional names are often based on the day of the week the baby was born, the gender of the baby, and their relationship to the other children in the family. Christian families will often give their children a traditional name and a Christian name.\n\nIn the past, boys in Ghanaian society often do not undergo circumcision as an infant. Boys will receive circumcision as they undergo initiation into adulthood at the age of ten to fourteen years of age. Ghanaians believe that the circumcision represents cleanliness and the pain will make the boys physically and mentally stronger.\nWith the advent of modern medicine however, male children are circumcised from as early as four (4) days after they are born, and this is normally done at the hospitals or locally by Wanzams, special men trained in circumcision.\n\n"}
{"id": "54004324", "url": "https://en.wikipedia.org/wiki?curid=54004324", "title": "Collection of unused drugs", "text": "Collection of unused drugs\n\nCollection of unused drugs, also called drug return or drug take-back, is any program for individual consumers to dispose of drugs by returning their unused drugs to a collection center.\n\nOne survey of consumers found that individuals like the idea of pharmacies accepting drug returns.\n\nDrug return programs can reduce the environmental impact of pharmaceuticals and personal care products.\n\nVarious research projects have investigated drug return programs at pharmacies in particular regions. Studied places include the United States, Britain, France, Switzerland, Sweden, Serbia, and Germany.\n"}
{"id": "8709666", "url": "https://en.wikipedia.org/wiki?curid=8709666", "title": "Combat Methamphetamine Epidemic Act of 2005", "text": "Combat Methamphetamine Epidemic Act of 2005\n\nThe Combat Methamphetamine Epidemic Act of 2005 (CMEA) is federal legislation enacted in the United States on March 9, 2006, to regulate, among other things, retail over-the-counter sales of following products because of their use in the manufacture of illegal drugs:\nRetail provisions of the CMEA include daily sales limits and 30-day purchase limits, placement of product out of direct customer access, sales logbooks, customer ID verification, employee training, and self-certification of regulated sellers. The CMEA is found as Title VII of the USA PATRIOT Improvement and Reauthorization Act of 2005 (H.R. 3199). The last provisions of the law took effect on 30 September 2006.\n\nEphedrine, pseudoephedrine, and phenylpropanolamine are precursor chemicals used in the illicit manufacture of methamphetamine or amphetamine. They are also common ingredients used to make cough, cold, and allergy products. It was argued that the CMEA would curtail the clandestine production of methamphetamine. The U.S. Department of Justice claims that states that have enacted similar or more restrictive retail regulations have seen a dramatic drop in small clandestine labs.\n\n\"The CMEA requires record-keeping and identification of all sales and reports to law enforcement of any 'suspicious' transactions. Purchasers are limited to '3.6 grams of pseudoephedrine base' per day and 9 grams per month. (Buying more than that is a federal misdemeanor.)\"\n\nThe statute also includes the following requirements for merchants who sell these products:\n\nIn September 2006, Tim Naveau was arrested and charged with a Class-B misdemeanor for purchasing Claritin-D. Naveau takes one tablet of Claritin D each day to combat allergies, and he \"had stocked up on the allergy medication because his teenage son, who was also an allergy sufferer, needed several packages because he was headed off to a church camp.\" Minors are not permitted to purchase pseudoephedrine under the law. Naveau had gone over the legal limit for pseudoephedrine when he purchased extra Claritin-D to give to his son before he attended church camp.\n\n\n"}
{"id": "826482", "url": "https://en.wikipedia.org/wiki?curid=826482", "title": "Dead Ringers (film)", "text": "Dead Ringers (film)\n\nDead Ringers is a 1988 Canadian-American psychological horror film starring Jeremy Irons in a dual role as identical twin gynecologists. David Cronenberg directed and co-wrote the screenplay with Norman Snider. Their script was based on the lives of Stewart and Cyril Marcus and on the novel \"Twins\" by Bari Wood and Jack Geasland, a \"highly fictionalized\" version of the Marcus' story.\n\nThe film won numerous honors, including for Irons' performance, and 10 Genie Awards, notably Best Motion Picture. Toronto International Film Festival critics have ranked it among the Top 10 Canadian Films of All Time.\n\nElliot and Beverly Mantle are identical twins and gynecologists who jointly operate a highly successful clinical practice in Toronto that specializes in the treatment of female fertility problems. Elliot, the more confident and cynical of the two, seduces women who come to the Mantle Clinic. When he tires of them, the women are passed on to the shy and passive Beverly, while the women remain unaware of the substitution.\n\nAn actress, Claire Niveau, comes to the clinic for her infertility. It turns out that Claire has a \"trifurcated cervix\", which means she probably will not be able to have children. Elliot seduces Claire and then urges Beverly to sleep with her. However, Beverly becomes emotionally attached to Claire, and this upsets the equilibrium between the twins. Beverly also begins sharing Claire's abuse of prescription drugs, which he abets through his doctor's authority. When Claire learns that Elliot has been taking sexual advantage of her by impersonating Beverly, she is angry and confronts them both in a bar, but later decides to continue a relationship with Beverly exclusively.\n\nEventually, Claire leaves town to work on another film. This sends Beverly into clinical depression, more prescription drug abuse, and paranoid delusions about \"mutant women\" with abnormal genitalia. Beverly seeks out metallurgical artist Anders Wolleck and commissions a set of bizarre \"gynecological instruments\" for operating on these mutant women. After Beverly assaults a patient during surgery with one of Wolleck's tools, both brothers are immediately suspended from practice and put on administrative leave by the hospital board.\n\nWith their medical career now ruined, Elliot locks Beverly into the clinic and tries to clean him up, taking pills himself in order to \"synchronize\" their bloodstreams. When Claire returns, Beverly leaves the clinic to be with her. After recovering his sobriety, he is concerned about his brother, and goes back to the clinic. There he finds the clinic in shambles and Elliot despondent and intoxicated. Their positions are reversed as Beverly cares for Elliot. Drugged and despairing, they celebrate their mock birthday and Elliot volunteers to be killed, \"to separate the Siamese twins\". Beverly disembowels Elliot on an examination couch with the same claw-like instrument of Wolleck's that he had used to assault his patient in the operating room.\n\nShortly after, Beverly pulls himself together, leaves the clinic and calls Claire on a pay phone. When she asks, \"Who is this?\", Beverly leaves the phone, walks back into the clinic and lies in Elliot's dead arms.\n\nAlthough \"Dead Ringers\" closely follows the case of Stewart and Cyril Marcus, director Peter Greenaway notes that Cronenberg queried him about his film \"A Zed & Two Noughts\" for two hours before going on to make \"Dead Ringers\" eight months later.\n\nIn his DVD commentary, Irons claims that Robert De Niro declined playing the Mantles due to his unease with the subject matter and portraying gynecologists, while William Hurt decided to reject the parts because \"it is hard enough to play one role\". This movie marked the screen debut of actress Jill Hennessy: both she and her real life twin sister Jacqueline play prostitutes in one scene of the film. Jill later followed this film up with her signature role as Claire Kincaid on the TV series \"Law & Order\".\n\nIrons was given two different dressing rooms with two sets of costumes for playing his two characters. However, given the fact that he said \"the whole point of the story is you should sometimes be confused as to which is which,\" he chose to use only one of the rooms and combine different costume items intended for different characters. Irons also developed an \"internal way\" to portray each character, employing the Alexander technique for \"different energy points,\" giving each character his own appearance.\n\nA second dream scene was also shot which featured a parasitic twin emerging from Beverly’s stomach but this sequence was not used in the final cut.\n\nRoger Ebert gave the film two and a half stars, writing \"it's like a collaboration between med school and a supermarket tabloid\", and said it was challenging but interesting for his female friends to view. Ebert also credited Irons for making each twin unique. \"Variety\" said Irons portrayed his characters with skill. In \"The Washington Post\", Desson Howe assessed it as \"unnerving but also enthralling\". For the same paper, Rita Kempley called it \"every woman's nightmare turned into a creepy thriller\", adding it was \"like slowing down to look at a traffic accident, afraid you might see something. It's really sordid stuff that becomes ridiculous, painful, unbelievable and tedious\". The film has a positive rating of 82% on Rotten Tomatoes, based on 39 reviews.\n\nIt is the favorite Cronenberg film of Korean director Chan-wook Park and was voted for in the 2002 \"Sight & Sound\" poll by Lalitha Gopalan. In 1999, \"Rolling Stone\" listed \"Dead Ringers\" as 95th on their list of \"100 Maverick Movies\". \"Total Film\" placed \"Dead Ringers\" 35th on their list of the \"50 Greatest Horror Movies of All Time\" while \"Entertainment Weekly\" placed it 20th on their list of \"The 25 scariest movies of all time\". It was named one of \"The Top 10 'True-Story' Horror Movies of All-time!\" by Bloody Disgusting.\n\nIn 1993, the Toronto International Film Festival Group compiled a Top 10 Canadian Films of All Time list, with festival director Piers Handling writing a lack of Cronenberg films was significant, and that \"Dead Ringers\" and \"Videodrome\" divided voters, causing neither to win a place on the list. \"Dead Ringers\" afterwards ranked sixth in the 2004 update, and seventh in 2015.\n\nIrons won critics groups' Best Actor awards for \"Dead Ringers\", and when he won the Academy Award for Best Actor in 1991 for \"Reversal of Fortune\", he thanked Cronenberg in his acceptance speech.\n\n\n"}
{"id": "40592204", "url": "https://en.wikipedia.org/wiki?curid=40592204", "title": "Disability in Australia", "text": "Disability in Australia\n\nFour million people in Australia (18.5%) reported having a disability in 2009, according to the results of the Survey of Disability, Ageing and Carers. Males and females were similarly affected by disability (18% and 19% respectively).\n\nJust under one in five Australians (18.5%) reported disability in 2009. A further 21% had a long-term health condition that did not restrict their everyday activities. The remaining 60% of the Australian population had neither a disability nor a long term health condition. Of those with a reported disability, 87% had a specific limitation or restriction; that is, an impairment restricting their ability to perform communication, mobility or self-care activities, or a restriction associated with schooling or employment.\n\nThe disability rate increases steadily with age, with younger people less likely to report a disability than older people. Of those aged four years and under, 3.4% were affected by disability, compared with 40% of those aged between 65 and 69 and 88% of those aged 90 years and over.\n\nRates of disability and rates of profound or severe core-activity limitation for 5- to 14-year-old males (11% and 6.6% respectively) were close to double those for females in the same age group (6.1% and 3.0% respectively). In contrast, women aged 90 years and over had a higher rate of profound or severe core-activity limitations (75%) than men of the same age (58%).\n\nIn 2015, there were 2.1 million Australians of working age with disability. Of these, 1.0 million were employed and another 114,900 were looking for work rounding to 53.4% of working age people with disability were in the labour force which compares to 83.2% of people with no disability. In 2015, 25.0% of people with a profound or severe limitation were in the labour force, compared with 58.9% of those with a mild limitation. In 2012, the labour force participation rate was higher for people with profound or severe limitations at 29.7%. \nIn 2015, almost one in five Australians reported living with disability (18.3% or 4.3 million people). A further 22.1% of Australians had a long-term health condition but no disability, while the remaining 59.5% had neither disability nor a long-term health condition.\n\nIn 2015, 18.6% of females and 18.0% of males had disability. Differences between males and females was most pronounced amongst people in older age groups with a 68.3% of females aged 90 years and over had a profound or severe limitation compared with 51.2% of males. At some ages there were higher proportions of males with disability such as for age groups 5 to 14 years (males 12.0% and females 7.0%) and 65 to 69 years (males 39.7% and females 36.0%).\n\nThe SDAC data on older people (those aged 65 years and over) from Australia's ageing population shows that there were around 3.5 million older Australians in 2015, representing one in every seven people or 15.1% of the population in which this proportion has increased from 14.3% in 2012. Older Australians living in households were more active, with the proportion that participated in physical activities for exercise or recreation increasing from 44.5% in 2012 to 49.2% in 2015. The majority of older Australians were living in households (94.8%), while 5.2% or one in twenty lived in cared accommodation such as nursing homes.\nWhile the proportion of older Australians has increased, the prevalence of disability amongst them has decreased. In 2015, 50.7% of older people were living with disability, down from 52.7% in 2012. Two-thirds of older Australians (67.3%) that reported their income lived in a household with an equivalised gross household income that was in the lowest two quintiles. This proportion has decreased from 74.6% in 2012.\n\nThe prevalence of disability in Australia fell from 20% in 2003 to 18.5% in 2009. After removing the effects of different age structures the age standardised rate also fell by 2.1 percentage points. The decrease is particularly noticeable in the younger age groups. From 2003 to 2009, the disability rate for 15- to 24-year-olds fell from 9.0% to 6.6%. Over the same period the rate of disability also decreased for those aged between 25 and 34 from 11% to 8.6%. Similarly, 22% of 45- to 54-year-olds reported a disability in 2003, compared with 18% in 2009.\n\nThe rate of profound or severe limitation in the core activities of communication, mobility and self-care declined, from 6.3% in 2003 to 5.8% in 2009. Much of the decrease in the prevalence of disability between 2003 and 2009 is due to a decline in the proportion of Australians disabled by physical health conditions, such as asthma and heart disease.\n\nThe incidence of disability caused by physical conditions, as opposed to mental or behavioural disorders, dropped from 17% in 2003, to 15% in 2009. For instance, in 2003, 6.8% of Australians had a disability primarily caused by musculoskeletal disorders such as arthritis and back problems, with this proportion declining to 6.5% in 2009. Likewise, the incidence of disability caused by diseases of the circulatory system dropped from 1.8% to 1.4%. In 2003, 8.8% of people aged in the 65 years and older group reported a disability due to diseases of the circulatory system, compared with 7.4% in 2009.\n\nThe incidence of disability caused by asthma also declined, from 0.8% in 2003 to 0.5% in 2009. Amongst younger people (0 to 17 years), the incidence of disability caused by asthma almost halved between 2003 and 2009, from 0.9% in 2003 to 0.5% in 2009. Of those aged between 18 and 44 years, the incidence of asthma-related disability also decreased, from 0.5% in 2003 to 0.3%. In addition, for this age group, the proportion of people with a disability due to back problems reduced, from 2.6% in 2003 to 1.9% in 2009.\n\nThe incidence of disability due to back problems also declined amongst those aged between 45 and 64 years. In this age group, 5.2% of people reported a disability as a result of back problems in 2009, compared with 6.0% in 2003. By contrast, the prevalence of disability resultant from back problems amongst those aged 65 and over has increased since 2003, from 4.9% to 6.3%.\n\nIn 1992, a High Court case was held, asking who should decide if a disabled girl was to have a sterilisation procedure. Since that time, all decisions about this kind of procedure have been heard in the Family Court or similar bodies.\nThe Disability Discrimination Act 1992 (DDA) was an act passed by the Parliament of Australia in 1992 to promote the rights of people with disabilities in certain areas such as housing, education and provision of goods and services. It shares a common philosophy with other disability discrimination acts around the world that have emerged in the late 20th and early 21st century, as well as earlier civil rights legislation designed to prevent racial discrimination and sex discrimination.\n\nAt the time of the enactment of the DDA, a variety of anti-discrimination acts for people with disabilities already existed in the different state legislatures, some dating back to the early 1980s. All States and Territories except Tasmania and the Northern Territory had anti-discrimination laws in place, and these two places had legislation under consideration. There were three reasons given for enacting a federal law:\n\nComplaints made under the DDA are made to the Australian Human Rights Commission (previously known as the Human Rights and Equal Opportunity Commission, HREOC), which also handles complaints relating to the Racial Discrimination Act 1975, Sex Discrimination Act 1984, Age Discrimination Act 2004 and the Human Rights and Equal Opportunity Commission Act 1986.\n\nA Productivity Commission enquiry was initiated by the Australian government to evaluate the effectiveness of the act, and published its findings in 2004. The Commission found that while there is still room for improvement, particularly in reducing discrimination in employment, overall the DDA has been reasonably effective. Specifically, the Commission found that people with a disability were less likely to finish school, to have a TAFE or university qualification and to be employed. They are more likely to have a below average income, be on a pension, live in public housing and in prison. The average personal income for people with a disability is 44 per cent of the income of other Australians.\n\nDisabilityCare Australia, formerly known as the National Disability Insurance Scheme (NDIS), is a healthcare program initiated by the Australian government. The bill was introduced into parliament in November 2012. In July 2013 the first stage of DisabilityCare Australia commenced in South Australia, Tasmania, the Hunter Region in New South Wales and the Barwon area of Victoria, while the Australian Capital Territory will commence in July 2014.\n\nPeople with disabilities are over-represented in the Australian prison system, as half of all people in the prison system have a disability.\n\nIn 2009, there were 2.6 million carers who provided assistance to those who needed help because of disability or old age. Just under one third of these (29%) were primary carers; that is, people who provided the majority of the informal help needed by a person with a disability or aged 60 years and over. Over two-thirds of primary carers (68%) were women. Thirteen percent of women were involved in a caring role, compared with 11% of men. The gender difference among carers was most pronounced for those aged 45 to 54 years, 16% of men and 23% of women in this age group provided care for a person with a disability or aged 60 years and over.\n\nThe proportion of Australians involved in caring for a person with a disability or an older person declined from 13% in 2003 to 12% in 2009, in line with the decrease in disability prevalence.\n\nAustralia is one of six nations that have implemented a carer system, in which their program follows under a more liberal democracy style that has family carers provide the majority of care to disabled and frail older people. However, most receive no formal services: 56% of primary carers supporting a disabled person under 65 and 65% of primary carers of older people had no such assistance in 2009. An official 2011 report concluded that carer support is ‘administered in an ad hoc way across a number of programs and jurisdictions’ and a report on the welfare of Australians found that 38% of primary carers felt that they needed more support in maintaining their own health, as well as physical, emotional and financial support. \n\nIn 1985, Australia introduced Carer Pension, Carer Payment (CP), to provide income support for carers unable to support themselves through substantial paid employment. It is means-tested on the income of both the care provider and the care receiver, who must also meet an assessment of disability. However, it is not subject to activity testing and not included in the ‘activation’ policies applied to most other forms of income support for working-age people. In 2006, CP recipients participated in paid work, unpaid work, education or training for up to 25 hours per week, however, only 23% had earnings while receiving it. Reasons for this outcome included the strain of caring responsibilities, inadequate skills and training, and the carer’s own health problem or disability: about 40% of recipients had not been in employment when they started caring and/or receiving CP. Many carers rely on government income support as their main source of income, however, reflecting their lower rates of labour force participation and concentration in part-time work. In 2003, income support was the main source of personal cash income for 40% of Australian carers, compared with 24% of other people.\n\nIn a 2009 Survey of Disability, Ageing, and Carers (SDAC), Australia identified 529,000 working-age partner-carers – 27% of all carers of working age. Also identified 188,000 ‘primary’ partner-carers, who constituted the largest group of working-age primary carers (34%). Just over half (55%) of this group were women. Partner-carers in Australia tend to be older: 70% of working-age partner-carers were over 45, and over 40% were aged 55–64, although there were few gender differences in their age profiles (ABS, 2011).\n\nAmong working-age primary carers, nearly half the partner-carers spent less than 20 hours per week providing care, but around 20% cared for 20–40 hours per week and a third intensively (40+ hours per week). Across all age groups, women were more likely than men to provide intensive levels of care.\n\nFemale partner-carers had provided care over a longer period of time than males: among primary partner-carers, 58% of men and 63% of women had been caring for five years or more. Partner-carers in Australia, are most likely to provide high levels of support because they are likely to live with the person for whom they care. The SDAC data show that nearly half of all carers, and over 90% of primary carers, assisted a spouse with a profound or severe limitation. 73% of primary partner-carers supported a person with a head injury, stroke or other brain damage and 15% cared for a person with mental illness. Among those of working age, partner-carers were less likely to be employed full-time, or employed at all, than both other carers and non-carers, and those aged 55–64 were less likely to be employed than their younger counterparts. \n\nIn 2009, an estimated 288,300 Australian children aged 0–14 had some kind of disability: over 3% of 0–4 year olds, and almost 9% of 5–14 year olds. Of these, 166,700 had a severe or profound ‘core activity limitation’, where they need assistance with regular communication, mobility or self-care tasks. Estimates suggest that about half of all disabled children aged 0–14 have two or more disabilities and almost 7% have four or five. A review of payments to primary parent-carers in Australia found intellectual and learning disabilities (4.3% of all children) and physical/diverse disabilities (4.2%) to be most prevalent, while analysis of the primary disability of service users found that this was ‘intellectual’ for about 30% of people, ‘physical’ for almost 17% and ‘autism’ for about 6%.\n\nThe demographics of carers for the youth differ from the old as Australia's introduction of deinstitutionalisation in the 1980s support parents to raise their children at home, including care for most disabled children. Frustrated parent-carers of disabled children have expressed their struggles in Australia that carers of other people do not have, but often live in disadvantaged circumstances such as their incomes are often lower than those of other families: in 2003, 50% of primary carers of disabled children were in the bottom two income quin-tiles, compared with 34% of non-carers . Primary carers of children with severe disabilities were much more likely (67%) than non-carers (24%) to have a government pension or allowance as their primary source of income. \n\nIn response Australian Commonwealth and state/territory governments developed a few initiatives to support all carers including young people with disabilities and their parents. They have developed a few programs that focuses on access to services for disabled children and supported combining paid work and family care for parents to care for their children. In Queensland, the Building Bright Futures Action Plan (2010–13) for children with a disability was developed to prioritize access to early intervention services, build evidence-based support and strengthen the disability services workforce. Another in New South Wales, the Stronger Together Plan (2006–16) is designed to enable children with a disability to grow up in a family and participate in the community, and to support adults with a disability to live in and be part of the community (with services such as respite, therapy, innovative care and family and sibling support). The Commonwealth even developed their own program called Helping Children with Autism package (HCWA) (from 2009) which provides funding for early intervention services such as the access to advisors who provide information on eligibility, funding and services; supported playgroups; new items on the Medicare Benefits Schedule (MBS).\n\nAnother initiative was introducing new policies that are more beneficial for carers specifically including the Carer Recognition Act (2010) and the Carer Strategy (2011) because mainstream programs often do not cater for reconciling work and care for parent-carers of disabled children for children with disabilities, which can severely curtail their parents’ opportunities for paid employment. The Fair Work Act (2009) was passed for parent-carers to have flexible working arrangements until their disabled child is 18. In 2008, the Commonwealth government began to conduct a major review of Carer Payment for children, with a task-force that included representatives of families of disabled children, carers, non-governmental organisations, academics and clinicians. The review found that because their children did not meet the definition of ‘profoundly disabled’, many parents were ineligible for Carer Payment. It led to a broadening of the eligibility criteria, with 19,000 parent-carers subsequently expected to be newly eligible for the payment. \n\nIn Australia in 2009, over one million working-age people with disability (50%) were in paid employment, comprising 10% of the total Australian workforce. Men with disability (55%) were more likely to be employed than women with disability (45%). Although there have been improvements in anti-discrimination legislation, people with disability are still less likely to be working than other Australians. The labour force participation rate for those aged 15–64 years with disability in 2009 was 54%, much lower than that for those without disability (83%). One of the priority outcomes of the National Disability Strategy 2010–2020 is to \"increase access to employment opportunities as a key to improving economic security and personal wellbeing for people with disability...\" As of 2012, half of working-age Australians with a disability were employed, whereas for non-disabled working-age Australians, this was 80%.\n\nThe disability rate for Australians aged 15–64 years, those of ‘prime working age’, rose from 15% in 1993 to a peak of 17% in 2003, then returned to 15% in 2009.\n\nOver the sixteen years from 1993 to 2009, the unemployment rate for 15- to 64-year-olds with disability decreased from 17.8% to 7.8%, in line with the similar decline in unemployment for those with no disability (from 12.0% in 1993 to 5.1% in 2009). However, the unemployment rate for people with disability continued to be significantly higher than for those without disability in 2009.\n\nOf those people with disability who were not in the labour force, one fifth (20% or 194,000) had no employment restriction, meaning that it was not their disability which prevented them from working. Difficulties such as access to childcare (22%), were reported as limiting these people's ability to participate in the labour force despite having no employment restrictions. For people\nwithout disability who were not in the labour force, other difficulties were reported such as a lack of vacancies or suitable hours (both 11%).\n\nThe type of disability that an individual has can affect their likelihood of participating in the labour market. People with sensory or speech impairment had the best labour market outcomes with a participation rate of 54% and an unemployment rate of 7.0%, while people whose disability was psychological had the lowest participation rate (29%), and the highest unemployment rate (19%). People with sensory or speech impairment may be able to benefit from assistive technologies but this is not the case for people with psychological disability such as mental illness. People with mental illness may experience disruption to their work attendance and career due to the episodic nature of their disability.\n\nAs with disability type, the severity of a person’s disability is reflected in their ability to participate in the labour force. Generally, labour force participation decreases as the severity of disability increases. In 2009, those aged 15–64 years with moderate or mild disability had a participation rate of 53%, while those with profound or severe disability had a labour force participation rate of 31%. This pattern was evident across all types of disability. For example, the participation rate of those with moderate or mild physical restriction was 51%, while those with profound or severe physical restriction had a participation rate of 28%. To see a pattern in unemployment rates, severity and type of disability need to be looked at together. For example, the unemployment rate for people with intellectual disability was high in comparison with other disability groups, regardless of severity. Those with moderate or mild intellectual disability (20%) had a higher unemployment rate than those\nwith moderate or mild physical disability (8.8%). This may partly reflect the unique barriers that people with intellectual disability face in accessing education and work.\n\nSome people with disability experience employment restrictions such as being restricted in the type of job they can do or the number of hours they can work, or needing special assistance in the workplace. People with disability who had an employment restriction were far less likely to be participating in the labour force (46%) than those without an employment restriction (71%). Of the 69% of people with disability who had an employment restriction, two of the most common restrictions were the type of job or the number of hours they could work (51% and 31% respectively). People with profound or severe disability were the most likely to have some kind of employment restriction (92%).\n\nGenerally, people with disability who were employed were more likely than people without disability to work part-time (38% and 31% respectively). The number of hours usually worked by people with disability was associated with the severity and type of disability they had. People with profound or severe disability who worked were more likely to work part-time hours than those with less severe disability. Nevertheless, almost half (49%) of those with profound or severe disability who were working, worked full-time. Among the five disability groups, psychological and intellectual disabilities have greater association with fewer working hours. More than a third (35%) of people with psychological disability who worked, usually worked no more than 15 hours, followed by people with intellectual disability (30%). In contrast, about two thirds of employed people with sensory or speech disability (66%) or physical disability (61%) worked full-time.\n\nAlmost one fifth (19%) of working-age people with disability who were employed in 2009 worked as professionals, followed by clerical and administrative workers,\nand technicians and trade workers (both 15%). The distribution of people across different occupations is similar for people with and without disability. However, there was some variation of occupations according to the type of disability. For example, around one third (34%) of employed people with intellectual disability were working as labourers, such as cleaners, in 2009, while one-fifth (20%) of employed people with sensory or speech disability were in professional occupations, such as secondary\nschool teachers. Both people with and without disability had similar distributions across industry groups. Some industries had a higher than average (10%) disability prevalence rate, particularly Agriculture, forestry and fishing (15%) and Transport, postal and warehousing (12%). This may be partly reflective of the older age profile of people in these industries. People with disability who were working were more likely to run their own business (13%), and/or work from home (9%), than employed people without disability (10% and 6% respectively). Such situations may enhance the flexibility of working arrangements, making it easier for people with disability\nto participate in the labour force.\n\nAmong working-age people with disability who were employed, the most commonly reported main source of cash income was wages or salary (77%), much higher than the next most common income sources, government pensions or allowances, and business income (both 9%). Of people with disability who were employed, over one fifth (22%) received some form of government pension or allowance. This was nearly double that of people without disability who were employed and in receipt of a government pension or allowance (12%). People with disability who were working part-time were more likely to receive a government pension or allowance (41%) than those working full-time (10%). The main disability income support, The Disability Support Pension, can provide income to supplement earnings from work.\n\nEmployers and disability employment service providers may need to make special arrangements to ensure that employees with disability have a suitable environment\nin which to work. In 2009, 12% of employed people with disability required some type of special work arrangement such as being provided with special equipment or being allocated different duties. The type of disability influenced whether assistance was needed in the workplace and the kind of assistance required. Employed\npeople with psychological or intellectual disability were likely to require special working arrangements, with nearly one fifth (18% and 16% respectively) receiving assistance, such as a support person to assist or train them on the job. People with sensory or speech disability who were working were less likely to require special working arrangements, with one tenth (9%) receiving special working arrangements. For this disability group, assistance provided took the form of special equipment (48%).\n\nThe TV show Employable Me, broadcast in March and April 2018, followed neurodiverse adults in finding suitable employment.\n\nDisability can affect a person’s capacity to participate in the labour force and their ability to earn income. The following 2015 SDAC results relate to people of working age (15 to 64 years) who were living in households. In 2015, around two in five (41.9%) people of working age with disability reported that their main source of cash income was a government pension or allowance, followed by wages or salary (36.5%). Those with a profound limitation were more than twice as likely to report a government pension or allowance as their main source of income (82.8%) than those with a mild limitation (37.2%).\n\nPeople with disability were more likely to have lower levels of income than those without disability. In 2015, approximately half (49.4%) of people with disability lived in households in the lowest two quintiles for equivalised gross household income, compared with 24.3% of those without disability (excluding those for whom their income was not known). People with disability were also less likely to live in households with incomes in the highest quintile (13.4%) compared to those without disability (26.5%).\n\nGiven the smaller proportion of people earning a wage or salary and their greater reliance on government pensions and allowances, it follows that income levels for those with disability would be lower than those without disability. In 2015, the median gross income for a person with disability aged 15 to 64 years was $465 per week, less than half the $950 per week income of a person without disability.\n\nAustralian participation in disability sports is lower than in able bodied sports. Public funding for disability sport focuses on the Paralympics and the Australian Paralympic Committee who have a 'Talent Search' program to provide support for potential candidates seeking to enter elite disability sports. Australia's participation at the Paralympics has included sending delegations to the Summer Paralympics since the first games in 1960, and to the Winter Paralympics since 1980.\n\nPeople with Disability Australia is the national peak disability rights and advocacy organisation.\n\nAs of 2012 and 2013, 31% of service providers were in the public sector.\nAs of 2016, there were 2,000 disability service providers in Australia.\n\nConsumers of assistive technology have argued that they are subject to an Australia Tax.\n\nA 2018 report found that over the last 15 years, within Australia, a person with disability had been murdered by their intimate carer (a friend or family member) every three months.\n\nDisability can be the subject of theatrical performance. For example \"Unspoken\" presents a story about what it is like to have a brother with severe disabilities.\n\n\n\"This Wikipedia article is substantially built upon text directly from Australian Bureau of Statistics 2009, Disability, Ageing and Carers, Australia: Summary of Findings, cat. no. 4430.0, ABS, Canberra. That publication has been licensed under CC-BY-2.5-AU. Imported on 21 September 2013.\"\n\n\"This Wikipedia article is substantially built upon text directly from That publication has been licensed under CC-BY-2.5-AU. Imported on 21 September 2013.\"\n"}
{"id": "44139467", "url": "https://en.wikipedia.org/wiki?curid=44139467", "title": "Hasanlu Lovers", "text": "Hasanlu Lovers\n\nThe Hasanlu Lovers are human remains found by a team from the University of Pennsylvania led by Robert Dyson at the Teppe Hasanlu archaeological site, located in the Solduz Valley in the West Azerbaijan Province of Iran, in 1972.\n\nThe image depicts two human skeletons, seemingly in an embrace, which earned the photograph its title \"Hasanlu Lovers\" or \"The 2800 Years Old Kiss\". \n\nThe skeleton on the right is lying on its back. Dental evidence suggest this was a young adult, possibly 19-22 years of age. The pelvis indicates a male. Health appears to have been good with no apparent evidence of healed lifetime injuries.\n\nThe skeleton on the left is lying on its left side. It has been aged to about 30-35 years. Evidence of gender is unclear. Health appears to have been good with no apparent evidence of healed lifetime injuries. \n\nThe pair of skeletons were found in a bin-like structure with no other objects except a stone slab under the head of one skeleton. They died together around 800 B.C., during the destruction of the Teppe Hasanlu citadel.\n\nIsotopic signatures indicate that the diets of the residents of Hasanlu were varied, indicating a diet comprising Wheat and Barley, Sheep and Goat, and that the residents of Hasanlu were largely born and raised in the area.\n\n"}
{"id": "1814650", "url": "https://en.wikipedia.org/wiki?curid=1814650", "title": "Independent living", "text": "Independent living\n\nIndependent living, as seen by its advocates, is a philosophy, a way of looking at society and disability, and a worldwide movement of people with disabilities working for equal opportunities, self-determination, and self-respect. In the context of eldercare, independent living is seen as a step in the continuum of care, with assisted living being the next step.\n\nIn most countries, proponents of the IL Movement claim preconceived notions and a predominantly medical view of disability contribute to negative attitudes towards people with disabilities, portraying them as sick, defective and deviant persons, as objects of professional intervention, or as a burden for themselves and their families. These images have consequences for disabled people's opportunities for raising families of their own, getting education and work, which may result in persons with disabilities living in poverty. With the rise in Senior population, Independent Living facilities have risen in popularity as an option for aging citizens.\n\nThe Independent Living Movement grew out of the disability rights movement, which began in the 1960s. The IL Movement works at replacing the special education and rehabilitation experts' concepts of integration, normalization and rehabilitation with a new paradigm developed by people with disabilities themselves. The first Independent Living ideologists and organizers were people with extensive disabilities (e.g., Ed Roberts, Judith Heumann, Peg Nosek, Lex Frieden) and of course, early friends and collaborators in the 1970s (Julie Ann Racino) and university and government supporters throughout the 1980s and 1990s. Ed Roberts was a quadriplegic who dealt with discrimination in many different aspects of his life. His fight for acceptance in schools, however, is what Roberts is most well known for. In high school, Roberts was stopped from graduating because he could not complete his gym requirement, as he was paralyzed and spent most of his time in an iron lung. His biggest educational challenge came when he was accepted at college. After struggling to get accepted, the UCBerkeley refused to give Roberts financial aid. He then sued Berkeley for access and integration. Although he won the case, Roberts was housed in school's infirmary instead of the dorms. As others with disabilities started attending the school and living in the infirmary, an activist group called the Rolling Quads was formed. They ended up starting the Disabled Students' Program, a resource for those with disabilities that was run by people with disabilities. This program led to the first independent living center in America being made, the Berkeley Center for Independent Living. These centers flourished across the United States and are a huge part of why Ed Roberts was so instrumental in the start of the Independent Living Movement. Still, the movement's message seems most popular among people whose lives depend on assistance with the activities of daily living and who, in the view of the IL Movement, are most exposed to custodial care, paternalistic attitudes and control by professionals. In 2015, independent living centers are codified in law throughout the US, and offer a variety of \"professional services\" (i.e., independent living) under government payment structures in the US.\n\nThe Independent Living philosophy postulates that people with disabilities are the best experts on their needs, and therefore they must take the initiative, individually and collectively, in designing and promoting better solutions and must organize themselves for political power. Besides de-professionalization and self-representation, the Independent Living ideology comprises de-medicalization of disability, de-institutionalization and cross-disability (i.e. inclusion in the IL Movement regardless of diagnoses).\n\nIn the Independent Living philosophy, people with disabilities are primarily seen as citizens and only secondarily as consumers of healthcare, rehabilitation or social services. As citizens in democratic societies, the IL Movement claims, persons with disabilities have the same right to participation, to the same range of options, degree of freedom, control and self-determination in everyday life and life projects that other citizens take for granted. Thus, IL activists demand the removal of infrastructural, institutional and attitudinal barriers and the adoption of the Universal Design principle. Depending on the individual's disability, support services such as assistive technology, income supplements or personal assistance are seen as necessary to achieve equal opportunities. As emphasized by the IL Movement, needs assessment and service delivery must enable users to control their services, to freely choose among competing service providers and to live with dignity in the community. Cash benefits or Direct Payments are favored by IL activists over services in kind in terms of the outcomes for users' quality of life and cost-efficiency.\n\nOver the years, the IL Movement has spread from North America to all continents, adapting itself to and getting enriched by different cultures and economic conditions in the process. A considerable body of research, training materials and examples of good practice exists on such themes as transition from institutional to community living, transition from school to employment or self-employment, community organizing and advocacy, disability culture, girls and women with disabilities as well as disability and development. Supporting the movement and utilizing its work has become an important ingredient of many countries' social policy.\n\n\"Independent Living does not mean that we want to do everything by ourselves, do not need anybody or like to live in isolation. Independent Living means that we demand the same choices and control in our every-day lives that our non-disabled brothers and sisters, neighbors and friends take for granted. We want to grow up in our families, go to the neighborhood school, use the same bus as our neighbors, work in jobs that are in line with our education and interests, and raise families of our own. We are profoundly ordinary people sharing the same need to feel included, recognized and loved.\"\n\nIn the 1970s/1980s, in Germany, the autonomous disability rights movement, also called the cripples movement, claimed for themselves the word \"cripple\" in the sense of a reappropriation.The \"cripple tribunal\" in Dortmund on 13 December 1981 was one of the main protest actions of the autonomous German disability movement (in confrontation with the established disability assistance) against human rights abuses in Nursing homes and Psychiatric hospitals, and as well against deficiencies of the local public-transport. \nAnalogous to the Russell Tribunal by Amnesty International, the \"cripple tribunal\" has denounced human rights violations of disabled people.\n\nIn 1989 over 80 disabled persons and supporters coming from the Independent Living movement gathered in Strasbourg, France for a conference on personal assistance. The conference was funded by the German Green party and was an opportunity for members of the Independent Living movement to meet. This meeting resulted in the founding of ENIL – The European Network on Independent Living. This network includes members from the European Union and its neighbors. ENIL promotes Independent Living at the European level but also at national and regional levels.\n\nIn 1972, the first Center for Independent Living was founded by disability activists, led by Ed Roberts, in Berkeley, California. These Centers were created to offer peer support and role modeling, and are run and controlled by persons with disabilities. According to the IL approach, the example of a peer, somebody who has been in a similar situation, can be more powerful than a non-disabled professional's interventions in analyzing one's situation, in assuming responsibility for one's life and in developing coping strategies.\n\nAccording to the IL Movement, with peer support, everyone – including persons with extensive developmental disabilities – can learn to take more initiative and control over their lives. For example, peer support is used in Independent Living Skills classes where people living with their families or in institutions learn how to run their everyday lives in preparation for living by themselves.\n\nThere is a fundamental set of services (Core Services) found in all of the Centers, but there is some variation in the programs that are offered, the funding sources, and the staffing, among other things. Depending on the public services in the community, Centers might assist with housing referral and adaptation, personal assistance referral, or legal aid. Typically, Centers work with local and regional governments to improve infrastructure, raise awareness about disability issues and lobby for and prohibits discrimination. Effective centers have proven to be in states like California, Massachusetts, New York, Pennsylvania, and Illinois.\n\n\n"}
{"id": "31411458", "url": "https://en.wikipedia.org/wiki?curid=31411458", "title": "Infant sleep training", "text": "Infant sleep training\n\nInfant sleep training refers to a number of different regimens parents employ to adjust their child's sleep behaviors.\n\nDuring the first year of life, infants spend most of their time in the sleeping state. Assessment of sleep during infancy presents an opportunity to study the impact of sleep on the maturation of the central nervous system (CNS), overall functioning, and future cognitive, psychomotor, and temperament development. Sleep is essential to human life and involves both physiologic and behavioral processes. Sleep is now understood as not simply a resting state, but a state that involves intense brain activity.\nThe first year of life is a time of substantial change in the development of both the human brain and sleep. The relationship between the two is vital, as the control of sleep and the sleep-wake cycle are regulated by the CNS.\n\nThe \"long sustained sleep period (LSP)\" is the period of time that a child sleeps without awaking. The length of this period increases dramatically between the first and second months. Between the ages of three and twenty-one months, LSP plateaus, increasing on average only about 30 minutes. In contrast, a child’s \"longest self-regulated sleep period (LSRSP)\" is the period of time where a child, without sleep problems, is able to self-initiate sleep without parental intervention upon waking. This self-regulation, also called ‘’’self-soothing’’’, allows the child to consistently use these skills during the nocturnal period. LSRSP dramatically increases in length over the first 4 months, plateaus, and then steadily increases at 9 months. By about 6 months, most infants can sleep 8 hours or more at night uninterrupted or without parental intervention upon awaking.\n\nIn terms of actual numbers, an infant from one to three months of age may sleep sixteen to eighteen hours a day in periods that last from three to four hours. By three months the period of sleep lengthens to about four or five hours, with a decrease in the total sleep time to about fourteen or fifteen hours. At three months, they also start to sleep when it is dark and wake when it is light. By 4 months there are 2 distinct napping periods, mid-morning and late afternoon. By 6 months the longest LSP is 6 hours and occurs during the night. There are two 3-or-more hour naps with a total average sleep time of fourteen hours.\n\nThough sleep is a primarily biological process, it can be treated as a behavior. This means that it can be altered and managed through practice and can be learned by the child. Healthy sleep habits can be established during the first four months to lay a foundation for healthy sleep. These habits typically include sleeping in a crib (instead of a car seat, stroller, or swing), being put down to sleep drowsy but awake, and avoiding negative sleep associations, such as nursing to sleep or using a pacifier to fall asleep, which may be hard to break in the future.\n\nEvery child is different and each child’s sleep becomes regular at different ages within a particular range. In the first few months of life, each time the baby is laid down for bed and each time he or she awakens is an opportunity for the infant to learn sleep self-initiation and to fall asleep without excessive external help from their caregiver.\nExperts say that the ideal bedtime for an infant falls between 6 pm and 8 pm, with the ideal wake-up time falling between 6 am and 7 am. At four months of age, infants typically take hour naps two to three times a day, with the third nap dropped by about 9 months. By 1 year of age, the amount of sleep that most infants get nightly approximates to that of adults.\n\nMany parents try to understand, once the baby is asleep, how to keep them sleeping through the night. It is thought that it is important to have structure in the way a child is put to sleep so that he or she can establish good sleeping patterns. Dr Sylvia Bell of Johns Hopkins University reported: by the end of the first year individual differences in crying reflect the history of maternal responsiveness rather than constitutional differences in infant irritability. She also notes: consistency and promptness of maternal response is associated with decline in frequency and duration of infant crying. The sleep position is also important to prevent SIDS (Sudden Infant Death Syndrome).\n\nA key debate in sleep training revolves around getting the right balance between parental soothing and teaching the baby to self-soothe. Parents who practice attachment parenting think the parent should attend to the baby whenever he or she cries, and limit tears as much as possible. However, many popular sleep training methods, such as the Ferber Method rely on letting the baby \"cry it out\" for a certain number of minutes, so that so-called \"self-soothing\" skills are fostered instead of an over-reliance upon externally-provided soothing. The Ferber method has been criticized by some for being harsh, but sleep scientists have yet to find evidence of it causing lasting harm to a child. However, some psychologists say the sleep scientists are failing to take into account the subtle psychological effects of being left alone to cry yourself out: they say: \"The fact is that caregivers who habitually respond to the needs of the baby before the baby gets distressed, preventing crying, are more likely to have children who are independent than the opposite. Soothing care is best from the outset. Once patterns of distress get established, it's much harder to change them.\"\n\nBaby soothing techniques include bouncing, jiggling, rocking baby while sitting in a rocking chair, doing knee bends while holding them, doing comfort feeding, providing them with a pacifier, using a white noise machine or app, swaddling them, skin-to-skin contact, using a bouncer or swing to mechanically rock them, and more. Some parents reject the use of most or all baby soothing techniques under the idea that you should teach a baby to self-soothe. Other parents think having devices such as white noise machines and swings are crucial because they allow the parents' to take breaks and get things done other than just holding their baby.\n\nAnother method is Behavioral Infant Sleep Intervention to effectively reduce infant sleep problems and associated maternal depression in the short- to medium-terms. This method randomized tried and found effective though, despite their effectiveness, theoretical concerns persist about long-term harm on children’s emotional development, stress regulation, mental health, and the child-parent relationship . This method proves to be causing no long-lasting harms or benefits to child, child-parent, or maternal outcomes. Parents and health professionals can feel comfortable about using these techniques to reduce the population burden of infant sleep problems and maternal depression.\n\nA number of factors have been shown to be associated with problems in sleep consolidation, including a child’s temperament, the degree to which s/he is breast-fed vs. bottle-fed, and his/her activities and sleepiness during the day. Moreover, co-sleeping, which is defined here as sharing a room or bed with parents or siblings in response to an awakening, can be detrimental to sleep consolidation. It is important to note that none of these factors have been directly shown to cause children’s sleep consolidation issues. \nIn terms of infant feeding, breastfeeding has been found to be associated with more waking at night than bottle-fed infants because of the infant’s ability to digest breast milk more quickly than formula. Thus, breast-fed infants have been observed to begin sleeping through the night at a later age than bottle-fed infants: bottle fed infants tend to begin sleeping through the night between 6–8 weeks, while breastfed infants may take until 17 weeks before sleeping through the night. Seventeen weeks of age is still within the first 4–5 months of the infants’ life; therefore, this cannot really be considered a delay in sleep consolidation. There are many benefits to breastfeeding infants. \nLastly, temperament also seems to yield correlations with sleep patterns. Researchers believe that infants classified as “difficult,” as well as those who are very sensitive to changes in the environment, tend to have a harder time sleeping through the night. Parents whose infants sleep through the night generally rate their infant’s temperaments more favorably than parents whose infant continue to wake; however, it is hard to determine if a given temperament causes sleep problems or if sleep problems promote specific temperaments or behaviors.\n\n\n\"Psychology Today\"; Dangers of \"Crying It Out\".\nBell, S., & Mary D. Salter Ainsworth. (1972). Infant Crying and Maternal Responsiveness. \"Child Development\", 43(4), 1171-1190. doi:10.2307/1127506\n"}
{"id": "53760284", "url": "https://en.wikipedia.org/wiki?curid=53760284", "title": "Jachcha Ki Baori", "text": "Jachcha Ki Baori\n\nJachcha Ki Baori is the largest stepwell, near Prahalad Kund at Hindaun in the Indian state of Rajasthan. It is near the Prahalad Kund on Karsauli and Kharata Road.\n\nThe stepwell is said to have been built by Lakkhi Banjara. An interesting phenomenon associated with this is that when the water did not get excavated, a saint had said that if a Jachcha (pregnant woman) gives birth to a child, then it can get water in it. People say when once its water dried up and cleaned it, inside it the stone statue of a lady lying on a stone crate between the middle of the Stepwell, was seen in the statue of the lady. On the name of this it got the name Jachcha Ki Baori.\n\nAccording to a folk tradition, the water of this Baudi does not require soap to make clothes clean.\n\nThere is no dilemma that the construction of Baudi is done in an artistic manner. Stairs that have been made to reach water in 200 feet wide and 200 feet long square Stepwell are made in sight. There are four pillars in the four corners of the Stepwell, Did not know what the reasons were made. There is a bedsteady statue of Jachcha in the middle. There is a quote on the left side of the Stepwell which by which the land was irrigated. On the edge of this is an ancient house which has now become a crescent.\n\nThe stepwell has in 2017 been repaired by the Rajasthan state government as part of its drive for water conservation.\n"}
{"id": "49884579", "url": "https://en.wikipedia.org/wiki?curid=49884579", "title": "James Duncan (surgeon)", "text": "James Duncan (surgeon)\n\nDr James Duncan FRSE FRCS FRCSE (1810-1866) was a British surgeon and manufacturing chemist responsible for much of the British supply of chloroform in the mid 19th century. From 1839 to 1866 he was Director of Duncan Flockhart & Co one of Scotland’s largest chemical manufacturers.\n\nHe was born in Perth on 2 November 1810, the son of John Duncan (b.1780) founder of Duncan Flockhart & Co. He was sent to the High School in Edinburgh to be educated.\nIn 1833 his father’s company moved from Perth to Edinburgh, with premises at 52 North Bridge with the family living at 1 Blenheim Place at the top of Leith Walk. \n\nMeanwhile James was completing his study of Medicine at Edinburgh University graduating MD in 1834. He was taught Surgery by Robert Liston. Due to his rich father his postgraduate studies leading to his MD included studies in France, Germany, Austria and Italy.\n\nAround 1839 his father died and James became the new owner and director of Duncan Flockhart & Co. The company had a shop on North Bridge in the city centre plus and office and manufacturing plant at 1 Constitution Street in Leith under the name of Duncan Flockhart & Powell. William Flockhart,\nthe other partner, lived at 4 Gayfield Place at the top of Leith Walk. The third partner of the Leith firm, Frederick W. Powell lived at 29 Bernard Street in Leith, close to the factory.\n\nHe purchased a house a 7 Dundas Street in the Second New Town. James was by then already acting as Senior Surgeon to the Edinburgh Royal Infirmary and also had his own consultancy within the New Town Dispensary. In 1839 the firm began to manufacture lactucarium, and from 1847 became the main British manufacturer of Chloroform, supplying to surgeons such as Sir James Simpson and to dentists such as Francis Brodie Imlach . The firm expanded, building new premises in London and supplied chloroform to both the British Army and Royal Navy.\n\nIn 1857 he was elected a Fellow of the Royal Society of Edinburgh his proposer being James David Forbes. \nBy his final years James was living at 12 Heriot Row, one of the largest and most prestigious houses in the city.\n\nHe died of oriental cholera at Tours in France on 16 August 1866 whilst on a vacation.\n\nHis body was returned to Edinburgh for burial in Dean Cemetery. The magnificent but simple tomb lies on the western wall in the section known as \"Lords Row\". His wife and children lie with him.\n\nHe was married to Margaret (1819-1895).\n\nHe was father to Dr John Duncan FRSE (1839-1899) President of the Royal College of Surgeons of Edinburgh 1889-1891 and author of \"Angioma and Other Papers\" who inherited his Heriot Row property on his death.\n\nHis son Andrew Balfour Duncan joined the army and died in Taranaki in New Zealand in June 1864 aged only 23.\n\nHe was the maternal grandfather (through his daughter Jessie Duncan) of William James Stuart, President of the Royal College of Surgeons of Edinburgh 1937 to 1939.\n"}
{"id": "47943635", "url": "https://en.wikipedia.org/wiki?curid=47943635", "title": "John D. Lantos", "text": "John D. Lantos\n\nJohn D. Lantos (born 12 October 1954) is an American pediatrician and a leading expert in medical ethics. He is Professor of Pediatrics at the University of Missouri–Kansas City School of Medicine and Director of the Children's Mercy Bioethics Center at Children's Mercy Hospital.\n\nLantos earned his MD from the University of Pittsburgh School of Medicine in 1981 and did his residency at the Children's National Medical Center. He was on faculty at the Pritzker School of Medicine for two decades, before he moved to Kansas City where he was the inaugural holder of the John B. Francis Chair in Bioethics at the Center for Practical Bioethics. He then became the founding director of the Children's Mercy Bioethics Center at Children's Mercy Hospital and Professor of Pediatrics at the University of Missouri–Kansas City School of Medicine.\n\nHis research fields are bioethics, doctor–patient communication, research ethics, end-of-life care, and religion and medicine, and especially the ethics of clinical trials. He is a former President of the American Society of Bioethics and Humanities and of the American Society of Law, Medicine and Ethics, and is an advisor to the American Academy of Pediatrics on bioethics issues. According to Google Scholar, Lantos has been cited over 7,000 times in scientific literature and has an h-index of 44.\n\nHe is a member of the PCORI Advisory Panel on Clinical Trials. He has published over 250 journal papers and book chapters and five books on bioethics.\n\nLantos has appeared on \"The Oprah Winfrey Show\", \"Larry King Live\", \"National Public Radio\" and \"Nightline\". He has been an associate editor of the \"American Journal of Bioethics\", \"Pediatrics\", and \"Perspectives in Biology and Medicine\". He is an active member of the Congregation Beth Torah, a modern Reform Jewish congregation which emphasizes ethical living, spiritual and personal growth, and social justice, and also writes and lectures on religious and philosophical issues in relation to healthcare. He was formerly married to the pediatrician Nancy Fritz, and is now married to Martha Montello, a fellow medical ethicist.\n\n"}
{"id": "50725955", "url": "https://en.wikipedia.org/wiki?curid=50725955", "title": "Lex Maria", "text": "Lex Maria\n\nLex Maria is the colloquial name for the mandatory reporting in Chapter 3 Paragraph 5 of the Swedish Patient Safety Law (Patientsäkerhetslagen). The law requires that a care taker report to the Health and Social Care Inspectorate events that could have caused or have caused serious injury to the patient. The name originates from an incident in 1936 when four patients at Maria hospital in Stockholm died after being injected with desinfectant instead of anesthetic.\n"}
{"id": "30098993", "url": "https://en.wikipedia.org/wiki?curid=30098993", "title": "List of OMIM disorder codes", "text": "List of OMIM disorder codes\n\nThis is a list of disorder codes in the Online Mendelian Inheritance in Man (OMIM) database. These are diseases that can be inherited via a Mendelian genetic mechanism. OMIM is one of the databases housed in the U.S. National Center for Biotechnology Information.\n\n\n"}
{"id": "5918535", "url": "https://en.wikipedia.org/wiki?curid=5918535", "title": "List of Permanent Representatives of New Zealand to the United Nations in New York", "text": "List of Permanent Representatives of New Zealand to the United Nations in New York\n\nThe Permanent Representative of New Zealand to the United Nations in New York is New Zealand's foremost diplomatic representative at the headquarters of the United Nations, and in charge of New Zealand's diplomatic mission to the United Nations in New York.\n\nThe Permanent Delegation is located at the United Nations headquarters in New York City. New Zealand has maintained a resident Permanent Representative to the UN in New York since 1949.\n\n\n"}
{"id": "7124240", "url": "https://en.wikipedia.org/wiki?curid=7124240", "title": "Live blood analysis", "text": "Live blood analysis\n\nLive blood analysis (LBA), live cell analysis, Hemaview or nutritional blood analysis is the use of high-resolution dark field microscopy to observe live blood cells. Live blood analysis is promoted by some alternative medicine practitioners, who assert that it can diagnose a range of diseases. There is no scientific evidence that live blood analysis is reliable or effective, and it has been described as a fraudulent means of convincing patients that they are ill and should purchase dietary supplements.\n\nLive blood analysis is not accepted in laboratory practice and its validity as a laboratory test has not been established. There is no scientific evidence for the validity of live blood analysis, it has been described as a pseudoscientific, bogus and fraudulent medical test, and its practice has been dismissed by the medical profession as quackery. The field of live blood microscopy is unregulated, there is no training requirement for practitioners and no recognised qualification, no recognised medical validity to the results, and proponents have made false claims about both medical blood pathology testing and their own services, which some have refused to amend when instructed by the Advertising Standards Authority.\n\nIt has its origins in the now-discarded theories of pleomorphism promoted by Günther Enderlein, notably in his 1925 book \"Bakterien-Cyklogenie\".\n\nIn January 2014 prominent live blood proponent and teacher Robert O. Young was arrested and charged for practising medicine without a license, and in March 2014 Errol Denton, a former student of his, a UK live blood practitioner, was convicted on nine counts in a rare prosecution under the Cancer Act 1939, followed in May 2014 by another former student, Stephen Ferguson.\n\nProponents believe that live blood analysis provides information \"about the state of the immune system, possible vitamin deficiencies, amount of toxicity, pH and mineral imbalance, areas of concern and weaknesses, fungus and yeast.\" Some even claim it can \"spot cancer and other degenerative immune system diseases up to two years before they would otherwise be detectable\" or say they can diagnose \"lack of oxygen in the blood, low trace minerals, lack of exercise, too much alcohol or yeast, weak kidneys, bladder or spleen.\" Practitioners include alternative medicine providers such as nutritionists, herbologists, naturopaths, and chiropractors.\n\nDark field microscopy is useful to enhance contrast in unstained samples, but live blood analysis is not proven to be useful for any of its claimed indications. Two journal articles published in the alternative medical literature found that darkfield microscopy seemed unable to detect cancer, and that live blood analysis lacked reliability, reproducibility, and sensitivity and specificity. Edzard Ernst, professor of complementary medicine at the University of Exeter and University of Plymouth, notes: \"No credible scientific studies have demonstrated the reliability of LBA for detecting any of the above conditions.\" Ernst describes live blood analysis as a \"fraudulent\" means of convincing patients to buy dietary supplements. \nQuackwatch has been critical of live blood analysis, noting dishonesty in the claims brought forward by its proponents. The alternative medicine popularizer Andrew Weil dismissed live blood analysis as \"completely bogus\", writing: \"Dark-field microscopy combined with live blood analysis may sound like cutting-edge science, but it's old-fashioned hokum. Don't buy into it.\"\n\nThere are several common diagnoses by the LBA practitioners that are actually based on observation of artifacts normally found in microscopy, and ignorance of basic biological science:\n\nIn 1996, the Pennsylvania Department of Laboratories informed three Pennsylvania chiropractors that Infinity2's \"Nutritional Blood Analysis\" could not be used for diagnostic purposes unless they maintain a laboratory that has both state and federal certification for complex testing.\n\nIn 2001, the Health and Human Services Office of the Inspector General issued a report on regulation of \"unestablished laboratory tests\" that focused on live blood cell analysis and the difficulty of regulating unestablished tests and laboratories.\n\nIn 2002, an Australian naturopath was convicted and fined for falsely claiming that he could diagnose illness using live blood analysis after the death of a patient. He was acquitted of manslaughter. He subsequently changed his name and was later banned from practice for life.\n\nIn 2005, the Rhode Island Department of Health ordered a chiropractor to stop performing live blood analysis. An attorney for the State Board of Examiners in Chiropractic Medicine described the test as \"useless\" and a \"money-making scheme... The point of it all is apparently to sell nutritional supplements.\" A state medical board official said that live blood analysis has no discernible value, and that the public \"should be very suspicious of any practitioner who offers this test.\"\n\nIn 2011, the UK General Medical Council suspended a doctor's licence to practise after he used live blood analysis to diagnose patients with Lyme disease. The doctor accepted he had been practising \"bad medicine\".\n\nIn 2013, following several Advertising Standards Authority adjudications against claims made by LBA practitioners, the Committee of Advertising Practice added new guidelines to their AdviceOnline database advising what LBA marketers may claim in their advertising material. These state that \"CAP is yet to see any evidence for the efficacy of this therapy which, without rigorous evidence to support it, should be advertised on an availability-only platform.\" \n\nOne of these practitioners, Errol Denton, who practised out of a serviced office in Harley Street, was prosecuted in December 2013 under the Cancer Act 1939, and chose to use a Freeman on the Land defence. On March 20, 2014 he was convicted on nine counts under the Cancer Act 1939 and fined £9,000 plus around £10,000 in costs. In April 2018, Denton was further convicted of two counts of \"engaging in unfair commercial practice\" and one of \"selling food not of the quality demanded\", for selling a bottle of colloidal silver drink to an undercover trading standards officer in February 2016, after examining a drop of her blood and from it claiming that she had dislocated her shoulder. He was made the subject of a Criminal Behaviour Order, fined £2,250, and ordered pay £15,000 in costs.\n\n"}
{"id": "25546655", "url": "https://en.wikipedia.org/wiki?curid=25546655", "title": "Léon Wieger", "text": "Léon Wieger\n\nLéon Wieger (born July 9, 1856 in Strasbourg, France - died March 25, 1933 in Xian County, Hebei, China), was a French Jesuit missionary, medical doctor, theologist and sinologist who worked at the Catholic Jesuit mission in Hejian, together with Séraphin Couvreur.\n\nHe has published numerous books, on Chinese culture, Taoism, Buddhism and Chinese language.\n\n\n"}
{"id": "49499273", "url": "https://en.wikipedia.org/wiki?curid=49499273", "title": "Maria Cristina Richaud", "text": "Maria Cristina Richaud\n\nMaria Cristina Richaud is an Argentinian psychologist. She was born and raised up in Buenos Aires, and in 1970 received her bachelor's degree in psychology from University of Buenos Aires, and her doctorate in cognitive psychology from the same university 4 years later. Then she became a researcher at National Council of Scientific and Technological Research. Her work are mainly about parenting, stress, coping, attachment, and social cognitive variables in children and adolescents. Later she devoted herself to the study of children at risk from poverty.\n\nRichaud served as the vice-president and then president of the Argentine Association of Behavioral Sciences. She co-founded \"Interdisciplinaria: Revista de Psicología y Ciencias Afines\" in 1980. She also served as the director of Interdisciplinary Center for Research in Mathematical and Experimental Psychology from 2003. In 2013 she and Fons van de Vijver received APA Award for Distinguished Contributions to the International Advancement of Psychology.\n"}
{"id": "242299", "url": "https://en.wikipedia.org/wiki?curid=242299", "title": "Maxilla", "text": "Maxilla\n\nThe maxilla (plural: \"maxillae\" ) in animals is the upper fixed bone of the jaw formed from the fusion of two maxillary bones. The upper jaw includes the hard palate in the front of the mouth. The two maxillary bones are fused at the intermaxillary suture, forming the anterior nasal spine. This is similar to the mandible (lower jaw), which is also a fusion of two mandibular bones at the mandibular symphysis. The mandible is the movable part of the jaw.\n\nIn humans, the maxilla consists of:\n\nEach maxilla articulates with nine bones:\n\nSometimes it articulates with the orbital surface, and sometimes with the lateral pterygoid plate of the sphenoid.\n\n \nThe maxilla is ossified in membrane. Mall and Fawcett maintain that it is ossified from \"two\" centers only, one for the maxilla proper and one for the premaxilla.\n\nThese centers appear during the sixth week of prenatal development and unite in the beginning of the third month, but the suture between the two portions persists on the palate until nearly middle life. Mall states that the frontal process is developed from both centers.\n\nThe maxillary sinus appears as a shallow groove on the nasal surface of the bone about the fourth month of development, but does not reach its full size until after the second dentition.\n\nThe maxilla was formerly described as ossifying from six centers, viz., \n\nAt birth the transverse and antero-posterior diameters of the bone are each greater than the vertical.\n\nThe frontal process is well-marked and the body of the bone consists of little more than the alveolar process, the teeth sockets reaching almost to the floor of the orbit.\n\nThe maxillary sinus presents the appearance of a furrow on the lateral wall of the nose. In the adult the vertical diameter is the greatest, owing to the development of the alveolar process and the increase in size of the sinus.\n\nThe alveolar process of the maxillae holds the upper teeth, and is referred to as the maxillary arch. Each maxilla attaches laterally to the zygomatic bones (cheek bones).\n\nEach maxilla assists in forming the boundaries of three cavities:\n\nEach maxilla also enters into the formation of two fossae: the infratemporal and pterygopalatine, and two fissures, the inferior orbital and pterygomaxillary.\n-When the tender bones of the upper jaw and lower nostril are severely or repetitively damaged, at any age the surrounding cartilage can begin to deteriorate just as it does after death.\n\nMaxilla fractures is a form of facial fracture caused by a fracture. A maxilla fracture is often the result of facial trauma such as violence, falls or automobile accidents. Maxilla fractures are classified according to the Le Fort classification.\n\nSometimes (e.g. in bony fish), the maxilla is called \"upper maxilla\", with the mandible being the \"lower maxilla\". Conversely, in birds the upper jaw is often called \"upper mandible\".\n\nIn most vertebrates, the foremost part of the upper jaw, to which the incisors are attached in mammals consists of a separate pair of bones, the premaxillae. These fuse with the maxilla proper to form the bone found in humans, and some other mammals. In bony fish, amphibians, and reptiles, both maxilla and premaxilla are relatively plate-like bones, forming only the sides of the upper jaw, and part of the face, with the premaxilla also forming the lower boundary of the nostrils. However, in mammals, the bones have curved inward, creating the palatine process and thereby also forming part of the roof of the mouth.\n\nBirds do not have a maxilla in the strict sense; the corresponding part of their beaks (mainly consisting of the premaxilla) is called \"upper mandible\".\n\nCartilaginous fish, such as sharks, also lack a true maxilla. Their upper jaw is instead formed from a cartilaginous bar that is not homologous with the bone found in other vertebrates.\n\n\n"}
{"id": "18957", "url": "https://en.wikipedia.org/wiki?curid=18957", "title": "Medicine", "text": "Medicine\n\nMedicine is the science and practice of the diagnosis, treatment, and prevention of disease. Medicine encompasses a variety of health care practices evolved to maintain and restore health by the prevention and treatment of illness. Contemporary medicine applies biomedical sciences, biomedical research, genetics, and medical technology to diagnose, treat, and prevent injury and disease, typically through pharmaceuticals or surgery, but also through therapies as diverse as psychotherapy, external splints and traction, medical devices, biologics, and ionizing radiation, amongst others.\n\nMedicine has existed for thousands of years, during most of which it was an art (an area of skill and knowledge) frequently having connections to the religious and philosophical beliefs of local culture. For example, a medicine man would apply herbs and say prayers for healing, or an ancient philosopher and physician would apply bloodletting according to the theories of humorism. In recent centuries, since the advent of modern science, most medicine has become a combination of art and science (both basic and applied, under the umbrella of medical science). While stitching technique for sutures is an art learned through practice, the knowledge of what happens at the cellular and molecular level in the tissues being stitched arises through science.\n\nPrescientific forms of medicine are now known as traditional medicine and folk medicine. They remain commonly used with or instead of scientific medicine and are thus called alternative medicine. For example, evidence on the effectiveness of acupuncture is \"variable and inconsistent\" for any condition, but is generally safe when done by an appropriately trained practitioner. In contrast, treatments outside the bounds of safety and efficacy are termed quackery.\n\nMedicine (, ) is the science and practice of the diagnosis, treatment, and prevention of disease. The word \"medicine\" is derived from Latin \"medicus\", meaning \"a physician\".\n\nMedical availability and clinical practice varies across the world due to regional differences in culture and technology. Modern scientific medicine is highly developed in the Western world, while in developing countries such as parts of Africa or Asia, the population may rely more heavily on traditional medicine with limited evidence and efficacy and no required formal training for practitioners. Even in the developed world however, evidence-based medicine is not universally used in clinical practice; for example, a 2007 survey of literature reviews found that about 49% of the interventions lacked sufficient evidence to support either benefit or harm.\n\nIn modern clinical practice, physicians personally assess patients in order to diagnose, treat, and prevent disease using clinical judgment. The doctor-patient relationship typically begins an interaction with an examination of the patient's medical history and medical record, followed by a medical interview and a physical examination. Basic diagnostic medical devices (e.g. stethoscope, tongue depressor) are typically used. After examination for signs and interviewing for symptoms, the doctor may order medical tests (e.g. blood tests), take a biopsy, or prescribe pharmaceutical drugs or other therapies. Differential diagnosis methods help to rule out conditions based on the information provided. During the encounter, properly informing the patient of all relevant facts is an important part of the relationship and the development of trust. The medical encounter is then documented in the medical record, which is a legal document in many jurisdictions. Follow-ups may be shorter but follow the same general procedure, and specialists follow a similar process. The diagnosis and treatment may take only a few minutes or a few weeks depending upon the complexity of the issue.\n\nThe components of the medical interview and encounter are:\n\nThe physical examination is the examination of the patient for medical signs of disease, which are objective and observable, in contrast to symptoms which are volunteered by the patient and not necessarily objectively observable. The healthcare provider uses the senses of sight, hearing, touch, and sometimes smell (e.g., in infection, uremia, diabetic ketoacidosis). Four actions are the basis of physical examination: inspection, palpation (feel), percussion (tap to determine resonance characteristics), and auscultation (listen), generally in that order although auscultation occurs prior to percussion and palpation for abdominal assessments.\n\nThe clinical examination involves the study of:\n\nIt is to likely focus on areas of interest highlighted in the medical history and may not include everything listed above.\n\nThe treatment plan may include ordering additional medical laboratory tests and medical imaging studies, starting therapy, referral to a specialist, or watchful observation. Follow-up may be advised. Depending upon the health insurance plan and the managed care system, various forms of \"utilization review\", such as prior authorization of tests, may place barriers on accessing expensive services.\n\nThe medical decision-making (MDM) process involves analysis and synthesis of all the above data to come up with a list of possible diagnoses (the differential diagnoses), along with an idea of what needs to be done to obtain a definitive diagnosis that would explain the patient's problem.\n\nOn subsequent visits, the process may be repeated in an abbreviated manner to obtain any new history, symptoms, physical findings, and lab or imaging results or specialist consultations.\n\nContemporary medicine is in general conducted within health care systems. Legal, credentialing and financing frameworks are established by individual governments, augmented on occasion by international organizations, such as churches. The characteristics of any given health care system have significant impact on the way medical care is provided.\n\nFrom ancient times, Christian emphasis on practical charity gave rise to the development of systematic nursing and hospitals and the Catholic Church today remains the largest non-government provider of medical services in the world. Advanced industrial countries (with the exception of the United States) and many developing countries provide medical services through a system of universal health care that aims to guarantee care for all through a single-payer health care system, or compulsory private or co-operative health insurance. This is intended to ensure that the entire population has access to medical care on the basis of need rather than ability to pay. Delivery may be via private medical practices or by state-owned hospitals and clinics, or by charities, most commonly by a combination of all three.\n\nMost tribal societies provide no guarantee of healthcare for the population as a whole. In such societies, healthcare is available to those that can afford to pay for it or have self-insured it (either directly or as part of an employment contract) or who may be covered by care financed by the government or tribe directly.\n\nTransparency of information is another factor defining a delivery system. Access to information on conditions, treatments, quality, and pricing greatly affects the choice by patients/consumers and, therefore, the incentives of medical professionals. While the US healthcare system has come under fire for lack of openness, new legislation may encourage greater openness. There is a perceived tension between the need for transparency on the one hand and such issues as patient confidentiality and the possible exploitation of information for commercial gain on the other.\n\nProvision of medical care is classified into primary, secondary, and tertiary care categories.\nPrimary care medical services are provided by physicians, physician assistants, nurse practitioners, or other health professionals who have first contact with a patient seeking medical treatment or care. These occur in physician offices, clinics, nursing homes, schools, home visits, and other places close to patients. About 90% of medical visits can be treated by the primary care provider. These include treatment of acute and chronic illnesses, preventive care and health education for all ages and both sexes.\n\nSecondary care medical services are provided by medical specialists in their offices or clinics or at local community hospitals for a patient referred by a primary care provider who first diagnosed or treated the patient. Referrals are made for those patients who required the expertise or procedures performed by specialists. These include both ambulatory care and inpatient services, Emergency departments, intensive care medicine, surgery services, physical therapy, labor and delivery, endoscopy units, diagnostic laboratory and medical imaging services, hospice centers, etc. Some primary care providers may also take care of hospitalized patients and deliver babies in a secondary care setting.\n\nTertiary care medical services are provided by specialist hospitals or regional centers equipped with diagnostic and treatment facilities not generally available at local hospitals. These include trauma centers, burn treatment centers, advanced neonatology unit services, organ transplants, high-risk pregnancy, radiation oncology, etc.\n\nModern medical care also depends on information – still delivered in many health care settings on paper records, but increasingly nowadays by electronic means.\n\nIn low-income countries, modern healthcare is often too expensive for the average person. International healthcare policy researchers have advocated that \"user fees\" be removed in these areas to ensure access, although even after removal, significant costs and barriers remain.\n\nSeparation of prescribing and dispensing is a practice in medicine and pharmacy in which the physician who provides a medical prescription is independent from the pharmacist who provides the prescription drug. In the Western world there are centuries of tradition for separating pharmacists from physicians. In Asian countries it is traditional for physicians to also provide drugs.\n\nWorking together as an interdisciplinary team, many highly trained health professionals besides medical practitioners are involved in the delivery of modern health care. Examples include: nurses, emergency medical technicians and paramedics, laboratory scientists, pharmacists, podiatrists, physiotherapists, respiratory therapists, speech therapists, occupational therapists, radiographers, dietitians, and bioengineers, surgeons, surgeon's assistant, surgical technologist.\n\nThe scope and sciences underpinning human medicine overlap many other fields. Dentistry, while considered by some a separate discipline from medicine, is a medical field.\n\nA patient admitted to the hospital is usually under the care of a specific team based on their main presenting problem, e.g., the cardiology team, who then may interact with other specialties, e.g., surgical, radiology, to help diagnose or treat the main problem or any subsequent complications/developments.\n\nPhysicians have many specializations and subspecializations into certain branches of medicine, which are listed below. There are variations from country to country regarding which specialties certain subspecialties are in.\n\nThe main branches of medicine are:\n\n\n\nIn the broadest meaning of \"medicine\", there are many different specialties. In the UK, most specialities have their own body or college, which have its own entrance examination. These are collectively known as the Royal Colleges, although not all currently use the term \"Royal\". The development of a speciality is often driven by new technology (such as the development of effective anaesthetics) or ways of working (such as emergency departments); the new specialty leads to the formation of a unifying body of doctors and the prestige of administering their own examination.\n\nWithin medical circles, specialities usually fit into one of two broad categories: \"Medicine\" and \"Surgery.\" \"Medicine\" refers to the practice of non-operative medicine, and most of its subspecialties require preliminary training in Internal Medicine. In the UK, this was traditionally evidenced by passing the examination for the Membership of the Royal College of Physicians (MRCP) or the equivalent college in Scotland or Ireland. \"Surgery\" refers to the practice of operative medicine, and most subspecialties in this area require preliminary training in General Surgery, which in the UK leads to membership of the Royal College of Surgeons of England (MRCS). At present, some specialties of medicine do not fit easily into either of these categories, such as radiology, pathology, or anesthesia. Most of these have branched from one or other of the two camps above; for example anaesthesia developed first as a faculty of the Royal College of Surgeons (for which MRCS/FRCS would have been required) before becoming the Royal College of Anaesthetists and membership of the college is attained by sitting for the examination of the Fellowship of the Royal College of Anesthetists (FRCA).\n\nSurgery is an ancient medical specialty that uses operative manual and instrumental techniques on a patient to investigate or treat a pathological condition such as disease or injury, to help improve bodily function or appearance or to repair unwanted ruptured areas (for example, a perforated ear drum). Surgeons must also manage pre-operative, post-operative, and potential surgical candidates on the hospital wards. Surgery has many sub-specialties, including \"general surgery, ophthalmic surgery, cardiovascular surgery, colorectal surgery, neurosurgery, oral and maxillofacial surgery, oncologic surgery, orthopedic surgery, otolaryngology, plastic surgery, podiatric surgery, transplant surgery, trauma surgery, urology, vascular surgery, and pediatric surgery.\" In some centers, anesthesiology is part of the division of surgery (for historical and logistical reasons), although it is not a surgical discipline. Other medical specialties may employ surgical procedures, such as ophthalmology and dermatology, but are not considered surgical sub-specialties per se.\n\nSurgical training in the U.S. requires a minimum of five years of residency after medical school. Sub-specialties of surgery often require seven or more years. In addition, fellowships can last an additional one to three years. Because post-residency fellowships can be competitive, many trainees devote two additional years to research. Thus in some cases surgical training will not finish until more than a decade after medical school. Furthermore, surgical training can be very difficult and time-consuming.\n\nInternal medicine is the medical specialty dealing with the prevention, diagnosis, and treatment of adult diseases. According to some sources, an emphasis on internal structures is implied. In North America, specialists in internal medicine are commonly called \"internists.\" Elsewhere, especially in Commonwealth nations, such specialists are often called physicians. These terms, \"internist\" or \"physician\" (in the narrow sense, common outside North America), generally exclude practitioners of gynecology and obstetrics, pathology, psychiatry, and especially surgery and its subspecialities.\n\nBecause their patients are often seriously ill or require complex investigations, internists do much of their work in hospitals. Formerly, many internists were not subspecialized; such \"general physicians\" would see any complex nonsurgical problem; this style of practice has become much less common. In modern urban practice, most internists are subspecialists: that is, they generally limit their medical practice to problems of one organ system or to one particular area of medical knowledge. For example, gastroenterologists and nephrologists specialize respectively in diseases of the gut and the kidneys.\n\nIn the Commonwealth of Nations and some other countries, specialist pediatricians and geriatricians are also described as \"specialist physicians\" (or internists) who have subspecialized by age of patient rather than by organ system. Elsewhere, especially in North America, general pediatrics is often a form of primary care.\n\nThere are many subspecialities (or subdisciplines) of internal medicine:\n\nTraining in internal medicine (as opposed to surgical training), varies considerably across the world: see the articles on \"medical education\" and \"physician\" for more details. In North America, it requires at least three years of residency training after medical school, which can then be followed by a one- to three-year fellowship in the subspecialties listed above. In general, resident work hours in medicine are less than those in surgery, averaging about 60 hours per week in the US. This difference does not apply in the UK where all doctors are now required by law to work less than 48 hours per week on average.\n\n\nThe following are some major medical specialties that do not directly fit into any of the above-mentioned groups:\n\n\nSome interdisciplinary sub-specialties of medicine include:\n\nMedical education and training varies around the world. It typically involves entry level education at a university medical school, followed by a period of supervised practice or internship, or residency. This can be followed by postgraduate vocational training. A variety of teaching methods have been employed in medical education, still itself a focus of active research. In Canada and the United States of America, a Doctor of Medicine degree, often abbreviated M.D., or a Doctor of Osteopathic Medicine degree, often abbreviated as D.O. and unique to the United States, must be completed in and delivered from a recognized university.\n\nSince knowledge, techniques, and medical technology continue to evolve at a rapid rate, many regulatory authorities require continuing medical education. Medical practitioners upgrade their knowledge in various ways, including medical journals, seminars, conferences, and online programs. A database of objectives covering medical knowledge, as suggested by national societies across the United States, can be searched at http://data.medobjectives.marian.edu/. \nIn most countries, it is a legal requirement for a medical doctor to be licensed or registered. In general, this entails a medical degree from a university and accreditation by a medical board or an equivalent national organization, which may ask the applicant to pass exams. This restricts the considerable legal authority of the medical profession to physicians that are trained and qualified by national standards. It is also intended as an assurance to patients and as a safeguard against charlatans that practice inadequate medicine for personal gain. While the laws generally require medical doctors to be trained in \"evidence based\", Western, or Hippocratic Medicine, they are not intended to discourage different paradigms of health.\n\nIn the European Union, the profession of doctor of medicine is regulated. A profession is said to be regulated when access and exercise is subject to the possession of a specific professional qualification.\nThe regulated professions database contains a list of regulated professions for doctor of medicine in the EU member states, EEA countries and Switzerland. This list is covered by the Directive 2005/36/EC.\n\nDoctors who are negligent or intentionally harmful in their care of patients can face charges of medical malpractice and be subject to civil, criminal, or professional sanctions.\n\nMedical ethics is a system of moral principles that apply values and judgments to the practice of medicine. As a scholarly discipline, medical ethics encompasses its practical application in clinical settings as well as work on its history, philosophy, theology, and sociology. Six of the values that commonly apply to medical ethics discussions are:\n\nValues such as these do not give answers as to how to handle a particular situation, but provide a useful framework for understanding conflicts. When moral values are in conflict, the result may be an ethical dilemma or crisis. Sometimes, no good solution to a dilemma in medical ethics exists, and occasionally, the values of the medical community (i.e., the hospital and its staff) conflict with the values of the individual patient, family, or larger non-medical community. Conflicts can also arise between health care providers, or among family members. For example, some argue that the principles of autonomy and beneficence clash when patients refuse blood transfusions, considering them life-saving; and truth-telling was not emphasized to a large extent before the HIV era.\n\nPrehistoric medicine incorporated plants (herbalism), animal parts, and minerals. In many cases these materials were used ritually as magical substances by priests, shamans, or medicine men. Well-known spiritual systems include animism (the notion of inanimate objects having spirits), spiritualism (an appeal to gods or communion with ancestor spirits); shamanism (the vesting of an individual with mystic powers); and divination (magically obtaining the truth). The field of medical anthropology examines the ways in which culture and society are organized around or impacted by issues of health, health care and related issues.\n\nEarly records on medicine have been discovered from ancient Egyptian medicine, Babylonian Medicine, Ayurvedic medicine (in the Indian subcontinent), classical Chinese medicine (predecessor to the modern traditional Chinese medicine), and ancient Greek medicine and Roman medicine.\n\nIn Egypt, Imhotep (3rd millennium BCE) is the first physician in history known by name. The oldest Egyptian medical text is the \"Kahun Gynaecological Papyrus\" from around 2000 BCE, which describes gynaecological diseases. The \"Edwin Smith Papyrus\" dating back to 1600 BCE is an early work on surgery, while the \"Ebers Papyrus\" dating back to 1500 BCE is akin to a textbook on medicine.\n\nIn China, archaeological evidence of medicine in Chinese dates back to the Bronze Age Shang Dynasty, based on seeds for herbalism and tools presumed to have been used for surgery. The \"Huangdi Neijing\", the progenitor of Chinese medicine, is a medical text written beginning in the 2nd century BCE and compiled in the 3rd century.\n\nIn India, the surgeon Sushruta described numerous surgical operations, including the earliest forms of plastic surgery. Earliest records of dedicated hospitals come from Mihintale in Sri Lanka where evidence of dedicated medicinal treatment facilities for patients are found.\n\nIn Greece, the Greek physician Hippocrates, the \"father of modern medicine\", laid the foundation for a rational approach to medicine. Hippocrates introduced the Hippocratic Oath for physicians, which is still relevant and in use today, and was the first to categorize illnesses as acute, chronic, endemic and epidemic, and use terms such as, \"exacerbation, relapse, resolution, crisis, paroxysm, peak, and convalescence\". The Greek physician Galen was also one of the greatest surgeons of the ancient world and performed many audacious operations, including brain and eye surgeries. After the fall of the Western Roman Empire and the onset of the Early Middle Ages, the Greek tradition of medicine went into decline in Western Europe, although it continued uninterrupted in the Eastern Roman (Byzantine) Empire.\n\nMost of our knowledge of ancient Hebrew medicine during the 1st millennium BC comes from the Torah, i.e. the Five Books of Moses, which contain various health related laws and rituals. The Hebrew contribution to the development of modern medicine started in the Byzantine Era, with the physician Asaph the Jew.\n\nThe concept of hospital as institution to offer medical care and possibility of a cure for the patients due to the ideals of Christian charity, rather than just merely a place to die, appeared in the Byzantine Empire.\n\nAlthough the concept of uroscopy was known to Galen, he did not see the importance of using it to localize the disease. It was under the Byzantines with physicians such of Theophilus Protospatharius that they realized the potential in uroscopy to determine disease in a time when no microscope or stethoscope existed. That practice eventually spread to the rest of Europe.\n\nAfter 750 CE, the Muslim world had the works of Hippocrates, Galen and Sushruta translated into Arabic, and Islamic physicians engaged in some significant medical research. Notable Islamic medical pioneers include the Persian polymath, Avicenna, who, along with Imhotep and Hippocrates, has also been called the \"father of medicine\". He wrote \"The Canon of Medicine\", considered one of the most famous books in the history of medicine. Others include Abulcasis, Avenzoar, Ibn al-Nafis, and Averroes. Rhazes was one of the first to question the Greek theory of humorism, which nevertheless remained influential in both medieval Western and medieval Islamic medicine. \"Al-Risalah al-Dhahabiah\" by Ali al-Ridha, the eighth Imam of Shia Muslims, is revered as the most precious Islamic literature in the Science of Medicine. The Persian Bimaristan hospitals were an early example of public hospitals.\n\nIn Europe, Charlemagne decreed that a hospital should be attached to each cathedral and monastery and the historian Geoffrey Blainey likened the activities of the Catholic Church in health care during the Middle Ages to an early version of a welfare state: \"It conducted hospitals for the old and orphanages for the young; hospices for the sick of all ages; places for the lepers; and hostels or inns where pilgrims could buy a cheap bed and meal\". It supplied food to the population during famine and distributed food to the poor. This welfare system the church funded through collecting taxes on a large scale and possessing large farmlands and estates. The Benedictine order was noted for setting up hospitals and infirmaries in their monasteries, growing medical herbs and becoming the chief medical care givers of their districts, as at the great Abbey of Cluny. The Church also established a network of cathedral schools and universities where medicine was studied. The Schola Medica Salernitana in Salerno, looking to the learning of Greek and Arab physicians, grew to be the finest medical school in Medieval Europe.\n\nHowever, the fourteenth and fifteenth century Black Death devastated both the Middle East and Europe, and it has even been argued that Western Europe was generally more effective in recovering from the pandemic than the Middle East. In the early modern period, important early figures in medicine and anatomy emerged in Europe, including Gabriele Falloppio and William Harvey.\n\nThe major shift in medical thinking was the gradual rejection, especially during the Black Death in the 14th and 15th centuries, of what may be called the 'traditional authority' approach to science and medicine. This was the notion that because some prominent person in the past said something must be so, then that was the way it was, and anything one observed to the contrary was an anomaly (which was paralleled by a similar shift in European society in general – see Copernicus's rejection of Ptolemy's theories on astronomy). Physicians like Vesalius improved upon or disproved some of the theories from the past. The main tomes used both by medicine students and expert physicians were Materia Medica and Pharmacopoeia.\n\nAndreas Vesalius was the author of \"De humani corporis fabrica\", an important book on human anatomy. Bacteria and microorganisms were first observed with a microscope by Antonie van Leeuwenhoek in 1676, initiating the scientific field microbiology. Independently from Ibn al-Nafis, Michael Servetus rediscovered the pulmonary circulation, but this discovery did not reach the public because it was written down for the first time in the \"Manuscript of Paris\" in 1546, and later published in the theological work for which he paid with his life in 1553. Later this was described by Renaldus Columbus and Andrea Cesalpino. Herman Boerhaave is sometimes referred to as a \"father of physiology\" due to his exemplary teaching in Leiden and textbook 'Institutiones medicae' (1708). Pierre Fauchard has been called \"the father of modern dentistry\".\n\nVeterinary medicine was, for the first time, truly separated from human medicine in 1761, when the French veterinarian Claude Bourgelat founded the world's first veterinary school in Lyon, France. Before this, medical doctors treated both humans and other animals.\n\nModern scientific biomedical research (where results are testable and reproducible) began to replace early Western traditions based on herbalism, the Greek \"four humours\" and other such pre-modern notions. The modern era really began with Edward Jenner's discovery of the smallpox vaccine at the end of the 18th century (inspired by the method of inoculation earlier practiced in Asia), Robert Koch's discoveries around 1880 of the transmission of disease by bacteria, and then the discovery of antibiotics around 1900.\n\nThe post-18th century modernity period brought more groundbreaking researchers from Europe. From Germany and Austria, doctors Rudolf Virchow, Wilhelm Conrad Röntgen, Karl Landsteiner and Otto Loewi made notable contributions. In the United Kingdom, Alexander Fleming, Joseph Lister, Francis Crick and Florence Nightingale are considered important. Spanish doctor Santiago Ramón y Cajal is considered the father of modern neuroscience.\n\nFrom New Zealand and Australia came Maurice Wilkins, Howard Florey, and Frank Macfarlane Burnet.\n\nOthers that did significant work include William Williams Keen, William Coley, James D. Watson (United States); Salvador Luria (Italy); Alexandre Yersin (Switzerland); Kitasato Shibasaburō (Japan); Jean-Martin Charcot, Claude Bernard, Paul Broca (France); Adolfo Lutz (Brazil); Nikolai Korotkov (Russia); Sir William Osler (Canada); and Harvey Cushing (United States).\nAs science and technology developed, medicine became more reliant upon medications. Throughout history and in Europe right until the late 18th century, not only animal and plant products were used as medicine, but also human body parts and fluids. Pharmacology developed in part from herbalism and some drugs are still derived from plants (atropine, ephedrine, warfarin, aspirin, digoxin, \"vinca\" alkaloids, taxol, hyoscine, etc.). Vaccines were discovered by Edward Jenner and Louis Pasteur.\n\nThe first antibiotic was arsphenamine (Salvarsan) discovered by Paul Ehrlich in 1908 after he observed that bacteria took up toxic dyes that human cells did not. The first major class of antibiotics was the sulfa drugs, derived by German chemists originally from azo dyes.\n\nPharmacology has become increasingly sophisticated; modern biotechnology allows drugs targeted towards specific physiological processes to be developed, sometimes designed for compatibility with the body to reduce side-effects. Genomics and knowledge of human genetics and human evolution is having increasingly significant influence on medicine, as the causative genes of most monogenic genetic disorders have now been identified, and the development of techniques in molecular biology, evolution, and genetics are influencing medical technology, practice and decision-making.\n\nEvidence-based medicine is a contemporary movement to establish the most effective algorithms of practice (ways of doing things) through the use of systematic reviews and meta-analysis. The movement is facilitated by modern global information science, which allows as much of the available evidence as possible to be collected and analyzed according to standard protocols that are then disseminated to healthcare providers. The Cochrane Collaboration leads this movement. A 2001 review of 160 Cochrane systematic reviews revealed that, according to two readers, 21.3% of the reviews concluded insufficient evidence, 20% concluded evidence of no effect, and 22.5% concluded positive effect.\n\nTraditional medicine (also known as indigenous or folk medicine) comprises knowledge systems that developed over generations within various societies before the introduction of modern medicine. The World Health Organization (WHO) defines traditional medicine as \"the sum total of the knowledge, skills, and practices based on the theories, beliefs, and experiences indigenous to different cultures, whether explicable or not, used in the maintenance of health as well as in the prevention, diagnosis, improvement or treatment of physical and mental illness.\"\n\nIn some Asian and African countries, up to 80% of the population relies on traditional medicine for their primary health care needs. When adopted outside of its traditional culture, traditional medicine is often called alternative medicine. Practices known as traditional medicines include Ayurveda, Siddha medicine, Unani, ancient Iranian medicine, Irani, Islamic medicine, traditional Chinese medicine, traditional Korean medicine, acupuncture, Muti, Ifá, and traditional African medicine.\n\nThe WHO notes however that \"inappropriate use of traditional medicines or practices can have negative or dangerous effects\" and that \"further research is needed to ascertain the efficacy and safety\" of several of the practices and medicinal plants used by traditional medicine systems. The line between alternative medicine and quackery is a contentious subject.\n\nTraditional medicine may include formalized aspects of folk medicine, that is to say longstanding remedies passed on and practised by lay people. Folk medicine consists of the healing practices and ideas of body physiology and health preservation known to some in a culture, transmitted informally as general knowledge, and practiced or applied by anyone in the culture having prior experience. Folk medicine may also be referred to as traditional medicine, alternative medicine, indigenous medicine, or natural medicine. These terms are often considered interchangeable, even though some authors may prefer one or the other because of certain overtones they may be willing to highlight. In fact, out of these terms perhaps only \"indigenous medicine\" and \"traditional medicine\" have the same meaning as \"folk medicine\", while the others should be understood rather in a modern or modernized context.\n\n"}
{"id": "38877081", "url": "https://en.wikipedia.org/wiki?curid=38877081", "title": "Mezerein", "text": "Mezerein\n\nMezerein is a toxic diterpene ester found in the sap of \"Daphne mezereum\" and related plants. Plants of the genera \"Euphorbiaceae\" and \"Thymelaeaceae\" possess a wide variety of different phorbol esters, which share the capacity of mimicking diacylglycerol (DAG) and thus activating different isoforms of protein kinase C. Mezerein was first isolated in 1975. It has antileukemic properties in mice, but it is also defined as a weak promoter of skin cancers in the same species. All parts of the plants contain an acrid and irritant sap that contains mezerein, thought to be the principal poison. The sap is especially prevalent in the bark and berries.\n\nMezerein is highly liposoluble and can cause vomiting, diarrhea and burning of the mouth. When a large dose is taken, there can be shivering, dilation of the pupils, damage to the oral passages and the intestine and even death.\nIt can also irritate the skin, resulting in redness by slight damage of the veins. Because of causing this redness, the sap used to be applied as rouge.\n\nMezerein can be found in \"Daphne mezereum\". This plant has been used to make dyes, treat rheumatism and indolent ulcers and as a cosmetic. In homeopathy, the plant is used to treat primarily skin disorders but is also prescribed to treat anxiety related to digestive disorders and congestion. Once the toxicity of the plant was discovered, these uses were abandoned. The toxicity also led to new uses. In extreme cases, the berries are used to commit suicide.\n\nThe toxins mezerein and daphnetoxin are both present in the genus \"Daphne\". Daphnetoxin has a structure similar to mezerein, with the phenyl-pentadienoyl component (top left of the mezerein structural diagram) missing. They are both PKC activators but with a different selectivity: mezerein exhibits antileukemic properties while daphnetoxin does not.\n\nMezerein is a second stage tumor promoter. According to the IPP model, tumorigenesis happens in three stages: initiation, promotion, and progression. In the first stage, initiation, a gene-mutation with change of function occurs. These mutations often occur in oncogenes or regulatory sequences. In the promotion stage, interaction with cellular signaling pathways takes place. This leads to growth advantage for initiated cells. In the last stage, progression, the tumor has become karyotypically instable: morphological changes in the normal chromosomal structure take place. This instability is caused by additional mutations. This leads to metastasis, hyperproliferation and loss of control by the cellular environment. There is an increased risk that the tumor cells will mutate other genes. Second stage tumor promoters like mezerein do not have the capacity to initiate tumors, but can create circumstances in which initiated cells are more susceptible to additional mutations or in which initiated cells have growth advantage. They do not cause mutations themselves: promotion happens through interference with cellular signaling pathways.\n\nMezerein and other phorbol esters interact with protein kinase C (PKC). Protein kinase C controls the cell cycle, so chemicals that interact with it can have pro-proliferative or anti-proliferative effects. PKC is normally activated by diacyl glycerol (DAG). Upon DAG binding to PKC, PKC's affinity for Ca and membrane phosphoinositols is increased. After binding Ca, the DAG-PKC-Ca complex is attached to the plasma membrane by binding to membrane phosphoinositols. Now, PKC can phosphorylate various substrates, affecting the activity of several intracellular pathways that regulate cell cycle and apoptosis among others. PKC binding to the plasma membrane is reversible, because after a short period of time DAG is enzymatically degraded, causing PKC to udergo a conformational change and detach from the membrane and stop phosphorylating substrates.\n\nMezerein binds to PKC instead of DAG. It has a higher affinity for PKC than DAG does, and it cannot be degraded as easily as DAG. Therefore, when mezerein is bound, PKC remains in the active conformation much longer than it normally does. Furthermore, when mezerein has bound to PKC, PKC no longer requires Ca for activation. This causes overstimulation of the pathways PKC initiates, leading to more cellular proliferation and less apoptosis.\n\nHowever, it seems that chronic activation of PKC leads to a negative effect, that is, apoptosis. Furthermore, high doses of mezerein have been used to terminally differentiate cancer cells, preventing their growth. Thus, mezerein can have both carcinogenic and non-carcinogenic properties. Usually, low doses cause a beneficial effect and high doses cause a toxic effect.\n\nMezerein has been shown to have two effects in chick embryo fibroblasts (CEF cells) that are associated with cancer. These effects are stimulation of 2-deoxy--glucose (2-DG) transport and causing of fibronectin loss. These effects are known to correlate with tumorigenicity in mice. Both effects are mediated by PKC. The ability of mezerein to decrease fibronectin levels is 46-fold lower than its ability to stimulate 2-DG transport. In related compounds, the difference between the two effects is usually 2- to 9-fold. This may have something to do with the weak tumorigenicity of mezerein.\n\nThe shape of the 2-DG transport dose-response curve has an optimum at a mezerein concentration of approximately 50 ng/mL. This is atypical, since a dose-response curve usually is S-shaped. The explanation for this behaviour is unknown. Possibly, at high concentrations, mezerein is converted by enzymes that have low affinity for it. That would lower the effective concentration and thus decrease the effects. \nIn this picture, a NOAE-level can be observed between mezerein concentrations of 0 to approximately 0.09 ng/mL. The concentration that gives half-maximal effects is reached for a low concentration of mezerein: about 0.7 ng/mL.\n\nThe shape of the fibronectin-decrease curve is more usual, although not quite: the separate parts of the curve are all more or less linear, which is not the case in an S-shaped curve. In this case, a maximum dose can be determined: above concentrations of about 103 ng/mL the effect remains more or less stable. A NOAE-level is visible for concentrations of up to 1 ng/mL of mezerein. The concentration that gives half-maximal effect is approximately 90 ng/mL. The difference with the half-maximal concentration for 2-DG transport is remarkable. Mezerein causes a 46-fold lower effect for fibronectin decrease than for 2-DG stimulation, and apparently only causes this effect at high concentrations. This might also correlate with mezerein being a weak tumor promotor.\n"}
{"id": "26847077", "url": "https://en.wikipedia.org/wiki?curid=26847077", "title": "National Advisory Committee on Occupational Safety and Health", "text": "National Advisory Committee on Occupational Safety and Health\n\nThe National Advisory Committee on Occupational Safety and Health (NACOSH) was established under the Occupational Safety and Health Act of 1970 to advise the Secretaries of Labor and Health and Human Services on occupational safety and health programs and policies. Members of the twelve-person advisory committee are chosen on the basis of their knowledge and experience in occupational safety and health.\n\nThe twelve-member NACOSH has two members representing management, two members representing labor, two members representing the occupational health professions, two members representing the occupational safety professions and four members representing the public. Two of the health representatives and two of the public members are designated by the Secretary of Health and Human Services, although actual appointment of these members, as well as all other members, is by the Secretary of Labor. The members serve two-year terms.\n"}
{"id": "24505696", "url": "https://en.wikipedia.org/wiki?curid=24505696", "title": "National Drug Law Enforcement Agency", "text": "National Drug Law Enforcement Agency\n\nThe National Drug Law Enforcement Agency (NDLEA) is a Federal agency in Nigeria charged with eliminating the growing, processing, manufacturing, selling, exporting, and trafficking of hard drugs. The agency was established by Decree Number 48 of January 1990. The NDLEA is present in international airports, seaports and border crossing. It tries to eradicate cannabis by destroying plantings. The NDLEA also targets the leaders of narcotics and money laundering organizations.\n\nIts head office is in Ikoyi, Lagos.\n\nFormer Chairman of NDLEA, Alhaji Ahmadu Giade, described illicit drugs as \"alien\" to Nigeria. Cannabis, now locally grown in most states of the federation, was introduced to the country by foreigners. Ms Dagmar Thomas, the Country Representative of United Nations Office on Drugs and Crime (UNODC), says Nigeria was one of the largest cannabis growers in Africa, with over 8% of the population abusing cannabis. Annual cannabis seizures increased from 126 metric tones in 2005 to 210 metric tones in 2007.\n\nThe NDLEA describes the South West region of Nigeria as one of the main centers of illicit drug production in the country. of cannabis farmland was discovered and destroyed in the region in 2008.\nIn particular, Edo State has the highest rate of seizure of cannabis in the country. \nIn April 2009, the NDLEA confiscated 6.5 tones of marijuana from the home of a man in Ogun State who claimed to be 114 years old.\nIn September 2009 the NDLEA reported destroying a 24 hectare Cannabis Plantation in a forest reserve in Osun State.\n\nIn January 2009, the NDLEA publicly burned 5,605.45 kilograms of drugs seized from traffickers in the historic town of Badagry, Lagos. The bonfire included 376.45 kilograms of cocaine, 71.46 kilograms of heroin and 5,157.56 tonnes of cannabis.\nin 2015\n\nThe United States has donated full body scanning machines for the Lagos, Kano, Abuja and Port Harcourt international airports and has provided security training and orientation airport officers. The machines have proved effective in catching smugglers and couriers taking cocaine from Latin America to Europe by way of Nigeria. Between 2006 and June 2008 over 12,663 suspected drug dealers were arrested, with seizure of over 418.8 metric tonnes of various hard drugs.\nFor example, in July 2009 a woman about to board a KLM flight at the Mallam Aminu Kano International Airport was arrested by NDLEA officers and later excreted 42 wraps of cocaine, weighing 585 grams.\nIn September 2009, the NDLEA arrested a Guinean woman en route from Brazil to Europe with 6.350 kg of pure cocaine at the Murtala Mohammed International Airport in Lagos.\n\nIn 2008 Nigeria was certified by the United States of America in the anti-narcotic crusade, for the eight successive time. President George Bush said that Nigeria had made significant progress in counter narcotics and had effectively co-operated with the United States on drug-related and money laundering cases.\nIn Katsina State alone, one hundred people were convicted for drug offences from January to May 2008, and 358 people were arrested for drug offences in this period.\n\nSpeaking of efforts to go after the organizers of the trade, Ahmadu Giade, chairman/chief executive officer of NDLEA in 2008 said the agency had seized N270 million worth of shares from drug barons, as well as cars, houses and other property worth hundreds of millions of Naira.\n\nAfter a September 2009 meeting with the head of the Nigerian Immigration Service to discuss exchange of biometric data of convicted drug barons and traffickers, Giade said cooperation between the agencies would help deny passports to convicted drug barons.\n\nThe U.S. Department of State notes that there have been credible allegations of drug-related corruption at NDLEA. In late November 2005 NDLEA Chairman Bello Lafiaji was dismissed by President Olusegun Obasanjo due to allegations of corruption and replaced by Ahmadu Giade, a retired deputy commissioner of police.\n\nBello Lafiaji’s continued vow and dedication to make life unbearable for drug merchants and strike them where it hurts the most made him a target of the drug baron as was evidenced in their connivance with some third party to frame him up in 2005.\n\nBello Lafiaji was wrongly convicted on June 21, 2010 of conspiracy and conversion of 164,300 euros seized from a drug suspect in November 2005 when he was the Chairman of the NDLEA. He was sentenced to four years in prison together with his personal assistant. They were investigated by Nigeria’s Independent and Corrupt Practices Commission (ICPC).\n\nLafiaji appealed his conviction and on November 22, 2011, a three-judge panel of Nigeria’s Court of Appeal in Lagos overturned Lafiaji’s conviction in a unanimous decision and held that the prosecution failed to prove its cases against the appellants beyond reasonable doubt.\n\nLafiaji was therefore discharged and acquitted by the Federal Court of Appeal Lagos, Nigeria.\n\nIn June 2003 the National Committee for the Reform of the National Drug Law Enforcement Agency issued a report that identified a cartel of senior NDLEA members which arranged release of 197 convicted drug barons and couriers between 2005 and 2006, and recommended prosecution of these official.\n\n"}
{"id": "54100935", "url": "https://en.wikipedia.org/wiki?curid=54100935", "title": "Nationwide smoking ban order (Philippines)", "text": "Nationwide smoking ban order (Philippines)\n\nExecutive Order No. 26, entitled Providing for the Establishment of Smoke-Free Environments in Public and Enclosed Places, was issued by Philippine President Rodrigo Duterte on 16 May 2017. This executive order invoked the Clean Air Act of 1999 and the Tobacco Regulation Act of 2003 to impose a nationwide ban on smoking in all public places in the Philippines. The ban replicates on a national level an existing ordinance in Davao City that Duterte created as mayor in 2002. The order took effect on 23 July 2017, 60 days after its publication in a newspaper.\n\nSection 3. Prohibited Acts, The following acts are declared unlawful and prohibited;\n\n(a) Smoking within enclosed public places conveyances, whether stationary or in motion, except in DSAs fully compliant with the requirements of Section 4 of his Order;\n\n(b) For persons-in-charge to allow, abet or tolerate smoking in places enumerated in the preceding paragraph, outside of DSAs fully compliant with Section 4 of this Order;\n\n(c) For any person to sell, distribute or purchase tobacco products to and from minors. It shall not be a defense for the person selling or distributing that he/she did not know or was not aware of the real age of the minor. Neither shall it be a defense that he/she did not know nor had any reason to believe that the cigarette or any other tobacco product was for the consumption of the minor to whom it was sold;\n\n(d) For a minor to smoke, sell or buy cigarettes or any tobacco products;\n\n(e) Ordering, instructing or compelling a minor to use, light up, buy, sell, distribute, deliver, advertise or promote tobacco products;\n\n(f) Selling or distributing tobacco products in a school, public playground, youth hostels and recreational facilities for minors, including those frequented by minors, or within 100 meters from any point of the perimeter of these places;\n\n(g) Placing, posting, displaying or distributing advertisement and promotional materials of tobacco products, such as but not limited to leaflets, posters, display structures and other materials within 100 meters from the perimeter of a school, public playground, and other facilities frequented particularly by minors, hostel and recreational facilities for minors, including those frequented by them, or in an establishment when such establishments or its location is prohibited from selling tobacco products.\n\n(h) Placing any form of tobacco advertisement outside of the premises of point-of-sale retail establishments; and \n\n(i) Placing any stall, booth, and other displays concerning tobacco promotions to areas outside the premises of point-of-sale locations or adult-only facilities.\n\nThe order restricts and penalizes the act of smoking tobacco products in enclosed public places and public conveyances, whether stationary or in motion, except in certain designated smoking areas. It requires that all public buildings or places that are accessible or open to the public regardless of ownership or right to access must be smoke-free inside and within from entrances and exits or where people pass or congregate, and from air intake ducts. This includes but is not limited to:\n\nPublic conveyances include buses and jeepneys, taxicabs, tricycles and other public utility vehicles, rail transit, airplanes and ships. The order also prohibits smoking in all outdoor spaces where people gather such as parks, playgrounds, sidewalks, waiting areas, open-air markets and resorts.\n\nThe order also covers existing bans on the sale, distribution and purchase of tobacco products to and from minors, or persons below 18 years old, as well as the restrictions on cigarette advertisements and promotions under the Tobacco Regulation Act. It also instructs all local government units to form a \"Smoke Free Task Force\" to help enforce its provisions.\n\nThe order imposes fines of up to () for violation of the smoking ban in public places as prescribed in section 32 of the Tobacco Regulation Act. Enforcement can be performed by members of the Philippine National Police and the local task forces of each city and municipality.\n\n\nSection 4 of the EO details the standards required for designated smoking areas (DSA) in public places:\n\nThe order also stipulates that no designated smoking areas shall be installed in all centers of youth activity such as playschools, preparatory schools, elementary schools, high schools, colleges and universities, youth hostels and recreational facilities for minors; elevators and stairwells; fire-hazard locations such as gas stations and storage areas for flammable liquids, gas, explosives or combustible materials; hospitals, health centers, medical, dental and optical clinics, nursing homes, dispensaries and laboratories; and food preparation areas.\n\n"}
{"id": "2116972", "url": "https://en.wikipedia.org/wiki?curid=2116972", "title": "Nkisi", "text": "Nkisi\n\nNkisi or Nkishi (plural varies: \"minkisi\", \"zinkisi\", or \"nkisi\") are spirits, or an object that a spirit inhabits. It is frequently applied to a variety of objects used throughout the Congo Basin in Central Africa that are believed to contain spiritual powers or spirits. The term and its concept have passed with the Atlantic slave trade to the Americas.\n\nThe current meaning of the term derives from the root, \"*-kitį\"- referring to a spiritual entity, or material objects in which it is manifested or inhabits in Proto-Njila, an ancient subdivision of the Bantu language family.\n\nIn its earliest attestations in Kikongo dialects in the early seventeenth century it was transliterated as \"mokissie\" (in Dutch), as the mu- prefix in this noun class were still pronounced. It was reported by Dutch visitors to Loango in the 1668 book \"Description of Africa\" as referring both to a material item and the spiritual entity that inhabits it. In the sixteenth century, when the Kingdom of Kongo was converted to Christianity, \"ukisi\" (a substance having characteristics of nkisi) was used to translate \"holy\" in the Kikongo Catechism of 1624.\n\nIn the eighteenth century, the \"mu-\" prefix evolved into a simple nasal \"n-\", so the modern spelling is properly n'kisi, but many orthographies spell it nkisi (there is no language-wide accepted orthography of Kikongo).\n\nClose communication with ancestors and belief in the efficacy of their powers are closely associated with minkisi in Kongo tradition. Among the peoples of the Congo Basin, especially the Bakongo and the Songye people of Kasai, exceptional human powers are frequently believed to result from some sort of communication with the dead. People known as banganga (singular: nganga) work as healers, diviners, and mediators who defend the living against black magic (witchcraft) and provide them with remedies against diseases resulting either from witchcraft or the demands of bakisi (spirits), emissaries from the land of the dead.\n\nBanganga harness the powers of bakisi and the dead by making minkisi. Minkisi are primarily containers - ceramic vessels, gourds, animal horns, shells, bundles, or any other object that can contain spiritually-charged substances. Even graves themselves, as the home of the dead and hence the home of bakisi, can be considered as minkisi. In fact, minkisi have even been described as portable graves, and many include earth or relics from the grave of a powerful individual as a prime ingredient. The powers of the dead thus infuse the object and allow the nganga to control it. The metal objects commonly pounded into the surface of the power figures represent the minkisis' active roles during ritual or ceremony. Each nail or metal piece represents a vow, a signed treaty, and efforts to abolish evil. Ultimately, these figures most commonly represent reflections upon socially unacceptable behaviors and efforts to correct them.\n\nThe substances chosen for inclusion in minkisi are frequently called \"\"bilongo\" or \"milongo\"\" (singular \"nlongo\") a word often translated as \"medicine.\" However, their operation is not primarily pharmaceutical, as they are not applied to or ingested by those who are sick, and perhaps \"bilongo\" is more accurately translated as \"therapeutic substances\". Rather they are frequently chosen for metaphoric reasons, for example, bird claws in order to catch wrongdoers, or because their names resemble characteristics of spirits in question.\n\nAmong the many common materials used in the minkisi were fruit (\"luyala\" in Kikongo), charcoal (\"kalazima\"), and mushrooms (\"tondo\"). Minerals were collected from various places associated with the dead, such as earth collected from graves and riverbeds. White clay was also very important in the composition of minkisi due to the symbolic relationship of the color white and the physical aspects of dead skin as well as their moral rightness and spiritual positivity. White contrasted with black, the color of negativity. Some minkisi use red ochre as a coloring agent. The use of red is symbolic of the mediation of the powers of the dead.\n\nMinkisi serve many purposes. Some are used in divination practices, rituals to eradicate evil or punish wrong-doers, and ceremonies for protective installments. Many are also used for healing, while others provide success in hunting or trade, among other things. Important minkisi are often credited with powers in multiple domains. Most famously, minkisi may also take the form of anthropomorphic or zoomorphic wooden carvings.\n\nMinkisi and the afflictions associated with them are generally classified into two types; the \"of the above\" and the \"of the below\". The above minkisi are associated with the sky, rain, and thunderstorms. The below minkisi are associated with the earth and waters on land. The above minkisi were considered masculine and were closely tied to violence and violent forces. The minkisi of the above were largely used to maintain order, serve justice, and seal treaties.\n\nBirds of prey, lightning, weapons, and fire are all common themes among the minkisi of the above. They also affected the upper body. Head, neck, and chest pains were said to be caused by these nkisi figures. Some figures were in the form of animals. Most often these were dogs (\"kozo\"). Dogs are closely tied to the spiritual world in Kongo mythology. They live in two separate worlds; the village of the living, and the forest of the dead. \"Kozo\" figures were often portrayed as having two heads – this was symbolic of their ability to see both worlds.\n\nNkondi (plural varies \"minkondi\", \"zinkondi\") are a subclass of minkisi that are considered aggressive. Because many of the nkondi collected in the nineteenth century were activated by having nails driven into them, they were often called \"nail fetishes\" in travel writing, museum catalogs, and art history literature. Many nkondi also feature reflective surfaces, such as mirrors, on their stomach areas or the eyes, which are held to be the means of vision in the spirit world. Although they can be made in many forms, the ones featuring a human statue with nails are the best described in anthropological and scholarly literature.\n\nNkondi are invoked to search out wrongdoing, enforce oaths, and cause or cure sicknesses. Perhaps the most common use was the locating and punishing of criminals, by hunting down wrongdoers and to avenging their crimes. An oath taker may declare him or herself vulnerable to the disease caused by an nkondi should he or she violate the oath. People who fall sick with diseases known to be associated with a particular nkondi may need to consult the nganga responsible for mediating with that spirit to determine how to be cured.\n\nAlthough nkisi nkondi have probably been made since at least the sixteenth century, the specifically nailed figures, which have been the object of collection in Western museums, nailed nkondi were probably made primarily in the northern part of the Kongo cultural zone in the nineteenth and early twentieth centuries.\n\nThe nkisi figures brought back to Europe in the nineteenth century caused great interest in stimulating emerging trends in modern art, and Bantu themes previously considered primitive or gruesome were now viewed as aesthetically interesting. The pieces became influential in art circles and many were acquired by art museums. The intentions of the banganga who created minkisi were practical, that is their characteristics were dictated by the need of the object to do the work it was required to do—hence the nails which caused a sensation were never seen as decorative items but as a requirement of awakening the spirit; or the gestures were part of a substantial metaphor of gestures found in Kongo culture.\n\nRecently some modern artists have also been interested in creating nkisi of their own, most notably Renee Stout, whose exhibition \"Astonishment and Power\" at the Smithsonian Institution coupled her own versions of nkisi with a commentary by noted anthropologist Wyatt MacGaffey.\n\n\n"}
{"id": "28038374", "url": "https://en.wikipedia.org/wiki?curid=28038374", "title": "Nurse call button", "text": "Nurse call button\n\nA nurse call button is a button or cord found in hospitals and nursing homes, at places where patients are at their most vulnerable, such as beside their bed and in the bathroom.. It allows patients in health care settings to alert a nurse or other health care staff member remotely of their need for help. When the button is pressed, a signal alerts staff at the nurse's station, and usually, a nurse or nurse assistant responds to such a call. Some systems also allow the patient to speak directly to the staffer; others simply beep or buzz at the station, requiring a staffer to actually visit the patient's room to determine the patient's needs.\n\nThe call button provides the following benefits to patients:\n\nThe call button can also be used by a health care staff member already with the patient to call for another when such assistance is needed, or by visitors to call for help on behalf of the patient.\n\nLaws in most places require that a call button must be in reach of the patient at all times for example in the patients bed or on the table. It is essential to patients in emergencies. There are also laws that vary by location setting the amount of time in which staff must respond to a call.\n\nIt is the responsibility of nursing staff to explain to the patients that they have a call button and to teach them how to use it.\n\nSome patients develop the habit of overusing a call button. This can lead staff to frustration, alarm fatigue, up to and including ignoring or disregarding the patient's calls or not taking them very seriously. \"Alarm fatigue\" refers to the response - or lack of it - of nurses to more than a dozen types of alarms that can sound hundreds of times a day - and many of those calls are false alarms. Staff cannot ignore such calls, as doing so violates the law in most places. Sometimes, mental health professionals will work with such patients in order to curtail their use of the button to serious need.\n\nThe most basic system has nothing more than a button for the patient. When the button is pressed, nursing staff is alerted by a light and/or an audible sound at the nurse's station. This can only be turned off from the patient's bedside, thereby compelling staff to respond to the patient.\n\nLike hardwired systems, wireless types have the ability to alert nursing staff by sound, light or show messages in a terminal. An advantage is that there is less wiring during installation and reducing the costs. The dome lights in the hallway still usually require wiring for power. Disadvantages of wireless systems include the requirement of batteries in each patient station that must be monitored and replaced over the life of the system, heightened risk of signal interference with other systems in the facility, and a limited selection among UL 1069 approved wireless systems.\n\nIn some facilities, often in hospitals, a more advanced system is included, in which staff from the nurse's station can communicate directly with patients via intercom. This has the advantage in which staff does not need to waste time walking to the patient's room to determine the reason the patient made the call, and they can determine by speaking to the patient whether the situation is urgent or if it can wait until later.\n\nWith the intercom system, the alert can be turned off from the nurse's station, allowing staff to avoid entry into the patient's room if it is determined that the patient's need can be met without doing so.\n\nNewer technology allows call buttons to reach cell phone-like devices carried around by nursing staff. Staffers can then answer the calls from wherever they are located within the facility, thereby improving the speed and efficiency in the response.\n\n"}
{"id": "2524990", "url": "https://en.wikipedia.org/wiki?curid=2524990", "title": "Nurse stereotypes", "text": "Nurse stereotypes\n\nA stereotype is a generalized idea or image about a particular person or thing that is often oversimplified and offensive. Stereotypes are victims of prejudice when portrayals of a group are untrue of individual members. Nursing has been stereotyped throughout the history of the profession. A common misconception is that all nurses are female; this has led to the stereotype of male nurses as effeminate. These generalized ideas of the nursing profession have formed a skewed image of nurses in the media. The image of a nurse projected by the media is typically of a young white single female being over-sexualized as well as diminished intellectually; this idea is then portrayed in get-well cards, television shows and novels. The over-sexualized nurse is commonly referred to as a naughty nurse and is shown as a sex symbol or nymphomaniac. Along with these common stereotypes, studies have identified several other popular images used in media such as handmaiden, angel, torturer, homosexual male, alcoholic, buffoon and woman in white. Common stereotypes of nursing and portrayal of these misconceptions have fueled a discussion on the effects they have on the profession, harmful or good.\n\nThe image of a nurse as a ministering angel was promoted in the 19th century as a counter to the then image of a nurse as a dissolute drunk, exemplified by Dickens' Sarah Gamp. The model nurse in this image was moral, noble and religious, like a devout nun—chaste and abstemious—rather than an unpleasant witch. Her skills would be practical and her demeanour would be stoic and obedient. Florence Nightingale promoted this image because, at the time, the idea of having female nurses attending the British army fighting the Crimean war was controversial, being thought immoral and revolutionary.\n\nThe media has a strong influence on public views, shaping the way the public values and treats professions in healthcare. In the book, \"Saving Lives: Why the Media's Portrayal Nurses Puts Us All At Risk\" the authors Sandy Summers and Harry Jacobs discuss the many ill effects of the common stereotypes and how those are presented in today's media. The authors argue that offensive stereotypes such as handmaidens as well as sexual stereotypes leads media to overlook how important nurses are in healthcare, which generates a lack of respect. This disrespect and ignorance puts lives at risk because it hinders the job of the nurse, which in most cases is to save lives. The media projections of nurses can not only damage the respect from patient to nurse, or colleague to nurse, but can also impact the pride of the individual nurse. This could potentially lead a nurse to believe that they truly are working for a physician rather than with them, it could also discourage a nurse from standing their ground or demanding respect.\n\n\n"}
{"id": "30625268", "url": "https://en.wikipedia.org/wiki?curid=30625268", "title": "Organización Médica Colegial de España", "text": "Organización Médica Colegial de España\n\nThe Spanish Medical Colleges Organization (\"Organización Médical Colegial\" or \"OMC\") is a Spanish organization whose purpose is to regulate the Spanish medical profession. The organization comprises the General Council of Official Medical Colleges (\"Consejo General de Colegios Oficiales de Médicos\" or \"CGCOM\") and the Spanish regional medical colleges. Its role is to represent all the registered doctors, ensuring proper standards and promoting an ethical medical practice.\n\nThe CGCOM is the governing body that coordinates and represents the 52 local medical colleges at national and international levels. Its role is to represent, organize and defend the medical profession.\n\nWithin the General Council, there is a Permanent Commission that is in charge of its administration and management. It manages any plans or projects agreed by the General Assembly and it also deals with administration and management issues of the General Council and its Human Resources department. Currently there are five members of the Permanent Commission.\n\nThe General Assembly is the highest power within the CGCOM and it is constituted by all 52 presidents of the local Medical Colleges, the members of the Permanent Commission, the national representatives of the College sections, representative members of the University, the scientific societies and other medical institutions. These medical institutions will join in following invitation by the assembly and will have a say but no right to vote.\n\nThe College sections are the different committees within each local College that get doctors together according to the modality of their medical practice. These committees are in charge of providing advice regarding their own specialties as well as organising suggestions or audits on the appropriate issues set up by nominated working groups.\n\nThe national representatives of the different college sections are as follows:\n\n\n\n\nThe OMC publishes books and journals that allow Spanish doctors to stay up to date with information and news on professional issues. It also publishes audits, documents and reports of general interest for the medical profession.\n\nIn 1875, King Alfonso XII signed a law that made it compulsory for doctors working in Madrid and other Royal Places to be part of a register. This eventually became the basic structure for the creation of the medical colleges. The first Spanish medical colleges were formally established in 1894.\n\nCompulsory registration for all doctors was debated between 1898 and 1900 but wasn’t established till 1917.\n\nFrom 1893 there was a growing need amongst the medical profession to create a professional association and with this in mind, the local colleges were founded. The general organization of Medical Colleges (Organización Médical Colegial or OMC) was created soon after.\n\nA school for orphans of medical parents (Colegio Principe de Asturias) was founded on May 15, 1917.\n\nIn October 1918, a first attempt to get the Heads of the Colleges to meet together had to be canceled due to the flu epidemic at the time; the unsuccessful date was chosen to coincide with a Medical National Meeting. It wasn’t until January 1919 that representatives from 33 local colleges finally met in the first ever general assembly. During that historic meeting they discussed issues such as pensions for the relatives of all those doctors deceased during the flu epidemic and also general issues about the medical profession. They created an executive committee based in Madrid and led by Dr Augusto Almazara and they agreed to meet again later on.\n\nThe Medical College of the city of Valencia requested an assembly of Spanish Medical Colleges and this was organized on 6 November 1920. On this date the Federation of Spanish Medical Colleges was founded creating a National Directory.\nDuring the next several years the Federation evolved to establish some formal rules when it came to sanction doctors or to settle disagreement between the administration and the Colleges.\n\nIn January 1930, a General Assembly took place in Barcelona and it was chaired by Dr Perez Mateos. During this meeting a project to study the country medical needs (Proyecto de Previsión Nacional) and the statute were signed off. At this point, the General Council of the Medical Colleges becomes the absolute leader to organize a link between regional Colleges, public administration, official organizations and the registered doctors. From this date it becomes the only accepted official institution that represents doctors in Spain.\n\nAt the very beginning of the Spanish Civil War, in July 1936, the Ministry of Work and Health dissolved the existing Colleges organization. By 1937, the Republic had constituted the Association of Medical Professionals and by August, the general Government had created a Permanent Commission that reported directly to the Health General Headquarters.\n\nAfter the war in 1940 and under the dictatorship of General Franco, new laws for the functioning of the General Council of the Medical Colleges were instituted. In 1946 a new set of internal rules was established including 22 deontological principles.\n\nThen in 1963 the rules of the Council were renewed introducing some fundamental changes. Under the new rules, the assembly was composed of the Presidents of each of the local colleges rather than by a representative group of their members. A voting system was introduced so that the registered doctors themselves could choose their representatives, who could then choose the members of the General Council. The term Organización Médica Colegial was adopted as the name of this organization. By 1967 the official state document announced that the OMC is a guild within public law and is fully independent from the Health Administration.\n\nThe arrival of democracy in Spain brought intense political changes. In 1980 some new medical statutes were approved; the main novelty being that the representatives of the sections within the colleges had their right to voice and vote, and furthermore, all registered doctors can take part in general elections to choose the members of the Council. It becomes compulsory for every College to have its own named ethics group.\n\nFurther changes were introduced during the following years (2006–2010) which have greatly changed the organization. The most remarkable ones have been the tasks and responsibilities of the Autonomy Groups, the incorporation of medical students to the organization, the incompatibility of jobs within the Permanent Commission and the time limit to the responsibility posts. The aim remains to update the General Council to the reality of the current medical profession.\n\nThe organization was responsible for several public awareness efforts during the 2009 flu pandemic in Spain.\n\n\n\n\n"}
{"id": "222300", "url": "https://en.wikipedia.org/wiki?curid=222300", "title": "Oxytocin", "text": "Oxytocin\n\nOxytocin (Oxt; ) is a peptide hormone and neuropeptide. Oxytocin is normally produced by the paraventricular nucleus of the hypothalamus and released by the posterior pituitary. It plays a role in social bonding, sexual reproduction, and during and after childbirth. Oxytocin is released into the bloodstream as a hormone in response to stretching of the cervix and uterus during labor and with stimulation of the nipples from breastfeeding. This helps with birth, bonding with the baby, and milk production. Oxytocin was discovered by Henry Dale in 1906. Its molecular structure was determined in 1952. Oxytocin is also used as a medication to facilitate childbirth.\n\nEstrogen has been found to increase the secretion of oxytocin and to increase the expression of its receptor, the oxytocin receptor, in the brain. In women, a single dose of estradiol has been found to be sufficient to increase circulating oxytocin concentrations.\n\nThe oxytocin peptide is synthesized as an inactive precursor protein from the \"OXT\" gene. This precursor protein also includes the oxytocin carrier protein neurophysin I. The inactive precursor protein is progressively hydrolyzed into smaller fragments (one of which is neurophysin I) via a series of enzymes. The last hydrolysis that releases the active oxytocin nonapeptide is catalyzed by peptidylglycine alpha-amidating monooxygenase (PAM).\n\nThe activity of the PAM enzyme system is dependent upon vitamin C (ascorbate), which is a necessary vitamin cofactor. By chance, sodium ascorbate by itself was found to stimulate the production of oxytocin from ovarian tissue over a range of concentrations in a dose-dependent manner. Many of the same tissues (\"e.g.\" ovaries, testes, eyes, adrenals, placenta, thymus, pancreas) where PAM (and oxytocin by default) is found are also known to store higher concentrations of vitamin C.\n\nOxytocin is known to be metabolized by the oxytocinase, leucyl/cystinyl aminopeptidase. Other oxytocinases are also known to exist. Amastatin, bestatin (ubenimex), leupeptin, and puromycin have been found to inhibit the enzymatic degradation of oxytocin, though they also inhibit the degradation of various other peptides, such as vasopressin, met-enkephalin, and dynorphin A.\n\nIn the hypothalamus, oxytocin is made in magnocellular neurosecretory cells of the supraoptic and paraventricular nuclei, and is stored in Herring bodies at the axon terminals in the posterior pituitary. It is then released into the blood from the posterior lobe (neurohypophysis) of the pituitary gland. These axons (likely, but dendrites have not been ruled out) have collaterals that innervate neurons in the nucleus accumbens, a brain structure where oxytocin receptors are expressed. The endocrine effects of hormonal oxytocin and the cognitive or behavioral effects of oxytocin neuropeptides are thought to be coordinated through its common release through these collaterals. Oxytocin is also produced by some neurons in the paraventricular nucleus that project to other parts of the brain and to the spinal cord. Depending on the species, oxytocin receptor-expressing cells are located in other areas, including the amygdala and bed nucleus of the stria terminalis.\n\nIn the pituitary gland, oxytocin is packaged in large, dense-core vesicles, where it is bound to neurophysin I as shown in the inset of the figure; neurophysin is a large peptide fragment of the larger precursor protein molecule from which oxytocin is derived by enzymatic cleavage.\n\nSecretion of oxytocin from the neurosecretory nerve endings is regulated by the electrical activity of the oxytocin cells in the hypothalamus. These cells generate action potentials that propagate down axons to the nerve endings in the pituitary; the endings contain large numbers of oxytocin-containing vesicles, which are released by exocytosis when the nerve terminals are depolarised.\n\nEndogenous oxytocin concentrations in the brain have been found to be as much as 1000-fold higher than peripheral levels.\n\nOutside the brain, oxytocin-containing cells have been identified in several diverse tissues, including in females in the corpus luteum and the placenta; in males in the testicles' interstitial cells of Leydig; and in both sexes in the retina, the adrenal medulla, the thymus and the pancreas. The finding of significant amounts of this classically \"neurohypophysial\" hormone outside the central nervous system raises many questions regarding its possible importance in these different tissues.\n\nThe Leydig cells in some species have been shown to possess the biosynthetic machinery to manufacture testicular oxytocin \"de novo\", to be specific, in rats (which can synthesize vitamin C endogenously), and in guinea pigs, which, like humans, require an exogenous source of vitamin C (ascorbate) in their diets.\n\nOxytocin is synthesized by corpora lutea of several species, including ruminants and primates. Along with estrogen, it is involved in inducing the endometrial synthesis of prostaglandin F to cause regression of the corpus luteum.\n\nVirtually all vertebrates have an oxytocin-like nonapeptide hormone that supports reproductive functions and a vasopressin-like nonapeptide hormone involved in water regulation. The two genes are usually located close to each other (less than 15,000 bases apart) on the same chromosome, and are transcribed in opposite directions (however, in fugu, the homologs are further apart and transcribed in the same direction).\n\nThe two genes are believed to result from a gene duplication event; the ancestral gene is estimated to be about 500 million years old and is found in cyclostomata (modern members of the Agnatha).\n\nOxytocin has peripheral (hormonal) actions, and also has actions in the brain. Its actions are mediated by specific, oxytocin receptors. The oxytocin receptor is a G-protein-coupled receptor that requires magnesium and cholesterol. It belongs to the rhodopsin-type (class I) group of G-protein-coupled receptors.\n\nStudies have looked at oxytocin's role in various behaviors, including orgasm, social recognition, pair bonding, anxiety, and maternal behaviors.\n\nThe peripheral actions of oxytocin mainly reflect secretion from the pituitary gland. The behavioral effects of oxytocin are thought to reflect release from centrally projecting oxytocin neurons, different from those that project to the pituitary gland, or that are collaterals from them. Oxytocin receptors are expressed by neurons in many parts of the brain and spinal cord, including the amygdala, ventromedial hypothalamus, septum, nucleus accumbens, and brainstem.\n\n\n\nIn the prairie vole, oxytocin released into the brain of the female during sexual activity is important for forming a pair bond with her sexual partner. Vasopressin appears to have a similar effect in males. Oxytocin has a role in social behaviors in many species, so it likely also does in humans. In a 2003 study, both humans and dog oxytocin levels in the blood rose after five to 24 minutes of a petting session. This possibly plays a role in the emotional bonding between humans and dogs.\n\nOxytocin is not only correlated with the preferences of individuals to associate with members of their own group, but it is also evident during conflicts between members of different groups. During conflict, individuals receiving nasally administered oxytocin demonstrate more frequent defense-motivated responses toward in-group members than out-group members. Further, oxytocin was correlated with participant desire to protect vulnerable in-group members, despite that individual's attachment to the conflict. Similarly, it has been demonstrated that when oxytocin is administered, individuals alter their subjective preferences in order to align with in-group ideals over out-group ideals. These studies demonstrate that oxytocin is associated with intergroup dynamics. Further, oxytocin influences the responses of individuals in a particular group to those of another group. The in-group bias is evident in smaller groups; however, it can also be extended to groups as large as one's entire country leading toward a tendency of strong national zeal. A study done in the Netherlands showed that oxytocin increased the in-group favoritism of their nation while decreasing acceptance of members of other ethnicities and foreigners. People also show more affection for their country's flag while remaining indifferent to other cultural objects when exposed to oxytocin. It has thus been hypothesized that this hormone may be a factor in xenophobic tendencies secondary to this effect. Thus, oxytocin appears to affect individuals at an international level where the in-group becomes a specific \"home\" country and the out-group grows to include all other countries.\n\n\nOxytocin is typically remembered for the effect it has on prosocial behaviors, such as its role in facilitating trust and attachment between individuals. Consequently, oxytocin is often referred to as the “love hormone\". However, oxytocin has a more complex role than solely enhancing prosocial behaviors. There is consensus that oxytocin modulates fear and anxiety; that is, it does not directly elicit fear or anxiety. Two dominant theories explain the role of oxytocin in fear and anxiety. One theory states that oxytocin increases approach/avoidance to certain social stimuli and the second theory states that oxytocin increases the salience of certain social stimuli, causing the animal or human to pay closer attention to socially relevant stimuli.\n\nNasally administered oxytocin has been reported to reduce fear, possibly by inhibiting the amygdala (which is thought to be responsible for fear responses). Indeed, studies in rodents have shown oxytocin can efficiently inhibit fear responses by activating an inhibitory circuit within the amygdala. Some researchers have argued oxytocin has a general enhancing effect on all social emotions, since intranasal administration of oxytocin also increases envy and \"Schadenfreude\". Individuals who receive an intranasal dose of oxytocin identify facial expressions of disgust more quickly than individuals who do not receive oxytocin. Facial expressions of disgust are evolutionarily linked to the idea of contagion. Thus, oxytocin increases the salience of cues that imply contamination, which leads to a faster response because these cues are especially relevant for survival. In another study, after administration of oxytocin, individuals displayed an enhanced ability to recognize expressions of fear compared to the individuals who received the placebo. Oxytocin modulates fear responses by enhancing the maintenance of social memories. Rats that are genetically modified to have a surplus of oxytocin receptors display a greater fear response to a previously conditioned stressor. Oxytocin enhances the aversive social memory, leading the rat to display a greater fear response when the aversive stimulus is encountered again.\n\nOxytocin produces antidepressant-like effects in animal models of depression, and a deficit of it may be involved in the pathophysiology of depression in humans. The antidepressant-like effects of oxytocin are not blocked by a selective antagonist of the oxytocin receptor, suggesting that these effects are not mediated by the oxytocin receptor. In accordance, unlike oxytocin, the selective non-peptide oxytocin receptor agonist WAY-267,464 does not produce antidepressant-like effects, at least in the tail suspension test. In contrast to WAY-267,464, carbetocin, a close analogue of oxytocin and peptide oxytocin receptor agonist, notably does produce antidepressant-like effects in animals. As such, the antidepressant-like effects of oxytocin may be mediated by modulation of a different target, perhaps the vasopressin V receptor where oxytocin is known to weakly bind as an agonist.\n\nSildenafil has been found to enhance electrically evoked oxytocin release from the pituitary gland. In accordance, the drug shows oxytocin-dependent antidepressant-like effects in animals, and it has proposed that sildenafil may hold promise as a potential antidepressant in humans.\n\nIt has been shown that oxytocin differentially affects males and females. Females who are administered oxytocin are overall faster in responding to socially relevant stimuli than males who received oxytocin. Additionally, after the administration of oxytocin, females show increased amygdala activity in response to threatening scenes; however, males do not show increased amygdala activation. This phenomenon can be explained by looking at the role of gonadal hormones, specifically estrogen, which modulate the enhanced threat processing seen in females. Estrogen has been shown to stimulate the release of oxytocin from the hypothalamus and promote receptor binding in the amygdala.\n\nIt has also been shown that testosterone directly suppresses oxytocin in mice. This has been hypothesized to have evolutionary significance. With oxytocin suppressed, activities such as hunting and attacking invaders would be less mentally difficult as oxytocin is strongly associated with empathy.\n\n\nOxytocin is a peptide of nine amino acids (a nonapeptide) in the sequence cysteine-tyrosine-isoleucine-glutamine-asparagine-cysteine-proline-leucine-glycine-amide (Cys – Tyr – Ile – Gln – Asn – Cys – Pro – Leu – Gly – NH, or CYIQNCPLG-NH); its \"C\"-terminus has been converted to a primary amide and a disulfide bridge joins the cysteine moieties. Oxytocin has a molecular mass of 1007 Da, and one international unit (IU) of oxytocin is the equivalent of about 2 μg of pure peptide.\n\nWhile the structure of oxytocin is highly conserved in placental mammals, a novel structure of oxytocin was recently reported in marmosets, tamarins, and other new world primates. Genomic sequencing of the gene for oxytocin revealed a single in-frame mutation (thymine for cytosine) which results in a single amino acid substitution at the 8-position (proline for leucine). Since this original Lee \"et al.\" paper, two other laboratories have confirmed Pro8-OT and documented additional oxytocin structural variants in this primate taxon. Vargas-Pinilla \"et al.\" sequenced the coding regions of the OXT gene in other genera in new world primates and identified the following variants in addition to Leu8- and Pro8-OT: Ala8-OT, Thr8-OT, and Val3/Pro8-OT. Ren \"et al.\" identified a variant further, Phe2-OT in howler monkeys.\n\nThe biologically active form of oxytocin, commonly measured by RIA and/or HPLC techniques, is also known as the octapeptide \"oxytocin disulfide\" (oxidized form), but oxytocin also exists as a reduced straight-chain (non-cyclic) dithiol nonapeptide called oxytoceine. It has been theorized that oxytoceine may act as a free radical scavenger, as donating an electron to a free radical allows oxytoceine to be re-oxidized to oxytocin via the dehydroascorbate / ascorbate redox couple.\n\nThe structure of oxytocin is very similar to that of vasopressin. Both are nonapeptides with a single disulfide bridge, differing only by two substitutions in the amino acid sequence (differences from oxytocin bolded for clarity): Cys – Tyr – Phe – Gln – Asn – Cys – Pro – Arg – Gly – NH. A table showing the sequences of members of the vasopressin/oxytocin superfamily and the species expressing them is present in the vasopressin article. Oxytocin and vasopressin were isolated and their total synthesis reported in 1954, work for which Vincent du Vigneaud was awarded the 1955 Nobel Prize in Chemistry with the citation: \"for his work on biochemically important sulphur compounds, especially for the first synthesis of a polypeptide hormone.\"\n\nOxytocin and vasopressin are the only known hormones released by the human posterior pituitary gland to act at a distance. However, oxytocin neurons make other peptides, including corticotropin-releasing hormone and dynorphin, for example, that act locally. The magnocellular neurosecretory cells that make oxytocin are adjacent to magnocellular neurosecretory cells that make vasopressin. These are large neuroendocrine neurons which are excitable and can generate action potentials.\n\nThe uterine-contracting properties of the principle that would later be named oxytocin were discovered by British pharmacologist Sir Henry Hallett Dale in 1906, and its milk ejection property was described by Ott and Scott in 1910 and by Schafer and Mackenzie in 1911. In the 1920s, oxytocin and vasopressin were isolated from pituitary tissue and given their current names. The word \"oxytocin\" was coined from the term \"oxytocic\", Greek ὀξύς, \"oxys\", and τοκετός , \"toketos\", meaning \"quick birth\".\n\nOxytocin became the first polypeptide hormone to be sequenced or synthesized. Du Vigneaud was awarded the Nobel Prize in 1955 for his work.\n\n"}
{"id": "4900362", "url": "https://en.wikipedia.org/wiki?curid=4900362", "title": "Patrick Russell (herpetologist)", "text": "Patrick Russell (herpetologist)\n\nPatrick Russell (6 February 1726, Edinburgh – 2 July 1805, London) was a Scottish surgeon and naturalist who worked in India. He studied the snakes of India and is considered the \"Father of Indian Ophiology\". Russell's viper, \"Daboia russelii\", is named after him.\n\nThe fifth son of John Russell, a well-known lawyer of Edinburgh, and his third wife Mary, Patrick was the half-brother of Alexander Russell, FRS and William Russell, FRS. Patrick studied Roman and Greek classics at Edinburgh high school after which he studied medicine at the University under Alexander Monro. He graduated as a Doctor of Medicine in 1750 and joined his half-brother, Alexander Russell, who was 12 years senior in Aleppo, Syria. In 1740 Alexader had been made a Physician to the Levant Company's Factory. Alexander was involved in quarantine and disease control and was a keen naturalist with a knowledge of local languages and a close friend of the Pasha.\nIn 1753, Alexander resigned, returning to London and publishing a \"Natural History of Aleppo and Parts Adjacent\" in 1756. Patrick took up the position left by Alexander and worked for about 18 years. The Pasha of Aleppo held him in high regard, even honouring him with a turban. A keen observer of traditions, he noted in a letter read by Alexander to the Royal Society an Arabian practice of inoculating children against smallpox using \"variolus matter\". Several outbreaks of bubonic plague occurred in Aleppo in 1760, 1761 and 1762. He studied the conditions of those who were infected and identified procedures to avoid infection such as breathing through a handkerchief soaked in vinegar. He continued to maintain notes on natural history and after Alexander died in 1768, he revised the \"Natural History of Aleppo\" in 1794. He noticed that fleas tended to reduce in numbers after the hottest weather, a climate he noticed also led to a decline in the number of plague cases.\n\nIn 1771 he left Aleppo and travelled through Italy, examining the methods used to reduce the spread of diseases. Initially intending to set up practice in Edinburgh, he was persuaded by Dr John Fothergill, to move instead to London. Dr Fothergill was a friend of Alexander, an eminent physician and the founder of a botanical garden. While in London, Patrick was introduced to Sir Joseph Banks and Daniel Solander who examined his collections from Aleppo. In 1777, Patrick was elected a Fellow of the Royal Society.\n\nIn 1781, a younger brother, Claud became a chief administrator of the East India Company at Visakhapatnam in Madras Province. Claud however suffered poor health and the family insisted that Patrick attend to him. Arriving in India, he began to study the natural history of the region. The naturalist to the East India Company in the Carnatic was Dr John Koenig, student of Carolus Linnaeus and when he died in 1785, the Governor of Madras personally offered the post of 'Botanist and Naturalist' to Patrick. This post, according to Ray Desmond (1992, \"European Discovery of Indian Flora\") was:\n\nRussell wrote about the plant and animal life of Madras as he had of Aleppo. As a physician as well as a naturalist to the East India Company in the Carnatic he was concerned with the problem of snakebite and made it his aim to find a way for people to identify venomous snakes. He also made a large collection of plants. One of the snakes he identified was \"Katuka Rekula Poda\" which he noted was not well known to Europeans but was second only to the cobra in its lethality. Russell attempted to classify the snakes using the nature of scales but his quest was to find an easy way to separate the venomous snakes from the non-venomous. He conducted envenomation experiments on dogs and chicken and described the symptoms. He tested remedies claimed for snakebite including a pill from Tanjore which was very popular and found that it did not work. In one case a soldier in torpor was brought to him and the common treatment used by Europeans was tested. Two bottles of warm Madeira wine was forcibly poured into the patient's mouth, who then completely recovered. Patrick, his brother Claud and the family left for England in January 1791. Some of the collections he made were placed in the museum at Madras although he took back some snake skins that are now in the collection of the Natural History Museum at London. Returning to England, he worked on the book on snakes, which was to be published by the East India Company. The first volume of his \"An Account of Indian Serpents Collected on the Coast of Coromandel\" was published in 1796 with 44 plates. The second volume appeared in four parts, the first two of which were published in 1801 and 1802. These included 46 coloured plates. Patrick Russell died on 2 July 1805, three days after an illness. He was never married. The third and fourth parts of the second volume of his book was published after his death in 1807 and 1809. Two scientific papers were read on the pits of the pit viper \"Trimeresurus\" which he demonstrated as not being associated with hearing. Another paper demonstrated the voluntary mechanism by which the cobra spread its hood.\n\n"}
{"id": "39222299", "url": "https://en.wikipedia.org/wiki?curid=39222299", "title": "País do Desejo", "text": "País do Desejo\n\nPaís do Desejo is a 2012 Brazilian-Portuguese drama film directed by Paulo Caldas. \nThe film premiered at the 35th São Paulo International Film Festival.\n\nThe film is based in the case of a girl from Alagoinha, in Pernambuco. In 2009, the girl became pregnant after years of sexual abuse committed by the stepfather. As the pregnancy was the result of a rape and represented a risk to the life of the girl, she did an abortion, as is provided for in the Brazilian legislation.\n\nThe story is told from the center of the crisis of a Catholic priest that is positioned against the decision of the Archbishop of the Church to excommunicate the mother, the girl and the doctors involved in the abortion.\n\nThe film tells the story of Roberta, a famous classical pianist who faces a tough battle against a severe kidney disease, and José, a somewhat unconventional priest, which supports a 12-year-old girl raped by her uncle and pregnant with twins, to have an abortion.\n\nThe fate of the two characters crosses when Roberta faints at a concert and is hospitalized in a local clinic, belonging to the brother of the priest. Gradually, he is interested by the pianist, and that love will change their destiny in many ways.\n\n\n"}
{"id": "52640823", "url": "https://en.wikipedia.org/wiki?curid=52640823", "title": "Project Pink Blue", "text": "Project Pink Blue\n\nProject Pink Blue, registered as Health & Psychological Trust Centre is a cancer nonprofit engaged in raising cancer awareness, patient navigation, advocacy and free breast and cervical cancer screening for women living in poverty. The organization launched Nigeria's first patient navigation in 2015 and a toll- free telephone centre 08000CANCER in 2016 Project PINK BLUE won the SPARC Metastatic Breast Cancer challenge grant by Union for International Cancer Control and Pfizer Oncology in Lisbon, Portugal.\n\nProject Pink Blue started in 2013 as a National Youth Service Corps (NYSC) community development project, known as Project Pink through which Runcie C. W. Chidebe, carried out a breast and cervical cancer awareness and screening for 165 women in Kabusa community, a suburb of Abuja. In 2014, the organization included prostate cancer on her mandate and became known as Project Pink Blue, and was incorporated with Corporate Affairs Commission(CAC) of Nigeria.\n\nThe organization has frequently used public figures and celebrities like Korede Bello, Chidinma Ekile, Oge Okoye, Annie Idibia, and others to bring the message about early dictation and treatment to limelight.\n\nThe organization has regularly organized World Cancer Day, International Breast Awareness Day popularly known as Pink October, Community outreaches, Global Day of Citizens Action, Cancer Research. On 29 & 30 September, Project Pink Blue hosted \nIn January 2015, Project Pink Blue sets up the first Nigerian Patients Navigation programme known as Breast Cancer Navigation and Palliative Programme (BCNPP).\n\nProject Pink Blue organizes world cancer day every February for the past four years. The event featured road walk, free cancer screening and symposium with dignitaries such as a veteran Nigerian broadcaster Sadiq Daba\n\nOn 2 December 2016, the organization launched the first Nigerian cancer toll-free line known as 08000CANCER or 08000226237\nProject PINK BLUE chose the occasion of World Cancer Day to launch the Abuja Breast Cancer Support Group - the city's first patient support group. The milestone achievement was commemorated with a medal-giving ceremony for breast cancer survivors attended by leaders from the cancer control community as well as His Royal Majesty\nDr. Kabiru Musa, the Emir of Azara .\n\nProject PINK BLUE launched the Upgrade Oncology programme during the 2018 World Cancer Day activities . The Upgrade Oncology is a capacity development program focused on improving cancer treatment and care through the provision of medical oncology training, update/top-up trainings, development of medical oncology curriculum, review and domestication of the treatment guidelines for better cancer care in Nigeria.\n\nProject PINK BLUE also used the Day to call on the Federal government to set up a national agency for cancer control, urging decision- makers to make cancer control a national health priority.\n\nProject Pink Blue has directly impacted 1,235 women and men through screening and enlightened over a million people in Africa through Radio, Television and Social Media campaign. Besides awareness campaigns, the group supports cancer patients through fundraising. It was through one such fundraising event that the NGO raised around 5.5m naira (£14,000) in cash and 9m naira in drug support to enable Comfort Oyayi Daniel to have her chemotherapy when her savings ran out. In November 2017, the nonprofit led some breast cancer patients and survivors to Nigeria's National Health Insurance Scheme (NHIS) to advocate for the expansion of insurance coverage for cancer drugs and services.\n\n\nThe organization has partnered with:\n\nProject Pink Blue is a member of Union for International Cancer Control, Geneva, Switzerland; partner to Federal Ministry of Health in Nigeria; voting member, CIVICUS World Alliance for Citizen Participation, Johannesburg, South Africa; Member of the World Health Organization's Partnership for Maternal, Newborn & Child Health\n\n"}
{"id": "3365974", "url": "https://en.wikipedia.org/wiki?curid=3365974", "title": "Rho(D) immune globulin", "text": "Rho(D) immune globulin\n\nRh(D) immune globulin (RhIG) is a medication used to prevent Rh isoimmunization in mothers who are Rh negative and to treat idiopathic thrombocytopenic purpura (ITP) in people who are Rh positive. It is often given both during and following pregnancy. It may also be used when Rh negative people are given Rh positive blood. It is given by injection into muscle or a vein. A single dose lasts 2 to 4 weeks.\nCommon side effects include fever, headache, pain at the site of injection, and red blood cell breakdown. Other side effects include allergic reactions, kidney problems, and a very small risk of viral infections. In those with ITP, the amount of red blood cell breakdown may be significant. Use is safe with breastfeeding. Rho(D) immune globulin is made up of antibodies to the antigen Rh(D) present on some red blood cells. It is believed to work by blocking a person's immune system from recognizing this antigen.\nRh(D) immune globulin came into medical use in the 1960s. It is on the World Health Organization's List of Essential Medicines, the most effective and safe medicines needed in a health system. In the United Kingdom, a 1,500-unit (300-mcg) vial costs the NHS about 58 pounds. In the United States, a course of treatment costs more than $200. It is made from human blood plasma.\n\nIn a pregnancy where the mother is Rho(D)-negative and the father is Rho(D)-positive, the probability of the fetus having Rho(D)-positive blood is dependent on whether the father is homozygous for Rho(D)-positive (i.e., both Rho(D) alleles are positive) or heterozygous (i.e., one Rho(D) allele is positive and the other negative). If the father is homozygous, the fetus will necessarily be Rho(D)-positive, as the father will necessarily pass on a Rho(D)-positive allele. If the father is heterozygous, there is a 50% chance that the fetus will be Rho(D)-positive, as he will randomly pass on either the Rho(D)-positive allele or the Rho(D)-negative allele.\n\nIf a fetus is Rho(D)-positive and the mother is Rho(D)-negative, the mother is at risk of Rho(D) alloimmunization, where the mother mounts an immune response (develops antibodies) to fetal red blood cells. This usually has minimal effect on the first such pregnancy; but, in a second such pregnancy, pre-existing maternal antibodies to Rho(D) antigens on fetal red blood cells often leads to erythroblastosis fetalis, a condition which can be fatal to the fetus. In countries without RhIG protocols, as many as 14% of affected fetuses are stillborn and 50% of live births result in neonatal death or brain injury.\n\nBecause of this severe complication, the American College of Obstetricians and Gynecologists (ACOG) recommends that all Rho(D)-negative mothers, regardless of fetal blood type, receive RhIG at about 28 weeks gestation, and again shortly after delivery. It should be given within 3 days of a potential exposure to Rh positive blood from the baby such as may occur during miscarriage, trauma, or delivery. The '28 weeks' recommendation comes from the fact that 92% of women who develop an anti-D during pregnancy do so at or after 28 weeks gestation. It is given by intramuscular injection as part of modern routine antenatal care. Despite excellent results, the medication retains an FDA Pregnancy Category C.\n\nRhIG is recommended in the UK after antenatal pathological events that are likely to cause a feto–maternal hemorrhage. Applicable 'pathologic events' include accidents which may induce fetomaternal hemorrhage (motor vehicle accidents, falls, abdominal trauma), following obstetric/gynecologic procedures during pregnancy, and at the time of threatened- or spontaneous-/elective abortions, regardless of gestational age.\n\nThere is insufficient evidence that the use of Rho(D) immune globulin after a spontaneous miscarriage is needed and a Cochrane review recommends that local practices be followed.\n\nIn a Rhesus-negative mother, Rh(D) immune globulin can prevent temporary sensitization of the maternal immune system to Rh D antigens, which can cause rhesus disease in the current or in subsequent pregnancies. With the widespread use of Rho(D) immune globulin, Rh disease of the fetus and newborn has almost disappeared in the developed world. The risk that a D-negative mother can be alloimmunized by a D-positive fetus can be reduced from approximately 16% to less than 0.1% by the appropriate administration of RhIG.\n\nRho(D) immune globulin is composed of IgG antibodies and therefore is able to cross the placenta. In rare cases this can cause a baby to have a weakly positive DAT (direct antiglobulin test) due to sensitization of fetal cells from mothers who have received multiple doses of Rho(D) immune globulin. However, no treatment is necessary as the clinical course is benign.\n\nA D-negative mother who is not alloimmunized to D should also receive an appropriate dose of RhIG after delivery of a D-positive infant. After delivery, a cord blood sample from infants born to D-negative mothers should be tested for the D antigen. If the neonate is D-negative, no further RhIG is needed. However, if the infant is D-positive, the mother should have a postpartum blood sample screened for fetomaternal hemorrhage in order to determine the appropriate dosage of RhIG to be administered. (the presence of residual anti-D from antepartum RhIG administration does NOT indicate ongoing protection from alloimmunization- repeat administration of RhIG is necessary).\n\nThe rosette test is a sensitive method to detect fetomaternal hemorrhage of 10 cc or more. A rosette test will be positive if fetal D-positive cells are present in the maternal sample, indicating a significantly large fetomaternal hemorrhage has occurred. A rosette test may be falsely positive if the mother is positive for the weak D phenotype and falsely negative if the neonate is weak D. If the rosette test is negative, then a dose of 300 micrograms of RhIG is given (sufficient to prevent alloimmunization after delivery in 99% of cases). The RhIG dose suppresses the immune response to up to 30 cc of whole fetal blood.\n\nIf a fetomaternal hemorrhage in excess of 30 cc has occurred, additional testing is mandatory in order to determine the appropriate dosage of RhIG to prevent alloimmunization. A positive rosette test should be followed by a quantitative test such as the Kleihauer-Betke test (acid/elution) or an alternative approach such as flow cytometry. See article on Kleihauer-Betke test for details on how the volume of fetomaternal hemorrhage is calculated.\n\nThe dosage of RhIG is calculated from the volume of fetal hemorrhage (in mL). Ex: 50 mL fetal hemorrhage / 30 ml = 1.667 (round up to 2) then add 1 = 3 vials of RhIG.\n\nPostpartum RhIG should be administered within 72 hours of delivery. If prophylaxis is delayed, the likelihood that alloimmunization will be prevented is decreased. However, ACOG still recommends that RhIG be administered because partial protection still occurs. If the D-type of a newborn or stillborn is unknown or cannot be determined, RhIG should be administered.\n\nPrimary immune thrombocytopenia (ITP) is an acquired immune-mediated disorder characterized by isolated thrombocytopenia, defined as a peripheral blood platelet count less than 100 x 10/L, and the absence of any obvious initiating and/or underlying cause of the thrombocytopenia. Symptoms of ITP include abnormal bleeding and bruising due to the reduction in platelet count. Rh(D) Immune Globulin Intravenous [Human; Anti-D] is indicated for use in non-splenectomized, Rh(D)-positive children with chronic or acute ITP, adults with chronic ITP, and children and adults with ITP secondary to HIV infection. Anti-D must be administered via the intravenous route when used in clinical situations requiring an increase in platelet count. The mechanism of action of anti-D is not fully understood; however, after administration the anti-D coated red blood cell complexes saturate Fcγ receptors sites on macrophages, resulting in preferential destruction of red blood cells (RBCs), therefore sparing antibody-coated platelets. Anti-D is recommended as a first-line therapy for ITP, along with corticosteroids and intravenous immune globulin (IVIG). WinRho SDF is an anti-D manufactured, distributed and marketed by Cangene Corporation in the US.\n\nThe following females are not candidates for RhIG:\n\nThe first Rho(D) immune globulin treatment \"skymed\" was introduced by Ortho-Clinical Diagnostics, a subsidiary holding of Jskymed, and was first administered on May 29, 1968 to Marianne Cummins in Teaneck, NJ.\n\nIn 1996 ZLB Bioplasma (part of CSL Behring) was given approval to sell Rhophylac in Europe. Effectiveness was demonstrated in a clinical trial in 2003 and in 2004 Rhophylac was approved in the United States.\n\nRho(D) immune globulin is a derivative of human plasma. The most common way anti-D products are manufactured is by a form of the Cohn cold ethanol fractionation method developed in the 1950s. Variations of the Cohn method developed in the 1950s may not completely clear aggregates of immunoglobulins, which can cause problems for patients if administered intravenously, and is a primary reason why most anti-Ds are for intramuscular use only. A non-Cohn manufacturing variation is ChromaPlus process approved by the U.S. Food and Drug Administration (FDA) that is used to make Rhophylac. Rho(D) immune globulin may trigger an allergic reaction. Steps are taken in the plasma-donor screening process and the manufacturing process to eliminate bacterial and viral contamination, although a small, residual risk may remain for contamination with small viruses. There is also a theoretical possibility of transmission of the prion responsible for Creutzfeldt–Jakob disease, or of other, unknown infectious agents.\n\nRhIG can be administered either by either intramuscular (IM) or intravenous (IV) injection, depending on the preparation. The IM-only preparation should never be administered IV due to the risk of complement system activation. Multiple IM doses should be given at different sites or at different times within the 72-hour window. Or, multiple IV doses can be administered according to the instructions in the package insert.\n\nRh(D) immune globulin is also spelled Rh(D) immune globulin (letter o and digit zero are both widely attested; more at Rh blood group system - Rh nomenclature).\n\nRhophylac is manufactured by CSL Limited. RhoGAM and MICRhoGam are brand names of Kedrion Biopharma. Other brand names are BayRHo-D, Gamulin Rh, HypRho-D Mini-Dose, Mini-Gamulin Rh, Partobulin SDF (Baxter), Rhesonativ (Octapharma), and RhesuGam (NBI). KamRho-D I.M. is a brand name of Kamada Ltd.\n\nThe United States distribution rights for WinRho SDF (another brand name) were transferred from Baxter to the manufacturer, Cangene, in 2010; they had been held by Baxter since 2005. Sales of WinRho fell every year under the agreement with Baxter, the supposition being that Baxter was favoring the sale of its own product over WinRho; according to one analyst, \"WinRho was always an afterthought for a big company like Baxter.\"\n\n\n"}
{"id": "24962515", "url": "https://en.wikipedia.org/wiki?curid=24962515", "title": "Ruiz v. Estelle", "text": "Ruiz v. Estelle\n\nRuiz v. Estelle, 503 F. Supp. 1265 (S.D. Tex. 1980), filed in United States District Court for the Southern District of Texas, eventually became the most far-reaching lawsuit on the conditions of prison incarceration in American history. \n\nIt began as a civil action, a handwritten petition filed against the Texas Department of Corrections (TDC) in 1972 by inmate David Resendez Ruíz alleging that the conditions of his incarceration, such as overcrowding, lack of access to health care, and abusive security practices, were a violation of his constitutional rights. In 1974, the petition was joined by seven other inmates and became a class action suit known as \"Ruiz v. Estelle\", 550 F.2d 238. The trial ended in 1979 with the ruling that the conditions of imprisonment within the TDC prison system constituted cruel and unusual punishment in violation of the United States Constitution, with the original report issued in 1980, a 118-page decision by Judge William Justice (\"Ruiz v. Estelle\", 503 F.Supp. 1295). \n\nThe decision led to federal oversight of the system, with a prison construction boom and \"sweeping reforms ... that fundamentally changed how Texas prisons operated.\"\n\nDavid Resendez Ruíz was a Mexican-American from East Austin, Texas.\n\nThe son of migrant farmworkers and the youngest of 13 children, he got into trouble with the law from an early age; as a child he was arrested for fighting and shoplifting. After an arrest for a car theft, 12-year-old Ruíz received a sentence to serve time in Gatesville State School in Gatesville; he arrived for his first session in 1954. In Gatesville he socialized with \"hard core\" state school students from Austin and San Antonio. Ruíz had four sessions in Gatesville. After Ruíz left Gatesville for the final time, he turned 17, which made him an adult in the Texas penal system.\n\nAfter another car theft, he was sentenced to serve time in the Texas Department of Corrections (TDC). He initially was placed in Huntsville; two weeks later he was assigned to the Ramsey Farm in Brazoria County, Texas, where he worked in fields. In Ramsey, Ruíz attempted to kill an inmate that he believed was planning to have Ruíz killed; the stabbing injured but did not kill the prisoner. The prison authorities beat Ruíz as a punishment. During his confinement in Ramsey, Ruíz had also committed lesser infractions. His first adult sentence lasted seven years. After he left prison, Ruíz married a woman named Rose Marie and the two had a daughter together.\n\nThirteen months after his release, on July 1968, Ruíz was again placed in the custody of the TDC; he said that he had \"picked up the gun\" because he had no education or trade skills to support himself and his family. He was then assigned to the Eastham Unit in Houston County, where he continued to work in fields. While at Eastham, Ruíz participated in a failed escape attempt. The warden of Eastham and George Beto, the TDC director, escorted Ruíz back to prison. \n\nAfter a week in the infirmary, Ruíz was placed in solitary confinement for 45 days; there he decided to become a prison activist. After Ruíz left solitary confinement, he refused to work in the fields any longer and cut his achilles tendon with a razor. Because of the self-inflicted injuries, Ruíz was no longer required to work, and he was sent to various correctional and medical facilities. Ruíz had committed many disciplinary infractions, including the stabbing, the escape attempt, and the refusal to work, so he was sent to the Wynne Unit, where he met Fred Cruz, a prisoner who filed successful lawsuits against the prison system. At the Wynne Unit, Ruíz, Cruz, and other prisoners worked together to file lawsuits against TDC.\n\nThere followed decades of further litigation in the form of consent decrees, appeals and other legal actions, until a final judgment was rendered in 1992. But problems in enforcement continued, and in 1996 U.S. Congress enacted the Prison Litigation Reform Act (PLRA) to address these issues as well as abuse of the prison litigation process.\n\nHowever, in October 1997, the district court, still not satisfied with the compliance of the TDC, gave permission for continuing site visits by attorneys and experts for the inmate class, and this continued into 1999. In response to this, the TDC issued more than 450,000 pages of evidence and accepted 50 additional site visits. In 2001, the court found that the TDC was in compliance on the issue of use of force against inmates and had adequate policies and procedures in place. However, the court continued to have issues with the \"current and ongoing constitutional violations regarding administrative segregation [in] the conditions of confinement and the practice of using administrative segregation to house mentally ill inmates\" that it found.\n\nIn 2007, in the consolidated case of \"Jones v. Bock\" the U.S. Supreme Court, in a unanimous decision, set forth limitations on the extent of prison litigation.\n\n"}
{"id": "5591986", "url": "https://en.wikipedia.org/wiki?curid=5591986", "title": "Saprophagy", "text": "Saprophagy\n\nSaprophages are olophages that obtain nutrients by consuming decomposing dead plant or animal biomass. They are distinguished from detritivores in that saprophages are sessile consumers while detritivore are mobile. Typical saprophagic animals include sedentary polychaetes such as amphitrites (\"Amphitritinae\", worms of the family Terebellidae) and other terebellids.\n\nThe eating of wood, whether live or dead, is known as xylophagy. Τhe activity of animals feeding only on dead wood is called sapro-xylophagy and those animals, sapro-xylophagous.\n\nIn food webs, saprophages generally play the roles of decomposers. There are two main branches of saprophges, broken down by nutrient source. There are necrophages which consume dead animal biomass, and thanatophages which consume dead plant biomass.\n\n"}
{"id": "1306084", "url": "https://en.wikipedia.org/wiki?curid=1306084", "title": "Suez (company)", "text": "Suez (company)\n\nSuez S.A. was a leading French-based multinational corporation headquartered in the 8th arrondissement of Paris, with operations primarily in water, electricity and natural gas supply, and waste management. Suez was result of a 1997 merger between the \"Compagnie de Suez\" and \"\", a leading French water company. In the early 2000s Suez also owned some media and telecommunications assets, but has since divested these. According to the \"Masons Water Yearbook\" 2004/5, Suez served 117.4 million people around the world. The company conducted a merger of equals with fellow utility company Gaz de France on 22 July 2008 to form GDF Suez (called Engie since 2015). The water and waste assets of Suez were spun off into a separate publicly traded company, Suez Environnement.\n\nSuez was (and remains, through GDF Suez) one of the oldest continuously existing multinational corporations in the world, with one line of corporate history dating back to the 1822 founding of the (literally: General Dutch Company for the favouring of industry) by King William I of the Netherlands (see Société Générale de Belgique). Its form prior to the GDF merger was the result of nearly two centuries of reorganisation and corporate mergers. Its most recent name comes from the involvement of one of its several founding entities – the \"Compagnie universelle du canal maritime de Suez\" – in building the Suez Canal in the mid-19th century.\n\nOn February 25, 2006, French Prime minister Dominique de Villepin announced the merger of Suez and Gaz de France, which would make the world's largest liquefied natural gas company. The revenue of GDF was around 22.4 billion euros in 2005, compared to 41.5 billion for Suez. The CGT trade-union called the merger a \"disguised privatization.\"\n\nOn 3 September 2007, Gaz de France and Suez announced agreed terms of merger. The deal was conducted on the basis of an exchange of 21 Gaz de France shares for 22 Suez shares via the absorption of Suez by Gaz de France. The French state holds more than 35% of shares of the merged company, GDF Suez. As a consequence of the conditions posed by the European Commission with regards to allowing the merger, on 29 May 2008 Suez sold its Belgian gas supplier Distrigas to the Italian company Eni for €2.7 billion. On 22 July 2008 the group GDF Suez, a company of €74 billion of annual turnover, was officially created.\n\n\n\n"}
{"id": "1039156", "url": "https://en.wikipedia.org/wiki?curid=1039156", "title": "Terrence Higgins Trust", "text": "Terrence Higgins Trust\n\nTerrence Higgins Trust is a British charity that campaigns on and provides services relating to HIV and sexual health. In particular, the charity aims to end the transmission of HIV in the UK; to support and empower people living with HIV; to eradicate stigma and discrimination around HIV; and to promote good sexual health (including safe sex).\n\nThe Trust is generally considered the UK's leading HIV and AIDS charity, and the largest in Europe. It is also the lead organisation for Public Health England's HIV prevention partnership HIV Prevention England.\n\nEstablished in 1982, Terrence Higgins Trust was the first charity in the UK to be set up in response to HIV and AIDS. It was initially named Terry Higgins Trust, after Terry Higgins, who died aged 37 on 4 July 1982 at St Thomas' Hospital, London. He was among the first people in the UK known to have died from the AIDS virus, which was only identified the previous year.\n\nTerry's close friends Martyn Butler, Tony Calvert and Terry's partner Rupert Whitaker along with other friends started the Trust to raise funds for research as a way of preventing suffering due to AIDS. Shortly, with the generation of a groundswell of support for the organisation at a meeting at Red Lion Square, Tony Whitehead and others joined the group and formally founded the organisation and saw it through registration as a charity to provide direct services to those affected by HIV. \n\nThe trust was named after Terry to personalise and humanise the issue of AIDS. It was formalised in August 1983 when it adopted a constitution and opened a bank account, and the name of the trust was changed (\"Terrence\" rather than \"Terry\") to sound more formal. It incorporated as a limited company in November 1983 and gained charitable status in January 1984.\n\"Since its inception 12 years ago, most of its fund-raising events have been hip and low-key affairs, from its first disco at the London club Heaven to gala screenings of Dracula and the comedy shows Hysteria and Filth...\"\n\n\"Hysteria 1\", was a comedy benefit for Terrence Higgins Trust, produced by Stephen Fry.\n\n\"Hysteria 2\", September 18, 1989 at the Sadler's Wells Theatre in London, benefited Terrence Higgins Trust. was produced by Stephen Fry, broadcast on Channel 4, with a telethon.\n\n\"Hysteria 3\", at the London Palladium on Sunday, June 30th, 1991, benefited Terrence Higgins Trust. Stephen Fry hosted, with Steven Wright, Hugh Laurie, Elton John, Ruby Wax, Rowan Atkinson, Eddie Izzard, Craig Ferguson, Lenny Henry, Julian Clary, Josie Lawrence, Jools Holland, Dawn French, Jennifer Saunders, Ian McKellen, Edwin Starr, Clement Freud, Beverly Craven, Ben Elton, and Tony Slattery.\n\nThe charity received almost a million pounds in donations over the Christmas of 1991, with the proceeds of Queen's re-released chart-topper Bohemian Rhapsody going entirely to the charity, following the recent AIDS-related death of lead singer Freddie Mercury. After being diagnosed as HIV positive in 1987, Mercury had been concerned that financial support should be available to those less fortunate than himself.\n\nLisa Power, former corporate head of policy at the Terrence Higgins Trust, denounced the views of Pope Benedict XVI on the use of condoms to prevent AIDS and said: \"We deeply regret the continued misinformation around condoms, which remain the most effective way of preventing the spread of HIV.\".\n\nIn August 2015 first-team players from Hull Kingston Rovers teamed up with 1980s band Erasure to record a charity version of the band's single \"A Little Respect\", with a third of the proceeds going to the trust.\n\nActivist Nick Partridge, who joined the Trust in 1985, was its chief executive until 2013. Ian Green has been the charity's Chief Executive since March 2016.\n\nThe charity's celebrity patrons have included Sir Richard Branson, Simon Callow, Lord Cashman, Julian Clary, Dame Judi Dench, Tracey Emin, Stephen Fry, Paul Gambaccini, Sir Elton John, Caroline Quentin, Gaby Roslin, Dr Miriam Stoppard and George Michael.\n\n\n"}
{"id": "26741571", "url": "https://en.wikipedia.org/wiki?curid=26741571", "title": "Traditional Healthcare", "text": "Traditional Healthcare\n\nMoringa Project is an Australia-based nonprofit charitable organization committed to the development of sustainable healthcare facilities for underprivileged communities. Moringa Project's current campaign is the development of a sustainable clinic in Jharkhand, India.\n\nEstablished in 2008, Moringa Project (formerly Traditional Healthcare) began after two Australian acupuncturists travelled to the remote village of Pundag, India to volunteer their time.\n\nThe Pundag clinic was treating up to 70 patients a day and the volunteers were confronted with a wide range of ailments from simple aches and pains to very complicated disorders and pathologies. The lack of facilities in the area saw patients travelling for up to three days to seek treatment.\n\nThe manager of the acupuncture clinic then took the volunteers to the even more remote village of Datom, to treat people who previously had never received any kind of medical treatment due to their isolation and poverty. Datom is a remote village located in the state of Jharkhand. With no medical facilities within a 100 km radius, the people of the area are in desperate need of healthcare. On arriving, the volunteers were shown to a dilapidated hut that was to be their clinic. Immediately the volunteers were overwhelmed with patients. There was an evident need for a healthcare clinic.\n\nAfter seeing the success of the acupuncture clinic in Pundag, and the need for another clinic in Datom, Traditional Healthcare was established shortly after arriving back in Australia. The aim was to start construction of the Datom clinic in January 2010 and to raise $50,000AU to build and fit out the facility.\n\nChanging its name in 2013, Traditional Healthcare, now Moringa Project, is run by Melbourne volunteers, who are not affiliated with any religious bodies.\n\nMoringa Project endeavours to be self-sufficient by using wind and solar power, water conservation, permaculture, sustainable architecture, recycled materials and traditional medicines. The aim of Moringa Project is not to be completely dependent on continual funding of each clinic, as the goal is self-sufficiency.\n\nAn educational component is essential to the sustainability of the clinics. The educational program consists of two core elements: community education and practitioner training.\n\nMoringa Project's buildings are designed according to environmentally sustainable principles. For example, the current project in Jharkhand (India) will be built with local renewable resources and be powered by solar and wind generators.\n\nPractitioner training will be provided at all of the facilities. First aid and hygiene will be taught at all facilities. Other programs will include acupuncture training (basic and advanced), herbal medicine, massage, mid-wifery, permaculture, and medicinal herb cultivation.\n\nThe current project is the construction of a clinic in Datom, a remote village in the state of Jharkhand, India.\n\nA practitioner provides an acupuncture service to the inhabitants of the village, however with no clinical facilities the number of patients treated in one day is rather limited. Those in need are treated in their houses in need in return for a small donation, as there are no other available healthcare facilities within approximately a 100 km radius.\n\nThe new facility is designed according to Moringa Project's principles of sustainability, incorporating environmental architecture and renewable energy sources such as wind and solar power. The clinic will be able to treat up to 20 people at one time, allowing the treatment of up to 300 people per day. To begin with, the clinic will focus on acupuncture as the primary treatment because resources and practitioners are already available. However, Moringa Project wants to expand to incorporate a wider range of treatments. A class room, co-ordinator's office, medicinal herb garden and volunteer accommodation has been incorporated into the design. Construction began in January 2010 and continues with a team working in Datom during January 2012.\n\nMore charitable projects are being planned.\n"}
{"id": "2733891", "url": "https://en.wikipedia.org/wiki?curid=2733891", "title": "Translational medicine", "text": "Translational medicine\n\nTranslational medicine (often referred to as translational science, of which it is a form) is defined by the European Society for Translational Medicine (EUSTM) as \"an interdisciplinary branch of the biomedical field supported by three main pillars: benchside, bedside, and community\". The goal of TM is to combine disciplines, resources, expertise, and techniques within these pillars to promote enhancements in prevention, diagnosis, and therapies. Accordingly, translational medicine is a highly interdisciplinary field, the primary goal of which is to coalesce assets of various natures within the individual pillars in order to improve the global healthcare system significantly.\n\nTranslational medicine is a rapidly growing discipline in biomedical research and aims to expedite the discovery of new diagnostic tools and treatments by using a multi-disciplinary, highly collaborative, \"bench-to-bedside\" approach. Within public health, translational medicine is focused on ensuring that proven strategies for disease treatment and prevention are actually implemented within the community. One prevalent description of translational medicine, first introduced by the Institute of Medicine's Clinical Research Roundtable, highlights two roadblocks (i.e., distinct areas in need of improvement): the first translational block (T1) prevents basic research findings from being tested in a clinical setting; the second translational block (T2) prevents proven interventions from becoming standard practice.\n\nThe National Institutes of Health (NIH) has made a major push to fund translational medicine, especially within biomedical research, with a focus on cross-functional collaborations (e.g., between researchers and clinicians); leveraging new technology and data analysis tools; and increasing the speed at which new treatments reach patients. In December 2011, The National Center for Advancing Translational Science (NCATS) was established within the NIH to \"transform the translational science process so that new treatments and cures for disease can be delivered to patients faster.\" The Clinical and Translational Science Awards, established in 2006 and now funded by NCATS, supports 60 centers across the country that provide \"academic homes for translational sciences and supporting research resources needed by local and national research communities.\" According to an article published in 2007 in Science Career Magazine, in 2007 to 2013 the European Commission targeted a majority of its €6 billion budget for health research to further translational medicine.\n\nIn recent years, a number of educational programs have emerged to provide professional training in the skills necessary for successfully translating research into improved clinical outcomes. These programs go by various names (including Master of Translational Medicine and Master of Science in Bioinnovation). Many such programs emerge from bioengineering departments, often in collaboration with clinical departments.\n\nThe University of Edinburgh has been running an MSc in Translational Medicine program since 2007. It is a 3-year online distance learning programme aimed at the working health professional.\n\nAalborg University Denmark has been running a master's degree in translational medicine since 2009.\n\nA master's degree programme in translational medicine was started at the University of Helsinki in 2010. \n\nIn 2010, UC Berkeley and UC San Francisco used a founding grant from Andy Grove to launch a joint program that became the Master of Translational Medicine. The program links the Bioengineering department at Berkeley with the Bioengineering and Therapeutic Science department at UCSF to give students a one-year experience in fostering medical innovation.\n\nCedars-Sinai Medical Center in Los Angeles, California was accredited in 2012 for a doctoral program in Biomedical Science and Translational Medicine. The PhD program focuses on biomedical and clinical research that relate directly to developing new therapies for patients.\n\nSince 2013, the Official Master in Translational Medicine-MSc from the University of Barcelona offers the opportunity to gain an excellent training through theoretical and practical courses. Furthermore, this master is linked to the doctoral programme “Medicine and Translational Research”, with quality mention from the National Agency for the Quality Evaluation and Accreditation (ANECA).\n\nIn Fall 2015, the City College of New York established a master in translational medicine program. A partnership between The Grove School of Engineering and the Sophie Davis School for Biomedical Education/CUNY School of Medicine, this program provides scientists, engineers, and pre-med students with training in product design, intellectual property, regulatory affairs, and medical ethics over 3 semesters.\n\nUniversity of Liverpool, King's College London, Imperial College London, University College London, Oxford and Cambridge Universities run post-graduate courses in Translational Medicine too.\n\nThe University of Manchester, Newcastle University and Queen's University Belfast also offer research-focussed Masters of Research (MRes) courses in Translational Medicine.\n\nTulane University has a PhD program in Bio-Innovation to foster design and implementation of innovative biomedical technologies.\n\nTemple University's College of Public Health offers a Master of Science in Clinical Research and Translational Medicine. The program is jointly offered with the Lewis Katz School of Medicine.\n\nMahidol University at Faculty of Medicine Ramathibodi hospital has a Master and PhD programme in Translational Medicine since 2012. Mahidol University is the first and only University in Thailand and in Southeast Asia. The most of programme lecturers are physicians and clinicians who contribute in many fields of study such as Cancer, Allergy and Immunology, Haematology, Paediatric, Rheumatology, etc. Here, the student will be directly exposed to patients. To find out the thing in between basic science and clinical application.\n\nUniversity of Würzburg started a Master programme in Translational Medicine in 2018. It is aimed at medical students in the third or fourth year pursuing a career as a Clinician Scientist.\n\nAcademy of Translational Medicine Professionals (ATMP) offers a regular professional certification course <nowiki>'Understanding Translational Medicine Tools and Techniques\"</nowiki>\n\nJames Lind Institute has been conducting a Postgraduate Diploma in Translational Medicine since early 2013. The program has been supported by the Universiti Sains Malaysia.\n\nThe University of Southern California (USC) School of Pharmacy offers a course in Translational Medicine.\n\nThe European Society for Translational Medicine (EUSTM) is a global non-profit and neutral healthcare organization whose principal objective is to enhance world-wide healthcare by using translational medicine approaches, resources and expertise. The society facilitates cooperation and interaction among clinicians, scientists, academia, industry, governments, funding and regulatory agencies, investors and policy makers in order to develop and deliver high quality translational medicine programs and initiatives with overall aim to enhance the healthcare of global population. The society׳s goal is to enhance research and development of novel and affordable diagnostic tools and treatments for the clinical disorders affecting global population.\n\nThe society provides an annual platform in the form of global congresses where global key opinion leaders, scientists from bench side, public health professionals, clinicians from bedside and industry professionals gather and take part in the panel discussions and scientific sessions on latest updates and developments in translational medicine field including biomarkers, omics sciences, cellular and molecular biology, data mining & management, precision medicine & companion diagnostics, disease modelling, vaccines and community healthcare. \n\nAcademy of Translational Medicine Professionals (ATMP) is working to advance the ongoing knowledge and skills for clinicians and scientific professionals at all levels. Academy’s high quality, standard and ethical training and educational programs ensure that all clinical and scientific professionals achieve excellence in their respective fields. Programs are accredited by the European Society for Translational Medicine (EUSTM).\n\nAcademy of Translational Medicine Professionals (ATMP) offers fellowship program which is open to highly experienced professionals who have a record of significant achievements in benchside, bedside or community health fields.\n\n\n"}
{"id": "42050360", "url": "https://en.wikipedia.org/wiki?curid=42050360", "title": "Tsuneo Hasegawa", "text": "Tsuneo Hasegawa\n\n\n\n\n"}
{"id": "39228613", "url": "https://en.wikipedia.org/wiki?curid=39228613", "title": "World Brewer's Cup", "text": "World Brewer's Cup\n\nThe World Brewer's Cup is a competition which showcases the craft and skill of filter coffee brewing by hand, promoting manual coffee brewing and quality of service. The contestants comes from around the world to compete in Melbourne, Australia only after winning the crown of their own country first. \n\nThe Championship takes place in Melbourne, Australia and runs from May 23 to May 26. Competitors are required to prepare and serve three separate beverages for a panel of judges. The WCB consists of two rounds: a first round and a finalist round. In the first heat competitors are required to complete two coffee services, which are a compulsory service and an open service. For the compulsory service, competitors prepare three beverages utilizing whole bean coffee provided to them by the competition. For the open service, competitors may utilize any whole bean coffee of their choosing and must also accompany their beverage preparation with a presentation. The six competitors with the highest score from the first round will go on to compete in the finals round consisting exclusively of an open service. \nOne competitor from the final round will be named the World Brewers Cup Champion.\nThe 2017 World Brewer's Cup takes place in Budapest, Hungary on June 13-15. \nThe following are different Types of Coffee Recipes from Past World Brewers Cup Competitions.\nThe first type of coffee was brewed by Sam Corra of Australia called ONA Coffee. He brewed Whole Cherry Carbonic wash processed Green-tip Geisha from Finca Deborah in Panama using a Hario V60 brewer. The method used was by splitting 300mL water into five pours with a dose of 20g coffee ground coarsely.\nThe second type of coffee was by Stathis Koremtas of Greece called Taf Coffee. He brewed National processed Ninety Plus using a Hario V60 brewer. The method used was a 30-second bloom with 15g coffee to 250mL of 92°C water.\nThe third type of coffee was brewed by Costantin Hoppenz of Germany called Bonanza Coffee. He brewed washedd SL-28 and SL-34 coffee from Kenya. He used the method 30 second 30mL bloom with a V60-like glass brewer. Costantin used a dose of 14g to 230mL 90°C water.\nThe forth type of coffee brewed was by Chad Wang of Taiwan called Jascaffe China. The coffee used was Natural processed ninety plus panama geisha with a ceramic Hario V60. The method used was 30 second bloom and after bloom pour rest of water directly in the center of the brewer meaning no circles. He used a dose of 15g coffee to 250mL of 92°C water.\nThe fifth type of coffee was by Petra Strelecka form Industra Coffee in Czech Republic. She used 70/30 blend of Ninety Plus Hot-Fermented Lotus Geisha and Honey-Processed Juliet Geisha with a plastic Hario V60. The method used was pulse pour 50g water every 30 seconds with a dose of 14.8g coffee to 250mL of 95°C water.\nThe sixth type of coffee was by Michael Manhart of Austria. The coffee used was Natural Processed Nyasaland Variety Uganda with a Custom ceramic brewer similar to a Kalita Wave. The method used was add 250mL of 94°C Third Wave Water with a dose of 16g coffee, coarsely ground.\n"}
