{"id": "20857787", "url": "https://en.wikipedia.org/wiki?curid=20857787", "title": "Abortion in Ecuador", "text": "Abortion in Ecuador\n\nAbortion in Ecuador is illegal except when performed in the case of a threat to the life or health of a pregnant woman (when this threat cannot be averted by other means) or when the pregnancy is the result of a sexual crime against a mentally disabled woman and her legal representative has consented. The Ministry of Public Health provides guidelines on therapeutic abortion.\n\nIn Ecuador, there is strong political opposition to abortion; in 2013 then president Rafael Correa threatened to resign if the abortion law was liberalized. As of 2015, nearly 100 criminal cases of illegal abortion were under investigation.\n\nIn 2015, Ecuador was urged by CEDAW to decriminalize abortion in cases of rape and incest (under current law abortion in this case is legal only if the woman is mentally disabled) and severe fetal impairment (which is also illegal).\n\n"}
{"id": "28558665", "url": "https://en.wikipedia.org/wiki?curid=28558665", "title": "Advisory Committee on Dangerous Pathogens", "text": "Advisory Committee on Dangerous Pathogens\n\nThe Advisory Committee on Dangerous Pathogens (ACDP) is a UK-wide advisory committee. It was established in 1981, and the terms of reference were revised in 1991 to allow for a wider remit.\n\nThe terms of reference of the ACDP are:\n\nThe Committee comprises a Chairman and up to 17 members. The membership includes scientific and medical experts, representing a range of disciplines.\n\nOn 31 December 2012, the Committee was reconstituted from an advisory non-departmental public body to an Expert Advisory Committee.\n\n"}
{"id": "32855228", "url": "https://en.wikipedia.org/wiki?curid=32855228", "title": "Advisory Committee on the Safety of Blood, Tissues and Organs", "text": "Advisory Committee on the Safety of Blood, Tissues and Organs\n\nThe Advisory Committee on the Safety of Blood, Tissues and Organs is a committee in the United Kingdom that advises the government on 'the most appropriate ways to ensure the safety of blood, cells, tissues and organs for transfusion / transplantation'.\n"}
{"id": "38612918", "url": "https://en.wikipedia.org/wiki?curid=38612918", "title": "American College of Clinical Pharmacology", "text": "American College of Clinical Pharmacology\n\nThe American College of Clinical Pharmacology (ACCP) is a national organization of clinical pharmacology healthcare professionals who seek to advance clinical pharmacology.\n\nIn the 1960s, a group of physicians formulated the concept of an organization dedicated to a new branch of pharmacology that dealt with the effectiveness and safety of drugs in humans. As a result of their efforts, the American College of Clinical Pharmacology (ACCP) was founded on September 11, 1969.\n\nThe mission is to improve health by optimizing therapeutics and to provide leadership and education that enables the generation, integration and translation of scientific knowledge to optimize research, development and utilization of medication.\n\nACCP is governed by a Board of Regents elected by ACCP Members. The Board is advised by a network of ACCP committees.\n\nLevels of membership in ACCP are Retired, Student, Member and Fellow (FCP). Fellowship is the organization's way of noting outstanding achievement in the discipline. Fellows require specific credentials and are recommended by their peers and reviewed by the Board of Regents.\n\n\"The Journal of Clinical Pharmacology\", published by ACCP is the main journal of the College. \"Clinical Pharmacology in Drug Development\" is another publication.\n\nThe College organizes a number of annual meetings and has several committees. There are special programs for students and to promote the discipline in developing countries.\n\nACCP promotes the rational use of medications in humans through research, development and regulation of medications, and education of healthcare professionals and patients on the optimal utilization of medications.\n\nACCP is an accredited provider of Continuing Medical Education (CME) and Continuing Pharmacy Education (CPE). Within clinical pharmacology its programs focus on: translational medicine, biomarkers, clinical pharmacokinetics and pharmacodynamics, quantitative methodologies, factors that can affect exposure-response, therapeutic biologics, pharmacoepidemiology / pharmacogenomics, pharmacoeconomics, drug monitoring, clinical trial design and, experimental pharmacology.\n\n"}
{"id": "10480264", "url": "https://en.wikipedia.org/wiki?curid=10480264", "title": "Bhojtal", "text": "Bhojtal\n\nBhojtal, formerly known as Upper Lake, is a large lake which lies on the western side of the capital city of Madhya Pradesh, Bhopal. It is a major source of drinking water for the residents of the city, serving around 40% of the residents with nearly of water per day. Bada talaab, along with the nearby Chhota Talaab, meaning small lake in Hindi, constitute Bhoj Wetland, which is now a Ramsar site.\n\nAccording to the local folklore, Bhojtal is said to have been built by the Paramara Raja Bhoj during his tenure as a king of Malwa (1005–1055). He is also said to have established the city of Bhopal (also named after him, then as Bhojpal) to secure the eastern frontier of his kingdom. There is a legend why they built the lake. Once king Bhoj suffered from skin disease and all \"Vaidyas\" (Doctor in English) failed to cure him. Then, one day a saint told the king to build a tank to combine 365 tributaries and then have a bath in it to wipe out the skin disease. Bhoj called upon his engineers to build up a huge tank. They spotted a place near river Betwa, which was 32 km away from Bhopal. It was found that it has only 359 tributaries. A Gond Commander Kalia fulfilled this shortage. He then gave the address of an invisible river. After merging the tributaries of this river the number 365 was completed.\n\nThe lake was created by constructing an earthen dam across the Kolans River. An eleven gate dam called the Bhadbhada dam was constructed at Bhadbhada in 1965 at the southeast corner of the Lake, and now controls the outflow to the river Kaliasote.\n\nThe lake was known as the Upper Lake or \"Bada Talab\" (\"Big Pond\") until March 2011 it was renamed to Bhojtaal in honour of the Great King Raja Bhoj who built it. A huge statue of Raja Bhoj, standing with sword, was also installed on a pillar on one corner of the lake to cement the name of Bhopal as the city of lakes\n\nIn July 2016 :Bhopal Upper Lake dam gates opened so early in July after 38 years.\n\nBhojtal is situated on the west central part of Bhopal city and is surrounded by Van Vihar National Park on the south, human settlements on the east and north, and agriculture fields on the west. It has an area of 31 km, and drains a catchment or watershed of 361 km. The watershed of the Upper Lake is mostly rural, with some urbanized areas around its eastern end. The Kolans was formerly a tributary of the Halali River, but with the creation of the lake using an earthen dam and a diversion channel, the upper reach of the Kolans River and Bada Talaab now drain into the Kaliasote River.\n\nSince the construction of the lake in the 11th century, Bhopal city has grown around it. The people are religiously and culturally attached to the lakes. The lakes meet their needs of water supply and they wash clothes in them (very harmful for the lake ecosystem), cultivate water chestnut in Bhojtal and lotus in Chhota Talaab. The idols of gods and goddesses are also immersed in the lake during religious festivals, though the local administration is advising devotees not to do so. The Takia island in Upper lake has a tomb of the Shah Ali Shah Rahamatullah Alla, which has religious and archaeological significance.\n\nThe fishing rights to Bhojtal have been given on long lease by the Bhopal Municipal Corporation to a co-operative consisting of some 500 fishermen families. Fishing is mainly done on its south-eastern shores. The lake also serves as the source of water for irrigating a large area. There are 87 villages in its catchment area in the Bhopal as well as Sehore districts. Agriculture is the main source of livelihood for people in these areas and most farmers have livestock as well. While some farmers have large landholdings, many farmers are small and marginal farmers with only a few acres of land.\n\nBhojtal attracts tourists due to its scenic beauty. The National Sailing School was established at the Boat Club on its eastern side. This club offers various water sports such as kayaking, canoeing, rafting, water skiing, parasailing etc. A number of operators provide facilities for exciting trips by sail, paddle and motor boats. Van Vihar National Park, situated on the south-eastern side of the lake, attracts tourists. The road passing through it has on one side the animals in their natural habitats, and on the other hand is the scenic beauty of the lake.\n\nThe other major attraction close to the lake is the Bhopal Zoo on the bank of the lake. This makes the lake a complete entertainment location for all visitors. The zoo is kept in quite natural way.\n\nThe two lakes support flora and fauna. White stork, blacknecked stork, barheaded goose, spoonbill etc., that have been rare sightings in the past, have started appearing. A recent phenomenon is the gathering of 100-120 sarus cranes in the lake. The largest bird of India, sarus crane (\"Grus antigone\") is known for its size, majestic flight and lifetime pairing.\n\n106 species of Macrophytes (belonging to 87 genera of 46 families), which includes 14 rare species and 208 species of Phytoplankton comprising 106 species of Chlorophyceae, 37 species of Cyano phyceae, 34 species of Euglenophyceae, 27 species of Bacilariophyceae and 4 species of Dinophyceae.\n\n105 species of zooplanktons, which includes (Rotifera 41, Protozoa 10, Cladocera 14, Copepoda 5, Ostracoda 9, Coleoptera 11, and Diptera 25). There are 43 species of fish (natural and cultured), 27 kinds of birds, 98 species of insect and more than 10 species of reptiles and amphibians (including 5 species of tortoise).\n\nThe lake is shrinking and being polluted due to human activities in recent times. The waste and drainage generated by nearby city of Bhopal is dumped in lake affecting its ecology.\n"}
{"id": "23204633", "url": "https://en.wikipedia.org/wiki?curid=23204633", "title": "Corneal keratocyte", "text": "Corneal keratocyte\n\nCorneal keratocytes (corneal fibroblasts) are specialized fibroblasts residing in the stroma. This corneal layer, representing about 85-90% of corneal thickness, is built up from highly regular collagenous lamellae and extracellular matrix components. Keratocytes play the major role in keeping it transparent, healing its wounds, and synthesizing its components. In the unperturbed cornea keratocytes stay dormant, coming into action after any kind of injury or inflammation. Some keratocytes underlying the site of injury, even a light one, undergo apoptosis immediately after the injury. Any glitch in the precisely orchestrated process of healing may cloud the cornea, while excessive keratocyte apoptosis may be a part of the pathological process in the degenerative corneal disorders such as keratoconus, and these considerations prompt the ongoing research into the function of these cells.\n\nKeratocytes are developmentally derived from the cranial population of neural crest cells, from whence they migrate to settle in the mesenchyme. In some species the migration from neural crest comes in two waves, with the first giving birth to the corneal endothelium and the second invading the epithelium-secreted stromal anlage devoid of cells; in other species both populations come from a single wave of migration. Once settled in the stroma, keratocytes start synthesizing collagen molecules of different types (I, V, VI) and keratan sulfate. By the moment of eye opening after birth the proliferation of keratocytes is all but finished and most of them are in the quiescent state.\n\nBy the end of eye development an interconnected keratocyte network is established in the cornea, with dendrites of neighbouring cells contacting each over. Quiescent keratocytes synthesize the so-called crystallins, known primarily for their role in the lens. Corneal crystallins, like the lens ones, are thought to help maintain the transparency and optimal refraction. They are also part of corneal antioxidant defense. Crystallins expressed by human keratocytes are ALDH1A1, ALDH3A1, ALDH2 and TKT. Different sets of crystallins are typical to distinct species. Keratan sulfate produced by keratocytes is thought to help maintain optimal corneal hydration; genetic disruption of its synthesis leads to the macular corneal dystrophy.\n\nAccording to one study, average keratocyte density in the human stroma is about 20500 cells per mm, or 9600 in a column of 1 mm in section. The highest density is observed in the upper 10% of the stroma. The number of keratocytes declines with age, at a rate approximately 0.45% per year.\n\nAfter an injury to the cornea, some keratocytes undergo apoptosis, prompted by the signaling molecules secreted by the upper layers, such as IL1 alpha and TNF-alpha. Other neighbouring keratocytes, when acted upon by the same molecules, become active, proliferate and start synthesizing matrix metalloproteinases that cause tissue remodeling. These activated cells are designated in different sources either as \"active keratocytes\" or \"fibroblasts\" or are said to assume a \"repair phenotype\". After heavier injuries or at the advanced stages of healing process a number of keratocytes transforms into myofibroblasts actively secreting ECM components; this transformation is thought to be caused by TGF-beta. As soon as the basement membrane of corneal epithelium is restored, TGF beta inflow into the stroma drastically decreases and myofibroblasts disappear, after which the remaining activated keratocytes continue for some time to reshape the extracellular matrix, secreting IL1-alpha in order to maintain their \"repair phenotype\".\n\nApoptosis of keratocytes, either in quiescent or active state, is a process that attracts special attention. In a healthy cornea the programmed cell death is a rare occasion, but immediately after an injury to the uppermost layer keratocytes directly under the injury site commit apoptosis. One hypothesis explains such rapid reaction by the need to stem the possible infection from spreading into the cornea, because due to the limitations of ocular immune system the immune cells take up to several hours to arrive at the site of injury. In a normal course of events, the lack of keratocytes is gradually replenished by the mitosis of the adjacent cells. Apoptosis is observed after eye operations, including keratotomy and laser surgery, and may play a role in the development of post-surgery complications.\n\nKeratocytes may play a role in different corneal disorders. According to comparative research, their functions drastically diverge from the norm in keratoconus, the most frequent form of corneal dystrophy. In keratoconic corneas they have been shown to commit apoptosis far away from any epithelial injury; a hypothesis exists that presents excessive keratocyte apoptosis as a major pathological event in keratoconus. According to one study, patient's keratocytes have decreased levels of one of the alcohol dehydrogenase subforms, they secrete significantly less superoxide dismutase 3, according to another.\n\n\n"}
{"id": "4463582", "url": "https://en.wikipedia.org/wiki?curid=4463582", "title": "Curve of Spee", "text": "Curve of Spee\n\nIn anatomy, the Curve of Spee (called also von Spee's curve or Spee's curvature) is defined as the curvature of the mandibular occlusal plane beginning at the premolar and following the buccal cusps of the posterior teeth, continuing to the terminal molar. According to another definition the curve of Spee is an anatomic curvature of the occlusal alignment of the teeth, beginning at the tip of the lower incisor, following the buccal cusps of the natural premolars and molars and continuing to the anterior border of the ramus. It is named for the German embryologist Ferdinand Graf von Spee (1855–1937), who was first to describe the anatomic relations of human teeth in the sagittal plane.\n\nThe pull of the main muscle of mastication, the masseter, is at a perpendicular angle with the curve of Spee to adapt for favorable loading of force on the teeth.\nThe long axis of each lower tooth is aligned nearly parallel to their individual arch of closure. \nThe Curve of Spee is, essentially, a series of sloped contact points. It is of importance to orthodontists as it may contribute to an increased overbite. A flat or mild curve of Spee was essential to an ideal occlusion.\n\nThe Curve of Spee is distinct from the Curve of Wilson, which is the upward (U-shaped) curvature of the maxillary and mandibular occlusal planes in the coronal plane.\n\nThe Curve of Spee is basically a part of a circle (8-inch diameter) which has its circumference as the anterior ramus of mandible. Ideally, it is aligned so that a continuation of this arc would extend through the condyles. The curvature of this arc would relate, on average, to part of a circle with a 4-inch radius. It is the only Anteroposterior curve of occlusion.\n\n"}
{"id": "554960", "url": "https://en.wikipedia.org/wiki?curid=554960", "title": "David Viscott", "text": "David Viscott\n\nDavid Steven Viscott (May 24, 1938 – October 10, 1996) was an American psychiatrist, author, businessman, and media personality. He was a graduate of Dartmouth (1959), Tufts Medical School and taught at University Hospital in Boston. He started a private practice in psychiatry in 1968 and later moved to Los Angeles in 1979 where he was a professor of psychiatry at UCLA. He founded and managed the Viscott Center for Natural Therapy in Beverly Hills, Newport Beach and Pasadena, California.\n\nIn 1980 Viscott began presenting his own full-time show on talk radio, and was notably one of the first psychiatrists to do so (talk station KABC). He screened telephone calls and gave considerable amount of free psychological counselling to his on-air \"patients.\"\n\nIn 1987 Viscott briefly had his own live syndicated TV show, \"Getting in Touch with Dr. David Viscott\", providing much the same service as his radio show. In fact, the shows ran concurrently. In the early 1990s he had a weekly call-in therapy television program on KNBC in Los Angeles early Sunday morning after Saturday Night Live, titled \"Night Talk with Dr. David Viscott\".\n\nViscott's signature style was to attempt to isolate an individual's source of emotional problems in a very short amount of time. Many of his books were of a self-help nature, written to assist the individual with his/her own examination of life. His autobiography, \"The Making of a Psychiatrist\", was a best-seller, a Book of the Month Club Main Selection, and nominated for the Pulitzer Prize.\n\nAlong with psychiatric advice, he would fall back on his medical knowledge to regularly devote entire segments of radio to answering medical questions. During these segments he would give medical advice. Many of the questions answered had to do with pharmacological advice. This was unique in the world of talk radio.\n\nViscott's popularity peaked in the early 1990s, and then fell sharply. A separation from his wife, followed by declining health, occurred at about the same time that he left the air waves. He died in 1996 of heart failure complicated by a diabetic condition. At the time, he was living alone in Los Angeles. He is survived by three of his children; Elizabeth, Penelope, and Jonathan. His daughter Melanie Random took her own life in February, 2015.\n\nIn the television show \"The Simpsons\", the character Dr. Marvin Monroe's voice was based on Viscott. Monroe has been retired since the seventh season because voicing the character strained Shearer's throat. The character's retirement was marked by the broadcast of a Dr. Marvin Monroe Memorial Hospital over Lou's walkie-talkie in \"Who Shot Mr. Burns? (Part Two)\". Since then, several references to Monroe being dead have been made: a glimpse of his gravestone in \"Alone Again, Natura-Diddily\", a Dr. Marvin Monroe Memorial Gymnasium seen in \"Bye Bye Nerdie\", and a trivia interstitial in the \"138th Episode Spectacular\" regarding which popular characters had recently died. However, Monroe is seen alive in the fifteenth season in \"Diatribe of a Mad Housewife\" purchasing a copy of Marge's novel, \"The Harpooned Heart\", stating simply that he had \"...been very sick\" when asked about his long absence by Marge. He was later seen as a ghost, claiming that he was \"stuck in limbo\" in \"Treehouse of Horror XXV\".\n\n"}
{"id": "23511207", "url": "https://en.wikipedia.org/wiki?curid=23511207", "title": "Debridement (dental)", "text": "Debridement (dental)\n\nIn dentistry, debridement refers to the removal by dental cleaning of accumulations of plaque and calculus (tartar) in order to maintain dental health.\n\nDental debridement is a procedure by which plaque and calculus (tartar) that have accumulated on the teeth is removed. Debridement may be performed in the process of personal or professional teeth cleaning. Professional debridement techniques include the use of ultrasonic instruments (which fracture the calculus, thereby facilitating its removal), as well as the use of hand tools, including periodontal scaler and curettes. Debridement may also be performed using saline solution. .\n\nPeriodontal Pockets\n\nA periodontal pocket is formed from a disease process; it is defined as the apical extension of the gingiva, resulting in detachment of the periodontal ligament (PDL). The PDL is a ligament that attaches the root of the tooth to the supporting alveolar bone. This ligament allows for occlusal force absorption. Plaque accumulates within the pocket initiating an inflammatory response due to an increased number of spirochetes. There are different types of bacteria that make up dental plaque. In cases of aggressive periodontitis three major species of bacteria have been identified within the periodontal pocket. These bacteria include Porphyromonas gingivalis, Prevotella intermedia, and Actinobacillus actinomycetemcomitans. Healthy gingiva consists of few microorganisms, mostly coccoid cells and straight rods. Diseased gingiva consists of increased numbers of spirochetes and mobile rods.Interactions between plaque and host inflammatory response determine the alterations in pocket depths. Bacterial plaque initiates a nonspecific host inflammatory response with the intention of eliminating necrotic cells and harmful bacteria. During this process cytokines, proteinases, and prostaglandins are produced which can cause damage, or kill healthy tissues such as macrophages, fibroblasts, neutrophiles, and epithelial cells. The exposure to connective tissue and blood capillaries, allows microorganisms to gain an entryway to the circulation. This suppresses host protection mechanisms, leading to further destruction of bone.\n\nPeriodontal pockets may occur from either coronal swelling or apical migration.Pockets that occur due to coronal swelling with no clinical attachment loss are considered pseudopockets.There are two types of periodontal pockets that are determined by the type of bone loss present. A suprabony pocket occurs when there is horizontal bone loss, the bottom of the pocket is coronal to the alveolar bone. An infrabony pocket occurs when there is vertical bone loss where the bottom of the pocket is apical to the alveolar bone.\n\nClinical signs of periodontal pockets include bluish-red, thickened gingiva, gingival bleeding, localized pain and in some cases exudate. Periodontal pockets can cause the loosening and loss of dentition due to destruction of supporting tissues including the alveolar bone, PDL and gingival tissue. Clinical diagnosis of periodontal pockets is achieved from full mouth periodontal probing performed by a dentist or dental hygienist.\n\nTreatment of periodontal pocketing requires professional and at home intervention. At home treatment for periodontal pockets include meticulous and routine plaque removal by brushing and interproximal cleaning. Professional treatment includes routine dental visits for debridement, scaling and root planing. Clinical treatment goals are set to control the inflammatory disease by removal of coronal and subgingival plaque containing destructive pathogens. With the consistent and complete removal of biofilm, the infection can be arrested and healthy periodontium can be achieved.\n\nAnother major risk factor of a periodontal pocket is smoking as it affects the severity and prevalence of pockets. Tobacco cessation is a necessary intervention to motivate patients to quit smoking and achieve periodontal health. Smoking also delays the healing process once debridement, scaling, root planing and adequate home care has been completed.\n\nHealing of periodontal pockets are shown by a reduction in pocket depth. Although pocket depths can be reduced by decreasing inflammation, it is important note that large changes will not occur. Two ways in which periodontal pocket reduction can occur is by either non-surgical periodontal therapy (NSPT) or surgical periodontal therapy. NSPT includes but is not limited to initial debridement, scaling, root planing, antibiotic treatment, and oral health education. If periodontal pocket depths are not controlled and maintained with NSPT during a re-evaluation appointment then surgical periodontal therapy is necessary.\nSurgical periodontal therapy creates a stable and maintainable environment for the patient by eliminating pathological changes in the pockets.The overall purpose of surgical therapy is to eliminate the pathogenic plaque in pocket walls to get a stable and easily maintainable state. This can promote periodontal regeneration.\n\nPeriodontal Scalers\n\nProfessional periodontal therapy includes initial debridement, scaling and root planing with specific periodontal instruments. These instruments include files, curettes, after fives and mini fives used for mechanical debridement.The shank of periodontal instruments can either be rigid, which works better with heavy and tenacious calculus or flexible for fine tuning and light deposit.\n\nPeriodontal files are used to crush larger, tenacious deposits to prepare them for further debridement with a scaler, curette or ultrasonic scalers. They have a series of blades on a base, therefore they are not suitable for root planing and fine scaling. Universal curettes are double-ended instruments with paired mirror working ends and a rounded toe. These instruments can be used on all surfaces of the tooth including root surfaces in a periodontal pocket. Gracey curettes have a stronger, rigid shank and angulated working blades that are area specific. They are best for subgingival scaling and root planing because the offset blade allowing for greater adaptation. After fives are similar to gracey’s except they have an extended shank to allow extension into deeper pockets (>5mm). They also have a thinner blade for heavy or tenacious calculus. Mini fives are a modification of after fives as their blades are half the length to allow for easier insertion and adaptation into deep pockets, furcations, developmental grooves and line angles. They also contribute to a reduction in tissue trauma. Ultrasonic scalers move in an elliptical motion and do not have a cutting edge. They operate at a frequency of 3,000-8,000 cycles per second and use magnetostrictive or piezo-electric technology, thus helping remove plaque and calculus while reducing operator wrist fatigue.\n\n"}
{"id": "14579684", "url": "https://en.wikipedia.org/wiki?curid=14579684", "title": "Diabetes Australia", "text": "Diabetes Australia\n\nDiabetes Australia is the third oldest diabetes association in the world, after the United Kingdom and Portugal. Originally established in the state of New South Wales (NSW) in 1937, the organisation's head office is now in the nation's capital, Canberra. Currently a federation of ten operational organisations is overseen: five State/Territory Associations of Diabetes Australia, the Australian Diabetes Society, the Australian Diabetes Educators Association, the Kellion Diabetes Foundation and The Diabetes Research Foundation - Western Australia.\n\nDiabetes Australia is a not-for-profit organisation supported financially by the community. In addition to its original mandate as an extended support group, Diabetes Australia raises funds to invest in research, health services, provision of self–management products and services, and public awareness programs. It also facilitates the development of national policies about diabetes.\n\nDiabetes Australia assists the Australian Government in the administration of the National Diabetes Services Scheme.\n\nDiabetes Australia works in partnership with diabetes health professionals, educators and researchers to minimise the impact of diabetes on the Australian community. Diabetes Australia is committed to turning diabetes around through awareness, prevention, detection, management and a cure.\n\nDiabetes Australia raises awareness about the seriousness of diabetes, promotes prevention and early detection strategies and advocates for better standards of care. Diabetes Australia is also a significant financial contributor to research into better treatments for diabetes and the search for a cure.\n\nDiabetes Australia is involved in two publications, one for medical specialists, and another for the broader public.\n\n\"Circle\" is a quarterly magazine covering health and welfare issues for people with diabetes, and available through membership of Diabetes Australia including state and territory based member-organisations. Circulation is around 140,000.\n\n\"Diabetes Management Journal\" (DMJ) is a free magazine containing information and advice for general practitioners, endocrinologists, diabetes educators, optometrists and podiatrists, who receive it through their professional associations. Its circulation is around 40,000.\n\nIn March 2012, the organisation released a report called \"Diabetes: the silent pandemic and its impact on Australia\". The report highlighted the statistic that 275 Australians were diagnosed with diabetes every day and that by 2025, type 2 diabetes is predicted to triple in prevalence. The statistics from the International Diabetes Federation's 2015 Diabetes Atlas show that diabetes is on the rise. Mr Johnson, the CEO of Diabetes Australia, said that 280 people are diagnosed with the diabetes in Australia every day and Australia's health system will not cope unless more is done about prevention of the disease. \n\n"}
{"id": "4996238", "url": "https://en.wikipedia.org/wiki?curid=4996238", "title": "Employment Standards Act", "text": "Employment Standards Act\n\nThe Employment Standards Act is an Act of the Legislature of Ontario. which regulates employment in the province, including wages, maximum work hours, and workplace health and safety. It differs from the Ontario Labour Relations Act, which regulates unionized labour in Ontario.\n\nThe Employment Standards Act, 2000 (ESA), sets out the minimum standards that employers and employees must follow in regards to:\n\nThe Ministry of Labour administers the ESA and its regulations by:\n\nThe Ministry of Labour offers publications and services to help employees and employers understand their rights and comply with their obligations. These include an employment standards poster, which employers are required to post in their workplaces; a catalogue of fact sheets and information sheets covering a variety of topics; and interactive online tools and calculators to assist employers and employees to understand provisions of the ESA, such as the Termination Tool, the Public Holiday Pay Calculator and the Severance Tool.\n\nEmployment standards officers conduct proactive inspections of payroll and other records, including a review of employment practices. An officer performing a proactive inspection will usually visit the employer's business location. Officers may notify the employer in writing before the inspection, but are not required to. A notice may set out a list of records and other documents the employer must provide during the inspection. The employer is required to produce the records requested and must answer questions that the officer thinks may be relevant. An officer is able to take away records or other information for review and copying.\n\nIn most cases, employees should attempt to contact their employer or former employer (or the client of a temporary help agency, if applicable) about the employment standards right(s) they believe have been violated. However, there might be a good reason for an employee to not contact their employer (e.g. If the employee is afraid to contact the employer or he or she is a young employee).\n\nEmployees have an opportunity to tell the ministry on the Claim Form why they did not contact their employer, or that they have already contacted their employer. If the employer contacted the employer but the issue was not resolved, the employee does not have to contact his or her employer again. If the parties are unable to resolve the issue on their own, and if the employee has provided all the required information on the Claim Form, the matter is assigned to an employment standards officer for investigation.\n\nThe employment standards officer may conduct his or her investigation by telephone, through written correspondence, by visiting the employer's premises or by requiring the employee and/or the employer to attend a meeting. During an investigation, both parties have the opportunity to present the facts and arguments they believe are important to their case. If a claim has been submitted against the client of a temporary help agency regarding a possible reprisal or unpaid wages, employment standards officers have the same powers of investigation with respect to the client as they do for an employer.\n\nThe officer will make a decision based on the best available evidence which may include employer records, client records, employee records, and interviews.After investigating a claim, the employment standards officer makes a decision about whether the employer has or has not followed the ESA; If the officer finds that the employer has complied with the ESA :\nSome small business owners have complained that the ESA treats them as though they were criminals.\nThere are records of employees using the ESA to abuse their employers.\n\n"}
{"id": "4329772", "url": "https://en.wikipedia.org/wiki?curid=4329772", "title": "Exertion", "text": "Exertion\n\nExertion is the physical or perceived use of energy. Exertion traditionally connotes a strenuous or costly \"effort,\"resulting in generation of force, initiation of motion, or in the performance of work. It often relates to muscularactivity and can be quantified, empirically and by measurable metabolic response.\n\nIn physics, \"exertion\" is the expenditure of energy against, or inductive of, inertia as described by Isaac Newton's third law of motion. In physics, force exerted equivocates work done. The ability to do work can be either positive or negative depending on the direction of exertion relative to gravity. For example, a force exerted upwards, like lifting an object, creates positive work done on that object.\n\nExertion often results in force generated, a contributing dynamic of general motion. In mechanics it describes the use of force against a body in the direction of its motion (see vector).\n\nExertion, physiologically, can be described by the initiation of exercise, or, intensive and exhaustive physical activity that causes cardiovascular stress or a sympathetic nervous response. This can be continuous or intermittent exertion.\n\nExertion requires, of the body, modified oxygen uptake, increased heart rate, and autonomic monitoring of blood lactate concentrations. Mediators of physical exertion include cardio-respiratory and musculoskeletal strength, as well as metabolic capability. This often correlates to an output of force followed by a refractory period of recovery. Exertion is limited by cumulative load and repetitive motions.\n\nMuscular energy reserves, or stores for biomechanical exertion, stem from metabolic, immediate production of ATP and increased O2 consumption. Muscular exertion generated depends on the muscle length and the velocity at which it is able to shorten, or contract.\n\nPerceived exertion can be explained as subjective, perceived experience that mediates response to somatic sensations and mechanisms. A rating of perceived exertion, as measured by the \"RPE-scale\", or Borg scale, is a quantitative measure of physical exertion.\n\nOften in health, exertion of oneself resulting in cardiovascular stress showed reduced physiological responses, like cortisol levels and mood, to stressors. Therefore, biological exertion is effective in mediating psychological exertion, responsive to environmental stress.\n\nOverexertion causes more than 3.5 million injuries a year. An overexertion injury can include sprains or strains, the stretching and tear of ligaments, tendons, or muscles caused by a load that exceeds the human ability to perform the work. Overexertion, besides causing acute injury, implies physical exertion beyond the person's capacity which leads to symptoms such as dizziness, irregular breathing and heart rate, and fatigue. Preventative measures can be taken based on biomechanical knowledge to limit possible overexertion injuries.\n\n\n"}
{"id": "11408", "url": "https://en.wikipedia.org/wiki?curid=11408", "title": "Female genital mutilation", "text": "Female genital mutilation\n\nFemale genital mutilation (FGM), also known as female genital cutting and female circumcision, is the ritual cutting or removal of some or all of the external female genitalia. The practice is found in Africa, Asia and the Middle East, and within communities from countries in which FGM is common. UNICEF estimated in 2016 that 200 million women living today in 30 countries—27 African countries, Indonesia, Iraqi Kurdistan and Yemen—have undergone the procedures.\n\nTypically carried out by a traditional circumciser using a blade, FGM is conducted from days after birth to puberty and beyond. In half the countries for which national figures are available, most girls are cut before the age of five. Procedures differ according to the country or ethnic group. They include removal of the clitoral hood and clitoral glans; removal of the inner labia; and removal of the inner and outer labia and closure of the vulva. In this last procedure, known as infibulation, a small hole is left for the passage of urine and menstrual fluid; the vagina is opened for intercourse and opened further for childbirth.\n\nThe practice is rooted in gender inequality, attempts to control women's sexuality, and ideas about purity, modesty and beauty. It is usually initiated and carried out by women, who see it as a source of honour and fear that failing to have their daughters and granddaughters cut will expose the girls to social exclusion. Adverse health effects depend on the type of procedure; they can include recurrent infections, difficulty urinating and passing menstrual flow, chronic pain, the development of cysts, an inability to get pregnant, complications during childbirth, and fatal bleeding. There are no known health benefits.\n\nThere have been international efforts since the 1970s to persuade practitioners to abandon FGM, and it has been outlawed or restricted in most of the countries in which it occurs, although the laws are poorly enforced. Since 2010 the United Nations has called upon healthcare providers to stop performing all forms of the procedure, including reinfibulation after childbirth and symbolic \"nicking\" of the clitoral hood. The opposition to the practice is not without its critics, particularly among anthropologists, who have raised difficult questions about cultural relativism and the universality of human rights.\n\nUntil the 1980s FGM was widely known in English as female circumcision, implying an equivalence in severity with male circumcision. From 1929 the Kenya Missionary Council referred to it as the sexual mutilation of women, following the lead of Marion Scott Stevenson, a Church of Scotland missionary. References to the practice as mutilation increased throughout the 1970s. In 1975 Rose Oldfield Hayes, an American anthropologist, used the term \"female genital mutilation\" in the title of a paper in \"American Ethnologist\", and four years later Fran Hosken, an Austrian-American feminist writer, called it mutilation in her influential \"The Hosken Report: Genital and Sexual Mutilation of Females\". The Inter-African Committee on Traditional Practices Affecting the Health of Women and Children began referring to it as female genital mutilation in 1990, and the World Health Organization (WHO) followed suit in 1991. Other English terms include \"female genital cutting\" (FGC) and \"female genital mutilation/cutting\" (FGM/C), preferred by those who work with practitioners.\n\nIn countries where FGM is common, the practice's many variants are reflected in dozens of terms, often alluding to purification. In the Bambara language, spoken mostly in Mali, it is known as \"bolokoli\" (\"washing your hands\") and in the Igbo language in eastern Nigeria as \"isa aru\" or \"iwu aru\" (\"having your bath\"). A common Arabic term for purification has the root \"t-h-r\", used for male and female circumcision (\"tahur\" and \"tahara\"). It is also known in Arabic as \"khafḍ\" or \"khifaḍ\". Communities may refer to FGM as \"pharaonic\" for infibulation and \"sunna\" circumcision for everything else. \"Sunna\" means \"path or way\" in Arabic and refers to the tradition of Muhammad, although none of the procedures are required within Islam. The term \"infibulation\" derives from \"fibula\", Latin for clasp; the Ancient Romans reportedly fastened clasps through the foreskins or labia of slaves to prevent sexual intercourse. The surgical infibulation of women came to be known as pharaonic circumcision in Sudan, and as Sudanese circumcision in Egypt. In Somalia it is known simply as \"qodob\" (\"to sew up\").\n\nThe procedures are generally performed by a traditional circumciser (cutter or \"exciseuse\") in the girls' homes, with or without anaesthesia. The cutter is usually an older woman, but in communities where the male barber has assumed the role of health worker he will perform FGM too. When traditional cutters are involved, non-sterile devices are likely to be used, including knives, razors, scissors, glass, sharpened rocks and fingernails. According to a nurse in Uganda, quoted in 2007 in \"The Lancet\", a cutter would use one knife on up to 30 girls at a time. Health professionals are often involved in Egypt, Kenya, Indonesia and Sudan; in Egypt 77 percent of FGM procedures, and in Indonesia over 50 percent, were performed by medical professionals as of 2008 and 2016. Women in Egypt reported in 1995 that a local anaesthetic had been used on their daughters in 60 percent of cases, a general anaesthetic in 13 percent, and neither in 25 percent (two percent were missing/don't know).\n\nThe WHO, UNICEF and UNFPA issued a joint statement in 1997 defining FGM as \"all procedures involving partial or total removal of the external female genitalia or other injury to the female genital organs whether for cultural or other non-therapeutic reasons\". The procedures vary considerably according to ethnicity and individual practitioners. During a 1998 survey in Niger, women responded with over 50 different terms when asked what was done to them. Translation problems are compounded by the women's confusion over which type of FGM they experienced, or even whether they experienced it. Several studies have suggested that survey responses are unreliable. A 2003 study in Ghana found that in 1995 four percent said they had not undergone FGM, but in 2000 said they had, while 11 percent switched in the other direction. In Tanzania in 2005, 66 percent reported FGM, but a medical exam found that 73 percent had undergone it. In Sudan in 2006, a significant percentage of infibulated women and girls reported a less severe type.\n\nStandard questionnaires from United Nations bodies ask women whether they or their daughters have undergone the following: (1) cut, no flesh removed (symbolic nicking); (2) cut, some flesh removed; (3) sewn closed; or (4) type not determined/unsure/doesn't know. The most common procedures fall within the \"cut, some flesh removed\" category and involve complete or partial removal of the clitoral glans. The World Health Organization (a UN agency) created a more detailed typology: Types I–III vary in how much tissue is removed; Type III is equivalent to the UNICEF category \"sewn closed\"; and Type IV describes miscellaneous procedures, including symbolic nicking.\nType I is \"partial or total removal of the clitoris and/or the prepuce\". Type Ia involves removal of the clitoral hood only. This is rarely performed alone. The more common procedure is Type Ib (clitoridectomy), the complete or partial removal of the clitoral glans (the visible tip of the clitoris) and clitoral hood. The circumciser pulls the clitoral glans with her thumb and index finger and cuts it off.\n\nType II (excision) is the complete or partial removal of the inner labia, with or without removal of the clitoral glans and outer labia. Type IIa is removal of the inner labia; Type IIb, removal of the clitoral glans and inner labia; and Type IIc, removal of the clitoral glans, inner and outer labia. \"Excision\" in French can refer to any form of FGM.\n\nType III (infibulation or pharaonic circumcision), the \"sewn closed\" category, involves the removal of the external genitalia and fusion of the wound. The inner and/or outer labia are cut away, with or without removal of the clitoral glans. Type III is found largely in northeast Africa, particularly Djibouti, Eritrea, Ethiopia, Somalia, and Sudan (although not in South Sudan). According to one 2008 estimate, over eight million women in Africa are living with Type III FGM. According to UNFPA in 2010, 20 percent of women with FGM have been infibulated. In Somalia \"[t]he child is made to squat on a stool or mat facing the circumciser at a height that offers her a good view of the parts to be handled. ... adult helpers grab and pull apart the legs of the girl. ... If available, this is the stage at which a local anaesthetic would be used\":\n\nThe element of speed and surprise is vital and the circumciser immediately grabs the clitoris by pinching it between her nails aiming to amputate it with a slash. The organ is then shown to the senior female relatives of the child who will decide whether the amount that has been removed is satisfactory or whether more is to be cut off.\n\nAfter the clitoris has been satisfactorily amputated ... the circumciser can proceed with the total removal of the labia minora and the paring of the inner walls of the labia majora. Since the entire skin on the inner walls of the labia majora has to be removed all the way down to the perineum, this becomes a messy business. By now, the child is screaming, struggling, and bleeding profusely, which makes it difficult for the circumciser to hold with bare fingers and nails the slippery skin and parts that are to be cut or sutured together. ...\n\nHaving ensured that sufficient tissue has been removed to allow the desired fusion of the skin, the circumciser pulls together the opposite sides of the labia majora, ensuring that the raw edges where the skin has been removed are well approximated. The wound is now ready to be stitched or for thorns to be applied. If a needle and thread are being used, close tight sutures will be placed to ensure that a flap of skin covers the vulva and extends from the mons veneris to the perineum, and which, after the wound heals, will form a bridge of scar tissue that will totally occlude the vaginal introitus.\nThe amputated parts might be placed in a pouch for the girl to wear. A single hole of 2–3 mm is left for the passage of urine and menstrual fluid. The vulva is closed with surgical thread, or agave or acacia thorns, and might be covered with a poultice of raw egg, herbs and sugar. To help the tissue bond, the girl's legs are tied together, often from hip to ankle; the bindings are usually loosened after a week and removed after two to six weeks. If the remaining hole is too large in the view of the girl's family, the procedure is repeated.\n\nThe vagina is opened for sexual intercourse, for the first time either by a midwife with a knife or by the woman's husband with his penis. In some areas, including Somaliland, female relatives of the bride and groom might watch the opening of the vagina to check that the girl is a virgin. The woman is opened further for childbirth (\"defibulation\" or \"deinfibulation\"), and closed again afterwards (\"reinfibulation\"). Reinfibulation can involve cutting the vagina again to restore the pinhole size of the first infibulation. This might be performed before marriage, and after childbirth, divorce and widowhood. Hanny Lightfoot-Klein interviewed hundreds of women and men in Sudan in the 1980s about sexual intercourse with Type III:\n\nThe penetration of the bride's infibulation takes anywhere from 3 or 4 days to several months. Some men are unable to penetrate their wives at all (in my study over 15%), and the task is often accomplished by a midwife under conditions of great secrecy, since this reflects negatively on the man's potency. Some who are unable to penetrate their wives manage to get them pregnant in spite of the infibulation, and the woman's vaginal passage is then cut open to allow birth to take place. ... Those men who do manage to penetrate their wives do so often, or perhaps always, with the help of the \"little knife\". This creates a tear which they gradually rip more and more until the opening is sufficient to admit the penis.\n\nType IV is \"[a]ll other harmful procedures to the female genitalia for non-medical purposes\", including pricking, piercing, incising, scraping and cauterization. It includes nicking of the clitoris (symbolic circumcision), burning or scarring the genitals, and introducing substances into the vagina to tighten it. Labia stretching is also categorized as Type IV. Common in southern and eastern Africa, the practice is supposed to enhance sexual pleasure for the man and add to the sense of a woman as a closed space. From the age of eight, girls are encouraged to stretch their inner labia using sticks and massage. Girls in Uganda are told they may have difficulty giving birth without stretched labia.\n\nA definition of FGM from the WHO in 1995 included gishiri cutting and angurya cutting, found in Nigeria and Niger. These were removed from the WHO's 2008 definition because of insufficient information about prevalence and consequences. Angurya cutting is excision of the hymen, usually performed seven days after birth. Gishiri cutting involves cutting the vagina's front or back wall with a blade or penknife, performed in response to infertility, obstructed labour and other conditions. In a study by Nigerian physician Mairo Usman Mandara, over 30 percent of women with gishiri cuts were found to have vesicovaginal fistulae (holes that allow urine to seep into the vagina).\n\nFGM harms women's physical and emotional health throughout their lives. It has no known health benefits. The short-term and late complications depend on the type of FGM, whether the practitioner has had medical training, and whether they used antibiotics and sterilized or single-use surgical instruments. In the case of Type III, other factors include how small a hole was left for the passage of urine and menstrual blood, whether surgical thread was used instead of agave or acacia thorns, and whether the procedure was performed more than once (for example, to close an opening regarded as too wide or re-open one too small).\nCommon short-term complications include swelling, excessive bleeding, pain, urine retention, and healing problems/wound infection. A 2014 systematic review of 56 studies suggested that over one in ten girls and women undergoing any form of FGM, including symbolic nicking of the clitoris (Type IV), experience immediate complications, although the risks increased with Type III. The review also suggested that there was under-reporting. Other short-term complications include fatal bleeding, anaemia, urinary infection, septicaemia, tetanus, gangrene, necrotizing fasciitis (flesh-eating disease), and endometritis. It is not known how many girls and women die as a result of the practice, because complications may not be recognized or reported. The practitioners' use of shared instruments is thought to aid the transmission of hepatitis B, hepatitis C and HIV, although no epidemiological studies have shown this.\n\nLate complications vary depending on the type of FGM. They include the formation of scars and keloids that lead to strictures and obstruction, epidermoid cysts that may become infected, and neuroma formation (growth of nerve tissue) involving nerves that supplied the clitoris. An infibulated girl may be left with an opening as small as 2–3 mm, which can cause prolonged, drop-by-drop urination, pain while urinating, and a feeling of needing to urinate all the time. Urine may collect underneath the scar, leaving the area under the skin constantly wet, which can lead to infection and the formation of small stones. The opening is larger in women who are sexually active or have given birth by vaginal delivery, but the urethra opening may still be obstructed by scar tissue. Vesicovaginal or rectovaginal fistulae can develop (holes that allow urine or faeces to seep into the vagina). This and other damage to the urethra and bladder can lead to infections and incontinence, pain during sexual intercourse and infertility. Painful periods are common because of the obstruction to the menstrual flow, and blood can stagnate in the vagina and uterus. Complete obstruction of the vagina can result in hematocolpos and hematometra (where the vagina and uterus fill with menstrual blood). The swelling of the abdomen that results from the collection of fluid, together with the lack of menstruation, can lead to suspicion of pregnancy; Asma El Dareer, a Sudanese physician, reported in 1979 that a girl in Sudan with this condition was killed by her family.\n\nFGM may place women at higher risk of problems during pregnancy and childbirth, which are more common with the more extensive FGM procedures. Infibulated women may try to make childbirth easier by eating less during pregnancy to reduce the baby's size. In women with vesicovaginal or rectovaginal fistulae, it is difficult to obtain clear urine samples as part of prenatal care, making the diagnosis of conditions such as pre-eclampsia harder. Cervical evaluation during labour may be impeded and labour prolonged or obstructed. Third-degree laceration (tears), anal-sphincter damage and emergency caesarean section are more common in infibulated women.\n\nNeonatal mortality is increased. The WHO estimated in 2006 that an additional 10–20 babies die per 1,000 deliveries as a result of FGM. The estimate was based on a study conducted on 28,393 women attending delivery wards at 28 obstetric centres in Burkina Faso, Ghana, Kenya, Nigeria, Senegal and Sudan. In those settings all types of FGM were found to pose an increased risk of death to the baby: 15 percent higher for Type I, 32 percent for Type II, and 55 percent for Type III. The reasons for this were unclear, but may be connected to genital and urinary tract infections and the presence of scar tissue. According to the study, FGM was associated with an increased risk to the mother of damage to the perineum and excessive blood loss, as well as a need to resuscitate the baby, and stillbirth, perhaps because of a long .\n\nAccording to a 2015 systematic review there is little high-quality information available on the psychological effects of FGM. Several small studies have concluded that women with FGM suffer from anxiety, depression and post-traumatic stress disorder. Feelings of shame and betrayal can develop when women leave the culture that practises FGM and learn that their condition is not the norm, but within the practising culture they may view their FGM with pride, because for them it signifies beauty, respect for tradition, chastity and hygiene. Studies on sexual function have also been small. A 2013 meta-analysis of 15 studies involving 12,671 women from seven countries concluded that women with FGM were twice as likely to report no sexual desire and 52 percent more likely to report dyspareunia (painful sexual intercourse). One third reported reduced sexual feelings.\n\nAid agencies define the prevalence of FGM as the percentage of the 15–49 age group that has experienced it. These figures are based on nationally representative household surveys known as Demographic and Health Surveys (DHS), developed by Macro International and funded mainly by the United States Agency for International Development (USAID); and Multiple Indicator Cluster Surveys (MICS) conducted with financial and technical help from UNICEF. These surveys have been carried out in Africa, Asia, Latin America and elsewhere roughly every five years, since 1984 and 1995 respectively. The first to ask about FGM was the 1989–1990 DHS in northern Sudan. The first publication to estimate FGM prevalence based on DHS data (in seven countries) was written by Dara Carr of Macro International in 1997.\n\nQuestions the women are asked during the surveys include: \"Was the genital area just nicked/cut without removing any flesh? Was any flesh (or something) removed from the genital area? Was your genital area sewn?\" Most women report \"cut, some flesh removed\" (Types I and II).\n\nType I is the most common form in Egypt, and in the southern parts of Nigeria. Type III (infibulation) is concentrated in northeastern Africa, particularly Djibouti, Eritrea, Somalia and Sudan. In surveys in 2002–2006, 30 percent of cut girls in Djibouti, 38 percent in Eritrea, and 63 percent in Somalia had experienced Type III. There is also a high prevalence of infibulation among girls in Niger and Senegal, and in 2013 it was estimated that in Nigeria three percent of the 0–14 age group had been infibulated. The type of procedure is often linked to ethnicity. In Eritrea, for example, a survey in 2002 found that all Hedareb girls had been infibulated, compared with two percent of the Tigrinya, most of whom fell into the \"cut, no flesh removed\" category.\n\nFGM is mostly found in what Gerry Mackie called an \"intriguingly contiguous\" zone in Africa—east to west from Somalia to Senegal, and north to south from Egypt to Tanzania. Nationally representative figures are available for 27 countries in Africa, as well as Indonesia, Iraqi Kurdistan and Yemen. Over 200 million women and girls are thought to be living with FGM in those 30 countries.\n\nThe highest concentrations among the 15–49 age group are in Somalia (98 percent), Guinea (97 percent), Djibouti (93 percent), Egypt (91 percent) and Sierra Leone (90 percent). As of 2013, 27.2 million women had undergone FGM in Egypt, 23.8 million in Ethiopia, and 19.9 million in Nigeria. There is a high concentration in Indonesia, where according to UNICEF Type I (clitoridectomy) and Type IV (symbolic nicking) are practised; the Indonesian Ministry of Health and Indonesian Ulema Council both say the clitoris should not be cut. The prevalence rate for the 0–11 group in Indonesia is 49 percent (13.4 million). Smaller studies or anecdotal reports suggest that FGM is also practised in Colombia, Jordan, Oman, Saudi Arabia and parts of Malaysia; in the United Arab Emirates; and in India by the Dawoodi Bohra. It is found within immigrant communities around the world.\n\nPrevalence figures for the 15–19 age group and younger show a downward trend. For example, Burkina Faso fell from 89 percent (1980) to 58 percent (2010); Egypt from 97 percent (1985) to 70 percent (2015); and Kenya from 41 percent (1984) to 11 percent (2014). Beginning in 2010, household surveys asked women about the FGM status of all their living daughters. The highest concentrations among girls aged 0–14 were in Gambia (56 percent), Mauritania (54 percent), Indonesia (49 percent for 0–11) and Guinea (46 percent). The figures suggest that a girl was one third less likely in 2014 to undergo FGM than she was 30 years ago. If the rate of decline continues, the number of girls cut will nevertheless rise, because of population growth, from 3.6 million a year in 2013 to 4.1 million in 2050.\n\nAccording to a 2018 study published in the British Medical Journal there has been a large decline in number of FGM cases among the 0-14 year old in Africa in the 1990-2017 period.\n\nSurveys have found FGM to be more common in rural areas, less common in most countries among girls from the wealthiest homes, and (except in Sudan and Somalia) less common in girls whose mothers had access to primary or secondary/higher education. In Somalia and Sudan the situation was reversed: in Somalia the mothers' access to secondary/higher education was accompanied by a rise in prevalence of FGM in their daughters, and in Sudan access to any education was accompanied by a rise.\n\nFGM is not invariably a rite of passage between childhood and adulthood, but is often performed on much younger children. Girls are most commonly cut shortly after birth to age 15. In half the countries for which national figures were available in 2000–2010, most girls had been cut by age five. Over 80 percent (of those cut) are cut before the age of five in Nigeria, Mali, Eritrea, Ghana and Mauritania. The 1997 Demographic and Health Survey in Yemen found that 76 percent of girls had been cut within two weeks of birth. The percentage is reversed in Somalia, Egypt, Chad and the Central African Republic, where over 80 percent (of those cut) are cut between five and 14. Just as the type of FGM is often linked to ethnicity, so is the mean age. In Kenya, for example, the Kisi cut around age 10 and the Kamba at 16.\n\nA country's national prevalence often reflects a high sub-national prevalence among certain ethnicities, rather than a widespread practice. In Iraq, for example, FGM is found mostly among the Kurds in Erbil (58 percent prevalence within age group 15–49, as of 2011), Sulaymaniyah (54 percent) and Kirkuk (20 percent), giving the country a national prevalence of eight percent. The practice is sometimes an ethnic marker, but it may differ along national lines. For example, in the northeastern regions of Ethiopia and Kenya, which share a border with Somalia, the Somali people practise FGM at around the same rate as they do in Somalia. But in Guinea all Fulani women responding to a survey in 2012 said they had experienced FGM, against 12 percent of the Fulani in Chad, while in Nigeria the Fulani are the only large ethnic group in the country not to practise it.\n\nDahabo Musa, a Somali woman, described infibulation in a 1988 poem as the \"three feminine sorrows\": the procedure itself, the wedding night when the woman is cut open, then childbirth when she is cut again. Despite the evident suffering, it is women who organize all forms of FGM. Anthropologist Rose Oldfield Hayes wrote in 1975 that educated Sudanese men who did not want their daughters to be infibulated (preferring clitoridectomy) would find the girls had been sewn up after the grandmothers arranged a visit to relatives. Gerry Mackie has compared the practice to footbinding. Like FGM, footbinding was carried out on young girls, nearly universal where practised, tied to ideas about honour, chastity and appropriate marriage, and \"supported and transmitted\" by women.\nFGM practitioners see the procedures as marking not only ethnic boundaries but also gender difference. According to this view, male circumcision defeminizes men while FGM demasculinizes women. Fuambai Ahmadu, an anthropologist and member of the Kono people of Sierra Leone, who in 1992 underwent clitoridectomy as an adult during a Sande society initiation, argued in 2000 that it is a male-centred assumption that the clitoris is important to female sexuality. African female symbolism revolves instead around the concept of the womb. Infibulation draws on that idea of enclosure and fertility. \"[G]enital cutting completes the social definition of a child's sex by eliminating external traces of androgyny,\" Janice Boddy wrote in 2007. \"The female body is then covered, closed, and its productive blood bound within; the male body is unveiled, opened and exposed.\"\n\nIn communities where infibulation is common, there is a preference for women's genitals to be smooth, dry and without odour, and both women and men may find the natural vulva repulsive. Some men seem to enjoy the effort of penetrating an infibulation. The local preference for dry sex causes women to introduce substances into the vagina to reduce lubrication, including leaves, tree bark, toothpaste and Vicks menthol rub. The WHO includes this practice within Type IV FGM, because the added friction during intercourse can cause lacerations and increase the risk of infection. Because of the smooth appearance of an infibulated vulva, there is also a belief that infibulation increases hygiene.\n\nCommon reasons for FGM cited by women in surveys are social acceptance, religion, hygiene, preservation of virginity, marriageability and enhancement of male sexual pleasure. In a study in northern Sudan, published in 1983, only 17.4 percent of women opposed FGM (558 out of 3,210), and most preferred excision and infibulation over clitoridectomy. Attitudes are changing slowly. In Sudan in 2010, 42 percent of women who had heard of FGM said the practice should continue. In several surveys since 2006, over 50 percent of women in Mali, Guinea, Sierra Leone, Somalia, Gambia and Egypt supported FGM's continuance, while elsewhere in Africa, Iraq and Yemen most said it should end, although in several countries only by a narrow margin.\n\nAgainst the argument that women willingly choose FGM for their daughters, UNICEF calls the practice a \"self-enforcing social convention\" to which families feel they must conform to avoid uncut daughters facing social exclusion. Ellen Gruenbaum reported that, in Sudan in the 1970s, cut girls from an Arab ethnic group would mock uncut Zabarma girls with \"Ya, Ghalfa!\" (\"Hey, unclean!\"). The Zabarma girls would respond \"Ya, mutmura!\" (A \"mutmara\" was a storage pit for grain that was continually opened and closed, like an infibulated woman.) But despite throwing the insult back, the Zabarma girls would ask their mothers, \"What's the matter? Don't we have razor blades like the Arabs?\"\n\nBecause of poor access to information, and because circumcisers downplay the causal connection, women may not associate the health consequences with the procedure. Lala Baldé, president of a women's association in Medina Cherif, a village in Senegal, told Mackie in 1998 that when girls fell ill or died, it was attributed to evil spirits. When informed of the causal relationship between FGM and ill health, Mackie wrote, the women broke down and wept. He argued that surveys taken before and after this sharing of information would show very different levels of support for FGM. The American non-profit group Tostan, founded by Molly Melching in 1991, introduced community-empowerment programs in several countries that focus on local democracy, literacy, and education about healthcare, giving women the tools to make their own decisions. In 1997, using the Tostan program, Malicounda Bambara in Senegal became the first village to abandon FGM. By 2018 over 8,000 communities in eight countries had pledged to abandon FGM and child marriage.\n\nSurveys have shown a widespread belief, particularly in Mali, Mauritania, Guinea and Egypt, that FGM is a religious requirement. Gruenbaum has argued that practitioners may not distinguish between religion, tradition and chastity, making it difficult to interpret the data. FGM's origins in northeastern Africa are pre-Islamic, but the practice became associated with Islam because of that religion's focus on female chastity and seclusion. There is no mention of it in the Quran. It is praised in a few \"daʻīf\" (weak) \"hadith\" (sayings attributed to Muhammad) as noble but not required, although it is regarded as obligatory by the Shafi'i version of Sunni Islam. In 2007 the Al-Azhar Supreme Council of Islamic Research in Cairo ruled that FGM had \"no basis in core Islamic law or any of its partial provisions\".\n\nThere is no mention of FGM in the Bible. Christian missionaries in Africa were among the first to object to FGM, but Christian communities in Africa do practise it. A 2013 UNICEF report identified 17 African countries in which at least 10 percent of Christian women and girls aged 15 to 49 had undergone FGM; in Niger 55 percent of Christian women and girls had experienced it, compared with two percent of their Muslim counterparts. The only Jewish group known to have practised it are the Beta Israel of Ethiopia. Judaism requires male circumcision, but does not allow FGM. FGM is also practised by animist groups, particularly in Guinea and Mali.\nThe practice's origins are unknown. Gerry Mackie has suggested that, because FGM's east-west, north-south distribution in Africa meets in Sudan, infibulation may have begun there with the Meroite civilization (c. 800 BCE – c. 350 CE), before the rise of Islam, to increase confidence in paternity. According to historian Mary Knight, Spell 1117 (c. 1991–1786 BCE) of the Ancient Egyptian Coffin Texts may refer in hieroglyphs to an uncircumcised girl (\"'m't\"):\n\na-m-a:X1-D53-B1\n\nThe spell was found on the sarcophagus of Sit-hedjhotep, now in the Egyptian Museum, and dates to Egypt's Middle Kingdom. (Paul F. O'Rourke argues that \"'m't\" probably refers instead to a menstruating woman.) The proposed circumcision of an Egyptian girl, Tathemis, is also mentioned on a Greek papyrus, from 163 BCE, in the British Museum: \"Sometime after this, Nephoris [Tathemis's mother] defrauded me, being anxious that it was time for Tathemis to be circumcised, as is the custom among the Egyptians.\"\n\nThe examination of mummies has shown no evidence of FGM. Citing the Australian pathologist Grafton Elliot Smith, who examined hundreds of mummies in the early 20th century, Knight writes that the genital area may resemble Type III because during mummification the skin of the outer labia was pulled toward the anus to cover the pudendal cleft, possibly to prevent sexual violation. It was similarly not possible to determine whether Types I or II had been performed, because soft tissues had deteriorated or been removed by the embalmers.\n\nThe Greek geographer Strabo (c. 64 BCE – c. 23 CE) wrote about FGM after visiting Egypt around 25 BCE: \"This is one of the customs most zealously pursued by them [the Egyptians]: to raise every child that is born and to circumcise [\"peritemnein\"] the males and excise [\"ektemnein\"] the females ...\" Philo of Alexandria (c. 20 BCE – 50 CE) also made reference to it: \"the Egyptians by the custom of their country circumcise the marriageable youth and maid in the fourteenth (year) of their age, when the male begins to get seed, and the female to have a menstrual flow.\" It is mentioned briefly in a work attributed to the Greek physician Galen (129 – c. 200 CE): \"When [the clitoris] sticks out to a great extent in their young women, Egyptians consider it appropriate to cut it out.\" Another Greek physician, Aëtius of Amida (mid-5th to mid-6th century CE), offered more detail in book 16 of his \"Sixteen Books on Medicine\", citing the physician Philomenes. The procedure was performed in case the clitoris, or \"nymphê\", grew too large or triggered sexual desire when rubbing against clothing. \"On this account, it seemed proper to the Egyptians to remove it before it became greatly enlarged,\" Aëtius wrote, \"especially at that time when the girls were about to be married\":\n\nThe surgery is performed in this way: Have the girl sit on a chair while a muscled young man standing behind her places his arms below the girl's thighs. Have him separate and steady her legs and whole body. Standing in front and taking hold of the clitoris with a broad-mouthed forceps in his left hand, the surgeon stretches it outward, while with the right hand, he cuts it off at the point next to the pincers of the forceps.\n\nIt is proper to let a length remain from that cut off, about the size of the membrane that's between the nostrils, so as to take away the excess material only; as I have said, the part to be removed is at that point just above the pincers of the forceps. Because the clitoris is a skinlike structure and stretches out excessively, do not cut off too much, as a urinary fistula may result from cutting such large growths too deeply.\n\nThe genital area was then cleaned with a sponge, frankincense powder and wine or cold water, and wrapped in linen bandages dipped in vinegar, until the seventh day when calamine, rose petals, date pits or a \"genital powder made from baked clay\" might be applied.\n\nWhatever the practice's origins, infibulation became linked to slavery. Mackie cites the Portuguese missionary João dos Santos, who in 1609 wrote of a group near Mogadishu who had a \"custome to sew up their Females, especially their slaves being young to make them unable for conception, which makes these slaves sell dearer, both for their chastitie, and for better confidence which their Masters put in them\". Thus, Mackie argues, a \"practice associated with shameful female slavery came to stand for honor\".\n\nGynaecologists in 19th-century Europe and the United States removed the clitoris to treat insanity and masturbation. A British doctor, Robert Thomas, suggested clitoridectomy as a cure for nymphomania in 1813.\nThe first reported clitoridectomy in the West, described in \"The Lancet\" in 1825, was performed in 1822 in Berlin by Karl Ferdinand von Graefe on a 15-year-old girl who was masturbating excessively.\n\nIsaac Baker Brown, an English gynaecologist, president of the Medical Society of London and co-founder in 1845 of St. Mary's Hospital, believed that masturbation, or \"unnatural irritation\" of the clitoris, caused hysteria, spinal irritation, fits, idiocy, mania and death. He therefore \"set to work to remove the clitoris whenever he had the opportunity of doing so\", according to his obituary. Brown performed several clitoridectomies between 1859 and 1866. In the United States, J. Marion Sims followed Brown's work and in 1862 slit the neck of a woman's uterus and amputated her clitoris, \"for the relief of the nervous or hysterical condition as recommended by Baker Brown\". When Brown published his views in \"On the Curability of Certain Forms of Insanity, Epilepsy, Catalepsy, and Hysteria in Females\" (1866), doctors in London accused him of quackery and expelled him from the Obstetrical Society.\n\nLater in the 19th century, A. J. Bloch, a surgeon in New Orleans, removed the clitoris of a two-year-old girl who was reportedly masturbating. According to a 1985 paper in the \"Obstetrical & Gynecological Survey\", clitoridectomy was performed in the United States into the 1960s to treat hysteria, erotomania and lesbianism. From the mid-1950s, James Burt, a gynaecologist in Dayton, Ohio, performed non-standard repairs of episiotomies after childbirth, adding more stitches to make the vaginal opening smaller. From 1966 until 1989, he performed \"love surgery\" by cutting women's pubococcygeus muscle, repositioning the vagina and urethra, and removing the clitoral hood, thereby making their genital area more appropriate, in his view, for intercourse in the missionary position. \"Women are structurally inadequate for intercourse,\" he wrote; he said he would turn them into \"horny little mice\". In the 1960s and 1970s he performed these procedures without consent while repairing episiotomies and performing hysterectomies and other surgery; he said he had performed a variation of them on 4,000 women by 1975. Following complaints, he was required in 1989 to stop practicing medicine in the United States.\n\nProtestant missionaries in British East Africa (present-day Kenya) began campaigning against FGM in the early 20th century, when Dr. John Arthur joined the Church of Scotland Mission (CSM) in Kikuyu. An important ethnic marker, the practice was known by the Kikuyu, the country's main ethnic group, as \"irua\" for both girls and boys. It involved excision (Type II) for girls and removal of the foreskin for boys. Unexcised Kikuyu women (\"irugu\") were outcasts.\n\nJomo Kenyatta, general secretary of the Kikuyu Central Association and later Kenya's first prime minister, wrote in 1938 that, for the Kikuyu, the institution of FGM was the \"\"conditio sine qua non\" of the whole teaching of tribal law, religion and morality\". No proper Kikuyu man or woman would marry or have sexual relations with someone who was not circumcised. A woman's responsibilities toward the tribe began with her initiation. Her age and place within tribal history was traced to that day, and the group of girls with whom she was cut was named according to current events, an oral tradition that allowed the Kikuyu to track people and events going back hundreds of years.\n\nBeginning with the CSM mission in 1925, several missionary churches declared that FGM was prohibited for African Christians. The CSM announced that Africans practising it would be excommunicated, which resulted in hundreds leaving or being expelled. The stand-off turned FGM into a focal point of the Kenyan independence movement; the 1929–1931 period is known in the country's historiography as the female circumcision controversy.\n\nIn 1929 the Kenya Missionary Council began referring to FGM as the \"sexual mutilation of women\", rather than circumcision, and a person's stance toward the practice became a test of loyalty, either to the Christian churches or to the Kikuyu Central Association. Hulda Stumpf, an American missionary with the Africa Inland Mission who opposed FGM in the girls' school she helped to run, was murdered in 1930. Edward Grigg, the governor of Kenya, told the British Colonial Office that the killer, who was never identified, had tried to circumcise her.\n\nIn 1956 the council of male elders (the \"Njuri Nchecke\") in Meru announced a ban on FGM. Over the next three years, thousands of girls cut each other's genitals with razor blades as a symbol of defiance. The movement came to be known as \"Ngaitana\" (\"I will circumcise myself\"), because to avoid naming their friends the girls said they had cut themselves. Historian Lynn Thomas described the episode as significant in the history of FGM because it made clear that its victims were also its perpetrators.\n\nThe first known non-colonial campaign against FGM began in Egypt in the 1920s, when the Egyptian Doctors' Society called for a ban. There was a parallel campaign in Sudan, run by religious leaders and British women. Infibulation was banned there in 1946, but the law was unpopular and barely enforced. The Egyptian government banned infibulation in state-run hospitals in 1959, but allowed partial clitoridectomy if parents requested it. (Egypt banned FGM entirely in 2007.)\n\nIn 1959, the UN asked the WHO to investigate FGM, but the latter responded that it was not a medical matter. Feminists took up the issue throughout the 1970s. The Egyptian physician and feminist Nawal El Saadawi criticized FGM in her book \"Women and Sex\" (1972); the book was banned in Egypt and El Saadawi lost her job as director general of public health. She followed up with a chapter, \"The Circumcision of Girls\", in her book \"The Hidden Face of Eve: Women in the Arab World\" (1980), which described her own clitoridectomy when she was six years old:\n\nI did not know what they had cut off from my body, and I did not try to find out. I just wept, and called out to my mother for help. But the worst shock of all was when I looked around and found her standing by my side. Yes, it was her, I could not be mistaken, in flesh and blood, right in the midst of these strangers, talking to them and smiling at them, as though they had not participated in slaughtering her daughter just a few moments ago.\nIn 1975, Rose Oldfield Hayes, an American social scientist, became the first female academic to publish a detailed account of FGM, aided by her ability to discuss it directly with women in Sudan. Her article in \"American Ethnologist\" called it \"female genital mutilation\", rather than female circumcision, and brought it to wider academic attention. Edna Adan Ismail, who worked at the time for the Somalia Ministry of Health, discussed the health consequences of FGM in 1977 with the Somali Women's Democratic Organization. Two years later Fran Hosken, an Austria-American feminist, published \"The Hosken Report: Genital and Sexual Mutilation of Females\" (1979), the first to offer global figures. She estimated that 110,529,000 women in 20 African countries had experienced FGM. The figures were speculative but consistent with later surveys. Describing FGM as a \"training ground for male violence\", Hosken accused female practitioners of \"participating in the destruction of their own kind\". The language caused a rift between Western and African feminists; African women boycotted a session featuring Hosken during the UN's Mid-Decade Conference on Women in Copenhagen in July 1980.\n\nIn 1979, the WHO held a seminar, \"Traditional Practices Affecting the Health of Women and Children\", in Khartoum, Sudan, and in 1981, also in Khartoum, 150 academics and activists signed a pledge to fight FGM after a workshop held by the Babiker Badri Scientific Association for Women's Studies (BBSAWS), \"Female Circumcision Mutilates and Endangers Women – Combat it!\" Another BBSAWS workshop in 1984 invited the international community to write a joint statement for the United Nations. It recommended that the \"goal of all African women\" should be the eradication of FGM and that, to sever the link between FGM and religion, clitoridectomy should no longer be referred to as \"sunna\".\n\nThe Inter-African Committee on Traditional Practices Affecting the Health of Women and Children, founded in 1984 in Dakar, Senegal, called for an end to the practice, as did the UN's World Conference on Human Rights in Vienna in 1993. The conference listed FGM as a form of violence against women, marking it as a human-rights violation, rather than a medical issue. Throughout the 1990s and 2000s governments in Africa and the Middle East passed legislation banning or restricting FGM. In 2003 the African Union ratified the Maputo Protocol on the rights of women, which supported the elimination of FGM. By 2015 laws restricting FGM had been passed in at least 23 of the 27 African countries in which it is concentrated, although several fell short of a ban.\n\nIn December 1993, the United Nations General Assembly included FGM in resolution 48/104, the Declaration on the Elimination of Violence Against Women, and from 2003 sponsored International Day of Zero Tolerance for Female Genital Mutilation, held every 6 February. UNICEF began in 2003 to promote an evidence-based social norms approach, using ideas from game theory about how communities reach decisions about FGM, and building on the work of Gerry Mackie on the demise of footbinding in China. In 2005 the UNICEF Innocenti Research Centre in Florence published its first report on FGM. UNFPA and UNICEF launched a joint program in Africa in 2007 to reduce FGM by 40 percent within the 0–15 age group and eliminate it from at least one country by 2012, goals that were not met and which they later described as unrealistic. In 2008 several UN bodies recognized FGM as a human-rights violation, and in 2010 the UN called upon healthcare providers to stop carrying out the procedures, including reinfibulation after childbirth and symbolic nicking. In 2012 the General Assembly passed resolution 67/146, \"Intensifying global efforts for the elimination of female genital mutilations\".\n\nImmigration spread the practice to Australia, New Zealand, Europe and North America, all of which outlawed it entirely or restricted it to consenting adults. Sweden outlawed FGM in 1982 with the \"Act Prohibiting the Genital Mutilation of Women\", the first Western country to do so. Several former colonial powers, including Belgium, Britain, France and the Netherlands, introduced new laws or made clear that it was covered by existing legislation. legislation banning FGM had been passed in 33 countries outside Africa and the Middle East.\n\nIn the United States an estimated 513,000 women and girls had experienced FGM or were at risk as of 2012. A Nigerian woman successfully contested deportation in March 1994 on the grounds that her daughters might be cut, and in 1996 Fauziya Kasinga from Togo became the first to be granted asylum to escape FGM. In 1996 the Federal Prohibition of Female Genital Mutilation Act made it illegal to perform FGM on minors for non-medical reasons, and in 2013 the Transport for Female Genital Mutilation Act prohibited transporting a minor out of the country for the purpose of FGM. A federal judge ruled in 2018 that the 1996 Act was unconstitutional, arguing that FGM is a \"local criminal activity\" that should be regulated by states, not by Congress; he made his ruling during a case against members of the Dawoodi Bohra community in Michigan accused of carrying out FGM. Twenty-four states had legislation banning FGM as of 2016. The first FGM conviction in the US was in 2006, when Khalid Adem, who had emigrated from Ethiopia, was sentenced to ten years for aggravated battery and cruelty to children after severing his two-year-old daughter's clitoris with a pair of scissors. The American Academy of Pediatrics opposes all forms of the practice, including pricking the clitoral skin. \n\nCanada recognized FGM as a form of persecution in July 1994, when it granted refugee status to Khadra Hassan Farah, who had fled Somalia to avoid her daughter being cut. In 1997 section 268 of its Criminal Code was amended to ban FGM, except where \"the person is at least eighteen years of age and there is no resulting bodily harm\". there had been no prosecutions. Canadian officials have expressed concern that a few thousand Canadian girls are at risk of \"vacation cutting\", whereby girls are taken overseas to undergo the procedure, but as of 2017 there were no firm figures.\n\nAccording to the European Parliament, 500,000 women in Europe had undergone FGM . France is known for its tough stance against FGM. Up to 30,000 women there were thought to have experienced it as of 1995. According to Colette Gallard, a family-planning counsellor, when FGM was first encountered in France, the reaction was that Westerners ought not to intervene. It took the deaths of two girls in 1982, one of them three months old, for that attitude to change. In 1991 a French court ruled that the Convention Relating to the Status of Refugees offered protection to FGM victims; the decision followed an asylum application from Aminata Diop, who fled an FGM procedure in Mali. The practice is outlawed by several provisions of France's penal code that address bodily harm causing permanent mutilation or torture. All children under six who were born in France undergo medical examinations that include inspection of the genitals, and doctors are obliged to report FGM. The first civil suit was in 1982, and the first criminal prosecution in 1993. In 1999 a woman was given an eight-year sentence for having performed FGM on 48 girls. By 2014 over 100 parents and two practitioners had been prosecuted in over 40 criminal cases.\n\nAround 137,000 women and girls living in England and Wales were born in countries where FGM is practised, as of 2011. Performing FGM on children or adults was outlawed under the Prohibition of Female Circumcision Act 1985. This was replaced by the Female Genital Mutilation Act 2003 and Prohibition of Female Genital Mutilation (Scotland) Act 2005, which added a prohibition on arranging FGM outside the country for British citizens or permanent residents. The United Nations Committee on the Elimination of Discrimination against Women (CEDAW) asked the government in July 2013 to \"ensure the full implementation of its legislation on FGM\". The first charges were brought in 2014 against a physician and another man; the physician had stitched an infibulated woman after opening her for childbirth. Both men were acquitted in 2015.\n\nAnthropologists have accused FGM eradicationists of cultural colonialism, and have been criticized in turn for their moral relativism and failure to defend the idea of universal human rights. According to critics of the eradicationist position, the biological reductionism of the opposition to FGM, and the failure to appreciate FGM's cultural context, serves to \"other\" practitioners and undermine their agency—in particular when parents are referred to as \"mutilators\".\n\nAfricans who object to the tone of FGM opposition risk appearing to defend the practice. The feminist theorist Obioma Nnaemeka, herself strongly opposed to FGM, argued in 2005 that renaming the practice \"female genital mutilation\" had introduced \"a subtext of barbaric African and Muslim cultures and the West's relevance (even indispensability) in purging [it]\". According to Ugandan law professor Sylvia Tamale, the early Western opposition to FGM stemmed from a Judeo-Christian judgment that African sexual and family practices, including not only FGM but also dry sex, polygyny, bride price and levirate marriage, required correction. African feminists \"take strong exception to the imperialist, racist and dehumanising infantilization of African women\", she wrote in 2011. Commentators highlight the voyeurism in the treatment of women's bodies as exhibits. Examples include images of women's vaginas after FGM or girls undergoing the procedure. The 1996 Pulitzer-prize-winning photographs of a 16-year-old Kenyan girl experiencing FGM were published by 12 American newspapers, without her consent either to be photographed or to have the images published.\n\nThe debate has highlighted a tension between anthropology and feminism, with the former's focus on tolerance and the latter's on equal rights for women. According to the anthropologist Christine Walley, a common position within anti-FGM literature has been to present African women as victims of false consciousness participating in their own oppression, a position promoted by feminists in the 1970s and 1980s, including Fran Hosken, Mary Daly and Hanny Lightfoot-Klein. It prompted the French Association of Anthropologists to issue a statement in 1981, at the height of the early debates, that \"a certain feminism resuscitates (today) the moralistic arrogance of yesterday's colonialism\".\n\nNnaemeka argues that the crucial question, broader than FGM, is why the female body is subjected to so much \"abuse and indignity\", including in the West. Several authors have drawn a parallel between FGM and cosmetic procedures. Ronán Conroy of the Royal College of Surgeons in Ireland wrote in 2006 that cosmetic genital procedures were \"driving the advance\" of FGM by encouraging women to see natural variations as defects. Anthropologist Fadwa El Guindi compared FGM to breast enhancement, in which the maternal function of the breast becomes secondary to men's sexual pleasure. Benoîte Groult, the French feminist, made a similar point in 1975, citing FGM and cosmetic surgery as sexist and patriarchal. Against this, the medical anthropologist Carla Obermeyer argued in 1999 that FGM may be conducive to a subject's social well-being in the same way that rhinoplasty and male circumcision are. Despite the 2007 ban in Egypt, Egyptian women wanting FGM for their daughters seek \"amalyet tajmeel\" (cosmetic surgery) to remove what they see as excess genital tissue.\nCosmetic procedures such as labiaplasty and clitoral hood reduction do fall within the WHO's definition of FGM, which aims to avoid loopholes, but the WHO notes that these elective practices are generally not regarded as FGM. Some legislation banning FGM, such as in Canada and the US, covers minors only, but several countries, including Sweden and the UK, have banned it regardless of consent. Sweden, for example, has banned operations \"on the outer female sexual organs with a view to mutilating them or bringing about some other permanent change in them, regardless of whether or not consent has been given for the operation\". Gynaecologist Birgitta Essén and anthropologist Sara Johnsdotter argue that the law seems to distinguish between Western and African genitals, and deems only African women (such as those seeking reinfibulation after childbirth) unfit to make their own decisions.\n\nThe philosopher Martha Nussbaum argues that a key concern with FGM is that it is mostly conducted on children using physical force. The distinction between social pressure and physical force is morally and legally salient, comparable to the distinction between seduction and rape. She argues further that the literacy of women in practising countries is generally poorer than in developed nations, which reduces their ability to make informed choices.\n\nSeveral commentators maintain that children's rights are violated not only by FGM but also by the genital alteration of intersex children, who are born with anomalies that physicians choose to correct. Arguments have been made that non-therapeutic male circumcision, practised by Muslims, Jews and some Christian groups, also violates children's rights. Globally about 30 percent of males over 15 are circumcised; of these, about two-thirds are Muslim. An American Academy of Pediatrics circumcision task force issued a policy statement in 2012 that the health benefits of male circumcision outweigh the risks; they recommended that it be carried out, if it is performed, by \"trained and competent practitioners ... using sterile techniques and effective pain management\". The statement met with protests from a group of 38 doctors in Europe, who accused the task force of \"cultural bias\". At least half the male population of the United States is circumcised, while most men in Europe are not.\n\n\nBooks and book chapters\n\nJournal articles\nUnited Nations reports\n\nPersonal stories\n"}
{"id": "305126", "url": "https://en.wikipedia.org/wiki?curid=305126", "title": "Field of view", "text": "Field of view\n\nThe field of view (FoV) is the extent of the observable world that is seen at any given moment. In the case of optical instruments or sensors it is a solid angle through which a detector is sensitive to electromagnetic radiation.\n\nIn the context of human vision, the term \"field of view\" is typically only used in the sense of a restriction to what is visible by external apparatus, like when wearing spectacles or virtual reality goggles. Note that eye movements are allowed in the definition but do not change the field of view.\n\nIf the analogy of the eye's retina working as a sensor is drawn upon, the corresponding concept in human (and much of animal vision) is the visual field. It is defined as \"the number of degrees of visual angle during stable fixation of the eyes\". Note that eye movements are excluded in the definition. Different animals have different visual fields, depending, among others, on the placement of the eyes. Humans have a slightly over 210-degree forward-facing horizontal arc of their visual field, while some birds have a complete or nearly complete 360-degree visual field. The vertical range of the visual field in humans is around 150 degrees.\n\nThe range of visual abilities is not uniform across the visual field, and varies from animal to animal. For example, binocular vision, which is the basis for stereopsis and is important for depth perception, covers 114 degrees (horizontally) of the visual field in humans; the remaining peripheral 40 degrees on each side have no binocular vision (because only one eye can see those parts of the visual field). Some birds have a scant 10 or 20 degrees of binocular vision.\n\nSimilarly, color vision and the ability to perceive shape and motion vary across the visual field; in humans color vision and form perception are concentrated in the center of the visual field, while motion perception is only slightly reduced in the periphery and thus has a relative advantage there. The physiological basis for that is the much higher concentration of color-sensitive cone cells and color-sensitive parvocellular retinal ganglion cells in the fovea – the central region of the retina, together with a larger representation in the visual cortex – in comparison to the higher concentration of color-insensitive rod cells and motion-sensitive magnocellular retinal ganglion cells in the visual periphery, and smaller cortical representation. Since cone cells require considerably brighter light sources to be activated, the result of this distribution is further that peripheral vision is much more sensitive at night relative to foveal vision (sensitivity is highest at around 20 deg eccentricity).\n\nMany optical instruments, particularly binoculars or spotting scopes, are advertised with their field of view specified in one of two ways: angular field of view, and linear field of view. Angular field of view is typically specified in degrees, while linear field of view is a ratio of lengths. For example, binoculars with a 5.8 degree (angular) field of view might be advertised as having a (linear) field of view of 102 mm per meter. As long as the FOV is less than about 10 degrees or so, the following approximation formulas allow one to convert between linear and angular field of view. Let formula_1 be the angular field of view in degrees. Let formula_2 be the linear field of view in millimeters per meter. Then, using the small-angle approximation:\n\nIn machine vision the lens focal length and image sensor size sets up the fixed relationship between the field of view and the working distance. Field of view is the area of the inspection captured on the camera’s imager. The size of the field of view and the size of the camera’s imager directly affect the image resolution (one determining factor in accuracy). Working distance is the distance between the back of the lens and the target object.\n\nIn remote sensing, the solid angle through which a detector element (a pixel sensor) is sensitive to electromagnetic radiation at any one time, is called \"instantaneous field of view\" or IFOV. A measure of the spatial resolution of a remote sensing imaging system, it is often expressed as dimensions of visible ground area, for some known sensor altitude. Single pixel IFOV is closely related to concept of \"resolved pixel size\", ground resolved distance, ground sample distance and modulation transfer function.\n\nIn astronomy, the field of view is usually expressed as an angular area viewed by the instrument, in square degrees, or for higher magnification instruments, in square arc-minutes. For reference the Wide Field Channel on the Advanced Camera for Surveys on the Hubble Space Telescope has a field of view of 10 sq. arc-minutes, and the High Resolution Channel of the same instrument has a field of view of 0.15 sq. arc-minutes. Ground-based survey telescopes have much wider fields of view. The photographic plates used by the UK Schmidt Telescope had a field of view of 30 sq. degrees. The 1.8 m (71 in) Pan-STARRS telescope, with the most advanced digital camera to date has a field of view of 7 sq. degrees. In the near infra-red WFCAM on UKIRT has a field of view of 0.2 sq. degrees and the VISTA telescope has a field of view of 0.6 sq. degrees. Until recently digital cameras could only cover a small field of view compared to photographic plates, although they beat photographic plates in quantum efficiency, linearity and dynamic range, as well as being much easier to process.\n\nIn photography, the field of view is that part of the world that is visible through the camera at a particular position and orientation in space; objects outside the FOV when the picture is taken are not recorded in the photograph. It is most often expressed as the angular size of the view cone, as an angle of view. For a normal lens, the diagonal field of view can be calculated FOV = 2 arctan(SensorSize/2f), where \"f\" is focal length.\n\nThe field of view in video games refers to the field of view of the camera looking at the game world, which is dependent on the scaling method used.\n"}
{"id": "25259540", "url": "https://en.wikipedia.org/wiki?curid=25259540", "title": "Freezing behavior", "text": "Freezing behavior\n\nFreezing behavior or the freeze response is a reaction to specific stimuli, most commonly observed in prey animals. When a prey animal has been caught and completely overcome by the predator, it may still be possible for the prey to escape by feigning death so that the predator stops the attack. Studies typically assess a conditioned freezing behavior response to stimuli that typically or innately do not cause fear, such as a tone or shock. Freezing behavior is most easily characterized by changes in blood pressure and lengths of time in crouching position, but it also is known to cause changes such as shortness of breath, increased heart rate, sweating, or choking sensation. However, since it is difficult to measure these sympathetic responses to fear stimuli, studies are typically confined to simple crouching times. A response to stimuli typically is said to be a \"fight or flight\", but is more completely described as \"fight, flight, or freeze.\" In addition, freezing is observed to occur before or after a fight or flight response.\n\nStudies suggest that specific areas of the brain are known to either elicit (or inhibit in the case of lesions) freezing behavior in subjects. The regions include the basolateral amygdala and the hippocampus.\n\nOne such study, conducted by Ann E. Power et al., investigated the effects of lesions in the basolateral amygdala. Rats were placed in a chamber containing either real cat hair or fake cat hair. Two groups of rats were tested: rats that had been lesioned in the basolateral amygdala and rats that were the control group (or sham-operated group). All rats at first froze briefly then retreated away from the stimulus upon initial contact. The results showed that the rats that were lesioned in the basolateral amygdala froze much less to the cat hair than the control group of rats. As expected, both groups of rats froze for a significantly less time when presented with the fake cat hair stimulus than when in the presence of the real cat hair. It was also shown that the both control group and the lesioned group made less contacts with the real cat hair than the fake cat hair. These data infer a connection between the basolateral amygdala and freezing behavior.\n\nAnother study, conducted by Gisquet-Verrier et al., tested the effects of the hippocampus, in three experiments, on both the freezing behavior and avoidance. The rats were lesioned with ibotenic acid, and were tested against a control group. They first investigated changes from conditioned fear, and results showed that lesions to the hippocampus did not alter freezing behavior and marginally affected avoidance. Next, they tested single conditioning sessions, and it was found that freezing behavior remained unchanged while avoidance was disrupted. Finally, they tested conditioning with a larger stimulus (footshock intensity). It was found that avoidance was unaltered while freezing behavior decreased. Not only did these investigations show that the hippocampus is involved with freezing behavior, but avoidance and freezing behavior do not seem to have similar ways of being quantified when it comes to fear conditioning.\n\nIt has been experimentally tested that particular areas of the brain are involved with freezing behavior. As mentioned before, Ann E. Power investigated the effect of basolateral amygdala on freezing behavior. It was also found that muscarinic cholinergic activation plays a role in the behavior. That suggests that neurotransmitters, in general, play a role in freezing behavior. Several investigations show that freezing behavior is influenced by the following:\n\nHashimoto et al. investigated the effects of conditioned fear on serotonin and freezing behavior in rats. Through in vivo microdialysis, certain concentrations of extracellular serotonin in the rat brain were able to be measured. It was found that conditioned fear stress increased the levels of the serotonin in the medial prefrontal cortex. This increase was correlated with an increased freezing behavior that was observed. The rats were then given an inhibitor for the extracellular serotonin, which resulted in a reduced freezing behavior. It can be suggested from these results that inhibition of serotonin can decrease freezing behavior and, also, anxiety.\n\nNot only does serotonin influence freezing behavior, but it has been shown that antipsychotic drugs(APDs), such as clozapine, ORG5222, and olanzapine, affect freezing behavior as well. Drugs were administered subcutaneously to rats 30 minutes before footshock stress. It was observed that, 24 hours following the footshock, freezing behavior was present without shocks. This is interesting to note, since there was a sympathetic response to no stimuli at all. This suggests that antipsychotic drugs alter freezing behavior, making the rats more sensitive to fear stimulus, for example.\n\nMethamphetamines have also shown that they could affect freezing behavior. Tsuchiya et al. conducted a study investigating the effect of methamphetamine pretreatment on freezing behavior. Rats were given the drug over a week, ramping up the doses. After that, there was a 5-day period without any drugs administered. The rats were then subjected to conditioned fear stress. Repeated but not single methamphetamine pretreatment resulted in a significantly increased freezing behavior. This evidence suggests that previous exposure to chronic methamphetamine results in an increased sensitivity to subsequent stress than a control group.\n\nJust as neurotransmitters influence freezing behavior, inhibitors, as expected, interrupt neurotransmitters and influence freezing behavior. This study examined the effects of monoamine oxidase inhibitors on freezing behavior. Rats were treated with specific inhibitors that target either monoamine oxidase A or B. The results showed that acute inhibition of both monoamine oxidase A and B reduce anxiety or freezing behavior. However, inhibition of monoamine oxidase A or B alone failed to do so.\n\nIt has been shown that parts of the brain are involved in freezing behavior and that neurotransmitters and similar chemicals influence freezing behavior, as well. In a related manner, hormones, progestogens and estrogen, also play a role in freezing behavior. First, the authors tested the rats in marble burying and conditioned fear when they were in behavioral estrous or diestrous. Female rats in behavioral estrous have elevated levels of these steroid hormones and also elicit more approach and less freezing behavior than diestrous rats. Results demonstrate that rats in this behavioral estrous show less impulsive burying and also less freezing behavior than diestrous rats. The authors then administered progesterone and estrogen in ovariectomized rats and tested them in marble burying and conditioned fear. The results for this experiment demonstrate that administration of progesterone or both estrogen and progesterone decreases impulsive burying. Both demonstrate a decrease in freezing behavior. The study concludes that \"progesterone and/or estrogen may mediate impulsive and/or avoidant behavior.\" Freezing behavior in a female's cycle is seen to be greatly impacted by levels of hormones. However, there may be future studies on whether testosterone influences freezing behavior as well.\n"}
{"id": "13597416", "url": "https://en.wikipedia.org/wiki?curid=13597416", "title": "HIV/AIDS in Uganda", "text": "HIV/AIDS in Uganda\n\nThe very high rate of HIV infection experienced in Uganda during the 1980s and early 1990s created an urgent need for people to know their HIV status. The only option available to them was offered by the National Blood Transfusion Service, which carries out routine HIV tests on all the blood that is donated for transfusion purposes. Because the need for testing and counseling was great, a group of local non-governmental organizations such as The AIDS Support Organisation (TASO), Uganda Red Cross, Nsambya Home Care, the National Blood Bank, the Uganda Virus Research Institute together with the Ministry of Health established the AIDS Information Centre in 1990 to provide HIV testing and counseling services with the knowledge and consent of the client involved.\n\nIn Uganda, HIV/AIDS has been approached as more than a health issue and in 1992 a Multi-sectoral AIDS Control Approach was adopted. In addition, the Uganda AIDS Commission, also founded in 1992, has helped develop a national HIV/AIDS policy. A variety of approaches to AIDS education have been employed, ranging from the promotion of condom use to 'abstinence only' programmes.\n\nTo further Uganda's efforts in establishing a comprehensive HIV/AIDS program, in 2000 the MOH implemented birth practices and safe infant feeding counseling. According to the WHO, around 41,000 women received Preventing Mother To child Transmission (PMTCT) services in 2001. Uganda was the first country to open a Voluntary Counselling and Testing (VCT) clinic in Africa called AIDS Information Centre and pioneered the concept of voluntary HIV testing centers in Sub-Saharan Africa.\n\nThe Ugandan government, through President Yoweri Museveni, has promoted this as a success story in the fight against HIV and AIDS, arguing it has been the most effective national response to the pandemic in sub-Saharan Africa. Though equally there has in recent years been growing criticism that these claims are exaggerated, and that the HIV infection rate in Uganda is on the rise.\n\nThere are striking similarities with the history of HIV/AIDS response in Senegal, where an equally high-level political response was encouraged by the fact that the HIV-2 strain of the disease was discovered by the Senegalese scientist Dr. Mboup.\n\nAn overarching policy known as \"ABC\", which consisted of abstinence, monogamy, and condoms, was set up with the aim of helping to curb the spread of AIDS in Uganda, where HIV infections reached epidemic proportions in the 1980s. The prevalence of HIV began to decline in the late 1980s and continued throughout the 1990s. Between 1991 and 2007, HIV prevalence rates declined dramatically. Various claims have been made on the extent of these declines, but mathematical models estimated falls from about 15 percent in 1991 to about 6 percent in 2007.\n\nShortly after he came into office in 1986, President Museveni spearheaded a mass education campaign promoting a three-pronged AIDS prevention message: abstinence from sexual activity until marriage; monogamy within marriage; and condoms as a last resort. The message became commonly known as ABC: Abstinence, be faithful, use a condom if A and B fail. This message also addressed the high rates of concurrency in Uganda, which refers to the widespread cultural practice of maintaining two or more sexual partners at a time. Mass media campaigns also targeting this practice including the \"Zero-Grazing\" and \"Love Carefully\" public health messages in the 1990s\n\nThe government used a multi-sector approach to spread its AIDS prevention message: it developed strong relationships with government, community and religious leaders who worked with the grassroots to teach ABC. Schools incorporated the ABC message into curricula, while faith-based communities trained leaders and community workers in ABC. The government also launched an aggressive media campaign using print, billboards, radio, and television to promote abstinence, monogamy, and condom use.\n\nCondoms were not the main element of the AIDS prevention message in the early years. President Museveni said, \"We are being told that only a thin piece of rubber stands between us and the death of our Continent ... they (condoms) cannot become the main means of stemming the tide of AIDS.\" He emphasized that condoms should be used, \"if you cannot manage A and B ... as a fallback position, as a means of last resort.\"\n\nSome reports suggest that the decline in AIDS prevalence in Uganda was due to monogamy and abstinence, rather than condom use. According to Edward C. Green, a medical anthropologist at the Harvard School of Public Health, the promotion of fidelity to one's partner and abstinence were the most important factors in Uganda's success because they disrupted the widespread practice of having multiple concurrent sexual partners. A 2004 study published in the journal \"Science\" also concluded that abstinence among young people and monogamy, rather than condom use, contributed to the decline of AIDS in Uganda.\n\nHowever, a field-study conducted in Rakai, a region in southern Uganda, showed that abstinence and fidelity rates had been declining during 1995–2002, but without the expected rise in HIV/AIDS rates, suggesting a greater role for condoms than acknowledged by Museveni. The other central finding of the Rakai study was that, due to Uganda's focus on prevention of the spread of HIV-AIDS, rather than treatment for those who had already contracted the disease, a large part of the decline in prevalence of HIV-AIDS is due to the premature death of those who have contracted it. This led to the popular play on the ABC campaign, 'A-B-C-D', with the D standing for Death. Because only prevalence is measured, incidence can actually increase while prevalence decreases if those who contract HIV are not treated for the disease, thereby dying younger. Later studies have seriously questioned the veracity of Uganda's miraculous HIV-AIDS claims .\n\nIn the 1990s there had been limited access to treatment in the form of anti-retrovirals for those who are HIV positive. Through the combined effort of US PEPFAR, the government of Uganda, and international agencies (Clinton HIV/AIDS Initiative, the Global Fund, UNITAID) this has improved. The country's HIV-AIDS campaign focuses solely on prevention rather than cure, and that prevention is of questionable success.\n\nThe scope of Uganda's success has come under scrutiny from new research. Research published in \"The Lancet\" medical journal in 2002 questions the dramatic decline reported. It is claimed statistics have been distorted through the inaccurate extrapolation of data from small urban clinics to the entire population, nearly 90% of whom live in rural areas. Also, recent trials of the HIV drug nevirapine have come under intense scrutiny and criticism.\n\nUS-sponsored abstinence promotions have received recent criticism from observers for denying young people information about any method of HIV prevention other than sexual abstinence until marriage. Human Rights Watch says that such programmes \"leave Uganda’s children at risk of HIV\". Alternatively, the Roman Catholic organization Human Life International says that \"condoms are adding to the problem, not solving it\" and that \"The government of Uganda believes its people have the human capacity to change their risky behaviors.\"\n\nIt is feared that HIV prevalence in Uganda may be rising again; at best it has reached a plateau where the number of new HIV infections matches the number of AIDS-related deaths. \nThere are many theories as to why this may be happening, including the government’s shift from abstinence-based prevention programmes, and a general complacency or 'AIDS fatigue'.\nIt has been suggested that antiretroviral drugs have changed the perception of AIDS from a death sentence to a treatable, manageable disease; this may have reduced the fear surrounding HIV, and in turn have led to an increase in risky behaviour. Although prevention interventions, like safe male circumcision, have been shown to effectively reduce HIV transmission, studies in Uganda have shown delayed uptake of these interventions and attributed this to contestations over evidence by high-level leaders.\n\nAlthough abstinence has always been part of the country’s prevention strategy it has come under scrutiny since 2003 following significant investment of money for abstinence-only programmes from PEPFAR, the American government’s initiative to combat the global HIV/AIDS epidemic. It is felt that PEPFAR has shifted the focus of prevention in Uganda from the comprehensive ABC approach of earlier years. PEPFAR is channelling large sums of money through pro-abstinence and even anti-condom organisations that are faith-based, and believe sexual abstinence should be the central pillar of the fight against HIV. Abstinence-only is also being encouraged by evangelical churches within Uganda, and by the First Lady, Janet Museveni.\n\nThis money is making a difference – some Ugandan teachers report being instructed by US contractors not to discuss condoms in schools because the new policy is 'abstinence only'. Dozens of billboards around the country have sprung up promoting only abstinence to prevent HIV infection and sometimes discouraging condom use. Some leaders of small community-based organisations also report they are aware that they are more likely to receive money from PEPFAR (which is the largest HIV-related donor to the country) if they mention abstinence in their funding proposal.\n\nThere have been calls for a more nuanced view of Uganda's response to HIV/AIDS. There is no doubt that there has been sustained, long term political commitment at the highest levels of government on this issue. In other countries such as Zimbabwe or South Africa, inept leadership has led to a serious crisis; some such as former President Thabo Mbeki deny the link between HIV and AIDS.\n\nOne aspect of the response to HIV in Uganda bridges the Millennium Development Goals and prevention—that is vertical transmission or Prevention of Mother To Child Transmission (PMTCT). Through the Global Fund's Born HIV Free campaign BornHIVFree the need and impact of PMTCT is made clear. Funding is encouraged by UNITAID and MassiveGood\n\nThe provision of all health services in Uganda is shared between three groups: the government staffed and funded medical facilities; private for profit or self-employed medics including midwives and traditional birth attendants; and, NGO or philanthropic medical services. The international health funding and research community, such as the Global Fund for AIDS, TB and Malaria, or bilateral donors are very active in Uganda. Part of the success in managing HIV/AIDS in Uganda has been due to the cooperation between the government and the non-government service providers and these international bodies. Public Private Partnerships in Health are often mentioned in Europe and North America to fund construction or research. In Uganda, it is more practical being the recognition by the (public) government and (public) donor that a (private) philanthropic health facility can receive free test kits for HIV screening, free mosquito nets and water purification to reduce opportunistic infections and free testing and treatment for basic infections of great danger to PLHA.\n\nSeveral studies, conducted in Uganda and its neighbors, indicate that adult male circumcision may be a cost-effective means of reducing HIV infection. A 2007 review of studies about the acceptability of adult male circumcision indicated the median proportion of uncircumcised men willing to become circumcised was 65 percent (range 29–87 percent). Sixty nine percent (range 47–79 percent) of women favored circumcision for their partners, and 71 percent (range 50–90 percent) of men and 81 percent (range 70–90 percent) of women were willing to circumcise their sons. The national AIDS Indicator survey in 2011 also indicated that over 48 percent of adult men were willing to be circumcised, generating a critical mass of demand for male circumcision.\n\nAn economic analysis by Bertran Auvert, a physiciann from the INSERM U687, Saint-Maurive, France, and colleagues estimated the cost of a roll-out over an initial 5-year period would be $1036 million ($748 – $1319 million) and $965 million ($763 – $1301 million) for private and public health sectors, respectively. The cumulative net cost over the first 10 years was estimated at $1271 million and $173 million for the private and public sectors, respectively.\n\n"}
{"id": "508158", "url": "https://en.wikipedia.org/wiki?curid=508158", "title": "Hesy-Ra", "text": "Hesy-Ra\n\nHesy-Ra (also read Hesy-Re and Hesire) was an Ancient Egyptian high official during the early 3rd dynasty. His most notable title was \"Wer-ibeh-senjw\", meaning either \"Great one of the ivory cutters\" or \"Great one of the dentists\", which would make him the earliest dentist whose name is known to us. His tomb is noted for its paintings and cedar wood panels.\n\nThanks to several clay seal impressions found in Hesy-Ra's tomb, it is today known that this high official lived and worked during the reign of king (pharaoh) Djoser and maybe also under king Sekhemkhet.\n\nHesy-Ra's name is of some interest to Egyptologists and Historians alike, because it is linked to the sun god Re. Hesy-Ra, alongside a few high officials at this time, belongs to the first high officials that were allowed to link their names to Re. However, they were not allowed to use the sun disk hieroglyph to write Re's name. This was permitted to the king only.\nAs a high-ranking official and priest, Hesy-Ra bore several elite and pious titularies:\n\nHesy-Ra is well known for certain, unique titles. The most discussed title is \"Wer-ibeh-senjw\", which can be translated in many ways. \"Ibeh\" can be translated as \"dentition\" and/or \"ivory\" as well. \"Senjw\" is a plural for \"arrows\", \"cutters\" and/or \"physicians\" alike. Thus, the full title \"Wer-ibeh-senjw\" can either be translated as \"Great one of the ivory cutters\" or as \"Great one of the dentists\". If the former translation was correct, Hesy-Ra was a professional ivory-cutter and artist - a profession that was fairly common and already attested in early dynastic inscriptions. If the latter translation was correct, Hesy-Ra would be the very first person in Egyptian history to be officially entitled as an occupational dentist.\n\nHesy-Ra is also well known for his richly decorated cedar wood panels found in his tomb. On these panels, Hesy-Ra is depicted in several stages of age. Indeed, the panels close to the entrance show Hesy-Ra as a pretty young man at the start of his career. Closer to the remembrance chapel, Hesy-Ra is depicted as a mid-age man at the heyday of his career. Finally, in the remembrance chapel, he is depicted as an old man, sitting on an offering table and being stuffed in a tight gown. The artist of the panels even accentuated facial mannerisms of age: Hesy-Ra's face change from pretty smooth to wrinkled and saggy, depending on the stage of age that was meant to be depicted.\nFurthermore, Hesy-Ra is known for the colorful wall paintings discovered inside and outside his tomb. Colors such as black, white, yellow, green and red were used. The ornaments include rhomboids, stripes and a green-yellowish reed mat imitation. The paintings were in such good state when found, that the excavators decided to fill the painted corridors with high quality rubble in attempt to preserve the colors. Close-by reliefs depict daily life goods and even game accessories, such as Mehen game boards and a Senet play set.\n\nPossible contemporary office partners included \"Netjeraperef\", \"Akhetaa\", \"Khabawsokar\", \"Pehernefer\" and \"Metjen\", who were also holding office under Huni and Sneferu. All their tomb inscriptions reveal that the time of both kings must have been a very prosperous one and economy and office administration flourished.\n\nHesy-Ra's tomb, mastaba \"S-2405\", is situated in Saqqara; it was discovered in 1861 by French archaeologists Auguste Mariette and Jacques de Morgan. Excavations started in 1910 and ended in 1912, organized and performed by British archaeologist James Edward Quibell. Hesy-Ra's tomb is squeezed in between dozens of others, approximately 260 m north-east of king Djoser's pyramid complex. In its original state, the mastaba was 43 m long, 22 m wide and 5 m high. It was made of hardened mud bricks. Inner and outer walls were once completely and smoothly covered with white limestone. The inner room structure consisted of a long, niched corridor and several rooms and chapels.\n"}
{"id": "48888279", "url": "https://en.wikipedia.org/wiki?curid=48888279", "title": "Human milk oligosaccharide", "text": "Human milk oligosaccharide\n\nHuman milk oligosaccharides (HMO, also known as human milk glycans) are sugar molecules, that are part of the oligosaccharides group and which can be found in high concentrations exclusively in human breast milk. \n\nHuman milk oligosaccharides form the third most abundant solid component (dissolved or emulsified or suspended in water) of human milk after lactose and fat. HMOs are present in a concentration of 0.35 - 0.88 oz / L. Approximately 200 structurally different human milk oligosaccharides are known. The composition of human milk oligosaccharides in breast milk is individual to each mother and varies over the period of lactation. The dominant oligosaccharide in 80% of all women is 2'-fucosyllactose, which is present in human breast milk at a concentration of approximately 0.088 oz/ L.\n\nIn contrast to the other components of breast milk that are absorbed by the infant through breastfeeding, HMOs are indigestible for the newborn child. However, they have a prebiotic effect and serve as food for intestinal bacteria, especially bifidobacteria. The dominance of these intestinal bacteria in the gut reduces the colonization with pathogenic bacteria (probiosis) and thereby ensures a healthy intestinal flora (intestinal microbiome) and a reduced risk of dangerous intestinal infections.\n\nRecent studies also suggest that HMOs significantly lower the risk of viral and bacterial infections and thus diminish the chance to get diarrhoea and respiratory diseases.\n\nThis protective function of the HMOs is activated when in contact with specific pathogens, such as certain bacteria or viruses. These have the ability to bind themselves to the glycan receptors (receptors for long chains of connected sugar molecules on the surface of human cells) located on the surface of the intestinal cells and can thereby infect the cells of the intestinal mucosa. Researchers have discovered that HMOs mimic these glycan receptors so the pathogens bind themselves to the HMOs rather than the intestinal cells. This reduces the risk of an infection with a pathogen.\nIn addition to this, HMOs seem to influence the reaction of specific cells of the immune system in a way that reduces inflammatory responses. It is also suspected that HMOs reduce the risk of premature infants becoming infected with the potentially life-threatening disease necrotizing enterocolitis (NEC).\n\nSome of the metabolites directly affect the nervous system or the brain and can sometimes influence the development and behavior of children in the long term. There are studies that indicate certain HMOs supply the child with sialic acid residues. Sialic acid is an essential nutrient for the development of the child’s brain and mental abilities.\n\nHMOs are used as supplements in baby food to ensure a provision of babies that are not being breastfed with this important component of the human milk.\n\nIn experiments designed to test the suitability of HMOs as a prebiotic source of carbon for intestinal bacteria it was discovered that they are highly selective for a commensal bacteria known as \"Bifidobacteria longum biovar infantis\". The presence of genes unique to \"B. infantis\", including co-regulated glycosidases, and its efficiency at using HMOs as a carbon source may imply a co-evolution of HMOs and the genetic capability of select bacteria to utilize them.\n"}
{"id": "47677150", "url": "https://en.wikipedia.org/wiki?curid=47677150", "title": "Immigrant health in Australia", "text": "Immigrant health in Australia\n\nImmigration is the movement of an individual or group of peoples to a foreign country to live permanently. Since 1788, when the first British settlers arrived in Botany Bay, immigrants have travelled from all four corners of the world to establish a life in Australia. The reason for people or groups of peoples moving to Australia varies. Such reasons can be due to seeking work or even\nrefuge from third world countries. The health of immigrants entering Australia varies depending on the individual's country of origin and the circumstance of which they came, as well as their state of travel to Australia. Immigrants are known to enter Australia both legally and illegally, and this can affect one's health immensely. Once in Australia, immigrants are given the opportunity to access a high quality of healthcare services, however, the usage of these services can differ dependent on the culture and place of birth of the individual. Researchers have proven this. Australia has strict health regulations that have to be met before one is allowed access into Australia and can determine if one is granted or denied such access. The quarantine process of immigrants into Australia has been in place since 1830, starting at the North Head Quarantine Station and continues all over Australia.\n\nIn 1788, the first fleet of British immigrants established a colony in Australia. The arrival of the Europeans in the 1800s, saw those from Italy, Greece, Poland, Malta, Russia and France land on the shores of Australia. Upon embarking, it is expected that the European immigrants would have carried infectious diseases aboard the ships, and given the nature of the vessels, any disease would have spread quickly amongst the passengers. Reports by Mark Stainforth suggest that many forms of bacteria and viruses caused high levels of illness and on-board deaths amongst those European immigrants travelling to Australia.\n\nBecause of these viruses and infections that spread amongst European immigrants before reaching Australia, upon arrival, a majority of immigrants were of ill health. The immigration of European settlers introduced such bacteria and viruses to Australia.\n\n1789: Smallpox\n\n1850: Measles were reported in Australia brought over by the European.\n\n1982: HIV was detected in a European man.\n\n1900: Bubonic plague founded amongst European immigrants\n\nAccording to the 1861 Colonial Census, Chinese-born persons made up 3.4% of the Australian population, equating to approximately 38,258 Chinese in Australia after 1842 when the Chinese first settled in Australia. Some arrived in Australia hoping to escape the civil disorder of China, although the majority of Chinese migrated to Australia after hearing about the gold rush.\n\nWithin Australia, the average life expectancy is 81.7 years of age, while those individuals who are born in China have an average of 74.7 years. The Queensland Health Multicultural Services suggests that Ischaemic heart disease, cancer and cerebrovascular disease are the major causes of China-born passing in Australia. Such cancers that have been identified include nasopyarynx, lung, intestine, rectum, stomach and liver cancer, all of which are prominent amongst Chinese immigrants.\n\nAfter the 1860s, those from Afghanistan settled in Australia establishing work in outback Australia. Afghans who migrate to Australia face several disadvantages when accessing health services. Disadvantages include language barriers, a lack of translators in Australian health Services, and the incompatibility of some Australian healthcare services and procedures with Islamic beliefs. These individuals experience a sense of alienation and lack of belonging as they are put in a situation that makes them feel like outcasts compared to the rest of Australia. However, while there are clear disadvantages for Afghans in regards to health services, the Queensland Health Multicultural Services have suggested that Afghans use Australian heath services just as much as Australian-born individuals.\n\nIt is reported that the average age expectancy of an Afghan, both male and female, is 44.6 years of age. Studies have also proven that Afghanistan has the fourth highest mortality rate and second highest infant mortality in the world.\n\nThe Royal Children's Hospital in Melbourne have conducted a study that identifies the following amongst children who have immigrated to Australia.\n\nIn the 1830s, the immigrants that arrived by boat on the shores of Sydney were suspected to be carrying contagious diseases. During this period, influenza, typhoid, scarlet fever and whopping cough were all found on European ships arriving in Australia. To contain such diseases, the ships were stopped at North Head Quarantine Station where the authority would place the passengers and crew into quarantine. The Australian Government (2015) suspects that each passenger would have spent on average, 40 days in quarantine before being released into Australia, as Australian residents. Each experience within North Head varied and was dependent on class. Some immigrants were exposed to further disease, disempowerment and in some cases death. In the beginning, North Head Quarantine Station provided tents for those immigrants who came to Australia. However, in 1837, those who appeared to be healthy were those who were granted accommodation in the tents, while those who were sick were required to stay on the ship. The Australian government suggests that in the year of 1837, 295 healthy immigrants were kept ashore while those who were contagious or otherwise were kept on the ship. The North Head Quarantine Station's facilities continued to grow. However, between 1860 and 1870, the quarantine station took a turn for the worst when the world economy decelerated along with immigration. As a result, an outbreak of smallpox occurred due to insufficient maintenance of the North Head Quarantine Station. It was only in 1909, when the Commonwealth government took over North Head Quarantine, that the station attained a maximum volume of 1200 people.\n\nIn order to ensure Australia's health and safety, the Government is required to examine all immigrants prior to entering Australia. Those who seek either a permanent or temporary visa are part of this application. All candidates are checked for Tuberculosis, HIV and Hepatitis, Yellow fever, Polio and the Ebola virus disease (EVD). In order to be allowed access into Australia, you must be cleared of all of these diseases and deemed to be no threat to Australian communities.\n\nThe Medicare system in Australia is a publicly funded system that is designed to provide heavily subsidised costs for all Australians, including those foreign born immigrants who either have a citizenship, a working visa, a permanent visa, or one who is married to an Australian citizen.\n\nFor those who migrate to Australia, there are many disadvantages in the way of healthcare. The Australian government provides well supported healthcare for immigrants, however, those immigrants who arrive in Australia are struck by culture and communication barriers as well as a lack of knowledge when it comes to the cost of health services and knowing their rights.\n\nCultural barriers felt by immigrants include the Australians perceptions of health as well as their behaviour when sick. Such differences are determined by ones cultural heritage. It is also prominent that some immigrants are concerned that doctors will not recognise or understand their specific cultural needs. Such examples include the needs of women from the Middle East. Such women are fearful of male doctors examining them. The ABC (2015) states that \"different cultural groups have specific expectations. For example, Jehovah's Witness followers will refuse to have blood transfusions, believing it is polluting.\"\n\nFor those immigrants coming to Australia from non-English speaking backgrounds, communication barriers can influence one's health experience, whether it be negative or positive. The ABC (2015) states that non-English speaking immigrants can turn to saying 'yes' in stressful situation in order to avoid further communication with healthcare services. This can have great impacts on the individual as they may not receive the help they need as a result, or they will choose not to contact health services as they are wary of the communication barriers and believe it is easier to avoid the situation. This could result in the individual's health deteriorating as a result of lack of attention.\n\nWhen immigrants arrive in Australia, it is evident that a lack of money and understanding of the healthcare system can lead to immigrants avoiding the services. Therefore, it is vital for immigrants, upon arrival, are noted of their rights in regards to free healthcare. Those immigrants who have access to Medicare are:\n\nIt has been identified that immigrants from non-English speaking backgrounds, tend to feel less empowered, resulting in the patients becoming reticent. Non-English speak immigrants tend not to ask questions. As a result, many individuals are discouraged from visiting the services again, which can lead to the ill health of immigrants that are new to Australia.\n\nThese factors can be reduced if new arrivals to Australia are aware of their rights. All patients have the right to:\nBetween 2007 and 2008, an Australian Charter of Healthcare Rights was established by the commission. This charter provides a clear outline of the rights of those seeking healthcare, and is directed at patients, consumers, families, carers and service providers. This charter can be applied in all health settings in Australia. These include, public hospitals. general practices and other environments.\n\nSee the Australian Charter of Healthcare Rights below:\n\nA study has been undertaken, comparing the mental health of men and women in Australia, identifying the differences between Australian born and foreign born\nindividuals. To achieve the following results, a diagnosis of mental health, current depression, medical service use and use of medication have been studied.\n\nThree groups of individuals:\n\nThe study has proven that both foreign born and Australian born groups of people access mental health services at an equal rate. However, non-English speaking foreign born\nmen have demonstrated an increased risk of mental health issues and access health services less than Australian born men. These results suggest that the government and health services need to be made aware of these particular health issues among non-English speaking foreign born men, in particular those men who unmarried, unemployed and live alone. The findings of this study identify the importance of social support to prevent mental health issues.\n\nWhilst most immigrants and refugees travel to Australia legally, there are others who travel to Australia illegally. Those who seek refuge in Australia illegally travel by boat, most of which are intercepted by the Royal Australian Navy patrolling the Australian border. From there, the Royal Australian Navy pass the refugees on to Australian Customs for detention. There are many risks associated with entering Australia illegally and considering the conditions of travel, the deterioration of persons on board is prevalent.\n\nMost immigrants and refugees have been exposed to a variety of infectious diseases and psychological trauma, making the health of these individuals paramount for the Health professionals working in detention centres. It is exceedingly difficult to acquire a precise medical history of the individuals considering the circumstances of which most refugees and illegal immigrants have escaped from.\n\nDependent on the individual and their country of origin, the degree of nutritional and infectious disease varies. However, it has been noted that upper and lower respiratory tract infection, parasitic and intestinal infections are frequent findings amounts illegal immigrants.\n\nWithin Australian detention centres and amongst illegal immigrants, the presence of HIV, tuberculosis and hepatitis A and B have been detected, prompting concerns for fellow detainees. However, such diseases appear to be low in Australia.\n\nWhilst the minority of illegal immigrants are females, pregnancy has been evident and therefore maternal healthcare is vital to ensure the health of both the mother and unborn child. Dental health amongst detainees is poor with the severe cases being acknowledged and attended to within the detention centres.\n\nMore than 20% of asylum seekers who seek refuge in Australia appear to have suffered as a result of torture prior to arriving in Australia. Therefore, psychiatric conditions are prevalent in detention camps. Anxiety and stress within the facilities has been emphasised by the lack of social support, unemployment and discrimination. This psychological stress felt by detainees can be enhanced by the confined environment of the detention camps. Amongst the men and women, children have also been reported for suffering prolonged psychological conditions. Whilst there are psychological impacts on the immigrants, there are also those who are affected by physical conditions such as osteomyelitis or epilepsy.\n\nIt is stated that vaccinations will not be given to those who arrive in Australia as refugees or asylum seekers due to the individual or group of people's country of origin and the different immunisation schedules that they uphold. However, it has been suggested that refugees and asylum seekers should be vaccinated according to the Australian National Immunisation Program Schedule; unless documentation of prior immunisation is provided, catch-up vaccinations are required. In this regard, it has been argued that it is necessary for immigrants to be immunised to ensure the health and safety of fellow Australians. Nevertheless, there is a competing principle to afford every immigrant and Australian citizen an equal opportunity to uphold their cultural beliefs. In this regard, some have questioned the Australian government's right to strip these refugees and asylum seekers of their cultural beliefs and understandings in regards to their health.\n\nIn 1992, mandatory detention was introduced for refugees and asylum seekers entering Australia illegally. However, the introduction of mandatory detention has triggered debate. The importance of protecting Australia's borders and integrity of Australia's immigration system was the initial reason for the policy, while those who question the policy and Australian Government's decision argue whether it is good for both parties (the refugees and Australian citizens), stating that it is inhumane and ineffective as it is proven to cause further health issues for the refugees and asylum seekers.\n\nPhilip Flood, the former Secretory of the Department of Foreign Affairs, undertook an investigation into mandatory detention. Several circumstances of self-harm, psychiatric problems and sexual, verbal and physical abuse of children were documented by Flood. The final report identifies and expresses concerns in regards to the condition of which the refugees and asylum seekers are living, as well the Department's management of long-term detention and the impact this can have on young children.\n\nThis debate against mandatory detention identifies the negatives and prolonged health issues that can arise from long-term detention. It has been recommended that once asylum seekers and refugees are cleared of the initial checks, such as those of identity and health, that they are then released into community detention or\ngranted a bridging visa whilst waiting for their refugee status to be determined. However, further studies prove that keeping such illegal immigrants in mandatory detention protects the Australian community whilst the legitimacy of the refugees and asylum seekers is being assessed.\n\nThose authorised officers within immigration detention centres are given the power to use force against asylum seekers and refugees. This is stated within the 2015 bill to maintain good order of immigration facilities. The Australian Human Rights Commission recognises the environment of particular detention facilities and force may be essential in specific circumstances. However, the Commission argues that the bill is deficient because:\n\nIt is evident that while force may be need in certain circumstances, the use of force must be appropriate and respect the immigrant's inherent right to be treated with respect. Concerns have arisen in regards to 'authorised officers' abusing the rights of the detained immigrants and therefore reinforcing the recommendation for the Bill to be altered and reworded by the Australian Human Rights Commission will be able to ensure that use of force used in immigration facilities is managed and acted out appropriately.\n\nThe particular environment of immigration detention facilities has been proven to have negative impacts on young children, compromising their mental health and physical health. The Australian Human Rights Commission has recommended that young children and their parents be removed from these facilities to ensure their health and safety. However, it has been suggested that if the Australian Government grant children and their parents access into Australian communities without undergoing the same tests and detention period as other refugees and asylum seekers, then it will be assumed that anyone can illegally enter Australia with the guarantee that children and their parents will not have to endure the detention period. While it is not suggested that the health and safety of children is not of high importance, they have still entered the country illegally and are therefore need to undergo the same tests and process period of all asylum seekers.\n\nThe issue of children in immigration detention facilities is of high importance and is therefore a controversial topic.\n\n"}
{"id": "34042734", "url": "https://en.wikipedia.org/wiki?curid=34042734", "title": "Isomaltooligosaccharide", "text": "Isomaltooligosaccharide\n\nIsomaltooligosaccharide (IMO) is a mixture of short-chain carbohydrates which has a digestion-resistant property. IMO is found naturally in some foods, as well as being manufactured commercially. The raw material used for manufacturing IMO is starch, which is enzymatically converted into a mixture of isomaltooligosaccharides.\n\nThe term \"oligosaccharide\" encompasses carbohydrates that are larger than simple di- or tri-saccharides, but smaller than polysaccharides (greater than 10 units). Isomalto-oligosaccharides (IMO) are glucose oligomers with α-D-(1,6)-linkages, including isomaltose, panose, isomaltotriose, isomaltotetraose, isomaltopentaose, nigerose, kojibiose, and higher branched oligosaccharides. While human intestinal enzymes readily digest α(1,4)-glycosidic bonds, α(1,6)-linkages are not easily hydrolyzed and exhibit a digestion-resistant property. Therefore, IMO are only partially digested in the upper gastrointestinal tract.\n\nIsomalto-oligosaccharides are a normal part of the human diet and occur naturally in fermented foods, such as rice miso, soy sauce, and sake. Isomaltose, one of the α(1,6)-linked disaccharide components of IMO, has been identified as a natural constituent of honey. IMO is a sweet-tasting, high-density syrup which could be spray-dried into powder form.\n\nFor manufacturing IMO on a commercial scale, food industries use starch processed from cereal crops like wheat, barley, pulses (peas, beans, lentils), oats, tapioca, rice, potato and others. This variety in sources could benefit consumers who have allergies or hypersensitivity to certain cereal crops. The manufacturing process controls the degree of polymerization (dp) and the α(1,6)-linkages to ensure a consistent quality of IMO from different starch sources. The starch is first converted, by means of simple enzymatic hydrolysis, into high maltose syrup with di-, tri and oligosaccharides (2, 3 or more glucose units) having α(1,4)-glycosidic linkages which are readily digestible in the human intestine. These α(1,4)-glycosidic linkages are further converted into digestion-resistant α(1,6)-glycosidic linkages, creating \"iso\" linkages between glucose moieties and forming Isomalto-oligosaccharide (IMO). \nThe majority of oligosaccharides found in IMO consist of three to six monosaccharide (glucose) units linked together. However, disaccharides, as well as longer polysaccharides (up to nine glucose units), are also present. The disaccharide fraction of IMO consists mainly of α(1,6)-linked isomaltose, while maltotriose, panose, and isomaltotriose make up the trisaccharide fraction. A mixture of isomaltotetraose, isomaltopentaose, maltohexaose, maltoheptaose, and small amounts of oligomers with 8 or more degrees of polymerization, comprise the remaining oligomers in IMO. It should be noted that longer oligomers do not have 100% α(1,6)-linkages; the ratio of α(1,4)- to α(1,6)-linkages is variable.\n\nHealth claims for the various classes of oligosaccharides have been investigated by the European Food Safety Authority (EFSA) and found to be insufficiently substantiated. Therefore, health claims for oligosaccharides and prebiotics are prohibited in the European Union.\n\nIMO is a multifunctional molecule which exerts positive effects on human digestive health; it acts as a prebiotic, decreases flatulence, has a low glycemic index, and prevents dental caries.\n\nPrebiotics are defined as \"non-digestible food ingredients that may beneficially affect the host by selectively stimulating the growth and/or activity of a limited number of bacteria in the colon\". Oligosaccharides that are not digested and absorbed in the small intestine, pass through to the colon where they are fermented by Bifidobacteria, thus enhancing the proliferation of the bacteria. In this respect, fermentable oligosaccharides may be considered prebiotics. The oligosaccharides in IMO mixtures are, at least partially, fermented by bacteria in the colon and may, therefore, stimulate the growth of bacterial subpopulations.\n\nShort chain oligosaccharides which confer prebiotic properties also produce short-chain fatty acids (like acetate, propionate and butyrate) as end-products of fermentation. These molecules decrease the intra-luminal pH, directly inhibiting the growth and activity of harmful micro-organisms (enteropathogens). This stimulates the growth of Bifidobacteria, which compete with the enteropathogens for nutrients and epithelial adhesion sites. The beneficial effects of IMO have been found in infants, children, and the elderly.\n\nDental caries is caused by the formation of insoluble glucan (plaque) on the surface of teeth, and the production of acids by bacteria in the plaque. These acids attack the hard tissues of the teeth. Studies with animal models showed that IMO, in place of sucrose, reduces the amount of plaque formed and also reduces the amount of enamel-attacking acids formed. Therefore, IMO acts as an anti-caries agent\n\nThe reported Glycemic Index (GI) for IMO is 34.66±7.65 (on a scale of 1–100) which represents a low GI. Consumption of IMO effectively improved bowel movements, stool output and microbial fermentation in the colon without any adverse effects in elderly people.\n\nThe American Association of Cereal Chemists (AACC) defines soluble fiber as \"the edible parts of plants or similar carbohydrates resistant to digestion and absorption in the human small intestine with complete or partial fermentation in the large intestine\". Dietary fiber consists of many plant components including oligosaccharides. For a dietary substrate to be classified as a fiber, it must be resistant to digestion and absorption in upper GI tract, and cause a bulking effect in defecation. IMO is considered a dietary fiber for the following reasons: it consists of glucose units linked together (mostly) by digestion-resistant linkages; it has a prebiotic effect; it retains moisture, producing a bulking effect and helping to move the stool forward.\n\nIMO is finding global acceptance by food manufacturers for use in a wide range of food products, especially beverages and snack/nutrition bars. In the United States, IMO is used mostly as a source of dietary fiber. However, IMO is also used as a low calorie sweetener in a variety of foods like bakery and cereal products. Since IMO is about 50% as sweet as sucrose (sugar), it cannot replace sugar in a one-to-one ratio. However, IMO has few side effects compared to other oligosaccharides of the same class. Therefore this carbohydrate molecule is receiving growing attention by food manufacturers across North America, as well as in Europe.\n\nGenerally, all digestion-resistant oligosaccharides, including IMO, have adverse side effects when consumed in amounts greater than permissible levels. The maximum permissible dose of IMO is 1.5 g/kg body weight, which is higher than for any other sugar substitute. However, the U.S. Food and Drug Administration (FDA) has recommended a maximum consumption of 30 g/day for IMO. Higher dosages (greater than 40 g/day), can cause gastrointestinal symptoms like flatulence, bloating, soft stool or diarrhea.\n\nIMO and other oligosaccharides have long been approved in China and Japan. In Japan, IMO is on the list of Foods for Specified Health Use (FOSHU) for more than 10 years. In 2002, over 50% of the FOSHU foods in Japan incorporated oligosaccharides as the functional component. The list includes many types of foods: soft drinks and other beverages, frozen yogurt, confectionery products, sweeteners, cookies, coffee drink mixes, bread, tofu, chocolate, and soup mixes. IMO has been imported into the United States for the last few years but has never been manufactured there or formally approved by the FDA. In 2009, a Canadian-based company, BioNeutra, received FDA-GRAS and Health Canada approval for IMO. The European Food Safety Agency (EFSA) recently authorized the sale of IMO in European countries.\n\nIMO is commercially manufactured mostly in China and Japan. However, most of this product is consumed locally or exported to neighboring Asian countries. In Japan, Meiji Dairies (Meiji Food Company) is one of the biggest IMO producers. IMO is marketed under several trade names like IMO-900 and IMO-800. Being a novel food ingredient, there wasn't a producer of IMO in North America and Europe until recently when BioNeutra North America, Inc. began to manufacture this product with the VitaFiber IMO trademark. US-based companies have been in producing other kinds of oligosaccharides, like GOS, FOS, and XOS.\n\n"}
{"id": "22016788", "url": "https://en.wikipedia.org/wiki?curid=22016788", "title": "Janell Moon", "text": "Janell Moon\n\nJanell Moon is an American author of spiritual non-fiction and poetry. In 2002, her book \"The Wise Earth Speaks to Your Spirit\" was voted one of the year's best spiritual books by Spirituality and Health Magazine.\n\nHer poetry awards include the National Main Street Rag Poetry Contest, The Stonewall Prize from the Chestnut Hills Press, the Salt Hill National Prize from Syracuse University in New York, the Whiskey Hill Award, the Georgia State University Randall Jared Award, the Billie Murray Denny Poetry Award, The Red Rock Review Prize, The Villa Montalvo Poetry Prize, the Gertrude Award, Comstock Poetry Award, and the Poet Lore Award.\n\nShe is also a hypnotherapist.\n\n\n"}
{"id": "46896553", "url": "https://en.wikipedia.org/wiki?curid=46896553", "title": "Journal of Women's Health", "text": "Journal of Women's Health\n\nThe Journal of Women's Health is a monthly peer-reviewed healthcare journal focusing on women's health care, including advancements in diagnostic procedures, therapeutic protocols for the management of diseases, and research in gender-based biology that impacts patient care and treatment. The journal was established in 1992 and is published by Mary Ann Liebert, Inc.. The editor-in-chief is Susan G. Kornstein (Virginia Commonwealth University). It is the official journal of the Academy of Women's Health and the American Medical Women's Association.\n\nThe journal is abstracted and indexed in:\n\nAccording to \"Journal Citation Reports\", the journal has a 201 impact factor of 2.322, ranking it 3rd out of 41 journals in the category \"Women's Studies\".\n\n\n"}
{"id": "8497545", "url": "https://en.wikipedia.org/wiki?curid=8497545", "title": "Kelly James", "text": "Kelly James\n\nJeffrey Kelly James (February 2, 1958 – c. December 11, 2006) was one of three experienced mountain climbers who died on Mount Hood in the U.S. state of Oregon in December 2006 in an incident which received worldwide attention.\n\nJames was a native of Dallas, Texas and a graduate of Texas Tech University in Lubbock. He was an accomplished landscape architect with his designs published in Metropolitan Home and Better Homes and Gardens. James was also a veteran mountaineer with more than 25 years experience climbing mountains including Mount McKinley, the Eiger, Alpamayo, and over 20 ascents of Mount Rainier. James resided in Dallas with his wife Karen James and four children. \nHe was also a devoted Christian.\n\nOn December 10, 2006, during his climb on Mount Hood, James made a cell phone call to his wife, Karen, and two older sons telling them that he was trapped in a snow cave and his two climbing partners Brian Hall and Jerry “Nikko” Cooke had gone for help. For more than a week, fierce weather thwarted rescue attempts. Finally on December 17, 2006, rescue workers located James’ body in a snow cave, 300 feet below the summit. Before he died of hypothermia, James removed his glove and extended his ring finger with his signet JKJ ring prominently displayed, which some have interpreted as a signal to his family.\n\nKelly's wife Karen James wrote a book \"Holding Fast: The Untold Story of the Mount Hood Tragedy\" about the accident.\n"}
{"id": "33625859", "url": "https://en.wikipedia.org/wiki?curid=33625859", "title": "Kemron", "text": "Kemron\n\nKemron is the name of a drug which was released in Kenya in 1991 and purported to be highly effective in removing the symptoms of AIDS. When put under international scrutiny, the treatment was seen to perform no better than placebo. The advent of the drug was notable for the government support and international attention it received.\n\nIn August 1990, Kenyan researcher Davy Koech, director of the Kenya Medical Research Institute, announced that when his HIV patients chewed wafers laced with tiny amounts of alpha interferon, most had greatly improved health and some cleared HIV from their blood entirely. The alpha interferon, named \"Kemron\" for this use, was a drug used to treat cancer in much greater doses.\n\nAfter reviewing the experimental data, internationally recognized AIDS experts and health officials in the United States said that there was no merit to the claim.\n\nKenyan president Daniel arap Moi announced that \"Fifty AIDS victims have already been cured\" and Koech dismissed the skepticism of Western scientists. Barbara Justice, a physician in New York, claimed that 82% of AIDS patients at the Abundant Life clinic saw increases in their appetites and \"other improvements.\"\n\nNeither the World Health Organization nor the NIH found evidence that Kemron performed better than placebo.\n\nSupport of the drug proved to be an embarrassment for many of its supporters as the drug failed to cure patients under controlled conditions.\n\nThe University of Pretoria and the Government of South Africa ultimately were not able to support the claim of the drug's efficacy.\n"}
{"id": "32611922", "url": "https://en.wikipedia.org/wiki?curid=32611922", "title": "List of United Nations Security Council Resolutions 2001 to 2100", "text": "List of United Nations Security Council Resolutions 2001 to 2100\n\nThis is a list of United Nations Security Council Resolutions 2001 to 2100 adopted between 28 July 2011 and 25 April 2013.\n"}
{"id": "30071322", "url": "https://en.wikipedia.org/wiki?curid=30071322", "title": "List of avalanches by death toll", "text": "List of avalanches by death toll\n\nThis is an incomplete list of notable avalanches.\n"}
{"id": "9621772", "url": "https://en.wikipedia.org/wiki?curid=9621772", "title": "Lithia water", "text": "Lithia water\n\nLithia water is defined as a type of mineral water characterized by the presence of lithium salts (as lithium carbonate or lithium chloride). Natural lithia mineral spring waters are rare and there are few commercially bottled lithia water products.\n\nBetween the 1880s and World War I, the consumption of bottled lithia mineral water was popular. One of the first commercially sold lithia waters in the United States was bottled at Lithia Springs, Georgia, in 1888. During this era there was such a demand for lithia water that there was a proliferation of bottled lithia water products, however only a few were natural lithia spring waters. Most of the bottled lithia water brands added lithium bicarbonate to spring water and called it lithia water. With the advent of World War I and the formation of the new US government food safety agency, mineral water bottlers were under scrutiny. The new agency posted large fines against mineral water bottlers for mislabeled, misrepresented, and adulterated products. These government actions and their publicity along with public works that made clean tap water readily accessible caused the American public to lose confidence and interest in bottled mineral water.\n\nLithia water contains various lithium salts, including the citrate. An early version of Coca-Cola available in pharmacies' soda fountains called Lithia Coke was a mixture of Coca-Cola syrup and lithia water. The soft drink 7Up was originally named \"Bib-Label Lithiated Lemon-Lime Soda\" when it was formulated in 1929 because it contained lithium citrate. The beverage was a patent medicine marketed as a cure for hangover. Lithium citrate was removed from 7Up in 1948.\n"}
{"id": "6098627", "url": "https://en.wikipedia.org/wiki?curid=6098627", "title": "Martek Biosciences Corporation", "text": "Martek Biosciences Corporation\n\nMartek Biosciences Corporation, a component of Royal DSM NV, produces nutritional supplements from cultivated fungi and microalgae. The company's products include the omega-3 fatty acid docosahexaenoic acid (DHA) and arachidonic acid (ARA), an omega-6 fatty acid. DHA is used in foods, beverages, infant formula, and supplements. The ARA is used in infant formulas. It is based in Columbia, Maryland, U.S.\n\nMartek Biosciences also produced and sold fluorescent algal proteins for use in life science research applications (microscopy, TR-FRET assays and flow cytometry). This business unit was spun off to form Columbia Biosciences in July 2007.\n\nIn early 2010, Martek announced that it is purchasing Amerifit Brands, a consumer health and wellness product company. In the spring of 2010, Martek joined with the National Center for Creative Aging, a DC-based non-profit organization, to develop a campaign called \"Beautiful Minds: Finding Your Lifelong Potential.\" The campaign is designed to inspire baby boomers to be more proactive about their brain health.\n\nDSM announced its acquisition of Martek for US$1.1 billion in December 2010.\n"}
{"id": "47843651", "url": "https://en.wikipedia.org/wiki?curid=47843651", "title": "Mental health in aviation", "text": "Mental health in aviation\n\nMental health in aviation is a major concern among airlines, regulators, and passengers. This topic gained more attention after the 2015 Germanwings crash, which was deliberately caused by the plane's copilot. There are many different causes of mental illness in pilots but, as of now, there is almost no action taken to try and combat this issue. Little data exists on mental health in aviation, but steps to gather relevant information and provide better solutions are underway. \n\nIn Brazil's General Aviation sector, 10.2% of pilots who did not exercise regularly and 23.7% of pilots who have a heavy workload, showed signs of common mental disorders.\nIt is commonly believed by the public that pilots are perfect; however this is not the case. Mental health problems are present in aviation, just like in any other industry, and more must be done to seek this out, both during the career of a pilot and at the hiring stages. It is important to have a wide variety of tests and screening processes to determine the true wellness of pilots.\n\nIn the United Kingdom, it was revealed that 350 pilots have been grounded since 2010 due to mental health issues.\n\nMental illness is second to cardiovascular disease in reasons for losing an aviation license. One major issue is that most pilots on anti-depressant drugs, for example, withhold this information from their doctor or the governing body of aviation in their country due to fear of losing their license.\nDetection is further complicated by the fact that few medical examiners fully understand the complexity and effects of the different disorders. Additionally, psychiatrists are unfamiliar with the regulations of pilots suffering from mental health issues. Even though pilots need to have their medical license renewed every 6 months by a certified medical examiner, there is little focus on mental health and no psychologist or psychiatrist follows up unless requested to do so by the pilot, which is rarely the case.\n\nAirlines are familiar with the consequences of mental health, which is why they administer personality tests during the selection process in order to identify any mental health issues. One example is the Minnesota Multiphasic Personality Inventory (MMPI). This long questionnaire can identify any at risk candidates, by asking a series of questions, worded differently, all around a similar subject. Pilots are aware of the importance of having perfect mental health. As a result, the pilots tended to be extremely defensive in their results, even more so then the average population, trying to show no signs of any mental health issues.\n\nDuring the course of a pilot's career, the prominence of mental health becomes an even bigger issue as the stresses of the occupation accumulate. Pilots are exposed to difficult working conditions that feature inconsistent schedules, extended periods away from home, and frequent encounters with fatigue.\n\nMental health is amplified by the lack of social support from home, a varying circadian rhythm, and the excessive job demands. Experts and airlines have been aware of these issues from as early as 1985, yet very little information about mental health in aviation exists. There is a macho attitude towards mental health in aviation, with a laissez-faire approach.\n\nBoth males and females are susceptible to mental health issues, however there is no greater likelihood that one gender will face more issues than the other.\n\nThere is a negative stigma around mental health in aviation. A study showed that fellow pilots assumed fellow pilots who were unsociable most likely have a mental health illness. This identifies that even factors that are not signs of mental health can stigmatize others and the negative consequences of being identified as having mental health issues, even when this is not the case.\n\nPilots and their employers must also be aware of recent life changes that may affect pilot performance and mental health characteristics. One such way is the Recent Life Change Questionnaire, which measures how susceptible someone is to change. This questionnaire identifies certain individuals who are more at risk of mental health issues and allows the airline to then provide support for them. It is crucial for airlines to develop a program to remove individuals who are in a high risk state and help them transition to a more suitable job.\nRegulators are also considering implementing random psychological screenings to pilots, however it has been suggested that this will not be foolproof in eliminating mental health issues within pilots. The Federal Aviation Administration also announced that it will conduct research into newer and more relevant data concerning mental health within pilots. It is equally important for pilots to feel open about their mental health and not be afraid of losing their career. This can be accomplished by removing the stigma associated with mental health, encourage self-reporting, and having companies work with pilots to help them find other aviation jobs and provide wellness centers.\n\nThroughout the hiring phase, pilots can be administered different personality tests to see if they are at risk of mental health issues.\n\nFollowing the Germanwings Flight 9525, both the IATA and the Civil Aviation Medical Association are looking into solutions, one being random psychological tests. The issue with a reactive instead of proactive method, is that rather than supporting pilots, it creates an even bigger stigma within the industry. The European Aviation Safety Agency also issued a similar statement, stating all pilots need to undergo psychological evaluation.\n\n"}
{"id": "23199747", "url": "https://en.wikipedia.org/wiki?curid=23199747", "title": "Nursing in Taiwan", "text": "Nursing in Taiwan\n\nNursing is a licensed profession in the Republic of China, plus additional of further nurse specialist training courses. Health law and regulation in Taiwan is overseen by the Ministry of Health and Welfare.\n\nTaiwanese nurses are segregated into three categories based on length of training, comparable to the licensed, registered, and advanced registered categories in the American system. Speciality nurses (專科學校) study for 5–7 years, normal nurses (普通大學) for 4, and inaugural nurses (在職專班) for two.\n\nUpon the cessation of training, nurses must pass an exam to be certified. After two years of practice, nurses may undergo further education in a speciality such as anaesthesia or burn care.\n\nIn Taiwan there are NP courses: emergency care, midwifery care, oncological care, orthopedic care, pediatric care, anesthesia care, outpatient care.\n\nThere are institutes that provide nursing education courses:\n\nTaiwan has more patients and clients per hour compare to United States; Taiwan focuses more on family care.\n\n\n\n\nThe Taiwan nursing act, known as Nursing Personnel Act, edited in January 2015 contained 57 articles and seven chapters.\n\n\n"}
{"id": "5854472", "url": "https://en.wikipedia.org/wiki?curid=5854472", "title": "Open Country", "text": "Open Country\n\n\"Open Country\" is a designation used for some UK access land. \n\nIt was first defined under the National Parks and Access to the Countryside Act 1949 (and extended by the Countryside Act 1968), and was land over which an appropriate access agreement had been made. In particular significant upland areas of the northern Peak District, where there had been much dispute over access prior to World War II, were so designated (see \"Mass trespass of Kinder Scout\").\n\nThe term is also used in the Countryside and Rights of Way Act 2000 to describe 'areas of mountain, moor, heath and down' that are generally available for access under that Act. (It appears that the rights conferred by this new definition are in general less comprehensive than those conferred under the 1949 Act, but will apply to a wider area.)\n\nThe Countryside Agency's publication \"Managing Public Access\" appears to envisage that most land originally designated under the 1949 Act will in due course receive redesignation under the CRoW Act, as the original access agreements lapse. \n"}
{"id": "12091162", "url": "https://en.wikipedia.org/wiki?curid=12091162", "title": "Oxford Tobacco Research Station", "text": "Oxford Tobacco Research Station\n\nThe Oxford Tobacco Research Station is a government agency conducting research on flue-cured tobacco and other crops. It is located in Oxford, North Carolina and has existed since 1911. Superintendents since 1911 have been Moss, Carr, Ayscue, Campbell, Clements, Priest and Smith. Much has been accomplished related to disease resistance and tobacco curing. Forestry, wildlife habitat and water quality studies were started during the 80's.\n\n"}
{"id": "24848067", "url": "https://en.wikipedia.org/wiki?curid=24848067", "title": "Patient Safety and Quality Improvement Act", "text": "Patient Safety and Quality Improvement Act\n\nThe Patient Safety and Quality Improvement Act of 2005 (PSQIA): , 42 U.S.C. ch. 6A subch. VII part C, established a system of patient safety organizations and a national patient safety database. To encourage reporting and broad discussion of adverse events, near misses, and dangerous conditions, it also established privilege and confidentiality protections for Patient Safety Work Product (as defined in the act). The PSQIA was introduced by Sen. Jim Jeffords [I-VT]. It passed in the Senate July 21, 2005 by unanimous consent, and passed the House of Representatives on July 27, 2005 with 428 Ayes, 3 Nays, and 2 Present/Not Voting.\n\nLexology, in cooperation with the Association of Corporate Counsel, predicts that this law will be one of the top 10 health care law issues in 2010.\n\nThe Notice of proposed rulemaking for this law describes the reason Congress passed it.\n\n\"Patient Safety Organization\" (PSO) must certify that it supports the requirements in the PSQIA and be listed on the Agency for Healthcare Research and Quality (AHRQ) web site.\n\nThe definition of \"Patient Safety Work Product\" (PSWP) is quite broad. Patient safety work product includes any data, reports, records, memoranda, analyses (such as root cause analyses), or written or oral statements (or copies of any of this material), which could improve patient safety, health care quality, or health care outcomes, that are assembled or developed by a provider for reporting to a PSO and are reported to a PSO. It also includes information that is documented as within a patient safety evaluation system that will be sent to a PSO and information developed by a PSO for the conduct of patient safety activities.\n\nHowever, patient safety work product does not include a patient's medical record, billing and discharge information, or any other original patient or provider information; nor does it include information that is collected, maintained, or developed separately, or exists separately, from a patient safety evaluation system.\n\nPatient Safety Work Product must not be disclosed, except in very specific circumstances and subject to very specific restrictions.\n\nNote: the Patient Safety Activities Exception is the most common one that providers and PSOs will be working with.\n\nPermitted Disclosures\n\n\nViolations & Enforcement\n\n\nThe Act is enforced by the Secretary of Health and Human Services\n\n\n\n\n"}
{"id": "1004486", "url": "https://en.wikipedia.org/wiki?curid=1004486", "title": "Pharmacogenomics", "text": "Pharmacogenomics\n\nPharmacogenomics is the study of the role of the genome in drug response. Its name (\"pharmaco-\" + \"genomics\") reflects its combining of pharmacology and genomics. Pharmacogenomics analyzes how the genetic makeup of an individual affects his/her response to drugs. It deals with the influence of acquired and inherited genetic variation on drug response in patients by correlating gene expression or single-nucleotide polymorphisms with pharmacokinetics (drug absorption, distribution, metabolism, and elimination) and pharmacodynamics (effects mediated through a drug's biological targets). The term \"pharmacogenomics\" is often used interchangeably with \"pharmacogenetics\". Although both terms relate to drug response based on genetic influences, pharmacogenetics focuses on single drug-gene interactions, while pharmacogenomics encompasses a more genome-wide association approach, incorporating genomics and epigenetics while dealing with the effects of multiple genes on drug response.\n\nPharmacogenomics aims to develop rational means to optimize drug therapy, with respect to the patients' genotype, to ensure maximum efficiency with minimal adverse effects. Through the utilization of pharmacogenomics, it is hoped that pharmaceutical drug treatments can deviate from what is dubbed as the \"one-dose-fits-all\" approach. Pharmacogenomics also attempts to eliminate the trial-and-error method of prescribing, allowing physicians to take into consideration their patient's genes, the functionality of these genes, and how this may affect the efficacy of the patient's current or future treatments (and where applicable, provide an explanation for the failure of past treatments). Such approaches promise the advent of precision medicine and even personalized medicine, in which drugs and drug combinations are optimized for narrow subsets of patients or even for each individual's unique genetic makeup. Whether used to explain a patient's response or lack thereof to a treatment, or act as a predictive tool, it hopes to achieve better treatment outcomes, greater efficacy, minimization of the occurrence of drug toxicities and adverse drug reactions (ADRs). For patients who have lack of therapeutic response to a treatment, alternative therapies can be prescribed that would best suit their requirements. In order to provide pharmacogenomic recommendations for a given drug, two possible types of input can be used: genotyping or exome or whole genome sequencing. Sequencing provides many more data points, including detection of mutations that prematurely terminate the synthesized protein (early stop codon).\n\nPharmacogenomics was first recognized by Pythagoras around 510 BC when he made a connection between the dangers of fava bean ingestion with hemolytic anemia and oxidative stress. This identification was later validated and attributed to deficiency of G6PD in the 1950s and called favism. Although the first official publication dates back to 1961, circa 1950s marked the unofficial beginnings of this science. Reports of prolonged paralysis and fatal reactions linked to genetic variants in patients who lacked butyryl-cholinesterase (‘pseudocholinesterase’) following administration of succinylcholine injection during anesthesia were first reported in 1956. The term pharmacogenetic was first coined in 1959 by Friedrich Vogel of Heidelberg, Germany (although some papers suggest it was 1957 or 1958). In the late 1960s, twin studies supported the inference of genetic involvement in drug metabolism, with identical twins sharing remarkable similarities to drug response compared to fraternal twins. The term pharmacogenomics first began appearing around the 1990s.\n\nThe first FDA approval of a pharmacogenetic test was in 2005 (for alleles in CYP2D6 and CYP2C19).\n\nThere are several known genes which are largely responsible for variances in drug metabolism and response. The focus of this article will remain on the genes that are more widely accepted and utilized clinically for brevity.\n\n\nThe most prevalent drug-metabolizing enzymes (DME) are the Cytochrome P450 (CYP) enzymes. The term Cytochrome P450 was coined by Omura and Sato in 1962 to describe the membrane-bound, heme-containing protein characterized by 450 nm spectral peak when complexed with carbon monoxide. The human CYP family consists of 57 genes, with 18 families and 44 subfamilies. CYP proteins are conveniently arranged into these families and subfamilies on the basis of similarities identified between the amino acid sequences. Enzymes that share 35-40% identity are assigned to the same family by an Arabic numeral, and those that share 55-70% make up a particular subfamily with a designated letter. For example, CYP2D6 refers to family 2, subfamily D, and gene number 6.\n\nFrom a clinical perspective, the most commonly tested CYPs include: CYP2D6, CYP2C19, CYP2C9, CYP3A4 and CYP3A5. These genes account for the metabolism of approximately 80-90% of currently available prescription drugs. The table below provides a summary for some of the medications that take these pathways.\n\nAlso known as debrisoquine hydroxylase (named after the drug that led to its discovery), CYP2D6 is the most well-known and extensively studied CYP gene. It is a gene of great interest also due to its highly polymorphic nature, and involvement in a high number of medication metabolisms (both as a major and minor pathway). More than 100 CYP2D6 genetic variants have been identified.\n\nDiscovered in the early 1980s, CYP2C19 is the second most extensively studied and well understood gene in pharmacogenomics. Over 28 genetic variants have been identified for CYP2C19, of which affects the metabolism of several classes of drugs, such as antidepressants and proton pump inhibitors.\n\nCYP2C9 constitutes the majority of the CYP2C subfamily, representing approximately 20% of the liver content. It is involved in the metabolism of approximately 10% of all drugs, which include medications with narrow therapeutic windows such as warfarin and tolbutamide. There are approximately 57 genetic variants associated with CYP2C9.\n\nThe CYP3A family is the most abundantly found in the liver, with CYP3A4 accounting for 29% of the liver content. These enzymes also cover between 40-50% of the current prescription drugs, with the CYP3A4 accounting for 40-45% of these medications. CYP3A5 has over 11 genetic variants identified at the time of this publication.\n\nThe vitamin K epoxide reductase complex subunit 1 (VKORC1) is responsible for the pharmacodynamics of warfarin. VKORC1 along with CYP2C9 are useful for identifying the risk of bleeding during warfarin administration. Warfarin works by inhibiting VKOR, which is encoded by the VKORC1 gene. Individuals with polymorphism in this have an affected response to warfarin treatment.\n\nThiopurine methyltransferase (TPMT) catalyzes the S-methylation of thiopurines, thereby regulating the balance between cytotoxic thioguanine nucleotide and inactive metabolites in hematopoietic cells. TPMT is highly involved in 6-MP metabolism and TMPT activity and TPMT genotype is known to affect the risk of toxicity. Excessive levels of 6-MP can cause myelosuppression and myelotoxicity.\n\nCodeine, clopidogrel, tamoxifen, and warfarin a few examples of medications that follow the above metabolic pathways.\n\nPatient genotypes are usually categorized into the following predicted phenotypes:\n\n\nThe two extremes of this spectrum are the poor metabolizers and ultra-rapid metabolizers. Efficacy of a medication is not only based on the above metabolic statuses, but also the type of drug consumed. Drugs can be classified into two main groups: active drugs and prodrugs. Active drugs refer to drugs that are inactivated during metabolism, and prodrugs are inactive until they are metabolized.\n\nFor example, we have two patients who are taking codeine for pain relief. Codeine is a prodrug, so it requires conversion from its inactive form to its active form. The active form of codeine is morphine, which provides the therapeutic effect of pain relief. If person A receives one *1 allele each from mother and father to code for the CYP2D6 gene, then that person is considered to have an extensive metabolizer (EM) phenotype, as allele *1 is considered to have a normal-function (this would be represented as CYP2D6 *1/*1). If person B on the other hand had received one *1 allele from the mother and a *4 allele from the father, that individual would be an Intermediate Metabolizer (IM) (the genotype would be CYP2D6 *1/*4). Although both individuals are taking the same dose of codeine, person B could potentially lack the therapeutic benefits of codeine due to the decreased conversion rate of codeine to its active counterpart morphine.\n\nEach phenotype is based upon the allelic variation within the individual genotype. However, several genetic events can influence a same phenotypic trait, and establishing genotype-to-phenotype relationships can thus be far from consensual with many enzymatic patterns. For instance, the influence of the CYP2D6*1/*4 allelic variant on the clinical outcome in patients treated with Tamoxifen remains debated today. In oncology, genes coding for DPD, UGT1A1, TPMT, CDA involved in the pharmacokinetics of 5-FU/capecitabine, irinotecan, 6-mercaptopurine and gemcitabine/cytarabine, respectively, have all been described as being highly polymorphic. A strong body of evidence suggests that patients affected by these genetic polymorphisms will experience severe/lethal toxicities upon drug intake, and that pre-therapeutic screening does help to reduce the risk of treatment-related toxicities through adaptive dosing strategies.\n\nThe list below provides a few more commonly known applications of pharmacogenomics:\nPharmacogenomics may be applied to several areas of medicine, including Pain Management, Cardiology, Oncology, and Psychiatry. A place may also exist in Forensic Pathology, in which pharmacogenomics can be used to determine the cause of death in drug-related deaths where no findings emerge using autopsy.\n\nIn cancer treatment, pharmacogenomics tests are used to identify which patients are most likely to respond to certain cancer drugs. In behavioral health, pharmacogenomic tests provide tools for physicians and care givers to better manage medication selection and side effect amelioration. Pharmacogenomics is also known as companion diagnostics, meaning tests being bundled with drugs. Examples include KRAS test with cetuximab and EGFR test with gefitinib. Beside efficacy, germline pharmacogenetics can help to identify patients likely to undergo severe toxicities when given cytotoxics showing impaired detoxification in relation with genetic polymorphism, such as canonical 5-FU.\n\nIn cardiovascular disorders, the main concern is response to drugs including warfarin, clopidogrel, beta blockers, and statins.\n\n\"Case A – Antipsychotic adverse reaction\"\n\nPatient A suffers from schizophrenia. Their treatment included a combination of ziprasidone, olanzapine, trazodone and benzotropine. The patient experienced dizziness and sedation, so they were tapered off ziprasidone and olanzapine, and transition to quetiapine. Trazodone was discontinued. The patient then experienced excessive sweating, tachycardia and neck pain, gained considerable weight and had hallucinations. Five months later, quetiapine was tapered and discontinued, with ziprasidone re-introduction into their treatment due to the excessive weight gain. Although the patient lost the excessive weight they gained, they then developed muscle stiffness, cogwheeling, tremor and night sweats. When benztropine was added they experienced blurry vision. After an additional five months, the patient was switched from ziprasidone to aripiprazole. Over the course of 8 months, patient A gradually experienced more weight gain, sedation, developed difficulty with their gait, stiffness, cogwheel and dyskinetic ocular movements. A pharmacogenomics test later proved the patient had a CYP2D6 *1/*41, with has a predicted phenotype of IM and CYP2C19 *1/*2 with predicted phenotype of IM as well.\n\n\"Case B – Pain Management\" \n\nPatient B is a woman who gave birth by caesarian section. Her physician prescribed codeine for post-caesarian pain. She took the standard prescribed dose, however experienced nausea and dizziness while she was taking codeine. She also noticed that her breastfed infant was lethargic and feeding poorly. When the patient mentioned these symptoms to her physician, they recommended that she discontinue codeine use. Within a few days, both the patient and her infant’s symptoms were no longer present. It is assumed that if the patient underwent a pharmacogenomic test, it would have revealed she may have had a duplication of the gene CYP2D6 placing her in the Ultra-rapid metabolizer (UM) category, explaining her ADRs to codeine use.\n\n\"Case C – FDA Warning on Codeine Overdose for Infants\"\n\nOn February 20, 2013, the FDA released a statement addressing a serious concern regarding the connection between children who are known as CYP2D6 UM and fatal reactions to codeine following tonsillectomy and/or adenoidectomy (surgery to remove the tonsils and/or adenoids). They released their strongest Boxed Warning to elucidate the dangers of CYP2D6 UMs consuming codeine. Codeine is converted to morphine by CYP2D6, and those who have UM phenotypes are at danger of producing large amounts of morphine due to the increased function of the gene. The morphine can elevate to life-threatening or fatal amounts, as became evident with the death of three children in August 2012.\n\nA potential role pharmacogenomics may play would be to reduce the occurrence of polypharmacy. It is theorized that with tailored drug treatments, patients will not have the need to take several medications that are intended to treat the same condition. In doing so, they could potentially minimize the occurrence of ADRs, have improved treatment outcomes, and can save costs by avoiding purchasing extraneous medications. An example of this can be found in psychiatry, where patients tend to be receiving more medications than even age-matched non-psychiatric patients. This has been associated with an increased risk of inappropriate prescribing.\n\nThe need for pharmacogenomics tailored drug therapies may be most evident in a survey conducted by the Slone Epidemiology Center at Boston University from February 1998 to April 2007. The study elucidated that an average of 82% of adults in the United States are taking at least one medication (prescription or nonprescription drug, vitamin/mineral, herbal/natural supplement), and 29% are taking five or more. The study suggested that those aged 65 years or older continue to be the biggest consumers of medications, with 17-19 % in this age group taking at least ten medications in a given week. Polypharmacy has also shown to have increased since 2000 from 23% to 29%.\n\nThe U.S. Food and Drug Administration (FDA) appears to be very invested in the science of pharmacogenomics as is demonstrated through the 120 and more FDA-approved drugs that include pharmacogenomic biomarkers in their labels. This number increased varies over the years. A study of the labels of FDA-approved drugs as of 20 June 2014 found that there were 140 different drugs with a pharmacogenomic biomarker in their label. Because a drug can have different biomarkers, this corresponded to 158 drug–biomarker pairs. Only 29% stated a requirement or recommendation for genetic biomarker testing but this was higher for oncology drugs (62%). On May 22, 2005, the FDA issued its first \"Guidance for Industry: Pharmacogenomic Data Submissions\", which clarified the type of pharmacogenomic data required to be submitted to the FDA and when. Experts recognized the importance of the FDA’s acknowledgement that pharmacogenomics experiments will not bring negative regulatory consequences. The FDA had released its latest guide \"Clinical Pharmacogenomics (PGx): Premarket Evaluation in Early-Phase Clinical Studies and Recommendations for Labeling\" in January, 2013. The guide is intended to address the use of genomic information during drug development and regulatory review processes.\n\nAlthough there appears to be a general acceptance of the basic tenet of pharmacogenomics amongst physicians and healthcare professionals, several challenges exist that slow the uptake, implementation, and standardization of pharmacogenomics. Some of the concerns raised by physicians include:\n\n\nIssues surrounding the availability of the test include:\n\nAlthough other factors contribute to the slow progression of pharmacogenomics (such as developing guidelines for clinical use), the above factors appear to be the most prevalent.\n\nSome alleles that vary in frequency between specific populations have been shown to be associated with differential responses to specific drugs. The beta blocker atenolol is an anti-hypertensive medication that is shown to more significantly lower the blood pressure of Caucasian patients than African American patients in the United States. This observation suggests that Caucasian and African American populations have different alleles governing oleic acid biochemistry, which react differentially with atenolol. Similarly, hypersensitivity to the antiretroviral drug abacavir is strongly associated with a single-nucleotide polymorphism that varies in frequency between populations.\n\nThe FDA approval of the drug BiDil (isosorbide dinitrate/hydralazine) with a label specifying African-Americans with congestive heart failure, produced a storm of controversy over race-based medicine and fears of genetic stereotyping, even though the label for BiDil did not specify any genetic variants but was based on racial self-identification.\n\nComputational advances in pharmacogenomics has proven to be a blessing in research. As a simple example, for nearly a decade the ability to store more information on a hard drive has enabled us to investigate a human genome sequence cheaper and in more detail with regards to the effects/risks/safety concerns of drugs and other such substances. Such computational advances are expected to continue in the future. The aim is to use the genome sequence data to effectively make decisions in order to minimise the negative impacts on, say, a patient or the health industry in general. A large amount of research in the biomedical sciences regarding Pharmacogenomics as of late stems from combinatorial chemistry, genomic mining, omic technologies and high throughput screening. In order for the field to grow, rich knowledge enterprises and business must work more closely together and adopt simulation strategies. Consequently, more importance must be placed on the role of computational biology with regards to safety and risk assessments. Here, we can find the growing need and importance of being able to manage large, complex data sets, being able to extract information by integrating disparate data so that developments can be made in improving human health.\n\nJournals:\n"}
{"id": "23325", "url": "https://en.wikipedia.org/wiki?curid=23325", "title": "Polonium", "text": "Polonium\n\nPolonium is a chemical element with symbol Po and atomic number 84. A rare and highly radioactive metal with no stable isotopes, polonium is chemically similar to selenium and tellurium, though its metallic character resembles that of its horizontal neighbors in the periodic table: thallium, lead, and bismuth. Due to the short half-life of all its isotopes, its natural occurrence is limited to tiny traces of the fleeting polonium-210 (with a half-life of 138 days) in uranium ores, as it is the penultimate daughter of natural uranium-238. Though slightly longer-lived isotopes exist, they are much more difficult to produce. Today, polonium is usually produced in milligram quantities by the neutron irradiation of bismuth. Due to its intense radioactivity, which results in the radiolysis of chemical bonds and radioactive self-heating, its chemistry has mostly been investigated on the trace scale only.\n\nPolonium was discovered in 1898 by Marie and Pierre Curie, when it was extracted from uranium ore and identified solely by its strong radioactivity: it was the first element to be so discovered. Polonium was named after Marie Curie's homeland of Poland. Polonium has few applications, and those are related to its radioactivity: heaters in space probes, antistatic devices, and sources of neutrons and alpha particles. This radioactivity makes polonium dangerously toxic.\n\nPo is an alpha emitter that has a half-life of 138.4 days; it decays directly to its stable daughter isotope, Pb. A milligram (5 curies) of Po emits about as many alpha particles per second as 5 grams of Ra. A few curies (1 curie equals 37 gigabecquerels, 1 Ci = 37 GBq) of Po emit a blue glow which is caused by ionisation of the surrounding air.\n\nAbout one in 100,000 alpha emissions causes an excitation in the nucleus which then results in the emission of a gamma ray with a maximum energy of 803 keV.\n\nPolonium is a radioactive element that exists in two metallic allotropes. The alpha form is the only known example of a simple cubic crystal structure in a single atom basis at STP, with an edge length of 335.2 picometers; the beta form is rhombohedral. The structure of polonium has been characterized by X-ray diffraction and electron diffraction.\n\nPo (in common with Pu) has the ability to become airborne with ease: if a sample is heated in air to , 50% of it is vaporized in 45 hours to form diatomic Po molecules, even though the melting point of polonium is and its boiling point is .\nMore than one hypothesis exists for how polonium does this; one suggestion is that small clusters of polonium atoms are spalled off by the alpha decay.\n\nThe chemistry of polonium is similar to that of tellurium, although it also shows some similarities to its neighbor bismuth due to its metallic character. Polonium dissolves readily in dilute acids but is only slightly soluble in alkalis. Polonium solutions are first colored in pink by the Po ions, but then rapidly become yellow because alpha radiation from polonium ionizes the solvent and converts Po into Po. This process is accompanied by bubbling and emission of heat and light by glassware due to the absorbed alpha particles; as a result, polonium solutions are volatile and will evaporate within days unless sealed. At pH about 1, polonium ions are readily hydrolyzed and complexed by acids such as oxalic acid, citric acid, and tartaric acid.\n\nPolonium has no common compounds, and almost all of its compounds are synthetically created; more than 50 of those are known. The most stable class of polonium compounds are polonides, which are prepared by direct reaction of two elements. NaPo has the antifluorite structure, the polonides of Ca, Ba, Hg, Pb and lanthanides form a NaCl lattice, BePo and CdPo have the wurtzite and MgPo the nickel arsenide structure. Most polonides decompose upon heating to about 600 °C, except for HgPo that decomposes at ~300 °C and the lanthanide polonides, which do not decompose but melt at temperatures above 1000 °C. For example, PrPo melts at 1250 °C and TmPo at 2200 °C. PbPo is one of the very few naturally occurring polonium compounds, as polonium alpha decays to form lead.\n\nPolonium hydride () is a volatile liquid at room temperature prone to dissociation; it is thermally unstable. Water is the only other known hydrogen chalcogenide which is a liquid at room temperature; however, this is due to hydrogen bonding. The two oxides PoO and PoO are the products of oxidation of polonium.\n\nHalides of the structure PoX, PoX and PoF are known. They are soluble in the corresponding hydrogen halides, i.e., PoCl in HCl, PoBr in HBr and PoI in HI. Polonium dihalides are formed by direct reaction of the elements or by reduction of PoCl with SO and with PoBr with HS at room temperature. Tetrahalides can be obtained by reacting polonium dioxide with HCl, HBr or HI.\n\nOther polonium compounds include potassium polonite as a polonite, polonate, acetate, bromate, carbonate, citrate, chromate, cyanide, formate, (II) and (IV) hydroxides, nitrate, selenate, selenite, monosulfide, sulfate, disulfate and sulfite.\n\nOxides\n\nHydrides\n\nHalides\n\nPolonium has 33 known isotopes, all of which are radioactive. They have atomic masses that range from 188 to 220 u. Po (half-life 138.376 days) is the most widely available and is made via neutron capture by natural bismuth. The longer-lived Po (half-life years, longest-lived of all polonium isotopes) and Po (half-life 2.9 years) can be made through the alpha, proton, or deuteron bombardment of lead or bismuth in a cyclotron.\n\nTentatively called \"radium F\", polonium was discovered by Marie and Pierre Curie in 1898, and was named after Marie Curie's native land of Poland (). Poland at the time was under Russian, German, and Austro-Hungarian partition, and did not exist as an independent country. It was Curie's hope that naming the element after her native land would publicize its lack of independence. Polonium may be the first element named to highlight a political controversy.\n\nThis element was the first one discovered by the Curies while they were investigating the cause of pitchblende radioactivity. Pitchblende, after removal of the radioactive elements uranium and thorium, was more radioactive than the uranium and thorium combined. This spurred the Curies to search for additional radioactive elements. They first separated out polonium from pitchblende in July 1898, and five months later, also isolated radium. German scientist Willy Marckwald successfully isolated 3 milligrams of polonium in 1902, though at the time he believed it was a new element, which he dubbed \"radio-tellurium\", and it was not until 1905 that it was demonstrated to be the same as polonium.\n\nIn the United States, polonium was produced as part of the Manhattan Project's Dayton Project during World War II. It was a critical part of the implosion-type nuclear weapon design used in the Fat Man bomb on Nagasaki in 1945. Polonium and beryllium were the key ingredients of the 'urchin' detonator at the center of the bomb's spherical plutonium pit. The urchin ignited the nuclear chain reaction at the moment of prompt-criticality to ensure the bomb did not fizzle.\n\nMuch of the basic physics of polonium was classified until after the war. The fact that it was used as an initiator was classified until the 1960s.\n\nThe Atomic Energy Commission and the Manhattan Project funded human experiments using polonium on five people at the University of Rochester between 1943 and 1947. The people were administered between of polonium to study its excretion.\n\nPolonium is a very rare element in nature because of the short half-life of all its isotopes. Po, Po, and Po appear in the decay chain of U; thus polonium can be found in uranium ores at about 0.1 mg per metric ton (1 part in 10), which is approximately 0.2% of the abundance of radium. The amounts in the Earth's crust are not harmful. Polonium has been found in tobacco smoke from tobacco leaves grown with phosphate fertilizers.\n\nBecause it is present in small concentrations, isolation of polonium from natural sources is a tedious process. The largest batch of the element ever extracted, performed in the first half of the 20th century, contained only (9 mg) of polonium-210 and was obtained by processing 37 tonnes of residues from radium production. Polonium is now usually obtained by irradiating bismuth with high-energy neutrons or protons.\n\nIn 1934, an experiment showed that when natural Bi is bombarded with neutrons, Bi is created, which then decays to Po via beta-minus decay. The final purification is done pyrochemically followed by liquid-liquid extraction techniques. Polonium may now be made in milligram amounts in this procedure which uses high neutron fluxes found in nuclear reactors. Only about 100 grams are produced each year, practically all of it in Russia, making polonium exceedingly rare.\n\nThis process can cause problems in lead-bismuth based liquid metal cooled nuclear reactors such as those used in the Soviet Navy's K-27. Measures must be taken in these reactors to deal with the unwanted possibility of Po being released from the coolant.\n\nThe longer-lived isotopes of polonium, Po and Po, can be formed by proton or deuteron bombardment of bismuth using a cyclotron. Other more neutron-rich and more unstable isotopes can be formed by the irradiation of platinum with carbon nuclei.\n\nPolonium-based sources of alpha particles were produced in the former Soviet Union. Such sources were applied for measuring the thickness of industrial coatings via attenuation of alpha radiation.\n\nBecause of intense alpha radiation, a one-gram sample of Po will spontaneously heat up to above generating about 140 watts of power. Therefore, Po is used as an atomic heat source to power radioisotope thermoelectric generators via thermoelectric materials. For example, Po heat sources were used in the Lunokhod 1 (1970) and Lunokhod 2 (1973) Moon rovers to keep their internal components warm during the lunar nights, as well as the Kosmos 84 and 90 satellites (1965).\n\nThe alpha particles emitted by polonium can be converted to neutrons using beryllium oxide, at a rate of 93 neutrons per million alpha particles. Thus Po-BeO mixtures or alloys are used as a neutron source, for example, in a neutron trigger or initiator for nuclear weapons and for inspections of oil wells. About 1500 sources of this type, with an individual activity of , have been used annually in the Soviet Union.\n\nPolonium was also part of brushes or more complex tools that eliminate static charges in photographic plates, textile mills, paper rolls, sheet plastics, and on substrates (such as automotive) prior to the application of coatings. Alpha particles emitted by polonium ionize air molecules that neutralize charges on the nearby surfaces. Some anti-static brushes contain up to of Po as a source of charged particles for neutralizing static electricity. In the US, the devices with no more than of (sealed) Po per unit can be bought in any amount under a \"general license\", which means that a buyer need not be registered by any authorities. Polonium needs to be replaced in these devices nearly every year because of its short half-life; it is also highly radioactive and therefore has been mostly replaced by less dangerous beta particle sources.\n\nTiny amounts of Po are sometimes used in the laboratory and for teaching purposes—typically of the order of , in the form of sealed sources, with the polonium deposited on a substrate or in a resin or polymer matrix—are often exempt from licensing by the NRC and similar authorities as they are not considered hazardous. Small amounts of Po are manufactured for sale to the public in the United States as 'needle sources' for laboratory experimentation, and they are retailed by scientific supply companies. The polonium is a layer of plating which in turn is plated with a material such as gold, which allows the alpha radiation (used in experiments such as cloud chambers) to pass while preventing the polonium from being released and presenting a toxic hazard. According to United Nuclear, they typically sell between four and eight such sources per year.\n\nPolonium spark plugs were marketed by Firestone from 1940 to 1953. While the amount of radiation from the plugs was minuscule and not a threat to the consumer, the benefits of such plugs quickly diminished after approximately a month because of polonium's short half-life and because buildup on the conductors would block the radiation that improved engine performance. (The premise behind the polonium spark plug, as well as Alfred Matthew Hubbard's prototype radium plug that preceded it, was that the radiation would improve ionization of the fuel in the cylinder and thus allow the motor to fire more quickly and efficiently.)\n\nPolonium is highly dangerous and has no biological role. By mass, polonium-210 is around 250,000 times more toxic than hydrogen cyanide (the for Po is less than 1 microgram for an average adult (see below) compared with about 250 milligrams for hydrogen cyanide). The main hazard is its intense radioactivity (as an alpha emitter), which makes it difficult to handle safely. Even in microgram amounts, handling Po is extremely dangerous, requiring specialized equipment (a negative pressure alpha glove box equipped with high-performance filters), adequate monitoring, and strict handling procedures to avoid any contamination. Alpha particles emitted by polonium will damage organic tissue easily if polonium is ingested, inhaled, or absorbed, although they do not penetrate the epidermis and hence are not hazardous as long as the alpha particles remain outside the body. Wearing chemically resistant and intact gloves is a mandatory precaution to avoid transcutaneous diffusion of polonium directly through the skin. Polonium delivered in concentrated nitric acid can easily diffuse through inadequate gloves (e.g., latex gloves) or the acid may damage the gloves.\n\nIt has been reported that some microbes can methylate polonium by the action of methylcobalamin. This is similar to the way in which mercury, selenium, and tellurium are methylated in living things to create organometallic compounds. Studies investigating the metabolism of polonium-210 in rats have shown that only 0.002 to 0.009% of polonium-210 ingested is excreted as volatile polonium-210.\n\nThe median lethal dose (LD) for acute radiation exposure is about 4.5 Sv. The committed effective dose equivalent Po is 0.51 µSv/Bq if ingested, and 2.5 µSv/Bq if inhaled. So a fatal 4.5 Sv dose can be caused by ingesting , about 50 nanograms (ng), or inhaling , about 10 ng. One gram of Po could thus in theory poison 20 million people of whom 10 million would die. The actual toxicity of Po is lower than these estimates because radiation exposure that is spread out over several weeks (the biological half-life of polonium in humans is 30 to 50 days) is somewhat less damaging than an instantaneous dose. It has been estimated that a median lethal dose of Po is , or 0.089 micrograms, still an extremely small amount. For comparison, one grain of table salt is about 0.06 mg = 60 μg.\n\nIn addition to the acute effects, radiation exposure (both internal and external) carries a long-term risk of death from cancer of 5–10% per Sv. The general population is exposed to small amounts of polonium as a radon daughter in indoor air; the isotopes Po and Po are thought to cause the majority of the estimated 15,000–22,000 lung cancer deaths in the US every year that have been attributed to indoor radon. Tobacco smoking causes additional exposure to polonium.\n\nThe maximum allowable body burden for ingested Po is only , which is equivalent to a particle massing only 6.8 picograms. The maximum permissible workplace concentration of airborne Po is about 10 Bq/m ( µCi/cm). The target organs for polonium in humans are the spleen and liver. As the spleen (150 g) and the liver (1.3 to 3 kg) are much smaller than the rest of the body, if the polonium is concentrated in these vital organs, it is a greater threat to life than the dose which would be suffered (on average) by the whole body if it were spread evenly throughout the body, in the same way as caesium or tritium (as TO).\n\nPo is widely used in industry, and readily available with little regulation or restriction. In the US, a tracking system run by the Nuclear Regulatory Commission was implemented in 2007 to register purchases of more than of polonium-210 (enough to make up 5,000 lethal doses). The IAEA \"is said to be considering tighter regulations ... There is talk that it might tighten the polonium reporting requirement by a factor of 10, to .\" As of 2013, this is still the only alpha emitting byproduct material available, as a NRC Exempt Quantity, which may be held without a radioactive material license.\n\nPolonium and its compounds must be handled in a glove box, which is further enclosed in another box, maintained at a slightly higher pressure than the glove box to prevent the radioactive materials from leaking out. Gloves made of natural rubber do not provide sufficient protection against the radiation from polonium; surgical gloves are necessary. Neoprene gloves shield radiation from polonium better than natural rubber.\n\nPolonium was administered to humans for experimental purposes from 1943 to 1947; it was injected into four hospitalised patients, and orally given to a fifth. Studies such as this were funded by the Manhattan Project and the AEC and conducted at the University of Rochester. The objective was to obtain data on human excretion of polonium to correlate with more extensive data from rats. Patients selected as subjects were chosen because experimenters wanted persons who had not been exposed to polonium either through work or accident. All subjects had incurable diseases. Excretion of polonium was followed, and an autopsy was conducted at that time on the deceased patient to determine which organs absorbed the polonium. Patients' ages ranged from 'early thirties' to 'early forties'. The experiments were described in Chapter 3 of Biological Studies with Polonium, Radium, and Plutonium, National Nuclear Energy Series, Volume VI-3, McGraw-Hill, New York, 1950. Not specified is the isotope under study, but at the time polonium-210 was the most readily available polonium isotope. The DoE factsheet submitted for this experiment reported no follow up on these subjects.\n\nIt has also been suggested that Irène Joliot-Curie was the first person to die from the radiation effects of polonium. She was accidentally exposed to polonium in 1946 when a sealed capsule of the element exploded on her laboratory bench. In 1956, she died from leukemia.\n\nAccording to the 2008 book \"The Bomb in the Basement\", several deaths in Israel during 1957–1969 were caused by Po. A leak was discovered at a Weizmann Institute laboratory in 1957. Traces of Po were found on the hands of Professor Dror Sadeh, a physicist who researched radioactive materials. Medical tests indicated no harm, but the tests did not include bone marrow. Sadeh died from cancer. One of his students died of leukemia, and two colleagues died after a few years, both from cancer. The issue was investigated secretly, and there was never any formal admission that a connection between the leak and the deaths had existed.\n\nThe cause of death in the 2006 homicide of Alexander Litvinenko, a Russian KGB agent who had defected to the British MI6 intelligence agency, was determined to be Po poisoning. According to Prof. Nick Priest of Middlesex University, an environmental toxicologist and radiation expert, speaking on Sky News on December 3, 2006, Litvinenko was probably the first person to die of the acute α-radiation effects of Po.\n\nAbnormally high concentrations of Po were detected in July 2012 in clothes and personal belongings of the Palestinian leader Yasser Arafat, a heavy smoker, who died on 11 November 2004 of uncertain causes. The spokesman for the Institut de Radiophysique in Lausanne, Switzerland, where those items were analyzed, stressed that the \"clinical symptoms described in Arafat's medical reports were not consistent with polonium-210 and that conclusions could not be drawn as to whether the Palestinian leader was poisoned or not\", and that \"the only way to confirm the findings would be to exhume Arafat's body to test it for polonium-210.\" On 27 November 2012 Arafat's body was exhumed, and samples were taken for separate analysis by experts from France, Switzerland and Russia. On 12 October 2013, \"The Lancet\" published the group's finding that high levels of the element were found in Arafat's blood, urine, and in saliva stains on his clothes and toothbrush. The French tests later found some polonium but stated it was from \"natural environmental origin\". Following later Russian tests, Vladimir Uiba, the head of the Russian Federal Medical and Biological Agency, stated in December 2013 that Arafat died of natural causes, and they had no plans to conduct further tests.\n\nIt has been suggested that chelation agents, such as British Anti-Lewisite (dimercaprol), can be used to decontaminate humans. In one experiment, rats were given a fatal dose of 1.45 MBq/kg (8.7 ng/kg) of Po;\nall untreated rats were dead after 44 days, but 90% of the rats treated with the chelation agent\nHOEtTTC remained alive for 5 months.\n\nPolonium-210 may be quantified in biological specimens by alpha particle spectrometry to confirm a diagnosis of poisoning in hospitalized patients or to provide evidence in a medicolegal death investigation. The baseline urinary excretion of polonium-210 in healthy persons due to routine exposure to environmental sources is normally in a range of 5–15 mBq/day. Levels in excess of 30 mBq/day are suggestive of excessive exposure to the radionuclide.\n\nPolonium-210 is widespread in the biosphere, including in human tissues, because of its position in the uranium-238 decay chain. Natural uranium-238 in the Earth's crust decays through a series of solid radioactive intermediates including radium-226 to the radioactive noble gas radon-222, some of which, during its 3.8-day half-life, diffuses into the atmosphere. There it decays through several more steps to polonium-210, much of which, during its 138-day half-life, is washed back down to the Earth's surface, thus entering the biosphere, before finally decaying to stable lead-206.\n\nAs early as the 1920s Antoine Lacassagne, using polonium provided by his colleague Marie Curie, showed that the element has a specific pattern of uptake in rabbit tissues, with high concentrations, particularly in liver, kidney, and testes. More recent evidence suggests that this behavior results from polonium substituting for its congener sulfur, also in group 16 of the periodic table, in sulfur-containing amino-acids or related molecules and that similar patterns of distribution occur in human tissues. Polonium is indeed an element naturally present in all humans, contributing appreciably to natural background dose, with wide geographical and cultural variations, and particularly high levels in arctic residents, for example.\n\nPolonium-210 in tobacco contributes to many of the cases of lung cancer worldwide. Most of this polonium is derived from lead-210 deposited on tobacco leaves from the atmosphere; the lead-210 is a product of radon-222 gas, much of which appears to originate from the decay of radium-226 from fertilizers applied to the tobacco soils.\n\nThe presence of polonium in tobacco smoke has been known since the early 1960s. Some of the world's biggest tobacco firms researched ways to remove the substance—to no avail—over a 40-year period. The results were never published.\n\nPolonium is found in the food chain, especially in seafood.\n\n\n\n"}
{"id": "40919832", "url": "https://en.wikipedia.org/wiki?curid=40919832", "title": "Reality testing", "text": "Reality testing\n\nReality testing is the psychotherapeutic function by which the objective or real world and one's relationship to it are reflected on and evaluated by the observer. This process of distinguishing the internal world of thoughts and feelings from the external world is a technique commonly used in psychoanalysis and behavior therapy, and was originally devised by Sigmund Freud.\n\nWithin psychotherapy and counseling settings, practitioners use reality testing to influence the patient or client to recognize their negative thoughts, evaluate the thoughts logically rather than emotionally, and then determine whether the thoughts are valid (ie: internally consistent and grounded in reality). The focus of reality testing is not necessarily concentrated on the source of the behavior or thought, but rather on the fact that current thoughts are occurring and influencing behaviors in the here-and-now. After undergoing this technique, the patient or client is often able to see that the thoughts he or she has been experiencing are, in fact, not valid or based on reality, and should therefore not be used as the basis for life decisions. Reality testing can be used in this way to help facilitate corrective emotional experiences by disconfirming and altering previously held negative or unrealistic expectations in favor of more adaptive functions.\nPsychotherapy methods such as rational emotive behavior therapy and cognitive behavioral therapy rely heavily on the client's ability to frequently self-examine internal thoughts and assess their preceding influence on perceptions, judgments, and behaviors. Continual reality testing directed by therapists can help educate clients on how to habitually examine their own thought patterns and behaviors without the ongoing need for a therapist. Constant and prolonged exposure to a multitude of corrective experiences can lead clients to form their own internal and enduring changes in thoughts, expectations, feelings, and behavior. Reality testing has also been identified as a curative factor when implemented within a group therapy setting. In group counseling, clients can use the perspectives of other group members as the basis of reality testing and receive instant feedback through group discussions, roleplaying, and other group activities.\n\nTherapists using reality testing techniques typically rely upon the client's mental processes of attention, perception, memory, and judgment in order to help guide them to the formation of logical conclusions about how their internal experiences are related to external reality.\nLimited reality testing capabilities can sometimes be a function of a mental disorder. People exhibiting limited reality testing might lack the insight and ability to distinguish between the external and internal world as a factor of psychosis. For example, hallucinations and delusions are often taken as signs of a failure of reality testing.\nReality testing has been identified as being one of the common therapeutic principles of change. Principles of change are shared by all theoretic orientations of therapy, and include strategies such as: promoting client belief in the effectiveness of therapy, the formation and maintenance of a therapeutic alliance with the client, facilitating client awareness of the factors influencing their problems, and encouraging the client to engage in corrective experiences.\n\nEmphasizing ongoing reality testing in the client's life has been demonstrated to be among the principles of change that can be used to explain and account for the underlying effectiveness of therapeutic counseling techniques, regardless of theoretical ideals. For this reason, aspects of reality testing can be included into a variety of therapeutic treatment plans.\n"}
{"id": "50816046", "url": "https://en.wikipedia.org/wiki?curid=50816046", "title": "Robert Freer (physician)", "text": "Robert Freer (physician)\n\nProf Robert Freer (or Frier) FRSE FRCPE (1745-1827) was a soldier and academic, who taught Medicine at the University of Glasgow. He was Regius Professor of Medicine from 1796 until death (31 years). He was twice President of the Medical Society of Edinburgh (1772-4 and 1775-8). He was President of the Royal College of Physicians and Surgeons of Glasgow in 1797.\n\nHe was born in Perthshire. He studied Medicine at Aberdeen University graduating MA in 1765. He then did further studies at both Edinburgh University and Leyden University in Holland.\nIn 1776 he was recorded as a member of the Glasgow Literary Society. In the same year he gave an eloquent speech to the Medical Society of Edinburgh whilst acting as their Senior President.\n\nIn early life he served as an Ensign and military surgeon both in Europe and in the American war of independence. In 1779 he settled in Glasgow as a physician, in which year Glasgow University gave him his doctorate (MD). From 1796 he was Professor of Medicine at Glasgow University jointly acting as head Physician at Glasgow Royal Infirmary. In the same year he was elected a Fellow of the Royal Society of Edinburgh, his proposers being Daniel Rutherford, James Finlayson and Thomas Charles Hope. He had previously (1793) been unsuccessfully proposed by Daniel Rutherford, James Hutton and John Walker.\n\nWhilst teaching he lived at the College Court within the university.\n\nIn 1816 he is listed as a Director of the Glasgow Lunatic Asylum.\n\nHe died in Glasgow on 9 April 1827.\n"}
{"id": "1723628", "url": "https://en.wikipedia.org/wiki?curid=1723628", "title": "Samuel George Morton", "text": "Samuel George Morton\n\nSamuel George Morton (January 26, 1799 – May 15, 1851) was an American physician and natural scientist. Morton, who was reared a Quaker but became Episcopalian in midlife, was born in Philadelphia, Pennsylvania, attended Westtown School, and graduated from the University of Pennsylvania in 1820. After earning an advanced degree from the University of Edinburgh in Scotland, he began practice in Philadelphia in 1824. He was one of the founders of the Pennsylvania Medical College in Philadelphia and served as its professor of anatomy from 1839 until his resignation 1843. He was elected a member of the American Antiquarian Society in 1844.\n\nMorton was a prolific writer of books on various subjects from 1823 to 1851. He wrote \"Geological Observations\" in 1828, and both \"Synopsis of the Organic Remains of the Cretaceous Group of the United States\" and \"Illustrations of Pulmonary Consumption\" in 1834. His first medical essay, on the user of cornine in intermittent fever, in 1825 was published in the \"Philadelphia Journal of the Medical and Physical Sciences\". His bibliography includes \"Hybridity in Animals and Plants\" (1847), \"Additional Observation on Hybridity\" (1851), and \"An Illustrated System of Human Anatomy\" (1849).\n\nSamuel George Morton is often thought of as the originator of \"American School\" ethnography, a school of thought in antebellum American science that claimed the difference between humans was one of species rather than variety and is seen by some as the origin of scientific racism.\n\nMorton argued against the single creation story of the Bible (monogenism) and instead supported a theory of multiple racial creations (polygenism). Morton claimed the Bible supported polygenism, and within working in a biblical framework his theory held that each race had been created separately and each was given specific, irrevocable characteristics.\n\nAfter inspecting three mummies from ancient Egyptian catacombs, Morton concluded that Caucasians and Negroes were already distinct three thousand years ago. Since the Bible indicated that Noah's Ark had washed up on Mount Ararat, only a thousand years ago before this, Morton claimed that Noah's sons could not possibly account for every race on earth. According to Morton's theory of polygenesis, races have been separate since the start.\n\nMorton claimed that he could define the intellectual ability of a race by the skull capacity. A large volume meant a large brain and high intellectual capacity, and a small skull indicated a small brain and decreased intellectual capacity. He was reputed to hold the largest collection of skulls, on which he based his research. He claimed that each race had a separate origin, and that a descending order of intelligence could be discerned that placed Caucasians at the pinnacle and Negroes at the lowest point, with various other race groups in between. Morton had many skulls from ancient Egypt, and concluded that the ancient Egyptians were not African, but were Caucasian. His results were published in three volumes between 1839 and 1849: the \"Crania Americana\", \"An Inquiry into the Distinctive Characteristics of the Aboriginal Race of America\" and \"Crania Aegyptiaca\". Morton's skull collection was held at the Academy of Natural Sciences of Philadelphia until 1966, when it was transferred to the Penn Museum, where it is presently curated.\n\nMorton's theories were very popular in his day, and he was a highly respected physician and scientist. The anthropologist Aleš Hrdlička called Morton \"the father of American physical anthropology\". Crispin Bates has noted that Morton's \"systematic justification\" for the separation of races, along with the work of Louis Agassiz, was also used by those who favoured slavery in the United States, with the \"Charleston Medical Journal\" noting at his death that \"We of the South should consider him as our benefactor for aiding most materially in giving to the negro his true position as an inferior race.\"\n\nMorton claimed in his \"Crania Americana\" that the Caucasians had the biggest brains, averaging 87 cubic inches (1,426 cc), Indians were in the middle with an average of 82 cubic inches (1,344 cc) and Negroes had the smallest brains with an average of 78 cubic inches (1,278 cc). Morton believed that the skulls of each race were so different that a wise creator from the beginning had created each race and positioned them in separate homelands to dwell in.\n\nMorton believed that cranial capacity determined intellectual ability, and he used his craniometric evidence in conjunction with his analysis of anthropological literature then available to argue in favor of a racial hierarchy which put Caucasians on the top rung and Africans on the bottom. His skull measurements (by volume) then came to serve as \"evidence\" for racial stereotypes. He described the Caucasian as \"distinguished by the facility with which it attains the highest intellectual endowments\"; Native Americans were described as \"averse to cultivation, and slow in acquiring knowledge; restless, revengeful, and fond of war, and wholly destitute of maritime adventure\" and the Africans he described as \"joyous, flexible, and indolent; while the many nations which compose this race present a singular diversity of intellectual character, of which the far extreme is the lowest grade of humanity\".\n\nMorton's followers, particularly Josiah C. Nott and George Gliddon in their monumental tribute to Morton's work, \"Types of Mankind\" (1854), carried Morton's ideas further and backed up his findings which supported the notion of polygenism – the premise that the different races were separately created by God. The publication of Charles Darwin's \"On The Origin of Species\" in 1859 changed the nature of the scholarly debate.\n\nIn a 1978 paper and later in \"The Mismeasure of Man\" (1981), Stephen Jay Gould asserted that Morton had, perhaps because of an unconscious bias, selectively reported data, manipulated sample compositions, made analytical errors, and mismeasured skulls in order to support his prejudicial views on intelligence differences between different populations. Gould's book became widely read and Morton came to be considered one of the main cases of the effects of unconscious bias in data collection, and as one of the main figures in the early history of scientific racism.\n\nSubsequently, two separate studies of Morton's data and methods, one conducted in 1988 and the other in 2011, argued that Gould had overstated or misrepresented the case, and that Morton's measurement's were essentially correct.\n\nIn the latter study, entitled \"The Mismeasure of Science: Stephen Jay Gould versus Samuel George Morton on Skulls and Bias\" and authored by six anthropologists, it was concluded that the bias came from Gould, who failed to examine and remeasure the crania in order to determine Morton's level of accuracy.\n\nHowever, this study was reviewed in an editorial in \"Nature\", which recommended a degree of caution, stating \"the critique leaves the majority of Gould's work unscathed,\" and noted that \"because they couldn't measure all the skulls, they do not know whether the average cranial capacities that Morton reported represent his sample accurately.\" The journal stated that Gould's opposition to racism may have biased his interpretation of Morton's data, but also noted that \"Lewis and his colleagues have their own motivations. Several in the group have an association with the University of Pennsylvania, and have an interest in seeing the valuable but understudied skull collection freed from the stigma of bias\" and did not accept Gould's theory \"that the scientific method is inevitably tainted by bias.\"\n\nA 2014 review of the paper by University of Pennsylvania philosophy professor Michael Weisberg, tended to support Gould's original accusations, concluding that \"there is prima facie evidence of a racial bias in Morton's measurements\". Weisberg concludes that although Gould did commit mistakes in his own treatment, Morton's work \"remains a cautionary example of racial bias in the science of human differences\".\n\nResearch by Paul Wolff Mitchell in 2018 argues that Morton was nevertheless guilty of bias, though not in data collection. Mitchell argues that Morton's interpretation of his data was erroneous; he investigated averages and ignored variations in skull size so large that there was significant overlap. A contemporary of Morton, Friedrich Tiedemann, had collected similar skull data and drawn conclusions opposite to Morton's on the basis of this overlap. <ref> Ars Technica \"There’s new evidence confirming bias of the “father of scientific racism”<\\ref>\n\n\n\n"}
{"id": "57221417", "url": "https://en.wikipedia.org/wiki?curid=57221417", "title": "Sengers syndrome", "text": "Sengers syndrome\n\nSengers syndrome is a rare autosomal recessive condition characterised by congenital cataract, hypertrophic cardiomyopathy, muscle weakness and lactic acidosis after exercise. \n\nThere are two forms of the disease - a lethal neonatal form and a chronic form. \n\nHypertrophic cardiomyopathy is diagnosed at birth in half. Death in the first year is usually due to cardiac failure. Marked lactic acidemia occurs with even limited muscular exertion. \n\nThe chronic form have stable cardiomyopathy and myopathy with a normal intellect. \n\nOther reported features include nystagmus, strabismus, hypotonia, hyporeflexia and delayed motor development. \n\nThis disease is caused by mutations in AGK or SLC25A4 genes. The AGK gene encodes the mitochondrial acylglycerol kinase which plays a role in the assembly of adenine nucleotide translocator. The SLC25A4 gene encodes the heart and muscle specific isoform 1 of the mitochondrial adenine nucleotide translocator.\n\nSengers syndrome is a rare disorder. About 40 cases have been reported worldwide.\n\nThe diagnosis may be provisionally made on clinical grounds. Further diagnostic tests include serum and urine analysis for lactic acid, a chest X-ray (or cardiac CT or MRI) and echocardiography. Biopsies from cardiac and skeletal muscle will show the presence of lipid and glycogen. testing for mitochondrial abnormalities, ANT deficiency and decreases of respiratory chain complexes I and IV can also be done. \n\n\nSurgery for the cataracts may be needed. Medical treatment for the cardiac failure will be required. Treatment is otherwise supportive. \n\nAbout half the patients die within the first year of life. Because of its rarity the prognosis for the chronic form is not well established but survivial into adulthood has been reported.\n\nThis condition was first described in 1975. \n"}
{"id": "48867645", "url": "https://en.wikipedia.org/wiki?curid=48867645", "title": "Sevastopol plague uprising", "text": "Sevastopol plague uprising\n\nA popular uprising in Sevastopol in 1830 in response to strict quarantine measures imposed upon the city to combat the spread of the plague.\n\nIn 1828, there was a plague in the southern part of the Russian Empire, which was then at war with The Ottoman Empire. Even though there was no plague in Sevastopol, a quarantine was called as a preventative measure, likely due to the city's strategic importance as a port for the Russian Navy.\n\nIn May 1828, a quarantine cordon was built around Sevastopol, and all traffic into and out of the city had to pass through a checkpoint. In the summer of 1829, the quarantine was made stricter, such that anyone travelling to the city had to spend 2–3 weeks in the quarantine zone. Anybody suspected of illness had to be put into isolation. This dissuaded local farmers from entering the city with goods. This led to a shortage of rations, which was compounded by hoarding of goods by quarantine officials.\n\nThe deficit and poor quality of the goods in the city contributed to illness and death amongst the population, and the effects were the most pronounced in the poorer parts of the city.\n\nMatters got so bad that an official commission was ordered from St. Petersburg to investigate the situation. Mass abuses were found, but no officials were punished and the commission was disbanded in November of that same year.\n\nIn March 1830, the quarantine was toughened further when all of the residents were ordered not to leave their homes. This ban was lifted in May, except for those residents of the poorer Korablenaya neighborhood, who were ordered to remain in quarantine for another 7 days, and upon passage of this week, for another 2 weeks. This enraged those residents, as well as their relations outside of the neighbourhood, and they refused the orders - despite the urging of Counter admiral Ivan Skalovsky and others in the military and religious establishment.\n\nThe quarantine was enforced by the infantry, and the desperate residents began planning an armed resistance, led by retired military amongst them. Many within the infantry sympathised with the residents. The two sides managed to avoid the outbreak of an armed confrontation.\n\nOn the 3rd of June, Lt. General and Military Governor of Sevastopol, Nikolai Stolypin, in response to the growing instability, positioned troops in the streets and around the governor's mansion. This further incensed the revolting residents, who descended upon the governor's mansion in an angry mob and murdered Stolypin.\n\nThey were supported by the military garrisons deployed to enforce the quarantine, some of these soldiers tore down the quarantine cordon around the neighbourhood. The revolting citizens and soldiers held control of the city for 22 hours, during which time they ordered 'plague' officials to sign written documents stating that there was no plague in the city.\n\nThe next day Lt. Gen. Andrei Turchaninov replaced the murdered Stolypin and signed a decree ordering the end of the quarantine.\n\nDespite accomplishing their immediate goal, the revolters did face repercussions when a division under Timofeyev arrived to regain control of the city. A committee under the governor of Novorossiya, Mikhail Semyonovich Vorontsov, considered the cases of about 6,000 people. 7 people were executed for leading the uprising and about 1,000 residents and sailors were sent to hard labor. Approximately 4,200 civilians were deported to other cities. The officers received disciplinary punishment.\n"}
{"id": "7236408", "url": "https://en.wikipedia.org/wiki?curid=7236408", "title": "Seven Stones Reef", "text": "Seven Stones Reef\n\nThe Seven Stones reef is a rocky reef nearly west-northwest (WNW) of Land's End, Cornwall and east-northeast (ENE) of the Isles of Scilly. The reef consists of two groups of rocks and is nearly long and in breadth. They rise out of deep water and are a navigational hazard for shipping with 71 named wrecks and an estimated 200 shipwrecks overall. The most infamous is the \"Torrey Canyon\" in 1967, which was at that time the world's costliest shipping disaster, and to date, still the worst oil spill on the coast of the United Kingdom. \n\nThe Sevenstones lightvessel has been situated to the east of the reef since 1841, to warn ships of the danger and to mark the western boundary of a major north/south shipping route between the Isles of Scilly and the Cornish coast. An automatic weather station is on the lightvessel.\n\nSituated between Cornwall and the Isles of Scilly, the Seven Stones reef consists of seven (or eight) peaks, some of which appear at half ebb and others at low tide. They rise out of deep water, at and extend nearly two miles from north-northwest (NNW) to south-southeast (SSE) and are about a mile wide. The sea always breaks over the reef and in good weather, breakers are visible up to away. The rocks consists of small-grained granite which is part of the larger Cornubian batholith. The batholith formed during the early Permian period, from about 300 to 275 Ma, at a late stage in the Variscan orogeny. Some of the stones have been given names and include Flat Ledge, Flemish Ledges, North-east Rocks, Pollard's Rock, South Rock and a ledge known as the Town.\n\nDuring the 1960s the reef was fished by a small fleet of French fishing vessels for crab, crayfish and lobster. Some of these vessels were the first on the scene when the \"Torrey Canyon\" sank in 1967. On the vertical surfaces of the more exposed rocks there are clusters of jewel anemones and hydroids, and plumose anemones.\n\nThe reef is a major hazard to shipping as it is on the western boundary of a major north/south shipping route between the Cornish coast and the Isles of Scilly. The lightvessel, which has been on site since 1841, is to the north-east, not on the reef, a safety measure, as the sea is less rough away from the reef and also to ensure passing ships give the reef a wide berth. It is estimated that there are over 200 shipwrecks although only 71 are named.\n\nThe first recorded wreck was in early March 1656. Two English men o'war, the \"Primrose\" and \"Mayflower\", were searching for two Spanish frigates which had been patrolling the area and had captured a Bristol bound vessel. The sixth rate, 22 gun man o'war, \"Primrose\" lost her main topmast near the Longships, off Land’s End and drifted onto the Seven Stones. She managed to free herself and later sank in taking sixteen men, two women and a child with her. At the investigation into the loss, Trinity House, on behalf of the Admiralty, stated that they could not find any chart that showed the reef. The Admiralty found that there was no neglect either by the Officers or company, the place of wreck being a rock not visible nor described in any chart they could find. The largest loss of life was on 27 February 1748 with the sinking of the fourteen gun sloop HMS \"Lizard\" which was wrecked with the loss of over one hundred crew.\n\nOn 18 March 1967 the world’s first major oil pollution incident occurred when a supertanker, the \"Torrey Canyon\" hit the Pollard’s Rock, tearing a gash in her side and spilling 860,000 barrels of oil into the sea. Westerly winds and currents caused the oil to pollute of the Cornish coast and of the Brittany coast. She now lies in of water. Large amounts of toxic detergent was sprayed on the oil in an attempt to disperse it and, along with the oil, accounted for the loss of much of the marine life and 15,000 birds. At the time it was the world's costliest shipping disaster, and to date, still the worst oil spill on the coast of the United Kingdom.\n\nThe government was first petitioned for a light on the reef in 1826 (with no success), and a second petition in 1839 was supported by the British Channel ports, Liverpool merchants and the Chamber of Commerce of Waterford. A meeting held on 21 February 1840 in Falmouth declared the reef would shorten the route around the Isles of Scilly by up to 36 hours, and on 31 July 1841 a lightvessel (also known as a lightship) was seen at nearby St Mary's, Isles of Scilly. \n\nA lightvessel was first moored near the reef on 20 August 1841 and exhibited its first light on 1 September 1841. She is permanently anchored in and is north-east (NE) of the reef. Just over a year later on 25 November 1842 her cable parted and she almost became a wreck when she drove over the reef at high tide. The crew steered the ship to New Grimsby, Tresco where she stayed until 6 January 1843. She broke adrift again that January and went over the reef a second time the following March. Two of the crew drowned on 15 October 1851 when one of the lightship's longboats capsized, in a squall, on a journey from Scilly with stores. A meteor exploded over the lightvessel, at 2 am on 13 November 1872, showering the deck with cinders. The ship was replaced with a lighted buoy during the Second World War after being frequently bombed and machine-gunned by German pilots. \n\nSince 1987, the ship has been automated and unmanned with the accommodation and storage areas filled with foam to help with buoyancy in the event of a collision.\n\nThe Seven Stones lightvessel also acts as an automatic weather station.\n\nThere is a legend, part of the King Arthur scenario, that there was once a land between Cornwall and the Isles of Scilly, known as Lyonesse, with several towns and 140 churches. In the legend it was flooded and became sea. Only one man survived, Trevelyan, who riding on a black horse managed to reach dry land. The area is also part of Arthurian legend when Merlin cast a spell to engulf the land and the forces of Mordred who were chasing the fleeing army of King Arthur who he had just slain in battle. Arthur’s supporters managed to reach high ground in the Isles of Scilly. Fishermen are said to be able to hear the sound of church bells.\n\n\n"}
{"id": "24554605", "url": "https://en.wikipedia.org/wiki?curid=24554605", "title": "Sexual relationship disorder", "text": "Sexual relationship disorder\n\nSexual relationship disorder is a disorder where a person has difficulties in forming or maintaining a sexual relationship because of their gender identity or sexual orientation. The World Health Organization lists sexual relationship disorder in the ICD-10, under \"Psychological and behavioural disorders associated with sexual development and orientation\". The WHO describes it thus:\n\nThe gender identity or sexual orientation (heterosexual, homosexual, or bisexual) is responsible for difficulties in forming or maintaining a relationship with a sexual partner. ()\n\nThe WHO applies the following note to the entirety of part F66: \"Sexual orientation by itself is not to be regarded as a disorder.\"\n\nA significant number of men and women experience conflict surrounding homosexual expression within a mixed-orientation marriage. Therapy may include helping the client feel more comfortable and accepting of same-sex feelings and to explore ways of incorporating same-sex and opposite-sex feelings into life patterns. Although a strong homosexual identity was associated with difficulties in marital satisfaction, viewing the same-sex activities as compulsive facilitated commitment to the marriage and to monogamy.\n\nOther LGBT people may want to have a family with an opposite-sex spouse. They may seek to change their sexual orientation. Research from the 1970s showed that a minority of patients who undergo therapy ultimately married someone of the opposite sex, though it is not clear what role therapy contributed to the marriage. Recent research does not permit any attribution of marital outcomes to therapy, though it appears that sexual orientation identity reconstruction can help clients develop a heterosexual orientation identity.\n\n"}
{"id": "2021078", "url": "https://en.wikipedia.org/wiki?curid=2021078", "title": "Sid the Slug", "text": "Sid the Slug\n\nSid the Slug is an advertising character employed by the Food Standards Agency (FSA) in the United Kingdom as the face of the \"Salt - Watch it\" campaign to warn the public of the risks of excessive salt consumption.\n\nThe multimedia campaign, including advertising hoardings, television commercials and Internet coverage, was based on the premise that salt kills slugs, and can harm humans too. The campaign infuriated the Salt Manufacturers' Association (SMA), who complained to the Advertising Standards Authority - their complaint being that the information presented was misleading. The Advertising Standards Authority did not uphold the SMA complaint in its adjudication.\n\nThe ASA had to deal with another complaint from a member of the public, that the use of the name \"Sid\" was offensive; this was also rejected, with the ASA instead arguing that most people would find it \"humorous\".\n\nA member of the public complained to the FSA that the Welsh subtitles in the ‘Sid the Slug’ TV advertisements meant the FSA was not treating English and Welsh equally, as is required by the FSA Welsh Language Scheme. The FSA replied that the animation could not have been dubbed into Welsh successfully, hence the subtitles. However, the FSA accepted that it had not complied with advertising conduct, as set by the Welsh Language Board.\n\n"}
{"id": "14990054", "url": "https://en.wikipedia.org/wiki?curid=14990054", "title": "Sleep in non-human animals", "text": "Sleep in non-human animals\n\nSleep in non-human animals refers to a behavioral and physiological state characterized by altered consciousness, reduced responsiveness to external stimuli, and homeostatic regulation. Sleep is observed in mammals, birds, reptiles, amphibians, and some fish, and, in some form, in insects and even in simpler animals such as nematodes. The internal circadian clock promotes sleep at night for diurnal organisms (such as humans) and in the day for nocturnal organisms (such as rodents). Sleep patterns vary widely among species. It appears to be a requirement for all mammals and most other animals.\n\nSleep can follow a physiological or behavioral definition. In the physiological sense, sleep is a state characterized by reversible unconsciousness, special brainwave patterns, sporadic eye movement, loss of muscle tone (possibly with some exceptions; see below regarding the sleep of birds and of aquatic mammals), and a compensatory increase following deprivation of the state. In the behavioral sense, sleep is characterized by minimal movement, non-responsiveness to external stimuli (i.e. increased sensory threshold), the adoption of a typical posture, and the occupation of a sheltered site, all of which is usually repeated on a 24-hour basis. The physiological definition applies well to birds and mammals, but in other animals (whose brain is not as complex), the behavioral definition is more often used. In very simple animals, behavioral definitions of sleep are the only ones possible, and even then the behavioral repertoire of the animal may not be extensive enough to allow distinction between sleep and wakefulness.\nSleep is quickly reversible, as opposed to hibernation or coma, and sleep deprivation is followed by longer or deeper rebound sleep.\n\nIf sleep were not essential, one would expect to find:\nOutside of a few basal animals that have no brain or a very simple one, no animals have been found to date that satisfy any of these criteria. While some varieties of shark, such as great whites and hammerheads, must remain in motion at all times to move oxygenated water over their gills, it is possible they still sleep one cerebral hemisphere at a time as marine mammals do. However it remains to be shown definitively whether any fish is capable of unihemispheric sleep.\n\nSleep as a phenomenon appears to have very old evolutionary roots. Unicellular organisms do not necessarily \"sleep\", although many of them have pronounced circadian rhythms. The jellyfish Cassiopea is the most primitive organism in which sleep-like states have been observed. The nematode \"C. elegans\" is another primitive organism that appears to require sleep. Here, a \"lethargus\" phase occurs in short periods preceding each moult, a fact which may indicate that sleep primitively is connected to developmental processes. Raizen \"et al.\"'s results furthermore suggest that sleep is necessary for changes in the neural system.\n\nThe electrophysiological study of sleep in small invertebrates is complicated. Insects go through circadian rhythms of activity and passivity but some do not seem to have a homeostatic sleep need. Insects do not seem to exhibit REM sleep. However, fruit flies appear to sleep, and systematic disturbance of that state leads to cognitive disabilities. There are several methods of measuring cognitive functions in fruit flies. A common method is to let the flies choose whether they want to fly through a tunnel that leads to a light source, or through a dark tunnel. Normally, flies are attracted to light. But if sugar is placed in the end of the dark tunnel, and something the flies dislike is placed in the end of the light tunnel, the flies will eventually learn to fly towards darkness rather than light. Flies deprived of sleep require a longer time to learn this and also forget it more quickly. If an arthropod is experimentally kept awake longer than it is used to, then its coming rest period will be prolonged. In cockroaches that rest period is characterized by the antennae being folded down and by a decreased sensitivity to external stimuli. Sleep has been described in crayfish, too, characterized by passivity and increased thresholds for sensory stimuli as well as changes in the EEG pattern, markedly differing from the patterns found in crayfish when they are awake. In honeybees, it has been suggested they could be able to dream.\n\nSleep in fish is not extensively studied. Typically fish exhibit periods of inactivity but show no significant reactions to deprivation of this condition. Some species that always live in shoals or that swim continuously (because of a need for ram ventilation of the gills, for example) are suspected never to sleep. There is also doubt about certain blind species that live in caves. Other fish seem to sleep, however. For example, zebrafish, tilapia, tench, brown bullhead, and swell shark become motionless and unresponsive at night (or by day, in the case of the swell shark); Spanish hogfish and blue-headed wrasse can even be lifted by hand all the way to the surface without evoking a response. A 1961 observational study of approximately 200 species in European public aquaria reported many cases of apparent sleep. On the other hand, sleep patterns are easily disrupted and may even disappear during periods of migration, spawning, and parental care.\n\nMammals, birds and reptiles evolved from amniotic ancestors, the first vertebrates with life cycles independent of water. The fact that birds and mammals are the only known animals to exhibit REM and NREM sleep indicates a common trait before divergence. Reptiles are therefore the most logical group to investigate the origins of sleep. Daytime activity in reptiles alternates between basking and short bouts of active behavior, which has significant neurological and physiological similarities to sleep states in mammals. It is proposed that REM sleep evolved from short bouts of motor activity in reptiles while SWS evolved from their basking state which shows similar slow wave EEG patterns.\n\nReptiles have quiescent periods similar to mammalian sleep, and a decrease in electrical activity in the brain has been registered when the animals have been asleep. However, the EEG pattern in reptilian sleep differs from what is seen in mammals and other animals. In reptiles, sleep time increases following sleep deprivation, and stronger stimuli are needed to awaken the animals when they have been deprived of sleep as compared to when they have slept normally. This suggests that the sleep which follows deprivation is compensatorily deeper.\nIn 2016, a study report the existence of REM- and NREM-like sleep stages in the Australian dragon \"Pogona vitticeps\". Amphibians have periods of inactivity but show high vigilance (receptivity to potentially threatening stimuli) in this state.\n\nThere are significant similarities between sleep in birds and sleep in mammals, which is one of the reasons for the idea that sleep in higher animals with its division into REM and NREM sleep has evolved together with warm-bloodedness. Birds compensate for sleep loss in a manner similar to mammals, by deeper or more intense SWS (slow-wave sleep).\n\nBirds have both REM and NREM sleep, and the EEG patterns of both have similarities to those of mammals. Different birds sleep different amounts, but the associations seen in mammals between sleep and variables such as body mass, brain mass, relative brain mass, basal metabolism and other factors (see below) are not found in birds. The only clear explanatory factor for the variations in sleep amounts for birds of different species is that birds who sleep in environments where they are exposed to predators have less deep sleep than birds sleeping in more protected environments.\n\nBirds do not necessarily exhibit sleep debt, but a peculiarity that birds share with aquatic mammals, and possibly also with certain species of lizards (opinions differ about that last point), is the ability for unihemispheric sleep. That is the ability to sleep with one cerebral hemisphere at a time, while the other hemisphere is awake (Unihemispheric slow-wave sleep). When only one hemisphere is sleeping, only the contralateral eye will be shut; that is, when the right hemisphere is asleep the left eye will be shut, and vice versa. The distribution of sleep between the two hemispheres and the amount of unihemispheric sleep are determined both by which part of the brain has been the most active during the previous period of wake—that part will sleep the deepest—and it is also determined by the risk of attacks from predators. Ducks near the perimeter of the flock are likely to be the ones that first will detect predator attacks. These ducks have significantly more unihemispheric sleep than those who sleep in the middle of the flock, and they react to threatening stimuli seen by the open eye.\n\nOpinions partly differ about sleep in migratory birds. The controversy is mainly about whether they can sleep while flying or not. Theoretically, certain types of sleep could be possible while flying, but technical difficulties preclude the recording of brain activity in birds while they are flying.\n\nMammals have wide diversity in sleep phenomena. Generally, they go through periods of alternating non-REM and REM sleep, but these manifest differently. Horses and other herbivorous ungulates can sleep while standing, but must necessarily lie down for REM sleep (which causes muscular atony) for short periods. Giraffes, for example, only need to lie down for REM sleep for a few minutes at a time. Bats sleep while hanging upside down. Inversely to humans and rats, male armadillos get erections during non-REM sleep. Early mammals engaged in polyphasic sleep, dividing sleep into multiple bouts per day. Higher daily sleep quotas and shorter sleep cycles in polyphasic species as compared to monophasic species, suggest that polyphasic sleep may be a less efficient means of attaining sleep’s benefits. Small species with higher BMR may therefore have less efficient sleep patterns. It follows that the evolution of monophasic sleep may hitherto be an unknown advantage of evolving larger mammalian body sizes and therefore lower BMR.\n\nSleep is sometimes thought to help conserve energy, though this theory is not fully adequate as it only decreases metabolism by about 5–10%. Additionally it is observed that mammals require sleep even during the hypometabolic state of hibernation, in which circumstance it is actually a net loss of energy as the animal returns from hypothermia to euthermia in order to sleep.\n\nNocturnal animals have higher body temperatures, greater activity, rising serotonin, and diminishing cortisol during the night—the inverse of diurnal animals. Nocturnal and diurnal animals \"both\" have increased electrical activity in the suprachiasmatic nucleus, and corresponding secretion of melatonin from the pineal gland, at night. Nocturnal mammals, which tend to stay awake at night, have higher melatonin at night just like diurnal mammals do. And, although removing the pineal gland in many animals abolishes melatonin rhythms, it does not stop circadian rhythms altogether—though it may alter them and weaken their responsiveness to light cues. Cortisol levels in diurnal animals typically rise throughout the night, peak in the awakening hours, and diminish during the day. In diurnal animals, sleepiness increases during the night.\n\nDifferent mammals sleep different amounts. Some, such as bats, sleep 18–20 hours per day, while others, including giraffes, sleep only 3–4 hours per day. There can be big differences even between closely related species. There can also be differences between laboratory and field studies: for example, researchers in 1983 reported that captive sloths slept nearly 16 hours a day, but in 2008, when miniature neurophysiological recorders were developed that could be affixed to wild animals, sloths in nature were found to sleep only 9.6 hours a day.\nAs with birds, the main rule for mammals (with certain exceptions, see below) is that they have two essentially different stages of sleep: REM and NREM sleep (see above). Mammals' feeding habits are associated with their sleep length. The daily need for sleep is highest in carnivores, lower in omnivores and lowest in herbivores. Humans sleep less than many other omnivores but otherwise not unusually much or unusually little in comparison with other mammals. Many herbivores, like Ruminantia (such as cattle), spend much of their wake time in a state of drowsiness, which perhaps could partly explain their relatively low need for sleep. In herbivores, an inverse correlation is apparent between body mass and sleep length; big mammals sleep less than smaller ones. This correlation is thought to explain about 25% of the difference in sleep amount between different mammals. Also, the length of a particular sleep cycle is associated with the size of the animal; on average, bigger animals will have sleep cycles of longer durations than smaller animals. Sleep amount is also coupled to factors like basal metabolism, brain mass, and relative brain mass. The duration of sleep among species is also directly related to basal metabolic rate (BMR). Rats, which have a high BMR, sleep for up to 14 hours a day, whereas elephants and giraffes, which have lower BMRs, sleep only 3–4 hours per day.\n\nIt has been suggested that mammalian species which invest in longer sleep times are investing in the immune system, as species with the longer sleep times have higher white blood cell counts. Mammals born with well-developed regulatory systems, such as the horse and giraffe, tend to have less REM sleep than the species which are less developed at birth, such as cats and rats. This appears to echo the greater need for REM sleep among newborns than among adults in most mammal species. Many mammals sleep for a large proportion of each 24-hour period when they are very young. The giraffe only sleeps 2 hours a day in about 5–15 minute sessions. Koalas are the longest sleeping-mammals, about 20–22 hours a day. However, killer whales and some other dolphins do not sleep during the first month of life. Instead, young dolphins and whales frequently take rests by pressing their body next to their mother’s while she swims. As the mother swims she is keeping her offspring afloat to prevent them from drowning. This allows young dolphins and whales to rest, which will help keep their immune system healthy; in turn, protecting them from illnesses. During this period, mothers often sacrifice sleep for the protection of their young from predators. However, unlike other mammals, adult dolphins and whales are able to go without sleep for a month.\n\n\n\nReasons given for the wide variations include the fact that mammals \"that nap in hiding, like bats or rodents tend to have longer, deeper snoozes than those on constant alert.\" Lions, which have little fear of predators also have relatively long sleep periods, while elephants have to eat most of the time to support their huge bodies. Little brown bats conserve their energy except for the few hours each night when their insect prey are available, and platypuses eat a high energy crustacean diet and, therefore, probably do not need to spend as much time awake as many other mammals.\n\nA study conducted by Datta indirectly supports the idea that memory benefits from sleep. A box was constructed wherein a single rat could move freely from one end to the other. The bottom of the box was made of a steel grate. A light would shine in the box accompanied by a sound. After a five-second delay, an electrical shock would be applied. Once the shock commenced, the rat could move to the other end of the box, ending the shock immediately. The rat could also use the five-second delay to move to the other end of the box and avoid the shock entirely. The length of the shock never exceeded five seconds. This was repeated 30 times for half the rats. The other half, the control group, was placed in the same trial, but the rats were shocked regardless of their reaction. After each of the training sessions, the rat would be placed in a recording cage for six hours of polygraphic recordings. This process was repeated for three consecutive days. During the posttrial sleep recording session, rats spent 25.47% more time in REM sleep after learning trials than after control trials.\n\nAn observation of the Datta study is that the learning group spent 180% more time in SWS than did the control group during the post-trial sleep-recording session. This study shows that after spatial exploration activity, patterns of hippocampal place cells are reactivated during SWS following the experiment. Rats were run through a linear track using rewards on either end. The rats would then be placed in the track for 30 minutes to allow them to adjust (PRE), then they ran the track with reward-based training for 30 minutes (RUN), and then they were allowed to rest for 30 minutes.\n\nDuring each of these three periods, EEG data were collected for information on the rats' sleep stages. The mean firing rates of hippocampal place cells during prebehavior SWS (PRE) and three ten-minute intervals in postbehavior SWS (POST) were calculated by averaging across 22 track-running sessions from seven rats. The results showed that ten minutes after the trial RUN session, there was a 12% increase in the mean firing rate of hippocampal place cells from the PRE level. After 20 minutes, the mean firing rate returned rapidly toward the PRE level. The elevated firing of hippocampal place cells during SWS after spatial exploration could explain why there were elevated levels of slow-wave sleep in Datta's study, as it also dealt with a form of spatial exploration.\n\nIn rats, sleep deprivation causes weight loss and reduced body temperature. Rats kept awake indefinitely develop skin lesions, hyperphagia, loss of body mass, hypothermia, and, eventually, fatal sepsis. Sleep deprivation also hinders the healing of burns on rats. When compared with a control group, sleep-deprived rats' blood tests indicated a 20% decrease in white blood cell count, a significant change in the immune system.\n\nA 2014 study found that depriving mice of sleep increased cancer growth and dampened the immune system's ability to control cancers. The researchers found higher levels of M2 tumor-associated macrophages and TLR4 molecules in the sleep deprived mice and proposed this as the mechanism for increased susceptibility of the mice to cancer growth. M2 cells suppress the immune system and encourage tumour growth. TRL4 molecules are signalling molecules in the activation of the immune system.\n\nSince monotremes (egg-laying mammals) are considered to represent one of the evolutionarily oldest groups of mammals, they have been subject to special interest in the study of mammalian sleep. As early studies of these animals could not find clear evidence for REM sleep, it was initially assumed that such sleep did not exist in monotremes, but developed after the monotremes branched off from the rest of the mammalian evolutionary line, and became a separate, distinct group. However, EEG recordings of the brain stem in monotremes show a firing pattern that is quite similar to the patterns seen in REM sleep in higher mammals. In fact, the largest amount of REM sleep known in any animal is found in the platypus. REM electrical activation does not extend at all to the forebrain in platypods, suggesting that they do not dream. The average sleep time of the platypus in a 24-hour period is said to be as long as 14 hours, though this may be because of their high-calorie crustacean diet.\n\nThe consequences of falling into a deep sleep for marine mammalian species can be suffocation and drowning, or becoming easy prey for predators. Thus, dolphins, whales, and pinnipeds (seals) engage in unihemispheric sleep while swimming, which allows one brain hemisphere to remain fully functional, while the other goes to sleep. The hemisphere that is asleep alternates, so that both hemispheres can be fully rested. Just like terrestrial mammals, pinnipeds that sleep on land fall into a deep sleep and both hemispheres of their brain shut down and are in full sleep mode. Aquatic mammal infants do not have REM sleep in infancy; REM sleep increases as they age.\n\nAmong others, seals and whales belong to the aquatic mammals. Earless seals and eared seals have solved the problem of sleeping in water via two different methods. Eared seals, like whales, show unihemispheric sleep. The sleeping half of the brain does not awaken when they surface to breathe. When one half of a seal's brain shows slow-wave sleep, the flippers and whiskers on its opposite side are immobile. While in the water, these seals have almost no REM sleep and may go a week or two without it. As soon as they move onto land they switch to bilateral REM sleep and NREM sleep comparable to land mammals, surprising researchers with their lack of \"recovery sleep\" after missing so much REM.\nEarless seals sleep bihemispherically like most mammals, under water, hanging at the water surface or on land. They hold their breath while sleeping under water, and wake up regularly to surface and breathe. They can also hang with their nostrils above water and in that position have REM sleep, but they do not have REM sleep underwater.\n\nREM sleep has been observed in the pilot whale, a species of dolphin. Whales do not seem to have REM sleep, nor do they seem to have any problems because of this. One reason REM sleep might be difficult in marine settings is the fact that REM sleep causes muscular atony; that is to say, a functional paralysis of skeletal muscles that can be difficult to combine with the need to breathe regularly.\n\nUnihemispheric sleep refers to sleeping with only a single cerebral hemisphere. The phenomenon has been observed in birds and aquatic mammals, as well as in several reptilian species (the latter being disputed: many reptiles behave in a way which could be construed as unihemispheric sleeping, but EEG studies have given contradictory results). Reasons for the development of unihemispheric sleep are likely that it enables the sleeping animal to receive stimuli—threats, for instance—from its environment, and that it enables the animal to fly or periodically surface to breathe when immersed in water. Only NREM sleep exists unihemispherically, and there seems to exist a continuum in unihemispheric sleep regarding the differences in the hemispheres: in animals exhibiting unihemispheric sleep, conditions range from one hemisphere being in deep sleep with the other hemisphere being awake to one hemisphere sleeping lightly with the other hemisphere being awake. If one hemisphere is selectively deprived of sleep in an animal exhibiting unihemispheric sleep (one hemisphere is allowed to sleep freely but the other is awoken whenever it falls asleep), the amount of deep sleep will selectively increase in the hemisphere that was deprived of sleep when both hemispheres are allowed to sleep freely.\n\nThe neurobiological background for unihemispheric sleep is still unclear. In experiments on cats in which the connection between the left and the right halves of the brain stem has been severed, the brain hemispheres show periods of a desynchronized EEG, during which the two hemispheres can sleep independently of each other. In these cats, the state where one hemisphere slept NREM and the other was awake, as well as one hemisphere sleeping NREM with the other state sleeping REM were observed. The cats were never seen to sleep REM sleep with one hemisphere while the other hemisphere was awake. This is in accordance with the fact that REM sleep, as far as is currently known, does not occur unihemispherically.\n\nThe fact that unihemispheric sleep exists has been used as an argument for the necessity of sleep. It appears that no animal has developed an ability to go without sleep altogether.\n\nAnimals that hibernate are in a state of torpor, differing from sleep. Hibernation markedly reduces the need for sleep, but does not remove it. Some hibernating animals end their hibernation a couple of times during the winter so that they can sleep. Hibernating animals waking up from hibernation often go into rebound sleep because of lack of sleep during the hibernation period. They are definitely well-rested and are conserving energy during hibernation, but need sleep for something else.\n\n"}
{"id": "2765707", "url": "https://en.wikipedia.org/wiki?curid=2765707", "title": "Smallholding", "text": "Smallholding\n\nA smallholding is a small farm. In developing countries, smallholdings are usually farms supporting a single family with a mixture of cash crops and subsistence farming. As a country becomes more affluent, smallholdings may not be self-sufficient but are valued primarily for the rural lifestyle that they provide for the owners, who often do not earn their livelihood from the farm. There are an estimated 500 million smallholder farms in the world, supporting almost 2 billion people. Today some companies try to include smallholdings into their value chain, providing seed, feed or fertilizer to improve production. Some say that this model shows benefits for both parties.\n\nIn British English usage, a smallholding is a piece of land and its adjacent living quarters for the smallholder and stabling for farm animals. It is usually smaller than a farm but larger than an allotment, usually under . It is often established for breeding farm animals organically on free-range pastures. Alternatively, the smallholder may concentrate on growing vegetables by traditional methods or, in a more modern way, using plastic covers, Polytunneling or cloches for quick growth.\n\nGenerally, a smallholding offers its owner a means of achieving self-sufficiency for their family's needs. They may be able to supplement their income by selling surplus produce at a farmers' market or at a permanent shop on the smallholding.\n\nIn a separate development, so-called \"pick-your-own\" farms have appeared over the years near towns, which in type of management belong more to the category of smallholdings than to farms. \"Pick your own Strawberries\" were pioneered in the UK by Ted Moult in 1961. This kind of business usually consists of a large field which has been subdivided into areas for fruit trees, shrubs, or various vegetables which ripen in different seasons. The smallholder maintains the gardens, and the consumers pay to harvest their own produce.\n\nIn western Australia, many small acre farms were established under the Agricultural Land Purchase Act to encourage settlement. The government purchased large land grants held by absentee owners and subdivided them according to the best use for the land: the development of orchids in Coondle, viticulture, horse breeding, sheep grazing, and high density crops like corn, and broad acre crops like wheat.\n\nA Hobby farm in Australian usage is a variety of smallholding that may be as small as 2 hectares up to a self-sustaining farm size, that allows the \"city farmer\" to have a house and a small number of animals or small crop fields or grape vines. In western Australia, these are often termed Special Rural Properties for planning purposes.\n\nIn New Zealand, a lifestyle block is a smallholding valued primarily for the rural lifestyle it affords. Planning restrictions on subdividing farm land often lead to the creation of lifestyle blocks of minimum permissible size near urban areas.\n\nIn many developing countries, a smallholding is a small plot of land with low rental value, used to grow crops. By some estimates, there are 525 million smallholder farmers in the world. Smallholders dominate production in certain key sectors such as coffee and cocoa. Various types of agribusinesses work with smallholding farmers in a range of roles including buying crops, providing seed, and acting as financial institutions.\n\n\n\n"}
{"id": "26661954", "url": "https://en.wikipedia.org/wiki?curid=26661954", "title": "Spalding's sign", "text": "Spalding's sign\n\nSpalding's sign is a sign used in obstetrics. It is named for Alfred Baker Spalding.\n\nIt is an indicator of fetal death. When fetal death has occurred loss of alignment and overriding of the bones of cranial vault occur due to shrinkage of cerebrum, abdominal sonar examination may reveal an overriding of the fetal cranial bones. Most\nestimates place the precise time of fetal death at about 4–7 days before overlapping and separation of the fetal\nskull bones appear.\n"}
{"id": "44869885", "url": "https://en.wikipedia.org/wiki?curid=44869885", "title": "Symmetrical tonic neck reflex", "text": "Symmetrical tonic neck reflex\n\nThe symmetrical tonic neck reflex (STNR) is a primitive reflex that normally emerges during the first year of an infant's life and is diminished by the age of 9-10 months. It is a bridging or transitional brainstem reflex that is an important developmental stage and is necessary for a baby to transition from lying on the floor to quadruped crawling or walking. In order to be able to do this the baby needs to have been successful in unlinking the automatic movement of the head from the automatic movement of the arms and legs to progress beyond this development stage.\n\nThe STNR is normally fully developed by 6–8 months and significantly diminished by 2–3 years. If this reflex is retained beyond 2–3 years to such a degree that it \"modifies voluntary movement\", the child is considered to have \"immature and abnormal reflex development\", and this can have broad effects on the child's later development.\n\nThe symmetrical tonic neck reflex can be tested by placing the child in quadruped position on the floor and passively flexing the head forward and then extend it backwards. The expected response would be forward head flexion producing flexion of the upper extremities and extension of the lower extremities while extension of the head will produce extension of the upper extremities and flexion of the lower extremities.\n\nThis reflex can help the child to come to quadruped or crawling position but does not allow crawling because when the neck flexes forward, the upper limbs flex and lower limbs go into extension. This reflex is not normally easily seen or elicited in normal infants but may be seen in an exaggerated form in many children with cerebral palsy.\n\n"}
{"id": "26064377", "url": "https://en.wikipedia.org/wiki?curid=26064377", "title": "Venetjoki Reservoir", "text": "Venetjoki Reservoir\n\nVenetjoki Reservoir () is a medium-sized lake in the Perhonjoki main catchment area. It is located in the Central Ostrobothnia region in Finland. The purpose of the reservoir is to prevent spring flooding in the lower parts of the river. On the river Perhonjoki there are also two smaller reservoirs, Patana Reservoir and Vissavesi Reservoir.\n\n"}
{"id": "52238830", "url": "https://en.wikipedia.org/wiki?curid=52238830", "title": "Water Supply and Sewerage Authority", "text": "Water Supply and Sewerage Authority\n\nWater Supply and Sewerage Authority or WASA is the main body administering Water supply, Drainage and Sanitation system in Bangladesh. It was established in the year 1963 as an independent organization, under the East Pakistan ordinance XIX. At present WASA operates according to the WASA act 1996.\n\nThere are four separate WASA in Bangladesh. They are:\n"}
{"id": "1957022", "url": "https://en.wikipedia.org/wiki?curid=1957022", "title": "Western Regional Examining Board", "text": "Western Regional Examining Board\n\nThe Western Regional Examining Board (WREB) is one of five examination agencies for dentists and dental hygienists in the United States. The other examination agencies are, Council of Interstate Testing Agencies, Central Regional Dental Testing Service, Northeast Regional Board of Dental Examiners, Southern Regional Testing Agency. These were organized to better standardize clinical exams for licensure.\n\nWREB members for Dental and Dental Hygiene, unless otherwise noted are Alaska, Arizona, Arkansas, California (Dental), Hawaii (Dental Hygiene) Idaho, Illinois, Iowa, Kansas, Minnesota, Mississippi (Dental Hygiene), Missouri, Montana, Nevada, New Mexico, North Dakota, Oklahoma, Oregon, Texas, Utah, Washington, West Virginia (affiliate member) and Wyoming. These states help create the exam.\n\nOther states that are currently accepting successful completion results of the WREB exam in support of initial licensure are Alabama, California (Dental Hygiene), Colorado, Connecticut, Indiana, Kentucky, Maine, Massachusetts, Michigan, Nebraska, Ohio, Pennsylvania, Rhode Island, South Dakota, Tennessee, Virginia, and Wisconsin.\n\n"}
