{"id": "4601453", "url": "https://en.wikipedia.org/wiki?curid=4601453", "title": "Agnews Developmental Center", "text": "Agnews Developmental Center\n\nAgnews Developmental Center was a psychiatric and medical care facility, located in Santa Clara, California.\n\nIn 1885, the center, originally known as \"The Great Asylum for the Insane\", was established as a facility for the care of the mentally ill. The main structure, a red brick edifice, was located on land near Agnew's Village, which later became part of Santa Clara. By the early twentieth century, Agnews boasted the largest institutional population in the South San Francisco Bay area, and was served by its own train station which stood at the west end of Palm Drive across Lafayette Street; the station building remained until vandalism and fire precipitated its demolition in the 1990s.\n\nIn 1926, the center was expanded to include a second campus about to the east in San Jose (). Individuals with developmental disabilities were first admitted to a special rehabilitation program in 1965. Programs for the mentally ill were discontinued in 1972. Since then, the center has been used exclusively for the care and treatment of persons with developmental disabilities.\n\nPrior to 1906, the Agnews State Hospital, constructed in 1885, was modeled after the Kirkbride Plan and designed by architect Theodore Lenzen. This building was destroyed in the 1906 San Francisco earthquake, and Agnews became infamous as the site of the Santa Clara Valley's greatest loss of life resulting from the quake. The \"Daily Palo Alto\" reported: \"The position of the people in Agnews is critical; a number of insane persons having escaped from the demolished asylum, are running at random about the country.\" 117 patients and staff were killed and buried in mass graves on the site. The main building and some others were irreparably damaged.\n\nFollowing this disaster, Agnews was rebuilt in the Mediterranean Revival architecture styles of Mission Revival—Spanish Colonial Revival, in a layout resembling a college campus of two-story buildings. It re-opened circa 1911 as \"Agnews State Mental Hospital\". The facility was a small self-contained town, including a multitude of construction trade \"shops\", a farm which raised pigs and vegetable crops, a steam generating power plant for heating the buildings by steam, and even a fire department.\n\nThe original west campus was closed in 1998 as part of a plan to reduce and eventually close the center. When the west campus closed, the use of the land was the subject of local controversy. In April 1997, it was announced the state would sell an parcel of the campus to Sun Microsystems for its corporate headquarters and R&D campus. Some objected to the arranged sale of this prime public land to a profitable corporation at the peak of a local economic and real-estate boom, while others valued the presence of a prominent high-tech employer. Also at issue was the preservation of and public access to historic Agnews Developmental Center buildings. Sun agreed to restore four of the historic buildings (the auditorium, the clock tower, the superintendent's villa, and the administration building) and to keep some of the facilities available for public use. An outdoor exhibit open to the public displays information and photographs regarding the center and its history.\n\nIn addition to the Sun deal, the Rivermark master planned community was allocated for a variety of residential, retail, public school, and open space uses.\n\nThe Agnews site was added to the National Register of Historic Places (under the name \"Agnews Insane Asylum\") on August 13, 1997.\n\nSun was acquired by Oracle Corporation in 2010; the campus continues to be used as an Oracle R&D facility and conference center.\n\nIn March 2009, the last patient moved out of the east campus and the residential facility was closed. The east campus provided outpatient clinic services through April 2011.\n\nIn July 2011, the Regional Project of the Bay Area and the Community State Staff Administration moved to Campbell and continued providing support to patients in the local area.\n\nWith the final sale of the land pending, the east campus was vacated and the land was turned over to the Department of General Services.\n\nIn July 2014, the City of San Jose and the Santa Clara Unified School District purchased the property from the State of California for $80 million with the intent to build a K-8, High School, and city park.\n\nThe punk rock band Green Day recorded the music video for their 1994 song \"Basket Case\" at Agnews.\n\n"}
{"id": "19205811", "url": "https://en.wikipedia.org/wiki?curid=19205811", "title": "Alliance for Open Society International", "text": "Alliance for Open Society International\n\nAlliance for Open Society International, Inc. (AOSI) is a U.S. public charity organized in 2003 under the laws of the State of Delaware.\n\nAOSI does not have any employees. AOSI promotes the values of open, democratic societies globally. AOSI coordinates, administers, and advises national and regional programs in Central Asia and elsewhere on a range of public health, education, and general civil society issues. It also educates the public about societies' attempts to become democratic market economies after totalitarian or authoritarian rule.\n\nIn the United States it works with the U.S. federal government on charitable projects that address challenges facing urban communities and centers. AOSI makes and receives grants in addition to cooperating with other charitable organizations to achieve these goals.\n\nChris Stone serves as Chair and President, Maija Arbolino serves as Member and Treasurer, and A. Nicole Campbell serves as Member and Secretary of AOSI.\n\nIn September 2005, AOSI sued the United States Agency for International Development and other U.S. Government agencies in response to the government extending an anti-prostitution pledge that was a component of HIV/AIDS policy during the George W. Bush administration to cover non-profit organizations based in the United States. The pledge required recipients of funding under the United States Leadership Against HIV/AIDS, Tuberculosis, and Malaria Act to state that they had a policy opposing prostitution and prohibited them from engaging in speech or activity the government deemed inconsistent with an anti-prostitution policy. The requirement covered recipient organizations as a whole, and therefore restricted speech or activity that took place outside the government-funded program and was paid for with entirely private funds.\nAs described by the online magazine \"Medical News Today\":\n\nAt issue in the case is a requirement that public health groups receiving U.S. funds pledge their \"opposition to prostitution\" in order to continue their life-saving HIV prevention work. Under this \"pledge requirement,\" recipients of U.S. funds are forced to censor even their privately funded speech regarding the most effective ways to engage high-risk groups in HIV prevention.\n\nJust prior to this case, the non-profit organization DKT International had brought a similar lawsuit, prevailing in District Court but losing on appeal in the United States Court of Appeals for the District of Columbia Circuit. The February 2007 ruling was based on the assumption that the government would allow speech regarding prostitution through affiliate organizations that did not receive federal funding.\n\nWith the backing of the American Civil Liberties Union, AOSI sued the United States Agency for International Development, the financial backers of its Central Asian drug rehabilitation programs. AOSI's initial co-plaintiffs were the Open Society Institute and Pathfinder International. They were joined later in the litigation by InterAction and the Global Health Council. Lawyers from the Brennan Center for Justice at New York University School of Law represented the plaintiffs.\n\nIn May 2006, Judge Victor Marrero, a federal judge of the U.S. District Court for the Southern District of New York, issued a preliminary injunction barring the government from requiring AOSI and Pathfinder International to sign the anti-prostitution pledge. The government appealed to the US Court of Appeals for the Second Circuit. During the oral argument in the case, the government stated that it intended to issue regulations that would allow legally and physically separate affiliates of recipient organizations to engage in the prohibited speech.\nThe government issued guidelines to this effect in July 2007. In November 2007, the Court of Appeals returned the case for trial to the District Court for reconsideration in light of the new guidelines but left the injunction in place.\n\nIn August 2008, the district court held that the new guidelines did not cure the constitutional problems with the requirement. The government appealed again to the Second Circuit. While the appeal was pending, the government again revised the affiliate guidelines. In July 2011, the Second Circuit held that the requirement was unconstitutional and that the new affiliate guidelines did not cure the violation. The Supreme Court granted review in January 2013.\n\nIn 2013, the U.S. Supreme Court ruled in \"Agency for International Development v. Alliance for Open Society International, Inc.\" that the requirement was unconstitutional. The Supreme Court explained that the requirement would \"plainly violate the First Amendment\" if “enacted as a direct regulation of speech,” and that the question in the case was whether the government could “nonetheless impose that requirement as a condition on the receipt of federal funds.” The Court noted that it had previously struck down “conditions that seek to leverage funding to regulate speech outside the contours of the program itself.” The Court concluded that the requirement was just such a condition because it “compel[led] as a condition of federal funding the affirmation of a belief that by its nature cannot be confined within the scope of the Government program,” and therefore that the requirement violated the First Amendment. The decision is significant for its holdings on the scope of the unconstitutional conditions doctrine, and it is likely to have continuing importance for evaluating the constitutionality of government attempts to restrict speech by recipients of government funding.\n\n"}
{"id": "37202949", "url": "https://en.wikipedia.org/wiki?curid=37202949", "title": "Ateronon", "text": "Ateronon\n\nAteronon is a nutraceutical composed of \"lactolycopene\", which is lycopene from tomato oleoresin embedded in a whey protein matrix which increases the bioavailability of lycopene.\n\nThere is no good evidence taking Ateronon has more benefit than eating tomato food products, or even that it provides any health benefit at all.\n\nAteronon was developed by Cambridge Theranostics Ltd, UK, and launched in 2009.\n"}
{"id": "49412208", "url": "https://en.wikipedia.org/wiki?curid=49412208", "title": "Baramotichi Vihir", "text": "Baramotichi Vihir\n\nBaramotichi Vihir is a stepwell in Limb, Satara village in Satara district in Maharashtra state in India. It was constructed between 1641 and 1646 by Virubai Bhosale. It is 110 ft deep and 50 ft in diameter. Limb village is situated around 16 km from Satara and approximately 99 km from Pune. The well is octagonal in shape and looks like dug-out Shivling.\n"}
{"id": "144638", "url": "https://en.wikipedia.org/wiki?curid=144638", "title": "Bhopal disaster", "text": "Bhopal disaster\n\nThe Bhopal disaster, also referred to as the Bhopal gas tragedy, was a gas leak incident on the night of 2–3 December 1984 at the Union Carbide India Limited (UCIL) pesticide plant in Bhopal, Madhya Pradesh, India. It is considered to be the world's worst industrial disaster. Over 500,000 people were exposed to methyl isocyanate (MIC) gas. The highly toxic substance made its way into and around the small towns located near the plant.\n\nEstimates vary on the death toll. The official immediate death toll was 2,259. The government of Madhya Pradesh confirmed a total of 3,787 deaths related to the gas release. A government affidavit in 2006 stated that the leak caused 558,125 injuries, including 38,478 temporary partial injuries and approximately 3,900 severely and permanently disabling injuries. Others estimate that 8,000 died within two weeks, and another 8,000 or more have since died from gas-related diseases. The cause of the disaster remains under debate. The Indian government and local activists argue that slack management and deferred maintenance created a situation where routine pipe maintenance caused a backflow of water into a MIC tank, triggering the disaster. Union Carbide Corporation (UCC) argues water entered the tank through an act of sabotage.\n\nThe owner of the factory, UCIL, was majority owned by UCC, with Indian Government-controlled banks and the Indian public holding a 49.1 percent stake. In 1989, UCC paid $470 million ($929 million in 2017 dollars) to settle litigation stemming from the disaster. In 1994, UCC sold its stake in UCIL to Eveready Industries India Limited (EIIL), which subsequently merged with McLeod Russel (India) Ltd. Eveready ended clean-up on the site in 1998, when it terminated its 99-year lease and turned over control of the site to the state government of Madhya Pradesh. Dow Chemical Company purchased UCC in 2001, seventeen years after the disaster.\n\nCivil and criminal cases were filed in the District Court of Bhopal, India, involving UCC and Warren Anderson, UCC CEO at the time of the disaster. In June 2010, seven former employees, including the former UCIL chairman, were convicted in Bhopal of causing death by negligence and sentenced to two years imprisonment and a fine of about $2,000 each, the maximum punishment allowed by Indian law. An eighth former employee was also convicted, but died before the judgement was passed. Anderson died on 29 September 2014.\n\nThe UCIL factory was built in 1969 to produce the pesticide Sevin (UCC's brand name for carbaryl) using methyl isocyanate (MIC) as an intermediate. An MIC production plant was added to the UCIL site in 1979. The chemical process employed in the Bhopal plant had methylamine reacting with phosgene to form MIC, which was then reacted with 1-naphthol to form the final product, carbaryl. Another manufacturer, Bayer, also used this MIC-intermediate process at the chemical plant once owned by UCC at Institute, West Virginia, in the United States.\n\nAfter the Bhopal plant was built, other manufacturers (including Bayer) produced carbaryl without MIC, though at a greater manufacturing cost. This \"route\" differed from the MIC-free routes used elsewhere, in which the same raw materials were combined in a different manufacturing order, with phosgene first reacting with naphthol to form a chloroformate ester, which was then reacted with methylamine. In the early 1980s, the demand for pesticides had fallen, but production continued, leading to build-up of stores of unused MIC where that method was used. \n\nIn 1976, two local trade unions complained of pollution within the plant. In 1981, a worker was accidentally splashed with phosgene as he was carrying out a maintenance job of the plant's pipes. In a panic, he removed his gas mask and inhaled a large amount of toxic phosgene gas, leading to his death just 72 hours later. Following these events, the journalist, Rajkumar Keswani began investigating and published his findings in Bhopal's local paper \"Rapat,\" in which he urged \"Wake up people of Bhopal, you are on the edge of a volcano\". \n\nIn January 1982, a phosgene leak exposed 24 workers, all of whom were admitted to a hospital. None of the workers had been ordered to wear protective masks. One month later, in February 1982, an MIC leak affected 18 workers. In August 1982, a chemical engineer came into contact with liquid MIC, resulting in burns over 30 percent of his body. Later that same year, in October 1982, there was another MIC leak. In attempting to stop the leak, the MIC supervisor suffered severe chemical burns and two other workers were severely exposed to the gases. During 1983 and 1984, there were leaks of MIC, chlorine, monomethylamine, phosgene, and carbon tetrachloride, sometimes in combination.\n\nThe Bhopal UCIL facility housed three underground 68,000 liters liquid MIC storage tanks: E610, E611, and E619. In the months leading up to the December leak, liquid MIC production was in progress and being used to fill these tanks. UCC safety regulations specified that no one tank should be filled more than 50% (here, 30 tons) with liquid MIC. Each tank was pressurized with inert nitrogen gas. This pressurization allowed liquid MIC to be pumped out of each tank as needed, and also kept impurities out of the tanks.\n\nIn late October 1984, tank E610 lost the ability to effectively contain most of its nitrogen gas pressure. It meant that the liquid MIC contained within could not be pumped out. At the time of this failure, tank E610 contained 42 tons of liquid MIC. Shortly after this failure, MIC production was halted at the Bhopal facility, and parts of the plant were shut down for maintenance. Maintenance included the shutdown of the plant's flare tower so that a corroded pipe could be repaired. With the flare tower still out of service, production of carbaryl was resumed in late November, using MIC stored in the two tanks still in service. An attempt to re-establish pressure in tank E610 on 1 December failed, so the 42 tons of liquid MIC contained within still could not be pumped out of it.\n\nIn early December 1984, most of the plant's MIC related safety systems were malfunctioning and many valves and lines were in poor condition. In addition, several vent gas scrubbers had been out of service as well as the steam boiler, intended to clean the pipes. During the late evening hours of 2 December 1984, water was believed to have entered a side pipe and into Tank E610 whilst trying to unclog it, which contained 42 tons of MIC that had been there since late October.\nThe introduction of water into the tank subsequently resulted in a runaway exothermic reaction, which was accelerated by contaminants, high ambient temperatures and various other factors, such as the presence of iron from corroding non-stainless steel pipelines. The pressure in tank E610, although initially normal at 10:30 p.m., had increased by a factor of five to 10 psi (34.5 to 69 kPa) by 11 p.m. Two different senior refinery employees assumed the reading was instrumentation malfunction. By 11:30 p.m., workers in the MIC area were feeling the effects of minor exposure to MIC gas, and began to look for a leak. One was found by 11:45 p.m., and reported to the MIC supervisor on duty at the time. The decision was made to address the problem after a 12:15 a.m. tea break, and in the meantime, employees were instructed to continue looking for leaks. The incident was discussed by MIC area employees during the break.\n\nIn the five minutes after the tea break ended at 12:40 a.m., the reaction in tank E610 reached a critical state at an alarming speed. Temperatures in the tank were off the scale, maxed out beyond , and the pressure in the tank was indicated at 40 psi (275.8 kPa). One employee witnessed a concrete slab above tank E610 crack as the emergency relief valve burst open, and pressure in the tank continued to increase to 55 psi (379.2 kPa) even after atmospheric venting of toxic MIC gas had begun. Direct atmospheric venting should have been prevented or at least partially mitigated by at least three safety devices which were malfunctioning, not in use, insufficiently sized or otherwise rendered inoperable:\n\n\nAbout 30 metric tons of MIC escaped from the tank into the atmosphere in 45 to 60 minutes. This would increase to 40 metric tons within two hours time. The gases were blown in a southeasterly direction over Bhopal.\n\nA UCIL employee triggered the plant's alarm system at 12:50 a.m. as the concentration of gas in and around the plant became difficult to tolerate. Activation of the system triggered two siren alarms: one that sounded inside the UCIL plant, and a second directed outward to the public and the city of Bhopal. The two siren systems had been decoupled from one another in 1982, so that it was possible to leave the factory warning siren on while turning off the public one, and this is exactly what was done: the public siren briefly sounded at 12:50 a.m. and was quickly turned off, as per company procedure meant to avoid alarming the public around the factory over tiny leaks. Workers, meanwhile, evacuated the UCIL plant, travelling upwind.\n\nBhopal's superintendent of police was informed by telephone, by a town inspector, that residents of the neighbourhood of Chola (about 2 km from the plant) were fleeing a gas leak at approximately 1 a.m. Calls to the UCIL plant by police between 1:25 and 2:10 a.m. gave assurances twice that \"everything is OK\", and on the last attempt made, \"we don't know what has happened, sir\". With the lack of timely information exchange between UCIL and Bhopal authorities, the city's Hamidia Hospital was first told that the gas leak was suspected to be ammonia, then phosgene. Finally, they received an updated report that it was \"MIC\" (rather than \"methyl isocyanate\"), which hospital staff had never heard of, had no antidote for, and received no immediate information about.\n\nThe MIC gas leak emanating from tank E610 petered out at approximately 2:00 a.m. Fifteen minutes later, the plant's public siren was sounded for an extended period of time, after first having been quickly silenced an hour and a half earlier. Some minutes after the public siren sounded, a UCIL employee walked to a police control room to both inform them of the leak (their first acknowledgement that one had occurred at all), and that \"the leak had been plugged.\" Most city residents who were exposed to the MIC gas were first made aware of the leak by exposure to the gas itself, or by opening their doors to investigate commotion, rather than having been instructed to shelter in place, or to evacuate before the arrival of the gas in the first place.\n\nThe initial effects of exposure were coughing, severe eye irritation and a feeling of suffocation, burning in the respiratory tract, blepharospasm, breathlessness, stomach pains and vomiting. People awakened by these symptoms fled away from the plant. Those who ran inhaled more than those who had a vehicle to ride. Owing to their height, children and other people of shorter stature inhaled higher concentrations, as methyl isocyanate gas is approximately twice as dense as air and hence in an open environment has a tendency to fall toward the ground.\n\nThousands of people had died by the following morning. Primary causes of deaths were choking, reflexogenic circulatory collapse and pulmonary oedema. Findings during autopsies revealed changes not only in the lungs but also cerebral oedema, tubular necrosis of the kidneys, fatty degeneration of the liver and necrotising enteritis. The stillbirth rate increased by up to 300% and neonatal mortality rate by around 200%.\n\nApart from MIC, based on laboratory simulation conditions, the gas cloud most likely also contained chloroform, dichloromethane, hydrogen chloride, methyl amine, dimethylamine, trimethylamine and carbon dioxide, that was either present in the tank or was produced in the storage tank when MIC, chloroform and water reacted. The gas cloud, composed mainly of materials denser than air, stayed close to the ground and spread in the southeasterly direction affecting the nearby communities. The chemical reactions may have produced a liquid or solid aerosol.\nLaboratory investigations by CSIR and UCC scientists failed to demonstrate the presence of hydrogen cyanide.\n\nIn the immediate aftermath, the plant was closed to outsiders (including UCC) by the Indian government, which subsequently failed to make data public, contributing to the confusion. The initial investigation was conducted entirely by the Council of Scientific and Industrial Research (CSIR) and the Central Bureau of Investigation. The UCC chairman and CEO Warren Anderson, together with a technical team, immediately traveled to India. Upon arrival Anderson was placed under house arrest and urged by the Indian government to leave the country within 24 hours. Union Carbide organized a team of international medical experts, as well as supplies and equipment, to work with the local Bhopal medical community, and the UCC technical team began assessing the cause of the gas leak.\n\nThe health care system immediately became overloaded. In the severely affected areas, nearly 70 percent were under-qualified doctors. Medical staff were unprepared for the thousands of casualties. Doctors and hospitals were not aware of proper treatment methods for MIC gas inhalation.\nThere were mass funerals and cremations. Photographer Pablo Bartholemew, on commission with press agency Rapho, took an iconic color photograph of a burial on 4 December, \"Bhopal gas disaster girl.\" Another photographer present, Raghu Rai, took a black and white photo. The photographers did not ask for the identity of the father or child as she was buried, and no relative has since confirmed it. As such, the identity of the girl remains unknown. Both photos became symbolic of the suffering of victims of the Bhopal disaster, and Bartholomew's went on to win the 1984 World Press Photo of the Year.\n\nWithin a few days, trees in the vicinity became barren and bloated animal carcasses had to be disposed of. 170,000 people were treated at hospitals and temporary dispensaries, and 2,000 buffalo, goats, and other animals were collected and buried. Supplies, including food, became scarce owing to suppliers' safety fears. Fishing was prohibited causing further supply shortages.\n\nLacking any safe alternative, on 16 December, tanks 611 and 619 were emptied of the remaining MIC by reactivating the plant and continuing the manufacture of pesticide. Despite safety precautions such as having water carrying helicopters continually overflying the plant, this led to a second mass evacuation from Bhopal. The Government of India passed the \"Bhopal Gas Leak Disaster Act\" that gave the government rights to represent all victims, whether or not in India. Complaints of lack of information or misinformation were widespread. An Indian government spokesman said, \"Carbide is more interested in getting information from us than in helping our relief work\".\n\nFormal statements were issued that air, water, vegetation and foodstuffs were safe, but warned not to consume fish. The number of children exposed to the gases was at least 200,000. Within weeks, the State Government established a number of hospitals, clinics and mobile units in the gas-affected area to treat the victims.\n\nLegal proceedings involving UCC, the United States and Indian governments, local Bhopal authorities, and the disaster victims started immediately after the catastrophe. The Indian Government passed the Bhopal Gas Leak Act in March 1985, allowing the Government of India to act as the legal representative for victims of the disaster, leading to the beginning of legal proceedings. Initial lawsuits were generated in the United States federal court system. On 17 April 1985, Federal District court judge John F. Keenan (overseeing one lawsuit) suggested that \"'fundamental human decency' required Union Carbide to provide between $5 million and $10 million to immediately help the injured\" and suggested the money could be quickly distributed through the International Red Cross. UCC, on the notion that doing so did not constitute an admission of liability and the figure could be credited toward any future settlement or judgement, offered a $5 million relief fund two days later. The Indian government turned down the offer.\n\nIn March 1986 UCC proposed a settlement figure, endorsed by plaintiffs' U.S. attorneys, of that would, according to the company, \"generate a fund for Bhopal victims of between over 20 years\". In May, litigation was transferred from the United States to Indian courts by a U.S. District Court ruling. Following an appeal of this decision, the U.S. Court of Appeals affirmed the transfer, judging, in January 1987, that UCIL was a \"separate entity, owned, managed and operated exclusively by Indian citizens in India\".\n\nThe Government of India refused the offer from Union Carbide and claimed US$. The Indian Supreme Court told both sides to come to an agreement and \"start with a clean slate\" in November 1988. Eventually, in an out-of-court settlement reached in February 1989, Union Carbide agreed to pay US$ for damages caused in the Bhopal disaster. The amount was immediately paid.\n\nThroughout 1990, the Indian Supreme Court heard appeals against the settlement. In October 1991, the Supreme Court upheld the original , dismissing any other outstanding petitions that challenged the original decision. The Court ordered the Indian government \"to purchase, out of settlement fund, a group medical insurance policy to cover 100,000 persons who may later develop symptoms\" and cover any shortfall in the settlement fund. It also requested UCC and its subsidiary UCIL \"voluntarily\" fund a hospital in Bhopal, at an estimated , to specifically treat victims of the Bhopal disaster. The company agreed to this.\n\nIn 1991, the local Bhopal authorities charged Anderson, who had retired in 1986, with manslaughter, a crime that carries a maximum penalty of 10 years in prison. He was declared a fugitive from justice by the Chief Judicial Magistrate of Bhopal on 1 February 1992 for failing to appear at the court hearings in a culpable homicide case in which he was named the chief defendant. Orders were passed to the Government of India to press for an extradition from the United States. The U.S. Supreme Court refused to hear an appeal of the decision of the lower federal courts in October 1993, meaning that victims of the Bhopal disaster could not seek damages in a U.S. court.\n\nIn 2004, the Indian Supreme Court ordered the Indian government to release any remaining settlement funds to victims. And in September 2006, the Welfare Commission for Bhopal Gas Victims announced that all original compensation claims and revised petitions had been \"cleared\". The Second Circuit Court of Appeals in New York City upheld the dismissal of remaining claims in the case of \"Bano v. Union Carbide Corporation\" in 2006. This move blocked plaintiffs' motions for class certification and claims for property damages and remediation. In the view of UCC, \"the ruling reaffirms UCC's long-held positions and finally puts to rest—both procedurally and substantively—the issues raised in the class action complaint first filed against Union Carbide in 1999 by Haseena Bi and several organisations representing the residents of Bhopal\".\n\nIn June 2010, seven former employees of UCIL, all Indian nationals and many in their 70s, were convicted of causing death by negligence: Keshub Mahindra, former non-executive chairman of Union Carbide India Limited; V. P. Gokhale, managing director; Kishore Kamdar, vice-president; J. Mukund, works manager; S. P. Chowdhury, production manager; K. V. Shetty, plant superintendent; and S. I. Qureshi, production assistant. They were each sentenced to two years imprisonment and fined Rs.100,000 (US$2,124). All were released on bail shortly after the verdict.\n\nUS Federal class action litigation, \"Sahu v. Union Carbide and Warren Anderson\", was filed in 1999 under the U.S. Alien Torts Claims Act (ATCA), which provides for civil remedies for \"crimes against humanity.\" It sought damages for personal injury, medical monitoring and injunctive relief in the form of clean-up of the drinking water supplies for residential areas near the Bhopal plant. The lawsuit was dismissed in 2012 and subsequent appeal denied. Anderson died in 2014.\n\nIn 2018, \"The Atlantic\" called it the \"world’s worst industrial disaster.\"\n\nSome data about the health effects are still not available. The Indian Council of Medical Research (ICMR) was forbidden to publish health effect data until 1994.\n\nA total of 36 wards were marked by the authorities as being \"gas affected,\" affecting a population of 520,000. Of these, 200,000 were below 15 years of age, and 3,000 were pregnant women. The official immediate death toll was 2,259, and in 1991, 3,928 deaths had been officially certified. Ingrid Eckerman estimated 8,000 died within two weeks.\n\nThe government of Madhya Pradesh confirmed a total of 3,787 deaths related to the gas release.\n\nLater, the affected area was expanded to include 700,000 citizens. A government affidavit in 2006 stated the leak caused 558,125 injuries including 38,478 temporary partial injuries and approximately 3,900 severely and permanently disabling injuries.\n\nA cohort of 80,021 exposed people was registered, along with a control group, a cohort of 15,931 people from areas not exposed to MIC. Nearly every year since 1986, they have answered the same questionnaire. It shows overmortality and overmorbidity in the exposed group. Bias and confounding factors cannot be excluded from the study. Because of migration and other factors, 75% of the cohort is lost, as the ones who moved out are not followed.\n\nA number of clinical studies are performed. The quality varies, but the different reports support each other. Studied and reported long term health effects are:\n\nMissing or insufficient fields for research are female reproduction, chromosomal aberrations, cancer, immune deficiency, neurological sequelae, post traumatic stress disorder (PTSD) and children born after the disaster. Late cases that might never be highlighted are respiratory insufficiency, cardiac insufficiency (cor pulmonale), cancer and tuberculosis. Bhopal now has high rates of birth defects and records a miscarriage rate 7x higher than the national average. \n\nA 2014 report in \"Mother Jones\" quotes a \"spokesperson for the Bhopal Medical Appeal, which runs free health clinics for survivors\" as saying \"An estimated 120,000 to 150,000 survivors still struggle with serious medical conditions including nerve damage, growth problems, gynecological disorders, respiratory issues, birth defects, and elevated rates of cancer and tuberculosis.\"\n\nThe Government of India had focused primarily on increasing the hospital-based services for gas victims thus hospitals had been built after the disaster. When UCC wanted to sell its shares in UCIL, it was directed by the Supreme Court to finance a 500-bed hospital for the medical care of the survivors. Thus, Bhopal Memorial Hospital and Research Centre (BMHRC) was inaugurated in 1998 and was obliged to give free care for survivors for eight years. BMHRC was a 350-bedded super speciality hospital where heart surgery and hemodialysis were done. There was a dearth of gynaecology, obstetrics and paediatrics. Eight mini-units (outreach health centres) were started and free health care for gas victims were to be offered until 2006. The management had also faced problems with strikes, and the quality of the health care being disputed. Sambhavna Trust is a charitable trust, registered in 1995, that gives modern as well as ayurvedic treatments to gas victims, free of charge.\n\nWhen the factory was closed in 1986, pipes, drums and tanks were sold. The MIC and the Sevin plants are still there, as are storages of different residues. Isolation material is falling down and spreading. The area around the plant was used as a dumping area for hazardous chemicals. In 1982 tubewells in the vicinity of the UCIL factory had to be abandoned and tests in 1989 performed by UCC's laboratory revealed that soil and water samples collected from near the factory and inside the plant were toxic to fish. Several other studies had also shown polluted soil and groundwater in the area. Reported polluting compounds include 1-naphthol, naphthalene, Sevin, tarry residue, mercury, toxic organochlorines, volatile organochlorine compounds, chromium, copper, nickel, lead, hexachloroethane, hexachlorobutadiene, and the pesticide HCH.\n\nIn order to provide safe drinking water to the population around the UCIL factory, Government of Madhya Pradesh presented a scheme for improvement of water supply. In December 2008, the Madhya Pradesh High Court decided that the toxic waste should be incinerated at Ankleshwar in Gujarat, which was met by protests from activists all over India. On 8 June 2012, the Centre for incineration of toxic Bhopal waste agreed to pay to dispose of UCIL chemical plants waste in Germany. On 9 August 2012, Supreme court directed the Union and Madhya Pradesh Governments to take immediate steps for disposal of toxic waste lying around and inside the factory within six months.\n\nA U.S. court rejected the lawsuit blaming UCC for causing soil and water pollution around the site of the plant and ruled that responsibility for remedial measures or related claims rested with the State Government and not with UCC. In 2005, the state government invited various Indian architects to enter their \"concept for development of a memorial complex for Bhopal gas tragedy victims at the site of Union Carbide\". In 2011, a conference was held on the site, with participants from European universities which was aimed for the same.\n\n33 of the 50 planned work-sheds for gas victims started. All except one was closed down by 1992. 1986, the MP government invested in the Special Industrial Area Bhopal. 152 of the planned 200 work sheds were built and in 2000, 16 were partially functioning. It was estimated that 50,000 persons need alternative jobs, and that less than 100 gas victims had found regular employment under the government's scheme. The government also planned 2,486 flats in two- and four-story buildings in what is called the \"widow's colony\" outside Bhopal. The water did not reach the upper floors and it was not possible to keep cattle which were their primary occupation. Infrastructure like buses, schools, etc. were missing for at least a decade.\n\nImmediate relieves were decided two days after the tragedy. Relief measures commenced in 1985 when food was distributed for a short period along with ration cards. Madhya Pradesh government's finance department allocated for victim relief in July 1985. Widow pension of /per month (later ) were provided. The government also decided to pay to families with monthly income or less. As a result of the interim relief, more children were able to attend school, more money was spent on treatment and food, and housing also eventually improved. From 1990 interim relief of was paid to everyone in the family who was born before the disaster.\n\nThe final compensation, including interim relief for personal injury was for the majority . For death claim, the average sum paid out was . Each claimant were to be categorised by a doctor. In court, the claimants were expected to prove \"beyond reasonable doubt\" that death or injury in each case was attributable to exposure. In 1992, 44 percent of the claimants still had to be medically examined.\n\nBy the end of October 2003, according to the Bhopal Gas Tragedy Relief and Rehabilitation Department, compensation had been awarded to 554,895 people for injuries received and 15,310 survivors of those killed. The average amount to families of the dead was $2,200.\n\nIn 2007, 1,029,517 cases were registered and decided. Number of awarded cases were 574,304 and number of rejected cases 455,213. Total compensation awarded was . On 24 June 2010, the Union Cabinet of the Government of India approved a aid package which would be funded by Indian taxpayers through the government.\n\nIn 1985, Henry Waxman, a California Democrat, called for a U.S. government inquiry into the Bhopal disaster, which resulted in U.S. legislation regarding the accidental release of toxic chemicals in the United States.\n\nThere are two main lines of argument involving the disaster.\nThe \"Corporate Negligence\" point of view argues that the disaster was caused by a potent combination of under-maintained and decaying facilities, a weak attitude towards safety, and an undertrained workforce, culminating in worker actions that inadvertently enabled water to penetrate the MIC tanks in the absence of properly working safeguards.\n\nThe \"Worker Sabotage\" point of view argues that it was not physically possible for the water to enter the tank without concerted human effort, and that extensive testimony and engineering analysis leads to a conclusion that water entered the tank when a rogue individual employee hooked a water hose directly to an empty valve on the side of the tank. This point of view further argues that the Indian government took extensive actions to hide this possibility in order to attach blame to UCC.\n\nTheories differ as to how the water entered the tank. At the time, workers were cleaning out a clogged pipe with water about 400 feet from the tank. They claimed that they were not told to isolate the tank with a pipe slip-blind plate. The operators assumed that owing to bad maintenance and leaking valves, it was possible for the water to leak into the tank.\n\nThis water entry route could not be reproduced despite strenuous efforts by motivated parties. UCC claims that a \"disgruntled worker\" deliberately connecting a hose to a pressure gauge connection was the real cause.\n\nEarly the next morning, a UCIL manager asked the instrument engineer to replace the gauge. UCIL's investigation team found no evidence of the necessary connection; the investigation was totally controlled by the government, denying UCC investigators access to the tank or interviews with the operators.\n\nThis point of view argues that management (and to some extent, local government) underinvested in safety, which allowed for a dangerous working environment to develop. Factors cited include the filling of the MIC tanks beyond recommended levels, poor maintenance after the plant ceased MIC production at the end of 1984, allowing several safety systems to be inoperable due to poor maintenance, and switching off safety systems to save money— including the MIC tank refrigeration system which could have mitigated the disaster severity, and non-existent catastrophe management plans. Other factors identified by government inquiries included undersized safety devices and the dependence on manual operations. Specific plant management deficiencies that were identified include the lack of skilled operators, reduction of safety management, insufficient maintenance, and inadequate emergency action plans.\n\nUnderinvestment\n\nUnderinvestment is cited as contributing to an environment. In attempts to reduce expenses, $1.25 million of cuts were placed upon the plant which affected the factory's employees and their conditions. Kurzman argues that \"cuts ... meant less stringent quality control and thus looser safety rules. A pipe leaked? Don't replace it, employees said they were told ... MIC workers needed more training? They could do with less. Promotions were halted, seriously affecting employee morale and driving some of the most skilled ... elsewhere\". Workers were forced to use English manuals, even though only a few had a grasp of the language.\n\nSubsequent research highlights a gradual deterioration of safety practices in regard to the MIC, which had become less relevant to plant operations. By 1984, only six of the original twelve operators were still working with MIC and the number of supervisory personnel had also been halved. No maintenance supervisor was placed on the night shift and instrument readings were taken every two hours, rather than the previous and required one-hour readings. Workers made complaints about the cuts through their union but were ignored. One employee was fired after going on a 15-day hunger strike. 70% of the plant's employees were fined before the disaster for refusing to deviate from the proper safety regulations under pressure from the management.\n\nIn addition, some observers, such as those writing in the Trade Environmental Database (TED) Case Studies as part of the Mandala Project from American University, have pointed to \"serious communication problems and management gaps between Union Carbide and its Indian operation\", characterised by \"the parent hands-off approach to its overseas operation\" and \"cross-cultural barriers\".\n\nAdequacy of equipment and safety regulations\n\nThe factory was not well equipped to handle the gas created by the sudden addition of water to the MIC tank. The MIC tank alarms had not been working for four years and there was only one manual back-up system, compared to a four-stage system used in the United States. The flare tower and several vent gas scrubbers had been out of service for five months before the disaster. Only one gas scrubber was operating: it could not treat such a large amount of MIC with sodium hydroxide (caustic soda), which would have brought the concentration down to a safe level. The flare tower could only handle a quarter of the gas that leaked in 1984, and moreover it was out of order at the time of the incident. To reduce energy costs, the refrigeration system was idle. The MIC was kept at 20 degrees Celsius, not the 4.5 degrees advised by the manual. Even the steam boiler, intended to clean the pipes, was non-operational for unknown reasons. Slip-blind plates that would have prevented water from pipes being cleaned from leaking into the MIC tanks, had the valves been faulty, were not installed and their installation had been omitted from the cleaning checklist. As MIC is water-soluble, deluge guns were in place to contain escaping gases from the stack. The water pressure was too weak for the guns to spray high enough to reach the gas which would have reduced the concentration of escaping gas significantly. In addition to it, carbon steel valves were used at the factory, even though they were known to corrode when exposed to acid.\n\nAccording to the operators, the MIC tank pressure gauge had been malfunctioning for roughly a week. Other tanks were used, rather than repairing the gauge. The build-up in temperature and pressure is believed to have affected the magnitude of the gas release. UCC admitted in their own investigation report that most of the safety systems were not functioning on the night of 3 December 1984. The design of the MIC plant, following government guidelines, was \"Indianized\" by UCIL engineers to maximise the use of indigenous materials and products. Mumbai-based Humphreys and Glasgow Consultants Pvt. Ltd., were the main consultants, Larsen & Toubro fabricated the MIC storage tanks, and Taylor of India Ltd. provided the instrumentation. In 1998, during civil action suits in India, it emerged that the plant was not prepared for problems. No action plans had been established to cope with incidents of this magnitude. This included not informing local authorities of the quantities or dangers of chemicals used and manufactured at Bhopal.\n\nSafety audits\n\nSafety audits were done every year in the US and European UCC plants, but only every two years in other parts of the world. Before a \"Business Confidential\" safety audit by UCC in May 1982, the senior officials of the corporation were well aware of \"a total of 61 hazards, 30 of them major and 11 minor in the dangerous phosgene/methyl isocyanate units\" in Bhopal. In the audit 1982, it was indicated that worker performance was below standards. Ten major concerns were listed. UCIL prepared an action plan, but UCC never sent a follow-up team to Bhopal. Many of the items in the 1982 report were temporarily fixed, but by 1984, conditions had again deteriorated. In September 1984, an internal UCC report on the West Virginia plant in the USA revealed a number of defects and malfunctions. It warned that \"a runaway reaction could occur in the MIC unit storage tanks, and that the planned response would not be timely or effective enough to prevent catastrophic failure of the tanks\". This report was never forwarded to the Bhopal plant, although the main design was the same.\n\nNow owned by Dow Chemical Company, Union Carbide maintains a website dedicated to the tragedy and claims that the incident was the result of sabotage, stating that sufficient safety systems were in place and operative to prevent the intrusion of water.\n\nThe impossibility of the \"negligence\" argument\n\nAccording to the \"Corporate Negligence\" argument, workers had been cleaning out pipes with water nearby. This water was diverted due to a combination of improper maintenance, leaking and clogging, and eventually ended up in the MIC storage tank. Indian scientists also suggested that additional water might have been introduced as a \"back-flow\" from a defectively designed vent-gas scrubber. None of these theoretical routes of entry were ever successfully demonstrated during tests by the Central Bureau of Investigators (CBI) and UCIL engineers.\n\nA Union Carbide commissioned analysis conducted by Arthur D. Little claims that the Negligence argument was impossible for several tangible reasons:\n\nThe Union Carbide commissioned Arthur D. Little report concludes that it is likely that a single employee secretly and deliberately introduced a large amount of water into the MIC tank by removing a meter and connecting a water hose directly to the tank through the metering port.\n\nUCC claims the plant staff falsified numerous records to distance themselves from the incident and absolve themselves of blame, and that the Indian Government impeded its investigation and declined to prosecute the employee responsible, presumably because that would weaken its allegations of negligence by Union Carbide.\n\nThe evidence in favor of this point of view includes:\n\nThe Little report argues that this evidence demonstrates that the following chronology took place:\nThe theory of design defect was floated by the central government in its endeavour to do justice to the victims of the tragedy. Everyone else who was part of investigations into the case \"just toed the line of the central government...\nThe government and the CBI suppressed the actual truth and saved the real perpetrators of the crime, the counsel, Anirban Roy told the court.\"\n\nIn November 2017, appearing for two accused S P Chaudhary and J Mukund, their advocate Anirban Roy told the district court on Monday that disgruntled plant operator M L Verma was behind the sabotage because he was unhappy with his seniors. Roy argued that the theory about defects in the plant causing the mishap was imaginary. He said truth had always been suppressed and it’s for the CBI to bring it out.\nThe counsel argued that there were discrepancies in the statements given by persons who were operating the plant at that time but the central agency chose not to investigate the case properly because it always wanted to prove that it was a mishap, and not sabotage. He alleged that Verma was unhappy with Chaudhary and Mukund.\n\nThe corporation denied the claim that the valves on the tank were malfunctioning, and claimed that the documented evidence gathered after the incident showed that the valve close to the plant's water-washing operation was closed and was leak-tight. Furthermore, process safety systems had prevented water from entering the tank by accident. Carbide states that the safety concerns identified in 1982 were all allayed before 1984 and had nothing to do with the incident.\n\nThe company admitted that the safety systems in place would not have been able to prevent a chemical reaction of that magnitude from causing a leak. According to Carbide, \"in designing the plant's safety systems, a chemical reaction of this magnitude was not factored in\" because \"the tank's gas storage system was designed to automatically prevent such a large amount of water from being inadvertently introduced into the system\" and \"process safety systems—in place and operational—would have prevented water from entering the tank by accident\". Instead, they claim that \"employee sabotage—not faulty design or operation—was the cause of the tragedy\".\n\nTactical response\n\nThe company stresses the immediate action taken after the disaster and its continued commitment to helping the victims. On 4 December, the day following the leak, Union Carbide sent material aid and several international medical experts to assist the medical facilities in Bhopal.\n\nFinancial response\n\nThe primary financial restitution paid by UCC was negotiated in 1989, when the Indian Supreme Court approved a settlement of US$470 million (). This amount was immediately paid by UCC to the Indian government. The company states that the restitution paid \"was $120 million more than plaintiffs' lawyers had told U.S. courts was fair\" and that the Indian Supreme Court stated in its opinion that \"compensation levels under the settlement were far greater than would normally be payable under Indian law.\"\n\nIn the immediate aftermath of the disaster, Union Carbide states on its website that it put $2 million into the Indian prime minister's immediate disaster relief fund on 11 December 1984. The corporation established the Employees' Bhopal Relief Fund in February 1985, which raised more than for immediate relief. According to Union Carbide, in August 1987, they made an additional in humanitarian interim relief available.\n\nUnion Carbide stated that it also undertook several steps to provide continuing aid to the victims of the Bhopal disaster. The sale of its 50.9 percent interest in UCIL in April 1992 and establishment of a charitable trust to contribute to the building of a local hospital. The sale was finalised in November 1994. The hospital was begun in October 1995 and was opened in 2001. The company provided a fund with around from sale of its UCIL stock. In 1991, the trust had amounted approximately . The hospital catered for the treatment of heart, lung and eye problems. UCC also provided a $2.2 million grant to Arizona State University to establish a vocational-technical center in Bhopal, which was opened, but was later closed by the state government. They also donated $5 million to the Indian Red Cross after the disaster. They also developed a Responsible Care system with other members of the chemical industry as a response to the Bhopal crisis, which was designed to help prevent such an event in the future.\n\nUCC chairman and CEO Warren Anderson was arrested and released on bail by the Madhya Pradesh Police in Bhopal on 7 December 1984. Anderson was taken to UCC's house after which he was released six hours later on $2,100 bail and flown out on a government plane. These actions were allegedly taken under the direction of then chief secretary of the state, who was possibly instructed from chief minister's office, who himself flew out of Bhopal immediately. Later in 1987, the Indian government summoned Anderson, eight other executives and two company affiliates with homicide charges to appear in Indian court. In response, Union Carbide said the company is not under Indian jurisdiction.\n\nFrom 2014, Dow is a named respondent in a number of ongoing cases arising from Union Carbide’s business in Bhopal.\n\nChemicals abandoned at the plant continue to leak and pollute the groundwater. Whether the chemicals pose a health hazard is disputed. Contamination at the site and surrounding area was not caused by the gas leakage. The area around the plant was used as a dumping ground for hazardous chemicals and by 1982 water wells in the vicinity of the UCIL factory had to be abandoned. UCC states that \"after the incident, UCIL began clean-up work at the site under the direction of Indian central and state government authorities\", which was continued after 1994 by the successor to UCIL. The successor, Eveready Industries India, Limited (EIIL), ended cleanup on the site in 1998, when it terminated its 99-year lease and turned over control of the site to the state government of Madhya Pradesh.\n\nUCC's laboratory tests in 1989 revealed that soil and water samples collected from near the factory were toxic to fish. Twenty-one areas inside the plant were reported to be highly polluted. In 1991 the municipal authorities declared that water from over 100 wells was hazardous for health if used for drinking. In 1994 it was reported that 21% of the factory premises were seriously contaminated with chemicals. Beginning in 1999, studies made by Greenpeace and others from soil, groundwater, well water and vegetables from the residential areas around UCIL and from the UCIL factory area show contamination with a range of toxic heavy metals and chemical compounds. Substances found, according to the reports, are naphthol, naphthalene, Sevin, tarry residues, alpha naphthol, mercury, organochlorines, chromium, copper, nickel, lead, hexachlorethane, hexachlorobutadiene, pesticide HCH (BHC), volatile organic compounds and halo-organics. Many of these contaminants were also found in breast milk of women living near the area.\nSoil tests were conducted by Greenpeace in 1999. One sample (IT9012) from \"sediment collected from drain under former Sevin plant\" showed mercury levels to be at \"20,000 and times\" higher than expected levels. Organochlorine compounds at elevated levels were also present in groundwater collected from (sample IT9040) a 4.4 meter depth \"bore-hole within the former UCIL site\". This sample was obtained from a source posted with a warning sign which read \"Water unfit for consumption\".\nChemicals that have been linked to various forms of cancer were also discovered, as well as trichloroethylene, known to impair fetal development, at 50 times above safety limits specified by the U.S. Environmental Protection Agency (EPA). In 2002, an inquiry by Fact-Finding Mission on Bhopal found a number of toxins, including mercury, lead, 1,3,5 trichlorobenzene, dichloromethane and chloroform, in nursing women's breast milk.\n\nA 2004 BBC Radio 5 broadcast reported the site is contaminated with toxic chemicals including benzene hexachloride and mercury, held in open containers or loose on the ground. A drinking water sample from a well near the site had levels of contamination 500 times higher than the maximum limits recommended by the World Health Organization. In 2009, the Centre for Science and Environment, a Delhi-based pollution monitoring lab, released test results showing pesticide groundwater contamination up to three kilometres from the factory. Also in 2009, the BBC took a water sample from a frequently used hand pump, located just north of the plant. The sample, tested in UK, was found to contain 1,000 times the World Health Organization's recommended maximum amount of carbon tetrachloride, a carcinogenic toxin.\n\nIn 2010, a British photojournalist who ventured into the abandoned Union Carbide factory to investigate allegations of abandoned, leaking toxins, was hospitalized in Bhopal for a week after he was exposed to the chemicals. Doctors at the Sambhavna Clinic treated him with oxygen, painkillers and anti-inflammatories following a severe respiratory reaction to toxic dust inside the factory.\n\nIn October 2011, the Institute of Environmental Management and Assessment published an article and video by two British environmental scientists, showing the current state of the plant, landfill and solar evaporation ponds and calling for renewed international efforts to provide the necessary skills to clean up the site and contaminated groundwater.\n\nIn 1999, a Hindi film dealing with the tragedy, \"Bhopal Express\", was released. The film stars Kay Kay Menon and Naseeruddin Shah.\n\nAmulya Malladi's 2002 novel \"A Breath of Fresh Air\" relates the story of a mother and son who develop health issues as a result of exposure to gas at Bhopal. The book is based on Malladi's recollections of Bhopal during the incident.\n\nIndra Sinha released \"Animal's People\" in 2007. The novel tells the story of a boy who is born with a spinal condition due to effects of the gas. The book was shortlisted for the Man Booker Prize.\n\nIn 2014, to coincide with the 30th anniversary of the disaster, historical-drama \"\" was released, starring Martin Sheen as Union Carbide CEO Warren Anderson, Kal Penn, and Mischa Barton.\n\nArundhati Roy's 2017 novel \"The Ministry of Utmost Happiness\" which deals with many contemporary political issues in India also features several characters still dealing with the aftermath of the gas leak.\n\nSince 1984, individual activists have played a role in the aftermath of the tragedy. The best-known is Satinath Sarangi (Sathyu), a metallurgic engineer who arrived at Bhopal the day after the leakage. He founded several activist groups, as well as Sambhavna Trust, the clinic for gas affected patients, where he is the manager. Other activists include Rashida Bee and Champa Devi Shukla, who received the Goldman Prize in 2004, Abdul Jabbar and Rachna Dhingra.\n\nSoon after the accident, representatives from different activist groups arrived. The activists worked on organising the gas victims, which led to violent repression from the police and the government.\n\nNumerous actions have been performed: demonstrations, sit-ins, hunger strikes, marches combined with pamphlets, books, and articles. Every anniversary, actions are performed. Often these include marches around Old Bhopal, ending with burning an effigy of Warren Anderson.\n\nCooperation with international NGOs including Pesticide Action Network UK and Greenpeace started soon after the tragedy. One of the earliest reports is the Trade Union report from ILO 1985.\n\nIn 1992, a session of the Permanent Peoples' Tribunal on Industrial Hazards and Human Rights took place in Bhopal, and in 1996, the \"Charter on Industrial Hazards and Human Rights\" was adopted.\n\nIn 1994, the International Medical Commission on Bhopal (IMCB) met in Bhopal. Their work contributed to long term health effects being officially recognised.\n\nImportant international actions have been the tour to Europe and United States in 2003, the marches to Delhi in 2006 and 2008, all including hunger strikes, and the Bhopal Europe Bus Tour in 2009.\n\nAt least 14 different NGOs were immediately engaged. The first disaster reports were published by activist organisations, Eklavya and the Delhi Science Forum.\n\nAround ten local organisations, engaged on long term, have been identified. Two of the most active organisations are the women's organisations—Bhopal Gas Peedit Mahila-Stationery Karmachari Sangh and Bhopal Gas Peedit Mahila Udyog Sangthan.\n\nMore than 15 national organisations have been engaged along with a number of international organisations.\n\nSome of the organisations are:\n\nOn 3 December 2004, the twentieth anniversary of the disaster, a man falsely claiming to be a Dow representative named Jude Finisterra was interviewed on BBC World News. He claimed that the company had agreed to clean up the site and compensate those harmed in the incident, by liquidating Union Carbide for . Dow quickly issued a statement saying that they had no employee by that name—that he was an impostor, not affiliated with Dow, and that his claims were a hoax. The BBC later broadcast a correction and an apology.\n\nJude Finisterra was actually Andy Bichlbaum, a member of the activist prankster group The Yes Men. In 2002, The Yes Men issued a fake press release explaining why Dow refused to take responsibility for the disaster and started up a website, at \"DowEthics.com\", designed to look like the real Dow website, but containing hoax information.\n\nThe release of an email cache related to intelligence research organisation Stratfor was leaked by WikiLeaks on 27 February 2012. It revealed that Dow Chemical had engaged Stratfor to spy on the public and personal lives of activists involved in the Bhopal disaster, including the Yes Men. E-mails to Dow representatives from hired security analysts list the YouTube videos liked, Twitter and Facebook posts made and the public appearances of these activists. Journalists, film-makers and authors who were investigating Bhopal and covering the issue of ongoing contamination, such as Jack Laurenson and Max Carlson, were also placed under surveillance. Stratfor released a statement condemning the revelation by Wikileaks while neither confirming nor denying the accuracy of the reports, and would only state that it had acted within the bounds of the law. Dow Chemical also refrained to comment on the matter.\n\nIngrid Eckerman, a member of the International Medical Commission on Bhopal, has been denied a visa to visit India.\n\n\n\n\n"}
{"id": "28062715", "url": "https://en.wikipedia.org/wiki?curid=28062715", "title": "Brookshire Katy Drainage District", "text": "Brookshire Katy Drainage District\n\nThe Brookshire Katy Drainage District (BKDD) is a political subdivision of the state of Texas which oversees the drainage of water within its boundaries. The district has its headquarters in Brookshire, Texas.\n\n"}
{"id": "27287411", "url": "https://en.wikipedia.org/wiki?curid=27287411", "title": "CIT Program Tumor Identity Cards", "text": "CIT Program Tumor Identity Cards\n\nThe \"Cartes d'Identité des Tumeurs (CIT)\" program, launched and financed by the French charity \"Ligue Nationale contre le Cancer\", aims at refining the molecular knowledge of multiple types of tumors with the prospect of improving or developing better targeted therapeutic approaches. The CIT program mainly relies on the large-scale and systematic profiling of large cohorts of tumors at various molecular levels including at least the genome, the epigenome, and the transcriptome.\n\n\n"}
{"id": "27120731", "url": "https://en.wikipedia.org/wiki?curid=27120731", "title": "Colliford Lake", "text": "Colliford Lake\n\nColliford Lake is a reservoir on Bodmin Moor, Cornwall, England, United Kingdom. Covering more than , it is the second-largest lake in Cornwall. It is situated south of the A30 trunk road near the village of Bolventor, the approximate centre of the lake being at . Dozmary Pool outfalls into the lake and the lake's own outfall forms one of the tributaries of the River Fowey.\n\nThe northernmost point of the lake is approximately three-quarters of a mile (1 km) south of Bolventor at and the headbank at the southernmost point is approximately three miles (5 km) south of Bolventor at .\n\nLeisure facilities on the site include angling and a adventure and nature park, Colliford Lake Park, which features trails and footpaths, play areas, wetlands, picnic areas and a cafe.\n\nColliford Lake is managed by the South West Lakes Trust, an environmental and recreational charity which manages fifty inland water sites in Cornwall, Devon, and Somerset.\n\n"}
{"id": "217631", "url": "https://en.wikipedia.org/wiki?curid=217631", "title": "Comorbidity", "text": "Comorbidity\n\nIn medicine, comorbidity is the presence of one or more additional diseases or disorders co-occurring with (that is, concomitant or concurrent with) a primary disease or disorder; in the countable sense of the term, a comorbidity (plural comorbidities) is each additional disorder or disease. The additional disorder may, also, be a behavioral or mental disorder.\n\nIn medicine, comorbidity describes the effect of all other diseases an individual patient might have other than the primary disease of interest.\n\nThe term can indicate either a condition existing simultaneously but independently with another condition or a related medical condition. The latter sense of the term causes some overlap with the concept of complications. For example, in longstanding diabetes mellitus, the extent to which coronary artery disease is an independent comorbidity versus a diabetic complication is not easy to measure, because both diseases are quite multivariate and there are likely aspects of both simultaneity and consequence. The same is true of intercurrent diseases in pregnancy. In other examples, the true independence or relation is not ascertainable because syndromes and associations are often identified long before pathogenetic commonalities are confirmed (and, in some examples, before they are even hypothesized). In psychiatric diagnoses it has been argued in part that this \"'use of imprecise language may lead to correspondingly imprecise thinking', [and] this usage of the term 'comorbidity' should probably be avoided.\" However, in many medical examples, such as comorbid diabetes mellitus and coronary artery disease, it makes little difference which word is used, as long as the medical complexity is duly recognized and addressed.\n\nMany tests attempt to standardize the \"weight\" or value of comorbid conditions, whether they are secondary or tertiary illnesses. Each test attempts to consolidate each individual comorbid condition into a single, predictive variable that measures mortality or other outcomes. Researchers have validated such tests because of their predictive value, but no one test is as yet recognized as a standard.\n\nThe term \"comorbid\" has three definitions:\n\nThe Charlson comorbidity index predicts the one-year mortality for a patient who may have a range of comorbid conditions, such as heart disease, AIDS, or cancer (a total of 22 conditions). Each condition is assigned a score of 1, 2, 3, or 6, depending on the risk of dying associated with each one. Scores are summed to provide a total score to predict mortality. Many variations of the Charlson comorbidity index have been presented, including the Charlson/Deyo, Charlson/Romano, Charlson/Manitoba, and Charlson/D'Hoores comorbidity indices.\n\nClinical conditions and associated scores are as follows:\n\nFor a physician, this score is helpful in deciding how aggressively to treat a condition. For example, a patient may have cancer with comorbid heart disease and diabetes. These comorbidities may be so severe that the costs and risks of cancer treatment would outweigh its short-term benefit.\n\nSince patients often do not know how severe their conditions are, nurses were originally supposed to review a patient's chart and determine whether a particular condition was present in order to calculate the index. Subsequent studies have adapted the comorbidity index into a questionnaire for patients.\n\nThe Charlson index, especially the Charlson/Deyo, followed by the Elixhauser have been most commonly referred by the comparative studies of comorbidity and multimorbidity measures.\n\nThe comorbidity–polypharmacy score (CPS) is a simple measure that consists of the sum of all known comorbid conditions and all associated medications. There is no specific matching between comorbid conditions and corresponding medications. Instead, the number of medications is assumed to be a reflection of the \"intensity\" of the associated comorbid conditions. This score has been tested and validated extensively in the trauma population, demonstrating good correlation with mortality, morbidity, triage, and hospital readmissions. Of interest, increasing levels of CPS were associated with significantly lower 90-day survival in the original study of the score in trauma population.\n\nThe Elixhauser comorbidity measure was developed using administrative data from a statewide California inpatient database from all non-federal inpatient community hospital stays in California (\"n\" = 1,779,167). The Elixhauser comorbidity measure developed a list of 30 comorbidities relying on the ICD-9-CM coding manual. The comorbidities were not simplified as an index because each comorbidity affected outcomes (length of hospital stay, hospital changes, and mortality) differently among different patients groups. The comorbidities identified by the Elixhauser comorbidity measure are significantly associated with in-hospital mortality and include both acute and chronic conditions. van Walraven et al. have derived and validated an Elixhauser comorbidity index that summarizes disease burden and can discriminate for in-hospital mortality. In addition, a systematic review and comparative analysis shows that among various comorbidities indices, Elixhauser index is a better predictor of the risk especially beyond 30 days of hospitalisation.\n\nPatients who are more seriously ill tend to require more hospital resources than patients who are less seriously ill, even though they are admitted to the hospital for the same reason. Recognizing this, the diagnosis-related group (DRG) manually splits certain DRGs based on the presence of secondary diagnoses for specific complications or comorbidities (CC). The same applies to Healthcare Resource Groups (HRGs) in the UK.\n\nIn psychiatry, psychology, and mental health counseling, comorbidity refers to the presence of more than one diagnosis occurring in an individual at the same time. However, in psychiatric classification, comorbidity does not necessarily imply the presence of multiple diseases, but instead can reflect our current inability to supply a single diagnosis that accounts for all symptoms. On the DSM Axis I, Major Depressive Disorder is a very common comorbid disorder. The Axis II personality disorders are often criticized because their comorbidity rates are excessively high, approaching 60% in some cases, indicating to critics the possibility that these categories of mental illness are too imprecisely distinguished to be usefully valid for diagnostic purposes and, thus, for deciding how treatment resources should be allocated.\n\nThe term 'comorbidity' was introduced in medicine by Feinstein (1970) to denote those cases in which a 'distinct additional clinical entity' occurred during the clinical course of a patient having an index disease. Although the term has recently become very fashionable in psychiatry, its use to indicate the concomitance of two or more psychiatric diagnoses is said to be incorrect because in most cases it is unclear whether the concomitant diagnoses actually reflect the presence of distinct clinical entities or refer to multiple manifestations of a single clinical entity. It has been argued that because \"'the use of imprecise language may lead to correspondingly imprecise thinking', this usage of the term 'comorbidity' should probably be avoided\".\n\nDue to its artifactual nature, psychiatric comorbidity has been considered as a Kuhnian anomaly leading the DSM to a scientific crisis and a comprehensive review on the matter considers comorbidity as an epistemological challenge to modern psychiatry.\n\nMany centuries ago the doctors propagated the viability of a complex approach in the diagnosis of disease and the treatment of the patient, however modern medicine, which boasts a wide range of diagnostic methods and variety of therapeutic procedures, stresses specification. This brought up a question: How to wholly evaluate the state of a patient who suffers from a number of diseases simultaneously, where to start from and which disease(s) require(s) primary and subsequent treatment? For many years this question stood out unanswered, until 1970, when a renowned American doctor epidemiologist and researcher, A.R. Feinstein, who had greatly influenced the methods of clinical diagnosis and particularly methods used in the field of clinical epidemiology, came out with the term of \"comorbidity\". The appearance of comorbidity was demonstrated by Feinstein using the example of patients physically suffering from rheumatic fever, discovering the worst state of the patients, who simultaneously suffered from multiple diseases. In due course of time after its discovery, comorbidity was distinguished as a separate scientific-research discipline in many branches of medicine.\n\nPresently there is no agreed-upon terminology of comorbidity. Some authors bring forward different meanings of comorbidity and multi-morbidity, defining the former, as the presence of a number of diseases in a patient, connected to each other through proven pathogenetic mechanisms and the latter, as the presence of a number of diseases in a patient, not having any connection to each other through any of the proven till date pathogenetic mechanisms. Others affirm that multi-morbidity is the combination of a number of chronic or acute diseases and clinical symptoms in a person and do not stress the similarities or differences in their pathogenesis. However the principle clarification of the term was given by H. C. Kraemer and M. van den Akker, determining comorbidity as the combination in a patient of 2 or more chronic diseases (disorders), pathogenetically related to each other or coexisting in a single patient independent of each disease's activity in the patient.\n\nWidespread study of physical and mental pathology found its place in psychiatry. I. Jensen (1975), J.H. Boyd (1984), W.C. Sanderson (1990), (1993), D.L. Robins (1994), A. B. Smulevich (1997), C.R. Cloninger (2002) and other renowned psychiatrists devoted many years for the discovery of a number of comorbid conditions in patients suffering from most diverse psychiatric disorders. These very researchers developed the first models of comorbidity. Some of the models studied comorbidity as the presence in a person (patient) of more than one disorders (diseases) at a certain period of life, whereas the others elaborated the relative risk, for a person having one disease, of picking up other disorders.\n\nThe influence of comorbidity on the clinical progression of the primary (basic) physical disorder, effectiveness of the medicinal therapy and immediate and long-term prognosis of the patients was researched by talented physicians and scientists of various medical fields in many countries across the globe. These scientists and physicians included: M. H. Kaplan (1974), T. Pincus (1986), M. E. Charlson (1987), F. G. Schellevis (1993), H. C. Kraemer (1995), M. van den Akker (1996), A. Grimby (1997), S. Greenfield (1999), M. Fortin (2004) & A. Vanasse (2004), C. Hudon (2005), L. B. Lazebnik (2005), (2008), G. E. Caughey (2008), F. I. Belyalov (2009), L. A. Luchikhin (2010) and many others.\n\n\nComorbidity is widespread among the patients admitted at multidiscipline hospitals. During the phase of initial medical help, the patients having multiple diseases simultaneously are a norm rather than an exception. Prevention and treatment of chronic diseases declared by the World Health Organization, as a priority project for the second decade of the 20th century, are meant to better the quality of the global population. This is the reason for an overall tendency of large-scale epidemiological researches in different medical fields, carried-out using serious statistical data. In most of the carried-out, randomized, clinical researches the authors study patients with single refined pathology, making comorbidity an exclusive criterion. This is why it is hard to relate researches, directed towards the evaluation of the combination of ones or the other separate disorders, to works regarding the sole research of comorbidity. The absence of a single scientific approach to the evaluation of comorbidity leads to omissions in clinical practice. It is hard not to notice the absence of comorbidity in the taxonomy (systematics) of disease, presented in ICD-10.\n\nAll the fundamental researches of medical documentation, directed towards the study of the spread of comorbidity and influence of its structure, were conducted till the 1990s. The sources of information, used by the researchers and scientists, working on the matter of comorbidity, were case histories, hospital records of patients and other medical documentation, kept by family doctors, insurance companies and even in the archives of patients in old houses.\n\nThe listed methods of obtaining medical information are mainly based on clinical experience and qualification of the physicians, carrying out clinically, instrumentally and laboratorially confirmed diagnosis. This is why despite their competence, they are highly subjective. No analysis of the results of postmortem of deceased patients was carried out for any of the comorbidity researches.\n\n\"It is the duty of the doctor to carry out autopsy of the patients they treat\", said once professor . Autopsy allows you to exactly determine the structure of comorbidity and the direct cause of death of each patient independent of his/her age, gender and gender specific characteristics. Statistical data of comorbid pathology, based on these sections, are mainly devoid of subjectivism.\n\nThe analysis of a decade long Australian research based on the study of patients having 6 widespread chronic diseases demonstrated that nearly half of the elderly patients with arthritis also had hypertension, 20% had cardiac disorders and 14% had type 2 diabetes. More than 60% of asthmatic patients complained of concurrent arthritis, 20% complained of cardiac problems and 16% had type 2 diabetes.\n\nIn patients with chronic kidney disease (renal insufficiency) the frequency of coronary heart disease is 22% higher and new coronary events 3.4 times higher compared to patients without kidney function disorders. Progression of CKD towards end stage renal disease requiring renal replacement therapy is accompanied by increasing prevalence of Coronary Heart Disease and sudden death from cardiac arrest.\nA Canadian research conducted upon 483 obesity patients, it was determined that spread of obesity related accompanying diseases was higher among females than males. The researchers discovered that nearly 75% of obesity patients had accompanying diseases, which mostly included dyslipidemia, hypertension and type 2 diabetes. It is to be noted that among the young obesity patients (from 18 to 29) more than two chronic diseases were found in 22% males and 43% females.\n\nFibromyalgia is a condition which is comorbid with several others, including but not limited to; depression, anxiety, headache, irritable bowel syndrome, chronic fatigue syndrome, systemic lupus erythematosus, rheumatoid arthritis, migraine, and panic disorder.\n\nThe number of comorbid diseases increases with age. Comorbidity increases by 10% in ages up to 19 years, up to 80% in people of ages 80 and older. According to data by M. Fortin, based on the analysis of 980 case histories, taken from daily practice of a family doctor, the spread of comorbidity is from 69% in young patients, up to 93% among middle aged people and up to 98% patients of older age groups. At the same time the number of chronic diseases varies from 2.8 in young patients and 6.4 among older patients.\n\nAccording to Russian data, based on the study of more than three thousand postmortem reports (n=3239) of patients of physical pathologies, admitted at multidisciplinary hospitals for the treatment of chronic disorders (average age 67.8 ± 11.6 years), the frequency of comorbidity is 94.2%. Doctors mostly come across a combination of two to three disorders, but in rare cases (up to 2.7%) a single patient carried a combination of 6–8 diseases simultaneously.\n\nThe fourteen-year research conducted on 883 patients of idiopathic thrombocytopenic purpura (Werlhof disease), conducted in Great Britain, shows that the given disease is related to a wide range of physical pathologies. In the comorbid structure of these patients, most frequently present are malignant neoplasms, locomotorium disorders, skin and genitourinary system disorders, as well as haemorrhagic complications and other autoimmune diseases, the risk of whose progression during the first five years of the primary disease exceeds the limit of 5%.\n\nIn a research conducted on 196 larynx cancer patients, it was determined that the survival rate of patients at various stages of cancer differs depending upon the presence or absence of comorbidity. At the first stage of cancer the survival rate in the presence of comorbidity is 17% and in its absence it is 83%, in the second stage of cancer the rate of survivability is 14% and 76%, in the third stage it is 28% and 66% and in the fourth stage of cancer it is 0% and 50% respectively. Overall the survivability rate of comorbid larynx cancer patients is 59% lower than the survivability rate of patients without comorbidity.\n\nExcept for therapists and general physicians, the problem of comorbidity is also often faced by specialists. Regretfully they seldom pay attention to the coexistence of a whole range of disorders in a single patient and mostly conduct the treatment of specific to their specialization diseases. In current practice urologists, gynecologists, ENT specialists, eye specialists, surgeons and other specialists all too often mention only the diseases related to \"own\" field of specialization, passing on the discovery of other accompanying pathologies \"under the control\" of other specialists. It has become an unspoken rule for any specialized department to carry out consultations of the therapist, who feels obliged to carry out symptomatic analysis of the patient, as well as to the form the diagnostic and therapeutic concept, taking in view the potential risks for the patient and his long-term prognosis.\n\nBased on the available clinical and scientific data it is possible to conclude that comorbidity has a range of undoubted properties, which characterize it as a heterogeneous and often encountered event, which enhances the seriousness of the condition and worsens the patient's prospects. The heterogeneous character of comorbidity is due to the wide range of reasons causing it.\n\n\nThe factors responsible for the development of comorbidity can be chronic infections, inflammations, involutional and systematic metabolic changes, iatrogenesis, social status, ecology and genetic susceptibility.\n\nThe division of comorbidity as per syndromal and nosological principles is mainly preliminary and inaccurate, however it allows us to understand that comorbidity can be connected to a singular cause or common mechanisms of pathogenesis of the conditions, which sometimes explains the similarity in their clinical aspects, which makes it difficult to differentiate between nosologies.\n\nThere are a number of rules for the formulation of clinical diagnosis for comorbid patients, which must be followed by a practitioner. The main principle is to distinguish in diagnosis the primary and background diseases, as well as their complications and accompanying pathologies.\n\nThere is no doubt in the significance of comorbidity, but how to evaluate (measure) it in a given patient?\n\nPatient S., 73 years, called an ambulance because of a sudden pressing pain in the chest. It was known from the case history that the patient suffered from CHD for many years. Such chest pains were experienced by her earlier as well, but they always disappeared after a few minutes of sublingual administration of organic nitrates. This time taking three tablets of nitroglycerine did not kill the pain. It was also known from the case history that the patient had twice suffered during the last ten years from myocardial infarction, as well as from Acute Cerebrovascular Event with sinistral hemiplegia more than 15 years ago. Apart from that the patient suffers from hypertension, type 2 diabetes with diabetic nephropathy, hysteromyoma, cholelithiasis, osteoporosis and varicose pedi-vein disease. It also came to knowledge that the patient regularly takes a number of antihypertensive drugs, urinatives and oral antihyperglycemic remedies, as well as statins, antiplatelet and nootropics. In the past the patient had undergone cholecystectomy due to cholelithiasis more than 20 years ago, as well as the extraction of crystalline tumor due to cataract of the right eye 4 years ago. The patient was admitted to cardiac intensive care unit at a general hospital diagnosed for acute transmural myocardial infarction. During the check-up moderate azotemia, mild erythronormoblastic anemia, proteinuria and lowering of left vascular ejection fraction were also identified.\n\nThere are currently several generally accepted methods of evaluating (measuring) comorbidity:\n\nAnalyzing the comorbid state of patient S, 73 years of age, using the most used international comorbidity assessment scales, a doctor would come across totally different evaluation. The uncertainty of these results would somewhat complicate the doctors judgment about the factual level of severity of the patient's condition and would complicate the process of prescribing rational medicinal therapy for the identified disorders. Such problems are faced by doctors on everyday basis, despite all their knowledge about medical science. The main hurdle in the way of inducting comorbidity evaluation systems in broad based diagnostic-therapeutic process is their inconsistency and narrow focus. Despite the variety of methods of evaluation of comorbidity, the absence of a singular generally accepted method, devoid of the deficiencies of the available methods of its evaluation, causes disturbance. The absence of a unified instrument, developed on the basis of colossal international experience, as well as the methodology of its use does not allow comorbidity to become doctor \"friendly\". At the same time due to the inconsistency in approach to the analysis of comorbid state and absence of components of comorbidity in medical university courses, the practitioner is unclear about its prognostic effect, which makes the generally available systems of associated pathology evaluation unreasoned and therefore un-needed as well.\n\nThe effect of comorbid pathologies on clinical implications, diagnosis, prognosis and therapy of many diseases is polyhedral and patient-specific. The interrelation of the disease, age and drug pathomorphism greatly affect the clinical presentation and progress of the primary nosology, character and severity of the complications, worsens the patient's life quality and limit or make difficult the remedial-diagnostic process. Comorbidity affects life prognosis and increases the chances of fatality. The presence of comorbid disorders increases bed days, disability, hinders rehabilitation, increases the number of complications after surgical procedures, and increases the chances of decline in aged people.\n\nThe presence of comorbidity must be taken into account when selecting the algorithm of diagnosis and treatment plans for any given disease. It is important to enquire comorbid patients about the level of functional disorders and anatomic status of all the identified nosological forms (diseases). Whenever a new, as well as mildly notable symptom appears, it is necessary to conduct a deep examination to uncover its causes. It is also necessary to be remembered that comorbidity leads to polypragmasy (polypharmacy), i.e. simultaneous prescription of a large number of medicines, which renders impossible the control over the effectiveness of the therapy, increases monetary expenses and therefore reduces compliance. At the same time, polypragmasy, especially in aged patients, renders possible the sudden development of local and systematic, unwanted medicinal side-effects. These side-effects are not always considered by the doctors, because they are considered as the appearance of comorbidity and as a result become the reason for the prescription of even more drugs, sealing-in the vicious circle. Simultaneous treatment of multiple disorders requires strict consideration of compatibility of drugs and detailed adherence of rules of rational drug therapy, based on E. M. Tareev's principles, which state: \"Each non-indicated drug is contraindicated\" and B. E. Votchal said: \"If the drug does not have any side-effects, one must think if there is any effect at all\".\n\nA study of inpatient hospital data in the United States in 2011 showed that the presence of a major complication or comorbidity was associated with a great risk of intensive-care unit utilization, ranging from a negligible change for acute myocardial infarction with major complication or comorbidity to nearly nine times more likely for a major joint replacement with major complication or comorbidity.\n\n\n\n"}
{"id": "12475649", "url": "https://en.wikipedia.org/wiki?curid=12475649", "title": "Edward Boyse", "text": "Edward Boyse\n\nEdward A. Boyse (August 11, 1923 – July 14, 2007) was a British-born, American physician and biologist best known for his research on the immune system and pheromones. Boyse was born in Worthing, England and studied medicine at the University of London.\n\nBoyse joined the staff of Sloan-Kettering in New York City in 1962 following an appointment at New York University. He was a professor of biology at Cornell University Medical College between 1969 and 1989 and a professor at the University of Arizona between 1989 and 1994.\n\nBoyse and others were among the earlier researchers to look at how the immune system responded to antigens using mice focussing on the role of white blood cells. In 1975, he won the Cancer Research Institute William B. Coley Award for distinguished research in immunology. in 1976 he won the Isaac Adler prize awarded jointly by Harvard and Rockefeller Universities. He later studied how animals can communicate through odors. Boyse was the first to propose that umbilical cord blood could be used in place of bone marrow for hematopoietic reconstitution.\n\nBoyse was a member of the National Academy of Sciences, a Fellow of the Royal Society, and a member of the American Academy of Arts and Sciences.\n\nBoyse retired in Tucson, Arizona where he died in 2007 from pneumonia, aged 83.\n"}
{"id": "40205426", "url": "https://en.wikipedia.org/wiki?curid=40205426", "title": "Ernesto Pinto-Bazurco", "text": "Ernesto Pinto-Bazurco\n\nErnesto Pinto-Bazurco was a Peruvian doctor. Born in Bellavista (El Callao, Peru) on 28 September 1913. During the Second World War he was assigned by the Swiss consulate to oversee the interests of Peruvian citizens living in Munich, Germany. During that time Ernesto Pinto-Bazurco issued visas that allowed some Jewish families to leave Germany.\n\nErnesto Pinto-Bazurco was born on 28 September 1913 to Moises Pinto-Bazurco and Rosa, née Alcalde. Moises Pinto-Bazurco was a master in mathematics and an officer in the Peruvian navy. Rosa Alcalde was a founder of a transport passenger business in Lima.\n\nIn 1934 Ernesto Pinto-Bazurco travelled to Germany, contracted by the pharmaceutical company Bayer to study medicine. Pinto-Bazurco participated in various activities at the Casa Peru in Munich, which was a Multi-Cultural centre frequented by mainly Peruvians, Latin Americans and locals to the region of Bavaria, Germany.\n\nThat is where Pinto-Bazurco met his future wife Hildegard Rittler, native of Rosenheim, Bavaria, with whom he had 3 children: Rosa, born in 1942; Oscar Aurelio, born in 1945 and Ernesto, born in 1946.\n\nAt the start of World War II many Latin-Americans immigrants in Germany fled to other countries. Ernesto Pinto-Bazurco decided to stay in Munich, serving gratitude to the country that allowed him to study medicine and realizing that during war times the demand for doctors would be high.\n\nDuring the first years of World War II Pinto-Bazurco worked as a doctor in a Munich hospital (Allgemeines Krankenhaus). The Nazi regime denied his 3 marriage attempts to Hildegard Rittler suggesting that it wasn’t acceptable that German women would marry a Peruvian where they were so many German men with whom she could marry.\n\nIn early 1942, after the Japanese attack on Pearl Harbour, Peru broke its relations with countries under the Axis alliance and withdrew its diplomats from Germany. The Peruvian consulate in Munich was transferred to Switzerland.\n\nOn 17 February, Ernesto Pinto-Bazurco was arrested by the Gestapo on the accusations of spying in the favour of the allies, on the grounds that his father was a Navy officer for a nation that was in opposition to the Nazi regime and the fact that he was one of a few Peruvians that stayed after the war had started. He was first imprisoned in the Munich Police station and then he was transferred to a fort in the city of Laufen very close to the Austrian border. He was liberated on 14 May of the same year without any arrest charges.\n\nAs soon as Peruvians relations broke with Germany, the Swiss Confederation assumed representation of Peru in Germany. The Swiss consulate in Germany assigned Ernesto Pinto-Bazurco to represent the interests of Peruvian citizens living in Munich.\n\nUnder these circumstances, Pinto-Bazurco is wanted by some Jewish families who were in hiding and who asked Pinto-Bazurco for help to escape Nazi persecution. When he asked one of these Jewish families (they were escaping to Argentina) why they sought his help, they told him that the surname Pinto is of Sephardic Jewish origin.\n\nPinto-Bazurco granted the necessary documentation without asking for any payment or benefit in return. Furthermore, Ernesto Pinto-Bazurco took care of some Jewish families, who dared not go to a public hospitals for fear of being arrested by the Nazis.\n\nWhen the Second World War ended, the Peruvian ship Rimac repatriated some Peruvians who had stayed during the war. Among them was the Pinto-Bazurco family. In Peru, Hildegard Rittler carried out peaceful activities and promoted the positives that German culture had to offer, earning her the German \"Cross of Merit\". Hildegard Rittler wrote her testimonial about those years lived with Ernesto Pinto-Bazurco in the novels \"Angels in Hell\" and \"When love conquers war\".\n"}
{"id": "26064110", "url": "https://en.wikipedia.org/wiki?curid=26064110", "title": "Food Balance Sheet", "text": "Food Balance Sheet\n\nFood balance sheet shows the a brief picture of the pattern of the food supply of a country. For each food item, it sketches the primary commodity availability for human consumption i.e. the sources of supply and its utilization in terms of nutrient value.\n\nA food balance sheet is a comprehensive compilation of a selected country's food supply during a specific time period. The food balance sheet shows the food items for human consumption, along with how it is produced, used, imported/exported, and how it benefits the society (per capita supply). The total quantity produced in a country added to the total quantity imported and adjusted to any change in stocks that may have occurred since the beginning of the reference period gives the supply available during that period. On the utilization side a distinction is made between the quantities exported, fed to livestock + used for seed, losses during storage and transportation, and food supplies available for human consumption. The per capita supply of each such food item available for human consumption is then obtained by dividing the respective quantity by the related data on the population actually partaking in it. Data on per capita food supplies are expressed in terms of quantity and by applying appropriate food composition factors for all primary and processed products also in terms of dietary energy value, protein and fat content.\n\nThe food balance sheet covers production, trade, feed and seed, waste, other utilization, availability, quantities, calories, proteins, and fats. By combining these elements, one is able to detect the food security of a country, how reliant it is on imported crops/foodstuffs, and how it attributes to world exports.\n\nAccording to Margaret Buchanan-Smith of the tatti Overseas Development Institute, London, the food balance sheet is the most traditional and widely used method of translating \"early warning data into food aid requirements.\"\n\nThe EAC Regional Food Balance has been developed by the EAC Secretariat in collaboration with EAC Partner States (relevant public and private sector organizations). It is developed further to the provisions of the EAC Food Security Action Plan.\nThe background behind its development is recognition of it being an important tool towards the regions efforts of managing the food security situation. In principle, the Regional Food Balance Sheet is expected to give accurate and reliable information about food availability in the current period. It is a departure from the known traditional food balance sheet which gave historical information.\nThe RFBS is therefore a powerful tool for use by Governments in determining adequacy of food supplies for purposes managing food security concerns. Where the RFBS posts food surplus, the immediate impact is to release funds that the Government would have tied up in securing food not knowing that the region has sufficient food. It also encourages Governments to come up with proactive trade policy measures to encourage flow of food from countries which the RFBS reveal as having surplus. It therefore leads to actualization of the objectives of the EAC Customs Union of free flow of goods within the EAC region.\nThe RFBS is a powerful tool for the private sector to use in planning exports and imports. First preference due to preferential duties is the EAC regional market, where the private sector has an advantage over other supplies coming from outside the EAC region. It enables the private sector to use the hedge of Common External Tariffs which is in favour of intra-EAC trade to exploit trade potential for the various products covered under the RFBS. The RFBS is also a tool that the private sector will use to engage Governments whenever there are reported short fall in supplies as revealed by RFBS, cannot meet. In such a case, the RFBS data will be used to lobby for temporary stay of application of Common External Tariff for the products facing regional shortages.\nThe RFBS is a powerful tool for Relief Agencies to use in determining regional food security crisis and planning for relief supplies. It is also a useful tool for the Relief Agencies to use in sourcing food supplies from the region based on the revealed food availability.\nThe scope of the comprehensive EAC Regional Food Balance Sheet has been informed by the EAC Food Security priority products that are covered under the following components\na) Cereals and Pulses\nb) Livestock (Meat, Dairy Products and Animal Fats)\nc) Fisheries\nd) Horticulture (Fruits, Vegetables and Roots and Tubers)\ne) Industrial Crops (Sugar and Sugar Products, Oil Crops and Vegetable Oils)\n\nMore information on the methodology can be found on http://www.rfbs.in \n\nIn Ivory Coast, a developing country, food security is an issue. This is apparent under Ivory Coast's 2009 (the most recent statistics) food balance sheet. Cereals- including wheat, corn, rye, oats, etc.- are a large part of the Ivorian diet. 1834000 metric tons are imported, 1167000 metric tons are produced, while only 112000 metric tons are exported. \nCocoa production in Ivory Coast is the largest cocoa industry in the world, accounting for 30% of the World's production as of 2009. While producing 1223000 metric tons, Ivorians export 1192000 metric tons, making cocoa a major cash crop.\n\n"}
{"id": "26565602", "url": "https://en.wikipedia.org/wiki?curid=26565602", "title": "Granulomatous prostatitis", "text": "Granulomatous prostatitis\n\nGranulomatous prostatitis is an uncommon disease of the prostate, an exocrine gland of the male reproductive system. It is a form of prostatitis, i.e. inflammation of the prostate, resulting from infection (bacterial, viral, or fungal), BCG vaccine, malacoplakia or systemic granulomatous diseases which involve the prostate.\n\nProstatic secretions escape into the stroma and elicit an inflammatory response.\n\nNoticeable destruction of Acini, surrounded by epitheloid cells, giant cells, lymphocytes, plasma cells and dense fibrosis.\n"}
{"id": "6632841", "url": "https://en.wikipedia.org/wiki?curid=6632841", "title": "Group home", "text": "Group home\n\nA group home is a private residence model of medical care for those with complex health needs. Traditionally, the model has been used for children or young people who cannot live with their families, people with chronic disabilities who may be adults or seniors, or people with dementia and related aged illnesses. Typically, there are no more than six residents, and there is at least one trained caregiver there 24 hours a day. In some early \"model programs\", a house manager, night manager, weekend activity coordinator, and 4 part-time skill teachers were reported. Originally, the term group home referred to homes of 8 to 16 individuals, which was a state-mandated size during deinstitutionalization. Residential nursing facilities, also included in this article, may be as large in 2015 as 100 individuals, which is no longer the case in field such as intellectual and developmental disabilities. Depending on the severity of the condition requiring one to need to live in a group home, some clients are able to attend day programs and most clients are able to live normal lifestyles.\n\nThe group homes highlighted in news articles in the late 1970s and 1980s, and by the late 2000s, have been cited internationally as a symbol or emblem of the community movement. Group homes were opened in local communities, often with site selection hearings, by state government and non-profit organizations including the international L'Arche, the local chapters of the Arc of the United States (then Association for Retarded Children), United Cerebral Palsy local agencies, agencies belonging to state associations such as ACLAIMH (Association of Community Living Administrators in Mental Health), and NYSACRA (New York State Association of Community Residence Administrators) in New York, and new, non-profit organizations in the field of mental health. Group homes are one category in a broader array, spectrum, continuum, or services systems plan for residential community services or Long-Term Services and Supports (LTSS).\n\nAnother context in which the expression “group home” is used is referring to residential child care communities and similar organizations, providing residential services as part of the foster care system. There is a considerable variety of different models, sizes and kinds of organizations caring for children and youth who cannot stay with their birth families.\n\nA group home in a local community is what the government and universities term a \"small group home.\" Group homes always have trained personnel, and administration located both for the home and outside the home at office locations. Larger homes often are termed residential facilities, as are campuses with homes located throughout a campus structure.\n\nK. C. Lakin of the University of Minnesota, a deinstitutionalization researcher, has indicated that a taxonomy of residential facilities for individuals with mental retardation includes program model, size and operator, and facilities also then vary by disability and age, among other primary characteristics. Prior residential facility classifications were described by Scheerenberger until the modern day classification by David Braddock on a state-by-state basis which includes individuals in residential settings of six or fewer, one categorical group. In 2014, typologies of residential services in intellectual disabilities include new categories of supported living, personal assistance services, individual and family support, and supported employment.\n\nResidents of group homes usually have a disability, such as autism, intellectual disability, chronic or long-term mental/psychiatric disorder, or physical or even multiple disabilities because those are the non-profit and state-regional organizations which began and operated the homes. Some group homes were funded as transitional homes to prepare for independent living (in an apartment or return to family or marriage and employment), and others were viewed as permanent community homes. Society may prevent people with significant needs from living in local communities with social acceptance key to community development. The residents sometimes need continual or supported assistance in order to complete daily tasks, such as taking medication or bathing, making dinners, having conversations, making appointments, and getting to work or a day service.\n\nGroup homes were revolutionary in that they offered individuals life opportunities to learn to cook and prepare meals (e.g., individuals with severe and even \"profound\" disabilities), budget their personal allowance, select photos for their room or album, meet neighbors and \"carry out civic duties,\" go grocery shopping, eat in restaurants, make emergency calls or inquiries, and exercise regularly.\n\nSome residents may also have behavioral problems that require a better daily routine, medical assessment for possible health care needs (e.g., pituitary problem, medication adjustment), environmental changes (e.g., different roommates), mental health counseling, specialist or physician consultation, or supervision; government may require a finding of involuntary care (i.e. dangerous to themselves or others) which is a hotly contested and disputed arena. Individuals who move from psychiatric hospitals (and intellectual disability institutions) also may need medications reduced, with psychiatric symptoms often only moderately addressed (\"modest efficacy\") in this manner with known side effects of long-term use. The community living movement has been very successful in the US and other countries, and is supported in 2015 by the \"UN Convention on the Rights of Persons with Disabilities\" (UN, 2006).\n\nPrior to the 1970s this function was served by institutions, asylums, poorhouses, and orphanages until long-term services and supports, including group homes were developed in the US. The primary frameworks in the US undergirding group homes are often termed social and functional competency-based (e.g., community participation, social role valorization, social and community acceptance, self-determination, functional home and community skills) and another, positive behavioral supports (which may be considered overly structured for homes and home life). Positive behavioral supports were developed, in part, to assist with \"management problems\" of the residential facilities. Group home residents may be found in workplaces, day services, parks and recreation programs, schools, shopping centers, travel locations, and with family, neighbors, community workers, coworkers, schoolmates, and friends.\n\nIn addition, new laws required that schools serve children with what was often known as \"special needs\" or \"exceptional children\" adapting school and afterschool programs to meet the needs of the new population groups. Douglas Biklen in his award winning \"Regular Lives\" highlighted 3 schools in Syracuse, New York integrating the severely disabled in conjunction with his new book, \"Achieving the Complete School: Strategies for Effective Mainstreaming\".\n\nPeople who live in a group home offering support services may be developmentally disabled, recovering from alcohol or drug addiction (e.g., who may have attended a youth drug court hosted by the judicial system), abused or neglected youths, youths with behavioral or emotional problems, and/or youths with criminal records (e.g., a person in need of supervision). Group homes or group facilities may also provide residential treatment for youth for a time-limited period, and then involve return of the youth to the family environment. Similarly, drug, addictions and alcohol programs may be time-limited, and involve residential treatment (e.g., Afrocentric model for 24 women and children, as part of Boston Consortium of Services).\n\nResidential treatment centers and other organized mental health care for children with emotional needs, among our highest health and human service efforts, was reported at 440 organizations nationally in 1988, representing 9% of mental health organizations. Residential treatment centers were considered largely inappropriate for many of the children who needed better community support services. Restructuring of these systems was proposed to promote better prevention and family support for children in mental health systems similar to international initiatives in \"individualized family support program\". Residential treatment is one part of an array of community services which include therapeutic foster care, family support, case management, crisis-emergency services, outpatient and day services, and home-based services. During this period, residential treatment was also compared to supported housing, also called supportive housing for its role in comprehensive service system developments, though often for adults who may need or desire services.\n\nGroup homes have a good community image, and were developed in the intellectual disability and mental health fields as a desirable middle class option located in good neighborhoods after a faulty start in poorer neighborhoods in the US. Group homes were often built in accordance with principle of normalization (people with disabilities), to blend into neighborhoods, to have access to shopping, banks, and transportation, and sometimes, universal access and design. Group homes may be part of residential services \"models\" offered by a service provider together with apartment programs, and other types of \"followalong\" services. Yet, in 2015, the homes and personnel continue to meet the challenges of a changing multicultural society, and changing and norms in areas such as gender expectations.\n\nA group home differs from a halfway house, the latter which is one of the most common terms describing community living opportunities in mental health in the 1970s' medical and psychiatric literatures. Specialized halfway houses, as half was between the institution and a regular home, may serve individuals with addictions or who may now be convicted of crimes, though very uncommon in the 1970s. Residents are usually encouraged or required to take an active role in the maintenance of the household, such as performing chores or helping to manage a budget. In 1984, New York's state office in intellectual and developmental disabilities described its service provision in 338 group homes serving 3,249 individuals. Some of these homes were certified as intermediate care facilities (ICF-MRs) and must respond to stricter facility-based standards.\n\nResidents may have their own room or share rooms, and share facilities such as laundry, bathroom, kitchen, and common living areas. The opening of group homes in neighborhoods is occasionally opposed by residents who fear that it will lead to a rise in crime and/or a drop in property values. However, repeated reviews since the 1970s indicate such views are unfounded, and the homes contribute to the neighborhoods. In the late 1970s, local hearings were conducted in states such as New York, and parents of children with disabilities (e.g., Josephine Scro in the \"Syracuse Post Standard\" on June 7, 1979), research experts, agency directors (e.g., Guy Caruso of the Onondaga County Arc, now at Temple University) and community-disability planners (late Bernice Schultz, county planner) spoke with community members to respond to their inquiries. The late Josephine Scro later became a director of a new family support agency in Syracuse, New York, to assist other families with children with disabilities with family supports in their own homes and local communities, too.\n\nA group home can also refer to family homes in which children and youth of the foster care system are placed, sometimes until foster families are found for them, sometimes for long-term care. Homes which are termed group foster care operate under other standards than those termed group homes, including different management systems and departments.\n\nUnrelated children or sibling groups live in a home-like setting with either a set of house parents or a rotating staff of trained caregivers. Specialized therapeutic or treatment group homes are available to meet the needs of children with emotional, intellectual, physical, medical and/or behavioral difficulties.\n\nGroup homes for children provide an alternative to traditional foster care, though family support to the birth, adoptive, and foster families are often first recommended. Several sources state that, in comparison to other placement alternatives, this form of care is the most restrictive for youth in the foster care system. The term group home is often confused with lock-down treatment centers, which are required to have eyes-on every so often due to behavioral and mental challenges of the children and youth they serve. There are also less restrictive forms of group homes, which often use the house parent model. Those organizations are due to their visual comparability to several foster families within a certain area as well as their connectedness to each other, the community and internally best described as residential child care communities.\n\nGroup homes and foster homes have been compared and studied in national samples. Group homes were studied as part of a national sample of community living for individuals with severe disabilities, and small group homes 6 or under were among the recommended options, often for adults.\n\nNewer options of group living were often termed supported living, supported housing, individual and family supports, or early on, \"individualized supportive living arrangements\" (e.g., apartment programs). These developments often followed analyses of homes as homes, ordinary housing and support services, versus group treatment or facilities, an important critique during the 1980s and 1990s reform period. Independent living continued to be a primary framework representing another emblem of community living more often associated with personal assistance and live-in attendants, home health services, and the now termed allied health services of physical and occupational therapy, speech, cognitive therapy, and psychological counseling. However, leading psychiatric survivors examined independent living in the context of supportive housing and necessary support services which did not need to be congregated in housing.\n\nPerhaps the largest group of group homes (now termed community residential services or residential care by other managements) fall under the heading of residential care homes for seniors, or both seniors and individuals with disabilities. Residential care categories include over 43 separate regulated categories by state governments and now have the new assisted living growing in the US. Group facilities (e.g., funded as large as 100 individuals in a nursing facility or on old-style campus of over 12 wards on the outskirts of cities) or homes for seniors (e.g., room and board) are designed for seniors who cannot live on their own due to physical or mental disabilities.\nGroup facilities, which may involve over half of the allotted beds or more (80%) funded by Medicaid, might also be found under Residential Care Home, Residential Care Facility for the Elderly, or Assisted Living Facility. Alternative community options for these seniors are home health care, hospice care, specialized care (e.g., Alzheimer's), day care at senior centers, meals on wheels, transportation drivers, and other aging and disability options.\n\nIn most countries, people can still vote and attend university while in a group home. Internet usage in group homes, however, may be severely limited. Trips to public libraries may vary depending on the distance from the group home to the library. While 93% of the Canadian population has easy access to a public library, it is uncertain about the percentage of Canadian group home residents who actually have unrestricted access to a public library in lieu of watching television.\n\nEmployment opportunities, where available, are encouraged for group home residents, depending on the home, operator, and characteristics of the residents. Since the 1970s, people with cognitive or mental health disabilities have been involved in community employment of all kinds and also have developed freestanding affirmative industries and supported employment services in conjunction with the government. These rights are protected under the Americans with Disabilities Act of 1990, now revised in 2008 Human rights laws, still operational in states, govern employment applications for employment, and the employer is restricted from asking pre-employment questions on criminal arrests or discriminating on this basis (See, Human Rights Laws of the state of New York). However, unbeknownst to many communities and organizations, management rights, instead of human rights, have been inserted in contracts in the US.\n\nIn the US it has been the position of state mental health commissioners that many people who are living independently should be placed in intensive treatment, as described in a mid-1980s article in the \"Community Mental Health Journal\". The authors held that only 12 of 3,068 individuals should be living independently (p. 199) based on their model predictions. In contrast, the continuum model has been critiqued as restrictive of rights, facility-based, and restrictive of community participation resulting in a US Supreme Court decision recognizing the most integrated setting (Consortium of Citizens with Disabilities, 2012).\n\nIncreasingly, concern has been voiced over the rise in community treatment orders, medical homes, invasive supervision in homes, in addition to decades of outcry over involuntary procedures in psychiatry in the US and restrictions on human rights. In this field, no viable recourse exists for reversing actions by personnel, including professional and medical malpractice, and the most successful programs are viewed as those that result in high compliance. High medication usage is required, often against the law, and the situation worsens during any police-enforced confinement. Group homes in the non-profit sector are often operated by other than the providers involved in state or private, for-profit involuntary care.\n\nThe nursing facility industry holds the position, often with its affiliated hospitals, that it decides on involuntary treatment of elders, which involves issues such as visitations. Nursing homes have had a very long history of reviews and complaints including to the federal level of the Government Accountability Office (GAO) in the US and have been the subject of major reform efforts. Today, a Red Cross ombudsman may be available in the homes, special needs units may be available to assist in areas such as bathing and eating, and in some cities, short term rehabilitation is provided for seniors at those sites instead of at community locations. Nursing facilities, unlike the small size standard of the Centers for Disease Control (CDC) for homes for individuals with intellectual disabilities, may have over 100 \"institutional clients\" on site and is reporting 2-3% restraint use.\n\nGroup home personnel are considered in 2015 to be Direct Support Professionals though paramount in this approach are maintaining a home atmosphere, routines, and community life. An abundance of literature in the 1980s and 1990s described the training needs of personnel, and today new expectations continue to occur as the homes become increasingly health care financed and more self-direction options become available.\n\nFoundational in all helping professions are what are called \"critical skill domains,\" which are congruent with a community support approach (e.g., values clarification, general fluency and flexibility of thought, perception and response, competence in academic content, verbal communications) (Cole & Lacefield, 1978). In addition, with the multicultural workforce, cultural awareness, even skills like using chopsticks, are desired in the adaptive skill domains and comparisons between fast food and sit down restaurants.\n\nBy the 1990s, greater emphasis was placed on community participation and belonging, in addition to welcoming support of the community and community members. In fact, several national research centers in the US were funded, in part, on the basis of community research studies in community participation \n\nEducation also occurs for special population groups or particular issues or needs; an example are the challenges gay men face in living with chronic illness including HIV-AIDS which may be addressed in supported housing options. Attention is also paid to developing residential services which meet the preferences of persons with serious mental illness and their families.\n\nEducation and training in independent living from long-term care institutions (e.g., acute care facilities, long-term rehabilitation facilities, skilled nursing or intermediate care facilities, community re-entry facilities) often involved changing from forced dependency to controlling and deciding one's own destiny called self-determination. Life skills ranged from health and hygiene, parenting/child care, home maintenance, money management, activities of daily living, community awareness and mobility, legal awareness, social/interpersonal skills, and family involvement (Condeluci, Cooperman, & Self, 1987). These services may be called post-acute services, and involve other personnel models, such as life coaches (Jones, Patrick, Evans, & Wuff, 1991). Independent living training has also proved effective in addressing the needs and expectations of individuals who have sensory impairments (e.g., hearing or blindness).\n\nResidential services costs have been studied in depth in areas that relate to group homes, family care homes or community residential services, especially on deinstitutionalization, Medicaid home and community-based waiver development, and community development. Residential treatment, often provided in larger facilities, may be higher in reimbursement rates to the provider so treatment billings will be found for higher-cost professional services (e.g., behavioral health). Surprisingly, except for very small sizes, the larger, medicalized facilities bill the highest costs per individual (e.g., intermediate care facilities over 16 in the state of New York).\n\nIn relationship to the individual or family, residential services are expensive for low or middle-class families, and federal, state and local government often contribute to these costs. Medicaid-funded options may require use of assets, and Social Security Disability or Social Security are also part of payment plans. New options called family-directed and user-directed involve transfer of funds to homes and families, and continue to be in process in states. Early organizations provided information on their management and financing to help local communities replicate or begin their own homes and programs.\n\nResidential care homes, run by the government or by the for-profit and non-profit industries, need not be low cost and/or low quality as many might initially guess, though traditional room and boards may be based primarily on a Social Security Disability payment and limited governmental personnel assistance. More expensive residential care homes now exist to offer a family-style, high quality, care option to the next class of senior care which is Assisted Living Facilities. These homes, operated often by the nursing care industry, are based on increasing need for assistance and decreasing independence. Unlike the proposals for upgraded community services in homes and communities for seniors with substantial needs, assisted living was primarily developed as facility types only; supported housing also was a new model as state initiatives.\n\nThere are various levels of residential care homes for seniors, which is the traditional medical system of assessments, which differs from developing person-centered plans and support services for persons who may have substantial health care needs and also from new managed Medicaid care plans. In addition, in some fields, the plan is for the individual to age in place in their group home setting. Personal care assistance is often associated with aging in place and independent living services; local governments have been reluctant to pay for other than limited services in the homes (one study stated up to 20 hours maximum, others 3–4 hours per week), in spite of a nationwide decades press toward our own governments. This position is similar to a governmental position to pay not for ordinary goods, but only for specialized services.\n\nHowever, senior services of other kinds, including the senior centers, low cost meals, transportation, Veteran's health services and independent clubs, specialized day care (e.g., day care for older adult policies in Great Britain), local case managers, local Offices of the Aging (with Disability coordinators in some locations), and so forth are often available. Senior programs may also involve joint integration initiatives by aging and disability agencies resulting in leading programs such as social model day programs in Oneida County, New York, Rhode Island's Apartment Residence, Madison County Integration Program, and supported retirement programs in the state of Utah.\n\nAssisted living is a modernization effort (e.g., more choices or menus of services) in the nursing care fields which primarily resulted in modernization, to some extent, of the large facility (i.e., nursing homes) or campus models. Large state initiatives can be found in Linking Housing and Services for Older Adults representing response to long-term criticism of a facility-based service industry. However, a recent nursing industry schema, reflecting a provider network, for levels of care states: \"Assisted Living With no Assistance\" (the most common use of \"assisted living\" involves little or no assistance, living at home with minimal amounts of home care), \"Assisted Living with Assistance\", and \"Assisted Living - Memory Care\". Memory care is for those dealing with memory loss, dementia, or Alzheimer's disease.\n\nHowever, the call nationwide is for caregiving services in the homes where aging parents often move to live with their adult children and their families. The provider sector desired are those that respect the wishes of the individual and the family, including for care at home through hospice. The \"New Politics of Old Age Policy\" (Robert Hudson, 2005/2010) calls for the government entertaining care credits or generous minimum benefits to assist US families to juggle paid and unpaid work in today's modernized world. In addition, as parents age, adults with disabilities who may be living at home will also need assistance that might not have been needed earlier (e.g., siblings, new home).\n\n"}
{"id": "15624665", "url": "https://en.wikipedia.org/wiki?curid=15624665", "title": "Gurgaon kidney scandal", "text": "Gurgaon kidney scandal\n\nThe multi-billion rupee Gurgaon kidney scandal came to light in January 2008 when police arrested several people for running a kidney transplant racket in Gurgaon, an industrial township near New Delhi, India. Kidneys from most of the victims, who were the poor hailing from the nearby western Uttar Pradesh, were transplanted into clients from the United States, United Kingdom, Canada, Saudi Arabia and Greece. The police raid was prompted by complaints by the locals from Moradabad about illegal kidney sales. The man accused of the scandal, Amit Kumar, was arrested in Nepal on 7 February 2008 and has denied any hand in criminal activity.\n\nOn 24 January 2008, police teams from Haryana and Uttar Pradesh raided a residential building and a guest house owned by Amit Kumar.\n\nAccording to the Gurgaon police, the scandal at a local clinic was going on for six to seven years. The donors were lured with offerings of about Rs. 30,000 for kidney removal. First, they were lured to the clinic on the pretext of job opportunities. They were instead asked for donating their kidneys for the fee and all those who resisted this were drugged against their will and subsequently operated upon.\n\nThe Haryana police, under whose jurisdiction the scandal happened, issued arrest warrants against Upendra Aggarwal, a general physician and an associate of Amit Kumar for his involvement in the scandal. However, at the time of the police raid, Kumar and his other accomplices escaped after the knowledge of possible arrests.\n\nThe raid helped rescue five people and shifted them to a Gurgaon hospital.\n\nOn 25 January 2008, the police detained a United States-based non-resident Indian couple and three Greek nationals, two among them being patients receiving the transplants.\n\nThe police revealed that Dr. Amit Kumar and his accomplices had performed 600 kidney transplants in the past decade. Additionally, at least two hospitals were involved in the after care of patients. Police, through the technology of fingerprinting, determined that Kumar went by many aliases and had been previously arrested at least four different times for illegal organ trade operations. It was further revealed that Kumar, his brother Jeevan Kumar, Upendra Aggarwal and Saraj Kumar, an anesthesiologist were previously arrested thrice on charges of illegal human organ transplantation in Delhi, Andhra Pradesh and Maharashtra. They were, however, released on bail. On 7 January 2008 Kumar was arrested by Delhi Police but was released on a bribe of Rs. 20 lakhs. Jeevan Kumar was later arrested on 17 February 2008 in Delhi.\n\nThe Indian Medical Association, arranged a probe by its three-member committee, and further requested investigation by Central Bureau of Investigation (CBI), India's higher investigation agency. The Haryana police further uncovered 2 hospitals and 10 laboratories in Greater Noida and Meerut, cities nearby to New Delhi for their alleged involvement in the scandal.\n\nIn the meanwhile, a Gurgaon court had issued arrest warrants for Amit Kumar and his brother, Jeevan Kumar Rawat. With growing suspicions that Kumar might have fled the country, the Haryana police requested the CBI to alert the Interpol. Thereafter, Red corner notices were issued for the Kumar siblings.\n\nOn 7 February 2008, Amit Kumar was arrested in the neighboring country of Nepal. He was hiding in a wildlife resort, about 35 miles from the Indo-Nepal border. He had a bank draft worth Rs. 936,000 along with a total of €145,000 and $18,900 in cash. At the resort he made an unsuccessful attempt to bribe the Nepali policemen to let him go. The charges filed against him by CBI are under sections 326 (voluntarily causing grievous hurt by dangerous weapon), sections 342 (wrongful confinement), sections 420 (cheating) and sections 120B (criminal conspiracy).\n\nIn March 2013, a CBI special court convicted five accused while acquitting another five in the case of a Gurgaon kidney transplant racket that was busted in 2008. Dr Upender Dublesh and Dr Amit Kumar, who was termed a \"quack\" in no uncertain terms by the court, got seven-year rigorous imprisonment (RI) besides a fine of over Rs. 60 lakh each.\n\nBut they were subsequently released as CBI was not able to provide evidence for their heinous crimes \n"}
{"id": "24567574", "url": "https://en.wikipedia.org/wiki?curid=24567574", "title": "Health Canada Sodium Working Group", "text": "Health Canada Sodium Working Group\n\nOn October 25, 2007, the Minister of Health announced that the Government of Canada would establish an expert Sodium Working Group to explore options for reducing sodium intake and cardiovascular disease among Canadians.\n\nIn announcing the creation of the Working Group, the Minister of Health said, \"Through the formation of this working group, our Government is taking a major step in helping Canadians improve their health, and the health of their families.\"\n\nSalt-reduction activist and member of the international salt reduction advocacy group WASH (World Action on Salt and Health), Dr. Norm Campbell, president of Blood Pressure Canada said, \"This is a wonderful demonstration of the government's leadership in forming collaborations to improve the health of Canadians to prevent stroke, heart and kidney disease -- three of the major causes of death and disability in Canada,\" says. \"Here we have everyone working together for common cause.\"\n\nIn establishing the Sodium Working Group, Health Canada included representatives from food manufacturing and food service industry groups, health-focused non-governmental organizations, the scientific community, consumer advocacy groups, health professional organizations and government representatives. The mandate of the Working Group was to develop and oversee the implementation of a strategy for reducing dietary sodium intake among Canadians.\n\nThe Working Group has met on several occasions to establish a common knowledge base and to develop strategies for reducing dietary sodium consumption among Canadians. The process that Health Canada is following is patterned after that carried out by the Food Standards Agency in the UK – that is, no discussion of the science, but rather an immediate move to sodium reduction programs and policies. The concerns over salt are chiefly based upon its ability to affect blood pressure.\n\nThere is some debate on the impact of sodium reduction upon blood pressure. The salt industry and some food and beverage producers emphasize the heterogeneous impact of sodium on individuals. For example, they observe that about 30% of normotensive individuals experience a drop in blood pressure, while about 20% of normotensive individuals experience an increase in blood pressure - the remaining population showing no effect. As a consequence, some argue that programs to reduce salt will not hold the same benefits for everyone and policies to arbitrarily promote salt reduction will discriminate against a certain segment of the population. They argue that an across the board reduction in dietary sodium may not be the right approach and the outcome may lead to unintended consequences for Canadian consumers.\n\nOn the other hand, groups concerned with cardiovascular health and nutrition emphasize the overall negative effects of high levels of sodium in the North American diet. Based upon a study carried out in the US in 1991 on a total of 62 people, the presumption made is that most of the sodium Canadians consume (77%) comes from processed foods sold in grocery stores and in food service outlets. Only about 11% is added during preparation or at the table, with the remainder occurring naturally in foods. And while the individual benefits of reducing sodium intake are variable, it has been theorized that dietary sodium reduction could eliminate hypertension for over a million Canadians, with a resulting savings of at least 430 million dollars annually in direct high blood pressure management costs (although this has never been confirmed through clinical trials). In other words, while not all Canadians need to reduce their intake of dietary sodium, many have been urged to. Moreover,theoretical estimates have projected that we may be better off because of a possible reduction of tax-supported health care.\n\nIn February 4, 2011, the Ottawa Citizen reported that the Health Canada Sodium Working Group had been disbanded. The Group had been charged with tracking whether companies were reducing the level of salt in processed foods over the next five years. This follows actions in the United Kingdom to abolish the dietary mandate of the FSA (Food Standards Agency) the government unit most actively involved in salt reduction advocacy.\n\n"}
{"id": "19562350", "url": "https://en.wikipedia.org/wiki?curid=19562350", "title": "Health Sciences Online", "text": "Health Sciences Online\n\nHealth Sciences Online (HSO) is a non-profit online health information resource that launched in December 2008. The website hosts a virtual learning center providing weblinks to a collection of more than 50,000 courses, references, textbooks, guidelines, lectures, presentations, cases, articles, images and videos, available in 42 different languages. The content includes medicine, public health, nursing, pharmacy, dentistry, nutrition, kinesiology and other health sciences resources.\n\nThe website aims to provide quality educational resources to health care providers in training and practice, especially in developing countries, thus bridging the digital divide (the global imbalance in access to information technology). The underlying aim is to support the United Nations' Millennium Development Goals of public health, but it is also intended to be useful for providers in industrialized countries. The hope is to create revolutions in democratizing health sciences education.\n\nThe four pillars of HSO are being comprehensive, authoritative, ad-free and free. The next step for HSO is to become an online health sciences learning centre, providing credentials and distance education degrees to help satisfy the great need for more and better-prepared health care professionals worldwide.\n\nHSO is an official supporting organisation of \"Healthcare Information For All by 2015\"\n\nHSO was conceived of by Founder and Executive Director Erica Frank, MD, MPH in 2001. A pilot of the site ran from 2006 until 2008, first only covering HIV/AIDS, and then moving on to include the entire site. The pilot ran in 11 countries in North America, Africa and Asia in order to assess the function, user interface, features, expectations, and needs of the users of HSO. The pilot was provided to health professionals at various levels of training and in practice.\n\nFounding collaborators include the U.S. Centers for Disease Control and Prevention, the World Bank, the American College of Preventive Medicine, the University of British Columbia, and the World Medical Association. \n\nFunding has been obtained from the Canadian and British Columbian governments, the World Health Organization, NATO’s Science for Peace Program, the Annenberg Physician Training Program, the Ulrich and Ruth Frank Foundation for International Health, and others. There has also been a large cadre of volunteers who have worked on the site’s development.\n\nHSO uses the Velocity Search Platform provided by Vivisimo to search all of its collected resources. In addition, Google Translate is used to provide results in 42 different languages.\n\nHSO is a web portal for searching health sciences resources that have been selected by a core team of volunteers, including health providers and scientists from different countries. The resources have been selected based on guidelines produced by several groups that help in assessing the quality of online health information. \n\nThe criteria used include:\n\nCurrently, HSO indexes over 50,000 resources. These resources come from government organizations, universities, and specialty societies such as:\n\n"}
{"id": "46627280", "url": "https://en.wikipedia.org/wiki?curid=46627280", "title": "Interventional oncology", "text": "Interventional oncology\n\nInterventional oncology (abbreviated IO) is a subspecialty field of interventional radiology that deals with the diagnosis and treatment of cancer and cancer-related problems using targeted minimally invasive procedures performed under image guidance. Interventional oncology has developed to a separate pillar of modern oncology and it employs X-ray, ultrasound, computed tomography (CT) or magnetic resonance imaging (MRI) to help guide miniaturized instruments (e.g. biopsy needles, ablation electrodes, intravascular catheters) to allow targeted and precise treatment of solid tumours (also known as neoplasms) located in various organs of the human body, including but not limited to the liver, kidneys, lungs, and bones. Interventional oncology treatments are routinely carried out by interventional radiologists in appropriate settings and facilities.\n\nInterventional oncology procedures are commonly applied to treat primary or metastatic cancer. Interventional oncology may be offered once traditional surgery, chemotherapy or radiotherapy have failed or are not considered safe. IO treatments may be also offered in combination with any of the above oncological therapies in order to augment the therapeutic outcome in more complex or widespread (metastatic) cancer cases. There is an increase in the variety of applications of interventional oncological treatments for primary and metastatic cancer in different human body organs:\n\n\nInterventional oncology procedures are generally divided between diagnostic procedures that help obtain tissue diagnosis of suspicious neoplasms and therapeutic ones that aim to cure or palliate the tumour. Therapeutic interventional oncology procedures may be classified further into ablation techniques that destroy neoplastic tissues by delivery of some form of heat, cryo or electromagnetic energy and embolization techniques that aim to occlude the blood vessels feeding the tumour and thereby destroy it by means of ischemia. Both ablation and embolization techniques are minimally invasive treatment, i.e. they may be delivered through the skin (in a percutaneous way) without the need for any skin incisions or other form of open surgery. Hence, most treatments are nowadays offered as day case or outpatient appointments and patients may enjoy rapid recovery and minimal pain and discomfort with low rates of complications.\n\n\n\n\nInterventional oncology has long been used to provide palliative care for patients. IO procedures can help reduce cancer-related pain and improve patients’ quality of life. Tumours can intrude into various ducts and blood vessels of the body, obstructing the vital passage of food, blood or waste. The interventional radiological treatment known as stenting can be used to re-open blockages, for example of the esophagus or bile ducts in cases of esophageal cancer or cholangiocarcinoma, respectively, considerably relieving the patient’s adverse symptoms.\n\nWhile the surgical resection of tumours is generally accepted to offer the best long-term solution, it is often not possible due the size, number or location of the tumour. IR therapies may be applied to shrink the tumour, making a surgical or interventional treatment possible. Some patient groups may also be too weak to undergo open surgery. IR treatments can be applied in these complex cases to provide effective and milder forms of treatment.\nInterventional oncological techniques can also be used in combination with other treatments to help increase their efficacy. For example, IO techniques can be used to shrink large tumours making them easier to excise. Chemotherapeutic drugs can also be administered intra-arterially, increasing their potency and removing the harsh effects of system-wide application.\nPatients can greatly benefit from IO treatments. The minimally invasive nature of the treatments means they cause less pain, fewer side effects and shorter recovery times. Many IO procedures can be performed on an outpatient basis, freeing up hospital beds and reducing costs.\n\nCancer is a multifaceted disease group that requires a multidisciplinary approach to treatment. Numerous studies have shown that cancer patients treated in multidisciplinary environments benefit greatly from the combined expertise. Interventional Radiologists are seen as playing a major role in multidisciplinary cancer teams where they provide innovative solutions to improve combined therapies and to treat complications.\n\nProper patient selection is the key element for the success of any medical procedure and improper conduct can have fatal consequences. Patient selection protocols must be strictly followed before treating patients with IO procedures.\n\nIO treatments are carried out under image guidance. For this reason practitioners must have attained solid training in radiation protection.\n\nInterventional radiology\n\n"}
{"id": "57226640", "url": "https://en.wikipedia.org/wiki?curid=57226640", "title": "Islamic hospitals", "text": "Islamic hospitals\n\nThis article is about Islamic Hospitals from the 700s to the 1800s.\n\nIn Persian, Islamic Hospitals are known as \"bimaristans.\" \"Bimaristan\" can also be called \"maristan.\" In Persian, “\"bimar”\" translates to \"“\"ill person\"”\" and \"“stan”\" translates to “place\".\"\" Therefore, Islamic Hospitals, \"bimaristans\", and \"maristans\" are all synonyms that can be used interchangeably. Islamic Hospitals served many purposes. \"Bimaristans\" served the purpose of being a designated place where medical treatment would be given to individuals in need. Along with being a central place for medical treatment to be given, Islamic Hospitals also served the purpose of being a designated place where recovering individuals could go to help gain back their strength. The individuals were typically recovering from accidents as well as sicknesses.\n\nMost Islamic Hospitals did not discriminate on who could be a patient; even wealthy individuals used the \"bimaristans\" when they became ill when traveling instead of an outpatient facility or home care. Islamic Hospitals were able to care for many diverse people who all had unique accidents, illnesses, injuries, and needs due to the different sections within the \"bimaristans\". Patients were divided into these different sections based on their needs. Patients were also divided up to minimize the risk of spreading illnesses to other patients. Not only were Islamic Hospitals used to provide care for individuals, they were also used to advance medical students’ knowledge in the medical field, especially the most well-known \"bimaristans\" located in Baghdad, Damascus, and Cairo. Islamic hospitals, books, and apprenticeships are the main three ways medical students learned different types of medical information to make tremendous advances in Islamic Medicine.\n\nIslamic Hospitals can best be understood by being viewed as a philanthropy because they gave public assistance to individuals who needed care. \"Bimaristans\" were able to provide these services at no costs due to \"waqfs, \"which were endowments that paid for the costs of creating bimaristans as well as maintaining them\".\"\n\nIslamic Hospitals were different than other hospitals because Muslims were led to form \"bimaristans\" by Muhammad, who taught that God would not create a disease without creating a cure. Mobile Hospitals were the first version of the \"bimaristans\". According to tradition, the first Mobile Hospital was located in a tent to treat war victims from the Battle of the Ditch (627 CE). Mobile Hospitals consisted of medications, food and water, doctors, and pharmacists to aid the patients. These services from the Mobile Hospital transitioned into the other Islamic Hospitals that were built as well. \"Bimaristans\" were generally located in urban areas. Though the Islamic realm was very large, Baghdad, Damascus, and Cairo housed the most well-known Islamic Hospitals. The first six \"bimaristans\" show major changes within Islamic hospitals in a short period of time. The first \"bimaristan\", built in 706 in Damascus by the Umayyad Caliph named Al-Walid, focused on treating individuals with leprosy. Around the same period the second \"bimaristan\" was built in Cairo. The third and fourth Islamic Hospitals were built in Baghdad. The third \"bimaristan\" was built in 805 by Caliph Harun al-Rashid. This Islamic Hospital in Baghdad was the first documented general hospital.\n\nMuhammad ibn Zakariya al-Razi, Hunayn ibn Ishaq, Ibn Sina (Avicenna), Ali ibn Isa al-Kahhal, Ibn al-Nafis, and Mir Mu’min Husayni Tunikabuni were all students that trained at Islamic Hospitals in order to advance their knowledge in the medical field. These students were key players in the advancement of \"bimaristans\" because of contributions to Islamic Medicine from their observations and writings. Muhammad ibn Zakariya al-Razi was an Islam philosopher, physician, and alchemist, who trained well in the Greek science and is known for his \"Comprehensive Book of Medicine.\" Hunayn ibn Ishaq was viewed as a mediator between Greek sciences and Arabs due to his translations of multiple documents that were tremendously important. Hunayn ibn Ishaq was also an optometrist. Avicenna was a physician as well as a government official. The \"Canon of Medicine\" constructed by Avicenna systemized medicine logically. Like Hunayn ibn Ishaq, Ali ibn Isa al-Kahhal was also an optometrist, who classified more than one hundred diseases of the eyes. Ibn al-Nafis was also a physician and an author, most known for his commentary on pulmonary circulation. Lastly, Mir Mu’min Husayni Tunikabuni focused on how yogic breath control can control the humours. While all of these people did many different things, they all contributed to advancements in medicine.\n\nThere is evidence that the beginning of the human study of medicine was around 3500 B.C.E. Religious priests and medicine men were the first medical practitioners. In ancient Mesopotamia, c. 3500 B.C.E., there were two kinds of medicine men–the \"ashipu\" who diagnosed the disease or injury, and the \"asu\" who practiced healing medicine and was practiced in herbal remedies. Practices in this early period included bandaging and making plasters for wounds. Although many patients were treated in their home, the first houses of medicine were often placed near rivers, so the evil substances and spirits would be washed away. This tradition continues into the Greek and Roman periods, both societies placing their centers of healing near the ocean, a river or spring. Temples to gods and goddesses of healing were often used as healing centers, with spiritual and scientific remedies practiced there.\n\nAncient Indian medicine, or the Vedic tradition, encouraged the balance of body, mind and spirit between 3000 B.C.E. to about 800 B.C.E. They had knowledge of surgery and even the beginnings of plastic surgery were founded here.\n\nAncient Egyptian medicine was quite advanced for c. 2500 B.C.E. They had a solid understanding of anatomy because human dissection did not have the taboo much later Christian societies experienced. There are a few medical texts that have been preserved to the present that help us understand the medical practices of ancient Egypt. The Edwin-Smith papyrus, written c. 17th century B.C.E. and probably based on much older knowledge, is a text on trauma surgery. It is suspected to be written based on military battles and battle wounds. Most medical practitioners were literate and often priests. Once again, there is a connection between gods and medicine – the earliest recorded physician was Imhotep, who lived c. 2725 B.C.E., and who was later lauded as a deity and called the god of medicine and healing. The practice of combining religious centers and medical centers was continued through the Greek and Roman periods, as well as into the medical practices of Muslims and Christians.\n\nBefore Islamic hospitals, there were the “Ascelpia” in Greece and the “Xendodocheia” in the Byzantine empire. These pre-Islamic healing centers were a mixture of spiritual healing and temples of faith. They offered cures via sacrifices to the gods and purgative drugs. These temples were worship sites for a healing god, usually the Greek god Asklepios. Patients would often come to the temple with an offering to the god and then sleep in the temple overnight, awaiting a visit from Asklepios in their dreams, sometimes in the form of a serpent which was said to have healing properties.\n\nThese ascelpia took the environment of their grounds into account as well as their healing rituals. Most of the ascelpia were centered on a spring or a fountain and were near a gymnasium or theater. This was encouraged by the medicine at the time, which focused on balance of the whole person, both of body and mind. There were no official schools of medicine at the time, most practitioners learned their art through apprenticeships or assisting a practitioner. Even after the Christian conversion of Constantine, the Askelpian cult continued their practices and rituals well into the fourth century CE. Towards the end of the century, Theodosius outlawed any pagan traditions and the cult finally died.\n\nAfter the Christening of Constantine, as well as the influx of refugees into cities as populations grew and land did not, the church became the center for healing. The term xendocheia began to be used, which came to mean a hostel for the poor. Bishops became known as spiritual healers, or physicians of the soul. One such bishop who was associated with an institution of healing was Saint Basil of Caesarea. Saint Basil built a hostel for the poor and the ill and placed it in the care of a monastery. It is described by fifth century historian Sozomen as “a storehouse of piety where disease is regarded in a religious light and sympathy put to the test.” Similar to the Askelpian cults, the idea of healing through incubation (sleeping in the temple) was also practiced under Christian empires. Patients would sleep in specific rooms that housed relics of a saint and hoped for the saint to appear in their dreams.\n\nThe church was often charged with the care of the sick, although students were not allowed to study theology and medicine at the same time. Statute 590 stated that students of theology could not even reside in the same area as medical students, because “books of the craft of the world should not be read with books of holiness in one light.”\n\nThese centers of health in antiquity helped shape the many new advances Islamicate scholars would pursue in medicine.\n\nAs for healing centers in the Islamicate world, the first bimaristans were mobile centers that date back to the time of Muhammed. They were transported by beast of burden, primarily camels. Before there were major towns and cities, the mobile bismaristans were able to travel to patients and the remote areas of the region.\n\nThe existence of hospitals, or bimaristans, in Baghdad has been documented since the 9th century CE, with the first having most likely been established by the vizier of caliph Harun al-Rashid. By the end of the 10th century CE, five more bimaristans had been built in Baghdad.\n\nAmong the most important of these was the Al-Adudi Hospital. The Al-Adudi Hospital was founded in 981 by the then ruler of Baghdad, Adud al-Dawlah, and was also named after him. Within Baghdad, the Al-Adudi Hospital's location was decided upon by its administrator, Abu-Bakr al-Razi. He determined where it should be located by \"hanging a piece of meat in several places for a few days and deciding in favor of the place where meat was found to be least infected.\" Al-Razi eventually decided to build the hospital along the Tigris River. At its inception, the Al-Adudi Hospital had twenty-five staff members, specializing in fields ranging from optics to surgery. In addition to these specialists, the Al-Adudi Hospital also served as a teaching hospital for new doctors. The Al-Adudi Hospital remained operational into the 12th century CE when, in 1184, it was described as \"...being like an enormous palace in size.\" Ultimately, the Al-Adudi Hospital was destroyed in 1258 by Mongols led by Hulagu Khan in the siege of Baghdad.\n\nOne of the first Egyptian hospitals was the Al-Fustat Hospital, which was founded in 872 CE. It was founded by Ahmed Ibn-Tulum and was so named because of its location within Cairo. The Al-Fustat Hospital shared many common features with modern hospitals. Among these were bath houses separated by gender, separate wards and the safekeeping of personal items during a patient's convalescence. In addition to these practices, the Al-Fustat Hospital is the first to have offered treatment for mental disorders. Beyond the practice of medicine, the Al-Fustat Hospital was also a teaching hospital and housed approximately 100,000 books. The Al-Fustat Hospital remained in operation for approximately 600 years. Another key feature of the Al-Fustat Hospital was that it offered all treatment for free. This was made possible by waqf revenue, which the Al-Fustat Hospital was likely the first to have been endowed with. Near the Al-Fustat Hospital, Ibn-Tulum also established a pharmacy to provide medical care in emergencies.\n\nThe Al-Mansuri Hospital was another hospital located in Cairo, and was completed in 1284 CE. Its founder, Al-Mansur Qalawun, was inspired to establish a hospital after his own experience being hospitalized in Damascus. Because of Al-Mansur's vision for the hospital, treatment was free to make the hospital accessible to both the rich and the poor. Furthermore, \"...upon discharge the patient was given food and money as a compensation for the wages he lost during his stay in the hospital.\" The Al-Mansuri Hospital was so accessible, in fact, that it treated roughly 4,000 patients every day. Like the Al-Fustat Hospital before it, the Al-Mansuri Hospital also treated mental patients and introduced music as a form of therapy. The Al-Mansuri also obtained the personal library of Ibn al-Nafis upon his death in 1258. The Al-Mansuri Hospital remained operational through the 15th century CE and still stands in Cairo today, though it is now known as \"Mustashfa Qalawun.\"\n\nDamascus is credited with being the home of the first ever Islamic hospital, which was established between 706 and 707 CE. Founded by Walid ibn 'Abdulmalik, this hospital was meant to serve as a treatment center for both those with chronic illnesses, like leprosy and blindness, as well as the poor or impoverished. This began with ibn 'Abdulmalik gathering lepers and preventing them from spreading the illness by providing them money. This was done to prevent them from begging strangers for money, thereby curtailing the spread of leprosy. To accomplish these objectives, separate wards existed for infectious diseases such as leprosy, and patients faced no cost to receive treatment. The Al-Walid Hospital has been compared to the Byzantine \"nosocomia\", which was a charitable institution tasked with treating \"...the sick, the lepers, the invalid, and the poor.\"\n\nThe Al-Nuri Hospital was founded in Damascus nearly four and a half centuries after the Al-Walid Hospital, in 1156 CE. It was named after Nur al-Din Zanji. The Al-Nuri Hospital, which operated for some 700 years, was the same hospital where Al-Mansur Qalawun was treated and inspired to establish his own hospital in Cairo. The Al-Nuri Hospital, in addition to bringing about the Al-Mansuri hospital, was innovative in its practices as it became the first hospital to begin maintaining medical records of its patients. The Al-Nuri Hospital was also a prestigious medical school, with one of its most noteworthy students being Ibn al-Nafis, who would later pioneer the theory of pulmonary circulation.\n\nThe first few Islamic hospitals that arose in Baghdad, in the early 9th century, were to quarantine those who suffered from leprosy. At first, it was considered by many as a \"leprosorium\" due to its limited purpose; nonetheless, these hospitals still salaried doctors whose specialties were not solely limited to leprosy. The function of these hospitals soon became diversified over time as newly built hospitals in Baghdad began to incorporate the knowledge from Islamic physicians, scientists such as the Al-Razi. Al-Razi's hospital in Baghdad, had 24 physicians on staff; these physicians had diverse specialties, including, physiologists, occultists, surgeons, and bonesetters.\n\nMadrasas, in Islamic civilizations, were teaching institutions, some of which focussed on medicine. These madrasas were often closely linked with hospitals so that students could learn in the institutions and put their theoretical knowledge into practice in the hospitals. Physicians were not exclusively Muslim; practicing physicians included Jews, and Christians. In the major hospitals at Cairo, Baghdad, and Damascus, students could visit patients often with the supervision of a practicing physician, in a system that is comparable to that of medical residents today.\n\nThe greatest contribution of the Islamic hospitals was the structure itself and how it functioned in Islamic culture. The first documented general hospital (hospitals that treated a multitude of pathologies, including mental illness) arose in Baghdad, in 805, built by vizier to the caliph, Harun al-Rashid. Although not much is known about this hospital due to poor documentation, the system of the general hospital itself set forth an example for the many other hospitals to come. Soon after, 34 new hospitals were built throughout the Islamic world with the number increasing annually. Islamic hospitals were even the first to specialize in treating convicts as the prison population was continually increasing.\n\nNewly founded hospitals strived to be better than their preceding counterparts in the Islamic world. The purpose was to compete with European hospitals as well in order to attract more physicians and scholars. Many of these hospitals also contained a conjoined library typically filled with any possible writings that may be relevant to the medicine practiced in the hospitals.\n\nIn order to meet the demands of these specialties, Islamic hospitals were subdivided into departments for surgery, ophthalmology, orthopedics, mental illness, and systemic diseases. The systemic disease department was designated for general illnesses that did not fall into categories of other departments. In some hospitals, they were further divided into subsections to address the needs of the patient depending on the symptoms. Examples include, but are not limited to, infections, fevers, and digestive issues. Hospital staff was not limited to physicians. Much like today's hospitals, they also relied on pharmacists, nurses, sanitary inspectors, supervising specialists, secretaries, and superintendents. The superintendents, or in Arabic, sa'ur, ensured that hospitals met certain standards in addition to managing the entire hospital institution. Pharmacists produced drugs as means for treatment of the hospitals' patients; they relied on a knowledge of chemistry, or Alchemia.\n\nBefore the 10th century, hospitals operated throughout the day and closed at night. Later hospitals operated on a 24 hour basis. Nonetheless, the practicing physicians worked a set amount of hours with their salaries prescribed by law; the physicians were paid generously enough so as to retain their talent. Chief of staff physician, Jabril ibn Bukhtishu, was salaried 4.9 million Dirham; for comparison, a medical resident worked significantly longer hours salaried at 300 Dirham per month.\n\nIslamic Hospitals attained their endowment through charitable donations or bequests, called waqfs. The legal documents establishing a waqf also set forth rules for how the hospital should be organized and operate in relation to the patient, stating that anyone can be admitted irrespective of race, gender, or citizenship. Patients of all socioeconomic statuses would have had access to full treatment, that is, costs were borne by the hospital itself. An example was the Al-Mansuri Hospital, in Cairo, built under the orders of the Mameluke ruler of Egypt, Al-Mansur Qalawun. Its maximum capacity was around 8000, and the annual endowment alone was said to be one-million Dirhams. The design was intended to accommodate various pathologies, for both men and women; as well as a pharmacy, a library, and lecture halls. The lecture halls were used for regular meetings on the status of the hospital, lecturing residents, and staff as well.\n\nWith the development and existence of early Islamic hospitals, came the need for new ways in which to treat patients. While the institution of a complex hospital was still comparatively new, Islamic Hospitals brought forth many groundbreaking medical advancements in Islamic culture during this time, which eventually spread to the entire world. These revolutionary medical practices came not only from Islamic Hospitals, but also distinguished physicians of this era, in the form of surgeries, techniques, discoveries and cures for ailments, and the invention of countless medical instruments.\n\nAmong the many developments stemming from Islamic Hospitals, were those designed to treat specific ailments, diseases, and anatomy. For example, a revolutionary treatment for cataracts was developed by al-Mawsili, a 10th century physician. The practice included a hollow syringe (which he developed) and removing the cataract through suction. Although this procedure has further developed throughout the centuries, the basic treatment remains the same even today. Diseases of the eye were further expanded upon during this era by ʻAli ibn ʻIsa al-Kahhal or Ibn Isa (died c. 1038), who practiced and taught in the Al-Adudi Hospital in Baghdad. He wrote and developed the \"Tadhkirat al-kaḥḥālīn\" (“The Notebook of the Oculist”), which detailed more than 130 eye diseases based on anatomical location. The work was separated into three portions consisting of 1) Anatomy of the eye, 2) Causes, symptoms and treatments of diseases, and 3) Less apparent diseases and their treatments. This work was translated into Latin in 1497, and then into several other languages which allowed it to benefit the medical community for centuries to come.\n\nPerhaps the largest contribution to Islamic surgical development, came from Abū al-Qāsim Khalaf ibn al-‘Abbās al-Zahrāwī, Abū al-Qāsim, or Al-Zahrawi (936-1013). He contributed to advancements in surgery by inventing and developing over 200 medical instruments which constituted the first independent work on surgery. Such instruments included tools like forceps, pincers, scalpels, catheters, cauteries, lancets, and specula, which were accompanied by detailed drawings of each tool. Al-Zahrawi also wrote the \"At-Taṣrīf limanʿajazʿan at-Taʾālīf,\" or \"At-Taṣrīf\" (“The Method”), which was a 30-part text based on earlier authorities, such as the \"Epitomae\" from the 7th-century Byzantine physician Paul of Aegina. It was largely composed of medical observations, including what is considered the earliest known description of hemophilia. The 30-volume encyclopedia also documented Zahrawi and his colleagues’ experiences with treatment of the ill or afflicted. Aside from the documentation of surgical instruments, the work included operating techniques, pharmacological methods to prepare tablets and drugs to protect the heart, surgical procedures used in midwifery, cauterizing and healing wounds, and the treatment of headaches. Although Zahrawi was somewhat disregarded by hospitals and physicians in the eastern Caliphate (no doubt due to his Spanish roots, being near Córdoba, Spain), his advancement and documentation of medical tools and observations contained in his work had a vast influence on the eventual medical development in Christian Europe, when it was translated into Latin during the 12th century.\n\nThe Abbasid Caliphate in Baghdad underwent extreme intellectual and medical experimentation during the 10th and 11th centuries. Among the many skilled physicians and intellectuals there was Abū Bakr Muḥammad ibn Zakariyyāʾ al-Rāzī, or in Latin, Rhazes (854-925). Rhazes served as chief physician in a hospital in Rayy, Iran, before holding a similar position in the Baghdad hospital. He developed two significant works regarding advancements in medicine and philosophy. The \"Kitāb al-Manṣūrī\" and the \"Kitāb al-ḥāwī,\" (“Comprehensive Book”) which surveyed early Greek, Syrian, and Arabic medicine, and added his own judgement and commentary. He also wrote several minor treatises, perhaps the most famous being \"Treatise on Small Pox and Measles\". This treatise was translated into several modern languages as well as Latin and Byzantine Greek for teaching purposes and medical treatment of such infectious diseases.\n\nAlthough surgical developments and advancements made in the medieval Islamic period are of extreme importance, the largest and most wholesome contribution to the medical world stemming from Islamic medicine and hospitals came from the Baghdad firmament from Ibn Sina, or “Avicenna” in the West. Ibn Sina, who had already become a doctor by the age of 18, developed the \"Al-Qanun fi al-Tibb\" (Canon of Medicine). This work is largely known as one of the most famous medical works of all time. The ways in which Ibn Sina’s Canon of Medicine worked to bring together various disciplines and cultures, essentially revived Greek authors and philosophers and fostered new thought patterns to develop much of the future medical practices we see today. Ibn Sina did this by combining the medical developments of Greek physician and philosopher Galen, with the philosophy of Aristotle. Furthermore, as Islamic medicine recognized that many diseases are contagious, such as leprosy, smallpox, and sexually transmitted diseases, Ibn Sina recognized tuberculosis as a contagious disease, among others which can be spread through soil and water. The Canon of Medicine continued to be studied by European medical professionals and institutions even into the 18th century. This combination and rationalization of practical science, religion, and thought highlighted the pinnacle of Muslim medical scholarship, and the nature of the true developments which were made in the medical world.\n\nMuch of the legacy surrounding the Islamic influence on modern hospitals and science can be found in the discoveries, techniques, and practices introduced by scholars and physicians working in these hospitals between the tenth and nineteenth century. This time period was extremely important to the advancement of modern medicinal practices, and is known as one the greatest periods of development. Many of these discoveries laid the foundation for medical development in Europe, and are still common practice in modern medicine. Among these discoveries in astronomy, chemistry, and metallurgy, scholars developed techniques for medicine such as the distillation and use of alcohol as an antiseptic, which is still being used in hospitals today. Not only did these discoveries lead to lasting improvements in medicine in the Muslim world, but through the influence of early Islamic and Arabian hospitals, medical institutions around the world were introduced to various new concepts and structures, increasing the efficiency and cleanliness which can still be found in modern day institutions.\n\nSome of these influential concepts include the implementation of separate wards based on disease and gender, pharmacies, housing of medical records, and the education associated with practicing medicine. Prior to the Islamic era, most European medical care was offered by priests in sanatoriums and annexes to temples. Islamic hospitals revolutionized this by being operated secularly and through a government entity, rather than being solely operated by the church. This introduction of government operated hospitals lead to not having any discrimination against people for any reason allowing the hospital to solely focus on their main goal of serving all people and working together to help everyone out.\n\nSeparate wards\n\nIslamic hospitals also brought about the idea of separate wards or segments of the hospital that were separated by patient diagnostic. When Islamic hospitals first brought this about, not only were the wards separated by diagnostic but by sex as well. While hospitals today aren’t as strict and don’t separate by sex anymore, they still separate people by disease or problem. By doing so, different wings could specialize in certain treatments specific to their patient. This practice not only still exists today in modern hospitals but also lead to the advancement of treatments back then that now comprise the “Canon of Medicine.” This separation of diseases not only helped the timely treatment of patients but also helped the patients and physicians from getting sick with other diseases that surrounded them because they only had to worry about the prevention of one disease. By separating patients, the specialization of certain wings and treatments really advanced the medicine and improved the efficiency of hospitals ultimately leading to how modern day hospitals are designed.\n\nMedical Records\n\nWith Islamic hospitals advancing medicine so quickly, they needed a way to catalogue all of their findings which in the end lead to the first medical records. This made hospitals more efficient as they were able to check records of other people with patients that had similar symptoms and hopefully treat them the same way they were able to with the other patients. Not only did physicians keep medical records but they kept notes on patients and provided them for peer review as a way to not be held responsible for malpractice. This information also enabled physicians to start to notice patterns in patients more making the medicinal practices more accurate as everything is with more trials. The efficiency gained from keeping records allowed hospitals to run more smoothly and treat patients faster. This information also enabled physicians to start to notice patterns in patients more making the medicinal practices more accurate as everything is with more trials. This keeping of records ultimately lead to the accumulation of the “Canon of Medicine,” which is a book of medicine compiled by the Persian philosopher Avicenna (Ibn Sina) that was completed in 1025.\n\nEducation and Qualification\n\nAnother legacy that vastly changed the way through which medical practices were developed, was the method of education and perpetuation of medical knowledge. Islamic hospitals modernized the qualification of physicians and education leading to a license to practice medicine in hospitals. In 931 CE, Caliph Al –Muqtadir started the movement of licensing physicians by telling Siban Ibn- Thabit to only give physician licenses to qualified people. Seeing as how one of the chief objectives of Islamic hospitals was the training of new physicians or students, senior physicians, and other medical officers would often hold instructive seminars in large lecture halls detailing diseases, cures, treatments, and techniques from class manuscripts. Islamic hospitals were also the first to adopt practices involving medical students, accompanied by experienced physicians, into the wards for rounds to participate in patient care. Hospitals doubling as schools not only provided hospitals with more fresh working hands but also helped in the advancement of medicine. Education in hospitals during the Islamic period modernized hospitals today in that now before people can become doctors they must complete a period of residency in which students study at a hospital and job shadow licensed physicians for a number of years before they are fully able to become licensed physicians. This came at a time when much of Europe’s medical practices were much less advanced, and with the compilation and creation of Avicenna’s (Ibn Sina) medical textbook, the “Canon of Medicine,” these groundbreaking Islamic discoveries were able to influence Europe and the rest of the world for centuries to come.\n\nPharmacies\n\nOne other lasting impression from the ninth century was the birth of pharmacies. Due to the major chemistry advancements, actual medicine during this time period flourished and became highly developed to the point of needing a separate place to distribute medicine which came to be known as a pharmacy. With the advancement of science and medicine, pharmacy became a specialized subject itself. This ultimately lead to pharmacy becoming its own established profession people could pursue.\n"}
{"id": "5279608", "url": "https://en.wikipedia.org/wiki?curid=5279608", "title": "Juklevatnet", "text": "Juklevatnet\n\nJuklevatnet is a lake in the municipalities of Hemsedal (in Buskerud county) and Lærdal (in Sogn og Fjordane county), Norway. It is located east of Borgund in Lærdal, just to the south of the mountain Høgeloft in the Filefjell range. The lake sits at an elevation of above sea level. It is located northeast of the lake Eldrevatnet and northeast of the lake Øljusjøen.\n\nThe lake is regulated by a hydroelectric dam which is part of a nearby power station. The water flows into the lake Vesle Juklevatnet, then the river Jukleåni, and then into the lake Eldrevatnet.\n\n"}
{"id": "33901323", "url": "https://en.wikipedia.org/wiki?curid=33901323", "title": "Keratic precipitate", "text": "Keratic precipitate\n\nKeratic precipitate (KP) is an inflammatory cellular deposit seen on corneal endothelium. Acute KPs are white and round in shape whereas old KPs are faded and irregular in shape. Mutton-fat KPs are large in shape and are greasy-white in color and are formed from macrophages and epithelioid cell. They are indicative of inflammatory disease. Mutton fat Kps are due to granulomatous iridocyclitis. Another variant called red KPs may be seen in hemorrhagic uveitis.\n"}
{"id": "19174962", "url": "https://en.wikipedia.org/wiki?curid=19174962", "title": "LegitScript", "text": "LegitScript\n\nLegitScript is a verification and monitoring service for online pharmacies. It is the only such service recognised by the National Association of Boards of Pharmacy (NABP) as adhering to its standards aside from the NABP's own VIPPS program.\n\nThe NABP has written that it endorses LegitScript on behalf of the government agencies that license and regulate pharmacies and pharmacists in the United States, Canada, and other jurisdictions for use \"by search engine advertising programs, Domain Name Registrars, registries, payment processing companies, social media companies, and other Internet platforms to ensure that Internet platforms operate in compliance with applicable healthcare laws and regulations, and are not utilized in furtherance of the illicit sale of unregulated healthcare products\". In addition, some EU government agencies have referred to LegitScript as the \"appropriate authority\" to which rogue Internet pharmacies should be reported. LegitScript has also worked with the U.S. Food and Drug Administration to monitor and investigate Internet pharmacy websites.\n\nLegitScript was founded by John Horton, who was a White House aide on drug policy issues from 2002 to 2007. Horton continues to serve as the company's President and also serves on the NASCAR Appeals Panel. The organization's main office was initially located in Arlington, Virginia, but is currently identified as being in Portland, Oregon. \n\nThe company is identified by the Alliance for Safe Online Pharmacies as a founding member.\n\nIn July 2008, LegitScript released a report about 150 websites that offer to sell anabolic steroids over the Internet without requiring a prescription.\nThis report was also featured in the \"New York Times\", as well as on CNN's \"Anderson Cooper 360\". The report resulted in the termination of large underground steroids rings and controversy on pro-steroids forums. LegitScript also released two reports analyzing Microsoft and Yahoo sponsored search results for Internet pharmacies. Subsequently, Google, Yahoo and Microsoft announced that they were updating their advertising policies related to Internet pharmacies, and would in the future require that Internet pharmacies be approved by the National Association of Boards of Pharmacy's VIPPS program. Shortly thereafter, LegitScript announced that it would be helping Google implement the new policy by monitoring all prescription drug and pharmacy ads for the search engine. LegitScript states that it performs Google monitoring throughout North America, the EU, Russia and Asia for Google ads, and also identifies Microsoft, Amazon, Twitter and the U.S. Food and Drug Administration as clients.\n\nIn November 2008, LegitScript launched its registrar notification program and reported that it had shut down 500 \"rogue\" Internet pharmacies by notifying their ISPs and domain name registrars. In May 2010, the company released a report regarding over 7,000 websites displaying a forged pharmacy license, indicating that it worked with 11 different domain name registrars to shut down the websites. In late 2012, the company stated that it had worked with registrars to shut down over 35,000 rogue Internet pharmacy websites in the previous four years.\n\nAs of early 2013, several domain name registrars explicitly state in their terms of services that they suspend and lock domain names identified as rogue Internet pharmacies by LegitScript, and outsource their abuse point-of-contact on Internet pharmacy issues to LegitScript. The company has stated that it works with most domain name registrars, and, in late 2012, it began publishing data showing that the websites of rogue Internet pharmacies \"cluster\" around a small number of registrars that LegitScript says ignore abuse notifications. Several Internet pharmacy affiliate marketers have publicly discussed the importance of registering Internet pharmacy domain names with registrars that do not work with LegitScript. In submitting abuse notifications to ICANN-accredited registrars, LegitScript has authority to act on the behalf of government regulators in some countries, such as Japan.\n\nIn March 2010, consumer protection website SiteJabber announced that it would begin integrating LegitScript's legitimacy determinations into its Internet pharmacy ratings. In May 2010, the WOT Services announced a similar initiative in which LegitScript Internet pharmacy legitimacy determinations would be integrated into its reputation rankings.\n\nLegitScript maintains a Healthcare Product Legitimacy program that monitors whether \"dietary supplements have been the subject of regulatory action, are considered unsafe, or are marketed as miracle cures\" and that is also endorsed by the National Association of Boards of Pharmacy. In its advertising policies for healthcare and medicines, Google indicates that it uses LegitScript's healthcare product legitimacy program to determine which supplements and designer drugs should be restricted from advertising.\n\nAs of February 2013, LegitScript's website indicated that it had approved over 250 online pharmacy websites and documented over 48,000 \"rogue\" online pharmacy websites. The program is also endorsed by the National Association of Boards of Pharmacy.\n\n"}
{"id": "11294705", "url": "https://en.wikipedia.org/wiki?curid=11294705", "title": "List of political parties in Central America by country", "text": "List of political parties in Central America by country\n"}
{"id": "30407439", "url": "https://en.wikipedia.org/wiki?curid=30407439", "title": "Louise Reiss", "text": "Louise Reiss\n\nLouise Marie Zibold Reiss (February 23, 1920 – January 1, 2011) was an American physician who coordinated what became known as the Baby Tooth Survey, in which deciduous teeth from children living in the St. Louis, Missouri area who were born in the 1950s and 1960s were collected and analyzed over a period of 12 years. The results of the survey showed that children born after 1963 had levels of strontium-90 in their teeth that were 50 times higher than those found in children born before the advent of widespread nuclear weapons testing. The findings helped convince U.S. President John F. Kennedy to sign the Partial Nuclear Test Ban Treaty with the United Kingdom and Soviet Union, which ended the above-ground testing of nuclear weapons that placed the greatest amounts of nuclear fallout into the atmosphere.\n\nBorn in the Queens borough of New York City on February 23, 1920, Reiss contracted polio as a child. She originally planned to study art in college, but decided to switch to science after the outbreak of World War II.\n\nShe earned her medical degree at the Woman's Medical College of Pennsylvania (now part of the Drexel University College of Medicine) and performed her internship and residency at Philadelphia General Hospital, where she met her future husband, the physician Eric Reiss. The couple first moved to San Antonio, Texas, then relocated to St. Louis after Eric Reiss received an appointment at the Washington University School of Medicine. Hired by the St. Louis city health department, Louise Reiss was involved in inoculating children with the polio vaccine.\n\nIn 1959, Reiss and her husband joined environmental scientist Barry Commoner and others to create the Greater St. Louis Citizens' Committee for Nuclear Information, which initiated the Baby Tooth Survey in conjunction with Saint Louis University and the Washington University School of Dental Medicine as a means of determining the effects of nuclear fallout on the human anatomy. Louise Reiss led the project from 1959 to 1961. The research focused on detecting the presence of strontium-90, a cancer-causing radioactive isotope created by the more than 400 atomic tests conducted above ground before 1963. Due to its chemical similarity to calcium, the radioactive strontium isotope is absorbed from water and dairy products into the bones and teeth of children, as their growing bodies need calcium. Visiting local schools and organizations, Louise Reiss convinced parents to have their children send in their lost baby teeth, in return for which they were sent a button reading \"I gave my tooth to science\". The team sent collection forms to area schools, and teeth were initially sent to the Reiss home, where they were sorted. In all, some 320,000 teeth from children of various ages were collected before the project was ended in 1970.\n\nThe results of the thousands of teeth analyzed, published in the November 24, 1961, issue of the journal \"Science\", revealed elevated levels of radioactive compounds in the first sets of teeth that had been collected. President John F. Kennedy was made aware of the research results while he was negotiating a treaty with the United Kingdom and Soviet Union to place controls on nuclear testing. His call to the Reiss home was answered by her son; the person on the other end of the phone said, \"This is John Kennedy, can I talk to your mom?\" Further analysis by the team led to the conclusion that children born in 1963 had absorbed levels of strontium-90 that were 50 times higher than those found in children born a decade earlier. Her husband, Eric Reiss, testified before the United States Senate when it was considering ratification of the Partial Nuclear Test Ban Treaty. Later research showed that levels of strontium-90 in the cohort born in 1968, after the treaty had gone into effect, had declined by 50 percent.\n\nA resident of Pinecrest, Florida, Reiss died at the age of 90 at her home on January 1, 2011, after suffering a myocardial infarction two months earlier. She was survived by her son, Eric Reiss, as well as by two grandchildren and three great-grandchildren.\n"}
{"id": "56820499", "url": "https://en.wikipedia.org/wiki?curid=56820499", "title": "Mad studies", "text": "Mad studies\n\nMad studies is a field of scholarship, theory, and activism about the lived experiences, history, cultures, and politics about people who may identify as Mad, mentally ill, psychiatric survivors, consumers, service users, patients, neurodiverse, and disabled. Mad studies originated from consumer/survivor movements organized in Canada, the United States, the United Kingdom, Australia, and in other parts of the world. The methods for inquiry draw from a number of academic disciplines such as women's studies, critical race studies, indigenous epistemologies, queer studies, psychological anthropology, and ethnography. This field shares theoretical similarities to critical disability studies, psychopolitics, and critical social theory. The academic movement formed, in part, as a response to recovery movements, which many mad studies scholars see as being \"co-opted\" by mental health systems.\n\nRichard A. Ingram, a senior research fellow in the School of Disability Studies at Ryerson University (2007), has been credited with coining the phrase \"Mad Studies\" at the First Regional Graduate/Undergraduate Student Disability Studies Conference at Syracuse University on May 3, 2008. In an academic article entitled \"Doing Mad Studies: Making (Non)sense Together,\" Ingram points to a number of theorists who created the intellectual groundwork for the field, including Nietzsche, Bataille, Blanchot, Deleuze, and Guattari.\n\nIn a 2014 \"Guardian\" article, Peter Beresford names Canadian scholars at the forefront of this academic field: \"Mad studies has been pioneered by Ryerson and York Universities in Toronto, with key figures such as mental health survivors, activists and educators David Reville and Geoffrey Reaume and academics Kathryn Church and Brenda LeFrancois.\" Journalist Alex Gillis summarizes the spread of mad studies programs in a November 2015 article: \"Soon after Ryerson and York launched mad studies courses in the early 2000s, similar courses began in Simon Fraser University’s department of sociology and anthropology, and more recently at Memorial University’s school of social work, Queen’s University’s school of kinesiology and health studies, and the history departments at Trent University and the University of Winnipeg. A few universities in England, Scotland and the Netherlands launched courses in the past two years, using Canadian courses as models.\"\n\nSome dimensions of this emerging field may include research on the \"social construction of 'mental illness, normalizing imperatives of the state and medicine, rapidly expanding nosologies (categories of pathology) for mental illness, collusion(s) of pharmaceutical corporations and professional associations within psychiatry, connections between ecocide and mental stress, psychiatrization of nonhuman animals, representation(s) of madness in media, history of consumer/survivor movement(s), and the rise and fall of mental treatments within scientific, medical, and lay communities.\"\n\n\n"}
{"id": "56194906", "url": "https://en.wikipedia.org/wiki?curid=56194906", "title": "Mental health informatics", "text": "Mental health informatics\n\nMental health informatics is a branch of health or clinical informatics concerned with the use of information technology (IT) and information to improve mental health. Like health informatics, mental health informatics is a multidisciplinary field that promotes care delivery, research and education as well as the technology and methodologies required to implement it.\n\nMajor areas of mental health informatics activity include:\n\n\nThe need for and application of health informatics in primary and secondary health care has been well established in developed countries for 20 years or more. The application of informatics in mental health has not become as pervasive, in spite of professional recognition the domain appearing well suited to computerisation and the need for quantified outcome evidence. A major impediment may be societal stigma associated with mental disorders as well as increased sensitivity about protecting the privacy and confidentiality of records in mental health care. There also may be a professional reluctance to effect changes in established working patterns that the introduction of systems necessarily entails.\n"}
{"id": "516838", "url": "https://en.wikipedia.org/wiki?curid=516838", "title": "Micro-g environment", "text": "Micro-g environment\n\nThe term micro-g environment (also µg, often referred to by the term microgravity) is more or less a synonym for \"weightlessness\" and \"zero-g\", but indicates that g-forces are not quite zero—just very small. The symbol for microgravity, \"µg\", was used on the insignias of Space Shuttle flights STS-87 and STS-107, because these flights were devoted to microgravity research in low Earth orbit.\n\nA \"stationary\" micro-g environment would require travelling far enough into deep space so as to reduce the effect of gravity by attenuation to almost zero. This is the simplest in conception, but requires traveling an enormous distance, rendering it most impractical. For example, to reduce the gravity of the Earth by a factor of one million, one needs to be at a distance of 6 million kilometers from the Earth, but to reduce the gravity of the Sun to this amount one has to be at a distance of 3.7 billion kilometers. (On Earth the gravity due to the rest of the Milky Way is already attenuated by a factor greater than one million, so we do not need to move away further from its center). Thus it is not impossible, but it has only been achieved so far by four interstellar probes (Voyager 1 and 2 of the Voyager program, and Pioneer 10 and 11 of the Pioneer program) and they did not return to Earth. To reduce the gravity to one thousandth of that on Earth's surface, one needs to be at a distance of 200,000 km.\n\nAt a distance relatively close to Earth (less than 3000 km), gravity is only slightly reduced. As an object orbits a body such as the Earth, gravity is still attracting objects towards the Earth and the object is accelerated downward at almost 1g. Because the objects are typically moving laterally with respect to the surface at such immense speeds, the object will not lose altitude because of the curvature of the Earth. When viewed from an orbiting observer, other close objects in space appear to be floating because everything is being pulled towards Earth at the same speed, but also moving forward as the Earth's surface \"falls\" away below. All these objects are in free fall, not zero gravity.\n\nCompare the gravitational potential at some of these locations.\n\nWhat remains is a micro-g environment moving in free fall, i.e. there are no forces other than gravity acting on the people or objects in this environment. To prevent air drag making the free fall less perfect, objects and people can free-fall in a capsule that itself, while not necessarily in free fall, is accelerated as in free fall. This can be done by applying a force to compensate for air drag. Alternatively free fall can be carried out in space, or in a vacuum tower or shaft.\n\nTwo cases can be distinguished: Temporary micro-g, where after some time the Earth's surface is or would be reached, and indefinite micro-g.\n\nA temporary micro-g environment exists in a drop tube (in a tower or shaft), a sub-orbital spaceflight, e.g. with a sounding rocket, and in an airplane such as used by NASA's Reduced Gravity Research Program, aka the Vomit Comet, and by the Zero Gravity Corporation. A temporary micro-g environment is applied for training of astronauts, for some experiments, for filming movies, and for recreational purposes.\n\nA micro-g environment for an indefinite time, while also possible in a spaceship going to infinity in a parabolic or hyperbolic orbit, is most practical in an Earth orbit. This is the environment commonly experienced in the International Space Station, Space Shuttle, etc. While this scenario is the most suitable for scientific experimentation and commercial exploitation, it is still quite expensive to operate in, mostly due to launch costs.\n\nObjects in orbit are not perfectly weightless due to several effects:\n\n\nIn a shot tower (now obsolete), molten metal (such as lead or steel) was dripped through a sieve into free fall. With sufficient height (several hundred feet), the metal would be solid enough to resist impact (usually in a water bath) at the bottom of the tower. While the shot may have been slightly deformed by its passage through the air and by impact at the bottom, this method produced metal spheres of sufficient roundness to be used directly in shotgun shells or to be refined by further processing for applications requiring higher accuracy.\n\nWhile not yet a commercial application, there has been interest in growing crystals in micro-g, as in a space station or automated artificial satellite, in an attempt to reduce crystal lattice defects. Such defect-free crystals may prove useful for certain microelectronic applications and also to produce crystals for subsequent X-ray crystallography.\n\nSpace Motion Sickness (SMS) is thought to be a subtype of motion sickness that plagues nearly half of all astronauts who venture into space. SMS, along with facial stuffiness from headward shifts of fluids, headaches, and back pain, is part of a broader complex of symptoms that comprise Space Adaptation Syndrome (SAS). SMS was first described in 1961 during the second orbit of the fourth manned spaceflight when the Cosmonaut, Gherman Titov aboard the Vostok 2, described feeling disoriented with physical complaints mostly consistent with motion sickness. It is one of the most studied physiological problems of spaceflight but continues to pose a significant difficulty for many astronauts. In some instances, it can be so debilitating that astronauts must sit out from their scheduled occupational duties in space – including missing out on a spacewalk they have spent months training to perform. In most cases, however, astronauts will work through the symptoms even with degradation in their performance.\n\nDespite their experiences in some of the most rigorous and demanding physical maneuvers on earth, even the most seasoned astronauts may be affected by SMS, resulting in symptoms of severe nausea, projectile vomiting, fatigue, malaise (feeling sick), and headache. These symptoms may occur so abruptly and without any warning that space travelers may vomit suddenly without time to contain the emesis, resulting in strong odors and liquid within the cabin which may affect other astronauts. Symptoms typically last anywhere from one to three days upon entering weightlessness, but may recur upon reentry to Earth’s gravity or even shortly after landing. SMS differs from terrestrial motion sickness in that sweating and pallor are typically minimal or absent and gastrointestinal findings usually demonstrate absent bowel sounds indicating reduced gastrointestinal motility.\n\nEven when the nausea and vomiting resolve, some central nervous system symptoms may persist which may degrade the astronaut’s performance. Graybiel and Knepton proposed the term “sopite syndrome” to describe symptoms of lethargy and drowsiness associated with motion sickness in 1976. Since then, their definition has been revised to include “…a symptom complex that develops as a result of exposure to real or apparent motion and is characterized by excessive drowsiness, lassitude, lethargy, mild depression, and reduced ability to focus on an assigned task.” Together, these symptoms may pose a substantial threat (albeit temporary) to the astronaut who must remain attentive to life and death issues at all times.\n\nSMS is most commonly thought to be a disorder of the vestibular system that occurs when sensory information from the visual system (sight) and the proprioceptive system (posture, position of the body) conflicts with misperceived information from the semicircular canals and the otoliths within the inner ear. This is known as the ‘neural mismatch theory’ and was first suggested in 1975 by Reason and Brand. Alternatively, the fluid shift hypothesis suggests that weightlessness reduces the hydrostatic pressure on the lower body causing fluids to shift toward the head from the rest of the body. These fluid shifts are thought to increase cerebrospinal fluid pressure (causing back aches), intracranial pressure (causing headaches), and inner ear fluid pressure (causing vestibular dysfunction).\n\nDespite a multitude of studies searching for a solution to the problem of SMS, it remains an ongoing problem for space travel. Most non-pharmacological countermeasures such as training and other physical maneuvers have offered minimal benefit. Thornton and Bonato noted, “Pre- and inflight adaptive efforts, some of them mandatory and most of them onerous, have been, for the most part, operational failures.” To date, the most common intervention is promethazine, an injectable antihistamine with antiemetic properties, but sedation can be a problematic side effect. Other common pharmacological options include metaclopromide, as well as oral and transdermal application of scopolamine, but drowsiness and sedation are common side effects for these medications as well.\n\nIn the space (or microgravity) environment the effects of unloading varies significantly among individuals, with sex differences compounding the variability. Differences in mission duration, and the small sample size of astronauts participating in the same mission also adds to the variability to the musculoskeletal disorders that are seen in space. In addition to muscle loss, microgravity leads to increased bone resorption, decreased bone mineral density, and increased fracture risks. Bone resorption leads to increased urinary levels of calcium, which can subsequently lead to an increased risk of nephrolithiasis.\n\nIn the first two weeks that the muscles are unloaded from carrying the weight of the human frame during space flight, whole muscle atrophy begins. Postural muscles contain more slow fibers, and are more prone to atrophy than non-postural muscle groups. The loss of muscle mass occurs because of imbalances in protein synthesis and breakdown. The loss of muscle mass is also accompanied by a loss of muscle strength, which was observed after only 2–5 days of spaceflight during the Soyuz-3 and Soyuz-8 missions. Decreases in the generation of contractile forces and whole muscle power have also been found in response to microgravity.\n\nTo counter the effects of microgravity on the musculoskeletal system, aerobic exercise is recommended. This often takes the form of in-flight cycling. A more effective regimen includes resistive exercises or the use of a penguin suit (contains sewn-in elastic bands to maintain a stretch load on antigravity muscles), centrifugation, and vibration. Centrifugation recreates Earth’s gravitational force on the space station, in order to prevent muscle atrophy. Centrifugation can be performed with centrifuges or by cycling along the inner wall of the space station. Whole body vibration has been found to reduce bone resorption through mechanisms that are unclear. Vibration can be delivered using exercise devices that use vertical displacements juxtaposed to a fulcrum, or by using a plate that oscillates on a vertical axis. The use of beta-2 adrenergic agonists to increase muscle mass, and the use of essential amino acids in conjunction with resistive exercises have been proposed as pharmacologic means of combating muscle atrophy in space.\n\nMicrogravity can also lead to height increases; in 2018 a Japanese astronout reported growing 2 centimetres in just three weeks of micro-gravity on board the International Space Station.\n\nNext to the skeletal and muscular system, the cardiovascular system is less strained in weightlessness than on Earth and is de-conditioned during longer periods spent in space. In a regular environment, gravity exerts a downward force, setting up a vertical hydrostatic gradient. When standing, some 'excess' fluid resides in vessels and tissues of the legs. In a micro-g environment, with the loss of a hydrostatic gradient, some fluid quickly redistributes toward the chest and upper body; sensed as 'overload' of circulating blood volume. In the micro-g environment, the newly sensed excess blood volume is adjusted by expelling excess fluid into tissues and cells (12-15% volume reduction) and red blood cells are adjusted downward to maintain a normal concentration (relative anemia). In the absence of gravity, venous blood will rush to the right atrium because the force of gravity is no longer pulling the blood down into the vessels of the legs and abdomen, resulting in increased stroke volume. These fluid shifts become more dangerous upon returning to a regular gravity environment as the body will attempt to adapt to the reintroduction of gravity. The reintroduction of gravity again will pull the fluid downward, but now there would be a deficit in both circulating fluid and red blood cells. The decrease in cardiac filling pressure and stroke volume during the orthostatic stress due to a decreased blood volume is what causes orthostatic intolerance. Orthostatic intolerance can result in temporary loss of consciousness and posture, due to the lack of pressure and stroke volume. More chronic orthostatic intolerance can result in additional symptoms such as nausea, sleep problems, and other vasomotor symptoms as well.\n\nMany studies on the physiological effects of weightlessness on the cardiovascular system are done in parabolic flights. It is one of the only feasible options to combine with human experiments, making parabolic flights the only way to investigate the true effects of the micro-g environment on a body without traveling into space. Parabolic flight studies have provided a broad range of results regarding changes in the cardiovascular system in a micro-g environment. Parabolic flight studies have increased the understanding of orthostatic intolerance and decreased peripheral blood flow suffered by Astronauts returning to Earth. Due to the loss of blood to pump, the heart can atrophy in a micro-g environment. A weakened heart can result in low blood volume, low blood pressure and affect the body's ability to send oxygen to the brain without the individual becoming dizzy. Heart rhythm disturbances have also been seen among astronauts, but it is not clear whether this was due to pre-existing conditions of effects of a micro-g environment. One current countermeasure includes drinking a salt solution, which increases the viscosity of blood and would subsequently increase blood pressure which would mitigate post micro-g environment orthostatic intolerance. Another countermeasure includes administration of midodrine, which is a selective alpha-1 adrenergic agonist. Midodrine produces arterial and venous constriction resulting in an increase in blood pressure by baroreceptor reflexes.\n\nSpace Motion Sickness can lead to degraded astronaut performance. SMS threatens operational requirements, reduces situational awareness, and threatens the safety of those exposed to micro-g environments. Lost muscle mass leads to difficulty with movement, especially when astronauts return to earth. This can pose a safety issue if the need for emergency egress were to arise. Loss of muscle power makes it extremely difficult, if not impossible, for astronauts to climb through emergency egress hatches or create unconventional exit spaces in the case of a crash upon landing. Additionally, bone resorption and inadequate hydration in space can lead to the formation of kidney stones, and subsequent sudden incapacitation due to pain. If this were to occur during critical phases of flight, a capsule crash leading to worker injury and/or death could result. Short-term and long-term health effects have been seen in the cardiovascular system from exposure to the micro-g environment that would limit those exposed after they return to Earth or a regular gravity environment. Steps need to be taken to ensure proper precautions are taken into consideration when dealing a micro-g environment for worker safety. Orthostatic intolerance can lead to temporary loss of consciousness due to the lack of pressure and stroke volume. This loss of consciousness inhibits and endangers those affected and can lead to deadly consequences.\n\n\n"}
{"id": "14022748", "url": "https://en.wikipedia.org/wiki?curid=14022748", "title": "Millennium Villages Project", "text": "Millennium Villages Project\n\nThe Millennium Villages Project is/was a demonstration project of the Earth Institute at Columbia University, the United Nations Development Programme, and Millennium Promise aimed at proving that its integrated approach to rural development can be used to achieve the Millennium Development Goals—eight globally endorsed targets that address the problems of poverty, health, gender equality, and disease—by 2015. \n\nEven though the website of the Millennium Village Project is still active, the project ended with final evaluation in 2015 because it was initially scaled to progress for a decade from 2005. The project was divided into two phases, from 2004-2010 for the first phase and 2011-2015 for the second phase. In the first phase, the project was focused at the following five stations: Agriculture(seed and fertilizer support, farmer training and storage expansion, crop diversification,etc.), health (installation of mosquito nets, vaccine supply and pest control, etc.), Education (Construction of schools, installation of water supply, etc.), infrastructure (sanitation, roads, etc.), business development (micro-credit, cooperative training, etc.). Main focus in the second phase of the MVP was to successfully enhance, strengthen, and complete of the programs that have started in Phase 1. \n\nBy improving access to clean water, primary education, basic health care, sanitation, and other science-based interventions such as improved seeds and fertilizer, Millennium Villages aims to ensure that communities living in extreme poverty have a real, sustainable opportunity to lift themselves out of the poverty trap.\n\nThe first Millennium village was launched in 2005 in Sauri, Kenya. \"This is a village that’s going to make history,\" is how Millennium Villages founder Jeffrey Sachs described Sauri in \"The Diary of Angelina Jolie and Dr. Jeffrey Sachs in Africa\", a 2005 MTV documentary. \"It’s a village that’s going to end extreme poverty.\"\n\nMillennium Villages are divided into different types. There are the original core villages which include different agro-ecological zones covering 14 sites in 10 countries in sub-Saharan Africa, including: Sauri and Dertu, Kenya; Koraro, Ethiopia; Mbola, Tanzania; Ruhiira, Uganda; Mayange, Rwanda; Mwandama and Gumulira, Malawi; Pampaida and Ikaram, Nigeria; Potou, Senegal; Tiby and Toya, Mali and Bonsaaso, Ghana.\n\nThere are additional Millennium Villages which are following the Millennium Village program but which are not directly supported through The Earth Institute at Columbia University. These additional villages are located in Liberia, Cambodia, Jordan, Mozambique, Haiti, Cameroon and Benin.\n\nThe project was originally funded through a combination of World Bank loans and private contributions, including $50 million from George Soros. Initially designed with a timeline of five years, the project was extended to ten years to allow more time to reach the intended goals. In her 2013 book about the project, \"The Idealist: Jeffrey Sachs and the Quest to End Poverty\", journalist Nina Munk quotes Sachs saying: \"The main thing is to add another block of time to really get the income levels significantly raised.\"\n\nIn 2012, the UK's Department for International Development (DfID) committed $18.1m (£11.5m) over five years to fund a new Millennium Village in Ghana.\n\nOn 13 August 2013, the Islamic Development Bank and the Earth Institute at Columbia University announced the expansion of an earlier partnership to work with African nations to support their efforts to end extreme poverty. Part of that partnership involved the Islamic Development Bank providing $104 million in interest-free loans to 8 countries towards ending extreme poverty, improving public health, and achieve more sustainable development. Of that sum, $29 million was to be used to scale-ups of the Millennium Villages Project in Mali, Senegal and Uganda.\n\nIn July 2013, the Ugandan government announced it would scale up the Millennium Villages Project in the original region around the Ruhiira Millennium Village through $US 9.75 million of funding from the Islamic Development Bank. However, the promised scale-up has not yet taken place.\n\nIn September 2013, the Rwandan government and the Millennium Villages Project announced a partnership to scale-up certain aspects of the Millennium Village approach, as part of Rwanda's Vision 2020 Umurenge Programme which addresses poverty nationwide.\n\nThe Millennium Villages Project has provided lessons and techniques for Nigeria's development programs designed to achieve the Millennium Development Goals.\n\n\nThe Government of Japan (through its Human Security Trust Fund) and private philanthropic donors (through the Earth Institute at Columbia University) provided the financing for the first set of Millennium Villages, reaching some 60,000 people.\n\nA core aspect of the Millennium Villages is that the poverty-ending investments in agriculture, health, education, and infrastructure can be financed by donors at an incremental cost of just $60 per villager per year—$250,000 per village per year. The overhead costs of managing the project in each village is $50,000 per year.\n\nOn a per-person basis, the total village cost of $120 per person includes:\n\nCritically, the external financing needs of $70 per capita are in line with the financial commitments made by the leaders of industrialized countries at the 2005 Summit in Gleneagles. G8 countries promised to raise their development assistance to Africa to the equivalent of $70 per capita by 2010.\n\nIn a review of the project undertaken by the Overseas Development Institute (ODI) crop yield increases of 85-350% were recorded as well as reductions in malaria incidence of over 50%. While agricultural surpluses are able to be channelled into school meals programmes, helping to increase enrolment, improvements in health status are reported to increase labour productivity.\n\nAccording to ODI policy conclusions, in order for wider scale, more sustainable implementation to be achieved, village projects need to identify shared goals, seeking evidence-based, cost-effective interventions by governments and implementing agencies. They will also need to focus on addressing upstream investments such as training facilities for front-line staff.\n\nWhen compared to the Ekwendeni village of the Soils, Food and Healthy Communities (SFHC), the Millennium Villages obtain only similar achievements at far greater expense. This is a result of the Millennium Villages' use of artificial fertilizers and hybrids seeds (often of plants such as corn, which are not indigenous to the area). SFHC, on the other hand, uses diverse legume crops to improve soil health: \"The SFHC research project attempts to improve child nutritional status, household food security and soil fertility through use of different legume options which can improve the quality and quantity of food available within the household as well as provide organic inputs to improve soil fertility.\" According to Rachel Bezner Kerr, use of fertilizers and genetically modified seeds leads to dependence of the farmers on expensive products being marketed by large industrial companies. By contrast, the use of crop diversity to improve soil health is a low cost, and thus far more sustainable, solution. Note this comparison is only to one component of the Millennium Villages Project which works in many different sectors including agriculture, education, health, infrastructure and business development.\n\nJournalist Nina Munk followed the progress of a group of Millennium Villages for several years, and in her 2013 book \"The Idealist: Jeffrey Sachs and the Quest to End Poverty,\" she said a basic flaw of the Millennium Villages program is that it is developed by academics living far away from the subject areas and with a poor understanding of local cultures, who do things such as promoting growing maize among people who have not historically eaten it or building a short-lived livestock market when there was no local demand. But even in Ms. Munk's book teams of experts are described who are from the countries and regions who run the Millennium Villages on the ground. In addition, many notable academics and development experts who work for the project have a long history in working and living in Africa and other developing countries. The Millennium Villages describes itself as a community-led project that works with local leaders, regional officials and national governments in the decision-making process.\n\nA rigorous evaluation of the MVP is impossible due to shortcomings in the project's evaluation design which include the \"subjective choice of intervention sites, the subjective choice of comparison sites, the lack of baseline data on comparison sites, the small sample size, and the short time horizon\". A self-published assessment comparing villages three years into the project to how they were initially estimated large impacts on health, agricultural yield, and a variety of other measures. As some of this may have been due to regional improvements unrelated to the MVP, an independent evaluation comparing the MV to surrounding areas finds the effects are much more modest. An additional independent evaluation found that while agricultural productivity increased, final household income was not increased by the MVP.\n\nIn 2012, the MVP published an assessment in the Lancet showing \"reductions in poverty, food insecurity, stunting, and malaria parasitaemia\". Objections were raised to some of the conclusions in the assessment, and the authors were forced to correct them subsequently: calculations of the under-5 child mortality rate were flawed and withdrawn as the rate appears to have improved less in the MVP sites than in the surrounding regions.\n\n\n\n"}
{"id": "30603358", "url": "https://en.wikipedia.org/wiki?curid=30603358", "title": "Norman Adler", "text": "Norman Adler\n\nNorman Tenner Adler (June 7, 1941 - September 11, 2016) through his research, teaching, writing, and academic administration, made major contributions to the modern study of biological psychology and in American higher education, having helped develop the fields that are now labeled behavioral neurobiology and evolutionary psychology. One of Adler's prominent experiments included an in depth analysis of mating performance of male rats and its relation to fertilization in the female, which led him to observe how behaviour could affect reproduction in species. With his students and colleagues, he has worked at the interface between biology and behavior. They have stressed the importance of combining the study of physiological mechanisms controlling behavior with the functional/adaptive significance of behavior in an evolutionary context. He was influenced in this approach by his undergraduate teachers at Harvard, especially Paul Rozin, Jerry Hogan, and Gordon Bermant, and his student colleagues like Don Pfaff with whom he has maintained scientific relationships over the years. His research was also impacted by Daniel Lehrman, and he worked closely with Lehrman's student, Barry Komisaruk, on hormones and neural functioning. Adler is also a prominent figure in American higher education, especially the role of behavioral neuroscience in liberal arts education and religion in the college classroom. He participated in Phillip Zimbardo's PBS TV series Discovering Psychology, one of the first distance-learning courses in psychology.\n\nA native of Chicago, where he attended public schools and received a Jewish education at the College of Jewish Studies. In high school, Adler was interested in the career of a rabbi and a psychoanalyst, but didn't know which one to choose. He became drawn towards science after taking biology with a teacher named Richard Boyajian, who motivated him to pursue a career in biology. During his years in Harvard college, he took a course in physiological psychology, which gave him the opportunity to acknowledge that there is in fact an area of study that can combine his interests in biology and psychology. Adler graduated from Harvard in 1962 with a major in Psychology and extensive coursework in biology. After College, he spent the next year traveling around the world under the auspices of Harvard's Frederick Sheldon Traveling Fellowship. He used the year to visit the European ethologists and study animal behavior and complex social systems in general ('The City' was the title of his summary essay from that year). To become familiar with the techniques of the classical ethologists, he spent time in Africa and other field sites. He also developed an early interest in comparative religion—but decided that the field of 'psychology of religion' was not yet ready to develop so stayed with biological psychology, which he felt was just beginning. At the latter part of his career, he has returned to the study of psychology and religion with his students and colleagues at Yeshiva University—profoundly impressed at the speed with which a dormant field 'took off' and with how important these issues were becoming on the American campus.\n\nAfter the post-baccalaureate year of travel, Adler went to UC Berkeley, where he was a graduate student of Frank Beach (one of the founders of the study of hormones and behavior), and received a Master's in zoology with Howard Bern (comparative endocrinology). He then went to UCLA for a post-doctoral year with C. H. Sawyer (neuroendocrinology). He began his work on the effects of copulatory behavior on reproductive physiology. By making behavior the 'independent variable' and physiology the 'dependent variable', he felt that the adaptive significance of behavior would perforce be recognized in the biological system.\n\nHe joined the faculty at the University of Pennsylvania, where he became the youngest tenured professor at the time. He is also widely known as the founder of the Biological Basis of Behavior Program at Penn which was the prototype of the integrative study of behavior from a biological perspective. This program became a national model and helped spur the development of behavioral neuroscience in the American undergraduate curriculum. For this work, he received the Dana Award for Pioneering Achievement in Higher Education. The University of Pennsylvania now presents an annual \"Norman Adler Lecture in the Biological Basis of Behavior\" in his honor.\n\nHe has had a number of distinguished students in the field, including Martha McClintock at the University of Chicago, Elizabeth Adkins-Regan at Cornell University, Avery Gilbert, Jack Yanovski, Stephen Zoloth at Northeastern, Penny Bernstein at Kent State University, Joseph Anisko, Rodney Pelchat, Karen Stewart, and James Toner.\n\nHe served as Dean of Penn's Undergraduate College of Arts and Sciences, Dean of Yeshiva College for a decade, and was also Vice-Provost for Research and Graduate Education, and Professor of Psychology, at Northeastern University. He has also held visiting, teaching, or research appointments at University of Edinburgh, Drexel University Dept. of Engineering, and Swarthmore College.\n\nHe was University Professor of Psychology at Yeshiva University and Special Assistant to the Provost for Curriculum and Research Initiatives.\n\nIn addition to his work in behavioral neuroscience and its dissemination across American academics, he is an authority in American higher education, specializing in interdisciplinary and integrative learning. He initiated the Penn Reading Project in 1991 which has continued for over 20 years as an integrative introduction to liberal learning for college freshmen newly arriving on campus. Subsequently, Freshman Reading Projects have been adopted widely as part of the \"first-year experience\" at many college campuses. For this work, in 1992 Adler was inducted as an honorary member of the University of Pennsylvania's Philomathean Society—the oldest continually existing collegiate literary society in the United States.\n\nHe currently serves on the editorial board of Liberal Education, the journal of the American Association of Colleges and Universities; chairs a project for AACU’s section on ‘Big Questions: Faith and Reason on the College Campus’. In this project, citing data that there is a high level of spirituality among students in College because they continually question the \"deeper\" issues in their lives, the modern curriculum needs to address this. He lectures widely and is a consultant to Universities and Foundations.\n\nHe has received numerous prizes and honors including two Guggenheim Fellowships, Sigma Xi National Lecture, Phi Beta Kappa, Fellowship to Center for Advanced Study in the Behavioral sciences, Lindback Award for Teaching Excellence, and was in the first group of recipients for the American Psychological Association's Distinguished Scientific Awards for Early Career Contributions to Psychology.\n\n\n"}
{"id": "13861749", "url": "https://en.wikipedia.org/wiki?curid=13861749", "title": "Operation Safety Net", "text": "Operation Safety Net\n\nOperation Safety Net (OSN) is a Street Medicine program in Pittsburgh, Pennsylvania. OSN was founded when Dr. Jim Withers and Mike Sallows began to make \"house calls\" together at night under the bridges, along the river banks and in the abandoned buildings of Pittsburgh. In time, other formerly homeless outreach workers and medical volunteers joined the effort. In 1993, OSN became a nonprofit organization under the Pittsburgh Mercy Health System with Linda Sheets as the program administrator. It has grown into the nation's first full-time street-based medical delivery system. OSN's primary functions are to improve the well-being of the unsheltered homeless of Pittsburgh, advocate for health-care justice, educate health-care students, and to assist other cities to develop their own street medicine programs.\n\nOperation Safety Net visits the unsheltered homeless directly where they sleep through walking teams of the formerly homeless and medical outreach staff. Most of OSN's work force are volunteers, but a dedicated office/case management staff support all the field work. Services are available year-round, 24-hours a day. The work of OSN has evolved to include medical student education in the streets, housing, preventive health services, hospital consults, severe weather shelter and response, as well as extensive public education. OSN has received numerous national and international awards and was the subject of the documentary One Bridge to the Next.\n\nOperation Safety Net became a model for other cities, such as, Santa Barbara, California; San Diego, California; Morgantown, West Virginia; Atlanta, Georgia; and Chicago, Illinois. Through extensive global networking, OSN helped to create a street medicine collaborative that hosts the annual International Street Medicine Symposium (supported by the Robert Wood Johnson Foundation and Glaxo Smith Kline). As of 2007, there are 21 US cities and 11 international cities represented.\n\n"}
{"id": "215975", "url": "https://en.wikipedia.org/wiki?curid=215975", "title": "Paternity law", "text": "Paternity law\n\nPaternity law refers to body of law underlying legal relationship between a father and his biological or adopted children and deals with the rights and obligations of both the father and the child to each other as well as to others. A child's paternity may be relevant in relation to issues of legitimacy, inheritance and rights to a putative father's title or surname, as well as the biological father's rights to child custody in the case of separation or divorce and obligations for child support.\n\nUnder common law, a child born to a married woman is presumed under common law to be the child of her husband by virtue of a \"presumption of paternity\" or presumption of legitimacy. In consideration of a possible non-paternity event (which may or may not include paternity fraud) these presumptions may be rebutted by evidence to the contrary, for example, in disputed child custody and child support cases during divorce, annulment or legal separation.\n\nIn the case of a father not married to a child's mother, depending on the laws of the jurisdiction:\n\nToday, when paternity is in dispute or doubt, paternity testing may be used to conclusively resolve the issue.\n\nThe legal process of determining paternity normally results in the naming of a man to a child's birth certificate as the child's legal father. A paternity finding resolves issues of legitimacy, and may be followed by court rulings that relate to child support and maintenance, custody and guardianship.\n\nGenerally, under common law, a biological father has a legal obligation for the maintenance or support of his biological offspring, whether or not he is legally competent to marry the child's mother.\n\nIn jurisdictions where there is no presumption of paternity there is a process for fathers to recognise their children and become the legal father of the child.\n\nIn the United States, where a child is conceived or born during wedlock, the husband is legally presumed to be the father of the child. Some states have a legal process for a husband to disavow paternity, such that a biological father can be named as the parent of a child conceived or born during a marriage. In most states, any claim of non-paternity by a husband must be heard by a court.\n\nIf a parents litigate a divorce case without raising the issue of paternity, in most states they will be barred from disputing the husband's paternity in a later court proceeding. Depending upon state law, it may nonetheless be possible for a man claiming to be the child's biological father to commence a paternity case following the divorce.\n\nWhere paternity of the child is in question, a party may ask the court to determine paternity of one or more possible fathers (called putative fathers), typically based initially upon sworn statements and then upon testimony or other evidence.\n\nOnce paternity has been legally established, if the court finds that to do so would be contrary to the best interest of the child, in most U.S. states a court may deny DNA testing or decline to remove a husband from a child's birth certificate based upon DNA testing.\n\nA successful application to the court results in an order assigning paternity to a specific man, possibly including support responsibility and/or visitation rights, or declaring that one or more men (possibly including the husband of the mother) are not the father of the child. A disavowal action is a legal proceeding where a putative father attempts to prove to the court that he is not the father; if successful, it relieves the former putative father of legal responsibility for the child.\n\nOn the other hand, it could be the case where several putative fathers are fighting to establish custody. In such case. In the United States, a state may legally bar a third party from disputing the paternity of a child born within an intact marriage.\n\nSome paternity laws assign full parental responsibility to fathers even in cases of women lying about contraception, using deceit (such as oral sex followed by self-artificial insemination) or statutory rape by a woman (Hermesmann v. Seyer).\n\nIf the context of inheritance rights, it will be the heirs of the deceased person who are attempting to dispute or establish paternity. In some states, DNA testing will be dispositive to establish paternity. In many jurisdictions, however, there are a variety of rules and time restrictions that can deny inheritance rights to biological children of a deceased father.\n\n"}
{"id": "20993429", "url": "https://en.wikipedia.org/wiki?curid=20993429", "title": "Psychiatric interview", "text": "Psychiatric interview\n\nThe psychiatric interview refers to the set of tools that a mental health worker (most times a psychiatrist or a psychologist but at times social workers or nurses) uses to complete a psychiatric assessment.\n\nThe goals of the psychiatric interview are:\n\nThe data collected through the psychiatric interview is mostly subjective, based on the patient's report, and many times can not be corroborated by objective measurements. As such, one the interview's goals is to collect data that is both valid and reliable. \n\nValidity refers to how the data compares to an ideal absolute truth that the interviewer needs to access and uncover. Challenges that might affect the interview validity include can be categorized as patient related factors and interviewer related factors. Patient's related factors include:\nInterviewer related factors include:\n\nReliability refers to how datasets collected by different interviewers or the same interview at different times compare with one another. Ideal reliability is when a dataset will be stable irrespective of changes in specifics of the data collection. \n\nDifferent interview techniques have been shown to result in variations in the validity and reliability of the collected data. Open-ended question (\"Tell me about your sleep.\") have been shown to have better validity but less reliability than closed-ended questions(\"Do you have sleeping difficulties?\")\n\nPsychiatric assessment\n"}
{"id": "31421742", "url": "https://en.wikipedia.org/wiki?curid=31421742", "title": "Reinhard Karl", "text": "Reinhard Karl\n\nReinhard Karl (3 November 1946 – 19 May 1982) was a German mountaineer, photographer and writer.\n\nKarl was born in Heidelberg. At the age of 14, he started working as a mechanic apprentice. Later on, he joined night classes to complete high school. When he was admitted to daily school, he left his work as a mechanic to continue his studies in Frankfurt. He later became a professional mountain photographer and also wrote several books. At the beginning of his career, climbing during weekends was his way to escape from his work as a mechanic, which he disliked.\n\nHe discovered his passion for mountains thanks to his readings; in his autobiography he specifically mentions \"Achttausend drüber und drunter\" by Hermann Buhl and a book on Everest's ascent by Edmund Hillary. His mother encouraged him by sending him to the Mountain Club in Heidelberg, where he met Hermann Kühn, partner in several ascents, including the Eiger's north face.\n\nKarl's career started with his first climbing experiences on Battert, near Baden-Baden. He mastered several styles of climbing, ranging from alpine style in Europe and South America, free climbing and eight-thousanders.\n\nHe climbed several famous mountains and routes in the Alps, such as:\n\nKarl visited Yosemite several times, where he experienced both Big wall climbing and free climbing.\nHe climbed several big walls. In 1975, he climbed Half Dome north-west face and the Nose on El Capitan, Salathé Wall in 1977 and Son of Heart in 1978 on El Capitan.\nHe also got in contact with American climbers spending their summers in Yosemite, such as Ron Kauk and John Bachar, masters of bouldering.\n\nAfter his experience at Yosemite, he applied his new vision of climbing in Battert crag.\nIn 1977, Karl and Helmut Kiene realised the first grade VII climbing with the free ascent of the Pumprisse on the Fleischbank, on the Kaisergebirge.\n\nKarl was the first German to reach the summit of Mount Everest (with oxygen) on 10 May 1978, with Oswald Olz. He was part, as photographer, of the same expedition which saw Reinhold Messner and Peter Habeler climb Mount Everest for the first time without supplemental oxygen.\nKarl's second eight-thousander was Gasherbrum II, in Karakorum, with Hans Schell in 1979.\n\nIn 1980 Karl's attempt, with Hans Martin Götz, to climb Cerro Torre failed when they spent the night at only 250 meters from the top.\nAlso his attempt to climb the \"Supercanaleta\" on Fitz Roy with Luis Fraga did not succeed.\nHis last mountain was Fitz Roy, which he climbed in 1982 with Peter Luthy on the south-west face (route \"Chouinard\") two weeks after the failed attempt on \"Supercanaleta\".\n\nKarl has also had an important part in documenting mountain climbing in his pictures and books.\n\nHis books are the following:\n\nKarl died on 19 May 1982 in an ice avalanche at Camp II on Cho Oyu, during his attempt to climb his third eight-thousander.\n\n"}
{"id": "55976234", "url": "https://en.wikipedia.org/wiki?curid=55976234", "title": "Resurrection University", "text": "Resurrection University\n\nResurrection University (often referred to as ResU) is a private, Catholic university located in Chicago, Illinois that was founded on February 17, 1914. It has two colleges, a College of Nursing and a College of Allied Health, and offers undergraduate and graduate/professional programs. The university has over 5,600 university alumni. Its campus is located in Wicker Park, a Chicago neighborhood.\n\n1914: Resurrection University, previously named West Suburban Hospital School for Nurses was founded with the West Suburban Hospital that began serving the Oak Park community on February 11th, 1914. The school officially began on February 17th, offering a diploma program.\n\n1925: A new building was completed for the school on the 7th floor of the medical center, providing students with classrooms, laboratories, dormitories, swimming pool, and a ballroom.\n\n1946: The West Suburban Hospital for Nurses entered into an affiliation with Wheaton College that lasted until 1982\n\n1953: The name of the school was changed to West Suburban Hospital School of Nursing\n\n1981: The University was recognized by the Illinois Board of Higher Education, giving degree granting and operating authority to offer a nursing degree.\n\n1982: The Baccalaureate Nursing Program established, including a generic and Registered Nurse completion option.\n\n1985: The University entered into an affiliation with Concordia College (now Concordia University Chicago). Again, the name was changed to: Concordia-West Suburban College of Nursing.\n\n2003: The affiliation with Concordia College ended and the name was revised to: West Suburban College of Nursing.\n\n2004: Resurrection Health Care purchased West Suburban Medical Center and the College of Nursing. West Suburban College of Nursing became a part of Resurrection Health Care.\n\n2008: Higher Learning Commission approved the baccalaureate degrees of health informatics and health administration.\n\n2008: The Master of Science in Nursing program became accredited at the university and the first class to graduate with the MSN degree occurred in 2009. \n\n2010: West Suburban College of Nursing became Resurrection University with a College of Nursing and a College of Allied Health.\n\n2011: Resurrection Health Care and Provena Health joined together to form Presence Health.\n\n2012: Resurrection University and Concordia University Chicago reestablish the nursing degree program partnership.\n\n2012: The University moves to the campus of Saint Elizabeth Medical Center in the Wicker Park neighborhood of Chicago.\n\n2015: The Saint Francis School of Radiography (SFSOR), was merged with theUniversity, which now offered a Bachelor of Science in Imaging Technology (B.S.I.T.) in the College of Allied Health. The degree is accredited by the Joint Review Committee on Education in Radiologic Technology (JRCERT).\n\n2016: Dr. Therese Scanlan assumed the role of President for Resurrection University.\n\n2017: The University's Doctor of Nursing Practice program began in the fall semester.\n\n2018: In March, the affiliation between Resurrection University and Presence Health ended, and Presence Legacy Association became a member of Resurrection University.\n\nResurrection University has two colleges: the College of Nursing and the College of Allied Health.\n\nHeld as a special elective, any ResU student enrolled at the university has the opportunity share their knowledge and compassion with people and in the Chicago community and out of state. Previous service learning locations have included communities in Chicago neighborhoods, Alabama and Honduras. Distance service learning projects are often planned during university breaks to expand opportunities for staff and faculty involvement.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "48916869", "url": "https://en.wikipedia.org/wiki?curid=48916869", "title": "Robin Room", "text": "Robin Room\n\nRobin Gerald Walden Room (born December 28, 1939) is an Australian sociologist and researcher who studies the health effects of alcohol and other drugs. He has been the director of the Centre for Alcohol Policy Research at Turning Point Alcohol and Drug Centre in Fitzroy, Victoria, Australia, as well as the Professor of Alcohol Policy Research at the School of Population Health of the University of Melbourne, since March 2006. He is also a professor at the Centre for Social Research on Alcohol and Drugs at Stockholm University.\n\nRoom received his B.A. from Princeton University in English in 1960, and his two M.A.'s (the first in English, the second in sociology) from the University of California, Berkeley in 1962 and 1967, respectively. He later received his PhD in sociology from Berkeley in 1978.\n\nRoom began working at the National Alcohol Research Centre in Berkeley, California in 1963 and remained there until 1991. He became the Centre's scientific director in 1977. From 1991 to 1998, he worked at the Addiction Research Foundation in Canada, as their vice president for research. In 1999, he was appointed professor and founding director of the Centre for Social Research on Alcohol and Drugs at Stockholm University, where he is still a professor as of December 2010. In 2006, he joined both the Centre for Alcohol Policy Research and the University of Melbourne in 2006.\n\nRoom's research pertains to alcohol, illegal drugs, and gambling behavior. For example, in a 2005 study, Room and his co-authors Thomas Babor and Jurgen Rehm found that about 4% of the global burden of disease was attributable to alcohol consumption, about the same amount as tobacco consumption.\n\nRoom is the editor-in-chief of the peer-reviewed journal \"Drug and Alcohol Review\".\n\nRoom received the Alfred R. Lindesmith Award for Achievement in the Field of Scholarship from the Drug Policy Alliance in 2015. He has also received the Jellinek Award for Alcohol Studies in 1983, the Lifetime Achievement Award from the Alcohol, Tobacco and Other Drugs Section of the American Public Health Association in 2002, and the Prime Minister’s Award for Excellence in Drug and Alcohol Endeavours in 2012.\n\n"}
{"id": "29825613", "url": "https://en.wikipedia.org/wiki?curid=29825613", "title": "Salt and cardiovascular disease", "text": "Salt and cardiovascular disease\n\nSalt consumption has been intensely studied for its role in human physiology and impact on human health. In particular, excessive dietary salt consumption over an extended period of time has been associated with hypertension and cardiovascular disease, in addition to other adverse health effects.\n\nCommon edible salt is composed of sodium chloride.\n\nThe human body has evolved to balance salt intake with need through means such as the renin–angiotensin system. In humans, salt has important biological functions. Relevant to risk of cardiovascular disease, salt is highly involved with the maintenance of body fluid volume, including osmotic balance in the blood, extracellular and intracellular fluids, and resting membrane potential.\n\nThe well known effect of sodium on blood pressure can be explained by comparing blood to a solution with its salinity changed by ingested salt. Artery walls are analogous to a selectively permeable membrane, and they allow solutes, including sodium and chloride, to pass through (or not), depending on osmosis.\n\nCirculating water and solutes in the body maintain blood pressure in the blood, as well as other functions such as regulation of body temperature. When salt is ingested, it is dissolved in the blood as two separate ions – Na and Cl. The water potential in blood will decrease due to the increase solutes, and blood osmotic pressure will increase. While the kidney reacts to excrete excess sodium and chloride in the body, water retention causes blood pressure to increase.\n\nThe DASH-Sodium study was a sequel to the original DASH (Dietary Approaches to Stop Hypertension) study. Both studies were designed and conducted by the National Heart, Lung, and Blood Institute in the United States, each involving a large, randomized sample. While the original study was designed to test the effects of several varying nutrients on blood pressure, DASH-Sodium varies only in salt content in the diet.\n\nParticipants were pre-hypertensive or at stage 1 hypertension, and either ate a DASH-Diet or a diet reflecting an \"average American Diet\". During the intervention phase, participants ate their assigned diets containing three distinct levels of sodium in random order. Their blood pressure is monitored during the control period, and at all three intervention phases.\n\nThe study concluded that the effect of a reduced dietary sodium intake alone on blood pressure is substantial, and that the largest decrease in blood pressure occurred in those eating the DASH eating plan at the lowest sodium level (1,500 milligrams per day). However, this study is especially significant because participants in both the control and DASH diet group showed lowered blood pressure with decreased sodium alone.\n\nIn agreement with studies regarding salt sensitivity, participants of African descent showed high reductions in blood pressure. See sodium sensitivity below.\n\nThere has been strong evidence from epidemiological studies, human and animal intervention experiments supporting the links between high rate of salt intake and hypertension. A Cochrane review and meta-analysis of clinical trials showed that reduced sodium intake reduces blood pressure in hypertensive and normotensive subjects. Since controlling hypertension is related to a reduced risk of cardiovascular disease, it is plausible that salt consumption is a risk factor for cardiovascular health. However, to properly study the effects of sodium intake levels on risk of development of cardiovascular disease, long-term studies of large groups using both dietary and biochemical measures are necessary. Several of these studies show that groups with sodium-reduced diets have lower incidences of cardiovascular disease in all demographics, and in particular lower blood pressure. One study showed lower incidence of cardiovascular disease after 15 years of sodium reduction in a randomised trial.\n\nMore data is needed to support the conclusions of observational studies which suffer from design flaws. Many of these studies are not large enough, nor last long enough to provide conclusions on clinical outcomes for the effect of dietary sodium intake on morbidity and mortality. Previous mixed results and inconclusive interpretation of non-experimental studies may also root from the way sodium is measured in the study. The study by Cook and colleagues itself was unable to isolate only a change in sodium. The techniques to reduce sodium included keeping a food diary and reading labels. Cook and colleagues listed other effects of those techniques, including a reduction in fat and calories per day (11g, 200cal), and weight loss of 1 to 3 pounds. A 2014 Cochrane review found evidence that salt reduction prevented cardiovascular disease, but that the magnitude of the effect was uncertain and \"larger than would be predicted from the small blood pressure reductions achieved.\"\n\nThe World Health Organization issued a 2014 fact sheet to encourage reducing global salt consumption by 30% through 2025.\n\nIn 2015, the United States Centers for Disease Control and Prevention began an initiative encouraging Americans to reduce their consumption of salty foods. The American Heart Association defined a daily sodium consumption limit should be 1500 milligrams (contained in less than 0.75 teaspoon of table salt).\n\nAccording to a 2012 Health Canada report, Canadians in all age groups are consuming 3400 mg per day of sodium, more than twice as much as needed. The US Centers for Disease Control and Prevention stated that the average daily sodium intake for Americans over 2 years of age is 3436 milligrams. The majority of sodium consumed by North Americans is from processed and restaurant foods, while only a small portion is added during cooking or at the table.\n\nIn the European Union, half of the member states legislated change in the form of taxation, mandatory nutrition labeling, and regulated nutrition and health claims to address overconsumption of sodium in response to a 2012 EU Salt Reduction Framework.\n\nDespite few claims to the contrary, most physicians and clinical scientists, the European Food Safety Authority and the US Centers for Disease Control recommend that consumers use less salt in their diets, mainly to reduce the risk of high blood pressure and associated cardiovascular diseases in adults and children.\n\nA limited group of researchers has cast doubts on the generally accepted theory that lowering sodium intake will improve the health of a given population.\n\nRather than create drastic salt policies based on conflicting data, Alderman and his colleague Hillel Cohen propose that the government sponsor a large, controlled clinical trial to see what happens to people who follow low-salt diets over time. Appel responds that such a trial \"cannot and will not be done,\" in part because it would be so expensive. But unless we have clear data, evangelical antisalt campaigns are not just based on shaky science; they are ultimately unfair. \"A great number of promises are being made to the public with regard to this enormous benefit and lives saved,\" Cohen says. But it is \"based on wild extrapolations.\"\n\n\"Taken together, our current findings refute the estimates of computer models of lives saved and health care costs reduced with lower salt intake. They do also not support the current recommendations of a generalized and indiscriminate reduction of salt intake at the population level. However, they do not negate the blood pressure-lowering effects of a dietary salt reduction in hypertensive patients.\"\n\nThe traditional Japanese diet is very high in salt intake and yet, the Japanese had the highest rate of longevity in the world, and low rates of cardiovascular disease. \". . .we have found that the Japanese dietary pattern is associated with lower CVD mortality, despite the fact that the Japanese dietary pattern appeared to be related to higher sodium intake and high prevalence of hypertension.\"\n\nSome studies have shown that salt consumption has been found to increase the risk of myocardial infarction, stroke, arterial stiffness and heart failure.\n\nIn the majority of studies, reducing sodium intake lowers the risk of hypertension and associated cardiovascular disorders.\n\nIn a few studies, both high (more than 7 g/day) and low salt intake (less than 3 g/day) were associated with an increased risk for cardiovascular disease and increased mortality.\n\nA diet high in sodium increases the risk of hypertension in people with sodium sensitivity, corresponding to an increase in health risks associated with hypertensions including cardiovascular disease.\n\nUnfortunately, there is no universal definition of sodium sensitivity; the method to assess sodium sensitivity varies from one study to another. In most studies, sodium sensitivity is defined as the change in mean blood pressure corresponding to a decrease or increase of sodium intake. The method to assess sodium sensitivity includes the measurement of circulating fluid volume and peripheral vascular resistance. Several studies have shown a relationship between sodium sensitivity and the increase of circulating fluid volume or peripheral vascular resistance.\n\nA number of factors have been found to be associated with sodium sensitivity. Demographic factors which affect sodium sensitivity include race, gender, and age. One study shows that the American population of African descent are significantly more salt sensitive than Caucasians. Women are found to be more sodium sensitive than men; one possible explanation is based on the fact that women have a tendency to consume more salt per unit weight, as women weigh less than men on average. Several studies have shown that the increase in age is also associated with the occurrence of sodium sensitivity.\n\nThe difference in genetic makeup and family history has a significant impact on salt sensitivity, and is being studied more with improvement on the efficiencies and techniques of genetic testing. In both hypertensive and non-hypertensive individuals, those with haptoglobin 1-1 phenotype are more likely to have sodium sensitivity than people with haptoglobin 2-1 or 2-2 phenotypes. More specifically, haptoglobin 2-2 phenotypes contribute to the characteristic of sodium-resistance in humans. Moreover, prevalence of a family history of hypertension is strongly linked with the occurrence of sodium sensitivity.\n\nThe influence of physiological factors including renal function and insulin levels on sodium sensitivity are shown in various studies. One study concludes that the effect of renal failure on sodium sensitivity is substantial due to the contribution of decreasing the Glomerular filtration rate (GFR) in the kidney. Moreover, insulin resistance is found to be related to sodium sensitivity; however, the actual mechanism is still unknown.\n\nPossible mechanisms by which high intakes of dietary potassium can decrease risk of hypertension and instances of cardiovascular disease have been proposed but not extensively studied. However, studies have found a strong inverse association between long-term adequate to high rates of potassium intake and the development of cardiovascular diseases.\n\nThe recommended dietary intake of potassium is higher than that of sodium. Unfortunately, the average absolute intake of potassium of studied populations is lower than that of sodium intake. According to Statistics Canada, Canadians' potassium intake in all age groups are lower than recommended, while sodium intake greatly exceed recommended intake in every age group.\n\nIt has been hypothesized that the ratio of potassium to sodium intake accounts for the large difference in the occurrence of hypertension between primitive cultures eating diets made up of mostly unprocessed foods and Western diets which tend to include highly processed foods.\n\nThe growing awareness of excessive sodium consumption in connection with hypertension and cardiovascular disease has increased the usage of salt substitutes at both a consumer and industrial level.\n\nOn a consumer level, salt substitutes, which usually substitute a portion of sodium chloride content with potassium chloride, can be used to increase the potassium to sodium consumption ratio. This change has been shown to blunt the effects of excess salt intake on hypertension and cardiovascular disease. It has also been suggested that salt substitutes can be used to provide an essential portion of daily potassium intake, and may even be more economical than prescription potassium supplements.\n\nIn the food industry, processes have been developed to create low-sodium versions of existing products. The meat industry especially have developed and fine-tuned methods to decrease salt contents in processed meats without sacrificing consumer acceptance. Research demonstrates that salt substitutes such as potassium chloride, and synergistic compounds such as phosphates, can be used to decrease salt content in meat products.\n\nThere have been concerns with certain populations' use of potassium chloride as a substitute for salt as high potassium loads are dangerous for groups with diabetes, renal diseases, or heart failure. The use of salts with minerals such as natural salts have also been tested, but like salt substitutes partially containing potassium, mineral salts produce a bitter taste above certain levels.\n\n"}
{"id": "2377367", "url": "https://en.wikipedia.org/wiki?curid=2377367", "title": "Source document", "text": "Source document\n\nA source document is a document in which data collected for a clinical trial is first recorded. This data is usually later entered in the case report form. The International Conference on Harmonisation of Technical Requirements for Registration of Pharmaceuticals for Human Use (ICH-GCP) guidelines define source documents as \"original documents, data, and records.\" Source documents contain source data, which is defined as \"all information in original records and certified copies of original records of clinical findings, observations, or other activities in a clinical trial necessary for the reconstruction and evaluation of the trial.\"\n\nThe Food and Drug Administration (FDA) does not define the term \"source document\".\n\n"}
{"id": "36819980", "url": "https://en.wikipedia.org/wiki?curid=36819980", "title": "Sulfozinum", "text": "Sulfozinum\n\nSulfozinum (sulfozin) is a pharmaceutical drug that causes a pyrogenic reaction (body temperature elevation) and severe pain. Sulfozinum is a 0.37 - 2% sterilized solution of purified elemental sulfur in peach oil or olive oil for intramuscular injections. The preparation is unstable, so it was prepared only in local hospital pharmacies. In the Soviet Union, it was used in the pyrogenic treatment of syphilitic encephalitis (mostly in the pre-antibiotics era), various psychiatric conditions, and alcoholism. Sulfozin was not used in American psychiatry.\n\nThe American delegation during its visit to the USSR in 1989 confirmed charges of the use of sulfozine injections. Psychiatrists in the USSR employed sulfozine treatment allegedly to increase treatment response to neuroleptic administration but were unable to present any research evidence of its efficiency for this purpose. The muscle necrosis, fever, immobility, and severe pain caused by sulfozine, as well as the pattern of its use in 10 persons, suggest that the medication was applied for punitive rather than therapeutic purposes.\n\nReal benefits of its use in psychiatry are disputable, but it was widely used due to its extremely painful action, lasting from several hours to 2–3 days, as a punishment for psychiatric patients and in political abuse of psychiatry. Sulfazine symbolised Soviet punitive psychiatry.\n\nIn 1989, during Perestroika, its use was restricted only to cases when its prescription was confirmed both by consilium and by informed consent of the patient or his representatives. Its present use is not known.\n\nSome psychiatrists in post-Soviet Russia call the criticism of sulfozin attacks on psychiatry and still believe that sulfozin was sometimes the only effective treatment when all other ones were ineffective in calming down violent patients. The psychiatrists say that sulfozin really brought a psychosis to remission.\n"}
{"id": "25878800", "url": "https://en.wikipedia.org/wiki?curid=25878800", "title": "The Leprosy Mission", "text": "The Leprosy Mission\n\nThe Leprosy Mission is an international Christian charity working towards the eradication of the causes and consequences of leprosy. It is active in over 50 countries around the world.\n\nThe Leprosy Mission is a member of ILEP (International Federation of Anti-Leprosy Associations) and Global Connections, a network of mission and development agencies.\n\nIn December 1869, Wellesley Bailey, a young Irishman who was working as a teacher in the Punjab in India, came across a row of huts inhabited by men and women with serious disabilities and physical deformities. A colleague explained that they were suffering from leprosy. Bailey was shocked by what he saw. Afterwards he wrote:'I almost shuddered, yet I was at the same time fascinated, and I felt that if there was ever a Christlike work in the world it was to go amongst these poor sufferers and bring them the consolation of the gospel.'\n\nOn returning to Ireland in 1874, Wellesley Bailey and his wife Alice began to hold meetings in Dublin to tell friends about their experiences of people affected by leprosy in India, and to raise money'. And so The Leprosy Mission, or The Mission to Lepers as it was known then, was born.\n\n1874-1893 – The Baileys travel extensively in India to see the need of people affected by leprosy and to encourage support work.\n\n1891 – Wellesley Bailey visits Mandalay, Burma, to open the first TLM home for leprosy-affected people outside India.\n\n1917 – The Mission has extended its work throughout India and the Far East and now has 87 programmes in 12 countries, with support offices in eight countries.\n\n1940s – In South India, Paul Brand pioneers medical research and reconstructive surgery on leprosy deformities in hands and feet.\n\n1940s-50s – The first effective cure for leprosy, Dapsone, is introduced. Over the next 15 years, millions of patients are successfully treated.\n\n1950s – The Mission’s work is extended into Africa.\n\n1954 – World Leprosy Day is founded by Raoul Follereau, a French writer, to make sure that people everywhere know that leprosy still exists and is completely curable. It is usually held each year on the last Sunday in January.\n\n1960s – Leprologists work to discover new drugs that are effective against leprosy as many people are discovered to have Dapsone-resistant leprosy.\n\n1965 – The Mission changes its name from 'The Mission to Lepers' to the Leprosy Mission to avoid the negative connotations of the word ‘leper.’\n\n1970s – TLM begins to extend its work to people's homes and communities, rather than just hospitals and asylums.\n\n1980 – Vincent Barry and his team win the 1980 UNESCO Science Prize for their discovery of anti-leprosy drug clofazimine, developed with the assistance of The Leprosy Mission.\n\n1981 – World Health Organization (WHO) recommends a new combination drug treatment for leprosy, MDT (Multi Drug Therapy). People are cured in as little as six months.\n\n1990s – As many more people are cured, caring for people with lasting disabilities through social, economic, and physical rehabilitation becomes increasingly important.\n\nTLM is a global network; the organisation works with governments, communities, local churches, partner health organisations, WHO, and other non-governmental organisations.\n\nIt runs leprosy projects and has staff in 28 countries in Africa, South Asia and East Asia Pacific.\n\nA large percentage of TLM's work is focused on India, where the prevalence of leprosy is still high.\n\nSupport offices, which raise funds for TLM's work and invest in the development of the field projects, operate in 26 countries, including Europe, the US, Canada, Australia and New Zealand.\n\nThe international office is based in London.\n\nThe Leprosy Mission runs 14 hospitals in India. Although these hospitals were initially established to care exclusively for people with leprosy, they have now been developed into community hospitals, providing a wide range of health care services. It is becoming more common for people who have leprosy to be treated on the same wards as those with general medical conditions. This helps to break down the stigma of leprosy. Historically, people have been very fearful of contracting leprosy, believing it to be highly contagious when this is not the case.\n\nReconstructive surgery: Leprosy attacks the nerves, and if not treated quickly people can lose sensation in their hands and feet leading to injury and disability. The Leprosy Mission provides reconstructive surgery for leprosy-affected people, restoring movement to hands and feet. Dr Paul Brand pioneered tendon transfer surgery which is still used by TLM doctors today.\n\nLeprosy is found predominantly in developing countries and areas where poverty is widespread. Because leprosy can cause disability, and because stigma is so often attached to the disease, leprosy can make poor people poorer.\n\nAlong with essential healthcare, The Leprosy Mission provides training and education for leprosy-affected people.\n\nChildren whose parents have leprosy, or who have had leprosy themselves, can benefit from school scholarship programmes. TLM pays for the child's schooling, allowing them to gain an education that they otherwise may have missed out on.\n\nTLM has six vocational training centres in India. Here, teenagers and young adults can learn a skill like tailoring, car mechanics, or IT.\n\nAt other projects in Africa and other parts of Asia, TLM provides skills training through workshops or one-to-one teaching to enable people to learn a trade with the hope that, in the long term, they will be better equipped to provide for themselves and their families.\n\nOnce a leprosy-affected person has lost feeling in their limbs or hands or feet, holding a hot cup, stepping on a nail, or getting too close to a fire, can cause injuries they may not notice. Then the wound may become infected because bacteria get in.\n\nIn most of its projects, The Leprosy Mission teaches what's called 'self-care'. Physiotherapists and prevention of disability advisers are on-hand to demonstrate how washing, soaking and oiling skin daily can help prevent infection. This process can also ensure that small cuts do not develop into larger, more difficult-to-treat, ulcers.\n\nThe Leprosy Mission provides microcredit loans to those leprosy-affected people in need of economic assistance. These loans enable people to set up a small business, such as small shops, animal breeding, tailoring or petty trading.\n\nLow Cost Houses are a way of improving the living conditions of leprosy-affected people. The Leprosy Mission provides the materials. The community, or a group of volunteers, provides the manpower to complete the building project. TLM often works with international charity Habitat for Humanity to develop housing projects such as these.\n\nIn some countries where TLM works, savings groups (or self-help groups) have been established. The members of these groups are encouraged to save a small amount of money each week. The money is then placed into a kitty. The kitty can then be used by the group as a whole - to set up a group business or a community project - or members can apply for an individual loan from the group.\n\nAdvocating for people's human rights is another aspect of The Leprosy Mission's work. When necessary, TLM will lobby governments in countries where there is any remaining anti-leprosy legislation, keeping up the pressure until the laws are repealed.\n\nTLM educates people about leprosy, explaining that it's not highly contagious, that it's not a curse from the gods and that it can be cured. This is an effective way of reducing negative attitudes and encouraging greater acceptance of leprosy-affected people.\n\nTLM also encourages leprosy-affected people to speak out for themselves and their rights.\n\nThis is a regional office, based in Peterborough. It is part of TLM's global network, raising money and providing support to projects in many of the countries where TLM works.\n\nIn June 2015, The Leprosy Mission England and Wales launched their Feet First campaign. By encouraging members of the British public to take on the Barefoot Challenge and go about their lives barefoot for one day, the charity aimed to transform the lives of some of the world's poorest and most marginalised people in Mozambique. Those that took on the challenge were encouraged to share their photos on social media and nominate three friends to also take part. All funds raised during the challenge up until 31 August 2015 were doubled by the UK government.\n\nTLM Northern Ireland's office is based in Lisburn. Much of TLM's support here comes from local churches.\n\nTLM Scotland's offices are based in Stirling. The organisation is led by CEO Linda Todd, and staffed by a combination of paid staff and volunteers. This regional office raises funds for many TLM projects around the world, and raises awareness of leprosy and TLM's work in Scotland.\n\nTLMS support seven of TLM International's implementing countries on their work. These countries are Nigeria, South Sudan, Angola, Nepal, India, Bangladesh and Myanmar. The Leprosy Mission Scotland seeks to educate, inspire and enable people, their churches, schools, community groups and governmental bodies, to uphold and support people affected by leprosy.\n\neffect:hope's office is based in Markham, near Toronto, Ontario. This office operates in partnership with The Leprosy Mission International but is independent. Its programmes serve people living with Neglected Tropical Diseases like leprosy, Buruli ulcer, and Lymphatic Filariasis. Programme countries include India, Bangladesh, Cote d'Ivoire, Kenya, DR Congo, Ghana, Liberia, Nepal, and Nigeria. Also, effect:hope raises awareness of leprosy and other disease of poverty in Canada and educates Canadians about Neglected Tropical Diseases. effect:hope also has a special program for churches to participate in World Leprosy Day each year.\nThis is TLM's trading arm, sourcing and selling many goods made by leprosy-affected or disability-affected people.\n\nThe Leprosy Mission was founded in 1874 as ‘The Mission to Lepers’ by Wellesley Bailey, in Ambala, India. Subsequently in 1973, The Leprosy Mission Trust India (TLMTI) was registered as a Society under the Societies Registration Act of 1860. TLMTI is the largest leprosy-focused non-governmental organisation in India and is headquartered in New Delhi, India. The organisation works with people affected by leprosy and other neglected tropical diseases (NTDs), people with disabilities, and marginalised communities, especially women.\n\nTLMTI has a diverse set of programmes – Healthcare, Sustainable Livelihood, Community Empowerment, Advocacy, and Research and Training. These programmes are implemented through 14 hospitals, six vocational training centres, five residential facilities for the care of elderly persons affected by leprosy and having leprosy-related disabilities, eight community development projects, a molecular biology research laboratory, advocacy and communication, and research and training functions, spread across 10 states of India.\nthis organization has not relation to The Leprosy Mission\n\n"}
{"id": "57732890", "url": "https://en.wikipedia.org/wiki?curid=57732890", "title": "Theodore T. Macadam", "text": "Theodore T. Macadam\n\nTheodore T. Macadam was a pharmaceutical industry executive. He was president of the History of Medicine Society of the Royal Society of Medicine from 1989 to 1990. He was treasurer of the Osler Club of London from 1978 to 1994.\n\n"}
{"id": "6840842", "url": "https://en.wikipedia.org/wiki?curid=6840842", "title": "Transient synovitis", "text": "Transient synovitis\n\nTransient synovitis of the hip (also called toxic synovitis; see below for more synonyms) is a self-limiting condition in which there is an inflammation of the inner lining (the synovium) of the capsule of the hip joint. The term irritable hip refers to the syndrome of acute hip pain, joint stiffness, limp or non-weightbearing, indicative of an underlying condition such as transient synovitis or orthopedic infections (like septic arthritis or osteomyelitis). In everyday clinical practice however, irritable hip is commonly used as a synonym for transient synovitis. It should not be confused with sciatica, a condition describing hip and lower back pain much more common to adults than transient synovitis but with similar signs and symptoms.\n\nTransient synovitis usually affects children between three and ten years old (but it has been reported in a 3-month-old infant and in some adults). It is the most common cause of sudden hip pain and limp in young children. Boys are affected two to four times as often as girls. The exact cause is unknown. A recent viral infection (most commonly an upper respiratory tract infection) or a trauma have been postulated as precipitating events, although these are reported only in 30% and 5% of cases, respectively.\n\nTransient synovitis is a diagnosis of exclusion. The diagnosis can be made in the typical setting of pain or limp in a young child who is not generally unwell and has no recent trauma. There is a limited range of motion of the hip joint. Blood tests may show mild inflammation. An ultrasound scan of the hip joint can show a fluid collection (effusion). Treatment is with nonsteroidal anti-inflammatory drugs and limited weight-bearing. The condition usually clears by itself within seven to ten days, but a small group of patients will continue to have symptoms for several weeks. The recurrence rate is 4–17%, most of which is in the first six months.\n\nTransient synovitis causes pain in the hip, thigh, groin or knee on the affected side. There may be a limp (or abnormal crawling in infants) with or without pain. In small infants, the presenting complaint can be unexplained crying (for example, when changing a diaper). The condition is nearly always limited to one side. The pain and limp can range from mild to severe.\n\nSome children may have a slightly raised temperature; high fever and general malaise point to other, more serious conditions. On clinical examination, the child typically holds the hip slightly bent, turned outwards and away from the middle line (flexion, external rotation and abduction). Active and passive movements may be limited because of pain, especially abduction and internal rotation. The hip can be tender to palpation. The log roll test involves gently rotating the entire lower limb inwards and outwards with the patient on his back, to check when muscle guarding occurs. The unaffected hip and the knees, ankles, feet and spine are found to be normal.\nIn the past, there have been speculations about possible complications after transient synovitis. The current consensus however is that there is no proof of an increased risk of complications after transient synovitis.\n\nOne such previously suspected complication was coxa magna, which is an overgrowth of the femoral head and broadening of the femoral neck, accompanied by changes in the acetabulum, which may lead to subluxation of the femur. There was also some controversy about whether continuous high intra-articular pressure in transient synovitis could cause avascular necrosis of the femoral head (Legg-Calvé-Perthes disease), but further studies did not confirm any link between the two conditions.\n\nThere are no set standards for the diagnosis of suspected transient synovitis, so the amount of investigations will depend on the need to exclude other, more serious diseases.\n\nInflammatory parameters in the blood may be slightly raised (these include erythrocyte sedimentation rate, C-reactive protein and white blood cell count), but raised inflammatory markers are strong predictors of other more serious conditions such as septic arthritis.\n\nX-ray imaging of the hip is most often unremarkable. Subtle radiographic signs include an accentuated pericapsular shadow, widening of the medial joint space, lateral displacement of the femoral epiphyses with surface flattening (Waldenström sign), prominent obturator shadow, diminution of soft tissue planes around the hip joint or slight demineralisation of the proximal femur. The main reason for radiographic examination is to exclude bony lesions such as occult fractures, slipped upper femoral epiphysis or bone tumours (such as osteoid osteoma). An anteroposterior and frog lateral (Lauenstein) view of the pelvis and both hips is advisable.\n\nAn ultrasound scan of the hip can easily demonstrate fluid inside the joint capsule (Fabella sign), although this is not always present in transient synovitis. However, it cannot reliably distinguish between septic arthritis and transient synovitis. If septic arthritis needs to be ruled out, needle aspiration of the fluid can be performed under ultrasound guidance. In transient synovitis, the joint fluid will be clear. In septic arthritis, there will be pus in the joint, which can be sent for bacterial culture and antibiotic sensitivity testing.\n\nMore advanced imaging techniques can be used if the clinical picture is unclear; the exact role of different imaging modalities remains uncertain. Some studies have demonstrated findings on magnetic resonance imaging (MRI scan) that can differentiate between septic arthritis and transient synovitis (for example, signal intensity of adjacent bone marrow). Skeletal scintigraphy can be entirely normal in transient synovitis, and scintigraphic findings do not distinguish transient synovitis from other joint conditions in children. CT scanning does not appear helpful.\nPain in or around the hip and/or limp in children can be due to a large number of conditions. Septic arthritis (a bacterial infection of the joint) is the most important differential diagnosis, because it can quickly cause irreversible damage to the hip joint. Fever, raised inflammatory markers on blood tests and severe symptoms (inability to bear weight, pronounced muscle guarding) all point to septic arthritis, but a high index of suspicion remains necessary even if these are not present. Osteomyelitis (infection of the bone tissue) can also cause pain and limp.\n\nBone fractures, such as a toddler's fracture (spiral fracture of the shin bone), can also cause pain and limp, but are uncommon around the hip joint. Soft tissue injuries can be evident when bruises are present. Muscle or ligament injuries can be contracted during heavy physical activity —however, it is important not to miss a slipped upper femoral epiphysis. Avascular necrosis of the femoral head (Legg-Calvé-Perthes disease) typically occurs in children aged 4–8, and is also more common in boys. There may be an effusion on ultrasound, similar to transient synovitis.\n\nNeurological conditions can also present with a limp. If developmental dysplasia of the hip is missed early in life, it can come to attention later in this way. Pain in the groin can also be caused by diseases of the organs in the abdomen (such as a psoas abscess) or by testicular disease. Rarely, there is an underlying rheumatic condition (juvenile idiopathic arthritis, Lyme arthritis, gonococcal arthritis, ...) or bone tumour.\n\nTreatment consists of rest, non-weightbearing and painkillers when needed. A small study showed that the nonsteroidal anti-inflammatory drug ibuprofen could shorten the disease course (from 4.5 to 2 days) and provide pain control with minimal side effects (mainly gastrointestinal disturbances). If fever occurs or the symptoms persist, other diagnoses need to be considered.\n\n"}
{"id": "10022123", "url": "https://en.wikipedia.org/wiki?curid=10022123", "title": "Vaccine efficacy", "text": "Vaccine efficacy\n\nVaccine efficacy is the percentage reduction of disease in a vaccinated group of people compared to an unvaccinated group, using the most favorable conditions. Vaccine efficacy was designed and calculated by Greenwood and Yule in 1915 for the cholera and typhoid vaccines. It is best measured using double- blind, randomized, clinical controlled trials, such that it is studied under “best case scenarios.” Vaccine effectiveness differs from vaccine efficacy in that vaccine effectiveness shows how well a vaccine works when they are always used and in a bigger population whereas vaccine efficacy shows how well a vaccine works in certain, often controlled, conditions. Vaccine efficacy studies are used to measure several possible outcomes such as disease attack rates, hospitalizations, medical visits, and costs.\n\nThe outcome data (vaccine efficacy) generally are expressed as a proportionate reduction in disease attack rate (AR) between the unvaccinated (ARU) and vaccinated (ARV) studies can be calculated from the relative risk (RR) of disease among the vaccinated group with use of the following formulas.\nThe basic formula is written as:formula_1with\nAn alternative, equivalent formulation of vaccine efficacy formula_5where formula_6 is the relative risk of developing the disease for vaccinated people compared to unvaccinated people.\n\nVaccine efficacy differs from vaccine effectiveness in the same way that an explanatory clinical trial differs from an intention to treat trial: vaccine efficacy shows how effective the vaccine could be given ideal circumstances and 100% vaccine uptake; vaccine effectiveness measures how well a vaccine performs when it is used in routine circumstances in the community. Since vaccine efficacy is based on a population that are placed in certain controlled environment, this study becomes more effective. If the criteria changed, such as if it was based on a larger population that wasn’t as restricted and in a more natural environment, that would be the vaccines effectiveness. What makes the vaccine efficacy applicable is that it also shows the disease attack rates as well as a tracking of vaccination status. Vaccine effectiveness is a lot more easily tracked than the vaccine efficacy considering the difference in environment; however, the vaccine efficacy is more expensive and very difficult to conduct. Because the trial is based on people who are taking the vaccination and people who aren’t, there is a risk for disease, and optimal treatment is needed for those who become infected. \nThe advantages of a vaccine efficacy have control for all biases that would be found with randomization, as well as prospective, active monitoring for disease attack rates, and careful tracking of vaccination status for a study population there is normally a subset as well, laboratory confirmation of the infectious outcome of interest and a sampling of vaccine immunogenicity. The major disadvantages of vaccine efficacy trials are the complexity and expense of performing them, especially for relatively uncommon infectious outcomes of diseases for which the sample size required is driven up to achieve clinically useful statistical power.\n\nVaccine efficacy is calculated on a population basis. It is therefore relatively easy to misunderstand its application.\n\nThe NEJM did a study on the A flu efficacy Influenza virus. A total of 1952 subjects were enrolled and received study vaccines in the fall of 2007. Influenza activity occurred from January through April 2008, with the circulation of influenza types:\nAbsolute efficacy against both types of influenza, as measured by isolating the virus in culture, identifying it on real-time polymerase-chain-reaction assay, or both, was 68 percent (95 percent confidence interval [CI], 46 to 81) for the inactivated vaccine and 36 percent (95 percent CI, 0 to 59) for the live attenuated vaccine. In terms of relative efficacy, there was a 50 percent (95 percent CI, 20 to 69) reduction in laboratory-confirmed influenza among subjects who received inactivated vaccine as compared with those given live attenuated vaccine. subjects were placed in a healthy adult population. The efficacy against the influenza A virus was 72 percent and for the inactivated was 29 percent with a relative efficacy of 60 percent. The influenza vaccine is not 100% efficacious in preventing disease, but it is as close to 100% safe, and much safer than the disease.\n\nSince 2004, clinical trials testing the efficacy of the influenza vaccine have been drifting in:\n2058 people were vaccinated in October and November 2005. Influenza activity was prolonged but of low intensity; type A (H3N2) was the virus that was generally going around the population, which was very alike to the vaccine itself . The efficacy of the inactivated vaccine was 16% (95% confidence interval [CI], -171% to 70%) for the virus identification end point (virus isolation in cell culture or identification through polymerase chain reaction) and 54% (95% CI, 4%-77%) for the primary end point (virus isolation or increase in serum antibody titer). The absolute efficacies of the live attenuated vaccine for these end points were 8% (95% CI, -194% to 67%) and 43% (95% CI, -15% to 71%).\n\nWith serologic end points included, efficacy was demonstrated for the inactivated vaccine in a year with low influenza attack rates. Influenza vaccines are effective in reducing cases of influenza, especially when the content predicts accurately circulating types and circulation is high. However, they are less effective in reducing cases of influenza-like illness and have a modest impact on working days lost. There is insufficient evidence to assess their impact on complications.\n"}
{"id": "12696167", "url": "https://en.wikipedia.org/wiki?curid=12696167", "title": "Victoria (3D figure)", "text": "Victoria (3D figure)\n\nVictoria is an articulated 3D female figure developed and sold by DAZ 3D. There have been several \"generations\" of the figure, all bearing the same name.\n\nThe figure was originally created as one of two standard characters which also included the male character \"Michael\" (\"Stephanie\" being a full body female morph of Michael).\n\nBesides being poseable, using preset morph dials the figure can be manipulated into a variety of different aesthetic and ethnic female forms. The figure comes pre-rigged with standardized features, and earlier versions can be exported to a variety of 3D modeling/animation applications.\n\nVictoria 1 was released by then-Zygote in February 1999 and was the first named medium-resolution figure for use with Poser. She was originally released as 'The Millennium Woman', but the resulting colloquial shortening to \"Millie\" led to Zygote renaming her as Victoria. Zygote also sold an anime-morphed character for Victoria named Aiko, and each successive generation of Victoria has so far had an accompanying Aiko (whose facial morphs can vary between anime and a less exaggerated Asian).\n\nVictoria 2 was the next step in the figure development, refining the rigging and coming with as a reduced polygon version. Released in 2001, V2 contained more body shaping features and a wider range of facial expressions but was rigged to be compatible with V1's clothing and accessories. From this point forward, the Stephanie character (following the Generation numbering) was no longer a remorphed Michael but a character based on (and requiring) Victoria. Victoria 2's eyebrows were separately transmapped objects.\n\nVictoria 3, released in 2002 yet again raised the poly count and resculpted the face, leaving out transmapped eyebrows in favor of painting them directly onto the face texture, but was no longer backwards compatible to Victoria 2's rigging, textures or clothing. This generation was the first to have both the male and female figures be modeled from the same base \"unimesh,\" allowing Victoria 3 and Michael 3 to easily share poses, texture maps and even morph targets. This was also the first figure for which neither Victoria nor Michael had genitals modeled on the body mesh; V3 provided a fine enough poly count in the region to make it possible for third party morphers to design their own while Michael 3's genitals were a parented prop (which continued with the next generation). This figure has an even higher density of polygons (74,510 polygons compare to 28,989 polys for V1 and V2 ) with improvement in rigging over the previous millennium figures. Victoria 3 was the first of DAZ 3D figures to support morph injection poses, which allowed users to apply morphs from the Pose Library rather than manually selecting body parts and loading morph target files. Victoria 3's eyebrows were not transmapped but textures painted onto the face texture.\n\nThe fourth and final standalone generation of Victoria was released in late 2006, which included a new body shape and added several technical improvements to the figure. As with V3, V4's proportions and rigging were not designed to be backwards compatible. The mesh for Victoria 4.0 was developed using Luxology's modo. Sporting an almost mannequin-like appearance, Victoria 4's head nevertheless contains enough polys to make photorealistic, real-person likenesses possible.\n\nVersion 4.1 was released in April 2007 and included a separate rigged eyeBrow prop and improved on the capability to load multiple sets of morphs into the character without causing conflicts. \n\nVersion 4.2 was released in February 2008 which added a set of built-in male morphs and further technical improvements.\n\nAs Stephanie and Aiko had been characters for Victoria's base figure, Victoria 5 was released near the end of 2011 as a character for DAZ Studio's Genesis Female rather than a standalone figure and was not initially compatible with Poser. Victoria 5 is rigged to support V4's texture maps and DAZ Studio's cloth fitting allows her to wear Victoria 4's clothing, but due to having a different mesh she cannot use the same morph targets as Victoria 4. During the beta testing phases of DAZ Studio 4, there was released information briefly regarding the preparation of Victoria 5 as a part of Genesis. As a Genesis figure, all clothing made for one Genesis-derived character auto-fits all others (a feature of DAZ Studio). Finally, the mesh of Victoria 5 was updated and made smoother, with the use of SubD, weight maps and other DAZ Studio native optimizations. Software like Poser can now load her using some plugins or DAZ Studio .cr2 export, but Daz Studio-specific features will not be supported in Poser.\n\nIn June 2013, in sync with the release of the Genesis 2 figures, DAZ released Victoria 6 as a Genesis 2 character.\n\nIn June 2015, DAZ released Victoria 7 as a Genesis 3 character.\n\nIn June 2017, DAZ released Victoria 8 as a Genesis 8 character. If used with the Daz Studio \"Autofit\" tool, Victoria 8 can use add-on wardrobe resources created for Victoria 5, Victoria 6, and Victoria 7.\n"}
{"id": "28684910", "url": "https://en.wikipedia.org/wiki?curid=28684910", "title": "Walkers are Welcome", "text": "Walkers are Welcome\n\nThe Walkers are Welcome scheme is a community-led initiative operating in England, Scotland and Wales. The scheme promotes towns and communities as 'walker-friendly', based on a number of criteria, aiming to benefit local economies by attracting tourism.\n\nThe scheme was first proposed in Summer 2006 by a local walkers group in the Yorkshire town of Hebden Bridge and formerly launched on 18 February 2007. Since then, it has expanded rapidly and more than ninety towns and villages have been granted Walkers are Welcome status.\n\nThe Walkers are Welcome Towns Network claims that the scheme helps strengthen a town’s reputation as a destination for visitors, and also brings benefits to the local economy, encouraging the towns to view walkers as \"economic assets\".\n\nThe Walkers are Welcome logo is widely used in towns with the status. Welsh, Scottish Gaelic and Cornish versions of the logo are in use in Wales, Scotland and Cornwall.\n\nThe scheme is run by the towns and villages themselves, who are all members of the Walkers are Welcome Towns Network. The Network operates through a committee that meets every six to eight weeks as well as an annual conference.\n\nThe scheme is different from most official accreditation schemes in being managed through peer review, rather than through a top-down agency. Direct community engagement is considered essential to the scheme’s success.\n\nThe current patron is the Ramblers Association vice-president Kate Ashbrook.\n\nThe main aim of the campaign is to get towns to be more supportive of hikers. Towns and villages wishing to receive Walkers are Welcome status are required to meet six criteria. These are:\n"}
{"id": "2497643", "url": "https://en.wikipedia.org/wiki?curid=2497643", "title": "Water resource management", "text": "Water resource management\n\nWater resource management is the activity of planning, developing, distributing and managing the optimum use of water resources. It is a sub-set of water cycle management. Ideally, water resource management planning has regard to all the competing demands for water and seeks to allocate water on an equitable basis to satisfy all uses and demands. As with other resource management, this is rarely possible in practice.\n\nWater is an essential resource for all life on the planet. Of the water resources on Earth only three percent of it is fresh and two-thirds of the freshwater is locked up in ice caps and glaciers. Of the remaining one percent, a fifth is in remote, inaccessible areas and much seasonal rainfall in monsoonal deluges and floods cannot easily be used. As time advances, water is becoming scarcer and having access to clean, safe, drinking water is limited among countries. At present only about 0.08 percent of all the world’s fresh water is exploited by mankind in ever increasing demand for sanitation, drinking, manufacturing, leisure and agriculture. Due to the small percentage of water remaining, optimizing the fresh water we have left from natural resources has been a continuous difficulty in several locations worldwide.\n\nMuch effort in water resource management is directed at optimizing the use of water and in minimizing the environmental impact of water use on the natural environment. The observation of water as an integral part of the ecosystem is based on integrated water resource management, where the quantity and quality of the ecosystem help to determine the nature of the natural resources.\n\nAs a limited resource, water supply sometimes supposes a challenge. This fact is assumed by the project DESAFIO (the acronym for \"Democratisation of Water and Sanitation Governance by Means of Socio-Technical Innovations\"), which has been developed along 30 months and funded by the European Union’s Seventh Framework Programme for research, technological development and demonstration. This project faced a difficult task for developing areas: eliminating structural social inequity in the access to indispensable water and public health services. The DESAFIO engineers worked on a water treatment system run with solar power and filters which provides safe water to a very poor community in the state of Minas Gerais.\n\nSuccessful management of any resources requires accurate knowledge of the resource available, the uses to which it may be put, the competing demands for the resource, measures to and processes to evaluate the significance and worth of competing demands and mechanisms to translate policy decisions into actions on the ground.\nFor water as a resource, this is particularly difficult since sources of water can cross many national boundaries and the uses of water include many that are difficult to assign financial value to and may also be difficult to manage in conventional terms. Examples include rare species or ecosystems or the very long term value of ancient groundwater reserves.\n\nAgriculture is the largest user of the world's freshwater resources, consuming 70 percent. As the world population rises it consumes more food (currently exceeding 6%, it is expected to reach 9% by 2050), the industries and urban developments expand, and the emerging biofuel crops trade also demands a share of freshwater resources, water scarcity is becoming an important issue. An assessment of water resource management in agriculture was conducted in 2007 by the International Water Management Institute in Sri Lanka to see if the world had sufficient water to provide food for its growing population or not . It assessed the current availability of water for agriculture on a global scale and mapped out locations suffering from water scarcity. It found that a fifth of the world's people, more than 1.2 billion, live in areas of physical water scarcity, where there is not enough water to meet all their demands. A further 1.6 billion people live in areas experiencing economic water scarcity, where the lack of investment in water or insufficient human capacity make it impossible for authorities to satisfy the demand for water.\n\nThe report found that it would be possible to produce the food required in future, but that continuation of today's food production and environmental trends would lead to crises in many parts of the world. Regarding food production, the World Bank targets agricultural food production and water resource management as an increasingly global issue that is fostering an important and growing debate. The authors of the book \"Out of Water: From abundance to Scarcity and How to Solve the World's Water Problems\", which laid down a six-point plan for solving the world's water problems. These are: 1) Improve data related to water; 2) Treasure the environment; 3) Reform water governance; 4) Revitalize agricultural water use; 5) Manage urban and industrial demand; and 6) Empower the poor and women in water resource management. To avoid a global water crisis, farmers will have to strive to increase productivity to meet growing demands for food, while industry and cities find ways to use water more efficiently.\n\nAs the carrying capacity of the Earth increases greatly due to technological advances, urbanization in modern times occurs because of economic opportunity. This rapid urbanization happens worldwide but mostly in new rising economies and developing countries. Cities in Africa and Asia are growing fastest with 28 out of 39 megacities (a city or urban area with more than 10 million inhabitants) worldwide in these developing nations. The number of megacities will continue to rise reaching approximately 50 in 2025. With developing economies water scarcity is a very common and very prevalent issue. Global freshwater resources dwindle in the eastern hemisphere either than at the poles, and with the majority of urban development millions live with insufficient fresh water. This is caused by polluted freshwater resources, overexploited groundwater resources, insufficient harvesting capacities in the surrounding rural areas, poorly constructed and maintained water supply systems, high amount of informal water use and insufficient technical and water management capacities.\n\nIn the areas surrounding urban centres, agriculture must compete with industry and municipal users for safe water supplies, while traditional water sources are becoming polluted with urban runoff. As cities offer the best opportunities for selling produce, farmers often have no alternative to using polluted water to irrigate their crops. Depending on how developed a city’s wastewater treatment is, there can be significant health hazards related to the use of this water. Wastewater from cities can contain a mixture of pollutants. There is usually wastewater from kitchens and toilets along with rainwater runoff. This means that the water usually contains excessive levels of nutrients and salts, as well as a wide range of pathogens. Heavy metals may also be present, along with traces of antibiotics and endocrine disruptors, such as oestrogens.\n\nDeveloping world countries tend to have the lowest levels of wastewater treatment. Often, the water that farmers use for irrigating crops is contaminated with pathogens from sewage. The pathogens of most concern are bacteria, viruses and parasitic worms, which directly affect farmers’ health and indirectly affect consumers if they eat the contaminated crops. Common illnesses include diarrhoea, which kills 1.1 million people annually and is the second most common cause of infant deaths. Many cholera outbreaks are also related to the reuse of poorly treated wastewater. Actions that reduce or remove contamination, therefore, have the potential to save a large number of lives and improve livelihoods. Scientists have been working to find ways to reduce contamination of food using a method called the 'multiple-barrier approach'.\n\nThis involves analysing the food production process from growing crops to selling them in markets and eating them, then considering where it might be possible to create a barrier against contamination. Barriers include: introducing safer irrigation practices; promoting on-farm wastewater treatment; taking actions that cause pathogens to die off; and effectively washing crops after harvest in markets and restaurants.\n\nUrban Decision Support System (UDSS) – is a wireless device with a mobile app that uses sensors attached to water appliances in urban residences to collect data about water usage and is an example of data-driven urban water management. The system was developed with a European Commission investment of 2.46 Million Euros to improve the water consumption behaviour of households. Information about every mechanism – dishwashers, showers, washing machines, taps – is wirelessly recorded and sent to the UDSS App on the user’s mobile device. The UDSS is then able to analyse and show homeowners which of their appliances are using the most water, and which behaviour or habits of the households are not encouraged in order to reduce the water usage, rather than simply giving a total usage figure for the whole property, which will allow people to manage their consumption more economically. The UDSS is based on university research in the field of Management Science, at Loughborough University School of Business and Economics, particularly Decision Support System in household water benchmarking, led by Dr Lili Yang, (Reader)\n\nOne of the biggest concerns for our water-based resources in the future is the sustainability of the current and even future water resource allocation. As water becomes more scarce, the importance of how it is managed grows vastly. Finding a balance between what is needed by humans and what is needed in the environment is an important step in the sustainability of water resources. Attempts to create sustainable freshwater systems have been seen on a national level in countries such as Australia, and such commitment to the environment could set a model for the rest of the world.\n\nThe field of water resources management will have to continue to adapt to the current and future issues facing the allocation of water. With the growing uncertainties of global climate change and the long term impacts of management actions,the decision-making will be even more difficult. It is likely that ongoing climate change will lead to situations that have not been encountered. As a result, alternative management strategies are sought for in order to avoid setbacks in the allocation of water resources.\n\n"}
{"id": "54005931", "url": "https://en.wikipedia.org/wiki?curid=54005931", "title": "World Conference on Women, 1980", "text": "World Conference on Women, 1980\n\nThe World Conference on Women, 1980 or the Second World Conference on Women took place between 14 and 30 July 1980 in Copenhagen, Denmark as the mid-decade assessment of progress and failure in implementing the goals established by the World Plan of Action at the 1975 inaugural conference on women. The most significant event to come out of the conference was that the formal signing of the Convention on the Elimination of All Forms of Discrimination Against Women took place during the opening ceremony of the conference. Marred by conflict and politicization of international and national events which had little to do with women's issues, the conference was viewed by some participants as a failure. They were able to secure passage of a modified World Programme of Action to expand on previous targets to improve women's status, and established a follow-up conference for the end of the decade.\n\nThe 1980 Conference held from 14 and 30 July in Copenhagen, Denmark was the direct result of the First World Conference on Women, which had been held in Mexico City in 1975, establishing the World Plan of Action and Declaration of Mexico on the Equality of Women and Their Contribution to Development and Peace. These documents took the United Nations themes—Development, Equality, and Peace—of their path for women and created guidelines for nations to reach long-term objectives to improve the lives of women. When they were adopted, the UN established 1975 to 1985 as the Decade for Women and put in motion a plan for subsequent conferences to evaluate progress being made. The format of the conference was the same, with the official session made up of delegates representing their governments and the Tribune, representing NGOs.\n\nAs with the previous conference, the Copenhagen conference was beset by the geopolitical divides of the Cold War and whether economics, racism, or sexism was the more important factor in the subordination of women. Initially planned to occur in Tehran, the Iranian Revolution of 1979 and the Iran hostage crisis, escalated the political backdrop as did the continuing tensions of conflict in the Middle East. Palestinian women, refugees, and Apartheid became topics that were added to the agenda and ensured that the event would be politicized by the various participants, rather than remaining focused on women's issues. To that end, the United States Congress issued instructions to its delegates that they would not approve any resolution which attempted to make what should be an apolitical conference into an indictment of government policy or any resolution which mentioned the word \"Zionism\". Saudi Arabia and South Africa boycotted the convention altogether. Having to hastily relocate the conference to Denmark, also impacted the accommodations available, in that there was no space large enough to accommodate the entire Tribune, which meant that rather than the entire group participating in exchange to create unity, the group was splintered into small venues.\n\nThe Conference was the mid-point review of the decade and the conference president was Lise Østergaard, Cultural Minister of Denmark. The Secretary-General of the conference was Lucille Mair, a Jamaican academic and single mother, whose primary focus was on the development theme of the triad. Discussion on the New International Economic Order subverted the discussion from being about what women needed to what the various governments needed from women or for women to reach their national goals. At one point, it was suggested that if Westernized nations would provide more funding for economic development, discrimination against women would vanish. Partisan political issues, such as insertion of the socialist economic system into the section dealing with the historical perspective on women, the repeated interruption of Israeli delegates with Muslim drummers and singers, and a storming of the conference by women protesting the Bolivian coup d'état were just some of the manifestations of the divides.\n\nOne hundred forty-five states with around 1500 delegates participated in the official session, including delegates like: of the USSR; Shirley Field-Ridley of Guyana; Ana Sixta González de Cuadros of Colombia; Helga Hörz of East Germany; of Senegal; Sheila Kaul of India; Ifigenia Martínez of Mexico; of Austria; Inonge Mbikusita-Lewanika of Zambia; Elizabeth Anne Reid of Australia; Ginko Sato of Japan; Umayya Toukan of Jordan; Sarah Weddington, who headed the US delegation, among many others.\n\nAfter opening remarks by Kurt Waldheim, Secretary-General of the United Nations, Queen Margrethe II of Denmark welcomed participants and expressed her hope that the conference would prove productive. Anker Jørgensen, Danish Prime Minister spoke briefly, followed by opening remarks by Lise Østergaard, followed by the general discussion. Debates were strongly affected by post-colonial and socialist claims of women's advancement in centrally planned economies in which the state had an obligation to prevent and an accountability for discrimination against women. Having established mechanisms in the previous conference to gather data on the status of women, review of the statistical data showed that women's security had dwindled over the preceding five-year period. Among the documentation were statistics showing that while women put in two-thirds of all working hours, they received only one-tenth of the income, owning one-hundredth of its assets. Economic stagnation and industrialization in the period had led to significant increases in unemployment and benefits for women. Jobs which were available confined women to insecure, low-paid, and sex-stereotyped jobs and as much of their labor was toward unpaid production, it was still invisible in compiled economic reports. Decreased earnings had elevated health concerns. While literacy rates for middle class women increased, overall illiteracy among women increased. One of the most contentious issues discussed was the situation of households headed by women. Many officials denied that there could be such a thing, as legally in their countries women were not allowed to be the head of a household. On the other hand, one of the most memorable moments was when the delegates signed the Convention on the Elimination of All Forms of Discrimination Against Women (CEDAW) on 17 July.\n\nThe first committee, under the chair Maïmouna Kane, with vice-chairs, Rafidah Aziz of Malaysia, Maria Groza of Romania, and Leónidas Páez de Virgili of Paraguay, with Rapporteur of Belgium, discussed the effects of Apartheid and the Israeli-occupied territories on women; the progress and obstacles in attaining the objectives of the World Plan of Action; and the proposal for the World Programme of Action for the second-half of the Decade for Women. Of major concern was labor insecurity, caused either by the introduction of technologies which replaced women laborers or by the informal nature of many jobs open to women in developing countries. The committee discussed that more effort should be made to retrain laborers when their positions were eliminated by technological advances and that legal protections should be enacted. Also of grave concern was education of women to not only eliminate illiteracy but to make them aware of social and political processes and how they could be part of decision-making mechanisms. Apartheid and racism were condemned by the committee, as was the Zionist policy of Israel which was linked to racism. The discussion on the Palestinian right to self-determination was endorsed, though it was noted that when the Palestinian people as a whole were denied basic human rights, discussing the rights of only women was futile. Various draft resolutions, including resolutions to improve education and training, address women with disabilities, provide support for women migrants and refugees, provide economic security for elderly women, and to address violence against women. With modifications, the committee recommended approval of the Programme and accepted CEDAW with few reservations.\n\nThe second committee, under the chair Sheila Kaul, with vice-chairs, Nermin Abadan-Unat of Turkey, María de Lourdes Castro e Silva de Vincenzi of Brazil, and Chavdar Kyuranov of Bulgaria, with Rapporteur Ali Benbouchta of Morocco, discussed an agenda identical to the first committee's focal points. Apartheid was rejected and the committee recommended that with its eradication, women in South Africa and bordering refugee states should be compensated with the means to reconstruct their societies in ways that created avenues for women's participation. With regard to the review of the Programme, the committee noted that without change in socio-economic systems, equality for women remained elusive. It was noted that globalization led to an increasing need to pursue paths for disarmament, peace and international cooperation. It was also noted that regional systems needed to be fully integrated to allow women's participation but additionally new programs organized specifically for women should be explored. The situation of refugee women, their vulnerabilities to exploitation and violence and the need to protect their human rights. On the question of Palestinian women, the committee recognized that material assistance would do little to stop insecurity unless Israel ended its colonization, returned land to its owners and worked toward a durable peace. The committee examined several draft resolutions regarding peace initiatives, refugees, water insecurity, and adding women to census figures, as well as draft resolutions on health and welfare, protecting families from defaulted support obligations, drug trafficking and forced disappearance, and integrating women into the UN system and programs. With modifications, the committee recommended approval of the Programme.\n\nThe planning of the 1980 Tribune, or Forum as it was called in Copenhagen, was led by Edith Ballantyne, the executive secretary of the Women's International League for Peace and Freedom (WILPF) and president of the United Nations Conference of Non-governmental Organizations (CONGO). Elizabeth Palmer, a representative of the YWCA chaired the actual Forum, which was hosted at the Copenhagen University Amager Campus. The success of developing transnational networks of women was evident in the expansion of attendees at the NGO Tribune from 6000 participants in Mexico City to around 8000 in Copenhagen. Many complained of the inadequacy of the forum facilities, including the fact that child care had not been considered. The forum was split into small sessions consisting of around 200 meetings per day. Because of the lack of translators, and the fact that conferences were labeled as of concern to developed or developing nations, in-depth discussion was difficult and often barely touched the surface of issues. Peggy Antrobus and Charlotte Bunch, with sponsorship from the University of the West Indies' Women and Development Department and the Asian Pacific Centre for Women and Development (APCWD), recently relocated from Tehran to Bangkok, organized an orientation video \"Feminist Strategies for the Decade\" which was shown daily. Irene Tinker\n\nThe politicization of the official conference also influenced the NGO Tribune, resulting in tensions and displays of nationalism, such as Iranian women holding a news conference to celebrate their revolution by calling for the use of the hijab as a protest against colonialism and Ukrainian women protesting for independence. When Nawal El Saadawi of Egypt presented a paper on female circumcision, western feminists were advised that the issue was a developing world problem and not their concern. Lesbians attendees hosted five workshops, which were well attended and less controversial than at the 1975 conference. Some of the prominent women attendees were Shulamit Aloni of Israel; Marie Assaad of Egypt; Charlotte Bunch, US lesbian activist; Phyllis Chesler a US expert on refugees; Betty Friedan, founder of the National Organization for Women (NOW); Natalia Malakhovskaia, Soviet exile; Marie-Angélique Savané of Senegal. As they had in Mexico City, the members of the Forum continued the tradition of presenting their additions to the Programme of Action at the official session. A group of women led by Domitila Barrios de Chungara were met by police and barred from entering the plenary meeting until Lucille Mair met with them and allowed the recommendations to be presented.\n\nThe most significant outcome of the conference was the official signing of CEDAW by the delegates at the opening ceremony. The conference adopted the official World Programme of Action with a vote of with ninety-four favorable votes, twenty-two abstentions, and four opposed—Australia, Canada, Israel and the United States. It included sections to create women's bureaus or agencies, defined the roles of NGO and grassroots organizations, and established target issues countries were to monitor. Those issues included child care, households headed by women, migrants, rural women, unemployment, and youth. The Programme also included a section regarding water insecurity, but the most significant changes to the previous Plan of Action were sections devoted to ensuring equal access to education, employment opportunities, and adequate health care. The Conference established the Third World Conference on Women for the end of the decade to be hosted in Nairobi, Kenya in 1985 with a back up location in Tokyo.\n\nThe conference was seen by those who rejected the Programme as having been a failed process. Rather than a serious discussion of the inequalities between men and women, the conference had limited discussion to politicizing international events, ideological principals and controversies, which obscured the real needs of women to participate in decision-making and economic development, and benefit equally in family matters, health, education and employment. Multiple countries which had abstained from voting expressed disappointment that the process, rather than dealing with women's issues, had duplicated work better suited for the General Assembly.\n\nOverall, the conference and forum conference were marred by conflict and politicization of international and national events which had little to do with women's issues. The official agenda was obscured by nationalist causes pitting the developing countries of the South and the developed countries of the North against each other. The question for continued participation in the conferences left many asking if the focus could be shifted away from political issues and return to the problems related to women: aging; credit for economic development; double duties between work and family; fertility versus infertility; shortage of heat and inadequate water, support systems and lack of them; women’s health; and violence against women.\n\n"}
{"id": "50742161", "url": "https://en.wikipedia.org/wiki?curid=50742161", "title": "Zimbabwe at the 2016 Summer Paralympics", "text": "Zimbabwe at the 2016 Summer Paralympics\n\nZimbabwe sent six athletes across two different sports to the 2016 Summer Paralympics in Rio de Janeiro, Brazil, from 7 September to 18 September 2016.\n\nEvery participant at the Paralympics has their disability grouped into one of five disability categories; amputation, the condition may be congenital or sustained through injury or illness; cerebral palsy; wheelchair athletes, there is often overlap between this and other categories; visual impairment, including blindness; Les autres, any physical disability that does not fall strictly under one of the other categories, for example dwarfism or multiple sclerosis. Each Paralympic sport then has its own classifications, dependent upon the specific physical demands of competition. Events are given a code, made of numbers and letters, describing the type of event and classification of the athletes competing. Some sports, such as athletics, divide athletes by both the category and severity of their disabilities, other sports, for example swimming, group competitors from different categories together, the only separation being based on the severity of the disability.\n\nFunding issues proved a major challenge for Zimbabwe's participation at the 2016 Summer Paralympics. This was in part a result of only 220,000 of the 2.5 million tickets to watch the Games in Rio being sold a month ahead of the Games. The NPC had been promised US$8,000 to cover the team's travel to Rio. Less than a month prior to the Games, they were informed they would get only US$6,000 and this money would not be delivered until after the conclusion of the Games.\n\nZimbabwe finished the 2016 Games ranked fourth all time for total medals won by African countries, with 23 gold, 26 silver and 20 bronze medals for a total of 69 medals. They were ahead of fifth ranked Algeria who had 19 gold, 13 silver and 25 bronze medals for 57 all time. They were behind third ranked Tunisia who had 74 all time, of which 32 were gold, 28 silver and 14 bronze.\n\nRowing made a commitment to developing the sport in Africa, with three countries getting four totals bearths to the Rio Games: Kenya, Zimbabwe and South Africa. Only the South African boat in the LTAMix4+ made it through to the finals.\n\nZimbabwe was granted a Bipartite Commission invitation for a team of four rowers plus a coxswain, to compete in the LTA4+, mixed coxed fours event.\n\nQualification Legend: FA=Final A (medal); FB=Final B (non-medal); R=Repechage\n\n"}
