{"id": "9307518", "url": "https://en.wikipedia.org/wiki?curid=9307518", "title": "An Bord Altranais", "text": "An Bord Altranais\n\nAn Bord Altranais (, meaning \"Irish Nursing Board\") is the statutory body that regulates the nursing and midwifery profession in Ireland. It provides for the registration, control and education of nurses. It holds a register of nurses and midwives, who having undergone the appropriate education, may practice as such. It can also remove nurses and midwives from the register if they are proven to be unfit to practice.\n\n"}
{"id": "56331424", "url": "https://en.wikipedia.org/wiki?curid=56331424", "title": "Anne McAllister (speech therapist)", "text": "Anne McAllister (speech therapist)\n\nAnne Hutchison McAllister (29 November 1892 – 5 April 1983) was a leading Scottish speech therapist and teacher.\n\nMcAllister was born in Biggar in Lanarkshire in 1892. Her father Robert Dempster McAllister was married to Anne Huchison McAllister. She took a first and then masters degree (in 1917) at Glasgow University.\n\nShe became a Lecturer in phonetics at Stow College She became a lecturer at Jordanhill College of Education and an experienced speech therapist. In 1924 she obtained a B.Ed from her alma mater. In 1935 she created the \"Glasgow School of Speech Therapy\" and she became its first director. The following year she established speech therapy for children who had undergone surgery to fix Cleft lip and cleft palates at Glasgow's Royal Hospital for Sick Children. This was following an invitation by Matthew White who was the surgeon carrying out the surgery.\n\nIn 1937 she published \"Clinical Studies in Speech Therapy\" which described her study of 21,000 children in Dumbartonshire and there speech problems. The survey also presented some advice and McAllister stressed the importance of a dual approach of psychology and reeducation.\n\nShe became a founding fellow of the College of Speech Therapists (now the Royal College of Speech and Language Therapists) in London in 1945. Allister was awarded an OBE in 1954. In 1964 she stood down from being the director of the Glasgow School of Speech Therapy.\n\nMcAllister died in Glasgow in 1983.\n"}
{"id": "40766630", "url": "https://en.wikipedia.org/wiki?curid=40766630", "title": "Atrophic vaginitis", "text": "Atrophic vaginitis\n\nAtrophic vaginitis is inflammation of the vagina as a result of tissue thinning due to not enough estrogen. Symptoms may include pain with sex, vaginal itchiness or dryness, and an urge to urinate or burning with urination. It generally does not resolve without ongoing treatment. Complications may include urinary tract infections. \nThe lack of estrogen typically occurs following menopause. Other causes may include when breastfeeding or as a result of specific medications. Risk factors include smoking. Diagnosis is typically based on symptoms.\nTreatment is generally with estrogen cream applied to the vagina. Other measures that may help include vaginal lubricants. It is recommended that soaps and other irritants are avoided. About half of postmenopausal women are affected. Many however are not being treated. Women often report reduced enjoyment in sex as well as life generally.\n\nAfter menopause the vaginal epithelium changes and becomes a few layers thick. Many of the signs and symptoms that accompany menopause occur in atrophic vaginitis. Genitourinary symptoms include \n\nSince women can have signs and symptoms that could be attributed to other causes, diagnosis is based upon the symptoms that cannot be better accounted for by another diagnosis. Lab tests usually do not provide information that will aid in diagnosing. A visual exam is useful. The observations of the following may indicate lower estrogen levels: little pubic hair, loss of the labial fat pad, thinning and resorption of the labia minora, and the narrowing of the vaginal opening. An internal exam will reveal the presence of low vaginal muscle tone, the lining of the vagina appears smooth, shiny, pale with loss of folds. The cervical fornices may have disappeared and the cervix can appear flush with the top of the vagina. Inflammation is apparent when the vaginal lining bleeds easily and appears swollen. The vaginal pH will be measured at 4.5 and higher.\n\nSymptoms of GSM will unlikely be resolved without treatment. Women may have many or a few symptoms so treatment is provided that best suits each woman. If other health problems are also present, these can be taken into account when determining the best course of treatment. For those who have symptoms related to sexual activities, a lubricant may be sufficient. If both urinary and genital symptoms exist, local, low-dose estrogen therapy can be effective. Those women who are survivors of hormone-sensitive cancer may need to be treated more cautiously. Some women can have symptoms that are widespread and may be at risk for osteoporosis. Estrogen and adjuvants may be best.\n\nTopical treatment with estrogen is effective when the symptoms are severe and relieves the disruption in pH to restore the microbiome of the vagina.When symptoms include those related to the urinary system, systematic treatment can be used. Recommendations for the use of the lowest effective dose for the shortest duration help to prevent adverse endometrial effects.\n\nSome treatments have been developed more recently. These include selective estrogen receptor modulators, vaginal dehydroepiandrosterone, and laser therapy. Other treatments are available without a prescription such as vaginal lubricants and moisturizers. Vaginal dilators may be helpful. Since GSM may also cause urinary problems related to pelvic floor dysfunction, a woman may benefit from pelvic floor strengthening exercises. Women and their partners have reported that estrogen therapy resulted in less painful sex, more satisfaction with sex, and an improvement in their sex life.\n\nUp to 50% of postmenopausal women have at least some degree of vaginal atrophy. It is likely to be underdiagnosed and undertreated.\n\n\"Vulvovaginal atrophy,\" and \"atrophic vaginitis\" have been the preferred terms for this condition and cluster of symptoms until recently. These terms are now regarded as inaccurate in describing changes to the entire genitourinary system occurring after menopause. The term \"atrophic vaginitis\" suggests that the vagina is inflamed or infected. Though this may be true, inflammation and infection are not the major components of postmenopausal changes to the vagina. The former terms do not describe the negative effects on the lower urinary tract which can be the most troubling symptoms of menopause for women. Genitourinary syndrome of menopause (GSM) was determined to be more accurate than \"vulvovaginal atrophy\" by two professional societies. The term atrophic vaginitis does not reflect the related changes of the labia, clitoris, vestibule, urethra and bladder.\n\nThe FDA has approved the use of lasers in the treatment of many disorders. The treatment of GSM is not specifically mentioned in the list of disorders by the United States' Food and Drug Administration but laser treatments have had success. Larger studies are still needed. The laser treatment works by resurfacing the vaginal epithelium and activating growth factors that increases blood flow, deposition of collage and the thickness of the vaginal lining. Women treated with laser therapy reported diminished symptoms of dryness, burning, itching, pain during sexual intercourse, and painful urination. Few adverse effects were noted.\n"}
{"id": "22303140", "url": "https://en.wikipedia.org/wiki?curid=22303140", "title": "Bikur Cholim Hospital", "text": "Bikur Cholim Hospital\n\nBikur Cholim Hospital () is a 200-bed general hospital in Jerusalem, Israel established in the 19th century. It is the oldest hospital in the country.\n\nBikur Cholim is known for its obstetrics and cardiac departments. The hospital also operates a modern neonatal intensive care unit, a pediatrics department, and bariatric and plastic surgery units. Bikur Holim treats some 60,000 patients annually. With 700 administrators, doctors, nurses, technicians and cleaners, it is one of Jerusalem's largest downtown employers. One-third of the doctors are Israeli Arabs, many of whom choose Bikur Holim for their residencies.\n\nAs of December 2012 the hospital has been taken over by Shaare Zedek Medical Center and will continue to function as a branch of Shaare Zedek.\n\nBikur Cholim opened in a rented building in the Old City around 1826. In 1843, it had three rooms for patients. It was run by the Bikur Cholim society. After the Mission established a medical facility in the Old City, hoping to attract Jews, the society intensified its activity. In 1854, a building was purchased which soon grew overcrowded. In 1864, another complex of buildings was acquired incorporating treatment rooms, a pharmacy, a hospice for the terminally ill and administrative offices. The Ashkenazi Perushim Hospital, as it was known, became the favorite charity of the British Jewish philanthropist Moses Montefiore, who described the facility in his diary in 1875. The general ward consisted of two rooms, each with eight beds. One room was reserved for men, and the other for women. In 1893, the hospital cared for 781 patients and treated 12,347 people in its out-patient clinics.\n\nBy 1907, hospitalizations exceeded 1,000 per annum. A decision was reached to build a new hospital outside the walls of the Old City. The cornerstone of the new building was laid in 1912, but construction work was delayed by the outbreak of World War I. \n\nThe building on Chancellor Avenue (now Strauss Street), just off Jaffa Road, was completed in 1925 and opened its doors to all residents of Jerusalem, Jews and non-Jews. The hospital in the Old City continued to treat the chronically ill until 1947.\n\nMany of the wounded from the 1929 Palestine riots and 1936–39 Arab revolt in Palestine were brought to Bikur Holim. Jewish underground fighters were hospitalized under fictitious names to keep the British mandatory police from finding them. During the War of Independence in 1948, the hospital came under artillery fire from Jordanian guns. Hadassah Hospital on Mount Scopus was evacuated, and many patients were transferred to Bikur Holim.\n\nIn 2007, the Russian-Israeli tycoon Arkadi Gaydamak saved the hospital from bankruptcy, taking it over from receivership. In 2010, Gaydamak stopped funding the hospital and returned to Russia.\n\nThe location of the hospital in downtown Jerusalem has proven critical in times of emergency. With Jerusalem's other hospitals are located far from the center, Bikur Holim was able to save the lives of many victims of terrorist attacks. According to the head of the emergency ward, “If somebody’s life is in danger and they need immediate help, realizing that the brain only has six minutes before it’s too late, then 20 minutes is too late. It’s coming dead on arrival.\" \n\nThe hospital's medical director, Raphael Pollack, says its financial difficulties are due to the system of discounts exacted from the hospitals by health maintenance organizations (HMOs) and debt repayment.\n\nSituated near the religious neighborhoods of Geula and Mea Shearim, Bikur Holim admits a very high percentage of Haredi Jews, and tries to cater to their needs. Shabbat is strictly observed. Non-Jewish employees record medical information and answer telephones on the Sabbath. Food is warmed in ovens operated by a timer, in keeping with Orthodox religious rulings.\n\nThe current building was designed by architect Zvi Joseph Barsky in the neo-classical style with modernist elements. Zeev Raban of Bezalel designed the bronze doors. When Arcady Gaydamak bought the hospital for $35 million in 2007, he commissioned plans, designed by the Architect Moti Bodek to build two hospitalization towers alongside the existing historical structure. \n\n\n\n"}
{"id": "13778615", "url": "https://en.wikipedia.org/wiki?curid=13778615", "title": "Capital punishment in Croatia", "text": "Capital punishment in Croatia\n\nCapital punishment in Croatia existed until 1990 when it was constitutionally abolished. The last execution had taken place under Yugoslavia in 1987.\n\nHanging was replaced by firing squads in 1959 in Yugoslavia. Capital punishment was abolished in SR Croatia in 1974, but remained legal for federal crimes within SFR Yugoslavia.\n\nIn the time of Yugoslavia, several court cases resulted in capital punishment:\n\n\nThe last civilian execution in Croatia was done in 1987, when a former truck driver Dušan Kosić was executed for the 1 March 1983 murder of his coworker Čedomir Matijević, his wife Slavica and their daughters Dragana (aged 2) and Snježana (8 months old). In the course of the investigation, Kosić provided the investigative judge and police officers a detailed admission to the murders, but retracted it during his later trial. The District Court in Karlovac found him guilty and sentenced him to death on 4 October 1983. Kosić was executed by firing squad on January 29, 1987.\n\nCapital punishment was outlawed by article 21 of the 1990 Constitution of Croatia.\n\nCroatia is a signatory of Protocol 13 of the European Convention on Human Rights that abolishes the death penalty in all circumstances. The convention was signed on 3 July 2002, ratified on 3 February 2003, and came into force on 1 July 2003.\n\nSource: SPSK Database\n"}
{"id": "40729970", "url": "https://en.wikipedia.org/wiki?curid=40729970", "title": "Capital punishment in Monaco", "text": "Capital punishment in Monaco\n\nCapital punishment in Monaco was abolished in 1962.\n\nThe Constitution of the Principality of Monaco of December 17, 1962 states:\n\nThe last execution took place in 1847.\n"}
{"id": "22920514", "url": "https://en.wikipedia.org/wiki?curid=22920514", "title": "Carolinas College of Health Sciences", "text": "Carolinas College of Health Sciences\n\nCarolinas College of Health Sciences is a public 2-year college located in Charlotte, North Carolina. Established in 1990, the college is a subsidiary of Carolinas Medical Center and owned by Atrium Health. The school offers programs in nursing, radiologic technology, radiation therapy, surgical technology, anesthesiology technician, medical laboratory science, and various continuing education programs and workshops. In addition to its traditional educational programs, the college is home to the region's only dual-accredited simulation center.\n\nCarolinas College of Health Sciences offers educational programs leading to entry-level employment in health care. These include nursing, radiologic technology, radiation therapy, surgical technology, and medical laboratory sciences. The continuing education department offers non-credit education in nurse aide, healing touch, phlebotomy, CPR of various levels, cancer massage, IV therapy, and other focused areas of health care.\n\nIt was ranked the number three 2-year college in the country by \"Washington Monthly\" in 2010, based on the college's high graduation rate and responses on a national, standardized survey of student engagement. The college is accredited by the Commission on Colleges of the Southern Association of Colleges and Schools to award the Associate of Applied Science degree.\n\n"}
{"id": "7591", "url": "https://en.wikipedia.org/wiki?curid=7591", "title": "Cholera", "text": "Cholera\n\nCholera is an infection of the small intestine by some strains of the bacterium \"Vibrio cholerae\". Symptoms may range from none, to mild, to severe. The classic symptom is large amounts of watery diarrhea that lasts a few days. Vomiting and muscle cramps may also occur. Diarrhea can be so severe that it leads within hours to severe dehydration and electrolyte imbalance. This may result in sunken eyes, cold skin, decreased skin elasticity, and wrinkling of the hands and feet. Dehydration can cause the skin to turn bluish. Symptoms start two hours to five days after exposure.\nCholera is caused by a number of types of \"Vibrio cholerae\", with some types producing more severe disease than others. It is spread mostly by unsafe water and unsafe food that has been contaminated with human feces containing the bacteria. Undercooked seafood is a common source. Humans are the only animal affected. Risk factors for the disease include poor sanitation, not enough clean drinking water, and poverty. There are concerns that rising sea levels will increase rates of disease. Cholera can be diagnosed by a stool test. A rapid dipstick test is available but is not as accurate.\nPrevention methods against cholera include improved sanitation and access to clean water. Cholera vaccines that are given by mouth provide reasonable protection for about six months. They have the added benefit of protecting against another type of diarrhea caused by \"E. coli\". The primary treatment is oral rehydration therapy—the replacement of fluids with slightly sweet and salty solutions. Rice-based solutions are preferred. Zinc supplementation is useful in children. In severe cases, intravenous fluids, such as Ringer's lactate, may be required, and antibiotics may be beneficial. Testing to see which antibiotic the cholera is susceptible to can help guide the choice.\nCholera affects an estimated 3–5 million people worldwide and causes 28,800–130,000 deaths a year. Although it is classified as a pandemic , it is rare in the developed world. Children are mostly affected. Cholera occurs as both outbreaks and chronically in certain areas. Areas with an ongoing risk of disease include Africa and Southeast Asia. The risk of death among those affected is usually less than 5% but may be as high as 50%. No access to treatment results in a higher death rate. Descriptions of cholera are found as early as the 5th century BC in Sanskrit. The study of cholera in England by John Snow between 1849 and 1854 led to significant advances in the field of epidemiology. Seven large outbreaks have occurred over the last 200 years with millions of deaths.\n\nThe primary symptoms of cholera are profuse diarrhea and vomiting of clear fluid. These symptoms usually start suddenly, half a day to five days after ingestion of the bacteria. The diarrhea is frequently described as \"rice water\" in nature and may have a fishy odor. An untreated person with cholera may produce of diarrhea a day. Severe cholera, without treatment, kills about half of affected individuals. If the severe diarrhea is not treated, it can result in life-threatening dehydration and electrolyte imbalances. Estimates of the ratio of asymptomatic to symptomatic infections have ranged from 3 to 100. Cholera has been nicknamed the \"blue death\" because a person's skin may turn bluish-gray from extreme loss of fluids.\n\nFever is rare and should raise suspicion for secondary infection. Patients can be lethargic, and might have sunken eyes, dry mouth, cold clammy skin, or wrinkled hands and feet. Kussmaul breathing, a deep and labored breathing pattern, can occur because of acidosis from stool bicarbonate losses and lactic acidosis associated with poor perfusion. Blood pressure drops due to dehydration, peripheral pulse is rapid and thready, and urine output decreases with time. Muscle cramping and weakness, altered consciousness, seizures, or even coma due to electrolyte imbalances are common, especially in children.\n\nCholera has been found in two animal populations: shellfish and plankton.\n\nTransmission is usually through the fecal-oral route of contaminated food or water caused by poor sanitation. Most cholera cases in developed countries are a result of transmission by food, while in the developing world it is more often water. Food transmission can occur when people harvest seafood such as oysters in waters infected with sewage, as \"Vibrio cholerae\" accumulates in planktonic crustaceans and the oysters eat the zooplankton.\n\nPeople infected with cholera often have diarrhea, and disease transmission may occur if this highly liquid stool, colloquially referred to as \"rice-water\", contaminates water used by others. A single diarrheal event can cause a one-million fold increase in numbers of \"V. cholerae\" in the environment. The source of the contamination is typically other cholera sufferers when their untreated diarrheal discharge is allowed to get into waterways, groundwater or drinking water supplies. Drinking any contaminated water and eating any foods washed in the water, as well as shellfish living in the affected waterway, can cause a person to contract an infection. Cholera is rarely spread directly from person to person.\n\n\"V. cholerae\" also exists outside the human body in natural water sources, either by itself or through interacting with phytoplankton, zooplankton, or biotic and abiotic detritus. Drinking such water can also result in the disease, even without prior contamination through fecal matter. Selective pressures exist however in the aquatic environment that may reduce the virulence of \"V. cholerae\". Specifically, animal models indicate that the transcriptional profile of the pathogen changes as it prepares to enter an aquatic environment. This transcriptional change results in a loss of ability of \"V. cholerae\" to be cultured on standard media, a phenotype referred to as 'viable but non-culturable' (VBNC) or more conservatively 'active but non-culturable' (ABNC). One study indicates that the culturability of \"V. cholerae\" drops 90% within 24 hours of entering the water, and furthermore that this loss in culturability is associated with a loss in virulence.\n\nBoth toxic and non-toxic strains exist. Non-toxic strains can acquire toxicity through a temperate bacteriophage.\n\nAbout 100million bacteria must typically be ingested to cause cholera in a normal healthy adult. This dose, however, is less in those with lowered gastric acidity (for instance those using proton pump inhibitors). Children are also more susceptible, with two- to four-year-olds having the highest rates of infection. Individuals' susceptibility to cholera is also affected by their blood type, with those with type O blood being the most susceptible. Persons with lowered immunity, such as persons with AIDS or malnourished children, are more likely to experience a severe case if they become infected. Any individual, even a healthy adult in middle age, can experience a severe case, and each person's case should be measured by the loss of fluids, preferably in consultation with a professional health care provider.\n\nThe cystic fibrosis genetic mutation known as delta-F508 in humans has been said to maintain a selective heterozygous advantage: heterozygous carriers of the mutation (who are thus not affected by cystic fibrosis) are more resistant to \"V. cholerae\" infections. In this model, the genetic deficiency in the cystic fibrosis transmembrane conductance regulator channel proteins interferes with bacteria binding to the intestinal epithelium, thus reducing the effects of an infection.\n\nWhen consumed, most bacteria do not survive the acidic conditions of the human stomach. The few surviving bacteria conserve their energy and stored nutrients during the passage through the stomach by shutting down protein production. When the surviving bacteria exit the stomach and reach the small intestine, they must propel themselves through the thick mucus that lines the small intestine to reach the intestinal walls where they can attach and thrive.\n\nOnce the cholera bacteria reach the intestinal wall, they no longer need the flagella to move. The bacteria stop producing the protein flagellin to conserve energy and nutrients by changing the mix of proteins which they express in response to the changed chemical surroundings. On reaching the intestinal wall, \"V. cholerae\" start producing the toxic proteins that give the infected person a watery diarrhea. This carries the multiplying new generations of \"V. cholerae\" bacteria out into the drinking water of the next host if proper sanitation measures are not in place.\n\nThe cholera toxin (CTX or CT) is an oligomeric complex made up of six protein subunits: a single copy of the A subunit (part A), and five copies of the B subunit (part B), connected by a disulfide bond. The five B subunits form a five-membered ring that binds to GM1 gangliosides on the surface of the intestinal epithelium cells. The A1 portion of the A subunit is an enzyme that ADP-ribosylates G proteins, while the A2 chain fits into the central pore of the B subunit ring. Upon binding, the complex is taken into the cell via receptor-mediated endocytosis. Once inside the cell, the disulfide bond is reduced, and the A1 subunit is freed to bind with a human partner protein called ADP-ribosylation factor 6 (Arf6). Binding exposes its active site, allowing it to permanently ribosylate the Gs alpha subunit of the heterotrimeric G protein. This results in constitutive cAMP production, which in turn leads to the secretion of water, sodium, potassium, and bicarbonate into the lumen of the small intestine and rapid dehydration. The gene encoding the cholera toxin was introduced into \"V. cholerae\" by horizontal gene transfer. Virulent strains of \"V. cholerae\" carry a variant of a temperate bacteriophage called CTXφ.\n\nMicrobiologists have studied the genetic mechanisms by which the \"V. cholerae\" bacteria turn off the production of some proteins and turn on the production of other proteins as they respond to the series of chemical environments they encounter, passing through the stomach, through the mucous layer of the small intestine, and on to the intestinal wall. Of particular interest have been the genetic mechanisms by which cholera bacteria turn on the protein production of the toxins that interact with host cell mechanisms to pump chloride ions into the small intestine, creating an ionic pressure which prevents sodium ions from entering the cell. The chloride and sodium ions create a salt-water environment in the small intestines, which through osmosis can pull up to six liters of water per day through the intestinal cells, creating the massive amounts of diarrhea. The host can become rapidly dehydrated unless an appropriate mixture of dilute salt water and sugar is taken to replace the blood's water and salts lost in the diarrhea.\n\nBy inserting separate, successive sections of \"V. cholerae\" DNA into the DNA of other bacteria, such as \"E. coli\" that would not naturally produce the protein toxins, researchers have investigated the mechanisms by which \"V. cholerae\" responds to the changing chemical environments of the stomach, mucous layers, and intestinal wall. Researchers have discovered a complex cascade of regulatory proteins controls expression of \"V. cholerae\" virulence determinants. In responding to the chemical environment at the intestinal wall, the \"V. cholerae\" bacteria produce the TcpP/TcpH proteins, which, together with the ToxR/ToxS proteins, activate the expression of the ToxT regulatory protein. ToxT then directly activates expression of virulence genes that produce the toxins, causing diarrhea in the infected person and allowing the bacteria to colonize the intestine. Current research aims at discovering \"the signal that makes the cholera bacteria stop swimming and start to colonize (that is, adhere to the cells of) the small intestine.\"\n\nAmplified fragment length polymorphism fingerprinting of the pandemic isolates of \"V. cholerae\" has revealed variation in the genetic structure. Two clusters have been identified: Cluster I and Cluster II. For the most part, Cluster I consists of strains from the 1960s and 1970s, while Cluster II largely contains strains from the 1980s and 1990s, based on the change in the clone structure. This grouping of strains is best seen in the strains from the African continent.\n\nIn many areas of the world, antibiotic resistance is increasing within cholera bacteria. In Bangladesh, for example, most cases are resistant to tetracycline, trimethoprim-sulfamethoxazole, and erythromycin. Rapid diagnostic assay methods are available for the identification of multi-drug resistant cases. New generation antimicrobials have been discovered which are effective against cholera bacteria in \"in vitro\" studies.\n\nA rapid dipstick test is available to determine the presence of \"V. cholerae\". In those samples that test positive, further testing should be done to determine antibiotic resistance. In epidemic situations, a clinical diagnosis may be made by taking a patient history and doing a brief examination. Treatment is usually started without or before confirmation by laboratory analysis.\n\nStool and swab samples collected in the acute stage of the disease, before antibiotics have been administered, are the most useful specimens for laboratory diagnosis. If an epidemic of cholera is suspected, the most common causative agent is \"V. cholerae\" O1. If \"V. cholerae\" serogroup O1 is not isolated, the laboratory should test for \"V. cholerae\" O139. However, if neither of these organisms is isolated, it is necessary to send stool specimens to a reference laboratory.\n\nInfection with \"V. cholerae\" O139 should be reported and handled in the same manner as that caused by \"V. cholerae\" O1. The associated diarrheal illness should be referred to as cholera and must be reported in the United States.\n\nThe World Health Organization (WHO) recommends focusing on prevention, preparedness, and response to combat the spread of cholera. They also stress the importance of an effective surveillance system. Governments can play a role in all of these areas.\n\nAlthough cholera may be life-threatening, prevention of the disease is normally straightforward if proper sanitation practices are followed. In developed countries, due to nearly universal advanced water treatment and sanitation practices present there, cholera is rare. For example, the last major outbreak of cholera in the United States occurred in 1910–1911. Cholera is mainly a risk in developing countries.\n\nEffective sanitation practices, if instituted and adhered to in time, are usually sufficient to stop an epidemic. There are several points along the cholera transmission path at which its spread may be halted:\n\nHandwashing with soap or ash after using a toilet and before handling food or eating is also recommended for cholera prevention by WHO Africa.\n\nSurveillance and prompt reporting allow for containing cholera epidemics rapidly. Cholera exists as a seasonal disease in many endemic countries, occurring annually mostly during rainy seasons. Surveillance systems can provide early alerts to outbreaks, therefore leading to coordinated response and assist in preparation of preparedness plans. Efficient surveillance systems can also improve the risk assessment for potential cholera outbreaks. Understanding the seasonality and location of outbreaks provides guidance for improving cholera control activities for the most vulnerable. For prevention to be effective, it is important that cases be reported to national health authorities.\n\nA number of safe and effective oral vaccines for cholera are available. The World Health Organization has three prequalified oral cholera vaccines (OCVs): Dukoral, Sanchol, and Euvichol. Dukoral, an orally administered, inactivated whole cell vaccine, has an overall efficacy of about 52% during the first year after being given and 62% in the second year, with minimal side effects. It is available in over 60 countries. However, it is not currently recommended by the Centers for Disease Control and Prevention (CDC) for most people traveling from the United States to endemic countries. The vaccine that the FDA recommends, Vaxchora, is an oral attenuated live vaccine, that is effective as a single dose.\n\nOne injectable vaccine was found to be effective for two to three years. The protective efficacy was 28% lower in children less than 5 years old. However, , it has limited availability. Work is under way to investigate the role of mass vaccination. The World Health Organization (WHO) recommends immunization of high-risk groups, such as children and people with HIV, in countries where this disease is endemic. If people are immunized broadly, herd immunity results, with a decrease in the amount of contamination in the environment.\n\nAn effective and relatively cheap method to prevent the transmission of cholera is the use of a folded \"sari\" (a long cloth garment) to filter drinking water. In Bangladesh this practice was found to decrease rates of cholera by nearly half. It involves folding a \"sari\" four to eight times. Between uses the cloth should be rinsed in clean water and dried in the sun to kill any bacteria on it. A nylon cloth appears to work as well but is not as affordable.\n\nContinued eating speeds the recovery of normal intestinal function. The World Health Organization recommends this generally for cases of diarrhea no matter what the underlying cause. A CDC training manual specifically for cholera states: \"Continue to breastfeed your baby if the baby has watery diarrhea, even when traveling to get treatment. Adults and older children should continue to eat frequently.\"\n\nThe most common error in caring for patients with cholera is to underestimate the speed\nand volume of fluids required. In most cases, cholera can be successfully treated with oral rehydration therapy (ORT), which is highly effective, safe, and simple to administer. Rice-based solutions are preferred to glucose-based ones due to greater efficiency. In severe cases with significant dehydration, intravenous rehydration may be necessary. Ringer's lactate is the preferred solution, often with added potassium. Large volumes and continued replacement until diarrhea has subsided may be needed. Ten percent of a person's body weight in fluid may need to be given in the first two to four hours. This method was first tried on a mass scale during the Bangladesh Liberation War, and was found to have much success. Despite widespread beliefs, fruit juices and commercial fizzy drinks like cola, are not ideal for rehydration of people with serious infections of the intestines, and their excessive sugar content may even harm water uptake.\n\nIf commercially produced oral rehydration solutions are too expensive or difficult to obtain, solutions can be made. One such recipe calls for 1 liter of boiled water, 1/2 teaspoon of salt, 6 teaspoons of sugar, and added mashed banana for potassium and to improve taste.\n\nAs there frequently is initially acidosis, the potassium level may be normal, even though large losses have occurred. As the dehydration is corrected, potassium levels may decrease rapidly, and thus need to be replaced. This may be done by consuming foods high in potassium, like bananas or coconut water.\n\nAntibiotic treatments for one to three days shorten the course of the disease and reduce the severity of the symptoms. Use of antibiotics also reduces fluid requirements. People will recover without them, however, if sufficient hydration is maintained. The World Health Organization only recommends antibiotics in those with severe dehydration.\n\nDoxycycline is typically used first line, although some strains of \"V. cholerae\" have shown resistance. Testing for resistance during an outbreak can help determine appropriate future choices. Other antibiotics proven to be effective include cotrimoxazole, erythromycin, tetracycline, chloramphenicol, and furazolidone. Fluoroquinolones, such as ciprofloxacin, also may be used, but resistance has been reported.\n\nAntibiotics improve outcomes in those who are both severely and not severely dehydrated. Azithromycin and tetracycline may work better than doxycycline or ciprofloxacin.\n\nIn Bangladesh zinc supplementation reduced the duration and severity of diarrhea in children with cholera when given with antibiotics and rehydration therapy as needed. It reduced the length of disease by eight hours and the amount of diarrhea stool by 10%. Supplementation appears to be also effective in both treating and preventing infectious diarrhea due to other causes among children in the developing world.\n\nIf people with cholera are treated quickly and properly, the mortality rate is less than 1%; however, with untreated cholera, the mortality rate rises to 50–60%.\n\nFor certain genetic strains of cholera, such as the one present during the 2010 epidemic in Haiti and the 2004 outbreak in India, death can occur within two hours of becoming ill.\n\nCholera affects an estimated 3–5 million people worldwide, and causes 58,000–130,000 deaths a year . This occurs mainly in the developing world. In the early 1980s, death rates are believed to have been greater than three million a year. It is difficult to calculate exact numbers of cases, as many go unreported due to concerns that an outbreak may have a negative impact on the tourism of a country. Cholera remains both epidemic and endemic in many areas of the world. In October 2016, an outbreak of cholera began in war-ravaged Yemen. WHO called it \"the worst cholera outbreak in the world\".\n\nAlthough much is known about the mechanisms behind the spread of cholera, this has not led to a full understanding of what makes cholera outbreaks happen in some places and not others. Lack of treatment of human feces and lack of treatment of drinking water greatly facilitate its spread, but bodies of water can serve as a reservoir, and seafood shipped long distances can spread the disease. Cholera was not known in the Americas for most of the 20th century, but it reappeared towards the end of that century.\n\nThe word cholera is from \"kholera\" from χολή \"kholē\" \"bile\". Cholera likely has its origins in the Indian subcontinent as evidenced by its prevalence in the region for centuries. Early outbreaks in the Indian subcontinent are believed to have been the result of poor living conditions as well as the presence of pools of still water, both of which provide ideal conditions for cholera to thrive. The disease first spread by trade routes (land and sea) to Russia in 1817, later to the rest of Europe, and from Europe to North America and the rest of the world. Seven cholera pandemics have occurred in the past 200 years, with the seventh pandemic originating in Indonesia in 1961.\n\nThe first cholera pandemic occurred in the Bengal region of India, near Calcutta starting in 1817 through 1824. The disease dispersed from India to Southeast Asia, the Middle East, Europe, and Eastern Africa. The movement of British Army and Navy ships and personnel is believed to have contributed to the range of the pandemic, since the ships carried people with the disease to the shores of the Indian Ocean, from Africa to Indonesia, and north to China and Japan. The second pandemic lasted from 1826 to 1837 and particularly affected North America and Europe due to the result of advancements in transportation and global trade, and increased human migration, including soldiers. The third pandemic erupted in 1846, persisted until 1860, extended to North Africa, and reached South America, for the first time specifically affecting Brazil. The fourth pandemic lasted from 1863 to 1875 spread from India to Naples and Spain. The fifth pandemic was from 1881–1896 and started in India and spread to Europe, Asia, and South America. The sixth pandemic started 1899–1923. These epidemics were less fatal due to a greater understanding of the cholera bacteria. Egypt, the Arabian peninsula, Persia, India, and the Philippines were hit hardest during these epidemics, while other areas, like Germany in 1892 and Naples from 1910–1911, also experienced severe outbreaks. The seventh pandemic originated in 1961 in Indonesia and is marked by the emergence of a new strain, nicknamed \"El Tor\", which still persists () in developing countries.\n\nSince it became widespread in the 19th century, cholera has killed tens of millions of people. In Russia alone, between 1847 and 1851, more than one million people perished of the disease. It killed 150,000 Americans during the second pandemic. Between 1900 and 1920, perhaps eight million people died of cholera in India. Cholera became the first reportable disease in the United States due to the significant effects it had on health. John Snow, in England, was the first to identify the importance of contaminated water as its cause in 1854. Cholera is now no longer considered a pressing health threat in Europe and North America due to filtering and chlorination of water supplies, but still heavily affects populations in developing countries.\n\nIn the past, vessels flew a yellow quarantine flag if any crew members or passengers were suffering from cholera. No one aboard a vessel flying a yellow flag would be allowed ashore for an extended period, typically 30 to 40 days. In modern sets of international maritime signal flags, the quarantine flag is yellow and black.\n\nHistorically many different claimed remedies have existed in folklore. Many of the older remedies were based on the miasma theory. Some believed that abdominal chilling made one more susceptible and flannel and cholera belts were routine in army kits. In the 1854–1855 outbreak in Naples homeopathic camphor was used according to Hahnemann. T. J. Ritter's \"Mother's Remedies\" book lists tomato syrup as a home remedy from northern America. Elecampane was recommended in the United Kingdom according to William Thomas Fernie.\n\nCholera cases are much less frequent in developed countries where governments have helped to establish water sanitation practices and effective medical treatments. The United States, for example, used to have a severe cholera problem similar to those in some developing countries. There were three large cholera outbreaks in the 1800s, which can be attributed to \"Vibrio cholerae\"'s spread through interior waterways like the Erie Canal and routes along the Eastern Seaboard. The island of Manhattan in New York City touched the Atlantic Ocean, where cholera collected just off the coast. At this time, New York City did not have as effective a sanitation system as it does today, so cholera was able to spread.\n\nCholera morbus is a historical term that was used to refer to gastroenteritis rather than specifically cholera.\n\nThe bacterium was isolated in 1854 by Italian anatomist Filippo Pacini, but its exact nature and his results were not widely known.\n\nSpanish physician Jaume Ferran i Clua developed a cholera inoculation in 1885, the first to immunize humans against a bacterial disease.\n\nRussian-Jewish bacteriologist Waldemar Haffkine developed the first cholera vaccine in July 1892.\n\nOne of the major contributions to fighting cholera was made by the physician and pioneer medical scientist John Snow (1813–1858), who in 1854 found a link between cholera and contaminated drinking water. Dr. Snow proposed a microbial origin for epidemic cholera in 1849. In his major \"state of the art\" review of 1855, he proposed a substantially complete and correct model for the cause of the disease. In two pioneering epidemiological field studies, he was able to demonstrate human sewage contamination was the most probable disease vector in two major epidemics in London in 1854. His model was not immediately accepted, but it was seen to be the more plausible, as medical microbiology developed over the next 30 years or so.\n\nCities in developed nations made massive investment in clean water supply and well-separated sewage treatment infrastructures between the mid-1850s and the 1900s. This eliminated the threat of cholera epidemics from the major developed cities in the world. In 1883, Robert Koch identified \"V. cholerae\" with a microscope as the bacillus causing the disease.\n\nRobert Allan Phillips, working at the US Naval Medical Research Unit Two in Southeast Asia, evaluated the pathophysiology of the disease using modern laboratory chemistry techniques and developed a protocol for rehydration. His research led the Lasker Foundation to award him its prize in 1967.\n\nMore recently, in 2002, Alam, \"et al.\", studied stool samples from patients at the International Centre for Diarrhoeal Disease in Dhaka, Bangladesh. From the various experiments they conducted, the researchers found a correlation between the passage of \"V. cholerae\" through the human digestive system and an increased infectivity state. Furthermore, the researchers found the bacterium creates a hyperinfected state where genes that control biosynthesis of amino acids, iron uptake systems, and formation of periplasmic nitrate reductase complexes were induced just before defecation. These induced characteristics allow the cholera vibrios to survive in the \"rice water\" stools, an environment of limited oxygen and iron, of patients with a cholera infection.\n\nIn many developing countries, cholera still reaches its victims through contaminated water sources, and countries without proper sanitation techniques have greater incidence of the disease. Governments can play a role in this. In 2008, for example, the Zimbabwean cholera outbreak was due partly to the government's role, according to a report from the James Baker Institute. The Haitian government's inability to provide safe drinking water after the 2010 earthquake led to an increase in cholera cases as well.\n\nSimilarly, South Africa's cholera outbreak was exacerbated by the government's policy of privatizing water programs. The wealthy elite of the country were able to afford safe water while others had to use water from cholera-infected rivers.\n\nAccording to Rita R. Colwell of the James Baker Institute, if cholera does begin to spread, government preparedness is crucial. A government's ability to contain the disease before it extends to other areas can prevent a high death toll and the development of an epidemic or even pandemic. Effective disease surveillance can ensure that cholera outbreaks are recognized as soon as possible and dealt with appropriately. Oftentimes, this will allow public health programs to determine and control the cause of the cases, whether it is unsanitary water or seafood that have accumulated a lot of \"Vibrio cholerae\" specimens. Having an effective surveillance program contributes to a government's ability to prevent cholera from spreading. In the year 2000 in the state of Kerala in India, the Kottayam district was determined to be \"Cholera-affected\"; this pronouncement led to task forces that concentrated on educating citizens with 13,670 information sessions about human health. These task forces promoted the boiling of water to obtain safe water, and provided chlorine and oral rehydration salts. Ultimately, this helped to control the spread of the disease to other areas and minimize deaths. On the other hand, researchers have shown that most of the citizens infected during the 1991 cholera outbreak in Bangladesh lived in rural areas, and were not recognized by the government's surveillance program. This inhibited physicians' abilities to detect cholera cases early.\n\nAccording to Colwell, the quality and inclusiveness of a country's health care system affects the control of cholera, as it did in the Zimbabwean cholera outbreak. While sanitation practices are important, when governments respond quickly and have readily available vaccines, the country will have a lower cholera death toll. Affordability of vaccines can be a problem; if the governments do not provide vaccinations, only the wealthy may be able to afford them and there will be a greater toll on the country's poor. The speed with which government leaders respond to cholera outbreaks is important.\n\nBesides contributing to an effective or declining public health care system and water sanitation treatments, government can have indirect effects on cholera control and the effectiveness of a response to cholera. A country's government can impact its ability to prevent disease and control its spread. A speedy government response backed by a fully functioning health care system and financial resources can prevent cholera's spread. This limits cholera's ability to cause death, or at the very least a decline in education, as children are kept out of school to minimize the risk of infection.\n\n\n"}
{"id": "23890708", "url": "https://en.wikipedia.org/wiki?curid=23890708", "title": "Circuit rider (water/wastewater)", "text": "Circuit rider (water/wastewater)\n\nRural water circuit riders are roving technical experts employed by State Rural Water Associations to provide training and assistance to rural and small water utilities within their state.\n\nThe National Rural Water Association began its circuit rider program in 1980. The program was intended to provide support for small utility systems that did not always have the experience, equipment, training or personnel to deal with large or persistent problems. Circuit riders usually operate within a specific area of their designated state, visiting the small utilities on a regular basis.\n\nCircuit riders are usually categorized as either drinking water or wastewater specialists. Drinking water circuit riders specialize in the supply, treatment and distribution of clean drinking water through a water utility. Wastewater circuit riders specialize in the processes required for the safe collection, treatment and disposal of wastewater and sewage. Circuit riders in both fields have extensive knowledge and experience with the state and federal regulations governing drinking water and wastewater.\n\nSome state associations have circuit riders who specialize in assisting utilities with management, accounting and record-keeping issues. These circuit riders are often asked to perform rate studies, which examine a utility's expenses and incomes and recommends rates that allows them to meet their financial obligations.\n\nNational Rural Water Association\n\n"}
{"id": "3751384", "url": "https://en.wikipedia.org/wiki?curid=3751384", "title": "Circuit training", "text": "Circuit training\n\nCircuit training is a form of body conditioning or endurance training or resistance training using high-intensity. It targets strength building or muscular endurance. An exercise \"circuit\" is one completion of all prescribed exercises in the program. When one circuit is complete, one begins the first exercise again for the next circuit. Traditionally, the time between exercises in circuit training is short, often with rapid movement to the next exercise.\n\nThe program was developed by R.E. Morgan and G.T. Anderson in 1953 at the University of Leeds in England.\n\nA circuit should work each section of the body individually. Typical activities include:\n\nUpper-body \nCore & trunk\nLower-body\nTotal-body\n\nStudies at Baylor University and The Cooper Institute show that circuit training is the most time efficient way to enhance cardiovascular fitness and muscle endurance. Studies show that circuit training helps women to achieve their goals and maintain them longer than other forms of exercise or diet.\n\nMorgan and Anderson claim:\nOne advantage is that reduced station times will encourage the participants to lift heavier weights, which means they can achieve overload with smaller number of repetitions: typically in the range of 25 to 50 depending on their training goals.\n\n\n"}
{"id": "310898", "url": "https://en.wikipedia.org/wiki?curid=310898", "title": "Croup", "text": "Croup\n\nCroup, also known as laryngotracheobronchitis, is a type of respiratory infection that is usually caused by a virus. The infection leads to swelling inside the trachea, which interferes with normal breathing and produces the classic symptoms of \"barking\" cough, stridor, and a hoarse voice. Fever and runny nose may also be present. These symptoms may be mild, moderate, or severe. Often it starts or is worse at night. It normally lasts one to two days.\nCroup can be caused by a number of viruses including parainfluenza and influenza virus. Rarely is it due to a bacterial infection. Croup is typically diagnosed based on signs and symptoms after potentially more severe causes, such as epiglottitis or an airway foreign body, have been ruled out. Further investigations—such as blood tests, X-rays, and cultures—are usually not needed.\nMany cases of croup are preventable by immunization for influenza and diphtheria. Croup is usually treated with a single dose of steroids by mouth. In more severe cases inhaled epinephrine may also be used. Hospitalization is required in one to five percent of cases.\nCroup is a relatively common condition that affects about 15% of children at some point. It most commonly occurs between 6 months and 5 years of age but may rarely be seen in children as old as fifteen. It is slightly more common in males than females. It occurs most often in autumn. Before vaccination, croup was frequently caused by diphtheria and was often fatal. This cause is now very rare in the Western world due to the success of the diphtheria vaccine.\n\nCroup is characterized by a \"barking\" cough, stridor, hoarseness, and difficulty breathing which usually worsens at night. The \"barking\" cough is often described as resembling the call of a seal or sea lion. The stridor is worsened by agitation or crying, and if it can be heard at rest, it may indicate critical narrowing of the airways. As croup worsens, stridor may decrease considerably.\n\nOther symptoms include fever, coryza (symptoms typical of the common cold), and indrawing of the chest wall–known as Hoover's sign. Drooling or a very sick appearance indicate other medical conditions, such as epiglottitis.\n\nCroup is usually deemed to be due to a viral infection. Others use the term more broadly, to include acute laryngotracheitis, spasmodic croup, laryngeal diphtheria, bacterial tracheitis, laryngotracheobronchitis, and laryngotracheobronchopneumonitis. The first two conditions involve a viral infection and are generally milder with respect to symptomatology; the last four are due to bacterial infection and are usually of greater severity.\n\nViral croup or acute laryngotracheitis is most commonly caused by parainfluenza virus (a member of the paramyxovirus family), primarily types 1 and 2, in 75% of cases. Other viral causes include influenza A and B, measles, adenovirus and respiratory syncytial virus (RSV). Spasmodic croup is caused by the same group of viruses as acute laryngotracheitis, but lacks the usual signs of infection (such as fever, sore throat, and increased white blood cell count). Treatment, and response to treatment, are also similar.\n\nBacterial croup may be divided into laryngeal diphtheria, bacterial tracheitis, laryngotracheobronchitis, and laryngotracheobronchopneumonitis. Laryngeal diphtheria is due to \"Corynebacterium diphtheriae\" while bacterial tracheitis, laryngotracheobronchitis, and laryngotracheobronchopneumonitis are usually due to a primary viral infection with secondary bacterial growth. The most common bacteria implicated are \"Staphylococcus aureus\", \"Streptococcus pneumoniae\", \"Hemophilus influenzae\", and \"Moraxella catarrhalis\".\n\nThe viral infection that causes croup leads to swelling of the larynx, trachea, and large bronchi due to infiltration of white blood cells (especially histiocytes, lymphocytes, plasma cells, and neutrophils). Swelling produces airway obstruction which, when significant, leads to dramatically increased work of breathing and the characteristic turbulent, noisy airflow known as stridor.\n\nCroup is typically diagnosed based on signs and symptoms. The first step is to exclude other obstructive conditions of the upper airway, especially epiglottitis, an airway foreign body, subglottic stenosis, angioedema, retropharyngeal abscess, and bacterial tracheitis.\n\nA frontal X-ray of the neck is not routinely performed, but if it is done, it may show a characteristic narrowing of the trachea, called the steeple sign, because of the subglottic stenosis, which resembles a steeple in shape. The steeple sign is suggestive of the diagnosis, but is absent in half of cases.\n\nOther investigations (such as blood tests and viral culture) are discouraged, as they may cause unnecessary agitation and thus worsen the stress on the compromised airway. While viral cultures, obtained via nasopharyngeal aspiration, can be used to confirm the exact cause, these are usually restricted to research settings. Bacterial infection should be considered if a person does not improve with standard treatment, at which point further investigations may be indicated.\n\nThe most commonly used system for classifying the severity of croup is the Westley score. It is primarily used for research purposes rather than in clinical practice. It is the sum of points assigned for five factors: level of consciousness, cyanosis, stridor, air entry, and retractions. The points given for each factor is listed in the adjacent table, and the final score ranges from 0 to 17.\n\n\n85% of children presenting to the emergency department have mild disease; severe croup is rare (<1%).\n\nMany cases of croup have been prevented by immunization for influenza and diphtheria. At one time, croup referred to a diphtherial disease, but with vaccination, diphtheria is now rare in the developed world.\n\nChildren with croup are generally kept as calm as possible. Steroids are given routinely, with epinephrine used in severe cases. Children with oxygen saturations under 92% should receive oxygen, and those with severe croup may be hospitalized for observation. If oxygen is needed, \"blow-by\" administration (holding an oxygen source near the child's face) is recommended, as it causes less agitation than use of a mask. With treatment, less than 0.2% of children require endotracheal intubation.\n\nCorticosteroids, such as dexamethasone and budesonide, have been shown to improve outcomes in children with all severities of croup. Significant relief is obtained as early as two hours after administration. While effective when given by injection, or by inhalation, giving the medication by mouth is preferred. A single dose is usually all that is required, and is generally considered to be quite safe. Dexamethasone at doses of 0.15, 0.3 and 0.6 mg/kg appear to be all equally effective.\n\nModerate to severe croup may be improved temporarily with nebulized epinephrine. While epinephrine typically produces a reduction in croup severity within 10–30 minutes, the benefits last for only about 2 hours. If the condition remains improved for 2–4 hours after treatment and no other complications arise, the child is typically discharged from the hospital.\n\nWhile other treatments for croup have been studied, none have sufficient evidence to support their use. Inhalation of hot steam or humidified air is a traditional self-care treatment, but clinical studies have failed to show effectiveness and currently it is rarely used. The use of cough medicines, which usually contain dextromethorphan or guaifenesin, are also discouraged. There is tentative evidence that breathing heliox (a mixture of helium and oxygen) to decrease the work of breathing is useful in those with severe disease. Since croup is usually a viral disease, antibiotics are not used unless secondary bacterial infection is suspected. In cases of possible secondary bacterial infection, the antibiotics vancomycin and cefotaxime are recommended. In severe cases associated with influenza A or B, the antiviral neuraminidase inhibitors may be administered.\n\nViral croup is usually a self-limiting disease, with half of cases resolving in a day and 80% of cases in two days. It can very rarely result in death from respiratory failure and/or cardiac arrest. Symptoms usually improve within two days, but may last for up to seven days. Other uncommon complications include bacterial tracheitis, pneumonia, and pulmonary edema.\n\nCroup affects about 15% of children, and usually presents between the ages of 6 months and 5–6 years. It accounts for about 5% of hospital admissions in this population. In rare cases, it may occur in children as young as 3 months and as old as 15 years. Males are affected 50% more frequently than are females, and there is an increased prevalence in autumn.\n\nThe word \"croup\" comes from the Early Modern English verb \"croup\", meaning \"to cry hoarsely\"; the name was first applied to the disease in Scotland and popularized in the 18th century. Diphtheritic croup has been known since the time of Homer's Ancient Greece and it was not until 1826 that viral croup was differentiated from croup due to diphtheria by Bretonneau. Viral croup was then called \"faux-croup\" by the French and often called \"false croup\" in English, as \"croup\" or \"true croup\" then most often referred to the disease caused by the diphtheria bacterium. False croup has also been known as pseudo croup or spasmodic croup. Croup due to diphtheria has become nearly unknown in affluent countries in modern times due to the advent of effective immunization.\n"}
{"id": "34413419", "url": "https://en.wikipedia.org/wiki?curid=34413419", "title": "Delusional intuition", "text": "Delusional intuition\n\nDelusional intuition is a term applicable in psychiatry that refers to a thought or belief that when expressed either in society, or as usually in a clinical setting, is apparent as being blatantly impossible or unlikely in that the semantic relations of subjects within the speech content have no basis in reality, i.e. that is of a thought that is delusional. This type of delusion in the context of the \"intuitive\" is rather an experience of faux intuition, the person experiences something that resembles the intuitive, but instead the experience is qualified as delusional. This delusion is also described as Autochthonous.\n\nThis description of a psychological phenomenon, that is as observed in the form of expression within behaviour abnormally, and communicated in abnormal speech, is translated from the German Wahneinfall. Wahn translated is specifically a whimsy, false opinion, or fancy.\n\nIs a term relevant to the fields of psychiatry and psychology and describe the expression of thought(s) that have no apparent basis in inference. A phenomenological understanding is of an occurrence that is very much like the expression of the spontaneous occurrence of an inspirational idea, \"sprung from the soil\", translated into a delusionary vehicle with the conviction of \"immediate enlightenment\" (Leon \"et al\" 1989) that occurs as a delire d'embléé \"id est\" complete in the actual instance. The delusion as defined in this context is known as \"primary\" (Jaspers 1963).\n\nThe delusion is found described in clinical settings as a description of medical symptom of the psychotic illness known as schizophrenia, and is known within that milieu as a first rank symptom The delusional ideation sometimes occurs from a \"prior delusional mood\" (Fish 1985). According to the Klaus Conrad 1958 account, \"\", the delusion occurs as a second order development of earlier delusionary thinking.\n\n\n"}
{"id": "55360114", "url": "https://en.wikipedia.org/wiki?curid=55360114", "title": "Directorate General of Nursing and Midwifery", "text": "Directorate General of Nursing and Midwifery\n\nDirectorate General of Nursing and Midwifery is the central government body responsible for public nursing and midwifery in Bangladesh and is located in Dhaka, Bangladesh. Tandra Sikder is the present director general of the directorate.\n\nDirectorate of Nursing Services was created in 1977 and was later changed to its current form. This was done to expand nursing training in Bangladesh and increase the number of nurses. The directorate falls under the jurisdiction of the Ministry of Health and Family Welfare (Bangladesh).\n"}
{"id": "42528326", "url": "https://en.wikipedia.org/wiki?curid=42528326", "title": "Emotional eating", "text": "Emotional eating\n\nEmotional eating is defined as overeating in order to relieve negative emotions. Thus, emotional eating is considered a maladaptive coping strategy. If an individual frequently engages in emotional eating, it can increase the risk of developing other eating disorders, like bulimia and anorexia nervosa. Research has also shown that the presence of an existing eating disorder increases the likelihood that an individual will engage in emotional eating. Given the relationship between serious eating disorders and emotional eating behavior, it is important for clinical psychologists and nutritionists to recognize the signs of emotional eating and provide individuals with treatment. Since emotional eating is utilized to manage negative emotions, treatment necessitates learning healthy and more effective coping strategies.\n\nEmotional eating is a form of disordered eating and is defined as \"an increase in food intake in response to negative emotions\" and can be considered a maladaptive strategy used to cope with difficult feelings. More specifically, emotional eating would qualify as a form of emotion-focused coping, which attempts to minimize, regulate and prevent emotional distress. A study conducted by Bennett et al. found that emotional eating sometimes does not reduce emotional distress but instead enhances emotional distress by sparking feelings of intense guilt after an emotional eating session. Not only is emotional eating a poor way to cope, but those individuals who frequently utilize emotional eating to cope with social or psychological stressors are at an increased risk of developing eating disorders. Emotional eaters are at an especially high risk of developing binge-eating disorder. 2.8% of Americans struggle with binge-eating disorder, which increases their risk of developing cardiovascular disease and high blood pressure. At the same time, the presence of other eating disorders increases the risk of an individual engaging in emotional eating. In a clinical setting, emotional eating disorders can be diagnosed by the Dutch Eating Behavior Questionnaire which contains a scale for restrained, emotional and external eating. While therapists may use positive psychology as a way to reduce the negative emotions that trigger emotional eating, reappraisal is often a complementary treatment with the primary treatment being focused on developing alternative coping strategies.\n\nCurrent research suggests that certain individual factors may increase one's likelihood of using emotional eating as a coping strategy. The inadequate affect regulation theory posits that individuals engage in emotional eating because they believe overeating alleviates negative feelings. Escape theory builds upon inadequate affect regulation theory by suggesting that people not only overeat to cope with negative emotions, but they find that overeating diverts their attention away from a stimuli that is threatening self-esteem to focus on a pleasurable stimuli like food. Restraint theory suggests that overeating as a result of negative emotions occurs among individuals who already restrain their eating. While these individuals typically limit what they eat, when they are faced with negative emotions they cope by engaging in emotional eating. Restraint theory supports the idea that individuals with other eating disorders are more likely to engage in emotional eating. Together these three theories suggest that an individual's aversion to negative emotions, particularly negative feelings that arise in response to a threat to the ego or intense self-awareness, increase the propensity for the individual to utilize emotional eating as a means of coping with this aversion.\n\nThe biological stress response may also contribute to the development of emotional eating tendencies. In a crisis, corticotropin-releasing hormone (CRH) is secreted by the hypothalamus, suppressing appetite and triggering the release of glucocorticoids from the adrenal gland. These steroid hormones increase appetite and, unlike CRH, remain in the bloodstream for a prolonged period of time, often resulting in hyperphagia. Those who experience this biologically instigated increase in appetite during times of stress are therefore primed to rely on emotional eating as a coping mechanism.\n\nEmotional eating differs from actual hunger in a number of ways. Emotional hunger tends to hit quickly and suddenly and feels urgent, yet physical hunger is usually not as urgent or sudden unless it has been a while since a person ate. Someone who is physically hungry will often eat anything, while someone who is emotionally hungry will want something specific, such as fries or a pizza; emotional hunger is usually associated with cravings for junk food or something unhealthy. Another aspect of emotional eating is mindless eating, which is when someone eats without paying attention to or enjoying what they are consuming. An example is eating an entire container of ice cream while watching television, having not intended to eat that much. This behavior usually happens with emotional eating, not eating through hunger. Emotional hunger does not originate from the stomach, such as with a rumbling or growling stomach, but tends to start when a person thinks about a craving or wants something specific to eat. Emotional responses are also different. Giving in to a craving, or eating because of stress can cause feelings of regret, shame, or guilt, and these responses tend to be associated with emotional hunger. On the other hand, satisfying a physical hunger is giving the body the nutrients or calories it needs to function and is not associated with negative feelings.\n\nOverall, high levels of the negative affect trait are related to emotional eating. Negative affectivity is a personality trait involving negative emotions and poor self-concept. It has been found that certain negative affect regulation scales predicted emotional eating. Additionally, a study conducted by Bennett et al. found that individuals engage in emotional eating only when they are experiencing negative emotions. More specifically, an inability to articulate and identify one's emotions made the individual feel inadequate at regulating negative affect and thus more likely to engage in emotional eating. A study conducted by Spoor et al. attempted to further delineate the relationship between negative affect and emotional eating. They found that negative affect was not significantly related to emotional eating when taking into consideration emotion focused coping and avoidance distraction behavior. This suggests that negative affect is not independently related to emotional eating but is instead indirectly related through emotional focused coping and avoidance distraction behavior. While Spence and Spoor's findings differed somewhat, they both suggest that negative affect does play a role in emotional eating but it may be accounted for by other variables.\n\nFor some people, emotional eating is a learned behavior. During childhood, their parents give them treats to help them deal with a tough day or situation, or as a reward for something good. Over time, the child who reaches for a cookie after getting a bad grade on a test may become an adult who grabs a box of cookies after a rough day at work. In an example such as this, the roots of emotional eating are deep, which can make breaking the habit extremely challenging.\n\nEmotional eating itself may be a precursor to developing eating disorders such as binge eating or bulimia nervosa. The relationship between emotional eating and other disorders is largely due to the fact that emotional eating and these disorders share key characteristics. More specifically, they are both related to emotion focused coping, maladaptive coping strategies, and a strong aversion to negative feelings and stimuli. It is important to note that the causal direction has not been definitively established, meaning that while emotional eating is considered a precursor to these eating disorders, it may be also be the consequence of these disorders. The latter hypothesis that emotional eating happens in response to another eating disorder is supported by research that has shown emotional eating to be more common among individuals already suffering from bulimia nervosa.\n\nStress affect food preferences. Numerous studies — granted, many of them in animals — have shown that physical or emotional distress increases the intake of food high in fat, sugar, or both. High cortisol levels, in combination with high insulin levels, may be responsible. Other research suggests that ghrelin, a \"hunger hormone,\" may have a role. Once ingested, fat- and sugar-filled foods seem to have a feedback effect that dampens stress related responses and emotions. These foods really are \"comfort\" foods in that they seem to counteract stress. This may contribute to people's stress-induced craving for those foods. Individual differences in the physiological stress response may also contribute to the development of emotional eating habits. Those whose adrenal glands naturally secrete larger quantities of glucocorticoids in response to a stressor are more inclined toward hyperphagia, which can act as a physiological catalyst for emotional eating. Additionally, those whose bodies require more time to clear the bloodstream of excess glucocorticoids are similarly predisposed. These biological factors can interact with environmental elements to further trigger hyperphagia, namely the type of stressor the individual is subjected to. Frequent intermittent stressors trigger repeated, sporadic releases of glucocorticoids broken up by intervals too short to allow for a complete return to baseline levels, leading to increased appetite. Those whose lifestyles or careers entail frequent intermittent stressors thus have greater biological incentive to develop patterns of emotional eating.\n\nEmotional eating may qualify as avoidant coping and/or emotion-focused coping. As coping methods that fall under these broad categories focus on temporary reprieve rather than practical resolution of stressors, they can initiate a vicious cycle of maladaptive behavior reinforced by fleeting relief from stress. Additionally, in the presence of high insulin levels characteristic of the recovery phase of the stress-response, glucocorticoids trigger the creation of an enzyme that stores away the nutrients circulating in the bloodstream after an episode of emotional eating as visceral fat, or fat located in the abdominal area. Therefore, those who struggle with emotional eating are at greater risk for abdominal obesity, which is in turn linked to a greater risk for metabolic and cardiovascular disease.\n\nThere are numerous ways in which individuals can reduce emotional distress without engaging in emotional eating. The most salient choice is to minimize maladaptive coping strategies and to maximize adaptive strategies. A study conducted by Corstorphine et al. in 2007 investigated the relationship between distress tolerance and disordered eating. These researchers specifically focused on how different coping strategies impact distress tolerance and disordered eating. They found that individuals who engage in disordered eating often employ emotional avoidance strategies. If an individual is faced with strong negative emotions, they may choose to avoid the situation by distracting themselves through overeating. Discouraging emotional avoidance is thus an important facet to emotional eating treatment. The most obvious way to limit emotional avoidance is to confront the issue through techniques like problem solving. Corstorphine et al. showed that individuals who engaged in problem solving strategies enhance one's ability to tolerate emotional distress. Since emotional distress is correlated to emotional eating, the ability to better manage one's negative affect should allow an individual to cope with a situation without resorting to overeating.\n\nOne way to combat emotional eating is to employ mindfulness techniques. For example, approaching cravings with a nonjudgmental inquisitiveness can help differentiate between hunger and emotionally-driven cravings. An individual may ask his or herself if the craving developed rapidly, as emotional eating tends to be triggered spontaneously. An individual may also take the time to note his or her bodily sensations, such as hunger pangs, and coinciding emotions, like guilt or shame, in order to make conscious decisions to avoid emotional eating.\n\nEmotional eating can also be improved by evaluating physical facets like hormone balance. Female hormones, in particular, can alter cravings and even self-perception of one's body. Additionally, emotional eating can be exacerbated by social pressure to be thin. The focus on thinness and dieting in our culture can make young girls, especially, vulnerable to falling into food restriction and subsequent emotional eating behavior. \n\nEmotional eating disorder predisposes individuals to more serious eating disorders and physiological complications. Therefore, combatting disordered eating before such progression takes place has become the focus of many clinical psychologists.\n\n"}
{"id": "26975440", "url": "https://en.wikipedia.org/wiki?curid=26975440", "title": "English Medium Medical Schools", "text": "English Medium Medical Schools\n\nAround 2006, the Chinese Government instituted the admission of foreign students to China for medical instruction using English as the language of instruction. Students are eligible to earn basic degrees in either Medicine (MBBS) or Dentistry (BDS).\n\nTo ensure adequate quality control, only a few choice medical schools are allowed specific quotas for admitting international students into these programs. Other Chinese medical institutions are not barred from admitting international students, but the government maintains that these other students would have to be admitted into the medical programs taught in Chinese.\n\nThis list of universities is reviewed each year and currently stands at 50 schools.\n\nThese schools are thus officially recognized by the WHO and the MOE.\n\n"}
{"id": "47260419", "url": "https://en.wikipedia.org/wiki?curid=47260419", "title": "Federation of (Ophthalmic and Dispensing) Opticians", "text": "Federation of (Ophthalmic and Dispensing) Opticians\n\nThe Federation of (Ophthalmic and Dispensing) Opticians is a trade organisation representing eye care providers and registered opticians in business in the UK and Republic of Ireland. It was founded in 1985.\n\nThe members of the Association deliver over three quarters of market activity and two thirds of eye examinations in the UK and 55 per cent in the Republic of Ireland.\n\nIts aim is to achieve eye health for all, delivered through world-class services, provided by regulated community-based professionals operating in a competitive environment. In the UK, they are founder members of the Optical Confederation and supporters of Vision 2020 UK and the UK Vision Strategy.\n\n"}
{"id": "22626984", "url": "https://en.wikipedia.org/wiki?curid=22626984", "title": "GamCare", "text": "GamCare\n\nGamCare is the leading provider of information, advice, support and free counselling for the prevention and treatment of problem gambling. GamCare is a national charity and was founded in 1997. In October 1997 Adrian Scarfe, Head of Clinical Services at GamCare, launched the HelpLine and counselling services. The president of GamCare is Lord Sharman of Redlynch.\n\nGamCare provides a variety of services for problem gamblers. GamCare operates the national freephone telephone HelpLine 0808 8020 133, provides online help and free face-to-face counselling. In 2011/12 GamCare answered nearly 100 calls a day and provided counselling for several thousand clients. Face-to-face counselling is available at GamCare and across Great Britain through the GamCare Partners Network. GamCare provides training and materials to the gambling industry to improve social responsibility and player protection. GamCare offer social responsibility training for the gaming and betting industries. Training programmes provide attendees with vital insights on how to recognise the signs of problem gambling behaviour and advice on how to interact with players to achieve a positive outcome.\n\nGame Care can affect applications to popular holiday destinations that either do not condone gambling, such as the middle East as well as Visa applications to gambling hot spots such as Las Vegas.\n\nGamCare work with land-based and online companies to advise and ensure they implement player protection and responsible gambling procedures. GamCare Certification is awarded to companies that successfully implement the GamCare Player Protection Code of Practice and social responsibility practices. Only certified companies are permitted to use the GamCare Certification logo.\n\nIn November 2012 GamCare launched the BigDeal website www.bigdeal.org.uk. The BigDeal website is targeted at 12- to 18-year-old and provides information about gambling and the risks involved with gambling. In addition the website provides information on how to get help for those who are experiencing a gambling problem and those who are concerned that someone they know might be experiencing a gambling problem.\n\n\n"}
{"id": "58052965", "url": "https://en.wikipedia.org/wiki?curid=58052965", "title": "George Mackay (surgeon)", "text": "George Mackay (surgeon)\n\nGeorge Mackay (1861-1949) was a British Ophthalmic Surgeon. He served in the Department of Ophthalmology of the University of Edinburgh and the Royal Infirmary of Edinburgh, and was a member of the Ophthalmological Society of the United Kingdom, the Scottish Ophthalmological Club and the French Ophthalmological Society. \n\nHe was born near Madras, but the family moved back to Scotland when he was 4 years old. He was educated at Clifton and Inverness Colleges and graduated M.B., C.M. with honours from Edinburgh University (1883) and M.D. in 1888. He was awarded gold medal for his thesis, which was titled: \"A contribution to the study of hemianopsia of central origin: with special reference to acquired colour blindness and a clinical report of 4 cases.\"He specialised in ophthalmic surgery. During his post-graduate studies he spent some time in Vienna.\n\nHe became a Member of the Royal Colleges of Surgeons of England in 1883 and Fellow of the Royal Colleges of Surgeons of Edinburgh in 1886.. After retiring from hospital practice he continued have an extensive private practice and became a Manager of the Royal Infirmary and President of the Royal College of Surgeons of Edinburgh( 1919 - 1921). He was most famous for performing cataract operations, done with the upmost steadiness and precision. \"We remember a distinguished figure, walking to and from the Infirmary irreproachably dressed, with a well-cut morning coat, shining tall hat, and immaculate linen.\"\n\nAs a student he enjoyed athletics and gymnastics, and in later life took up golf and fishing. He was also a member of the Royal Company of Archers, the King's Bodyguard for Scotland. Gaelic literature, archaeology, anthropology and geology were also his interests.\n\nHe is the son of Surgeon-General George Mackay, who served in the Indian Army, and was instrumental to the establishment of the Medical College of Madras.\n\nHis great-grandfather was Lt-Col. George Mackay, whose wife's portrait was painted by Henry Raeburn.\n"}
{"id": "3590247", "url": "https://en.wikipedia.org/wiki?curid=3590247", "title": "H. Lou Gibson", "text": "H. Lou Gibson\n\nHenry Louis Gibson (1906–1992) a British-born American pioneering medical photographer, was born in Truro, Cornwall, England, United Kingdom and died in Rochester, New York State, United States of America. \nHe was for many years editor and consultant in medical, biological, scientific, and technical photography for the Eastman Kodak Company. He received his B.Sc. degree in physics from the University of Illinois. He was a president of the Biological Photographic Association (renamed the Biocommunications Association). He was made a Fellow of the association and in 1960 received its highest honor, the Louis Schmidt Award for his outstanding contribution to scientific photography.\n\nHe was an expert in medical uses of infrared radiation and had pioneered its use in detecting breast cancer.\n\n\n\n"}
{"id": "53987048", "url": "https://en.wikipedia.org/wiki?curid=53987048", "title": "Harvey Borovetz", "text": "Harvey Borovetz\n\nHarvey S. Borovetz is an American bioengineer currently a Distinguished Professor and the Robert L. Hardesty Professor at University of Pittsburgh and an Elected Fellow of the American Institute for Medical and Biological Engineering, Biomedical Engineering Society, Council on Arteriosclerosis, American Heart Association.\n"}
{"id": "28484156", "url": "https://en.wikipedia.org/wiki?curid=28484156", "title": "Hellenic Red Cross", "text": "Hellenic Red Cross\n\nThe Hellenic Red Cross () is the Greek national Red Cross Society, founded on 10 June 1877.\n\n"}
{"id": "41272705", "url": "https://en.wikipedia.org/wiki?curid=41272705", "title": "History of medical cannabis", "text": "History of medical cannabis\n\nThe history of medical cannabis goes back to ancient times. Ancient physicians in many parts of the world mixed cannabis into medicines to treat pain and other ailments. In the 19th century, cannabis was introduced for therapeutic use in Western Medicine. Since then, there have been several advancements in how the drug is administered. Initially, cannabis was reduced to a powder and mixed with wine for administration. In the 1970's, synthetic THC was created to be administered as the drug Marinol in a capsule. However, the main mode of administration for cannabis is smoking because its effects are almost immediate when the smoke is inhaled. Between 1996 and 1999, eight U.S. states supported cannabis prescriptions opposing policies of the federal government. Most people who are prescribed marijuana for medical purposes use it to alleviate severe pain.\n\nCannabis, called \"má\" 麻 (meaning \"hemp; cannabis; numbness\") or \"dàmá\" (with \"big; great\") in Chinese, was used in Taiwan for fiber starting about 10,000 years ago.\nThe botanist Hui-lin Li wrote that in China, \"The use of Cannabis in medicine was probably a very early development. Since ancient humans used hemp seed as food, it was quite natural for them to also discover the medicinal properties of the plant.\" The oldest Chinese pharmacopeia, the (c. 100 AD) \"Shennong Bencaojing\" 神農本草經 (\"Shennong's Materia Medica Classic\"), describes \"dama\" \"cannabis\".\nThe flowers when they burst (when the pollen is scattered) are called 麻蕡 [\"mafen\"] or 麻勃 [\"mabo\"]. The best time for gathering is the seventh day of the seventh month. The seeds are gathered in the ninth month. The seeds which have entered the soil are injurious to man. It grows in <nowiki>[Taishan]</nowiki> (in <nowiki>[Shandong]</nowiki> ...). The flowers, the fruit (seed) and the leaves are officinal. The leaves and the fruit are said to be poisonous, but not the flowers and the kernels of the seeds.\n\nThe early Chinese surgeon Hua Tuo (c. 140-208) is credited with being the first recorded person to use cannabis as an anesthetic. He reduced the plant to powder and mixed it with wine for administration prior to conducting surgery. The Chinese term for \"anesthesia\" (\"mázui\" 麻醉) literally means \"cannabis intoxication\". Elizabeth Wayland Barber says the Chinese evidence \"proves a knowledge of the narcotic properties of \"Cannabis\" at least from the 1st millennium B.C.\" when \"ma\" was already used in a secondary meaning of \"numbness; senseless.\" \"Such a strong drug, however, suggests that the Chinese pharmacists had now obtained from far to the southwest not THC-bearing \"Cannabis sativa\" but \"Cannabis indica\", so strong it knocks you out cold.\n\nThe Dutch sinologist Frank Dikötter's history of drugs in China says,\nThe medical uses were highlighted in a pharmacopeia of the Tang, which prescribed the root of the plant to remove a blood clot, while the juice from the leaves could be ingested to combat tapeworm. The seeds of cannabis, reduced to powder and mixed with rice wine, were recommended in various other materia medica against several ailments, ranging from constipation to hair loss. The Ming dynasty \"Mingyi bielu\" provided detailed instructions about the harvesting of the heads of the cannabis sativa plant (\"mafen\", \"mabo\"), while the few authors who acknowledged hemp in various pharmacopoeias seemed to agree that the resinous female flowering heads were the source of dreams and revelations. After copious consumption, according to the ancient \"Shennong bencaojing\", one could see demons and walk like a madman, even becoming 'in touch with the spirits' over time. Other medical writers warned that ghosts could be seen after ingesting a potion based on raw seeds blended with calamus and podophyllum (\"guijiu\").\n\nCannabis is one of the 50 \"fundamental\" herbs in traditional Chinese medicine, and is prescribed to treat diverse indications. FP Smith writes in \"Chinese Materia Medica: Vegetable Kingdom\": Every part of the hemp plant is used in medicine ... The flowers are recommended in the 120 different forms of (風 \"feng\") disease, in menstrual disorders, and in wounds. The achenia, which are considered to be poisonous, stimulate the nervous system, and if used in excess, will produce hallucinations and staggering gait. They are prescribed in nervous disorders, especially those marked by local anaesthesia. The seeds ... are considered to be tonic, demulcent, alternative [restorative], laxative, emmenagogue, diuretic, anthelmintic, and corrective. ... They are prescribed internally in fluxes, post-partum difficulties, aconite poisoning, vermillion poisoning, constipation, and obstinate vomiting. Externally they are used for eruptions, ulcers, favus, wounds, and falling of the hair. The oil is used for falling hair, sulfur poisoning, and dryness of the throat. The leaves are considered to be poisonous, and the freshly expressed juice is used as an anthelmintic, in scorpion stings, to stop the hair from falling out and to prevent it from turning gray. ... The stalk, or its bark, is considered to be diuretic ... The juice of the root is ... thought to have a beneficial action in retained placenta and post-partum hemorrhage. An infusion of hemp ... is used as a demulcent drink for quenching thirst and relieving fluxes.\n\nIn 2007, a late Neolithic grave attributed to the Beaker culture (found near , Gelderland; dated 2459-2203 BCE) was found containing an unusually large concentration of pollen. After five years of careful investigation these pollen were concluded to be mostly cannabis along with a smaller amount of meadowsweet. Due to the fever-reducing properties of meadowsweet, the archeologists speculated that the person in the grave had likely been very ill, in which case the cannabis would have served as painkiller.\n\nThe Ebers Papyrus (c. 1550 BC) from Ancient Egypt describes medical cannabis. Other ancient Egyptian papyri that mention medical cannabis are the Ramesseum III Papyrus (1700 BC), the Berlin Papyrus (1300 BC) and the Chester Beatty Medical Papyrus VI (1300 BC). The ancient Egyptians used hemp (cannabis) in suppositories for relieving the pain of hemorrhoids. Around 2,000 BCE, the ancient Egyptians used cannabis to treat sore eyes. The egyptologist Lise Manniche notes the reference to \"plant medical cannabis\" in several Egyptian texts, one of which dates back to the eighteenth century BCE.\n\nCannabis was a major component in religious practices in ancient India as well as in medicinal practices. For many centuries, most parts of life in ancient India incorporated cannabis of some form. Surviving texts from ancient India confirm that cannabis' psychoactive properties were recognized, and doctors used it for treating a variety of illnesses and ailments. These included insomnia, headaches, a whole host of gastrointestinal disorders, and pain: cannabis was frequently used to relieve the pain of childbirth. One Indian philosopher expressed his views on the nature and uses of bhang (a form of cannabis), which combined religious thought with medical practices. \"A guardian lives in the bhang leaf. …To see in a dream the leaves, plant, or water of bhang is lucky. …A longing for bhang foretells happiness. It cures dysentry and sunstroke, clears phlegm, quickens digestion, sharpens appetite, makes the tongue of the lisper plain, freshens the intellect and gives alertness to the body and gaiety to the mind. Such are the useful and needful ends for which in His goodness the Almighty made bhang.\"\n\nThe Ancient Greeks used cannabis to dress wounds and sores on their horses. In humans, dried leaves of cannabis were used to treat nose bleeds, and cannabis seeds were used to expel tapeworms. The most frequently described use of cannabis in humans was to steep green seeds of cannabis in either water or wine, later taking the seeds out and using the warm extract to treat inflammation and pain resulting from obstruction of the ear.\n\nIn the 5th century BC Herodotus, a Greek historian, described how the Scythians of the Middle East used cannabis in steam baths. These baths drove the people to a frenzied state.\n\nIn the medieval Islamic world, Arabic physicians made use of the diuretic, antiemetic, antiepileptic, anti-inflammatory, analgesic and antipyretic properties of \"Cannabis sativa\", and used it extensively as medication from the 8th to 18th centuries.\n\nIn the mid 19th century, medical interest in the use of cannabis began to grow in the West. In the 19th century cannabis was one of the secret ingredients in several so called patent medicines. There were at least 2000 cannabis medicines prior to 1937, produced by over 280 manufacturers. The advent of the syringe and injectable medicines contributed to an eventual decline in the popularity of cannabis for therapeutic uses, as did the invention of new drugs such as aspirin.\n\nAn Irish physician, William Brooke O'Shaughnessy, is credited with introducing the therapeutic use of cannabis to Western medicine. He was Assistant-Surgeon and Professor of Chemistry at the Medical College of Calcutta, and conducted a cannabis experiment in the 1830s, first testing his preparations on animals, then administering them to patients to help treat muscle spasms, stomach cramps or general pain. Modern medical and scientific inquiry began with doctors like O'Shaughnessy and Moreau de Tours, who used it to treat melancholia and migraines, and as a sleeping aid, analgesic and anticonvulsant. At the local level authorities introduced various laws that required the mixtures that contained cannabis, that was not sold on prescription, must be marked with warning labels under the so-called poison laws. In 1905 Samuel Hopkins Adams published an exposé entitled \"The Great American Fraud\" in \"Collier's Weekly\" about the patent medicines that led to the passage of the first Pure Food and Drug Act in 1906. This statute did not ban the alcohol, narcotics, and stimulants in the medicines; rather, it required medicinal products to be labeled as such and curbed some of the more misleading, overstated, or fraudulent claims that previously appeared on labels.\n\nAt the turn of the 20th century the Scandinavian maltose- and cannabis-based drink \"Maltos-Cannabis\" was widely available in Denmark and Norway. Promoted as \"an excellent lunch drink, especially for children and young people\", the product had won a prize at the Exposition Internationale d'Anvers in 1894. A Swedish encyclopedia from 1912 claim that European hemp, the raw material for Maltos-Sugar, almost lacked the narcotic effect that is typical for Indian hemp and that products from Indian hemp was abandon by modern science for medical use. Maltos-Cannabis was promoted with text about its content of maltose sugar.\n\nLater in the century, researchers investigating methods of detecting cannabis intoxication discovered that smoking the drug reduced intraocular pressure. In 1955 the antibacterial effects were described at the Palacký University of Olomouc. Since 1971 Lumír Ondřej Hanuš was growing cannabis for his scientific research on two large fields in authority of the University. The marijuana extracts were then used at the University hospital as a cure for aphthae and haze. In 1973 physician Tod H. Mikuriya reignited the debate concerning cannabis as medicine when he published \"Marijuana Medical Papers\". High intraocular pressure causes blindness in glaucoma patients, so he hypothesized that using the drug could prevent blindness in patients. Many Vietnam War veterans also found that the drug prevented muscle spasms caused by spinal injuries suffered in battle.\n\nIn 1964, Dr. Albert Lockhart and Manley West began studying the health effects of traditional cannabis use in Jamaican communities. They discovered that Rastafarians had unusually low glaucoma rates and local fishermen were washing their eyes with cannabis extract in the belief that it would improve their sight. Lockhart and West developed, and in 1987 gained permission to market, the pharmaceutical Canasol: one of the first cannabis extracts. They continued to work with cannabis, developing more pharmaceuticals and eventually receiving the Jamaican Order of Merit for their work.\n\nLater, in the 1970s, a synthetic version of THC was produced and approved for use in the United States as the drug Marinol. It was delivered as a capsule, to be swallowed. Patients complained that the violent nausea associated with chemotherapy made swallowing capsules difficult. Further, along with ingested cannabis, capsules are harder to dose-titrate accurately than smoked cannabis because their onset of action is so much slower. Smoking has remained the route of choice for many patients because its onset of action provides almost immediate relief from symptoms and because that fast onset greatly simplifies titration. For these reasons, and because of the difficulties arising from the way cannabinoids are metabolized after being ingested, oral dosing is probably the least satisfactory route for cannabis administration. Relatedly, some studies have indicated that at least some of the beneficial effects that cannabis can provide may derive from synergy among the multiplicity of cannabinoids and other chemicals present in the dried plant material.\n\nVoters in eight U.S. states showed their support for cannabis prescriptions or recommendations given by physicians between 1996 and 1999, including Alaska, Arizona, California, Colorado, Maine, Michigan, Nevada, Oregon, and Washington, going against policies of the federal government. In May 2001, \"The Chronic Cannabis Use in the Compassionate Investigational New Drug Program: An Examination of Benefits and Adverse Effects of Legal Clinical Cannabis\" (Russo, Mathre, Byrne et al.) was completed. This three-day examination of major body functions of four of the five living US federal cannabis patients found \"mild pulmonary changes\" in two patients.\n\nAmong the more than 108,000 persons in Colorado who in 2012 had received a certificate to use marijuana for medical purposes, 94% said that severe pain was the reason for the requested certificate, followed by 3% for cancer and 1% for HIV/Aids. The typical card holder was a 41-year-old male. Twelve doctors had issued 50% of the certificates. Opponents of the card system claim that most card holders are drug abusers who are faking or exaggerating their illnesses; three-fourths male patients is not the normal pattern for pain patients, it is the normal pattern for drug addicts, claim the critics. After the implementation of medical cannabis in Colorado has also the past 30-day use of marijuana by teens increased significant compared with the average in the U.S. (Prescription to teens is not allowed in Colorado).\n\n\n"}
{"id": "2928275", "url": "https://en.wikipedia.org/wiki?curid=2928275", "title": "Horst Seehofer", "text": "Horst Seehofer\n\nHorst Lorenz Seehofer (born 4 July 1949) is a German politician serving as Leader of the Christian Social Union (CSU) since 2008 and Minister of the Interior, Building and Community since 2018 under Chancellor Angela Merkel. From 2008 to 2018, he was Minister President of Bavaria; he also served as President of the Bundesrat between 2011 and 2012.\n\nFirst elected to the Bundestag in 1980, he served as Federal Minister for Health and Social Security from 1992 to 1998 and as Federal Minister of Food, Agriculture and Consumer Protection in the cabinet of Angela Merkel from 2005 to 2008. In October 2008 he became Leader of the CSU and the 18th Minister President of Bavaria. From 1 November 2011 until 31 October 2012 he served as President of the Bundesrat and \"ex officio\" deputy to the President of Germany. Because of that he was Acting President of Germany after the resignation of President Christian Wulff on 17 February 2012 and before the election of Joachim Gauck as Wulff's successor on 18 March 2012.\n\nA staunch opponent of Chancellor Angela Merkel's refugee policy, Seehofer has threatened to file a formal complaint against the Merkel government's refugee policy with Germany's Constitutional Court and is a proponent of a federal cap on the number of refugees the German government is to take in. \nAfter secondary school, Seehofer started working as civil servant in the local administration in Ingolstadt.\n\nSeehofer served as member of the German federal parliament (Bundestag) from 1980 until 2008.\n\nSeehofer was Federal Minister for Health and Social Security from 1992 to 1998 in the cabinet of Chancellor Helmut Kohl.\n\nIn 1993, Seehofer ordered that Germany's 117-year-old Federal Health Agency be dissolved following a review of how the government in the 1980s handled the cases of thousands of hemophiliacs who were infected through blood contaminated with HIV. The Health Ministry took over the agency's responsibilities. Also, Seehofer announced that Germany would contribute to an emergency fund for victims of the scandal. In the context of the crisis, he came under considerable pressure to resign.\nSeehofer became deputy chairman of the CDU/CSU parliamentary group of the Bundestag in October 1998, which was led at the time by Wolfgang Schäuble. He served as Health Minister Ulla Schmidt's counterpart in negotiating the cross-party healthcare bill of 2003. Because of his disagreement with CDU leader Angela Merkel on flat-rate contributions (\"Gesundheitsprämie\") to the federal health insurance he resigned from his post on 22 November 2004 but remained the deputy chairman of the CSU and kept his mandate. After joining the \"Bundestag\" Seehofer kept his mandate as a directly elected delegate (\"Direktkandidat\") from his Constituency Ingolstadt. At the 2005 federal election he received 65.9 percent of the votes in his district.\n\nSeehofer was appointed Federal Minister of Food, Agriculture and Consumer Protection in the cabinet of Angela Merkel and stayed in office from 2005 to 2008.\n\nAfter his party lost more than 17% of the popular vote in the Bavarian state elections of 2008, incumbent Minister-President Günther Beckstein and Chairman of the CSU, Erwin Huber, announced their resignations. Seehofer was quickly proposed as their successor. At a party convention on 25 October he was affirmed as the new Chairman of the CSU with 90% of the votes, and on 27 October he was elected Minister-President by the Landtag with votes from the Free Democratic Party, forming the first coalition government in Bavaria since 1962.\n\nDuring the term 2011–2012, Seehofer served as President of the German Bundesrat. As such, he acted as Acting President of Germany between Christian Wulff's resignation on 17 February 2012 and the election of Joachim Gauck on 18 March 2012.\n\nUnder Seehofer's leadership, the State of Bavaria took to the Federal Constitutional Court in 2012 in order to dispute the legality of Germany's post-World War II system of financial redistribution among the country's 16 states. Bavaria, a beneficiary of the system until 1988, had paid more in 2011 than it got out in the 40 years it was a net recipient. The State of Hesse, another per-capita contributor, joined the lawsuit.\n\nAlso under Seehofer's leadership, the CSU won an absolute majority in the 2013 state elections, heralding strong momentum for the conservative parties in the federal elections the following week. Together with Angela Merkel and Sigmar Gabriel, he later led the negotiations to form a coalition government on the national level. In late 2013, Seehofer won a record 95.3 percent of the party's votes to continue as chairman.\n\nIn early 2015, under pressure from younger rivals, Seehofer announced he would retire at the next state elections in 2018. Later that year, when was chosen the fifth time as leader of the CSU, he received 87.2 percent of the vote, some 8 percent down on the result he achieved in 2013.\nIn August 2016, Seehofer said he may break with party unity and run a separate campaign in the 2017 national elections, a move widely seen as an effort to keep pressure on Merkel to shift to a more restrictive refugee policy in the European migrant crisis. He also announced to stay on as CSU leader beyond 2018. When the CSU's share of the vote in Bavaria fell 10 percentage points compared with 2013, to below 39 per cent, Seehofer faced demands to resign. On 4 December 2017, he announced to step down as Minister President and not running as leading candidate in the 2018 state elections; instead, he said he would hand over the office to Markus Söder in the first quarter of 2018.\n\nOn 1 March 2018, Seehofer confirmed that he will be in Merkel's cabinet if the SPD party members vote in favour of the coalition. He took over the role of Interior Minister. A policy Seehofer announced is that he has a \"master plan for faster asylum procedures, and more consistent deportations.\" He wants a \"zero tolerance\" policy toward criminals. On 15 March 2018, Seehoher stated that he disagreed with the belief that the Islamic faith is part of German culture. He noted that certain public holidays correspond to certain church holidays.\n\nIn June 2018, Seehofer backed down from a threat to bypass Chancellor Angela Merkel in a disagreement over immigration policy Until she comes back on July 1 during while she attempts to find a solution at European level.\n\nUnder Seehofer plan Germany would rejected migrants who have already been deported or have an entry ban and would instruct police to turn away all migrants who have registered elsewhere in the EU.\n\nOn 1 July 2018, Seehofer offered to resign after rejecting Chancellor Angela Merkel's EU migration deal. On 2 July 2018, however, Seehofer and Merkel announced they had settled their differences and agreed to instead accept a compromise of tighter border control. As a result of the agreement, Seehofer agreed to not resign.\n\nIn 2010, remarks made by Seehofer according to which Turkish and Arab migrants were no longer needed in Germany were strongly criticized by the Turkish community and by Chancellor Angela Merkel's government.\n\nIn 2011, Seehofer took the debate further when he said those who wanted to stay in Germany should be ready to sign up to German values. He proposed a change to the Bavarian Constitution so that the authorities in the state would be under obligation to help with the integration process but that minorities, too, should be prepared to actively support the integration process.\n\nIn late 2015, Seehofer and the CSU sharply criticized Chancellor Angela Merkel's refugee policy, as the party's home turf of Bavaria was the main entry point for refugees and other migrants arriving in Germany. Under pressure from Seehofer and his allies, Merkel later restricted cash benefits for refugees and added Kosovo, Albania and Montenegro to the list of \"safe\" countries to which migrants can be returned. He repeatedly called on the federal government to set a cap on the number of refugees Germany should be taking in, saying that the country was able to manage only \"200,000 applicants [per year] for asylum … at the most.\" Seehofer later threatened to file a complaint against the government's refugee policy with Germany's Constitutional Court.\n\nSeehofer is opposed to Turkey's becoming a member of the European Union. In 2009, he stated that Turkey \"as a self-proclaimed representative of the Muslim world, clearly doesn't fit in\".\n\nIn December 2010 and November 2011, Seehofer was the first Minister-President of Bavaria who visited the neighbouring Czech Republic; this was considered an important step in the dispute over the expulsion of the Sudeten Germans after the Second World War. In February 2013, Seehofer received Petr Nečas as the first Czech Prime Minister for an official visit to Bavaria.\n\nIn an interview with news magazine \"Der Spiegel\" in late 2014, Seehofer warned Germany's foreign minister Frank-Walter Steinmeier and his fellow Social Democrats (SPD) against pursuing a more friendly approach towards Russia in the Ukrainian crisis, arguing that \"if Mr. Steinmeier is pursuing his own form of diplomacy alongside the chancellor, that would be highly dangerous.\" He added that, even within his own party, there was already too much friendly sentiment towards Russia that had to be kept in check. However, in 2015, he held that it would be \"Realpolitik\" to try to involve Russia in tackling global crises. In early 2016, his joint visit with Edmund Stoiber to Moscow for talks with Russian President Vladimir Putin was met by harsh criticism, even from CDU politicians. By early 2017, Seehofer reiterated his calls to lift the EU sanctions against Russia.\n\nIn September 2018, a few days after Chemnitz protests against migrants and refugees, Seehofer criticized the debate on migration again saying it is \"the mother of all political problems\" in Germany.\n\nIn 2012, Seehofer demanded that the German constitution be changed to permit referendums on decisions to deepen European integration and transfer powers to European institutions. That same year, he criticized International Monetary Fund managing director Christine Lagarde's proposal for measures that would result in a mutualization of Eurozone debt, arguing that shared liability for sovereign debt and a banking union would remove pressure from governments to carry out economic policy changes.\n\nIn 2013, Seehofer made Peter Gauweiler a deputy leader of the CSU in a bid to court the party's euro critics; however, Gauweiler quit after two years in protest against the extension of Greece's aid program.\n\n\n\n\nSeehofer is married to Karin Seehofer and resides in the Ingolstadt district of Gerolfing. A father of three, Seehofer failed in a 2007 bid for the CSU leadership when it emerged that he had a daughter born out of wedlock, from an extramarital affair with a much younger staffer of the German Bundestag. After a period of indecision, he opted to return to his wife.\n\nIn 2002, Seehofer survived a serious myocarditis. His health again became a subject of public debate when he collapsed during a speech at a party event in early 2015.\n\n"}
{"id": "22026602", "url": "https://en.wikipedia.org/wiki?curid=22026602", "title": "List of inventors killed by their own inventions", "text": "List of inventors killed by their own inventions\n\nThis is a list of inventors whose deaths were in some manner caused by or related to a product, process, procedure, or other innovation that they invented or designed.\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "2639544", "url": "https://en.wikipedia.org/wiki?curid=2639544", "title": "List of least carbon efficient power stations", "text": "List of least carbon efficient power stations\n\nThis is a list of least carbon efficient power stations in selected countries. Lists were created by the WWF and lists the most polluting power stations in terms of the level of carbon dioxide produced per unit of electricity generated. In 2005 WWF created list of power stations from 30 industrialised countries, also list for EU, in 2007 WWF published updated EU list. In 2009 European Commission list with absolute emissions only, also in 2014 Climate Action Network Europe, WWF, European Environmental Bureau, Health and Environment Alliance and Climate Alliance Germany.\n\nIn 2015 the Stranded Assets Programme at the University of Oxford’s Smith School of Enterprise and the Environment published Stranded Assets and Subcritical Coal report analyzing inter alia carbon intensity of subcritical coal-fired power stations of 100 largest companies having these power stations.\n"}
{"id": "22704344", "url": "https://en.wikipedia.org/wiki?curid=22704344", "title": "Master of Medicine", "text": "Master of Medicine\n\nMaster of Medicine (MMed) is a postgraduate academic degree awarded by medical schools to physicians following a period of instruction and examination. The degree is awarded by both surgical and medical subspecialties and usually includes a dissertation component involving original research. The degree may complement an existing fellowship in the chosen specialty or be the sole qualification necessary for registration as a specialist.\n\nThe following universities in the following countries are known to award MMed degrees, in the following subjects, as of May 2009:\n\n\n\n\n\nThe following MMed courses are available at Texila American University:\n\n\nVarious MMed courses are available at University of Nairobi:\n\n\n\n\nThe Master of Medicine (MMed) is conferred by the National University of Singapore (NUS) Division of Graduate Medical Studies.\n\nThe following MMed courses are available at the University of Cape Town\n\nThe following MMed courses are available at the University of Pretoria\n\n\nThe following MMed courses are available at Muhimbili University:\n\n\nThe following MMed courses are available at Gulu University School of Medicine:\n\n\nThe following MMed courses are available at Makerere University School of Medicine:\n\n\nThe following MMed courses are available at Mbarara University School of Medicine:\n\n\nThe following MMed courses are available at Uganda Martyrs University School of Medicine:\n\n\nThe following MMed courses are available at copperbelt university:\n\n\nThe following MMed courses are available at University of Zimbabwe:\n\n\n\n"}
{"id": "1503460", "url": "https://en.wikipedia.org/wiki?curid=1503460", "title": "McNeill's law", "text": "McNeill's law\n\nIn human geography, McNeill's law is the process outlined in William H. McNeill's book \"Plagues and Peoples\". The process described concerns the role of microbial disease in the conquering of people-groups.\n\nAccording to McNeill's Law, the microbiological aspect of conquest and invasion has been the deciding principle or one of the deciding principles in both the expansion of certain empires (as during the emigration to the Americas) and the containment in others (as during the crusades).\n\nThe first people-group fully wiped out due to European expansion (with the possible exception of the Arawaks) was the Guanches of the Canary Islands. Despite an inbred ferocity, superior knowledge of the land and even a possible tactical superiority, they were eventually wiped out through the concentrated efforts of the Spanish and Portuguese. McNeill's Law would place the deciding factor squarely on the introduction of deadly diseases and parasites from the mainland to the previously geographically isolated islanders.\n\nThis is the likely explanation, as what records still exist show numerous deaths by disease on the islands and a declining birth-rate, leading eventually to the almost complete end of the Guanches as a race.\n\nOther instances include the devastation of the Incas by smallpox.\n"}
{"id": "21919496", "url": "https://en.wikipedia.org/wiki?curid=21919496", "title": "Menopause (journal)", "text": "Menopause (journal)\n\nMenopause is a monthly peer-reviewed medical journal covering all aspects of gynecology dealing with topics related to menopause. It was established in 1994 and is published by Lippincott Williams & Wilkins. The editor-in-chief is Isaac Schiff (Harvard Medical School). It is an official journal of the North American Menopause Society. According to the \"Journal Citation Reports\", the journal has a 2014 impact factor of 3.361.\n"}
{"id": "984232", "url": "https://en.wikipedia.org/wiki?curid=984232", "title": "Methylcholanthrene", "text": "Methylcholanthrene\n\nMethylcholanthrene is a highly carcinogenic polycyclic aromatic hydrocarbon produced by burning organic compounds at very high temperatures. Methylcholanthrene is also known as 3-methylcholanthrene, 20-methylcholanthrene or the IUPAC name 3-methyl-1,2-dyhydrobenzo[j]aceanthrylene. The short notation often used is 3-MC or MCA. This compound forms pale yellow solid crystals when crystallized from benzene and ether. It has a melting point around 180 °C and its boiling point is around 280 °C at a pressure of 80 mmHg. Methylcholanthrene is used in laboratory studies of chemical carcinogenesis. It is an alkylated derivative of benz[\"a\"]anthracene and has a similar UV spectrum. The most common isomer is 3-methylcholanthrene, although the methyl group can occur in other places.\n\n3-Methylcholanthrene, a known carcinogen which builds up in the prostate due to cholesterol breakdown, is implicated in prostate cancer. It \"readily produces\" primary sarcomas in mice.\n\nIn 1933, the first article about methylcholanthrene was published. Here they described the synthesis of the compound. Not many years later, it became clear that this compound had toxic properties to humans and animals. Therefore, a lot of interest was shown in the compound and it was used often in toxicological research. Methylcholanthrene is often tested on mice and rats to derive information for cancer medicine development. Due to the influence of the compound on the central nervous system, its responses and change in response are compared. It is also known that due to genetic mutations, the compound causes cancer cells to develop. \nIn 1982, the last article appeared on the synthesis of methylcholanthrene. The yield of 93% was reached and therefore no further adjustments were made to the synthesis scheme.\n\nFirst 3-MC was synthesized with the method of reference. Later the synthesis of the compound was improved. The synthesis of 3-MC consists of a few steps, visualized in figure 1; the first step is the key to success for the synthesis. 4-methylindanone (1) reacts in condensation with lithium salt of N,N-diethyl-1-naphthamide (2). At -60 ̊C the reaction of 1 and 2 afforded evenly to the lactone (3), the carbonyl addition product which underwent conversion on treatment with acid. The free acid (4) was obtained when the latter was cleaved reductively with zinc and alkali. Cyclization of the product occurred when treated with ZnCl in acetic acid anhydride and gave the compound 6-acetoxy-3-MC (5). Reducing this product with hydriodic acid in propionic acid resulted in 3-MC.\n\n3-MC has an inhibitory function in a dimethylnitrosamine demethylase process in rat livers. Inhibition could happen on by interfering in demethylase conformations or by interfering in synthesis and/or degradation of demethylase. Experiments showed that the Km doesn’t change after 3-MC treatment. This strongly indicates that enzyme affinity is not influenced by 3-MC. Instead, incubation with 3-MC leads to a decrease in the amount of enzyme activity. These results point towards inhibition of demethylase synthesis and/or induction of demetylase degradation. Unpublished observations of Venkatesan, Argus and Arcos suggest that demethylase synthesis inhibition is most plausible.\n\nA possible mechanism for this reaction is depicted in figure (2). 3-MC is metabolized, via epoxidation, hydrolysis and another epoxidation, to a very reactive epoxide. Epoxidations are realized by the enzyme cytochrome P450. The second epoxide is not hydrolysed immediately because it is localized next to a bay region, which shields the epoxide. This way, the metabolite is able to travel and bind to DNA in figure (2). The mechanism is derived from the binding mechanism of benzo[\"a\"]pyrene to DNA. This is likely because it is plausible that two polycyclic aromatic hydrocarbons are metabolized via the same pathway. Deoxyguanosine is used in the figure, since that base appears to be bound to Benzo[a]pyrene far more often than the other bases.\n\nThere appears to be an equilibrium in 3-MC-free and 3-MC-bound. It is hard to determine how when the equilibrium is formed due to difficulties with radioactive measurements. A probable saturating dosis is thought to be around 40 mg 3-MC/kg.\n\nResearch on the effect of 3-MC in rat uteri concludes that 3-MC acts as an estrogen antagonist. The sexhormone is, like 3-MC, a polycyclic aromatic hydrocarbon. 3-MC and estrogen bind to estrogenreceptors competitively, reducing the estrogen expression.\n\nMC is metabolized by rat liver microsomes into oxygenated forms which alkylate DNA. These oxygenated metabolites bind to double-stranded and single-stranded. Empirical data show that MC tends to bind mostly to G-bases8.\nWhen injected in lung, kidney or liver tissue of rats, it appears that the liver is able to reverse the MC-binding to DNA. Lung and kidney tissue are not capable of doing this, which may explain why MC is more carcinogenic in lungs and kidneys than in the liver. To be carcinogenic, the MC metabolite has to be covalently bound to DNA. Therefor it’s necessary for MC to be oxygenated in order to carcinogenic.\nInjected MC does not move away from the injection site. In a rat body MC has a half-life of about 4 weeks. After 8 weeks, 80% of MC is metabolized into water-soluble metabolites. MC and it’s metabolites mainly exit the body via feces (a ninefold more urine). Three months after injection with MC, 85% of the rats are reported to have tumors. 82% of the tumors is a form of spindle-cell sarcoma.\n\nMethylcholanthrene is often used to induce tumors in rodents for carcinogenesis and mutagenesis research. \nIn a study from 1991, lung precancerous and cancerous lesions were induced in Wistar rats by one intrabronchial injection of 3-MC. After 30 days, atypical hyperplasia of bronchiolar epithelium, adenoid hyperplasia or adenomas, and squamous cell carcinoma occurred in 15 (88.2%), 12 (70.6%) and 3 (17.7%) out of 17 rats respectively. After 60 days, the incidences were 15/18 (83.3%), 4/17 (23.5%) and 7/18 (38.9%). All of the precancerous lesions and carcinomas showed positive expression of gamma-glutamyltranspeptidase (GGT). \nJin et al. (2013) found that the cellular redox balance is altered by acute exposure to 3-MC. This causes the nuclear factor erythroid 2-related factor 2 (Nrf2)-regulated response pathway to induce antioxidant responses.\n\n3-MC is a ligand of the aryl hydrocarbon receptor (AhR), which stimulates transcription directed by xenobiotic response elements. AhR ligands can induce formation of an AhR-estrogen receptor (ER) complex. 3-MC was found to elicit estrogenic activity by this mechanism, and by stimulation of the expression of some endogenous ER target genes. \n3-MC may cause respiratory tract irritation, skin irritation or eye irritation.\n\n3-MC is mutagenic to human cells. Curren et al. (1978) were the first to report successfully induced mutations in human cells with 3-MC. Skin epithelial cells are thought to metabolize the compound to mutagenic products. \nThe ability to metabolize mutagens may express genetically regulated differences within a species such as man or mouse, causing environmental chemicals to show a different level of mutagenicity and carcinogenicity to specific individuals.\n\nThe administration of 3-MC to pregnant mice results in the formation of lung tumors in the offspring. Miller et al. (1990) compared the effects of fetal versus adult exposure to 3-MC on both induction of aryl hydrocarbon hydroxylase (AHH) activity in lung and dependence of lung tumorigenesis on the Ah genotype. A single ip injection (in inducible fetal lung supernatants) of 100 mg/kg of 3-MC to the mothers resulted in a maximal 50-fold induction of AHH activity by 8 hr, which persisted for 48 hr. The same injections to adult F1 mice revealed only a 4- to 7—fold increase in lung AHH activity, compared to the large fetal induction ratio.\n\n"}
{"id": "15955126", "url": "https://en.wikipedia.org/wiki?curid=15955126", "title": "Michael Healy (statistician)", "text": "Michael Healy (statistician)\n\nMichael John Romer Healy (26 November 1923 – 17 July 2016) was a British statistician known for his contributions to statistical computing, auxology, laboratory statistics and quality control, and methods for analysing longitudinal data, among other areas. He was professor of medical statistics at the London School of Hygiene and Tropical Medicine from 1977 until his retirement. The Royal Statistical Society awarded him the Guy Medal in Silver in 1979 and Gold in 1999, and he also acted as chairman of its medical section. He was the author or co-author of three books and over 200 scientific papers.\n\nHe died on 17 July 2016 at the age of 92.\n\n"}
{"id": "25444252", "url": "https://en.wikipedia.org/wiki?curid=25444252", "title": "Molecular processor", "text": "Molecular processor\n\nA molecular processor is a processor that is based on a molecular platform rather than on an inorganic semiconductor in integrated circuit format.\n\nMolecular processors are currently in their infancy and currently only a few exist. At present a basic molecular processor is any biological or chemical system that uses a complementary DNA (cDNA) template to form a long chain amino acid molecule. A key factor that differentiates molecular processors is \"the ability to control output\" of protein or peptide concentration as a function of time. Simple formation of a molecule becomes the task of a chemical reaction, bioreactor or other polymerization technology. Current molecular processors take advantage of cellular processes to produce amino acide based proteins and peptides. The formation of a molecular processor currently involves integrating cDNA into the genome and should not replicate and re-insert, or be defined as a virus after insertion. Current molecular processors are replication incompetent, non-communicable and cannot be transmitted from cell to cell, animal to animal or human to human. All must have a method to terminate if implanted. The most effective methodology for insertion of cDNA (template with control mechanism) uses capsid technology to insert a payload into the genome. A viable molecular processor is one that dominates cellular function by re-task and or reassignment but does not terminate the cell. It will continuously produce protein or produce on demand and have method to regulate dosage if qualifying as a \"drug delivery\" molecular processor. Potential applications range from up-regulation of functional CFTR in Cystic Fibrosis and Hemoglobin in Sickle Cell Anemia to angiogenesis in cardiovascular stenosis to account for protein deficiency (used in gene therapy.)\n\nA vector inserted to form a molecular processor is described in part. The objective was to promote angiogenesis, blood vessel formation and improve cardiovasculature. Vascular endothelial growth factor (VEGF) and enhanced green fluorescent protein (EGFP) cDNA was ligated to either side of an internal ribosomal re-entry site (IRES) to produce inline production of both the VEGF and EGFP proteins. After in vitro insertion and quantification of integrating units (IUs), engineered cells produce a bioluminescent marker and a chemotactic growth factor. In this instance, increased fluorescence of EGFP is used to show VEGF production in individual cells with active molecular processors. The production was exponential in nature and regulated through use of an integrating promoter, cell numbers, the number of integrated units (IUs) of molecular processors and or cell numbers. The measure the molecular processors efficacy was performed by FC/FACS to indirectly measure VEGF through fluorescence intensity. Proof of functional molecular processing was quantified by ELISA to show VEGF effect through chemotactic and angiogenesis models. The result involved directed assembly and coordination of endothelial cells for tubule formation by engineered cells on endothelial cells. The research goes on to show implantation and VEGF with dosage capabilities to promote revascularization, validating mechanisms of molecular processor control.\n\n"}
{"id": "5839596", "url": "https://en.wikipedia.org/wiki?curid=5839596", "title": "Mouth prop", "text": "Mouth prop\n\nA mouth prop (also bite block) is a wedge-shaped implement used in dentistry for dentists working with children and other patients who have difficulty keeping their mouths open wide and steady during a procedure, or during procedures where the patient is sedated. It has a rubber-like texture and is typically made from thermoplastic vulcanizate (TPV) material. They come in several different sizes, from pediatric to adult, and are typically ridged as to use the back teeth to hold them in place.\n\n"}
{"id": "1684475", "url": "https://en.wikipedia.org/wiki?curid=1684475", "title": "Panda crossing", "text": "Panda crossing\n\nThe panda crossing was a type of signal-controlled pedestrian crossing used in the United Kingdom from 1962 to 1967.\n\nIn the early-1960s, the British Ministry of Transport, headed by Ernest Marples, was looking for a way to make pedestrian crossings safer under increasingly heavy traffic conditions. The successful zebra crossing design was not considered safe enough for busy roads and could create traffic delays as pedestrians crossed whenever they wanted. Off-the-shelf light-controlled systems were available but were too expensive for widespread use. Some cities had innovated their own one-off crossings but the lack of standardisation was considered a safety issue. Furthermore, all existing signalled crossings tended to have two major drawbacks: stopping traffic for long periods of time and appearing to violate contemporary right-of-way law by signalling \"Don't cross\" to pedestrians (in reality: the 'Don't Cross' indication was not a legally enforceable instruction). \n\nThe panda crossing was introduced in 1962 as an attempt to combine the best features of available and experimental crossing systems. The first public example was opened on 2 April of that year outside London Waterloo railway station. The majority of the initial sites used for this experiment were in Guildford where all thirteen existing crossings were converted, and in Lincoln where ten crossings were converted. Further sites across England and Wales increased the size of the experiment to more than forty sites in all.\n\nThe layout was superficially similar to a traditional zebra crossing, with a painted area on the road announced by Belisha beacons. For distinction, the panda road pattern was different (triangles rather than stripes) and the beacons were striped, not plain. The main additions were the light signals on the beacon poles. The traffic signals consisted of two lamps, red and amber, while the pedestrians had a single signal displaying the word \"Cross\" when appropriate.\n\nIn the idle state, no lights were lit. A pedestrian wanting to cross would press a button on the beacon pole and be instructed to wait by an illuminated sign near the button. The system allowed for a pause between crossings in order to avoid traffic delays, and so the pedestrian might wait a while before anything happened. The amber traffic light would pulsate for a few seconds to inform motorists that someone was about to cross; a red light was then the signal to stop. At this point, the pedestrians' \"Cross\" signal began to flash. After a few seconds, the \"Cross\" light started to flash faster and the red traffic light was changed to a flashing amber (this \"flashing\" phase was considered distinct from the initial \"pulsating\" amber light).\n\nThe panda crossing deliberately omitted any sort of \"Don't cross\" message for pedestrians in order to avoid breaching the aforementioned right-of-way laws. The measured pause between crossings helped to keep traffic flowing. The light sequence also prevented long delays by allowing traffic to move after a few seconds if nobody was crossing. However, despite its apparent rationality, the design was not a success. In particular, the distinction between the flashing and pulsating amber phases was subtle yet highly significant.\n\nBy 1967, the panda crossing was a matter of concern for the Ministry of Transport, and so a new type of crossing, the X-way, was introduced. The new system was not phased in gradually by replacement, rather the pandas were removed seemingly as a matter of urgency. The replacement was so urgent that although the X-way lights replaced the panda crossing lights, the road initially retained the black-and-white triangular markings until they could be removed at a later date. The X-way itself soon disappeared when, in 1969, the modern-day pelican crossing was introduced.\n\n"}
{"id": "19638941", "url": "https://en.wikipedia.org/wiki?curid=19638941", "title": "Physiotherapists Tribunal", "text": "Physiotherapists Tribunal\n\nThe Physiotherapists Tribunal is a former tribunal established in the Australian state of New South Wales which dealt with appeals and complaints of professional misconduct by physiotherapists. \n\nOn 1 January 2014 the tribunal's functions were assumed by the newly established New South Wales Civil and Administrative Tribunal.\n\nThe tribunal generally heard matters after the Physiotherapists Board of New South Wales had made a decision, such as hearing an appeal against the cancellation of a physiotherapist’s registration. The tribunal heard matters in an informal manner in an attempt to do justice in the matter. The tribunal also conducted inquiries into complaints referred by the New South Wales Health Care Complaints Commission.\n\nIn common with other health professionals in New South Wales, physiotherapists are required to be registered. Boards, such as the Physiotherapists Board, are established to register those health professionals, as well as provide other support services to the public. Tribunals are established to deal with allegations of misconduct and to determine whether a health professional should be suspended or de-registered. In New South Wales, the tribunal is unique, as it is a separate tribunal specially set up for this health speciality. This is in contrast to other Australian States, such as Victoria, which have one super tribunal that deals with all health professionals.\n\nThe tribunal is established under section 100 of the Physiotherapists Act 2001 (NSW). The Governor of New South Wales may appoint an Australian lawyer to be the chairperson of the tribunal. The governor may also appoint any number of Australian lawyers to be deputy chairpersons of the tribunal. The appointment can be for up to seven years and the person can be re-appointed again for up to another seven years.\n\nThe tribunal has jurisdiction to hear:\n\nWhen either of these situations arises, the Physiotherapists Registration Board will inform the chairperson of the tribunal of the matter. The Board will also appoint three persons to sit on the tribunal. Two of the persons selected must be registered physiotherapists. Generally the board will select persons who are peers of the relevant person. The other person on the tribunal must be a lay person (i.e. a person who is not a physiotherapist). This latter person is selected from a panel established by the Minister for Health. The chairperson may personally hear the matter or may instead delegate the hearing to a deputy chairperson. The four persons together constitute the tribunal.\n\nThe jurisdiction of the Tribunal is protective and its purpose is to protect the public. The object is not punitive and the Tribunal is not there to punish the physiotherapist but is there to maintain proper standards in the profession.\n\nThe members of the tribunal are required to conduct an inquiry to any complaint, matter, application or appeal referred to the tribunal. The chairperson or deputy chairperson must decide when the inquiry is to be heard and must give at least fourteen days' notice to the physiotherapist the subject of the inquiry. The chairperson must also give notice to any person who made the complaint, the Department of Health, the Health Care Complaints Commission, and the Physiotherapists Board. Both the person who made the complaint, and the physiotherapist concerned, are entitled to be legally represented if they so wish.\nAny inquiry is open to the public, so members of the public or the press are able to observe the proceedings. The tribunal may conduct the inquiry in private if necessary. The tribunal has a range of powers to suppress the publication of information given at the inquiry. For example, the tribunal may prohibit the publication of the names of the witnesses, the complainant or the physiotherapist concerned. Where a person or organisation breaches this requirement, the person or organisation may be prosecuted for a criminal offence. The chairperson or deputy chairman decides all questions of law or matters of legal principle. Their decision is final on the point. In matters of fact or evidence, three of the four members must agree. If the decision is split two-two, then the chairperson or deputy chairperson has the casting vote. Generally the rules of evidence (which the limit the type of evidence which a court can recognise) do not need to be observed in the tribunal. However, because of the serious nature of the proceedings, they are generally heard with a high degree of formality. Witnesses are called, and their testimony is given on oath in the same way that evidence is given in a court of law. The tribunal is empowered to issue summons to compel people to attend the tribunal to give evidence. The tribunal may also direct a written notice for a person or organisation to produce documentation.\n\nIf the tribunal after an inquiry determines the matters proved against the physiotherapist, the tribunal has a number of options available to it. The tribunal may caution or only reprimand the person concerned. It may direct the person to seek medical or psychiatric treatment or counselling. It may order that the person complete an educational course. The tribunal may impose conditions on the continuing practice of the physiotherapist. In extreme cases, the tribunal may order the suspension or removal of the physiotherapist from practising. The tribunal is required to given written reasons of its decision within 30 days of the finalisation of the case. There is a right of appeal in certain circumstances to the Supreme Court of New South Wales against the decision of the tribunal. The tribunal heard one appeal in 2006.\n\nThe decisions of the tribunal are available on the internet beginning with decisions made in 2007. One of the tribunal’s first decisions concerning a complaint against a physiotherapist who found to be guilty of “unsatisfactory professional conduct”. The physiotherapist concerned had been convicted of making false claims for services under Workers Compensation laws in the Local Court of New South Wales. The physiotherapist was reprimanded and suspended for six months and ordered to have ongoing counselling.\n\nphysiotherapy at home \nhome physiotherapy in delhi\nphysiotherapy at home\n\n"}
{"id": "19454783", "url": "https://en.wikipedia.org/wiki?curid=19454783", "title": "Prevention through design", "text": "Prevention through design\n\nPrevention through design (PtD), also called safety by design usually in Europe, is the concept of applying methods to minimize occupational hazards early in the design process, with an emphasis on optimizing employee health and safety throughout the life cycle of materials and processes. It is a concept and movement that encourages construction or product designers to \"design out\" health and safety risks during design development. The concept supports the view that along with quality, programme and cost; safety is determined during the design stage. It increases the cost-effectiveness of enhancements to occupational safety and health.\n\nThis method for reducing workplace safety risks lessens workers' reliance on personal protective equipment, which is the least effective of the hierarchy of hazard control.\n\nEach year in the U.S., 55,000 people die from work-related injuries and diseases, 294,000 are made sick, and 3.8 million are injured. The annual direct and indirect costs have been estimated to range from $128 billion to $155 billion. Recent studies in Australia indicate that design is a significant contributor in 37% of work-related fatalities; therefore, the successful implementation of prevention through design concepts can have substantial impacts on worker health and safety.\n\nThe National Institute for Occupational Safety and Health (NIOSH) in the United States is a major contributor and promoter of PtD policy and guidelines. NIOSH considers PtD to be \"the most effective and reliable type\" of prevention of occupational injuries. A core tenet of PtD philosophy the concept of addressing workplace hazards using methods at the top of the Hierarch of Controls, namely elimination and substitution.\n\nWithin Europe, construction designers are legally bound to design out risks during design development to reduce hazards in the construction and end use phases via the Mobile Worksite Directive (also known as CDM regulations in the UK). The concept supports this legal requirement. Some Notified Bodies provide testing and design verification services to ensure compliance with the safety standards defined in regulation codes such as the American Society of Mechanical Engineers. Many non-governmental organizations have been established to support this aim, principally in the UK, Australia and the United States.\n\nWhile engineering as a rule factors human safety into the design process, a modern appraisal of specific links to design and workers' safety can be seen in efforts beginning in the 1800s. Trends included the widespread implementation of guards for machinery, controls for elevators, and boiler safety practices. This was followed by enhanced design for ventilation, enclosures, system monitors, lockout/tagout controls, and hearing protectors. More recently, there has been the development of chemical process safety, ergonomically engineered tools, chairs, and work stations, lifting devices, retractable needles, latex-free gloves, and a parade of other safety devices and processes.\n\nIn 2007, the National Institute for Occupational Health and Safety began its National Initiative on Prevention through Design with the goal of promoting prevention through design philosophy, practice, and policy.\n\nPrevention through design represents a shift in approach for on-the-job safety. It involves evaluating potential risks associated with processes, structures, equipment, and tools. It takes into consideration the construction, maintenance, decommissioning, and disposal or recycling of waste material.\nThe idea of redesigning job tasks and work environments has begun to gain momentum in business and government as a cost-effective means to enhance occupational safety and health. Many U.S. companies openly support PtD concepts and have developed management practices to implement them. Other countries are actively promoting PtD concepts as well. The United Kingdom began requiring construction companies, project owners, and architects to address safety and health during the design phase of projects in 1994. Australia developed the Australian National OHS Strategy 2002–2012, which set \"eliminating hazards at the design stage\" as one of five national priorities. As a result, the Australian Safety and Compensation Council (ASCC) developed the Safe Design National Strategy and Action Plans for Australia encompassing a wide range of design areas.\n\nThe National Institute for Occupational Safety and Health is a large contributor to prevention through design efforts in the United States. Several NIOSH initiatives and guidelines directly or indirectly advocate for PtD practices. Through NIOSH efforts, the U.S. Green Building Council posted new PtD credits available for Leadership in Energy and Environmental Design (LEED) certification for construction. Additionally, they provide a wide variety of educational and guidance materials on the topic of PtD The NIOSH \"Buy Quiet\" initiative uses elements of prevention through design to encourage companies to buy quieter machinery, thereby reducing occupational hearing loss for their workers.\n\n\n"}
{"id": "274035", "url": "https://en.wikipedia.org/wiki?curid=274035", "title": "Quantitative marketing research", "text": "Quantitative marketing research\n\nQuantitative marketing research is the application of quantitative research techniques to the field of marketing. It has roots in both the positivist view of the world, and the modern marketing viewpoint that marketing is an interactive process in which both the buyer and seller reach a satisfying agreement on the \"four Ps\" of marketing: Product, Price, Place (location) and Promotion.\n\nAs a social research method, it typically involves the construction of questionnaires and scales. People who respond (respondents) are asked to complete the survey. Marketers use the information to obtain and understand the needs of individuals in the marketplace, and to create strategies and marketing plans.\n\nSimply put, there are five major and important steps involved in the research process:\n\n\nA brief discussion on these steps is:\n\n\nThe design step may involve a pilot study in order to discover any hidden issues. The codification and analysis steps are typically performed by computer, using statistical software. The data collection steps, can in some instances be automated, but often require significant manpower to undertake. Interpretation is a skill mastered only by experience.\n\nThe data acquired for quantitative marketing research can be analysed by almost any of the range of techniques of statistical analysis, which can be broadly divided into descriptive statistics and statistical inference. An important set of techniques is that related to statistical surveys. In any instance, an appropriate type of statistical analysis should take account of the various types of error that may arise, as outlined below.\n\nResearch should be tested for reliability, generalizability, and validity.\n\nGeneralizability is the ability to make inferences from a sample to the population.\n\nReliability is the extent to which a measure will produce consistent results.\n\nValidity asks whether the research measured what it intended to.\n\nValidity implies reliability: A valid measure must be reliable. Reliability does not necessarily imply validity, however: A reliable measure does not imply that it is valid.\n\nRandom sampling errors:\nResearch design errors:\nInterviewer errors:\nRespondent errors:\nHypothesis errors:\n\n\n"}
{"id": "24221912", "url": "https://en.wikipedia.org/wiki?curid=24221912", "title": "Radiation dose reconstruction", "text": "Radiation dose reconstruction\n\nRadiation dose reconstruction refers to the process of estimating radiation doses that were received by individuals or populations in the past as a result of particular exposure situations of concern. The basic principle of radiation dose reconstruction is to characterize the radiation environment to which individuals have been exposed using available information. In cases where radiation exposures can not be fully characterized based on available data, default values based on reasonable scientific assumptions can be used as substitutes. The extent to which the default values are used depends on the purpose of the reconstruction(s) being undertaken.\n\nThe methods and techniques used in dose reconstructions have been growing and evolving rapidly. It wasn’t until the late 1970s that dose reconstruction emerged as a scientific discipline and it has been used in practice in the United States for the last two decades. The scientific methods and practices used to complete dose reconstructions are often based on the standards published by international consensus organizations such as the International Commission on Radiological Protection. \n\nWhen conducted properly, dose reconstruction is a scientifically valid process for estimating radiation dose received by an individual or group of individuals. It is commonly used in occupational epidemiological studies to determine the amount of radiation workers may have received as part of their employment. For these types of studies, dose reconstruction is similar to the process of estimating how much radiation current workers receive, for example at a nuclear facility, except dose reconstructions evaluate past exposures. The terms historical and retrospective often are used to describe a dose reconstruction. Dose estimation is the term sometimes used to describe the process used to determine radiation exposures to current populations or individuals. \n\nDose reconstruction methods have also commonly been applied in environmental settings to assess radionuclide releases into the environment from nuclear sites. One such environmentally focused study was published in 1983 by the U.S. Nuclear Regulatory Commission entitled Radiological Risk Assessment: A Textbook on Environmental Dose Analysis. This book was updated with major revisions in 2008 and it details the steps of radiological assessments, which uses similar methods and techniques as a dose reconstruction.\nDose reconstruction methods are not limited to just measuring exposures to radiation. Dose reconstruction principles can be used to reconstruct exposures to other hazardous materials and to determine the health effects of those toxins to populations or individuals.\n\nThe dose reconstruction process has several basic elements, which have been identified as follows:\n\n\"Summary of Basic Elements of Dose Reconstruction Process as found in A Review of the Dose Reconstruction Program of the Defense Threat Reduction Agency\"\n\nRadiation dose reconstruction methods are used to a large extent in occupational, environmental, and medical epidemiological research studies. \nThe Centers for Disease Control and Prevention (CDC) has been involved in several dose reconstruction projects. \n\nSeveral CDC agencies are involved in dose reconstruction projects: the Agency for Toxic Substances and Disease Registry (ATSDR), the National Center for Environmental Health (NCEH), and the National Institute for Occupational Safety and Health (NIOSH).\n\nThe Agency for Toxic Substances and Disease Registry (ATSDR) conducts dose reconstructions in relation to work done at Superfund sites. ATSDR defines exposure-dose reconstruction as an approach that uses computational models and other approximation techniques to estimate cumulative amounts of hazardous substances internalized by individuals presumed to be or who are actually at risk from contact with substances associated with hazardous waste sites.\n\nIn March 1993, ATSDR established the Exposure-Dose Reconstruction Program (EDRP). EDRP represents a coordinated, comprehensive effort to develop sensitive, integrated, science-based methods for improving health scientists’ and assessors’ access to current and historical exposure-dose characterization. EDRP was created to confront the challenge that faced health scientists and assessors who have not always had access to information-especially historical information regarding an individual’s direct measure of exposure to and dose of chemicals associated with hazardous waste sites.\n\nThe National Center for Environmental Health (NCEH) coordinates program and conducts environmental epidemiological health studies using dose reconstruction principles. NCEH has undertaken a series of studies to assess the possible health consequences of off-site emissions of radioactive materials from DOE-managed nuclear facilities in the United States. Dose reconstruction as used by NCEH is defined as the process of estimating doses to the public from past releases to the environment of radionuclides or chemicals. These doses form the basis for estimating health risks. Past exposures are the focus of the NCEH studies.\n\nThe National Institute for Occupational Safety and Health (NIOSH) completes dose reconstructions as a component of ongoing worker health studies. The NIOSH Occupational Energy Research Program’s mission is to conduct relevant, unbiased research to identify and quantify health effects among workers exposed to ionizing radiation and other agents; to develop and refine exposure assessment methods; to effectively communicate study results to workers, scientists, and the public; to contribute scientific information for the prevention of occupational injury and illness; and to adhere to the highest standards of professional ethics and concern for workers’ health, safety and privacy.\n\nOne of the largest mass applications of individual dose reconstruction principles is also being undertaken by NIOSH. NIOSH is the designated agency responsible for completing radiation dose reconstructions for individuals under the Energy Employees Occupational Illness Compensation Program of 2000 (the Act). Under the Act, individuals, and in some cases their survivors, are eligible for compensation for specified illnesses they received from occupational exposures to beryllium, asbestos, toxic materials, and radiation if they worked at a covered Department of Energy (DOE) facility or a facility that contracted with DOE to produce nuclear weapons or components, known as Atomic Weapons Employers (AWE). The program is administered by the Department of Labor. NIOSH’s responsibility under the Act is to determine the probability that an individual’s cancer was a result of their occupational radiation exposure at a DOE or AWE facility. This probability is determined by DOL and is based on the radiation dose reconstruction completed by NIOSH. The dose reconstructions are completed by individuals trained in the field of health physics. \n\nThe science behind the NIOSH dose reconstruction process has been published in the peer-reviewed professional journal Health Physics: The Radiation Safety Journal in July 2008. This edition of the Journal was dedicated entirely to the NIOSH Radiation Dose Reconstruction Program.\n\nThe Department of Veterans Affairs uses dose reconstructions to process claims under the Nuclear Test Personnel Review (NTPR) program. The NTPR is a Department of Defense program that works to confirm veteran participation in U.S. atmospheric nuclear tests from 1945 to 1962, and the occupation forces of Hiroshima and Nagasaki, Japan. If the veteran is a confirmed participant of these events, NTPR may provide either an actual or estimated radiation dose received by the veteran. The Defense Threat Reduction Agency completes the dose reconstructions for the NTPR program.\n\n"}
{"id": "51369596", "url": "https://en.wikipedia.org/wiki?curid=51369596", "title": "Reference price", "text": "Reference price\n\nA reference price (RP) is the price that a purchaser announces that it is willing to pay for a good or service. It is used by high-volume purchasers to inform suppliers.\n\nRP requires consumers to have access to price and quality information, which is not general practice in many industries. Further, it does not help consumers with urgent needs, cognitive and/or other impairments.\n\nReference pricing requires sufficient competition. Otherwise, consumers have no choice about providers, who in turn face less pricing pressure. Reference pricing could encourage lower quality.\n\nSome insurers use reference pricing to reduce their provider costs. The insurer announces prices that it is willing to pay for specific surgical procedures, pharmaceuticals and other services. If the provider charges a higher price, the patient is responsible for the balance. One study estimated that about 40 percent of health care spending is for services for which patients could shop. Appropriate services include hip and knee replacement, colonoscopy, magnetic resonance imaging (MRI) of the spine, computerized tomography (CT) scan of the head or brain, nuclear stress test of the heart, and echocardiogram because they have relatively uniform protocols and are less likely to experience variation in quality.\n\nIn health care, a more common alternative is to limit patients to a specific network of providers who have accepted the pricing and other terms specified by the insurer. RP provides consumers a broader choice of providers. Further, some large employers contract with regional \"centers of excellence,\" such as Cleveland Clinic.\n\nOn May 2, 2014, the Obama administration published its approval for large/self-insured firms to use RP for health care services and to use the price of generic drugs as the RP for other drugs, which would encourage patients to favor generics over non-generics. Costs that exceed the RP are not counted for ACA's out-of-pocket limits.\n\nIn 2011 California Public Employees' Retirement System (CalPERS) adopted reference pricing for 450,000 members. The approach was used for knee and hip replacement surgery, arthroscopy, colonoscopies, cataract removal surgery and other elective procedures. For example, the RP for a knee or hip replacement surgery was $30,000. As before, the patient was responsible for 20 percent of the first $15,000, paying a maximum of $3,000. The insurer covered the next $15,000. However, if the procedure cost $40,000, the patient was responsible for the final $10,000. Initially 41 of the hundreds of hospitals provided knee and hip replacement procedures at or below the RP with acceptable quality. Some hospitals charged more than $100,000. Multiple studies found that patients sought out the lower cost facilities. Prices for knee and hip replacements and cataract surgery fell by an average 20%. Other procedures fell by similar percentages. Many higher price facilities lowered their prices. Quality was apparently unaffected. By contrast, prices paid by employer-sponsored plans rose by about 5.5 percent.\n\nCalPERS reduced cost-sharing for patients who chose a (lower cost) outpatient surgical center, instead of a (higher cost) hospital.\n\nThe Canadian province of Alberta operates an RP scheme for natural gas. It is a monthly, weighted average of the consumer price in Alberta and a price at the Alberta border, with transporting and marketing adjustments. The system became effective in 1994.\n"}
{"id": "28193791", "url": "https://en.wikipedia.org/wiki?curid=28193791", "title": "Restaurant Nora", "text": "Restaurant Nora\n\nRestaurant Nora is America's first certified organic restaurant, located in Washington, D.C..\n\nTo gain the certification, Chef Nora Pouillon proved that at least 95 percent of the ingredients used—including meats, vegetables, dairy, flour, coffee, chocolate, and oils—were from certified organic farmers and suppliers. As a result, the restaurant's menu changes seasonally.\n\nIn January 2010, President Barack Obama held a surprise birthday party for First Lady Michelle Obama at the restaurant. Restaurant Nora has held lunches, dinners and events for dignitaries, congressional members and White House administrations, and was a favorite of Hillary Clinton.\n\n"}
{"id": "9398143", "url": "https://en.wikipedia.org/wiki?curid=9398143", "title": "Rostokino Aqueduct", "text": "Rostokino Aqueduct\n\nRostokino Aqueduct, also known as Millionny Bridge, is a stone aqueduct over Yauza river in Rostokino District of Moscow, Russia, built in 1780-1804. It is the only remaining aqueduct in Moscow, once a part of Mytishchi Water Supply, Moscow's first centralized water utility.\n\nThe aqueduct was commissioned by Catherine II of Russia to engineer Friedrich Wilhelm Bauer. Catherine authorized 1.1 million rouble expenditure and 400 soldiers. Builders used second-hand stone left by demolition of Bely Gorod fortifications. Construction, frequently interrupted, dragged for 25 years, as the soldier engineers were summoned to the war with Turkey (1787-1792) and various other jobs. In the process, both Catherine and Bauer died. Colonel Ivan Gerard lead the project after Bauer's death in 1783. Catherine's son, Paul I, had to issue 400,000 roubles financing; Alexander I added 200,000 roubles. Finally, the aqueduct was completed at an unprecedented cost of 2 million roubles, thus the name \"Millionny Bridge\".\n\nThe total length is , the height over Yauza river level is . There are 21 arches, each spans . Original masonry water canal wide and high was replaced with iron pipe in the 1850s.\n\nWater flowed naturally through a 20-kilometre masonry canal to a system of fountain taps; in 1892, the system was uprated with construction of pumps and water reservoires in the city. Mytishchi Water Supply was closed in 1937, when a superior water supply, a part of Moscow Canal project, was completed.\n\nA smaller, single-span aqueduct over Ichka river was built during the 1888-1892 modernization of Mytishchi system. It was destroyed in 1997 to make way for MKAD highway.\n\n"}
{"id": "3160457", "url": "https://en.wikipedia.org/wiki?curid=3160457", "title": "Royal College of Surgeons", "text": "Royal College of Surgeons\n\nA Royal College of Surgeons or Royal Surgical College is a type of organisation found in many present and former members of the Commonwealth of Nations. These organisations are responsible for training surgeons and setting their examinations. In this context, the term chartered implies the awarding of a Royal charter.\n\nThe origins of the first Royal College of Surgeons go back to the fourteenth century with the foundation of the Guild of Surgeons Within the City of London. There was dispute between the surgeons and barber surgeons until an agreement was signed between them in 1493, giving the fellowship of surgeons the power of incorporation. The Guild of Barbers of Dublin received a Royal Charter of Henry Vi in 1446, making it the earliest Royal Medical incorporation in Britain or Ireland. This was followed in 1505 by the incorporation of the Barber Surgeons of Edinburgh as a Craft Guild of Edinburgh. This body was granted a royal charter in 1506 by King James IV of Scotland. It was followed by the Royal College of Physicians and Surgeons of Glasgow, royally chartered by James VI in 1599, as the Glasgow Faculty.\n\nThe union in London was formalised further in 1540 by Henry VIII of England between the Worshipful Company of Barbers (incorporated 1462) and the Guild of Surgeons to form the Company of Barber-Surgeons. In 1745 the surgeons broke away from the barbers to form the Company of Surgeons. In 1800 the Company was granted a Royal Charter to become the Royal College of Surgeons in London. A further charter in 1843 granted it the present title of the Royal College of Surgeons of England. In 2010, Prof. Eilis McGovern became President of the Royal College of Surgeons in Ireland and thereby the first female President of any surgical Royal College in the world.\n\n\n"}
{"id": "44442017", "url": "https://en.wikipedia.org/wiki?curid=44442017", "title": "Sleep and weight", "text": "Sleep and weight\n\nBaseline levels of insulin do not signal muscle and fat cells to absorb glucose. When glucose levels are elevated, the pancreas responds by releasing insulin. Blood sugar will then rapidly drop. This can progress to type 2 diabetes.\n\nSleep loss can affect the basic metabolic functions of storing carbohydrates and regulating hormones. Reduction of sleep from eight hours to four hours produces changes in glucose tolerance and endocrine function. Researchers from the University of Chicago Medical Center followed 11 healthy young men for 16 consecutive nights. The first 3 nights, the young men slept for the normal 8 hours. The next 6 nights, they slept for 4 hours. The next 7 nights, they spent 12 hours in bed. They all had the same diet. They found that there were changes in glucose metabolism that resemble that of type 2 diabetes patients. When the participants were tested after sleep deprivation, they took 40% longer than normal to regulate blood sugar levels after a high-carbohydrate meals. The secretion of insulin and the body's response to insulin decrease by 30%. Sleep deprivation also alters the productions of hormones, lowering the secretion of thyroid stimulating hormone and increasing blood levels of cortisol.\n\nIt has also been shown that when slow-wave sleep was suppressed for three nights, young healthy subjects were 25% less sensitive to insulin. They needed more insulin to get rid of the same amount of glucose. If the body does not release more insulin to compensate, the blood-glucose levels will increase. This resembles impaired glucose tolerance, which can result in type 2 diabetes.\n\nLack of sleep has been strongly associated with weight gain in a variety of studies across all ages, though research suggests children and adolescents are particularly vulnerable. Sleep deprivation is believed to influence the brain's response to high-calorie food, making it more attractive, while also affecting the production of hormones that control appetite.\n\nMatthew P Walker, a psychology and neuroscience professor at UC Berkley, published a study during which the participants were deprived of sleep for one night. The New York Times summarized his study as such, \"On days when the subjects had not had proper sleep, fattening foods like potato chips and sweets stimulated stronger responses in a part of the brain that helps govern the motivation to eat. But at the same time, the subjects experienced a sharp reduction in activity in the frontal cortex, a higher level part of the brain where consequences are weighted and rational decisions are made.” A brain that has been deprived of sleep for one night is more likely to respond more intensely to junk food but also has the decreased ability to curb that desire. These results were consistent even as the subjects were given extra calories to compensate for the amount of energy expended during those extra hours that the subjects stayed awake, which indicates that one’s craving for junk food is not a response to offset an energy deficit. Walker further speculates that one of the biological basis for this reaction could the buildup of adenosine, a metabolic byproduct that may degrade communication between networks in the brain. Adenosine is cleared from the brain during sleep.\n\nPrevious research by the University of Chicago had also associated short sleep with an increase in calorie consumption from snacks, albeit with no change in overall calorie intake, hormone levels or energy expenditure across the different sleep schedules tested. A 2014 review of studies on the link between sleep debt and obesity also reported no association between short sleep duration and total energy expenditure.\n\nAnother explanation of the relationship stems from the balance between two hormones, leptin and ghrelin, which act on the nuclei of the hypothalamus to monitor energy and food intake. Leptin primarily inhibits appetite while ghrelin, which is the released by the stomach, works to stimulate appetite. Sleep deprivation has been associated with increased levels of ghrelin and decreased levels of leptin in multiple studies. In addition to the hormonal variation, other research has also associated shortened sleep durations with a proportional increase in subjects' BMIs.\n\nMetabolism\n\nMetabolism involves two biochemical processes that occur in living organisms. The first is anabolism, which refers to the buildup of molecules. The second is catabolism, the breakdown of molecules. These two processes work to regulate the amount of energy the body uses to maintain itself. During non-REM sleep, metabolic rate and brain temperature are lowered to deal with damages that may have occurred during time of wakefulness.\n\nSleep is important in regulating metabolism. Mammalian sleep can be sub-divided into two distinct phases - REM (rapid eye movement) and non-REM (NREM) sleep. In humans, NREM sleep has four stages, where the third and fourth stages are considered slow-wave sleep (SWS). SWS is considered deep sleep, when metabolism is least active.\nIn normal metabolic function, the pancreas releases insulin after blood glucose levels raise. Insulin signals muscle and fat cells to absorb glucose from food. As a result, blood glucose levels return to normal.\n\nSleep loss can affect the basic metabolic functions of storing carbohydrates and regulating hormones. Reduction of sleep from eight hours to four hours produces changes in glucose tolerance and endocrine function. Researchers from the University of Chicago Medical Center followed 11 healthy young men for 16 consecutive nights. The first 3 nights, the young men slept for the normal 8 hours. The next 6 nights, they slept for 4 hours. The next 7 nights, they spent 12 hours in bed. They all had the same diet. They found that there were changes in glucose metabolism that resemble that of type 2 diabetes patients. When the participants were tested after sleep deprivation, they took 40% longer than normal to regulate blood sugar levels after a high-carbohydrate meals. The secretion of insulin and the body's response to insulin decrease by 30%. Sleep deprivation also alters the productions of hormones, lowering the secretion of thyroid stimulating hormone and increasing blood levels of cortisol.\n\nA regular sleep schedule can be a vital piece to increasing weight loss. While it is important to sleep more than an average of 6.5 hours per night, sleeping over 8.5 hours per night has been shown to contribute negatively to weight loss. Getting adequate sleep can also help account for any excess snacking that occurs throughout the course of the day due to feeling lethargic.\n\nAccording to a recent study at Brigham Young University, a regular sleep schedule can make an almost immediate difference on the body’s ability to metabolize fat cells. In this specific study design, 300 college aged women (19–26 years old) were followed for a week and given an activity tracker which not only monitored movements, but also sleep patterns. The study also found that participants with lower BMI had higher quality of sleep, while those with higher BMI’s had lower quality of sleep. But was the reverse relationship also true?\n\nWhile health professionals almost unanimously agree that if an individual is already getting 8 hours of sleep, then another half hour won’t make them lose weight; however, in cases where a person is used to getting 5 hours of sleep per night and they start getting 7–8 hours, it is common to see them start to shed pounds, especially in obese individuals. What causes this phenomenon? Two hormones, ghrelin and leptin, closely associated with appetite are key in the sleep and weight relationship. Ghrelin is the hormone that controls appetite and tells the body when to eat, and when one is sleep-deprived, they have more ghrelin. Leptin on the other hand, is the hormone that tells one to stop eating, and when an individuals is sleep deprived, they have less leptin. In order to keep these hormone levels in balance, a person needs to get adequate sleep so they do not feel the need to excessively eat in an effort to gain energy and reduce fatigue.\n\nThere have been many connections made between oversleeping and certain disorders. Many of these have been made without any identifiable reason for correlation, and are mainly observational. WebMD reports that sleep apnea may cause oversleeping because of disruptions in the normal sleep cycle, that individuals who sleep more may be more prone to headaches because of neurotransmitters imbalances, back pain can increase with oversleeping because a certain level of physical activity is not being maintained, and that oversleeping may be correlated with depression and higher death rates.\n\nHowever, these connections have not been subject to rigorous examination. On the other hand, there have been studies that have looked into the potential physical side effects of oversleeping on weight and weight – related conditions. One study at Université Laval's Faculty of Medicine in Quebec studied the life habits of 276 subjects over a 6-year period and found that about 20% of those with long (9+ hours) of sleeping time developed type 2 diabetes or impaired glucose tolerance as compared to 7% in those that slept an average amount of time.\nA further look into the 6-year Quebec Family Study demonstrates that long-duration sleepers (9–10 hours) were 25% more likely to experience a 5-kg weight gain, and a 21% increase in risk of obesity, when adjusted for age, sex, and baseline BMI, as compared to average duration sleepers (7–8 hours) (NCBI). Even when the researchers adjusted for energy expenditure and physical activity levels (among other covariates), these relationships remained significant. The researcher’s results indicate a U-shaped relationship between hours of sleep and type 2 diabetes, coronary heart disease and weight, but the metabolic mechanisms affected by long sleep duration are less clear than with sleep restriction and remain somewhat speculative.\n\nA Nurses’ Health Study analyzed a group of about 72,000 US women who did not report having coronary heart disease at the onset of the study, and assessed the relationship between their reported sleep durations and incidence of a CHD event over a period of 10 years (JAMA). At the conclusion of the study, the data indicated that women who slept a longer duration (9–11 hours) were 38% more likely to have CHD than women who slept 8 hours. However, the researchers had no plausible explanation for a cause-and-effect relationship \n\nSleep apnea\n\nSleep apnea is a sleep disorder which causes an individual to have short pauses in breathing or very shallow breaths while sleeping. These pauses in breathing can often cause the individual to wake up, snore, choke, or just generally disrupt their sleep. As a result, sufferers of the disease do not get quality sleep during the night and are tired during the daytime. Sleep apnea is very difficult to diagnose because doctors can’t exactly tell if a person has sleep apnea in a regular routine visit. Additionally the patient himself may not even realize he has sleep apnea because it occurs during sleep, so a partner or roommate is usually the first to notice symptoms. \nThere are two types of sleep apnea, obstructive and central. Obstructive sleep apnea is more common among overweight patients, and occurs when the airway is fully or partially blocked at times during sleep. Any air that does sneak by the blocked passage can cause loud snoring. The second type of sleep apnea, central sleep apnea, is much more rare and has to do with the part of the brain that regulates breathing. The signal from the brain to the lungs is disrupted, resulting in pauses in breathing. \nTreating obstructive sleep apnea is much easier than central sleep apnea, and the treatment plan may include things such as lifestyle changes, mouthpieces, surgery, and breathing devices\n\nAs obesity has become an issue of nationwide focus, all forms of media have begun to explore and report on the link between sleep and weight. The coverage spans from articles in Women’s Health Magazine on “6 Ways Sleep Can Help you Lose Weight,” to NPR’s story on the research linking a lack of sleep to obesity, to Harvard School of Public Health’s discussion of sleep as an “obesity prevention source” on their site. No matter how diverse the range of broadcasting, media sources seem to agree that, for whatever reasons, sleep is an important part of maintaining a healthy weight, and could be an effective approach to the obesity epidemic.\n"}
{"id": "52905034", "url": "https://en.wikipedia.org/wiki?curid=52905034", "title": "Social Support Questionnaire", "text": "Social Support Questionnaire\n\nThe Social Support Questionnaire (SSQ) is a quantitative, psychometrically sound survey questionnaire intended to measure social support and satisfaction with said social support from the perspective of the interviewee. Degree of social support has been shown to influence the onset and course of certain psychiatric disorders such as clinical depression or schizophrenia. The SSQ was approved for public release in 1981 by Irwin Sarason, Henry Levine, Robert Basham and Barbara Sarason under the University of Washington Department of Psychology and consists of 27 questions. Overall, the SSQ has good test-retest reliability and convergent internal construct validity.\n\nThe questionnaire is designed so that each question has a two-part answer. The first part asks the interviewee to list up to nine people available to provide support that meet the criteria stated in the question. These support individuals are specified using their initials in addition to the relationship to the interviewee. Example questions from the first part includes questions such as “Whom could you count on to help if you had just been fired from your job or expelled from school?” and “Whom do you feel would help if a family member very close to you died?”.\n\nThe second part asks the interviewee to specify how satisfied they are with each of the people stated in the first part. The SSQ respondents use a 6 -point Likert scale to indicate their degree of satisfaction with the support from the above people ranging from “1 - very dissatisfied” to “6 - very satisfied”.\n\nThe Social Support Questionnaire has multiple short forms such as the SSQ3 and the SSQ6.\n\nThe SSQ is based on 4 original studies. The first study set out to determine whether the SSQ had the desired psychometric properties. The second study tried to relate SSQ and a diversity of personality measures such as anxiety, depression and hostility in connection with the Multiple Affect Adjective Checklist. The third study considered the relationship between social support, the prior year’s negative and positive life events, internal-external locus of control and self- esteem in conjunction with the Life Experiences Survey. The fourth study tested the idea that social support could serve as a buffer when faced with difficult life situations via trying to solve a maze and subsequently completing the Cognitive Interference Questionnaire.\n\nThe overall support score (SSQN) is calculated by taking an average of the individual scores across the 27 items. A high score on the SSQ indicates more optimism about life than a low score. Respondents with low SSQ scores have a higher prevalence of negative life events and illness. Scoring is as follows:\n\n1. Add the total number of people for all 27 items (questions). (Max. is 243). Divide by 27 for average item score. This gives you SSQ Number Score, or SSQN.\n\n2. Add the total satisfaction scores for all 27 items (questions). (Max is 162). Divide by 27 for average item score. This gives you SSQ Satisfaction score or SSQS.\n\n3. Finally, you can average the above for the total number of people that are family members - this results in the SSQ family score.\n\nAccording to Sarason, the SSQ takes between fifteen to eighteen minutes to properly administer and has “good” test-retest reliability.\n\nThe SSQ was compared with the depression scale and validity tests show significant negative correlation ranging from -0.22 to -0.43. The SSQ and the optimism scale have a correlation of 0.57. The SSQ and the satisfaction score have a correlation of 0.34. The SSQ has high internal consistency among items.\n\nThe SSQ has been used to show a positive correlation and dependence between Post Traumatic Stress Disorder and Social Support in a study of adolescents and long-term outcomes in Gonaives, Haiti. The study looked at the traumas stemming from the natural disasters of 2004, 2008 and 2010. The SSQ has also been used to show that higher levels of social support correlated with less suicide ideation in Military Medical University Soldiers in Iran in 2015. A low level of social support is an important risk factor in women for dysmenorrhea or menstrual cramps. Low Social Support is the strongest predictor of dysmenorrhea when compared to affect, personality and alexithymia.\n\nThe SSQ3 is a short form of the SSQ and has only three questions. The SSQ3 has acceptable test-test reliability and correlation with personality variables as compared to the long form of the Social Support Questionnaire. The internal reliability was borderline but this low level of internal reliability is as expected since there are only three questions.\n\nThe SSQ6 is a short form of the SSQ. The SSQ6 has been shown to have high correlation with: the SSQ, SSQ personality variables and internal reliability. In the development of the SSQ6, the research suggests that professed social support in adults may be a connected to “early attachment experience.” The SSQ6 consists of the below 6 questions:\n\n1. Whom can you really count on to be dependable when you need help?\n\n2. Whom can you really count on to help you feel more relaxed when you are under pressure or tense?\n\n3. Who accepts you totally, including both your worst and your best points?\n\n4. Whom can you really count on to care about you, regardless of what is happening to you?\n\n5. Whom can you really count on to help you feel better when you are feeling generally down-in-the-dumps?\n\n6. Whom can you count on to console you when you are very upset?\n\nThe Interpersonal Support Evaluation List includes 40 items (questions) with four sub-scales in the areas of Tangible Support, Belonging Support, Self-Esteem Support and Appraisal Support. The interviewee rates each item based on how true or false they feel the item is for themselves. The four total response options are “Definitely True”, “Probably True”, “Probably False”, and “Definitely False”.\n\n"}
{"id": "22343567", "url": "https://en.wikipedia.org/wiki?curid=22343567", "title": "Starfish Greathearts Foundation", "text": "Starfish Greathearts Foundation\n\nStarfish Greathearts Foundation is an international non-governmental organisation formed in response to the tragedy of children orphaned or left vulnerable by the HIV/AIDS pandemic in South Africa. Its mission is to help make a difference to the lives of these children through community-based organisations working at grassroots level. This enables individual communities to develop their own solutions to the challenges they face. As of January 2009, Starfish projects reach more than 36,000 children in 120 communities across South Africa.\n\nStarfish Greathearts Foundation is registered as a charity (1093862) and company (4528018) in the United Kingdom. Starfish Greathearts Foundation UK delivers development programmes through Starfish Greathearts Foundation South Africa, which is a non-profit organisation (039-447-NPO), Section 21 Company (2003/002865/08) and Public Benefit Organisation (930008639) in South Africa. Starfish launched Starfish Greathearts Foundation USA as a full-time office in New York in 2007 and also has a volunteer division in Canada.\n\nStarfish Greathearts Foundation UK is a member of the UK Consortium on AIDS and International Development as well as the Fundraising Standards Board, a self-regulatory scheme to ensure best practice in fundraising.\n\nStarfish supports newly orphaned children, or those left to look after sick and dying parents, with food parcels to enable them to stay alive during times of crisis. The parcels typically contain maize meal, beans, peanut butter, sugar and salt to enable a family of four to survive for a month.\n\nEmergency food parcels enable staff to build up a relationship with the children. The parcels are used as a short term tool to enable families and children to survive until they can be linked to a more secure and sustainable source of food such as vegetable gardening and government grants, in addition to long term support structures and social services.\n\nSchool represents the one sustainable means of building independent and economically active lives for children. As well as enabling orphaned children to apply for school fees exemption, Starfish projects crucially provide the necessary support around education such as the provision of school uniforms, shoes, stationery and books.\n\nIt is widely accepted, based on experience of organisations in South Africa and throughout the rest of the continent where large populations of AIDS orphans exist, that the best models of care for vulnerable and orphaned children are found within the children’s communities, not in institutions.\n\nFor centuries, communities in Africa have helped neighbours in crisis. But the huge numbers of children orphaned by HIV/AIDS in South Africa needing support mean that community groups require assistance to effectively care for children. Starfish Greathearts provides financial support and expertise to help community-based groups deliver best practice care and assistance to orphaned and vulnerable children.\nStarfish project partners provide:\n\nThere currently exists thousands of Community Based Organisations (CBOs) that work to support orphaned children and adults at risk due to HIV/AIDS. These organisations deliver a range of education and health support services. However, donors and the government are unable to work with these CBOs because they have no formal framework, infrastructures such as bank accounts, or non-profit organisation registration. Staff and volunteers require training and skills around budgeting and financial management, fundraising, business plans and codes of conduct. The majority of staff and volunteers have also received little or no training in regards to care of orphans and children's rights.\nStarfish works to increase the capacity and impact of these grassroots organisations through the provision of training and mentoring, empowering communities to respond to the needs of orphans in their area in the long term. The aim is to develop a large number of stable and well run CBOs capable of working with the government to ensure delivery of care, resources and services to children made orphaned by the HIV/AIDS pandemic.\n\nThe Starfish training and mentoring programme focuses on CBOs that have reached a certain level of development, and require a programme of training and support with a capacity building approach to achieve their full potential and output levels. Training includes orphan care, children's rights, bookkeeping and donor acquisition to enable the CBOs to become sustainable in the long term.\n\nKey income generating activities include:\n\nCampaigns and Community Support\nThe supporter base of Starfish Greathearts Foundation is based on widespread support from individuals through donations and income generated through successful campaigns. The Dinners of Hope Campaign is the most popular ongoing campaign that Starfish has developed. It is simple in the fact that everyone has to eat and therefore everyone can participate in the campaign. The emphasis of this campaign brings to the table the chance to educate, communicate and fundraise with minimal input and optimal results. The concept is simple in that supporters are asked to host a meal for family, friends or colleagues and then ask guests for a doantion rather than a contribution to the meal. The campaign also works as a powerful tool in promoting HIV/AIDS awareness and education at all levels. The campaign has mobilised more than 20,000 individuals and 120 corporations worldwide to participate.\n\nCharitable Trust and Foundations Support\nSupport from charitable trusts, foundations and development agencies constitutes an important part of maintaining the work of Starfish. The organisation has a track record of securing grants of varying sizes in the UK, USA and in South Africa. The Coca-Cola Africa Foundation donated over R1.2 million to Starfish in 2003 for key projects in KwaZulu-Natal, whilst more recently The Makro Foundation donated R240,000 towards children's educational needs in the Thandanani Programme.\n\nIn the UK, the charity is supported by more than 50 charitable trusts and foundations with grants ranging from £200 to more than £70,000 from organisations. Starfish has previously received statuory support from Isle of Man Government Overseas Aid Committee.\n\nCorporate Support\nCorporate supporters globally has included Anglo American plc, BP, Coca-Cola, SABMiller plc, Virgin Atlantic, Virgin Media, Virgin Megastores and Xstrata. Saracens Premiership Rugby Club, also known as Saracens F.C., are the latest major partner to lend their support to the charity having visited one of their projects in January 2009.\n\nUS President's Emergency Plan For AIDS Relief\nMuch of the expansion of the charity's work in South Africa has only been possible through the support of the President's Emergency Plan For AIDS Relief, which has donated more than R30 million to date. In 2003, the US President's Emergency Plan for AIDS Relief (PEPFAR) was launched to combat global HIV/AIDS – the largest commitment by any nation to combat a single disease in history.\n\n\n"}
{"id": "14279263", "url": "https://en.wikipedia.org/wiki?curid=14279263", "title": "Subepithelial connective tissue graft", "text": "Subepithelial connective tissue graft\n\nIn dentistry, the subepithelial connective tissue graft (SECT graft, and sometimes referred to simply as a \"connective tissue (CT) graft\") is an oral and maxillofacial surgical procedure first described by Alan Edel in 1974. Currently, it is generally used to obtain root coverage following gingival recession, which was a later development by Burt Langer in the early 1980s.\n\nSimilar to the free gingival graft, the SECT graft can be described as a \"free autogenous graft\". The term \"free\" describes how the graft is completely removed from the donor site rather than remaining attached via a pedicle. The term \"autogenous\", from the Greek root \"auto-\" (\"self\"), describes how the individual who receives the graft is the same individual who provides the donor tissue. The connective tissue is generally taken from the hard palate, although it may be taken from other sites as well, such as the maxillary tuberosity area. Because the connective tissue for the graft is transplanted without the superficial epithelium from the donor site, it is termed \"subepithelial\".\n\nAs initially described by Edel, the treatment objective was to increase the zone of keratinized tissue. Others, including Broome and Taggert and Donn also described the use of SECT grafts for increasing the zone of keratinized tissue.\n\nOf the various ways of preparing the graft recipient site, Edel described using two vertical incisions, mesial and distal to the teeth at which the zone of keratinized tissue was intended to be widened.\nAt the donor site, Edel described three methods for choosing and preparing the donor site to obtain connective tissue for the SECT graft:\n\nContrary to the donor site for a free gingival graft, the surgeon is able to achieve primary closure at the donor site for a SECT.\n\nLanger later described the SECT as a method by which to augment concavities and irregularities of the alveolar ridge following traumatic extractions, advanced periodontitis or developmental defects. Currently, though, such augmentation of hard tissue defects tends to be done with hard tissue replacements, namely bone graft materials.\n\nHowever, it was only in 1985 that Langer proposed the SECT for root coverage following gingival recession.\n\nThe SECT graft is a sort of hybrid procedure that combines the pedicle flap with the free gingival graft and enjoys the benefits of both. Pedicle flaps alone, such as the coronally advanced flap, frequently suffer from retraction and muscle pull.\n\nAlthough there are various ways in which to carry out this procedure, all share a common sequence of steps:\nThe donor site might be sutured closed either before or after securing the donor tissue to the recipient site\n"}
{"id": "5087808", "url": "https://en.wikipedia.org/wiki?curid=5087808", "title": "Terma Foundation", "text": "Terma Foundation\n\nThe Terma Foundation was founded in 1993 as the Tibet Child Nutrition Project (TCNP), by Dr. Nancy S. Harris and now implements public health programs including nutrition, education, primary and preventive health care, acknowledging traditional belief systems, and integrating low-tech, low-cost western technology where appropriate in Tibet.\n\nTerma's work in the Tibet Autonomous Region and adjacent ethnic Tibetan areas of the People's Republic of China is carried out by a multidisciplinary coalition of Tibetans, Chinese, and Westerners in successful cooperation with PRC nationals and local health authorities.\n\nDr. Harris gained notoriety when she published an article in the New England Journal of Medicine with the results of her study on stunted growth in the Himalaya region. She measured 2,500 children in rural villages and concluded that malnutrition was the cause rather than altitude, as previously suspected. To aid their work with local communities, the organization conducted a systematic study on the effectiveness of traditional Tibetan medicine in preventing and treating child health problems. They also studied the impact of vitamins and sunlight exposure in preventing rickets. The foundation's field operations, based in Lhasa, now reach an estimated 300,000 child and community beneficiaries each year.\n\nThe objectives of the foundation include:\n\n• Focusing on the health and welfare of children, women, and the elderly who maintain indigenous values while confronting contemporary challenges;\n\n• Promoting environmentally appropriate traditional agriculture, consumption of high nutrient indigenous foods, and cultivation of Tibetan herbs;\n\n• Supporting educational programs in public health, literacy, vocational training, the arts, and international education exchange;\n\n• Developing microeconomic initiatives at the grassroots level which will sustain the local health infrastructure; and\n\n• Facilitating interaction between Tibetans and other indigenous communities internationally and on issues relating to health, education, and the environment.\n\n"}
{"id": "46633538", "url": "https://en.wikipedia.org/wiki?curid=46633538", "title": "Tinnitracks", "text": "Tinnitracks\n\nTinnitracks is a software for the treatment of tinnitus based on neuro acoustic modulation developed and marketed by Sonormed GmbH. It can be accessed by internet both via computer and via mobile devices and uses a multistage process to individually adapt regular music songs to the unique tinnitus frequency of each patient. The concept behind this treatment has been developed and researched by the University of Muenster, Germany. Tinnitracks was developed based on this research and provides access to the treatment beyond academic research programs. It has received some awards and won several start-up competitions, among them the SXSW Accelerator in the category “Digital Health & Life Sciences Technologies” at the 2015 South by Southwest Interactive Festival. \nPatients suffering from tinnitus can use the Tinnitracks software to filter their individual tinnitus frequency from their music songs. By processing a song, the defined frequency is extracted creating a gap called “notch” in the frequency-profile that covers the exact tinnitus frequency as well as about an octave around it. Afterwards, the software checks if the audio profile of the song meets the criteria for the tinnitus treatment. Accuracy of frequency measurement is paramount in the process, which is why patients are required to visit ENT specialists or acousticians to create an accurate tinnitus audiogram. \nTinnitracks is based on the Tailor-Made Notched Music Training (TMNMT)). This approach uses filtered music to reduce the tinnitus loudness. The name TMNMT already stresses the importance of individualization in the whole set-up and refers both to the patient’s individual tinnitus frequency and the use of the patient’s favourite music. By filtering the tinnitus frequency from the music the frequency profile of a song is changed affecting the reactions induced in certain areas of the auditory cortex. Those cortical areas correlated to the frequency that got filtered are no longer stimulated. \n"}
{"id": "41079152", "url": "https://en.wikipedia.org/wiki?curid=41079152", "title": "Vinohrady Water Tower", "text": "Vinohrady Water Tower\n\nVinohrady Water Tower () is a building in Vinohrady in Prague 10 which was originally built as a water tower. Today its architecture is acknowledged as culturally important although it is now converted to accommodate offices and apartments. The viewing platform at the top is 40 metres above the street level.\n\nThe Vinohrady water tower building was created in 1882 to house steam engines and an underground reservoir. The engines pumped water up from the reservoir, creating a gravitational feed to nearby homes and businesses. Antonin Turek designed the building while he was the municipal architect. \n\nThe seventh floor originally contained a tank that could hold 200 cubic metres of water. Initially this was pumped from the River Vltava until 1912, when the water source was switched to a plant at Káraný. The water descended from the tank to supply Strašnice, Žižkov and Vršovice with drinking water.\n\nIn 1914 the tower's steam engines were replaced with electric motors. The revised system used water that was treated at a plant at Podolí and then stored in water tanks underground before being pumped to the top of this tower for use.\n\nToday its architecture is acknowledged to be of national historical importance, even though the building serves no modern purpose for water supply, as the motors, pumps and pipes have been removed. In 1993, the inside was converted to accommodate offices and apartments. The viewing platform at the top is 40 metres above the street level – once this provided a commanding view of the area, but today it is only slightly taller than the tenements that surround the tower. On each corner there are the statues of angels with clock faces between them. Below the clocks are four medallions celebrating Vinohrady. In 1991 the tower was recognised as being of cultural importance and was listed by the Czech Ministry of Culture.\n"}
{"id": "247588", "url": "https://en.wikipedia.org/wiki?curid=247588", "title": "Willi Unsoeld", "text": "Willi Unsoeld\n\nWilli Unsoeld (October 5, 1926 – March 4, 1979) was an American mountaineer who, along with Tom Hornbein, were members of the first American expedition to summit Mount Everest on May 22, 1963. Unsoeld and Hornbein's legendary climb was the first ascent from the peak's west ridge, and the first major traverse of a Himalayan peak. His subsequent activities included working as a U.S. Forest Service Smokejumper, Peace Corps director in Nepal, speaker for Outward Bound, faculty member at Oregon State University and The Evergreen State College and mountaineering guide. An avalanche during a winter climb of Mount Rainier took his life.\n\nBorn in Arcata, California, Unsoeld was raised in Eugene, Oregon, and received degrees from Oregon State University, the University of California, Berkeley, and the University of Washington. He helped to create the OSC Mountain Club during his time at Oregon State University.\n\nIn the late 1950s he was a leading climbing guide in the Grand Teton Mountains. He climbed Mt. Rainier over 200 times.\n\nUnsoeld and Tom Hornbein ascended Everest’s difficult West Ridge route in May 1963 on a National Geographic Society sponsored expedition while Barry Bishop and Lute Jerstad followed Edmund Hillary and Tenzing Norgay’s South Col route established during their 1953 climb. It was the first simultaneous attempt from two directions.\n\nThe grueling expedition cost Unsoeld nine of his toes and required several months of recovery in the hospital. Unsoeld and the team reunited in July 1963 when they were presented with the National Geographic Society’s highest honor, the Hubbard Medal, by John F. Kennedy.\n\nAfter his stint in the Peace Corps, Unsoeld joined Outward Bound and traveled about the country giving speeches and promoting the organization. \n\nAfter leaving Outward Bound he became one of the founding faculty at The Evergreen State College in Washington State. He was highlighted prominently in the first recruiting video in 1971, advising that \"not every student should come to Evergreen\".\n\nIt was at The Evergreen State College where he conceived an Outdoor Education Program. This program consisted of four distinct Habitat Groups, one of which was the Winter Mountaineering Group. Academic programs at the college at the time were designed to last a year or less.\n\nUnsoeld married Jolene Bishoprick in the 1950s; they had two daughters and two sons. Jolene Unsoeld and their two sons, Krag and Regon, reside in Olympia, Washington. Their daughter Terres Unsoeld lives in California. Jolene Unsoeld served three terms in the U.S. Congress from 1989 to 1995.\n\nIn 1976, Unsoeld and his daughter Nanda Devi were on an expedition to climb her namesake mountain Nanda Devi, the second highest peak in India. His daughter died during the climb, which was plagued by accidents and eventual tragedy. The reason for her death was blood clotting caused by the high altitude of the mountain. Asked at his home (where a picture of Devi was displayed over the fireplace) how he could continue climbing after losing his daughter, Willi responded: \"What, you want me to die of a heart attack, drinking beer, eating potato chips, and watching a golf tournament on TV?\"\n\nUnsoeld died in an avalanche during an Outdoor Education Winter Expedition climb of Mt. Rainier in March 4, 1979, at the age of 52. He was leading over a dozen students from The Evergreen State College on an ascent of Mt. Rainier at the time. He died during the descent from their high camp in Cadaver Gap along with one student, Janie Diepenbrock from Sacramento, California.\n\nA later analysis of the mishap, excerpts of which were published by the American Alpine Club, said in part: \"There are many guides who would not have taken on this particular climb with this particular group, but this is a matter of personal preference rather than a determination as to whether this climb was proper to attempt or not.\" \n\nKnown as \"The Father of Experiential Education,\" Willi Unsoeld influenced the growth of outdoor education, inspiring educational leaders like Simon Priest. His philosophical approach to living and global perspective mentored environmental visionaries like Caril Ridley.\n\nHis philosophy focused on experiencing the sacred in nature, the importance of risk in education and getting personal experience rather than relying on the experience of others. His dynamic style of mentoring inspired thousands of followers. \n\nWhy don’t you stay in the wilderness? Because that isn’t where it is at; it’s back in the city, back in downtown St. Louis, back in Los Angeles. The final test is whether your experience of the sacred in nature enables you to cope more effectively with the problems of people. If it does not enable you to cope more effectively with the problems – and sometimes it doesn’t, it sometimes sucks you right out into the wilderness and you stay there the rest of your Life – then when that happens, by my scale of value; it’s failed. You go to nature for an experience of the sacred...to re-establish your contact with the core of things, where it’s really at, in order to enable you to come back to the world of people and operate more effectively. Seek ye first the kingdom of nature, that the kingdom of man might be realized. \nEvergreen’s annual Willi Unsoeld Seminar is held as a living memorial to Unsoeld as a mountaineer, a philosopher and a theologian. \n\n\n"}
