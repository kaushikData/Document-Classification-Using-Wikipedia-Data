{"id": "57028333", "url": "https://en.wikipedia.org/wiki?curid=57028333", "title": "Abigail Ashley", "text": "Abigail Ashley\n\nAbigail Ashley is a Ghanaian television presenter, radio host, health advocate and the project leader for Behind My Smiles Foundation - a non-governmental organization (NGO) focusing on kidney health. She is also the author of the book \"A Decade of My Life\" Behind My Smiles.\n\nAshley is the presenter for \"My Health, My Life\" on United Television Ghana. In 2017 she was nominated for \"50 Young Most Influential people in Ghana\".\n"}
{"id": "39335909", "url": "https://en.wikipedia.org/wiki?curid=39335909", "title": "Action on Addiction", "text": "Action on Addiction\n\nAction on Addiction is a UK-based charity that works with people affected by drug and alcohol addiction. It works in the areas of research, prevention, treatment, aftercare, as well as professional education and family support. Catherine, Duchess of Cambridge has been patron since January 2012.\n\nThe charity was formed in 2007 by the merger of three charities: The Chemical Dependency Centre (established in 1985), Clouds (established in 1987) and the original Action on Addiction (established in 1989). The new charity assumed the name Action on Addiction.\n\nThe charity’s head office is in Wiltshire, with centers and programs operating in London, Wiltshire, Bournemouth, Essex and Liverpool. In Spring 2011, the charity introduced a family treatment program at a prison in Bridgend, South Wales.\n\nTreatment centers include Clouds House and Hope House. Clouds House is a Grade II listed building located near the village of East Knoyle. The charity also has a recovery dry bar in Liverpool, called The Brink.\n\nAction on Addiction provides abstinence-based Twelve-step programs in residential treatment, structured day treatment and relapse prevention programs at various venues around England for substance misusers as well as counseling and a brief residential program for families, partners and friends.\n\nThe M-PACT program (Moving Parents and Children Together, operated under the charity’s ‘For Families’ division) focuses specifically on the impact of drug addiction on families. It is an eight-week programme designed to help children aged 8–17 whose parents have drug and/or alcohol addictions. The program is based on the belief that healing the whole family, not just individual members, leads to the longest lasting and most successful outcomes. It is provided at various venues around England and Wales.\n\nThe charity works with researchers at the National Addiction Centre, part of the Institute of Psychiatry at King's College London. The charity played a part in establishing the National Addiction Centre and funds a chair in Addiction Psychiatry.\n\nThe charity (through its Centre for Addiction Treatment Studies ‘CATS’) is part of the Faculty of Humanities and Social Sciences at the University of Bath which offers accredited Foundation and BSc(Hons) Degrees, and continuing professional development in Addictions Counseling.\n\nIn January 2012, Catherine, The Duchess of Cambridge became Patron of Action on Addiction, one of the first four charities she chose to support in this way. In October 2012, The Duchess also gave her backing to the charity’s M-PACT program. The Duchess has visited the charity’s facilities on several occasions.\nOn 14 February 2012 she visited The Brink, the charity’s dry bar in Liverpool that offers a night out without the pressure to drink alcohol.\n\nOn 3 February 2013 she visited Clouds House treatment center in Wiltshire.\n\nOn 24 October 2013 The Duchess attended the 100 Women in Hedge Funds philanthropic initiatives reception dinner held at Kensington Palace in aid of the charity. She attended the event one day after christening of her son, Prince George. She is a patron of both Action on Addiction and 100WHF, an organisation for professionals in the alternative investments industry.\n\nOn 23 April 2013, in a speech at the launch of a new Manchester primary school counseling program, a joint project with the charity Place2Be, The Duchess said, “Through my Patronage of Action on Addiction, I feel fortunate to have met a wide range of inspirational people who have overcome addiction. It is so encouraging to see that with the right help… it can be conquered.”\n\nOn 19 February 2013 she visited Hope House, the charity’s residential treatment center for women recovering from addiction. It was her first official engagement following the announcement of her pregnancy.\n\nOn 1 July 2014 The Duchess visited Blessed Sacrament School in Islington, north London, to review the progress of M-PACT Plus, a joint project with the charity Place2Be, which addresses addiction in families. She was joined by comedian John Bishop.\n\nOn 23 October 2014 The Duchess attended the charity's Autumn Gala Evening dinner and reception in London, where she was joined by comedian and impressionist Rory Bremner. At the time, The Duchess was just over 12 weeks pregnant with her second child. It was \"her third official engagement in three days since returning to the spotlight after her battle against an aggressive type of morning sickness\".\n\nThe charity is mentioned on page 317 of J.K. Rowling's first adult novel, \"The Casual Vacancy\". \n\nIn 2016, the charity was mentioned on page 10 of Ben Starling's novel, \"Something in the Water\".\n\nOn 27 August 2014 Kirby Gregory, Director of Treatment and Care at the charity, and Claire Clarke, Referrals Consultant, took part in Channel 4's \"Addicts' Symphony\", counseling classical musicians in recovery who wrote an original classical piece within the London Symphony Orchestra Discovery Programme.\n\nOn 29 September 2013 Nick Barton, the charity's CEO, was interviewed by Phil Gayle on BBC Radio Oxford, and discussed getting people into recovery and keeping them there.\n\nOn 26 February 2012 the charity's treatment center, Clouds House, was featured in a BBC One documentary called \"Panorama: Britain's Hidden Alcoholics\" with Alasdair Campbell.\n\nIn 2009 the charity's M-PACT program was featured in a BBC True Vision documentary: \"Brought Up By Booze\" hosted by Calum Best (footballer George Best's son). Moving Parents and Children Together 'supports children/young people aged 8–17 who are experiencing the effects of parental substance misuse within the family...' and 'offers a Whole Family Approach'.\n\nIn May and June 2001, Clouds House was also featured in a four-part series created by BBC Two, \"Inside Clouds: A Drink & Drugs Clinic\". This series traced the progress of several residents as they struggled through six weeks of a (six-month) program of rehabilitation.\n\nCelebrity supporters of the charity (and of the founding charities) from television, music and fashion at various times have included: Bryan Adams, Eric Clapton, Sir David Frost, Joanna Lumley, Emilia Fox, Patsy Palmer, Roger Black, Edward Fox, Tania Bryer, Siân Lloyd, Trinny Woodall, Susannah Constantine, Laura Bailey, Tamara Beckwith, Sophie Anderton, Caprice Bourret, Lucy Ferry, and David Shilling.\n\nCelebrity supporters from sport have included: British tennis players Annabel Croft and Andrew Castle; Romanian former World No. 1 professional tennis player Ilie Năstase; English footballer Dion Dublin; British rower Sir Matthew Pinsent; West Indian cricket player Alvin Kallicharan; and British sailor Tracy Edwards; as well as former track and field athlete, Chairman of the board of the bid company for the London 2012 Olympics and British politician, Sebastian Coe.\n\nAction on Addiction is a registered charity (number 1117988) and a company registered in England and Wales limited by guarantee (number 05947481). Its registered address is: Head Office, East Knoyle, Salisbury, Wiltshire SP3 6BE.\n\n"}
{"id": "1173614", "url": "https://en.wikipedia.org/wiki?curid=1173614", "title": "Adult T-cell leukemia/lymphoma", "text": "Adult T-cell leukemia/lymphoma\n\nAdult T-cell leukemia/lymphoma (ATL or ATLL) is a rare cancer of the immune system's own T-cells.\n\nHuman T cell leukemia/lymphotropic virus type 1 (HTLV-1) is believed to be the cause of it, in addition to several other diseases.\n\nATL is usually a highly aggressive non-Hodgkin's lymphoma with no characteristic histologic appearance except for a diffuse pattern and a mature T-cell phenotype. Circulating lymphocytes with an irregular nuclear contour (leukemic cells) are frequently seen. Several lines of evidence suggest that HTLV-1 causes ATL. This evidence includes the frequent isolation of HTLV-1 from patients with this disease and the detection of HTLV-1 proviral genome in ATL leukemic cells. ATL is frequently accompanied by visceral involvement, hypercalcemia, skin lesions, and lytic bone lesions. Bone invasion and osteolysis, features of bone metastases, commonly occur in the setting of advanced solid tumors, such as breast, prostate, and lung cancers, but are less common in hematologic malignancies. However, patients with HTLV-1–induced ATL and multiple myeloma are predisposed to the development of tumor-induced osteolysis and hypercalcemia. One of the striking features of ATL and multiple myeloma induced bone disease is that the bone lesions are predominantly osteolytic with little associated osteoblastic activity. In patients with ATL, elevated serum levels of IL-1, TGFβ, PTHrP, macrophage inflammatory protein (MIP-1α), and receptor activator of nuclear factor-κB ligand (RANKL) have been associated with hypercalcemia. Immunodeficient mice that received implants with leukemic cells from patients with ATL or with HTLV-1–infected lymphocytes developed hypercalcemia and elevated serum levels of PTHrP. Most patients die within one year of diagnosis.\n\nInfection with HTLV-1, like infection with other retroviruses, probably occurs for life and can be inferred when antibody against HTLV-1 is detected in the serum.\n\nTransmission of HTLV-1 is believed to occur from mother to child; by sexual contact; and through exposure to contaminated blood, either through blood transfusion or sharing of contaminated needles.\nTreatment options that have been tried include zidovudine and the CHOP regimen. Pralatrexate has also been investigated. Most therapy is directed towards the cancer rather than the virus itself.\nRecently, it has been reported that the traditional glucocorticoid-based chemotherapy toward ATL are largely mediated by thioredoxin binding protein-2 (TBP-2/TXNIP/VDUP1), suggesting the potential use of a TBP-2 inducer as a novel therapeutic target.\n\nRecently, mogamulizumab, has been approved for the treatment of ATL in Japan.\n\nAt a medical conference in December 2013, researchers reported anywhere from 21-50% of ATL patients have disease expressing CD30. This suggests treatment with CD30-targeting brentuximab vedotin may be beneficial.\n\nHTLV-1 infection in the United States appears to be rare. Although little serologic data exist, prevalence of infection is thought to be highest among blacks living in the Southeast. A prevalence rate of 30% has been found among black intravenous drug abusers in New Jersey, and a rate of 49% has been found in a similar group in New Orleans. It is possible that prevalence of infection is increasing in this risk group. Studies of HTLV-1 antibody indicate that the virus is endemic in southern Japan, in the Caribbean, South America, and in Africa.\n\nATL is relatively uncommon among those infected with HTLV-1. The overall incidence of ATL is estimated at about 1 per 1,500 adult HTLV-1 carriers per year. Those cases that have been reported have occurred mostly among persons from the Caribbean or blacks from the Southeast (National Institutes of Health, unpublished data). There appears to be a long latent period between HTLV-1 infection and the start of ATL.\n\nNovel approaches to the treatment of PTCL in the relapsed or refractory setting are under investigation. Pralatrexate is one compound currently under investigations for the treatment of PTCL.\n\nLeukemia is rarely associated with pregnancy, affecting only about 1 in 10,000 pregnant women. How it is handled depends primarily on the type of leukemia. Acute leukemias normally require prompt, aggressive treatment, despite significant risks of miscarriage and birth defects, especially if chemotherapy is given during the developmentally sensitive first trimester.\n"}
{"id": "4690137", "url": "https://en.wikipedia.org/wiki?curid=4690137", "title": "Advanced maternal age", "text": "Advanced maternal age\n\nAdvanced maternal age, in a broad sense, is the instance of a woman being of an older age at a stage of reproduction, although there are various definitions of specific age and stage of reproduction. The variability in definitions is in part explained by the effects of increasing age occurring as a continuum rather than as a threshold effect.\n\nIn Western, Northern, and Southern Europe, first-time mothers are on average 27 to 29 years old, up from 23 to 25 years at the start of the 1970s. In a number of European countries (Spain), the mean age of women at first childbirth has crossed the 30 year threshold.\n\nThis process is not restricted to Europe. Asia, Japan and the United States are all seeing average age at first birth on the rise, and increasingly the process is spreading to countries in the developing world like China, Turkey and Iran. In the U.S., the average age of first childbirth was 26 in 2013.\n\nAdvanced maternal age is associated with adverse reproductive effects such as increased risk of infertility, and that the children have chromosomal abnormalities. The corresponding paternal age effect is less pronounced.\n\nIn present generations it is more common to have children at an older age. Several factors may influence the decisions of parents when having their first baby. Such factors include educational, social and economic status.\n\nHaving children later was not exceptional in the past, when families were larger and women often continued bearing children until the end of their reproductive age. What is so radical about this recent transformation is that it is the age at which women give birth to their first child which is becoming comparatively high, leaving an ever more constricted window of biological opportunity for second and subsequent children, should they be desired. Unsurprisingly, high first-birth ages and high rates of birth postponement are associated with the arrival of low, and lowest-low fertility.\n\nThis association has now become especially clear, since the postponement of first births in a number of countries has now continued unabated for more than three decades, and has become one of the most prominent characteristics of fertility patterns in developed societies. A variety of authors (in particular Lesthaeghe) have argued that fertility postponement constitutes the ‘hallmark’ of what has become known as the second demographic transition.\n\nOthers have proposed that the postponement process itself constitutes a separate 'third transition'. On this latter view, modern developed societies exhibit a kind of dual fertility pattern, with the majority of births being concentrated either among very young or increasingly older mothers. This is sometimes known as the 'rectangularisation' of fertility patterns.\n\nIn the USA, the average age at which women bore their first child advanced from 21.4 years old in 1970, to 25 years old in 2006.\n\nThe German Federal Institute for Population Research claimed in 2015 the percentage for women with an age of at least 35 giving birth to a child was 25.9%. This figure rose from 7.6% in 1981.\n\nThere are many factors that may influence childbearing age in women, although they are mostly correlations without certain causations.\n\nTwo studies show that generous parental leave allowances in Britain encourage young motherhood and that parental-leave allowance reduces postponement in Sweden.\n\nA woman's fertility peaks lasts during the twenties and first half of thirties, after which it starts to decline, with advanced maternal age causing an increased risk of female infertility.\n\nAccording to Henri Leridon, PhD, an epidemiologist with the French Institute of Health and Medical Research, of women trying to get pregnant, without using fertility drugs or in vitro fertilization:\n\nA woman's risk of having a baby with chromosomal abnormalities increases with her age. Down syndrome is the most common chromosomal birth defect, and a woman's risk of having a baby with Down syndrome is:\n\nAdvanced maternal age is associated with adverse outcomes in the perinatal period, which may be caused by detrimental effects on decidual and placental development.\n\nThe risk of the mother dying before the child becomes an adult increases by more advanced maternal age, such as can be demonstrated by the following data from France in 2007:\nAdvanced maternal age continues to be associated with a range of adverse pregnancy outcomes including low birth weight, pre-term birth, stillbirth, unexplained fetal death, and increased rates of Caesarean section.\n\nOn the other hand, advanced maternal age is associated with a more stable family environment, higher socio-economic position, higher income and better living conditions, as well as better parenting practices, but it is more or less uncertain whether these entities are \"effects\" of advanced maternal age, are \"contributors\" to advanced maternal age, or common effects of a certain state such as personality type.\n\nKalberer et al. have shown that despite the older maternal age at birth of the first child, the time span between the birth of the first and the second child (= interpregnancy interval) decreased over the last decades. If purely biological factors were at work, it could be argued that interpregnancy interval should have increased, as fertility declines with age, which would make it harder for the woman to get a second child after postponed birth of the first one. This not being the case shows that sociologic factors (see above) prime over biological factors in determining interpregnancy interval.\n\nWith technology developments cases of post-menopausal pregnancies have occurred, and there are several known cases of older women carrying a pregnancy to term, usually with in vitro fertilization of a donor egg. A 61-year-old Brazilian woman with implantation of a donor egg expected gave birth to twins in October 2011..\n\nAs women age, they experience a decline in reproductive performance leading to menopause. This decline is tied to a decline in the number of ovarian follicles. Although about 1 million oocytes are present at birth in the human ovary, only about 500 (about 0.05%) of these ovulate, and the rest do not (ovarian follicle atresia). The decline in ovarian reserve appears to occur at a constantly increasing rate with age, and leads to nearly complete exhaustion of the reserve by about age 51. As ovarian reserve and fertility decline with age, there is also a parallel increase in pregnancy failure and meiotic errors resulting in chromosomally abnormal conceptions.\n\nTitus et al. have proposed an explanation for the decline in ovarian reserve with age. They showed that as women age, double-strand breaks accumulate in the DNA of their primordial follicles. Primordial follicles are immature primary oocytes surrounded by a single layer of granulosa cells. An enzyme system is present in oocytes that normally accurately repairs DNA double-strand breaks. This repair system is referred to as homologous recombinational repair, and it is especially active during meiosis. Meiosis is the general process by which germ cells are formed in eukaryotes, and it appears to be an adaptation for efficiently removing damages in germ line DNA by homologous recombinational repair (see Origin and function of meiosis also). Human primary oocytes are present at an intermediate stage of meiosis, that is prophase I (see Oogenesis). Titus et al. also showed that expression of four key DNA repair genes that are necessary for homologous recombinational repair (\"BRCA1\", \"MRE11\", \"Rad51\" and \"ATM\") decline in oocytes with age. This age-related decline in ability to repair double-strand damages can account for the accumulation of these damages, which then likely contributes to the decline in ovarian reserve.\n\nWomen with an inherited mutation in the DNA repair gene \"BRCA1\" undergo menopause prematurely, suggesting that naturally occurring DNA damages in oocytes are repaired less efficiently in these women, and this inefficiency leads to early reproductive failure. Genomic data from about 70,000 women were analyzed to identify protein-coding variation associated with age at natural menopause. Pathway analyses identified a major association with DNA damage response genes, particularly those expressed during meiosis and including a common coding variant in the \"BRCA1\" gene.\n\n\n\n"}
{"id": "7023718", "url": "https://en.wikipedia.org/wiki?curid=7023718", "title": "Albedo (alchemy)", "text": "Albedo (alchemy)\n\nIn alchemy, albedo is one of the four major stages of the magnum opus; along with nigredo, citrinitas and rubedo. It is a Latinicized term meaning \"whiteness\". Following the chaos or \"massa confusa\" of the nigredo stage, the alchemist undertakes a purification in albedo, which is literally referred to as \"ablutio\" – the washing away of impurities. In this process, the subject is divided into two opposing principles to be later coagulated to form a unity of opposites or \"coincidentia oppositorum\" during rubedo.\n\nTitus Burckhardt interprets the albedo as the end of the lesser work, corresponding to a spiritualization of the body. The goal of this portion of the process is to regain the original purity and receptivity of the soul. Psychologist Carl Jung equated the albedo with unconscious contrasexual soul images; the anima in men and animus in women. It is a phase where insight into shadow projections are realized, and inflated ego and unneeded conceptualizations are removed from the psyche.\n\n"}
{"id": "58250690", "url": "https://en.wikipedia.org/wiki?curid=58250690", "title": "American Society of Pediatric Hematology/Oncology", "text": "American Society of Pediatric Hematology/Oncology\n\nThe American Society of Pediatric Hematology/Oncology (abbreviated ASPHO) is an American multidisciplinary professional organization dedicated to improving care in the medical disciplines of pediatric hematology and oncology. As of 2018, it had 2,000 members.\n\nThe American Society of Pediatric Hematology/Oncology was founded in 1981, largely because of the efforts of physician Carl Pochedly, who served as its secretary-treasurer for fourteen years. After holding meetings jointly with the American Pediatric Society and the Society for Pediatric Research, the ASPHO held its first independent meeting in Chicago, Illinois in 1988. The society was incorporated in the state of Illinois in 1989.\n\nSince 2004, the official journal of ASPHO has been \"Pediatric Blood & Cancer\". Its official journal was originally the \"Journal of Pediatric Hematology/Oncology\", which was established by Pochedly in 1979 as the \"American Journal of Pediatric Hematology/Oncology\".\n"}
{"id": "4940355", "url": "https://en.wikipedia.org/wiki?curid=4940355", "title": "Benzydamine", "text": "Benzydamine\n\nBenzydamine (also known as Tantum Verde and branded in some countries as Difflam and Septabene), available as the hydrochloride salt, is a locally-acting nonsteroidal anti-inflammatory drug (NSAID) with local anaesthetic and analgesic properties for pain relief and anti-inflammatory treatment of inflammatory conditions of the mouth and throat.\n\nIt may be used alone or as an adjunct to other therapy giving the possibility of increased therapeutic effect with little risk of interaction.\n\nIn some markets, the drug is supplied as an over-the-counter cream (Lonol in Mexico from Boehringer Ingelheim) used for topical treatment of musculoskeletal system disorders: sprains, strains, bursitis, tendinitis, synovitis, myalgia, periarthritis.\n\nStudies indicate that benzydamine has notable in vitro antibacterial activity and also shows synergism in combination with other antibiotics, especially tetracyclines, against antibiotic-resistant strains of Staphylococcus aureus and Pseudomonas aeruginosa.\n\nThere are no contraindications to the use of benzydamine except for known hypersensitivity.\n\nBenzydamine is well tolerated. Occasionally oral tissue numbness or stinging sensations may occur, as well as itching, a skin rash, skin swelling or redness, difficulty breathing and wheezing.\n\nIt selectively binds to inflamed tissues (Prostaglandin synthetase inhibitor) and is normally free of adverse systemic effects.\nUnlike other NSAIDs, it does not inhibit cyclooxygenase or lipooxygenase, and is not ulcerogenic.\n\nBenzydamine has been used recreationally. In overdosages it acts as a deliriant and CNS stimulant. Such use, particularly among teenagers, has been reported in Poland, Brazil and Romania.\n\nSynthesis starts with the reaction of the \"N\"-benzyl derivative from methyl anthranilate with nitrous acid to give the \"N\"-nitroso derivative. Reduction by means of sodium thiosulfate leads to the transient hydrazine (3), which undergoes spontaneous internal hydrazide formation. Treatment of the enolate of this amide with 3-chloro-1-dimethylamkino propane gives benzydamine (5). Please note there is an error in this section: US3318905 states that the nitroso derivative is reduced with sodium hydrosulfite (sodium dithionite) and not with sodium hyposulfite (sodium thiosulfate), as shown in the above scheme and stated in text. \n\nAn interesting alternative synthesis of this substance starts by sequential reaction of \"N\"-benzylaniline with phosgene, and then with sodium azide to product the corresponding carbonyl azide. On heating, nitrogen is evolved and a separatable mixture of nitrene insertion product and the desired ketoindazole # results. The latter reaction appears to be a Curtius rearrangement type product to produce an N-isocyanate #, which then cyclizes. Alkylation of the enol with sodium methoxide and 3-dimethylaminopropyl chloride gives benzydamine.\n\nAlternatively, use of chloroacetamide in the alkylation step followed by acid hydrolysis produces bendazac instead.\n\n\n"}
{"id": "56557", "url": "https://en.wikipedia.org/wiki?curid=56557", "title": "Blood glucose monitoring", "text": "Blood glucose monitoring\n\nBlood glucose monitoring is a way of testing the concentration of glucose in the blood (glycemia). Particularly important in diabetes management, a blood glucose test is typically performed by piercing the skin (typically, on the finger) to draw blood, then applying the blood to a chemically active disposable 'test-strip'. Different manufacturers use different technology, but most systems measure an electrical characteristic, and use this to determine the glucose level in the blood. The test is usually referred to as capillary blood glucose.\n\nHealthcare professionals advise patients with diabetes mellitus on the appropriate monitoring regimen for their condition. Most people with type 2 diabetes test at least once per day. The Mayo Clinic generally recommends that diabetics who use insulin (all type 1 diabetics and many type 2 diabetics) test their blood sugar more often (4–8 times per day for type 1 diabetics, 2 or more times per day for type 2 diabetics), both to assess the effectiveness of their prior insulin dose and to help determine their next insulin dose.\n\nBlood glucose monitoring reveals individual patterns of blood glucose changes, and helps in the planning of meals, activities, and at what time of day to take medications.\n\nAlso, testing allows for quick response to high blood sugar (hyperglycemia) or low blood sugar (hypoglycemia). This might include diet adjustments, exercise, and insulin (as instructed by the health care provider).\n\nA blood glucose meter is an electronic device for measuring the blood glucose level. A relatively small drop of blood is placed on a disposable test strip which interfaces with a digital meter. Within several seconds, the level of blood glucose will be shown on the digital display.\n\nNeeding only a small drop of blood for the meter means that the time and effort required for testing is reduced and the compliance of diabetic people to their testing regimens is improved. Although the cost of using blood glucose meters seems high, it is believed to be a cost benefit relative to the avoided medical costs of the complications of diabetes.\n\nRecent advances include:\n\nA continuous glucose monitor determines glucose levels on a continuous basis (every few minutes). A typical system consists of:\n\nContinuous glucose monitors measure the concentration of glucose in a sample of interstitial fluid. Shortcomings of CGM systems due to this fact are:\nPatients therefore require traditional fingerstick measurements for calibration (typically twice per day) and are often advised to use fingerstick measurements to confirm hypo- or hyperglycemia before taking corrective action.\n\nThe lag time discussed above has been reported to be about 5 minutes. Anecdotally, some users of the various systems report lag times of up to 10–15 minutes. This lag time is insignificant when blood sugar levels are relatively consistent. However, blood sugar levels, when changing rapidly, may read in the normal range on a CGM system while in reality the patient is already experiencing symptoms of an out-of-range blood glucose value and may require treatment. Patients using CGM are therefore advised to consider both the absolute value of the blood glucose level given by the system as well as any trend in the blood glucose levels. For example, a patient using CGM with a blood glucose of 100 mg/dl on their CGM system might take no action if their blood glucose has been consistent for several readings, while a patient with the same blood glucose level but whose blood glucose has been dropping steeply in a short period of time might be advised to perform a fingerstick test to check for hypoglycemia.\n\nContinuous monitoring allows examination of how the blood glucose level reacts to insulin, exercise, food, and other factors. The additional data can be useful for setting correct insulin dosing ratios for food intake and correction of hyperglycemia. Monitoring during periods when blood glucose levels are not typically checked (e.g. overnight) can help to identify problems in insulin dosing (such as basal levels for insulin pump users or long-acting insulin levels for patients taking injections). Monitors may also be equipped with alarms to alert patients of hyperglycemia or hypoglycemia so that a patient can take corrective action(s) (after fingerstick testing, if necessary) even in cases where they do not feel symptoms of either condition. While the technology has its limitations, studies have demonstrated that patients with continuous sensors experience less hyperglycemia and also reduce their glycosylated hemoglobin levels.\n\nCurrently, continuous blood glucose monitoring is not automatically covered by health insurance in the United States in the same way that most other diabetic supplies are covered (e.g. standard glucose testing supplies, insulin, and even insulin pumps). However, an increasing number of insurance companies do cover continuous glucose monitoring supplies (both the receiver and disposable sensors) on a case-by-case basis if the patient and doctor show a specific need. The lack of insurance coverage is exacerbated by the fact that disposable sensors must be frequently replaced. Some sensors have been U.S. Food and Drug Administration (FDA) approved for 7- and 3-day use, (although some patients wear sensors for longer than the recommended period) and the receiving meters likewise have finite lifetimes (less than 2 years and as little as 6 months). This is one factor in the slow uptake in the use of sensors that have been marketed in the United States.\n\nThe principles, history and recent developments of operation of electrochemical glucose biosensors are discussed in a chemical review by Joseph Wang.\n\nInvestigations on the use of test strips have shown that the required self-injury acts as a psychological barrier restraining the patients from sufficient glucose control. As a result, secondary diseases are caused by excessive glucose levels. A significant improvement of diabetes therapy might be achieved with an implantable sensor that would continuously monitor blood sugar levels within the body and transmit the measured data outside. The burden of regular blood testing would be taken from the patient, who would instead follow the course of their glucose levels on an intelligent device like a laptop or a smart phone.\n\nGlucose concentrations do not necessarily have to be measured in blood vessels, but may also be determined in the interstitial fluid, where the same levels prevail – with a time lag of a few minutes – due to its connection with the capillary system. However, the enzymatic glucose detection scheme used in single-use test strips is not directly suitable for implants. One main problem is caused by the varying supply of oxygen, by which glucose is converted to glucono lactone and HO by glucose oxidase. Since the implantation of a sensor into the body is accompanied by growth of encapsulation tissue, the diffusion of oxygen to the reaction zone is continuously diminished. This decreasing oxygen availability causes the sensor reading to drift, requiring frequent re-calibration using finger-sticks and test strips.\n\nOne approach to achieving long-term glucose sensing is to measure and compensate for the changing local oxygen concentration. Other approaches replace the troublesome glucose oxidase reaction with a reversible sensing reaction, known as an affinity assay. This scheme was originally put forward by Schultz & Sims in 1978. A number of different affinity assays have been investigated, with fluorescent assays proving most common. MEMS technology has recently allowed for smaller and more convenient alternatives to fluorescent detection, via measurement of viscosity. Investigation of affinity-based sensors has shown that encapsulation by body tissue does not cause a drift of the sensor signal, but only a time lag of the signal compared to the direct measurement in blood.\n\nSome new technologies to monitor blood glucose levels will not require access to blood to read the glucose level. Non-invasive technologies include near IR detection, ultrasound<ref name=\"Diabetes U/S SMBG\"></ref> and dielectric spectroscopy. These may free the person with diabetes from finger sticks to supply the drop of blood for blood glucose analysis.\n\nMost of the non-invasive methods under development are continuous glucose monitoring methods and offer the advantage of providing additional information to the subject between the conventional finger stick, blood glucose measurements and over time periods where no finger stick measurements are available (i.e. while the subject is sleeping).\n\nFor patients with diabetes mellitus type 2, the importance of monitoring and the optimal frequency of monitoring are not clear. A 2011 study found no evidence that blood glucose monitoring leads to better patient outcomes in actual practice. One randomized controlled trial found that self-monitoring of blood glucose did not improve glycosylated hemoglobin (HbA1c) among \"reasonably well controlled non-insulin treated patients with type 2 diabetes\". However a recent meta-analysis of 47 randomized controlled trials encompassing 7677 patients showed that self-care management intervention improves glycemic control in diabetics, with an estimated 0.36% (95% CI, 0.21–0.51) reduction in their glycosylated hemoglobin values. Furthermore, a recent study showed that patients described as being \"Uncontrolled Diabetics\" (defined in this study by HbA1C levels >8%) showed a statistically significant decrease in the HbA1C levels after a 90-day period of seven-point self-monitoring of blood glucose (SMBG) with a relative risk reduction (RRR) of 0.18% (95% CI, 0.86–2.64%, p<.001). Regardless of lab values or other numerical parameters, the purpose of the clinician is to improve quality of life and patient outcomes in diabetic patients. A recent study included 12 randomized controlled trials and evaluated outcomes in 3259 patients. The authors concluded through a qualitative analysis that SMBG on quality of life showed no effect on patient satisfaction or the patients' health-related quality of life. Furthermore, the same study identified that patients with type 2 diabetes mellitus diagnosed greater than one year prior to initiation of SMBG, who were not on insulin, experienced a statistically significant reduction in their HbA1C of 0.3% (95% CI, -0.4 – -0.1) at six months follow up, but a statistically insignificant reduction of 0.1% (95% CI, -0.3 – 0.04) at twelve months follow up. Conversely, newly diagnosed patients experienced a statistically significant reduction of 0.5% (95% CI, -0.9 – -0.1) at 12 months follow up. A recent study found that a treatment strategy of intensively lowering blood sugar levels (below 6%) in patients with additional cardiovascular disease risk factors poses more harm than benefit. For type 2 diabetics who are not on insulin, exercise and diet are the best tools. Blood glucose monitoring is, in that case, simply a tool to evaluate the success of diet and exercise. Insulin-dependent type 2 diabetics do not need to monitor their blood sugar as frequently as type 1 diabetics.\n\nThe National Institute for Health and Clinical Excellence (NICE), UK released updated diabetes recommendations on 30 May 2008, which recommend that self-monitoring of plasma glucose levels for people with newly diagnosed type 2 diabetes must be integrated into a structured self-management education process.\nThe recommendations have been updated in August 2015 for children and young adults with type 1 diabetes. See: NICE Guideline for Continuous Blood Glucose Monitoring.\n"}
{"id": "20631387", "url": "https://en.wikipedia.org/wiki?curid=20631387", "title": "Botswana Red Cross Society", "text": "Botswana Red Cross Society\n\nBotswana Red Cross Society also known as LRC was founded in 1968.It has its headquarters in Gaborone, Botswana.\n\n"}
{"id": "52737281", "url": "https://en.wikipedia.org/wiki?curid=52737281", "title": "Caroline Watkins", "text": "Caroline Watkins\n\nDame Caroline Leigh Watkins , is an English academic, the Professor of Stroke and Older People's Care - and Director of Research and Innovation - of the College of Health and Wellbeing, University of Central Lancashire. She is the UK's sole nursing professor of stroke care.\n\nShe was awarded the honour of Dame (DBE) in the 2017 New Year Honours, for services to Nursing and Older People's Care.\n"}
{"id": "2974157", "url": "https://en.wikipedia.org/wiki?curid=2974157", "title": "Computer facial animation", "text": "Computer facial animation\n\nComputer facial animation is primarily an area of computer graphics that encapsulates methods and techniques for generating and animating images or models of a character face. The character can be a human, a humanoid, an animal, a fantasy creature or character, etc. Due to its subject and output type, it is also related to many other scientific and artistic fields from psychology to traditional animation. The importance of human faces in verbal and non-verbal communication and advances in computer graphics hardware and software have caused considerable scientific, technological, and artistic interests in computer facial animation.\n\nAlthough development of computer graphics methods for facial animation started in the early-1970s, major achievements in this field are more recent and happened since the late 1980s.\n\nThe body of work around computer facial animation can be divided into two main areas: techniques to generate animation data, and methods to apply such data to a character. Techniques such as motion capture and keyframing belong to the first group, while morph targets animation (more commonly known as blendshape animation) and skeletal animation belong to the second. Facial animation has become well-known and popular through animated feature films and computer games but its applications include many more areas such as communication, education, scientific simulation, and agent-based systems (for example online customer service representatives). With the recent advancements in computational power in personal and mobile devices, facial animation has transitioned from appearing in pre-rendered content to being created at runtime.\n\nHuman facial expression has been the subject of scientific investigation for more than one hundred years. Study of facial movements and expressions started from a biological point of view. After some older investigations, for example by John Bulwer in the late 1640s, Charles Darwin’s book \"The Expression of the Emotions in Men and Animals\" can be considered a major departure for modern research in behavioural biology.\n\nComputer based facial expression modelling and animation is not a new endeavour. The earliest work with computer based facial representation was done in the early-1970s. The first three-dimensional facial animation was created by Parke in 1972. In 1973, Gillenson developed an interactive system to assemble and edit line drawn facial images. in 1974, Parke developed a parameterized three-dimensional facial model.\n\nOne of the most important attempts to describe facial movements was Facial Action Coding System (FACS). Originally developed by Carl-Herman Hjortsjö in the 1960s and updated by Ekman and Friesen in 1978, FACS defines 46 basic facial Action Units (AUs). A major group of these Action Units represent primitive movements of facial muscles in actions such as raising brows, winking, and talking. Eight AU's are for rigid three-dimensional head movements, (i.e. turning and tilting left and right and going up, down, forward and backward). FACS has been successfully used for describing desired movements of synthetic faces and also in tracking facial activities.\n\nThe early-1980s saw the development of the first physically based muscle-controlled face model by Platt and the development of techniques for facial caricatures by Brennan. In 1985, the animated short film \"Tony de Peltrie\" was a landmark for facial animation. This marked the first time computer facial expression and speech animation were a fundamental part of telling the story.\n\nThe late-1980s saw the development of a new muscle-based model by Waters, the development of an abstract muscle action model by Magnenat-Thalmann and colleagues, and approaches to automatic speech synchronization by Lewis and Hill. The 1990s have seen increasing activity in the development of facial animation techniques and the use of computer facial animation as a key storytelling component as illustrated in animated films such as \"Toy Story\" (1995), \"Antz\" (1998), \"Shrek\", and \"Monsters, Inc.\" (both 2001), and computer games such as \"Sims\". \"Casper\" (1995), a milestone in this decade, was the first movie in which a lead actor was produced exclusively using digital facial animation.\n\nThe sophistication of the films increased after 2000. In \"The Matrix Reloaded\" and \"The Matrix Revolutions\", dense optical flow from several high-definition cameras was used to capture realistic facial movement at every point on the face. \"Polar Express (film)\" used a large Vicon system to capture upward of 150 points. Although these systems are automated, a large amount of manual clean-up effort is still needed to make the data usable. Another milestone in facial animation was reached by \"The Lord of the Rings\", where a character specific shape base system was developed. Mark Sagar pioneered the use of FACS in entertainment facial animation, and FACS based systems developed by Sagar were used on \"Monster House\", \"King Kong\", and other films.\n\nThe generation of facial animation data can be approached in different ways: 1.) marker-based motion capture on points or marks on the face of a performer, 2.) markerless motion capture techniques using different type of cameras, 3.) audio-driven techniques, and 4.) keyframe animation.\n\n\nThe main techniques used to apply facial animation to a character are: 1.) morph targets animation, 2.) bone driven animation, 3.) texture-based animation (2D or 3D), and 4.) physiological models.\n\n\nMany face animation languages are used to describe the content of facial animation. They can be input to a compatible \"player\" software which then creates the requested actions. Face animation languages are closely related to other multimedia presentation languages such as SMIL and VRML. Due to the popularity and effectiveness of XML as a data representation mechanism, most face animation languages are XML-based. For instance, this is a sample from Virtual Human Markup Language (VHML):\n\nMore advanced languages allow decision-making, event handling, and parallel and sequential actions. Following is an example from Face Modeling Language (FML):\n\n\n\n"}
{"id": "50597152", "url": "https://en.wikipedia.org/wiki?curid=50597152", "title": "Deborah L. Birx", "text": "Deborah L. Birx\n\nDeborah L. Birx is an American physician and diplomat who currently serves as Ambassador-at-Large and as United States Global AIDS Coordinator. She was nominated by President Barack Obama and confirmed by the Senate. She was sworn in April 4, 2014. In this role she is responsible for PEPFAR's US$6. 6 billion program in 65 countries supporting HIV/AIDS treatment and prevention programs.\n\nBirx is the daughter of Dr. Donald and Adele Sparks Birx. Birx majored in chemistry at Houghton College in 1976 and then earned her medical degree from the Hershey School of Medicine at Pennsylvania State University. In 1980 she began in internal medicine and basic and clinical immunology at the Walter Reed Army Medical Center and the National Institutes of Health.\n\nBirx served as a physician in the United States Army, rising to the rank of Colonel before she retired from military service. She started her career with the United States Department of Defense as a clinician in immunology, focusing on HIV/AIDS vaccine research. She then served as an Assistant Chief of the Hospital Immunology Service at Walter Reed Army Medical Center from 1985 to 1989. In 1996, she became the Director of the United States Military HIV Research Program at the Walter Reed Army Institute of Research, a role she held until 2005. \n\nFrom 2005-2014, Ambassador Birx served as the Director of CDC's Division of Global HIV/AIDS (DGHA), which is part of the agency's Center for Global Health.\n\nIn her role as ambassador she is leading the organization to meet the HIV prevention and treatment targets set by President Barack Obama in 2015 and achieving the goal of ending the AIDS epidemic by 2030. She says that PEPFAR has cut pediatric HIV infection rates by 50 percent in several African countries. \n\nBirx has two adult daughters, Devynn Birx-Raybuck and Danielle Birx-Raybuck.\n"}
{"id": "57388876", "url": "https://en.wikipedia.org/wiki?curid=57388876", "title": "Epigenetics of anxiety and stress-related disorders", "text": "Epigenetics of anxiety and stress-related disorders\n\nEpigenetics of anxiety and stress-related disorders is the field studying the relationship between epigenetic modifications of genes and anxiety and stress-related disorders, including mental health disorders such as generalized anxiety disorder (GAD), post-traumatic stress disorder, obsessive-compulsive disorder (OCD), and more.\n\nEpigenetic modifications play a role in the development and heritability of these disorders and related symptoms. For example, regulation of the hypothalamus-pituitary axis by glucocorticoids plays a major role in stress response and is known to be epigenetically regulated.\n\nAs of 2015 most work has been done in animal models in laboratories, and little work has been done in humans; the work is not yet applicable to clinical psychiatry.\n\nEpigenetic changes are performed by enzymes known as writers, which can add epigenetic modifications, erasers, which erase epigenetic modifications, and readers, which can recognize epigenetic modifications and cause a downstream effect. Stress-induced modifications of these writers, erasers, and readers result in important epigenetic modifications such as DNA methylation and acetylation.\n\nDNA methylation is a type of epigenetic modification in which methyl groups are added to cytosines of DNA. DNA methylation is an important regulator of gene expression and is usually associated with gene repression.\n\nLaboratory studies have found that early life stress in rodents can cause phosphorylation of methyl CpG binding protein 2 (MeCP2), a protein that preferentially binds CpGs and is most often associated with suppression of gene expression. Stress-dependent phosphorylation of MeCP2 causes MeCP2 to dissociate from the promoter region of a gene called \"arginine vasopressin\" (\"avp\"), causing \"avp\" to become demethylated and upregulated. This may be significant because arginine vasopressin is known to regulate mood and cognitive behavior. Additionally, arginine vasopressin upregulates corticotropin-releasing hormone (CRH), which is a hormone important for stress response. Thus, stress-induced upregulation of \"avp\" due to demethylation might alter mood, behavior, and stress responses. Demethylation of this locus can be explained by reduced binding of DNA methyl transferases (DNMT), an enzyme that adds methyl groups to DNA, to this locus.\n\nMeCP2 is known to have interactions with several other enzymes that modify chromatin (for example, HDAC-containing complexes and co-repressors) and in turn regulate activity of genes that modulate stress response either by increasing or decreasing stress tolerance. For example, epigenetic upregulation of genes that increase stress response may cause decreased stress tolerance in an organism. These interactions are dependent on the phosphorylation status of MeCP2, which as previously mentioned, can be altered by stress.\n\nDNA methyltransferase 1 (DNMT1) belongs to a family of proteins known as DNA methyltransferases, which are enzymes that add methyl groups to DNA. DNMT1 is specifically involved in maintaining DNA methylation; hence it is also known as the maintenance methylase DNMT1. DNMT1 aids in regulation of gene expression by methylating promoter regions of genes, causing transcriptional repression of these genes.\n\nDNMT1 is transcriptionally repressed under stress-mimicking exposure both \"in vitro\" and \"in vivo\" using a mouse model. Accordingly, transcriptional repression of DNMT1 in response to long-term stress-mimicking exposure causes decreased DNA methylation, which is a marker of gene acitvation. In particular, there is decreased methylation of a gene called \"fkbp5\", which plays a role in stress response as a glucocorticoid-responsive gene. Thus, chronic stress may cause demethylation and hyperactivation of a stress-related gene, causing increased stress response.\n\nAdditionally, \"DNMT1\" gene locus has increased methylation in individuals who were exposed to trauma and developed post-traumatic stress disorder (PTSD). Increased methylation of \"DNMT1\" did not occur in trauma-exposed individuals who did not develop PTSD. This may indicate an epigenetic phenotype that can differentiate PTSD-susceptible and PTSD-resilient individuals after exposure to trauma.\n\nTranscription factors are proteins that bind DNA and modulate the transcription of genes into RNA such as mRNA, tRNA, rRNA, and more; thus they are essential components of gene activation. Stress and trauma can affect expression of transcription factors, which in turn alter DNA methylation patterns.\n\nFor example, transcription factor nerve growth-induced protein A (NGFI-A, also called NAB1) is up-regulated in response to high maternal care in rodents, and down-regulated in response to low maternal care (a form of early life stress). Decreased NGFI-A due to low maternal care increases methylation of a glucocorticoid receptor promoter in rats. Glucocorticoid is known to play a role in downregulating stress response; therefore, downregulation of glucocorticoid receptor by methylation causes increased sensitivity to stress.\n\nHistone acetylation is a type of epigenetic modification in which acetyl groups are added to lysine on histone tails. Histone acetylation, performed by enzymes known as histone acetyltransferases (HATs), removes the positive charge from lysine and results in gene activation by weakening the histone's interaction with negatively-charged DNA. In contrast, histone deacetylation performed by histone deacetylases (HDACs) results in gene deactivation.\n\nTranscriptional activity and expression of HDACs is altered in response to early life stress. For animals exposed to early life stress, HDAC expression tends to be lower when they are young and higher when they are older. This suggests an age-dependent effect of early life stress on HDAC expression. These HDACs may result in deacetylation and thus activation of genes that upregulate stress response and decrease stress tolerance.\n\nGenome-wide association studies have shown that psychiatric disorders are partly heritable; however, heritability cannot be fully explained by classical Mendelian genetics. Epigenetics has been postulated to play a role. This is because there is strong evidence of transgenerational epigenetic effects in general. For example, one study found transmission of DNA methylation patterns from fathers to offspring during spermatogenesis. More specifically to mental illnesses, several studies have shown that traits of psychiatric illnesses (such as traits of PTSD and other anxiety disorders) can be transmitted epigenetically. Parental exposure to various stimuli, both positive and negative, can cause these transgenerational epigenetic and behavioral effects.\n\nTrauma and stress experienced by a parent can cause epigenetic changes to its offspring. This has been observed both in population and experimental studies.\n\nAn epidemiological study investigating behavioral, physiological, and molecular changes in the children of Holocaust survivors found epigenetic modifications of a glucocorticoid receptor gene, \"Nr3c1\". This is significant because glucocorticoid is a regulator of the hypothalamus-pituitary-adrenal axis (HPA) and is known to affect stress response. These stress-related epigenetic changes were accompanied by other characteristics that indicated higher stress and anxiety in these offspring, including increased symptoms of PTSD, greater risk of anxiety, and higher levels of the stress hormone cortisol.\n\nThe effect of parental exposure to stress has been tested experimentally as well. For example, male mice who were put under early life stress through poor maternal care—a scenario analogous to human childhood trauma—passed on epigenetic changes that resulted in behavioral changes in offspring. Offspring experienced altered DNA methylation of stress-response genes such as CB1 and CRF2 in the cortex, as well as epigenetic alterations in transcriptional regulation gene MeCP2. Offspring were also more sensitive to stress, which is in accordance with the altered epigenetic profile. These changes persisted for up to three generations.\n\nIn another example, male mice were socially isolated as a form of stress. Offspring of these mice had increased anxiety in response to stressful conditions, increased stress hormone levels, dysregulation of the HPA axis which plays a key role in stress response, and several other characteristics that indicated increased sensitivity to stress.\n\nStudies have found that early life stress induced through poor maternal care alters sperm epigenome in male mice. In particular, expression patterns of small-noncoding RNAs (sncRNAs) are altered in the sperm, as well as in stress-related regions of the brain. Offspring of these mice exhibited the same sncRNA expression changes in the brain, but not in the sperm. These changes were coupled with behavioral changes in the offspring that were comparable to behavior of the stressed fathers, especially in terms of stress response. Additionally, when the sncRNAs in the fathers' sperm were isolated and injected into fertilized eggs, the resulting offspring inherited the stress behavior of the father. This suggests that stress-induced modifications of sncRNAs in sperm can cause inheritance of stress phenotype independent of the father's DNA.\n\nJust as parental stress can alter epigenetics of offspring, parental exposure to positive environmental factors cause epigenetic modifications as well. For example, male mice that participated in voluntary physical exercise resulted in offspring that had reduced fear memory and anxiety-like behavior in response to stress. This behavioral change likely occurred due to expressions of small non-coding RNAs that were altered in sperm cells of the fathers.\n\nAdditionally, exposing fathers to enriching environments can reverse the effect of early life stress on their offspring. When early life stress is followed by environmental enrichment, anxiety-like behavior in offspring is prevented. Similar studies have been conducted in humans and suggest that DNA methylation plays a role.\n\nPost-traumatic stress disorder (PTSD) is a stress-related mental health disorder that emerges in response to traumatic or highly stressful experiences. It is believed that PTSD develops as a result of an interaction between these traumatic experiences and genetic factors. Evidence suggests epigenetics is a key element in this.\n\nThrough a number of human studies, PTSD is known to affect DNA methylation of cytosine residues in a several genes involved in stress response, neurotransmitter activity, and more.\nThe hypothalamus-pituitary-adrenal (HPA) axis plays a key role in stress response. Based on several findings, the HPA axis appears to be dysregulated in PTSD. A common pathway dysregulated in HPA axis involves a hormone known as glucocorticoid and its receptor, which aid in stress tolerance by downregulating stress response. Dysregulation of glucocorticoid and/or glucocorticoid receptor can disrupt stress tolerance and increase risk of stress-related disorders such as PTSD. Epigenetic modifications play a role in this dysregulation, and these modifications are likely caused by the traumatic/stressful experience that triggered PTSD.\n\n\"Nr3c1\" is a gene that encodes a glucocorticoid receptor (GR) and contains many GR response elements. Early life stress increases methylation of the 1 promoter in this gene (or the 1 promoter analog in rodents). Because of its role in stress response and its link to early life stress, this gene has been of particular interest in the context of PTSD and has been studied in PTSD of both combat veterans and civilians.\n\nIn studies involving combat veterans, those who developed PTSD had lowered methylation of the \"Nr3c1\" 1 promoter compared to those who did not develop PTSD. Additionally, veterans who developed PTSD and had higher \"Nr3c1\" promoter methylation responded better to long-term psychotherapy compared to veterans with PTSD who had lower methylation. These findings were recapitulated in studies involving civilians with PTSD. In civilians, PTSD is linked to lower methylation levels in the T-cells of exons 1 and 1 of \"Nr3c1\", as well as higher GR expression. Thus, it seems that PTSD causes lowered methylation levels of GR loci and increased GR expression.\n\nAlthough these results of decreased methylation and hyperactivation of GR conflict with the effect of early life stress at the same loci, these results match previous findings that distinguish HPA activity in early life stress versus PTSD. For example, cortisol levels of HPA in response to early life stress is hyperactive, whereas it is hypoactive in PTSD. Thus, the timing of trauma and stress—whether early or later in life—can cause differing effects on HPA and GR.\n\n\"Fkbp5\" encodes a GR-responsive protein known as Fk506 binding protein 51 (FKBP5). FKBP5 is induced by GR activation and functions in negative feedback by binding GR and reducing GR signaling. There is particular interest in this gene because some \"FKBP5\" alleles have been correlated with increased risk of PTSD and development of PTSD symptoms—especially in PTSD caused by early life adversity. Therefore, \"FKBP5\" likely plays an important role in PTSD.\n\nAs mentioned previously, certain \"FKBP5\" alleles are correlated to increase PTSD risk, especially due to early life trauma. It is now known that epigenetic regulation of these alleles is also an important factor. For example, CpG sites in intron 7 of FKBP5 are demethylated after exposure to childhood trauma, but not adult trauma. Additionally, methylation of FKBP5 is alters in response to PTSD treatment; thus methylation levels of FKBP5 might correspond to PTSD disease progression and recovery.\n\nPituitary adenylate cyclase-activating polypeptide (ADCYAP1) and its receptor (ADCYAP1R1) are stress responsive genes that play a role in modulating stress, among many other functions. Additionally, high levels of ADCYAP1 in peripheral blood is correlated to PTSD diagnosis in females who have experienced trauma, thus making ADCYAP1 a gene of interest in the context of PTSD.\n\nEpigenetic regulation of these loci in relation to PTSD still require further investigation, but one study has found that high methylation levels of CpG islands in ADCYAP1R1 can predict PTSD symptoms in both males and females.\n\nPTSD is often linked with immune dysregulation. This is because trauma exposure can disrupt the HPA axis, thus altering peripheral immune function.\n\nEpigenetic modifications have been observed in immune-related genes of individuals with PTSD. For example, deployed military members who developed PTSD have higher methylation in immune-related gene interleukin-18 (IL-18). This has interested scientists because high levels of IL-18 increase cardiovascular disease risk, and individuals with PTSD have elevated cardiovascular disease risk. Thus, stress-induced immune dysregulation via methylation of IL-18 may play a role in cardiovascular disease in individuals with PTSD.\n\nAdditionally, an epigenome-wide study found that individuals with PTSD have altered levels of methylation in the following immune-related genes: TPR, CLEC9A, APC5, ANXA2, TLR8, IL-4, and IL-2. This again shows that immune function in PTSD is disrupted, especially by epigenetic changes that are likely stress-induced.\n\nAlcohol dependence and stress interact in many ways. For example, stress-related disorders such as anxiety and PTSD are known to increase risk of alcohol use disorder (AUD), and they are often co-morbid. This may in part be due to the fact that alcohol can alleviate some symptoms of these disorders, thus promoting dependence on alcohol. Conversely, early exposure to alcohol can increase vulnerability to stress and stress-related disorders. Moreover, alcohol dependence and stress are known to follow similar neuronal pathways, and these pathways are often dysregulated by similar epigenetic modifications.\n\nHistone acetylation is dysregulated by alcohol exposure and dependence, often through dysregulated expression and activity of HDACs, which modulate histone acetylation by removing acetyl groups from lysines of histone tails. For example, HDAC expression is upregulated in chronic alcohol use models. Monocyte-derived dendritic cells of alcohol users have increased HDAC gene expression compared to non-users. These results are also supported by \"in vivo\" rat studies, which show that HDAC expression is higher in alcohol-dependent mice that in non-dependent mice. Furthermore, knockout of HDAC2 in mice helps lower alcohol dependence behaviors. The same pattern of HDAC expression is seen in alcohol withdrawal, but acute alcohol exposure has the opposite effect; \"in vivo\", HDAC expression and histone acetylation markers are decreased in the amygdala.\n\nDysregulation of HDACs is significant because it can cause upregulation or downregulation of genes that have important downstream effects both in alcohol dependence and anxiety-like behaviors, and the interaction between the two. A key example is BDNF (see \"BDNF\" below).\n\nBrain-derived neurotrophic factor (BDNF) is a key protein that is dysregulated by HDAC dysregulation. BDNF is a protein that regulates the structure and function of neuronal synapses. It plays an important role in neuronal activation, synaptic plasticity, and dendritic morphology—all of which are factors that may affect cognitive function. Dysregulation of BDNF is seen both in stress-related disorders and alcoholism; thus BDNF is likely an important molecule in the interaction between stress and alcoholism.\n\nFor example, BDNF is dysregulated by acute ethanol exposure. Acute ethanol exposure causes phosphorylation of CREB, which can cause increased histone acetylation at BDNF loci. Histone acetylation upregulates BDNF, in turn upregulating a downstream BDNF target called activity-regulated cytoskeleton associated protein (Arc), which is a protein responsible for dendritic spine structure and formation. This is significant because activation of Arc can be associated with anxiolytic (anxiety-reducing) effects. Therefore, ethanol consumption can cause epigenetic changes that alleviate stress and anxiety, thereby creating a pattern of stress-induced alcohol dependence.\n\nAlcohol dependence is exacerbated by ethanol withdrawal. This is because ethanol withdrawal has the opposite effect of ethanol exposure; it causes lowered CREB phosphorylation, lowered acetylation, downregulation of BDNF, and increase in anxiety. Consequently, ethanol withdrawal reinforces desire for anxiolytic effects of ethanol exposure. Moreover, it is proposed that chronic ethanol exposure results in upregulation of HDAC activity, causing anxiety-like effects that can no longer be alleviated by acute ethanol exposure.\n\n\n"}
{"id": "1468375", "url": "https://en.wikipedia.org/wiki?curid=1468375", "title": "Fat fetishism", "text": "Fat fetishism\n\nFat fetishism is sexual attraction to overweight or obese people due to their weight and size.\n\nA variety of fat fetishism is \"feederism\" or \"gaining\", where sexual gratification is obtained not from the fat itself but from the process of gaining, or helping others gain, body fat. Fat fetishism also incorporates \"stuffing\" and \"padding\", whereas the focus of arousal is on the sensations and properties of a real or \"simulated\" gain.\n\nA 2009 study found that heterosexual male fat admirers preferred females that were clinically overweight and rated both overweight and obese women more positively than slighter individuals. The study also found that participants reacted positively to a much wider range of figures than a control group, even rating emaciated figures higher. It concludes \"these findings suggest that an explanation for fat admiration may be that FAs are rejecting sociocultural norms of attractiveness\".\n\nGainers and feedees are people who enjoy the fantasy or reality of gaining weight themselves. Encouragers and feeders enjoy the fantasy of helping someone else gain weight. Gainer and encourager are common labels among gay men, while both straight men and women as well as lesbian women often identify as feeders and feedees. Gainers and feedees have a wide array of personal weight-gain goals--only 10 percent of gainers and 13 percent of feedees express interest in immobility as a fantasy or reality.\n\nWhile gaining and feeding are often considered fetishes, many within the gainer and feederism communities report viewing them more as a lifestyle, identity or sexual orientation. \n\nThe gay gainer community grew out of the Girth & Mirth movement in the 70s. By 1988 there were gainer-specific newsletters and in 1992, the first gainer event, called EncourageCon, was held in New Hope, PA. In 1996, GainRWeb launched, the first website dedicated to gay men into weight gain, ushering in the internet era.\n\n\n"}
{"id": "9796766", "url": "https://en.wikipedia.org/wiki?curid=9796766", "title": "French Connection", "text": "French Connection\n\nThe French Connection was a scheme through which heroin was smuggled from Turkey to France and then to the United States through Canada. The operation reached its peak in the late 1960s and early 1970s, and was responsible for providing the vast majority of the heroin used in the United States. The operation was headed by Corsican criminals Paul Carbone (and his associate François Spirito) and Antoine Guérini, and also involved Auguste Ricord, Paul Mondoloni and Salvatore Greco. Most of the operation's starting capital came from assets that Ricord had stolen during World War II when he worked for Henri Lafont, one of the heads of the Carlingue (French Gestapo) during the German occupation in World War II.\n\nIllegal heroin labs were first discovered near Marseille, France, in 1937. These labs were run by Corsican gang leader Paul Carbone. For years, the Corsican underworld had been involved in the manufacturing and trafficking of heroin, primarily to the United States. It was this heroin network that eventually became known as \"the French Connection\".\n\nThe Corsican Gang was protected by the Central Intelligence Agency (CIA) and the SDECE after World War II in exchange for working to prevent French Communists from bringing the Old Port of Marseille under their control.\n\nHistorically, the raw material for most of the heroin consumed in the United States came from Indochina, then Turkey. Turkish farmers were licensed to grow opium poppies for sale to legal drug companies, but many sold their excess to the underworld market, where it was manufactured into heroin and transported to the United States. The morphine paste was refined in Corsican laboratories in Marseille, one of the busiest ports in the western Mediterranean Sea. The Marseille heroin was reputed for its quality.\n\nMarseille served as a perfect shipping point for all types of illegal goods, including the excess opium that Turkish farmers cultivated for profit. The convenience of the port at Marseille and the frequent arrival of ships from opium-producing countries made it easy to smuggle the morphine base to Marseille from the Far East or the Near East. The French underground would then ship large quantities of heroin from Marseille to New York City.\n\nThe first significant post-World War II seizure was made in New York on February 5, 1947, when seven pounds (3 kg) of heroin were seized from a Corsican sailor disembarking from a vessel that had just arrived from France.\n\nIt soon became clear that the French underground was increasing not only its participation in the illegal trade of opium, but also its expertise and efficiency in heroin trafficking. On March 17, 1947, 28 pounds (13 kg) of heroin were found on the French liner, \"St. Tropez\". On January 7, 1949, more than 50 pounds (22.75 kg) of opium and heroin were seized on the French ship, \"Batista\".\n\nAfter Paul Carbone's death, the Guérini clan was the ruling dynasty of the Unione Corse and had systematically organized the smuggling of opium from Turkey and other Middle Eastern countries. The Guérini clan was led by Marseilles mob boss Antoine Guérini and his brothers, Barthelemy, Francois and Pascal.\n\nThe first major French Connection case occurred in 1960. In June, an informant told a drug agent in Lebanon that Mauricio Rosal, the Guatemalan Ambassador to Belgium, the Netherlands and Luxembourg, was smuggling morphine base from Beirut, Lebanon to Marseille. Narcotics agents had been seizing about 200 pounds (90 kg) of heroin in a typical year, but intelligence showed that the Corsican traffickers were smuggling in 200 pounds (90 kg) every other week. Rosal alone, in one year, had used his diplomatic status to bring in about 440 pounds (200 kg).\n\nThe Federal Bureau of Narcotics's 1960 annual report estimated that from 2,600 to 5,000 pounds (1,200 to 2,300 kg) of heroin were coming into the United States annually from France. The French traffickers continued to exploit the demand for their illegal product, and by 1969, they were supplying the United States with 80 to 90 percent of its heroin.\n\nBecause of this increasing volume, heroin became readily available throughout the United States. In an effort to limit the source, US officials went to Turkey to negotiate the phasing out of opium production. Initially, the Turkish government agreed to limit their opium production starting with the 1968 crop.\n\nAt the end of the 1960s, after Robert Blemant's assassination by Antoine Guerini, a gang war sparked in Marseille, caused by competition over casino revenues. Blemant's associate, Marcel Francisci, continued the war over the next years.\n\nFormer New York City Police Department Narcotics Bureau detective Sonny Grosso has stated that the kingpin of the French Connection heroin ring during the 1950s into the 1960s was Corsican Jean Jehan. Although Jehan arranged the famous 1962 deal gone wrong of 64 pounds of \"pure\" heroin, he was never arrested for his involvement in international heroin smuggling. According to Grosso, all warrants for the arrest of Jehan were left open. For years thereafter, Jehan was reported to be seen arranging and operating drug activities at will throughout Europe. According to William Friedkin, director of the 1971 film \"The French Connection\", Jehan had been a member of the French Resistance to Nazi Occupation during World War II and, because of that, French law enforcement officials refused to arrest him. Friedkin was told that Jehan died peacefully of old age at his home in Corsica.\n\nFollowing five subsequent years of concessions, combined with international cooperation, the Turkish government finally agreed in 1971 to a complete ban on the growing of Turkish opium, effective June 29, 1971. During these protracted negotiations, law enforcement personnel went into action. One of the major roundups began on January 4, 1972, when agents from the U.S. Bureau of Narcotics and Dangerous Drugs (BNDD) and French authorities seized 110 pounds (50 kg) of heroin at the Paris airport. Subsequently, traffickers Jean-Baptiste Croce and Joseph Mari were arrested in Marseille. One such French seizure from the French Connection in 1973 netted 210 pounds (95 kg) of heroin worth $38 million.\n\nIn February 1972, French traffickers offered a United States Army sergeant $96,000 () to smuggle 240 pounds (109 kg) of heroin into the United States. He informed his superior who in turn notified the BNDD. As a result of this investigation, five men in New York and two in Paris were arrested with 264 pounds (120 kg) of heroin, which had a street value of $50 million. In a 14-month period, starting in February 1972, six major illicit heroin laboratories were seized and dismantled in the suburbs of Marseille by French national narcotics police in collaboration with U.S. drug agents. On February 29, 1972, French authorities seized the shrimp boat, \"Caprice des Temps\", as it put to sea near Marseille heading towards Miami. It was carrying 915 pounds (415 kg) of heroin. Drug arrests in France skyrocketed from 57 in 1970 to 3,016 in 1972. Also broken up as part of this investigation was the crew of Vincent Papa, whose members included Anthony Loria Sr. and Virgil Alessi. The well-organized gang was responsible for distributing close to a million dollars worth of heroin up and down the East Coast of the United States during the early 1970s, which in turn led to a major New York Police Department (NYPD) corruption scheme. The scope and depth of this scheme are still not known, but officials suspect it involved corrupt NYPD officers who allowed Papa, Alessi, and Loria access to the NYPD property/evidence storage room, where hundreds of kilograms of heroin lay seized from the now-infamous French Connection bust, and from which the men would help themselves and replace missing heroin with flour and cornstarch to avoid detection.\n\nThe substitution was discovered only when officers noticed insects eating all the bags of \"heroin\". By that point an estimated street value of approximately $70 million worth of heroin had already been taken. The racket was brought to light and arrests were made. Certain plotters received jail sentences, including Papa, who was later murdered in federal prison in Atlanta, Georgia.\n\nUltimately, the Guérini clan was exterminated during internecine wars within the French underworld. In 1971, Marcel Francisci was accused by police forces in the U.S. Bureau of Narcotics of being involved in the trafficking of heroin between Marseilles and New York City. On 16 January 1982, Marcel Francisci was shot to death as he was entering his car in the parking lot of the building where he lived in Paris, France.\n\n\n\n\n\n"}
{"id": "54711924", "url": "https://en.wikipedia.org/wiki?curid=54711924", "title": "Friedrich Weleminsky", "text": "Friedrich Weleminsky\n\nDr Joseph Friedrich (\"Fritz\") Weleminsky (20 January 1868, Golčův Jeníkov1 January 1945, London), was a physician, a scientist and a \"privatdozent\" in Hygiene (now called Microbiology) at the German University, Prague who, in the early 20th century, created an alternative treatment for tuberculosis, \"tuberculomucin Weleminsky\".\n\nHe was born into a Jewish family on 20 January 1868 at Golčův Jeníkov in Bohemia, which was then part of the Austrian Empire and is now in the Czech Republic. His parents were Jacob Weleminsky (1834–1905), a general medical practitioner (GP) in Golčův Jeníkov, and his wife Bertha (\"née\" Kohn; 1844–1914). Friedrich was their second child; he had an elder sister, Paula (1867–1936), who in 1888 married a Dresden lawyer, Felix Popper, and a younger brother, Josef (\"Pepi\") (1870–1937) who, like Friedrich, studied medicine in Prague and who went on to become a laryngologist.\n\nThe family moved to Dresden in 1879 when Jacob obtained a position as GP there, and later to Prague. Friedrich attended the Kreuzschule in Dresden and studied medicine in Prague.\n\nFriedrich Weleminsky enrolled in the medical faculty of the German University in Prague in 1893 and obtained a habilitation qualification as Dr.Med. in 1900. He was appointed to a teaching post in the university's medical faculty as a \"privatdozent\" in Hygiene in July 1900.\n\nDuring the First World War, Weleminsky was in charge of the reserve hospital \"Halicz\" which was stationed in various parts of Austria and Hungary. While stationed in Kleinreifling, a village in the district of Steyr-Land in Upper Austria, he successfully brought a local typhoid epidemic under control, for which he was made an Ehrenbürger of\nWeyer.\n\nWeleminsky's particular area of interest was vaccination against tuberculosis. In 1935, an editorial in the \"American Journal of Clinical Pathology\" cited one of his articles as providing \"a good review of the voluminous literature accumulated on BCG\".\n\nIn 1912 Weleminsky, who was then second assistant to Ferdinand Hueppe, the head of the Institute for Hygiene at the German University of Prague, published his discovery of a new treatment for tuberculosis, which he named tuberculomucin (Tbm). It was tested on guinea pigs, with number 1769 being the first to survive due to the treatment in 1909. He also used \"tuberculomucin Weleminsky\" (also spelt \"tuberkulomucin Weleminsky\" and \"tuberkulomuzin Weleminsky\") to treat cattle which he kept at his country retreat, Schloss Thalheim.\n\nMore than 60 papers were published in German describing tuberculomucin's use in humans, but very few of them were read by an English-speaking audience. By the mid-1920s it was known as \"tuberculomucin Weleminsky\" and at least two companies were involved in producing and marketing the treatment. In 1938, Sanders, a Belgian pharmaceutical company, planned to manufacture Tbm and to make it available in Western Europe and other parts of the developed world. However, Weleminsky fled from Prague in 1939, a couple of weeks before the Nazi invasion of Czechoslovakia, and these plans and further development of the treatment ceased.\n\nOn 4 December 1905 he married Jenny Elbogen (1882–1957), at her parents' country home, Schloss Thalheim, Lower Austria. The married couple lived in Prague and at Schloss Thalheim, which Jenny inherited from her father after his death in 1918 and which they ran as a model dairy farm.\n\nThey had four children together. Their eldest daughter, Marianne (born 1906), and their son, Anton (born 1908), came to Britain just before the Second World War. Two of their daughters emigrated in the early 1930s to Mandatory Palestine where they took new names – Eliesabeth (born 1909) became Jardenah, and Dorothea (born 1912) was known as Leah. Like his wife, Friedrich Weleminsky was a secular Jew and an atheist but he did not share her opposition to Zionism. Jenny ceased all contact with the two daughters after they left Austria to live in Palestine, but Friedrich communicated with them clandestinely through their sister Marianne.\n\nFacing Nazi persecution for being Jewish, Friedrich and Jenny Weleminsky found sanctuary in 1939 in Britain.\n\nFriedrich Weleminsky died of pneumonia on 1 January 1945 at Fulham Hospital, London and is buried at Golders Green Jewish Cemetery. His wife Jenny, who was 14 years younger, survived him by 12 years. Their grandchildren and great-grandchildren now live in Britain, Israel, Australia, Sweden and Germany.\n\nIn 2011, following an approach by Weleminsky’s eldest granddaughter, Dr Charlotte Jones, a retired general practitioner, a team at the University College London's Department of Science and Technology Studies resumed research on \"tuberculomucin Weleminsky\".\n\n\n"}
{"id": "45449726", "url": "https://en.wikipedia.org/wiki?curid=45449726", "title": "Fungistatics", "text": "Fungistatics\n\nFungistatics are anti-fungal agents that inhibit the growth of fungus (without killing the fungus). The term \"fungistatic\" may be used as both a noun and an adjective. Fungistatics have applications in agriculture, the food industry, the paint industry, and medicine.\n\nFluconazole is a fungistatic antifungal medication that is administered orally or intravenously. It is used to treat a variety of fungal infections, especially Candida infections of the vagina (\"yeast infections'), mouth, throat, and bloodstream. It is also used to prevent infections in people with weak immune systems, including those with neutropenia due to cancer chemotherapy, transplant patients, and premature babies. Its mechanism of action involves interfering with synthesis of the fungal cell membrane.\n\nItraconazole (R51211), invented in 1984, is a triazole fungistatic antifungal agent prescribed to patients with fungal infections. The drug may be given orally or intravenously. Itraconazole has a broader spectrum of activity than fluconazole (but not as broad as voriconazole or posaconazole). In particular, it is active against Aspergillus, which fluconazole is not. The mechanism of action of itraconazole is the same as the other azole antifungals: it inhibits the fungal-mediated synthesis of ergosterol.\n\nSodium benzoate and potassium sorbate are both examples of fungistatic substances that are widely used in the preservation of food and beverages.\n\n"}
{"id": "57731681", "url": "https://en.wikipedia.org/wiki?curid=57731681", "title": "Gerard Macklin", "text": "Gerard Macklin\n\nGerard Macklin (c.1767 - 9 August 1848) was the president of the Royal College of Surgeons in Ireland (RCSI) in 1806.\n\n"}
{"id": "20348862", "url": "https://en.wikipedia.org/wiki?curid=20348862", "title": "HIV/AIDS in New Zealand", "text": "HIV/AIDS in New Zealand\n\nThere is a relatively low prevalence of HIV/AIDS in New Zealand, with an estimated 2,900 people out a population of 4.51 million living with HIV/AIDS as of 2014. The rate of newly diagnosed HIV infections was stable at around 100 annually through the late 1980s and the 1990s but rose sharply from 2000 to 2005. It has since stabilised at roughly 200 new cases annually. Male-to-male sexual contact has been the largest contributor to new HIV cases in New Zealand since record began in 1985. Heterosexual contact is the second largest contributor to new cases, but unlike male-to-male contact, they are mostly acquired outside New Zealand.\n\nThe first recorded death in New Zealand due to AIDS was in New Plymouth in 1983.\n\nIn 1985 Eve van Grafhorst was ostracised in Australia since she had contracted HIV/AIDS caused by a transfusion of infected blood. The family moved to New Zealand where she died at the age of 11. By the time of her death, her plight had significantly raised the level of AIDS awareness in New Zealand.\n\nWorld AIDS Day is observed in New Zealand.\n\nThe Ministry of Health is the government department which deals with health issues, including HIV/AIDS.\n\nThe New Zealand AIDS Foundation is a registered charitable trust which focuses on prevention of AIDS in the most at-risk group, namely men who have sex with men.\n\nThe Pharmaceutical Management Agency (Pharmac) manages the national schedule of subsidised medications. As of 2014, twenty-one different antiretroviral medications were subsidised for people with confirmed HIV/AIDS or for post-exposure prophylaxis.\n\nIn March 2018, New Zealand became one of the first countries in the world to publically fund pre-exposure prophylaxis medication for those at a high risk of contracting HIV.\n\n\n"}
{"id": "23433314", "url": "https://en.wikipedia.org/wiki?curid=23433314", "title": "Health in Ivory Coast", "text": "Health in Ivory Coast\n\nThe public medical services of Ivory Coast are more important than the small number of private physicians and clinics. , there were an estimated 9 physicians, 31 nurses, and 15 midwives per 100,000 people. About 77 percent of the population had access to safe water in 2000. Total health care expenditures were estimated at 3.7 percent of GDP.\n\nMalaria, yellow fever, sleeping sickness, yaws, leprosy, trachoma, and meningitis are endemic. A broad program was set up in 1961 to control these and other diseases. Compulsory vaccination against smallpox and yellow fever was instituted, efforts by mobile health units to track down cases and provide treatment were intensified, and general health measures were tightened both within the country and at the borders. In 1999, Ivory Coast immunized children up to one year old as follows: diphtheria, pertussis, and tetanus, 62 percent, and measles, 62 percent. Malnutrition affected 24 percent of children under the age of five.\n\nThe HIV/AIDS in Ivory Coast prevalence was 0.60 per 100 adults in 2003. As of 2004, there were approximately 570,000 people living with HIV/AIDS in the country. There were an estimated 47,000 deaths from AIDS in 2003.\n\nAccording to the CIA World Factbook, in Ivory there are a total of 460,100 people who live with HIV/AIDS as of 2014. \n\nThe high incidence of HIV/AIDS is attributed to a lack of HIV education programs.\n\nThe 2010 maternal mortality rate per 100,000 births for Ivory Coast is 470. This is compared with 944.1 in 2008 and 580.3 in 1990. The under 5 mortality rate, per 1,000 births is 121 and the neonatal mortality as a percentage of under 5's mortality is 33. In Ivory Coast the number of midwives per 1,000 live births is 4 and the lifetime risk of death for pregnant women 1 in 44.\n\nAbout 36% of women have undergone female genital mutilation (as of 2006). The birth rate in 1999 was 41.8 per 1,000. The infant mortality rate in 2005 was 90.83 per 1,000 live births, and 14 percent of all births were classified as low weight. In 2005, average life expectancy in Ivory Coast was estimated at 48.62.\n\nThere are a lot of disease and illness caused by hunger. Ivory Coast decided to contain some diseases or illness in their National Development Plan. These were all problems relating to hunger. Whether it is not being able to access the right food or starving from hunger. \n\nDiseases or illnesses that Ivory Coast wants to reduce:\n\n\n\n"}
{"id": "23393445", "url": "https://en.wikipedia.org/wiki?curid=23393445", "title": "Health in the Central African Republic", "text": "Health in the Central African Republic\n\nIn the Central African Republic, mobile crews treat local epidemic diseases, conduct vaccination and inoculation campaigns, and enforce local health regulations.\nThey conduct research on sleeping sickness, malaria, and other tropical diseases and devise prophylactic methods best suited to the rural population.\n\nIn 2004, it was estimated that there were fewer than 3 physicians and 9 nurses per 100,000 people. In 2000, 60 percent of the population had access to safe drinking water and 31% had adequate sanitation.\n\nThe 2014 CIA estimated average life expectancy in the Central African Republic was 51.81 years.\n\nThe most common diseases are bilharziasis, leprosy, malaria, tuberculosis, poliomyelitis and yaws. The Central African Republic is a yellow fever endemic zone country. The Pasteur Institute at Bangui cooperates actively with vaccination campaigns. All medicine, antibiotics, and vaccine imports must be authorized by the Ministry of Health.\n\nAs of 1999, the immunization rates for children up to one year old were as follows: diphtheria, pertussis, and tetanus, 33 percent and measles, 39%.\n\nThe 2010 maternal mortality rate per 100,000 births for Central African Republic is 850. This is compared with 1570.4 in 2008 and 1757.1 in 1990. The under 5 mortality rate, per 1,000 births is 172 and the neonatal mortality as a percentage of under 5's mortality is 26. In the Central African Republic the number of midwives per 1,000 live births is 3 and the lifetime risk of death for pregnant women 1 in 27.\n\nThe Central African Republic is one of several African nations with a high incidence of AIDS. The HIV/AIDS prevalence was 13.50 per 100 adults in 2003. As of 2004, there were approximately 260,000 people living with HIV/AIDS in the country. There were an estimated 23,000 deaths from AIDS in 2003. This number dropped to 13,000 deaths from HIV/AIDS in 2007. and 11,000 in 2009. 160,000 people with HIV/AIDS were reported in 2007, and 140,000 in 2009, for a prevalence rate of about 3.2%. By 2016, the prevalence rate had dropped to about 2.8%, although the prevalence in gay males was over 25%.\n\n"}
{"id": "993407", "url": "https://en.wikipedia.org/wiki?curid=993407", "title": "Hibakusha", "text": "Hibakusha\n\nThe Atomic Bomb Survivors Relief Law defines \"hibakusha\" as people who fall into one or more of the following categories: within a few kilometers of the hypocenters of the bombs; within 2 km of the hypocenters within two weeks of the bombings; exposed to radiation from fallout; or not yet born but carried by pregnant women in any of these categories. The Japanese government has recognized about 650,000 people as \"hibakusha\". , 154,859 were still alive, mostly in Japan. The government of Japan recognizes about 1% of these as having illnesses caused by radiation. \"Hibakusha\" are entitled to government support. They receive a certain amount of allowance per month, and the ones certified as suffering from bomb-related diseases receive a special medical allowance.\n\nThe memorials in Hiroshima and Nagasaki contain lists of the names of the \"hibakusha\" who are known to have died since the bombings. Updated annually on the anniversaries of the bombings, , the memorials record the names of almost 495,000 \"hibakusha\"; 314,118 in Hiroshima and 179,226 in Nagasaki.\n\nIn 1957, the Japanese Parliament passed a law providing for free medical care for \"hibakusha\". During the 1970s, non-Japanese \"hibakusha\" who suffered from those atomic attacks began to demand the right for free medical care and the right to stay in Japan for that purpose. In 1978, the Japanese Supreme Court ruled that such persons were entitled to free medical care while staying in Japan.\n\nDuring the war, Japan brought many Korean conscripts to both Hiroshima and Nagasaki to work as slaves. According to recent estimates, about 20,000 Koreans were killed in Hiroshima and about 2,000 died in Nagasaki. It is estimated that one in seven of the Hiroshima victims was of Korean ancestry. For many years, Koreans had a difficult time fighting for recognition as atomic bomb victims and were denied health benefits. However, most issues have been addressed in recent years through lawsuits.\n\nIt was a common practice before the war for American Issei, or first-generation immigrants, to send their children on extended trips to Japan to study or visit relatives. More Japanese immigrated to the U.S. from Hiroshima than from any other prefecture, and Nagasaki also sent a high number of immigrants to Hawai'i and the mainland. There was, therefore, a sizable population of American-born Nisei and Kibei living in their parents' hometowns of Hiroshima and Nagasaki at the time of the atomic bombings. The actual number of Japanese Americans affected by the bombings is unknown – although estimates put approximately 11,000 in Hiroshima city alone – but some 3,000 of them are known to have survived and returned to the U.S. after the war.\n\nA second group of \"hibakusha\" counted among Japanese American survivors are those who came to the U.S. in a later wave of Japanese immigration during the 1950s and 1960s. Most in this group were born in Japan and migrated to the U.S. in search of educational and work opportunities that were scarce in post-war Japan. Many were \"war brides\", or Japanese women who had married American men related to the U.S. military's occupation of Japan.\n\nAs of 2014, there are about 1,000 recorded Japanese American \"hibakusha\" living in the United States. They receive monetary support from the Japanese government and biannual medical checkups with Hiroshima and Nagasaki doctors familiar with the particular concerns of atomic bomb survivors. The U.S. government provides no support to Japanese American \"hibakusha\".\n\nWhile one British Commonwealth citizen\nand seven Dutch POWs (two names known) died in the Nagasaki bombing, at least two POWs reportedly died postwar from cancer thought to have been caused by the atomic bomb.\nOne American POW, Joe Kieyoomia, was in Nagasaki at the time of the bombing but survived, reportedly having been shielded from the effects of the bomb by the concrete walls of his cell.\n\nPeople who suffered the effects of both bombings are known as \"nijū hibakusha\" in Japan.\n\nA documentary called \"Twice Survived: The Doubly Atomic Bombed of Hiroshima and Nagasaki\" was produced in 2006. The producers found 165 people who were victims of both bombings, and the production was screened at the United Nations.\n\nOn March 24, 2009, the Japanese government officially recognized Tsutomu Yamaguchi (1916–2010) as a double \"hibakusha\". Tsutomu Yamaguchi was confirmed to be 3 kilometers from ground zero in Hiroshima on a business trip when the bomb was detonated. He was seriously burnt on his left side and spent the night in Hiroshima. He got back to his home city of Nagasaki on August 8, a day before the bomb in Nagasaki was dropped, and he was exposed to residual radiation while searching for his relatives. He was the first officially recognized survivor of both bombings. Tsutomu Yamaguchi died at the age of 93 on January 4, 2010, of stomach cancer.\n\n\"Hibakusha\" and their children were (and still are) victims of severe discrimination when it comes to prospects of marriage or work due to public ignorance about the consequences of radiation sickness, with much of the public believing it to be hereditary or even contagious. This is despite the fact that no statistically demonstrable increase of birth defects/congenital malformations was found among the later conceived children born to survivors of the nuclear weapons used at Hiroshima and Nagasaki, or found in the later conceived children of cancer survivors who had previously received radiotherapy.\nThe surviving women of Hiroshima and Nagasaki, that could conceive, who were exposed to substantial amounts of radiation, went on and had children with no higher incidence of abnormalities/birth defects than the rate which is observed in the Japanese average.\n\nStuds Terkel's book \"The Good War\" includes a conversation with two \"hibakusha\". The postscript observes:\n\nThe is a group formed by \"hibakusha\" in 1956 with the goals of pressuring the Japanese government to improve support of the victims and lobbying governments for the abolition of nuclear weapons.\n\nSome estimates are that 140,000 people in Hiroshima (38.9% of the population) and 70,000 people in Nagasaki (28.0% of the population) died in 1945, but how many died immediately as a result of exposure to the blast, heat, or due to radiation, is unknown. One Atomic Bomb Casualty Commission report discusses 6,882 people examined in Hiroshima, and 6,621 people examined in Nagasaki, who were largely within 2000 meters from the hypocenter, who suffered injuries from the blast and heat but died from complications frequently compounded by acute radiation syndrome(ARS), all within about 20–30 days.\n\nIn the rare cases of survival for individuals who were in utero at the time of the bombing and yet who still were close enough to be exposed to less than or equal to 0.57 Gy, no difference in their cognitive abilities was found, suggesting a threshold dose for pregnancies below which, no life-limiting issues arise. In 50 or so children who survived the gestational process and were exposed to more than this dose, putting them within about 1000 meters from the hypocenter, Microcephaly was observed, this is the only elevated birth defect issue observed in the Hibakusha, occurring in approximately 50 in-utero individuals who were situated less than 1000 meters from the bombings.\n\nFor those who did not suffer ARS, or who survived it. In a strictly dependent manner dependent on their distance from the hypocenter, in the 1987 \"Life Span Study\", conducted by the Radiation Effects Research Foundation, a statistical excess of 507 cancers, of undefined lethality, were observed in 79,972 hibakusha who had still been living between 1958–1987 and who took part in the study.\n\nAn epidemiology study by the RERF estimates that from 1950 to 2000, 46% of leukemia deaths and 11% of solid cancers, of unspecified lethality, could be due to radiation from the bombs, with the statistical excess being estimated at 200 leukemia deaths and 1,700 solid cancers of undeclared lethality.\n\n\n\n\n\n\n"}
{"id": "4758015", "url": "https://en.wikipedia.org/wiki?curid=4758015", "title": "Instruments used in general surgery", "text": "Instruments used in general surgery\n\nSurgical instruments can be generally divided into five classes by function. These classes are:\n\nInstruments used in general surgery are:\n"}
{"id": "4807947", "url": "https://en.wikipedia.org/wiki?curid=4807947", "title": "International Classification of Primary Care", "text": "International Classification of Primary Care\n\nThe International Classification of Primary Care (ICPC) is a classification method for primary care encounters. It allows for the classification of the patient’s reason for encounter (RFE), the problems/diagnosis managed, primary or general health care interventions, and the ordering of the data of the primary care session in an episode of care structure. It was developed by the WONCA International Classification Committee (WICC), and was first published in 1987 by Oxford University Press (OUP). A revision and inclusion of criteria and definitions was published in 1998. The second revision was accepted within the World Health Organization's (WHO) Family of International Classifications.\n\nThe classification was developed in a context of increasing demand for quality information on primary care as part of growing worldwide attention to global primary health care objectives, including the WHO's target of \"health for all\".\n\nThe first version of ICPC, which was published in 1987, is referred to as \"ICPC-1\". A subsequent revision which was published in the 1993 publication \"The International Classification of Primary Care in the European Community: With a Multi-Language Layer\" is known as \"ICPC-E\".\n\nThe 1998 publication, of version 2, is referred to as \"ICPC-2\". The acronym \"ICPC-2-E\", refers to a revised electronic version, which was released in 2000. Subsequent revisions of ICPC-2 are also labelled with a release date.\n\nThe ICPC contains 17 chapters:\n\nThe ICPC classification, within each chapter, is based on 3 components coming from 3 different classifications:\n\n\n\n"}
{"id": "24340483", "url": "https://en.wikipedia.org/wiki?curid=24340483", "title": "John Wilson (Caddo)", "text": "John Wilson (Caddo)\n\n\"John Wilson the Revealer of Peyote\" (c.1845–1901) was a Caddo-Delaware-French medicine man who introduced the Peyote plant into a religion, became a major leader in the Ghost Dance, and introduced a new peyote ceremony with teachings of Christ. John Wilson's Caddo name was Nishkû'ntu, meaning \"Moon Head.\"\n\nThough he was of half-Delaware descent, quarter-blood French, and quarter-blood Caddo, John Wilson spoke only the Caddo language and identified only as a Caddo. He is believed to have been born in 1845, when his band of Caddo were still living in Texas. They were driven into Indian Territory in 1859.\n\nWilson, being interested in religion and only known as a medicine man, sought out a path to be a peyote roadman in 1880. As the Ghost Dance ceremonies regained popularity in Oklahoma, he became one of its most active leaders in the Indian Territory.\n\nDuring a two-week period, Wilson consumed peyote for spiritual reasons and said he was shown essential astronomical symbols representing the life of Jesus Christ. These messages became part of his own teaching, which nevertheless remained reliant purely on peyote. He recalled that Peyote spoke to him, telling him to keep indulging in, and to keep walking in its \"road\" until the day he died of the peyote to create a higher enlightenment.\n\nThe tribe had been exposed to the Half Moon peyote ceremony, but Wilson introduced the Big Moon ceremony to the tribe. The Caddo tribe remains very active in the Native American Church today.\n\nHe is the single human most known for the changes to the religious altar and the peyote ceremony. His changes to the altar, unintentionally persuaded the image of the cross in Christian churches.\n\nWilson died at the age of 61 in 1901.\n\n"}
{"id": "1076794", "url": "https://en.wikipedia.org/wiki?curid=1076794", "title": "Medical Subject Headings", "text": "Medical Subject Headings\n\nMedical Subject Headings (MeSH) is a comprehensive controlled vocabulary for the purpose of indexing journal articles and books in the life sciences; it serves as a thesaurus that facilitates searching. Created and updated by the United States National Library of Medicine (NLM), it is used by the MEDLINE/PubMed article database and by NLM's catalog of book holdings. MeSH is also used by ClinicalTrials.gov registry to classify which diseases are studied by trials registered in ClinicalTrials.gov.\n\nMeSH was introduced in 1960, with the NLM's own index catalogue and the subject headings of the Quarterly Cumulative Index Medicus (1940 edition) as precursors. The yearly printed version of MeSH was discontinued in 2007 and MeSH is now available online only. It can be browsed and downloaded free of charge through PubMed. Originally in English, MeSH has been translated into numerous other languages and allows retrieval of documents from different languages.\n\nThe 2009 version of MeSH contains a total of 25,186 \"subject headings\", also known as \"descriptors\". Most of these are accompanied by a short description or definition, links to related descriptors, and a list of synonyms or very similar terms (known as \"entry terms\"). This additional information and the hierarchical structure (see below) make the MeSH essentially a thesaurus, rather than a plain subject headings list.\n\nThe \"descriptors\" or \"subject headings\" are arranged in a hierarchy. A given descriptor may appear at several locations in the hierarchical tree. The tree locations carry systematic labels known as \"tree numbers\", and consequently one descriptor can carry several tree numbers. For example, the descriptor \"Digestive System Neoplasms\" has the tree numbers C06.301 and C04.588.274; C stands for Diseases, C06 for Digestive System Diseases and C06.301 for Digestive System Neoplasms; C04 for Neoplasms, C04.588 for Neoplasms By Site, and C04.588.274 also for Digestive System Neoplasms. The tree numbers of a given descriptor are subject to change as MeSH is updated. Every descriptor also carries a unique alphanumerical ID that will not change.\n\nMost subject headings come with a short description or definition. See the MeSH description for diabetes type 2 as an example. The explanatory text is written by the MeSH team based on their standard sources if not otherwise stated. References are mostly encyclopaedias and standard textbooks of the subject areas. References for specific statements in the descriptions are not given, instead readers are referred to the bibliography.\n\nIn addition to the descriptor hierarchy, MeSH contains a small number of standard \"qualifiers\" (also known as \"subheadings\"), which can be added to descriptors to narrow down the topic. For example, \"Measles\" is a descriptor and \"epidemiology\" is a qualifier; \"Measles/epidemiology\" describes the subheading of epidemiological articles about Measles. The \"epidemiology\" qualifier can be added to all other disease descriptors. Not all descriptor/qualifier combinations are allowed since some of them may be meaningless. In all there are 83 different qualifiers.\n\nIn addition to the descriptors, MeSH also contains some 139,000 \"supplementary concept records\". These do not belong to the controlled vocabulary as such; instead they enlarge the thesaurus and contain links to the closest fitting descriptor to be used in a MEDLINE search. Many of these records describe chemical substances.\n\nIn MEDLINE/PubMed, every journal article is indexed with about 10–15 subject headings, subheadings and supplementary concept records, with some of them designated as \"major\" and marked with an asterisk, indicating the article's major topics. When performing a MEDLINE search via PubMed, entry terms are automatically translated into (i.e. mapped to) the corresponding descriptors with a good degree of reliability; it is recommended to check the 'Details tab' in PubMed to see how a search formulation was translated. By default, a search for a descriptor will include all the descriptors in the hierarchy below the given one.\n\nIn ClinicalTrials.gov, each trial has keywords that describe the trial. The ClinicalTrials.gov team assigns each trial two sets of MeSH terms. One set for the conditions studied by the trial and another for the set of interventions used in the trial. The XML file that can be downloaded for each trial contains these MeSH keywords. The XML file also has a comment that says: \"the assignment of MeSH keywords is done by imperfect algorithm\".\n\nThe top-level categories in the MeSH descriptor hierarchy are:\n\n\n"}
{"id": "5094817", "url": "https://en.wikipedia.org/wiki?curid=5094817", "title": "Mental Health Tribunal for Scotland", "text": "Mental Health Tribunal for Scotland\n\nThe Mental Health Tribunal for Scotland is a tribunal of the Scottish Government to hear applications for, and appeals against, Compulsory Treatment Order, and appeals against Short Term Detention Certificates made under the Mental Health (Care and Treatment) (Scotland) Act 2003, and other matters in relation to that Act, for example, appeals against Compulsion and Restriction Orders.\n\nThe Mental Health Tribunal for Scotland was established on 5 October 2005, under the Mental Health (Care and Treatment) (Scotland) Act 2003.\n\nIts headquarters are located in Hamilton, although it has staff who work throughout Scotland.\n\nThe Mental Health Tribunal has been criticised regarding its opacity and its authority. In 2009, WikiLeaks published a membership list which had previously been hosted on the Tribunal's website but later removed, along with several observations to this effect.\n\n\n"}
{"id": "5257726", "url": "https://en.wikipedia.org/wiki?curid=5257726", "title": "Mission Florentino", "text": "Mission Florentino\n\nMission Florentino was a Bolivarian mission organized by the government of Venezuelan President, Hugo Chávez, to coordinate the populace to vote \"No\" in the Venezuelan recall referendum of 2004 to keep him in office. The organizational centers of the Mission were named Comando Maisanta, as the ideological central headquarters (election brigades) for those who wished to keep Chávez as the President of Venezuela for the remainder of his presidential term.\n\nThe mission's name was inspired from a poem by Alberto Arvelo Torrealba entitled \"Florentino y el Diablo (Florentino and the Devil)\" in which a singer, Florentino, is tempted by the Devil to join him. Chávez claimed that reading the poem reminded him of the political situation in Venezuela at the time and encouraged his supporters to follow the example of Florentino fighting the Devil (those who were going to vote in favor of removing him from office). The name of the \"Comandos\" came from the Caudillo Pedro Pérez Delgado, great grandfather of Chávez and nicknamed \"Maisanta\", who fought as a guerrilla fighter against Juan Vicente Gómez for control of the country during the early years of the 20th Century.\n\nOn June 2004, Chávez announced that the campaign conducted by the Mission Florentino for the referendum will bear the name of \"Battle of Santa Inés\". The campaign will have the intention of reenacting the battle fought in 1859, in a town near the city of Barinas, in which Ezequiel Zamora lured the government army of the west into an area where he could defeat them in a counterattack . Chávez referred to Zamora's enemies during the Federal War as members of an oligarchy and, in the same way, to all of those who wished to vote against him in the Referendum.\n\nThe official objectives of the Mission were defined as:\n\nThe \"National Comando Maisanta\" was conformed by members directly and solely appointed by President Chávez, for the purpose of:\n\n\n\n"}
{"id": "25678466", "url": "https://en.wikipedia.org/wiki?curid=25678466", "title": "Monthly nurse", "text": "Monthly nurse\n\nA monthly nurse is a woman who looks after a mother and her baby during the postpartum or postnatal period.\n\nHistorically, women were confined to their beds or their homes for extensive periods after giving birth; care was provided either by her female relatives (mother or mother-in-law), or, for those who could afford it, by the monthly nurse. These weeks were called confinement or lying-in, and ended with the re-introduction of the mother to the community in the Christian ceremony of the churching of women. The term \"monthly nurse\" was most common in 18th and 19th century England.\n\nThe job still exists, although it now might be described as \"postnatal doula\" or \"maternity nurse\" or \"newborn care specialist\" - all specialist sorts of nannies. A modern version of this rest period has evolved, to give maximum support to the new mother, especially if she is recovering from a difficult labour and delivery. It is especially popular in China and its diaspora, where postpartum confinement is known as \"sitting the month\".\n\nFrom long ago, the delivery of children and care of the mothers was a profession often handed down from mother to daughter, with the daughter spending many years as the pupil or apprentice. The Church supported that by a system of licensing, which required midwives to swear to certain rules relating to contraception, abortion and concealment of births and also to deliver the newborn infants for baptism or, in extreme cases, to perform the ceremony themselves.\n\nIn the mid-18th century the legal status of midwives was withdrawn and the responsibility for delivery was vested in the surgeon. The work of the nurse element had to be covered, as \"who was to look after the baby?\" Clearly, the first thought that would naturally occur to a mother was that the best person to look after her baby was a woman who had had one herself. Often, the task was allotted to motherly or grandmotherly hands and, from that requirement for postnatal care, the monthly nurse originated. \"The Nursing Record\" reported that \"there was little or no attempt at knowledge or instruction, and we know as a fact that ignorance, prejudice and neglect resulted in a goodly crop of errors, wrongs, and woes as regards the hapless infant\".\n\nThe term \"monthly nurse\" is one that is frequently used to describe the nurse who cares for lying-in cases, certainly because such a nurse frequently remains with the patient for four weeks. The term \"monthly\" is somewhat inaccurate, as there is no reason for the nurse's services to be dispensed with after ten days or retained for much longer, but it is entirely a matter of arrangement.\n\n\"The Nursing Record\" reported that \"nurses who attend the 'artisan' classes in their confinements as a rule pay a visit daily for ten days and then give up the case, as few working class mothers can afford to lie up for longer\".\n\nA monthly nurse could earn more than a midwife, as the monthly nurse was employed for periods between 10 days and often much longer and might attend several women on a part time basis. She often \"lived in\". The midwife's only duty was perceived as \"being trained to assist the parturient woman while nature does her own work and able to call upon a surgeon who could step in where nature fails and skill and science are required\". Many certified midwives transferred to the ranks of monthly nurses to benefit from an increased income.\n\nAlthough 'registration' was not available for women to act as midwives or monthly nurses a system of 'certification' was in being in the late 19th century and continued into the early 20th century. To qualify, a candidate monthly nurse would attend a course in a lying-in hospital for four or five weeks and a midwife for up to three months. The prospective midwives and monthly nurses, as a rule, paid their own charges in respect of hospital expenses and then entered practice on their own responsibility. In 1893, a Miss Gosling reported that \"although the certificated monthly nurse could be relied upon as being trustworthy and efficient, there were a number of women who attend lectures for a short time and through one cause or another fail to pass their examination and obtain a certificate nevertheless enter a 'Nurses Home' or open one for themselves\".\n\nAs might be expected rogue institutions issued certificates and diplomas “for a price”. Another that reporting on a lying in hospital and signed herself a ‘victim of the system’ said that she “witnessed the first phase of the system which turns out yearly hundreds of midwives and monthly nurses on an unsuspecting public. These would be nurses represented almost every grade of the lower classes and every degree of lack of education, and one woman, I remember could not write. Personally I found many to be dishonest, untruthful, indescribably dirty in their habits and persons, utterly unprincipled, shockingly coarse and deficient intelligence, and with not the faintest idea of discipline”’\n\nIn the late 19th century, reformers were calling not only for registration and recognition of the profession of midwife but also for the two functions of midwife and monthly nurse to be amalgamated: \"The work of midwives lies, for the most part, amongst the poor and the poor lying-in woman needs not only to be delivered, but to be visited for some ten days subsequent to her confinement\". The registration of midwives was opposed by members of the House of Lords and Parliament for many years, who argued that the delivery of infants was the responsibility of trained doctors and to allow women to do the job, even in straightforward cases, would take away doctors' income. It was not until the Midwives Act 1902, following 12 years of representation by women, that midwives were \"registered\", but it would still take several years for it to be accepted. The professional training and formal qualification of midwives, and eventually, the postnatal care offered by the National Health Service, saw the end of the monthly nurse.\n\n\n"}
{"id": "41050844", "url": "https://en.wikipedia.org/wiki?curid=41050844", "title": "NHS health check", "text": "NHS health check\n\nNHS health checks are available to people in England between the ages of 40 and 74. The health check consist of an appointment with a healthcare professional at which people are asked about their family history and lifestyle and have their body mass index, blood pressure, and cholesterol concentration measured. Further investigations may then follow.\n\nIn January 2008, the then prime minister, Gordon Brown, announced “everyone in England will have access to the right preventative health check-up . . . there will soon be check-ups on offer to monitor for heart disease, strokes, diabetes, and kidney disease.” He also pledged a national screening committee, an independent clinical body, that “will look at the evidence and advise on what additional screening procedures would be genuinely useful in detecting other conditions.”\n\nEvery local authority in England is obliged to secure the provision of health checks to be offered to eligible persons (aged from 40 to 74 years) in its area.\n\nThe programme of health checks has been criticised as being without evidence of effectiveness by Dr Margaret McCartney. The director of the UK National Screening Committee is reported as saying “There are certainly some aspects of the programme that look and feel like screening. However it is not run as a systematic ‘call-recall’ programme nor does it have quality assurance\". John Ashton, president of the Faculty of Public Health, said he has “grave reservations” about health checks. “We are not convinced about the evidence base. There is a danger of medicalising social inequalities—in many ways health checks could be seen as playing into the pharmaceutical agenda. We should be focusing on disadvantaged communities—not finding more worried well.”\n\nIn September 2014 Professor Kevin Fenton, head of health and wellbeing at Public Health England, claimed the programme was being run on sound principles and rejected calls from to change track and focus on more opportunistic checking in people known to be at high risk. A study published in the British Journal of General Practice found no significant differences in the change to the prevalence of diabetes, hypertension, Chronic Heart Disease, Chronic Kidney Disease or Atrial Fibrillation in GP practices providing NHS Health Checks compared with control practices.\n\nPeter Walsh, deputy director of the Strategy Group at NHS England admitted that take-up of the checks was poor in January 2016, after a study showed that 20% of those eligible aged 60-74 attended and 9.0% of those between 40–59.\n\nIn May 2016 researchers from Imperial College London concluded that the checkup reduced the 10-year risk of cardiovascular disease by 0.21%, equivalent to one stroke or heart attack avoided every year for 4,762 people who attend. The programme cost £165 million a year.\n\nA retrospective observational study by the Centre for Primary Care and Public Health, Queen Mary University of London found that take up in an ethnically diverse and socially deprived area of East London had increased from 7.3% of eligible patients in 2009 to 85.0% in\n2013–2014. New diagnoses of diabetes were 30% more likely in attendees than nonattendees, hypertension 50%, and Chronic Kidney Disease 80%.\n\n\n"}
{"id": "44880421", "url": "https://en.wikipedia.org/wiki?curid=44880421", "title": "NORWAC", "text": "NORWAC\n\nNORWAC (Norwegian Aid Committee) is a Norwegian non-governmental organization founded in 1983, and since 1994 organised as a foundation. \n\nThe organisation is part of the Norwegian left-wing pro-Palestinian solidarity movement. It has worked in support of Palestinians in the Palestinian territories and in Lebanon since its foundation. From 1999 to 2011 the organisation also worked in the Balkans, and it has worked in Syria since 2012. The group has been profiled by the medical doctors Mads Gilbert and Erik Fosse (president of NORWAC) during the conflicts in Gaza.\n\nIn 1989 the organisation came under scrutiny from the research team of Marianne Heiberg from NUPI as part of a broad investigation of aid organisations funded by public allocations. The organisation was criticised for minimal-to-none developmental effects, lack of understanding of local needs, lack of control mechanisms and unprofessional management. NORWAC was also criticised for supporting fringe factions in Palestinian politics, and concluded that it did not meet the most elementary requirements for an aid organisation, in particular due to its \"sectarian\" work and taking political concerns. The report recommended to NORAD to stop all public allocations to the organisation. NORWAC was also scrutinised during a subsequent investigation of funds allocated by the Ministry of Foreign Affairs.\n\nDuring the conflicts in Gaza in 2009 and 2014, Gilbert and Fosse were accused in Norway and abroad of facilitating propaganda by Hamas. Their work at the Al-Shifa Hospital has been particularly noted amid allegations of the hospital's use as a Hamas headquarter. In 2014 Gilbert was banned indefinitely from entering Gaza through Israel, officially for \"security reasons\", but according to intelligence sources because there had been revealed \"close ties\" between Gilbert and Hamas-leaders.\n\nIn 2014 it was reported that a hospital run by NORWAC in Syria in the town Tell Abyad treated wounded soldiers from terrorist organisation ISIL (Islamic State of Iraq and the Levant). Since 2013, when the hospital started being funded through NORWAC by Norwegian public aid, it was reported that ISIL had taken full control of the hospital and the surrounding area. According to both Arab and Kurdish sources, the hospital had become reserved entirely for ISIL soldiers and supporters. Erik Fosse rejected the latter reports, while admitting that the hospital treated ISIL jihadists. After subsequent investigations by the Ministry of Foreign Affairs found that ISIL had enforced religious rules at the hospital, including rituals for prayer, gender segregation and regulations on clothing, Foreign Minister Børge Brende cancelled all financial support directed to the hospital. While conforming to the decision, Fosse and NORWAC continued to deny that there were any enforcements by ISIL at the hospital.\n"}
{"id": "6203937", "url": "https://en.wikipedia.org/wiki?curid=6203937", "title": "Normal curve equivalent", "text": "Normal curve equivalent\n\nIn educational statistics, a normal curve equivalent (NCE), developed for the United States Department of Education by the RMC Research Corporation, is a way of standardizing scores received on a test into a 0-100 scale similar to a percentile-rank, but preserving the valuable equal-interval properties of a z-score. It is defined as:\n\nor, approximately\n\nwhere \"z\" is the standard score or \"z-score\", i.e. \"z\" is how many standard deviations above the mean the raw score is (\"z\" is negative if the raw score is below the mean). The reason for the choice of the number 21.06 is to bring about the following result: If the scores are normally distributed (i.e. they follow the \"bell-shaped curve\") then\n\nThis relationship between normal equivalent scores and percentile ranks does not hold at values other than 1, 50, and 99. It also fails to hold in general if scores are not normally distributed.\n\nThe number 21.06 was chosen because\nNormal curve equivalents are on an equal-interval scale (see and for examples). This is advantageous compared to percentile rank scales, which suffer from the problem that the difference between any two scores is not the same as that between any other two scores (see below or percentile rank for more information). \n\nThe major advantage of NCEs over percentile ranks is that NCEs can be legitimately averaged.\n\nCareful consideration is required when computing effect sizes using NCEs. NCEs differ from other scores, such as raw and scaled scores, in the magnitude of the effect sizes. Comparison of NCEs typically results in smaller effect sizes, and using the typical ranges for other effect sizes may result in interpretation errors.\n\nExcel formula for conversion from Percentile to NCE:\n\nExcel formula for conversion from NCE to Percentile:\n\n"}
{"id": "49507052", "url": "https://en.wikipedia.org/wiki?curid=49507052", "title": "Obstetrical &amp; Gynecological Survey", "text": "Obstetrical &amp; Gynecological Survey\n\nObstetrical & Gynecological Survey is a monthly peer-reviewed medical journal covering obstetrics and gynaecology. It was established in 1946 and is published by Wolters Kluwer. The editors-in-chief are Lee A. Learman (Florida Atlantic University), Aaron B. Caughey (Oregon Health & Science University), and Mary E. Norton (University of California, San Francisco). According to the \"Journal Citation Reports\", the journal has a 2014 impact factor of 1.863, ranking it 38th out of 79 journals in the category \"Obstetrics & Gynecology\".\n"}
{"id": "1895094", "url": "https://en.wikipedia.org/wiki?curid=1895094", "title": "Opportunistic infection", "text": "Opportunistic infection\n\nAn opportunistic infection is an infection caused by pathogens (bacteria, viruses, fungi, or protozoa) that take advantage of an opportunity not normally available, such as a host with a weakened immune system, an altered microbiota (such as a disrupted gut microbiota), or breached integumentary barriers. Many of these pathogens do not cause disease in a healthy host that has a normal immune system. However, a compromised immune system, which is seriously debilitated and has lowered resistance to infection, a penetrating injury, or a lack of competition from normal commensals presents an opportunity for the pathogen to infect.\n\nImmunodeficiency or immunosuppression can be caused by:\n\nThe lack of or the disruption of normal vaginal microbiota allows the proliferation of opportunistic microorganisms and will cause the opportunistic infection - bacterial vaginosis.\n\nA partial listing of opportunistic organisms includes:\n\n\nSince opportunistic infections can cause severe disease, much emphasis is placed on measures to prevent infection. Such a strategy usually includes restoration of the immune system as soon as possible, avoiding exposures to infectious agents, and using antimicrobial medications (\"prophylactic medications\") directed against specific infections.\n\n\n\nIndividuals at higher risk are often prescribed prophylactic medication to prevent an infection from occurring. A patient's risk level for developing an opportunistic infection is approximated using the patient's CD4 T-cell count and sometimes other markers of susceptibility. Common prophylaxis treatments include the following:\nTreatment depends on the type of opportunistic infection, but usually involves different antibiotics.\n\nOpportunistic infections caused by Feline Leukemia Virus and Feline immunodeficiency virus retroviral infections can be treated with Lymphocyte T-Cell Immune Modulator.\n"}
{"id": "28192164", "url": "https://en.wikipedia.org/wiki?curid=28192164", "title": "Pharmavite", "text": "Pharmavite\n\nPharmavite is a dietary supplements company that was founded in 1971 by a California pharmacist, Barry Pressman, and that was acquired by Otsuka Pharmaceutical in 1989. Its \"Nature Made\" vitamin brand was launched the following year.\n\nPharmavite works with the United States Pharmacopeia's (USP) Dietary Supplements Verification Program on some of its products.\n"}
{"id": "15082727", "url": "https://en.wikipedia.org/wiki?curid=15082727", "title": "Pre-crime", "text": "Pre-crime\n\nPre-crime (or precrime) is a term coined by science fiction author Philip K. Dick. It is increasingly used in academic literature to describe and criticise the tendency in criminal justice systems to focus on crimes not yet committed. Pre-crime has been defined as \"substantive coercive state interventions targeted at non-imminent crimes\". Pre-crime intervenes to punish, disrupt, incapacitate or restrict those deemed to embody future crime threats. The term pre-crime embodies a temporal paradox, suggesting both that a crime has not occurred and that the crime that has not occurred is a foregone conclusion (McCulloch and Wilson 2016).\n\nGeorge Orwell introduced a similar concept in his 1948 novel \"Nineteen Eighty-Four\" using the term thoughtcrime to describe illegal thoughts which held banned opinions about the ruling government or intentions to act against it. A large part of how it differs from pre-crime is in its absolute prohibition of anti-authority ideas and emotions, regardless of the consideration of any physical revolutionary acts. However, Orwell was describing behaviour he saw in governments of his day as well as extrapolating on that behaviour, and so his ideas were themselves rooted in real political history and current events.\n\nIn Philip K. Dick's 1956 science fiction short story \"The Minority Report\", Precrime is the name of a criminal justice agency, the task of which is to identify and eliminate persons who will commit crimes in the future. The agency’s work is based on the existence of \"precog mutants\", a trio of \"vegetable-like\" humans whose \"every incoherent utterance\" is analyzed by a punch card computer. As Anderton, the chief of the Precrime agency, explains the advantages of this procedure: \"in our society we have no major crimes ... but we do have a detention camp full of would-be criminals\". He cautions about the basic legal drawback to pre-crime methodology: \"We’re taking in individuals who have broken no law.\"\n\nThe concept was brought to wider public attention by Steven Spielberg's film \"Minority Report\", loosely adapted from the story.\n\nPre-crime in criminology dates back to the positivist school in the late 19th century, especially to Cesare Lombroso's idea that there are \"born criminals\", who can be recognized, even before they have committed any crime, on the basis of certain physical characteristics. Biological, psychological and sociological forms of criminological positivisms informed criminal policy in the early 20th century. For born criminals, criminal psychopaths and dangerous habitual offenders eliminatory penalties (capital punishment, indefinite confinement, castration etc.) were seen as appropriate. Similar ideas were advocated by the Social Defense movement and, more recently, by what is seen and criticized as an emerging \"new criminology\" (Feeley & Simon 1992) or \"actuary justice\" (Feeley & Simon 1994). The new \"pre-crime\" or \"security society\" requires a radically new criminology (Fitzgibbon 2004; Zedner 2007; Zedner 2009; Zedner 2010; Zedner 2014).\n\nRichard Nixon's psychiatrist, Arnold Hutschnecker, suggested, in a memorandum to the then president, to run mass tests of \"pre-delinquency\" and put those juveniles in \"camps\". Hutschnecker, a refugee from Nazi Germany and a vocal critic of Hitler at the time of his exodus, has rejected the interpretation of the memorandum that he advocated concentration camps:\nFrontline of a criminal justice system increasingly preoccupied with anticipating threats' and is the antithesis of the traditional criminal justice systems focus on past crimes (McCulloch and Wilson 2016). Traditionally, criminal justice and punishment presupposes evidence of a crime being committed. This time-honored principle is violated once punishment is meted out \"for crimes never committed\" (Anttila 1975, who criticised the measure of security detention). Today, a clear example of this trend is \"nachträgliche Sicherungsverwahrung\" (retrospective security detention), which became an option in German criminal law in 2004. This \"measure of security\" can be decided upon at the end of a prison sentence on a purely prognostic basis (Boetticher/Feest 2008, 263 sq.). In France, a similarly retrospective measure was introduced in 2008 as \"rétention de sûreté\" (security detention). The German measure was viewed as violating the Charter of Fundamental Rights of the European Union by the European Court of Human Rights in 2009. It was, however, never completely abolished in Germany and new legislation is envisaged to continue this practice under the new name \"Therapieunterbringung\" (detention for therapy).. A similar provision for indefinite administrative detention was found in Finnish law, but it was not enforced after mid-1970s. Pre-crime is most obvious and advanced in the context of counter-terrorism, though it is argued that far from countering terrorism, pre-crime produces the futures it purports to preempt (McCulloch/Pickering 2009).\n\nSpecialist software now exists for crime-prediction by analysing data.\n\n\n"}
{"id": "29243665", "url": "https://en.wikipedia.org/wiki?curid=29243665", "title": "Prenatal nutrition", "text": "Prenatal nutrition\n\nNutrition and weight management before and during :pregnancy has a profound effect on the development of infants. This is a rather critical time for healthy fetal development as infants rely heavily on maternal stores and nutrient for optimal growth and health outcome later in life. Prenatal nutrition addresses nutrient recommendations before and during pregnancy. Prenatal nutrition has a strong influence on birth weight and further development of the infant. There was a study at the National Institution of Health which found that babies born from an obese mother have a higher probability to fail tests of fine motor skills which is the movement of small muscles such as the hands and fingers.\n\nA common saying that 'a woman is eating for two while pregnant' implies that a mother should consume twice as much during pregnancy. However, in reality, this is not true. Although maternal consumption will directly affect both herself and the growing fetus, overeating excessively will compromise the baby's health as the infant will have to work extra hard to become healthy in the future. Compared with the infant, the mother possesses the least biological risk. Therefore, excessive calories, rather than going to the infant, often get stored as fat in the mother. On the other hand, insufficient consumption will result in lower birth weight.\n\nMaintaining a healthy weight during gestation lowers adverse risks on infants such as birth defects, as well as chronic conditions in adulthood such as obesity, diabetes, and cardiovascular disease (CVD). Ideally, the rate of weight gain should be monitored during pregnancy to support the most ideal infant development.\n\nThe \"Barker Hypothesis\", or Thrifty phenotype, states that conditions during pregnancy will have long-term effects on adult health. Associated risk of lifelong diseases includes cardiovascular disease, type-2 diabetes, obesity, and hypertension. Babies born lighter in weight appear to have an increased rate of mortality than babies born at a heavier weight. This does not mean that heavy babies are less of a concern. Death rate would rise as birth weight increases beyond normal birth weight range. Therefore, it is important to maintain a healthy gestational weight gain throughout pregnancy for achieving the optimal infant birth weight.\n\nWhen this theory was first proposed, it was not well accepted and was met with much skepticism. The main criticism was that confounding variables such as environmental factors could attribute to many of the chronic diseases such that low birth weight alone should not be dictated as an independent risk factor. Subsequent research studies supporting the theory attempted to adjust these environmental factors and in turn, provided more convincing results with minimal confounding variables.\n\n\"Barker's Hypothesis\" is also known as \"Fetal Programming Hypothesis\". The word \"programming\" illustrates the idea that during critical periods in early fetal development, there are persisting changes in the body structure and function that are caused by environmental stimuli. This relates to the concept of developmental plasticity where our genes can express different ranges of physiological or morphological states in response to the environmental conditions during fetal development.\n\nIf the mother has an inadequate diet then it signals the baby that the living condition in the long term will be impoverished. Consequently, the baby adapts by changing its body size and metabolism to prepare for harsh conditions of food shortages after birth. Physiological and metabolic processes in the body undergo long-term changes as a result of restricted growth. When the living environment switches from the condition of malnutrition to a society of abundant supply of nutrients, this exposes the baby to a bountiful environment that goes against what its body is designed for and this places the baby at a higher risk of adult diseases later in adulthood. By the same token, if the fetus growing in the womb of a healthy mother is exposed to prolonged famine after birth, the infant would be less adaptive to the harsh environment than low-birth-weight babies.\n\nIn 1952 the Danish physician Jørgen Pedersen of the University of Copenhagen, formulated the hypothesis that maternal hyperglycemia during pregnancy might cause fetal hyperglycemia, thus exposing the fetus to elevated insulin levels. This would result in an increased risk of fetal macrosomia and neonatal hypoglycemia.\n\nThe blood glucose concentration in humans is mainly dependent on diet, especially energy-ingestion and the percentage of carbohydrates in the diet. High glucose concentrations in the blood of pregnant women cause an intensified transfer of nutrient to the fetus, increasing fetal growth. Studies could link higher maternal glucose to an increase in infant birth weight as well as different extents of morbidity, among other things the incidence of congenital malformations, supporting the Hypothesis, that even moderately increased blood glucose in the absence of diabetes positively influences growth in the fetus.\n\nSubsequently alterations of Pedersen's Hypothesis took place: Nutrients other than sugar and their linkage to fetal overgrowth in diabetic pregnancy were taken into account, too, but the crucial role of the fetal hyperinsulinism and monitoring of motherly glucose was nevertheless stressed. Recent studies pointed out that diabetes in the mother could foster even more lasting effects on the child's health than previously thought, even raising the risk of obesity and type 2 diabetes.\n\nVarious nutritional conditions, both times of scarcity and of abundance occurred time and again in different societies at different times, and thus in some cases epidemiological studies have exposed a correlation between the nutritional status of pregnant women and the health of their children or even grandchildren.\n\nSince small birth weight is associated with an increased risk of chronic diseases in later life, and poor maternal nutrition during gestation contributes to restricted fetal development, maternal malnutrition may be a cause of increased disease susceptibility in adulthood.\n\nThe Dutch famine of 1944 or the \"Hunger Winter\" during World War II serves as an epidemiological study that is used to examine the effects of maternal under-nutrition during different gestational stages. The famine was a period (roughly five to six months) of extreme food shortage in the west of Netherlands. The famine was imposed on a previously well-nourished population and the official daily ration for the general adult population gradually decreased from 1800 calories in December 1943 to 1400 calories in October 1944 to below 1000 calories in the late November 1944. December 1944 to April 1945 was the peak of the famine where the official daily ration fell abruptly to about 400~800 calories. Even though pregnant and lactating women had extra food during the famine, these extra supplies could no longer be provided at the height of the famine. In the early May 1945, the liberation of the Netherlands restored the food supply. The daily ration had increased to more than 2000 calories in June 1945. What is unique about Dutch Famine as an experimental study on the effects of maternal malnutrition is that the population was strictly circumscribed in time and place and the sudden onset and relief of the famine was imposed on a previously well-nourished population.\n\nThe Dutch Famine during World War II had a profound effect on the health condition of the general public, especially women who conceived during the period of time. The period of maternal starvation is shown to have limited intrauterine growth and has been identified as one of the most important contributors to coronary heart disease as well as other chronic diseases later in life. These findings agree well with Barker's hypothesis; it supports the theory that maternal under-nutrition leads to a lower birth weight due to restricted intrauterine development and ultimately leads to higher risks of chronic conditions in adult life.\n\nThe French paradox regards the seemingly paradoxical fact that people living in France since many generations suffer from a relatively little incidence of heart disease, although the traditional French cuisine is high in saturated fatty acids.\n\nOne explanation suggested for the paradox is the potential impact of nutritional enhancements during pregnancy and the first months and years of life that would positively influence the health of following generations: After the defeat in the Franco-German War, a nutrition program for pregnant women and small children with the aim of strengthening future generations of soldiers was introduced by the French Government. This might be one explanation for positive health-outcomes in following generations.\n\nGestation is the period of embryo development from conception to birth. Gestation is about 40 weeks in humans and is divided into three trimesters, each spanning 3 months. Gestational stages, on the other hand, are based on physiological fetal development, which include blastogenesis, embryonic stage and fetal stage.\n\nBlastogenesis is the stage from fertilization to about 2 weeks. The fertilized egg or the zygote becomes a blastocyst where the outer layer and the inner cell mass differentiate to form placenta and the fetus respectively. Implantation occurs at this stage where the blastocyst becomes buried in the endometrium.\n\nEmbryonic stage is approximately from 2 weeks to 8 weeks. It is also in this stage where the blastocyst develops into an embryo, where all major features of human are present and operational by the end of this stage.\n\nFetal stage is from 9 weeks to term. During this period of time, the embryo develops rapidly and becomes a fetus. Pregnancy becomes visible at this stage.\n\nThe pattern and amount of weight gain is closely associated with gestational stages. Additional energy is required during pregnancy due to the expansion of maternal tissues and stores in order to support fetal development.\n\nIn the first trimester (blastogenesis and early embryonic stages), the mother experiences a minimal weight gain (approximately 0.5-2 kilograms), while the embryo weighs only 6 grams, which is approximately the weight of 6 raisins.\n\nIn the second trimester and third trimester (late embryonic and fetal stages), the fetus undergoes rapid weight growth and the weight increases to about 3000~4000 grams. It is also in this period that the mother experiences the bulk of her gestational weight gain but the amount of weight gain varies greatly. The amount of weight gain depends strongly on their pre-pregnant weight.\n\nGenerally, a normal weight is strongly recommended for mothers when entering gestation, as it promotes overall health of infants. Maternal body weight is determined by the Body Mass Index (BMI) which is defined as the weight in kilograms divided by the square of the height in meters.\nWhile pregnant, body weight should be managed within the recommended gestational weight gain range as it is shown to have a positive effect on pregnancy outcomes. Gestational weight gain should also be progressive and the recommended weight depends on pre-pregnant body weight.\n\nSince the total weight gain depends on pre-pregnant body weight, it is recommended that underweight women should undergo a larger weight gain for healthy pregnancy outcomes, and overweight or obese women should undergo a smaller weight gain.\n\nWomen having a BMI of 18.5~24.9 are classified as having a \"normal or healthy\" body weight. This group have the lowest risk of adverse birth outcomes. Their babies are least likely to either be low-birth weight or high-birth weight. It is advised that women with a normal weight before pregnancy should gain a total of 11.5 kilograms to 16.0 kilograms throughout gestation, which is approximately 0.4 kilogram per week in the second and third trimesters.\n\nIn order to maintain a steady weight gain, the mother should engage in mild physical activities. Participating in aerobic activities such as walking and swimming 3 to 4 times a week is usually adequate. Vigorous physical activity is not recommended since an excessive loss of calories is induced which is not sufficient to support fetal development.\n\nA proper diet is also essential to healthy weight gain. The common saying \"a woman is eating for two\" often leads to mothers thinking that they should eat twice as much. In reality, only a small increase in caloric intake is needed to provide for the fetus; approximately 350 calories more in the second trimester and 450 calories more in the third trimester. Also, healthy choices should be emphasized for these extra calories such as whole grain products, fruits and vegetables as well as low-fat dairy alternatives.\n\nWomen are classified as \"underweight\" if they have a pre-pregnant BMI of 18.5 or below. Low pre-pregnancy BMI increases the risk of low birth weight infants, but the risk can be balanced by an appropriate gestational weight gain from 12.5 to 18.0 kilograms in total, or about 0.5 kilogram each week in the second and third trimesters.\n\nUnderweight women usually have inadequate nutrient stores that are not enough to provide for both herself and the fetus. While exercise and a proper diet are both needed to maintain the recommended weight gain, a balance between the two is very important. As such, underweight mothers should seek individualized advice tailored especially for themselves.\n\nWomen with a high pre-pregnancy weight are classified as \"overweight or obese\", defined as having a BMI of 25 or above. Women with BMI between 25 and 29.9 are in the overweight category and should gain between 7.0 and 11.5 kilograms in total, corresponding to approximately 0.28 kilogram each week during the second and third trimesters. Whereas women with BMI of 30 or above are in the obese category and should gain only between 5.0 and 9.0 kilograms overall, which equates to roughly 0.2 kilogram per week in the second and third trimesters.\n\nDiet, exercise or a combination of both has been seen to reduce weight gain in pregnancy by 20% and reduce high blood pressure. Diet with exercise may reduce the risk of caesarean section, having a large baby and having a baby with serious breathing problems. Diet and exercise help pregnant women not gain too much weight during pregnancy when compared with giving the women no help too control weight gain or routine care (usually one session in the pregnancy).\n\nIn general, walking is encouraged for mothers classified in this category. Unfortunately, estimated energy requirements for them are not available. As such, they are encouraged to record activity and intake level. This can be done with the help of tools such as My Food Guide Servings Tracker from Health Canada and EATracker that are available online. In extreme cases where the BMI exceeds 35, help from a registered dietitian is recommended.\nThe following table summarizes the recommended rate of weight gain and total weight gain according to pre-pregnancy BMI for singleton pregnancies. The first column categorizes the type of body weight based on the Body Mass Index. The second column summarizes the total recommended weight gain for each type of body weight, and the third column presents the corresponding weekly weight gain during the period when the fetus undergoes rapid growth (during second and third trimesters). In extreme cases, the amount of total and weekly weight gain can vary by a factor of two depending on a woman's pre-pregnant weight. For example, a woman in the obese category is recommended to gain a total of 5~9 kilograms, whereas an underweight woman needs to gain up to 18 kilograms in weight.\n\nIn order to have a good estimate of birth weight, ultrasonography or ultrasound during pregnancy and the date of last menstrual period are needed. Measured values from ultrasonography are compared with the growth chart to estimate fetal weight.\n\nCrown-rump length can be used as the best ultrasonographic measurement for correct diagnosis of gestational age during the first trimester. This correlation between crown-rump length and gestational age would be most effectively shown when no growth defects are observed in the first trimester. If growth defects were observed in the first trimester, then the measurement of the date of last menstrual period becomes quite important since the crown-heel length has become less of a reliable indicator of gestational age.\n\nAfter the 20th week of pregnancy, the mother would need to visit the doctor for the measurement of fundal height, which is the length from the top portion of the uterus to the pubic bone. The length measured in centimeters should correspond to the number of weeks that the mother has been pregnant. If the measured number is higher or lower than 2 centimetres, further tests using ultrasound would be needed to check the results. Another way to estimate fetal size is to look at the mother's weight gain. How much weight the mother gains can be used to indicate fetal size.\n\nThere are two ways to determine small for gestational age (SGA) infants. Many research studies agree that SGA babies are those with birth weight or crown-heel length measured at two standard deviations or more below the mean of the infant's gestational age, based on data consisting of a reference population. Other studies classify SGA babies as those with birth weight values below the 10th percentile of the growth chart for babies of the same gestational age. This indicates that these babies are weighing less than 90% of babies of the same gestational age.\n\nMany factors, including maternal, placental, and fetal factors, contribute to the cause of impaired fetal growth. There are a number of maternal factors, which include age, nutritional status, alcohol abuse, smoking, and medical conditions. Insufficient uteroplacental perfusion is an example of a placental factor. Chromosomal abnormalities and genetic diseases are examples of fetal factors.\nIdentification of the causes of SGA for individual cases aids health professionals in finding ways to handle each unique case. Nutritional counseling, education, and consistent monitoring can be helpful to assist women bearing SGA infants.\n\nComplications for the infant include limitations in body growth since the number and size of cells in tissues is smaller. The infant likely did not receive enough oxygen during pregnancy so the oxygen level is low. It is also more difficult to maintain body temperature since there is less blood flow within the small body.\n\nAs such, it is necessary to monitor oxygen level to make sure that it doesn't go too low. If the baby can't suck well, then it may be necessary for tube-feed. Since the baby cannot maintain body temperature sufficiently, a temperature-controlled bed would help to keep their bodies from losing heat. There are ways to help prevent SGA babies. Monitoring fetal growth can help identify the problem during pregnancy well before birth. It would be beneficial to seek professional help and counseling.\n\nResearch shows that when birth weights of infants are greater than the 90th percentile of the growth chart for babies of the same gestational age, they are considered large for gestational age or LGA. This indicates that these babies are weighing more than 90% of babies of the same gestational age.\n\nMany factors account for LGA babies, including genetics and excessive nutrient supply. It seems that a common factor for LGA babies is whether or not the mother has diabetes when she is pregnant. An indicator for excessive growth, regardless of gestational age, is the appearance of macrosomia. Many complications are observed for LGA babies and their mothers. A longer delivery time may be expected since it is a difficult birth. The infant would likely suffer hypoglycemia (low glucose level in the blood) after birth. The infant would also have difficulty breathing.\n\nThere might be a need for early delivery if the baby gets too big and perhaps Caesarean section would be needed. Since the baby is bigger in size, there's a higher chance of injury when coming out of the mother's body. To increase the blood glucose level in blood, a glucose/water solution can be offered to the infant.\n\nThere are ways to help prevent LGA babies. It is necessary to monitor fetal growth and perform pregnancy examinations to determine health status and detect any possibility of unrecognized diabetes. For diabetic mothers, careful management of diabetes during pregnancy period would be helpful in terms of lowering some of the risks of LGA.\n\nThe goal of pregnancy is to have a healthy baby. Maintaining healthy and steady weight gain during pregnancy promotes overall health and reduces the incidence of prenatal morbidity and mortality. This, in turn, has a positive effect on the baby's health.\n\nSince conditions during pregnancy will have long-term effects on adult health, \"moderation\" should be taken into account for both dietary and physical activity recommendations. Most importantly, the total recommended pregnancy weight gain depends on pre-pregnant body weight, and weight issues should be addressed before pregnancy.\n\nIt is reasonable to expect higher weight gain for multiple gestations. Recommendations for women carrying twins are given but more research should be done to precisely determine the total weight gain, as these ranges are wide. Also, the ranges for underweight women carrying twins is unknown. There was not enough information to recommend weight gain cutoffs and guidelines for women carrying three or more babies, women of short stature (<157 centimetres), and pregnant teens. Estimated energy requirements (EER) for overweight/obese women are unavailable so more research is needed to evaluate on that.\nThere are also important links between nutrition and mental health across pregnancy. For example, a women experiencing low mood may be more likely to smoke, use alcohol or neglect her diet\n\nThe following general tips can be helpful to pregnant women. It would be beneficial to maintain adequate physical activity to meet energy needs from the food consumed. Eating a balanced diet would be optimal for healthy pregnancy results. To prevent problems like dehydration and constipation, it is important to drink enough fluids, especially water, to support blood volume increases during pregnancy. It is recommended to accompany regular meals with a daily prenatal vitamin supplement that has sufficient folic acid and iron content. \nIf the fetus is predicted to have low birth weight, in addition to the general recommendations, it would be ideal to increase caloric intake, which can be done by having extra Food Guide Servings daily. \nIf the fetus is predicted to have high birth weight, smaller and more frequent meals should be consumed to allow better weight management. Moderate sugar intake, such as fruit juices, is also suggested. It is essential to limit food and beverages with high calories and salt content.\n\n"}
{"id": "4496150", "url": "https://en.wikipedia.org/wiki?curid=4496150", "title": "Professional sports league organization", "text": "Professional sports league organization\n\nProfessional sports leagues are organized in numerous ways. The two most significant types are one that developed in Europe, characterised by a tiered structure using promotion and relegation to determine participation in a hierarchy of leagues or divisions, and a North American originated model characterized by its use of \"franchises,\" closed memberships, and minor leagues. Both these systems remain most common in their area of origin, although both systems are used worldwide.\n\nThe term league has many different meanings in different areas around the world, and its use for different concepts can make comparisons confusing. Usually a league is a group of teams that play each other during the season. It is also often used for the name of the governing body that oversees the league, as in America's Major League Baseball or England's Football League. Because most European football clubs participate in different competitions during a season, regular-season home-and-away games are often referred to as league games and the others as non-league or cup games, even though the separate competitions may be organised by the same governing body. Also, there is a rugby football code called rugby league, which is distinct from rugby union.\n\nProfessional sports leagues in North America comprise a stipulated number of clubs, known as franchises, which field one team each. \nThe franchises have territorial rights, usually exclusive territories large enough to cover major metropolitan areas, so that they have no local rivals. New teams may enter the competition only by a vote of current members; typically, a new place is put up for bid by would-be owners. This system is often called a \"franchise system.\" It was introduced in baseball with the formation of the National League in 1876 and later adopted by the other North American leagues.\n\nAlthough member clubs are corporate entities separate from their leagues, they operate only under league auspices. Partly because that relationship is so close, and partly because the four major team sports leagues represent the top level of play in the world, North American teams almost never play competitive games against outside opponents, although National Hockey League (NHL) and National Basketball Association (NBA) teams have played against European hockey and basketball teams in preseason exhibitions. The North American league, rather than any sport governing body, determines the playing rules and scoring rules of its game, and the rules under which players join and change teams.\n\nThe teams are organized with a view to each major city having a team to support. Only the largest cities such as New York, Chicago or Los Angeles have more than one team. As such the teams are often referred to as franchises. Even though they are not technically franchises in a business sense, the league is organised in a way that assures teams continued existence in the league from year to year, which fosters an ongoing connection with the team's supporters. On occasion a league may decide to grow the sport by admitting a new expansion team into the league. Most of the teams in the four major North American pro sports leagues were created as part of a planned league expansion or through the merger of a rival league. Only a handful of teams in the National Hockey League, for example, existed before becoming part of the NHL. The rest of the teams were created \"ex novo\" as expansion teams or as charter members of the World Hockey Association, which merged with the NHL in 1979.\n\nThe best teams in a given season reach a playoff tournament, and the winner of the playoffs is crowned champion of the league, and, in some cases as world champions. American and Canadian sports leagues typically have such \"playoff\" systems. These have their roots in long travel distances common in US and Canadian sports; to cut down on travel, leagues are typically aligned in geographic divisions and feature unbalanced schedules with teams playing more matches against opponents in the same division. Due to the unbalanced schedule typical in US and Canadian leagues, not all teams face the same opponents, and some teams may not meet during a regular season at all. This results in teams with identical records that have faced different opponents differing numbers of times, making team records alone an imperfect measure of league supremacy. The playoffs allow for head-to-head elimination-style competition between teams to counterbalance this.\n\nMajor League Soccer is a North American league that exhibits some aspects of the European structure because soccer has a European rather than American origin. Major League Soccer is technically not an association of franchises but a single business entity, though each team has an owner-operator; the team owners are actually shareholders in the league. The league, not the individual teams, contracts with the players. Unlike teams in the four major sports, several Major League Soccer teams qualify to play competitive matches in the CONCACAF Champions League against teams from outside the U.S. and Canada, and MLS uses playing rules set by the international governing body of its sport. MLS followed its own playing rules until 2004, when it adopted FIFA rules. In another parallel with the European model, both the U.S. and Canada have separate knockout cup competitions during the MLS season that include teams from lower leagues. In the U.S., the Lamar Hunt U.S. Open Cup has had MLS participation from the league's inception; since the 2012 cup, each competition has featured all American-based MLS sides. Similarly, all of Canada's MLS teams compete in the Canadian Championship. However, the league structure of MLS follows the North American model, with a single premier league and no promotion or relegation.\n\nMajor League Rugby, which began play in 2018 as the second attempt to launch a professional rugby union league in the U.S., has a business structure identical to that of MLS—it is a single business entity, with each team owner-operator being a league shareholder, and all player contracts are held by the league. Also similar to MLS, MLR uses playing rules set by its sport's international governing body. However, MLR teams do not play competitive matches against teams from other leagues, and there is currently no cup competition in U.S. rugby similar to soccer's U.S. Open Cup. The MLR league structure also follows the North American model of one premier league without promotion or relegation.\n\nA more rarely seen business model in North America is the pure single-entity league, typified by the now-defunct XFL. In the pure single-entity model, there are no individual owners or investors, with all teams centrally owned by a single corporation and operated by league employees. Many upstart leagues begin their existence as pure single-entity leagues before they secure investors for teams (such as the NWHL) or are forced to operate in the pure single-entity model when investors fail to materialize (such as the Stars Football League).\n\nSome other North American systems also have a hierarchical structure but without the promotion and relegation of clubs exhibited in the European model. Instead, the sports leagues generally use a minor league system. For example, Major League Baseball has an affiliation with Minor League Baseball to develop young talent. Most minor league clubs are independently owned, but each one contracts with a major league club that hires and pays players and assigns them to its various affiliated clubs. The minor clubs do not move up or down in the hierarchy by on-field success or failure. Professional ice hockey has a system somewhat similar to baseball's (without as many levels), while the National Basketball Association operates a single developmental league. The National Football League does not have a minor league system as of 2011 but it has operated or affiliated with minor leagues in the 1930s, 1940s, 1960s, 1990s, and the early 2000s and invested in another in the late 2000s.\n\nTo prevent conflicts of interest, most North American sports leagues that operate on the franchise system do not allow individual owners to hold more than one franchise at once. (This is a stipulation that arose out of a crisis in 1899, when the Cleveland Spiders were bought out by a rival team and had its roster raided, leading to the remains of the Spiders organization being stuffed with subpar talent and setting records for futility.) Most such leagues also use revenue sharing and a salary cap or luxury tax system in an effort to maintain some level of parity between franchises.\n\nFootball in England developed a very different system from the North American one, and it has been adopted for football in most other European countries, as well as to many other sports founded in Europe and played across the world. The features of the system are:\n\n\nEuropean football clubs are members both of a league and of a governing body. In the case of England, all competitive football clubs are members of The Football Association, while the top 20 teams also are members of the Premier League, a separate organization. The 72 teams in the three levels below the Premier League are members of still another body, the English Football League. The FA operates the national football team and tournaments that involve teams from different leagues (except the EFL Cup, operated by the English Football League and open to its own teams and those in the Premier League). In conjunction with FIFA and other countries' governing bodies, it also sets the playing rules and the rules under which teams can sell players' contracts to other clubs.\n\nThe rules or Laws of the Game are determined by the International Football Association Board.\n\nThe Premier League negotiates television contracts for its games. However, although the national league would be the dominating competition in which a club might participate, there are many non-league fixtures a club might play in a given year. In European football there are national cup competitions, which are single elimination knock-out tournaments, are played every year and all the clubs in the league participate. Also, the best performing clubs from the previous year may participate in pan-European tournaments such as the UEFA Champions League, operated by the Union of European Football Associations. A Premier League team might play a league game one week, and an FA Cup game against a team from a lower-level league the next, followed by an EFL Cup game against a team in the EFL, and then a fourth game might be against a team from across Europe in the Champions League.\n\nThe promotion and relegation system is generally used to determine membership of leagues. Most commonly, a pre-determined number of teams that finish the bottom of a league or division are automatically dropped down, or relegated, to a lower level for the next season. They are replaced by teams who are promoted from that lower tier either by finishing with the best records or by winning a playoff. In England, in the 2010–2011 season, the teams Birmingham City, Blackpool and West Ham United were relegated from the Premier League to the Football League Championship, the second level of English football. They were replaced by the top two teams from the second level, Queens Park Rangers and Norwich City, both of which won automatic promotion, as well as Swansea City (a Welsh club that plays in the English system), which won a playoff tournament of the teams that finished third through sixth. In the 2011–12 season, the teams Wolverhampton Wanderers, Blackburn, and Bolton were relegated to the Championship. They were replaced by Reading, Southampton, and West Ham. The two former teams had won automatic promotion, while the latter occupied the last promotion spot when they defeated Cardiff 5-0 on aggregate in the semifinals, and defeated Blackpool 2-1 at the final in Wembley Stadium.\n\nThe system originated in England in 1888 when twelve clubs decided to create a professional Football League. It then expanded by merging with the Football Alliance in 1892, with the majority of the Alliance teams occupying the lower Second Division, due to the divergent strengths of the teams. As this differential was overcome over the next five years, the winners of the Second Division went into a playoff with the worst placed team in the First Division, and if they won, were promoted into the top tier. The first club to achieve promotion was Sheffield United, which replaced the relegated Accrington F.C.\n\nRelegation often has devastating financial consequences for club owners who not only lose TV, sponsorship and gate income but may also see the asset value of their shares in the club collapse. Some leagues offer a \"parachute payment\" to its relegated teams for the following years in order to protect them from bankruptcy. If a team is promoted back to the higher tier the following year then the parachute payment for the second season is distributed among the teams of the lower division. There is of course a corresponding bonanza for promoted clubs.\n\nThe league does not choose which cities are to have teams in the top division. For example, Leeds, the fourth-biggest city in England, saw their team, Leeds United, relegated from the Premier League to the Championship in 2004, and relegated again to the third-tier League One in 2007 before returning to the Championship in 2010, where they remain to this day. Leeds will remain without a Premiership team as long as it takes for United, or in theory any other local club, to play well enough to be promoted into the Premiership. Famously, the French Ligue 1 lacked a team from Paris, France's capital and largest city, for some years. Likewise, Berlin clubs in the Bundesliga have been rare, due to the richer clubs being all located in the former West Germany.\n\nAs well as having no right to being in a certain tier, a club also has no territorial rights to its own area. A successful new team in a geographical location can come to dominate the incumbents. In Munich, for example, TSV 1860 München were initially more successful than the city's current biggest team Bayern München. As of the current 2018–19 season, London has 11 teams in the top four league levels, including six Premier League teams.\n\nClubs may be sold privately to new owners at any time, but this does not happen often where clubs are based on community membership and agreement. Such clubs require agreement from members who, unlike shareholders of corporations, have priorities other than money when it comes to their football club such as tradition or local identity. For similar reasons, relocation of clubs to other cities is very rare. This is mostly because virtually all cities and towns in Europe have a football club of some sort, the size and strength of the club usually relative to the town's size and importance. Buying an existing top-flight club and moving it to a new location is problematic, as the supporters of the town's original club are unlikely to switch allegiance to an interloper. This means anyone wanting ownership of a high ranked club in their native city must buy a local club as it stands and work it up through the divisions, usually by hiring better talent. There have been some cases where existing owners have chosen to relocate out of a difficult market, to better facilities, or simply to realize the market value of the land that the current stadium is built upon. As in the U.S., team relocations have been controversial as supporters of the club will protest at its loss.\n\nA rare example of a North American league that is attempting to develop a European-style model is the Canadian Premier League, set to launch in 2019 as a purely domestic top-flight soccer league, sharing said status with MLS. Unlike other North American leagues, the CPL is operating on a club-based model. While it will begin as a single league, it plans to eventually adopt promotion and relegation.\n\nLeagues around the world generally follow one or the other of these systems with some variation. Most sport leagues in Australia are similar to the North American model, using post-season playoffs and no relegation, but without geographical divisions, with the most notable examples being the Australian Football League (Aussie rules) and National Rugby League (rugby league). Nippon Professional Baseball in Japan uses the North American system due to American influence on the game. In cricket, the Indian Premier League, launched in 2008, also operates on this system. The Super League, which is the top level of rugby league in the United Kingdom and France, was run on a franchise basis from 2009 to 2014, but returned to a promotion-relegation model with the 2015 season. Another example of a franchised league in European sport is ice hockey's Kontinental Hockey League, centered mainly in Russia with teams also located in Belarus, China, Finland, Kazakhstan, Latvia, and Slovakia.\n\nThe promotion-relegation system is widely used in football around the world, notably in Africa and Latin America as well as Europe. The most notable variation has developed in Latin America where many countries have two league seasons per year, which scheduling allows because many Latin American nations lack a national cup competition. Promotion and relegation has historically been used in other team sports founded in the United Kingdom, such as rugby union, rugby league and cricket.\n\nThe European model is also used in Europe even when the sports were founded in America, showing that the league system adopted is not determined by the sport itself, but more on the tradition of sports organisation in that region. Sports such as basketball in Spain and Lithuania use promotion and relegation. In the same vein, the Australian A-League does not use the pyramid structure normally found in football, but instead follows the tradition of Australian sports having a franchise model and a post-season playoff system that better suits a country with a few important central locations where a sport needs to ensure there is a team playing with no risk of relegation. Likewise, another notable example of \"European\" sports using the American model is rugby union's Super Rugby, currently featuring 15 franchises. Thirteen of these are from the competition's founding countries of South Africa, Australia and New Zealand, and the other two are teams from Argentina and Japan that joined the competition in 2016.\n\nIn east Asia, places such as Japan, China, South Korea and Taiwan have a particular differentiation among leagues: \"European\" sports such as football and rugby use promotion and relegation, while \"American\" sports such as baseball and basketball use franchising and minor leagues, with a few differences varying from country to country. A similar situation exists in countries in Central America and the Caribbean, where football and baseball share several close markets.\n\nA major factor in the development of the North American closed membership system during the 19th Century was the distances between cities, with some teams separated by half of the North American continent, resulting in high traveling costs. When the National League of Professional Base Ball Clubs was established in 1876, its founders judged that in order to prosper, they must make baseball's highest level of competition a \"closed shop\", with a strict limit on the number of teams, and with each member having exclusive local rights. This guarantee of a place in the league year after year would permit each club owner to monopolize fan bases in their respective exclusive territories and give them the confidence to invest in infrastructure, such as improved ballparks. This in turn would guarantee the revenues needed to support traveling across the continent.\n\nIn contrast, the shorter distances between urban areas in England allowed more clubs to develop large fan bases without incurring the same travel costs as in North America. When The Football League, now known as the English Football League, was founded in 1888, it was not intended to be a rival of The Football Association but rather the top competition within it. The new league was not universally accepted as England's top-calibre competition right away. To help win fans of clubs outside The Football League, a system was established in which the worst teams at the end of each season would need to win re-election against any clubs wishing to join. A rival league, the Football Alliance, was then formed in 1889. When the two merged in 1892, it was not on equal terms; rather, most of the Alliance clubs were put in the new Football League Second Division, whose best teams would move up to the First Division in place of its worst teams. Another merger, with the top division of the Southern League in 1920, helped form the Third Division in similar fashion, firmly establishing the principle of promotion and relegation.\n\n\n"}
{"id": "18123971", "url": "https://en.wikipedia.org/wiki?curid=18123971", "title": "Royal Australian and New Zealand College of Obstetricians and Gynaecologists", "text": "Royal Australian and New Zealand College of Obstetricians and Gynaecologists\n\nThe Royal Australian and New Zealand College of Obstetricians and Gynaecologists (RANZCOG) is the body responsible for training and examining obstetricians and gynaecologists in New Zealand and Australia.\n\nThe head office of the College is in Melbourne, Australia.\n\nRANZCOG was formed in 1998 with the amalgamation of the Royal Australian College of Obstetricians and Gynaecologists (RACOG) and the Royal New Zealand College of Obstetricians and Gynaecologists (RNZCOG). The Australian Regional Council of the Royal College of Obstetricians and Gynaecologists (RCOG) was appointed in 1947 by the RCOG, a British organisation that had been in existence since 1929.\n\nAs of 2005, the College had over 4600 members of various classes. Of these, 58% were male and 42% female.\n\nThere are 6 categories of membership:\n\nThe College has publicly supported the decriminalisation of abortion, expanded rural health services, and has been a strong advocate for the implementation of vaccination programmes relating to cervical cancers, such as Gardasil. \n\nThe College is a non-government body, and is also independent of universities.\n\n\"RANZCOG training program\" is a 6-year structured post-graduate program which leads first to certification as a Member of RANZCOG (MRANZCOG) and then to certification as a Fellow of RANZCOG (FRANZCOG). FRANZCOG status is the only post-graduate qualification which leads to recognition as a specialist obstetrician & gynaecologist in Australia and New Zealand.\n\nThe first 4 years of general obstetric and gynaecological training is known as the Integrated Training Program (ITP).\n\n"}
{"id": "17423139", "url": "https://en.wikipedia.org/wiki?curid=17423139", "title": "Saint Mary's Regional Medical Center (Maine)", "text": "Saint Mary's Regional Medical Center (Maine)\n\nSaint Mary's Regional Medical Center is a hospital at 93 Campus Avenue in Lewiston, Maine adjacent to Bates College. It is part of a complex of care facilities now operated by Covenant Health Systems, a Roman Catholic non-profit organization based in Massachusetts. The hospital has 233 beds. It was founded in 1888 by the Grey Nuns, and has been a major provided of emergency health services in the Lewiston-Auburn area since.\n\nThe medical center is located just south of the Bates College campus, and occupies a triangular lot bounded by Campus Avenue and Golder and Sabattus Streets. Separating the medical center from the campus is St. Mary's d'Youville Pavilion, a senior care facility also operated by Covenant Health. Access to the hospital is via a circular drive on Sabattus Street. The facility's largest buildings are the result of expansion in the 1960s in 1970s.\n\nThe hospital was founded in June 1888 by the Sisters of Charity of St. Hyacinthe, and was the first Catholic-run hospital in the state. The sisters originally converted the Sarah J. Golder house into a 30-bed hospital and orphanage. Saint Mary's hospital was also known as the Sister's Hospital, the French Hospital, or the Catholic Hospital. The first purpose-built hospital building was constructed in 1902. This Late Gothic Revival building was designed by Lewiston architect William R. Miller, and still forms part of the facility today. It was listed on the National Register of Historic Places in 1987 for its architecture and historical significance.\n\n\n"}
{"id": "5279894", "url": "https://en.wikipedia.org/wiki?curid=5279894", "title": "Samsjøen (Trøndelag)", "text": "Samsjøen (Trøndelag)\n\nSamsjøen is a lake in Trøndelag county, Norway. The lake sits on the border of the municipalities of Midtre Gauldal and Melhus. Most of the lake lies in Midtre Gauldal, about north of the village of Singsås. The lake has a dam at the northwest end, which is used for hydroelectric power production. The water flows out of the lake and into the Lundesokna river which flows into the river Gaula.\n\n"}
{"id": "3854512", "url": "https://en.wikipedia.org/wiki?curid=3854512", "title": "Shark cartilage", "text": "Shark cartilage\n\nShark cartilage is a dietary supplement made from the dried and powdered cartilage of a shark; that is, from the tough material that composes a shark's skeleton. Shark cartilage is marketed under a variety of brand names, including Carticin, Cartilade, or BeneFin, and is marketed explicitly or implicitly as a treatment or preventive for various illnesses, including cancer.\n\nThere is no scientific evidence that shark cartilage is useful in treating or preventing cancer or other diseases. Controlled trials have shown no benefit to shark cartilage supplements, and shark cartilage contains potentially toxic compounds linked to Alzheimer's disease and amyotrophic lateral sclerosis. However, shark cartilage supplements are still marketed using the misconception that sharks do not get cancer, a myth that was as popularized by the 1992 book \"Sharks Don't Get Cancer\". In the United States, the Federal Trade Commission has taken legal action against such fraudulent promoters.\n\nTumors of many kinds, some metastatic, have been found in numerous species of sharks. The first shark tumor was recorded in 1908. Scientists have since discovered 40 benign and cancerous tumors in 18 of the 1,168 species of sharks. Scarcity of studies on shark physiology has perhaps allowed the myth to be accepted as fact for so many years. Numerous cancers in sharks, including tumors in shark cartilage, were documented by Gary Ostrander and his colleagues from the University of Hawaii in research published in 2004.\n\nThe ongoing consumption of shark cartilage supplements has been linked to a significant decline in shark populations and the popularity of these supplements has been described as a triumph of pseudoscience and marketing over scientific evaluation.\n\nManufacturers of shark cartilage supplements provide anecdotal testimonials from those who claim to have experienced relief from arthritis symptoms and pain, as a result of taking shark cartilage supplements.\n\nOpponents cite existing studies of shark cartilage on a variety of cancers that produced negligible to non-existent results in the prevention or treatment of cancer. Most notable among these was a breast-cancer trial conducted by the Mayo Clinic that stated that the trial \"was unable to demonstrate any suggestion of efficacy for this shark cartilage product in patients with advanced cancer.\" The results of another clinical trial were presented at the 43rd annual meeting of the American Society of Clinical Oncology. In that study, sponsored by the National Cancer Institute, \"researchers did not find a statistical difference in survival\" between patients receiving shark cartilage and those taking a placebo. Scientific evidence does not support the efficacy of shark cartilage nor the ability of effective components to remove cancer cells. The fact that people believe eating shark cartilage can cure cancer shows the serious potential impacts of pseudoscience.\n\nDetractors also purport that previous beliefs in regards to sharks and cancer have been overturned, as forty-two varieties of cancer have now been discovered in sharks and related species. Also, many opponents feel that non-existent (or even limited) results do not justify the rampant over-fishing of many endangered species of sharks, further threatening their extinction.\n\nIn the summer of 2004, Lane Labs, the manufacturers of BeneFin, was ordered to cease the promotion of BeneFin as a treatment or cure for cancer, as they had not conducted any research as to their claims for the product, much less reported any potential side effects. Thus, the FDA ordered Lane Labs to \"pay restitution to all of its customers from September of 1999 to the present.\"\n\n"}
{"id": "25716089", "url": "https://en.wikipedia.org/wiki?curid=25716089", "title": "Silver–Russell syndrome", "text": "Silver–Russell syndrome\n\nSilver–Russell syndrome (SRS), also called Silver–Russell dwarfism or Russell–Silver syndrome (RSS) is a growth disorder occurring in approximately 1/50,000 to 1/100,000 births. In the United States it is usually referred to as Russell–Silver syndrome, and Silver–Russell syndrome elsewhere. It is one of 200 types of dwarfism and one of five types of primordial dwarfism. \n\nThere is no statistical significance of the syndrome occurring preferentially in either males or females.\n\nAlthough confirmation of a specific genetic marker is in a significant number of individuals, there are no tests to clearly determine if this is what a person has. As a 'syndrome' a diagnosis is typically given for children upon confirmation of the 'presence' of several 'symptoms' listed below. Symptoms are Intrauterine Growth Restriction (IUGR) combined with some of the following:\n\nThe average adult height for patients without growth hormone treatment is 4'11\" for males and 4'7\" for females.\n\nIts exact cause is unknown, but present research points toward a genetic and epigenetic component, possibly following maternal genes on chromosomes 7 and 11.\n\nIt is estimated that approximately 50% of Silver Russell patients have hypomethylation of \"H19\" and \"IGF2\". This is thought to lead to low expression of IGF2 and over-expression of the H19 gene.\n\nIn 10% of the cases the syndrome is associated with maternal uniparental disomy (UPD) on chromosome 7. This is an imprinting error where the person receives two copies of chromosome 7 from the mother (maternally inherited) rather than one from each parent.\n\nOther genetic causes such as duplications, deletions and chromosomal aberrations have also linked to Silver–Russell syndrome.\n\nInterestingly, Silver-Russell patients have variable hypomethylation levels in different body tissues, suggesting a mosaic pattern and a postzygotic epigenetic modification issue. This could explain the body asymmetry of the SRS phenotype.\n\nLike other imprinting disorders (e.g. Prader–Willi syndrome, Angelman syndrome, and Beckwith–Wiedemann syndrome), Silver–Russell syndrome may be associated with the use of assisted reproductive technologies such as in vitro fertilization.\n\nFor many years the diagnosis of Silver-Russell Syndrome was clinical. However, this lead to overlaps with syndromes with similar clinical features such as Temple Syndrome and 12q14 microdeletion syndrome. In 2017, an international consensus was published - detailing the steps clinicians should take to diagnose Silver-Russell Syndrome. It is now recommended to test for 11p15 loss of methylation and mUPD7 first. If they are negative, then testing for mUPD16, mUPD20 should take place. Testing for 14q32 should also be considered, to rule out Temple syndrome as a differential diagnosis. If these tests come back inconclusive, then a clinical diagnosis should be made.\n\nIt is recommended that the Netchine-Harbison clinical scoring system (NH-CSS) is used to group the clinical features together in a point based score.\n\nThe caloric intake of children with SRS must be carefully controlled in order to provide the best opportunity for growth. If the child is unable to tolerate oral feeding, then enteral feeding may be used, such as the percutaneous endoscopic gastrostomy.\n\nIn children with limb-length differences or scoliosis, physiotherapy can alleviate the problems caused by these symptoms. In more severe cases, surgery to lengthen limbs may be required. To prevent aggravating posture difficulties children with leg length differences may require a raise in their shoe.\n\nGrowth hormone therapy is often prescribed as part of the treatment of SRS. The hormones are given by injection typically daily from the age of 2 years old through teenage years. It may be effective even when the patient does not have a growth hormone deficiency. Growth hormone therapy has been shown to increase the rate of growth in patients and consequently prompts 'catch up' growth. This may enable the child to begin their education at a normal height, improving their self-esteem and interaction with other children. The effect of growth hormone therapy on mature and final height is as yet uncertain. There are some theories suggesting that the therapy also assists with muscular development and managing hypoglycemia.\n\nIt is named for Henry Silver and Alexander Russell.\n\n"}
{"id": "8174973", "url": "https://en.wikipedia.org/wiki?curid=8174973", "title": "Sink strainer", "text": "Sink strainer\n\nIn plumbing, a sink strainer is a type of perforated metal sieve or mesh strainer used to strain or filter out solid debris in the water system. Different varieties are used in residential premises and for industrial or commercial applications. Such strainer elements are generally made from stainless steel for corrosion resistance.\n\nIn houses, sink strainers are often used as drain covers in sinks, showers and bathtubs.\n\nWater lines or kitchen systems can get gravel, deposits that break free, and other stray items in the line. Due to the velocity of the water pushing them, they can severely damage or clog devices installed in the flow stream of the water line, for example p-traps or pipes. A strainer is essentially a screen installed to allow water to pass through, but not larger items. The larger items fall to the bottom or are held in a basket for later clean out. They normally have an access that allows for them to be cleaned or have the strainer plate or basket replaced.\n\nStrainers come in several different styles based on the needs. A plate strainer is the simplest, in which water flows through a perforated plate. Often the plate is corrugated shape to increase surface area. A basket strainer is a design where the strainer is shaped like a basket and usually installed in a vertical system. The basket strainer is easier to clean, since debris is captured in the basket. It can also sometimes offer more straining surface area than a plate strainer, improving flow rates, or decreasing pressure loss through the strainer.\n\nThey can be made of stainless steel AISI 304, 202 etc.\n\n"}
{"id": "17481462", "url": "https://en.wikipedia.org/wiki?curid=17481462", "title": "Subtypes of HIV", "text": "Subtypes of HIV\n\nOne of the obstacles to treatment of the human immunodeficiency virus is its high genetic variability. HIV can be divided into two major types, HIV type 1 (HIV-1) and HIV type 2 (HIV-2). HIV-1 is related to viruses found in chimpanzees and gorillas living in western Africa, while HIV-2 viruses are related to viruses found in the endangered west African primate sooty mangabey.<ref name=\"doi10.1101/cshperspect.a006841\"></ref> HIV-1 viruses may be further divided into groups. The HIV-1 group M viruses predominate and are responsible for the AIDS pandemic. Group M can be further subdivided into subtypes based on genetic sequence data. Some of the subtypes are known to be more virulent or are resistant to different medications. Likewise, HIV-2 viruses are thought to be less virulent and transmissible than HIV-1 M group viruses, although HIV-2 is known to cause AIDS.\n\nHIV-1 is the most common and pathogenic strain of the virus. Scientists divide HIV-1 into a major group (Group M) and two or more minor groups, namely Group N, O and possibly a group P. Each group is believed to represent an independent transmission of SIV into humans (but subtypes within a group are not). A total of 39 ORFs are found in all six possible reading frames (RFs) of HIV-1 complete genome sequence, but only a few of them are functional.\n\nWith 'M' for \"major\", this is by far the most common type of HIV, with more than 90% of HIV/AIDS cases deriving from infection with HIV-1 group M. This major HIV virus which was the source of pre-1960 pandemic viruses originated in the 1920s in Kinshasa, which is now known as the capital of the Democratic Republic of Congo (DRC). The M group is subdivided further into clades, called subtypes, that are also given a letter. There are also \"circulating recombinant forms\" or CRFs derived from recombination between viruses of different subtypes which are each given a number. CRF12_BF, for example, is a recombination between subtypes B and F.\n\nThe spatial movement of these subtypes moved along the railways and waterways of the Democratic Republic of Congo (DRC) from Kinshasa to these other areas. These subtypes are sometimes further split into sub-subtypes such as A1 and A2 or F1 and F2. In 2015, the strain CRF19, a recombinant of subtype A, subtype D and subtype G, with a subtype D protease, was found to be strongly associated with rapid progression to AIDS in Cuba. This is not thought to be a complete or final list, and further types are likely to be found.\n\nThe 'N' stands for \"non-M, non-O\". This group was discovered by a Franco-Cameroonia team in 1998, when they identified and isolated the HIV-1 variant strain, YBF380, from a Cameroonian woman who died of AIDS in 1995. When tested, the YBF380 variant reacted with an envelope antigen from SIVcpz rather than with those of Group M or Group O, indicating it was indeed a novel strain of HIV-1. As of 2015, less than 20 Group N infections have been recorded.<ref name=\"doi10.1089/aid.2006.22.83\"></ref>\n\nThe O (\"Outlier\") group has infected about 100,000 individuals located in West-Central Africa and is not usually seen outside of that area. It is reportedly most common in Cameroon, where a 1997 survey found that about 2% of HIV-positive samples were from Group O. The group caused some concern because it could not be detected by early versions of the HIV-1 test kits. More advanced HIV tests have now been developed to detect both Group O and Group N.\n\nIn 2009, a newly analyzed HIV sequence was reported to have greater similarity to a simian immunodeficiency virus recently discovered in wild gorillas (SIVgor) than to SIVs from chimpanzees (SIVcpz). The virus had been isolated from a Cameroonian woman residing in France who was diagnosed with HIV-1 infection in 2004. The scientists reporting this sequence placed it in a proposed Group P \"pending the identification of further human cases\".\n\nHIV-2 has not been widely recognized outside of Africa. The first case in the United States was in 1987. Many test kits for HIV-1 will also detect HIV-2.\n\nAs of 2010, there are 8 known HIV-2 groups (A to H). Of these, only groups A and B are pandemic. Group A is found mainly in West Africa, but has also spread globally to Angola, Mozambique, Brazil, India, Europe, and the US. Despite the presence of HIV-2 globally, Group B is mainly confined to West Africa. Despite its relative confinement, HIV-2 should be considered in all patients exhibiting symptoms of HIV that not only come from West Africa, but also anyone who has had any body fluid transfer with a person from West Africa (i.e. needle sharing, sexual contact, etc.).\n\nHIV-2 is closely related to simian immunodeficiency virus endemic in sooty mangabeys (\"Cercocebus atys atys\") (SIVsmm), a monkey species inhabiting the forests of Littoral West Africa. Phylogenetic analyses show that the virus most closely related to the two strains of HIV-2 which spread considerably in humans (HIV-2 groups A and B) is the SIVsmm found in the sooty mangabeys of the Tai forest, in western Ivory Coast.\n\nThere are six additional known HIV-2 groups, each having been found in just one person. They all seem to derive from independent transmissions from sooty mangabeys to humans. Groups C and D have been found in two people from Liberia, groups E and F have been discovered in two people from Sierra Leone, and groups G and H have been detected in two people from the Ivory Coast. Each of these HIV-2 strains, for which humans are probably dead-end hosts, is most closely related to SIVsmm strains from sooty mangabeys living in the same country where the human infection was found.\n\nHIV-2 diagnosis can be made when a patient has no symptoms but positive blood work indicating the individual has HIV. The Multispot HIV-1/HIV-2 Rapid Test is currently the only FDA approved method for such differentiation between the two viruses. Recommendations for the screening and diagnosis of HIV has always been to use enzyme immunoassays that detect HIV-1, HIV-1 group O, and HIV-2. When screening the combination, if the test is positive followed by an indeterminate HIV-1 western blot, a follow up test, such as amino acid testing, must be performed to distinguish which infection is present. According to the NIH, a differential diagnosis of HIV-2 should be considered when a person is of West African descent or has had sexual contact or shared needles with such a person. West Africa is at the highest risk as it is the origin of the virus.\n\nHIV-2 has been found to be less pathogenic than HIV-1. The mechanism of HIV-2 is not clearly defined, nor the difference from HIV-1, however the transmission rate is much lower in HIV-2 than HIV-1. Both infections can lead to AIDS in affected individuals and both can mutate to develop drug resistance. Disease Monitoring in patients with HIV-2 includes clinical evaluation and CD4 cell counts, while treatment includes Anti-Retroviral Therapy (ART), Nucleoside Reverse Transcriptase Inhibitors (NRTIs), Protease Inhibitors (PI), and Non-Nucleoside Reverse Transcriptase Inhibitors (NNRTIs) with the addition of CCR5 co-receptor antagonists and fusion inhibitors.\n\nChoice of initial and/or second-line therapy for HIV-2 has not yet been defined. HIV-2 appears to be resistant to NNRTIs intrinsically, but may be sensitive to NRTIs, though the mechanism is poorly understood. Protease inhibitors have shown variable effect, while integrase inhibitors are also being evaluated. Combination regimens of the above listed therapies are being looked into as well, also showing variable effect depending on the types of therapies combined. While the mechanisms are not clearly understood for HIV-1 and HIV-2, it is known that they use different pathways and patterns, making the algorithms used to evaluate HIV-1 resistance-associated mutations irrelevant to HIV-2.\n\nEach virus can be contracted individually, or they can be contracted together in what is referred to as co-infection. HIV-2 seems to have lower mortality rates, less severe symptoms and slower progression to AIDS than HIV-1 alone or the co-infection. In co-infection, however, this is largely dependent on which virus was contracted first. HIV-1 tends to out compete HIV-2 for disease progression. Co-infection seems to be a growing problem globally as time progresses, with most cases being identified in West African countries, as well as some cases in the US.\n\n If a pregnant mother is exposed, screening is performed as normal. If HIV-2 is present, a number of perinatal ART drugs may be given as a prophylactic to lower the risk of mother-to-child transmission. After the child is born, a standard 6-week regimen of these prophylactics should be initiated. Breast milk may also contain particles of HIV-2; therefore, breastfeeding is strictly advised against.\n\nThe rapid evolution of HIV can be attributed to the high mutation rate it has. During the early stages of mutation, evolution appears to be neutral due to the absence of an evolutionary response. However, when examining the virus in several different individuals, convergent mutations can be found appearing in these viral populations independently.\n\nHIV evolution within a host influences factors including the virus' set-point viral load. If the virus has a low set-point viral load, the host will live longer, and there is a greater probability that the virus will be transmitted to another individual. If the virus has a high set-point viral load, the host will live for a shorter amount of time and there is a lower probability that the virus will be transmitted to another individual. HIV has evolved to maximize the number of infections to other hosts, and this tendency for selection to favor intermediate strains shows that HIV undergoes stabilizing selection.\n\nThe virus has also evolved to become more infectious between hosts. There are three different mechanisms that allow HIV to evolve at a population level. One includes the continuous battle to evolve and overcome the immune system which slows down the evolution of HIV and shifts the virus’ focus towards a population level. Another includes the slow evolution of viral load due to viral load mutations being neutral within the host. The last mechanism focuses on the virus' preference to transmit founding viral strains stored during the early stages of infection. This preference of the virus to transmit its stored genome copies explains why HIV evolves more quickly within the host than between hosts.\n\nHIV is evolving to a milder form but is \"an awfully long way\" from no longer being deadly.\n\nIsolates of HIV-1 and HIV-2 with resistance to antiretroviral drugs arise through natural selection and genetic mutations, which have been tracked and analyzed. The Stanford HIV Drug Resistance Database and the International AIDS Society publish lists of the most important of these; first year listing 80 common mutations, and the latest year 93 common mutations, and made available through the Stanford HIV RT and Protease Sequence Database.\n\n\n"}
{"id": "32345607", "url": "https://en.wikipedia.org/wiki?curid=32345607", "title": "TEDMED", "text": "TEDMED\n\nTEDMED is an annual conference focusing on health and medicine, with a year-round web-based community. TEDMED is an independent event operating under license from the nonprofit TED conference.\n\n, TEDMED staff operates from Stamford, Connecticut.\n\nTalks given at TEDMED combine “the nexus of health, information and technology” with “compelling personal stories” and “a glimpse into the future of healthcare.”\n\nThe intent of the conference has been described as “a gathering of geniuses” that brings together “some of the most innovative, thoughtful pioneers of healthcare technology, media, and entertainment into one big four-day ‘dinner party’ to learn from one another and mix people up from different disciplines and industries to solve big problems in healthcare.”\n\nTEDMED was founded in 1998 by TED’s founder Richard Wurman. TEDMED was inactive for a number of years, and in 2008 Wurman sold the rights to TEDMED to entrepreneur Marc Hodosh.\n\nHodosh recreated TEDMED and launched its first conference under his guidance in San Diego in October 2009.\n\nIn January 2010, TED.com began including videos of TEDMED talks on the TED website.\n\nIn October 2010, TEDMED was held in San Diego again and sold out for a second year, attracting notable healthcare leaders and Hollywood celebrities.\n\nIn 2011, Jay Walker and a group of executives and investors purchased TEDMED from Hodosh for $16 million with future additional payments of as much as $9 million. The conference was then moved to Washington, DC.\n\nIn November 2016, TEDMED was held in Palm Springs, CA.\n\n"}
{"id": "889672", "url": "https://en.wikipedia.org/wiki?curid=889672", "title": "Tropical disease", "text": "Tropical disease\n\nTropical diseases are diseases that are prevalent in or unique to tropical and subtropical regions. The diseases are less prevalent in temperate climates, due in part to the occurrence of a cold season, which controls the insect population by forcing hibernation. However, many were present in northern Europe and northern America in the 17th and 18th centuries before modern understanding of disease causation. The initial impetus for tropical medicine was to protect the health of colonialists, notably in India under the British Raj. Insects such as mosquitoes and flies are by far the most common disease carrier, or vector. These insects may carry a parasite, bacterium or virus that is infectious to humans and animals. Most often disease is transmitted by an insect \"bite\", which causes transmission of the infectious agent through subcutaneous blood exchange. Vaccines are not available for most of the diseases listed here, and many do not have cures.\n\nHuman exploration of tropical rainforests, deforestation, rising immigration and increased international air travel and other tourism to tropical regions has led to an increased incidence of such diseases.\n\nIn 1975 the Special Programme for Research and Training in Tropical Diseases (TDR) was established to focus on neglected infectious diseases which disproportionately affect poor and marginalized populations in developing regions of Africa, Asia, Central America and South America. It was established at the World Health Organization, which is the executing agency, and is co-sponsored by the United Nations Children's Fund, United Nations Development Programme, the World Bank and the World Health Organization.\n\nTDR's vision is to foster an effective global research effort on infectious diseases of poverty in which disease endemic countries play a pivotal role. It has a dual mission of developing new tools and strategies against these diseases, and to develop the research and leadership capacity in the countries where the diseases occur. The TDR secretariat is based in Geneva, Switzerland, but the work is conducted throughout the world through many partners and funded grants.\n\nSome examples of work include helping to develop new treatments for diseases, such as ivermectin for onchocerciasis (river blindness); showing how packaging can improve use of artemesinin-combination treatment (ACT) for malaria; demonstrating the effectiveness of bednets to prevent mosquito bites and malaria; and documenting how community-based and community-led programmes increases distribution of multiple treatments. TDR history\n\nThe current TDR disease portfolio includes the following entries:\n\n\nAdditional neglected tropical diseases include:\n\nSome tropical diseases are very rare, but may occur in sudden epidemics, such as the Ebola hemorrhagic fever, Lassa fever and the Marburg virus. There are hundreds of different tropical diseases which are less known or rarer, but that, nonetheless, have importance for public health.\n\nThe so-called \"exotic\" diseases in the tropics have long been noted both by travelers, explorers, etc., as well as by physicians. One obvious reason is that the hot climate present during all the year and the larger volume of rains directly affect the formation of breeding grounds, the larger number and variety of natural reservoirs and animal diseases that can be transmitted to humans (zoonosis), the largest number of possible insect vectors of diseases. It is possible also that higher temperatures may favor the replication of pathogenic agents both inside and outside biological organisms. Socio-economic factors may be also in operation, since most of the poorest nations of the world are in the tropics. Tropical countries like Brazil, which have improved their socio-economic situation and invested in hygiene, public health and the combat of transmissible diseases have achieved dramatic results in relation to the elimination or decrease of many endemic tropical diseases in their territory.\n\nClimate change, global warming caused by the greenhouse effect, and the resulting increase in global temperatures, are possibly causing tropical diseases and vectors to spread to higher altitudes in mountainous regions, and to higher latitudes that were previously spared, such as the Southern United States, the Mediterranean area, etc. For example, in the Monteverde cloud forest of Costa Rica, global warming enabled Chytridiomycosis, a tropical disease, to flourish and thus force into decline amphibian populations of the Monteverde Harlequin frog. Here, global warming raised the heights of orographic cloud formation, and thus produced cloud cover that would facilitate optimum growth conditions for the implicated pathogen, B. dendrobatidis.\n\nSome of the strategies for controlling tropical diseases include: \n\n\n\n\n"}
{"id": "32161", "url": "https://en.wikipedia.org/wiki?curid=32161", "title": "Urinary tract infection", "text": "Urinary tract infection\n\nA urinary tract infection (UTI) is an infection that affects part of the urinary tract. When it affects the lower urinary tract it is known as a bladder infection (cystitis) and when it affects the upper urinary tract it is known as kidney infection (pyelonephritis). Symptoms from a lower urinary tract include pain with urination, frequent urination, and feeling the need to urinate despite having an empty bladder. Symptoms of a kidney infection include fever and flank pain usually in addition to the symptoms of a lower UTI. Rarely the urine may appear bloody. In the very old and the very young, symptoms may be vague or non-specific.\nThe most common cause of infection is \"Escherichia coli\", though other bacteria or fungi may rarely be the cause. Risk factors include female anatomy, sexual intercourse, diabetes, obesity, and family history. Although sexual intercourse is a risk factor, UTIs are not classified as sexually transmitted infections (STIs). Kidney infection, if it occurs, usually follows a bladder infection but may also result from a blood-borne infection. Diagnosis in young healthy women can be based on symptoms alone. In those with vague symptoms, diagnosis can be difficult because bacteria may be present without there being an infection. In complicated cases or if treatment fails, a urine culture may be useful.\nIn uncomplicated cases, UTIs are treated with a short course of antibiotics such as nitrofurantoin or trimethoprim/sulfamethoxazole. Resistance to many of the antibiotics used to treat this condition is increasing. In complicated cases, a longer course or intravenous antibiotics may be needed. If symptoms do not improve in two or three days, further diagnostic testing may be needed. Phenazopyridine may help with symptoms. In those who have bacteria or white blood cells in their urine but have no symptoms, antibiotics are generally not needed, although during pregnancy is an exception. In those with frequent infections, a short course of antibiotics may be taken as soon as symptoms begin or long-term antibiotics may be used as a preventative measure.\nAbout 150 million people develop a urinary tract infection in a given year. They are more common in women than men. In women, they are the most common form of bacterial infection. Up to 10% of women have a urinary tract infection in a given year, and half of women have at least one infection at some point in their lifetime. They occur most frequently between the ages of 16 and 35 years. Recurrences are common. Urinary tract infections have been described since ancient times with the first documented description in the Ebers Papyrus dated to c. 1550 BC.\n\nLower urinary tract infection is also referred to as a bladder infection. The most common symptoms are burning with urination and having to urinate frequently (or an urge to urinate) in the absence of vaginal discharge and significant pain. These symptoms may vary from mild to severe and in healthy women last an average of six days. Some pain above the pubic bone or in the lower back may be present. People experiencing an upper urinary tract infection, or pyelonephritis, may experience flank pain, fever, or nausea and vomiting in addition to the classic symptoms of a lower urinary tract infection. Rarely, the urine may appear bloody or contain visible pus in the urine.\n\nIn young children, the only symptom of a urinary tract infection (UTI) may be a fever. Because of the lack of more obvious symptoms, when females under the age of two or uncircumcised males less than a year exhibit a fever, a culture of the urine is recommended by many medical associations. Infants may feed poorly, vomit, sleep more, or show signs of jaundice. In older children, new onset urinary incontinence (loss of bladder control) may occur.\n\nUrinary tract symptoms are frequently lacking in the elderly. The presentations may be vague with incontinence, a change in mental status, or fatigue as the only symptoms, while some present to a health care provider with sepsis, an infection of the blood, as the first symptoms. Diagnosis can be complicated by the fact that many elderly people have preexisting incontinence or dementia.\n\nIt is reasonable to obtain a urine culture in those with signs of systemic infection that may be unable to report urinary symptoms, such as when advanced dementia is present. Systemic signs of infection include a fever or increase in temperature of more than from usual, chills, and an increased white blood cell count.\n\nUropathogenic \"E. coli\" from the gut is the cause of 80–85% of community-acquired urinary tract infections, with \"Staphylococcus saprophyticus\" being the cause in 5–10%. Rarely they may be due to viral or fungal infections. Healthcare-associated urinary tract infections (mostly related to urinary catheterization) involve a much broader range of pathogens including: \"E. coli\" (27%), \"Klebsiella\" (11%), \"Pseudomonas\" (11%), the fungal pathogen \"Candida albicans\" (9%), and \"Enterococcus\" (7%) among others. Urinary tract infections due to \"Staphylococcus aureus\" typically occur secondary to blood-borne infections. \"Chlamydia trachomatis\" and \"Mycoplasma genitalium\" can infect the urethra but not the bladder. These infections are usually classified as a urethritis rather than urinary tract infection.\n\nIn young sexually active women, sexual activity is the cause of 75–90% of bladder infections, with the risk of infection related to the frequency of sex. The term \"honeymoon cystitis\" has been applied to this phenomenon of frequent UTIs during early marriage. In post-menopausal women, sexual activity does not affect the risk of developing a UTI. Spermicide use, independent of sexual frequency, increases the risk of UTIs. Diaphragm use is also associated. Condom use without spermicide or use of birth control pills does not increase the risk of uncomplicated urinary tract infection.\n\nWomen are more prone to UTIs than men because, in females, the urethra is much shorter and closer to the anus. As a woman's estrogen levels decrease with menopause, her risk of urinary tract infections increases due to the loss of protective vaginal flora. Additionally, vaginal atrophy that can sometimes occur after menopause is associated with recurrent urinary tract infections.\n\nChronic prostatitis in the forms of chronic prostatitis/chronic pelvic pain syndrome and chronic bacterial prostatitis (not acute bacterial prostatitis or asymptomatic inflammatory prostatitis) may cause recurrent urinary tract infections in males. Risk of infections increases as males age. While bacteria is commonly present in the urine of older males this does not appear to affect the risk of urinary tract infections.\n\nUrinary catheterization increases the risk for urinary tract infections. The risk of bacteriuria (bacteria in the urine) is between three and six percent per day and prophylactic antibiotics are not effective in decreasing symptomatic infections. The risk of an associated infection can be decreased by catheterizing only when necessary, using aseptic technique for insertion, and maintaining unobstructed closed drainage of the catheter.\n\nMale scuba divers using condom catheters and female divers using external catching devices for their dry suits are also susceptible to urinary tract infections.\n\nA predisposition for bladder infections may run in families. This is believed to be related to genetics. Other risk factors include diabetes, being uncircumcised, and having a large prostate. In children UTIs are associated with vesicoureteral reflux (an abnormal movement of urine from the bladder into ureters or kidneys) and constipation.\n\nPersons with spinal cord injury are at increased risk for urinary tract infection in part because of chronic use of catheter, and in part because of voiding dysfunction. It is the most common cause of infection in this population, as well as the most common cause of hospitalization. Additionally, use of cranberry juice or cranberry supplement appears to be ineffective in prevention and treatment in this population.\n\nThe bacteria that cause urinary tract infections typically enter the bladder via the urethra. However, infection may also occur via the blood or lymph. It is believed that the bacteria are usually transmitted to the urethra from the bowel, with females at greater risk due to their anatomy. After gaining entry to the bladder, \"E. Coli\" are able to attach to the bladder wall and form a biofilm that resists the body's immune response.\n\n\"Escherichia coli\" is the single most common microorganism, followed by \"Klebsiella\" and \"Proteus\" spp., to cause urinary tract infection. \"Klebsiella\" and \"Proteus\" spp., are frequently associated with stone disease. The presence of Gram positive bacteria such as \"Enterococcus\" and \"Staphylococcus\" increased.\n\nThe increased resistance of urinary pathogens to quinolones has been reported worldwide and might be the consequence of overuse and misuse of quinolones.\n\nIn straightforward cases, a diagnosis may be made and treatment given based on symptoms alone without further laboratory confirmation. In complicated or questionable cases, it may be useful to confirm the diagnosis via urinalysis, looking for the presence of urinary nitrites, white blood cells (leukocytes), or leukocyte esterase. Another test, urine microscopy, looks for the presence of red blood cells, white blood cells, or bacteria. Urine culture is deemed positive if it shows a bacterial colony count of greater than or equal to 10 colony-forming units per mL of a typical urinary tract organism. Antibiotic sensitivity can also be tested with these cultures, making them useful in the selection of antibiotic treatment. However, women with negative cultures may still improve with antibiotic treatment. As symptoms can be vague and without reliable tests for urinary tract infections, diagnosis can be difficult in the elderly.\n\nA urinary tract infection may involve only the lower urinary tract, in which case it is known as a bladder infection. Alternatively, it may involve the upper urinary tract, in which case it is known as pyelonephritis. If the urine contains significant bacteria but there are no symptoms, the condition is known as asymptomatic bacteriuria. If a urinary tract infection involves the upper tract, and the person has diabetes mellitus, is pregnant, is male, or immunocompromised, it is considered complicated. Otherwise if a woman is healthy and premenopausal it is considered uncomplicated. In children when a urinary tract infection is associated with a fever, it is deemed to be an upper urinary tract infection.\n\nTo make the diagnosis of a urinary tract infection in children, a positive urinary culture is required. Contamination poses a frequent challenge depending on the method of collection used, thus a cutoff of 10 CFU/mL is used for a \"clean-catch\" mid stream sample, 10 CFU/mL is used for catheter-obtained specimens, and 10 CFU/mL is used for suprapubic aspirations (a sample drawn directly from the bladder with a needle). The use of \"urine bags\" to collect samples is discouraged by the World Health Organization due to the high rate of contamination when cultured, and catheterization is preferred in those not toilet trained. Some, such as the American Academy of Pediatrics recommends renal ultrasound and voiding cystourethrogram (watching a person's urethra and urinary bladder with real time x-rays while they urinate) in all children less than two years old who have had a urinary tract infection. However, because there is a lack of effective treatment if problems are found, others such as the National Institute for Health and Care Excellence only recommends routine imaging in those less than six months old or who have unusual findings.\n\nIn women with cervicitis (inflammation of the cervix) or vaginitis (inflammation of the vagina) and in young men with UTI symptoms, a \"Chlamydia trachomatis\" or \"Neisseria gonorrheae\" infection may be the cause. These infections are typically classified as a urethritis rather than a urinary tract infection. Vaginitis may also be due to a yeast infection. Interstitial cystitis (chronic pain in the bladder) may be considered for people who experience multiple episodes of UTI symptoms but urine cultures remain negative and not improved with antibiotics. Prostatitis (inflammation of the prostate) may also be considered in the differential diagnosis.\n\nHemorrhagic cystitis, characterized by blood in the urine, can occur secondary to a number of causes including: infections, radiation therapy, underlying cancer, medications and toxins. Medications that commonly cause this problem include the chemotherapeutic agent cyclophosphamide with rates of 2 to 40%. Eosinophilic cystitis is a rare condition where eosinophiles are present in the bladder wall. Signs and symptoms are similar to a bladder infection. Its cause is not entirely clear; however, it may be linked to food allergies, infections, and medications among others.\n\nA number of measures have not been confirmed to affect UTI frequency including: urinating immediately after intercourse, the type of underwear used, personal hygiene methods used after urinating or defecating, or whether a person typically bathes or showers. There is similarly a lack of evidence surrounding the effect of holding one's urine, tampon use, and douching. In those with frequent urinary tract infections who use spermicide or a diaphragm as a method of contraception, they are advised to use alternative methods. In those with benign prostatic hyperplasia urinating in a sitting position appears to improve bladder emptying which might decrease urinary tract infections in this group.\n\nUsing urinary catheters as little and as short of time as possible and appropriate care of the catheter when used prevents catheter-associated urinary tract infections.\nThey should be inserted using sterile technique in hospital however non-sterile technique may be appropriate in those who self catheterize. The urinary catheter set up should also be kept sealed. Evidence does not support a significant decrease in risk when silver-alloy catheters are used.\n\nFor those with recurrent infections, taking a short course of antibiotics when each infection occurs is associated with the lowest antibiotic use. A prolonged course of daily antibiotics is also effective. Medications frequently used include nitrofurantoin and trimethoprim/sulfamethoxazole (TMP/SMX). Methenamine is another agent used for this purpose as in the bladder where the acidity is low it produces formaldehyde to which resistance does not develop. Some recommend against prolonged use due to concerns of antibiotic resistance.\n\nIn cases where infections are related to intercourse, taking antibiotics afterwards may be useful. In post-menopausal women, topical vaginal estrogen has been found to reduce recurrence. As opposed to topical creams, the use of vaginal estrogen from pessaries has not been as useful as low dose antibiotics. Antibiotics following short term urinary catheterization decreases the subsequent risk of a bladder infection. A number of vaccines are in development as of 2018.\n\nThe evidence that preventive antibiotics decrease urinary tract infections in children is poor. However recurrent UTIs are a rare cause of further kidney problems if there are no underlying abnormalities of the kidneys, resulting in less than a third of a percent (0.33%) of chronic kidney disease in adults. Whether routine circumcisions prevents UTIs has not been well studied as of 2011.\n\nSome research suggests that cranberry (juice or capsules) may decrease the number of UTIs in those with frequent infections. A Cochrane review concluded that the benefit, if it exists, is small. Long-term tolerance is also an issue with gastrointestinal upset occurring in more than 30%. Cranberry juice is thus not currently recommended for this indication. As of 2015, probiotics require further study to determine if they are beneficial. The role of the urinary microbiome in maintaining urinary tract health is not well understood as of 2015.\n\nThe mainstay of treatment is antibiotics. Phenazopyridine is occasionally prescribed during the first few days in addition to antibiotics to help with the burning and urgency sometimes felt during a bladder infection. However, it is not routinely recommended due to safety concerns with its use, specifically an elevated risk of methemoglobinemia (higher than normal level of methemoglobin in the blood). Acetaminophen (paracetamol) may be used for fevers. There is no good evidence for the use of cranberry products for treating current infections.\n\nThose who have bacteria in the urine but no symptoms should not generally be treated with antibiotics. This includes those who are old, those with spinal cord injuries, and those who have urinary catheters. Pregnancy is an exception and it is recommended that women take 7 days of antibiotics. If not treated it causes up to 30% of mothers to develop pyelonephritis and increases risk of low birth weight and preterm birth. Some also support treatment of those with diabetes mellitus and treatment before urinary tract procedures which will likely cause bleeding.\n\nUncomplicated infections can be diagnosed and treated based on symptoms alone. Antibiotics taken by mouth such as trimethoprim/sulfamethoxazole (TMP/SMX), nitrofurantoin, or fosfomycin are typically first line. Cephalosporins, amoxicillin/clavulanic acid, or a fluoroquinolone may also be used. However, resistance to fluoroquinolones among the bacterial that cause urinary infections has been increasing. The FDA recommends against the use of fluoroquinolones when other options are available due to higher risks of serious side effects. These medications substantially shorten the time to recovery with all being equally effective. A three-day treatment with trimethoprim, TMP/SMX, or a fluoroquinolone is usually sufficient, whereas nitrofurantoin requires 5–7 days. Fosfomycin may be used as a single dose but has been associated with lower rates of efficacy.\n\nWith treatment, symptoms should improve within 36 hours. About 50% of people will recover without treatment within a few days or weeks. Fluoroquinolones are not recommended as a first treatment. The Infectious Diseases Society of America states this due to the concern of generating resistance to this class of medication. Amoxicillin-clavulanate appears less effective than other options. Despite this precaution, some resistance has developed to all of these medications related to their widespread use. Trimethoprim alone is deemed to be equivalent to TMP/SMX in some countries. For simple UTIs, children often respond to a three-day course of antibiotics. Women with recurrent simple UTIs may benefit from self-treatment upon occurrence of symptoms with medical follow-up only if the initial treatment fails.\n\nComplicated UTIs are more difficult to treat and usually requires more aggressive evaluation, treatment and follow-up. It may require identifying and addressing the underlying complication. Increasing antibiotic resistance is causing concern about the future of treating those with complicated and recurrent UTI.\n\nPyelonephritis is treated more aggressively than a simple bladder infection using either a longer course of oral antibiotics or intravenous antibiotics. Seven days of the oral fluoroquinolone ciprofloxacin is typically used in areas where the resistance rate is less than 10%. If the local resistance rates are greater than 10%, a dose of intravenous ceftriaxone is often prescribed. Trimethoprim/sulfamethoxazole or amoxicillin/clavulanate orally for 14 days is another reasonable option. In those who exhibit more severe symptoms, admission to a hospital for ongoing antibiotics may be needed. Complications such as urinary obstruction from a kidney stone may be considered if symptoms do not improve following two or three days of treatment.\n\nUrinary tract infections are the most frequent bacterial infection in women. They occur most frequently between the ages of 16 and 35 years, with 10% of women getting an infection yearly and more than 40–60% having an infection at some point in their lives. Recurrences are common, with nearly half of people getting a second infection within a year. Urinary tract infections occur four times more frequently in females than males. Pyelonephritis occurs between 20–30 times less frequently. They are the most common cause of hospital acquired infections accounting for approximately 40%. Rates of asymptomatic bacteria in the urine increase with age from two to seven percent in women of child bearing age to as high as 50% in elderly women in care homes. Rates of asymptomatic bacteria in the urine among men over 75 are between 7-10%. Asymptomatic bacteria in the urine occurs in 2% to 10% of pregnancies.\n\nUrinary tract infections may affect 10% of people during childhood. Among children, urinary tract infections are most common in uncircumcised males less than three months of age, followed by females less than one year. Estimates of frequency among children, however, vary widely. In a group of children with a fever, ranging in age between birth and two years, two to 20% were diagnosed with a UTI.\n\nIn the United States, urinary tract infections account for nearly seven million office visits, a million emergency department visits, and one hundred thousand hospitalizations every year. The cost of these infections is significant both in terms of lost time at work and costs of medical care. In the United States the direct cost of treatment is estimated at 1.6 billion USD yearly.\n\nUrinary tract infections have been described since ancient times with the first documented description in the Ebers Papyrus dated to c. 1550 BC. It was described by the Egyptians as \"sending forth heat from the bladder\". Effective treatment did not occur until the development and availability of antibiotics in the 1930s before which time herbs, bloodletting and rest were recommended.\n\nUrinary tract infections are more concerning in pregnancy due to the increased risk of kidney infections. During pregnancy, high progesterone levels elevate the risk of decreased muscle tone of the ureters and bladder, which leads to a greater likelihood of reflux, where urine flows back up the ureters and towards the kidneys. While pregnant women do not have an increased risk of asymptomatic bacteriuria, if bacteriuria is present they do have a 25–40% risk of a kidney infection. Thus if urine testing shows signs of an infection—even in the absence of symptoms—treatment is recommended. Cephalexin or nitrofurantoin are typically used because they are generally considered safe in pregnancy. A kidney infection during pregnancy may result in premature birth or pre-eclampsia (a state of high blood pressure and kidney dysfunction during pregnancy that can lead to seizures). Some women have UTIs that keep coming back in pregnancy and currently there is not enough research on how to best treat these infections.\n"}
{"id": "311354", "url": "https://en.wikipedia.org/wiki?curid=311354", "title": "Wernicke–Korsakoff syndrome", "text": "Wernicke–Korsakoff syndrome\n\nWernicke–Korsakoff syndrome (WKS) is the combined presence of Wernicke encephalopathy (WE) and alcoholic Korsakoff syndrome. Due to the close relationship between these two disorders, people with either are usually diagnosed with WKS as a single syndrome. \nThe cause of the disorder is thiamine (vitamin B) deficiency, which can cause a range of disorders including beriberi, Wernicke encephalopathy, and alcoholic Korsakoff syndrome. These disorders may manifest together or separately. WKS is usually secondary to alcohol abuse. It mainly causes vision changes, ataxia and impaired memory.\n\nWernicke encephalopathy and WKS are most commonly seen in people who are alcoholic, and only 20% of cases are identified before death. This failure in diagnosis of WE and thus treatment of the disease leads to death in approximately 20% of cases, while 75% are left with permanent brain damage associated with WKS. Of those affected, 25% require long-term institutionalization in order to receive effective care.\n\nThe syndrome is a combined manifestation of two namesake disorders, Wernicke encephalopathy and alcoholic Korsakoff syndrome. It involves an acute Wernicke-encephalopathy phase, followed by the development of a chronic alcoholic Korsakoff syndrome phase.\n\nWE is characterized by the presence of a triad of symptoms;\n\n\nThis triad of symptoms results from a deficiency in vitamin B which is an essential coenzyme. The aforementioned changes in mental state occur in approximately 82% of patients' symptoms of which range from confusion, apathy, inability to concentrate, and a decrease in awareness of the immediate situation they are in. If left untreated, WE can lead to coma or death. In about 29% of patients, ocular disturbances consist of nystagmus and paralysis of the lateral rectus muscles or other muscles in the eye. A smaller percentage of patients experience a decrease in reaction time of the pupils to light stimuli and swelling of the optic disc which may be accompanied by retinal hemorrhage. Finally, the symptoms involving stance and gait occur in about 23% of patients and result from dysfunction in the cerebellum and vestibular system. Other symptoms that have been present in cases of WE are stupor, low blood pressure (hypotension), elevated heart rate (tachycardia), as well as hypothermia, epileptic seizures and a progressive loss of hearing.\n\nAbout 19% of patients have none of the symptoms in the classic triad at first diagnosis of WE; however, usually one or more of the symptoms develops later as the disease progresses.\n\nKS is described as an acute onset of severe memory impairment without any dysfunction in intellectual abilities. The DSM IV lists the following criteria for the diagnosis of alcoholic Korsakoff syndrome:\n\nOne of:\n\nIn addition, the DSM-IV indicates that normal activities and function will be impaired by the memory deficits and that the experience of amnesia must occur outside of times where the individual is in a state of delirium, intoxification, or withdrawal. The criteria for diagnosis also maintain that there must be evidence that the amnesia is caused by the use of alcohol.\n\nDespite the assertion that alcoholic Korsakoff syndrome must be caused by the use of alcohol, there have been several cases where it has developed from other instances of thiamine deficiency resulting from gross malnutrition due to conditions such as; stomach cancer, anorexia nervosa, and gastrectomy.\n\nSeveral cases have been documented where Wernicke–Korsakoff syndrome has been seen on a large scale. In 1947, 52 cases of WKS were documented in a prisoner of war hospital in Singapore where the prisoners' diets included less than 1 mg of thiamine per day. Such cases provide an opportunity to gain an understanding of what effects this syndrome has on cognition. In this particular case, cognitive symptoms included insomnia, anxiety, difficulties in concentration, loss of memory for the immediate past, and gradual degeneration of mental state; consisting of confusion, confabulation, and hallucinations. In other cases of WKS, cognitive effects such as severely disrupted speech, giddiness, and heavy headedness have been documented. In addition to this, it has been noted that some patients displayed an inability to focus, and the inability of others to catch patients' attention.\n\nIn a study conducted in 2003 by Brand et al. on the cognitive effects of WKS, the researchers used a neuropsychological test battery which included tests of intelligence, speed of information processing, memory, executive function and cognitive estimation. They found that patients suffering from WKS showed impairments in all aspects of this test battery but most noticeably, on the cognitive estimation tasks. This task required subjects to estimate a physical quality such as size, weight, quantity or time (i.e. What is the average length of a shower?), of a particular item. Patients with WKS performed worse than normal control participants on all of the tasks in this category. The patients found estimations involving time to be the most difficult, whereas quantity was the easiest estimation to make. Additionally, the study included a category for classifying \"bizarre\" answers, which included any answer that was far outside of the normal range of expected responses. WKS patients did give answers that could fall into such a category and these included answers such as 15s or 1 hour for the estimated length of a shower, or 4 kg or 15 tonnes as the weight of a car.\n\nAs mentioned previously, the amnesic symptoms of WKS include both retrograde and anterograde amnesia. The retrograde deficit has been demonstrated through an inability of WKS patients to recall or recognize information for recent public events. The anterograde memory loss is demonstrated through deficits in tasks that involve encoding and then recalling lists of words and faces, as well as semantic learning tasks. WKS patients have also demonstrated difficulties in preservation as evidenced by a deficit in performance on the Wisconsin Card Sorting Test. The retrograde amnesia that accompanies WKS can extend as far back as twenty to thirty years, and there is generally a temporal gradient seen, where earlier memories are recalled better than more recent memories. It has been widely accepted that the critical structures that lead to the memory impairment in WKS are the mammillary bodies, and the thalamic regions. Despite the aforementioned memory deficits, non-declarative memory functions appear to be intact in WKS patients. This has been demonstrated through measures that assess perceptual priming.\n\nOther studies have shown deficits in recognition memory and stimulus-reward associative functions in patients with WKS. The deficit in stimulus-reward functions was demonstrated by Oscar-Berman and Pulaski who presented patients with reinforcements for certain stimuli but not others, and then required the patients to distinguish the rewarded stimuli from the non-rewarded stimuli. WKS patients displayed significant deficits in this task. The researchers were also successful in displaying a deficit in recognition memory by having patients make a yes/no decision as to whether a stimulus was familiar (previously seen) or novel (not previously seen). The patients in this study also showed a significant deficit in their ability to perform this task.\n\nPeople with WKS often show confabulation, spontaneous confabulation being seen more frequently than provoked confabulation. Spontaneous confabulations refer to incorrect memories that the patient holds to be true, and may act on, arising spontaneously without any provocation. Provoked confabulations can occur when a patient is cued to give a response, this may occur in test settings. The spontaneous confabulations viewed in WKS are thought to be produced by an impairment in source memory, where they are unable to remember the spatial and contextual information for an event, and thus may use irrelevant or old memory traces to fill in for the information that they cannot access. It has also been suggested that this behaviour may be due to executive dysfunction where they are unable to inhibit incorrect memories or because they are unable to shift their attention away from an incorrect response.\n\nWKS is usually found in chronic alcoholics. Wernicke–Korsakoff syndrome results from thiamine deficiency. It is generally agreed that Wernicke encephalopathy results from severe acute deficiency of thiamine (vitamin B), whilst Korsakoff's psychosis is a chronic neurologic sequela of Wernicke encephalopathy. The metabolically active form of thiamine is thiamine pyrophosphate, which plays a major role as a cofactor or coenzyme in glucose metabolism. The enzymes that are dependent on thiamine pyrophosphate are associated with the citric acid cycle (also known as the Krebs cycle), and catalyze the oxidation of pyruvate, α-ketoglutarate and branched chain amino acids. Thus, anything that encourages glucose metabolism will exacerbate an existing clinical or sub-clinical thiamine deficiency.\n\nAs stated above, Wernicke–Korsakoff syndrome in the United States is usually found in malnourished chronic alcoholics, though it is also found in patients who undergo prolonged intravenous (IV) therapy without vitamin B supplementation, gastric stapling, intensive care unit (ICU) stays or hunger strikes. In some regions, physicians have observed thiamine deficiency brought about by severe malnutrition, particularly in diets consisting mainly of polished rice, which is thiamine-deficient. The resulting nervous system ailment is called beriberi. In individuals with sub-clinical thiamine deficiency, a large dose of glucose (either as sweet food, etc. or glucose infusion) can precipitate the onset of overt encephalopathy.\n\nWernicke–Korsakoff syndrome in alcoholics particularly is associated with atrophy/infarction of specific regions of the brain, especially the mamillary bodies. Other regions include the anterior region of the thalamus (accounting for amnesic symptoms), the medial dorsal thalamus, the basal forebrain, the median and dorsal raphe nuclei, and the cerebellum.\n\nOne as-yet-unreplicated study has associated susceptibility to this syndrome with a hereditary deficiency of transketolase, an enzyme that requires thiamine as a coenzyme.\n\nThe fact that gastrointestinal surgery can lead to the development of WKS was demonstrated in a study that was completed on three patients who recently undergone a gastrectomy. These patients had developed WKS but were not alcoholics and had never suffered from dietary deprivation. WKS developed between 2 and 20 years after the surgery. There were small dietary changes that contributed to the development of WKS but overall the lack of absorption of thiamine from the gastrointestinal tract was the cause. Therefore, it must be ensured that patients who have undergone gastrectomy have a proper education on dietary habits, and carefully monitor their thiamine intake. Additionally, an early diagnosis of WKS, should it develop, is very important.\n\nStrong evidence suggests that ethanol interferes directly with thiamine uptake in the gastrointestinal tract. Ethanol also disrupts thiamine storage in the liver and the transformation of thiamine into its active form. The role of alcohol consumption in the development of WKS has been experimentally confirmed through studies in which rats were subjected to alcohol exposure and lower levels thiamine through a low-thiamine diet. In particular, studies have demonstrated that clinical signs of the neurological problems that result from thiamine deficiency develop faster in rats that have received alcohol and were also deficient in thiamine than rats who did not receive alcohol. In another study, it was found that rats that were chronically fed alcohol had significantly lower liver thiamine stores than control rats. This provides an explanation for why alcoholics with liver cirrhosis have a higher incidence of both thiamine deficiency and WKS.\n\nBrain atrophy associated with WKS occurs in the following regions of the brain: the mammillary bodies, the thalamus, the periaqueductal grey, the walls of the 3rd ventricle, the floor of the 4th ventricle, the cerebellum, and the frontal lobe. In addition to the damage seen in these areas there have been reports of damage to cortex, although it was noted that this may be due to the direct toxic effects of alcohol as opposed to thiamine deficiency that has been attributed as the underlying cause of Wernicke-Korsakoff Syndrome.\n\nThe amnesia that is associated with this syndrome is a result of the atrophy in the structures of the diencephalon (the thalamus, hypothalamus and mammillary bodies), and is similar to amnesia that is presented as a result of other cases of damage to the medial temporal lobe. It has been argued that the memory impairments can occur as a result of damage along any part of the mammillo-thalamic tract, which explains how WKS can develop in patients with damage exclusively to either the thalamus or the mammillary bodies.\n\nDiagnosis of Wernicke–Korsakoff syndrome is by clinical impression and can sometimes be confirmed by a formal neuropsychological assessment. Wernicke encephalopathy typically presents with ataxia and nystagmus, and Korsakoff's psychosis with anterograde and retrograde amnesia and confabulation upon relevant lines of questioning.\n\nFrequently, secondary to thiamine deficiency and subsequent cytotoxic edema in Wernicke encephalopathy, patients will have marked degeneration of the mamillary bodies. Thiamine (vitamin B) is an essential coenzyme in carbohydrate metabolism and is also a regulator of osmotic gradient. Its deficiency may cause swelling of the intracellular space and local disruption of the blood-brain barrier. Brain tissue is very sensitive to changes in electrolytes and pressure and edema can be cytotoxic. In Wernicke this occurs specifically in the mammillary bodies, medial thalami, tectal plate, and periaqueductal areas. Sufferers may also exhibit a dislike for sunlight and so may wish to stay indoors with the lights off. The mechanism of this degeneration is unknown, but it supports the current neurological theory that the mammillary bodies play a role in various \"memory circuits\" within the brain. An example of a memory circuit is the Papez circuit.\n\nThe onset of Wernicke encephalopathy is considered a medical emergency, and thus thiamine administration should be initiated immediately when the disease is suspected. Prompt administration of thiamine to patients with Wernicke encephalopathy can prevent the disorder from developing into Wernicke–Korsakoff syndrome, or reduce its severity. Treatment can also reduce the progression of the deficits caused by WKS, but will not completely reverse existing deficits. WKS will continue to be present, at least partially, in 80% of patients. Patients suffering from WE should be given a minimum dose of 500 mg of thiamine hydrochloride, delivered by infusion over a 30-minute period for two to three days. If no response is seen then treatment should be discontinued but for those patients that do respond, treatment should be continued with a 250 mg dose delivered intravenously or intramuscularly for three to five days unless the patient stops improving. Such prompt administration of thiamine may be a life-saving measure. Banana bags, a bag of intravenous fluids containing vitamins and minerals, is one means of treatment.\n\nAs described, alcoholic Korsakoff syndrome usually follows or accompanies Wernicke encephalopathy. If treated quickly, it may be possible to prevent the development of AKS with thiamine treatments. This treatment is not guaranteed to be effective and the thiamine needs to be administered adequately in both dose and duration. A study on Wernicke-Korsakoff syndrome showed that with consistent thiamine treatment there were noticeable improvements in mental status after only 2–3 weeks of therapy. Thus, there is hope that with treatment Wernicke encephalopathy will not necessarily progress to WKS.\n\nIn order to reduce the risk of developing WKS it is important to limit the intake of alcohol or drink in order to ensure that proper nutrition needs are met. A healthy diet is imperative for proper nutrition which, in combination with thiamine supplements, may reduce the chance of developing WKS. This prevention method may specifically help heavy drinkers who refuse to or are unable to quit.\n\nInternationally, the prevalence rates of WKS are relatively standard, being anywhere between zero and two percent. Despite this, specific sub-populations seem to have higher prevalence rates including people who are homeless, older individuals (especially those living alone or in isolation), and psychiatric inpatients. Additionally, studies show that prevalence is not connected to alcohol consumption per capita. For example, in France, a country that is well known for its consumption and production of wine, prevalence was only 0.4% in 1994, while Australia had a prevalence of 2.8%.\n\nCarl Wernicke discovered Wernicke encephalopathy in 1881. His first diagnosis noted symptoms including paralyzed eye movements, ataxia, and mental confusion. Also noticed were hemorrhages in the gray matter around the third and fourth ventricles and the cerebral aqueduct. Brain atrophy was only found upon post-mortem autopsy. Wernicke believed these hemorrhages were due to inflammation and thus the disease was named polioencephalitis haemorrhagica superior. Later, it was found that Wernicke encephalopathy and alcoholic Korsakoff syndrome are products of the same cause.\n\nSergei Korsakoff was a Russian physician after whom the disease \"Korsakoff's syndrome\" was named. In the late 1800s Korsakoff was studying long-term alcoholic patients and began to notice a decline in their memory function. At the 13th International Medical Congress in Moscow in 1897, Korsakoff presented a report called: \"\"On a special form of mental illness combined with degenerative polyneuritis\".\" After the presentation of this report the term \"Korsakoff's syndrome\" was coined.\n\nAlthough WE and AKS were discovered separately, these two syndromes are usually referred to under one name, Wernicke–Korsakoff syndrome, due to the fact that they are part of the same cause and because the onset of AKS usually follows WE if left untreated.\n\nThe British neurologist Oliver Sacks describes case histories of some of his patients with the syndrome in the book \"The Man Who Mistook His Wife for a Hat\" (1985).\n\n"}
{"id": "1140327", "url": "https://en.wikipedia.org/wiki?curid=1140327", "title": "World Toilet Organization", "text": "World Toilet Organization\n\nThe World Toilet Organization (WTO) is a global non-profit organization committed to improving toilet and sanitation conditions worldwide. It was founded in 2001 with 15 members and has now grown to 151 member organizations in 53 countries. All these members work towards eliminating the toilet taboo and delivering sustainable sanitation solutions worldwide. Furthermore, the WTO is also the organizer of the World Toilet Summit, the Urgent Run and initiated the United Nations World Toilet Day.\n\nWTO was founded by Jack Sim in Singapore on 19 November 2001. Since its inception, WTO has brought together governments, academia, civil society, multilateral agencies and the private sector to explore innovative and sustainable solutions to end the global sanitation crisis. WTO’s mission is to promote the global sanitation movement through collaborative action that inspires and drives demand for sanitation and provides innovative solutions to achieve sustainable sanitation for all.\n\nKey pillars of WTO’s work:\n\nJack Sim is the founder of the World Toilet Organization, the BoP HUB and is a global advocate for sanitation. Formerly in the construction industry, he founded WTO in 2001 after attaining financial independence at age of 40 and deciding to devote the rest of his life to social work. For ‘creating good will and bringing the subject into the open’ and ‘mobilizing national support in providing on-the-ground expertise’ Jack Sim received the Schwab Foundation award for Social Entrepreneur of the Year in 2001. In 2007, Jack became one of the key members to convene the Sustainable Sanitation Alliance composed of key players for sanitation. He is an Ashoka Global Fellow, and was named one of the Heroes of the Environment for 2008 by Time Magazine.\n\nThe World Toilet Summit & Expo (WTS) is a global platform that brings together sanitation stakeholders to share, learn and collaborate to address the global sanitation challenge. The annual summit is jointly organised by a host government or organization involving policy makers, toilet associations, non-profits and for-profit entities and private sector stakeholders in the sanitation sector. WTS aims to empower these key players to exchange knowledge, expertise and resources in scaling up impact and innovation in the sanitation marketplace.\n\nWTO was founded on 19 November 2001 and the inaugural WTS was held on the same day. WTO recognized the need for an international day to draw global attention to the sanitation crisis – and so established World Toilet Day. NGOs, the private sector, civil society organisations and the international community joined in to mark the global day.\nEach year, WTO commemorates WTD with the Urgent Run. The Urgent Run is a call for urgent action to end the sanitation crisis and aims to bring communities around the world together to raise awareness for the global sanitation challenge and engage people with sanitation issues in their local communities. For the past few years, in the lead-up to UN World Toilet Day, communities worldwide have come together to organize sanitation-themed Urgent Runs in varying formats and include fun runs, educational events, awareness walks, toilet cleaning programs, carnivals and even motorbike parades. They are organised by community groups, companies, universities, volunteers and NGOs to engage their local communities on their sanitation challenges.\n\nMany school toilets in rural China face problems of having old, unhygienic dry system toilets, absent hand washing facilities, and are often designed where the excreta disposal site is located right behind the toilet building – uncovered and exposed to the environment. Often, these schools lack a cohesive management system to upkeep and maintain the facilities, resulting in the toilet falling into a state of disrepair and neglect. This coupled with students who do not practice good hygiene habits, like handwashing with soap, can result in long-lasting health and environmental consequences.\n\nWith the aim of inspiring positive, long-lasting behavioural change among Chinese students, WTO`s Rainbow School Toilet Initiative was launched in 2015. In 2016 4 rural schools, with an estimated 1,300 students (average 300 students per school) benefited from the new toilet buildings equipped with a recyclable wastewater treatment plant.\n\nUntil recently, no proper sanitation solution existed for the almost 100,000 people living in floating communities on Cambodia’s Tonle Sap Lake. To address sanitation issues in these floating communities, Wetlands Work! (WW) developed the HandyPod, a product that contains the raw sewage and treats it by harnessing various biological processes. This project aims to eliminate open defecation by providing sanitation systems to floating schools and teaching students to use toilets; improve sanitation and hygiene; reduce school absences due to diarrhea; increase school attendance especially for girls, as well as driving demand for household toilets.\n\nWTO and WW raised funds for the project through various platforms and in 2016 a total 8 HandyPods have been installed befitting approximately 900 students and 650 indirect beneficiaries in their households. The project was developed with a holistic and sustainable approach of technology (Handypods) and behavioural change (hygiene awareness classes) to the communities.\n\nThe World Toilet College (WTC) started as a social enterprise in 2005 with the belief that there is a need for an independent world body to ensure best practices and standards in toilet design, cleanliness and sanitation technologies. While the lack of toilets is an endemic problem, poor management and hygienic maintenance are equally serious issues. A well-kept toilet will encourage proper usage and prevent deadly diseases.\n\nThe goal of WTC’s programmes is to ensure the dignity of sanitation workers and elevate the otherwise poor image (and consequent low pay) reserved for this employment category in many places around the world. WTC does this by training and providing toilet cleaners with professional skills in both cleaning and performance of small repairs, thereby boosting their self-confidence. This empowers them with the opportunity to master the profession while at the same time enhancing their productivity. Since 2005 WTC has trained more than 5,000 people across its various courses and conducted programmes and courses.\n\nWTO created SaniShop, a social enterprise that improves sanitation conditions globally by empowering local entrepreneurs. The organization started its market-based approach in Cambodia in 2009 in collaboration with the University of North Carolina, Lien Aid and iDE in Kampung Speu. Since then SaniShop has built more than 12,000 household toilets and trained more than 526 sales entrepreneurs in 7 provinces.\n\nThe SaniShop model is a self-sustaining market ecosystem. WTO teaches community members to educate and spread messaging regarding the importance of good hygiene and sanitation, whilst training masons to build new toilets and to market them to their community. WTO believes in addressing the issue of inadequate sanitation and to end open defecation via a multi-pronged approach that inspires behavioural change both in the individual, and community.\n\n\n"}
