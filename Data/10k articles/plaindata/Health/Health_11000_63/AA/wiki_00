{"id": "33234548", "url": "https://en.wikipedia.org/wiki?curid=33234548", "title": "2011 United States listeriosis outbreak", "text": "2011 United States listeriosis outbreak\n\nThe 2011 United States listeriosis outbreak was a widespread outbreak of \"Listeria monocytogenes\" food poisoning across 28 US states that resulted from contaminated cantaloupes linked to Jensen Farms of Holly, Colorado. As of the final report on August 27, 2012, there were 33 deaths and 147 total confirmed cases since the beginning of the first recorded case on July 31, 2011. It was the worst foodborne illness outbreak in the United States, measured by the number of deaths, since the Centers for Disease Control and Prevention began tracking outbreaks in the 1970s, or tied with the worst, an outbreak of listeria from cheese in 1985, depending on which CDC report is used.\n\nListeriosis is an infection caused by the bacterium \"Listeria monocytogenes\". The outbreak was determined to originate from Jensen Farms in Holly, Colorado after \"Listeria monocytogenes\" was found in cantaloupe samples at a Jensen Farms store in Denver, Colorado and at the farm's packaging plant. The batch of cantaloupes had been shipped out over a period from July 29 through September 10 to twenty-five states, including Arkansas, Arizona, California, Colorado, Idaho, Illinois, Kansas, Minnesota, Missouri, Montana, Nebraska, New Jersey, New Mexico, New York, North Dakota, Ohio, Oklahoma, Pennsylvania, South Dakota, Tennessee, Texas, Utah, Virginia, and Wyoming.\n\nThe outbreak was first reported by the Centers for Disease Control on September 12, where they stated that \"fifteen people in four states had been infected\". On September 21, a new report was released by the CDC, bringing the number of deaths to 13 and the number of confirmed cases to 72. The report also stated that further deaths were being investigated to determine if they had also been caused by Listeria infection. The CDC report also stated that, as Listeria \"only sickens the elderly, pregnant women and others with compromised immune systems\", the median age of all the people that had been infected was 78. On September 30, an update was released by the CDC, reporting that as of 11 am (EDT) Sep 29, 2011 the number of confirmed cases was 84, number of deaths was 15 and the number of states involved was 19. On October 4, the CDC updated their report to 100 infected individuals in 20 states and a total of 18 deaths from the outbreak. The outbreak was shown to have continued to spread to new states, with the CDC update on October 7 stating that the number of cases had risen to 109 in 23 states and that three more people had died to bring the death toll to 21. The CDC update on October 12 put the number of cases at 116 with 23 deaths. An update on October 18 increased the number of cases to 123 and the number of deaths to 25. The October 25 update raised the number of cases to 133, with three more people dying to raise the total to 28. A final update on August 27 confirmed 147 cases and 33 deaths. Fatalities occurred in Colorado (9), Indiana (1), Kansas (3), Louisiana (2), Maryland (1), Missouri (3), Montana (1) Nebraska (1), New Mexico (5), New York (2), Oklahoma (1), Texas (2), and Wyoming (2). Among persons who died, ages ranged from <1 to 96 years, with a median age of 78 years. In addition, one woman pregnant at the time of illness had a miscarriage.\n\nListeria infections can cause pregnant women to miscarry; the first miscarriage attributed to the 2011 outbreak was reported in early October, in a woman living in Iowa. Pregnant women often are advised to avoid foods, such as unpasteurized cheese and hot dogs, that are known to have the potential to carry Listeria, but fruits such as cantaloupe had not previously been identified as sources of concern.\n\nNo list of retailers selling the infected cantaloupes was released by either the government or Jensen Farms. Although the last shipment was September 10 and the fruit had a two-week shelf life, as of September 29, the number of illnesses and deaths were expected to continue rising, because the incubation period could exceed one month.\n\nRecalls by retailers which had sold the Jensen Farms cantaloupes included Kroger (September 15), Safeway (September 15), Aldi (September 16), and US Foods (September 16).\n\nAn investigation by the Food and Drug Administration (FDA) found that the contaminated cantaloupe harvest contained four separate \"Listeria monocytogenes\" strains, which the governmental agency found to be \"unusual\", but was still trying to determine the reason. On October 20, it was reported that the FDA officials had found listeria on dirty, corroded equipment used by Jensen Farms, which had been bought used and was previously utilized for potato farming. It was stated by the government that the \"equipment's past use may have played a role in the contamination\". Water contaminated with listeria was also found on the floor of the packing plant and it was determined that the workers moving around the plant had spread it, as the contaminated water was also found on the cantaloupe conveyor belt. It was noted by officials that Jensen Farms had \"passed a food safety audit by an outside contractor\" six days before the outbreak.\n\nThe method of how the listeria bacteria first came to be in the plant remains unknown, as the soil on the farm was determined to be clear of the bacteria. It is suspected, however, that a \"dump truck used to take culled melons to a cattle farm...could have brought bacteria to the facility\". Furthermore, Bacteria growth may have been caused by condensation stemming from the lack of a pre-cooling step to remove field heat from the cantaloupe before cold storage.\n\nOn October 21, the House Energy and Commerce Committee, a committee panel of the United States House of Representatives, began its own investigation into the outbreak. The Committee \"requested a staff briefing from Jensen Farms\" and all of the documents they had on the incident. They also requested information from the FDA, CDC, and other governmental groups.\n\nIn response to the initial reports by the CDC on the contaminated cantaloupe, Jensen Farms issued a voluntary recall on September 15 of the entire harvest crop of 300,000 cantaloupe that it had distributed to its chain stores. The FDA made the public announcement for the recall after Listeria infection was confirmed by Jensen Farms at its main Colorado branch. Jensen Farms was also forced to temporarily shut down its processing plant while the recall is ongoing. Government officials have been investigating the company's main facility in Colorado to determine if there was \"animal or water contamination\", but there have been no results from the investigation thus far. Holly, Colorado residents were described as being left \"reeling and in fear\" because of the disaster for its local producer.\n\nThe FDA has stated in response to the extensive bacterial outbreak that it is \"yet another reason to fully implement the Food Safety Modernization Act.\" Sherri McGarry, a senior adviser for the FDA, stated that, \"We're going to take these lessons learned, share that with our partners and industries, CDC and the states, and what we want to do is we want to really prevent this from happening in the future.\"\n\nAlso, in response to an auditor passing Jensen Farms food safety methods and failing to notice the listeria bacteria in the plant, the deputy commissioner of foods, Michael R. Taylor, had stated that he intended to \"establish standards for how auditors should be trained and how audits should be conducted.\"\n\nOn September 15, a lawsuit was filed against Jensen Farms by the first victim of the contaminated cantaloupe crop, who had fallen ill and been kept in the hospital for several weeks. He and his wife were involved in the legal proceedings. In addition to Jensen Farms, the couple also sued a Walmart branch in Colorado Springs, Colorado, where they had bought the cantaloupe, for selling unsafe food.\n\n"}
{"id": "49599791", "url": "https://en.wikipedia.org/wiki?curid=49599791", "title": "Action Damien", "text": "Action Damien\n\nAction Damien (in French) or Damiaanactie (in Dutch) is a national Belgian NGO, founded in 1964 under the name Les amis du père Damien (\"Friends of Father Damien\"). At its first founding, people and groups in Belgium were organizing the \"World Leprosy Day\", created by Raoul Follereau. The organization would later be called Fondation Damien, before taking its current names in January 2008. Pluralist and non-denominational, Action Damien / Damiaanactie is now active in 13 countries in the world. Thanks to its local workers ( for about ten expatriates), the organization detects and gives every year a treatment to 250 000 people sick with leprosy, tuberculosis, and leishmaniosis.\n\nEvery year in Belgium, with the help of hundreds or even thousands of volunteers, Action Damien / Damiaanactie organizes an awareness and fund-raising campaign during the last weekend of January. The donations, received throughout the year, cover more than the half of its expenses. Action Damien / Damiaanactie works within the framework of International Federation of Anti-Leprosy Associations (ILEP) (Action Damien contributed to its foundation). Action Damien / Damiaanactie always works at the request of local authorities.\n\nAction Damien draws its inspiration from three great people. First, Raoul Follereau (the \"Advocate of the Lepers\") was a journalist, a philosopher, a lawyer and a writer who spent most of his life speaking out on behalf of all those with leprosy. He initiated World Leprosy Day. Second, Frans Hemerijcks (the \"Doctor of the Lepers\") was a Belgian doctor, specialized in leprosy, who invented the concept of a \"Clinic under the trees\" (rather than isolate those with leprosy from their loved ones, it was better to treat them within their communities). Third, Father Damien (the \"Apostle of the Lepers\"), is without doubt the first person to have seen the human being alongside the sick one, during his life at Molokai.\n"}
{"id": "39641264", "url": "https://en.wikipedia.org/wiki?curid=39641264", "title": "Adverse health effects from lunar dust exposure", "text": "Adverse health effects from lunar dust exposure\n\nThe respirable fraction of lunar dusts may be toxic to humans. NASA has therefore determined that an exposure standard is necessary to limit the amount of respirable airborne lunar dusts to which astronauts will be exposed. The nominal toxicity that is expected from ordinary mineral dust may be increased for lunar dusts due to the large and chemically reactive surfaces of the dust grains. Human exposures to mineral dusts during industrial operations and from volcanic eruptions give researchers some sense of the relative toxicity of lunar dust, although the Earth-based analogs have serious limitations. Animal and cellular studies provide further evidence that mineral dusts can be somewhat toxic. Earth-based research of mineral dust has shown that freshly fractured surfaces are chemically reactive and can elicit an increased toxic response. Since lunar dust is formed in space vacuum from highly energetic processes, the grain surfaces can be expected to be indefinitely reactive on the lunar surface. NASA predicts that this chemical reactivity will change once the dust is brought into a habitable environment.\n\nDust from lunar soil that was carried into spacecraft during the Apollo missions proved to be a nuisance. The lack of gravity, or the existence of gravity at a small fraction of the gravitational force of the Earth, increases the time during which dust remains airborne, thereby increasing the probability that these dust particles will be inhaled. Lunar dust particles that are generated by impaction in a deep vacuum have complex shapes and highly reactive surfaces that are coated with a thin layer of vapor-deposited mineral phase. Airborne mineral dust in a variety of forms has been shown to present a serious health hazard to ground-based workers. The health hazards that are associated with volcanic ash, which is a commonly used analog of lunar dust, have not been reported to be especially serious; however, this type of ash quickly loses its reactive surfaces and is often aggregated into particles that are not readily respirable into the deep lung. Crew members who will be at a lunar outpost can be directly exposed to lunar dust in several ways. After crew members perform spacewalks or EVAs, they will introduce into the habitat a large quantity of dust that will have collected on spacesuits and boots. Cleaning of the suits between EVAs and changing of the Environmental Control Life Support System (ECLSS) filters are other operations that could result in direct exposure to lunar dusts. In addition, if the final spacesuit design is based on the current spacesuit design, EVAs may cause dermal injuries, and the introduction of lunar dusts into the suits' interior, which may enhance skin abrasions. When the crew leaves the lunar surface and returns to microgravity, the dust that is introduced into the crew return vehicle will \"float,\" thus increasing the opportunity for ocular and respiratory injury.\n\nIn 2004, President George W. Bush unveiled a plan directing NASA to return humans to the moon by the year 2015, and to use the lunar outpost as a stepping-stone for future human trips to Mars and beyond. To meet this objective, NASA will build an outpost on the lunar surface near the south pole for long-duration human habitation and research. Because of the various activities that will require the astronauts to go in and out of this habitat on numerous spacewalks (EVAs), the living quarters at the lunar outpost are expected to be contaminated by lunar dust.\n\nThe president's \"Vision for Space Exploration\" and charge to return to the moon have resulted in questions about health hazards from exposure to lunar dust. Lunar dust resides in near-vacuum conditions, so the grain surfaces are covered in \"unsatisfied\" chemical bonds, thus making them very reactive. When the reactive dust is inhaled, it can be expected to react with lung surfactant and pulmonary cells. The fine, respirable lunar dust could thus be toxic if the astronauts are exposed to it during mission operations at a lunar base. Although a few early attempts were made to understand the toxicity of the lunar dust that was obtained by the Apollo astronauts or the Luna probes, no scientifically defensible toxicological studies have been performed on authentic lunar dust.\n\nAwareness of the toxicity of terrestrial dusts has increased greatly since the original Apollo flights, which occurred circa 1970, in which the crew members were exposed to lunar dust for a relatively brief time. The first National Ambient Air Quality Standard (NAAQS) was issued by the Environmental Protection Agency (EPA) in 1971 and was indexed to total suspended particles (TSP) on a mass per unit volume basis. In 1987, this NAAQS was refined to include only particles that were of less than 10 μm in aerodynamic diameter (PM10) because this was the size that was most likely to reach the bronchial tree and deeper into the lung. Finally, in 1997, the EPA Administrator issued standards for particles that were less than 2.5 μm in aerodynamic diameter (PM2.5) based primarily on epidemiological associations of increased mortality, exacerbation of asthma, and increased hospital admissions for cardiopulmonary symptoms. None of these standards specified the composition of the particles. In fact, the last standard was a bit contentious because mechanisms of toxic action were not understood.\n\nIn a review article, Schlesinger et al. list the properties of particulate matter that might elicit adverse effects. The properties that seem pertinent to lunar dust include: size distribution, mass concentration, particle surface area, number concentration, acidity, particle surface chemistry, particle reactivity, metal content, water solubility, and geometric form. In attempting to consider each of these properties, one property emerges as the most difficult to study; particle surface chemistry may be difficult to understand because the environment on the lunar surface is unlike any on Earth, and is likely to alter the surface of dust grains in a way that will render them highly reactive. Recreating the processes that could affect grain surface reactivity on the moon is difficult in an Earth-bound laboratory. Freshly fractured quartz is distinctly more toxic to the rat respiratory system than aged quartz. Quartz and lunar dust may have similar toxic properties, but breaking of surface bonds on mineral substrates has been shown to increase the toxicity of the well-studied mineral quartz.\n\nThe site at which various sizes of particles are deposited is critical to an understanding of any aspect of their toxic action. The fractional regional deposition of particles shows that between 10 and 1 μm, the portion of particles that is deposited in the upper airways falls off from 80% to 20%, whereas the pulmonary deposition increases from near zero to about 20%. Pulmonary deposition, after falling off near 1 μm, peaks again near 40% for particles of 0.03 μm, whereas upper airway deposition remains low until a new peak deposition is found at less than 0.01 μm. The portion and pattern of deposition can be modified under conditions of reduced gravity; however, human data during flights of the gravity research aircraft show that particles in the 0.5 to 1 μm range are deposited less in the respiratory system at lunar gravity than at Earth gravity. This finding is consistent with the reduced sedimentation of the particles when the gravity is less. However, a larger portion of the particles is deposited peripherally in reduced gravity.\n\nThe first encounter in which a particle deposits in the distal airways occurs with the broncho-alveolar lining fluid (BALF). The thickness of this fluid in the lung varies as the alveolar sacs expand and contract, but lies in the range of 0.1 to 0.9 μm. In the case of biological particles such as bacteria, this fluid opsonizes the particles to facilitate ingestion by macrophages. A similar process has been demonstrated for nonbiological carbonaceous particles. This process removes some components of the BALF that participate in opsonization, and it is postulated that this might enhance the toxicity of particles with a surface chemistry that is capable of selectively removing opsonizing components. The agglomeration of the grains is also affected by the interactions between the BALF and the grains. Preliminary data on authentic lunar dust has shown that in aqueous suspension, lunar particles agglomerate rapidly. Artificial surfactant has been found to greatly reduce this particulate agglomeration.\n\nParticles that are deposited in the pulmonary region are eliminated according to their surface area and chemical composition. If a particle is relatively soluble, its dissolution products end up in the bloodstream. Relatively insoluble particles are ingested by macrophages and removed by mucociliary clearance or the lymphatic system, or they persist in the interstitial areas of the lung. Ultrafine particles (<0.1 μm) that deposit in the upper airways have been shown, under some conditions, to translocate to the brain, whereas similar particles reaching the pulmonary regions can translocate to adjacent organs such as the liver.\n\nThe effects of particles on the respiratory system include de novo causation of clinical disease as well as exacerbation of existing disease. If particulate inhalation is to directly cause disease, the exposure levels typically must be at levels that are encountered in industrial settings. For example, silicosis is a well-known disease of persons working for years in conditions in which dust containing quartz is inhaled. Epidemiological studies show that ambient dust levels such as those that are encountered in some cities can exacerbate respiratory conditions such as asthma and chronic obstructive pulmonary disease. At certain times, sand dust that originates in Asia or Arizona, for example, has been associated with exacerbation of allergenic respiratory inflammation.\n\nOf particular concern in addition to the respiratory system is the ability of small particles to affect the cardiovascular system. Epidemiological studies suggest that exposure to ambient particulate matter increases the incidence of angina, arrhythmia, and myocardial infarctions. The increased acute mortality that is associated with particle \"events\" is attributed to cardiovascular disease. Clinical studies involving concentrated ambient air particulate have shown increased blood fibrinogen and reduced heart-rate variability; exposure to ultrafine particles causes \"blunted\" repolarization response following exercise. The role of C-reactive protein in mediating the effect of ambient particle exposures on the causation of CAD has been reviewed. Batalha has drawn attention to the ability of particles to elicit vasoconstriction of small pulmonary arteries. Although the mechanistic details have not been fully elucidated, the evidence favors a strong link between exposure to particulates and to both acute and chronic heart disease. There is some evidence from the Apollo missions that, in susceptible individuals, lunar dust exposure may lead to cardiovascular effects that are similar to those produced through exposure to air pollution.\n\nThe fact that no accepted health standards or policies exist concerning exposure limits to lunar dust is a critical challenge to the design of vehicle systems in the CxP. The multi-center Lunar Airborne Dust Toxicology Assessment Group (LADTAG) was formed and responded to a request from the Office of the Chief Health and Medical Officer to \"… develop recommendations for defining risk criteria for human lunar dust exposure and a plan for the subsequent development of a lunar dust permissible exposure limit.\" The LADTAG is composed of technical experts in lunar geology, inhalation toxicology, biomedicine, cellular chemistry, and biology from within the agency as well as of leading U.S. experts in these fields. Based on the opinions that were expressed by the LADTAG experts, NASA scientists will develop and execute a plan to build a database on which a defensible exposure standard can be set.\n\nLADTAG experts recommend that the toxicity of lunar dust on the lungs (pulmonary toxicity), eyes (ocular toxicity), and skin (dermal toxicity) be investigated, and that this investigation is to be conducted by the Lunar Dust Toxicity Research Project (LDTRP) using various assays including in vivo and in vitro methods. In an initial LADTAG workshop that was held in 2005, experts noted that they were unable to reconcile individual expert opinions to set an inhalation standard. The array of opinions from these experts spanned a 300-fold range (i.e., 0.01 to 3 mg/m3). The members of the LADTAG concluded that research is necessary to narrow this wide uncertainty range, the lower end of which cannot be met by known methods of environmental control, and that there is an urgency to determine the standard so that environmental systems for the lunar vehicle can be appropriately designed. Therefore, in keeping with the LADTAG experts' recommendations, members of the LDTRP have reviewed first-hand accounts of Apollo astronauts who were exposed to lunar dust during their missions as well as of terrestrial-based human exposures to dust generated in the mining industry and to volcanic ash. In accordance with the LADTAG recommendations to increase our evidence base, the LDTRP is conducting studies of Apollo spacesuits, filters, vacuum bags, and rock-collection boxes. These studies will enable us to focus our understanding of the grain-size distribution that is present in the lunar surface samples and in the habitat, but the dust surfaces are expected to be fully passivated.\n\nGround-based evidence includes data that are derived from people who are exposed occupationally to mineral dusts in industrial settings, from people who live in close proximity to active volcanoes and have been exposed to volcanic ash, and from animals and cells that are in controlled experimental studies. Mechanistic insights also guide our thinking concerning the potential toxicity of lunar dusts.\n\nWorkers in the mining industry are often exposed to dust from freshly fractured mineral deposits. When these workers use inadequate, or lack, respiratory protection completely, the consequences are devastating. A prime example of this is the Hawks Nest mining activity in West Virginia beginning in 1927. During the boring of a tunnel, deposits of silica were identified and mined; however, the workers did not use respiratory protection during the operations. Estimates of the proportion of workers who died, often within a few years, are typically about 30% of the 2,000 exposed workers. This rapidly lethal form of silicosis has been called \"acute silicosis,\" which is characterized by alveolar proteinosis and interstitial inflammation. The respiratory effects are not exactly like those one would expect from simple silicosis, a disease that usually requires decades to develop after prolonged exposure to lower concentrations of silica dust. The latter disease is characterized by silicotic nodules that are clearly distinct from surrounding tissue and often surrounded by an inflammatory response.\n\nVolcanic ash originates from processes resulting in explosive eruptions into the atmosphere or pyroclastic flows oozing from the surface and discharging ash as they cool, or some combination thereof. Under any plausible condition, the ash will have had hours to days to react with the oxygen and water vapor of the atmosphere to passivate all surfaces before being inhaled by humans. The mineral composition of ash is determined by the composition of the magma. The particle size, mineral composition, and form of the minerals vary considerably from volcano to volcano as well as from one eruption to another eruption of the same volcano.\n\nShortly after Mount St. Helens erupted in 1980, a number of experts began to investigate the effects of volcanic ash on those who had been exposed to the dust. The crystalline silica content of this dust ranged from 3% to 7%. The primary acute effects were reflected in increased emergency room visits for asthma, bronchitis, and eye discomfort. The ash was noted to exacerbate chronic respiratory conditions. The increase in hospital admissions lasted approximately 3 weeks, and immune parameters were affected even 1 year later. The British West Indian Montserrat volcano began erupting in 1995, causing an ash fall from pyroclastic flows that contained 10% to 24% crystalline silica. Recorded incidences of childhood wheezing increased as a result of relatively intense exposures to the ash. To our knowledge, sustained long-term health effects have not been reported in association with exposures to volcanic ash, although there is speculation that the high cristobalite content of the Montserrat ash could lead to silicosis many years later.\n\nAnimal studies that focused on the biological effects of chronic inhalation exposure to Mount St. Helens volcanic ash or quartz, under controlled laboratory conditions, indicate significant dose-response to both materials. The quartz that came from the volcano was found to be markedly toxic and fibrogenic; by contrast, the volcanic ash was much less toxic. Similar results were noted in other animal studies, suggesting that quartz is a much more potent pulmonary toxicant than volcanic ash. However, the presence of volcanic ash in the inhaled air did increase the \"histamine sensitivity\" of the epithelial irritant receptors as well as inhibit the ability of alveolar macrophages to protect against infection.\n\nThe toxicity of volcanic ashes has been evaluated in rats that were dosed once by intratracheal instillation. Ashes that were obtained from the San Francisco volcano field in Arizona (lunar dust simulant) and from a Hawaiian volcano (martian dust simulant) were compared to the toxicity of titanium dioxide and quartz. Lungs of mice that have been harvested 90 days after receiving a dose of 1 mg of lunar simulant showed chronic inflammation, septal thickening, and some fibrosis. No changes were seen at the low dose of 0.1 mg/mouse. The martian dust simulant elicited a response that was similar to that of the lunar simulant, except that there was an inflammatory and fibrotic response even at a dose of 0.1 mg/mouse. The response of the mouse lungs to 0.1-mg quartz was comparable to the response to the martian dust simulant. In another study, the effect of these same simulants was assessed on human alveolar macrophages.<ref name=\"https://ntrs.nasa.gov/archive/nasa/casi.ntrs.nasa.gov/20070031950_2007031183.pdf\"></ref> The lunar dust simulant was comparable in cell viability reduction and apoptosis induction to the TiO2 (titanium dioxide) negative control. Both were less toxic than the quartz positive control. Both simulants showed a dose-dependent increase in cytotoxicity.\n\nInhalation of freshly ground quartz, when compared to inhalation of aged quartz, results in a significant increase in animal lung injury. Freshly ground quartz has increased reactive silicon-based oxygen radicals, and animals that are exposed to freshly ground quartz have been found to have decreased concentrations of antioxidant enzymes. Activated quartz particles decay with age in ambient air. Quartz dusts containing surface iron as an impurity have been shown to deplete cellular glutathione, contributing to the oxidative damage that is caused by particle and cell-derived ROS. Castranova et al. suggest that freshly ground quartz dust that is contaminated with trace levels of iron may be more pathogenic than quartz dust alone.\n\nCrystalline silica exposure studies indicate that the generation of oxidants and nitric oxide, which play an important role in the initiation of silicosis, has been shown to cause pulmonary inflammation in rats. Other studies indicate that the mode of action of crystalline silica cytotoxicity and pathogenicity lies in the ability of the mineral to induce lipid peroxidation. Respiratory exposure to freshly ground silica causes greater generation of ROS from macrophages than exposure to aged silica, which is one piece of evidence that proves that freshly fractured silica is more toxic than aged silica.\n\nFurther evidence linking increased toxicity to surface activation must await data that show that lunar dust that is activated by methods other than grinding adversely affects cells. NASA has been able to demonstrate that dust that is activated by processes that are analogous to those that are understood to be present at the lunar surface (i.e., ultraviolet [UV] irradiation in a vacuum) are able to produce more ROS in aqueous solution than dust that is not activated by these processes (Wallace, unpublished data). In addition, mineral coupons that are activated by proton and alpha-particle bombardment that is analogous to the solar wind show increased ROS (Kuhlman, unpublished data). NASA was expected to assess the impact of these activation techniques on cellular systems by early 2009.\n\nLunar geologists state (Category I22 evidence) that iron is present in lunar dust, especially in the fraction of its smallest particles (nano-Fe), and that it can be postulated that a reaction involving iron could be important for activated lunar dust when it comes in contact with the mucous lining of the respiratory system. A good model of the issues and problems that are associated with testing surface-activated dust can be found in the studies of freshly fractured silica, which is highly toxic to the respiratory system via oxidative damage, and perhaps also in the testing of volcanic ash. The problem of the enhancement of toxicity in quartz by freshly fractured surfaces has been extensively investigated in animal and cellular systems. Fracturing silica cleaves the Si-O bonds, leaving Si and SiO radicals, which, in turn, produce OH radicals in an aqueous environment. Aged crystalline silica still produces radicals, but at a much lower level, perhaps by the Fenton reaction that occurs between iron and H2O2 that is generated by macrophage phagocytosis of the particles.\n\nSince surface activation, which is produced primarily by grinding, is known to increase the toxicity of various mineral dusts, it is critical to ask how quickly surface activation disappears once the dust encounters an oxygen- and water-vapor-rich environment. Vallyathan et al. demonstrated a bimodal decay by measuring the rate of disappearance of hydroxyl radical formation in an aqueous medium from silicon-based radicals on the surface of ground silica, when that ground silica was kept in air until the time of assay. The half-life of the fast decay was approximately 30 hours, whereas even after 4 weeks approximately 20% of the original activity that was induced by grinding was present on the surface of the quartz. This is similar to the ability of the 24-hour half-life in air of freshly fractured quartz to produce OH radicals. Although quartz is not lunar dust and grinding is merely a surrogate for activation of dust at the lunar surface, the longevity of the surface reactivity requires careful attention to better understand how surface-activated lunar dust becomes passivated in a habitable environment.\n\nSamples of lunar dust that have been returned to Earth have enabled NASA to learn the mineralogical properties of the dust at several lunar landing sites. First and foremost, one must keep in mind that the properties of the lunar dust may vary considerably depending on location; hence, lunar dusts may show a range of toxicity. Initially, NASA assessed the expected nature of dust at the proposed South Pole landing site on the rim of Shackleton crater.\n\nAll space flight evidence pertaining to the effect of lunar dust on astronauts is anecdotal (Category III). The post-flight debriefing reports of the Apollo astronauts serve as a base of evidence. Although the astronauts attempted to remove the lunar dust before they reentered the command module (CM) by brushing the spacesuits or vacuuming, a significant amount of dust was returned to the spacecraft, which caused various problems. For instance, astronaut Harrison Schmitt complained of \"hay fever\" effects caused by the dust, and the abrasive nature of the material was found to cause problems with various joints and seals of the spacecraft and spacesuits. In these reports, the Apollo crews provided several accounts of problems with lunar dust exposure as follows:\n\n\nNASA crew surgeon (Category IV), Dr. Bill Carpentier, observed his own, as well as Apollo mission crew members', post-flight allergic-type responses. Dr. Carpentier recalls an increase in eosinophil and basophil blood cell counts after the crew members were exposed to lunar dust, which may have indicated an allergic response.\n\nAlthough no substantive evidence exists that astronaut performance was impaired by lunar dust, one can imagine that if a crew member were \"almost blinded\" and had to \"remain in the suit loop as much as possible because of the dust and debris floating around,\" the dust did have some impact on performance.\n\nDust from the lunar soil that was carried into the spacecraft during the Apollo missions proved to be a significant, intermittent problem. With the return to the moon and planned long-duration stays on the lunar surface, the dust toxicity and contamination problems are potentially much more serious than those that were experienced during the Apollo missions. Physical evidence also suggests that lunar dust could be a health hazard at a lunar outpost. Gravity at one-sixth that of the gravitational force of the Earth increases the time in which dust remains airborne, thereby increasing the probability that these dust particles will be inhaled.\n\nSome examples of lunar dust grains are provided in figure 13-1.\n\nMultiple, probable scenarios exist in which crew members could be exposed to lunar dust during both lunar sortie and lunar outpost missions. Further, there are opportunities for crew members to be directly exposed to lunar dust after they perform EVAs. Post EVA, lunar crew members will introduce into the habitat and lunar lander the dust that has collected on their spacesuits and boots. Cleaning of the suits between EVAs may also directly expose crew members to lunar dust. For crew members, changing of Environmental Control and Life Support System filters is yet another potential route of direct exposure to lunar dusts. These episodic periods of increased lunar dust exposure must be taken into account when long-term exposure limits are calculated. As missions become longer, the greater dose and/or duration of lunar dust exposure will increase the potential human health risk. When a crew returns to microgravity, if lunar dust is introduced into the crew return vehicle, there will be an increased opportunity for ocular exposure if particles of dust are floating throughout the cabin. EVAs cause dermal injuries when suits that are based on the current design are used, and the introduction of lunar dusts may enhance injuries that will be sustained from contact with the EVA suit. In addition, NASA is considering the use of a rover design that will allow shirtsleeve operation of the vehicle. Thus, the rover, which must be kept in an interior space to be entered without a spacesuit, may also bring dust into the habitat.\n\nThe evidence base shows that prolonged exposure to respirable lunar dust could be detrimental to human health. Lunar dust is known to have a large surface area (i.e., it is porous), and a substantial portion is in the respirable range. The surface of the lunar dust particles is known to be chemically activated by processes ongoing at the surface of the moon. Predictions are that this reactivity will disappear on entry into the habitable volume; however, it is not known how quickly the passivation of chemical reactivity will occur, nor is it known how toxic the deactivated dust may prove to be. Although many Apollo astronauts seemed to tolerate lunar dust, their exposure times were brief and time (duration) exposure factors need to be determined. Other Apollo crew members and ground support personnel noted that the lunar dust was a sensory irritant. Finally, the size characteristics of the dust that actually was present in the atmosphere of the lunar lander have never been determined. Obtaining data will help NASA understand the size distribution of the particles that are expected to be found in future lunar habitats. It is important to design experiments that will close or, at a minimum, narrow knowledge gaps so that a scientifically defensible exposure standard can be set by NASA for protection of crew health.\n\n\n"}
{"id": "32627229", "url": "https://en.wikipedia.org/wiki?curid=32627229", "title": "Aluminium toxicity in people on dialysis", "text": "Aluminium toxicity in people on dialysis\n\nAluminium toxicity in people on dialysis (or aluminium toxicity) is a problem for people on haemodialysis. The dialysis process does not efficiently remove excess aluminium from the body, so it may build up over time. Aluminium is a potentially toxic metal, and aluminium poisoning may lead to mainly three disorders: aluminium-induced bone disease, microcytic anemia and neurological dysfunction (encephalopathy). Such conditions are more prominently observed in people with chronic renal failure and especially in people on haemodialysis.\n\nAbout 5–10 mg of aluminium enters our body daily through different sources like water, food, occupational exposure to aluminium in industries etc. In people with normal renal function, serum aluminium is normally lower than 6 microgram/L. Baseline levels of serum aluminium should be <20 microgram/L. According to AAMI standards aluminum levels in the dialysis fluid should be less than 0.01milligram/L. \n"}
{"id": "11854886", "url": "https://en.wikipedia.org/wiki?curid=11854886", "title": "American School Health Association", "text": "American School Health Association\n\nThe American School Health Association (ASHA) is a professional association. It claims a membership of 2,000 members in 56 nations. More than half practice in K-12 schools or administer health education or health services programs in school districts or state departments of education. \n\nFounded in 1927 as the American Association of School Physicians by 325 physicians attending the annual meeting of the American Public Health Association in Cincinnati, Ohio, its mission was to protect and promote the health of children and youth by supporting coordinated school health programs as a foundation for school success. In 1936, the organization opened its membership to all professionals interested in promoting school health and adopted its current name.\n\nThe current president is Jeffrey K. Clark.\n\n\n"}
{"id": "52628750", "url": "https://en.wikipedia.org/wiki?curid=52628750", "title": "Artesunate/mefloquine", "text": "Artesunate/mefloquine\n\nArtesunate/mefloquine is a medication used to treat malaria. It is a fixed dose combination of artesunate and mefloquine. Specifically it is recommended to treat uncomplicated falciparum malaria. It is taken by mouth.\nSide effects are similar to the medications being used separately. Use is recommended as it decreases the possibility of either medications being used alone. Dose forms appropriate for children are also available.\nArtesunate/mefloquine came into commercial use in 2008. It is on the World Health Organization's List of Essential Medicines, the most effective and safe medicines needed in a health system. It is approved for medical use in Brazil, India, and Malaysia. In 2012 a course of treatment cost 2.50 USD. It is not commercially available in the United States.\n\nArtesunate/mefloquine is a recommended treatment in Southeast Asia while in Africa artesunate/amodiaquine, artemether/lumefantrine, artesunate/sulfadoxine/pyrimethamine are often preferred.\n\n"}
{"id": "707323", "url": "https://en.wikipedia.org/wiki?curid=707323", "title": "Attachment disorder", "text": "Attachment disorder\n\nAttachment disorder is a broad term intended to describe disorders of mood, behavior, and social relationships arising from a failure to form normal attachments to primary care giving figures in early childhood. Such a failure would result from unusual early experiences of neglect, abuse, abrupt separation from caregivers between 6 months and three years of age, frequent change or excessive numbers of caregivers, or lack of caregiver responsiveness to child communicative efforts resulting in a lack of basic trust. A person's \"attachment style\" is permanently established before the age of three. A problematic history of social relationships occurring after about age three may be distressing to a child, but does not result in attachment disorder.\n\nThe term attachment disorder is used to describe emotional and behavioral problems of young children, and also applied to school-age children, teenagers and adults. The specific difficulties implied depend on the age of the individual being assessed, and a child's attachment-related behaviors may be very different with one familiar adult than with another, suggesting that the disorder is within the relationship and interactions of the two people rather than an aspect of one or the other personality. No list of symptoms can legitimately be presented but generally the term attachment disorder refers to the absence or distortion of age appropriate social behaviors with adults. For example, in a toddler, attachment-disordered behavior could include a failure to stay near familiar adults in a strange environment or to be comforted by contact with a familiar person, whereas in a six-year-old attachment-disordered behavior might involve excessive friendliness and inappropriate approaches to strangers.\n\nThere are currently two main areas of theory and practice relating to the definition and diagnosis of attachment disorder, and considerable discussion about a broader definition altogether. The first main area is based on scientific enquiry, is found in academic journals and books and pays close attention to attachment theory. It is described in ICD-10 as reactive attachment disorder, or \"RAD\" for the inhibited form, and disinhibited attachment disorder, or \"DAD\" for the disinhibited form. In DSM-IV-TR both comparable inhibited and disinhibited types are called reactive attachment disorder or \"RAD\".\n\nThe second area is controversial and considered pseudoscientific. It is found in clinical practice, on websites and in books and publications, but has little or no evidence base. It makes controversial claims relating to a basis in attachment theory. The use of these controversial diagnoses of attachment disorder is linked to the use of pseudoscientific attachment therapies to treat them.\n\nSome authors have suggested that attachment, as an aspect of emotional development, is better assessed along a spectrum than considered to fall into two non-overlapping categories. This spectrum would have at one end the characteristics called secure attachment; midway along the range of disturbance would be insecure or other undesirable attachment styles; at the other extreme would be\nnon-attachment. Agreement has not yet been reached with respect to diagnostic criteria.\n\nFinally, the term is also sometimes used to cover difficulties arising in relation to various attachment styles which may not be disorders in the clinical sense.\n\nAttachment theory is primarily an evolutionary and ethological theory. In relation to infants, it primarily consists of \"proximity seeking\" to an \"attachment figure\" in the face of threat, for the purpose of survival. Although an attachment is a \"tie,\" it is not synonymous with love and affection, despite their often going together and a healthy attachment is considered to be an important foundation of all subsequent relationships. Infants become attached to adults who are sensitive and responsive in social interactions with the infant, and who remain as consistent caregivers for some time. Parental responses lead to the development of patterns of attachment which in turn lead to 'internal working models' which will guide the individual's feelings, thoughts and expectations in later relationships.\n\nA fundamental aspect of attachment is called \"basic trust\". Basic trust is a broader concept than attachment in that it extends beyond the infant-caregiver relationship to \"...the wider social network of trustable and caring others.\" and \"...links confidence about the past with faith about the future.\" \"Erikson argues that the sense of trust in oneself and others is the foundation of human development\" and with a balance of mistrust produces hope.\n\nIn the clinical sense, a disorder is a condition requiring treatment as opposed to risk factors for subsequent disorders. There is a lack of consensus about the precise meaning of the term 'attachment disorder' although there is general agreement that such disorders only arise following early adverse caregiving experiences. Reactive attachment disorder indicates the absence of either or both the main aspects of \"proximity seeking\" to an identified \"attachment figure\". This can occur either in institutions, or with repeated changes of caregiver, or from extremely neglectful primary caregivers who show persistent disregard for the child's basic attachment needs after the age of 6 months. Current official classifications of RAD under DSM-IV-TR and ICD-10 are largely based on this understanding of the nature of attachment.\n\nThe words \"attachment style\" or \"pattern\" refer to the various types of attachment arising from early care experiences, called \"secure\", \"anxious-ambivalent\", \"anxious-avoidant\", (all organized), and \"disorganized\". Some of these styles are more problematic than others, and, although they are not disorders in the clinical sense, are sometimes discussed under the term 'attachment disorder'.\n\nDiscussion of the disorganized attachment style sometimes includes this style under the rubric of attachment disorders because disorganized attachment is seen as the beginning of a developmental trajectory that will take the individual ever further from the normal range, culminating in actual disorders of thought, behavior, or mood. Early intervention for disorganized attachment, or other problematic styles, is directed toward changing the trajectory of development to provide a better outcome later in the person's life.\n\nZeanah and colleagues proposed an alternative set of criteria (see below) of three categories of attachment disorder, namely \"no discriminated attachment figure\", \"secure base distortions\" and \"disrupted attachment disorder\". These classifications consider that a disorder is a variation that requires treatment rather than an individual difference within the normal range.\n\nMany leading attachment theorists, such as Zeanah and Leiberman, have recognized the limitations of the DSM-IV-TR and ICD-10 criteria and proposed broader diagnostic criteria. There is as yet no official consensus on these criteria. The APSAC Taskforce recognised in its recommendations that \"attachment problems extending beyond RAD, are a real and appropriate concern for professionals working with children\", and set out recommendations for assessment.\n\nBoris and Zeanah (1999), have offered an approach to attachment disorders that considers cases where children have had no opportunity to form an attachment, those where there is a distorted relationship, and those where an existing attachment has been abruptly disrupted. This would significantly extend the definition beyond the ICD-10 and DSM-IV-TR definitions because those definitions are limited to situations where the child has no attachment or no attachment to a \"specified\" attachment figure.\n\nBoris and Zeanah use the term \"disorder of attachment\" to indicate a situation in which a young child has no preferred adult caregiver. Such children may be indiscriminately sociable and approach all adults, whether familiar or not; alternatively, they may be emotionally withdrawn and fail to seek comfort from anyone. This type of attachment problem is parallel to Reactive Attachment Disorder as defined in DSM and ICD in its inhibited and disinhibited forms as described above.\n\nBoris and Zeanah also describe a condition they term \"secure base distortion\". In this situation, the child has a preferred familiar caregiver, but the relationship is such that the child cannot use the adult for safety while gradually exploring the environment. Such children may endanger themselves, may cling to the adult, may be excessively compliant, or may show role reversals in which they care for or punish the adult.\n\nThe third type of disorder discussed by Boris and Zeanah is termed \"disrupted attachment\". This type of problem, which is not covered under other approaches to disordered attachment, results from an abrupt separation or loss of a familiar caregiver to whom attachment has developed. The young child's reaction to such a loss is parallel to the grief reaction of an older person, with progressive changes from protest (crying and searching) to despair, sadness, and withdrawal from communication or play, and finally detachment from the original relationship and recovery of social and play activities.\n\nMost recently, Daniel Schechter and Erica Willheim have shown a relationship between maternal violence-related posttraumatic stress disorder and secure base distortion (see above) which is characterized by child recklessness, separation anxiety, hypervigilance, and role-reversal.\n\nThe majority of 1-year-old children can tolerate brief separations from familiar caregivers and are quickly comforted when the caregivers return. These children also use familiar people as a \"secure base\" and return to them periodically when exploring a new situation. Such children are said to have a secure attachment style, and characteristically continue to develop well both cognitively and emotionally.\n\nSmaller numbers of children show less positive development at age 12 months. Their less desirable attachment styles may be predictors of poor later social development. Although these children's behavior at 12 months is not a serious problem, they appear to be on developmental trajectories that will end in poor social skills and relationships. Because attachment styles may serve as predictors of later development, it may be appropriate to think of certain attachment styles as part of the range of attachment disorders.\n\nInsecure attachment styles in toddlers involve unusual reunions after separation from a familiar person. The children may snub the returning caregiver, or may go to the person but then resist being picked up. They may reunite with the caregiver, but then persistently cling to him/her, and fail to return to their previous play. These children are more likely to have later social problems with peers and teachers, but some of them spontaneously develop better ways of interacting with other people.\n\nA small group of toddlers show a distressing way of reuniting after a separation. Called a disorganized/disoriented style, this reunion pattern can involve looking dazed or frightened, freezing in place, backing toward the caregiver or approaching with head sharply averted, or showing other behaviors that seem to imply fearfulness of the person who is being sought. Disorganized attachment has been considered a major risk factor for child psychopathology, as it appears to interfere with regulation or tolerance of negative emotions and may thus foster aggressive behavior. Disorganized patterns of attachment have the strongest links to concurrent and subsequent psychopathology, and considerable research has demonstrated both within-the-child and environmental correlates of disorganized attachment.\n\nOne study has reported a connection between a specific genetic marker and disorganized attachment (not RAD) associated with problems of parenting. Another author has compared atypical social behavior in genetic conditions such as Williams syndrome with behaviors symptomatic of RAD.\n\nTypical attachment development begins with unlearned infant reactions to social signals from caregivers. The ability to send and receive social communications through facial expressions, gestures and voice develops with social experience by seven to nine months. This makes it possible for an infant to interpret messages of calm or alarm from face or voice. At about eight months, infants typically begin to respond with fear to unfamiliar or startling situations, and to look to the faces of familiar caregivers for information that either justifies or soothes their fear. This developmental combination of social skills and the emergence of fear reactions results in attachment behavior such as proximity-seeking, if a familiar, sensitive, responsive, and cooperative adult is available. Further developments in attachment, such as negotiation of separation in the toddler and preschool period, depend on factors such as the caregiver's interaction style and ability to understand the child's emotional communications.\n\nWith insensitive or unresponsive caregivers, or frequent changes, an infant may have few experiences that encourage proximity seeking to a familiar person. An infant who experiences fear but who cannot find comforting information in an adult's face and voice may develop atypical ways of coping with fearfulness such as the maintenance of distance from adults, or the seeking of proximity to all adults. These symptoms accord with the DSM criteria for reactive attachment disorder. Either of these behavior patterns may create a developmental trajectory leading ever farther from typical attachment processes such as the development of an internal working model of social relationships that facilitates both the giving and the receiving of care from others.\n\nAtypical development of fearfulness, with a constitutional tendency either to excessive or inadequate fear reactions, might be necessary before an infant is vulnerable to the effects of poor attachment experiences.\n\nAlternatively, the two variations of RAD may develop from the same inability to develop \"stranger-wariness\" due to inadequate care. Appropriate fear responses may only be able to develop after an infant has first begun to form a selective attachment. An infant who is not in a position to do this cannot afford not to show interest in any person as they may be potential attachment figures. Faced with a swift succession of carers the child may have no opportunity to form a selective attachment until the possible biologically-determined sensitive period for developing stranger-wariness has passed. It is thought this process may lead to the disinhibited form.\n\nIn the inhibited form infants behave as if their attachment system has been \"switched off\". However the innate capacity for attachment behavior cannot be lost. This may explain why children diagnosed with the inhibited form of RAD from institutions almost invariably go on to show formation of attachment behavior to good carers. However children who suffer the inhibited form as a consequence of neglect and frequent changes of caregiver continue to show the inhibited form for far longer when placed in families.\n\nAdditionally, the development of Theory of Mind \"may\" play a role in emotional development. Theory of Mind is the ability to know that the experience of knowledge and intention lies behind human actions such as facial expressions. Although it is reported that very young infants have different responses to humans than to non-human objects, Theory of Mind develops relatively gradually and possibly results from predictable interactions with adults. However, some ability of this kind must be in place before mutual communication through gaze or other gesture can occur, as it does by seven to nine months. Some neurodevelopmental disorders, such as autism, have been attributed to the absence of the mental functions that underlie Theory of Mind. It is possible that the congenital absence of this ability, or the lack of experiences with caregivers who communicate in a predictable fashion, could underlie the development of reactive attachment disorder.\n\nRecognised assessment methods of attachment styles, difficulties or disorders include the Strange Situation procedure (Mary Ainsworth), the separation and reunion procedure and the Preschool Assessment of Attachment (\"PAA\"), the Observational Record of the Caregiving Environment (\"ORCE\") and the Attachment Q-sort (\"AQ-sort\").\nMore recent research also uses the Disturbances of Attachment Interview or \"DAI\" developed by Smyke and Zeanah, (1999). This is a semi-structured interview designed to be administered by clinicians to caregivers. It covers 12 items, namely having a discriminated, preferred adult, seeking comfort when distressed, responding to comfort when offered, social and emotional reciprocity, emotional regulation, checking back after venturing away from the care giver, reticence with unfamiliar adults, willingness to go off with relative strangers, self endangering behavior, excessive clinging, vigilance/hypercompliance and role reversal.\nICD-10 describes Reactive Attachment Disorder of Childhood, known as RAD, and Disinhibited Disorder of Childhood, less well known as DAD. DSM-IV-TR also describes Reactive Attachment Disorder of Infancy or Early Childhood. It divides this into two subtypes, Inhibited Type and Disinhibited Type, both known as RAD. The two classifications are similar and both include:\nICD-10 includes in its diagnosis psychological and physical abuse and injury in addition to neglect. This is somewhat controversial, being a \"commission\" rather than \"omission\" and because abuse in and of itself does not lead to attachment disorder.\n\nThe inhibited form is described as \"a failure to initiate or respond...to most social interactions, as manifest by excessively inhibited responses\" and such infants do not seek and accept comfort at times of threat, alarm or distress, thus failing to maintain 'proximity', an essential element of attachment behavior. The disinhibited form shows \"indiscriminate sociability...excessive familiarity with relative strangers\" (DSM-IV-TR) and therefore a lack of 'specificity', the second basic element of attachment behavior. The ICD-10 descriptions are comparable. 'Disinhibited' and 'inhibited' are not opposites in terms of attachment disorder and can co-exist in the same child. The inhibited form has a greater tendency to ameliorate with an appropriate caregiver whilst the disinhibited form is more enduring.\n\nWhile RAD is likely to occur following neglectful and abusive childcare, there should be no automatic diagnosis on this basis alone as children can form stable attachments and social relationships despite marked abuse and neglect. Abuse can occur alongside the required factors but on its own does not explain attachment disorder. Experiences of abuse are associated with the development of disorganised attachment, in which the child prefers a familiar caregiver, but responds to that person in an unpredictable and somewhat bizarre way. Within official classifications, attachment disorganization is a risk factor but not in itself an attachment disorder. Further although attachment disorders tend to occur in the context of some institutions, repeated changes of primary caregiver or extremely neglectful identifiable primary caregivers who show persistent disregard for the child's basic attachment needs, not all children raised in these conditions develop an attachment disorder.\n\nThere are a variety of mainstream prevention programs and treatment approaches for attachment disorder, attachment problems and moods or behaviors considered to be potential problems within the context of attachment theory. All such approaches for infants and younger children concentrate on increasing the responsiveness and sensitivity of the caregiver, or if that is not possible, changing the caregiver. Such approaches include 'Watch, wait and wonder,' manipulation of sensitive responsiveness, modified 'Interaction Guidance,'. 'Preschool Parent Psychotherapy,'. Circle of Security', Attachment and Biobehavioral Catch-up (ABC), the New Orleans Intervention, and Parent-Child psychotherapy. Other known treatment methods include Developmental, Individual-difference, Relationship-based therapy (DIR) (also referred to as \"Floor Time\") by Stanley Greenspan, although DIR is primarily directed to treatment of pervasive developmental disorders Some of these approaches, such as that suggested by Dozier, consider the attachment status of the adult caregiver to play an important role in the development of the emotional connection between adult and child. This includes foster parents, as children with poor attachment experiences often do not elicit appropriate caregiver responses from their attachment behaviors despite 'normative' care.\n\nTreatment for reactive attachment disorder for children usually involves a mix of therapy, counseling, and parenting education. These must be designed to make sure the child has a safe environment to live in and to develop positive interactions with caregivers and improves their relationships with their peers.\n\nMedication can be used as a way to treat similar conditions, like depression, anxiety, or hyperactivity; however, there is no quick fix for treating reactive attachment disorder. A pediatrician may recommend a treatment plan. For example, a mix of family therapy, individual psychological counseling, play therapy, special education services and parenting skills classes.\n\nIn the absence of officially recognized diagnostic criteria, and beyond the ambit of the discourse on a broader set of criteria discussed above, the term attachment disorder has been increasingly used by some clinicians to refer to a broader set of children whose behavior may be affected by lack of a primary attachment figure, a seriously unhealthy attachment relationship with a primary caregiver, or a disrupted attachment relationship. Although there are no studies examining diagnostic accuracy, concern is expressed as to the potential for over-diagnosis based on broad checklists and 'snapshots'. This form of therapy, including diagnosis and accompanying parenting techniques, is scientifically unvalidated and is not considered to be part of mainstream psychology or, despite its name, to be based on attachment theory, with which it is considered incompatible. It has been described as potentially abusive and a pseudoscientific intervention, that has resulted in tragic outcomes for children.\n\nA common feature of this form of diagnosis within attachment therapy is the use of extensive lists of \"symptoms\" which include many behaviours that are likely to be a consequence of neglect or abuse, but are not related to attachment, or not related to any clinical disorder at all. Such lists have been described as \"wildly inclusive\". The APSAC Taskforce (2006) gives examples of such lists ranging across multiple domains from some elements within the DSM-IV criteria to entirely non-specific behavior such as developmental lags, destructive behaviors, refusal to make eye contact, cruelty to animals and siblings, lack of cause and effect thinking, preoccupation with fire, blood and gore, poor peer relationships, stealing, lying, lack of a conscience, persistent nonsense questions or incessant chatter, poor impulse control, abnormal speech patterns, fighting for control over everything, and hoarding or gorging on food. Some checklists suggest that among infants, \"prefers dad to mom\" or \"wants to hold the bottle as soon as possible\" are indicative of attachment problems. The APSAC Taskforce expresses concern that high rates of false positive diagnoses are virtually certain and that posting these types of lists on web sites that also serve as marketing tools may lead many parents or others to conclude inaccurately that their children have attachment disorders.\"\n\nThere is also a considerable variety of treatments for alleged attachment disorders diagnosed on the controversial alternative basis outlined above, popularly known as attachment therapy. These therapies have little or no evidence base and vary from talking or play therapies to more extreme forms of physical and coercive techniques, of which the best known are holding therapy, rebirthing, rage-reduction and the Evergreen model. In general these therapies are aimed at adopted or fostered children with a view to creating attachment in these children to their new caregivers. Critics maintain these therapies are not based on an accepted version of attachment theory. The theoretical base is broadly a combination of regression and catharsis, accompanied by parenting methods which emphasise obedience and parental control. These therapies concentrate on changing the child rather than the caregiver. An estimated six children have died as a consequence of the more coercive forms of such treatments and the application of the accompanying parenting techniques.\n\nTwo of the most well-known cases are those of Candace Newmaker in 2001 and the Gravelles in 2003 through 2005. Following the associated publicity, some advocates of attachment therapy began to alter views and practices to be less potentially dangerous to children. This change may have been hastened by the publication of a Task Force Report on the subject in January 2006, commissioned by the American Professional Society on the Abuse of Children (APSAC) which was largely critical of attachment therapy, although these practices continue. In April 2007, ATTACh, an organisation originally set up by attachment therapists, formally adopted a White Paper stating its unequivocal opposition to the use of coercive practices in therapy and parenting.\n\n\n"}
{"id": "39343926", "url": "https://en.wikipedia.org/wiki?curid=39343926", "title": "Attractive toxic sugar baits", "text": "Attractive toxic sugar baits\n\nAttractive toxic sugar baits or ATSBs are oral insecticides designed to reduce malaria infections by killing the host vector - the mosquito - rather than the parasite itself.\n\nAttractive toxic sugar baits are manufactured from readily available, inexpensive ingredients in tropical and sub-tropical areas. They broadly consist of an oral toxic component, a sugar component to encourage feeding on the ATSB, and a scented component attractive to mosquitos or other target vectors. Typical ATSBs consist of boric acid as the oral toxin, unrefined cane sugar as the sugar source, and fruit, flowers, seeds and other scented material taken from local plants known to be popular feeding sources for mosquitos.\n\nMosquitoes require sugar as their main source of energy. By mimicking the scent of sugar-providing plants that are naturally attractive to mosquitoes, it is possible to attract the mosquitoes to insecticide-laden traps. The traps can be set next to areas with significant mosquito populations (e.g., reservoirs, roadside drainage ponds and culverts). This use of traps attractive to mosquitoes prevents the need for indiscriminate insecticide spraying.\n\nAttractive toxic sugar bait sprayed on vegetation has been successful in controlling Anopheles mosquitoes in outdoor environments. Additionally, indoor ATSB shows promise as a supplement to mosquito nets for controlling mosquitoes. Indoor ATSB constitute a novel application method for insecticide classes that act as stomach poisons and have not hitherto been exploited for mosquito control. Combined with long lasting insecticidal nets (LLINs), indoor use of ATSB has the potential to serve as a strategy for managing insecticide resistance. Mortality rates of indoor ATSB were comparable to LLINs previously tested against the same species in the same area.\n\nBoric acid is only marginally more toxic to most lifeforms than normal table salt, with exposure in humans and other mammals widely regarded as being safe. Its use as an insecticide in malarial control (instead of compounds which demonstrate high levels of mammalian toxicity or carcinogenicity) is thus seen as advantageous.\n"}
{"id": "22715614", "url": "https://en.wikipedia.org/wiki?curid=22715614", "title": "Austrian syndrome", "text": "Austrian syndrome\n\nAustrian syndrome is a medical condition first described by Robert Austrian in 1957. The classical triad consists of pneumonia, endocarditis, and meningitis, all caused by \"Streptococcus pneumoniae\". It is associated with alcoholism, due to the presence of hyposplenia (reduced splenic functioning), and can be seen in males between 40 and 60 years old.\n"}
{"id": "25", "url": "https://en.wikipedia.org/wiki?curid=25", "title": "Autism", "text": "Autism\n\nAutism is a developmental disorder characterized by troubles with social interaction and communication, and by restricted and repetitive behavior. Parents usually notice signs during the first two or three years of their child's life. These signs often develop gradually, though some children with autism reach their developmental milestones at a normal pace before worsening.\nAutism is associated with a combination of genetic and environmental factors. Risk factors during pregnancy include certain infections, such as rubella, and toxins including valproic acid, alcohol, cocaine, pesticides and air pollution. Controversies surround other proposed environmental causes; for example, the vaccine hypotheses, which have been disproven. Autism affects information processing in the brain by altering how nerve cells and their synapses connect and organize; how this occurs is not well understood. In the DSM-5, autism and less severe forms of the condition, including Asperger syndrome and pervasive developmental disorder not otherwise specified (PDD-NOS), have been combined into the diagnosis of autism spectrum disorder (ASD).\nEarly speech or behavioral interventions can help children with autism gain self-care, social, and communication skills. Although there is no known cure, there have been cases of children who recovered. Not many children with autism live independently after reaching adulthood, though some are successful. An autistic culture has developed, with some individuals seeking a cure and others believing autism should be accepted as a difference and not treated as a disorder.\nGlobally, autism is estimated to affect 24.8 million people as of 2015. In the 2000s, the number of people affected was estimated at 1–2 per 1,000 people worldwide. In the developed countries, about 1.5% of children are diagnosed with ASD , a more than doubling from 0.7% in 2000 in the United States. It occurs four-to-five times more often in boys than girls. The number of people diagnosed has increased dramatically since the 1960s, partly due to changes in diagnostic practice; the question of whether actual rates have increased is unresolved.\n\nAutism is a highly variable neurodevelopmental disorder that first appears during infancy or childhood, and generally follows a steady course without remission. People with autism may be severely impaired in some respects but normal, or even superior, in others. Overt symptoms gradually begin after the age of six months, become established by age two or three years and tend to continue through adulthood, although often in more muted form. It is distinguished not by a single symptom but by a characteristic triad of symptoms: impairments in social interaction; impairments in communication; and restricted interests and repetitive behavior. Other aspects, such as atypical eating, are also common but are not essential for diagnosis. Individual symptoms of autism occur in the general population and appear not to associate highly, without a sharp line separating pathologically severe from common traits.\n\nSocial deficits distinguish autism and the related autism spectrum disorders (ASD; see Classification) from other developmental disorders. People with autism have social impairments and often lack the intuition about others that many people take for granted. Noted autistic Temple Grandin described her inability to understand the social communication of neurotypicals, or people with normal neural development, as leaving her feeling \"like an anthropologist on Mars\".\n\nUnusual social development becomes apparent early in childhood. Autistic infants show less attention to social stimuli, smile and look at others less often, and respond less to their own name. Autistic toddlers differ more strikingly from social norms; for example, they have less eye contact and turn-taking, and do not have the ability to use simple movements to express themselves, such as pointing at things. Three- to five-year-old children with autism are less likely to exhibit social understanding, approach others spontaneously, imitate and respond to emotions, communicate nonverbally, and take turns with others. However, they do form attachments to their primary caregivers. Most children with autism display moderately less attachment security than neurotypical children, although this difference disappears in children with higher mental development or less severe ASD. Older children and adults with ASD perform worse on tests of face and emotion recognition although this may be partly due to a lower ability to define a person's own emotions.\n\nChildren with high-functioning autism suffer from more intense and frequent loneliness compared to non-autistic peers, despite the common belief that children with autism prefer to be alone. Making and maintaining friendships often proves to be difficult for those with autism. For them, the quality of friendships, not the number of friends, predicts how lonely they feel. Functional friendships, such as those resulting in invitations to parties, may affect the quality of life more deeply.\n\nThere are many anecdotal reports, but few systematic studies, of aggression and violence in individuals with ASD. The limited data suggest that, in children with intellectual disability, autism is associated with aggression, destruction of property, and tantrums.\n\nAbout a third to a half of individuals with autism do not develop enough natural speech to meet their daily communication needs. Differences in communication may be present from the first year of life, and may include delayed onset of babbling, unusual gestures, diminished responsiveness, and vocal patterns that are not synchronized with the caregiver. In the second and third years, children with autism have less frequent and less diverse babbling, consonants, words, and word combinations; their gestures are less often integrated with words. Children with autism are less likely to make requests or share experiences, and are more likely to simply repeat others' words (echolalia) or reverse pronouns. Joint attention seems to be necessary for functional speech, and deficits in joint attention seem to distinguish infants with ASD: for example: they may look at a pointing hand instead of the pointed-at object, and they consistently fail to point at objects in order to comment on or share an experience. Children with autism may have difficulty with imaginative play and with developing symbols into language.\n\nIn a pair of studies, high-functioning children with autism aged 8–15 performed equally well as, and as adults better than, individually matched controls at basic language tasks involving vocabulary and spelling. Both autistic groups performed worse than controls at complex language tasks such as figurative language, comprehension and inference. As people are often sized up initially from their basic language skills, these studies suggest that people speaking to autistic individuals are more likely to overestimate what their audience comprehends.\n\nAutistic individuals can display many forms of repetitive or restricted behavior, which the Repetitive Behavior Scale-Revised (RBS-R) categorizes as follows.\n\n\nNo single repetitive or self-injurious behavior seems to be specific to autism, but autism appears to have an elevated pattern of occurrence and severity of these behaviors.\n\nAutistic individuals may have symptoms that are independent of the diagnosis, but that can affect the individual or the family.\nAn estimated 0.5% to 10% of individuals with ASD show unusual abilities, ranging from splinter skills such as the memorization of trivia to the extraordinarily rare talents of prodigious autistic savants. Many individuals with ASD show superior skills in perception and attention, relative to the general population. Sensory abnormalities are found in over 90% of those with autism, and are considered core features by some, although there is no good evidence that sensory symptoms differentiate autism from other developmental disorders. Differences are greater for under-responsivity (for example, walking into things) than for over-responsivity (for example, distress from loud noises) or for sensation seeking (for example, rhythmic movements). An estimated 60–80% of autistic people have motor signs that include poor muscle tone, poor motor planning, and toe walking; deficits in motor coordination are pervasive across ASD and are greater in autism proper.\n\nUnusual eating behavior occurs in about three-quarters of children with ASD, to the extent that it was formerly a diagnostic indicator. Selectivity is the most common problem, although eating rituals and food refusal also occur; this does not appear to result in malnutrition. Although some children with autism also have gastrointestinal symptoms, there is a lack of published rigorous data to support the theory that children with autism have more or different gastrointestinal symptoms than usual; studies report conflicting results, and the relationship between gastrointestinal problems and ASD is unclear.\n\nParents of children with ASD have higher levels of stress. Siblings of children with ASD report greater admiration of and less conflict with the affected sibling than siblings of unaffected children and were similar to siblings of children with Down syndrome in these aspects of the sibling relationship. However, they reported lower levels of closeness and intimacy than siblings of children with Down syndrome; siblings of individuals with ASD have greater risk of negative well-being and poorer sibling relationships as adults.\n\nIt has long been presumed that there is a common cause at the genetic, cognitive, and neural levels for autism's characteristic triad of symptoms. However, there is increasing suspicion that autism is instead a complex disorder whose core aspects have distinct causes that often co-occur.\nAutism has a strong genetic basis, although the genetics of autism are complex and it is unclear whether ASD is explained more by rare mutations with major effects, or by rare multigene interactions of common genetic variants. Complexity arises due to interactions among multiple genes, the environment, and epigenetic factors which do not change DNA sequencing but are heritable and influence gene expression. Many genes have been associated with autism through sequencing the genomes of affected individuals and their parents.\n\nStudies of twins suggest that heritability is 0.7 for autism and as high as 0.9 for ASD, and siblings of those with autism are about 25 times more likely to be autistic than the general population. However, most of the mutations that increase autism risk have not been identified. Typically, autism cannot be traced to a Mendelian (single-gene) mutation or to a single chromosome abnormality, and none of the genetic syndromes associated with ASDs have been shown to selectively cause ASD. Numerous candidate genes have been located, with only small effects attributable to any particular gene. Most loci individually explain less than 1% of cases of autism. The large number of autistic individuals with unaffected family members may result from spontaneous structural variation — such as deletions, duplications or inversions in genetic material during meiosis. Hence, a substantial fraction of autism cases may be traceable to genetic causes that are highly heritable but not inherited: that is, the mutation that causes the autism is not present in the parental genome. Autism may be underdiagnosed in women and girls due to an assumption that it is primarily a male condition.\n\nSeveral lines of evidence point to synaptic dysfunction as a cause of autism. Some rare mutations may lead to autism by disrupting some synaptic pathways, such as those involved with cell adhesion. Gene replacement studies in mice suggest that autistic symptoms are closely related to later developmental steps that depend on activity in synapses and on activity-dependent changes. All known teratogens (agents that cause birth defects) related to the risk of autism appear to act during the first eight weeks from conception, and though this does not exclude the possibility that autism can be initiated or affected later, there is strong evidence that autism arises very early in development.\n\nExposure to air pollution during pregnancy, especially heavy metals and particulates, may increase the risk of autism. Environmental factors that have been claimed without evidence to contribute to or exacerbate autism include certain foods, infectious diseases, solvents, PCBs, phthalates and phenols used in plastic products, pesticides, brominated flame retardants, alcohol, smoking, illicit drugs, vaccines, and prenatal stress. Some such as the MMR vaccine have been completely disproven.\n\nParents may first become aware of autistic symptoms in their child around the time of a routine vaccination. This has led to unsupported theories blaming vaccine \"overload\", a vaccine preservative, or the MMR vaccine for causing autism. The latter theory was supported by a litigation-funded study that has since been shown to have been \"an elaborate fraud\". Although these theories lack convincing scientific evidence and are biologically implausible, parental concern about a potential vaccine link with autism has led to lower rates of childhood immunizations, outbreaks of previously controlled childhood diseases in some countries, and the preventable deaths of several children.\n\nAutism's symptoms result from maturation-related changes in various systems of the brain. How autism occurs is not well understood. Its mechanism can be divided into two areas: the pathophysiology of brain structures and processes associated with autism, and the neuropsychological linkages between brain structures and behaviors. The behaviors appear to have multiple pathophysiologies.\n\nUnlike many other brain disorders, such as Parkinson's, autism does not have a clear unifying mechanism at either the molecular, cellular, or systems level; it is not known whether autism is a few disorders caused by mutations converging on a few common molecular pathways, or is (like intellectual disability) a large set of disorders with diverse mechanisms. Autism appears to result from developmental factors that affect many or all functional brain systems, and to disturb the timing of brain development more than the final product. Neuroanatomical studies and the associations with teratogens strongly suggest that autism's mechanism includes alteration of brain development soon after conception. This anomaly appears to start a cascade of pathological events in the brain that are significantly influenced by environmental factors. Just after birth, the brains of children with autism tend to grow faster than usual, followed by normal or relatively slower growth in childhood. It is not known whether early overgrowth occurs in all children with autism. It seems to be most prominent in brain areas underlying the development of higher cognitive specialization. Hypotheses for the cellular and molecular bases of pathological early overgrowth include the following:\n\nThe immune system is thought to play an important role in autism. Children with autism have been found by researchers to have inflammation of both the peripheral and central immune systems as indicated by increased levels of pro-inflammatory cytokines and significant activation of microglia. Biomarkers of abnormal immune function have also been associated with increased impairments in behaviors that are characteristic of the core features of autism such as, deficits in social interactions and communication. Interactions between the immune system and the nervous system begin early during the embryonic stage of life, and successful neurodevelopment depends on a balanced immune response. It is thought that activation of a pregnant mother's immune system such as from environmental toxicants or infection can contribute to causing autism through causing a disruption of brain development. This is supported by recent studies that have found that infection during pregnancy is associated with an increased risk of autism.\n\nThe relationship of neurochemicals to autism is not well understood; several have been investigated, with the most evidence for the role of serotonin and of genetic differences in its transport. The role of group I metabotropic glutamate receptors (mGluR) in the pathogenesis of fragile X syndrome, the most common identified genetic cause of autism, has led to interest in the possible implications for future autism research into this pathway. Some data suggests neuronal overgrowth potentially related to an increase in several growth hormones or to impaired regulation of growth factor receptors. Also, some inborn errors of metabolism are associated with autism, but probably account for less than 5% of cases.\n\nThe mirror neuron system (MNS) theory of autism hypothesizes that distortion in the development of the MNS interferes with imitation and leads to autism's core features of social impairment and communication difficulties. The MNS operates when an animal performs an action or observes another animal perform the same action. The MNS may contribute to an individual's understanding of other people by enabling the modeling of their behavior via embodied simulation of their actions, intentions, and emotions. Several studies have tested this hypothesis by demonstrating structural abnormalities in MNS regions of individuals with ASD, delay in the activation in the core circuit for imitation in individuals with Asperger syndrome, and a correlation between reduced MNS activity and severity of the syndrome in children with ASD. However, individuals with autism also have abnormal brain activation in many circuits outside the MNS and the MNS theory does not explain the normal performance of children with autism on imitation tasks that involve a goal or object.\nASD-related patterns of low function and aberrant activation in the brain differ depending on whether the brain is doing social or nonsocial tasks.\nIn autism there is evidence for reduced functional connectivity of the default network (a large-scale brain network involved in social and emotional processing), with intact connectivity of the task-positive network (used in sustained attention and goal-directed thinking). In people with autism the two networks are not negatively correlated in time, suggesting an imbalance in toggling between the two networks, possibly reflecting a disturbance of self-referential thought.\n\nThe underconnectivity theory of autism hypothesizes that autism is marked by underfunctioning high-level neural connections and synchronization, along with an excess of low-level processes. Evidence for this theory has been found in functional neuroimaging studies on autistic individuals and by a brainwave study that suggested that adults with ASD have local overconnectivity in the cortex and weak functional connections between the frontal lobe and the rest of the cortex. Other evidence suggests the underconnectivity is mainly within each hemisphere of the cortex and that autism is a disorder of the association cortex.\n\nFrom studies based on event-related potentials, transient changes to the brain's electrical activity in response to stimuli, there is considerable evidence for differences in autistic individuals with respect to attention, orientation to auditory and visual stimuli, novelty detection, language and face processing, and information storage; several studies have found a preference for nonsocial stimuli. For example, magnetoencephalography studies have found evidence in children with autism of delayed responses in the brain's processing of auditory signals.\n\nIn the genetic area, relations have been found between autism and schizophrenia based on duplications and deletions of chromosomes; research showed that schizophrenia and autism are significantly more common in combination with 1q21.1 deletion syndrome. Research on autism/schizophrenia relations for chromosome 15 (15q13.3), chromosome 16 (16p13.1) and chromosome 17 (17p12) are inconclusive.\n\nFunctional connectivity studies have found both hypo- and hyper-connectivity in brains of people with autism. Hypo-connectivity seems to dominate, especially for interhemispheric and cortico-cortical functional connectivity.\n\nTwo major categories of cognitive theories have been proposed about the links between autistic brains and behavior.\n\nThe first category focuses on deficits in social cognition. Simon Baron-Cohen's empathizing–systemizing theory postulates that autistic individuals can systemize—that is, they can develop internal rules of operation to handle events inside the brain—but are less effective at empathizing by handling events generated by other agents. An extension, the extreme male brain theory, hypothesizes that autism is an extreme case of the male brain, defined psychometrically as individuals in whom systemizing is better than empathizing. These theories are somewhat related to Baron-Cohen's earlier theory of mind approach, which hypothesizes that autistic behavior arises from an inability to ascribe mental states to oneself and others. The theory of mind hypothesis is supported by the atypical responses of children with autism to the Sally–Anne test for reasoning about others' motivations, and the mirror neuron system theory of autism described in \"Pathophysiology\" maps well to the hypothesis. However, most studies have found no evidence of impairment in autistic individuals' ability to understand other people's basic intentions or goals; instead, data suggests that impairments are found in understanding more complex social emotions or in considering others' viewpoints.\n\nThe second category focuses on nonsocial or general processing: the executive functions such as working memory, planning, inhibition. In his review, Kenworthy states that \"the claim of executive dysfunction as a causal factor in autism is controversial\", however, \"it is clear that executive dysfunction plays a role in the social and cognitive deficits observed in individuals with autism\". Tests of core executive processes such as eye movement tasks indicate improvement from late childhood to adolescence, but performance never reaches typical adult levels. A strength of the theory is predicting stereotyped behavior and narrow interests; two weaknesses are that executive function is hard to measure and that executive function deficits have not been found in young children with autism.\n\nWeak central coherence theory hypothesizes that a limited ability to see the big picture underlies the central disturbance in autism. One strength of this theory is predicting special talents and peaks in performance in autistic people. A related theory—enhanced perceptual functioning—focuses more on the superiority of locally oriented and perceptual operations in autistic individuals. Yet another, monotropism, posits that autism stems from a different cognitive style, tending to focus attention (or processing resources) intensely, to the exclusion of other stimuli. These theories map well from the underconnectivity theory of autism.\n\nNeither category is satisfactory on its own; social cognition theories poorly address autism's rigid and repetitive behaviors, while the nonsocial theories have difficulty explaining social impairment and communication difficulties. A combined theory based on multiple deficits may prove to be more useful.\n\nDiagnosis is based on behavior, not cause or mechanism. Under the DSM-5, autism is characterized by persistent deficits in social communication and interaction across multiple contexts, as well as restricted, repetitive patterns of behavior, interests, or activities. These deficits are present in early childhood, typically before age three, and lead to clinically significant functional impairment. Sample symptoms include lack of social or emotional reciprocity, stereotyped and repetitive use of language or idiosyncratic language, and persistent preoccupation with unusual objects. The disturbance must not be better accounted for by Rett syndrome, intellectual disability or global developmental delay. ICD-10 uses essentially the same definition.\n\nSeveral diagnostic instruments are available. Two are commonly used in autism research: the Autism Diagnostic Interview-Revised (ADI-R) is a semistructured parent interview, and the Autism Diagnostic Observation Schedule (ADOS) uses observation and interaction with the child. The Childhood Autism Rating Scale (CARS) is used widely in clinical environments to assess severity of autism based on observation of children. The Diagnostic interview for social and communication disorders (DISCO) may also be used.\n\nA pediatrician commonly performs a preliminary investigation by taking developmental history and physically examining the child. If warranted, diagnosis and evaluations are conducted with help from ASD specialists, observing and assessing cognitive, communication, family, and other factors using standardized tools, and taking into account any associated medical conditions. A pediatric neuropsychologist is often asked to assess behavior and cognitive skills, both to aid diagnosis and to help recommend educational interventions. A differential diagnosis for ASD at this stage might also consider intellectual disability, hearing impairment, and a specific language impairment such as Landau–Kleffner syndrome. The presence of autism can make it harder to diagnose coexisting psychiatric disorders such as depression.\n\nClinical genetics evaluations are often done once ASD is diagnosed, particularly when other symptoms already suggest a genetic cause. Although genetic technology allows clinical geneticists to link an estimated 40% of cases to genetic causes, consensus guidelines in the US and UK are limited to high-resolution chromosome and fragile X testing. A genotype-first model of diagnosis has been proposed, which would routinely assess the genome's copy number variations. As new genetic tests are developed several ethical, legal, and social issues will emerge. Commercial availability of tests may precede adequate understanding of how to use test results, given the complexity of autism's genetics. Metabolic and neuroimaging tests are sometimes helpful, but are not routine.\n\nASD can sometimes be diagnosed by age 14 months, although diagnosis becomes increasingly stable over the first three years of life: for example, a one-year-old who meets diagnostic criteria for ASD is less likely than a three-year-old to continue to do so a few years later. In the UK the National Autism Plan for Children recommends at most 30 weeks from first concern to completed diagnosis and assessment, though few cases are handled that quickly in practice. Although the symptoms of autism and ASD begin early in childhood, they are sometimes missed; years later, adults may seek diagnoses to help them or their friends and family understand themselves, to help their employers make adjustments, or in some locations to claim disability living allowances or other benefits. Girls are often diagnosed later than boys.\n\nUnderdiagnosis and overdiagnosis are problems in marginal cases, and much of the recent increase in the number of reported ASD cases is likely due to changes in diagnostic practices. The increasing popularity of drug treatment options and the expansion of benefits has given providers incentives to diagnose ASD, resulting in some overdiagnosis of children with uncertain symptoms. Conversely, the cost of screening and diagnosis and the challenge of obtaining payment can inhibit or delay diagnosis. It is particularly hard to diagnose autism among the visually impaired, partly because some of its diagnostic criteria depend on vision, and partly because autistic symptoms overlap with those of common blindness syndromes or blindisms.\n\nAutism is one of the five pervasive developmental disorders (PDD), which are characterized by widespread abnormalities of social interactions and communication, and severely restricted interests and highly repetitive behavior. These symptoms do not imply sickness, fragility, or emotional disturbance.\n\nOf the five PDD forms, Asperger syndrome is closest to autism in signs and likely causes; Rett syndrome and childhood disintegrative disorder share several signs with autism, but may have unrelated causes; PDD not otherwise specified (PDD-NOS; also called \"atypical autism\") is diagnosed when the criteria are not met for a more specific disorder. Unlike with autism, people with Asperger syndrome have no substantial delay in language development. The terminology of autism can be bewildering, with autism, Asperger syndrome and PDD-NOS often called the \"autism spectrum disorders\" (ASD) or sometimes the \"autistic disorders\", whereas autism itself is often called \"autistic disorder\", \"childhood autism\", or \"infantile autism\". In this article, \"autism\" refers to the classic autistic disorder; in clinical practice, though, \"autism\", \"ASD\", and \"PDD\" are often used interchangeably. ASD, in turn, is a subset of the broader autism phenotype, which describes individuals who may not have ASD but do have autistic-like traits, such as avoiding eye contact.\n\nThe manifestations of autism cover a wide spectrum, ranging from individuals with severe impairments—who may be silent, developmentally disabled, and locked into hand flapping and rocking—to high functioning individuals who may have active but distinctly odd social approaches, narrowly focused interests, and verbose, pedantic communication. Because the behavior spectrum is continuous, boundaries between diagnostic categories are necessarily somewhat arbitrary. Sometimes the syndrome is divided into low-, medium- or high-functioning autism (LFA, MFA, and HFA), based on IQ thresholds, or on how much support the individual requires in daily life; these subdivisions are not standardized and are controversial. Autism can also be divided into syndromal and non-syndromal autism; the syndromal autism is associated with severe or profound intellectual disability or a congenital syndrome with physical symptoms, such as tuberous sclerosis. Although individuals with Asperger syndrome tend to perform better cognitively than those with autism, the extent of the overlap between Asperger syndrome, HFA, and non-syndromal autism is unclear.\n\nSome studies have reported diagnoses of autism in children due to a loss of language or social skills, as opposed to a failure to make progress, typically from 15 to 30 months of age. The validity of this distinction remains controversial; it is possible that regressive autism is a specific subtype, or that there is a continuum of behaviors between autism with and without regression.\n\nResearch into causes has been hampered by the inability to identify biologically meaningful subgroups within the autistic population and by the traditional boundaries between the disciplines of psychiatry, psychology, neurology and pediatrics. Newer technologies such as fMRI and diffusion tensor imaging can help identify biologically relevant phenotypes (observable traits) that can be viewed on brain scans, to help further neurogenetic studies of autism; one example is lowered activity in the fusiform face area of the brain, which is associated with impaired perception of people versus objects. It has been proposed to classify autism using genetics as well as behavior.\n\nAbout half of parents of children with ASD notice their child's unusual behaviors by age 18 months, and about four-fifths notice by age 24 months. According to an article, failure to meet any of the following milestones \"is an absolute indication to proceed with further evaluations. Delay in referral for such testing may delay early diagnosis and treatment and affect the long-term outcome\".\n\nThe United States Preventive Services Task Force in 2016 found it was unclear if screening was beneficial or harmful among children in whom there is no concerns. The Japanese practice is to screen all children for ASD at 18 and 24 months, using autism-specific formal screening tests. In contrast, in the UK, children whose families or doctors recognize possible signs of autism are screened. It is not known which approach is more effective. Screening tools include the Modified Checklist for Autism in Toddlers (M-CHAT), the Early Screening of Autistic Traits Questionnaire, and the First Year Inventory; initial data on M-CHAT and its predecessor, the Checklist for Autism in Toddlers (CHAT), on children aged 18–30 months suggests that it is best used in a clinical setting and that it has low sensitivity (many false-negatives) but good specificity (few false-positives). It may be more accurate to precede these tests with a broadband screener that does not distinguish ASD from other developmental disorders. Screening tools designed for one culture's norms for behaviors like eye contact may be inappropriate for a different culture. Although genetic screening for autism is generally still impractical, it can be considered in some cases, such as children with neurological symptoms and dysmorphic features.\n\nWhile infection with rubella during pregnancy causes fewer than 1% of cases of autism, vaccination against rubella can prevent many of those cases.\n\nThe main goals when treating children with autism are to lessen associated deficits and family distress, and to increase quality of life and functional independence. In general, higher IQs are correlated with greater responsiveness to treatment and improved treatment outcomes. No single treatment is best and treatment is typically tailored to the child's needs. Families and the educational system are the main resources for treatment. Services should be carried out by behavior analysts, special education teachers, speech pathologists, and licensed psychologists. Studies of interventions have methodological problems that prevent definitive conclusions about efficacy. However, the development of evidence-based interventions has advanced in recent years. Although many psychosocial interventions have some positive evidence, suggesting that some form of treatment is preferable to no treatment, the methodological quality of systematic reviews of these studies has generally been poor, their clinical results are mostly tentative, and there is little evidence for the relative effectiveness of treatment options. Intensive, sustained special education programs and behavior therapy early in life can help children acquire self-care, communication, and job skills, and often improve functioning and decrease symptom severity and maladaptive behaviors; claims that intervention by around age three years is crucial are not substantiated. While medications have not been found to help with core symptoms, they may be used for associated symptoms, such as irritability, inattention, or repetitive behavior patterns.\n\nEducational interventions often used include applied behavior analysis (ABA), developmental models, structured teaching, speech and language therapy, social skills therapy, and occupational therapy. Among these approaches, interventions either treat autistic features comprehensively, or focalize treatment on a specific area of deficit. The quality of research for early intensive behavioral intervention (EIBI)—a treatment procedure encompassing over thirty hours per week of the structured type of ABA that is carried out with very young children—is currently low, and more vigorous research designs with larger sample sizes are needed. Two theoretical frameworks outlined for early childhood intervention include structured and naturalistic ABA interventions, and developmental social pragmatic models (DSP). One interventional strategy utilizes a parent training model, which teaches parents how to implement various ABA and DSP techniques, allowing for parents to disseminate interventions themselves. Various DSP programs have been developed to explicitly deliver intervention systems through at-home parent implementation. Despite the recent development of parent training models, these interventions have demonstrated effectiveness in numerous studies, being evaluated as a probable efficacious mode of treatment.\n\nEarly, intensive ABA therapy has demonstrated effectiveness in enhancing global functioning in preschool children, and is well-established for improving the intellectual performance of that age group. Similarly, a teacher-implemented intervention that utilizes a more naturalistic form of ABA combined with a developmental social pragmatic approach has been found to be beneficial in improving social-communication skills in young children, although there is less evidence in its treatment of global symptoms. Neuropsychological reports are often poorly communicated to educators, resulting in a gap between what a report recommends and what education is provided. It is not known whether treatment programs for children lead to significant improvements after the children grow up, and the limited research on the effectiveness of adult residential programs shows mixed results. The appropriateness of including children with varying severity of autism spectrum disorders in the general education population is a subject of current debate among educators and researchers.\n\nMedications may be used to treat ASD symptoms that interfere with integrating a child into home or school when behavioral treatment fails. They may also be used for associated health problems, such as ADHD or anxiety. More than half of US children diagnosed with ASD are prescribed psychoactive drugs or anticonvulsants, with the most common drug classes being antidepressants, stimulants, and antipsychotics. The atypical antipsychotic drugs risperidone and aripiprazole are FDA-approved for treating associated aggressive and self-injurious behaviors. However, their side effects must be weighed against their potential benefits, and people with autism may respond atypically. Side effects, for example, may include weight gain, tiredness, drooling, and aggression. SSRI antidepressants, such as fluoxetine and fluvoxamine, have been shown to be effective in reducing repetitive and ritualistic behaviors, while the stimulant medication methylphenidate is beneficial for some children with co-morbid inattentiveness or hyperactivity. There is scant reliable research about the effectiveness or safety of drug treatments for adolescents and adults with ASD. No known medication relieves autism's core symptoms of social and communication impairments. Experiments in mice have reversed or reduced some symptoms related to autism by replacing or modulating gene function, suggesting the possibility of targeting therapies to specific rare mutations known to cause autism.\n\nAlthough many alternative therapies and interventions are available, few are supported by scientific studies. Treatment approaches have little empirical support in quality-of-life contexts, and many programs focus on success measures that lack predictive validity and real-world relevance. Some alternative treatments may place the child at risk. A 2008 study found that compared to their peers, autistic boys have significantly thinner bones if on casein-free diets; in 2005, botched chelation therapy killed a five-year-old child with autism. Another alternative medicine practice with no evidence is CEASE therapy, a mixture of homeopathy, supplements, and 'vaccine detoxing'.\n\nAlthough popularly used as an alternative treatment for people with autism, there is no good evidence that a gluten-free diet is of benefit. In the subset of people who have gluten sensitivity there is limited evidence that suggests that a gluten free diet may improve some autistic behaviors. There is tentative evidence that music therapy may improve social interactions, verbal communication, and non-verbal communication skills. There has been early research looking at hyperbaric treatments in children with autism.\n\nThe emergence of the autism rights movement has served as an attempt to encourage people to be more tolerant of those with autism. Through this movement, people hope to cause others to think of autism as a difference instead of a disease. Proponents of this movement wish to seek \"acceptance, not cures.\" There have also been many worldwide events promoting autism awareness such as World Autism Awareness Day, Light It Up Blue, Autism Sunday, Autistic Pride Day, Autreat, and others. There have also been many organizations dedicated to increasing the awareness of autism and the effects that autism has on someone's life. These organizations include Autism Speaks, Autism National Committee, Autism Society of America, and many others. Social-science scholars have had an increased focused on studying those with autism in hopes to learn more about \"autism as a culture, transcultural comparisons... and research on social movements.\" Media has had an influence on how the public perceives those with autism. \"Rain Man\", a film that won 4 Oscars including Best Picture, depicts a character with autism who has incredible talents and abilities. While many autistic individuals don't have these special abilities, there are some who have been successful in their fields.\n\nTreatment is expensive; indirect costs are more so. For someone born in 2000, a US study estimated an average lifetime cost of $ (net present value in dollars, inflation-adjusted from 2003 estimate), with about 10% medical care, 30% extra education and other care, and 60% lost economic productivity. Publicly supported programs are often inadequate or inappropriate for a given child, and unreimbursed out-of-pocket medical or therapy expenses are associated with likelihood of family financial problems; one 2008 US study found a 14% average loss of annual income in families of children with ASD, and a related study found that ASD is associated with higher probability that child care problems will greatly affect parental employment. US states increasingly require private health insurance to cover autism services, shifting costs from publicly funded education programs to privately funded health insurance. After childhood, key treatment issues include residential care, job training and placement, sexuality, social skills, and estate planning.\n\nThere is no known cure. Children recover occasionally, so that they lose their diagnosis of ASD; this occurs sometimes after intensive treatment and sometimes not. It is not known how often recovery happens; reported rates in unselected samples have ranged from 3% to 25%. Most children with autism acquire language by age five or younger, though a few have developed communication skills in later years. Most children with autism lack social support, meaningful relationships, future employment opportunities or self-determination. Although core difficulties tend to persist, symptoms often become less severe with age.\n\nFew high-quality studies address long-term prognosis. Some adults show modest improvement in communication skills, but a few decline; no study has focused on autism after midlife. Acquiring language before age six, having an IQ above 50, and having a marketable skill all predict better outcomes; independent living is unlikely with severe autism. Most people with autism face significant obstacles in transitioning to adulthood.\n\nMost recent reviews tend to estimate a prevalence of 1–2 per 1,000 for autism and close to 6 per 1,000 for ASD, and 11 per 1,000 children in the United States for ASD as of 2008; because of inadequate data, these numbers may underestimate ASD's true rate. Globally, autism affects an estimated 24.8 million people as of 2015, while Asperger syndrome affects a further 37.2 million. In 2012, the NHS estimated that the overall prevalence of autism among adults aged 18 years and over in the UK was 1.1%. Rates of PDD-NOS's has been estimated at 3.7 per 1,000, Asperger syndrome at roughly 0.6 per 1,000, and childhood disintegrative disorder at 0.02 per 1,000. CDC's most recent estimate is that 1 out of every 68 children, or 14.7 per 1,000, has an ASD as of 2010.\n\nThe number of reported cases of autism increased dramatically in the 1990s and early 2000s. This increase is largely attributable to changes in diagnostic practices, referral patterns, availability of services, age at diagnosis, and public awareness, though unidentified environmental risk factors cannot be ruled out. The available evidence does not rule out the possibility that autism's true prevalence has increased; a real increase would suggest directing more attention and funding toward changing environmental factors instead of continuing to focus on genetics.\n\nBoys are at higher risk for ASD than girls. The sex ratio averages 4.3:1 and is greatly modified by cognitive impairment: it may be close to 2:1 with intellectual disability and more than 5.5:1 without. Several theories about the higher prevalence in males have been investigated, but the cause of the difference is unconfirmed; one theory is that females are underdiagnosed.\n\nAlthough the evidence does not implicate any single pregnancy-related risk factor as a cause of autism, the risk of autism is associated with advanced age in either parent, and with diabetes, bleeding, and use of psychiatric drugs in the mother during pregnancy. The risk is greater with older fathers than with older mothers; two potential explanations are the known increase in mutation burden in older sperm, and the hypothesis that men marry later if they carry genetic liability and show some signs of autism. Most professionals believe that race, ethnicity, and socioeconomic background do not affect the occurrence of autism.\n\nSeveral other conditions are common in children with autism. They include:\n\nA few examples of autistic symptoms and treatments were described long before autism was named. The \"Table Talk\" of Martin Luther, compiled by his notetaker, Mathesius, contains the story of a 12-year-old boy who may have been severely autistic. Luther reportedly thought the boy was a soulless mass of flesh possessed by the devil, and suggested that he be suffocated, although a later critic has cast doubt on the veracity of this report. The earliest well-documented case of autism is that of Hugh Blair of Borgue, as detailed in a 1747 court case in which his brother successfully petitioned to annul Blair's marriage to gain Blair's inheritance. The Wild Boy of Aveyron, a feral child caught in 1798, showed several signs of autism; the medical student Jean Itard treated him with a behavioral program designed to help him form social attachments and to induce speech via imitation.\n\nThe New Latin word \"autismus\" (English translation \"autism\") was coined by the Swiss psychiatrist Eugen Bleuler in 1910 as he was defining symptoms of schizophrenia. He derived it from the Greek word \"autós\" (αὐτός, meaning \"self\"), and used it to mean morbid self-admiration, referring to \"autistic withdrawal of the patient to his fantasies, against which any influence from outside becomes an intolerable disturbance\".\n\nThe word \"autism\" first took its modern sense in 1938 when Hans Asperger of the Vienna University Hospital adopted Bleuler's terminology \"autistic psychopaths\" in a lecture in German about child psychology. Asperger was investigating an ASD now known as Asperger syndrome, though for various reasons it was not widely recognized as a separate diagnosis until 1981. Leo Kanner of the Johns Hopkins Hospital first used \"autism\" in its modern sense in English when he introduced the label \"early infantile autism\" in a 1943 report of 11 children with striking behavioral similarities. Almost all the characteristics described in Kanner's first paper on the subject, notably \"autistic aloneness\" and \"insistence on sameness\", are still regarded as typical of the autistic spectrum of disorders. It is not known whether Kanner derived the term independently of Asperger.\n\nDonald Triplett was the first person diagnosed with autism. He was diagnosed by Kanner after being first examined in 1938, and was labeled as \"case 1\". Triplett was noted for his savant abilities, particularly being able to name musical notes played on a piano and to mentally multiply numbers. His father, Oliver, described him as socially withdrawn but interested in number patterns, music notes, letters of the alphabet, and U.S. president pictures. By the age of 2, he had the ability to recite the 23rd Psalm and memorized 25 questions and answers from the Presbyterian catechism. He was also interested in creating musical chords.\n\nKanner's reuse of \"autism\" led to decades of confused terminology like \"infantile schizophrenia\", and child psychiatry's focus on maternal deprivation led to misconceptions of autism as an infant's response to \"refrigerator mothers\". Starting in the late 1960s autism was established as a separate syndrome.\n\nAs late as the mid-1970s there was little evidence of a genetic role in autism; while in 2007 it was believed to be one of the most heritable psychiatric conditions. Although the rise of parent organizations and the destigmatization of childhood ASD have affected how ASD is viewed, parents continue to feel social stigma in situations where their child's autistic behavior is perceived negatively, and many primary care physicians and medical specialists express some beliefs consistent with outdated autism research.\n\nIt took until 1980 for the DSM-III to differentiate autism from childhood schizophrenia. In 1987, the DSM-III-R provided a checklist for diagnosing autism. In May 2013, the DSM-5 was released, updating the classification for pervasive developmental disorders. The grouping of disorders, including PDD-NOS, autism, Asperger syndrome, Rett syndrome, and CDD, has been removed and replaced with the general term of Autism Spectrum Disorders. The two categories that exist are impaired social communication and/or interaction, and restricted and/or repetitive behaviors.\n\nThe Internet has helped autistic individuals bypass nonverbal cues and emotional sharing that they find difficult to deal with, and has given them a way to form online communities and work remotely. Societal and cultural aspects of autism have developed: some in the community seek a cure, while others believe that autism is simply another way of being.\n\n"}
{"id": "22523849", "url": "https://en.wikipedia.org/wiki?curid=22523849", "title": "Benzopyrene", "text": "Benzopyrene\n\nA benzopyrene is an organic compound with the formula CH. Structurally speaking, the colorless isomers of benzopyrene are pentacyclic hydrocarbons and are fusion products of pyrene and a phenylene group. Two isomeric species of benzopyrene are benzo[\"a\"]pyrene and the less common benzo[\"e\"]pyrene. They belong to the chemical class of polycyclic aromatic hydrocarbons.\n\nRelated compounds include cyclopentapyrenes, dibenzopyrenes, indenopyrenes and naphthopyrenes. Benzopyrene is a component of pitch and occurs together with other related pentacyclic aromatic species such as picene, benzofluoranthenes, and perylene. It is naturally emitted by forest fires and volcanic eruptions and can also be found in coal tar, cigarette smoke, wood smoke, and burnt foods such as coffee. Fumes that develop from fat dripping on blistering charcoal are rich in benzopyrene, which can condense on grilled goods.\n\nBenzopyrenes are harmful because they form carcinogenic and mutagenic metabolites (such as (+)-benzo[\"a\"]pyrene-7,8-dihydrodiol-9,10-epoxide from benzo[\"a\"]pyrene) which intercalate into DNA, interfering with transcription. They are considered pollutants and carcinogens. The mechanism of action of benzo[a]pyrene-related DNA modification has been investigated extensively and relates to the activity of cytochrome P450 subclass 1A1 (CYP1A1). Seemingly, the high activity of CYP1A1 in the intestinal mucosa prevents major amounts of ingested benzo[a]pyrene from entering portal blood and systemic circulation. The intestinal (but not hepatic) detoxification mechanism seems to depend on receptors that recognize bacterial surface components (TLR2).\n\nEvidence exists to link benzo[\"a\"]pyrene to the formation of lung cancer.\n\nIn February 2014, NASA announced a greatly upgraded database for tracking polycyclic aromatic hydrocarbons (PAHs), including benzopyrene, in the universe. According to scientists, more than 20% of the carbon in the universe may be associated with PAHs, possible starting materials for the formation of life. PAHs seem to have been formed shortly after the Big Bang, are widespread throughout the universe, and are associated with new stars and exoplanets.\n\n"}
{"id": "11898226", "url": "https://en.wikipedia.org/wiki?curid=11898226", "title": "Bethany Christian Trust", "text": "Bethany Christian Trust\n\nBethany Christian Trust is a Scottish charity based in Edinburgh, registered charity no SC003783. It was set up to relieve the suffering and meet the long-term needs of homeless and vulnerable people. \n\nThe Trust's vision is that through Christian love in action, homelessness will be reduced and vulnerable people empowered to live independently within society. \n\nThe Trust's partners include Edinburgh City Council, West Lothian Council, East Lothian Council, Fife Council, over 100 churches, Edinburgh City Mission, Christian Concern for the Homeless, CHAI, Rock Trust. Its areas of work are Edinburgh and the Lothians, Aberdeen (City & Shire), Dundee, Fife, Dumfries & Galloway and Glasgow.\n\nThe charity was established in 1983 by Rev Alan Berry, then a minister in Leith, Edinburgh, in response to the needs of the many homeless and vulnerable people he confronted daily. \n\nBethany supports around 4,000 homeless and vulnerable people every year through its services. Bethany has nearly 200 full-time and part-time staff and over 1000 volunteers. Its turnover was £5.6 million in 2012.\n\nOperated in conjunction with Edinburgh City Mission, the Care Van offers rolls, soup, clothing, and support to Edinburgh's rough sleepers every night in the city centre. A team from one of over 30 churches staffs the van every night on a rota basis.\nEvery night from late October to the end of March, the Care Shelter provides emergency accommodation in churches around Edinburgh for men and women over the age of 16 who have no other accommodation. A volunteer team provides and serves hot meals and Bethany staff provide professional advice and support to help move people off the streets and into longer-term accommodation. Anyone without a place to sleep may use the Shelter, which is free of charge. Edinburgh City Mission and local churches make this service possible.\n\nBethany House is a resettlement unit for homeless men and women aged 16+. In order to use the services at Bethany House, the resident must cope with personal care, be claiming benefits and be willing to apply for more permanent housing.\n\nBethany Christian Centre is a supportive working community for men aged 17–70 that aims to help those in need to resettle back into a stable environment, free from alcohol and drugs. Aid in addiction issues, budgeting, legal matters, health issues, family relationships, housing applications, benefit claims, and contact with counselling services and other agencies are among a myriad of services the Centre provides. Residents learn to relate and work together as they participate in community meetings, cooking, cleaning, group activities and house holidays and are encouraged to get involved with further education and voluntary work placements within Bethany or further afield. Bethany Addiction Team offer group work and one-to-one counselling to residents. Most people are directed to the Centre through a referral agent, but it is possible to self-refer. Acceptance onto the programme and residency is agreed after an informal interview.\n\nThe service provides accommodation for homeless young men and visiting support for young women in the community. An initial 12-week stay (that can be extended to a maximum of 36 weeks) gives residents 24-hour support with the goal of moving residents to their own tenancy. All residents are encouraged to participate in the support programme, which includes help on tenancy issues, group work, outings, sport, special interests, topical discussions, and house meetings. A full-time staff is always present, and each resident has a support worker to help him or her.\n\nBethany Housing Services provides fully furnished, single and shared accommodation for homeless and vulnerable people in Edinburgh, Dumfries and Fife as well as support provided to individuals in their own tenancies. Support and advice is available to help people adjust to independent living. Help with budgeting, addiction relapse prevention, independent living skills, emotional issues, benefits, household management and education and training is provided to clients in their own homes by the Supported Housing Team working in partnership with other relevant statutory and voluntary agencies in the community.\n\nThe addiction team works with those men and women concerned with their addiction and wishing to address the problem. Regardless of whether or not he or she has stopped the addictive behaviour, the team combines a variety of counselling approaches to help the service user, including cognitive-behavioural treatment, coping skills training, relapse prevention and person-centred counselling.\n\nBethany Homemaker helps families and individuals on low incomes gain access to furniture, appliances, and other essential household items. Clients have the opportunity to choose the items they need. An extensive range of both new and second hand goods are made sure to be in the highest standard possible, and all electrical appliances are guaranteed. Working on a referral basis, homemaker can help anyone on a low income or those on benefits. All clients are interviewed by project staff to assess their needs. Homemaker partners with Freshstart to provide Starter Packs containing crockery, pots & pans and bedding for free to every client who needs them.\n\nThe Bethany Community Education Team runs a variety of groups and courses to provide opportunities for learning and to develop new skills and building confidence. Group attendees are encouraged to find their voice and realise their potential and role within the wider community.\n\nGroups include: Basic computer skills; Adult literacy; help with reading, writing & numerical skills; Art group, Bugle Magazine group. Development work includes groups designed specifically for Women and for Men.\n\nBethany runs several community-based resettlement projects under the Passing the Baton umbrella.\nThese projects engage volunteers from the local community to help people who have previously been homeless to resettle and put down roots by getting involved in the places they live. Passing the Baton members are encouraged to get involved in community social activities and supported by \"hit-squads\" to turn their houses into homes.\n\nCaring Christmas Trees is a social enterprise operated by Bethany Enterprises Ltd. Winner of the Institute of Fundraising Scotland’s Community Fundraising Award 2006, Caring Christmas Trees was set up by Scottish charity Bethany Christian Trust in 2005 to raise funds for their front-line services to help homeless people.\n\nThe Caring Christmas Trees scheme is a pre-order model, whereby customers select their preferred tree size, collection point and date for collection online. \n\nThe marketing campaign focuses on the concept that the cost of a Christmas tree is around the same cost of giving someone shelter, food and support. Customers are encouraged to think ethically about their Christmas tree buying and support Caring Christmas Trees because all the profits from tree sales go towards helping homeless people.\nCaring Christmas Trees are freshly cut, Scottish-grown, grade 1 standard, needle-last trees and are sourced from managed, sustainable forests operated by members of the British Christmas Tree Growers Association who ensure high environmental standards. \n\nTo keep costs down Caring Christmas Trees uses around 5000 volunteers to promote and distribute trees. This means the maximum profit from every tree helps provide services for homeless people.\n\nThe proceeds from Caring Christmas Tree sales help fund Bethany’s homelessness services in Scotland.\n\nThe Big Sleep-Out is an annual sponsored fundraising event to encourage the community to be active in combating homelessness across Scotland. The Sleep-Out raises awareness about Bethany's work with homeless and vulnerable people by giving participants an idea of what it is like to sleep rough for one night. First held in Edinburgh in 2002, the event takes place in Spring at various locations across Scotland.\n\n"}
{"id": "18905402", "url": "https://en.wikipedia.org/wiki?curid=18905402", "title": "Bill Compton (The Southern Vampire Mysteries)", "text": "Bill Compton (The Southern Vampire Mysteries)\n\nWilliam Erasmus \"Bill\" Compton is a fictional character from \"The Southern Vampire Mysteries/Sookie Stackhouse\" series by author Charlaine Harris. He is a vampire and is introduced in the first novel in the series, \"Dead Until Dark,\" and has appeared in all of the novels since. In the fifth season of the television adaption, Bill plays the role of the main antagonist.\n\nIn the book series, Bill was born on April 9, 1840. He lived in Bon Temps, Louisiana, and fought for the South during the Civil War. He was a married farmer with three children (there is inconsistency in the storyline, with Bill telling Sookie that he had five living children with his wife in \"Dead Until Dark\"). In 1865, he was made a vampire by Lorena, with whom he had a long and stormy relationship. Later in the book series, Bill discovers that he is related to the Bellefleur family in Bon Temps and secretly provides them with funds to aid in the repair of their ancestral home. The Bellefleurs - who do not know he is the source of the income - initially warm to him before coming to dislike him, but he is still able to assist Portia Bellefleur during her murder investigation in \"Living Dead in Dallas\".\n\nIn \"Living Dead in Dallas\" Bill's middle name is given as Thomas, rather than Erasmus.\n\nAt the end of the first novel, \"Dead Until Dark\", Bill is concerned about the power of Area Five sheriff Eric over him and Sookie. He decides to apply for that area's investigator position and he gets it. It was revealed that Bill works for Queen Sophie-Anne and she has sent him to Bon Temps to investigate Sookie Stackhouse's telepathic abilities.\n\nBill is also the creator of a valuable database in which all the vampires in North America are listed along with their significant information. Vampires from other areas of the world, such as Peru, have also provided information for this database. The database itself has been controversial because of potential security issues and is only available for purchase to other vampires. While most older vampires have good memories, Bill's ability to recall precise details, faces and conversations is reputed to be exceptional, even among other long-lived supernaturals.\n\n\n\n\n\n\n\nIn \"True Blood\", an HBO series based on the books, this character is played by Stephen Moyer, and first appeared in the BloodCopy.com video “Bon Temps Vampire speaks”, The storyline of Bill differs significantly from that in the novels. In season one, as a punishment for staking a vampire, Bill is forced to turn a teenage girl named Jessica Hamby into a vampire, thus creating his first and only progeny in the series. In season 3, Bill has his maker, Lorena, staked by Sookie. In desperate need of blood, Sookie slits her arm to feed him. However, he quickly overpowers her and nearly drains her to death before realizing his actions. After being kicked out into the sunlight by an enraged Tara, he learns that faerie blood temporarily makes vampires immune to the sun. In season 4, Bill is named the vampire king of Louisiana after staging a coup d'état against Queen Sophie Anne with the help of the Authority. In season 5, he and Eric Northman are arrested for the murder of Nan Flanigan. The two are brought to the Authority's underground base, where he eventually accepts the Book of Lilith. Bill is left competing with Salome for the vial of Lilith's blood, which he eventually wins by killing her. After drinking Lilith's blood, despite Eric and Sookie trying to convince him not to, Bill begins to bleed out and dies. Moments later, however, he is resurrected as a more powerful vampire, now having the strength of Lilith as well. While the extent of his new powers are not fully understood, it has been revealed in season 6 that he is impervious to staking, he no longer needs an invitation to enter a human home, and that he involuntarily has visions of the future. He is also able to command Warlow, Lilith's only known progeny and Sookie's love interest. After drinking Warlow's blood, Bill is able to walk in the sun. Using this power, he is able to kill Governor Truman Burrell and his guards in an effort to save Jessica. By the end of the series in Season 7, Bill has been infected with the Hep V virus. Instead of taking a cure, he is resigned to die, and eventually convinces Sookie to stake him as he lies in his Civil War grave, giving him the \"true death\".\n"}
{"id": "453080", "url": "https://en.wikipedia.org/wiki?curid=453080", "title": "Cedars-Sinai Medical Center", "text": "Cedars-Sinai Medical Center\n\nCedars-Sinai Medical Center is a non-profit, tertiary 958-bed hospital and multi-specialty academic health science center located in the Beverly Grove neighborhood of Los Angeles, California. Part of the Cedars-Sinai Health System, the hospital employs a staff of over 2,000 physicians and 10,000 employees. A team of 2,000 volunteers and more than 40 community groups support.\n\nCedars-Sinai focuses on biomedical research and technologically advanced medical education—based on an interdisciplinary collaboration between physicians and clinical researchers. The facility has research centers covering cardiovascular, genetics, gene therapy, gastroenterology, neuroscience, immunology, surgery, organ transplantation, stem cells, biomedical imaging and cancer—with more than 800 research projects underway (led by 230 principal investigators).\n\nCertified as a level I trauma center for adults and pediatrics, Cedars-Sinai trauma-related services range from prevention to rehabilitation and are provided in concert with the hospital's Department of Surgery. Cedars-Sinai is affiliated with the California Heart Center, University of Southern California and David Geffen School of Medicine at the University of California, Los Angeles (UCLA).\n\nAs of 2017, \"U.S. News & World Report\" ranked Cedars-Sinai #4 in the western United States, with number one being the UCSF Medical Center. Cedars-Sinai also earned national rankings in 12 adult specialties including #5 for gastroenterology, #9 in cardiology and heart surgery, #9 in orthopedics, #10 in urology, #12 in gynecology, #14 in diabetes and endocrinology, and #14 in neurology and neurosurgery. Located in the Harvey Morse Auditorium, Cedars-Sinai's patient care is depicted in the \"Jewish Contributions to Medicine\" mural. The heart transplantation program at Cedars-Sinai Medical Center has experienced unprecedented growth since 2010. Statistically, Cedars-Sinai currently performs more annual heart transplants than any other medical center in the world, having performed 95 heart transplants in 2012 and 87 in 2011.\n\nFounded and financed by businessman Kaspare Cohn, Cedars-Sinai was established as the Kaspare Cohn Hospital in 1902. At the time, Cohn donated a two-story Victorian home at 1441 Carroll Avenue in the Angeleno Heights neighborhood of Los Angeles to the Hebrew Benevolent Society to create the hospital as a memorial to his brother Samuel. The hospital had just 12 beds when it opened on September 21, 1902, and its services were initially free.\n\nFrom 1906 to 1910, Dr. Sarah Vasen, the first female doctor in Los Angeles, acted as superintendent. In 1910, the hospital relocated and expanded to Stephenson Avenue (now Whittier Boulevard), where it had 50 beds and a backhouse containing a 10-cot tubercular ward. It gradually transformed from a charity-based hospital to a general hospital and began to charge patients.\n\nThe hospital relocated again in 1930 to 4833 Fountain Avenue, where it was renamed Cedars of Lebanon after the religiously significant Lebanon Cedars, which were used to build King Solomon's Temple in Jerusalem in the Bible. Cedars of Lebanon could accommodate 279 patients.\n\nIn 1918, the Bikur Cholim Society opened a second Jewish hospital, the Bikur Cholim Hospice, when the Great Influenza Pandemic hit America. In 1921, the hospice relocated to an eight-bed facility in Boyle Heights and was renamed Bikur Cholim Hospital. In 1923 the Bikur Cholim Hospital became Mount Sinai Home for the Incurables.\n\nOn November 7, 1926, a newly named Mount Sinai Hospital moved to a 50-bed facility on Bonnie Beach Place. In 1950, Emma and Hyman Levine donated their property adjacent to Beverly Hills, and by 1955 the construction completed and Mount Sinai Hospital opened at 8700 Beverly Boulevard (now Cedars-Sinai Medical Center). Cedars of Lebanon and Mount Sinai Hospitals merged in 1961 to form Cedars-Sinai Medical Center.\n\nDonations from the Max Factor Family Foundation allowed the construction of the current main hospital building, which broke ground on November 5, 1972, and opened on April 3, 1976.\n\nIn 1994, the Cedars-Sinai Health System was established, comprising the Cedars-Sinai Medical Care Foundation, the Burns and Allen Research Institute and Cedars-Sinai Medical Center. The Burns and Allen Research Institute, named for George Burns and his wife, Gracie Allen, is located inside the Barbara and Marvin Davis Research Building. Opened in 1996, it houses biomedical research aimed at discovering genetic, molecular and immunological factors that trigger disease.\n\nIn 1994, the original building was damaged in the Northridge earthquake and demolished.\n\nIn 2006, Cedars-Sinai added the Saperstein Critical Care Tower with 150 ICU beds.\n\n, Cedars-Sinai served 54,947 inpatients and 350,405 outpatients, and there were 77,964 visits to the emergency room. Cedars-Sinai received high rankings in 11 of the 16 specialties, ranking in the top 10 for digestive disorders and in the top 25 for five other specialties as listed below.\n\nIn 2013, Cedars-Sinai opened its 800,000-square-foot Advanced Health Sciences Pavilion, which consists of eight stories of program space located over a six-story parking structure, on the eastern edge of its campus at the corner of San Vicente Boulevard and Gracie Allen Drive. Designed by architectural firm HOK, the Pavilion brings patient care and translational research together in one site. The Advanced Health Sciences Pavilion houses the Cedars-Sinai's neurosciences programs, the Cedars-Sinai Heart Institute and Regenerative Medicine Institute laboratories, as well as outpatient surgery suites, an imaging area and an education center.\nIn 2018, famous Marvel-creator Stan Lee dies at the Cedars-Sinai Medical Center\n\nCedars-Sinai ranked as follows in the nationwide U.S. News Best Hospitals 2013–14 report:\nCedars-Sinai ranked as follows in the 2009 Los Angeles area residents' \"Most Preferred Hospital for All Health Needs\" ranking:\nIn 2013, Cedars-Sinai Hospital was ranked in 12 specialties by \"U.S. News & World Report\".\n\nWorth Magazine selected Cedars-Sinai Heart Institute as one of the United States' Top 25 Hospitals for Cardiac Bypass Surgery.\n\nCedars-Sinai's Gynecologic Oncology Division was named among the nation's Top 10 Clinical Centers of Excellence by Contemporary OB/GYN in 2009.<ref name = \"Contemporary OB/GYN\"></ref>\n\n\n\n\n\n\n\n\n\nAccording to articles in the \"Los Angeles Times\" in 2009, Cedars-Sinai was under investigation for significant radiation overdoses of 206 patients during CT brain perfusion scans during an 18-month period. Since the initial investigation, it was found that GE sold several products to various medical centers with faulty radiation monitoring devices.\n\nState regulators had also found that Cedars-Sinai had placed the Quaid twins and others in immediate jeopardy by its improper handling of blood-thinning medication.\n\nIn 2011, Cedars-Sinai again created controversy by denying a liver transplant to medical marijuana patient Norman Smith. They removed Mr. Smith from a transplant waiting list for \"non-compliance of our substance abuse contract\", despite his own oncologist at Cedars-Sinai having recommended that he use the marijuana for his pain and chemotherapy. Dr. Steven D. Colquhoun, director of the Liver Transplant Program, said that the hospital \"must consider issues of substance abuse seriously\", but the transplant center did not seriously consider whether Mr. Smith was \"using\" marijuana versus \"abusing\" it. In 2012, Cedars-Sinai denied a liver transplant to a second patient, Toni Trujillo, after her Cedars-Sinai doctors knew and approved of her legal use of medical marijuana. In both cases, the patients acceded to the hospital's demand and stopped using medical marijuana, despite its therapeutic benefits for them, but were both sent six years back to the bottom of the transplant list. His death inspired Americans for Safe Access to lobby for the California Medical Cannabis Organ Transplant Act (AB 258), which was enacted in July 2015 to protect future patients from dying at the hands of medical establishments prejudiced against the legal use of medical cannabis.\n\nOn June 23, 2014, an unencrypted employee laptop was stolen from an employee's home. The laptop contained patient Social Security numbers and patient health data. On June 18 through June 24, 2013, six employees were terminated for inappropriately accessing 14 patient records around the time Kim Kardashian and Kanye West's daughter was born at the hospital.\n\nFirst developed by philanthropists Frederick and Marcia Weisman, Cedars-Sinai's modern and contemporary art collection dates to 1976 and includes more than 4,000 original paintings, sculptures, new media installations and limited-edition prints by the likes of Andy Warhol, Robert Rauschenberg, Richard Diebenkorn, Sam Francis, Claes Oldenburg, Willem de Kooning, Raymond Pettibon and Pablo Picasso. Ninety to 95 percent of the collection is on display at any given time. Nine large-scale works are located in courtyards, parking lots and public walkways throughout the approximately 30-acre campus. The collection consists entirely of gifts from donors, other institutions and occasionally the artists themselves.\n\n\n"}
{"id": "9350832", "url": "https://en.wikipedia.org/wiki?curid=9350832", "title": "Collaborative Study on the Genetics of Alcoholism", "text": "Collaborative Study on the Genetics of Alcoholism\n\nThe Collaborative Studies on the Genetics of Alcoholism (COGA) is an eleven-center research project in the United States designed to identify and understand the genetic basis of alcoholism. Research is conducted at University of Connecticut, Indiana University, University of Iowa, SUNY Downstate Medical Center at Brooklyn, Washington University in St. Louis, University of California at San Diego, Rutgers University, University of Texas Health Science Center at San Antonio, Virginia Commonwealth University, Icahn School of Medicine at Mount Sinai, and Howard University. \n\nHenri Begleiter, Ph.D. and Theodore Reich, M.D. were founding PI and Co-PI of COGA since its inception. Since 1991, COGA has interviewed more than 17,000 members of more than 2,200 families from around the United States, many of whom have been longitudinally assessed. Family members, including adults, children, and adolescents, have been carefully characterized across a variety of domains, including other alcohol and other substance-related phenotypes, co-occurring disorders (e.g., depression), electrophysiology, key precursor behavioral phenotypes (e.g., conduct disorder), and environmental risk factors (e.g., stress). This has provided us with a very rich phenotypic dataset to complement the large repository of cell lines and DNA for current and future studies. We have made this dataset widely available to advance the field: hundreds of researchers have worked with data generated as part of COGA through a variety of different mechanisms including data sharing through dbGaP and the Genetic Analysis Workshops, as COGA collaborators, through meta-analysis consortia, and as independent requestors for COGA samples and data. \n\nIn studying alcoholism, COGA hopes to find better ways of treating alcoholism and improving the lives of the millions of people who suffer from alcoholism. The COGA project has achieved national and international acclaim for its accomplishments, and numerous articles about the study have been published in scientific journals. This project is funded by the federal government and is one of the largest of its kind to be done in the United States.\n\nCOGA's aim is to identify the genes involved in alcoholism. There is a large body of twin-studies and adoption-studies that show that alcoholism is inherited. COGA is trying to determine more specifically how it is inherited and what genes are involved. There is no one gene that controls alcoholism, rather it is polygenetically controlled. \n\nThat means there are several different genes that influence risk factors involved in alcoholism. Examples of these risk factors include one's level of response to alcohol, a person's neuroelectrochemistry, and other psychiatric disorders, such as Antisocial Personality Disorder or clinical depression.\n\nCOGA researchers will interview subjects using the SSAGA, or Semi-Structured Assessment for the Genetics of Alcoholism, specifically created for the COGA project. Each subject is asked to participate in the SSAGA, with different versions for adolescents and for parents being interviewed about their children. The SSAGA is a polydiagnostic psychiatric interview that will cover any drug or alcohol use as well as any emotional and/or medical problems the subject may have experienced. The SSAGA is designed to use diagnostic criterion from the DSM III-R, DSM IV, and ICD-10. The SSAGA has been translated into nine languages and has been used in over 275 studies. The wide adoption of the SSAGA ensures that data from COGA families are highly compatible with data from other studies and allows COGA to participate in numerous consortia that use meta-analytic methods to combine data and results.\n\nCOGA requests that their subjects provide a blood sample, which is then processed and examined at Rutgers University. Genes not only carry information about eye and skin color, but also carry vulnerability to specific diseases, and probably cause a susceptibility to alcoholism as well. COGA is attempting to find those chromosomes involved in alcoholism and have located specific loci thought to be involved.\n\nThe brain wave is called an event-related potential (ERP) and is similar to an EEG. Brainwaves are monitored in a non-invasive procedure that involves placing a cap on the subject's head, similar to a bathing cap. This cap will monitor the brain's natural electrical activity and will record the brain's response when the subject is presented with various stimuli. Subjects respond to computer programs on a screen in front of them that flash various stimuli on the screen. \n\nAfter a significant event, or an important stimuli to which the subject is instructed to respond, there is an increase in electrical activity. COGA is investigating, for example, the P3 (or P300) peak of the ERP that occurs approximately 300 ms after the stimuli presentation. Varying amplitudes of the wave have different behavioral implications. The amplitude is fairly consistent throughout families and steady across time, suggesting an inherited component, as opposed to an environmental component.\n\nSubject's confidentiality is important to COGA. Subjects' names are never connected with the information they give. Rather, an ID number is assigned to them. COGA has also received the Confidentiality Certificate from the Department of Health and Human Services (DHHS) to protect researchers from being forced, even by court subpoena, to identify you. The Certificate adds protection only for the research information and does not mean the Secretary of DHHS approves or disapproves of the project.\n\n\n \n"}
{"id": "37949053", "url": "https://en.wikipedia.org/wiki?curid=37949053", "title": "EMMS Nazareth Hospital", "text": "EMMS Nazareth Hospital\n\nThe EMMS Nazareth Hospital, also known as Scottish Hospital and English Hospital, is the general hospital of the city of Nazareth, Israel. It was founded as a Christian mission by Dr. Kaloost Vartan and the Edinburgh Medical Missionary Society in 1861. The hospital now houses 147 beds, employs over 500 staff, and receives over 50,000 visits annually.\n\nThe Nazareth Hospital project was originally led by Dr. Pacradooni Kaloost Vartan. Born in Constantinople to an Armenian family in 1835, he attended an American Protestant School for Armenian Boys. During the Crimean War, he served as an interpreter for British forces. There, he was moved and inspired by the poor conditions of war, and by the care provided at the hospitals run by Florence Nightingale. Vartan moved to Edinburgh, Scotland to study medicine. His studies were funded by the newly founded Edinburgh Medical Missionary Society (EMMS). Established in 1841 by a group of doctors, the EMMS aimed to provide medical services to foreign countries, to \"circulate information on medical mission [sic], help other institutions engaged in the same work and assist as many missionary stations as their funds would permit.\"\nDr. Vartan was first funded by the Syrian Asylum Fund in 1861 to aid massacre victims in Beirut. After the Syrian Asylum Committee was dissolved, the EMMS recruited his services for a new mission in Nazareth starting in 1866.\nDuring this time, Palestine was governed by the Ottoman Empire. Lack of adequate social services and environmental factors contributed to poor public health. Citizens suffered from cholera, dysentery, malaria, and tuberculosis, and infant mortality was high. Palestine was en route for those participating in the Hajj, which made locals susceptible to a steady influx of foreign diseases. Traditional medicine included herbal treatments, cauterization, bloodletting, bonesetting, cupping, and leeching, among various tribal rituals. In 1861 the average life expectancy was 22 for males and 24 for females. Although Ottoman law required the provision of medical services, in reality, it did little more than employ a few municipal part-time workers. In the 1860s, Nazareth was home to nearly 5,000 citizens, but the nearest doctor or hospital was as far away as Damascus or Beirut. Ultimately, the only care was provided by charity.\n\nIn 1861, Dr. Vartan arrived in Nazareth to begin work. He joined Johannes or John Zeller, a missionary of the Church Mission Society. Zeller was an Anglican priest, and helped Dr. Vartan carry out surgeries with chloroform anesthesia. Initially, Dr. Vartan was confronted with a campaign against him by local healers, who saw his plan of establishment a threat to their practices. There was also a question of funding, as the committee that sent Dr. Vartan to Nazareth disbanded in 1864. Through these tribulations, Dr. Vartan continued to practice medicine. In 1865, he treated the son of the Mayor of Nazareth, and won the title, \"The Great Doctor.\"\n\nDr. Vartan first received patients in his own home. Dr. Burns Thompson, superintendent of the Coogate dispensary, supported him for two years after the original committee dissolved. In 1866, the EMMS officially adopted the Nazareth Hospital project. The society provided £100 for a few beds for surgical cases, which were installed in the upper room of Dr. Vartan’s house. As the only hospital near Nazareth, these accommodations were obviously insufficient, and Dr. Vartan soon received approval from the EMMS for the purchase for two adjacent houses. One would serve as a hospital, and the other his residence and dispensary. Only a year after the expansion, 36 inpatients were admitted, and 70–100 patients were treated daily.\nThis small dispensary was located on the east side of Nazareth. A small flow of clean water was available there, but rainwater was used more frequently. The hospital staff ran a boys’ Sunday school and soup kitchen, as well as an artists’ club, in addition to normal hospital operations. As a Christian mission, prayers were held every morning at 7am. Most nurses and staff were native to Ottoman Palestine, and had minimal English skills, which proved challenging to Dr. Vartan and his wife. Both patients and staff came from various religious denominations. Most were Christian, but Jews and Muslims worked for and were treated by the hospital, as well. An early survey of patients shows that in 1899, 5,747 Christians and 2,680 Muslims sought care in the hospital.\n\nIn 1879, a plot of land overlooking Nazareth was purchased for the establishment of a bigger hospital. Before anything could be done, Dr. Vartan was tasked with getting permission, a \"firman\", from the Ottoman authorities to establish a dispensary. The Turkish government put a stop to his efforts to build after a misleading promise to provide a permit. No more progress was made until 1895, when the EMMS directors approved a new site west of Nazareth. Dr. Vartan died in 1908, before any construction could begin.\n\nDr. Frederick John Scrimgeour succeeded Dr. Vartan as the head doctor of the mission, along with two European nurses. The new hospital was almost complete, when World War I broke out through Europe and the Middle East, and British physicians were ordered to leave Palestine. The new hospital was requisitioned by the military, stripped, and used as stables. While Dr. Scrimgeour was stuck in Egypt and unable to return; two nurses from the hospital operated in his rented residence downtown.\n\nThe hospital was restored in 1919, and normal operations slowly resumed. In that year, the hospital was recognized by the British government’s mandate as a hospital for training nurses. In 1924, the hospital was officially inaugurated and, by 1935, it had electricity and a sanitary annex for tuberculosis patients. The mission began extending services beyond elementary care. For example, by 1924, plans were adopted for infant welfare clinics in a nearby village. In 1933, an east wing was added to the hospital, which contained the \"Abercrombie Ward\". It soon took on the name, the \"Hospital on the Hill\". By this time, Dr. Scrimgeour had retired, and Dr. William D. Bathgate and Sister Mary Parkinson had taken charge of the hospital.\n\nIn May 1948, the state of Israel was established. Dr. Runa MacKay, who joined the mission a few years later, reported that hospital operations were generally uninhibited by the political upheaval, and the majority Arab population of Nazareth remained. However, an influx of refugees created more demand on the workers. At this time, the hospital still only had 100 beds.\nThis pressure resulted in the recruitment of more doctors. In addition to Dr. MacKay, Drs. Hester and Bernath joined the mission in May 1956. Their work inspired major changes in the hospital. Dr. Tester joined the hospital staff in 1952. Under his supervision, the hospital installed a modern laundry system, a new outpatient facility, nurses' home and chapel, and new accommodations for staff from 1957 to 1964.\n\nDr. Bernath arrived at the mission in 1956, and was appointed as administrator of the hospital in 1969. He worked to continue updating the hospital, and oversaw the development of a new maternity clinic, kitchen, and most importantly, the recruitment of specialists to provide dialysis and physiotherapy. Other significant improvements include the addition of the \"Tower Block,” an intensive care unit, in 1976. A unit for renal dialysis was opened in October 1981, and the first computer was installed the following year. This period saw another increase in hospital staff. Many of the newly hired doctors came from the Arab community, in particular, Dr. Nakhle Bishara, who became the medical director of the hospital in 1981. In that same year, Israeli healthcare reform named the Nazareth Hospital as the official district hospital for the Nazareth area. This, in addition to a strike by Israeli doctors in 1983, caused a massive influx in patients, leaving the hospital once again inadequate.\n\nIn 1984, a delegation from Edinburgh was sent to review the hospital's operations and establish a plan to accommodate the renewed demand for services. Mr. Fred Aitken was appointed as head the implementation of the plan, which aimed to increase the standards of the facilities, rather than the capacity of the hospital, and had an operating budget of 3.5 million pounds.\nIn 1988 Dr. Robert W. Martin succeeded Dr. Bernath as superintendent. In 1995, further healthcare reforms were instituted which guaranteed basic healthcare to all residents of Israel. The hospital recruited help, namely from Mr. G. Anthony Holt, who became interim general administrator, and Mr. Derek Thompson, a hospital administrator from the United Kingdom, to make further transitions to a large-scale medical facility. In 2001, The EMMS split into two separate organizations, EMMS International and EMMS Nazareth. Thompson served as the first CEO of EMMS Nazareth, followed in 2007 Mr. Joseph Maine. In that same year, Mr. Elia Abdo became the General Director of the Nazareth Hospital. In 2010, Dr. Bishara Bisharat became the head of the hospital. In 2009, the title \"EMMS Nazareth\" was replaced with \"The Nazareth Trust.\" After almost 150 years, the Nazareth Hospital and nursing school are still operational.\n\nToday, the hospital continues to expand its practices, led by the head of the hospital, Dr. Fahid Hakeem, Dr Kemal Kem (MD MBBS DCH DRCOG DFFP MRCGP) and a faithful management team. For example, in October 2012, the hospital opened a pregnancy and childbirth complex in the nearby village of Umm Al-Fahm. Nazareth is now home to 250,000 people, and the hospital is equipped with 147 beds and over 500 staff. The hospital is now also home to biochemistry and hematology labs, as well as a blood bank. The hospital has various departments, including childbirth and gynecology, orthopedic surgery, general surgery, urology, cosmetic surgery, emergency, radiology, cardiology, preterm birth, anesthesia, and research. Although it remains Christian in principle, the hospital neither hires nor treats patients preferentially.\n\n"}
{"id": "30389901", "url": "https://en.wikipedia.org/wiki?curid=30389901", "title": "Edouard Wyss-Dunant", "text": "Edouard Wyss-Dunant\n\nEdouard Wyss-Dunant (17 April 1897 – 30 April 1983) was a Swiss physician and alpinist. He had a distinguished career in medicine, both in his own country and abroad. He published a number of treatises in his professional capacity and was the author of several mountaineering books. He is best known for his leadership of the Swiss Expedition to Everest of 1952.\n\nWyss-Dunant had a Swiss-German father and a Vaudoise mother. He spent his childhood in Alsace, where his father managed a chemical works. He went on to study medicine in Geneva. After receiving a doctorate in radiology in Zurich he set himself up as a practitioner in radiology in Bern and became a member of the \"Berner Akademische Alpen-Klub\" (Bern Academical Alpine Club). Later, he moved into a practice in Geneva, where he met his future wife, Lucrece, and there, with the exception of a period spent in North Africa, he made his home for the rest of his life.\n\nDuring his stay in Bern, Wyss-Dunant climbed all the main summits of the Bernese Oberland: among the classic routes in his record at that time were the Mittellegi Ridge of the Eiger, the North Ridge of the Mönch and many good ascents in the chain of the Grosses Engelhorn. In his early twenties, he traversed the Dent d'Hérens from the Tiefenmatten-Joch to the Col du Lion and made a double traverse, in two days, of both the Matterhorn and the Dent d'Hérens. He also achieved a solo traverse of the Matterhorn and climbed it by its formidable Furggen- with Alexander Taugwalder. He also climbed extensively in the Mont Blanc massif with Marcel Kurz.\n\nWyss-Dunant also participated in several expeditions further afield: in Mexico (1936), East Africa (1937), Greenland (1938), Tibesti (Chad) (1946) and in the Himalaya (1947 and 1952). The accounts of these exploits are recorded in his books: \"Appels des Sommets\", \"Au dela des Cîmes\", \"Sur les Hauts Plateaux Mexicains\", \"Mes Ascensions en Afrique\", \"Mirages Groenlandais\" and \"Forets et Cîmes Himalayennes\".\n\nThe climax of his career as an alpinist was his selection by the Swiss Foundation for Alpine Exploration as leader of the Geneva-based Swiss Expedition to Everest in the spring of 1952.\n\nWyss-Dunant also served as president of the Swiss Alpine Club and the Union Internationale des Associations d'Alpinisme. In tribute to these services and to his lifelong record as a climber he was made an honorary member of the Alpine Club in 1963.\n\nEdouard Wyss-Dunant, was appointed leader of this expedition. All the expedition’s participants were from Geneva, they almost all belonged to the exclusive «L'Androsace» climbing club and knew each other very well. The city and Canton of Geneva provided moral and financial support for the expedition, and the University of Geneva provided the scientific contingent.\n\nThe mountaineering task that this team had set itself was primarily exploring the access to the South Col, the conquest of the labyrinthine Khumbu Icefall, and possibly the advance to the South Col. The team never even considered an attempt at an ascent of Everest.\n\nRaymond Lambert and Sherpa Tenzing Norgay were able to reach a height of about on the southeast ridge, setting a new climbing altitude record. Tenzing's experience was useful when he was hired to be part of the British expedition in 1953 during which he summited with Edmund Hillary.\n\nThe results of this first Swiss Everest expedition are remarkable, and exceeded even the most optimistic expectations. At the first attempt, they had opened up a new route to Everest, and had reached an extraordinary height on the south-western ridge in difficult conditions. In the opinion of the extremely critical Marcel Kurz, this expedition could almost be compared to a victory. It paved the way for further successes by other expeditions.\n\nAmong his research works, Edouard Wyss-Dunant coined the term \"Death Zone\" in an article about acclimatisation published in the journal of the Swiss Foundation for Alpine Research.\n"}
{"id": "21499583", "url": "https://en.wikipedia.org/wiki?curid=21499583", "title": "Epidemiology of cancer", "text": "Epidemiology of cancer\n\nThe epidemiology of cancer is the study of the factors affecting cancer, as a way to infer possible trends and causes. The study of cancer epidemiology uses epidemiological methods to find the cause of cancer and to identify and develop improved treatments.\n\nThis area of study must contend with problems of lead time bias and length time bias. Lead time bias is the concept that early diagnosis may artificially inflate the survival statistics of a cancer, without really improving the natural history of the disease. Length bias is the concept that slower growing, more indolent tumors are more likely to be diagnosed by screening tests, but improvements in diagnosing more cases of indolent cancer may not translate into better patient outcomes after the implementation of screening programs. A related concern is overdiagnosis, the tendency of screening tests to diagnose diseases that may not actually impact the patient's longevity. This problem especially applies to prostate cancer and PSA screening.\n\nSome cancer researchers have argued that negative cancer clinical trials lack sufficient statistical power to discover a benefit to treatment. This may be due to fewer patients enrolled in the study than originally planned.\n\nState and regional cancer registries are organizations that abstract clinical data about cancer from patient medical records. These institutions provide information to state and national public health groups to help track trends in cancer diagnosis and treatment. One of the largest and most important cancer registries is Surveillance Epidemiology and End Results (SEER), administered by the US Federal government.\n\nHealth information privacy concerns have led to the restricted use of cancer registry data in the United States Department of Veterans Affairs and other institutions. The American Cancer Society predicts that approximately 1,690,000 new cancer cases will be diagnosed and 577,000 Americans will ultimately die of cancer in 2012.\n\nObservational epidemiological studies that show associations between risk factors and specific cancers mostly serve to generate hypotheses about potential interventions that could reduce cancer incidence or morbidity. Randomized controlled trials then test whether hypotheses generated by epidemiological studies and laboratory research actually result in reduced cancer incidence and mortality. In many cases, findings from observational epidemiological studies are not confirmed by randomized controlled trials.\n\nThe most significant risk factor is age. According to cancer researcher Robert A. Weinberg, \"If we lived long enough, sooner or later we all would get cancer.\" Essentially all of the increase in cancer rates between prehistoric times and people who died in England between 1901 and 1905 is due to increased lifespans.\n\nAlthough the age-related increase in cancer risk is well-documented, the age-related patterns of cancer are complex. Some types of cancer, like testicular cancer, have early-life incidence peaks, for reasons unknown. Besides, the rate of age-related increase in cancer incidence varies between cancer types with, for instance, prostate cancer incidence accelerating much faster than brain cancer.\n\nOver a third of cancer deaths worldwide (and about 75-80% of cancers in the United States) are due to potentially modifiable risk factors. The leading modifiable risk factors worldwide are:\n\nMen with cancer are twice as likely as women to have a modifiable risk factor for their disease.\n\nOther lifestyle and environmental factors known to affect cancer risk (either beneficially or detrimentally) include the use of exogenous hormones (e.g., hormone replacement therapy causes breast cancer), exposure to ionizing radiation and ultraviolet radiation, and certain occupational and chemical exposures.\n\nEvery year, at least 200,000 people die worldwide from cancer related to their workplace. Millions of workers run the risk of developing cancers such as pleural and peritoneal mesothelioma from inhaling asbestos fibers, or leukemia from exposure to benzene at their workplaces. Currently, most cancer deaths caused by occupational risk factors occur in the developed world. It is estimated that approximately 20,000 cancer deaths and 40,000 new cases of cancer each year in the U.S. are attributable to occupation.\n\nIn the U.S. cancer is second only to cardiovascular disease as the leading cause of death; in the UK it is the leading cause of death. In many developing countries cancer incidence (insofar as this can be measured) appears much lower, most likely because of the higher death rates due to infectious disease or injury. With the increased control over malaria and tuberculosis in some Third World countries, incidence of cancer is expected to rise; in the Eastern Mediterranean region, for example, cancer incidence is expected to increase by 100% to 180% in the next 15 years due to increases in life expectancy, an increasing proportion of elderly people, and the successful control of childhood disease. This is termed the epidemiologic transition in epidemiological terminology.\n\nCancer epidemiology closely mirrors risk factor spread in various countries. Hepatocellular carcinoma (liver cancer) is rare in the West but is the main cancer in China and neighbouring countries, most likely due to the endemic presence of hepatitis B and aflatoxin in that population. Similarly, with tobacco smoking becoming more common in various Third World countries, lung cancer incidence has increased in a parallel fashion.\n\nAccording to the National Cancer Registry Programme of the India Council of Medical Research (ICMR), more than 1300 Indians die every day due to cancer. Between 2012 and 2014, the mortality rate due to cancer increased by approximately 6%. In 2012, there were 478,180 deaths out of 2,934,314 cases reported. In 2013 there were 465,169 deaths out of 3,016,628 cases. In 2014, 491,598 people died in out of 2,820,179 cases.\nAccording to the Population Cancer Registry of Indian Council of Medical Research, the incidence and mortality of cancer is highest in the north-eastern region of the country. Breast cancer is the most common, and stomach cancer is the leading cause of death by cancer for the population as a whole. Breast cancer and lung cancer kill the most women and men respectively.\n\nIn Canada, as of 2007, cancer is the number one cause of death, contributing to 29.6% of all deaths in the country. The second highest cause of death is cardiovascular diseases resulting in 21.5% of deaths. As of 2011, prostate cancer was the most common form of cancer among males (about 28% of all new cases) and breast cancer the most common in females (also about 28% of all new cases).\n\nThe leading cause of death in both males and females is lung cancer, which contributes to 26.8% of all cancer deaths. Statistics indicate that between the ages of 20 and 50 years, the incidence rate of cancer is higher amongst women whereas after 50 years of age, the incidence rate increases in men. Predictions by the Canadian Cancer Society indicate that with time, there will be an increase in the rates of incidence of cancer for both males and females. Cancer will thus continue to be a persistent issue in years to come.\n\nIn the United States, cancer is responsible for 25% of all deaths with 30% of these from lung cancer. The most commonly occurring cancer in men is prostate cancer (about 25% of new cases) and in women is breast cancer (also about 25%). Cancer can occur in children and adolescents, but it is uncommon (about 150 cases per million in the U.S.), with leukemia the most common. In the first year of life the incidence is about 230 cases per million in the U.S., with the most common being neuroblastoma. Data from 2004-2008 in the United States indicates that the overall age-adjusted incidence of cancer was approximately 460 per 100,000 men and women per year.\n\nCancer is responsible for about 25% of all deaths in the U.S., and is a major public health problem in many parts of the world. The statistics below are estimates for the U.S. in 2008, and may vary substantially in other countries. They exclude basal and squamous cell skin cancers, and carcinoma in situ in locations other than the urinary bladder. As seen, breast/prostate cancer, lung cancer and colorectal cancer are responsible for approximately half of cancer incidence. The same applies for cancer mortality, but with lung cancer replacing breast/prostate cancer as the main cause.\n\nIn 2016, an estimated 1,685,210 new cases of cancer will be diagnosed in the United States and 595,690 people will die from the disease.\n\nIn the developed world, one in three people will develop cancer during their lifetimes. If all cancer patients survived and cancer occurred randomly, the normal lifetime odds of developing a second primary cancer (not the first cancer spreading to a new site) would be one in nine. However, cancer survivors have an increased risk of developing a second primary cancer, and the odds are about two in nine. About half of these second primaries can be attributed to the normal one-in-nine risk associated with random chance.\n\nThe increased risk is believed to be primarily due to the same risk factors that produced the first cancer, such as the person's genetic profile, alcohol and tobacco use, obesity, and environmental exposures, and partly due, in some cases, to the treatment for the first cancer, which might have included mutagenic chemotherapeutic drugs or radiation. Cancer survivors may also be more likely to comply with recommended screening, and thus may be more likely than average to detect cancers.\n\nChildhood cancer and cancer in adolescents is rare (about 150 cases per million yearly in the US). Leukemia (usually acute lymphoblastic leukemia) is the most common cancer in children aged 1–14 in the U.S., followed by the central nervous system cancers, neuroblastoma, Wilms' tumor, and non-Hodgkin's lymphoma. Statistics from the SEER program of the US NCI demonstrate that childhood cancers increased 19% between 1975 and 1990, mainly due to an increased incidence in acute leukemia. Since 1990, incidence rates have decreased.\n\nThe age of peak incidence of cancer in children occurs during the first year of life, in infants. The average annual incidence in the United States, 1975–1995, was 233 per million infants. Several estimates of incidence exist. According to SEER, in the United States:\n\nTeratoma (a germ cell tumor) often is cited as the most common tumor in this age group, but most teratomas are surgically removed while still benign, hence not necessarily cancer. Prior to the widespread routine use of prenatal ultrasound examinations, the incidence of sacrococcygeal teratomas diagnosed at birth was 25 to 29 per million births.\n\nFemale and male infants have essentially the same overall cancer incidence rates, a notable difference compared to older children.\n\nWhite infants have higher cancer rates than black infants. Leukemias accounted for a substantial proportion of this difference: the average annual rate for white infants (48.7 per million) was 66% higher than for\nblack infants (29.4 per million).\n\nRelative survival for infants is very good for neuroblastoma, Wilms' tumor and retinoblastoma, and fairly good (80%) for leukemia, but not for most other types of cancer.\n\n\nGeneral:\n\n"}
{"id": "23875799", "url": "https://en.wikipedia.org/wiki?curid=23875799", "title": "Ernesto Lomasti", "text": "Ernesto Lomasti\n\nErnesto Luigi Lomasti (29 October 1959 – 12 June 1979) was an Italian mountaineer and alpino skier. He is regarded as one of the Italian pioneers of climbing as a sport. He died during training in the Valle d'Aosta, Arnad, in the local alpini training place called \"la gruviera\" (the holey cheese), he fell apparently being struck by lightning. Ironically, he is also remembered for techniques on the Machaby pillar, in Arnaud, on May 1979. The pillar today is named Lomasti pillar.\n\nErnesto Lomasti was born in Udine but spent his early life in Pontebba and Tarvisio, where he attended the local school. During childhood, he contracted pertussis and his parents and uncle used to bring him up on high hills.\nOn 29 June 1973, he left middle school and passed the exam, earning his father a climb on the Jôf di Montasio, Montasio along with a local mountain guide.\nFollowing an accident at age three, he severely damaged his hearing in his right ear. This disability would severely impair his climbing career because of having difficulties hearing climbing instructions. The problem was solved on 29 August 1977 when he decided to undergo surgery. He served under the Alpini corps attending the SMALP (alpini military school) until his sudden death.\n\nLomasti is remembered for a series of ascents of extraordinary difficulty\nHis life as a climber spanned for three years, primarily on Julian Alps from 1976 to 1979, with an impressive list of great gradients. He is remembered especially for Cima Grande della Scala, north anti-peak, on 3 September 1978. All his ascents were conquered using hard boots, as he apparently never had the chance to use light shoes.\n\nDespite his short career, there are several places that are named after Lomasti.\n\n\"\"[...] per me Lomasti e' stato il piu' grande alpinista friulano di sempre...\"\n\"\"[...] to me Lomasti has been the greatest friulan mountaineer ever...\"\nIgnazio Piussi\n\n"}
{"id": "2657094", "url": "https://en.wikipedia.org/wiki?curid=2657094", "title": "Field hunter", "text": "Field hunter\n\nA field hunter, or a fox hunter, is a type of horse used in the hunt field for fox hunting and stag hunting. \n\nThe field hunter may be of any breed, but should possess stamina, a level head, and bravery. The horse should have a safe jump, so as not to get caught on any of the solid obstacles found in the hunt field. The type of terrain is also an important factor: wide open, flat land is generally best for horses of a Thoroughbred type, while rockier, more unforgiving land may be best suited by a draft-cross or tougher breed.\n\nField hunter trials are regularly held to test these horses, and have become a popular form of equestrian competition. Often the horses are judged over several days of fox hunting, with the best of the group performing in the \"handy hunter\" class. The handy hunter class may ask for the horse and rider pair to trot a log, open and close a gate while mounted, jump several fences, and for the rider to dismount and remount. The horse is judged on its manners, way of going, as well as its suitability as a hunter.\n\nIn some ways, the field hunter is more similar to a good cross-country horse seen in eventing than a show-ring hunter, as it must gallop and jump over varied terrain, jump ditches, coops, up and down banks, and occasionally go through water.\n\nUnlike the field hunter, the horse known in the US as a show hunter and in the UK as a working hunter performs in a ring, usually over a course of 8-10 fences. The judging of the American show hunter is based on the requirements of a horse in the hunt field, focusing on the horse's manners, movement, jumping form, rhythm, and smoothness around the course. Show hunters in the US are usually warmblood or Thoroughbred types. They do not have to have the bravery required of the field hunter, nor do they travel over the same type of terrain, as the field or arena is usually fairly level. Although the fences in a show hunter course are usually \"natural\" poles and standards, as opposed to the brightly colored fences seen in show jumping, the show hunter course does not include rock walls, ditches, or banks that might be seen in the hunt field. The British working hunter is not required to jump obstacles exactly like those met in the hunting field, although a water tray is sometimes used to simulate a ditch, and natural dips in the ground, banks etc. are often incorporated into the course in order to make it more challenging.\n\nHorses of field hunter type may also compete in certain race completions such as point-to-pointing. In the United Kingdom, with the exception of Hunt Members races, all the horses that compete in point-to-point must be registered by Weatherbys - in the General Stud Book or Non-Thoroughbred Register. Horses and jockeys must have qualified with a pack of foxhounds, harriers, bloodhounds or draghounds by \"riding to hounds\". Horses must be ridden to hounds on four or more occasions during the hunting season that immediately precedes the point-to-point season, and belong to a member, subscriber or farmer of a recognised pack.\n"}
{"id": "13225048", "url": "https://en.wikipedia.org/wiki?curid=13225048", "title": "Fitness boot camp", "text": "Fitness boot camp\n\nA fitness boot camp is a type of group physical training program conducted by gyms, personal trainers, and former military personnel. These programs are designed to build strength and fitness through a variety of types of exercise. Indoor and outdoor boot camp workouts became popular in the United States in the late 1990s. Fitness boot camps as outdoor group fitness classes grew in popularity in the 2000s. These originated independently in Australia, the United States, the United Kingdom, and Canada. \n\nMilitary forces have emphasized fitness since ancient times. In the United States, military-style exercise and \"correctional boot camps\" have been used to rehabilitate civilian and military criminals since the nineteenth century. The term \"boot\" refers to US Navy and Marine recruits in the Spanish–American War (1898) who wore leggings called boots. These recruits were trained in \"boot\" camps. \n\nWritten by Bill Orban, the Royal Canadian Air Force Exercise Plans published in 1961 and United States Air Force Colonel Kenneth Cooper's books \"\"Aerobics\" (1968) and mass-market version \"The New Aerobics\"\" (1979) helped to launch modern fitness culture. In the United States, a \"Boot Camp Workout\" audio compact cassette recorded by a Marine Corps drill instructor was released in 1984. Indoor \"boot camp workouts\" at health clubs around the U.S. were popular in 1998.\n\nFitness boot camps as outdoor fitness group classes developed independently in Australia in 1991 (Original Bootcamp), the United States in the 1990s, the United Kingdom in 1999 (British Military Fitness), and Canada in 2001 (The Original Boot Camp). Outdoor group fitness classes such as these have been growing in popularity ever since and have become a distinct commercial market within the fitness industry.\n\nIn the United States, the television series Boot Camp aired in 2001. In the United Kingdom, reality television program Celebrity Fit Club, which involved boot camp-style fitness, aired from 2002–2006. The series was brought over to the United States in 2005 as Celebrity Fit Club which aired until 2010.\n\nBoot camp training often commences with dynamic stretching and running, followed by a wide variety of interval training, including lifting weights/objects, pulling rubber TRX straps, pushups/situps, plyometrics, and various types of intense explosive routines. Sessions usually finish with yoga stretching. Many other exercises using weights and/or body weight, similar to CrossFit routines, are used to lose body fat, increase cardiovascular efficiency, increase strength, and help people get into a routine of regular exercise. Many programs offer nutrition advice as well. It is called \"boot camp\" because it trains groups of people, may be outdoors, and may or may not be similar to military basic training.\n\nThe term \"boot camp\" is currently used in the fitness industry to describe group fitness classes that promote fat loss, camaraderie, and team effort. They are designed to push people a little bit further than they would normally push themselves in the gym alone. Boot Camps are sometimes organized outdoors in parks using bodyweight exercises like push ups, squats, suspension training and burpees, interspersed with running and competitive games. The idea is that everyone involved works at their own pace as they team up and work towards one goal, either in pairs, small teams of three or four, or even two teams head on.\n\nBoot camps provide social support for those taking part. This provides a different environment for those exercisers who get bored in a gym and so find it hard to develop a habit of exercise. Participants make friends and socialize as they exercise, although how strict the trainers or drill instructors in charge can be will depend on the company running the camp. Members of fitness boot camps are usually tested for fitness on the first day and then retested at the end of the camp, which usually runs for between 4 and 6 weeks.\n\nFitness boot camps are often based on the military style of training, although that has started changing over the last few years. An advantage of a boot camp is that the large group dynamic will often help motivate the participants. A growing trend in fitness boot camps are the indoor locations which prove to be climate proof and provide a better workout environment for the members. Additionally, some camps include extracurricular fitness activities off-site.\n\nThere are many other benefits of a fitness boot camp, which includes mental health. It has long been known that regular aerobic exercise can help to reduce high blood pressure, hypertension and combat stress. Part of this is due to the release of endorphins, which act as a mood elevator.\n\nSome \"Holistic Bootcamps\" provide the mental coaching required to sustain motivation after people leave the camp. Themed fitness bootcamps often consist of the use of one particular training implement to the exclusion of others. Kettlebells are the preferred tool for kettlebell fitness bootcamps run by RKC instructors and TRX suspension trainers are the preferred tools for TRX instructors. Boxing themed fitness bootcamps often use heavy bags. The use of themes varies widely between fitness bootcamps and their instructors according to the preferences between the instructor and the needs and likes of the clientele.\n\n\n"}
{"id": "1196268", "url": "https://en.wikipedia.org/wiki?curid=1196268", "title": "Glomerulonephritis", "text": "Glomerulonephritis\n\nGlomerulonephritis (GN), also known as glomerular nephritis, is a term used to refer to several kidney diseases (usually affecting both kidneys). Many of the diseases are characterised by inflammation either of the glomeruli or of the small blood vessels in the kidneys, hence the name, but not all diseases necessarily have an inflammatory component.\n\nAs it is not strictly a single disease, its presentation depends on the specific disease entity: it may present with isolated hematuria and/or proteinuria (blood or protein in the urine); or as a nephrotic syndrome, a nephritic syndrome, acute kidney injury, or chronic kidney disease.\n\nThey are categorized into several different pathological patterns, which are broadly grouped into non-proliferative or proliferative types. Diagnosing the pattern of GN is important because the outcome and treatment differs in different types. Primary causes are intrinsic to the kidney. Secondary causes are associated with certain infections (bacterial, viral or parasitic pathogens), drugs, systemic disorders (SLE, vasculitis), or diabetes.\n\nGlomerulonephritis refers to an inflammation of the glomerulus, which is the unit involved in filtration in the kidney. This inflammation typically results in one or both of the nephrotic or nephritic syndromes.\n\nThe nephrotic syndrome is characterised by the finding of edema in a person with increased protein in the urine and decreased protein in the blood, with increased fat in the blood. Inflammation that affects the cells surrounding the glomerulus, podocytes, increases the permeability to proteins, resulting in an increase in excreted proteins. When the amount of proteins excreted in the urine exceeds the liver's ability to compensate, fewer proteins are detected in the blood - in particular albumin, which makes up the majority of circulating proteins. With decreased proteins in the blood, there is a decrease in the oncotic pressure of the blood. This results in edema, as the oncotic pressure in tissue remains the same. Although decreased intravascular oncotic (i.e. osmotic) pressure partially explains the patient's edema, more recent studies have shown that extensive sodium retention in the distal nephron (collecting duct) is the predominant cause of water retention and edema in the nephrotic syndrome. This is worsened by the secretion of the hormone aldosterone by the adrenal gland, which is secreted in response to the decrease in circulating blood and causes sodium and water retention. Hyperlipidemia is thought to be a result of the increased activity of the liver.\n\nThe nephritic syndrome is characterised by blood in the urine (especially Red blood cell casts with dysmorphic red blood cells) and a decrease in the amount of urine in the presence of hypertension. In this syndrome, inflammatory damage to cells lining the glomerulus are thought to result in destruction of the epithelial barrier, leading to blood being found in the urine. At the same time, reactive changes, e.g. proliferation of mesangial cells, may result in a decrease in kidney blood flow, resulting in a decrease in the production of urine. The renin–angiotensin system may be subsequently activated, because of the decrease in perfusion of juxtaglomerular apparatus, which may result in hypertension.\n\nThis is characterised by forms of glomerulonephritis in which the number of cells is not changed. These forms usually result in the nephrotic syndrome. Causes include:\n\nMinimal change disease is characterised as a cause of nephrotic syndrome without visible changes in the glomerulus on microscopy. Minimal change disease typically presents with edema, an increase in proteins passed from urine and decrease in blood protein levels, and an increase in circulating lipids (i.e., nephrotic syndrome) and is the most common cause of the nephrotic syndrome in children. Although no changes may be visible by light microscopy, changes on electron microscopy within the glomerules may show a fusion of the foot processes of the podocytes (cells lining the basement membrane of the capillaries of glomerulus). It is typically managed with corticosteroids and does not progress to chronic kidney disease.\n\nFocal segmental glomerulosclerosis is characterised by a sclerosis of segments of some glomerules. It is likely to present as a nephrotic syndrome. This form of glomerulonephritis may be associated with conditions such as HIV and heroin abuse, or inherited as Alport syndrome. The cause of about 20–30% of focal-segmental glomerulosclerosis is unknown. On microscopy, affected glomerules may show an increase in hyalin, a pink and homogenous material, fat cells, an increase in the mesangial matrix and collagen. Treatment may involve corticosteroids, but up to half of people with focal segmental glomerulonephritis continue to have progressive deterioration of kidney function, ending in kidney failure.\n\nMembranous glomerulonephritis may cause either nephrotic or a nephritic picture. About two-thirds are associated with auto-antibodies to phospholipase A2 receptor, but other associations include cancers of the lung and bowel, infections such as hepatitis B and malaria, drugs including penicillamine, and connective tissue diseases such as systemic lupus erythematosus. Individuals with cerebral shunts are at risk of developing shunt nephritis, which frequently produces MGN.\n\nMicroscopically, MGN is characterized by a thickened glomerular basement membrane without a hyperproliferation of the glomerular cells. Immunofluorescence demonstrates diffuse granular uptake of IgG. The basement membrane may completely surround the granular deposits, forming a \"spike and dome\" pattern. Tubules also display the symptoms of a typical Type III hypersensitivity reaction, which causes the endothelial cells to proliferate, which can be seen under a light microscope with a PAS stain.\n\nPrognosis follows the rule of thirds: one-third remain with MGN indefinitely, one-third remit, and one-third progress to end-stage kidney failure. As the glomerulonephritis progresses, the tubules of the kidney become infected, leading to atrophy and hyalinisation. The kidney appears to shrink. Treatment with corticosteroids is attempted if the disease progresses.\n\nIn extremely rare cases, the disease has been known to run in families, usually passed down through the females. This condition, similarly, is called Familial Membranous Glomerulonephritis. There have only been about nine documented cases in the world.\n\nThin basement membrane disease is an autosomal dominant inherited disease characterized by thin glomerular basement membranes on electron microscopy. It is a benign condition that causes persistent microscopic hematuria. This also may cause proteinuria which is usually mild and overall prognosis is excellent. \n\nProliferative glomerulonephritis is characterised by an increased number of cells in the glomerulus. These forms usually present with a triad of blood in the urine, decreased urine production, and hypertension, the nephritic syndrome. These forms usually progress to end-stage kidney failure (ESKF) over weeks to years (depending on type).\n\nIgA nephropathy, also known as \"Berger's disease\", is the most common type of glomerulonephritis, and generally presents with isolated visible or occult hematuria, occasionally combined with low grade proteinuria, and rarely causes a nephritic syndrome characterised by protein in the urine, and visible blood in the urine. IgA nephropathy is classically described as a self-resolving form in young adults several days after a respiratory infection. It is characterised by deposits of IgA in the space between glomerular capillaries.\n\nHenoch–Schönlein purpura refers to a form of IgA nephropathy, typically affecting children, characterised by a rash of small bruises affecting the buttocks and lower legs, with abdominal pain.\n\nPost-infectious glomerulonephritis can occur after essentially any infection, but classically occurs after infection with the bacteria \"Streptococcus pyogenes\". It typically occurs 1–4 weeks after a pharyngeal infection with this bacterium, and is likely to present with malaise, a slight fever, nausea and a mild nephritic syndrome of moderately increased blood pressure, gross haematuria, and smoky-brown urine. Circulating immune complexes that deposit in the glomerules may lead to an inflammatory reaction. \n\nDiagnosis may be made on clinical findings or through antistreptolysin O antibodies found in the blood. A biopsy is seldom done, and the disease is likely to self-resolve in children in 1–4 weeks, with a poorer prognosis if adults are affected. \n\nMembranoproliferative GN (MPGN), also known as \"mesangiocapillary glomerulonephritis,\" is characterised by an increase in the number of cells in the glomerulus, and alterations in the glomerular basement membrane. These forms present with the nephritic syndrome, hypocomplementemia, and have a poor prognosis. Two primary subtypes exist:\n\nRapidly progressive glomerulonephritis, also known as \"crescentic GN\", is characterised by a rapid, progressive deterioration in kidney function. People with rapidly progressive glomerulonephritis may present with a nephritic syndrome. In management, steroid therapy is sometimes used, although the prognosis remains poor. Three main subtypes are recognised:\n\nHistopathologically, the majority of glomeruli present \"crescents\". Formation of crescents is initiated by passage of fibrin into the Bowman space as a result of increased permeability of glomerular basement membrane. Fibrin stimulates the proliferation of endothelial cells of Bowman capsule, and an influx of monocytes. Rapid growing and fibrosis of crescents compresses the capillary loops and decreases the Bowman space, which leads to kidney failure within weeks or months. \n\nSome forms of glomerulonephritis are diagnosed clinically, based on findings on history and examination. Other tests may include: \n\n"}
{"id": "6458802", "url": "https://en.wikipedia.org/wiki?curid=6458802", "title": "Gunning S. Bedford", "text": "Gunning S. Bedford\n\nGunning S. Bedford (1806 - 5 September 1870) was a medical writer, teacher and founder of the United States' first obstetrical clinic for those too poor to pay a doctor's fee.\n\nDr. Bedford graduated in 1825 at Mount Saint Mary's University (then Mount Saint Mary's College), Emmitsburg, Maryland, and took his medical degree from Rutgers College. He spent two years studying abroad and in 1833 became professor of obstetrics in Charleston Medical College. After this he became a professor in the Albany Medical College.\n\nHe later founded the University Medical College in which he established an obstetrical clinic for those too poor to afford a doctor which was the first of in the United States. He retired from teaching for health reasons in 1862 and he died in 1870. His funeral panegyric was preached by Archbishop John McCloskey a fellow student at Mount St. Mary's.\n\nTwo books written by him, \"Diseases of Women\" and \"Practice of Obstetrics\" went through a number of editions, were translated into French and German and adopted as textbooks in American schools.\n\nHis son Gunning S. Bedford (1837–1893) was Assistant New York County District Attorney under A. Oakey Hall from 1865 to 1868, a New York City judge from 1869 to 1878, and again Asst. D.A. under Randolph B. Martine, John R. Fellows and De Lancey Nicoll from 1885 until his death.\n\nHis uncle Gunning Bedford was one of the framers of the United States Constitution; another uncle, also named Gunning Bedford, was an aide-de-camp to General George Washington and later became the Governor of Delaware.\n\n"}
{"id": "420922", "url": "https://en.wikipedia.org/wiki?curid=420922", "title": "Haing S. Ngor", "text": "Haing S. Ngor\n\nDr. Haing Somnang Ngor (Khmer: ហាំង សំណាង ង៉ោ; ; March 22, 1940 – February 25, 1996) was a Cambodian American gynecologist, obstetrician, actor and author. He is best remembered for winning the Academy Award for Best Supporting Actor in 1985 for his debut performance in the film \"The Killing Fields\" (1984), in which he portrayed Cambodian journalist and refugee Dith Pran.\n\nNgor is the first (and to date, only) actor of Asian descent to win an Academy Award for Best Supporting Actor. He survived three terms in Cambodian prison camps, using his medical knowledge to keep himself alive by eating beetles, termites, and scorpions; he eventually crawled between Khmer Rouge and Vietnamese lines to safety in a Red Cross refugee camp. His mother was Khmer and his father was of Chinese Teochew descent. Ngor and Harold Russell are the only two non-professional actors to win an Academy Award in an acting category.\n\nNgor continued acting for the rest of his life, most notably in \"My Life\" (1993), portraying spiritual healer Mr. Ho opposite Michael Keaton and Nicole Kidman.\n\nBorn in Samrong Young, Bati district, Takeo province, Cambodia, Ngor trained as a surgeon and gynecologist. He was practicing in the capital, Phnom Penh, in 1975 when Pol Pot's Khmer Rouge seized control of the country and proclaimed it Democratic Kampuchea. He was compelled to conceal his education, medical skills, and even the fact that he wore glasses to avoid the new regime's intense hostility to intellectuals and professionals. He was expelled from Phnom Penh along with the bulk of its two million inhabitants as part of the Khmer Rouge's \"Year Zero\" social experiment and imprisoned in a concentration camp along with his wife, My-Huoy, who subsequently died giving birth. Although a gynecologist, he was unable to treat his wife, who required a Caesarean section, because he would have been exposed, and both he and his wife (as well as the child) would very probably have been killed. After the fall of the Khmer Rouge in 1979, Ngor worked as a doctor in a refugee camp in Thailand and left with his niece for the United States on August 30, 1980. In America, Ngor was unable to resume his medical practice, and he did not remarry.\n\nIn 1988, he wrote \"Haing Ngor: A Cambodian Odyssey\", describing his life under the Khmer Rouge in Cambodia. In the second edition of \"Survival in the Killing Fields,\" Roger Warner, Ngor's co-author, adds an epilogue telling the story of Ngor's life after winning the Academy Award.\n\nThe Dr. Haing S. Ngor Foundation was founded in his honor in 1997 to assist in raising funds for Cambodian aid. As part of his humanitarian efforts, Ngor built an elementary school and operated a small sawmill that provided jobs and an income for local families. Ngor's niece, Sophia Ngor Demetri, who testified at the trial of his murderers and with whom he arrived in the U.S., is the current president of the Foundation.\n\nNgor, despite having no previous acting experience, was cast as Dith Pran in \"The Killing Fields\" (1984), a role for which he won (among many honors) the Academy Award for Best Supporting Actor, becoming the first (and only) Asian to win Best Supporting Actor in debut performance, the second Asian actor to ever win an Oscar, and one of very few amateur actors to win an Oscar. Ngor was not initially interested in the role of Dith Pran, but interviews with the filmmakers changed his mind, as he recalled that he promised his late wife to tell Cambodia's story to the world. After appearing in \"The Killing Fields\" he told \"People\" magazine, \"I wanted to show the world how deep starvation is in Cambodia, how many people die under communist regime. My heart is satisfied. I have done something perfect.\"\n\nNgor went on to appear in various other onscreen projects, most memorably in Oliver Stone's \"Heaven & Earth\" (1993) and the \"Vanishing Son\" miniseries. He also appeared in the Hong Kong film \"Eastern Condors\" (1987), which was directed by and starred Sammo Hung.\n\nHe also appeared in a supporting role in the 1989 Vietnam War drama \"The Iron Triangle\". He guest-starred in a two-episode storyline on the acclaimed series \"China Beach\" (episodes \"How to Stay Alive in Vietnam 1 & 2\") as a wounded Cambodian POW who befriends Colleen McMurphy while under her care. He also guest-starred in an episode of \"Miami Vice\" called \"The Savage / Duty and Honor\".\n\nNext to \"The Killing Fields\", Ngor's most prominent feature film role was in \"My Life\" (1993), the directorial debut of Academy Award-winning screenwriter Bruce Joel Rubin. Ngor portrayed a spiritual healer, Mr. Ho, who provides guidance for protagonist Bob Jones (Michael Keaton) and his wife Gail (Nicole Kidman) after Bob is diagnosed with terminal cancer, months before the birth of his and Gail's first child.\n\nThe Dr. Haing S. Ngor Foundation was organized in 1990 by Ngor and Jack Ong. The two actors met in 1989 while filming \"The Iron Triangle\" and soon after, Pastor Ong's church (Venice Christian Community in Venice, California) launched Project Cambodia to raise funds to care for orphans and help rebuild the devastated country's infrastructure. Project Cambodia was the original foundation for The Dr. Haing S. Ngor Foundation, which was incorporated in 1997 after Ngor's homicide (Feb. 25, 1996) as a 501 (C) (3) charitable organization. The goals of the Foundation include preserving the legacy of Ngor's accomplishments and human rights endeavors as well as the promotion of Cambodia's history and culture through education, activism and the arts. Ngor's niece, Sophia Ngor Demetri, who testified at the trial of his murderers and whom he brought to the U.S., is the current president of the foundation; Ong serves as executive director.\n\nOn February 25, 1996, Ngor was shot dead outside his home in Chinatown, in downtown Los Angeles, California. Many Cambodians claimed that they had a stake in his estate, with one woman claiming that he had married her after coming to the United States. Most of Ngor's Cambodian assets went to his brother, Chan Sarun, while his American assets were used up in legal fees staving off claims to his estate. He was buried at Rose Hills Memorial Park in Whittier, California.\n\nCharged with the murder were three reputed members of the \"Oriental Lazy Boyz\" street gang, who had prior arrests for snatching purses and jewelry. They were tried together in the Superior Court of Los Angeles, though their cases were heard by three separate juries. Prosecutors argued that they killed Ngor because, after handing over his gold Rolex watch willingly, he refused to give them a locket that contained a photo of his deceased wife, My-Huoy. Defense attorneys suggested that the murder was a politically motivated killing carried out by sympathizers of the Khmer Rouge but offered no evidence to support this theory.\nKang Kek Iew, a former Khmer Rouge official on trial in Cambodia, claimed in November 2009 that Ngor was murdered on Pol Pot's orders, but U.S. investigators did not find him credible.\n\nSome criticized the theory that Ngor was killed in a bungled robbery, pointing to $2,900 in cash that had been left behind and the fact that the thieves had not rifled his pockets. Why the thieves would have demanded his locket has never been answered; Ngor typically wore the locket next to his skin under his clothing, so it would not have been in plain sight. , the locket has not been recovered.\n\nAll three were found guilty on April 16, 1998, the same day that Pol Pot's death was confirmed in Cambodia. Tak Sun Tan was sentenced to 56 years to life; Indra Lim to 26 years to life; and Jason Chan to life without parole. In 2004, the U.S. District Court for the Central District of California granted Tak Sun Tan's \"habeas corpus\" petition, finding that prosecutors had manipulated the jury's sympathy by presenting false evidence. This decision was reversed, and the conviction was ultimately upheld by the United States Court of Appeals for the Ninth Circuit in July 2005.\n\nAfter the release of \"The Killing Fields\", Ngor had told a \"New York Times\" reporter, \"If I die from now on, OK! This film will go on for a hundred years.\"\n\nDith Pran, whom Ngor portrayed in \"The Killing Fields\", said of Ngor's death, \"He is like a twin with me. He is like a co-messenger and right now I am alone.\"\n\nIn Season 7, episode 12 of \"The Simpsons\", \"Team Homer\", Homer reveals that he stole Ngor's Best Supporting Actor Oscar and replaced the name on the plaque with his own. About a month after \"Team Homer\" aired in 1996, Ngor was murdered just outside his home in Los Angeles. In subsequent reruns, animators changed the Oscar in question to that of Don Ameche out of respect for Ngor, and to avoid the implication that Homer killed him for his Oscar.\n\nIn Season 1, episode 4 of \"The Critic\", Miserable, Ngor is listed as one of the protagonist Jay Sherman's many enemies and someone who would have a motive to kidnap him. A brief flashback then shows Sherman criticizing Ngor's acting abilities by saying that Ngor \"Should have gone to the acting fields.\"\n\n\n"}
{"id": "51601432", "url": "https://en.wikipedia.org/wiki?curid=51601432", "title": "Hassan Tarighat Monfared", "text": "Hassan Tarighat Monfared\n\nMohammad-Hassan Tarighat Monfared (, born 1951) is an Iranian physician and conservative politician who served as the Minister of Health and Medical Education under President Mahmoud Ahmadinejad from March to August 2013.\n"}
{"id": "57562843", "url": "https://en.wikipedia.org/wiki?curid=57562843", "title": "Healthcare in the Isle of Man", "text": "Healthcare in the Isle of Man\n\nHealth and social care on the Isle of Man is the responsibility of the Department of Health and Social Care (Isle of Man). Healthcare is free for residents and visitors from the UK and there is a reciprocal health agreement with the UK. For several years it has required a supplementary vote to balance its budget at the end of each year \n\nIn 2013 about £178.4 million was spent on healthcare. £138 million came from the Manx government and about £40 million from income National Insurance contributions, prescription charges and private patient fees. About £80 million was spent on Noble's hospital. In March 2018 Sir Jonathan Michael was asked to conduct a review of the service because of concern that current funding levels are \"unsustainable\". The Council of Ministers estimated that funding would have to be increased by 23% - an additional £60 million by 2022/23.\n\nThe 14 General practices have a list system. Patients have to register with a GP. Temporary residents from the UK can use the service without charge. The practices on the island have an average of 1900 patients per GP. A digital facility for patients to make GP appointments and view their own health information online was launched in September 2017. The Manx Emergency Doctor Service is available when the practices are closed, from 6pm to 8am and at weekends and bank holidays.\n\nDentistry services are available on a similar basis to those in the UK, with charging on a banded basis and exemptions for children, students, pensioners, pregnant women and people in receipt of Income Support, Income Based Job Seekers Allowance or Employed Persons Allowance, war disablement pensioners and blind people.\n\nAnnual eye tests are provided free to Isle of Man residents. NHS vouchers are available towards the cost of spectacles.\n\nManx prescription charges have been set at £3.85 per item since September 2010, less than half the charge in England. People get free prescriptions who have chronic conditions, as well as people who are: under 16 years old, under 19 years old and in school full-time, pregnant, on public assistance, prisoners, on a pension and under 75 years old, or over 75 years old.\n\nIn April 2018 there was an announcement that NHS dentists on the island were at full capacity and that it could take years for new patients to be registered.\n\nThere is one hospital on the island, Noble’s Hospital, with 314 beds, giving about 4 beds per 1000 residents, around the European average, but considerably higher than in the UK. Tertiary services are provided by the English NHS.\n\nThe Isle of Man Ambulance Service is based at Cronk Coar on the Noble's Hospital site. It has 9 Accident and Emergency Vehicles, 4 Patient Transport vehicles and rapid response vehicles.\n\nUnder the Termination of Pregnancy Act 1995 abortions can only be carried out if a pregnancy is the result of rape or because of mental health concerns but proposals have been made to alter the law. There have been no prosecutions under the act and women from the island travel to the UK for abortions which they have to pay for.\n\nBefore 1948 there was a Mental Hospital Board which was responsible for Ballamona Hospital under the Poor Relief Acts and three voluntary hospitals: Ramsey Cottage Hospital, Noble’s Hospital and the Jane Crookall Maternity Home, each of which had its own Management Committee, on the island. There was a Hospitals Contributory Scheme. White Hoe Hospital was run by the Corporation of Douglas. There were proposals, by the Corrin Trustees who had collected funds, to establish a hospital in Peel but they were halted by the outbreak of war. Domiciliary services were provided by District Nurses, employed by the twelve District Nursing Associations. There were also two Infant Welfare Health Visitors and seven Infant Welfare Clinics.\n\nThe Isle of Man Medical Society had begun preparing for the establishment of the NHS in 1946 and asking the Manx government to establish a process. The two existing government bodies mostly closely involved were the Local Government Board and the Health Insurance and Pensions Board - which became the Board of Social Services in 1947 - submitted a report in April 1947.\nThe free health service started on the island on 5 July 1948, just as in the United Kingdom, but without any legislation and a good deal of confusion. 5 July on the island is, coincidently, Tynwald day, a public holiday. It was clear that the Board would have to be collecting National Health Insurance contributions from that date to ensure reciprocity with the United Kingdom.\n\nThe Mental Hospital Board was renamed the Health Services Board under the Health Services Board Act which was passed on 15 June 1948 and took on responsibility for the Cronk Ruagh Sanatorium and White Hoe Isolation Hospital. The voluntary hospitals Management Committees continued, but were now overseen by and responsible to politicians - something they did not welcome. One-third of their members were now appointed by the Board, to be responsible to the Board, and their accounts and activities were subject to monthly scrutiny. They were assimilated in 1963.\n\nThe Isle of Man Health Services Act was not passed until 10 August 1948. It was agreed that the term \"people of the Isle of Man\" could and should be interpreted as including anybody physically present on the Island at the time, so that visitors - mostly from the UK - would be covered. In return the English NHS agreed that Manx patients could be sent, as necessary, for free treatment in English hospitals, and for other treatment not available on the Island.\n\n32 General practitioners, 40 chemists, 16 dentists and 10 opticians joined the service at its inception. There were 28 specialist hospital doctors: 9 full-time, 12 part-time, and 7 visiting from England. For the first nine months of operation to 31 March 1949 the costs came to £197,200. The estimate for the first full year to 31 March 1950, was £454,668. By 1954 the Board was under pressure to reduce its expenditure by 20%. The Board responded by insisting that this would require reductions in the service which would have to be determined by the Government. The National Health Service Act 1950, widened the Board's powers to introduce charges, and in November 1950 a charge of 6d. per prescription form was introduced. This caused all but six of the chemists to withdraw their services for a fortnight. In December 1950 the Board introduced a range of exemptions from charges. Charges for dental treatment were introduced in 1950 and, in 1952, charges for spectacles at 10/- a lens, and the whole cost of an approved frame.\n\nThere was considerable administrative support from the English NHS. The Manchester Prescription Pricing Bureau dealt with the invoicing of prescribed medicine. A Prescribing Committee, was established of whose six members four were doctors and one a chemist. Charges for prescriptions were increased in April 1953 to 1/- per form.\n\nIn 1954 the Board took responsibility for the Welfare Food Scheme. Orange juice, cod liver oil, and vitamin tablets were distributed through the Infant Welfare Clinics, of which there were then ten. They were free to all children under five, and to expectant and nursing mothers in necessitous circumstances. In the year to March, 1956 25,884 bottles of orange juice, 4,776 bottles of cod liver oil, and 594 packets of vitamin tablets were distributed. It also ran the Cheap Milk Scheme. From 1951 this was subject to a means test. In 1955/6, this was a household income not exceeding £5 a week, with the addition of 6/- for each dependent member. There were 176 beneficiaries, including 6 nursing or expectant mothers. A Home Help Scheme was also established and by 1959 there were 25 Home Helps.\n\nThe School Dental Service provided free treatment for children of school age, and they were originally not eligible for free treatment from other dentists except in an emergency.\n\nHealth in the Isle of Man\n"}
{"id": "16730844", "url": "https://en.wikipedia.org/wiki?curid=16730844", "title": "Hogenakkal Integrated Drinking Water Project", "text": "Hogenakkal Integrated Drinking Water Project\n\nHogenakkal Integrated Drinking Water Project is a fluorosis mitigation drinking water project being undertaken at Hogenakkal, Dharmapuri district, state of Tamil Nadu, India. It is scheduled to be executed by Tamil Nadu Water Supply and Drainage Board (TWAD), with funding from Japan Bank for International Cooperation (JBIC) using Tamil Nadu's share of Cauvery river water. The project aims to supply safe drinking water to drought prone & fluorosis affected Dharmapuri and Krishnagiri districts of Tamil Nadu.\n\nDharmapuri and Krishnagiri districts of Tamil Nadu are drought prone and had been the cause of high political debates and riots. Although the Kaveri river enters the state at Biligundulu in Dharmapuri district, it does not contribute to irrigation or drinking water purposes there. With 10 of the 29 constituent districts in Tamil Nadu affected by fluorosis, Dharmapuri district has the highest concentration of endemic fluoride in the State.\n\nThe total cost of this entire fluorosis mitigation project is Rs. 13.34 billion. 1.4 tmc feet of water is to be utilised for the Hogenakkal drinking water project. It will be from Tamil Nadu's share of Cauvery water, thereby placing no extra demand on Karnataka.\n\nThe Tamil Nadu government has received a no objection certificate from the central government for the Hogenakkal drinking water scheme. A similar agreement with the J. H. Patel led government of Karnataka in 1998 was based on the premise that both states won't obstruct drinking water schemes from Cauvery as long as the water drawn for such a project is sourced from the respective state's share of Cauvery water, whose proceedings have been recorded.A copy of the letter given to the then Tamil Nadu Government Chief Secretary from the Union Ministry of Water Resources can be found here .\n\nThe project is expected to cover 6,755 households in three municipal areas, 17 panchayats and 18 town panchayats, benefiting about three million people. Drinking water will be pumped to a master balancing reservoir at Madam, about 11 km from Hogenakkal. After treatment, water will be pumped for 145 km to cover areas such as Palakkodu, Marandahalli and Hosur in Tamil Nadu. Remaining areas in Krishnagiri and Dharmapuri districts will be covered by taking advantage of altitude gradient.\n\nIn 1998, Karnataka agreed to abide by the conditions imposed by the Union Water Resources Ministry if Tamil Nadu withdrew its objections to the Cauvery water being used to augment supply to Bangalore, according to the minutes of a meeting convened by the Union Secretary (Water Resources) and attended by officers of the Cauvery basin States on drinking water supply schemes of Karnataka and Tamil Nadu on June 29, 1998.\n\nChief Minister of Tamil Nadu, M. Karunanidhi laid the foundation stone for the project on February 26, 2008.\n\nYeddiurappa, Chief Minister of Karnataka claimed that the proposed drinking water project site in Hogenakkal is situated in Karnataka and thus sparked controversy. Soon after Yeddiurappa's claim other political parties in Karnataka followed suit. The agitations that followed in Karnataka targeted the Tamils until the Assembly elections got over there. As a result of the protests, Government of Tamil Nadu announced on April 5, 2008 that it will wait till a new government takes charge in Karnataka. M Karunanidhi, the chief Minister of Tamil Nadu reportedly said, \"the project will not be shelved\".\n\nAs a first step toward the implementation of the project, the unit office of the Hogenakkal water supply project was opened at Oddapatti near Dharmapuri on 17 June 2008.\n\nOn Wednesday (29/05/2013) Chief Minister J Jayalalithaa launches the Hogenakkal Drinking Water Scheme through videoconferencing from Chennai.\n\n"}
{"id": "1910198", "url": "https://en.wikipedia.org/wiki?curid=1910198", "title": "Holding tank dump station", "text": "Holding tank dump station\n\nA dump station is a place where raw sewage may be entered into a sanitary sewer system in a safe and responsible way. Dump stations are often used by owners of motorhomes, campervans, recreational vehicles or boats that are equipped with toilet facilities and a sewage holding tank, also known as a blackwater holding tank. The holding tank can be safely emptied at a dump station. Greywater holding tanks can also be emptied at a dump station.\n\nDump stations are often located at campgrounds, RV parks, truck stops, highway rest areas, recreation vehicle dealerships, marinas and other places that are frequented by recreational vehicles and boats. Dump station owners may charge a fee for use, or offer them as a free public service.\n"}
{"id": "870890", "url": "https://en.wikipedia.org/wiki?curid=870890", "title": "Hospital medicine", "text": "Hospital medicine\n\nHospital medicine in the United States is the medical specialty concerned with the care of acutely ill hospitalized patients. Physicians whose primary professional focus is caring for hospitalized patients only while they are in the hospital are called hospitalists. This type of medical practice has extended beyond the United States into Australia and Canada. The vast majority of physicians who refer to themselves as hospitalists focus their practice upon hospitalized patients but do not necessarily have board certification in hospital medicine. \n\nThe term \"hospitalist\" was first coined by Robert Wachter and Lee Goldman in a 1996 \"New England Journal of Medicine\" article. The scope of hospital medicine includes acute patient care, teaching, research, and executive leadership related to the delivery of hospital-based care. Hospital medicine, like emergency medicine, is a specialty organized around a site of care (the hospital), rather than an organ (like cardiology), a disease (like oncology), or a patient’s age (like pediatrics). The emergence of hospital medicine has both similarities with and differences from acute medicine in the United Kingdom, reflecting health system differences.\n\nHospitalists are physicians with a Doctor of Medicine (M.D.), Doctor of Osteopathic Medicine (D.O.), or a Bachelor of Medicine/Bachelor of Surgery (MBBS/MBChB) degree. Contrary to popular belief, most hospitalists practicing in hospitals in the United States lack board certification in hospital medicine. While it was commonly believed that any residency program with a heavy inpatient component provided good hospitalist training, studies have found that general residency training is inadequate because common hospitalist problems like neurology, hospice and palliative care, consultative medicine, and quality improvement tend to be glossed over. To address this, residency programs are starting to develop hospitalist tracks with more tailored education. Several universities have also started fellowship programs specifically geared toward hospital medicine.\n\nAccording to the State of Hospital Medicine Survey by the Medical Group Management Association and the Society of Hospital Medicine, 89.60% of hospitalists specialize in general internal medicine, 5.5% in a pediatrics subspecialty, 3.7% in family practice and 1.2% in internal medicine pediatrics. Data from the survey also reported that 53.5% of hospitalists are employed by hospitals/integrated delivery system and 25.3% are employed by independent hospitalists groups.\n\nAccording to recent data, there are more than 50,000 hospitalists practicing in approximately 75% of U.S. hospitals, including all highly ranked academic medical centers.\n\nIn Australia, Hospitalists are career hospital doctors; they are generalist medical practitioners whose principal focus is the provision of clinical care to patients in hospitals; they are typically beyond the internship-residency phase of their career, but have decidedly chosen as a conscious career choice not to partake in vocational-specialist training to acquire fellowship specialist qualification. Whilst not specialists, these clinicians are nonetheless experienced in their years of medical practice, and depending on their scope of practice, they typically work with a reasonable degree of independence and autonomy under the auspices of their specialist colleagues and supervisors. Hospitalists form a demographically small but important workforce of doctors in hospitals across Australia where on-site specialist coverage is otherwise unavailable. \n\nHospitalists are typically employed in a variety of public and private hospital settings on a contractual or salaried basis. Dependent on their place of employment and duties, the responsibilities and remuneration of non-specialist hospitalists are usually comparable to somewhere between registrars and consultants. Despite the common trend for clinicians to specialise nowadays, non-specialist hospitalist clinicians have an important role in fulfilling shortages in the medical workforce, especially when specialist coverage or accessibility is unavailable and where there is an area-of-need or after-hours or on-site medical care is required. These clinicians and employed across Australia in a variety of environments which include Medical & Surgical Wards, Intensive Care Units and Emergency Departments. Nonetheless, these clinicians work closely and continually consult with the relevant attending specialists on-call; that is, final responsibility and care for the patient ultimately still rests with the attending specialist.\n\nThey are also known as: Career Medical Officers (CMO), Senior Medical Officers (SMO) and Multi-skilled Medical Officers (MMO). \n\nHospitalits are represented by the Australian Medical Association (AMA), Australasian Society of Career Medical Officers (ASCMO) and Australian Salaried Medical Officers Federation (AMSOF). Despite being non-specialist clinicians, they are still required to meet continuing professional development requirements and frequently attend courses facilitated by these organisations and hospitals to keep their practice and skillets up-to-date alongside their specialist registered colleagues.\n\nIn Canada, there are currently no official residency programs specializing in hospital medicine. Nevertheless, some universities, such as McGill University in Montreal, have come up with family medicine enhanced skills programs focused on hospital medicine. This program, which is available to practicing physicians and family medicine residents, has a duration of six or twelve months. The main goal behind the program is to prepare medical doctors with training in family practice to assume shared care roles with other specialists, such as cardiologists, neurologists, and nephrologists, in a hospital setting. Moreover, the program prepares family physicians by giving them a set of skills required for caring for their complicated hospitalized patients.\n\nHospital medicine is a relatively new phenomenon in American medicine and as such is the fastest growing specialty in the history of medicine. Almost unheard of a generation ago, this type of practice arose from three powerful shifts in medical practice:\n\nIn addition to patient care duties, hospitalists are often involved in developing and managing aspects of hospital operations such as inpatient flow and quality improvement. The formation of hospitalist training tracks in residency programs has been driven in part by the need to educate future hospitalists about business and operational aspects of medicine, as these topics are not covered in traditional residencies.\n\nAs a relatively new specialty, only recently has certification for specialty experience and training for hospital medicine been offered. The American Board of Hospital Medicine (ABHM), a Member Board of the American Board of Physician Specialties (ABPS), was founded in 2009. The ABHM was North America’s first board of certification devoted exclusively to hospital medicine. In September 2009, the American Board of Internal Medicine (ABIM) created a program that provides general internists practicing in hospital settings the opportunity to maintain Internal Medicine Certification with a Focused Practice in Hospital Medicine (FPHM).\n\nResearch shows that hospitalists reduce the length of stay, treatment costs and improve the overall efficiency of care for hospitalized patients. Hospitalists are leaders on several quality improvement initiatives in key areas including transitions of care, co-management of patients, reducing hospital acquired diseases and optimizing the care of patients.\n\nThe number of available hospitalists positions grew exponentially from 2006 to 2010 but has since then leveled off. However, the job market still remained very active with some hospitals maintaining permanent openings for capable hospitalists. Salaries are generally very competitive, averaging almost $230,000 per year for adult hospitalists. Hospitalists who are willing to work night shifts only (nocturnists) are generally compensated higher than their day shift peers.\n\nThough hospital medicine is a young field, there have been attempts at further division of labor in the field. \n\nA nocturnist is a hospitalist who typically covers the twelve-hour shift at night and admits patients as well as receives calls about already admitted patients.\n\nA proceduralist is generally defined as a hospitalist who primarily does procedures in the hospital such as central venous catheter insertions, lumbar punctures, and paracenteses.\n\nA neurohospitalist cares for hospitalized patients with or at risk for neurological problems.\n\nA surgicalist is a surgeon who specializes and focuses on surgical care in the hospital setting.\n\nThe following are other commonly used monikers:\n\nAn admitologist or admitter is a hospitalist who only admits patients and does not round on the already admitted ones, or discharge the admitted patients.\n\nA rounder is a hospitalist who only sees the already admitted patients.\n\n\n\n"}
{"id": "30669787", "url": "https://en.wikipedia.org/wiki?curid=30669787", "title": "Hypertrichosis cubiti", "text": "Hypertrichosis cubiti\n\nHypertrichosis cubiti (also known as \"hairy elbow syndrome\") is a cutaneous condition characterized by multiple terminal hairs on both elbows in children.\n\nOne known cause of hypertrichosis cubiti is Wiedemann-Steiner syndrome.\n"}
{"id": "19769058", "url": "https://en.wikipedia.org/wiki?curid=19769058", "title": "Industrial Hygiene Foundation", "text": "Industrial Hygiene Foundation\n\nThe Industrial Hygiene Foundation of America, originally named the Air Hygiene Foundation and also called the Industrial Health Foundation, is a business trade organization concerned with occupational health in industrial businesses. It was founded in 1935 by the Mellon Institute of Industrial Research in response to the Hawks Nest Tunnel Disaster and is based in Pittsburgh, Pennsylvania. In 1941 the organization changed its name from \"Air Hygiene Foundation\" to \"Industrial Hygiene Foundation.\"\n\nThe Industrial Health Foundation hired ENVIRON to conduct tests of hexavalent chromium. It has been generally associated with defending corporations from environmental and safety regulations.\n\n"}
{"id": "49633248", "url": "https://en.wikipedia.org/wiki?curid=49633248", "title": "Institut de microbiologie et des maladies infectieuses", "text": "Institut de microbiologie et des maladies infectieuses\n\nThe Institut de microbiologie et des maladies infectieuses is a unit of the French national medical research institute INSERM. It is a member of GLOPID-R.\n\n"}
{"id": "24640055", "url": "https://en.wikipedia.org/wiki?curid=24640055", "title": "List of rampage killers (workplace killings)", "text": "List of rampage killers (workplace killings)\n\nThe first part of this section of the list of rampage killers contains those mass murders where the perpetrators predominantly targeted their (former) co-workers, while the second part focuses on cases where soldiers willfully killed their own comrades.\n\nA rampage killer has been defined as follows:\n\nThis list should contain every case with at least one of the following features:\n\nAll abbreviations used in the tables are explained below.\n\nW – A basic description of the weapons used in the murders\n"}
{"id": "31676015", "url": "https://en.wikipedia.org/wiki?curid=31676015", "title": "Lost to follow-up", "text": "Lost to follow-up\n\nIn the clinical research trial industry, lost to follow-up refers to patients who at one point in time were actively participating in a clinical research trial, but have become lost (either by error in a computer tracking system or by being unreachable) at the point of follow-up in the trial. These patients can become lost for many reasons. Without properly informing the investigator associated with the clinical trial, they may have opted to withdraw from the clinical trial, moved away from the particular study site during the clinical trial, or become ill and unable to communicate or are deceased.\n\nPatients who become lost to follow-up during a clinical research trial result in many negative effects on the outcome of the trial and on the pharmaceutical company sponsoring the clinical research trial. Patients who are lost-to-follow-up lead to incomplete study results, which in turn can put a bias on the result of the study as well as a bias on the investigational study medication. A lack of complete results leads to intensified FDA scrutiny of the particular study drug, as well as the pharmaceutical company sponsoring the clinical research study. Biased study outcomes also lead to issues of HIPAA standards and compliance.\n\nAside from partial study data and regulatory issues, patients that are not retained due to being lost-to-follow-up can lead to problems with the progression of any clinical research study. Low rates of retention and high rates of patient’s lost-to-follow-up have many side-effects, including longer clinical research trial periods and more monetary expenditures because extra resources may need to be dedicated to the recruitment efforts.\n\nCurrently there are no standards or guidelines that express the process or methods that can be used to attempt to reach patients who have become lost to follow-up. Only recently have government institutions like the FDA taken action over the recovery of or communication with patients lost-to-follow-up. Section 4.3.4 of the ICH E-6 Good Clinical Practice: Consolidated Guidance reads, \"Although a subject is not obliged to give his/her reason(s) for withdrawing prematurely from a trial, the investigator should make a reasonable effort to ascertain the reason(s), while fully respecting the subject's rights.\" This excerpt expresses the need for physicians associated with clinical research trials to make a first-hand effort to contact patients who are lost-to-follow-up. In doing so pharmaceutical companies not only look out for the best interest of the patients who enroll in their clinical research trials, but also protect the data outcome of their clinical trials.\n\nIt is important for patients, physicians associated with the study and regulatory departments, like the FDA, that more information is provided to the appropriate people whenever possible. The information should be provided by the patient but when that is not feasible it should be provided by the investigator. Proper ways of obtaining information from and about the patients must be developed and proper processes as to the handling of this information must be implemented in order to protect the patients and the integrity of the study outcome.\n\n"}
{"id": "2329154", "url": "https://en.wikipedia.org/wiki?curid=2329154", "title": "Madness and Civilization", "text": "Madness and Civilization\n\nMadness and Civilization: A History of Insanity in the Age of Reason () is a 1964 abridged edition of a 1961 book by the French philosopher Michel Foucault. An English translation of the complete 1961 edition, titled History of Madness, was published in June 2006.\n\nFoucault's first major book, \"Madness and Civilization\" is an examination of the evolving meaning of madness in European culture, law, politics, philosophy and medicine from the Middle Ages to the end of the eighteenth century, and a critique of historical method and the idea of history. It marks a turning in Foucault's thought away from phenomenology toward structuralism: though he uses the language of phenomenology to describe an evolving experience of the mad as \"the other\", he attributes this evolution to the influence of specific powerful social structures.\n\nThe book developed out of Foucault's earlier writing on psychology, his own psychological difficulties, and his experiences working in a mental hospital, and was written mainly between 1955 and 1959 while working in cultural-diplomatic and educational posts in Sweden (as director of a French cultural centre attached to the University of Uppsala), Germany, and Poland.\n\nFoucault traces the evolution of the concept of madness through three phases: the Renaissance, the \"Classical Age\" (the later seventeenth and most of the eighteenth centuries) and the modern experience. He argues that in the Renaissance the mad were portrayed in art as possessing a kind of wisdom – a knowledge of the limits of our world – and portrayed in literature as revealing the distinction between what men are and what they pretend to be. Renaissance art and literature depicted the mad as engaged with the reasonable while representing the mysterious forces of cosmic tragedy such as the \"Ship of fools\", but the Renaissance also marked the beginning of an \"objective\" description of reason and unreason (as though seen from above) compared with the more intimate medieval descriptions from within society.\n\nFoucault contends that at the dawn of the age of reason, in the mid-seventeenth century, the rational response to the mad, who until then had been consigned to society's margins, was to separate them completely from society by confining them, along with prostitutes, vagrants, blasphemers and the like, in newly created institutions all over Europe – a process he calls \"the Great Confinement\".\n\nThe condition of these outcasts was seen as one of moral error. They were viewed as having freely chosen prostitution, vagrancy, blasphemy, unreason, etc. and the regimes of these new rational institutions were meticulous programs of punishment and reward aimed at causing them to reverse those choices.\n\nThe social forces Foucault sees driving this confinement include the need for an extra-judicial mechanism for getting rid of undesirables, and the wish to regulate unemployment and wages (the cheap labour of the workhouses applied downward pressure on the wages of free labour). He argues that the conceptual distinction between the mad and the rational was in a sense a product of this physical separation into confinement: confinement made the mad conveniently available to medical doctors who began to view madness as a natural object worthy of study and then as an illness to be cured.\n\nFor Foucault the modern experience began at the end of the eighteenth century with the creation of places devoted solely to the confinement of the mad under the supervision of medical doctors, and these new institutions were the product of a blending of two motives: the new goal of \"curing\" the mad away from their family who could not afford the necessary care at home, and the old purpose of \"confining\" undesirables for the protection of society. These distinct purposes were lost sight of, and the institution soon came to be seen as the only place where therapeutic treatment can be administered. He sees the nominally more enlightened and compassionate treatment of the mad in these modern medical institutions as just as cruel and controlling as their treatment in the earlier, rational institutions had been.\n\nThe sociologist José Guilherme Merquior discusses \"Madness and Civilization\" in \"Foucault\" (1985). Merquior argues that while Foucault raises important questions about the influence of social forces on the meaning of, and responses to, deviant behavior, \"Madness and Civilization\" is nonetheless so riddled with serious errors of fact and interpretation as to be of very limited value. Merquior notes that there is abundant evidence of widespread cruelty to and imprisonment of the insane during eras when Foucault contends that the mad were perceived as possessing wisdom, and that Foucault has thus selectively cited data that supports his assertions while ignoring contrary data. Madness was typically linked with sin by Christian Europeans, noted Merquior, and was therefore regarded as much less benign than Foucault tends to imply. Merquior sees \"Madness and Civilization\" as \"a call for the liberation of the Dionysian id\" similar to Norman O. Brown's \"Life Against Death\" (1959), and an inspiration for Gilles Deleuze and Félix Guattari's \"Anti-Oedipus\" (1972).\n\nKenneth Lewes writes that \"Madness and Civilization\" is an example of the \"critique of the institutions of psychiatry and psychoanalysis\" that occurred as part of the \"general upheaval of values in the 1960s\". Lewes sees Foucault's work as being similar to, but more profound than, Thomas Szasz's \"The Myth of Mental Illness\" (1961).\n\nThe philosopher Gary Gutting writes in \"Michel Foucault's Phanomenologie des Krankengeistes\" (1994):\n\n\n"}
{"id": "32070752", "url": "https://en.wikipedia.org/wiki?curid=32070752", "title": "Microcephaly lymphoedema chorioretinal dysplasia", "text": "Microcephaly lymphoedema chorioretinal dysplasia\n\nMicrocephaly lymphoedema chorioretinal dysplasia (MLCRD syndrome) is a genetic condition associated with:\n\nIn 1992, Feingold and Bartoshesky described two unrelated children with microcephaly, lymphoedema and chorioretinal dysplasia (MIM 152950) as a distinct entity. Since then there have been further reports of children with these three features (Angle et al. 1994, Fryns et al. 1995, Limwongse et al. 1999, Casteels et al. 2001)\n\nChildren have also been seen with two of the above features:\n\nThe former (microcephaly and lymphoedema) has been described as an autosomal dominant (MIM 156590) or X-linked trait, while the latter (microcephaly and chorioretinal dysplasia) has been described as autosomal dominant, autosomal recessive (MIM 251270 or Mirhosseini-Holmes-Walton syndrome) or X-linked trait.\n\nThe distinct facial feature include upslanting palpebral fissures, a broad nose with rounded tip, long philtrum with a thin upper lip, pointed chin and prominent ears (Vasudevan 2005)\n\nJuly 2005 - Volume 14 - Issue 3 - pp 109–116\n"}
{"id": "5470358", "url": "https://en.wikipedia.org/wiki?curid=5470358", "title": "Microgreen", "text": "Microgreen\n\nMicrogreens are a vegetable green, harvested after sprouting as shoots, that are used both as a visual and flavor component or ingredient primarily in fine dining restaurants. Fine dining chefs use microgreens to enhance the attractiveness and taste of their dishes with their delicate textures and distinctive flavors. Smaller than “baby greens,” and harvested later than sprouts, microgreens can provide a variety of leaf flavors, such as sweet and spicy. They are also known for their various colors and textures. Among upscale markets, they are now considered a specialty genre of greens that are good for garnishing salads, soups, plates, and sandwiches.\n\nEdible young greens and grains are produced from various kinds of vegetables, herbs or other plants. They range in size from , including the stem and leaves. A microgreen has a single central stem which has been cut just above the soil line during harvesting. It has fully developed cotyledon leaves and usually has one pair of very small, partially developed true leaves. The average crop-time for most microgreens is 10–14 days from seeding to harvest.\n\nMicrogreens began showing up on chefs' menus as early as the 1980s, in San Francisco, California.\nIn Southern California, microgreens have been grown since about the mid1990s. There were initially very few varieties offered. Those available were such as arugula, basil, beets, kale, cilantro and a mixture called Rainbow Mix. Having spread eastward from California, they are now being grown in most areas of the country with an increasing number of varieties being produced. Today, the U.S. microgreens industry consists of a variety of seed companies and growers.\n\nMicrogreens have three basic parts: a central stem, cotyledon leaf or leaves, and typically the first pair of very young true leaves. They vary in size depending upon the specific variety grown, with the typical size being in total length. When the green grows beyond this size, it should no longer be considered a microgreen. Larger sizes have been called petite greens. Microgreens are typically 2–4 weeks old from germination to harvest. Both baby greens and microgreens lack any legal definition. The terms \"baby greens\" and \"microgreens\" are marketing terms used to describe their respective categories. Sprouts are germinated seeds and are typically consumed as an entire plant (root, seed, and shoot), depending on the species. For example, sprouts from almond, pumpkin, and peanut reportedly have a preferred flavor when harvested prior to root development. Sprouts are legally defined, and have additional regulations concerning their production and marketing due to their relatively high risk of microbial contamination compared to other greens. Growers interested in producing sprouts for sale need to be aware of the risks and precautions summarized in the FDA publication Guidance for Industry: Reducing Microbial Food Safety Hazards for Sprouted Seeds (FDA 1999).\n\nGrowing microgreens is relatively easy; many small \"backyard\" growers have sprung up selling their greens at farmers' markets or to restaurants. A shallow plastic container with drainage holes, such as a nursery flat or prepackaged-salad box, will facilitate sprouting and grow out on a small scale. Growing and marketing high-quality microgreens commercially is much more difficult.\n\nResearchers at the USDA Agricultural Research Service have published, as of early 2014, several studies that identify the nutritional make-up and the shelf life of microgreens. Twenty-five varieties were tested, key nutrients measured were ascorbic acid (vitamin C), tocopherols (vitamin E), phylloquinone (vitamin K), and beta-carotene (a vitamin A precursor), plus other related carotenoids in the cotyledons.\n\nAmong the 25 microgreens tested, red cabbage, cilantro, garnet amaranth, and green daikon radish had the highest concentrations of vitamin C, carotenoids, vitamin K, and vitamin E, respectively. In general, microgreens contained considerably higher levels of vitamins and carotenoids—about five times greater—than their mature plant counterparts, an indication that microgreens may be worth the trouble of delivering them fresh during their short lives.A nutritional study of microgreens was done in the summer of 2012 by the Department of Nutrition and Food Science, University of Maryland, indicating promising potential that microgreens may indeed have particularly high nutritional value compared to mature vegetables. Bhimu Patil, a professor of horticulture and director of the Vegetable and Fruit Improvement Center at Texas A&M University, agrees that microgreens may potentially have higher levels of nutrients than mature vegetables. But he says more studies are needed to compare the two side by side. \"This is a very good start, but there can be a lot of variation in nutrients depending on where you grow it, when you harvest, and the soil medium,\" Patil says. When choosing a microgreen, researchers say to look for the most intensely colored ones, which will be the most nutritious.\nResults of the microgreens research project conducted by the University of Maryland and the USDA has garnered attention from several national media outlets including National Public Radio (NPR) and The Huffington Post.\n\nSprouts are germinated or partially germinated seeds. A sprout consists of the seed, root, stem, while microgreens are harvested without the roots.\n\nMicrogreens have stronger flavors compared to sprouts, and come in a wide selection of leaf shapes, textures and colors.\n\nMicrogreens are grown in soil or soil-like materials such as peat moss. Microgreens require high light levels, preferably natural sunlight with low humidity and good air circulation. Microgreens are planted with very low seed density compared to sprout processing. Crop times are generally one to two weeks for most varieties, though some can take four to six weeks. Microgreens are ready to harvest when the leaves are fully expanded. Harvesting is usually with scissors cutting just above the soil surface, excluding any roots. Some growers sell them while still growing, rooted in the growing trays so that they can be cut later. Once removed from their growing environment, these trays of microgreens must be used quickly or they will rapidly begin to elongate and lose color and flavor.\n\nSprout seeds are soaked in water for usually eight hours and then drained. A high density of seed is placed inside of sprouting equipment or enclosed containers. The seed germinates rapidly due to the high moisture and humidity levels maintained in the enclosures. Seeds can also be sprouted in cloth bags that are repeatedly soaked in water. The sprouting process occurs in dark or very low light conditions. These dark, wet, crowded conditions are ideal for the rapid proliferation of dangerous pathogenic bacteria. After a few days of soaking and repeated rinsing in water (several times a day to minimize spoilage), the processing is complete and the sprouts are ready to consume.\n\nThe conditions that are ideal for properly grown microgreens do not encourage the growth of dangerous pathogens. These growing methods would not work for the production of sprouts.\n\nHowever, the potential for food safety issues with microgreens may be increasing due to the number of indoor microgreen growing operations in which excessive seed density, low light intensity, low air circulation or most commonly, a lack of GAP (good agricultural practices) and GMP (good manufacturing practices) based food safety procedures. Certain provisions of the Guidance for Industry: Reducing Microbial Food Safety Hazards For Sprouted Seeds may be beneficial and prudent for growers of microgreens to follow.\n\nMicrogreens have a short shelf life and better methods of storing and transporting microgreens are currently being studied, which at this time are mainly focusing on buckwheat. Commercial microgreens are most often stored in plastic clamshell containers, which do not provide the right balance of oxygen and carbon dioxide for any live greens to breathe. Among package materials called films, differences in permeability, (see Permeation), are referred to as the oxygen transmission rate.\n\nThe ARS researchers found that buckwheat microgreens packaged in films with an oxygen transmission rate of 225 cubic centimeters per square inch per day had a fresher appearance and better cell membrane integrity than those packaged in other films tested. Following these steps, the team maintained acceptable buckwheat microgreen quality for more than 14 days—a significant extension, according to authors. This study was published in LWT-Food Science and Technology in 2013.\n\nLight-emitting diodes. otherwise known as LEDs, now provide the ability to measure impacts of narrow-band wavelengths of light on seedling physiology. The carotenoid zeaxanthin has been hypothesized to be a blue light receptor in plant physiology. A study was carried out to measure the impact of short-duration blue light on phytochemical compounds, which impart the nutritional quality of sprouting broccoli microgreens. Broccoli microgreens were grown in a controlled environment under LEDs using growing pads. Short-duration blue light acted to increase important phytochemical compounds influencing the nutritional value of broccoli microgreens.\n"}
{"id": "51082622", "url": "https://en.wikipedia.org/wiki?curid=51082622", "title": "Ministry of Health (Uganda)", "text": "Ministry of Health (Uganda)\n\nThe Ministry of Health is a cabinet-level government ministry of Uganda. It is responsible for planning, delivering, and maintaining an efficient and effective healthcare delivery system, including preventive, curative, and rehabilitative services, in a humane, affordable, and sustainable manner. The ministry is headed by Minister of Health Jane Aceng.\n\nThe headquarters of the ministry are located at Plot 6 Lourdel Road, in the Wandegeya neighborhood, Kampala Central Division, in Kampala, Uganda's capital and largest city, about north of the city's business district. The coordinates of the building are 0°19'59.0\"N, 32°34'39.0\"E (Latitude:0.333044; Longitude:32.577486).\n\n\n"}
{"id": "13535002", "url": "https://en.wikipedia.org/wiki?curid=13535002", "title": "Peer support specialist", "text": "Peer support specialist\n\nA certified peer support specialist, also known as a certified peer specialist, is a person with significant life-altering experience. This is also referred to as \"lived experience\". These specialists support individuals with struggles pertaining to mental health, psychological trauma or substance use. Because of their lived experience, such persons have expertise that professional training cannot replicate. This is not to be confused with peer educators, who may not consider recovery a suitable goal for everyone and may focus instead on the principles of harm reduction.\n\nTasks performed by peer support specialists may include assisting their peers in articulating their goals for recovery, learning and practicing new skills, helping them monitor their progress, supporting them in their treatment, modeling effective coping techniques and self-help strategies based on the specialist's own recovery experience, and supporting them in advocating for themselves to obtain effective services.\n\nA peer recovery support specialist (P-RSS) is an occupational title of trained individuals who engage with peers in a community-based recovery center, or outside it around any number of activities, or over the telephone. The peer support specialist can work with individuals as they develop and implement a personal recovery plan, which can also serve as a contract for engagement.\n\nThe Veterans Administration has increased its number of peer specialists to 800 The VA uses peer support specialists much like local, state, and private agencies.\n\nRecovery plans can take many forms. A key component of the recovery management model is a personal recovery plan which is drawn up by the individual looking for support, and reviewed with an RSS. This plan is instrumental for individuals in the process of their recovery.\n\nCentral to such plans are the overall health and well-being of each individual, not just their mental health. Components often include support groups and individual therapy, basic health care maintenance, stable housing, improvements in family life and personal relationships, and community connections. The plan may also include education goals, vocational development and employment. Some plans outline a timetable for monitoring, and/or a plan for re-engagement when needed to balance the health and overall quality of life of each individual. \n\nPeer recovery support specialists can be found in an increasing variety of settings, including community-based recovery centers. Funding for peer recovery programs comes from a combination of federal and state agencies as well as local and national charities and grant programs, such as Catholic Charities and the United Way.\n\nWhen peer support specialists work in publicly funded services, they are required to meet government and state certification requirements. Since the adaptation of the Recovery Management Model by state and federal agencies, peer support specialist courses have been offered by numerous state, nonprofit and for-profit entities such as Connecticut Community for Addiction Recovery, PRO-ACT (Pennsylvania Recovery Organization-Achieving Community Together), The McShin Foundation, Tennessee Certified Peer Recovery Specialist Training and Program, Appalachian Consulting Group, and the State of New York's Office of Addiction Services. PARfessionals has developed the first internationally approved online training program for peer support specialists in the fields of mental health and addiction recovery. In addition, numerous for-profit firms offer peer support specialist training. Training includes courses on the ethics of a recovery coach, recovery coaching core competencies, clinical theories as stages of change, motivational interviewing, and co-occurring disorders.\n\nAdapted for the recovery support specialist by William L. White:\n\n\n\n"}
{"id": "24889851", "url": "https://en.wikipedia.org/wiki?curid=24889851", "title": "Poison Dust", "text": "Poison Dust\n\nPoison Dust is a 2005 American documentary film starring Ramsey Clark, Juan Gonzalez, Rosalie Bertell, Helen Caldicott, Michio Kaku and directed by Sue Harris.\n\nThe film is a documentary about U.S. soldiers returning from Iraq who had been exposed to radioactive dust from dirty bombs when artillery shells coated with depleted uranium or DU are fired. Many suffer mysterious illnesses and have children with birth defects.\n"}
{"id": "15454519", "url": "https://en.wikipedia.org/wiki?curid=15454519", "title": "Ralph Frederick Sommer", "text": "Ralph Frederick Sommer\n\nRalph Frederick Sommer (1898 – 1971) is known as one of the two leading pioneers in the development of endodontics. He is also noted as a pioneer in the treatment of root canal infections and the development of the root canal operation.\n\nAt the University of Michigan, Sommer was a faculty member (1924–1968) as well as Chair of the Department of Endodontics at the University of Michigan. He also held status as the charter member and second president of the American Association of Endodontists.\n"}
{"id": "58303367", "url": "https://en.wikipedia.org/wiki?curid=58303367", "title": "Relative Fat Mass", "text": "Relative Fat Mass\n\nRelative Fat Mass (RFM) is a simple formula for the estimation of overweight or obesity in humans which requires only a calculation based on a ratio of height and waist measurements.\n\nHigh body-fat is associated with increased risks of poor health and early mortality. RFM is a simple anthropometric procedure that is claimed to be more convenient than body fat percentage and more accurate than the traditional body mass index (BMI).\n\nThe ratio or the patient's height and waist measurement is multiplied by 20 before being subtracted from a number (shown in bold below) that adjusts for differences in gender and height: \n\n\nAlthough generally validated on a database of some 12,000 adults RFM has not yet been evaluated in longitudinal studies of large populations to identify normal or abnormal RFM in relation to obesity-related health problems.\n"}
{"id": "32044512", "url": "https://en.wikipedia.org/wiki?curid=32044512", "title": "Shuntaro Hida", "text": "Shuntaro Hida\n\nShuntaro Hida (1 January 1917 – 20 March 2017) was a Japanese physician who was an eyewitness when the \"Little Boy\" atomic bomb was dropped on Hiroshima by the \"Enola Gay\" on 6 August 1945. He treated survivors as a medical doctor and wrote about the effects of radiation on the human body.\n\nThe night before the bomb was dropped 28-year-old Dr. Hida left the Hiroshima Military Hospital where he was stationed as an army medical officer to attend to a sick child in the village of Hesaka. He was therefore approximately 6 kilometers from ground zero when the bomb was dropped and he looked up and saw the Boeing B-29 Superfortress aircraft which he described as appearing like a \"tiny silver drop\". He then felt the heat and blast from the explosion and saw the mushroom cloud over the city. As a medical doctor he treated the wounded and saw the short- and long-term effects of radiation on the human body.\n\nAfter the war he continued to treat atomic bomb survivors (known as Hibakusha) for many years and he became the Director of the Hibakusha Counselling Centre. He also sought compensation from the United States government and advocated the abolition of nuclear weapons. In 2005 he was interviewed for the BBC drama documentary \"\" and his experiences were re-enacted in a dramatic reconstruction of events. He was also interviewed for the documentary \"\" in 2006.\n\nDr. Shuntaro Hida appears in the documentary \"Atomic Wounds\" by Journeyman Pictures. \"At 89, Doctor Hida, a survivor of the 1945 atomic bomb at Hiroshima, continues to care for some of the other quarter of a million survivors. \"Atomic Wounds\" retraces his dedicated journey and highlights how the terrible danger of radiation was concealed by successive American administrations of the 1950s to 1970s so that nuclear power could be freely developed, with no concern for public health.\" He died from pneumonia on 20 March 2017 at the age of 100.\n\n"}
{"id": "34168772", "url": "https://en.wikipedia.org/wiki?curid=34168772", "title": "Smoking in Pakistan", "text": "Smoking in Pakistan\n\nTobacco smoking in Pakistan is legal, but under certain circumstances is banned. If calculated on per day basis, 177 million cigarettes per day were consumed in FY-14. According to the Pakistan Demographic Health Survey, 46 per cent men and 5.7 per cent women smoke tobacco. The habit is mostly found in the youth of Pakistan and in farmers, and is thought to be responsible for various health problems and deaths in the country. Smoking produces many health problems in smokers. Pakistan has the highest consumption of tobacco in South Asia.\n\nPakistanis spent Rs. 250 billion on over 64 billion cigarettes in the financial year FY14, disclosed a State Bank report recently issued. The State Bank’s Statistical Bulletin reports that Pakistanis smoked 64.48bn cigarettes in the year FY-14. The average price of cigarette is considered Rs4 (conservative estimate) and the total price of 64.48bn cigarettes comes to an estimated Rs258bn. The most popular brand Gold Leaf is available at Rs 120 per pack of 20 cigarettes or Rs 6 per cigarette. The minimum price of cigarettes available in the market is Rs 50 per pack of 20 cigarettes or Rs2.5 per cigarette. Costly imported cigarettes are also available in the market which can go up to Rs 150 plus per pack. The main 3 companies took major part in tobacco industry. There are a number of smuggled cigarettes which find their way from Afghanistan whose landing port is in Karachi, Pakistan.\n\nLung cancer in Pakistan is caused directly by tobacco in 90% of cases. It claims lives of 100,000 people every year.\n\nA High amount of the youth in Karachi is addicted to tobacco smoking. It has become fashion for students to smoke Hookah in Hookah lounges.\n\n\n"}
{"id": "1840387", "url": "https://en.wikipedia.org/wiki?curid=1840387", "title": "Social grooming", "text": "Social grooming\n\nSocial grooming is a behaviour in which social animals, including humans, clean or maintain one another's body or appearance. A related term, allogrooming, indicates social grooming between members of the same species. Grooming is a major social activity, and a means by which animals who live in close proximity may bond and reinforce social structures, family links, and build companionships. Social grooming is also used as a means of conflict resolution, maternal behaviour and reconciliation in some species. Mutual grooming typically describes the act of grooming between two individuals, often as a part of social grooming, pair bonding, or a precoital activity.\n\nThere are a variety of proposed mechanisms by which social grooming behavior has been hypothesized to increase fitness. These evolutionary advantages may come in the form of health benefits including reduced disease transmission and reduced stress levels, maintaining social structure, and direct improvement of fitness as a measure of survival.\n\nSocial grooming behavior has been shown to elicit an array of health benefits in a variety of species.  For example, group member connection has the potential to mitigate the potentially harmful effects of stressors. In macaques, social grooming has been proven to reduce heart rate. Social affiliation during a mild stressor was shown to correlate with lower levels of mammary tumor development and longer lifespan in rats, while lack of this affiliation was demonstrated to be a major risk factor. Grooming has also been shown to play an integral role in reducing tick load in wild baboons (\"Papio cynocephalus\"). These ectoparasitic ticks carry the potential to act as vectors for the spreading of disease and infection by common tick-borne parasites such as haemoprotozoan. Baboons with lower tick loads show decreased occurrence of such infections and display signs of greater health status, evidenced by higher hematocrit (packed red cell volume) levels.\n\nOne of the most critical functions of social grooming is to establish social networks and relationships. In many species, individuals form close social connections dubbed “friendships.” In primates especially, grooming is known to have major social significance and function in the formation and maintenance of these friendships. Grooming networks in black crested gibbons have been proven to contribute greater social cohesion and stability. Groups of gibbons with more stable social networks formed grooming networks that were significantly more complex, while groups with low stability networks formed far fewer grooming pairs. In meerkats, social grooming has been shown to carry the role of maintaining relationships that increase fitness. In this system, researchers have observed that dominant males receive more grooming while grooming others less, thereby indicating that less dominant males groom more dominant individuals to maintain relationships. In addition to primates, animals such as deer, cows, horses, vole, mice, meerkats, coati, lions, birds, bats also form social bonds through grooming behavior. Social grooming is critical for vampire bats especially, since it is necessary for them to maintain food-sharing relationships in order to sustain their food regurgitation sharing behavior.\n\nSocial grooming relationships have been proven to provide direct fitness benefits to a variety of species. In particular, grooming in yellow baboons (\"Papio cynocephalus\") has been studied extensively, with numerous studies showing an increase in fitness as a result of social bonds formed through social grooming behavior. One such study, which collected 16 years of behavioral data on wild baboons, highlights the effects that sociality has on infant survival. A positive relationship is established between infant survival to one year and a composite sociality index, a measure of sociality based on proximity and social grooming. Evidence has also been provided for the effect of sociality on adult survival in wild baboons. Direct correlations between measures of social connectedness (which focuses on social grooming) and median survival time for both female and male baboons were modeled. \nSocial bonds established by grooming may provide an adaptive advantage in the form of conflict resolution and protection from aggression. In wild savannah baboons, social affiliations are shown to augment fitness by increasing tolerance from more dominant group members and increasing the chance of obtaining aid from conspecifics during instances of within-group contest interactions. In the yellow baboon, adult females form relationships with their kin, who offer support during times of violent conflict within social groups. In Barbary macaques\",\" social grooming results in the formation of crucial relationships among partners. These social relationships serve to aid cooperation and facilitate protection against combative groups composed of other males, which can oftentimes cause physical harm. Furthermore, social relationships have also been proven to decrease risk of infanticide in several primates.\n\nAltruism, in the biological sense, refers to a behavior performed by an individual that increases the fitness of another individual while decreasing the fitness of the one performing the behavior. This differs from the philosophical concept of altruism which requires the conscious intention of helping another. As a behavior, altruism is not evaluated in moral terms, but rather as a consequence of an action for reproductive fitness. It is often questioned why the behavior persists if it is costly to the one performing it, however, Charles Darwin proposed group selection as the mechanism behind the clear advantages of altruism.\n\nSocial grooming is considered a behavior of facultative altruism- the behavior itself is a temporary loss of direct fitness (with potential for indirect fitness gain), followed by personal reproduction. This tradeoff has been compared to the Prisoner's Dilemma model, and out of this comparison came Robert Trivers reciprocal altruism theory under the title \"tit-for-tat\". In conjunction with altruism, kin selection bears an emphasis on favoring the reproductive success of an organism's relatives, even at a cost to the organism's own survival and reproduction. Because of this, kin selection is an instance of inclusive fitness, which combines the number of offspring produced with the number an individual can ensure the production of by supporting others, such as siblings.\n\nformula_1\n\nDeveloped by W.D. Hamilton, this rule governs the idea that kin selection causes genes to increase in frequency when the genetic relatedness (r) of a recipient to an actor multiplied by the benefit to the recipient (B) is greater than the reproductive cost to the actor (C). Thus, it is advantageous for an individual to partake in altruistic behaviors, such as social grooming, so long as the individual receiving the benefits of the behavior is related to the one providing the behavior.\n\nIt was questioned whether some animals are instead using altruistic behaviors as a market strategy to trade for something desirable. In olive baboons, \"Papio anubis,\" it has been found that individuals perform altruistic behaviors as a form of trade in which a behavior is provided in exchange for benefits, such as reduced aggression. The grooming was evenly balanced across multiple bouts rather than single bouts, suggesting that females are not constrained to complete exchanges with single transactions and use social grooming to solidify long term relationships with those in their social group.\n\nIn addition, white-handed gibbons (\"Hylobates lar)\" confirmed that males were more attentive to social grooming during estrus of the females in their group. Though the behavior of social grooming itself was not beneficial to the one providing the service, the opportunity to mate and subsequent fertilization increases the reproductive fitness of those participating in the behavior. This study was also successful in finding that social grooming performance cycled with that of the females ovarian cycle.\n\nMany animals groom each other in the form of stroking, scratching, and massaging. This activity often serves to remove foreign material from the body to promote the communal success of these socially active animals. There exists a wide array of socially grooming animals throughout the kingdom, including primates, insects, birds, and bats. While thorough research has still yet to be engaged, much has been learned about social grooming in non-human animals via the study of primates. The driving force behind mammal social grooming is primarily believed to be rooted in adaptation to consolatory behavior as well as utilitarian purposes in the exchange of resources such as food, sex, and communal hygiene.\n\nIn insects, grooming often involves the important role of removing foreign material from the body. The honey bee, for example, engages in social grooming by cleaning body parts that cannot be reached by the receiving bee. The receiving bee extends their wings perpendicular to their body while their wings, mouth parts, and antennae are cleaned in order to remove dust and pollen. This removal of dust and pollen allows for sharpening of olfactory senses in contributing to the overall well being of the group.\n\nRecent studies have determined that vampire bats engage in social grooming much more than other types of bats to promote the well-being of the group. Facing higher levels of parasitic infection, vampire bats engage in cleaning one another as well as sharing food via regurgitation. This activity prevents ongoing infection while also promoting group success.\n\nPrimates provide perhaps one of the best example of mutual grooming due to the intensive research performed regarding their varying lifestyles, and the direct variation of means of social grooming across different species. Among primates, social grooming plays a significant role in animal consolation behavior whereby the primates engage in establishing and maintaining alliances through dominance hierarchies, pre-existing coalitions, and for reconciliation after conflicts. Primates groom socially in moments of boredom as well, and the act has been shown to reduce tension and stress. This reduction in stress is often associated with observed periods of relaxed behavior, and primates have been known to fall asleep while receiving grooming.\n\nGrooming in primates is not only utilized for alliance formation and maintenance, but to exchange resources such as communal food, sex, and hygiene. Wild baboons have been found to utilize social grooming as an activity to remove ticks and other insects from others. In this grooming, the body areas receiving significant attention appear to be the regions where the baboons themselves cannot reach. Grooming activity in these regions is used to remove parasites, dirt, dead skin, as well as tangled fur to help keep the animal’s health in good condition despite an individual inability to reach and clean certain areas.\n\nRecent studies regarding chimpanzees have determined the direct correlation of the release of oxytocin to consolatory behavior. This behavior as well as release has been noted in primates such as the Vervet monkey, a primate species that actively engages in social grooming from early childhood to adulthood. Vervet monkey siblings often have conflict over grooming allocation by their mother, yet, grooming remains an activity that mediates tension and is low cost for alliance formation and maintenance. This grooming occurs both between the siblings as well as involving the mother.\n\nRecent studies regarding the crab-eating macaques have shown that males will groom females in order to procure sex. One study found that a female has a greater likelihood to engage in sexual activity with a male if he had recently groomed her, compared to males who had not groomed her.\n\nIn 2010, researchers determined the existence of a form of social grooming as a consolation behavior within ravens via a form of bystander contact, whereby observer ravens would act to console a distressed victim via contact sitting, preening, as well as beak-to-beak touching.\n\nHorses engage in mutual grooming via the formation of 'pair bonds' where parasites and other contaminants on the surface of the body are actively removed. This removal of foreign material is primarily performed in hard-to-reach areas such as the neck via nibbling.\n\nAllogrooming is a behaviour commonly seen in many types of cattle, including dairy and beef breeds. The act of social licking can be seen specifically in heifers to initiate social dominance, emphasize companionship and improve hygiene of oneself or others. This behaviour seen in cows may provide advantages including reduced parasite loads, social tension and competition at the feed bunk. It is understood that social licking can provide long term benefits such as promoting positive emotions and a relaxed environment.\n\nSocial grooming has shown to be correlated with changes in endocrine levels within individuals. Specifically, there is a large correlation between the brain's release of oxytocin and social grooming. Oxytocin is hypothesized to promote prosocial behaviours due to its positive emotional response when released. Further, social grooming also releases beta-endorphins which promote physiological responses in stress reduction. These responses can occur from the production of hormones and endorphins, or through the growth or reduction in nerve structures. For example, in studies of suckling rats, rats who received warmth and touch when feeding had lower blood pressure levels than rats who did not receive any touch. This was found to be a result of an increased vagal nerve tone, meaning they had had higher parasympathetic nervous response and lower sympathetic nervous response to stimulus, resulting in a lower stress response. Social grooming is a form of innocuous sensory activation. Innocuous sensory activation, characterized by non aggressive contact, stimulates an entirely separate neural pathway from nocuous aggressive sensory activation. Innocuous sensations are transmitted through the dorsal column-medial lemniscal system.\n\nOxytocin is a peptide hormone known to help express social emotions such as altruism, which in turn provides a positive feedback mechanism for social behaviours. For example, studies in vampire bats have shown that intranasal injections of oxytocin have increased the amount of allogrooming done by female bats. The release of oxytocin, found to be stimulated by positive touch( such as allogrooming), positive smells and sounds, can have physiological benefits to the individual. Benefits can include: relaxation, healing, and digestion stimulation. Further, reproductive benefits have been found such as studies in rats have shown that the release of oxytocin can increase male reproductive success. The role of oxytocin is important in maternal pair bonding, and is hypothesized to promote similar bonding in social groups as a result of positive feedback loops from social interactions.\n\nGrooming stimulates the release of beta-endorphin, which is one physiological reason for why grooming appears to be relaxing. Beta-endorphins are found in neurons in the hypothalamus and the pituitary gland. Beta-endorphins are found to be opioid agonists. Opioids are molecules that act on receptors to promote feelings of relaxation, and reduce pain. A study in monkeys shows the changes in opiate expression in the body, mirroring changes in beta-endorphin levels, influences desire for social grooming. In using opiate receptor blockades, which decrease the level of beta-endorphins, the monkeys responded with an increased desire to be groomed. In contrast, when the monkeys were given morphine, the desire to be groomed dropped significantly. Beta- endorphins have been difficult to measure in animal species, differently from oxytocin which can be measured by sampling cerebrospinal fluid, and therefore have not been linked as strongly with social behaviours.\n\nGlucocorticoids are steroid hormones that are synthesized in the adrenal cortex and are a part of the group of corticosteroids. Glucocorticoids are involved in immune function, and are a part of the feedback system that reduces inflammation. Further, glucocorticoids are involved in glucose metabolism. Studies in macaques have shown that increased social stress results in glucocorticoid resistance, further inhibiting immune function. Macaques who participated in social grooming showed decreased levels of viral load, which points toward decreased levels of social stress resulting in increased immune function and glucocorticoid sensitivity. Additionally, an article published in 1997 concluded that an increase in maternal grooming resulted in a proportionate increase in Glucocorticoid receptors on target tissue in the neonatal rat. In the study on neonatal rats, it was found that the receptor number was altered because of a change in both serotonin and thyroid-stimulating hormone concentrations. An increase in the number of receptors might influence the amount of negative feedback on corticosteroid secretion and prevent the undesirable side effects of an abnormal physiologic stress response. Social grooming can change the number of glucocorticoid receptors, which can result in increased immune function.\n\nStudies have also shown that male baboons who participate more in social grooming show lower basal cortisol concentrations.\n\n"}
{"id": "1463296", "url": "https://en.wikipedia.org/wiki?curid=1463296", "title": "Stayhealthy", "text": "Stayhealthy\n\nStayhealthy, Inc. is a privately held corporation that creates web-enabled healthcare monitoring products. It was founded in the dot-com era by CEO John Collins, and produces devices that monitor and measure such aspects of one's health like blood pressure, heart rate, body weight, body mass index, body composition, body fat (adipose tissue), visual perception, and calories burned.\n\nThe company's current products include: the Body Composition Analyzer which reads an individual’s body composition with no appreciable difference from hydrostatic weighing or the DXA scan; a HealthCENTER Kiosk which has numerous configurations (Body Composition, Blood Pressure, Weight, Hearing, and Vision); Research Activity Monitor; and the Stayhealthy Activity Monitor (SAM) calorie tracker.\n\nIn May 2011, Tommy Thompson, former Secretary of the U.S. Department of Health & Human Services and Governor of Wisconsin, was elected to serve as Chairman of the Board.\n\nThe company has a deal with Albertsons to place kiosks in their stores.\n\nThe company has a deal with SuperValu (United States) to place kiosks in their stores.\n\nThe company has a deal with Kroger to place kiosks in their stores.\n\nOn 13 March 2004, Stayhealthy sued AmerisourceBergen.\n\nIn 2005, Stayhealthy unveiled the Fitness Expert product, a fusion of the Body Composition Analyzer and the Calorie Tracker, which was to be sold via multi-level marketing company Quixtar. This \ndeal has since fallen through.\n\nThe company was acquired by higi in 2014 .\n\n"}
{"id": "305050", "url": "https://en.wikipedia.org/wiki?curid=305050", "title": "Trichloroethylene", "text": "Trichloroethylene\n\nThe chemical compound trichloroethylene is a halocarbon commonly used as an industrial solvent. It is a clear non-flammable liquid with a sweet smell. It should not be confused with the similar 1,1,1-trichloroethane, which is commonly known as \"chlorothene\".\n\nThe IUPAC name is trichloroethene. Industrial abbreviations include TCE, trichlor, Trike, Tricky and tri. It has been sold under a variety of trade names. Under the trade names Trimar and Trilene, trichloroethylene was used as a volatile anesthetic and as an inhaled obstetrical analgesic in millions of patients.\n\nGroundwater and drinking water contamination from industrial discharge is a major concern for human health and has precipitated numerous incidents and lawsuits.\n\nPioneered by Imperial Chemical Industries in Britain, its development was hailed as an anesthetic revolution. Originally thought to possess less hepatotoxicity than chloroform, and without the unpleasant pungency and flammability of ether, TCE use was nonetheless soon found to have several pitfalls. These included promotion of cardiac arrhythmias, low volatility and high solubility preventing quick anesthetic induction, reactions with soda lime used in carbon dioxide absorbing systems, prolonged neurologic dysfunction when used with soda lime, and evidence of hepatotoxicity as had been found with chloroform.\n\nThe introduction of halothane in 1956 greatly diminished the use of TCE as a general anesthetic. TCE was still used as an inhalation analgesic in childbirth given by self-administration. Fetal toxicity and concerns for carcinogenic potential of TCE led to its abandonment in developed countries by the 1980s.\n\nDue to concerns about its toxicity, the use of trichloroethylene in the food and pharmaceutical industries has been banned in much of the world since the 1970s. Legislation has forced the substitution of trichloroethylene in many processes in Europe as the chemical was classified as a carcinogen carrying an R45 risk phrase, \"May cause cancer\". Many degreasing chemical alternatives are being promoted such as Ensolv and Leksol; however, each of these is based on \"n\"-propyl bromide which carries an R60 risk phrase of \"May impair fertility\", and they would not be a legally acceptable substitute.\n\nGroundwater contamination by TCE has become an important environmental concern for human exposure.\n\nIn 2005 it was announced by the United States Environmental Protection Agency that the agency had completed its Final Health Assessment for Trichloroethylene and released a list of new TCE toxicity values. The results of the study have formally characterized the chemical as a human carcinogen and a non-carcinogenic health hazard. A 2011 toxicological review performed by the EPA continues to list trichloroethylene as a known carcinogen.\n\nPrior to the early 1970s, most trichloroethylene was produced in a two-step process from acetylene. First, acetylene was treated with chlorine using a ferric chloride catalyst at 90 °C to produce 1,1,2,2-tetrachloroethane according to the chemical equation\n\nThe 1,1,2,2-tetrachloroethane is then dehydrochlorinated to give trichloroethylene. This can either be accomplished with an aqueous solution of calcium hydroxide\n\nor in the vapor phase by heating it to 300-500 °C on a barium chloride or calcium chloride catalyst\n\nToday, however, most trichloroethylene is produced from ethylene. First, ethylene is chlorinated over a ferric chloride catalyst to produce 1,2-dichloroethane.\n\nWhen heated to around 400 °C with additional chlorine, 1,2-dichloroethane is converted to trichloroethylene\n\nThis reaction can be catalyzed by a variety of substances. The most commonly used catalyst is a mixture of potassium chloride and aluminum chloride. However, various forms of porous carbon can also be used. This reaction produces tetrachloroethylene as a byproduct, and depending on the amount of chlorine fed to the reaction, tetrachloroethylene can even be the major product. Typically, trichloroethylene and tetrachloroethylene are collected together and then separated by distillation.\n\nTrichloroethylene is an effective solvent for a variety of organic materials.\n\nWhen it was first widely produced in the 1920s, trichloroethylene's major use was to extract vegetable oils from plant materials such as soy, coconut, and palm. Other uses in the food industry included coffee decaffeination and the preparation of flavoring extracts from hops and spices. It has also been used for drying out the last bit of water for production of 100% ethanol.\n\nFrom the 1930s through the 1970s, both in Europe and in North America, trichloroethylene was used as a volatile anesthetic almost invariably administered with nitrous oxide. Marketed in the UK by ICI under the trade name Trilene it was coloured blue (with a dye called waxoline blue) to avoid confusion with the similar smelling chloroform. TCE replaced earlier anesthetics chloroform and ether in the 1940s, but was itself replaced in the 1960s in developed countries with the introduction of halothane, which allowed much faster induction and recovery times. Trilene was also used as a potent inhaled analgesic, mainly during childbirth. It was used with halothane in the Tri-service field anaesthetic apparatus used by the UK armed forces under field conditions. As of 2000, however, TCE was still in use as an anesthetic in Africa.\n\nIt has also been used as a dry cleaning solvent, although replaced in the 1950s by tetrachloroethylene (also known as perchloroethylene), except for spot cleaning where it was used until the year 2000.\n\nTrichloroethylene was marketed as 'Ecco 1500 Anti-Static Film Cleaner and Conditioner' until 2009, for use in automatic movie film cleaning machines, and for manual cleaning with lint-free wipes.\n\nPerhaps the greatest use of TCE has been as a degreaser for metal parts. The demand for TCE as a degreaser began to decline in the 1950s in favor of the less toxic 1,1,1-trichloroethane. However, 1,1,1-trichloroethane production has been phased out in most of the world under the terms of the Montreal Protocol, and as a result trichloroethylene has experienced some resurgence in use as a degreaser.\n\nTCE has also been used in the United States to clean kerosene-fueled rocket engines (TCE was not used to clean hydrogen-fueled engines such as the Space Shuttle Main Engine). During static firing, the RP-1 fuel would leave hydrocarbon deposits and vapors in the engine. These deposits had to be flushed from the engine to avoid the possibility of explosion during engine handling and future firing. TCE was used to flush the engine's fuel system immediately before and after each test firing. The flushing procedure involved pumping TCE through the engine's fuel system and letting the solvent overflow for a period ranging from several seconds to 30–35 minutes, depending upon the engine. For some engines, the engine's gas generator and liquid oxygen (LOX) dome were also flushed with TCE prior to test firing. The F-1 rocket engine had its LOX dome, gas generator, and thrust chamber fuel jacket flushed with TCE during launch preparations.\n\nTCE is also used in the manufacture of a range of fluorocarbon refrigerants such as 1,1,1,2-tetrafluoroethane more commonly known as HFC 134a. TCE was also used in industrial refrigeration applications due to its high heat transfer capabilities and its low temperature specification. Many industrial refrigeration applications used TCE up to the 1990s in applications such as car testing facilities.\n\nDespite its widespread use as a metal degreaser, trichloroethylene itself is unstable in the presence of metal over prolonged exposure. As early as 1961 this phenomenon was recognized by the manufacturing industry, when stabilizing additives were added to the commercial formulation. Since the reactive instability is accentuated by higher temperatures, the search for stabilizing additives was conducted by heating trichloroethylene to its boiling point in a reflux condenser and observing decomposition. The first widely used stabilizing additive was dioxane; however, its use was patented by Dow Chemical Company and could not be used by other manufacturers. Considerable research took place in the 1960s to develop alternative stabilizers for trichloroethylene. Other chemical stabilizers include ketones such as methyl ethyl ketone.\n\nWhen inhaled, trichloroethylene produces central nervous system depression resulting in general anesthesia. These effects may be mediated by trichloroethylene acting as a positive allosteric modulator of inhibitory GABA and glycine receptors. Its high blood solubility results in a less desirable slower induction of anesthesia. At low concentrations it is relatively non-irritating to the respiratory tract. Higher concentrations result in tachypnea. Many types of cardiac arrhythmias can occur and are exacerbated by epinephrine (adrenaline). It was noted in the 1940s that TCE reacted with carbon dioxide (CO) absorbing systems (soda lime) to produce dichloroacetylene and phosgene. Cranial nerve dysfunction (especially the fifth cranial nerve) was common when TCE anesthesia was given using CO absorbing systems. These nerve deficits could last for months. Occasionally facial numbness was permanent. Muscle relaxation with TCE anesthesia sufficient for surgery was poor. For these reasons as well as problems with hepatotoxicity, TCE lost popularity in North America and Europe to more potent anesthestics such as halothane by the 1960s.\n\nThe symptoms of acute non-medical exposure are similar to those of alcohol intoxication, beginning with headache, dizziness, and confusion and progressing with increasing exposure to unconsciousness. Respiratory and circulatory depression can result in death.\n\nMuch of what is known about the human health effects of trichloroethylene is based on occupational exposures. Beyond the effects to the central nervous system, workplace exposure to trichloroethylene has been associated with toxic effects in the liver and kidney. Over time, occupational exposure limits on trichloroethylene have tightened, resulting in more stringent ventilation controls and personal protective equipment use by workers.\n\nResearch from Cancer bioassays performed by the National Cancer Institute (later the National Toxicology Program) showed that exposure to trichloroethylene is carcinogenic in animals, producing liver cancer in mice, and kidney cancer in rats.\n\nThe National Toxicology Program’s 11th Report on Carcinogens categorizes trichloroethylene as “reasonably anticipated to be a human carcinogen”, based on limited evidence of carcinogenicity from studies in humans and sufficient evidence of carcinogenicity from studies in experimental animals.\n\nOne recent review of the epidemiology of kidney cancer rated cigarette smoking and obesity as more important risk factors for kidney cancer than exposure to solvents such as trichloroethylene. In contrast, the most recent overall assessment of human health risks associated with trichloroethylene states, \"[t]here is concordance between animal and human studies, which supports the conclusion that trichloroethylene is a potential kidney carcinogen\". The evidence appears to be less certain at this time regarding the relationship between humans and liver cancer observed in mice, with the US NAS suggesting that low-level exposure might not represent a significant liver cancer risk in the general population.\n\nRecent studies in laboratory animals and observations in human populations suggest that exposure to trichloroethylene might be associated with congenital heart defects While it is not clear what levels of exposure are associated with cardiac defects in humans, there is consistency between the cardiac defects observed in studies of communities exposed to trichloroethylene contamination in groundwater, and the effects observed in laboratory animals. A study published in August 2008, has demonstrated effects of TCE on human mitochondria. The article questions whether this might impact female reproductive function.\n\nOccupational exposure to TCE was reported to correlate with development of symptoms of Parkinson's Disease in three laboratory workers. A retrospective twin study of pairs discordant for Parkinson's showed a six-fold increase in Parkinson's risk associated with TCE workplace exposure.\n\nThe health risks of trichloroethylene have been studied extensively. The U.S. Environmental Protection Agency (EPA) sponsored a \"state of the science\" review of the health effects associated with exposure to trichloroethylene. The National Academy of Sciences concluded that evidence on the carcinogenic risk and other potential health hazards from exposure to TCE has strengthened since EPA released their toxicological assessment of TCE, and encourages federal agencies to finalize the risk assessment for TCE using currently available information, so that risk management decisions for this chemical can be expedited.\n\nIn Europe, the Scientific Committee on Occupational Exposure Limit Values (SCOEL) recommends for trichloroethylene an occupational exposure limit (8 hour time-weighted average) of 10 ppm and a short-term exposure limit (15 min) of 30 ppm.\n\nExposure to TCE occurs mainly through contaminated drinking water. With a specific gravity greater than 1, trichloroethylene can be present as a dense non-aqueous phase liquid (DNAPL) if sufficient quantities are spilled in the environment. Another significant source of vapor exposure in Superfund sites that had contaminated groundwater, such as the Twin Cities Army Ammunition Plant, was by showering. TCE readily volatilizes out of hot water and into the air. Long, hot showers would then volatilize more TCE into the air. In a home closed tightly to conserve the cost of heating and cooling, these vapors would then recirculate.\n\nThe first known report of TCE in groundwater was given in 1949 by two English public chemists who described two separate instances of well contamination by industrial releases of TCE. Based on available federal and state surveys, between 9% to 34% of the drinking water supply sources tested in the U.S. may have some TCE contamination, though EPA has reported that most water supplies are in compliance with the maximum contaminant level (MCL) of 5 ppb. In addition, a growing concern in recent years at sites with TCE contamination in soil or groundwater has been vapor intrusion in buildings, which has resulted in indoor air exposures, such is in a recent case in the McCook Field Neighborhood of Dayton, Ohio. Trichloroethylene has been detected in 852 Superfund sites across the United States, according to the Agency for Toxic Substances and Disease Registry (ATSDR). Under the Safe Drinking Water Act of 1974, and as amended annual water quality testing is required for all public drinking water distributors. The EPA'S current guidelines for TCE are online. The EPA's table of \"TCE Releases to Ground\" is dated 1987 to 1993, thereby omitting one of the largest Superfund cleanup sites in the nation, the North IBW in Scottsdale, Arizona. Earlier, TCE was dumped here, and was subsequently detected in the municipal drinking water wells in 1982, prior to the study period.\n\nIn 1988, the EPA discovered tons of TCE that had been leaked or dumped into the ground by the United States military and semiconductor industry (companies including Fairchild Semiconductor, Intel Corporation, and Raytheon Company) just outside NASA Ames in Moffett Field, Mountain View, California.\n\nIn 1998, the View-Master factory supply well in Beaverton, Oregon was found to have been contaminated with high levels of TCE. It was estimated that 25,000 factory workers had been exposed to it from 1950–2001.\n\nIn the case of Lisle, Illinois, releases of trichloroethylene (TCE) had allegedly occurred on the Lockformer property beginning in 1968 and continuing for an undetermined period. The company used TCE in the past as a degreaser to clean metal parts. Contamination at the Lockformer site is presently under investigation by the U.S. Environmental Protection Agency (USEPA) and Illinois EPA. In 1992, Lockformer conducted soil sampling on their property and found TCE in the soil at levels as high as 680 parts per million (ppm). During the summer of 2000, a group of residents hired legal counsel, and on October 11, 2000, these residents had their private well water tested by a private environmental consultant. The group owned homes south of the Lockformer property in the suspected path of groundwater flow. The consultant collected a second round of well water samples on November 10, 2000, and TCE was detected in some of the wells sampled. Beginning in December 2000, Illinois EPA collected about 350 more private well water samples north and south of the Lockformer property.\n\nAs of 2007, 57,000 pounds, or 28.5 tons of TCE have been removed from the system of wells that once supplied drinking water to the residents of Scottsdale, Arizona. One of the three drinking water wells previously owned by the City of Phoenix and ultimately sold to the City of Scottsdale, tested at 390 ppb TCE when it was closed in 1982. The City of Scottsdale recently updated its website to clarify that the contaminated wells were \"in the Scottsdale area,\" and amended all references to the measured levels of TCE discovered when the wells were closed (including \"390 ppb\") to \"trace\".\n\nMarine Corps Base Camp Lejeune in North Carolina may be the largest TCE contamination site in the country. Legislation could force the EPA to establish a health advisory and a national public drinking water regulation to limit trichloroethylene.\n\nFor over twenty years of operation, RCA Corporation had been pouring toxic wastewater into a well in its Taoyuan City, Taiwan facility. The pollution from the plant was not revealed until 1994, when former workers brought it to light. Investigation by the Taiwan Environmental Protection Administration confirmed that RCA had been dumping chlorinated organic solvents into a secret well and caused contamination to the soil and groundwater surrounding the plant site. High levels of TCE and tetrachloroethylene (PCE) can be found in groundwater drawn as far as two kilometers from the site. An organization of former RCA employees reports 1375 cancer cases, 216 cancer deaths, and 102 cases of various tumors among its members.\n\nIn June 2012, residents of an area off of Stony Hill Road, Wake Forest, NC were contacted by the EPA and DWQ about possible TCE contamination after authorities followed up on existing TCE contamination in 2005. Subsequent EPA testing found multiple sites with detectable levels of TCE and several with levels above the MCL.\n\nThe 1998 film A Civil Action dramatizes the EPA lawsuit \"Anne Anderson, et al., v. Cryovac, Inc.\" concerning trichloroethylene contamination that occurred in Woburn, Massachusetts in the 1980s.\n\nUntil recent years, the US Agency for Toxic Substances and Disease Registry (ATSDR) contended that trichloroethylene had little-to-no carcinogenic potential, and was probably a co-carcinogen—that is, it acted in concert with other substances to promote the formation of tumors.\n\nState, federal, and international agencies classify trichloroethylene as a known or probable carcinogen. In 2014, the International Agency for Research on Cancer updated its classification of trichloroethylene to Group 1, indicating that sufficient evidence exists that it causes cancer of the kidney in humans as well as some evidence of cancer of the liver and non-Hodgkins lymphoma. California EPA regulators consider it a known carcinogen and issued a risk assessment in 1999 that concluded that it was far more toxic than previous scientific studies had shown.\n\nIn the European Union, the Scientific Committee on Occupational Exposure Limit Values (SCOEL) recommends an exposure limit for workers exposed to trichloroethylene of 10 ppm (54.7 mg/m) for 8-hour TWA and of 30 ppm (164.1 mg/m) for STEL (15 minutes).\n\nExisting EU legislation aimed at protection of workers against risks to their health (including Chemical Agents Directive 98/24/EC and Carcinogens Directive 2004/37/EC) currently do not impose binding minimum requirements for controlling risks to workers health during the use phase or throughout the life cycle of trichloroethylene. However, in case the ongoing discussions under the Carcinogens Directive will result in setting of a binding Occupational Exposure Limit for trichloroethylene for protection of workers; this conclusion may be revisited.\n\nThe Solvents Emissions Directive 1999/13/EC and Industrial Emissions Directive 2010/75/EC impose binding minimum requirements for emissions of trichloroethylene to the environment for certain activities, including surface cleaning. However, the activities with solvent consumption below a specified threshold are not covered by these minimum requirements.\n\nAccording to European regulation, the use of trichloroethylene is prohibited for individuals at a concentration greater than 0.1%. In industry, trichloroethylene should be substituted before April 21, 2016 (unless an exemption is requested before October 21, 2014) by other products such as tetrachloroethylene (perchloroethylene), methylene chloride (dichloromethane), or other hydrocarbon derivatives (ketones, alcohols, ...)\n\nIn 2001, a draft report of the Environmental Protection Agency (EPA) laid the groundwork for tough new standards to limit public exposure to trichloroethylene. The assessment set off a fight between the EPA and the Department of Defense (DoD), the Department of Energy, and NASA, who appealed directly to the White House. They argued that the EPA had produced junk science, its assumptions were badly flawed, and that evidence exonerating the chemical was ignored.\n\nThe DoD has about 1,400 military properties nationwide that are contaminated with trichloroethylene. Many of these sites are detailed and updated by www.cpeo.org and include a former ammunition plant in the Twin Cities area. Twenty three sites in the Energy Department's nuclear weapons complex — including Lawrence Livermore National Laboratory in the San Francisco Bay area, and NASA centers, including the Jet Propulsion Laboratory in La Cañada Flintridge are reported to have TCE contamination.\n\nPolitical appointees in the EPA sided with the Pentagon and agreed to pull back the risk assessment. In 2004, the National Academy of Sciences was given a $680,000 contract to study the matter, releasing its report in the summer of 2006. The report has raised more concerns about the health effects of TCE.\n\nIn response to the heightened awareness of environmental toxins such as TCE and the role they may be playing in childhood disease, in 2007, Senator Barack Obama proposed S1068, co-sponsored by Hillary Clinton and John Kerry. This legislation aimed to inform and protect communities that are threatened with environmental contamination. Senator Clinton's own bill, S1911, is known as the TCE Reduction Act.\n\nIn recent times, there has been a substantial reduction in the production output of trichloroethylene; alternatives for use in metal degreasing abound, chlorinated aliphatic hydrocarbons being phased out in a large majority of industries due to the potential for irreversible health effects and the legal liability that ensues as a result.\n\nThe U.S. military has virtually eliminated its use of the chemical, purchasing only 11 gallons in 2005. About 100 tons of it is used annually in the U.S. as of 2006.\n\nRecent research has focused on the in-place remediation of trichloroethylene in soil and ground water instead of removal for off-site treatment and disposal. Naturally-occurring bacteria have been identified with the ability to degrade TCE. \"Dehalococcoides\" sp. degrade trichloroethylene by reductive dechlorination under anaerobic conditions. Under aerobic conditions, \"Pseudomonas fluorescens\" can co-metabolize TCE. Soil and ground water contamination by TCE has also been successfully remediated by chemical treatment and extraction. The bacteria \"Nitrosomonas europaea\" can degrade a variety of halogenated compounds including trichloroethylene. Toluene dioxygenase has been reported to be involved in TCE degradation by \"Pseudomonas putida\". In some cases, \"Xanthobacter autotrophicus\" can convert up to 51% of TCE to CO and .\n\n\n"}
{"id": "54027296", "url": "https://en.wikipedia.org/wiki?curid=54027296", "title": "William J. Binder", "text": "William J. Binder\n\nWilliam J. Binder is an American facial plastic and reconstructive surgeon. He is best known for his 1992 discovery of the use of Botox to alleviate chronic migraine. In 2010, FDA approved the use of Botox as a treatment to migraine.\n\nBinder holds an M.D. degree from University of Medicine and Dentistry of New Jersey.\nBinder was elected as a member of the Board of Directors of the American Board of Facial Plastic and Reconstructive Surgery, for which he also serves as a National Board Examiner. He is a member of the National Review Committee for the American Academy of Neurology Therapeutics and Technologies Subcommittee on the Assessment of Botulinum Toxin Therapy and also an adjunct reviewer for the Archives of Facial Plastic Surgery Journal.\nBinder is board certified by both the American Board of Facial Plastic and Reconstructive Surgery and the American Board of Otolaryngology and Head and Neck Surgery.\nBinder published around 70 articles in national medical and surgical journals. He edited three textbooks in Facial Plastic Surgery and Head and Neck Surgery and Otolaryngology. He speaks and lecturers on various aspects of facial plastic surgery, including Rhinoplasty and multi-level concept of facelift surgery. Binder filed 15 U.S. and European patents in the area of Custom Plastic Surgery Implants, Method for Reduction of Migraine Headache Pain, Flexible support wrap, and Anesthesia conduit.\nIn 1992, Binder discovered the use of Botox to alleviate sufferers of chronic migraine headaches.\nBinder reported in 2000 that patients who had cosmetic injections around the face reported relief from chronic headache. This was initially thought to be an indirect effect of reduced muscle tension, but it is now known that the toxin inhibits release of peripheral nociceptive neurotransmitters, suppressing the central pain processing systems responsible for migraine headache. In 2010, the FDA approved intramuscular botulinum toxin injections for prophylactic treatment of chronic migraine headache.\n"}
{"id": "44974523", "url": "https://en.wikipedia.org/wiki?curid=44974523", "title": "Women's police station", "text": "Women's police station\n\nWomen's police stations (also units or offices) – , – are police stations specializing in crimes with female victims. They were first introduced in 1985 in Brazil and are numerous in Latin America. According to \"Latin American Perspectives\", the first women's police station was opened in Sao Paulo, Brazil and \"In the first six months of operation, the DDM processed 2,083 reports.\"\n\nOfficers at these stations are only allowed to respond to certain crimes, such as psychological violence, domestic violence, family violence, as well as specific types of threats and sexual violence. Some units offer financial help, counseling, and medical care for women who are having trouble.\n\nIn India, a study found \"the establishment of 188 women's police stations resulted in a 23 percent increase in reporting of crimes against women and children and a higher conviction rate between 2002 and 2004\".\n\nWomen's police stations are located in mostly Latin American countries where rates of rape and violence against women are high. Americasquarterly.org states, \"Femicide—the killing of women—has reached alarming levels in Latin America. The most recent region-wide statistics available, from 2003, show that seven Latin American countries score among the worst 10 nations when measuring the rate of femicide per one million women in 40 countries.\" Women's police stations are also in Ghana, India, Pakistan, Kosovo, Liberia, Nicaragua, Peru, Sierra Leone, South Africa, Uganda and Uruguay. A policewoman at a station in Pakistan states, \"Even if a woman is being beaten and psychologically tortured, she's told to consider her husband's honor and not go to the police station.\" Some women in Latin America do not even know their rights, Endvawow.org states, \"Only in Brazil had a majority of women surveyed receiving training or information about their rights one or more times (by any source): 54% in Brazil, 42% in Nicaragua, 34% in Peru, and 23% in Ecuador.\" According to Hautzinger in her article \"Criminalising Male Violence in Brazil's Women's Police Stations\", in Salvador, Brazil in regular police stations in spousal violence cases less than 2% actually went to court and the punishments the men did get were very minor. Endvawnow states that women police stations are an important first step for crimes to enter the justice system.\n\nWomen's police stations have greatly expanded since 1985. Endvawnow.org states, \"In 2010, there were 475 WPS in Brazil, 34 in Ecuador, 59 in Nicaragua, and 27 in Peru.\" In Santos' article \"EN-GENDERING THE POLICE\" states, \"They [women's police stations] expanded victims' citizenship rights, allowing them to denounce a violence that not long ago was invisible and considered a private matter. In 2000, for example, 310,058 complaints of violence against women were registered in the women's police stations of Sao Paulo.\" Language barriers and the inability to get to a station is still a problem. According to Endvawnow.org, women's police stations are located in more populated areas making it hard for women in rural areas to get to them and women who do not speak the same language as the policewomen can not communicate effectively. Endvawnow.org also states \"It was also found that poor and less educated women are sometimes ignored in the WPS. Also, despite psychological violence being illegal in all four countries, operators frequently prioritise those cases in which women have severe visible physical injuries, and may resist accepting complaints of psychological violence.\"\n\n"}
{"id": "4986141", "url": "https://en.wikipedia.org/wiki?curid=4986141", "title": "World Vegetarian Day", "text": "World Vegetarian Day\n\nWorld Vegetarian Day is observed annually around the planet on October 1. It is a day of celebration established by the North American Vegetarian Society in 1977 and endorsed by the International Vegetarian Union in 1978, \"To promote the joy, compassion and life-enhancing possibilities of vegetarianism.\" It brings awareness to the ethical, environmental, health, and humanitarian benefits of a vegetarian lifestyle.\n\nWorld Vegetarian Day initiates the month of October as Vegetarian Awareness Month, which ends with November 1, World Vegan Day, as the end of that month of celebration. Vegetarian Awareness Month has been known variously as \"Reverence for Life\" month, \"Month of Vegetarian Food\", and more. \n\nSeveral additional days of vegetarian significance are included in Vegetarian Awareness Month:\n and World Animal Day (always includes The Feast Day of St. Francis of Assisi). This may have been initiated by the now-defunct INRA (International Network for Religion and Animals), founded in 1985 by Virginia Bouraquardez (also known as Ginnie Bee), and later led by UCC minister, Rev. Marc Wessels. \n\n\nMeatless Monday is part of the Healthy Monday initiative. Healthy Monday encourages Americans to make healthier decisions at the start of every week. Other Healthy Monday campaigns include: Do The Monday 2000, Quit and Stay Quit Monday, Move it Monday, Monday Mile and others.\n\nVarious graphic and artistic representations are used; there is no one logo to represent World Vegetarian Day. Some of the other dates within Vegetarian Awareness Month have their own logos, or a series of logo representations, if they are sponsored in part or totally by identifiable organizations.\n\nThere is a common practice for some Chinese people to be vegetarian twice a lunar month.\nThe first day and the 15th day of each lunar month. (初一)﹑(十五). \nThe 15th day of each lunar month is the day/night with full moon.\n\nLocal vegetarian restaurants are particularly busy on those 2 days.\n\nThe origin of such practice is related to religious beliefs.\n\n\n"}
