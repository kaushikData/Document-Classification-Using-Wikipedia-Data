{"id": "18160509", "url": "https://en.wikipedia.org/wiki?curid=18160509", "title": "100% renewable energy", "text": "100% renewable energy\n\nThe endeavor to use 100% renewable energy for electricity, heating/cooling and transport is motivated by global warming, pollution and other environmental issues, as well as economic and energy security concerns. Shifting the total global primary energy supply to renewable sources requires a transition of the energy system. In 2013 the Intergovernmental Panel on Climate Change said that there are few fundamental technological limits to integrating a portfolio of renewable energy technologies to meet most of total global energy demand. Renewable energy use has grown much faster than even advocates anticipated.\n\nIn 2014, renewable sources such as wind, geothermal, solar, biomass, and burnt waste provided 19% of the total energy consumed worldwide, with roughly half of that coming from traditional use of biomass. The most important sector is electricity with a renewable share of 22.8%, most of it coming from hydropower with a share of 16.6%, followed by wind with 3.1%. According to the REN21 2017 global status report, these figures had increased to 19.3% for energy in 2015 and 24.5% for electricity in 2016. There are many places around the world with grids that are run almost exclusively on renewable energy. At the national level, at least 30 nations already have renewable energy contributing more than 20% of the energy supply.\n\nProfessors S. Pacala and Robert H. Socolow of Princeton University have developed a series of “climate stabilization wedges” that can allow us to maintain our quality of life while avoiding catastrophic climate change, and \"renewable energy sources,\" in aggregate, constitute the largest number of their \"wedges.\"\n\nMark Z. Jacobson, professor of civil and environmental engineering at Stanford University and director of its Atmosphere and Energy program, says that producing all new energy with wind power, solar power, and hydropower by 2030 is feasible, and that existing energy supply arrangements could be replaced by 2050. Barriers to implementing the renewable energy plan are seen to be \"primarily social and political, not technological or economic\". Jacobson says that energy costs today with a wind, solar, and water system should be similar to today's energy costs from other optimally cost-effective strategies. The main obstacle against this scenario is the lack of political will. Jacobson's conclusions have been disputed by other researchers.\n\nSimilarly, in the United States, the independent National Research Council has noted that “sufficient domestic renewable resources exist to allow renewable electricity to play a significant role in future electricity generation and thus help confront issues related to climate change, energy security, and the escalation of energy costs … Renewable energy is an attractive option because renewable resources available in the United States, taken collectively, can supply significantly greater amounts of electricity than the total current or projected domestic demand.\"\n\nThe main barriers to the widespread implementation of large-scale renewable energy and low-carbon energy strategies are political rather than technological. According to the 2013 \"Post Carbon Pathways\" report, which reviewed many international studies, the key roadblocks are: climate change denial, the fossil fuels lobby, political inaction, unsustainable energy consumption, outdated energy infrastructure, and financial constraints.\n\nUsing 100% renewable energy was first suggested in a Science paper\npublished in 1975 by Danish physicist Bent Sørensen, which was followed by several other proposals. In 1976 energy policy analyst Amory Lovins coined the term \"soft energy path\" to describe an alternative future where energy efficiency and appropriate renewable energy sources steadily replace a centralized energy system based on fossil and nuclear fuels.\n\nIn 1998 the first detailed analysis of scenarios with very high shares of renewables were published. These were followed by the first detailed 100% scenarios. In 2006 a PhD thesis was published by Czisch in which it was shown that in a 100% renewable scenario energy supply could match demand in every hour of the year in Europe and North Africa. In the same year Danish Energy professor Henrik Lund published a first paper in which he addresses the optimal combination of renewables, which was followed by several other papers on the transition to 100% renewable energy in Denmark. Since then Lund has been publishing several papers on 100% renewable energy. After 2009 publications began to rise steeply, covering 100% scenarios for countries in Europe, America, Australia and other parts of the world.\n\nEven in the early 21st century it was extraordinary for scientists and decision-makers to consider the concept of 100 per cent renewable electricity. However, renewable energy progress has been so rapid that things have totally changed since then:\n\nSolar photovoltaic modules have dropped about 75 per cent in price. Current scientific and technological advances in the laboratory suggest that they will soon be so cheap that the principal cost of going solar on residential and commercial buildings will be installation. On-shore wind power is spreading over all continents and is economically competitive with fossil and nuclear power in several regions. Concentrated solar thermal power (CST) with thermal storage has moved from the demonstration stage of maturity to the limited commercial stage and still has the potential for further cost reductions of about 50 per cent.\nRenewable energy use has grown much faster than even advocates had anticipated. Wind turbines generate 39 percent of Danish electricity, and Denmark has many biogas digesters and waste-to-energy plants as well. Together, wind and biomass provide 44% of the electricity consumed by the country's six million inhabitants. In 2010, Portugal's 10 million people produced more than half their electricity from indigenous renewable energy resources. Spain's 40 million inhabitants meet one-third of their electrical needs from renewables.\n\nRenewable energy has a history of strong public support. In America, for example, a 2013 Gallup survey showed that two in three Americans want the U.S. to increase domestic energy production using solar power (76%), wind power (71%), and natural gas (65%). Far fewer want more petroleum production (46%) and more nuclear power (37%). Least favored is coal, with about one in three Americans favouring it.\n\nREN21 says renewable energy already plays a significant role and there are many policy targets which aim to increase this:\n\nAt the national level, at least 30 nations around the world already have renewable energy contributing more than 20% of energy supply. National renewable energy markets are projected to continue to grow strongly in the coming decade and beyond, and some 120 countries have various policy targets for longer-term shares of renewable energy, including a binding 20% by 2020 target for the European Union. Some countries have much higher long-term policy targets of up to 100% renewables. Outside Europe, a diverse group of 20 or more other countries target renewable energy shares in the 2020–2030 time frame that range from 10% to 50%.\nNuclear power involves substantial accident risks (e.g., Fukushima nuclear disaster, Chernobyl disaster) and the unsolved problem of safe long-term high-level radioactive waste management, and carbon capture and storage has rather limited safe storage potentials. These constraints have also led to an interest in 100% renewable energy. A well established body of academic literature has been written over the past decade, evaluating scenarios for 100% renewable energy for various geographical areas. In recent years, more detailed analyses have emerged from government and industry sources. The incentive to use 100% renewable energy is created by global warming and ecological as well as economic concerns, post peak oil.\n\nThe first country to propose 100% renewable energy was Iceland, in 1998. Proposals have been made for Japan in 2003, and for Australia in 2011. Albania, Iceland, and Paraguay obtain essentially all of their electricity from renewable sources (Albania and Paraguay 100% from hydroelectricity, Iceland 72% hydro and 28% geothermal). Norway obtains nearly all of its electricity from renewable sources (97 percent from hydropower). Iceland proposed using hydrogen for transportation and its fishing fleet. Australia proposed biofuel for those elements of transportation not easily converted to electricity. The road map for the United States, commitment by Denmark, and Vision 2050 for Europe set a 2050 timeline for converting to 100% renewable energy, later reduced to 2040 in 2011. Zero Carbon Britain 2030 proposes eliminating carbon emissions in Britain by 2030 by transitioning to renewable energy. In 2015, Hawaii enacted a law that the Renewable Portfolio Standard shall be 100 percent by 2045. This is often confused with renewable energy. If electricity produced on the grid is 65 GWh from fossil fuel and 35 GWh from renewable energy and rooftop off grid solar produces 80 GWh of renewable energy then the total renewable energy is 115 GWh and the total electricity on the grid is 100 GWh. Then the RPS is 115 percent.\n\nCities like Paris and Strasbourg in France, planned to use 100% renewable energy by 2050.\n\nIt is estimated that the world will spend an extra $8 trillion over the next 25 years to prolong the use of non-renewable resources, a cost that would be eliminated by transitioning instead to 100% renewable energy. Research that has been published in Energy Policy suggests that converting the entire world to 100% renewable energy by 2030 is both possible and affordable, but requires political support. It would require building many more wind turbines and solar power systems but wouldn't utilize bioenergy. Other changes involve use of electric cars and the development of enhanced transmission grids and storage. As part of the Paris Agreement, countries periodically update their climate change targets for the future, by 2018 no G20 country had committed to a 100% renewable target.\n\n\"\" is a German documentary film released in 2010. It shows the vision of a global society, which lives in a world where the energy is produced 100% with renewable energies, showing a complete reconstruction of the economy, to reach this goal. In 2011, Hermann Scheer wrote the book \"The Energy Imperative: 100 Percent Renewable Now\", published by Routledge.\n\n\"Reinventing Fire\" is a book by Amory Lovins released in October 2011. By combining reduced energy use with energy efficiency gains, Lovins says that there will be a $5 trillion saving and a faster-growing economy. This can all be done with the profitable commercialization of existing energy-saving technologies, through market forces, led by business. Bill Clinton says the book is a \"wise, detailed and comprehensive blueprint\". The first paragraph of the preface says:\n\nImagine fuel without fear. No climate change. No oil spills, dead coal miners, dirty air, devastated lands, lost wildlife. No energy poverty. No oil-fed wars, tyrannies, or terrorists. Nothing to run out. Nothing to cut off. Nothing to worry about. Just energy abundance, benign and affordable, for all, for ever.\nThe Intergovernmental Panel on Climate Change has said that there are few fundamental technological limits to integrating a portfolio of renewable energy technologies to meet most of total global energy demand. In a 2011 review of 164 recent scenarios of future renewable energy growth, the report noted that the majority expected renewable sources to supply more than 17% of total energy by 2030, and 27% by 2050; the highest forecast projected 43% supplied by renewables by 2030 and 77% by 2050.\n\nIn 2011, the International Energy Agency has said that solar energy technologies, in its many forms, can make considerable contributions to solving some of the most urgent problems the world now faces:\n\nThe development of affordable, inexhaustible and clean solar energy technologies will have huge longer-term benefits. It will increase countries’ energy security through reliance on an indigenous, inexhaustible and mostly import-independent resource, enhance sustainability, reduce pollution, lower the costs of mitigating climate change, and keep fossil fuel prices lower than otherwise. These advantages are global. Hence the additional costs of the incentives for early deployment should be considered learning investments; they must be wisely spent and need to be widely shared.\nIn 2011, the refereed journal \"Energy Policy\" published two articles by Mark Z. Jacobson, a professor of engineering at Stanford University, and research scientist Mark A. Delucchi, about changing our energy supply mix and \"Providing all global energy with wind, water, and solar power\". The articles analyze the feasibility of providing worldwide energy for electric power, transportation, and heating/cooling from wind, water, and sunlight (WWS), which are safe clean options. In Part I, Jacobson and Delucchi discuss WWS energy system characteristics, aspects of energy demand, WWS resource availability, WWS devices needed, and material requirements. They estimate that 3,800,000 5 MW wind turbines, 5350 100 MW geothermal power plants, and 270 new 1300 MW hydroelectric power plants will be required. In terms of solar power, an additional 49,000 300 MW concentrating solar plants, 40,000 300 MW solar photovoltaic power plants, and 1.7 billion 3 kW rooftop photovoltaic systems will also be needed. Such an extensive WWS infrastructure could decrease world power demand by 30%. In Part II, Jacobson and Delucchi address variability of supply, system economics, and energy policy initiatives associated with a WWS system. The authors advocate producing all new energy with WWS by 2030 and replacing existing energy supply arrangements by 2050. Barriers to implementing the renewable energy plan are seen to be \"primarily social and political, not technological or economic\". Energy costs with a WWS system should be similar to today's energy costs.\n\nIn general, Jacobson has said wind, water and solar technologies can provide 100 per cent of the world's energy, eliminating all fossil fuels. He advocates a \"smart mix\" of renewable energy sources to reliably meet electricity demand:\n\nBecause the wind blows during stormy conditions when the sun does not shine and the sun often shines on calm days with little wind, combining wind and solar can go a long way toward meeting demand, especially when geothermal provides a steady base and hydroelectric can be called on to fill in the gaps.\nA 2012 study by the University of Delaware for a 72 GW system considered 28 billion combinations of renewable energy and storage and found the most cost-effective, for the PJM Interconnection, would use 17 GW of solar, 68 GW of offshore wind, and 115 GW of onshore wind, although at times as much as three times the demand would be provided. 0.1% of the time would require generation from other sources.\n\nIn March 2012, Denmark's parliament agreed on a comprehensive new set promotional programs for energy efficiency and renewable energy that will lead to the country getting 100 percent of electricity, heat and fuels from renewables by 2050.\nIRENEC is an annual conference on 100% renewable energy started in 2011 by Eurosolar Turkey. The 2013 conference was in Istanbul.\n\nMore recently, Jacobson and his colleagues have developed detailed proposals for switching to 100% renewable energy produced by wind, water and sunlight, for New York, California and Washington states, by 2050. , a more expansive new plan for the 50 states has been drawn up, which includes an online interactive map showing the renewable resource potential of each of the 50 states. The 50-state plan is part of The Solutions Project, an independent outreach effort led by Jacobson, actor Mark Ruffalo, and film director Josh Fox.\n\n, many detailed assessments show that the energy service needs of a world enjoying radically higher levels of wellbeing, can be economically met entirely through the diverse currently available technological and organisational innovations around wind, solar, biomass, biofuel, hydro, ocean and geothermal energy. Debate over detailed plans remain, but transformations in global energy services based entirely around renewable energy are in principle technically practicable, economically feasible, socially viable, and so realisable. This prospect underpins the ambitious commitment by Germany, one of the world's most successful industrial economies, to undertake a major energy transition, Energiewende.\n\nIn 2015 a study was published in \"Energy and Environmental Science\" that describes a pathway to 100% renewable energy in the United States by 2050 without using biomass. Implementation of this roadmap is regarded as both environmentally and economically feasible and reasonable, as by 2050 it would save about $600 Billion Dollars health costs a year due to reduced air pollution and $3.3 Trillion global warming costs. This would translate in yearly cost savings per head of around $8300 compared to a business as usual pathway. According to that study, barriers that could hamper implementation are neither technical nor economic but social and political, as most people didn't know that benefits from such a transformation far exceeded the costs.\n\nIn June 2017, twenty-one researchers published an article in the Proceedings of the National Academy of Sciences of the United States of America rejecting Jacobson's earlier PNAS article, accusing him of modeling errors and of using invalid modeling tools. They further asserted he made implausible assumptions through his reliance upon increasing national energy storage from 43 minutes to 7 weeks, increasing hydrogen production by 100,000%, and increasing hydropower by the equivalent of 600 Hoover Dams. Article authors David G. Victor called Jacobson's work \"dangerous\" and Ken Caldeira emphasized that increasing hydropower output by 1,300 gigawatts, a 25% increase, is the equivalent flow of 100 Mississippi Rivers. Jacobson published a response in the same issue of the PNAS and also authored a blog post where he asserted the researchers were advocates of the fossil fuel industry. Another study published in 2017 confirmed the earlier results for a 100% renewable power system for North America, without changes in hydropower assumptions, but with more realistic emphasis on a balanced storage portfolio, in particular seasonal storage, and for competitive economics.\n\nIn 2015, Jacobson and Delucchi, together with Mary Cameron and Bethany Frew, examined with computer simulation (LOADMATCH), in more detail how a wind-water-solar (WWS) system can track the energy demand from minute to minute. This turned out to be possible in the United States for 5 years.\nIn 2017, the plan was further developed for 139 countries by a team of 27 researchers and in 2018, Jacobson and Delucchi with Mary Cameron and Brian Mathiesen published the LOADMATCH results for 20 regions in which the 139 countries in the world are divided. According to this research, a WWS system can follow the demand in all regions.\n\nThe program LOADMATCH receives as input estimated series, per half minute during 2050-2055, of\nand specifications of\n\nThe program has been carried out for each region 10-20 times with adapted input for the storage capacities, until a solution was found in which the energy demand was followed, per half minute for 5 years, with low costs.\n\nThe WWS system is assumed to connect in the electric network\n\nThe following places meet 90% or more of their average yearly electricity demand with renewable energy (incomplete list):\nSome other places have high percentages, for example the electricity sector in Denmark, , is 40% wind power, with plans in place to reach 85%. The electricity sector in Canada and the electricity sector in New Zealand have even higher percentages, 65% and 75% respectively, and Austria is approaching 70%. , the electricity sector in Germany sometimes meets almost 100% of the electricity demand with PV and wind power, and renewable electricity is over 25%. Albania has 94.8% of installed capacity as hydroelectric, 5.2% diesel generator; but Albania imports 39% of its electricity. In 2016, Portugal achieved 100% renewable electricity for four days between 7 May and 11 May, partly because efficient energy use had reduced electricity demand. France and Sweden have low carbon intensity, since they predominately use a mixture of nuclear power and hydroelectricity.\n\nAlthough electricity is currently a big fraction of primary energy; it is to be expected that with renewable energy deployment primary energy use will go down sharply as electricity use increases, as it is likely to be combined with some degree of further electrification. For example, electric cars achieve much better miles per gallon equivalent than fossil fuel cars, and another example is renewable heat such as in the case of Denmark which is proposing to move to greater use of heat pumps for heating buildings which provide multiple kilowatts of heat per kilowatt of electricity.\n\nThe most significant barriers to the widespread implementation of large-scale renewable energy and low carbon energy strategies, at the pace required to prevent runaway climate change, are primarily political and not technological. According to the 2013 \"Post Carbon Pathways\" report, which reviewed many international studies, the key roadblocks are:\n\nNASA Climate scientist James Hansen discusses the problem with rapid phase out of fossil fuels and said that while it is conceivable in places such as New Zealand and Norway, \"suggesting that renewables will let us phase rapidly off fossil fuels in the United States, China, India, or the world as a whole is almost the equivalent of believing in the Easter Bunny and Tooth Fairy.\" In 2013, Smil analyzed proposals to depend on wind and solar-generated electricity including the proposals of Jacobson and colleagues, and writing in an issue of \"Spectrum\" prepared by the Institute of Electrical and Electronics Engineers, he identified numerous points of concern, such as cost, intermittent power supply, growing NIMBYism, and a lack of infrastructure as negative factors and said that \"History and a consideration of the technical requirements show that the problem is much greater than these advocates have supposed.\" Smil and Hansen are concerned about the variable output of solar and wind power, but American physicist Amory Lovins has said that the electricity grid can cope, just as it routinely backs up nonworking coal-fired and nuclear plants with working ones.\n\nIn 1999 American academic Dr. Gregory Unruh published a dissertation identifying the systemic barriers to the adoption and diffusion of renewable energy technologies. This theoretical framework was called Carbon Lock-in and pointed to the creation of self-reinforcing feedbacks that arise through the co-evolution of large technological systems, like electricity and transportation networks, with the social and political institutions that support and benefit from system growth. Once established, these techno-institutional complexes become \"locked-in\" and resist efforts to transform them towards more environmentally sustainable systems based on renewable sources.\n\nLester R. Brown founder and president of the Earth Policy Institute, a nonprofit research organization based in Washington, D.C., says a rapid transition to 100% renewable energy is both possible and necessary. Brown compares with the U.S. entry into World War II and the subsequent rapid mobilization and transformation of the US industry and economy. A quick transition to 100% renewable energy and saving of our civilization is proposed by Brown to follow an approach with similar urgency.\n\nThe International Energy Agency says that there has been too much attention on issue of the variability of renewable electricity production. The issue of intermittent supply applies to popular renewable technologies, mainly wind power and solar photovoltaics, and its significance depends on a range of factors which include the market penetration of the renewables concerned, the balance of plant and the wider connectivity of the system, as well as the demand side flexibility. Variability will rarely be a barrier to increased renewable energy deployment when dispatchable generation such as hydroelectricity or solar thermal storage is also available. But at high levels of market penetration it requires careful analysis and management, and additional costs may be required for back-up or system modification. Renewable electricity supply in the 20-50+% penetration range has already been implemented in several European systems, albeit in the context of an integrated European grid system:\n\nIn 2011, the Intergovernmental Panel on Climate Change, the world's leading climate researchers selected by the United Nations, said \"as infrastructure and energy systems develop, in spite of the complexities, there are few, if any, fundamental technological limits to integrating a portfolio of renewable energy technologies to meet a majority share of total energy demand in locations where suitable renewable resources exist or can be supplied\". IPCC scenarios \"generally indicate that growth in renewable energy will be widespread around the world\". The IPCC said that if governments were supportive, and the full complement of renewable energy technologies were deployed, renewable energy supply could account for almost 80% of the world's energy use within forty years. Rajendra Pachauri, chairman of the IPCC, said the necessary investment in renewables would cost only about 1% of global GDP annually. This approach could contain greenhouse gas levels to less than 450 parts per million, the safe level beyond which climate change becomes catastrophic and irreversible.\n\nIn November 2014 the Intergovernmental Panel on Climate Change came out with their fifth report, saying that in the absence of any one technology (such as bioenergy, carbon dioxide capture and storage, nuclear, wind and solar), climate change mitigation costs can increase substantially depending on which technology is absent. For example, it may cost 40% more to reduce carbon emissions without carbon dioxide capture. (Table 3.2)\n\nGoogle spent $30 million on their RE<C project to develop renewable energy and stave off catastrophic climate change. The project was cancelled after concluding that a best-case scenario for rapid advances in renewable energy could only result in emissions 55 percent below the fossil fuel projections for 2050.\n\n\n\n"}
{"id": "99861", "url": "https://en.wikipedia.org/wiki?curid=99861", "title": "Algorithmics", "text": "Algorithmics\n\nAlgorithmics is the science of algorithms. It includes algorithm design, the art of building a procedure which can solve efficiently a specific problem or a class of problem, algorithmic complexity theory, the study of estimating the hardness of problems by studying the properties of algorithm that solves them, or algorithm analysis, the science of studying the properties of a problem, such as quantifying resources in time and memory space needed by this algorithm to solve this problem.\n"}
{"id": "2347000", "url": "https://en.wikipedia.org/wiki?curid=2347000", "title": "Ancient Mesopotamian units of measurement", "text": "Ancient Mesopotamian units of measurement\n\nAncient Mesopotamian units of measurement originated in the loosely organized city-states of Early Dynastic Sumer. Each city, kingdom and trade guild had its own standards until the formation of the Akkadian Empire when Sargon of Akkad issued a common standard. This standard was improved by Naram-Sin, but fell into disuse after the Akkadian Empire dissolved. The standard of Naram-Sin was readopted in the Ur III period by the Nanše Hymn which reduced a plethora of multiple standards to a few agreed upon common groupings. Successors to Sumerian civilization including the Babylonians, Assyrians, and Persians continued to use these groupings. Akkado-Sumerian metrology has been reconstructed by applying statistical methods to compare Sumerian architecture, architectural plans, and issued official standards such as Statue B of Gudea and the bronze cubit of Nippur.\n\nThe systems that would later become the classical standard for Mesopotamia were developed in parallel with writing during Uruk Period Sumer (c 4000 BCE). Studies of protocuneiform indicate twelve separate counting systems used in Uruk. \nIn Early Dynastic Sumer (c 2900–2300 BCE) metrology and mathematics were indistinguishable and treated as a single scribal discipline. The idea of an abstract number did not yet exist, thus all quantities were written as metrological symbols and never as numerals followed by a unit symbol. For example there was a symbol for one-sheep and another for one-day but no symbol for one. About 600 of these metrological symbols exist, for this reason archaic Sumerian metrology is complex and not fully understood. In general however, length, volume, and mass are derived from a theoretical standard cube, called 'gur', filled with barley, wheat, water, or oil. The mass of a gur-cube, called 'gun' is defined as the weight a laden ass can carry. However, because of the different specific gravities of these substances combined with dual numerical bases (sexagesimal or decimal), multiple sizes of the gur-cube were used without consensus. The different gur-cubes are related by proportion, based on the water gur-cube, according to four basic coefficients and their cubic roots. These coefficients are given as:\n\n\nOne official government standard of measurement of the archaic system was the Cubit of Nippur (2650 BCE). It is a Euboic \"Mana\" + 1 Diesis (432g). This standard is the main reference used by archaeologists to reconstruct the system.\n\nA major improvement came in 2150 BCE during the Akkadian Empire under the reign of Naram-Sin when the competing systems were unified by a single official standard, the royal gur-cube. His reform is considered the first standardized system of measure in Mesopotamia. The royal gur-cube (Cuneiform: LU.GAL.GUR, ; Akkadian: \"šarru kurru\") was a theoretical cuboid of water approximately 6m × 6m × 0.5m from which all other units could be derived. The Neo-Sumerians continued use of the royal gur-cube as indicated by the Letter of Nanse issued in 2000 BCE by Gudea . Use of the same standard continued through the Babylonian, Assyrian, and Persian Empires.\n\nUnits of length are prefixed by the logogram DU () a convention of the archaic period counting system from which it was evolved. Basic length was used in architecture and field division. \nDistance units were geodectic as distinguished from non-geodectic basic length units. Sumerian geodesy divided latitude into seven zones between equator and pole.\n\nThe GAN system G counting system evolved into area measurements. A special unit measuring brick quantity by area was called the brick-garden (Cuneiform: SIG.SAR ; Sumerian: šeg-sar; Akkadian: \"libittu\"-\"mūšaru\") which held 720 bricks.\n\nCapacity was measured by either the ŠE system Š for dry capacity or the ŠE system Š for wet capacity\n\nMass was measured by the EN system E \n\nValues below are an average of weight artifacts from Ur and Nippur. The ± value represents 1 standard deviation. All values have been rounded to second digit of the standard deviation.\n\nIn the Archaic System time notation was written in the U System U. Multiple lunisolar calendars existed; however the civil calendar from the holy city of Nippur (Ur III period) was adopted by Babylon as their civil calendar. The calendar of Nippur dates to 3500 BCE and was itself based on older astronomical knowledge of an uncertain origin. The main astronomical cycles used to construct the calendar were the synodic month, equinox year, and sidereal day.\n\nThe Classical Mesopotamian system formed the basis for Elamite, Hebrew, Urartian, Hurrian, Hittite, Ugaritic, Phoenician, Babylonian, Assyrian, Persian, Arabic, and Islamic metrologies. The Classical Mesopotamian System also has a proportional relationship, by virtue of standardized commerce, to Bronze Age Harappan and Egyptian metrologies.\n\n\n\n\n"}
{"id": "48049", "url": "https://en.wikipedia.org/wiki?curid=48049", "title": "Autonomous robot", "text": "Autonomous robot\n\nAn autonomous robot is a robot that performs behaviors or tasks with a high degree of autonomy. Autonomous robotics is usually considered to be a subfield of artificial intelligence, robotics, and information engineering.\n\nAutonomous robots are particularly desirable in fields such as spaceflight, household maintenance (such as cleaning), waste water treatment, and delivering goods and services.\n\nSome modern factory robots are \"autonomous\" within the strict confines of their direct environment. It may not be that every degree of freedom exists in their surrounding environment, but the factory robot's workplace is challenging and can often contain chaotic, unpredicted variables. The exact orientation and position of the next object of work and (in the more advanced factories) even the type of object and the required task must be determined. This can vary unpredictably (at least from the robot's point of view).\n\nOne important area of robotics research is to enable the robot to cope with its environment whether this be on land, underwater, in the air, underground, or in space.\n\nA fully autonomous robot can:\n\nAn autonomous robot may also learn or gain new knowledge like adjusting for new methods of accomplishing its tasks or adapting to changing surroundings.\n\nLike other machines, autonomous robots still require regular maintenance.\n\nThe first requirement for complete physical autonomy is the ability for a robot to take care of itself. Many of the battery-powered robots on the market today can find and connect to a charging station, and some toys like Sony's \"Aibo\" are capable of self-docking to charge their batteries.\n\nSelf-maintenance is based on \"proprioception\", or sensing one's own internal status. In the battery charging example, the robot can tell proprioceptively that its batteries are low and it then seeks the charger. Another common proprioceptive sensor is for heat monitoring. Increased proprioception will be required for robots to work autonomously near people and in harsh environments. Common proprioceptive sensors include thermal, optical, and haptic sensing, as well as the Hall effect (electric).\n\nExteroception is sensing things about the environment. Autonomous robots must have a range of environmental sensors to perform their task and stay out of trouble.\n\n\nSome robotic lawn mowers will adapt their programming by detecting the speed in which grass grows as needed to maintain a perfectly cut lawn, and some vacuum cleaning robots have dirt detectors that sense how much dirt is being picked up and use this information to tell them to stay in one area longer.\n\nThe next step in autonomous behavior is to actually perform a physical task. A new area showing commercial promise is domestic robots, with a flood of small vacuuming robots beginning with iRobot and Electrolux in 2002. While the level of intelligence is not high in these systems, they navigate over wide areas and pilot in tight situations around homes using contact and non-contact sensors. Both of these robots use proprietary algorithms to increase coverage over simple random bounce.\n\nThe next level of autonomous task performance requires a robot to perform conditional tasks. For instance, security robots can be programmed to detect intruders and respond in a particular way depending upon where the intruder is.\n\nFor a robot to associate behaviors with a place (localization) requires it to know where it is and to be able to navigate point-to-point. Such navigation began with wire-guidance in the 1970s and progressed in the early 2000s to beacon-based triangulation. Current commercial robots autonomously navigate based on sensing natural features. The first commercial robots to achieve this were Pyxus' HelpMate hospital robot and the CyberMotion guard robot, both designed by robotics pioneers in the 1980s. These robots originally used manually created CAD floor plans, sonar sensing and wall-following variations to navigate buildings. The next generation, such as MobileRobots' PatrolBot and autonomous wheelchair, both introduced in 2004, have the ability to create their own laser-based maps of a building and to navigate open areas as well as corridors. Their control system changes its path on the fly if something blocks the way.\n\nAt first, autonomous navigation was based on planar sensors, such as laser range-finders, that can only sense at one level. The most advanced systems now fuse information from various sensors for both localization (position) and navigation. Systems such as Motivity can rely on different sensors in different areas, depending upon which provides the most reliable data at the time, and can re-map a building autonomously.\n\nRather than climb stairs, which requires highly specialized hardware, most indoor robots navigate handicapped-accessible areas, controlling elevators, and electronic doors. With such electronic access-control interfaces, robots can now freely navigate indoors. Autonomously climbing stairs and opening doors manually are topics of research at the current time.\n\nAs these indoor techniques continue to develop, vacuuming robots will gain the ability to clean a specific user-specified room or a whole floor. Security robots will be able to cooperatively surround intruders and cut off exits. These advances also bring concomitant protections: robots' internal maps typically permit \"forbidden areas\" to be defined to prevent robots from autonomously entering certain regions.\n\nOutdoor autonomy is most easily achieved in the air, since obstacles are rare. Cruise missiles are rather dangerous highly autonomous robots. Pilotless drone aircraft are increasingly used for reconnaissance. Some of these unmanned aerial vehicles (UAVs) are capable of flying their entire mission without any human interaction at all except possibly for the landing where a person intervenes using radio remote control. Some drones are capable of safe, automatic landings, however. An autonomous ship was announced in 2014—the Autonomous spaceport drone ship—and is scheduled to make its first operational test in December 2014.\n\nOutdoor autonomy is the most difficult for ground vehicles, due to:\nThe Seekur robot was the first commercially available robot to demonstrate MDARS-like capabilities for general use by airports, utility plants, corrections facilities and Homeland Security.\n\nThe Mars rovers MER-A and MER-B (now known as Spirit rover and Opportunity rover) can find the position of the sun and navigate their own routes to destinations on the fly by:\n\n\nThe planned ESA Rover, ExoMars Rover, is capable of vision based relative localisation and absolute localisation to autonomously navigate safe and efficient trajectories to targets by:\n\n\nDuring the final NASA Sample Return Robot Centennial Challenge in 2016, a rover, named Cataglyphis, successfully demonstrated fully autonomous navigation, decision-making, and sample detection, retrieval, and return capabilities. The rover relied on a fusion of measurements from inertial sensors, wheel encoders, Lidar, and camera for navigation and mapping, instead of using GPS or magnetometers. During the 2 hour challenge, Cataglyphis traversed over 2.6 km and returned five different samples to its starting position.\n\nThe DARPA Grand Challenge and DARPA Urban Challenge have encouraged development of even more autonomous capabilities for ground vehicles, while this has been the demonstrated goal for aerial robots since 1990 as part of the AUVSI International Aerial Robotics Competition.\n\nBetween 2013 and 2017, Total S.A. has held the ARGOS Challenge to develop the first autonomous robot for oil and gas production sites. The robots had to face adverse outdoor conditions such as rain, wind and extreme temperatures.\n\nThere are several open problems in autonomous robotics which are special to the field rather than being a part of the general pursuit of AI. According to George A. Bekey's \"Autonomous Robots: From Biological Inspiration to Implementation and Control\", problems include things such as making sure the robot is able to function correctly and not run into obstacles autonomously.\n\nResearchers concerned with creating true artificial life are concerned not only with intelligent control, but further with the capacity of the robot to find its own resources through foraging (looking for food, which includes both energy and spare parts).\n\nThis is related to autonomous foraging, a concern within the sciences of behavioral ecology, social anthropology, and human behavioral ecology; as well as robotics, artificial intelligence, and artificial life.\n\nA delivery robot is an autonomous robot used for delivering goods. As of February 2017 there were several notable companies developing delivery robots (some with pilot deliveries in progress):\n\n\nIn March 2016 a bill was introduced in Washington, D.C., allowing pilot ground robotic deliveries. The program was to take place from September 15 through the end of December 2017. The robots were limited to a weight of 50 pounds unloaded and maximum speed of 10 miles per hour. In case the robot stopped moving because of malfunction the company was required to remove it from the streets within 24 hours. There were allowed only 5 robots to be tested per company at a time. A 2017 version of the Personal Delivery Device Act bill was under review as of March 2017.\n\nIn February 2017 a bill was passed in the US state of Virginia (the House bill, HB2016, and the Senate bill, SB1207) that will allow autonomous delivery robots to travel on sidewalks and use crosswalks statewide beginning on July 1, 2017. The robots will be limited to a maximum speed of 10 mph and maximum weight of 50 pounds. In the states of Idaho and Florida there are also talks about passing similar legislature.\n\nIt has been discussed that robots with similar characteristics to invalid carriages (e.g. 10 mph maximum, limited battery life) might be a workaround for certain classes of applications. If the robot was sufficiently intelligent and able to recharge itself using existing electric vehicle (EV) charging infrastructure it would only need minimal supervision and a single arm with low dexterity might be enough to enable this function if its visual systems had enough resolution.\n\nIn November 2017, the San Francisco Board of Supervisors announced that companies would need to get a city permit in order to test these robots. In addition, sidewalk delivery robots have been banned from making non-research deliveries.\n\n"}
{"id": "8522837", "url": "https://en.wikipedia.org/wiki?curid=8522837", "title": "Bad tendency", "text": "Bad tendency\n\nIn U.S. law, the bad tendency principle is a test which permits restriction of freedom of speech by government if it is believed that a form of speech has a sole tendency to incite or cause illegal activity. The principle, formulated in \"Patterson v. Colorado\", (1907) was seemingly overturned with the \"clear and present danger\" principle used in the landmark case \"Schenck v. United States\" (1919), as stated by Justice Oliver Wendell Holmes, Jr. Yet eight months later, at the start of the next term in \"Abrams v. United States\" (1919) the Court again used the bad tendency test to uphold the conviction of a Russian immigrant who published and distributed leaflets calling for a general strike and otherwise advocated revolutionary, anarchist, and socialist views. Holmes dissented in \"Abrams\", explaining how the \"clear and present danger test\" should be employed to overturn Abrams' conviction. The arrival of the \"bad tendency\" test resulted in a string of politically incorrect rulings such as \"Whitney v. California\" (1927), where a woman was convicted simply because of her association with the Communist Party. The court ruled unanimously that although she had not committed any crimes, her relationship with the Communists represented a \"bad tendency\" and thus was unprotected. The \"bad tendency\" test was finally overturned in \"Brandenburg v. Ohio\" (1969) and was replaced by the \"imminent lawless action\" test.\n\n\n"}
{"id": "8088700", "url": "https://en.wikipedia.org/wiki?curid=8088700", "title": "Circular points at infinity", "text": "Circular points at infinity\n\nIn projective geometry, the circular points at infinity (also called cyclic points or isotropic points) are two special points at infinity in the complex projective plane that are contained in the complexification of every real circle.\n\nA point of the complex projective plane may be described in terms of homogeneous coordinates, being a triple of complex numbers , where two triples describe the same point of the plane when the coordinates of one triple are the same as those of the other aside from being multiplied by the same nonzero factor. In this system, the points at infinity may be chosen as those whose \"z\"-coordinate is zero. The two circular points at infinity are two of these, usually taken to be those with homogeneous coordinates\n\nA real circle, defined by its center point (\"x\",\"y\") and radius \"r\" (all three of which are real numbers) may be described as the set of real solutions to the equation\nConverting this into a homogeneous equation and taking the set of all complex-number solutions gives the complexification of the circle. The two circular points have their name because they lie on the complexification of every real circle. More generally, both points satisfy the homogeneous equations of the type \nThe case where the coefficients are all real gives the equation of a general circle (of the real projective plane). In general, an algebraic curve that passes through these two points is called circular.\n\nThe circular points at infinity are the points at infinity of the isotropic lines.\nThey are invariant under translations and rotations of the plane.\n\nThe concept of angle can be defined using the circular points, natural logarithm and cross-ratio:\nSommerville configures two lines on the origin as formula_3 Denoting the circular points as \"ω\" and ω\"′, he obtains the cross ratio\n\n"}
{"id": "71537", "url": "https://en.wikipedia.org/wiki?curid=71537", "title": "Coming out", "text": "Coming out\n\nComing out of the closet, or simply coming out, is a metaphor for LGBT people's self-disclosure of their sexual orientation or of their gender identity. The term \"coming out\" can also be used in various non-LGBT applications (e.g. atheists).\n\nFramed and debated as a privacy issue, coming out of the closet is described and experienced variously as a psychological process or journey; decision-making or risk-taking; a strategy or plan; a mass or public event; a speech act and a matter of personal identity; a rite of passage; liberation or emancipation from oppression; an ordeal; a means toward feeling gay pride instead of shame and social stigma; or even career suicide. Author Steven Seidman writes that \"it is the power of the closet to shape the core of an individual's life that has made homosexuality into a significant personal, social, and political drama in twentieth-century America\".\n\nAmerican gender theorist Judith Butler argues that the process of \"coming out\" does not free gay people from oppression. Although they may feel free to act as themselves, the opacity involved in entering a non-heterosexual territory insinuates judgment upon their identity, she argues in \"Imitation and Gender Insubordination\" (1991).\n\n\"Coming out of the closet\" is the source of other gay slang expressions related to voluntary disclosure or lack thereof. LGBT people who have already revealed or no longer conceal their sexual orientation or gender identity are \"out\", i.e. openly LGBT. Oppositely, LGBT people who have yet to come out or have opted not to do so are labelled as closeted or being in the closet. Outing is the deliberate or accidental disclosure of an LGBT person's sexual orientation or gender identity, without their consent. By extension, \"outing oneself\" is self-disclosure. \"Glass closet\" means the open secret of when public figures' being LGBT is considered a widely accepted fact even though they have not officially come out.\n\nIn 1869, one hundred years before the Stonewall riots, the German homosexual rights advocate Karl Heinrich Ulrichs introduced the idea of self-disclosure as a means of emancipation. Claiming that invisibility was a major obstacle toward changing public opinion, he urged homosexual people to reveal their same-sex attractions. In his 1906 work, \"Das Sexualleben unserer Zeit in seinen Beziehungen zur modernen Kultur\" (\"The Sexual Life of Our Time in its Relation to Modern Civilization\"), Iwan Bloch, a German-Jewish physician, entreated elderly homosexuals to self-disclose to their family members and acquaintances. In 1914, Magnus Hirschfeld revisited the topic in his major work \"The Homosexuality of Men and Women\", discussing the social and legal potentials of several thousand homosexual men and women of rank revealing their sexual orientation to the police in order to influence legislators and public opinion.\n\nThe first prominent American to reveal his homosexuality was the poet Robert Duncan. In 1944, using his own name in the anarchist magazine \"Politics\", he wrote that homosexuals were an oppressed minority. The decidedly clandestine Mattachine Society, founded by Harry Hay and other veterans of the Wallace for President campaign in Los Angeles in 1950, moved into the public eye after Hal Call took over the group in San Francisco in 1953, with many gays emerging from the closet.\n\nIn 1951, Donald Webster Cory published his landmark \"The Homosexual in America\", exclaiming, \"Society has handed me a mask to wear...Everywhere I go, at all times and before all sections of society, I pretend.\" Cory was a pseudonym, but his frank and openly subjective descriptions served as a stimulus to the emerging homosexual self-awareness and the nascent homophile movement.\n\nIn the 1960s, Frank Kameny came to the forefront of the struggle. Having been fired from his job as an astronomer for the Army Map service in 1957 for homosexual behavior, Kameny refused to go quietly. He openly fought his dismissal, eventually appealing it all the way to the U.S. Supreme Court. As a vocal leader of the growing movement, Kameny argued for unapologetic public actions. The cornerstone of his conviction was that, \"we must instill in the homosexual community a sense of worth to the individual homosexual\", which could only be achieved through campaigns openly led by homosexuals themselves.\nWith the spread of consciousness raising (CR) in the late 1960s, coming out became a key strategy of the gay liberation movement to raise political consciousness to counter heterosexism and homophobia. At the same time and continuing into the 1980s, gay and lesbian social support discussion groups, some of which were called \"coming-out groups\", focused on sharing coming-out \"stories\" (experiences) with the goal of reducing isolation and increasing LGBT visibility and pride.\n\nThe present-day expression \"coming out\" is understood to have originated in the early 20th century from an analogy that likens homosexuals' introduction into gay subculture to a débutante's \"coming-out party.\" This is a celebration for a young upper-class woman who is making her début – her formal presentation to society – because she has reached adult age or has become eligible for marriage. As historian George Chauncey points out:\nGay people in the pre-war years [pre-WWI]... did not speak of coming out of what we call the gay closet but rather of coming out \"into\" what they called homosexual society or the gay world, a world neither so small, nor so isolated, nor... so hidden as closet implies\nIn fact, as Elizabeth Kennedy observes, \"using the term 'closet' to refer to\" previous times such as \"the 1920s and 1930s might be anachronistic\".\n\nAn article on coming out in the online encyclopedia glbtq.com states that sexologist Evelyn Hooker's observations introduced the use of \"coming out\" to the academic community in the 1950s. The article continues by echoing Chauncey's observation that a subsequent shift in connotation occurred later on. The pre-1950s focus was on \"entrance\" into \"a new world of hope and communal solidarity\" whereas the post-Stonewall Riots overtone was an \"exit\" from the oppression of the closet. This change in focus suggests that \"coming out \"of the closet\"\" is a mixed metaphor that joins \"coming out\" with the closet metaphor: an evolution of \"skeleton in the closet\" specifically referring to living a life of denial and secrecy by concealing one's sexual orientation. The closet metaphor, in turn, is extended to the forces and pressures of heterosexist society and its institutions.\n\nWhen coming out is described as a gradual process or a journey, it is meant to include becoming aware of and acknowledging one's gender identity or non-heteronormative sexual orientation. This preliminary stage, which involves soul-searching or a personal epiphany, is often called \"coming out to oneself\" and constitutes the start of self-acceptance. Many LGBT people say that this stage began for them during adolescence or childhood, when they first became aware of their sexual orientation toward members of the same sex. Coming out has also been described as a process because of a recurring need or desire to come out in new situations in which LGBT people are assumed to be heterosexual or cisgender, such as at a new job or with new acquaintances. As Diana Fuss (1991) explains, \"the problem of course with the inside/outside rhetoric...is that such polemics disguise the fact that most of us are both inside and outside at the same time\".\n\nEvery coming out story is the person trying to come to terms with who they are and their sexual orientation. Several models have been created to describe coming out as a process for gay and lesbian identity development, e.g. Dank, 1971; Cass, 1984; Coleman, 1989; Troiden, 1989. Of these models, the most widely accepted is the Cass identity model established by Vivienne Cass. This model outlines six discrete stages transited by individuals who successfully come out: identity confusion, identity comparison, identity tolerance, identity acceptance, identity pride, and identity synthesis. However, not every LGBT person follows such a model. For example, some LGBT youth become aware of and accept their same-sex desires or gender identity at puberty in a way similar to which heterosexual teens become aware of their sexuality, i.e. free of any notion of difference, stigma or shame in terms of the gender of the people to whom they are attracted. Regardless of whether LGBT youth develop their identity based on a model, the typical age at which youth in the United States come out has been dropping. High school students and even middle school students are coming out.\n\nEmerging research suggests that gay men from religious backgrounds are likely to come out online via Facebook and Blogs as it offers a protective interpersonal distance. This largely contradicts the growing movement in social media research indicating that online use, particularly Facebook, can lead to negative mental health outcomes such as increased levels of anxiety. While further research is needed to assess whether these results generalize to a larger sample, these recent findings open the door to the possibility that gay men's online experiences may differ from heterosexuals' in that it may be more likely to provide mental health benefits than consequences.\n\nIn areas of the world where homosexual acts are penalized or prohibited, gay men, lesbians, and bisexual people can suffer negative legal consequences for coming out. In particular, where homosexuality is a crime, coming out may constitute self-incrimination. These laws still exist in 76 countries worldwide, including Egypt, Iran, Singapore, and Afghanistan.\n\nIn the early stages of the lesbian, gay or bisexual identity development process, people feel confused and experience turmoil. In 1993, Michelangelo Signorile wrote \"Queer in America\", in which he explored the harm caused both to a closeted person and to society in general by being closeted.\n\nBecause LGBT people have historically been marginalized as sexual minorities, coming out of the closet remains a challenge for most of the world's LGBT population and can lead to a backlash of heterosexist discrimination and homophobic violence.\n\nOn the personal and relationship levels, effects of not coming out have been the subject of studies. For example, it has been found that same-sex couples who have not come out are not as satisfied in their relationships as same-sex couples who have.\nFindings from another study indicate that the fewer people know about a lesbian's sexual orientation, the more anxiety, less positive affectivity, and lower self-esteem she has. Further, Gay.com states that closeted individuals are reported to be at increased risk for suicide.\n\nDepending on the relational bond between parents and children, a child coming out as lesbian, gay, bisexual or transgender can be positive or negative. Strong, loving relationships between children and their parents may be strengthened but if a relationship is already strained, those relationships may be further damaged or destroyed by the child coming out. If people coming out are accepted by their parents, this allows open discussions of dating and relationships and allows parents to help their children with coping with discrimination and to make healthier decisions regarding HIV/AIDS.\n\nA number of studies have been done on the effect of people coming out to their parents. A 1989 report by Robinson et al. of parents of out gay and lesbian children in the United States found that 21% of fathers and 28% of mothers had suspected that their child was gay or lesbian, largely based on gender atypical behaviour during childhood. The 1989 study found that two-thirds of parents reacted negatively. A 1995 study (that used young people's reactions) found that half of the mothers of gay or bisexual male college students \"responded with disbelief, denial or negative comments\" while fathers reacted slightly better. 18% of parents reacted \"with acts of intolerance, attempts to convert the child to heterosexuality, and verbal threats to cut off financial or emotional support\".\n\nHomelessness is a common effect among LGBT youth during the coming out process. LGBT youth are among the largest population of homeless youth; this has typically been caused by the self-identification and acknowledgment of being gay or identifying with the LGBT community. About 20% to 30% of homeless youth identify as LGBT. 55% of LGBQ and 67% of transgender youth are forced out of their homes by their parents or run away because of their sexual orientation or gender identity and expression. Homelessness among LGBT youth also impacts many areas of an individual's life, leading to higher rates of victimization, depression, suicidal ideation, substance abuse, risky sexual behavior, and participation in more illegal and dangerous activities. A 2016 study on homelessness pathways among Latino LGBT youth found that homelessness among LGBT individuals can also be attributed to structural issues like systems of care and sociocultural and economic factors.\n\nJimmie Manning performed a study in 2015 on positive and negative behavior performed during the coming out conversation. During his study, he learned that almost all of his participants would only attribute negative behaviors with themselves during the coming out conversations and positive behaviors with the recipient of the conversation. Manning suggests further research into this to figure out a way for positive behaviors to be seen and performed equally by both the recipient and the individual coming out.\n\nThe closet narrative sets up an implicit dualism between being \"in\" or being \"out\" wherein those who are \"in\" are often stigmatized as living false, unhappy lives. Likewise, philosopher and critical analyst Judith Butler (1991) states that the \"in/out\" metaphor creates a binary opposition which pretends that the closet is dark, marginal, and false and that being out in the \"light of illumination\" reveals a true (or essential) identity. Nonetheless, Butler is willing to appear at events as a lesbian and maintains that \"it is possible to argue that...there remains a political imperative to use these necessary errors or category mistakes...to rally and represent an oppressed political constituency\".\n\nIn addition Diana Fuss (1991) explains, \"the problem of course with the inside/outside rhetoric...is that such polemics disguise the fact that most of us are both inside and outside at the same time\". Further, \"To be out, in common gay parlance, is precisely to be no longer out; to be out is to be finally outside of exteriority and all the exclusions and deprivations such outsiderhood imposes. Or, put another way, to be out is really to be in—inside the realm of the visible, the speakable, the culturally intelligible.\" In other words, coming out constructs the closet it supposedly destroys and the self it supposedly reveals, \"the first appearance of the homosexual as a 'species' rather than a 'temporary aberration' also marks the moment of the homosexual's disappearance—into the closet\".\n\nFurthermore, Seidman, Meeks, and Traschen (1999) argue that \"the closet\" may be becoming an antiquated metaphor in the lives of modern-day Americans for two reasons.\n\nObserved annually on October 11, by members of the LGBT communities and their straight allies, National Coming Out Day is a civil awareness day for coming out and discussing LGBT issues among the general populace in an effort to give a familiar face to the LGBT rights movement. This day was the inspiration for holding LGBT History Month in the United States in October. The day was founded in 1988, by Robert Eichberg, his partner William Gamble, and Jean O'Leary to celebrate the Second National March on Washington for Lesbian and Gay Rights one year earlier, in which 500,000 people marched on Washington, DC, United States, for gay and lesbian equality. In the United States, the Human Rights Campaign manages the event under the National Coming Out Project, offering resources to LGBT individuals, couples, parents, and children, as well as straight friends and relatives, to promote awareness of LGBT families living honest and open lives. Candace Gingrich became the spokesperson for the day in April 1995. Although still named \"\"National\" Coming Out Day\", it is observed in Canada, Germany, The Netherlands, and Switzerland also on October 11, and in the United Kingdom on 12 October. To celebrate National Coming Out Day on October 11, 2002, Human Rights Campaign released an album bearing the same title as that year's theme: \"Being Out Rocks\". Participating artists include Kevin Aviance, Janis Ian, k.d. lang, Cyndi Lauper, Sarah McLachlan, and Rufus Wainwright.\n\nIn 1987, Barney Frank U.S. House Representative for publicly came out as gay, thus becoming the second member of the Massachusetts delegation to the United States Congress to do so. In 1983, U.S. Rep Gerry Studds, D-Mass., came out as a homosexual during the 1983 Congressional page sex scandal. In 1988, Svend Robinson was the first Canadian Member of Parliament to come out. Governor of New Jersey Jim McGreevey announced his decision to resign, publicly came out as \"a gay American\" and admitted to having had an extramarital affair with a man, Golan Cipel, an Israeli citizen and veteran of the Israeli Defense Forces, whom McGreevey appointed New Jersey homeland security adviser. In 1999, Australian Senator Brian Greig came out as being gay in his maiden speech to parliament, the first Australian politician to do so.\n\nThe first US professional team-sport athlete to come out was former NFL running back David Kopay, who played for five teams (San Francisco, Detroit, Washington, New Orleans and Green Bay) between 1964 and 1972. He came out in 1975 in an interview in the \"Washington Star\". The first professional athlete to come out while still playing was Czech-American professional tennis player Martina Navratilova, who came out as a lesbian during an interview with \"The New York Times\" in 1981. English footballer Justin Fashanu came out in 1990 and was subject to homophobic taunts from spectators, opponents and teammates for the rest of his career.\n\nIn 1995 while at the peak of his playing career, Ian Roberts became the first high-profile Australian sports person and first rugby footballer in the world to come out to the public as gay. John Amaechi, who played in the NBA with the Utah Jazz, Orlando Magic and Cleveland Cavaliers (as well as internationally with Panathinaikos BC of the Greek Basketball League and Kinder Bologna of the Italian Basketball League), came out in February 2007 on ESPN's \"Outside the Lines\" program. He also released a book \"Man in the Middle\", published by ESPN Books () which talks about his professional and personal life as a closeted basketball player. He was the first NBA player (former or current) to come out.\n\nIn 2008, Australian diver Matthew Mitcham became the first openly gay athlete to win an Olympic gold medal. He achieved this at the Beijing Olympics in the men's 10 metre platform event.\n\nThe first Irish county GAA player to come out while still playing was hurler Dónal Óg Cusack in October 2009 in previews of his autobiography. Gareth Thomas, who played international rugby union and rugby league for Wales, came out in a \"Daily Mail\" interview in December 2009 near the end of his career.\n\nIn 2013, basketball player Jason Collins (a member of the Washington Wizards) came out as gay, becoming the first active male professional athlete in a major North American team sport to publicly come out as gay.\n\nOn August 15, 2013, WWE wrestler Darren Young came out, making him the first openly gay active professional wrestler.\n\nOn February 9, 2014, former Missouri defensive lineman Michael Sam came out as gay. He was drafted by the St. Louis Rams on May 10, 2014, with the 249th overall pick in the seventh round, making him the first openly gay player to be drafted by an NFL franchise. He was released by St. Louis and waived by the Dallas Cowboys practice squad. Sam was on the roster for the Montreal Alouettes, but has since retired from football.\n\nIn 1997 on The Oprah Winfrey Show, actress Ellen DeGeneres came out as a lesbian. Her real-life coming out was echoed in the sitcom \"Ellen\" in \"The Puppy Episode\" in which the eponymous character Ellen Morgan played by DeGeneres outs herself over the airport public address system.\n\nOn March 29, 2010, Puerto Rican pop singer Ricky Martin came out publicly in a post on his official web site by stating, \"I am proud to say that I am a fortunate homosexual man. I am very blessed to be who I am.\" Martin said that \"these years in silence and reflection made me stronger and reminded me that acceptance has to come from within and that this kind of truth gives me the power to conquer emotions I didn't even know existed.\" Singer Adam Lambert came out after pictures of him kissing another man were publicly circulated while he was a participant on \"American Idol\" season 8.\n\nIn 1975, Leonard Matlovich, while serving in the United States Air Force, came out to challenge the U.S. military's policies banning service by homosexuals. Widespread coverage included a \"Time\" magazine cover story and a television movie on NBC.\n\nIn 2011, as the U.S. prepared to lift restrictions on service by openly gay people, Senior Airman Randy Phillips conducted a social media campaign to garner support for coming out. The video he posted on YouTube of the conversation in which he told his father he was gay went viral. In one journalist's summation, he \"masterfully used social media and good timing to place himself at the centre of a civil rights success story\".\n\nIn October, 2010, megachurch pastor Bishop Jim Swilley came out to his congregation. The YouTube video of the service went viral. Interviews with \"People\" magazine, Joy Behar, Don Lemon ABC News and NPR focused on the bullycides that prompted Bishop Swilley to \"come out\". One year and five years later, he confirmed the costs but also the freedom he has experienced. \"To be able to have freedom is something that I wouldn't trade anything for.\" \"Being married as yourself, preaching as yourself and living your life as yourself is infinitely better than doing those things as someone else.\" Bishop Swilley's son, Jared Swilley, bass player and front man of Black Lips said, \"It was definitely shocking, but I was actually glad when he told me. I feel closer to him now...\"\nBishop Swilley's other son, Judah Swilley, a cast member on the Oxygen show \"Preachers of Atlanta\", is confronting homophobia in the church.\n\nIn 1996, the acclaimed British film \"Beautiful Thing\" had a positive take in its depiction of two teenage boys coming to terms with their sexual identity. In 1987, a two-part episode of the Quebec television series \"Avec un grand A\", “Lise, Pierre et Marcel”, depicted a married closeted man who has to come out when his wife discovers that he has been having an affair with another man. In the Emmy Award-nominated episode \"Gay Witch Hunt\" of \"The Office\", Michael inadvertently outs Oscar to the whole office.\n\nEllen DeGeneres's coming out in the media as well as an episode of \"Ellen\", \"The Puppy Episode\", \"ranks, hands down, as the single most public exit in gay history\", changing media portrayals of lesbians in Western culture. In 1999, Russell T Davies's \"Queer as Folk\", a popular TV series shown on Channel 4 (UK) debuted and focused primarily on the lives of young gay men; in particular on a 15-year-old going through the processes of revealing his sexuality to those around him. This storyline was also featured prominently in the U.S. version of \"Queer As Folk\", which debuted in 2000.\n\nThe television show \"The L Word\", which debuted in 2004, focuses on the lives of a group of lesbian and bisexual women, and the theme of coming out is prominently featured in the storylines of multiple characters.\n\n\"Coming Out\", which debuted in 2013, is the first Quebec television program about being gay.\n\nSeason 3 of the Norwegian Teen drama series SKAM focused on a main character coming out and his relationship with another boy.\n\n\"Love, Simon\", based on the book, Simon vs. the Homo Sapiens Agenda, debuted in 2018, and is the first major studio film about a gay teenager coming out.\n\n\"Out\" is a common word or prefix used in the titles of LGBT-themed books, films, periodicals, organizations, and TV programs. Some high-profile examples are Out Magazine, the defunct OutWeek Magazine, and OutTV.\n\nIn political, casual, or even humorous contexts, \"coming out\" means by extension the self-disclosure of a person's secret behaviors, beliefs, affiliations, tastes, identities, and interests that may cause astonishment or bring shame. Some examples include: \"coming out as an alcoholic\", coming out as a BDSM participant\", \"coming out of the broom closet\" (as a witch), \"coming out as a conservative\", \"coming out as disabled\", \"coming out as a liberal\", \"coming out as intersex\", \"coming out as multiple\", \"coming out as polyamorous\", \"coming out as a sex worker\", and \"coming out of the shadows\" as an undocumented immigrant within the United States. The term is also used by members of online BIID communities to refer to the process of telling friends and families about their condition.\n\nWith its associated metaphors, the figure of speech has also been extended to atheism, e.g., \"coming out as an atheist\". A public awareness initiative for freethought and atheism, entitled the \"Out Campaign\", makes ample use of the \"out\" metaphor. This campaign was initiated by Robin Elisabeth Cornwell, and is endorsed by prominent atheist Richard Dawkins, who states \"there is a big closet population of atheists who need to 'come out'\".\n\n\n"}
{"id": "24710172", "url": "https://en.wikipedia.org/wiki?curid=24710172", "title": "Commoner", "text": "Commoner\n\nThe common people, also known as the common man, commoners, or the masses, are the ordinary people in a community or nation who lack any significant social status, especially those who are members of neither royalty, nobility, the clergy, nor any member of the aristocracy.\n\nWhereas historically many civilizations have institutionalized the notion of a \"common\" class within society, since the 20th century the term \"common people\" has been used in a more general sense to refer to typical members of society in contrast to the highly privileged (in either wealth or influence). In the United States, Andrew Jackson gave hope to the common man by his own example of working his way from being a poor Scots-Irish American to becoming the seventh President of the United States (1829-1837). The administration of Andrew Jackson utilized a system of choosing appointments to federal office which one contemporary congressman labeled the spoils system, thereby enabling previously unmeritorious individuals opportunities to hold office. Additionally, his presidency was marked by the common man’s right of suffrage (which had originally been limited to property-owning or tax-paying white males) as part of his administration’s greater democracy for the common man. Economically, Jackson’s administration fostered trade with Europe, leading to an increase in jobs for the common man in agriculture and industry.\n\nIn Europe, a distinct concept analogous to \"common people\" arose in the Classical civilization of ancient Rome around the 6th century BC, with the social division into patricians (nobles) and plebeians (commoners). The division may have been instituted by Servius Tullius, as an alternative to the previous clan based divisions that had been responsible for internecine conflict. The ancient Greeks generally had no concept of class and their leading social divisions were simply non-Greeks, free-Greeks and slaves. The early organisation of Ancient Athens was something of an exception with certain official roles like archons, magistrates and treasurers being reserved for only the wealthiest citizens – these class-like divisions were weakened by the democratic reforms of Cleisthenes who created new \"vertical\" social divisions in contrasting fashion to the \"horizontal\" ones thought to have been created by Tullius.\nWith the growth of Christianity in the 4th century AD, a new world view arose that would underpin European thinking on social division until at least early modern times. Saint Augustine postulated that social division was a result of the Fall of Man. The three leading divisions were considered to be the priesthood (clergy), the nobility, and the common people. Sometimes this would be expressed as \"those who prayed\", \"those who fought\" and \"those who worked\". The Latin terms for the three classes – \"oratores\", \"bellatores\" and \"laboratores\" – are often found even in modern textbooks, and have been used in sources since the 9th century.\nThis threefold division was formalised in the estate system of social stratification, where again commoners were the bulk of the population who are neither members of the nobility nor of the clergy. They were the third of the Three Estates of the Realm in medieval Europe, consisting of peasants and artisans.\n\nSocial mobility for commoners was limited throughout the Middle Ages. Generally, the serfs were unable to enter the group of the \"bellatores\". Commoners could sometimes secure entry for their children into the \"oratores\" class; usually they would serve as rural parish priests. In some cases they received education from the clergy and ascended to senior administrative positions; in some cases nobles welcomed such advancement as former commoners were more likely to be neutral in dynastic feuds. There were cases of serfs becoming clerics in the Holy Roman Empire, though from the Carolingian era, clergy were generally recruited from the nobility. Of the two thousand bishops serving from the 8th to the 15th century, just five came from the peasantry.\n\nUp until the late 15th-century European social order was relatively stable. There were periods where the common people felt oppressed in certain regions, but often they were content with their lot. In 12th-century England for example, while the common people would sometimes complain about the \"Norman yoke\" there was almost no unemployment and the average commoner only had to work only about 20 hours per week.\nThough incidents of savage brutality still occurred in Europe, especially when one set of nobles displaced another, in general nobles were seen as just protectors of the common people, as was encouraged by Christian teaching. \nWith early medieval times being a period of close to absolute faith, the clergy were also highly valued by the common people, bringing much happiness as at the time there was close to universal belief that through sacraments such as confession, the priest had the ability to ensure salvation.\n\nThe social and political order of medieval Europe was shaken by the development of the mobile cannon in the 15th century. Up until that time a noble with a small force could hold their castle or walled town for years even against large armies - and so they were rarely disposed. Once effective cannons were available, walls were of far less defensive value and rulers needed expensive field armies to keep control of a territory. This encouraged the formation of princely and kingly states, which needed to tax the common people much more heavily to pay for the expensive weapons and armies required to provide security in the new age. Up until the late 15th century, surviving medieval treaties on government were concerned with advising rulers on how to serve the common good: Assize of Bread is an example of medieval law specifically drawn up in the interests of the common people. But then works by Philippe de Commines, Niccolò Machiavelli and later Cardinal Richelieu began advising rulers to consider their own interests and that of the state ahead of what was \"good\", with Richelieu explicitly saying the state is above morality in doctrines such as \"Raison d'Etat\". This change of orientation among the nobles left the common people less content with their place in society. A similar trend occurred regarding the clergy, where many priests began to abuse the great power they had due to the sacrament of contrition. The Reformation was a movement that aimed to correct this, but even afterward the common people's trust in the clergy would continue to decline – priests were often seen as greedy and lacking in true faith. An early major social upheaval driven in part by the common people's mistrust of both the nobility and clergy occurred in Great Britain with the English Revolution of 1642. After the forces of Oliver Cromwell triumphed, movements like the Levellers rose to prominence demanding equality for all. When the general council of Cromwell's army met to decide on a new order at the Putney Debates of 1647, one of the commanders, Colonel Thomas Rainsborough, requested that political power be given to the common people. According to historian Roger Osbourne, the Colonel's speech was the first time a prominent person spoke in favour of universal male suffrage, but it was not to be granted until 1918. After much debate it was decided that only those with considerable property would be allowed to vote, and so after the revolution political power in England remained largely controlled by the nobles, with at first only a few of the most wealthy or well-connected common people sitting in Parliament.\n\nThe rise of the bourgeoisie during the Late Middle Ages, had seen an intermediate class of wealthy commoners develop, which ultimately gave rise to the modern middle classes. Middle-class people could still be called commoners however, for example in England Pitt the Elder was often called \"the Great Commoner\", and this appellation was later used for the 20th-century American anti-elitist campaigner William Jennings Bryan. The interests of the middle class were not always aligned with their fellow commoners of the working class.\n\nSocial historian Karl Polanyi wrote that in 19th-century Britain, the new middle class turned against their fellow commoners by seizing political power from the upper classes via the Reform Act 1832. Early industrialisation had been causing economic distress to large numbers of working class commoners, leaving them unable to earn a living. The upper classes had provided protection such as workhouses where inmates could happily \"doss\" about and also a system of \"outdoor\" relief both for the unemployed and those on low income. Though early middle class opposition to the Poor Law reform of William Pitt the Younger had prevented the emergence of a coherent and generous nationwide provision, the resulting Speenhamland system did generally save working class commoners from starvation. In 1834 outdoor relief was abolished, and workhouses were deliberately made into places so dehumanising that folk would often prefer to starve rather than enter them. For Polanyi this related to the economic doctrine prevalent at the time which held that only the spur of hunger could make workers flexible enough for the proper functioning of the free market. Later the same Laissez-faire free market doctrine led to British officials turning a blind eye to the suffering in the Irish potato famine and various Indian famines and acts of exploitation in colonial adventures. By the late 19th century, at least in mainland Britain, economic progress has been sufficient that even the working class were generally able to earn a good living, so working and middle class interests began to converge, lessening the division within the ranks of common people. Polanyi writes that on continental Europe middle and working class interests did not diverge anywhere near as markedly as they had in Britain.\n\nAfter the French Revolution, the Napoleonic wars and with industrialization, the division in three estates - nobility, clergy and commoners - had become somewhat outdated. The term \"common people\" continued to be used, but now in a more general sense to refer to regular people as opposed to the privileged elite.\n\nCommunist theory divided society into capitalists on one hand, and the proletariat or \"the masses\" on the other. In Marxism, the people are considered to be the creator of history. By using the word \"people\", Marx did not gloss over the class differences, but united certain elements, capable of completing the revolution. The Intelligentsia's sympathy for the common people gained strength in the 19th century in many countries. For example, in Imperial Russia a big part of the intelligentsia was striving for its emancipation. Several great writers (Nekrasov, Herzen, Tolstoy etc.) wrote about sufferings of the common people. Organizations, parties and movements arose, proclaiming the liberation of the people. These included among others: \"People's Reprisal\", \"People’s Will\", \"Party of Popular Freedom\" and the \"People's Socialist Party\".\n\nIn the United States, a famous 1942 speech by vice president Henry A. Wallace proclaimed the arrival of the \"century of the common man\" saying that all over the world the \"common people\" were on the march, specifically referring to Chinese, Indians, Russians, and as well as Americans.\nWallace's speech would later inspire the widely reproduced popular work \"Fanfare for the Common Man\" by Aaron Copland.\nIn 1948, U.S. President Harry S. Truman made a speech saying there needs to be a government \"that will work in the interests of the common people and not in the interests of the men who have all the money.\"\n\nComparative historian Oswald Spengler found the social separation into nobility, priests and commoners to occur again and again in the various civilisations that he surveyed (although the division may not exist for pre-civilised society).\nAs an example, in the Babylonian civilisation, The Code of Hammurabi made provision for punishments to be harsher for harming a noble than a commoner.\n\n"}
{"id": "54347", "url": "https://en.wikipedia.org/wiki?curid=54347", "title": "Complement (set theory)", "text": "Complement (set theory)\n\nIn set theory, the complement of a set refers to elements not in .\n\nWhen all sets under consideration are considered to be subsets of a given set , the absolute complement of is the set of elements in but not in .\n\nThe relative complement of with respect to a set , also termed the difference of sets and , written , is the set of elements in but not in .\n\nIf is a set, then the absolute complement of (or simply the complement of ) is the set of elements not in . In other words, if is the universe that contains all the elements under study, and there is no need to mention it because it is obvious and unique, then the absolute complement of is the relative complement of in :\n\nFormally:\n\nThe absolute complement of is usually denoted by formula_3. Other notations include formula_4, formula_5, formula_6, and formula_7.\n\n\nLet and be two sets in a universe . The following identities capture important properties of absolute complements:\n\nThe first two complement laws above show that if is a non-empty, proper subset of , then is a partition of .\n\nIf and are sets, then the relative complement of in , also termed the set difference of and , is the set of elements in but not in .\n\nThe relative complement of in is denoted according to the ISO 31-11 standard. It is sometimes written , but this notation is ambiguous, as in some contexts it can be interpreted as the set of all elements , where is taken from and from .\n\nFormally:\n\n\nLet , , and be three sets. The following identities capture notable properties of relative complements:\n\nIn the LaTeX typesetting language, the command codice_1 is usually used for rendering a set difference symbol, which is similar to a backslash symbol. When rendered, the codice_1 command looks identical to codice_3 except that it has a little more space in front and behind the slash, akin to the LaTeX sequence codice_4. A variant codice_5 is available in the amssymb package.\n\nSome programming languages allow for manipulation of sets as data structures, using these operators or functions to construct the difference of sets codice_6 and codice_7:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "442603", "url": "https://en.wikipedia.org/wiki?curid=442603", "title": "Cone of Silence (Get Smart)", "text": "Cone of Silence (Get Smart)\n\nThe Cone of Silence is one of many recurring joke devices from \"Get Smart\", an American comedy television series of the 1960s about an inept spy. The essence of the joke is that the apparatus, designed for secret conversations, makes it impossible for those inside the device – and easy for those outside the device – to hear the conversation. The end result being neither secret nor communication.\n\nIn popular culture, \"cone of silence\" is a slang phrase meaning that the speaker wishes to keep the indicated information secret and that the conversation should not be repeated to anyone not currently present. For example: \"We aren't inviting Cindy and her boyfriend to the movies because they embarrass us, but keep that in the cone of silence.\"\n\nAlthough popularized by \"Get Smart\", the term \"Cone of Silence\" actually originated on the syndicated TV show \"Science Fiction Theatre\", in an episode titled \"Barrier of Silence\" written by Lou Huston and first airing September 3, 1955. The story focuses on finding a cure for Professor Richard Sheldon, who had been returned to the United States in a confused, altered state of mind after abduction by enemy agents while visiting Milan. Scientists discover that placing Sheldon in an environment of total silence had been the means of brainwashing, a precursor to later ideas of sensory deprivation, celebrated in such films as \"Altered States\" and sundry spy thrillers. Sheldon is placed on a chair in the \"Cone of Silence\", which consists of a raised circular platform suspended by three wires tied to a common vertex. Although the cone's surface is open, noise canceling sound generators located just below the vertex shroud anyone sitting inside in a complete silence impossible in natural surroundings. It is also demonstrated that anyone speaking inside the cone could not be heard outside, which was the feature later parodied in \"Get Smart\". Only a speculative, \"science fiction\" possibility at that time, such technology is now commonplace in active noise canceling electronics for personal and industrial use.\n\nThe concept is also explored in Arthur C. Clarke's 1950 short story \"Silence Please\", which features a device capable of cancelling sound waves.\n\nIn Frank Herbert's science fiction novel \"Dune\"—first serialized in \"Analog\" from 1963 to 1965 and then published independently in August 1965—the Baron Harkonnen employs a \"cone of silence\" when having a private discussion with Count Fenring. In the novel's glossary, Herbert describes the device as the sound-deadening \"field of a distorter that limits the carrying power of the voice or any other vibrator by damping the vibrations with an image-vibration 180 degrees out of phase\". Used for privacy, the field does not visually obscure lip movement. Herbert had previously mentioned the cone of silence, on a much smaller scale, in his 1955 short story \"Cease Fire\".\n\nThe larger, plastic version of the \"Cone of Silence\", appeared in the pilot episode of \"Get Smart\", entitled \"Mr. Big\", which aired on September 18, 1965. Mel Brooks and Buck Henry, the original screenwriters for the series, devised many of the running jokes. Henry either borrowed or independently came up with the Cone of Silence concept, which debuted in the pilot along with other show standards, like Fang, the improperly trained dog-agent, and Max's shoe phone. The Cone of Silence scene was shot ahead of the rest of the pilot episode, and was used to sell the series to NBC.\n\nCones of Silence appear in \"The Nude Bomb\" (1980), the first attempt at a theatrical \"Get Smart\" movie. Max, the Chief, and the delegates all have their own cone placed over them. Neither the characters nor the audience hear what is being said. In the later sequel movie, \"Get Smart, Again!\" (1989), when Maxwell is reactivated as a secret agent, he insists on following protocol to ensure secrecy by using the Cone of Silence. However, the device is considered to be completely outdated (however Max and 99 still have one at home), and the current methods used were the following:\n\n\nA new version of the Cone of Silence appears in the 2008 \"Get Smart\" film. One of the early versions of the Cone used in the television series is on display in the CONTROL museum seen in the beginning of the film. The new version has an appearance more consistent with the cones of silence used in \"The Nude Bomb\" than in the television series. It was apparently constructed by the lab guys Bruce and Lloyd, and was untested at the time it was used. It seems much more high-tech, being a small handheld device which, when the button is pressed, creates a cone-shaped beam of light shining down from the ceiling, forming a force field around the person highlighted. This field ought to block all exterior sound, making external communication all but impossible. However, as usual, this updated version is ineffective. The force field was shown to be solid, though, to the point where a panicking Larrabee found he could not escape, to the cause of his greater panic. When Max himself attempts to use the device to hide his glee at being named field agent, it malfunctions and does not even raise the field, permitting everybody to hear his embarrassing shouts. However, in fairness to the manufacturers, this was because Max didn't push the button hard enough.\n\nThroughout the five seasons of \"Get Smart\", the Cone of Silence appears many times. For security reasons, Maxwell Smart insists upon using it to discuss his case. Despite this, its use is always counterproductive in some way:\n\nThe series also employs related technology:\n\n\n"}
{"id": "1096446", "url": "https://en.wikipedia.org/wiki?curid=1096446", "title": "Consent of the governed", "text": "Consent of the governed\n\nIn political philosophy, the phrase consent of the governed refers to the idea that a government's legitimacy and moral right to use state power is only justified and lawful when consented to by the people or society over which that political power is exercised. This theory of consent is historically contrasted to the divine right of kings and had often been invoked against the legitimacy of colonialism. Article 21 of the United Nation's 1948 Universal Declaration of Human Rights states that \"The will of the people shall be the basis of the authority of government\".\n\nIn his 1937 book \"A History of Political Theory\", George Sabine collected the views of many political theorists on consent of the governed. He notes the idea mentioned in 1433 by Nicholas of Cusa in \"De Concordantia Catholica\". In 1579 Theodore Beza wrote \"Vindiciae contra Tyrannos\" which Sabine paraphrases: \"The people lay down the conditions which the king is bound to fulfill. Hence they are bound to obedience only conditionally, namely, upon receiving the protection of just and lawful government…the power of the ruler is delegated by the people and continues only with their consent.\" In England, the Levellers also held to this principle of government.\n\nJohn Milton wrote\nSimilarly, Sabine notes the position of John Locke in \"Essay concerning Human Understanding\": \nHowever, with David Hume a contrary voice is heard. Sabine interprets Hume's skepticism by noting\nSabine revived the concept from its status as a political myth after Hume, by referring to Thomas Hill Green. Green wrote that government required \"will not force\" for administration. As put by Sabine,\nConsent of the governed, within the social liberalism of T. H. Green, was also described by Paul Harris:\n\"Consent of the governed\" is a phrase found in the United States Declaration of Independence.\n\nUsing thinking similar to that of John Locke, the founders of the United States believed in a state built upon the consent of \"free and equal\" citizens; a state otherwise conceived would lack legitimacy and Rational-legal authority. This was expressed, among other places, in the 2nd paragraph of the Declaration of Independence (emphasis added):\n\nWe hold these truths to be self-evident, that all men are created equal, that they are endowed by their Creator with certain unalienable Rights, that among these are Life, Liberty and the pursuit of Happiness.--That to secure these rights, Governments are instituted among Men, deriving their just powers from the , --That whenever any Form of Government becomes destructive of these ends, it is the Right of the People to alter or to abolish it, and to institute new Government, laying its foundation on such principles and organizing its powers in such form, as to them shall seem most likely to effect their Safety and Happiness.\n\nAnd in the earlier Virginia Declaration of Rights, especially Section 6, quoted below, founding father George Mason wrote:\n\nThat elections of members to serve as representatives of the people, in assembly, ought to be free; and that all men, having sufficient evidence of permanent common interest with, the attachment to, the community, have the right of suffrage, and cannot be taxed or deprived of their property for public uses without their own consent, or that of their representatives so elected, nor bound by any law to which they have not, in like manner, assented, for the public good.\"\nAlthough the Continental Congress at the outset of the American Revolution had no explicit legal authority to govern, it was delegated by the states with all the functions of a national government, such as appointing ambassadors, signing treaties, raising armies, appointing generals, obtaining loans from Europe, issuing paper money (called \"Continentals\"), and disbursing funds. The Congress had no authority to levy taxes, and was required to request money, supplies, and troops from the states to support the war effort. Individual states frequently ignored these requests. According to the \"Cyclopædia of Political Science\". New York: Maynard, Merrill, and Co., 1899, commenting on the source of the Congress' power:\nThe appointment of the delegates to both these congresses was generally by popular conventions, though in some instances by state assemblies. But in neither case can the appointing body be considered the original depositary of the power by which the delegates acted; for the conventions were either self-appointed \"committees of safety\" or hastily assembled popular gatherings, including but a small fraction of the population to be represented, and the state assemblies had no right to surrender to another body one atom of the power which had been granted to them, or to create a new power which should govern the people without their will. The source of the powers of congress is to be sought solely in the acquiescence of the people, without which every congressional resolution, with or without the benediction of popular conventions or state legislatures, would have been a mere \"brutum fulmen\"; and, as the congress unquestionably exercised national powers, operating over the whole country, the conclusion is inevitable that the will of the whole people is the source of national government in the United States, even from its first imperfect appearance in the second continental congress...\n\nA key question is whether the unanimous consent of the governed is required; if so, this would imply the right of secession for those who do not want to be governed by a particular collective. All democratic governments today allow decisions to be made even over the dissent of a minority of voters, which in some theorists' view, calls into question whether said governments can rightfully claim, in all circumstances, to act with the consent of the governed.\n\nThe theory of hypothetical consent of the governed holds that one's obligation to obey government depends on whether the government is such that one \"ought\" to consent to it, or whether the people, if placed in a state of nature without government, would agree to said government. This theory has been rejected by some scholars, who argue that since government itself can commit aggression, creating a government to safeguard the people from aggression would be similar to the people, if given the choice of what animals to be attacked by, trading \"polecats and foxes for a lion\", a trade that they would not make.\n\nAnother division that is sometimes made is between overt consent and tacit consent. Overt consent, to be valid, would require voluntariness, a specific act on the part of the consenters, a particular act consented to, and specific agents who perform this action. Immigrating into a particular jurisdiction is sometimes regarded as an overt act indicating consent to be ruled by that jurisdiction's government. Not all who are ruled by a particular government have immigrated to that jurisdiction, however; some were born there; however others argue that the power to emigrate from (i.e. leave) a jurisdiction \"implies\" such consent omission.\n\nIt has been pointed out that in jurisdictions where proportional representation is not used, but candidates are instead elected by plurality vote, a candidate can be elected despite the overt dissent of a majority of the people. Not every voter has necessarily had an opportunity to vote on the constitutional provisions specifying that plurality voting should be used; according to some theorists, this calls into question whether said voters have consented to be governed by the candidates who obtain plurality support. A counterargument is that, by failing to act through the process of constitutional amendment to change such provisions, the people have consented to them. A rebuttal to this is that in some jurisdictions, the means of amending the constitution are not completely in the hands of the electorate; the same issues arise, in claiming that the constitution left in place by the decisions of the people's elected representatives is consented to by the people, as arise in claiming that any other actions taken by said representatives are consented to by the people. Some proponents of the \"overt consent\" theory hold that the act of voting implies consent, while others question the connection between voting and consenting to a particular scheme of representative, since some voters may oppose the system as a whole but desire to influence decisions on particular issues or candidates.\n\nThe theory of tacit consent of the governed holds that if the people live in a country that is not undergoing a rebellion, they have consented to the rule of that country's government. However, this likewise meets with the criticism that tacit consent is incompatible with \"express\" opposition: i.e. the People cannot consent if they have no \"right\" to refuse.\n\nIn 1988, Edward S. Herman and Noam Chomsky described a propaganda model of news media in the United States in which coverage of current events was skewed by corporations and the state in order to manufacture the consent of the governed. According to the propagandist Edward L. Bernays, the public may be manipulated by its subconscious desires to render votes to a political candidate. The public relations techniques were described in his essay and book \"The Engineering of Consent\" (1955). Consent thus obtained undermines the legitimacy of government: \"The basic principle involved is simple but important: If the opinions of the public are to control the government, these opinions must not be controlled by the government.\"\n\nThe theory of literal consent holds the logical position that valid consent must denote final authority belonging to the people, rather than elected officials, therefore this implies that the people have the absolute sovereign power to overrule their government at any time via popular vote (or as stated in the Declaration of Independence, \"the right of the People to alter or abolish\" their government). Without this unfettered power, theorists hold that true consent cannot exist and that any government is therefore despotism via governing the people by force without their actual consent.\n\n\n"}
{"id": "1812304", "url": "https://en.wikipedia.org/wiki?curid=1812304", "title": "Discounted utility", "text": "Discounted utility\n\nIn economics, discounted utility is the utility (desirability) of some future event, such as consuming a certain amount of a good, as perceived at the present time as opposed to at the time of its occurrence. It is calculated as the present discounted value of future utility, and for people with time preference for sooner rather than later gratification, it is less than the future utility. The utility of an event \"x\" occurring at future time \"t\" under utility function \"u\", discounted back to the present (time 0) using discount factor formula_1 Is\n\nSince more distant events are less liked, formula_3\n\nDiscounted utility calculations made for events at various points in the future as well as at the present take the form \n\nwhere formula_5 is the utility of some choice formula_6 at time formula_7 and \"T\" is the time of the most distant future satisfaction event. Here, since utility comparisons are being made across time when the utilities are combined in a single evaluation, the utility function is necessarily cardinal in nature.\n\nIn a typical intertemporal consumption model, the above summation of utilities discounted from various future times would be maximized with respect to the amounts \"x\" consumed in each period, subject to an intertemporal budget constraint that says that the present value of current and future expenditures does not exceed the present value of financial resources available for spending.\n\nThe interpretation of formula_8 is not straightforward. Sometimes it is explained as the degree of a person's patience. Given the interpretation of economic agents as rational, this exempts time-valuations from rationality judgments, so that someone who spends and borrows voraciously is just as rational as someone who spends and saves moderately, or as someone who hoards his wealth and never spends it: different people have different rates of time preference.\n\nSome formulations treat formula_8 not as a constant, but as a function formula_10 that itself varies over time, for example in models which use the concept of hyperbolic discounting. This view is consistent with empirical observations that humans display inconsistent time preferences. For example, experiments by Tversky and Kahneman showed that the same people who would choose 1 candy bar now over 2 candy bars tomorrow, would choose 2 candy bars 101 days from now over 1 candy bar 100 days from now. (This is inconsistent because if the same question were posed 100 days from now, the person would ostensibly again choose 1 candy bar immediately instead of 2 candy bars the next day.)\n\nDespite arguments about how formula_8 should be interpreted, the basic idea is that all other things equal, the agent prefers to have something now as opposed to later (hence formula_12).\n\n"}
{"id": "6822184", "url": "https://en.wikipedia.org/wiki?curid=6822184", "title": "Doris Stevens", "text": "Doris Stevens\n\nDoris Stevens (October 26, 1888 – March 22, 1963) was an American suffragist, woman's legal rights advocate and author. She was the first female member of the American Institute of International Law and first chair of the Inter-American Commission of Women.\n\nBorn in 1888 in Omaha, Nebraska, Stevens became involved in the fight for suffrage while a college student at Oberlin College. After graduating with a degree in sociology in 1911, she taught briefly before becoming a paid regional organizer for the National American Woman Suffrage Association's Congressional Union for Woman Suffrage (CUWS). When the CUWS broke from the parent organization in 1914, Stevens became the national strategist. She was in charge of coordinating the women's congress, held at the Panama Pacific Exposition in 1915. When the CUWS became the National Woman's Party (NWP) in 1916, Stevens organized party delegates for each of the 435 Congressional Districts in an effort to attain national women's enfranchisement and defeat candidates who were opposed to women's rights. Between 1917 and 1919, Stevens was a prominent participant in the Silent Sentinels vigil at Woodrow Wilson's White House to urge the passage of a constitutional amendment for women's voting rights and was arrested several times for her involvement. After the 19th Amendment secured women's right to vote, she wrote a book, titled \"Jailed for Freedom\" (1920), which recounted the sentinel's ordeals.\n\nOnce the right to vote was secured, Stevens turned her attention to women's legal status. She supported passage of the Equal Rights Amendment and worked with Alice Paul from 1927–1933 on a volume of work comparing varying impact on law for women and men. The goal in compiling the data was to obtain an international law protecting women's right of citizenship. The research was completed with the help of feminists in 90 countries and evaluated laws controlling women's nationality from every country. Gaining approval for the work from the League of Nations in 1927, Stevens presented the proposal Pan American Union in 1928, convincing the governing body to create the Inter-American Commission of Women (CIM). In 1931, she joined the American Institute of International Law, becoming its first female member. In 1933, her work resulted in the first treaty to secure international rights for women. The Convention on the Nationality of Women established that women retained their citizenship after marriage and Convention on Nationality provided that neither marriage nor divorce could affect the nationality of the members of a family, extending citizenship protection to children.\n\nOusted from the CIM in 1938, and the NWP in 1947 over policy disputes, Stevens became vice president of the Lucy Stone League in 1951, of which she had been a member since the 1920s. She fought the roll-back of policies removing the gains women had made to enter the work force during World War II and worked to establish feminism as an academic field of study. She continued fighting for feminist causes until her death in 1963.\n\nDora Caroline Stevens was born on October 26, 1888 in Omaha, Nebraska to Caroline D. (née Koopman) and Henry Henderbourck Stevens. Her father was a pastor of the Dutch Reformed Church for forty years and her mother was a first generation immigrant from Holland. One of four children, Stevens grew up in Omaha and graduated in 1905 from Omaha High School. She went on to further her education graduating from Oberlin College in 1911 with a degree in sociology, though she had originally pursued music. While in college, she was known for her romances and for being a spirited suffragette. Her unruly behavior and disdain for feminine propriety were cultivated during her college years. After graduation, Stevens worked as a music teacher and social worker in Ohio, Michigan. and Montana before moving to Washington, D.C., where she became a regional organizer with the National American Woman Suffrage Association (NAWSA).\n\nIn 1913, Stevens arrived in Washington to take part in the June picketing of the Senate. She did not plan to stay, but Alice Paul convinced her to do so. She was hired by the NAWSA, and was assigned to the newly formed Congressional Union for Woman Suffrage (CUWS), which had been created by Alice Paul and Mary Ritter Beard. At that time, the Congressional Union was a subdivision of the NAWSA, though it operated independently. Stevens was hired to serve as executive secretary in Washington, D. C., as well as serve as regional organizer and was assigned the eastern district. Paul had divided the nation into quadrants of twelve states each and assigned Stevens to the eastern area, Mabel Vernon to the middle west, Anne Martin to the far west, and Maud Younger to the south. The regional organizers were tasked with educating groups about the suffrage bills that were in Congress and garnering support from each state for ratifying national suffrage. Rather than follow the previous strategy of achieving enfranchisement on a state-by-state basis, the Congressional Union Strategy was full federal approval. This issue, caused a rift in the suffrage movement at the 1913 Convention, causing Paul and her supporters to break ties with the NAWSA and become an independent organization.\nWith the fissure, the Congressional Union began a reorganization to push for campaigns against Democratic candidates because they had not supported suffrage while they were in control of the legislature. Paul established an all-woman advisory council of suffrage workers and prominent women which included Bertha Fowler, Charlotte Perkins Gilman, Helen Keller, Belle Case La Follette, May Wright Sewall and educators such as Emma Gillett, Maria Montessori, and Clara Louise Thompson, a Latin Professor at Rockford College, among others, to lend credibility to the new organization. Stevens became the national organizer, charged with organizing women in states in which they were able to vote to use their ballots and oppose any candidate not in favor of full enfranchisement of women. One of the first places Stevens traveled to was Colorado, where CUWS was successful in attaining commitment from one congressman to support the women's cause. Returning from that success in January 1915, she went to New York City and Newport, Rhode Island to campaign before heading west. She campaigned in Kansas, hoping to secure delegates for a convention planned in San Francisco for September.\n\nArriving in California in June, Stevens accompanied a group of women led by Charlotte Anita Whitney to meet with House Appropriations Committee members who were meeting at the Palace Hotel, in San Francisco. The women had been assured that they would be able to present their issues, but the chair, Representative John J. Fitzgerald of New York, refused to allow it. Undaunted, Whitney and Stevens continued their planning efforts for the Panama Pacific Exposition CUWS Congress in San Francisco. In San Francisco at the CUWS headquarters in 1915, Stevens discussed the strategy of employing a \"million-vote smile\", positing that smiling was a useful tool in the fight to win over men's support. \"Smile on men and they will give you a vote. Look severe and they won't,\" she stated. However, when Alice Paul arrived two weeks before the event, she canceled choral events, a parade and a mass meeting that had been planned for the Scottish Rite Hall. Stevens had been involved in supervising each of these events, though local women planned and orchestrated them. Paul did keep the luncheon and a ball to be held at the California building of the exposition. After the September Congress, Stevens had planned to remain in San Francisco and run the exposition booth of CUWS, but she was forced to return to Washington because the eastern delegate Margaret Whittemore had left due to her marriage. Stevens immediately began planning for a convention to be held in Washington in December. \n\nAt the beginning of 1916, Stevens announced the policy that the CUWS had organized in twenty-two states and planned on recruiting delegates for each of the 435 House Districts. The delegates were required to form committees to press Congressional Members to favor suffrage and make them aware that their constituents were in favor of women attaining the vote. Another strategy Stevens began implementing early in 1916 required CUWS members to go to other states in which women were allowed to vote, establish residence and register to vote. In this way, they could vote in state and national elections in the hope of filling the legislature with legislators who favored suffrage. Stevens registered to vote in Kansas that year. On 5 June 1916, the CUWS became the National Woman's Party (NWP), having a single platform to acquire a constitutional amendment for national women's suffrage. After attending the NWP convention in Chicago in June, Stevens headed to a convention in Colorado. By October, Stevens was organizing and managing the NWP election campaign in California.\n\nDue to the United States' entry into World War I, some suffragists stopped their activism in 1917 because it might be seen as \"unpatriotic;\" Stevens, instead, insisted that it was \"arrogant of Wilson to fight for democracy abroad when women were not included in democracy at home.\" In January after a delegation of NWP members had a disappointing meeting with President Woodrow Wilson, it was decided that they would protest at the White House every day, standing as Silent Sentinels until Wilson recognized the importance of their cause. The women maintained their post for over a year disregarding weather conditions and the threat of arrest. Though she performed other organizational tasks, such as organizing the North Carolina branch of the NWP in March, Stevens participated as a sentinel. She and fifteen other women were arrested for picketing at the White House on Bastille Day, in July 1917, charged with obstructing the sidewalk, and served three days of their 60-day sentence at Occoquan Workhouse before receiving a pardon from President Wilson. The women were placed within the prison population, given no toothbrushes, combs or toiletries and were surprised that they were required to share a water dipper with the rest of the prisoners.\n\nStevens met her first husband, Dudley Field Malone, when he represented her for her protest in front of the White House. He had been serving as an Assistant Secretary of State in the Wilson cabinet, but was converted to the suffragist cause and resigned his post. He appeared with Stevens at fundraising events and helped raise thousands of dollars for their cause, which was gaining momentum, as President Wilson finally endorsed enfranchisement. Between 1918 and 1919, Stevens continued alternating speaking engagements and picketing. She was arrested again, along with Elsie Hill, Alice Paul and three \"Jane Doe\" suffragists at the NWP demonstration of the Metropolitan Opera House in New York in March 1919. On 4 September 1920, the fight was won when Secretary of State Bainbridge Colby proclaimed the necessary 36 states had ratified the 19th Amendment with Tennessee's ratification. Stevens published the quintessential insider account of the imprisonment of NWP activists, \"Jailed for Freedom\", in 1920.\n\nOver the years, Stevens held several important NWP leadership positions, including Legislative Chairman and membership on the executive committee. In 1920, Alva Belmont was elected president of the NWP and Stevens served as Belmont’s personal assistant, even writing Belmont's autobiography. Belmont and Steven's relationship was contentious, but the younger Stevens accepted years of control by Belmont over many of her personal actions. Traveling to Europe with Belmont for work of the NWP, Belmont insisted that Steven's fiance could not join them and when he did, Belmont removed to France without Stevens.\n\nOn 5 December 1921 in Peekskill, New York, Stevens and Malone were secretly married by a hardware store owner who was a Justice of the Peace and immediately sailed for their two-month honeymoon in Paris. Stevens announced she would not take Malone's name and would remain \"Doris Stevens\". From the middle of the 1920s, Stevens lived primarily in Croton-on-Hudson, New York, where she became friends with leading members of the Greenwich Village radical scene and bohemians, including Louise Bryant, Max and Crystal Eastman, Edna St. Vincent Millay, John Reed and others. Stevens divorced Malone in 1929 after a string of infidelities on both sides and failed attempts at reconciliation.\n\nThe focus of the NWP shifted to equality under the law, including equal employment opportunities, jury service, nationality for married women and any other provision which legally prohibited women from having full legal equality. In 1923, the Equal Rights Amendment was introduced by Daniel Read Anthony, Jr. and the women pushed for its passage, lobbying for support from both political parties. Stevens served as vice chair of NWP’s New York branch, spearheading the NWP Women for Congress campaign in 1924. Unable to run herself due to her having established a legal residence in France, Stevens worked toward the goal of securing the election of 100 women to Congress in states where female candidates were among contenders for office. The campaign had negligible results and the women shifted back to equality measures. Beginning in 1926, one of the proposals Stevens focused on for the next several years was the \"Wages for Wives\" marriage contract. Campaigning vigorously for its adoption, the \"Wages for Wives\" proposal called for a flexible contract which split marital assets 50-50 rather than treating married couples as a single entity and called for women to be paid a wage for domestic services and raising children as a protection for children's continuous support.\n\nFrom the end of the War, a growing belief among women's organizations was the notion that all women faced similar problems as subordinates to men and that combining their interests might lead to gains. At the International Council of Women (ICW) conference held in Washington in 1925, the sentiment was expressed by Lady Aberdeen, welcoming all women to the \"sisterhood, of whatever creed, party, section or class they may belong\". In 1927, Stevens and Alice Paul undertook a massive study of how laws affected women's nationality; studying for example, if they lost their nationality by marrying or even became stateless. Stevens met with feminists throughout Europe and held public meetings to gather data, including Dr. Luisa Baralt of Havana, Dr. Ellen Gleditsch of Oslo, Chrystal Macmillan and Sybil Thomas, Viscountess Rhondda of the UK, the Marquesa del Ter of Spain, Maria Vérone of France and Hélène Vacaresco of Romania, as well as various officers of the International Federation of University Women and others. Paul reviewed the laws of each country. Together, they compiled a monumental report, which indexed all laws controlling women's nationality from every country in its native language and then translated each law on an accompanying page. Tables were provided for easy comparison and a synopsis of the laws was given. The report was initially prepared for a meeting that was to take place at the League of Nations in 1930 to discuss codification of international laws. Stevens felt that nationality of women should be included in that discussion and spearheaded the research, believing \"feminism should strive for equal rights for women, and that women should be considered first and foremost as human beings.\" In September 1927, she attended a preliminary meeting of the League of Nations in Geneva and obtained their unanimous support of her proposal. She continued meeting with women and gathering data until January 1928, when she attended the Pan-American Conference in Havana. Stevens convinced the governing body of the Pan American Union to create the Inter-American Commission of Women () (CIM) on 4 April 1928. \n\nThe initial Inter-American Commission of Women (CIM) was made up of seven women delegates who were charged with finalizing the report for the next Pan-American Conference (1933) to review civil and political equality for women. Stevens served as chair of the CIM from its creation in 1928 until her ouster in 1938. By August, Stevens was back in Paris working on the report. She and other suffragists picketed the French president, Gaston Doumergue, in 1928 in an attempt to get the world peace delegates to support an equal rights treaty. They were dismissively described by a journalist who did cover the event as \"militant suffragettes,\" and a Paris paper called the protest \"an amusing incident.\" Though arrested, they were released upon providing proof of their identities.\n\nIn 1929, Stevens returned to the United States and began to study law, taking classes at the American University and Columbia University in international law and foreign policy. In 1930, she returned to Havana in February for the first meeting of the CIM women which included Flora de Oliveira Lima (Brazil), Aída Parada (Chile), Lydia Fernández (Costa Rica), Elena Mederos de González (Cuba), Gloria Moya de Jiménez (Dominican Republic), Irene de Peyré (Guatemala), Margarita Robles de Mendoza (Mexico), Juanita Molina de Fromen (Nicaragua), Clara González (Panama), Teresa Obregoso de Prevost (Peru). From Cuba, she went to The Hague for the first World Conference on the Codification of International Law held on 13 March. Presenting her data on what had been accomplished in the Americas, Stevens asked that the international community enact laws to protect women's citizenship. She returned to the United States and her studies. Though she didn't graduate, in 1931 she became the first woman member of the American Institute of International Law. That same year, she, Belmont and Paul attended the League of Nations meeting in September to present their nationality findings.\n\nStevens was very active in working with Latina feminists through the CIM. At the Seventh Pan-American Conference, held in 1933 in Montevideo, Uruguay, the women presented their analysis of the legal status of women in each of the 21 member countries. The first report ever to study in detail the civil and political rights of women, it had been prepared solely by women. They proposed a Treaty on the Equality of Rights for Women, and it was rejected by the conference, though it was signed by Cuba, Ecuador, Paraguay, and Uruguay. Three of those states had already granted suffrage to women, and none of the four ratified the Treaty after the conference. However, the women had presented the first international resolution to recommend suffrage for women. Next, Stevens presented their materials which showed the disparity between rights of men and women. For example, in 16 countries of the Americas women could not vote at all, in two countries they could vote with restrictions, and in three countries they had equal enfranchisement. In 19 of the American countries, women did not have equal custody over their children, including in seven US states, and only two countries allowed joint authority for women of their own children. None of the Latin American countries allowed women to serve on juries, and 27 US states prohibited women from participating in juries. Divorce grounds in 14 countries and 28 states were disparate for men and women, and a woman could not administer her own separate property in 13 countries and two US states.\n\nAfter reviewing the data, the conference approved the first international agreement ever adopted on women's rights. The Convention on the Nationality of Women made it clear that should a woman marry a man of a different nationality, her citizenship could be retained. The text stated, \"There shall be no distinction based on sex as regards to nationality\". The conference also passed the Convention on Nationality, which established that neither marriage nor divorce could affect the nationality of the members of a family, extending citizenship protection to children as well. The Roosevelt administration, hoping to get rid of Stevens, then argued that the women's task was completed and the CIM should be abandoned. Not wanting to bow to US pressure, the Conference did not vote to continue the CIM, but instead voted as a unit, with the exception of Argentina, to block the US proposal.\n\nIt would take FDR another five years, with the help of the League of Women Voters to replace Stevens. Making the argument that Stevens was appointed by the Conference of the Pan-American States and not as a U.S. delegate, FDR agreed to give permanent status to the CIM, if each state was allowed to appoint their own delegates. Securing approval, he then immediately replaced Stevens with Mary Nelson Winslow. Stevens did not go quietly and the clash continued throughout 1939 with Eleanor Roosevelt backing Winslow and suffragists backing Stevens. Eleanor's objection to Stevens was multi-faced, in that she did not think that the Equal Rights Amendment would protect women and on a personal level, she believed Stevens behaved in an unladylike manner.\nIn 1940, Stevens was elected to serve on the National Council of the National Woman's Party. The following year, when Alice Paul returned from a two-year trip to Switzerland to establish the World Woman’s Party (WWP), difficulties arose. Paul experienced both challenges to the direction she was taking the NWP and had personality conflicts with members, including Stevens. When Alva Belmont died in 1933, the bequest she had promised Stevens for years of personal service was instead directed to the NWP. Stevens sued the estate, eventually receiving US$12,000, but she believed that Paul had sabotaged her relationship with Belmont. After Paul's resignation in 1945, Stevens did not support Paul's hand-selected replacement, Anita Pollitzer and led an unsuccessful attempt to challenge her leadership. Pollitzer was seen as a figurehead for Paul and an internal dispute arose over the NWP’s emphasis on the WWP and international rights rather than domestic organizing. During these tensions, a dissenting faction of NWP members tried to take over party headquarters and elect their own slate of officers, but Pollitzer’s claim to leadership was supported by a ruling of a federal district judge.\n\nStevens parted ways with the NWP in 1947 and turned instead to activity in the Lucy Stone League, a women’s rights organization based on Lucy Stone's retention of her maiden name after marriage. After World War II ended, the organization was revived in 1950 because the rights women had seen surge during the war were reverting to their pre-war state. Stevens was one of the reorganizers along with Freda Kirchwey, Frances Perkins and others. Stevens had long been a proponent of a woman retaining her own name and did not take her husband's name in either of her marriages. She had remarried to Jonathan Mitchell on August 31, 1935 in Portland, Maine. Mitchell was a reporter for \"The New Republic\" during the Roosevelt years and later for the \"National Review\", and was an anti-communist. He took part in the McCarthy hearings and Stevens, after her marriage to him, moved politically to the right, from her previously socialist leanings.\n\nFrom 1951 to 1963, Stevens served as vice-president of the Lucy Stone League, though she struggled with maintaining militancy. Stevens was not anti-male, rather pro-female. She did not follow a belief that for women to succeed, men had to be omitted; rather, she believed that collaboration with men was essential. In her last years, Stevens supported the establishment of feminist studies as a legitimate field of academic inquiry in American universities and tried to establish a Lucy Stone Chair of Feminism at Radcliffe College.\n\nStevens died on March 22, 1963 in New York City, two weeks after having a stroke. Princeton University has an endowed chair in women's studies created by the Doris Stevens Foundation in 1986.\n\nIn 1986, Princeton University established an endowed chair through the Doris Stevens Foundation in women’s studies. In 2004 the HBO film \"Iron Jawed Angels\" was made about the early days of the suffrage movement. Doris Stevens was portrayed by Laura Fraser.\n\n\n\n\n"}
{"id": "1057638", "url": "https://en.wikipedia.org/wiki?curid=1057638", "title": "Einstein tensor", "text": "Einstein tensor\n\nIn differential geometry, the Einstein tensor (named after Albert Einstein; also known as the trace-reversed Ricci tensor) is used to express the curvature of a pseudo-Riemannian manifold. In general relativity, it occurs in the Einstein field equations for gravitation that describe spacetime curvature in a manner consistent with energy and momentum conservation.\n\nThe Einstein tensor formula_1 is a tensor of order 2 defined over pseudo-Riemannian manifolds. In index-free notation it is defined as\n\nwhere formula_3 is the Ricci tensor, formula_4 is the metric tensor and formula_5 is the scalar curvature. In component form, the previous equation reads as\n\nThe Einstein tensor is symmetric\n\nand, like the stress–energy tensor, divergenceless\n\nThe Ricci tensor depends only on the metric tensor, so the Einstein tensor can be defined directly with just the metric tensor. However, this expression is complex and rarely quoted in textbooks. The complexity of this expression can be shown using the formula for the Ricci tensor in terms of Christoffel symbols:\n\nwhere formula_10 is the Kronecker tensor and the Christoffel symbol formula_11 is defined as\n\nBefore cancellations, this formula results in formula_13 individual terms. Cancellations bring this number down somewhat. \n\nIn the special case of a locally inertial reference frame near a point, the first derivatives of the metric tensor vanish and the component form of the Einstein tensor is considerably simplified:\n\nwhere square brackets conventionally denote antisymmetrization over bracketed indices, i.e.\n\nThe trace of the Einstein tensor can be computed by contracting the equation in the definition with the metric tensor formula_16. In formula_17 dimensions (of arbitrary signature):\n"}
{"id": "15895296", "url": "https://en.wikipedia.org/wiki?curid=15895296", "title": "Equisatisfiability", "text": "Equisatisfiability\n\nIn logic, two formulae are equisatisfiable if the first formula is satisfiable whenever the second is and vice versa; in other words, either both formulae are satisfiable or both are not. Equisatisfiable formulae may disagree, however, for a particular choice of variables. As a result, equisatisfiability is different from logical equivalence, as two equivalent formulae always have the same models.\n\nEquisatisfiability is generally used in the context of translating formulae, so that one can define a translation to be correct if the original and resulting formulae are equisatisfiable. Examples of translations involving this concept are Skolemization and some translations into conjunctive normal form. \n\nA translation from propositional logic into propositional logic in which every binary disjunction formula_1 is replaced by formula_2, where formula_3 is a new variable (one for each replaced disjunction) is a transformation in which satisfiability is preserved: the original and resulting formulae are equisatisfiable. Note that these two formulae are not equivalent: the first formula has the model in which formula_4 is true while formula_5 and formula_3 are false, and this is not a model of the second formula, in which formula_3 has to be true in this case.\n"}
{"id": "277206", "url": "https://en.wikipedia.org/wiki?curid=277206", "title": "Freethought", "text": "Freethought\n\nFreethought (or \"free thought\") is a philosophical viewpoint which holds that positions regarding truth should be formed on the basis of logic, reason, and empiricism, rather than authority, tradition, revelation, or dogma. In particular, freethought is strongly tied with rejection of traditional social or religious belief systems. The cognitive application of freethought is known as \"freethinking\", and practitioners of freethought are known as \"freethinkers\". The term first came into use in the 17th century in order to indicate people who inquired into the basis of traditional religious beliefs.\n\nFreethinkers hold that knowledge should be grounded in facts, scientific inquiry, and logic. The skeptical application of science implies freedom from the intellectually limiting effects of confirmation bias, cognitive bias, conventional wisdom, popular culture, prejudice, or sectarianism.\n\nModern freethinkers consider freethought as a natural freedom of all negative and illusive thoughts acquired from the society \n\nAtheist author Adam Lee defines freethought as thinking which is independent of revelation, tradition, established belief, and authority, and considers it as a \"broader umbrella\" than atheism \"that embraces a rainbow of unorthodoxy, religious dissent, skepticism, and unconventional thinking.\"\n\nThe basic summarizing statement of the essay \"The Ethics of Belief\" by the 19th-century British mathematician and philosopher William Kingdon Clifford is: \"It is wrong always, everywhere, and for anyone, to believe anything upon insufficient evidence.\" The essay became a rallying cry for freethinkers when published in the 1870s, and has been described as a point when freethinkers grabbed the moral high ground. Clifford was himself an organizer of freethought gatherings, the driving force behind the Congress of Liberal Thinkers held in 1878.\n\nRegarding religion, freethinkers typically hold that there is insufficient evidence to support the existence of supernatural phenomena. According to the Freedom from Religion Foundation, \"No one can be a freethinker who demands conformity to a bible, creed, or messiah. To the freethinker, revelation and faith are invalid, and orthodoxy is no guarantee of truth.\" and \"Freethinkers are convinced that religious claims have not withstood the tests of reason. Not only is there nothing to be gained by believing an untruth, but there is everything to lose when we sacrifice the indispensable tool of reason on the altar of superstition. Most freethinkers consider religion to be not only untrue, but harmful.\"\n\nHowever, philosopher Bertrand Russell wrote the following in his 1944 essay \"The Value of Free Thought:\"\n\nThe whole first paragraph of the essay makes it clear that a freethinker is not necessarily an atheist or an agnostic, as long as he or she satisfies this definition:\nFred Edwords, former executive of the American Humanist Association, suggests that by Russell's definition, liberal religionists who have challenged established orthodoxies can be considered freethinkers.\n\nOn the other hand, according to Bertrand Russell, atheists and/or agnostics are not necessarily freethinkers. As an example, he mentions Stalin, whom he compares to a \"pope\":\nIn the 18th and 19th century, many thinkers regarded as freethinkers were deists, arguing that the nature of God can only be known from a study of nature rather than from religious revelation. In the 18th century, \"deism\" was as much of a 'dirty word' as \"atheism\", and deists were often stigmatized as either atheists or at least as freethinkers by their Christian opponents. Deists today regard themselves as freethinkers, but are now arguably less prominent in the freethought movement than atheists.\n\nThe pansy serves as the long-established and enduring symbol of freethought; literature of the American Secular Union inaugurated its usage in the late 1800s. The reasoning behind the pansy as the symbol of freethought lies both in the flower's name and in its appearance. The pansy derives its name from the French word \"pensée\", which means \"thought\". It allegedly received this name because the flower is perceived by some to bear resemblance to a human face, and in mid-to-late summer it nods forward as if deep in thought.\n\nCritical thought has flourished in the Hellenistic Mediterranean, in the repositories of knowledge and wisdom in Ireland and in the Iranian civilizations (for example in the era of Khayyam (1048–1131) and his unorthodox Sufi \"Rubaiyat\" poems), and in other civilizations, such as the Chinese (note for example the seafaring renaissance of the Southern Song dynasty of 420–479), and on through heretical thinkers on esoteric alchemy or astrology, to the Renaissance and the Protestant Reformation.\n\nFrench physician and writer Rabelais celebrated \"rabelaisian\" freedom as well as good feasting and drinking (an expression and a symbol of freedom of the mind) in defiance of the hypocrisies of conformist orthodoxy in his utopian Thelema Abbey (from θέλημα: free \"will\"), the device of which was \"Do What Thou Wilt\":\n\nSo had Gargantua established it. In all their rule and strictest tie of their order there was but this one clause to be observed, Do What Thou Wilt; because free people ... act virtuously and avoid vice. They call this honor.\nWhen Rabelais's hero Pantagruel journeys to the \"Oracle of The Div(in)e Bottle\", he learns the lesson of life in one simple word: \"Trinch!\", Drink! Enjoy the simple life, learn wisdom and knowledge, as a free human. Beyond puns, irony, and satire, Gargantua's prologue-metaphor instructs the reader to \"break the bone and suck out the substance-full marrow\" (\"la substantifique moëlle\"), the core of wisdom.\n\nThe year 1600 is considered a landmark in the era of modern freethought. It was the year of the execution in Italy of Giordano Bruno, a former Dominican Monk, by the Inquisition.\n\nThe term \"free-thinker\" emerged towards the end of the 17th century in England to describe those who stood in opposition to the institution of the Church, and the literal belief in the Bible. The beliefs of these individuals were centered on the concept that people could understand the world through consideration of nature. Such positions were formally documented for the first time in 1697 by William Molyneux in a widely publicized letter to John Locke, and more extensively in 1713, when Anthony Collins wrote his \"Discourse of Free-thinking,\" which gained substantial popularity. This essay attacks the clergy of all churches and it is a plea for deism.\n\n\"The Freethinker\" magazine was first published in Britain in 1881.\n\nIn France, the concept first appeared in publication in 1765 when Denis Diderot, Jean le Rond d'Alembert, and Voltaire included an article on \"Liberté de penser\" in their Encyclopédie. The European freethought concepts spread so widely that even places as remote as the Jotunheimen, in Norway, had well-known freethinkers such as Jo Gjende by the 19th century.\n\nFrançois-Jean Lefebvre de la Barre (1745–1766) was a young French nobleman, famous for having been tortured and beheaded before his body was burnt on a pyre along with Voltaire's \"Philosophical Dictionary\". La Barre is often said to have been executed for not saluting a Roman Catholic religious procession, but the elements of the case were far more complex.\n\nIn France, Lefebvre de la Barre is widely regarded a symbol of the victims of Christian religious intolerance; La Barre along with Jean Calas and Pierre-Paul Sirven, was championed by Voltaire. A second replacement statue to de la Barre stands nearby the Basilica of the Sacred Heart of Jesus of Paris at the summit of the butte Montmartre (itself named from the \"Temple of Mars\"), the highest point in Paris and an 18th arrondissement street nearby the Sacré-Cœur is also named after Lefebvre de la Barre.\n\nIn Germany, during the period 1815–1848 and before the March Revolution, the resistance of citizens against the dogma of the church increased. In 1844, under the influence of Johannes Ronge and Robert Blum, belief in the rights of man, tolerance among men, and humanism grew, and by 1859 they had established the \"Bund Freireligiöser Gemeinden Deutschlands\" (literally \"Union of Free Religious Communities of Germany\"), an association of persons who consider themselves to be religious without adhering to any established and institutionalized church or sacerdotal cult. This union still exists today, and is included as a member in the umbrella organization of free humanists. In 1881 in Frankfurt am Main, Ludwig Büchner established the \"Deutscher Freidenkerbund\" (German Freethinkers League) as the first German organization for atheists and agnostics. In 1892 the \"Freidenker-Gesellschaft\" and in 1906 the \"Deutscher Monistenbund\" were formed.\n\nFreethought organizations developed the \"Jugendweihe\" (literally \"Youth consecration\"), a secular \"confirmation\" ceremony, and atheist funeral rites. The Union of Freethinkers for Cremation was founded in 1905, and the Central Union of German Proletariat Freethinker in 1908. The two groups merged in 1927, becoming the German Freethinking Association in 1930.\n\nMore \"bourgeois\" organizations declined after World War I, and \"proletarian\" Freethought groups proliferated, becoming an organization of socialist parties. European socialist freethought groups formed the International of Proletarian Freethinkers (IPF) in 1925. Activists agitated for Germans to disaffiliate from their respective Church and for seculari-zation of elementary schools; between 1919–21 and 1930–32 more than 2.5 million Germans, for the most part supporters of the Social Democratic and Communist parties, gave up church membership. Conflict developed between radical forces including the Soviet League of the Militant Godless and Social Democratic forces in Western Europe led by Theodor Hartwig and Max Sievers. In 1930 the Soviet and allied delegations, following a walk-out, took over the IPF and excluded the former leaders.\nFollowing Hitler's rise to power in 1933, most freethought organizations were banned, though some right-wing groups that worked with so-called \"Völkische Bünde\" (literally \"\"ethnic\" associations\" with nationalist, xenophobic and very often racist ideology) were tolerated by the Nazis until the mid-1930s.\n\nThe Université Libre de Bruxelles and the Vrije Universiteit Brussel, along with the two Circles of Free Inquiry (Dutch and French speaking), defend the freedom of critical thought, lay philosophy and ethics, while rejecting the argument of authority.\n\nIn the Netherlands, freethought has existed in organized form since the establishment of De Dageraad (now known as De Vrije Gedachte) in 1856. Among its most notable subscribing 19th century individuals were Johannes van Vloten, Multatuli, Adriaan Gerhard and Domela Nieuwenhuis.\n\nIn 2009, Frans van Dongen established the Atheist-Secular Party, which takes a considerably restrictive view of religion and public religious expressions.\n\nSince the 19th century, Freethought in the Netherlands has become more well known as a political phenomenon through at least three currents: liberal freethinking, conservative freethinking, and classical freethinking. In other words, parties which identify as freethinking tend to favor non-doctrinal, rational approaches to their preferred ideologies, and arose as secular alternatives to both clerically aligned parties as well as labor-aligned parties. Common themes among freethinking political parties are \"freedom\", \"liberty\", and \"individualism\".\n\nThe Free Thought movement first organized itself in the United States as the \"Free Press Association\" in 1827 in defense of George Houston, publisher of \"The Correspondent\", an early journal of Biblical criticism in an era when blasphemy convictions were still possible. Houston had helped found an Owenite community at Haverstraw, New York in 1826–27. The short-lived \"Correspondent\" was superseded by the \"Free Enquirer\", the official organ of Robert Owen's New Harmony community in Indiana, edited by Robert Dale Owen and by Fanny Wright between 1828 and 1832 in New York. During this time Robert Dale Owen sought to introduce the philosophic skepticism of the Free Thought movement into the Workingmen's Party in New York City. The \"Free Enquirer\"'s annual civic celebrations of Paine's birthday after 1825 finally coalesced in 1836 in the first national Free Thinkers organization, the \"United States Moral and Philosophical Society for the General Diffusion of Useful Knowledge\". It was founded on August 1, 1836, at a national convention at the Lyceum in Saratoga Springs with Isaac S. Smith of Buffalo, New York, as president. Smith was also the 1836 Equal Rights Party's candidate for Governor of New York and had also been the Workingmen's Party candidate for Lt. Governor of New York in 1830. The Moral and Philosophical Society published \"The Beacon\", edited by Gilbert Vale.\nDriven by the revolutions of 1848 in the German states, the 19th century saw an immigration of German freethinkers and anti-clericalists to the United States (see Forty-Eighters). In the United States, they hoped to be able to live by their principles, without interference from government and church authorities.\n\nMany Freethinkers settled in German immigrant strongholds, including St. Louis, Indianapolis, Wisconsin, and Texas, where they founded the town of Comfort, Texas, as well as others.\n\nThese groups of German Freethinkers referred to their organizations as \"Freie Gemeinden\", or \"free congregations\". The first Freie Gemeinde was established in St. Louis in 1850. Others followed in Pennsylvania, California, Washington, D.C., New York, Illinois, Wisconsin, Texas, and other states.\n\nFreethinkers tended to be liberal, espousing ideals such as racial, social, and sexual equality, and the abolition of slavery.\n\nThe \"Golden Age of Freethought\" in the US came in the late 1800s. The dominant organization was the National Liberal League which formed in 1876 in Philadelphia. This group re-formed itself in 1885 as the American Secular Union under the leadership of the eminent agnostic orator Robert G. Ingersoll. Following Ingersoll's death in 1899 the organization declined, in part due to lack of effective leadership.\n\nFreethought in the United States declined in the early twentieth century. Its anti-religious views alienated would-be sympathizers. The movement also lacked cohesive goals or beliefs. By the early twentieth century, most Freethought congregations had disbanded or joined other mainstream churches. The longest continuously operating Freethought congregation in America is the Free Congregation of Sauk County, Wisconsin, which was founded in 1852 and is still active . It affiliated with the American Unitarian Association (now the Unitarian Universalist Association) in 1955. D. M. Bennett was the founder and publisher of \"The Truth Seeker\" in 1873, a radical freethought and reform American periodical.\n\nGerman Freethinker settlements were located in:\n\nIn 1873 a handful of secularists founded the earliest known secular organization in English Canada, the Toronto Freethought Association. Reorganized in 1877 and again in 1881, when it was renamed the Toronto Secular Society, the group formed the nucleus of the Canadian Secular Union, established in 1884 to bring together freethinkers from across the country.\n\nA significant number of the early members appear to have come from the educated labour \"aristocracy\", including Alfred F. Jury, J. Ick Evans and J. I. Livingstone, all of whom were leading labour activists and secularists. The second president of the Toronto association, T. Phillips Thompson, became a central figure in the city's labour and social-reform movements during the 1880s and 1890s and arguably Canada's foremost late nineteenth-century labour intellectual. By the early 1880s scattered freethought organizations operated throughout southern Ontario and parts of Quebec, eliciting both urban and rural support.\n\nThe principal organ of the freethought movement in Canada was \"Secular Thought\" (Toronto, 1887–1911). Founded and edited during its first several years by English freethinker Charles Watts (1835–1906), it came under the editorship of Toronto printer and publisher James Spencer Ellis in 1891 when Watts returned to England.\n\nIn 1968 the Humanist Association of Canada (HAC) formed to serve as an umbrella group for humanists, atheists, and freethinkers, and to champion social justice issues and oppose religious influence on public policy—most notably in the fight to make access to abortion free and legal in Canada. HAC, also known as Humanist Canada, operates as an active voice for humanism in Canada and supports the activities of groups who wish to raise awareness about secular issues.\n\nThe Canadian Secular Alliance is an active community.\n\nIn the United States, \"freethought was a basically anti-christian, anti-clerical movement, whose purpose was to make the individual politically and spiritually free to decide for himself on religious matters. A number of contributors to \"Liberty\" were prominent figures in both freethought and anarchism. The individualist anarchist George MacDonald was a co-editor of \"Freethought\" and, for a time, \"The Truth Seeker.\" E.C. Walker was co-editor of the freethought/free love journal \"Lucifer, the Light-Bearer\".\" \"Many of the anarchists were ardent freethinkers; reprints from freethought papers such as \"Lucifer, the Light-Bearer\", \"Freethought\" and \"The Truth Seeker\" appeared in \"Liberty\"...The church was viewed as a common ally of the state and as a repressive force in and of itself.\"\n\nIn Europe, a similar development occurred in French and Spanish individualist anarchist circles. \"Anticlericalism, just as in the rest of the libertarian movement, in another of the frequent elements which will gain relevance related to the measure in which the (French) Republic begins to have conflicts with the church...Anti-clerical discourse, frequently called for by the French individualist André Lorulot, will have its impacts in \"Estudios\" (a Spanish individualist anarchist publication). There will be an attack on institutionalized religion for the responsibility that it had in the past on negative developments, for its irrationality which makes it a counterpoint of philosophical and scientific progress. There will be a criticism of proselitism and ideological manipulation which happens on both believers and agnostics\". These tendencies will continue in French individualist anarchism in the work and activism of Charles-Auguste Bontemps and others. In the Spanish individualist anarchist magazines \"Ética\" and \"Iniciales\" \"there is a strong interest in publishing scientific news, usually linked to a certain atheist and anti-theist obsession, philosophy which will also work for pointing out the incompatibility between science and religion, faith, and reason. In this way there will be a lot of talk on Darwin´s theories or on the negation of the existence of the soul\".\n\nIn 1901, Catalan anarchist and freethinker Francesc Ferrer i Guàrdia established \"modern\" or progressive schools in Barcelona in defiance of an educational system controlled by the Catholic Church. The schools' stated goal was to \"educate the working class in a rational, secular and non-coercive setting\". Fiercely anti-clerical, Ferrer believed in \"freedom in education\", education free from the authority of church and state. Ferrer's ideas, generally, formed the inspiration for a series of Modern Schools in the United States, Cuba, South America and London. The first of these was started in New York City in 1911. It also inspired the Italian newspaper \"Università popolare\", founded in 1901.\n\n\n"}
{"id": "583532", "url": "https://en.wikipedia.org/wiki?curid=583532", "title": "Function type", "text": "Function type\n\nIn computer science, a function type (or arrow type or exponential) is the type of a variable or parameter to which a function has or can be assigned, or an argument or result type of a higher-order function taking or returning a function.\n\nA function type depends on the type of the parameters and the result type of the function (it, or more accurately the unapplied type constructor codice_1, is a higher-kinded type). In theoretical settings and programming languages where functions are defined in curried form, such as the simply typed lambda calculus, a function type depends on exactly two types, the domain \"A\" and the range \"B\". Here a function type is often denoted , following mathematical convention, or , based on there existing exactly (exponentially many) set-theoretic functions mappings \"A\" to \"B\" in the category of sets. The class of such maps or functions is called the exponential object. The act of currying makes the function type adjoint to the product type; this is explored in detail in the article on currying.\n\nThe function type can be considered to be a special case of the dependent product type, which among other properties, encompasses the idea of a polymorphic function.\n\nThe syntax used for function types in several programming languages can be summarized, including an example type signature for the higher-order function composition function:\nWhen looking at the example type signature of, for example C#, the type of the function is actually codice_2.\n\nDue to type erasure in C++11's codice_3, it is more common to use templates for higher order function parameters and type inference (codice_4) for closures.\n\nThe function type in programming languages does not correspond to the space of all set-theoretic functions. Given the countably infinite type of natural numbers as the domain and the booleans as range, then there are an uncountably infinite number (2 = c) of set-theoretic functions between them. Clearly this space of functions is larger than the number of functions that can be defined in any programming language, as there exist only countably many programs (a program being a finite sequence of a finite number of symbols) and one of the set-theoretic functions effectively solves the halting problem.\n\nDenotational semantics concerns itself with finding more appropriate models (called domains) to model programming language concepts such as function types. It turns out that restricting expression to the set of computable functions is not sufficient either if the programming language allows writing non-terminating computations (which is the case if the programming language is Turing complete). Expression must be restricted to the so-called \"continuous functions\" (corresponding to continuity in the Scott topology, not continuity in the real analytical sense). Even then, the set of continuous function contains the \"parallel-or\" function, which cannot be correctly defined in all programming languages.\n\n\n"}
{"id": "8150709", "url": "https://en.wikipedia.org/wiki?curid=8150709", "title": "Geek Pride Festival", "text": "Geek Pride Festival\n\nThe Geek Pride Festival was the name of a number of events between 1998 and 2000, organized by Tim McEachern and devoted to computer geek activities and interests. The name of the festival is most often associated with the large event held on March 31 and April 1, 2000 at the Park Plaza Castle in Boston, United States.\n\nBefore that there were two events at the now closed Big House Brewery in Albany, New York. WAMC, the local NPR affiliate, sponsored the events which were organized by Tim McEachern.\n\nThe 2000 event was a major production, organized with the help of Susan Kaup, Chris O'Brien and many volunteers.\n\nThe event began Friday night, with a swap meet / social event at the Modern Lounge in Boston's Landsdowne Street nightclub district. Drink tickets were offered at the door, and the DJ played computer-themed music (e.g., \"It's All About The Pentiums\").\n\nOn Saturday, the main event occurred at the Castle, where admission was free. The middle of the floor held the \"Email Garden\", comprising about a dozen tables with PCs running Red Hat Linux, in a wired LAN network and providing email, Web, and general Internet access. At the front of the hall was a stage, which hosted a number of invited guests, including Rob Malda of Slashdot, Eric S. Raymond, the video game cover band Everyone. The stage was also host to the final round of a Quake III tournament, held in a back room, displayed on the stage's projection screen, as well as the final round of \"Stump the Geek\", a geek trivia contest.\n\nAside from the main events, the main floor had computer workstations displaying live webcam feeds of \"satellite\" Festivals in remote locations. A live Shoutcast feed was also provided of the Boston event. A poll for \"greatest geek hero\" was also held; the official winner was Alan Turing.\n\nAccording to Science/AAAS magazine, 2,000 people attended, though the open-door free admission made an official count impossible.\n\n\n\nThe 2000 event is widely referred to as the \"first annual\" event, although McEachern organized at least one previous event named Geek Pride Festival (and/or Geek Pride Day) at a bar in Albany, New York. Some sources refer to the Boston event as the \"third\" annual.\n\nMcEachern planned another event to take place later the same year in San Francisco, but was never realized.\n\n"}
{"id": "30051538", "url": "https://en.wikipedia.org/wiki?curid=30051538", "title": "Grothendieck category", "text": "Grothendieck category\n\nIn mathematics, a Grothendieck category is a certain kind of abelian category, introduced in Alexander Grothendieck's Tôhoku paper of 1957 in order to develop the machinery of homological algebra for modules and for sheaves in a unified manner. The theory of these categories was further developed in Peter Gabriel's seminal thèse in 1962.\n\nTo every algebraic variety formula_1 one can associate a Grothendieck category formula_2, consisting of the quasi-coherent sheaves on formula_1. This category encodes all the relevant geometric information about formula_1, and formula_1 can be recovered from formula_2 (the Gabriel–Rosenberg reconstruction theorem). This example gives rise to one approach to noncommutative algebraic geometry: the study of \"non-commutative varieties\" is then nothing but the study of (certain) Grothendieck categories.\n\nBy definition, a Grothendieck category formula_7 is an AB5 category with a generator. Spelled out, this means that\n\nThe name \"Grothendieck category\" neither appeared in Grothendieck's Tôhoku paper nor in Gabriel's thesis; it came into use in the second half of the 1960s by authors including J.-E. Roos, B. Stenström, U. Oberst, and B. Pareigis. (Some authors use a different definition in that they don't require the existence of a generator.)\n\n\n\nEvery Grothendieck category contains an injective cogenerator. For example, an injective cogenerator of the category of abelian groups is the quotient group formula_55.\n\nEvery object in a Grothendieck category formula_7 has an injective hull in formula_7. This allows to construct injective resolutions and thereby the use of the tools of homological algebra in formula_7, such as derived functors. (Note that not all Grothendieck categories allow projective resolutions for all objects; examples are categories of sheaves of abelian groups on many topological spaces, such as on the space of real numbers.)\n\nIn a Grothendieck category, any family of subobjects formula_59 of a given object formula_17 has a supremum (or \"sum\") formula_61 as well as an infimum (or \"intersection\") formula_62, both of which are again subobjects of formula_17. Further, if the family formula_59 is directed (i.e. for any two objects in the family, there is a third object in the family that contains the two), and formula_1 is another subobject of formula_17, we have\n\nGrothendieck categories are well-powered (sometimes called \"locally small\", although that term is also used for a different concept), i.e. the collection of subobjects of any given object forms a set (rather than a proper class).\n\nAn object formula_17 in a Grothendieck category is called \"finitely generated\" if the sum of every directed family of proper subobjects of formula_17 is again a proper subobject of formula_17. (In the case formula_71 of module categories, this notion is equivalent to the familiar notion of finitely generated modules.) A Grothendieck category need not contain any non-zero finitely generated objects. A Grothendieck category is called \"locally finitely generated\" if it has a set of finitely generated generators. In such a category, every object is the sum of its finitely generated subobjects.\n\nIt is a rather deep result that every Grothendieck category formula_7 is complete, i.e. that arbitrary limits (and in particular products) exist in formula_7. By contrast, it follows directly from the definition that formula_7 is co-complete, i.e. that arbitrary colimits and coproducts (direct sums) exist in formula_7. Coproducts in a Grothendieck category are exact (i.e. the coproduct of a family of short exact sequences is again a short exact sequence), but products need not be exact.\n\nA functor formula_76 from a Grothendieck categories formula_7 to an arbitrary category formula_78 has a left adjoint if and only if it commutes with all limits, and it has a right adjoint if and only if it commutes with all colimits. This follows from Freyd's s\"pecial adjoint functor theorem\" and its dual.\n\nThe Gabriel–Popescu theorem states that any Grothendieck category formula_7 is equivalent to a full subcategory of the category formula_26 of right modules over some unital ring formula_24 (which can be taken to be the endomorphism ring of a generator of formula_7), and formula_7 can be obtained as a Gabriel quotient of formula_26 by some localizing subcategory.\n\nAs a consequence of Gabriel–Popescu, one can show that every Grothendieck category is locally presentable.\n\nEvery small abelian category formula_40 can be embedded in a Grothendieck category, in the following fashion. The category formula_86 of left-exact additive (covariant) functors formula_87 (where formula_88 denotes the category of abelian groups) is a Grothendieck category, and the functor formula_89, with formula_90, is full, faithful and exact. A generator of formula_7 is given by the coproduct of all formula_92, with formula_93. The category formula_7 is equivalent to the category formula_95 of ind-objects of formula_40 and the embedding formula_97 corresponds to the natural embedding formula_98.\n\n\n"}
{"id": "42172529", "url": "https://en.wikipedia.org/wiki?curid=42172529", "title": "Institutional analysis and development framework", "text": "Institutional analysis and development framework\n\nThe Institutional Analysis and Development framework (IAD) was developed by Elinor Ostrom, an American political scientist, also known as the first woman to receive the Nobel Memorial Prize in Economic Sciences in 2009. The IAD relates a set of concepts to help in the analysis of collective action problems that involve social structures, positions, and rules. Under the rational choice models, the IAD was devised in an attempt to \"explain and predict\" outcomes by formalising the structures, positions, and rules involved in collective choice problems. Thus, it can be seen as a systematic method to collect policy analysis functions similar to analytic technique commonly used in physical and social sciences and understand how institution operate and change over a period of time. \n\nOstrom thought of the IAD as a \"multi-level conceptual map\" with which one could zoom in and out of particular hierarchical parts of the regularised interactions in an established social system. The IAD framework helps to perceive complex collective action problems by dividing them into 'action arenas', that are smaller pieces of practically understandable function. The analyst assumes that the structure of the action situation is fixed in the short-term. For an action situation to exist, there must be \"actors in positions\" (the number of possible roles that are available in this recurring interaction situation). Actors have choices within the existing (rule) structure. In the study of outcomes from collective choice situations, actors are influenced by the institutional arrangements, the socio-economic conditions, and the physical environment. The institutional arrangements can be studied by seven rule types (as per below).\n"}
{"id": "42693718", "url": "https://en.wikipedia.org/wiki?curid=42693718", "title": "Intergenerational struggle", "text": "Intergenerational struggle\n\nThe intergenerational struggle is the economic conflict between successive generations of workers because of the public pension system where the first generation has better pension benefit and the last must pay more taxes, have a greater tax wedge and a lower pension benefit due to the public debt that the states make in order to pay the current public spending.\n"}
{"id": "14843", "url": "https://en.wikipedia.org/wiki?curid=14843", "title": "Interstellar travel", "text": "Interstellar travel\n\nInterstellar travel is the term used for crewed or uncrewed travel between stars or planetary systems. Interstellar travel will be much more difficult than interplanetary spaceflight; the distances between the planets in the Solar System are less than 30 astronomical units (AU)—whereas the distances between stars are typically hundreds of thousands of AU, and usually expressed in light-years. Because of the vastness of those distances, interstellar travel would require a high percentage of the speed of light; huge travel time, lasting from decades to millennia or longer; or a combination of both.\n\nThe speeds required for interstellar travel in a human lifetime far exceed what current methods of spacecraft propulsion can provide. Even with a hypothetically perfectly efficient propulsion system, the kinetic energy corresponding to those speeds is enormous by today's standards of energy development. Moreover, collisions by the spacecraft with cosmic dust and gas can produce very dangerous effects both to passengers and the spacecraft itself.\n\nA number of strategies have been proposed to deal with these problems, ranging from giant arks that would carry entire societies and ecosystems, to microscopic space probes. Many different spacecraft propulsion systems have been proposed to give spacecraft the required speeds, including nuclear propulsion, beam-powered propulsion, and methods based on speculative physics.\n\nFor both crewed and uncrewed interstellar travel, considerable technological and economic challenges need to be met. Even the most optimistic views about interstellar travel see it as only being feasible decades from now. However, in spite of the challenges, if or when interstellar travel is realised, a wide range of scientific benefits is expected.\n\nMost interstellar travel concepts require a developed space logistics system capable of moving millions of tons to a construction / operating location, and most would require gigawatt-scale power for construction or power (such as Star Wisp or Light Sail type concepts). Such a system could grow organically if space-based solar power became a significant component of Earth's energy mix. Consumer demand for a multi-terawatt system would automatically create the necessary multi-million ton/year logistical system.\n\nDistances between the planets in the Solar System are often measured in astronomical units (AU), defined as the average distance between the Sun and Earth, some . Venus, the closest other planet to Earth is (at closest approach) 0.28 AU away. Neptune, the farthest planet from the Sun, is 29.8 AU away. As of January 2018, Voyager 1, the farthest man-made object from Earth, is 141.5 AU away.\n\nThe closest known star, Proxima Centauri, is approximately away, or over 9,000 times farther away than Neptune.\n\nBecause of this, distances between stars are usually expressed in light-years, defined as the distance that a light photon travels in a year. Light in a vacuum travels around per second, so this is some or in a year. Proxima Centauri is 4.243 light-years away.\n\nAnother way of understanding the vastness of interstellar distances is by scaling: One of the closest stars to the Sun, Alpha Centauri A (a Sun-like star), can be pictured by scaling down the Earth–Sun distance to . On this scale, the distance to Alpha Centauri A would be .\n\nThe fastest outward-bound spacecraft yet sent, Voyager 1, has covered 1/600 of a light-year in 30 years and is currently moving at 1/18,000 the speed of light. At this rate, a journey to Proxima Centauri would take 80,000 years.\n\nA significant factor contributing to the difficulty is the energy that must be supplied to obtain a reasonable travel time. A lower bound for the required energy is the kinetic energy formula_1 where formula_2 is the final mass. If deceleration on arrival is desired and cannot be achieved by any means other than the engines of the ship, then the lower bound for the required energy is doubled to formula_3.\n\nThe velocity for a manned round trip of a few decades to even the nearest star is several thousand times greater than those of present space vehicles. This means that due to the formula_4 term in the kinetic energy formula, millions of times as much energy is required. Accelerating one ton to one-tenth of the speed of light requires at least (world energy consumption 2008 was 143,851 terawatt-hours), without factoring in efficiency of the propulsion mechanism. This energy has to be generated onboard from stored fuel, harvested from the interstellar medium, or projected over immense distances.\n\nA knowledge of the properties of the interstellar gas and dust through which the vehicle must pass is essential for the design of any interstellar space mission. A major issue with traveling at extremely high speeds is that interstellar dust may cause considerable damage to the craft, due to the high relative speeds and large kinetic energies involved. Various shielding methods to mitigate this problem have been proposed. Larger objects (such as macroscopic dust grains) are far less common, but would be much more destructive. The risks of impacting such objects, and methods of mitigating these risks, have been discussed in the literature, but many unknowns remain and, owing to the inhomogeneous distribution of interstellar matter around the Sun, will depend on direction travelled. Although a high density interstellar medium may cause difficulties for many interstellar travel concepts, interstellar ramjets, and some proposed concepts for decelerating interstellar spacecraft, would actually benefit from a denser interstellar medium.\n\nThe crew of an interstellar ship would face several significant hazards, including the psychological effects of long-term isolation, the effects of exposure to ionizing radiation, and the physiological effects of weightlessness to the muscles, joints, bones, immune system, and eyes. There also exists the risk of impact by micrometeoroids and other space debris. These risks represent challenges that have yet to be overcome.\n\nIt has been argued that an interstellar mission that cannot be completed within 50 years should not be started at all. Instead, assuming that a civilization is still on an increasing curve of propulsion system velocity and not yet having reached the limit, the resources should be invested in designing a better propulsion system. This is because a slow spacecraft would probably be passed by another mission sent later with more advanced propulsion (the incessant obsolescence postulate). On the other hand, Andrew Kennedy has shown that if one calculates the journey time to a given destination as the rate of travel speed derived from growth (even exponential growth) increases, there is a clear minimum in the total time to that destination from now. Voyages undertaken before the minimum will be overtaken by those that leave at the minimum, whereas voyages that leave after the minimum will never overtake those that left at the minimum.\n\nThere are 59 known stellar systems within 40 light years of the Sun, containing 81 visible stars. The following could be considered prime targets for interstellar missions:\n\nExisting and near-term astronomical technology is capable of finding planetary systems around these objects, increasing their potential for exploration\n\nSlow interstellar missions based on current and near-future propulsion technologies are associated with trip times starting from about one hundred years to thousands of years. These missions consist of sending a robotic probe to a nearby star for exploration, similar to interplanetary probes such as used in the Voyager program. By taking along no crew, the cost and complexity of the mission is significantly reduced although technology lifetime is still a significant issue next to obtaining a reasonable speed of travel. Proposed concepts include Project Daedalus, Project Icarus, Project Dragonfly, Project Longshot, and more recently Breakthrough Starshot.\n\nNear-lightspeed nano spacecraft might be possible within the near future built on existing microchip technology with a newly developed nanoscale thruster. Researchers at the University of Michigan are developing thrusters that use nanoparticles as propellant. Their technology is called \"nanoparticle field extraction thruster\", or nanoFET. These devices act like small particle accelerators shooting conductive nanoparticles out into space.\n\nMichio Kaku, a theoretical physicist, has suggested that clouds of \"smart dust\" be sent to the stars, which may become possible with advances in nanotechnology. Kaku also notes that a large number of nanoprobes would need to be sent due to the vulnerability of very small probes to be easily deflected by magnetic fields, micrometeorites and other dangers to ensure the chances that at least one nanoprobe will survive the journey and reach the destination.\n\nGiven the light weight of these probes, it would take much less energy to accelerate them. With onboard solar cells, they could continually accelerate using solar power. One can envision a day when a fleet of millions or even billions of these particles swarm to distant stars at nearly the speed of light and relay signals back to Earth through a vast interstellar communication network.\n\nAs a near-term solution, small, laser-propelled interstellar probes, based on current CubeSat technology were proposed in the context of Project Dragonfly.\n\nIn crewed missions, the duration of a slow interstellar journey presents a major obstacle and existing concepts deal with this problem in different ways. They can be distinguished by the \"state\" in which humans are transported on-board of the spacecraft.\n\nA generation ship (or world ship) is a type of interstellar ark in which the crew that arrives at the destination is descended from those who started the journey. Generation ships are not currently feasible because of the difficulty of constructing a ship of the enormous required scale and the great biological and sociological problems that life aboard such a ship raises.\n\nScientists and writers have postulated various techniques for suspended animation. These include human hibernation and cryonic preservation. Although neither is currently practical, they offer the possibility of sleeper ships in which the passengers lie inert for the long duration of the voyage.\n\nA robotic interstellar mission carrying some number of frozen early stage human embryos is another theoretical possibility. This method of space colonization requires, among other things, the development of an artificial uterus, the prior detection of a habitable terrestrial planet, and advances in the field of fully autonomous mobile robots and educational robots that would replace human parents.\n\nInterstellar space is not completely empty; it contains trillions of icy bodies ranging from small asteroids (Oort cloud) to possible rogue planets. There may be ways to take advantage of these resources for a good part of an interstellar trip, slowly hopping from body to body or setting up waystations along the way.\n\nIf a spaceship could average 10 percent of light speed (and decelerate at the destination, for manned missions), this would be enough to reach Proxima Centauri in forty years. Several propulsion concepts have been proposed that might be eventually developed to accomplish this (see § Propulsion below), but none of them are ready for near-term (few decades) developments at acceptable cost.\n\nAssuming faster-than-light travel is impossible, one might conclude that a human can never make a round-trip farther from Earth than 20 light years if the traveler is active between the ages of 20 and 60. A traveler would never be able to reach more than the very few star systems that exist within the limit of 20 light years from Earth. This, however, fails to take into account relativistic time dilation. Clocks aboard an interstellar ship would run slower than Earth clocks, so if a ship's engines were capable of continuously generating around 1 g of acceleration (which is comfortable for humans), the ship could reach almost anywhere in the galaxy and return to Earth within 40 years ship-time (see diagram). Upon return, there would be a difference between the time elapsed on the astronaut's ship and the time elapsed on Earth.\n\nFor example, a spaceship could travel to a star 32 light-years away, initially accelerating at a constant 1.03g (i.e. 10.1 m/s) for 1.32 years (ship time), then stopping its engines and coasting for the next 17.3 years (ship time) at a constant speed, then decelerating again for 1.32 ship-years, and coming to a stop at the destination. After a short visit, the astronaut could return to Earth the same way. After the full round-trip, the clocks on board the ship show that 40 years have passed, but according to those on Earth, the ship comes back 76 years after launch.\n\nFrom the viewpoint of the astronaut, onboard clocks seem to be running normally. The star ahead seems to be approaching at a speed of 0.87 light years per ship-year. The universe would appear contracted along the direction of travel to half the size it had when the ship was at rest; the distance between that star and the Sun would seem to be 16 light years as measured by the astronaut.\n\nAt higher speeds, the time on board will run even slower, so the astronaut could travel to the center of the Milky Way (30,000 light years from Earth) and back in 40 years ship-time. But the speed according to Earth clocks will always be less than 1 light year per Earth year, so, when back home, the astronaut will find that more than 60 thousand years will have passed on Earth.\n\nRegardless of how it is achieved, a propulsion system that could produce acceleration continuously from departure to arrival would be the fastest method of travel. A constant acceleration journey is one where the propulsion system accelerates the ship at a constant rate for the first half of the journey, and then decelerates for the second half, so that it arrives at the destination stationary relative to where it began. If this were performed with an acceleration similar to that experienced at the Earth's surface, it would have the added advantage of producing artificial \"gravity\" for the crew. Supplying the energy required, however, would be prohibitively expensive with current technology.\n\nFrom the perspective of a planetary observer, the ship will appear to accelerate steadily at first, but then more gradually as it approaches the speed of light (which it cannot exceed). It will undergo hyperbolic motion. The ship will be close to the speed of light after about a year of accelerating and remain at that speed until it brakes for the end of the journey.\n\nFrom the perspective of an onboard observer, the crew will feel a gravitational field opposite the engine's acceleration, and the universe ahead will appear to fall in that field, undergoing hyperbolic motion. As part of this, distances between objects in the direction of the ship's motion will gradually contract until the ship begins to decelerate, at which time an onboard observer's experience of the gravitational field will be reversed.\n\nWhen the ship reaches its destination, if it were to exchange a message with its origin planet, it would find that less time had elapsed on board than had elapsed for the planetary observer, due to time dilation and length contraction.\n\nThe result is an impressively fast journey for the crew.\n\nAll rocket concepts are limited by the rocket equation, which sets the characteristic velocity available as a function of exhaust velocity and mass ratio, the ratio of initial (\"M\", including fuel) to final (\"M\", fuel depleted) mass.\n\nVery high specific power, the ratio of thrust to total vehicle mass, is required to reach interstellar targets within sub-century time-frames. Some heat transfer is inevitable and a tremendous heating load must be adequately handled.\n\nThus, for interstellar rocket concepts of all technologies, a key engineering problem (seldom explicitly discussed) is limiting the heat transfer from the exhaust stream back into the vehicle.\n\nA type of electric propulsion, spacecraft such as Dawn use an ion engine. In an ion engine, electric power is used to create charged particles of the propellant, usually the gas xenon, and accelerate them to extremely high velocities. The exhaust velocity of conventional rockets is limited by the chemical energy stored in the fuel’s molecular bonds, which limits the thrust to about 5 km/s. They produce a high thrust (about 10⁶ N), but they have a low specific impulse, and that limits their top speed. By contrast, ion engines have low force, but the top speed in principle is limited only by the electrical power available on the spacecraft and on the gas ions being accelerated. The exhaust speed of the charged particles range from 15 km/s to 35 km/s.<ref name=\"http://www.iflscience.com\"></ref>\n\nNuclear-electric or plasma engines, operating for long periods at low thrust and powered by fission reactors, have the potential to reach speeds much greater than chemically powered vehicles or nuclear-thermal rockets. Such vehicles probably have the potential to power solar system exploration with reasonable trip times within the current century. Because of their low-thrust propulsion, they would be limited to off-planet, deep-space operation. Electrically powered spacecraft propulsion powered by a portable power-source, say a nuclear reactor, producing only small accelerations, would take centuries to reach for example 15% of the velocity of light, thus unsuitable for interstellar flight during a single human lifetime.\n\nFission-fragment rockets use nuclear fission to create high-speed jets of fission fragments, which are ejected at speeds of up to . With fission, the energy output is approximately 0.1% of the total mass-energy of the reactor fuel and limits the effective exhaust velocity to about 5% of the velocity of light. For maximum velocity, the reaction mass should optimally consist of fission products, the \"ash\" of the primary energy source, so no extra reaction mass need be bookkept in the mass ratio.\n\nBased on work in the late 1950s to the early 1960s, it has been technically possible to build spaceships with nuclear pulse propulsion engines, i.e. driven by a series of nuclear explosions. This propulsion system contains the prospect of very high specific impulse (space travel's equivalent of fuel economy) and high specific power.\n\nProject Orion team member Freeman Dyson proposed in 1968 an interstellar spacecraft using nuclear pulse propulsion that used pure deuterium fusion detonations with a very high fuel-burnup fraction. He computed an exhaust velocity of 15,000 km/s and a 100,000-tonne space vehicle able to achieve a 20,000 km/s delta-v allowing a flight-time to Alpha Centauri of 130 years. Later studies indicate that the top cruise velocity that can theoretically be achieved by a Teller-Ulam thermonuclear unit powered Orion starship, assuming no fuel is saved for slowing back down, is about 8% to 10% of the speed of light (0.08-0.1c). An atomic (fission) Orion can achieve perhaps 3%-5% of the speed of light. A nuclear pulse drive starship powered by fusion-antimatter catalyzed nuclear pulse propulsion units would be similarly in the 10% range and pure matter-antimatter annihilation rockets would be theoretically capable of obtaining a velocity between 50% to 80% of the speed of light. In each case saving fuel for slowing down halves the maximum speed. The concept of using a magnetic sail to decelerate the spacecraft as it approaches its destination has been discussed as an alternative to using propellant, this would allow the ship to travel near the maximum theoretical velocity. Alternative designs utilizing similar principles include Project Longshot, Project Daedalus, and Mini-Mag Orion. The principle of external nuclear pulse propulsion to maximize survivable power has remained common among serious concepts for interstellar flight without external power beaming and for very high-performance interplanetary flight.\n\nIn the 1970s the Nuclear Pulse Propulsion concept further was refined by Project Daedalus by use of externally triggered inertial confinement fusion, in this case producing fusion explosions via compressing fusion fuel pellets with high-powered electron beams. Since then, lasers, ion beams, neutral particle beams and hyper-kinetic projectiles have been suggested to produce nuclear pulses for propulsion purposes.\n\nA current impediment to the development of \"any\" nuclear-explosion-powered spacecraft is the 1963 Partial Test Ban Treaty, which includes a prohibition on the detonation of any nuclear devices (even non-weapon based) in outer space. This treaty would, therefore, need to be renegotiated, although a project on the scale of an interstellar mission using currently foreseeable technology would probably require international cooperation on at least the scale of the International Space Station.\n\nAnother issue to be considered, would be the g-forces imparted to a rapidly accelerated spacecraft, cargo, and passengers inside (see Inertia negation).\n\nFusion rocket starships, powered by nuclear fusion reactions, should conceivably be able to reach speeds of the order of 10% of that of light, based on energy considerations alone. In theory, a large number of stages could push a vehicle arbitrarily close to the speed of light. These would \"burn\" such light element fuels as deuterium, tritium, He, B, and Li. Because fusion yields about 0.3–0.9% of the mass of the nuclear fuel as released energy, it is energetically more favorable than fission, which releases <0.1% of the fuel's mass-energy. The maximum exhaust velocities potentially energetically available are correspondingly higher than for fission, typically 4–10% of c. However, the most easily achievable fusion reactions release a large fraction of their energy as high-energy neutrons, which are a significant source of energy loss. Thus, although these concepts seem to offer the best (nearest-term) prospects for travel to the nearest stars within a (long) human lifetime, they still involve massive technological and engineering difficulties, which may turn out to be intractable for decades or centuries.\n\nEarly studies include Project Daedalus, performed by the British Interplanetary Society in 1973–1978, and Project Longshot, a student project sponsored by NASA and the US Naval Academy, completed in 1988. Another fairly detailed vehicle system, \"Discovery II\", designed and optimized for crewed Solar System exploration, based on the DHe reaction but using hydrogen as reaction mass, has been described by a team from NASA's Glenn Research Center. It achieves characteristic velocities of >300 km/s with an acceleration of ~1.7•10 \"g\", with a ship initial mass of ~1700 metric tons, and payload fraction above 10%. Although these are still far short of the requirements for interstellar travel on human timescales, the study seems to represent a reasonable benchmark towards what may be approachable within several decades, which is not impossibly beyond the current state-of-the-art. Based on the concept's 2.2% burnup fraction it could achieve a pure fusion product exhaust velocity of ~3,000 km/s.\n\nAn antimatter rocket would have a far higher energy density and specific impulse than any other proposed class of rocket. If energy resources and efficient production methods are found to make antimatter in the quantities required and store it safely, it would be theoretically possible to reach speeds of several tens of percent that of light. Whether antimatter propulsion could lead to the higher speeds (>90% that of light) at which relativistic time dilation would become more noticeable, thus making time pass at a slower rate for the travelers as perceived by an outside observer, is doubtful owing to the large quantity of antimatter that would be required.\n\nSpeculating that production and storage of antimatter should become feasible, two further issues need to be considered. First, in the annihilation of antimatter, much of the energy is lost as high-energy gamma radiation, and especially also as neutrinos, so that only about 40% of \"mc\" would actually be available if the antimatter were simply allowed to annihilate into radiations thermally. Even so, the energy available for propulsion would be substantially higher than the ~1% of \"mc\" yield of nuclear fusion, the next-best rival candidate.\n\nSecond, heat transfer from the exhaust to the vehicle seems likely to transfer enormous wasted energy into the ship (e.g. for 0.1\"g\" ship acceleration, approaching 0.3 trillion watts per ton of ship mass), considering the large fraction of the energy that goes into penetrating gamma rays. Even assuming shielding was provided to protect the payload (and passengers on a crewed vehicle), some of the energy would inevitably heat the vehicle, and may thereby prove a limiting factor if useful accelerations are to be achieved.\n\nMore recently, Friedwardt Winterberg proposed that a matter-antimatter GeV gamma ray laser photon rocket is possible by a relativistic proton-antiproton pinch discharge, where the recoil from the laser beam is transmitted by the Mössbauer effect to the spacecraft.\n\nRockets deriving their power from external sources, such as a laser, could replace their internal energy source with an energy collector, potentially reducing the mass of the ship greatly and allowing much higher travel speeds. Geoffrey A. Landis has proposed for an interstellar probe, with energy supplied by an external laser from a base station powering an Ion thruster.\n\nA problem with all traditional rocket propulsion methods is that the spacecraft would need to carry its fuel with it, thus making it very massive, in accordance with the rocket equation. Several concepts attempt to escape from this problem:\n\nIn 1960, Robert W. Bussard proposed the Bussard ramjet, a fusion rocket in which a huge scoop would collect the diffuse hydrogen in interstellar space, \"burn\" it on the fly using a proton–proton chain reaction, and expel it out of the back. Later calculations with more accurate estimates suggest that the thrust generated would be less than the drag caused by any conceivable scoop design. Yet the idea is attractive because the fuel would be collected \"en route\" (commensurate with the concept of \"energy harvesting\"), so the craft could theoretically accelerate to near the speed of light. The limitation is due to the fact that the reaction can only accelerate the propellant to 0.12c. Thus the drag of catching interstellar dust and the thrust of accelerating that same dust to 0.12c would be the same when the speed is 0.12c, preventing further acceleration.\n\nA light sail or magnetic sail powered by a massive laser or particle accelerator in the home star system could potentially reach even greater speeds than rocket- or pulse propulsion methods, because it would not need to carry its own reaction mass and therefore would only need to accelerate the craft's payload. Robert L. Forward proposed a means for decelerating an interstellar light sail in the destination star system without requiring a laser array to be present in that system. In this scheme, a smaller secondary sail is deployed to the rear of the spacecraft, whereas the large primary sail is detached from the craft to keep moving forward on its own. Light is reflected from the large primary sail to the secondary sail, which is used to decelerate the secondary sail and the spacecraft payload. In 2002, Geoffrey A. Landis of NASA's Glen Research center also proposed a laser-powered, propulsion, sail ship that would host a diamond sail (of a few nanometers thick) powered with the use of solar energy. With this proposal, this interstellar ship would, theoretically, be able to reach 10 percent the speed of light.\n\nA magnetic sail could also decelerate at its destination without depending on carried fuel or a driving beam in the destination system, by interacting with the plasma found in the solar wind of the destination star and the interstellar medium.\n\nThe following table lists some example concepts using beamed laser propulsion as proposed by the physicist Robert L. Forward:\n\nThe following table is based on work by Heller, Hippke and Kervella.\n\nAchieving start-stop interstellar trip times of less than a human lifetime require mass-ratios of between 1,000 and 1,000,000, even for the nearer stars. This could be achieved by multi-staged vehicles on a vast scale. Alternatively large linear accelerators could propel fuel to fission propelled space-vehicles, avoiding the limitations of the Rocket equation.\n\nScientists and authors have postulated a number of ways by which it might be possible to surpass the speed of light, but even the most serious-minded of these are highly speculative.\n\nIt is also debatable whether faster-than-light travel is physically possible, in part because of causality concerns: travel faster than light may, under certain conditions, permit travel backwards in time within the context of special relativity. Proposed mechanisms for faster-than-light travel within the theory of general relativity require the existence of exotic matter and it is not known if this could be produced in sufficient quantity.\n\nIn physics, the Alcubierre drive is based on an argument, within the framework of general relativity and without the introduction of wormholes, that it is possible to modify a spacetime in a way that allows a spaceship to travel with an arbitrarily large speed by a local expansion of spacetime behind the spaceship and an opposite contraction in front of it. Nevertheless, this concept would require the spaceship to incorporate a region of exotic matter, or hypothetical concept of negative mass.\n\nA theoretical idea for enabling interstellar travel is by propelling a starship by creating an artificial black hole and using a parabolic reflector to reflect its Hawking radiation. Although beyond current technological capabilities, a black hole starship offers some advantages compared to other possible methods. Getting the black hole to act as a power source and engine also requires a way to convert the Hawking radiation into energy and thrust. One potential method involves placing the hole at the focal point of a parabolic reflector attached to the ship, creating forward thrust. A slightly easier, but less efficient method would involve simply absorbing all the gamma radiation heading towards the fore of the ship to push it onwards, and let the rest shoot out the back.\n\nWormholes are conjectural distortions in spacetime that theorists postulate could connect two arbitrary points in the universe, across an Einstein–Rosen Bridge. It is not known whether wormholes are possible in practice. Although there are solutions to the Einstein equation of general relativity that allow for wormholes, all of the currently known solutions involve some assumption, for example the existence of negative mass, which may be unphysical. However, Cramer \"et al.\" argue that such wormholes might have been created in the early universe, stabilized by cosmic string. The general theory of wormholes is discussed by Visser in the book \"Lorentzian Wormholes\".\n\nThe Enzmann starship, as detailed by G. Harry Stine in the October 1973 issue of \"Analog\", was a design for a future starship, based on the ideas of Robert Duncan-Enzmann. The spacecraft itself as proposed used a 12,000,000 ton ball of frozen deuterium to power 12–24 thermonuclear pulse propulsion units. Twice as long as the Empire State Building and assembled in-orbit, the spacecraft was part of a larger project preceded by interstellar probes and telescopic observation of target star systems.\n\nProject Hyperion, one of the projects of Icarus Interstellar.\n\nNASA has been researching interstellar travel since its formation, translating important foreign language papers and conducting early studies on applying fusion propulsion, in the 1960s, and laser propulsion, in the 1970s, to interstellar travel.\n\nThe NASA Breakthrough Propulsion Physics Program (terminated in FY 2003 after a 6-year, $1.2-million study, because \"No breakthroughs appear imminent.\") identified some breakthroughs that are needed for interstellar travel to be possible.\n\nGeoffrey A. Landis of NASA's Glenn Research Center states that a laser-powered interstellar sail ship could possibly be launched within 50 years, using new methods of space travel. \"I think that ultimately we're going to do it, it's just a question of when and who,\" Landis said in an interview. Rockets are too slow to send humans on interstellar missions. Instead, he envisions interstellar craft with extensive sails, propelled by laser light to about one-tenth the speed of light. It would take such a ship about 43 years to reach Alpha Centauri if it passed through the system without stopping. Slowing down to stop at Alpha Centauri could increase the trip to 100 years, whereas a journey without slowing down raises the issue of making sufficiently accurate and useful observations and measurements during a fly-by.\n\nThe 100 Year Starship (100YSS) is the name of the overall effort that will, over the next century, work toward achieving interstellar travel. The effort will also go by the moniker 100YSS. The 100 Year Starship study is the name of a one-year project to assess the attributes of and lay the groundwork for an organization that can carry forward the 100 Year Starship vision.\n\nHarold (\"Sonny\") White from NASA's Johnson Space Center is a member of Icarus Interstellar, the nonprofit foundation whose mission is to realize interstellar flight before the year 2100. At the 2012 meeting of 100YSS, he reported using a laser to try to warp spacetime by 1 part in 10 million with the aim of helping to make interstellar travel possible.\n\n\nA few organisations dedicated to interstellar propulsion research and advocacy for the case exist worldwide. These are still in their infancy, but are already backed up by a membership of a wide variety of scientists, students and professionals.\n\nThe energy requirements make interstellar travel very difficult. It has been reported that at the 2008 Joint Propulsion Conference, multiple experts opined that it was improbable that humans would ever explore beyond the Solar System. Brice N. Cassenti, an associate professor with the Department of Engineering and Science at Rensselaer Polytechnic Institute, stated that at least 100 times the total energy output of the entire world [in a given year] would be required to send a probe to the nearest star.\n\nAstrophysicist Sten Odenwald stated that the basic problem is that through intensive studies of thousands of detected exoplanets, most of the closest destinations within 50 light years do not yield Earth-like planets in the star's habitable zones. Given the multi-trillion-dollar expense of some of the proposed technologies, travelers will have to spend up to 200 years traveling at 20% the speed of light to reach the best known destinations. Moreover, once the travelers arrive at their destination (by any means), they will not be able to travel down to the surface of the target world and set up a colony unless the atmosphere is non-lethal. The prospect of making such a journey, only to spend the rest of the colony's life inside a sealed habitat and venturing outside in a spacesuit, may eliminate many prospective targets from the list.\n\nMoving at a speed close to the speed of light and encountering even a tiny stationary object like a grain of sand will have fatal consequences. For example, a gram of matter moving at 90% of the speed of light contains a kinetic energy corresponding to a small nuclear bomb (around 30kt TNT).\n\nExplorative high-speed missions to Alpha Centauri, as planned for by the Breakthrough Starshot initiative, are projected to be realizable within the 21st century. It is alternatively possible to plan for unmanned slow-cruising missions taking millennia to arrive. These probes would not be for human benefit in the sense that one can not foresee whether there would be anybody around on earth interested in then back-transmitted science data. An example would be the Genesis mission, which aims to bring unicellular life, in the spirit of directed panspermia, to habitable but otherwise barren planets. Comparatively slow cruising Genesis probes, with a typical speed of formula_5, corresponding to about formula_6, can be decelerated using a magnetic sail. Unmanned missions not for human benefit would hence be feasible.\n\nIn February 2017, NASA announced that its Spitzer Space Telescope had revealed seven Earth-size planets in the TRAPPIST-1 system orbiting an ultra-cool dwarf star 40 light-years away from our solar system. Three of these planets are firmly located in the habitable zone, the area around the parent star where a rocky planet is most likely to have liquid water. The discovery sets a new record for greatest number of habitable-zone planets found around a single star outside our solar system. All of these seven planets could have liquid water – the key to life as we know it – under the right atmospheric conditions, but the chances are highest with the three in the habitable zone.\n\n\n\n"}
{"id": "3096764", "url": "https://en.wikipedia.org/wiki?curid=3096764", "title": "Jewish question", "text": "Jewish question\n\nThe \"Jewish question\", also referred to as the \"Jewish problem\", was a wide-ranging debate in 19th- and 20th-century European society pertaining to the appropriate status and treatment of Jews in society. The debate was similar to other so-called \"national questions\" and dealt with the civil, legal, national and political status of Jews as a minority within society, particularly in Europe in the 18th, 19th and 20th centuries.\n\nThe debate started within societies, politicians and writers in western and central Europe influenced by the Age of Enlightenment and the ideals of the French Revolution. The issues included the legal and economic Jewish disabilities (e.g. Jewish quotas and segregation), Jewish assimilation, Jewish emancipation and Jewish Enlightenment.\n\nThe expression has been used by antisemitic movements from the 1880s onwards, culminating in the Nazi phrase \"the Final Solution to the Jewish Question\". Similarly, the expression was used by proponents for and opponents of the establishment of an autonomous Jewish homeland or a sovereign Jewish state.\n\nThe term \"Jewish question\" was first used in Great Britain around 1750 when the expression \"Jewish question\" appeared during the Jew Bill of 1753 debates in England. According to Holocaust scholar Lucy Dawidowicz, the term \"Jewish Question,\" as introduced in western Europe, was a neutral expression for the negative attitude toward the apparent and persistent singularity of the Jews as a people against the background of the rising political nationalisms and new nation-states. Dawidowicz writes that \"the histories of Jewish emancipation and of European antisemitism are replete with proffered 'solutions to the Jewish question.'\"\n\nThe question was next discussed in France (\"la question juive\") after the French Revolution in 1789. It has arrived in Germany in 1843 via Bruno Bauer's treatise \"Die Judenfrage\" (\"The Jewish Question\"). He argued that Jews can achieve political emancipation only if they let go their religious consciousness as he proposed that political emancipation required a secular state. In 1898, Theodore Herzl's treatise, \"Der Judenstaat\", advocates Zionism as a \"modern solution for the Jewish question\" by creating an independent Jewish state, preferably in Palestine.\n\nAccording to Otto Dov Kulka of Hebrew University, the term became widespread in the nineteenth century when it was used in discussions about Jewish emancipation in Germany (\"Judenfrage\"). In the 19th century hundreds of tractates, pamphlets, newspaper articles and books were written on the subject, with many offering solutions including resettlement, deportation and assimilation of the Jewish population. Similarly, hundreds of pieces of literature were written opposing these solutions and have offered solutions such as re-integration and education. This debate however, could not decide whether the problem of the Jewish Question had more to do with the problems posed by the German Jews' opponents or vice versa: the problem posed by the existence of the German Jews to their opponents.\n\nFrom around 1860 the notion took on an increasingly antisemitic tendency: Jews were described under this title as a stumbling block to the identity and cohesion of the German nation and as enemies within the Germans' own country. Antisemites such as Wilhelm Marr, Karl Eugen Dühring, Theodor Fritsch, Houston Stewart Chamberlain, Paul de Lagarde and others declared it a racial problem unsolvable through integration, in order to make their demands for the \"de-jewifying\" of the press, education, culture, state and economy, plausible, along with their demands for the condemnation of inter-marriage between Jews and non-Jews. They also used this definition to oust the Jews out of their supposedly socially dominant positions.\n\nBy far the most infamous use of this expression was by the Nazis in the early- and mid- twentieth century, culminating in the implementation of their \"Final Solution to the Jewish question\" during World War II.\n\nIn his book \"The Jewish Question\", published in 1843, Bauer argued that Jews can achieve political emancipation only if they relinquish their particular religious consciousness, since political emancipation requires a secular state, which he assumes does not leave any \"space\" for social identities such as religion. According to Bauer, such religious demands are incompatible with the idea of the \"Rights of Man.\" True political emancipation, for Bauer, requires the abolition of religion.\n\nKarl Marx replied to Bauer in his 1844 essay \"On the Jewish Question\". Marx contradicted Bauer's view that the nature of the Jewish religion prevented Judaism's assimilation. Instead he focused on the specific social and economic role of the Jewish group in Europe which, according to him, was lost when capitalism, the material basis for Judaism, assimilated the European societies as a whole.\n\nMarx uses Bauer's essay as an occasion for his own analysis of liberal rights. Marx argues that Bauer is mistaken in his assumption that in a \"secular state\", religion will no longer play a prominent role in social life, and, as an example refers to the pervasiveness of religion in the United States, which, unlike Prussia, had no state religion. In Marx's analysis, the \"secular state\" is not opposed to religion, but rather actually requires it. The removal of religious or property qualifications for citizens does not mean the abolition of religion or property, but only introduces a way of regarding individuals in abstraction from them.\nOn this note Marx moves beyond the question of religious freedom to his real concern with Bauer's analysis of \"political emancipation.\" Marx concludes that while individuals can be 'spiritually' and 'politically' free in a secular state, they can still be bound to material constraints on freedom by economic inequality, an assumption that would later form the basis of his critiques of capitalism.\n\nWerner Sombart praised Jews for their capitalism and presented the seventeenth–eighteenth century court Jews as integrated and a model for integration. By the turn of the twentieth century, the debate was still widely discussed and raised to prominence by the Dreyfus Affair in France. Within the religious and political elite, some continued to favor assimilation and political engagement in Europe while others, such as Theodore Herzl, proposed the advancement of a separate Jewish state and the Zionist cause.\nBetween 1880 and 1920, millions of other Jews sought their own solution for the pogroms of eastern Europe by emigration to other places, such as the United States and western Europe.\n\nIn Nazi Germany, the term \"Jewish Question\" (in ) referred to the sense that the existence of Jews in Germany posed a problem for the state. In 1933 two Nazi theorists, Johann von Leers and Achim Gercke, both proposed that the Jewish Question could be solved by resettling Jews in Madagascar or elsewhere in Africa or South America. Both intellectuals discussed the pros and cons of supporting the German Zionists as well, but von Leers asserted that establishing a Jewish homeland in British Palestine would create humanitarian and political problems for the region. Upon achieving power in 1933, Adolf Hitler and the Nazi state began to implement increasingly severe measures aimed at segregating and ultimately removing the Jewish people from Germany and (eventually) all of Europe. The next stage was persecution of Jews and the stripping of Jews of their citizenship through the Nuremberg Laws. Later, during World War II, it became state-sponsored internment in concentration camps and finally, the systematic extermination of the Jewish people (The Holocaust), which took place as the so-called \"Final Solution to the Jewish Question\".\n\nNazi propaganda was produced to manipulate the public, most notably based on writings from people such as Eugen Fischer, Fritz Lenz and Erwin Baur in \"Foundations of Human Heredity Teaching and Racial Hygiene\". The work (\"Allowing the Destruction of Life Unworthy of Living\") by Karl Binding and Alfred Hoche and the pseudo-scholarship created by Gerhard Kittel also played a role. In occupied France, the collaborationist regime established its own Institute for studying the Jewish Questions.\n\nA dominant anti-Semitic conspiracy theory is that Jewish people have undue influence over the media, banking and politics. Based on this conspiracy theory certain groups and activists discuss the \"Jewish Question\" and propose to \"address\" it. They often refer to it as the JQ. More recently, white nationalists, alt-righters, and neo-Nazis have used the initialism JQ to refer to the Jewish question.\n\n\nNotes\nFurther reading\n\n"}
{"id": "52747535", "url": "https://en.wikipedia.org/wiki?curid=52747535", "title": "Kay Bullitt", "text": "Kay Bullitt\n\nKatharine \"Kay\" Bullitt (born February, 22, 1925) is a Seattle education reformer, civil rights activist and philanthropist. Bullitt was instrumental in the desegregation of Seattle public schools.\n\nKatharine \"Kay\" Bullitt was born in 1925 in Boston and was raised in Arlington, Massachusetts. Her mother had a long career as dean of women at Colorado College before marrying in her late 30's and having three daughters. Kay was the middle child between Marion (\"Barney\") and Margaret (\"Margie\"). Bullitt attended the Shady Hill School in Cambridge.\n\nBullitt's interest in civic projects began early and focused on education and peace. She attended Radcliffe College which both her mother and sister had attended. While in college, Bullitt worked in a community center in Cambridge which primarily serviced African-American children. Kay's sister had written her thesis on the difference between African-American students from Barbados and the South. Kay's own senior thesis was on the role of Federal Government in Education. During World War II, Bullitt spent a summer at the Hampton Institute in Virginia as part of an interracial farm project. Bullitt was also active with the Massachusetts Fair Employment Practice.\n\nAfter the end of World War II, Bullitt worked in Germany for two summers.\n\nAfter college, Bullitt taught at the Shady Hill School in Cambridge which she had attended as a child. She taught the fourth grade there for five years. Inspired by the ability that she had observed in the young African-Americans she had worked with, Bullitt decided to take a trip across the country to observe what education and work experience was like for children. In her wanderings, Bullitt came to Seattle and decided to stay in 1951.\n\nIn the 1960s, Bullitt hosted an integrated day camp at her home on Capitol Hill. At first, Bullitt said that the camp was \"self-serving,\" as she had three older (step-)children and three under six, all with different interests. She recruited teachers and counselors from the League of Women Voters and The Little School in Seattle. From after the World's Fair in 1962 through the early seventies, the camp was integrated and grew to host 100 children and 35 teenagers running it.\n\nAlso during this time in the 1960s, Bullitt began work in Seattle to integrate schools through voluntary transfer. Her children were going to Lowell Elementary School in the Capitol Hill neighborhood and they began a partnership with Madrona School, a primarily African-American school in Seattle, and had about 50 children going between the schools. Bullitt also started the Voluntary Instruction Program (VIP) which brought in volunteers to teach subjects in small groups, starting with Madrona, but then went to other Central District schools. The program disappeared when they were unable to convince the district to fund a directorship position.\n\nBullitt briefly headed a program called School Affiliation Service which was based on her visits to Germany after World War II. Bullitt wanted to have people come from the South to see how the integration in Seattle was working. This program eventually evolved into the Coalition for Quality Integrated Education (CQIE) in 1968.\n\nIn 1963 after reading an article in the local paper about the \"Wawona\", an historic schooner, Bullitt began efforts to save and restore the ship. The 165 foot-long ship was launched in 1897 and was initially used to haul lumber up and down the Pacific Coast. The schooner also served as a fishing schooner in the Bering Sea and was a military barge during Word War II. After 46 years and numerous fundraising and volunteer efforts, it was determined it would be too costly to restore and they were unable to secure permanent moorage. The ship, which was profiled in \"Shipbuilders, Sea Captains and Fishermen\" by Joe Follansbee, was dismantled in 2009, with portions being saved for the Seattle Museum of History & Industry.\n\nBullitt was instrumental in founding several Civic and community projects in Seattle. She helped found Bumbershoot, an annual international music and arts festival in Seattle, which takes place every Labor Day Weekend at the Seattle Center. In the seventies, Bullitt helped found a savings and loan bank for women, called Sound Savings & Loan. Bullitt helped restore Pioneer Square.\n\nIn 1982 Bullitt, then a director of the Municipal League, helped organize Target Seattle, which was a week-long symposium on the dangers of nuclear war. Speakers included Louis Harris, David Brower, Dr. Jonas Salk, Dr. John E. Mack, Richard Wall Lyman, and Archibald Cox.The event finished with a speech by Cox at the Seattle Kingdome and with an attendance of 10,000- 20,000 people.\n\nEvery year for 60 years, on Wednesday evenings in July, Bullitt hosted a picnic for family and friends at her Capitol Hill home. Bullitt also hosted Seattle's Middle East Peace Camp for Children for a number of years on the property. Bullitt, with her former late husband, Stimson Bullitt, donated the property to the city to be a park after her death.\n\nBullitt has many awards including a United Nations Human Rights Prize.\n\nIn 1954, Bullitt was married to Stimson Bullitt, son of Dorothy Stimson Bullitt, the founder of KING Broadcasting Company. Their marriage lasted for 25 years, ending in divorce in 1979. Stimson Bullitt was the President of KING Broadcasting in Seattle from 1962-72. He was also an attorney, author, judge and outdoorsman.\n\nIn addition to her step-children, Ashley, Scott -- who changed his name to Fred Nemo, and Jill, Kay and Stim had three more children: Dorothy, Ben (who preceded her in death in Nov, 1981), and Margaret.\n\n"}
{"id": "218842", "url": "https://en.wikipedia.org/wiki?curid=218842", "title": "Malicious prosecution", "text": "Malicious prosecution\n\nMalicious prosecution is a common law intentional tort, while like the tort of abuse of process, its elements include (1) intentionally (and maliciously) instituting and pursuing (or causing to be instituted or pursued) a legal action (civil or criminal) that is (2) brought without probable cause and (3) dismissed in favor of the victim of the malicious prosecution. In some jurisdictions, the term \"malicious prosecution\" denotes the wrongful initiation of criminal proceedings, while the term \"malicious use of process\" denotes the wrongful initiation of civil proceedings.\n\nCriminal prosecuting attorneys and judges are protected from tort liability for malicious prosecution by doctrines of prosecutorial immunity and judicial immunity. Moreover, the mere filing of a complaint cannot constitute an abuse of process. The parties who have abused or misused the process have gone beyond merely filing a lawsuit. The taking of an appeal, even a frivolous one, is not enough to constitute an abuse of process. The mere filing or maintenance of a lawsuit, even for an improper purpose, is not a proper basis for an abuse of process action.\n\nDeclining to expand the tort of malicious prosecution, a unanimous California Supreme Court in the case of \"Sheldon Appel Co. v. Albert & Oliker\", 47 Cal. 3d 863, 873 (1989) observed: \"While the filing of frivolous lawsuits is certainly improper and cannot in any way be condoned, in our view the better means of addressing the problem of unjustified litigation is through the adoption of measures facilitating the speedy resolution of the initial lawsuit and authorizing the imposition of sanctions for frivolous or delaying conduct within that first action itself, rather than through an expansion of the opportunities for initiating one or more additional rounds of malicious prosecution litigation after the first action has been concluded.\"\n\nThe tort originates in the (now defunct) legal maxim that \"the King pays no costs\"; that is, The Crown could not be forced to pay the legal costs of a person it prosecuted, even if that person was found innocent. As \"The London Magazine\" stated in 1766: \"if a groundless and vexatious prosecution be commenced in the King's name, his ministers who commenced, or advised commencing that prosecution, ought at least to be obliged to pay the costs which an innocent subject has thereby been put to\".\n\nSixteen U.S. states require another element of malicious prosecution. This element, commonly called the English Rule, states that, in addition to fulfilling all other malicious prosecution elements, one must also prove injury other than the normal downside of being sued. This rule is limited to equitable damages, such as loss of profit, and excludes damages that cannot be measured by the law (\"e.g.\", damage to reputation).\n\nCanadian jurisprudence has changed in that if any individual takes legal action that meet the above criteria, they may be sued. Legal action may be taken against the police or the Crown Attorney or the Attorney General, as they are no longer exempt from suit.\n\nThe tort of malicious prosecution was recently reviewed in 2009 by the Supreme Court of Canada in \"Miazga v. Kvello Estate\", and specifically how it applied to public prosecutors in Canada. The court outlined the four required elements for the tort of malicious prosecution: (i) The prosecution must be initiated by the defendant; (ii) The prosecution must be terminated in the plaintiff's favour. (iii) There was a lack of reasonable and probable grounds to commence or continue the prosecution; and (iv) The defendant was motivated to commence or continue to the prosecution due to malice.\n\nIn 2014, the Quebec Court of Appeal held that the contents of plea bargaining negotiations held in the context of criminal cases could be admitted as evidence in the context of a civil suit for malicious prosecution, despite the general evidentiary rule prohibiting adducing settlement discussions into proof at trial. More specifically, the Court held that introducing into evidence the contents of such negotiations was possible when it tended to demonstrate that the prosecution initiated or maintained criminal charges on the basis of improper motives.\n\nNotably, the tort of malicious prosecution only protects the right of defendants to be free of frivolous lawsuits brought by malicious plaintiffs. For a variety of reasons grounded in public policy, courts have consistently refused to authorize the converse — a tort of malicious defense which would protect the right of plaintiffs to be free of frivolous defenses raised by defendants.\n\n"}
{"id": "4102640", "url": "https://en.wikipedia.org/wiki?curid=4102640", "title": "Meaning (philosophy of language)", "text": "Meaning (philosophy of language)\n\nIn the philosophy of language, the nature of meaning, its definition, elements, and types, was discussed by philosophers Aristotle, Augustine, and Aquinas. According to them \"meaning is a relationship between two sorts of things: \"signs\" and the kinds of things they \"mean\" (intend, express or signify)\". One term in the relationship of meaning necessarily causes something else to come to the mind. In other words: \"a sign is defined as an entity that indicates another entity to some agent for some purpose\". As Augustine states, a sign is \"something that shows itself to the senses and something other than itself to the mind\" (\"Signum est quod se ipsum sensui et praeter se aliquid animo ostendit\"; \"De dial.\", 1975, 86).\n\nThe types of meanings vary according to the types of the thing that is being represented. Namely:\n\nAll subsequent inquiries emphasize some particular perspectives within the general AAA framework.\n\nThe major contemporary positions of meaning come under the following partial definitions of meaning:\n\nThe evaluation of meaning according to each one of the five major substantive theories of meaning and truth is presented below. The question of what is a proper basis for deciding how words, symbols, ideas and beliefs may properly be considered to truthfully denote meaning, whether by a single person or an entire society, is dealt with by the five most prevalent substantive theories listed below. Each theory of meaning as evaluated by these respective theories of truth are each further researched by the individual scholars supporting each one of the respective theories of truth and meaning.\n\nBoth hybrid theories of meaning and alternative theories of meaning and truth have also been researched, and are subject to further assessment according to their respective and relative merits.\n\nCorrespondence theories emphasise that true beliefs and true statements of meaning correspond to the actual state of affairs and that associated meanings must be in agreement with these beliefs and statements. This type of theory stresses a relationship between thoughts or statements on one hand, and things or objects on the other. It is a traditional model tracing its origins to ancient Greek philosophers such as Socrates, Plato, and Aristotle. This class of theories holds that the truth or the falsity of a representation is determined in principle entirely by how it relates to \"things\", by whether it accurately describes those \"things.\" An example of correspondence theory is the statement by the Thirteenth Century philosopher/theologian Thomas Aquinas: \"Veritas est adaequatio rei et intellectus\" (\"Truth is the equation [or adequation] of things and intellect\"), a statement which Aquinas attributed to the Ninth Century neoplatonist Isaac Israeli. Aquinas also restated the theory as: \"A judgment is said to be true when it conforms to the external reality\"\n\nCorrespondence theory centres heavily around the assumption that truth and meaning are a matter of accurately copying what is known as \"objective reality\" and then representing it in thoughts, words and other symbols. Many modern theorists have stated that this ideal cannot be achieved without analysing additional factors. For example, language plays a role in that all languages have words to represent concepts that are virtually undefined in other languages. The German word \"Zeitgeist\" is one such example: one who speaks or understands the language may \"know\" what it means, but any translation of the word apparently fails to accurately capture its full meaning (this is a problem with many abstract words, especially those derived in agglutinative languages). Thus, some words add an additional parameter to the construction of an accurate truth predicate. Among the philosophers who grappled with this problem is Alfred Tarski, whose semantic theory is summarized further below in this article.\n\nFor coherence theories in general, the assessment of meaning and truth requires a proper fit of elements within a whole system. Very often, though, coherence is taken to imply something more than simple logical consistency; often there is a demand that the propositions in a coherent system lend mutual inferential support to each other. So, for example, the completeness and comprehensiveness of the underlying set of concepts is a critical factor in judging the validity and usefulness of a coherent system. A pervasive tenet of coherence theories is the idea that truth is primarily a property of whole systems of propositions, and can be ascribed to individual propositions only according to their coherence with the whole. Among the assortment of perspectives commonly regarded as coherence theory, theorists differ on the question of whether coherence entails many possible true systems of thought or only a single absolute system.\n\nSome variants of coherence theory are claimed to describe the essential and intrinsic properties of formal systems in logic and mathematics. However, formal reasoners are content to contemplate axiomatically independent and sometimes mutually contradictory systems side by side, for example, the various alternative geometries. On the whole, coherence theories have been rejected for lacking justification in their application to other areas of truth, especially with respect to assertions about the natural world, empirical data in general, assertions about practical matters of psychology and society, especially when used without support from the other major theories of truth.\n\nCoherence theories distinguish the thought of rationalist philosophers, particularly of Spinoza, Leibniz, and G.W.F. Hegel, along with the British philosopher F.H. Bradley. Other alternatives may be found among several proponents of logical positivism, notably Otto Neurath and Carl Hempel.\n\nSocial constructivism holds that meaning and truth are constructed by social processes, is historically and culturally specific, and that it is in part shaped through the power struggles within a community. Constructivism views all of our knowledge as \"constructed,\" because it does not reflect any external \"transcendent\" realities (as a pure correspondence theory might hold). Rather, perceptions of truth are viewed as contingent on convention, human perception, and social experience. It is believed by constructivists that representations of physical and biological reality, including race, sexuality, and gender, are socially constructed.\n\nGiambattista Vico was among the first to claim that history and culture along with their meaning were man-made. Vico's epistemological orientation gathers the most diverse rays and unfolds in one axiom\"verum ipsum factum\"\"truth itself is constructed\". Hegel and Marx were among the other early proponents of the premise that truth is, or can be, socially constructed. Marx, like many critical theorists who followed, did not reject the existence of objective truth but rather distinguished between true knowledge and knowledge that has been distorted through power or ideology. For Marx, scientific and true knowledge is \"in accordance with the dialectical understanding of history\" and ideological knowledge is \"an epiphenomenal expression of the relation of material forces in a given economic arrangement\".\n\nConsensus theory holds that meaning and truth are whatever is agreed upon, or in some versions, might come to be agreed upon, by some specified group. Such a group might include all human beings, or a subset thereof consisting of more than one person.\n\nAmong the current advocates of consensus theory as a useful accounting of the concept of \"truth\" is the philosopher Jürgen Habermas. Habermas maintains that truth is what would be agreed upon in an ideal speech situation. Among the current strong critics of consensus theory is the philosopher Nicholas Rescher.\n\nThe three most influential forms of the \"pragmatic theory of truth\" and meaning were introduced around the turn of the 20th century by Charles Sanders Peirce, William James, and John Dewey. Although there are wide differences in viewpoint among these and other proponents of pragmatic theory, they hold in common that meaning and truth are verified and confirmed by the results of putting one's concepts into practice.\n\nPeirce defines truth as follows: \"Truth is that concordance of an abstract statement with the ideal limit towards which endless investigation would tend to bring scientific belief, which concordance the abstract statement may possess by virtue of the confession of its inaccuracy and one-sidedness, and this confession is an essential ingredient of truth.\" This statement stresses Peirce's view that ideas of approximation, incompleteness, and partiality, what he describes elsewhere as \"fallibilism\" and \"reference to the future\", are essential to a proper conception of meaning and truth. Although Peirce uses words like \"concordance\" and \"correspondence\" to describe one aspect of the pragmatic sign relation, he is also quite explicit in saying that definitions of truth based on mere correspondence are no more than \"nominal\" definitions, which he accords a lower status than \"real\" definitions.\n\nWilliam James's version of pragmatic theory, while complex, is often summarized by his statement that \"the 'true' is only the expedient in our way of thinking, just as the 'right' is only the expedient in our way of behaving.\" By this, James meant that truth is a \"quality\", the value of which is confirmed by its effectiveness when applying concepts to practice (thus, \"pragmatic\").\n\nJohn Dewey, less broadly than James but more broadly than Peirce, held that inquiry, whether scientific, technical, sociological, philosophical or cultural, is self-corrective over time \"if\" openly submitted for testing by a community of inquirers in order to clarify, justify, refine and/or refute proposed meanings and truths.\n\nThough not widely known, a new variation of the pragmatic theory was defined and wielded successfully from the 20th century forward. Defined and named by William Ernest Hocking, this variation is known as \"negative pragmatism\". Essentially, what works may or may not be true, but what fails cannot be true because the truth and its meaning always works. James and Dewey's ideas also ascribe meaning and truth to repeated testing which is \"self-corrective\" over time.\n\nPragmatism and negative pragmatism are also closely aligned with the coherence theory of truth in that any testing should not be isolated but rather incorporate knowledge from all human endeavors and experience. The universe is a whole and integrated system, and testing should acknowledge and account for its diversity. As Feynman said, \"... if it disagrees with experiment, it is wrong.\"\n\nSome have asserted that meaning is nothing substantially more or less than the truth conditions they involve. For such theories, an emphasis is placed upon reference to actual things in the world to account for meaning, with the caveat that reference more or less explains the greater part (or all) of meaning itself.\n\nThe logical positivists argued that the meaning of a statement arose from how it is verified.\n\nIn his paper \"Über Sinn und Bedeutung\" (now usually translated as \"On Sense and Reference\"), Gottlob Frege argued that proper names present at least two problems in explaining meaning.\n\n\nFrege can be interpreted as arguing that it was therefore a mistake to think that the meaning of a name is the thing it refers to. Instead, the meaning must be something else—the \"sense\" of the word. Two names for the same person, then, can have different senses (or meanings): one referent might be picked out by more than one sense. This sort of theory is called a mediated reference theory. Frege argued that, ultimately, the same bifurcation of meaning must apply to most or all linguistic categories, such as to quantificational expressions like \"All boats float\". It is now accepted by many philosophers as applying to all expressions \"but\" proper names .\n\nLogical analysis was further advanced by Bertrand Russell and Alfred North Whitehead in their groundbreaking \"Principia Mathematica\", which attempted to produce a formal language with which the truth of all mathematical statements could be demonstrated from first principles.\n\nRussell differed from Frege greatly on many points, however. He rejected Frege's sense-reference distinction. He also disagreed that language was of fundamental significance to philosophy, and saw the project of developing formal logic as a way of eliminating all of the confusions caused by ordinary language, and hence at creating a perfectly transparent medium in which to conduct traditional philosophical argument. He hoped, ultimately, to extend the proofs of the \"Principia\" to all possible true statements, a scheme he called logical atomism. For a while it appeared that his pupil Wittgenstein had succeeded in this plan with his \"Tractatus Logico-Philosophicus\".\n\nRussell's work, and that of his colleague G. E. Moore, developed in response to what they perceived as the nonsense dominating British philosophy departments at the turn of the 20th century, which was a kind of British Idealism most of which was derived (albeit very distantly) from the work of Hegel. In response Moore developed an approach (\"Common Sense Philosophy\") which sought to examine philosophical difficulties by a close analysis of the language used in order to determine its meaning. In this way Moore sought to expunge philosophical absurdities such as \"time is unreal\". Moore's work would have significant, if oblique, influence (largely mediated by Wittgenstein) on Ordinary language philosophy.\n\nThe Vienna Circle, a famous group of logical positivists from the early 20th century (closely allied with Russell and Frege), adopted the verificationist theory of meaning. The verificationist theory of meaning (in at least one of its forms) states that to say that an expression is meaningful is to say that there are some conditions of experience that could exist to show that the expression is true. As noted, Frege and Russell were two proponents of this way of thinking.\n\nA semantic theory of truth was produced by Alfred Tarski for formal semantics. According to Tarski's account, meaning consists of a recursive set of rules that end up yielding an infinite set of sentences, \"'p' is true if and only if p\", covering the whole language. His innovation produced the notion of propositional functions discussed on the section on universals (which he called \"sentential functions\"), and a model-theoretic approach to semantics (as opposed to a proof-theoretic one). Finally, some links were forged to the correspondence theory of truth (Tarski, 1944).\n\nPerhaps the most influential current approach in the contemporary theory of meaning is that sketched by Donald Davidson in his introduction to the collection of essays \"Truth and Meaning\" in 1967. There he argued for the following two theses:\n\nThe result is a theory of meaning that rather resembles, by no accident, Tarski's account.\n\nDavidson's account, though brief, constitutes the first systematic presentation of truth-conditional semantics. He proposed simply translating natural languages into first-order predicate calculus in order to reduce meaning to a function of truth.\n\nSaul Kripke examined the relation between sense and reference in dealing with possible and actual situations. He showed that one consequence of his interpretation of certain systems of modal logic was that the reference of a proper name is \"necessarily\" linked to its referent, but that the sense is not. So for instance \"Hesperus\" necessarily refers to Hesperus, even in those imaginary cases and worlds in which perhaps Hesperus is not the evening star. That is, Hesperus is necessarily Hesperus, but only contingently the morning star.\n\nThis results in the curious situation that part of the meaning of a name — that it refers to some particular thing — is a necessary fact about that name, but another part — that it is used in some particular way or situation — is not.\n\nKripke also drew the distinction between speaker's meaning and semantic meaning, elaborating on the work of ordinary language philosophers Paul Grice and Keith Donnellan. The speaker's meaning is what the speaker intends to refer to by saying something; the semantic meaning is what the words uttered by the speaker mean according to the language.\n\nIn some cases, people do not say what they mean; in other cases, they say something that is in error. In both these cases, the speaker's meaning and the semantic meaning seem to be different. Sometimes words do not actually express what the speaker wants them to express; so words will mean one thing, and what people intend to convey by them might mean another. The meaning of the expression, in such cases, is ambiguous.\n\nW.V. Quine attacked both verificationism and the very notion of meaning in his famous essay, \"Two Dogmas of Empiricism\". In it, he suggested that meaning was nothing more than a vague and dispensable notion. Instead, he asserted, what was more interesting to study was the synonymy between signs. He also pointed out that verificationism was tied to the distinction between analytic and synthetic statements, and asserted that such a divide was defended ambiguously. He also suggested that the unit of analysis for any potential investigation into the world (and, perhaps, meaning) would be the entire body of statements taken as a collective, not just individual statements on their own.\n\nOther criticisms can be raised on the basis of the limitations that truth-conditional theorists themselves admit to. Tarski, for instance, recognized that truth-conditional theories of meaning only make sense of statements, but fail to explain the meanings of the lexical parts that make up statements. Rather, the meaning of the parts of statements is presupposed by an understanding of the truth-conditions of a whole statement, and explained in terms of what he called \"satisfaction conditions\".\n\nStill another objection (noted by Frege and others) was that some kinds of statements don't seem to have any truth-conditions at all. For instance, \"Hello!\" has no truth-conditions, because it doesn't even attempt to tell the listener anything about the state of affairs in the world. In other words, different propositions have different grammatical moods.\n\nDeflationist accounts of truth, sometimes called 'irrealist' accounts, are the staunchest source of criticism of truth-conditional theories of meaning. According to them, \"truth\" is a word with no serious meaning or function in discourse. For instance, for the deflationist, the sentences \"It's true that Tiny Tim is trouble\" and \"Tiny Tim is trouble\" are equivalent. In consequence, for the deflationist, any appeal to truth as an account of meaning has little explanatory power.\n\nThe sort of truth-theories presented here can also be attacked for their formalism both in practice and principle. The principle of formalism is challenged by the informalists, who suggest that language is largely a construction of the speaker, and so, not compatible with formalization. The practice of formalism is challenged by those who observe that formal languages (such as present-day quantificational logic) fail to capture the expressive power of natural languages (as is arguably demonstrated in the awkward character of the quantificational explanation of definite description statements, as laid out by Bertrand Russell).\n\nFinally, over the past century, forms of logic have been developed that are not dependent exclusively on the notions of truth and falsity. Some of these types of logic have been called modal logics. They explain how certain logical connectives such as \"if-then\" work in terms of necessity and possibility. Indeed, modal logic was the basis of one of the most popular and rigorous formulations in modern semantics called the Montague grammar. The successes of such systems naturally give rise to the argument that these systems have captured the natural meaning of connectives like if-then far better than an ordinary, truth-functional logic ever could.\n\nThroughout the 20th century, English philosophy focused closely on analysis of language. This style of analytic philosophy became very influential and led to the development of a wide range of philosophical tools.\n\nThe philosopher Ludwig Wittgenstein was originally an artificial language philosopher, following the influence of Russell and Frege. In his \"Tractatus Logico-Philosophicus\" he had supported the idea of an ideal language built up from atomic statements using logical connectives. However, as he matured, he came to appreciate more and more the phenomenon of natural language. \"Philosophical Investigations\", published after his death, signalled a sharp departure from his earlier work with its focus upon ordinary language use. His approach is often summarised by the aphorism \"the meaning of a word is its use in a language\". However, following in Frege's footsteps, in the \"Tractatus\", Wittgenstein declares: \"... Only in the context of a proposition has a name meaning.\"\n\nHis work would come to inspire future generations and spur forward a whole new discipline, which explained meaning in a new way. Meaning in a natural language was seen as primarily a question of how the speaker uses words within the language to express intention.\n\nThis close examination of natural language proved to be a powerful philosophical technique. Practitioners who were influenced by Wittgenstein's approach have included an entire tradition of thinkers, featuring P. F. Strawson, Paul Grice, R. M. Hare, R. S. Peters, and Jürgen Habermas.\n\nAt around the same time Ludwig Wittgenstein was re-thinking his approach to language, reflections on the complexity of language led to a more expansive approach to meaning. Following the lead of George Edward Moore, J. L. Austin examined the use of words in great detail. He argued against fixating on the meaning of words. He showed that dictionary definitions are of limited philosophical use, since there is no simple \"appendage\" to a word that can be called its meaning. Instead, he showed how to focus on the way in which words are used in order to do things. He analysed the structure of utterances into three distinct parts: locutions, illocutions and perlocutions. His pupil John Searle developed the idea under the label \"speech acts\". Their work greatly influenced pragmatics.\n\nPast philosophers had understood reference to be tied to words themselves. However, Sir Peter Strawson disagreed in his seminal essay, \"On Referring\", where he argued that there is nothing true about statements on their own; rather, only the uses of statements could be considered to be true or false.\n\nIndeed, one of the hallmarks of the ordinary use perspective is its insistence upon the distinctions between meaning and use. \"Meanings\", for ordinary language philosophers, are the \"instructions\" for usage of words — the common and conventional definitions of words. \"Usage\", on the other hand, is the actual meanings that individual speakers have — the things that an individual speaker in a particular context wants to refer to. The word \"dog\" is an example of a meaning, but pointing at a nearby dog and shouting \"This dog smells foul!\" is an example of usage. From this distinction between usage and meaning arose the divide between the fields of Pragmatics and Semantics.\n\nYet another distinction is of some utility in discussing language: \"mentioning\". \"Mention\" is when an expression refers to itself as a linguistic item, usually surrounded by quotation marks. For instance, in the expression \"'Opopanax' is hard to spell\", what is referred to is the word itself (\"opopanax\") and not what it means (an obscure gum resin). Frege had referred to instances of mentioning as \"opaque contexts\".\n\nIn his essay, \"Reference and Definite Descriptions\", Keith Donnellan sought to improve upon Strawson's distinction. He pointed out that there are two uses of definite descriptions: \"attributive\" and \"referential\". Attributive uses provide a description of whoever is being referred to, while referential uses point out the actual referent. Attributive uses are like mediated references, while referential uses are more directly referential.\n\nThe philosopher Paul Grice, working within the ordinary language tradition, understood \"meaning\" — in his 1957 article — to have two kinds: \"natural\" and \"non-natural\". \"Natural meaning\" had to do with cause and effect, for example with the expression \"these spots mean measles\". \"Non-natural\" meaning, on the other hand, had to do with the intentions of the speaker in communicating something to the listener.\n\nIn his essay, \"Logic and Conversation\", Grice went on to explain and defend an explanation of how conversations work. His guiding maxim was called the \"cooperative principle\", which claimed that the speaker and the listener will have mutual expectations of the kind of information that will be shared. The principle is broken down into four maxims: \"Quality\" (which demands truthfulness and honesty), \"Quantity\" (demand for just enough information as is required), \"Relation\" (relevance of things brought up), and \"Manner\" (lucidity). This principle, if and when followed, lets the speaker and listener figure out the meaning of certain implications by way of inference.\n\nThe works of Grice led to an avalanche of research and interest in the field, both supportive and critical. One spinoff was called Relevance theory, developed by Dan Sperber and Deirdre Wilson during the mid-1980s, whose goal was to make the notion of \"relevance\" more clear. Similarly, in his work, \"Universal pragmatics\", Jürgen Habermas began a program that sought to improve upon the work of the ordinary language tradition. In it, he laid out the goal of a valid conversation as a pursuit of mutual understanding.\n\nMichael Dummett argued against the kind of truth-conditional semantics presented by Davidson. Instead, he argued that basing semantics on \"assertion conditions\" avoids a number of difficulties with truth-conditional semantics, such as the transcendental nature of certain kinds of truth condition. He leverages work done in proof-theoretic semantics to provide a kind of inferential role semantics, where:\nA semantics based upon assertion conditions is called a verificationist semantics: cf. the verificationism of the Vienna Circle.\n\nThis work is closely related, though not identical, to one-factor theories of conceptual role semantics.\n\nSometimes between the 1950-1990s, cognitive scientist Jerry Fodor said that use theories (of the Wittgensteinian kind) seem to assume that language is solely a public phenomenon, that there is no such thing as a \"private language\". Fodor thinks it is necessary to create or describe the \"language of thought\", which would seemingly require the existence of a \"private language\".\n\nIn the 1960s, David Kellogg Lewis described meaning as use, a feature of a social convention and conventions as regularities of a specific sort. Lewis' work was an application of game theory in philosophical topics. Conventions, he argued, are a species of coordination equilibria.\n\nThe idea theory of meaning (also ideational theory of meaning), most commonly associated with the British empiricist John Locke, claims that meanings are mental representations provoked by signs. \n\nThe term \"ideas\" is used to refer to either mental representations, or to mental activity in general. Those who seek an explanation for meaning in the former sort of account endorse a stronger sort of idea theory of mind than the latter. Those who seek an explanation for meaning in the former sort of account endorse a stronger sort of idea theory of meaning than the latter.\n\nEach idea is understood to be necessarily \"about\" something external and/or internal, real or imaginary. For example, in contrast to the abstract meaning of the universal \"dog\", the referent \"this dog\" may mean a particular real life chihuahua. In both cases, the word is about something, but in the former it is about the class of dogs as generally understood, while in the latter it is about a very real and particular dog in the real world.\n\nJohn Locke, considered all ideas to be both imaginable objects of sensation and the very \"un\"imaginable objects of reflection. Locke said in his \"Essay Concerning Human Understanding\", that words are used both as signs for ideas—but also to signify the lack of certain ideas. David Hume held that thoughts were kinds of imaginable entities. (See Hume's \"Enquiry Concerning Human Understanding\", section 2). Hume argued that any words that could not call upon any past experience were without meaning.\n\nNonetheless, George Berkeley and Ludwig Wittgenstein held, in contrast to Locke and Hume, that ideas alone are unable to account for the different variations within a general meaning. For example, any hypothetical image of the meaning of \"dog\" has to include such varied images as a chihuahua, a pug, and a Black Lab; and this seems impossible to imagine, all of those particular breeds looking very different from one another. Another way to see this point is to question why it is that, if we have an image of a specific type of dog (say of a chihuahua), it should be entitled to represent the entire concept.\n\nAnother criticism is that some meaningful words, known as non-lexical items, don't have any meaningfully associated image. For example, the word \"the\" has a meaning, but one would be hard-pressed to find a mental representation that fits it. Still another objection lies in the observation that certain linguistic items name something in the real world, and are meaningful, yet which we have no mental representations to deal with. For instance, it is not known what Newton's father looked like, yet the phrase \"Newton's father\" still has meaning.\n\nAnother problem is that of composition — that it is difficult to explain how words and phrases combine into sentences if only ideas were involved in meaning.\n\nEleanor Rosch and George Lakoff advanced the theory of prototypes, which suggests that many lexical categories, at least on the face of things, have \"radial structures\". That is to say, there are some ideal member(s) in the category that seem to represent the category better than other members. For example, the category of \"birds\" may feature the \"robin\" as the prototype, or the ideal kind of bird. With experience, subjects might come to evaluate membership in the category of \"bird\" by comparing candidate members to the prototype and evaluating for similarities. So, for example, a penguin or an ostrich would sit at the fringe of the meaning of \"bird\", because a penguin is unlike a robin.\n\nIntimately related to these researches is the notion of a \"psychologically basic level\", which is both the first level named and understood by children, and \"the highest level at which a single mental image can reflect the entire category\". (Lakoff 1987:46) The \"basic level\" of cognition is understood by Lakoff as crucially drawing upon \"image-schemas\" along with various other cognitive processes.\n\nThe philosophers (Ned Block, Gilbert Harman, H. Field) and the cognitive scientists (G. Miller and P. Johnson-Laird) say that the meaning of a term can be found by investigating its role in relation to other concepts and mental states. They endorse a view called \"conceptual role semantics\". Those proponents of this view who understand meanings to be exhausted by the content of mental states can be said to endorse \"one-factor\" accounts of conceptual role semantics. and thus fit within the tradition of idea theories.\n\n\n"}
{"id": "923762", "url": "https://en.wikipedia.org/wiki?curid=923762", "title": "Merger doctrine", "text": "Merger doctrine\n\nThe phrase merger doctrine or doctrine of merger may refer to one of several legal doctrines:\n"}
{"id": "9966817", "url": "https://en.wikipedia.org/wiki?curid=9966817", "title": "Modes of convergence", "text": "Modes of convergence\n\nIn mathematics, there are many senses in which a sequence or a series is said to be convergent. This article describes various modes (senses or species) of convergence in the settings where they are defined. For a list of modes of convergence, see Modes of convergence (annotated index)\n\nNote that each of the following objects is a special case of the types preceding it: sets, topological spaces, uniform spaces, TAGs (topological abelian groups), normed spaces, Euclidean spaces, and the real/complex numbers. Also, note that any metric space is a uniform space.\n\nConvergence can be defined in terms of sequences in first-countable spaces. Nets are a generalization of sequences that is useful in spaces which are not first countable. Filters further generalize the concept of convergence.\n\nIn metric spaces, one can define Cauchy sequences. Cauchy nets and filters are generalizations to uniform spaces. Even more generally, Cauchy spaces are spaces in which Cauchy filters may be defined. Convergence implies \"Cauchy-convergence\", and Cauchy-convergence, together with the existence of a convergent subsequence implies convergence. The concept of completeness of metric spaces, and its generalizations is defined in terms of Cauchy sequences.\n\nIn a topological abelian group, convergence of a series is defined as convergence of the sequence of partial sums. An important concept when considering series is unconditional convergence, which guarantees that the limit of the series is invariant under permutations of the summands.\n\nIn a normed vector space, one can define absolute convergence as convergence of the series of norms (formula_1). Absolute convergence implies Cauchy convergence of the sequence of partial sums (by the triangle inequality), which in turn implies absolute-convergence of some grouping (not reordering). The sequence of partial sums obtained by grouping is a subsequence of the partial sums of the original series. The norm convergence of absolutely convergent series is an equivalent condition for a normed linear space to be Banach (i.e.: complete).\n\nAbsolute convergence and convergence together imply unconditional convergence, but unconditional convergence does not imply absolute convergence in general, even if the space is Banach, although the implication holds in formula_2.\n\nThe most basic type of convergence for a sequence of functions (in particular, it does not assume any topological structure on the domain of the functions) is pointwise convergence. It is defined as convergence of the sequence of values of the functions at every point. If the functions take their values in a uniform space, then one can define pointwise Cauchy convergence, uniform convergence, and uniform Cauchy convergence of the sequence.\n\nPointwise convergence implies pointwise Cauchy-convergence, and the converse holds if the space in which the functions take their values is complete. Uniform convergence implies pointwise convergence and uniform Cauchy convergence. Uniform Cauchy convergence and pointwise convergence of a subsequence imply uniform convergence of the sequence, and if the codomain is complete, then uniform Cauchy convergence implies uniform convergence.\n\nIf the domain of the functions is a topological space, local uniform convergence (i.e. uniform convergence on a neighborhood of each point) and compact (uniform) convergence (i.e. uniform convergence on all compact subsets) may be defined. Note that \"compact convergence\" is always short for \"compact uniform convergence,\" since \"compact pointwise convergence\" would mean the same thing as \"pointwise convergence\" (points are always compact).\n\nUniform convergence implies both local uniform convergence and compact convergence, since both are local notions while uniform convergence is global. If \"X\" is locally compact (even in the weakest sense: every point has compact neighborhood), then local uniform convergence is equivalent to compact (uniform) convergence. Roughly speaking, this is because \"local\" and \"compact\" connote the same thing.\n\nPointwise and uniform convergence of series of functions are defined in terms of convergence of the sequence of partial sums.\n\nFor functions taking values in a normed linear space, absolute convergence refers to convergence of the series of positive, real-valued functions formula_3 . \"Pointwise absolute convergence\" is then simply pointwise convergence of formula_3.\nNormal convergence is convergence of the series of non-negative real numbers obtained by taking the uniform (i.e. \"sup\") norm of each function in the series (uniform convergence of formula_3). In Banach spaces, pointwise absolute convergence implies pointwise convergence, and normal convergence implies uniform convergence.\n\nFor functions defined on a topological space, one can define (as above) local uniform convergence and compact (uniform) convergence in terms of the partial sums of the series. If, in addition, the functions take values in a normed linear space, then local normal convergence (local, uniform, absolute convergence) and compact normal convergence (absolute convergence on compact sets) can be defined.\n\nNormal convergence implies both local normal convergence and compact normal convergence. And if the domain is locally compact (even in the weakest sense), then local normal convergence implies compact normal convergence.\n\nIf one considers sequences of measurable functions, then several modes of convergence that depend on measure-theoretic, rather than solely topological properties, arise. This includes pointwise convergence almost-everywhere, convergence in \"p\"-mean and convergence in measure. These are of particular interest in probability theory.\n\n"}
{"id": "24796807", "url": "https://en.wikipedia.org/wiki?curid=24796807", "title": "Modular art", "text": "Modular art\n\nModular art is art created by joining together standardized units (modules) to form larger, more complex compositions. In some works the units can be subsequently moved, removed and added to – that is, \"modulated\" – to create a new work of art, different from the original or ensuing configurations.\n\nHistorically, alterable objects of art have existed since the Renaissance, for example, in the Triptych \"The Garden of Earthly Delights\" by Hieronymus Bosch or in the so-called \"alterable altarpieces\", such as the Isenheim Altarpiece by Matthias Grünewald, or Albrecht Dürer's Paumgartner altarpiece, where changing motifs could be revised in accord with the changing themes of the ecclesiastical calendar.\n\nBeginning in the first half of the 20th century, a number of contemporary artists sought to incorporate kinetic techniques into their work in an attempt to overcome what they saw as the predominantly static nature of art. Alexander Calder's mobiles are among the most widely known demonstrations of physical dynamism in the visual arts, in which form has the potential to continually vary through perpetual motion, sometimes even without the agency of the human hand. Jean Tinguely's efforts to create a self-destructive art machine constitute perhaps the ultimate expression of art's mutability, in this case by taking the form of its total eradication. Victor Vasarely postulated in his \"Manifest Jaune\" in 1955 in Paris that works of art should feature the properties of being multiplicable and repeatable in series. More recently, the notion that visual art need not be conceived solely in terms of perpetually fixed objects is embodied in performance and installation art by virtue of their unfolding and temporary qualities. \n\nModularity enters the modern artistic repertory largely through the disciplines of industrial design and architecture. Belgian architect Louis Herman De Koninck led a team of countrymen in creating one of the first modular product systems in their Cubex kitchen series of 1932. The series consisted of standardized and industrially produced components that could be combined and arrayed in limitless combinations to accommodate almost any size kitchen. New York designer Gilbert Rohde crafted several lines of modular casework for the Herman Miller Corporation in the 1930s and 40s; like De Koninck, Rohde standardized the units in dimensions, materials and configurations to facilitate mass production and interchangeability. His Executive Office Group (EOG) line, launched in 1942, was a similarly ground-breaking systems approach to office furniture. Just a year before Eero Saarinen and Charles Eames had jointly produced a suite of modular domestic furniture for the Red Lion Company, a result of a competition held by the Museum of Modern Art in New York. In 1950 Herman Miller brought out the EAS Storage Unit series by Charles and Ray Eames, a very successful modular shelving and wall unit system that remains in production today.\n\nThe module enjoys a long history in the realm of architecture. In antiquity architects utilized the module primarily as a unit of measurement guiding the proportions of plan and elevation. The Roman author and architect Vitruvius deployed the modular method in his descriptions of the classical orders and the composition of buildings in his treatise \"Ten Books on Architecture\", the only complete text on architecture to survive from antiquity. Architects of the Renaissance perpetuated the Vitruvian system in their resurrection of the classical orders, a tradition which continues to the present day. Among modern architects, the module is frequently employed as a design and planning tool.\n\nArchitecture and modular sculpture intersected starting in the 1950s in the work of Norman Carlberg, Erwin Hauer and Malcolm Leland. All three received commissions to design perforated architectural screens built out of repetitive modular motives cast in concrete. Non-structural, the screens were used on building exteriors to divide space, filter light and create visual interest. Their work has come to be described as Modular Constructivism, reflecting both its compositional methodology and its architectural context. Each created stand-alone modular-themed sculptures into the 1960s and after as well.\n\nRobert Rauschenberg's \"White Painting\" of 1951; consisting of just four equal white squares, with its geometry of interlocking forms, is among the earliest statements of modularity as an autonomous subject of art. Rauschenberg explored this theme that same year in a three- and seven-panel format; the linear array of rectangular panels in these versions suggests their potentially infinite replication. The cool abstraction of these canvases presages the emergence of modularity as a full-fledged topic of Minimalist art in the 1960s. Tony Smith, Sol LeWitt, Dan Flavin and Donald Judd are among this school's most prolific exponents during the period. In particular, the work of Smith is key to understanding the transformation of modularity from a compositional and production tool into a broadly investigated artistic theme in its own right.\n\nSmith began his career as an architectural designer. To further his education he apprenticed himself on some projects by Frank Lloyd Wright for a couple of years starting in 1938. From Wright he learned to utilize modular systems in generating architectural designs in two-dimensional plans as well as in three-dimensional applications, such as the development of building sections and interior built-ins. As an architect, Wright himself was part of a centuries-old continuum stretching back through Vitruvius to Greco-Roman antiquity in which the module was utilized to proportion built and sculpted form. In the case of Wright, the interest in modular design also may have derived from his familiarity with modular practice in traditional Japanese architecture.\n\nWright's Hanna House of 1937 is a clear example of the master's facility with modern modular design in multiple dimensions. Its striking angled forms are built up from the individual hexagonal modules that define the floor plan and various vertical elements. Wright's use of the hexagon here is by no means an arbitrary aesthetic choice, but an example of how he rooted his architecture in nature by drawing from its forms and principles – the interlocking hexagonal cells of the bee's honeycomb being nature's most perfect representation of modular design. Not surprisingly, this project is sometimes referred to as the \"Honeycomb House\".\n\nSmith would employ the hexagon and other elemental geometries in his own architectural practice and again in the sculpture he began to fabricate in the early 1960s. Freed from the programmatic and extensive structural requirements of his architectural work, Smith's sculptures are three-dimensional extrusions of modular form with no ostensible pragmatic purpose beyond aesthetic contemplation.\n\nSignificantly, Smith himself did not manually fabricate the final version of his sculptures. Instead, he outsourced their production to skilled ironworkers in foundries and industrial facilities, who worked off his drawings and models to manufacture the designs. In part this reflects Smith's training as an architect, who customarily designs and documents but does not construct his art. It further reinforces the idea of modular art as a generative system in which the arrangement of pre-determined formal units – rather than wholesale imaginative invention – defines the creative act. Finally, it is consistent with the notion implicit in modularity that the supply of modules in a modular system must be infinite, that is, that they be industrially rather than artisanally produced, for the system to be realized. (Bees in a honeycomb are essentially operating as an industrial enterprise insofar as the production of cells is without end.)\n\nThe work of Smith and the minimalist school constitute the most far-ranging exploration of modularity in art before the millennium. However, neither it nor the explorations of movable and alterable art in the preceding centuries synthesized the two central features of modular art. Mobiles and other kinetic pieces were not modular, and the modular work of the mid-century Minimalist artists was, with a few exceptions, not changeable.\n\nA school of thought coming out of the United States emphasizes modular art's alignment with the post-industrial character of 21st century culture and its contrast with traditional notions of art. Core characteristics of post-industrialism, as largely defined by the theorist Daniel Bell in his 1973 book \"The Coming of Post-Industrial Society\", include the emergence of a service economy in place of a manufacturing one; the social and economic pre-eminence of the creative, professional and technical classes; the central place of theoretical knowledge as a font of innovation; the strong influence of technology on daily life; and high levels of urbanization.\n\nModular art appears to synchronize perfectly with several of these criteria. For example, its manual changeability opens up the possibility of co-creative art, in which the collector or user collaborates with the originating modular artist to jointly determine the appearance of the work of art. This presupposes the existence of creative people capable of and interested in serving such a role – a demographic evolution that was already underway in Bell's time and that has since been studied in works like Richard Florida's \"Rise of the Creative Class\" (2002).\n\nCo-creation is closely associated with mass customization, a production model that combines the opportunity for individual personalization with mass production. Modular art and mass customization share a commonality in their synthesis of two opposing qualities. On the one hand, as previously stated the very concept of modularity implies a limitless supply of identical modules such as only industrial production can provide; on the other hand, the ability of the individual to re-arrange these modules in the work of art based on aesthetic criteria re-injects a subjective and purely human dimension into the creative act.\n\nMass customization is itself only made possible with the advent of computers and a type of software known as a configurator. A configurator is a software tool used by the buyer to configure a product from the options made available by the vendor. Applied to the purpose of composing a work of modular art onscreen, it can greatly facilitate the design of a modular assembly by allowing the user to study multiple design options more quickly and in far greater depth than by using analog methods. Once the design is established a computer file is then sent over the air to a manufacturing facility where robotically controlled equipment produces the object according to its specifications. Not only does this computer-aided manufacturing (CAM) allow for the customization of mass-produced objects, it also enables a much higher level of precision and fit – qualities critical for modules to be physically joined together. Bell's identification of technology as a central axis of post-industrial life is underscored in the intertwining of the digital with the physical realization of modular art.\n\nIn Europe, where the 1960s Minimalist school of modular art was often seen as a principally American phenomenon, the discussion of modularity often focuses on its changeability. For example, the mutability of art is a core principle of Arte Povera, a contemporaneous movement that emerged in Italy which holds that works of art \"should not be seen as fixed entities\", but as objects of change and movement to \"include time and space in a new manner. At stake is the issue of transferring the phenomenology of human experience\" into the arts.\n\nMore recently, the artist Leda Luss Luyken has produced modular paintings, composed of movable painted panels set in steel frames. Luss Luyken has dubbed her work \"ModulArt\". In her work, changing the configuration of a modular painting constitutes a form of motion, offering the spectator alternative views and alternative interpretations, and thus aligning the work more closely than a static object to the dynamism of physical human experience. Art historian and theorist Denys Zacharopoulos called this \"a new way of motion in painting\". The concept of modular art allows the user to de-compose and to re-compose a work of art that is already completed by re-arranging its parts, thus providing numerous possibilities for ever newer pictures not yet imagined. The original painting can be re-contextualized ad libitum and ad infinitum.\n\nWorking in the 1950-60s in Manchester (UK), American artist Mitzi Cunliffe developed sculptures consisting of multiple blocks about twelve inches square which she put together in a variety of combinations to give a sculptured effect on a large scale. She referred to them as \"modular sculptures\". The University of Manchester and the University of Manchester Institute of Science and Technology (UMIST) acquired some of these works, although there are no references to them in published accounts of her work.\n\nSculptor and ceramicist Malcolm Leland designed in 1954 a similar modular sculpture system based on a single 23-inch tall module that could be stacked vertically by means of a centering pin; the module has a generally biomorphic, curvilinear outline that yields an undulating silhouette when multiple modules are placed on top of each other. The technique of stacking repetitive elements in the round recalls Brancusi's \"Endless Column\" of 1938.\n\nStarting in the 1970s, Leland's contemporary Norman Carlberg produced groups of square framed prints which he placed together on a wall in a tight grid, each print conceived as an independent module. The viewer is then invited to rotate or reposition them to generate new composite images. The abstract quality of the prints enhanced the creative possibilities of their re-orientation insofar as they are non-directional and geometrically inter-related.\n\nGreek-born conceptual artist Leda Luss Luyken, who was initially trained as an architect, has been exploring modularity in the medium of painting since the 1990s. In her work standardized canvas panels are mounted as modules onto a steel frame within which they can be moved and rotated.\n\nIn the U.S. Moshé Elimelech creates what he has called \"Cubic Constructions\". These are multiple groupings of approximately three-inch cubes set inside pockets in a framed shadow box. On each cube he applies paint in fields of bright color and abstract pattern with precise, controlled brush strokes. Like Carlsberg, Elimelech then invites the viewer to reposition any or all of the cubes to display one of their six sides, each of which is painted in a different pattern. Exhibiting since the 1980s, Elimelech shows primarily in California galleries, and has work represented in several museum design stores as well.\n\nAnother portfolio of interactive modular art comes out of Studio for A.R.T. and Architecture, a New York-based firm headed by Donald Rattner. Rattner has designed modular art in the media of wall sculpture, rotational paintings, tapestries, artist's wallpapers and artist's books. To bring his work and those of other \"modulartists\" to the marketplace, Rattner founded A.R.T. (art-rethought), an art storefront focused on co-creative and modular work. In his writings Rattner has emphasized the post-industrial aspect of the most recent trends in modular art, coining the term \"New Industrialism\" to denote mass customization, production on demand, open innovation, co-creative design, tele-fabrication, robotics and other computer-driven technologies that are re-defining how things are made in the global marketplace.\n\nModularity in music can be seen as bringing two key elements of musical composition and film into the world of painting: variation of a theme and movement of and within a picture. For this very reason the contemporary composer Minas Borboudakis has dedicated the third part of his trilogy ROAI III for piano and electronics to the modular methodology.\n\nItalian composer and arts theoretician Stefano Vagnini has developed a theory of open-source composition based on modular aggregation. The concept of a musical work of art being something closed, limited and immobile disappears in favor of a process of numerous aggregations that allow a composition to become infinite in principle. Several such compositions were performed in Europe, South America, Asia and in North America and discussed through conferences in Europe. The approach is being academically discussed at the University of West Georgia and the Carrollton Cultural Arts Centre in the USA. Writer, painter, and art theorist Gian Ruggero Manzoni described the modularity of Vagnini's compositions as “circular like the existence, his works are not finished, but merely stimulus for new voices”. \n\n\n\n"}
{"id": "53471572", "url": "https://en.wikipedia.org/wiki?curid=53471572", "title": "Monte Carlo polarization", "text": "Monte Carlo polarization\n\nIn Analytic Business Theory Monte Carlo Polarization is an opinion generation algorithm for a given prototype or design idea. \nThe algorithm expands on traditional Monte Carlo Aggregation which operates by placing candidates together and selecting a subset at random.\nEach member of this subset is then asked for an opinion usually by filling out a form.\nA resultant opinion scalar can be generated by application of the Softmax function over the generated form set.\nHowever Monte Carlo Polarization goes a step further and attempts to construct the subset with the greatest standard deviation in response, referred to as the form data response eigen-norm vector scalar.\nThe idea of Monte-Carlo Polarization was firstly invented in Athens, (more commonly known as Thens), by Errikos Babudopoulos in 1978, but was mostly used in research in the 1990s by famous mathematicians, such as Grigori Perelman, in proving the soul conjecture.\n\nThe origins of Monte Carlo Polarization came from the following observations made in early 1922:\nWhere the validity of an opinion is defined by the Emotional Intelligence Hierarchical metric space, using the obvious distance function.\n\nGiven an array \"A\" of \"n\" elements with values or records \"A\" ... \"A\", sorted such that \"A\" ≤ ... ≤ \"A\", and target value \"T\", the following subroutine uses Monte Carlo Polarization to find the index of \"T\" in \"A\".\n\nExtract the emotional category isofunctor morphismvector and append this to the business manifesto.\n\nNote: This can be done in matrix form. This is left as an exercise for the reader.\n\nAlthough this a very recent cutting-edge technique, it has seen a couple of variations upon the basic algorithm in the last couple of month, most notably JSON driven resolution methods. the basic idea is that instead of supplying the algorithm with \"n\" records, it is more useful to provide the algorithm with emotional meta-data to guide its search and improve its complexity beyond the usual logarithmic bounds and this by a factor of \"log(n)/2\". It allows to select intermediate \"m\" values for the search index and skew them towards the wanted emotional value in the initial records.\n\nThe acceleration structures permitted by the Monte Carlo Polarization consist mainly in BVH and EBVH hierarchies. The logical subdivision of the kernel space leads to a logarithmic complexity, which is key to the scalability of the sentient analysis tools.\n\nA key application is the direct targeting of hidden nodes in neural networks. By applying a Monte Carlo Polarization filter to the input layer of the neural system, hidden layers will be systematically and dynamically selected based on user-defined characteristics. Only the specified layers and units will receive and process the data.\n\nCompared to standard drop-off methods, Monte Carlo Polarization is both more effective and more secure. Instead of having all nodes receiving the data and selecting output from a subset, the unnecessary nodes are directly filtered out. The result is a greatly increased level of accuracy and protection, as unreliable and malicious nodes will be left out, and a higher degree of efficiency.\n\nThe neural system that is created using the aforementioned method is the basis for many Computer Vision projects. A specific highlight is the American web-animated sitcom \"F is for Family\". \n\nMonte Carlo Polarization can be easily deployed through NodeJS\n\nThe library provides a basic implementation of Monte Carlo Polarization, and shows the kernel space learning algorithm applied to session tokens.\n\nThe native support of JSON files by NodeJS's Javascript language is an example of the application of JSON Driven Monte Carlo Polarization.\n\nBeing a cutting edge technology, researchers are experimenting the expandability of the current technology to support Asynchronous Transport Protocol for JSON, and to provide an api for classic AJAX (Asynchronous Javascript and XML) interface by tunneling the data through Socket.IO packets secured by Blockchain technology.\n"}
{"id": "882887", "url": "https://en.wikipedia.org/wiki?curid=882887", "title": "Musha incident", "text": "Musha incident\n\nThe Musha Incident (; ), also known as the Wushe Rebellion and several other similar names, began in October 1930 and was the last major uprising against colonial Japanese forces in Japanese Taiwan. In response to long-term oppression by Japanese authorities, the Seediq indigenous group in Musha (Wushe) attacked the village, killing over 130 Japanese. In response, the Japanese led a relentless counter-attack, killing over 600 Seediq in retaliation. The handling of the incident by the Japanese authorities was strongly criticised, leading to many changes in aboriginal policy.\n\nPrevious armed resistance to Japanese imperial authority had been dealt with sternly, as evident in responses to previous uprisings such as the Tapani Incident, which resulted in a cycle of rebel attacks and strict Japanese retaliation. However, by the 1930s, armed resistance had largely been replaced by organised political and social movements among the younger Taiwanese generation. Direct police involvement in local administration had been relaxed, many stern punishments were abolished, and some elements of self-government, albeit of questionable effectiveness, had been introduced to colonial Taiwan.\n\nHowever, a different approach was used in order to control Taiwan's indigenous peoples. Formosa Island aborigines were still designated as , and treated as savages rather than equal subjects. Tribes were 'tamed' through assimilation, the process of disarming traditional hunting tribes and forcing them to relocate to the plains and lead an agrarian existence. Further resistance was then dealt with by military campaigns, isolation and containment. In order to access natural resources in mountainous and forested indigenous-controlled areas, Governor-General Sakuma Samata adopted a more aggressive terrain policy, attempting to pacify or eradicate aboriginal groups in areas scheduled for logging within five years; by 1915, this policy had been largely successful, although resistance still existed in more remote areas.\n\nThe Seediq aborigines in the vicinity of Musha had been considered by Japanese authorities to be one of the most successful examples of this \"taming\" approach, with Chief Mouna Rudao being one of 43 indigenous leaders selected for a tour of Japan a few years earlier. However, resentment still lingered, due largely to police misconduct, the continuing practice of forced labor, and the ill treatment of indigenous beliefs and customs.\n\nIn the days immediately prior to the incident, chief Mona Rudao held a traditional wedding banquet for his son Daho Mona, during which animals were slaughtered and wine was prepared and drunk. A Japanese police officer named Katsuhiko Yoshimura was on patrol in the area, and was offered a cup of wine by Daho Mona as a symbolic gesture. The officer refused, saying that Daho Mona's hands were soiled with blood from the slaughtered animals. Daho Mona attempted to take hold of the officer, insisting he participate, and the officer struck him with his stick. Fighting ensued, and the officer was injured. Chief Mona Rudao attempted to apologize by presenting a flagon of wine at the officer's house, but was rejected. The simmering resentment among the Seediq in Musha was finally pushed to the limit.\n\nOn 27 October 1930, hundreds of Japanese converged on Musha for an athletics meet at the Musyaji Elementary School. Shortly before dawn, Mona Rudao led over 300 Seediq warriors in a raid of strategic police sub-stations to capture weapons and ammunition. They then moved on the elementary school, concentrating their attack on the Japanese in attendance. A total of 134 Japanese, including women and children, were killed in the attack. Two Han Taiwanese dressed in Japanese clothing were also mistakenly killed, one of whom was a girl wearing a Japanese kimono. The Aboriginals aimed to only kill Japanese specifically.\n\nThe Japanese authorities responded with unprecedentedly harsh military action. A press blackout was enforced, and Governor General Eizo Ishizuka ordered a counter-offensive of two thousand troops to be sent to Musha, forcing the Seediq to retreat into the mountains and carry out guerrilla attacks by night. Unable to root out the Seediq despite their superior numbers and firepower, the Japanese faced a political need for a faster solution. Consequently, Japan's army air corps in Taiwan ordered bombing runs over Musha to smoke out the rebels, dropping mustard gas bombs in violation of the Geneva Protocol in what was allegedly the first such use of chemical warfare in Asia. The uprising was swiftly quelled, with any remaining resistance suppressed by the third week of December 1930; Mona Rudao had committed suicide on November 28, but the uprising had continued under other leaders. Of the 1,200 Seediq directly involved in the uprising, 644 died, 290 of whom committed suicide to avoid dishonor.\n\nDue to internal and external criticism of their handling of the incident, Governor-General Kamiyama and Goto Fumio, his chief civil administrator, were forced to resign in January 1931. However, Kamiyama's replacement, Ota Masahiro, also took a harsh approach to controlling Taiwan's indigenous peoples: certain tribes were disarmed and left unprotected, giving their aboriginal enemies an opportunity to annihilate them on behalf of the Japanese administration. Around 500 of the Seediq involved in the Musha Incident surrendered and were subsequently confined to a village near Musha. However, on 25 April 1931, indigenous groups working with the Japanese authorities attacked the village, beheading all remaining males over the age of 15. This came to be known as the \"Second Musha Incident\".\n\nHowever, the uprising did affect a change in the authorities' attitudes and approaches towards aborigines in Taiwan. Musha had been regarded as the most \"enlightened and compliant\" of the aboriginal territories, and the colonial power's inability to prevent the massacre provoked a fear of similar nationalist movements starting in Taiwan, Korea, and Japan itself. A change in policy was clearly needed. Ching suggests that the ideology of imperialisation (\"kominka\") became the dominant form of colonial control; aborigines became represented as imperial subjects on equal footing with other ethnic groups in Taiwan, and were upgraded in status from \"raw savages\" to . Furthermore, Japanization education was intensified, promoting Japanese culture and loyalty to the emperor in the younger generation.\n\nDuring the Musha Incident Seediq Tkdaya under Mona Rudao revolted against the Japanese while the Truku and Toda did not. The rivalry between the Seediq Tkdaya vs the Toda and Truku (Taroko) was aggravated by Musha Incident, since the Japanese had long played them off against each other. Tkdaya land was given to the Truku (Taroko) and Toda by the Japanese after the incident.\n\nThe Musha Incident has been depicted three times in movies, in 1957 in the film \"Qing Shan bi xue\" (), in the 2003 TV drama , and in the 2011 Taiwanese film \"Seediq Bale\". \n\nThe Chinese novel \"Remains of Life\" (originally published in Chinese in 1999, published in English translation in 2017) is a fictionalized account of the aftermath of this incident.\n\n\n"}
{"id": "161824", "url": "https://en.wikipedia.org/wiki?curid=161824", "title": "Neue Slowenische Kunst", "text": "Neue Slowenische Kunst\n\nNeue Slowenische Kunst (a German phrase meaning \"New Slovenian Art\"), a.k.a. NSK, is a controversial political art collective that formed in Slovenia in 1984, when Slovenia was part of Yugoslavia. NSK's name, being German, is compatible with a theme in NSK works: the complicated relationship Slovenes have had with Germans. The name of NSK's music wing, Laibach, is also the German name of the Slovene capital Ljubljana. The name created controversy because some felt it evoked memories of the Nazi annexation of Slovenia during the Second World War. It also refers to Slovenia's previous seven centuries as part of the Habsburg Monarchy.\n\nNSK's best-known member is also its founder, the musical group Laibach. Other NSK members include groups such as IRWIN (visual art), Scipion Nasice Sisters Theatre (also known as Red Pilot and Cosmokinetic Theatre Noordung), New Collective Studio (graphics; also known as New Collectivism), Retrovision (film and video), and the Department of Pure and Applied Philosophy (theory). The founding groups of the NSK were Laibach, IRWIN, and Scipion Nasice Sisters Theater. Membership has traditionally been open to all artistic groups who show an interest in challenging the norms and taboos of Slovene national identity.\n\nNSK art often draws on symbols drawn from totalitarian or extreme nationalist movements, often reappropriating totalitarian kitsch in a visual style reminiscent of Dada. NSK artists often juxtapose symbols from different (and often incompatible) political ideologies. For example, a 1987 NSK-designed poster caused a scandal by winning a competition for the Yugoslavian Youth Day Celebration. The poster appropriated a painting by Nazi artist Richard Klein, replacing the flag of Nazi Germany with the Yugoslav flag and the German eagle with a dove. Intended as an ironic joke, the painting soon fell foul of the authorities, who interpreted it as equating Marshal Josip Broz Tito with Adolf Hitler. It was later reproduced on the cover of \"Mladina\" magazine, and that particular issue was subsequently banned.\n\nBoth IRWIN and Laibach are emphatic about their work being collective rather than individual. Laibach's original songs and arrangements are always credited to the group itself, and individual musicians are not listed on their album covers. At one point, there were even two separate groups touring under the name Laibach at the same time. Both had members of the original group. Similarly, the IRWIN artists never sign their work individually; instead, they are \"signed\" with a stamp or certificate indicating approval as a work from the IRWIN collective.\n\nThe NSK was the subject of a 1996 documentary film entitled \"Predictions of Fire\" (\"Prerokbe Ognja\"), which was written and directed by Michael Benson. Among those interviewed is Slovenian intellectual Slavoj Žižek.\n\nSince 1991, the NSK has claimed to be a sovereign state of sorts, a claim similar to that of micronations. As such, displays of its members' work are performed under the guise of an embassy, or a territory of the supposed \"state\". Since 1994, the NSK has maintained consulates in several cities, including Umag, Croatia. It has also gone so far as to issue passports and postage stamps. In 2006, Laibach recorded the NSK National Anthem for their LP \"Volk\". The anthem adopts its melody from another Laibach song, \"The Great Seal.\" It includes a recitation of an excerpt from Winston Churchill's famous \"We shall fight them on the beaches/We shall never surrender\" speech.\n\nNSK passports are considered an art project. As such, they are not valid for travel. However, many desperate people have fallen for a scam in which they are issued a NSK passport. Most of these scams originate in Nigeria and Egypt.\n\nIn 2010, the first NSK Citizens Congress was held in Berlin. It was followed by a \"NSK Rendez-Vous\" in Lyon, France, where Alexei Monroe revealed NSK's aim : to make people \"aware that totalitarianism isn't a discrete historical phenomenon which went on from 1933 to 1989 and then it's over so let's have a nice triumph of liberal democracy\". On February 26, 2011, another \"NSK Rendez-Vous\" took place in London. In 2012, yet another \"NSK Rendez-Vous\" was set to take place in New York City's Museum of Modern Art.\n\n2017 saw NSK set up a pavilion at the Venice Biennale where Slavoj Žižek stated that \"the uniqueness of NSK is this idea of the 'stateless state'.\n\nA diverse collection of intriguing artefacts created by the citizens of the NSK State in Time. This virtual state, formed in 1992, today contains over fourteen thousand citizens from Dublin and Taipei to Sarajevo and New York. As it has evolved over the last 20 years, members have developed a strong sense of collective identity, which has manifested itself in the creation of a unique Folk Art, expressing citizens' devotion to the state and ideologies of NSK.\n\nNSK Folk Art at Calvert 22\n\nThis is presented in parallel with IRWIN - Time for A New State Calvert 22 exhibits a selection of 'NSK Folk Art'. Works on display include NSK passports, stamps, plates and films made by NSK state members. Exhibited works are from IRWIN and NSKSTATE.COM collection NSK Folk Art. NSK Folk Art is part of a London wide presentation in cooperation with Tate, who will be hosting a Symposium and which will also include a music performance at Tate Modern's Turbine Hall, an exhibition of archival material at Chelsea Space (Chelsea College of Art) and a seminar at UCL.\nArtists: Peter Blase, Christian Chrobok, Charles Krafft, Danaja, Christian Matzke, Public Movement, Astrid Thingplatz, Valnoir \n\n1st NSK Folk Art Biennale - NSK: Past – Present – Future\n\nThe exhibition \"NSK: Past – Present – Future\" marks the 30th anniversary of the founding of NSK by depicting the development of the Slovenian artist collective \"Neue Slowenische Kunst\" from its birth within the context of Yugoslavian alternative culture in the 80s through the founding of the NSK State in Time in 1991, and offers a glimpse into the future.\nSelected artists that participated in the '1st NSK Folk Art Biennale' : Annelise Bully (France), Julio Canto Chollet (Brasil), Jacques Gassmann (Germany), Kurt Grüng (Great Britain) Martin Höfer (Germany), KavecS (Greece), Kenji Konishi (Japan), Avi Pitchon (Israel), Detlef Schweiger (Germany) \nThe 2nd NSK State Folk Art Biennale\n\nBurren College of Art is pleased to announce 'The 2nd NSK State Folk Art Biennale' as this year's Burren Annual exhibition. The 2nd NSK State Folk Art Biennale will bring together over 30 international artists and collectives including special guests and NSK founding members, IRWIN. The NSK State was created in 1992 as a project of the Neue Slowenische Kunst artists' collective (founded in Ljubljana, Slovenia, in 1984). In response to the break up of Yugoslavia in the early 1990s, the NSK collective created a virtual state in time and without territory, to examine and question nationalism, national boundaries and the utopian desires underpinning social formations. The NSK State produces passports, opens temporary consulates, and currently has over 14,000 citizens. The 1st NSK Folk Art Biennale took place in Leipzig in 2014, and featured a wide range of works generated by NSK citizens and artists who critically examined the contemporary nation state and questions of power, history and identity, as well as the artistic legacy created by NSK. The 2nd NSK State Folk Art Biennale will be the second major NSK related event in Ireland, following the NSK State Dublin week of events held in Dublin in 2004, in which all the NSK groups participated and at which an NSK passport office was opened.\nArtists: Athanasios Anagnostopoulos (GR), Caul Audiac (IE), Zbigniew Bogusławski (PL), Vera Bremerton (IT), Carsten Busse (DE), Christian Chrobok (DE), Patrick Corcoran (IE), Dominic Corrigan (IE), Shane Cullen (IE), Haris Hararis (GR), IRWIN (SL), Marilyn Lennon (IE), Llewyn Maire (US), Russell MacEwan (UK), Emily McMehen & Geoffrey Sautner (CA & UK/IE), Valnoir (FR), Nikkita Morgan (IE), Lili Anamarija No (SL), Kevin Noble (US), NSK Lipsk (DE), NSKNY (US), Noël O'Callaghan (IE), Maurice O'Connell (IE), Alan Phelan (IE), Avi Pitchon (IS), David K. Thompson (US), Darn Thorn (IE), Tanja Ravlić (HR), Florian Schaurer (DE), Berthold Schymura (DE), Andre Stitt (UK), Sz.Berlin (UK), Enemy Tone (US), TLO (Three Letter Organisation) (IE), Rose Vidal (FR)\n\n\n"}
{"id": "26565579", "url": "https://en.wikipedia.org/wiki?curid=26565579", "title": "Neuroscience of free will", "text": "Neuroscience of free will\n\nNeuroscience of free will, a part of neurophilosophy, is the study of the interconnections between free will and neuroscience.\n\nAs it has become possible to study the human living brain, researchers have begun to watch decision making processes at work. Findings could carry implications for our sense of agency, moral responsibility, and our understanding of consciousness in general. One of the pioneering studies in this domain was designed by Benjamin Libet, while other studies have attempted to predict participant actions before they make them. \nThe field remains highly controversial. There is no consensus among researchers about the significance of findings, their meaning, or what conclusions may be drawn. The precise role of consciousness in decision making therefore remains unclear.\n\nThinkers like Daniel Dennett or Alfred Mele consider the language used by researchers. They explain that \"free will\" means many different things to different people (e.g. some notions of free will are dualistic, some not). Dennett insists that many important and common conceptions of \"free will\" are compatible with the emerging evidence from neuroscience.\n\nOne significant finding of modern studies is that a person's brain seems to commit to certain decisions before the person becomes aware of having made them. Researchers have found delays of about half a second (discussed in sections below). With contemporary brain scanning technology, other scientists in 2008 were able to predict with 60% accuracy whether subjects would press a button with their left or right hand up to 10 seconds before the subject became aware of having made that choice. These and other findings have led some scientists, like Patrick Haggard, to reject some forms of \"free will\". To be clear, no single study would disprove all forms of free will. This is because the term \"free will\" can encapsulate different hypotheses, each of which must be considered in light of existing empirical evidence.\n\nThere have also been a number of problems regarding studies of free will. Particularly in earlier studies, research relied too much on the introspection of the participants, but introspective estimates of event timing were found to be inaccurate. Many brain activity measures have been insufficient and primitive as there is no good independent brain-function measure of the conscious generation of intentions, choices, or decisions. The conclusions drawn from measurements that \"have\" been made are debatable too, as they don't necessarily tell, for example, what a sudden dip in the readings is representing. In other words, the dip might have nothing to do with unconscious decision, since many other mental processes are going on while performing the task. Some of the research mentioned here has gotten more advanced, however, even recording individual neurons in conscious volunteers. Researcher Itzhak Fried says that available studies do at least suggest consciousness comes in a later stage of decision making than previously expected – challenging any versions of \"free will\" where intention occurs at the beginning of the human decision process.\n\nIt is quite likely that a large range of cognitive operations are necessary to freely press a button. Research at least suggests that our conscious self does not initiate all behavior. Instead, the conscious self is somehow alerted to a given behavior that the rest of the brain and body are already planning and performing. These findings do not forbid conscious experience from playing some moderating role, although it is also possible that some form of unconscious process is what is causing modification in our behavioral response. Unconscious processes may play a larger role in behavior than previously thought.\n\nIt may be possible, then, that our intuitions about the role of our conscious 'intentions' have led us astray; it may be the case that we have confused correlation with causation by believing that conscious awareness necessarily causes the body's movement. This possibility is bolstered by findings in neurostimulation, brain damage, but also research into introspection illusions. Such illusions show that humans do not have full access to various internal processes. The discovery that humans possess a determined will would have implications for moral responsibility. Neuroscientist and author Sam Harris believes that we are mistaken in believing the intuitive idea that intention initiates actions. In fact, Harris is even critical of the idea that free will is 'intuitive': he says careful introspection can cast doubt on free will. Harris argues - \"Thoughts simply arise in the brain. What else could they do? The truth about us is even stranger than we may suppose: The illusion of free will is itself an illusion\". Neuroscientist Walter Jackson Freeman III nevertheless talks about the power of even unconscious systems and actions to change the world according to our intentions. He writes \"our intentional actions continually flow into the world, changing the world and the relations of our bodies to it. This dynamic system is the self in each of us, it is the agency in charge, not our awareness, which is constantly trying to keep up with what we do.\" To Freeman, the power of intention and action can be independent of awareness.\n\nSome thinkers like neuroscientist and philosopher Adina Roskies think these studies can still only show, unsurprisingly, that physical factors in the brain are involved before decision making. In contrast, Haggard believes that \"We feel we choose, but we don't\". Researcher John-Dylan Haynes adds \"How can I call a will 'mine' if I don't even know when it occurred and what it has decided to do?\". Philosophers Walter Glannon and Alfred Mele think some scientists are getting the science right, but misrepresenting modern philosophers. This is mainly because \"free will\" can mean many things: It is unclear what someone means when they say \"free will does not exist\". Mele and Glannon say that the available research is more evidence against any dualistic notions of free will – but that is an \"easy target for neuroscientists to knock down\". Mele says that most discussions of free will are now had in materialistic terms. In these cases, \"free will\" means something more like \"not coerced\" or that \"the person could have done otherwise at the last moment\". The existence of these types of free will is debatable. Mele agrees, however, that science will continue to reveal critical details about what goes on in the brain during decision making.\nThis issue may be controversial for good reason: There is evidence to suggest that people normally associate a belief in free will with their ability to affect their lives. Philosopher Daniel Dennett, author of \"Elbow Room\" and a supporter of deterministic free will, believes scientists risk making a serious mistake. He says that there are types of free will that are incompatible with modern science, but he says those kinds of free will are not worth wanting. Other types of \"free will\" are pivotal to people's sense of responsibility and purpose (see also \"believing in free will\"), and many of these types are actually compatible with modern science.\n\nThe other studies described below have only just begun to shed light on the role that consciousness plays in actions and it is too early to draw very strong conclusions about certain kinds of \"free will\". It is worth noting that such experiments – so far – have dealt only with free will decisions made in short time frames (seconds) and \"may\" not have direct bearing on free will decisions made (\"thoughtfully\") by the subject over the course of \"many\" seconds, minutes, hours or longer. Scientists have also only so far studied extremely simple behaviors (e.g. moving a finger). Adina Roskies points out five areas of neuroscientific research: 1.) action initiation, 2.) intention, 3). decision, 4.) Inhibition and control, and 5.) the phenomenology of agency, and for each of these areas Roskies concludes that the science may be developing our understanding of volition or \"will\", but it yet offers nothing for developing the \"free\" part of the \"free will\" discussion.\n\nThere is also the question of the influence of such interpretations in people's behaviour. In 2008, psychologists Kathleen Vohs and Jonathan Schooler published a study on how people behave when they are prompted to think that determinism is true. They asked their subjects to read one of two passages: one suggesting that behaviour boils down to environmental or genetic factors not under personal control; the other neutral about what influences behaviour. The participants then did a few math problems on a computer. But just before the test started, they were informed that because of a glitch in the computer it occasionally displayed the answer by accident; if this happened, they were to click it away without looking. Those who had read the deterministic message were more likely to cheat on the test. \"Perhaps, denying free will simply provides the ultimate excuse to behave as one likes,\" Vohs and Schooler suggested.\n\nA pioneering experiment in this field was conducted by Benjamin Libet in the 1980s, in which he asked each subject to choose a random moment to flick their wrist while he measured the associated activity in their brain (in particular, the build-up of electrical signal called the Bereitschaftspotential (BP), which was discovered by Kornhuber & Deecke in 1965). Although it was well known that the Bereitschaftspotential (sometimes also termed \"readiness potential\") preceded the physical action, Libet asked how the Bereitschaftspotential corresponded to the felt intention to move. To determine when the subjects felt the intention to move, he asked them to watch the second hand of a clock and report its position when they felt that they had felt the conscious will to move.\nLibet found that the \"unconscious\" brain activity leading up to the \"conscious\" decision by the subject to flick their wrist began approximately half a second \"before\" the subject consciously felt that they had decided to move. Libet's findings suggest that decisions made by a subject are first being made on a subconscious level and only afterward being translated into a \"conscious decision\", and that the subject's belief that it occurred at the behest of their will was only due to their retrospective perspective on the event.\n\nThe interpretation of these findings has been criticized by Daniel Dennett, who argues that people will have to shift their attention from their intention to the clock, and that this introduces temporal mismatches between the felt experience of will and the perceived position of the clock hand. Consistent with this argument, subsequent studies have shown that the exact numerical value varies depending on attention. Despite the differences in the exact numerical value, however, the main finding has held. Philosopher Alfred Mele criticizes this design for other reasons. Having attempted the experiment himself, Mele explains that \"the awareness of the intention to move\" is an ambiguous feeling at best. For this reason he remained skeptical of interpreting the subjects' reported times for comparison with their 'Bereitschaftspotential'.\n\nIn a variation of this task, Haggard and Eimer asked subjects to decide not only when to move their hands, but also to decide \"which hand to move\". In this case, the felt intention correlated much more closely with the \"lateralized readiness potential\" (LRP), an ERP component which measures the difference between left and right hemisphere brain activity. Haggard and Eimer argue that the feeling of conscious will must therefore follow the decision of which hand to move, since the LRP reflects the decision to lift a particular hand.\n\nA more direct test of the relationship between the Bereitschaftspotential and the \"awareness of the intention to move\" was conducted by Banks and Isham (2009). In their study, participants performed a variant of the Libet's paradigm in which a delayed tone followed the button press. Subsequently, research participants reported the time of their intention to act (e.g., Libet's \"W\"). If W were time-locked to the Bereitschaftspotential, W would remain uninfluenced by any post-action information. However, findings from this study show that W in fact shifts systematically with the time of the tone presentation, implicating that W is, at least in part, retrospectively reconstructed rather than pre-determined by the Bereitschaftspotential.\n\nA study conducted by Jeff Miller and Judy Trevena (2009) suggests that the Bereitschaftspotential (BP) signal in Libet's experiments doesn't represent a decision to move, but that it's merely a sign that the brain is paying attention. In this experiment the classical Libet experiment was modified by playing an audio tone indicating to volunteers to decide whether to tap a key or not. The researchers found that there was the same RP signal in both cases, regardless of whether or not volunteers actually elected to tap, which suggests that the RP signal doesn't indicate that a decision has been made.\n\nIn a second experiment, researchers asked volunteers to decide on the spot whether to use left hand or right to tap the key while monitoring their brain signals, and they found no correlation among the signals and the chosen hand. This criticism has itself been criticized by free-will researcher Patrick Haggard, who mentions literature that distinguishes two different circuits in the brain that lead to action: a \"stimulus-response\" circuit and a \"voluntary\" circuit. According to Haggard, researchers applying external stimuli may not be testing the proposed voluntary circuit, nor Libet's hypothesis about internally triggered actions.\n\nLibet's interpretation of the ramping up of brain activity prior to the report of conscious \"will\" continues to draw heavy criticism. Studies have questioned participants' ability to report the timing of their \"will\". Authors have found that preSMA activity is modulated by attention (attention precedes the movement signal by 100ms), and the prior activity reported could therefore have been product of paying attention to the movement. They also found that the perceived onset of intention depends on neural activity that takes place after the execution of action. Transcranial magnetic stimulation (TMS) applied over the preSMA after a participant performed an action shifted the perceived onset of the motor intention backward in time, and the perceived time of action execution forward in time.\n\nOthers have speculated that the preceding neural activity reported by Libet may be an artefact of averaging the time of \"will\", wherein neural activity does not always precede reported \"will\". In a similar replication they also reported no difference in electrophysiological signs before a decision not to move, and before a decision to move.\n\nDespite his findings, Libet himself did not interpret his experiment as evidence of the inefficacy of conscious free will — he points out that although the tendency to press a button may be building up for 500 milliseconds, the conscious will retains a right to veto any action at the last moment. According to this model, unconscious impulses to perform a volitional act are open to suppression by the conscious efforts of the subject (sometimes referred to as \"free won't\"). A comparison is made with a golfer, who may swing a club several times before striking the ball. The action simply gets a rubber stamp of approval at the last millisecond. Max Velmans argues however that \"free won't\" may turn out to need as much neural preparation as \"free will\" (see below).\n\nSome studies have however replicated Libet's findings, whilst addressing some of the original criticisms. A recent study has found that individual neurons were found to fire 2 seconds before a reported \"will\" to act (long before EEG activity predicted such a response). Itzhak Fried replicated Libet's findings in 2011 at the scale of the single neuron. This was accomplished with the help of volunteer epilepsy patients, who needed electrodes implanted deep in their brain for evaluation and treatment anyway. Now able to monitor awake and moving patients, the researchers replicated the timing anomalies that were discovered by Libet and are discussed in the following study. Similarly to these tests, Chun Siong Soon, Anna Hanxi He, Stefan Bode and John-Dylan Haynes have conducted a study in 2013 claiming to be able to predict the choice to sum or subtract before the subject reports it.\n\nWilliam R. Klemm pointed out the inconclusiveness of these tests due to design limitations and data interpretations and proposed less ambiguous experiments, while affirming a stand on the existence of free will like Roy F. Baumeister or Catholic neuroscientists such as Tadeusz Pacholczyk. Adrian G. Guggisberg and Annaïs Mottaz have also challenged Itzhak Fried's findings.\n\nA study by Aaron Schurger and colleagues published in PNAS challenged assumptions about the causal nature of the Bereitschaftspotential itself (and the \"pre-movement buildup\" of neural activity in general), thus denying the conclusions drawn from studies such as Libet's and Fried's. See The Information Philosopher and New Scientist for commentary on this study.\n\nA study by Masao Matsuhashi and Mark Hallett, published in 2008, claims to have replicated Libet's findings without relying on subjective report or clock memorization on the part of participants. The authors believe that their method can identify the time (T) at which a subject becomes aware of his own movement. Matsuhashi and Hallet argue that this time not only varies, but often occurs after early phases of movement genesis have already begun (as measured by the readiness potential). They conclude that a person's awareness cannot be the cause of movement, and may instead only notice the movement.\n\nMatsuhashi and Hallett's study can be summarized thus. The researchers hypothesized that, if our conscious intentions are what causes movement genesis (i.e. the start of an action), then naturally, our conscious intentions should always occur before any movement has begun. Otherwise, if we ever become aware of a movement only after it has already been started, our awareness could not have been the cause of that particular movement. Simply put, conscious intention must precede action if it is its cause.\n\nTo test this hypothesis, Matsuhashi and Hallet had volunteers perform brisk finger movements at random intervals, while not counting or planning when to make such (future) movements, but rather immediately making a movement as soon as they thought about it. An externally controlled \"stop-signal\" sound was played at pseudo random intervals, and the volunteers had to cancel their intent to move if they heard a signal while being aware of their own immediate intention to move. Whenever there \"was\" an action (finger movement), the authors documented (and graphed) any tones that occurred before that action. The graph of tones before actions therefore only shows tones (a) before the subject is even aware of his \"movement genesis\" (or else they would have stopped or \"vetoed\" the movement), and (b) after it is too late to veto the action. This second set of graphed tones is of little importance here.\n\nIn this work, \"movement genesis\" is defined as the brain process of making movement, of which physiological observations have been made (via electrodes) indicating that it may occur before conscious awareness of intent to move (see Benjamin Libet).\n\nBy looking to see when tones started preventing actions, the researchers supposedly know the length of time (in seconds) that exists between when a subject holds a conscious intention to move and performs the action of movement. This moment of awareness (as seen in the graph below) is dubbed \"T\" (the mean time of conscious intention to move). It can be found by looking at the border between tones and no tones. This enables the researchers to estimate the timing of the conscious intention to move without relying on the subject's knowledge or demanding them to focus on a clock. The last step of the experiment is to compare time T for each subject with their Event-related potential (ERP) measures (e.g. seen in this page's lead image), which reveal when their finger movement genesis first begins.\n\nThe researchers found that the time of the conscious intention to move T normally occurred \"too late\" to be the cause of movement genesis. See the example of a subject's graph below on the right. Although it is not shown on the graph, the subject's readiness potentials (ERP) tells us that his actions start at –2.8 seconds, and yet this is substantially earlier than his conscious intention to move, time \"T\" (−1.8 seconds). Matsuhashi and Hallet concluded that the feeling of the conscious intention to move does not cause movement genesis; both the feeling of intention and the movement itself are the result of unconscious processing.\n\nThis study is similar to Libet's in some ways: volunteers were again asked to perform finger extensions in short, self-paced intervals. In this version of the experiment, researchers introduced randomly timed \"stop tones\" during the self paced movements. If participants were not conscious of any intention to move, they simply ignored the tone. On the other hand, if they were aware of their intention to move at the time of the tone, they had to try to veto the action, then relax for a bit before continuing self-paced movements. This experimental design allowed Matsuhashi and Hallet to see when, once the subject moved his finger, any tones occurred. The goal was to identify their own equivalent of Libet's W, their own estimation of the timing of the conscious intention to move, which they would call \"T\"(time)\n\nTesting the hypothesis that 'conscious intention occurs after movement genesis has already begun' required the researchers to analyse the distribution of responses to tones before actions. The idea is that, after time T, tones will lead to vetoing and thus a reduced representation in the data. There would also be a point of no return P where a tone was too close to the movement onset for the movement to be vetoed. In other words, the researchers were expecting to see the following on the graph: many unsuppressed responses to tones while the subjects are not yet aware of their movement genesis, followed by a drop in the number of unsuppressed responses to tones during a certain period of time during which the subjects are conscious of their intentions and are stopping any movements, and finally a brief increase again in unsuppressed responses to tones when the subjects do not have the time to process the tone and prevent an action – they have passed the action's \"point of no return\". That is exactly what the researchers found (see the graph on the right, below).\nThe graph shows the times at which unsuppressed responses to tones occurred when the volunteer moved. He showed many unsuppressed responses to tones (dubbed \"tone events\" on the graph) on average up until 1.8 seconds before movement onset, but a significant decrease in tone events immediately after that time. Presumably this is because the subject usually became aware of his intention to move at about −1.8 seconds, which is then labelled point T. Since most actions are vetoed if a tone occurs after point T, there are very few tone events represented during that range. Finally, there is a sudden increase in the number of tone events at 0.1 seconds, meaning this subject has passed point P. Matsuhashi and Hallet were thus able to establish an average time T (−1.8 seconds) without subjective report. This, they compared to ERP measurements of movement, which had detected movement beginning at about −2.8 seconds on average for this participant. Since T — like Libet's original W — was often found after movement genesis had already begun, the authors concluded that the generation of awareness occurred afterwards or in parallel to action, but most importantly, that it was probably not the cause of the movement.\n\nHaggard describes other studies at the neuronal levels as providing \"a reassuring confirmation of previous studies that recorded neural populations\" such as the one just described. Note that these results were gathered using finger movements, and may not necessarily generalize to other actions such as thinking, or even other motor actions in different situations. Indeed, the human act of planning has implications for free will and so this ability must also be explained by any theories of unconscious decision making. Philosopher Alfred Mele also doubts the conclusions of these studies. He explains that simply because a movement may have been initiated before our \"conscious self\" has become aware of it does not mean our consciousness does not still get to approve, modify, and perhaps cancel (called vetoing) the action.\n\nThe possibility that human \"free won't\" is also the prerogative of the subconscious is being explored.\n\nRecent research by Simone Kühn and Marcel Brass suggests that our consciousness may not be what causes some actions to be vetoed at the last moment. First of all, their experiment relies on the simple idea that we ought to know when we consciously cancel an action (i.e. we should have access to that information). Secondly, they suggest that access to this information means humans should find it \"easy\" to tell, just after completing an action, whether it was impulsive (there being no time to decide) and when there was time to deliberate (the participant decided to allow/not to veto the action). The study found evidence that subjects could not tell this important difference. This again leaves some conceptions of free will vulnerable to the introspection illusion. The researchers interpret their results to mean that the decision to \"veto\" an action is determined subconsciously, just as the initiation of the action may have been subconscious in the first place.\n\nThe experiment involved asking volunteers to respond to a go-signal by pressing an electronic \"go\" button as quickly as possible. In this experiment the go-signal was represented as a visual stimulus shown on a monitor (e.g. a green light as shown on the picture). The participants' reaction times (RT) were gathered at this stage, in what was described as the \"primary response trials\".\n\nThe primary response trials were then modified, in which 25% of the go-signals were subsequently followed by an additional signal – either a \"stop\" or \"decide\" signal. The additional signals occurred after a \"signal delay\" (SD), a random amount of time up to 2 seconds after the initial go-signal. They also occurred equally, each representing 12.5% of experimental cases. These additional signals were represented by the initial stimulus changing colour (e.g. to either a red or orange light). The other 75% of go-signals were not followed by an additional signal – and was therefore considered the \"default\" mode of the experiment. The participants' task of responding as quickly as possible to the initial signal (i.e. pressing the \"go\" button) remained.\n\nUpon seeing the initial go-signal, the participant would immediately intend to press the \"go\" button. The participant was instructed to cancel their immediate intention to press the \"go\" button if they saw a stop signal. The participant was instructed to select randomly (at their leisure) between either pressing the \"go\" button, or not pressing it, if they saw a decide signal. Those trials in which the decide signal was shown after the initial go-signal (\"decide trials\"), for example, required that the participants prevent themselves from acting impulsively on the initial go-signal and then decide what to do. Due to the varying delays, this was sometimes impossible (e.g. some decide signals simply appeared too \"late\" in the process of them both intending to and pressing the go button for them to be obeyed).\n\nThose trials in which the subject reacted to the go-signal impulsively without seeing a subsequent signal show a quick RT of about 600 ms. Those trials in which the decide signal was shown too late, and the participant had already enacted their impulse to press the go-button (i.e. had not decided to do so), also show a quick RT of about 600 ms. Those trials in which a stop signal was shown and the participant successfully responded to it, do not show a response time. Those trials in which a decide signal was shown, and the participant decided not to press the go-button, also do not show a response time. Those trials in which a decide signal was shown, and the participant had not already enacted their impulse to press the go-button, but (in which it was theorised that they) had had the opportunity to decide what to do, show a comparatively slow RT, in this case closer to 1400 ms.\n\nThe participant was asked at the end of those \"decide trials\" in which they had actually pressed the go-button whether they had acted impulsively (without enough time to register the decide signal before enacting their intent to press the go-button in response to the initial go-signal stimulus), or had acted based upon a conscious decision made after seeing the decide signal. Based upon the response time data however, it appears there was discrepancy between when the user thought they had had the opportunity to decide (and had therefore not acted on their impulses) – in this case deciding to press the go-button, and when they thought they had acted impulsively (based upon the initial go-signal) – where the decide signal came too late to be obeyed.\n\nKuhn and Brass wanted to test participant self-knowledge. The first step was that after every decide trial, participants were next asked whether they had actually had time to decide. Specifically, the volunteers were asked to label each decide trial as either failed-to-decide (the action was the result of acting impulsively on the initial go-signal) or successful decide (the result of a deliberated decision). See the diagram on the right for this decide trial split: failed-to-decide and successful decide; the next split in this diagram (participant correct or incorrect) will be explained at the end of this experiment. Note also that the researchers sorted the participants’ successful decide trials into \"decide go\" and \"decide nogo\", but were not concerned with the nogo trials since they did not yield any RT data (and are not featured anywhere in the diagram on the right). Note that successful stop trials did not yield RT data either.\nKuhn and Brass now knew what to expect: primary response trials, any failed stop trials, and the \"failed-to-decide\" trials were all instances where the participant obviously acted impulsively – they would show the same quick RT. In contrast, the \"successful \"decide\"\" trials (where the decision was a \"go\" and the subject moved) should show a slower RT. Presumably, if deciding whether to veto is a conscious process, volunteers should have no trouble distinguishing impulsivity from instances of true deliberate continuation of a movement. Again, this is important since decide trials require that participants rely on self-knowledge. Note that stop trials cannot test self-knowledge because if the subject \"does\" act, it is obvious to them that they reacted impulsively.\n\nUnsurprisingly, the recorded RTs for the primary response trials, failed stop trials, and \"failed-to-decide\" trials all showed similar RTs: 600 ms seems to indicate an impulsive action made without time to truly deliberate. What the two researchers found next was not as easy to explain: while some \"successful decide\" trials did show the tell-tale slow RT of deliberation (averaging around 1400 ms), participants had also labelled many impulsive actions as \"successful decide\". This result is startling because participants should have had no trouble identifying which actions were the results of a conscious \"I will not veto\", and which actions were un-deliberated, impulsive reactions to the initial go-signal. As the authors explain:\n\nIn decide trials the participants, it seems, were not able to reliably identify whether they had really had time to decide – at least, not based on internal signals. The authors explain that this result is difficult to reconcile with the idea of a conscious veto, but simple to understand if the veto is considered an unconscious process. Thus it seems that the intention to move might not only arise from the subconscious, but it may only be inhibited if the subconscious says so. This conclusion could suggest that the phenomenon of \"consciousness\" is more of narration than direct arbitration (i.e. unconscious processing causes all thoughts, and these thoughts are again processed subconsciously).\n\nAfter the above experiments, the authors concluded that subjects sometimes could not distinguish between \"producing an action without stopping and stopping an action before voluntarily resuming\", or in other words, they could not distinguish between actions that are immediate and impulsive as opposed to delayed by deliberation. To be clear, one assumption of the authors is that all the early (600 ms) actions are unconscious, and all the later actions are conscious. These conclusions and assumptions have yet to be debated within the scientific literature or even replicated (it is a very early study).\n\nThe results of the trial in which the so-called \"successful decide\" data (with its respective longer time measured) was observed may have possible implications for our understanding of the role of consciousness as the modulator of a given action or response — and these possible implications cannot merely be omitted or ignored without valid reasons, specially when the authors of the experiment suggest that the late decide trials were actually deliberated.\n\nIt is worth noting that Libet consistently referred to a veto of an action that was initiated endogenously. That is, a veto that occurs in the absence of external cues, instead relying on only internal cues (if any at all). This veto may be a different type of veto than the one explored by Kühn and Brass using their decide signal.\n\nDaniel Dennett also argues that no clear conclusion about volition can be derived from Benjamin Libet's experiments supposedly demonstrating the non-existence of conscious volition. According to Dennett, ambiguities in the timings of the different events involved. Libet tells when the readiness potential occurs objectively, using electrodes, but relies on the subject reporting the position of the hand of a clock to determine when the conscious decision was made. As Dennett points out, this is only a report of where it \"seems\" to the subject that various things come together, not of the objective time at which they actually occur.\nSuppose Libet knows that your readiness potential peaked at millisecond 6,810 of the experimental trial, and the clock dot was straight down (which is what you reported you saw) at millisecond 7,005. How many milliseconds should he have to add to this number to get the time you were conscious of it? The light gets from your clock face to your eyeball almost instantaneously, but the path of the signals from retina through lateral geniculate nucleus to striate cortex takes 5 to 10 milliseconds — a paltry fraction of the 300 milliseconds offset, but how much longer does it take them to get to \"you\". (Or are you located in the striate cortex?) The visual signals have to be processed before they arrive at wherever they need to arrive for you to make a conscious decision of simultaneity. Libet's method presupposes, in short, that we can locate the \"intersection\" of two trajectories:\nso that these events occur side-by-side as it were in place where their simultaneity can be noted.\n\nIn early 2016, PNAS published a paper by researchers in Berlin, Germany, \"The point of no return in vetoing self-initiated movements\", in which the authors set out to investigate whether human subjects had the ability to veto an action (in this study, a movement of the foot) after the detection of its Bereitschaftspotential (BP). The Bereitschaftspotential, which was discovered by Kornhuber & Deecke in 1965, is an instance of unconscious electrical activity within the motor cortex, quantified by the use of EEG, that occurs moments before a motion is performed by a person: it is considered a signal that the brain is \"getting ready\" to perform the motion. The study found evidence that these actions can be vetoed even after the BP is detected (i. e. after it can be seen that the brain has started preparing for the action). The researchers maintain this is evidence for the existence of at least some degree of free will in humans: previously, it had been argued that, given the unconscious nature of the BP and its usefulness in predicting a person's movement, these are movements that are initiated by the brain without the involvement of the conscious will of the person. The study showed that subjects were able to \"override\" these signals and stop short of performing the movement that was being anticipated by the BP. Furthermore, researchers identified what was termed a \"point of no return\": once the BP is detected for a movement, the person could refrain from performing the movement only if they attempted to cancel it 200 milliseconds or longer before the onset of the movement. After this point, the person was unable to avoid performing the movement. Previously, Kornhuber & Deecke underlined that absence of conscious will during the early Bereitschaftspotential (termed BP1) is not a proof of the non-existence of free will, as also unconscious agendas may be free and non-deterministic. According to their suggestion, man has relative freedom, i.e. freedom in degrees, that can be in- or decreased through deliberate choices that involve both conscious and unconscious (panencephalic) processes.\n\nDespite criticisms, experimenters are still trying to gather data that may support the case that conscious \"will\" can be predicted from brain activity. fMRI machine learning of brain activity (multivariate pattern analysis) has been used to predict the user choice of a button (left/right) up to 7 seconds before their reported will of having done so. Brain regions successfully trained for prediction included the frontopolar cortex (anterior medial prefrontal cortex) and precuneus/posterior cingulate cortex (medial parietal cortex). In order to ensure report timing of conscious \"will\" to act, they showed the participant a series of frames with single letters (500ms apart), and upon pressing the chosen button (left or right) they were required to indicate which letter they had seen at the moment of decision. This study reported a statistically significant 60% accuracy rate, which may be limited by experimental setup; machine learning data limitations (time spent in fMRI) and instrument precision.\n\nAnother version of the fMRI multivariate pattern analysis experiment was conducted using an abstract decision problem, in an attempt to rule out the possibility of the prediction capabilities being product of capturing a built-up motor urge. Each frame contained a central letter like before, but also a central number, and a surrounding 4 possible \"answers numbers\". The participant first chose in their mind whether they wished to perform an addition or difference (subtraction) operation (and noted the central letter on the screen at the time of this decision). The participant then performed the mathematical operation based on the central numbers shown in the next two frames. In the following frame the participant then chose the \"answer number\" corresponding to the result of the operation. They were further presented with a frame which allowed them to indicate the central letter appearing on the screen at the time of their original decision. This version of the experiment discovered a brain prediction capacity of up to 5 seconds before the conscious will to act.\n\nMultivariate pattern analysis using EEG has suggested that an evidence based perceptual decision model may be applicable to free will decisions. It was found that decisions could be predicted by neural activity immediately after stimulus perception. Furthermore, when the participant was unable to determine the nature of the stimulus the recent decision history predicted the neural activity (decision). The starting point of evidence accumulation was in effect shifted towards a previous choice (suggesting a priming bias). Another study has found that subliminally priming a participant for a particular decision outcome (showing a cue for 13ms) could be used to influence free decision outcomes. Likewise, it has been found that decision history alone can be used to predict future decisions. The prediction capacities of the Soon et al. (2008) experiment were successfully replicated using a linear SVM model based on participant decision history alone (without any brain activity data). Despite this, a recent study has sought to confirm the applicability of a perceptual decision model to free will decisions. When shown a masked and therefore invisible stimulus, participants were asked to either guess between a category or make a free decision for a particular category. Multivariate pattern analysis using fMRI could be trained on \"free decision\" data to successfully predict \"guess decisions\", and trained on \"guess data\" in order to predict \"free decisions\" (in the precuneus and cuneus region).\n\nContemporary voluntary decision prediction tasks have been criticised based on the possibility the neuronal signatures for pre-conscious decisions could actually correspond to lower conscious processing rather than unconscious processing. People may be aware of their decisions before making their report yet need to wait several seconds to be certain. Such a model does not however explain what is left unconscious if everything can be conscious at some level (and the purpose of defining separate systems). Yet limitations remain in free will prediction research to date. In particular, the prediction of considered judgements from brain activity involving thought processes beginning minutes rather than seconds before a conscious will to act, including the rejection of a conflicting desire. Such are generally seen to be the product of sequences of evidence accumulating judgements.\n\nIt has been suggested that sense authorship is an illusion. Unconscious causes of thought and action might facilitate thought and action, while the agent experiences the thoughts and actions as being dependent on conscious will. We may over-assign agency because of the evolutionary advantage that once came with always suspecting there might be an agent doing something (e.g. predator). The idea behind retrospective construction is that, while part of the \"yes, I did it\" feeling of agency seems to occur during action, there also seems to be processing performed after the fact – after the action is performed – to establish the full feeling of agency.\n\nUnconscious agency processing can even alter, in the moment, how we perceive the timing of sensations or actions. Kühn and Brass apply retrospective construction to explain the two peaks in \"successful decide\" RT's. They suggest that the late decide trials were actually deliberated, but that the impulsive early decide trials that should have been labelled \"failed to decide\" were mistaken during unconscious agency processing. They say that people \"persist in believing that they have access to their own cognitive processes\" when in fact we do a great deal of automatic unconscious processing before conscious perception occurs.\n\nIt should be noted that criticism to Wegner's claims regarding the significance of introspection illusion for the notion of free will has been published.\n\nSome research suggests that TMS can be used to manipulate the perception of authorship of a specific choice. Experiments showed that neurostimulation could affect which hands people move, even though the experience of free will was intact. An early TMS study revealed that activation of one side of the neocortex could be used to bias the selection of one's opposite side hand in a forced-choice decision task. Ammon and Gandevia found that it was possible to influence which hand people move by stimulating frontal regions that are involved in movement planning using transcranial magnetic stimulation in the left or right hemisphere of the brain.\n\nRight-handed people would normally choose to move their right hand 60% of the time, but when the right hemisphere was stimulated they would instead choose their left hand 80% of the time (recall that the right hemisphere of the brain is responsible for the left side of the body, and the left hemisphere for the right). Despite the external influence on their decision-making, the subjects continued to report that they believed their choice of hand had been made freely. In a follow-up experiment, Alvaro Pascual-Leone and colleagues found similar results, but also noted that the transcranial magnetic stimulation must occur within 200 milliseconds, consistent with the time-course derived from the Libet experiments.\n\nIn late 2015, a team of researchers from the UK and the US published a paper demonstrating similar findings. The researchers concluded that \"motor responses and the choice of hand can be modulated using tDCS\". However, a different attempt by Sohn \"et al.\" failed to replicate such results; later, Jeffrey Gray wrote in his book \"Consciousness: Creeping up on the Hard Problem\" that tests looking for the influence of electromagnetic fields on brain function have been universally negative in their result.\n\nVarious studies indicate that the perceived intention to move (have moved) can be manipulated. Studies have focused on the pre-supplementary motor area (pre-SMA) of the brain, in which readiness potential indicating the beginning of a movement genesis has been recorded by EEG. In one study, directly stimulating the pre-SMA caused volunteers to report a feeling of intention, and sufficient stimulation of that same area caused physical movement. In a similar study, it was found that people with no visual awareness of their body can have their limbs be made to move without having any awareness of this movement, by stimulating premotor brain regions. When their parietal cortices were stimulated, they reported an urge (intention) to move a specific limb (that they wanted to do so). Furthermore, stronger stimulation of the parietal cortex resulted in the illusion of having moved without having done so.\n\nThis suggests that awareness of an intention to move may literally be the \"sensation\" of the body's early movement, but certainly not the cause. Other studies have at least suggested that \"The greater activation of the SMA, SACC, and parietal areas during and after execution of internally generated actions suggests that an important feature of internal decisions is specific neural processing taking place during and after the corresponding action. Therefore, awareness of intention timing seems to be fully established only after execution of the corresponding action, in agreement with the time course of neural activity observed here.\"\n\nAnother experiment involved an electronic ouija board where the device's movements were manipulated by the experimenter, while the participant was led to believe they were entirely self-conducted. The experimenter stopped the device on occasions and asked the participant how much they themselves felt like they wanted to stop. The participant also listened to words in headphones; and it was found that if experimenter stopped next to an object that came through the headphones they were more likely to say they wanted to stop there. If the participant perceived having the thought at the time of the action, then it was assigned as intentional. It was concluded that a strong illusion of perception of causality requires: priority (we assume the thought must precede the action), consistency (the thought is about the action), and exclusivity (no other apparent causes or alternative hypotheses).\n\nLau et al. set up an experiment where subjects would look at an analogue-style clock, and a red dot would move around the screen. Subjects were told to click the mouse button whenever they felt the intention to do so. One group was given a transcranial magnetic stimulation (TMS) pulse, and the other was given a sham TMS. Subjects in the intention condition were told to move the cursor to where it was when they felt the inclination to press the button. In the movement condition, subjects moved their cursor to where it was when they physically pressed the button. Results showed the TMS was able to shift the perceived intention forward by 16 ms, and shifted back the 14 ms for the movement condition. Perceived intention could be manipulated up to 200 ms after the execution of the spontaneous action, indicating that the perception of intention occurred after the executive motor movements. Often it is thought that if free will were to exist, it would require intention to be the causal source of behavior. These results show that intention may not be the causal source of all behavior.\n\nThe idea that intention co-occurs with (rather than causes) movement is reminiscent of \"forward models of motor control\" (or FMMC, which have been used to try to explain inner speech). FMMCs describe parallel circuits: movement is processed in parallel with other predictions of movement; if the movement matches the prediction – the feeling of agency occurs. FMMCs have been applied in other related experiments. Metcalfe and her colleagues used an FMMC to explain how volunteers determine whether they are in control of a computer game task. On the other hand, they acknowledge other factors too. The authors attribute feelings of agency to desirability of the results (see self serving biases) and top-down processing (reasoning and inferences about the situation).\n\nIn this case, it is by the application of the forward model that one might imagine how other consciousness processes could be the result of efferent, predictive processing. If the conscious self is the efferent copy of actions and vetoes being performed, then the consciousness is a sort of narrator of what is already occurring in the body, and an incomplete narrator at that. Haggard, summarizing data taken from recent neuron recordings, says \"these data give the impression that conscious intention is just a subjective corollary of an action being about to occur\". Parallel processing helps explain how we might experience a sort of contra-causal free will even if it were determined.\n\nHow the brain constructs consciousness is still a mystery, and cracking it open would have a significant bearing on the question of free will. Numerous different models have been proposed, for example, the Multiple Drafts Model which argues that there is no central Cartesian theater where conscious experience would be represented, but rather that consciousness is located all across the brain. This model would explain the delay between the decision and conscious realization, as experiencing everything as a continuous 'filmstrip' comes behind the actual conscious decision. In contrast, there exist models of Cartesian materialism that have gained recognition by neuroscience, implying that there might be special brain areas that store the contents of consciousness; this does not, however, rule out the possibility of a conscious will. Other models such as epiphenomenalism argue that conscious will is an illusion, and that consciousness is a by-product of physical states of the world. Work in this sector is still highly speculative, and researchers favor no single model of consciousness. (See also: Philosophy of mind.)\n\nVarious brain disorders implicate the role of unconscious brain processes in decision making tasks. Auditory hallucinations produced by Schizophrenia seem to suggest a divergence of will and behaviour. The left brain of people whose hemispheres have been disconnected has been observed to invent explanations for body movement initiated by the opposing (right) hemisphere, perhaps based on the assumption that their actions are consciously willed. Likewise, people with 'alien hand syndrome' are known to conduct complex motor movements against their will.\n\nA neural model for voluntary action proposed by Haggard comprises two major circuits. The first involving early preparatory signals (basal ganglia substantia nigra and striatum), prior intention and deliberation (medial prefrontal cortex), motor preparation/readiness potential (preSMA and SMA), and motor execution (primary motor cortex, spinal cord and muscles). The second involving the parietal-pre-motor circuit for object-guided actions, for example grasping (premotor cortex, primary motor cortex, primary somatosensory cortex, parietal cortex, and back to the premotor cortex). He proposed that voluntary action involves external environment input ('when decision'), motivations/reasons for actions (early 'whether decision'), task and action selection ('what decision'), a final predictive check (late 'whether decision') and action execution.\n\nAnother neural model for voluntary action also involves what, when, and whether (WWW) based decisions.\nThe 'what' component of decisions is considered a function of the anterior cingulate cortex, which is involved in conflict monitoring. The timing ('when') of the decisions are considered a function of the preSMA and SMA, which is involved in motor preparation.\nFinally, the 'whether' component is considered a function of the dorsal medial prefrontal cortex.\n\nMartin Seligman and others criticize the classical approach in science which views animals and humans as \"driven by the past\", and suggest instead that people and animals draw on experience to evaluate prospects they face, and act accordingly. The claim is made that this purposive action includes evaluation of possibilities that have never occurred before, and is experimentally verifiable.\n\nSeligman and others argue that free will and the role of subjectivity in consciousness can be better understood by taking such a \"prospective\" stance on cognition, and that \"accumulating evidence in a wide range of research suggests [this] shift in framework\".\n\n\n"}
{"id": "39614604", "url": "https://en.wikipedia.org/wiki?curid=39614604", "title": "Obeya", "text": "Obeya\n\nObeya or Oobeya (from Japanese 大部屋 \"large room\" or \"war room\") refers to a form of project management used in Asian companies (including Toyota) and is a component of lean manufacturing and in particular the Toyota Production System. Analogies have been drawn between an obeya and the bridge of a ship, a war room and even a brain.\n\nDuring the product and process development, all individuals involved in managerial planning meet in a \"great room\" to speed communication and decision-making. This is intended to reduce \"departmental thinking\" and improve on methods like email and social networking. The \"Obeya\" can be understood as a team spirit improvement tool at an administrative level.\n\nConceptually akin to traditional “war rooms,” an \"Obeya\" will contain visually engaging charts and graphs depicting such information as program timing, milestones and progress-to-date and countermeasures to existing technical or scheduling issues.\n\nAt Toyota, vehicle development is possible in significantly less than 20 months; by comparison, the average for other car makers is 36 months.\n\n"}
{"id": "3126358", "url": "https://en.wikipedia.org/wiki?curid=3126358", "title": "Operad theory", "text": "Operad theory\n\nOperad theory is a field of mathematics concerned with prototypical algebras that model properties such as commutativity or anticommutativity as well as various amounts of associativity. Operads generalize the various associativity properties already observed in algebras and coalgebras such as Lie algebras or Poisson algebras by modeling computational trees within the algebra. Algebras are to operads as group representations are to groups. An operad can be seen as a set of operations, each one having a fixed finite number of inputs (arguments) and one output, which can be composed one with others. They form a category-theoretic analog of universal algebra.\n\nOperads originate in algebraic topology from the study of iterated loop spaces by J. Michael Boardman and Rainer M. Vogt, and J. Peter May. The word \"operad\" was created by May as a portmanteau of \"operations\" and \"monad\" (and also because his mother was an opera singer). Interest in operads was considerably renewed in the early 90s when, based on early insights of Maxim Kontsevich, Victor Ginzburg and Mikhail Kapranov discovered that some duality phenomena in rational homotopy theory could be explained using Koszul duality of operads. Operads have since found many applications, such as in deformation quantization of Poisson manifolds, the Deligne conjecture, or graph homology in the work of Maxim Kontsevich and Thomas Willwacher.\n\nA non-symmetric operad (sometimes called an operad without permutations, or a non-formula_1 or plain operad) consists of the following:\n\nsatisfying the following coherence axioms:\n\n(the number of arguments corresponds to the arities of the operations).\n\nAlternatively, a plain operad is a multicategory with one object.\n\nA symmetric operad (often just called operad) is a non-symmetric operad formula_11 as above, together with a right action of the symmetric group formula_12 on formula_13, satisfying the above associative and identity axioms, as well as\n\n(where by abuse of notation, formula_16 on the right hand side of the first equivariance relation is the element\nof formula_17 that acts on the set formula_18 by breaking it into formula_3 blocks, the first of size formula_20, the second of size formula_21, through the formula_3th block of size formula_23, and then permutes these formula_3 blocks by formula_16).\n\nThe permutation actions in this definition are vital to most applications, including the original application to loop spaces.\n\nA morphism of operads formula_26 consists of a sequence\nwhich:\n\nWe have so far considered only operads in the category of sets. It is actually possible to define operads in any symmetric monoidal category (or, for non-symmetric operads, any monoidal category).\n\nA common example would be given by the category of topological space, with the monoidal product given by the Cartesian product. In this case, a topological operad is given by a sequence of \"spaces\" (instead of sets) formula_33. The structure maps of the operad (the composition and the actions of the symmetric groups) must then be assumed to be continuous. The result is called a \"topological operad\". Similarly, in the definition of a morphism, it would be necessary to assume that the maps involved are continuous.\n\nOther common settings to define operads include, for example, module over a ring, chain complexes, groupoids (or even the category of categories itself), coalgebras, etc.\n\n\"Associativity\" means that \"composition\" of operations is associative\n(the function formula_34 is associative), analogous to the axiom in category theory that formula_35; it does \"not\" mean that the operations \"themselves\" are associative as operations.\nCompare with the associative operad, below.\n\nAssociativity in operad theory means that one can write expressions involving operations without ambiguity from the omitted compositions, just as associativity for operations allows one to write products without ambiguity from the omitted parentheses.\n\nFor instance, suppose that formula_29 is a binary operation, which is written as formula_37 or formula_38. Note that formula_29 may or may not be associative.\n\nThen what is commonly written formula_40 is unambiguously written operadically as formula_41 . This sends formula_42 to formula_43 (apply formula_29 on the first two, and the identity on the third), and then the formula_29 on the left \"multiplies\" formula_46 by formula_47.\nThis is clearer when depicted as a tree:\n\nwhich yields a 3-ary operation:\n\nHowever, the expression formula_48 is \"a priori\" ambiguous:\nit could mean formula_49, if the inner compositions are performed first, or it could mean formula_50,\nif the outer compositions are performed first (operations are read from right to left).\nWriting formula_51, this is formula_52 versus formula_53. That is, the tree is missing \"vertical parentheses\":\n\nIf the top two rows of operations are composed first (puts an upward parenthesis at the formula_54 line; does the inner composition first), the following results:\n\nwhich then evaluates unambiguously to yield a 4-ary operation.\nAs an annotated expression:\n\nIf the bottom two rows of operations are composed first (puts a downward parenthesis at the formula_56 line; does the outer composition first), following results:\n\nwhich then evaluates unambiguously to yield a 4-ary operation:\n\nThe operad axiom of associativity is that \"these yield the same result,\" and thus that the expression formula_48 is unambiguous.\n\nThe identity axiom (for a binary operation) can be visualized in a tree as:\n\nmeaning that the three operations obtained are equal: pre- or post- composing with the identity makes no difference. Note that, as for categories, formula_58 is a corollary of the identity axiom.\n\nA little discs operad or, little balls operad or, more specifically, the little \"n\"-discs operad is a topological operad defined in terms of configurations of disjoint \"n\"-dimensional discs inside a unit \"n\"-disc centered in the origin of R. The operadic composition for little 2-discs is illustrated in the figure.\n\nOriginally the little \"n\"-cubes operad or the little intervals operad (initially called little \"n\"-cubes PROPs) was defined by Michael Boardman and Rainer Vogt in a similar way, in terms of configurations of disjoint axis-aligned \"n\"-dimensional hypercubes (n-dimensional intervals) inside the unit hypercube. Later it was generalized by May to little convex bodies operad, and \"little discs\" is a case of \"folklore\" derived from the \"little convex bodies\".\n\nAnother class of examples of operads are those capturing the structures of algebraic structures, such as associative algebras, commutative algebras and Lie algebras. Each of these can be exhibited as a finitely presented operad, in each of these three generated by binary operations.\n\nThus, the associative operad is generated by a binary operation formula_59, subject to the condition that\n\nThis condition \"does\" correspond to associativity of the binary operation formula_59; writing formula_62 multiplicatively, the above condition is formula_63. This associativity of the \"operation\" should not be confused with associativity of \"composition\"; see the axiom of associativity, above.\n\nThis operad is terminal in the category of non-symmetric operads, as it has exactly one \"n\"-ary operation for each \"n,\" corresponding to the unambiguous product of \"n\" terms: formula_64. For this reason, it is sometimes written as 1 by category theorists (by analogy with the one-point set, which is terminal in the category of sets).\n\nThe terminal symmetric operad is the operad whose algebras are commutative monoids, which also has one \"n\"-ary operation for each \"n\", with each formula_65 acting trivially; this triviality corresponds to commutativity, and whose \"n\"-ary operation is the unambiguous product of \"n\"-terms, where order does not matter:\nfor any permutation formula_67.\n\nThere is an operad for which each formula_13 is given by the symmetric group formula_65. The composite formula_70 permutes its inputs in blocks according to formula_71, and within blocks according to the appropriate formula_72. Similarly, there is a non-formula_1 operad for which each formula_13 is given by the Artin braid group formula_75. Moreover, this non-formula_1 operad has the structure of a braided operad, which generalizes the notion of an operad from symmetric to braid groups.\n\nIn linear algebra, one can consider vector spaces to be algebras over the operad formula_77 (the infinite direct sum, so only finitely many terms are non-zero; this corresponds to only taking finite sums), which parametrizes linear combinations: the vector formula_78 for instance corresponds to the linear combination\n\nSimilarly, one can consider affine combinations, conical combinations, and convex combinations to correspond to the sub-operads where the terms sum to 1, the terms are all non-negative, or both, respectively. Graphically, these are the infinite affine hyperplane, the infinite hyper-octant, and the infinite simplex. This formalizes what is meant by formula_80 being or the standard simplex being model spaces, and such observations as that every bounded convex polytope is the image of a simplex. Here suboperads correspond to more restricted operations and thus more general theories.\n\nThis point of view formalizes the notion that linear combinations are the most general sort of operation on a vector space – saying that a vector space is an algebra over the operad of linear combinations is precisely the statement that \"all possible\" algebraic operations in a vector space are linear combinations. The basic operations of vector addition and scalar multiplication are a generating set for the operad of all linear combinations, while the linear combinations operad canonically encodes all possible operations on a vector space.\n\n\n"}
{"id": "5529328", "url": "https://en.wikipedia.org/wiki?curid=5529328", "title": "Process consultant", "text": "Process consultant\n\nPart of the field called Human Systems Intervention, process consultation is a philosophy of helping, a general theory and methodology of intervening (Schein, Process Consultations, 1992 revisited). \n\nA process consultant is a highly qualified professional that has insights into and understands the psychosocial dynamics of working with various client systems such as whole organizations, groups, and individuals. \n\nGiven the complex nature of intervening, a process consultant's expertise includes the following (and many other) aspects:\na) works concomitantly with groups and individuals (managers/directors) towards a larger change process such as strategic visioning, strategic planning, etc.\nb) based on the context, selects from a variety of methods, tools and change theories a 'facilitative intervention' that will most benefit the client system;\nc) stays aware of covert organizational processes, group dynamics, and interpersonal issues;\n\nIn organization development, a process consultant is a specialized type of consultant who acts as a facilitator to help groups deal with issues involving the process in a meeting, rather than with the actual tasks themselves.\nA process consultant may be used at any time during the Stages of Group Development. Occasionally, a process consultant is used when a group is either in its formative stage, or normative stage. However, more often than not, they participate when the group is in conflict.\n\nOften a group finds itself in conflict over facts, goals, methods or values. It is the role of the process consultant to help the group reach consensus over the type of conflict it faces.\n\nOnce the type of conflict is identified, the process consultant then helps the group work through the steps required to break the impasse.\n\nIt is important to note that the process consultant's role is not to solve the problem, but to help the group solve its own problem. The reason for this is because it is the group, not the consultant who will have to live with the consequences of its decision.\n\nOccasionally, due to the nature of conflict, the process consultant may need to guide the group toward conflict management rather than conflict resolution.\n\nInitially a process consultant will not lead or participate in a group meeting, but rather will act as an observer. During this time, they observe the group dynamics to determine what interpersonal relationships may contribute to the group's issues.\n\nAt some point, s/he will begin to actively participate in the meeting, by asking clarifying questions or paraphrasing. Eventually, s/he will make his/her observations known by giving the group feedback.\n\nTo enter this field, a background in psychology and small group learning is helpful. Experience in reading body language and possessing Analytical skills is also useful. However, receiving some training in experiential education will probably be the most beneficial.\n\n"}
{"id": "15772735", "url": "https://en.wikipedia.org/wiki?curid=15772735", "title": "Project Reason", "text": "Project Reason\n\nProject Reason is a U.S. 501(c)(3) foundation whose main aims have been variously described as the promotion of scientific knowledge and secular values in society, and the encouragement of critical thinking and wise public policy through a variety of interrelated projects.\n\nIn 2007, Sam Harris co-founded, alongside his wife, the (501(c)3) non-profit foundation called Project Reason. Harris is also chief executive of this organization. Its website describes itself in the following terms:\n\nProject Reason is a 501(c)(3) nonprofit foundation devoted to spreading scientific knowledge and secular values in society. The project will draw on the talents of prominent and creative thinkers from a wide range of disciplines—science, law, literature, entertainment, information technology, etc.—to encourage critical thinking and wise public policy. It will convene conferences, produce films, sponsor scientific research and opinion polls, award grants to other non-profit organizations, and offer material support to religious dissidents and public intellectuals—with the purpose of eroding the influence of dogmatism, superstition and bigotry in the world.\n\nThe organization maintains several projects, as well as an internet forum, that attempt to debunk religion using empirical and rational critiques.\n\n\"The Scripture Project\", undertaken under the mantle of Project Reason, stated the following as its goal:\n\nSteve Wells, creator of the Skeptics Annotated Bible, Qur'an, and Book of Mormon has generously donated the full contents of his website to Project Reason. Using this as a foundation, we intend to make the Scripture Project the best source for scriptural criticism on the Internet.\n\n\"Vatican Justice\", another project undertaken by Project Reason, has the following as its mission statement: \n\nI would like to announce that Project Reason has joined Hitchens and Dawkins (both of whom sit on our advisory board) in an effort to end the \"diplomatic immunity\" which the Vatican claims protects the Pope from any responsibility. We would greatly appreciate your support in this cause. All donations are tax-deductible in the United States.\n\nOne of the most active sub-projects in Project Reason is the \"Secular Islam\" sub-project, with several initiatives falling under it.\n\nOne project within Secular Islam is titled \"Modern Developments in Koranic Criticism: Christmas in the Koran\" with the following description:\n\nThis project is being run by foundation advisor Ibn Warraq and takes as its focus the pioneering work in Syriac and Arabic linguistics of Christof Luxenberg. Luxenberg is a native Arab speaker and linguist, living in the West and writing under a pseudonym. Luxenberg's work has given new impetus to the discipline of Koranic studies, largely by revealing that many acknowledged obscurities in the Koran can be clarified by treating certain passages as a poor translations of Syriac into Arabic.\n\nThe Secular Islam sub-project supports other projects, such as \"The Institute for Research on Early Islamic History and the Koran\", having funded their conferences in 2008, 2012, and 2014. It pledges collaboration with the Ayaan Hirsi Ali Foundation, supports the Ex-Muslims of North America, and the Quilliam Foundation.\n\nThe organization listed three trustees: Sam Harris, Annaka Harris, and Jai Lakshman.\n\nAdvisory board:\n\n\n\n"}
{"id": "2041594", "url": "https://en.wikipedia.org/wiki?curid=2041594", "title": "Qadariyah", "text": "Qadariyah\n\nQadariyah (or Qadariya) is an originally derogatory term designating early Islamic theologians who asserted that humans possess free will, whose exercise makes them responsible for their actions, justifying divine punishment and absolving God of responsibility for evil in the world. Some of their doctrines were later adopted by the Mu'tazilis and rejected by the Ash'aris.\n\nQadariya was one of the first philosophical schools in Islam. The earliest document associated with the movement is the \"Risala\" by Hasan al-Basri, which was composed between 75/694 and 80/699, though debates about free will in Islam probably predate this text.\n\nAccording to Sunni sources, the Qadariyah were censured by Muhammad himself by being compared to Zoroastrians, who likewise deny predestination. It is reported in Sunan Abu Dawood: \nNarrated Abdullah ibn Umar: The Prophet said, \"The Qadariyyah are the Magians of this community. If they are ill, do not pay a sick visit to them, and if they die, do not attend their funerals.\"\n\n\n\n"}
{"id": "25217", "url": "https://en.wikipedia.org/wiki?curid=25217", "title": "Qi", "text": "Qi\n\nIn traditional Chinese culture, qi or ch'i () is believed to be a vital force forming part of any living entity. \"Qi\" translates as \"air\" and figuratively as \"material energy\", \"life force\", or \"energy flow\". \"Qi\" is the central underlying principle in Chinese traditional medicine and in Chinese martial arts. The practice of cultivating and balancing \"qi\" is called \"qigong\".\n\nBelievers of \"qi\" describe it as a vital energy whose flow must be balanced for health. Qi is a pseudoscientific, unverified concept, which has never been directly observed, and is unrelated to the concept of energy used in science (vital energy is itself an abandoned scientific notion).\n\nThe cultural keyword \"qì\" is analyzable in terms of Chinese and Sino-Xenic pronunciations. Possible etymologies include the logographs 氣, 气, and 気 with various meanings ranging from \"vapor\" to \"anger\", and the English loanword \"qi\" or \"ch'i\".\n\nThe logograph 氣 is read with two Chinese pronunciations, the usual \"qì\" 氣 \"air; vital energy\" and the rare archaic \"xì\" 氣 \"to present food\" (later disambiguated with 餼).\n\nPronunciations of 氣 in modern varieties of Chinese with standardized IPA equivalents include: Standard Chinese \"qì\" /t͡ɕʰi⁵¹/, Wu Chinese \"qi\" /t͡ɕʰi³⁴/, Southern Min \"khì\" /kʰi²¹/, Eastern Min \"ké\" /kʰɛi²¹³/, Standard Cantonese \"hei\" /hei̯³³/, and Hakka Chinese \"hi\" /hi⁵⁵/.\n\nPronunciations of 氣 in Sino-Xenic borrowings include: Japanese \"ki\", Korean \"gi\", and Vietnamese \"khi\".\n\nReconstructions of the Middle Chinese pronunciation of 氣 standardized to IPA transcription include: /kʰe̯i/ (Bernard Karlgren), /kʰĭəi/ (Wang Li), /kʰiəi/ (Li Rong), /kʰɨj/ (Edwin Pulleyblank), and /kʰɨi/ (Zhengzhang Shangfang).\n\nReconstructions of the Old Chinese pronunciation of 氣 standardized to IPA transcription include: /*kʰɯds/ (Zhengzhang Shangfang) and /*C.qʰəp-s/ (William H. Baxter and Laurent Sagart).\n\nThe etymology of \"qì\" interconnects with Kharia \"kʰis\" \"anger\", Sora \"kissa\" \"move with great effort\", Khmer \"kʰɛs\" \"strive after; endeavor\", and Gyalrongic \"kʰɐs\" \"anger\".\n\nIn the East Asian languages, \"qì\" has three logographs:\nIn addition, \"qì\" 炁 is an uncommon character especially used in writing Daoist talismans. Historically, the word \"qì\" was generally written as 气 until the Han dynasty (206 BCE–220 CE), when it was replaced by the 氣 graph clarified with \"mǐ\" 米 \"rice\" indicating \"steam (rising from rice as it cooks.)\"\n\nThis primary logograph 气, the earliest written character for \"qì,\" consisted of three wavy horizontal lines seen in Shang dynasty (c. 1600–1046 BCE) oracle bone script, Zhou dynasty (1046–256 BCE) bronzeware script and large seal script, and Qin dynasty (221–206 BCE) small seal script. These oracle, bronze, and seal scripts logographs 气 were used in ancient times as a phonetic loan character to write \"qǐ\" 乞 \"plead for; beg; ask\" which did not have an early character.\n\nThe vast majority of Chinese characters are classified as radical-phonetic characters. Such characters combine a semantically suggestive \"radical characters\" with a phonetic element approximating ancient pronunciation. For example, the widely known word \"dào\" 道 \"the Dao; the way\" graphically combines the \"walk\" radical 辶 with a \"shǒu\" 首 \"head\" phonetic. Although the modern \"dào\" and \"shǒu\" pronunciations are dissimilar, the Old Chinese \"*lˤuʔ-s\" 道 and \"*l̥uʔ-s\" 首 were alike. The regular script character \"qì\" 氣 is unusual because \"qì\" 气 is both the \"air radical\" and the phonetic, with \"mǐ\" 米 \"rice\" semantically indicating \"steam; vapor\".\n\nThis \"qì\" 气 \"air/gas radical\" was only used in a few native Chinese characters like \"yīnyūn\" 氤氲 \"thick mist/smoke\", but was also used to create new scientific characters for gaseous chemical elements. Some examples are based on pronunciations in European languages: \"fú\" 氟 (with a \"fú\" 弗 phonetic) \"fluorine\" and \"nǎi\" 氖 (with a \"nǎi\" 乃 phonetic) \"neon\". Others are based on semantics: \"qīng\" 氫 (with a \"jīng\" 巠 phonetic, abbreviating \"qīng\" 輕 \"light-weight\") \"hydrogen (the lightest element)\" and \"lǜ\" 氯 (with a \"lù\" 彔 phonetic, abbreviating \"lǜ\" 綠 \"green\") \"(greenish-yellow) chlorine\".\n\n\"Qì\" 氣 is the phonetic element in a few characters such as \"kài\" 愾 \"hate\" with the \"heart-mind radical\" 忄or 心, \"xì\" 熂 \"set fire to weeds\" with the \"fire radical\" 火, and \"xì\" 餼 \"to present food\" with the \"food radical\" 食.\n\nThe first Chinese dictionary of characters, the \"Shuowen Jiezi\"(121 CE) notes that the primary \"qì\" 气 is a pictographic character depicting 雲气 \"cloudy vapors\", and that the full 氣 combines 米 \"rice\" with the phonetic \"qi\" 气, meaning 饋客芻米 \"present provisions to guests\" (later disambiguated as \"xì\" 餼).\n\nQi is a polysemous word. The unabridged Chinese-Chinese character dictionary \"Hanyu Da Cidian\" defines it as \"present food or provisions\" for the \"xì\" pronunciation but also lists 23 meanings for the \"qì\" pronunciation. The modern \"ABC Chinese-English Comprehensive Dictionary,\" which enters \"xì\" 餼 \"grain; animal feed; make a present of food\", and a \"qì\" 氣 entry with seven translation equivalents for the noun, two for bound morphemes, and three equivalents for the verb. n. ① air; gas ② smell ③ spirit; vigor; morale ④ vital/material energy (in Ch[inese] metaphysics) ⑤ tone; atmosphere; attitude ⑥ anger ⑦ breath; respiration b.f. ① weather 天氣 \"tiānqì\" ② [linguistics] aspiration 送氣 \"sòngqì\" v. ① anger ② get angry ③ bully; insult.\n\nQi was an early Chinese loanword in English. It was romanized as \"k'i\" in Church Romanization in the early-19th century, as \"ch'i\" in Wade–Giles in the mid-19th century (sometimes misspelled \"chi\" omitting the apostrophe), and as qi in Pinyin in the mid-20th century. The \"Oxford English Dictionary\" entry for \"qi\" gives the pronunciation as IPA (tʃi), the etymology from Chinese \"qì\" \"air; breath\", and a definition of \"The physical life-force postulated by certain Chinese philosophers; the material principle.\" It also gives eight usage examples, with the first recorded example of \"k'í\" in 1850 (\"The Chinese Repository\"), of \"ch'i\" in 1917 (\"The Encyclopaedia Sinica\"), and \"qi\" in 1971 (Felix Mann's \"Acupuncture\")\n\nReferences to concepts analogous to qi are found in many Asian belief systems. Philosophical conceptions of qi from the earliest records of Chinese philosophy (5th century BCE) correspond to Western notions of humours, the ancient Hindu yogic concept of \"prana\", and the traditional Jewish concept of \"nefesh\". An early form of qi comes from the writings of the Chinese philosopher Mencius (4th century BCE).\n\nThe ancient Chinese described qi as \"life force\". They believed it permeated everything and linked their surroundings together. Qi was also linked to the flow of energy around and through the body, forming a cohesive functioning unit. By understanding the rhythm and flow of qi, they believed they could guide exercises and treatments to provide stability and longevity.\n\nAlthough the concept has been important within many Chinese philosophies, over the centuries the descriptions of qi have varied and have sometimes been in conflict. Until China came into contact with Western scientific and philosophical ideas, the Chinese had not categorized all things in terms of matter and energy. Qi and \"li\" (理: \"pattern\") were 'fundamental' categories similar to matter and energy.\n\nFairly early on, some Chinese thinkers began to believe that there were different fractions of qi—the coarsest and heaviest fractions formed solids, lighter fractions formed liquids, and the most ethereal fractions were the \"lifebreath\" that animated living beings. \"Yuán qì\" is a notion of innate or prenatal qi which is distinguished from acquired qi that a person may develop over their lifetime.\n\nThe earliest texts that speak of qi give some indications of how the concept developed. In the Analects of Confucius qi could mean \"breath\". Combining it with the Chinese word for blood (making 血氣, \"xue\"–\"qi\", blood and breath), the concept could be used to account for motivational characteristics:\n\nThe philosopher Mozi used the word qi to refer to noxious vapors that would in eventually arise from a corpse were it not buried at a sufficient depth. He reported that early civilized humans learned how to live in houses to protect their qi from the moisture that troubled them when they lived in caves. He also associated maintaining one's qi with providing oneself with adequate nutrition. In regard to another kind of qi, he recorded how some people performed a kind of prognostication by observing qi (clouds) in the sky.\n\nMencius described a kind of qi that might be characterized as an individual's vital energies. This qi was necessary to activity and it could be controlled by a well-integrated willpower. When properly nurtured, this qi was said to be capable of extending beyond the human body to reach throughout the universe. It could also be augmented by means of careful exercise of one's moral capacities. On the other hand, the qi of an individual could be degraded by adverse external forces that succeed in operating on that individual.\n\nLiving things were not the only things believed to have qi. Zhuangzi indicated that wind is the \"qi\" of the Earth. Moreover, cosmic yin and yang \"are the greatest of qi. He described qi as \"issuing forth\" and creating profound effects. He also said \"Human beings are born [because of] the accumulation of \"qi\". When it accumulates there is life. When it dissipates there is death... There is one \"qi\" that connects and pervades everything in the world.\"\n\nAnother passage traces life to intercourse between Heaven and Earth: \"The highest Yin is the most restrained. The highest Yang is the most exuberant. The restrained comes forth from Heaven. The exuberant issues forth from Earth. The two intertwine and penetrate forming a harmony, and [as a result] things are born.\"\n\nThe Guanzi essay \"Neiye\" (Inward Training) is the oldest received writing on the subject of the cultivation of vapor \"[qi]\" and meditation techniques. The essay was probably composed at the Jixia Academy in Qi in the late fourth century B.C.\n\nXun Zi, another Confucian scholar of the Jixia Academy, followed in later years. At 9:69/127, Xun Zi says, \"Fire and water have \"qi\" but do not have life. Grasses and trees have life but do not have perceptivity. Fowl and beasts have perceptivity but do not have \"yi\" (sense of right and wrong, duty, justice). Men have \"qi\", life, perceptivity, and \"yi\".\" Chinese people at such an early time had no concept of radiant energy, but they were aware that one can be heated by a campfire from a distance away from the fire. They accounted for this phenomenon by claiming \"\"qi\" radiated from fire. At 18:62/122, he also uses \"qi\"\" to refer to the vital forces of the body that decline with advanced age.\n\nAmong the animals, the gibbon and the crane were considered experts at inhaling the \"qi\". The Confucian scholar Dong Zhongshu (ca. 150 BC) wrote in Luxuriant Dew of the Spring and Autumn Annals: \"The gibbon resembles a macaque, but he is larger, and his color is black. His forearms being long, he lives eight hundred years, because he is expert in controlling his breathing.\" (\"猿似猴。大而黑。長前臂。所以壽八百。好引氣也。\")\n\nLater, the syncretic text assembled under the direction of Liu An, the Huai Nan Zi, or \"Masters of Huainan\", has a passage that presages most of what is given greater detail by the Neo-Confucians:\n\nThe \"Huangdi Neijing\" \"(\"\"The Yellow Emperor's Classic of Medicine\", circa 2nd century BCE) is historically credited with first establishing the pathways, called meridians, through which qi circulates in the human body.\n\nIn traditional Chinese medicine, symptoms of various illnesses are believed to be either the product of disrupted, blocked, and unbalanced \"qi\" movement through meridians or deficiencies and imbalances of qi in the \"Zang Fu\" organs. Traditional Chinese medicine often seeks to relieve these imbalances by adjusting the circulation of \"qi\" using a variety of techniques including herbology, food therapy, physical training regimens (qigong, t'ai chi ch'uan, and other martial arts training), moxibustion, \"tui na\", or acupuncture.\n\nThe nomenclature of Qi in the human body is different depending on its sources, roles, and locations. For sources there is a difference between so-called \"Primordial Qi\" (acquired at birth from one's parents) and Qi acquired throughout one's life. Or again Chinese medicine differentiates between Qi acquired from the air we breathe (so called \"Clean Air\") and Qi acquired from food and drinks (so-called \"Grain Qi\"). Looking at roles Qi is divided into \"Defensive Qi\" and \"Nutritive Qi\". Defensive Qi's role is to defend the body against invasions while Nutritive Qi's role is to provide sustenance for the body. Lastly, looking at locations, Qi is also named after the Zang-Fu organ or the Meridian in which it resides: \"Liver Qi\", \"Spleen Qi\", etc. \nA qi field (\"chu-chong\") refers to the cultivation of an energy field by a group, typically for healing or other benevolent purposes. A qi field is believed to be produced by visualization and affirmation. They are an important component of Wisdom Healing'Qigong (\"Zhineng Qigong\"), founded by Grandmaster Ming Pang.\n\nConcepts similar to qi can be found in many cultures.\n\n\"Prana\" in Hinduism and Indian culture, \"chi\" in the Igbo religion, \"pneuma\" in ancient Greece, \"mana\" in Hawaiian culture, \"lüng\" in Tibetan Buddhism, \"manitou\" in the culture of the indigenous peoples of the Americas, \"ruah\" in Jewish culture. In Western philosophy, notions of \"energeia\", \"élan vital\", or vitalism are purported to be similar.\n\nSome elements of the \"qi\" concept can be found in the term 'energy' when used in the context of various esoteric forms of spirituality and alternative medicine.\n\nElements of the concept of Qi can also be found in Eastern and Western popular culture:\n\n\nQi is a non-scientific, unverifiable concept. A 1997 consensus statement on acupuncture by the United States National Institutes of Health noted that concepts such as qi \"are difficult to reconcile with contemporary biomedical information\".\n\nThe 2014 Skeptoid podcast episode titled \"Your Body's Alleged Energy Fields\" related a Reiki practitioner's report of what was happening as she passed her hands over a subject's body:\n\nEvaluating these claims, author and scientific skeptic Brian Dunning reported:\n\nThe traditional Chinese art of geomancy, the placement and arrangement of space called feng shui, is based on calculating the balance of qi, interactions between the five elements, yin and yang, and other factors. The retention or dissipation of qi is believed to affect the health, wealth, energy level, luck, and many other aspects of the occupants. Attributes of each item in a space affect the flow of qi by slowing it down, redirecting it or accelerating it. This is said to influence the energy level of the occupants.\n\nOne use for a \"luopan\" is to detect the flow of qi. The quality of qi may rise and fall over time. Feng shui with a compass might be considered a form of divination that assesses the quality of the local environment.\n\nQìgōng (气功 or 氣功) involves coordinated breathing, movement, and awareness. It is traditionally viewed as a practice to cultivate and balance qi. With roots in traditional Chinese medicine, philosophy and martial arts, \"qigong\" is now practiced worldwide for exercise, healing, meditation, and training for martial arts. Typically a \"qigong\" practice involves rhythmic breathing, slow and stylized movement, a mindful state, and visualization of guiding qi.\n\nQi is a didactic concept in many Chinese, Korean and Japanese martial arts. Martial qigong is a feature of both internal and external training systems in China and other East Asian cultures. The most notable of the qi-focused \"internal\" force (jin) martial arts are Baguazhang, Xing Yi Quan, T'ai Chi Ch'uan, Southern Praying Mantis, Snake Kung Fu, Southern Dragon Kung Fu, Aikido, Kendo, Hapkido, Aikijujutsu, Luohan Quan, and Liu He Ba Fa.\n\nDemonstrations of \"qi\" or \"ki\" are popular in some martial arts and may include the unraisable body, the unbendable arm, and other feats of power. Some of these feats can alternatively be explained using biomechanics and physics.\n\nAcupuncture is a part of traditional Chinese medicine that involves insertion of needles into superficial structures of the body (skin, subcutaneous tissue, muscles) at acupuncture points to balance the flow of qi. This is often accompanied by moxibustion, a treatment that involves burning mugwort on or near the skin at an acupuncture point.\n\n\n"}
{"id": "3009396", "url": "https://en.wikipedia.org/wiki?curid=3009396", "title": "Rational basis review", "text": "Rational basis review\n\nIn U.S. constitutional law, rational basis review is the normal standard of review that courts apply when considering constitutional questions, including due process or equal protection questions under the Fifth Amendment or Fourteenth Amendment. Courts applying rational basis review seek to determine whether a law is \"rationally related\" to a \"legitimate\" government interest, whether real or hypothetical. The higher levels of scrutiny are intermediate scrutiny and strict scrutiny. Heightened scrutiny is applied where a suspect or quasi-suspect classification is involved, or a fundamental right is implicated.\n\nIn United States Supreme Court jurisprudence, the nature of the interest at issue determines the level of scrutiny applied by appellate courts. When courts engage in rational basis review, only the most egregious enactments, those not rationally related to a legitimate government interest, are overturned.\n\nRational basis review tests whether the government's actions are \"rationally related\" to a \"legitimate\" government interest. The Supreme Court has never set forth standards for determining what constitutes a legitimate government interest. Under rational basis review, it is \"entirely irrelevant\" what end the government is actually seeking and statutes can be based on \"rational speculation unsupported by evidence or empirical data.\" Rather, if the court can merely hypothesize a \"legitimate\" interest served by the challenged action, it will withstand rational basis review. Judges following the Supreme Court's instructions understand themselves to be \"obligated to seek out other conceivable reasons for validating\" challenged laws if the government is unable to justify its own policies.\n\nThe concept of rational basis review can be traced to an influential 1893 article, \"The Origin and Scope of American Constitutional Law,\" by Harvard law professor James Bradley Thayer. Thayer argued that statutes should be invalidated only if their unconstitutionality is \"so clear that it is not open to rational question.\" Justice Oliver Wendell Holmes, Jr., a student of Thayer's, articulated a version of what would become rational basis review in his canonical dissent in \"Lochner v. New York\", arguing that \"the word 'liberty,' in the 14th Amendment, is perverted when it is held to prevent the natural outcome of a dominant opinion, unless it can be said that a rational and fair man necessarily would admit that the statute proposed would infringe fundamental principles as they have been understood by the traditions of our people and our law.\"\n\nHowever, the court's extensive application of economic substantive due process during the years following Lochner meant that Holmes' proposed doctrine of judicial deference to state interest was not immediately adopted. It was not until \"Nebbia v. New York\" that the Court began to formally apply rational basis review, when it stated that \"a State is free to adopt whatever economic policy may reasonably be deemed to promote public welfare, and to enforce that policy by legislation adapted to its purpose.\" In \"United States v. Carolene Products Co.\" the Court in Footnote Four left open the possibility that laws that seem to be within \"a specific prohibition of the Constitution,\" which restrict the political process, or which burden \"discrete and insular minorities\" might receive more exacting review. Today, such laws receive strict scrutiny, whereas laws implicating unenumerated rights that the Supreme Court has not recognized as fundamental receive rational basis review.\n\nIn modern constitutional law, the rational basis test is applied to constitutional challenges of both federal law and state law (via the Fourteenth Amendment). This test also applies to both legislative and executive action whether those actions be of a substantive or procedural nature.\n\nThe rational basis test prohibits the government from imposing restrictions on liberty that are irrational or arbitrary, or drawing distinctions between persons in a manner that serves no constitutionally legitimate end. While a \"law enacted for broad and ambitious purposes often can be explained by reference to legitimate public policies which justify the incidental disadvantages they impose on certain persons\", it must nevertheless, at least, \"bear a rational relationship to a legitimate governmental purpose\".\n\nTo understand the concept of rational basis review, it is easier to understand what it is not. Rational basis review is \"not\" a genuine effort to determine the legislature's actual reasons for enacting a statute, nor to inquire into whether a statute does in fact further a legitimate end of government. A court applying rational basis review will virtually always uphold a challenged law unless every conceivable justification for it is a grossly illogical non sequitur. In 2008, Justice John Paul Stevens reaffirmed the lenient nature of rational basis review in a concurring opinion: \"[A]s I recall my esteemed former colleague, Thurgood Marshall, remarking on numerous occasions: 'The Constitution does not prohibit legislatures from enacting stupid laws.'\"\n\n"}
{"id": "1547077", "url": "https://en.wikipedia.org/wiki?curid=1547077", "title": "Shanti Sena", "text": "Shanti Sena\n\nThe Shanti Sena or \"Peace army\" was made up of Gandhi's non-violent followers in India.\n\nOther movements have developed, inspired by this one, sometimes also using the name used by Gandhi's group. These may include World Peace Brigade, Nonviolent Peaceforce, Swaraj Peeth, the organisation Peace Brigades International and participants in the Rainbow Gathering, and have served as a basis for the practice of Third Party Non-violent Intervention.\n\n\"Shanti Sena\" is a term first coined by Gandhi when he conceptualized a nonviolent volunteer peacekeeping program dedicated to minimizing communal violence within the Indian populace. The words \"Shanti\" and \"Sena\" both come from Sanskrit. Shanti means peace and Sena means army, or a drilled band of men. The word \"Sena\" has been criticized for its connection to militarism, but for Gandhi, it had strong metaphorical and spiritual qualities connected to its use in the Hindu vedas.\n\nIn the aftermath of the Gandhian era, Shanti Sena has appeared in various incarnations. Two Gandhian followers developed separate groups based on their interpretations of it: Vinoba Bhave established a Shanti Sena that prioritized Gandhi's spiritual approach towards the program, while JP established a program that focused more on the political motivations of the program. The Shanti Sena program also became institutionalized into India's Gandhigram Rural University, where it was incorporated into the university's constitution. Currently Shanti Sena is also very active in Sri Lanka as a part of the organization Sarvodaya.\n\n"}
{"id": "562695", "url": "https://en.wikipedia.org/wiki?curid=562695", "title": "Simulink", "text": "Simulink\n\nSimulink, developed by MathWorks, is a graphical programming environment for modeling, simulating and analyzing multidomain dynamical systems. Its primary interface is a graphical block diagramming tool and a customizable set of block libraries. It offers tight integration with the rest of the MATLAB environment and can either drive MATLAB or be scripted from it. Simulink is widely used in automatic control and digital signal processing for multidomain simulation and Model-Based Design.\n\nMathWorks and other third-party hardware and software products can be used with Simulink. For example, Stateflow extends Simulink with a design environment for developing state machines and flow charts.\n\nMathWorks claims that, coupled with another of their products, Simulink can automatically generate C source code for real-time implementation of systems. As the efficiency and flexibility of the code improves, this is becoming more widely adopted for production systems, in addition to being a tool for embedded system design work because of its flexibility and capacity for quick iteration. Embedded Coder creates code efficient enough for use in embedded systems.\n\nSimulink Real-Time (formerly known as xPC Target), together with x86-based real-time systems, is an environment for simulating and testing Simulink and Stateflow models in real-time on the physical system. Another MathWorks product also supports specific embedded targets. When used with other generic products, Simulink and Stateflow can automatically generate synthesizable VHDL and Verilog.\nSimulink Verification and Validation enables systematic verification and validation of models through modeling style checking, requirements traceability and model coverage analysis. Simulink Design Verifier uses formal methods to identify design errors like integer overflow, division by zero and dead logic, and generates test case scenarios for model checking within the Simulink environment.\n\nSimEvents is used to add a library of graphical building blocks for modeling queuing systems to the Simulink environment, and to add an event-based simulation engine to the time-based simulation engine in Simulink.\n\nTherefore in Simulink any type of simulation can be done and the model can be simulated at any point in this environment.\n\nDifferent type of blocks can be accessed using the Simulink library browser. And therefore the benefit could be taken out from this environment efficiently.\n\n"}
{"id": "19079770", "url": "https://en.wikipedia.org/wiki?curid=19079770", "title": "Societal impact of nanotechnology", "text": "Societal impact of nanotechnology\n\nThe societal impact of nanotechnology are the potential benefits and challenges that the introduction of novel nanotechnological devices and materials may hold for society and human interaction. The term is sometimes expanded to also include nanotechnology's health and environmental impact, but this article will only consider the social and political impact of nanotechnology.\n\nAs nanotechnology is an emerging field and most of its applications are still speculative, there is much debate about what positive and negative effects that nanotechnology might have.\n\nBeyond the toxicity risks to human health and the environment which are associated with first-generation nanomaterials, nanotechnology has broader societal implications and poses broader social challenges. Social scientists have suggested that nanotechnology's social issues should be understood and assessed not simply as \"downstream\" risks or impacts. Rather, the challenges should be factored into \"upstream\" research and decision making in order to ensure technology development that meets social objectives\n\nMany social scientists and organizations in civil society suggest that technology assessment and governance should also involve public participation\n\nSome observers suggest that nanotechnology will build incrementally, as did the 18-19th century industrial revolution, until it gathers pace to drive a nanotechnological revolution that will radically reshape our economies, our labor markets, international trade, international relations, social structures, civil liberties, our relationship with the natural world and even what we understand to be human. Others suggest that it may be more accurate to describe change driven by nanotechnology as a “technological tsunami”. Just like a tsunami, analysts warn that rapid nanotechnology-driven change will necessarily have profound disruptive impacts. As the APEC Center for Technology Foresight observes:\nIf nanotechnology is going to revolutionize manufacturing, health care, energy supply, communications and probably defense, then it will transform labour and the workplace, the medical system, the transportation and power infrastructures and the military. None of these latter will be changed without significant social disruption.\nThose concerned with the negative impact of nanotechnology suggest that it will simply exacerbate problems stemming from existing socio-economic inequity and unequal distributions of power, creating greater inequities between rich and poor through an inevitable nano-divide (the gap between those who control the new nanotechnologies and those whose products, services or labour are displaced by them). Analysts suggest the possibility that nanotechnology has the potential to destabilize international relations through a nano arms race and the increased potential for bioweaponry; thus, providing the tools for ubiquitous surveillance with significant implications for civil liberties. Also, many critics believe it might break down the barriers between life and non-life through nanobiotechnology, redefining even what it means to be human.\n\nNanoethicists posit that such a transformative technology could exacerbate the divisions of rich and poor – the so-called “nano divide.” However nanotechnology makes the production of technology, e.g. computers, cellular phones, health technology etcetera, cheaper and therefore accessible to the poor.\n\nIn fact, many of the most enthusiastic proponents of nanotechnology, such as transhumanists, see the nascent science as a mechanism to changing human nature itself – going beyond curing disease and enhancing human characteristics. Discussions on nanoethics have been hosted by the federal government, especially in the context of “converging technologies” – a catch-phrase used to refer to nano, biotech, information technology, and cognitive science.\n\nPossible military applications of nanotechnology have been suggested in the fields of soldier enhancement () and chemical weapons amongst others. However, more socially disruptive weapon systems are to be expected from molecular manufacturing, a potential future form of nanotechnology that would make it possible to build complex structures at atomic precision. Molecular manufacturing requires significant advances in nanotechnology, but its supporters posit that once achieved it could produce highly advanced products at low costs and in large quantities in nanofactories weighing a kilogram or more. If nanofactories gain the ability to produce other nanofactories production may only be limited by relatively abundant factors such as input materials, energy and software.\n\nMolecular manufacturing might be used to cheaply produce, among many other products, highly advanced, durable weapons. Being equipped with compact computers and motors these might be increasingly autonomous and have a large range of capabilities.\n\nAccording to Chris Phoenix and Mike Treder from the Center for Responsible Nanotechnology as well as Anders Sandberg from the Future of Humanity Institute the military uses of molecular manufacturing are the applications of nanotechnology that pose the most significant global catastrophic risk. Several nanotechnology researchers state that the bulk of risk from nanotechnology comes from the potential to lead to war, arms races and destructive global government. Several reasons have been suggested why the availability of nanotech weaponry may with significant likelihood lead to unstable arms races (compared to e.g. nuclear arms races): (1) A large number of players may be tempted to enter the race since the threshold for doing so is low; (2) the ability to make weapons with molecular manufacturing might be cheap and easy to hide; (3) therefore lack of insight into the other parties' capabilities can tempt players to arm out of caution or to launch preemptive strikes; (4) molecular manufacturing may reduce dependency on international trade, a potential peace-promoting factor; (5) wars of aggression may pose a smaller economic threat to the aggressor since manufacturing is cheap and humans may not be needed on the battlefield.\n\nSelf-regulation by all state and non-state actors has been called hard to achieve, so measures to mitigate war-related risks have mainly been proposed in the area of international cooperation. International infrastructure may be expanded giving more sovereignty to the international level. This could help coordinate efforts for arms control. Some have put forth that international institutions dedicated specifically to nanotechnology (perhaps analogously to the International Atomic Energy Agency IAEA) or general arms control may also be designed. One may also jointly make differential technological progress on defensive technologies. The Center for Responsible Nanotechnology also suggest some technical restrictions. Improved transparency regarding technological capabilities may be another important facilitator for arms-control.\n\nOn the structural level, critics of nanotechnology point to a new world of ownership and corporate control opened up by nanotechnology. The claim is that, just as biotechnology's ability to manipulate genes went hand in hand with the patenting of life, so too nanotechnology's ability to manipulate molecules has led to the patenting of matter. The last few years has seen a gold rush to claim patents at the nanoscale. Academics have warned that the resultant patent thicket is harming progress in the technology and have argued in the top journal \"Nature\" that there should be a moratorium on patents on \"building block\" nanotechnologies. Over 800 nano-related patents were granted in 2003, and the numbers are increasing year to year. Corporations are already taking out broad-ranging patents on nanoscale discoveries and inventions. For example, two corporations, NEC and IBM, hold the basic patents on carbon nanotubes, one of the current cornerstones of nanotechnology. Carbon nanotubes have a wide range of uses, and look set to become crucial to several industries from electronics and computers, to strengthened materials to drug delivery and diagnostics. Carbon nanotubes are poised to become a major traded commodity with the potential to replace major conventional raw materials. However, as their use expands, anyone seeking to (legally) manufacture or sell carbon nanotubes, no matter what the application, must first buy a license from NEC or IBM. \n\nThe United States' essential facilities doctrine may be of importance as well as other anti-trust laws.\n\nNanotechnologies may provide new solutions for the millions of people in developing countries who lack access to basic services, such as safe water, reliable energy, health care, and education. The United Nations has set Millennium Development Goals for meeting these needs. The 2004 UN Task Force on Science, Technology and Innovation noted that some of the advantages of nanotechnology include production using little labor, land, or maintenance, high productivity, low cost, and modest requirements for materials and energy.\n\nMany developing countries, for example Costa Rica, Chile, Bangladesh, Thailand, and Malaysia, are investing considerable resources in research and development of nanotechnologies. Emerging economies such as Brazil, China, India and South Africa are spending millions of US dollars annually on R&D, and are rapidly increasing their scientific output as demonstrated by their increasing numbers of publications in peer-reviewed scientific publications.\n\nPotential opportunities of nanotechnologies to help address critical international development priorities include improved water purification systems, energy systems, medicine and pharmaceuticals, food production and nutrition, and information and communications technologies. Nanotechnologies are already incorporated in products that are on the market. Other nanotechnologies are still in the research phase, while others are concepts that are years or decades away from development.\n\nApplying nanotechnologies in developing countries raises similar questions about the environmental, health, and societal risks described in the previous section. Additional challenges have been raised regarding the linkages between nanotechnology and development.\n\nProtection of the environment, human health and worker safety in developing countries often suffers from a combination of factors that can include but are not limited to lack of robust environmental, human health, and worker safety regulations; poorly or unenforced regulation which is linked to a lack of physical (e.g., equipment) and human capacity (i.e., properly trained regulatory staff). Often, these nations require assistance, particularly financial assistance, to develop the scientific and institutional capacity to adequately assess and manage risks, including the necessary infrastructure such as laboratories and technology for detection.\n\nVery little is known about the risks and broader impacts of nanotechnology. At a time of great uncertainty over the impacts of nanotechnology it will be challenging for governments, companies, civil society organizations, and the general public in developing countries, as in developed countries, to make decisions about the governance of nanotechnology.\n\nCompanies, and to a lesser extent governments and universities, are receiving patents on nanotechnology. The rapid increase in patenting of nanotechnology is illustrated by the fact that in the US, there were 500 nanotechnology patent applications in 1998 and 1,300 in 2000. Some patents are very broadly defined, which has raised concern among some groups that the rush to patent could slow innovation and drive up costs of products, thus reducing the potential for innovations that could benefit low income populations in developing countries.\n\nThere is a clear link between commodities and poverty. Many least developed countries are dependent on a few commodities for employment, government revenue, and export earnings. Many applications of nanotechnology are being developed that could impact global demand for specific commodities. For instance, certain nanoscale materials could enhance the strength and durability of rubber, which might eventually lead to a decrease in demand for natural rubber. Other nanotechnology applications may result in increases in demand for certain commodities. For example, demand for titanium may increase as a result of new uses for nanoscale titanium oxides, such as titanium dioxide nanotubes that can be used to produce and store hydrogen for use as fuel. Various organizations have called for international dialogue on mechanisms that will allow developing countries to anticipate and proactively adjust to these changes.\n\nIn 2003, Meridian Institute began the Global Dialogue on Nanotechnology and the Poor: Opportunities and Risks (GDNP) to raise awareness of the opportunities and risks of nanotechnology for developing countries, close the gaps within and between sectors of society to catalyze actions that address specific opportunities and risks of nanotechnology for developing countries, and identify ways that science and technology can play an appropriate role in the development process. The GDNP has released several publicly accessible papers on nanotechnology and development, including \"Nanotechnology and the Poor: Opportunities and Risks - Closing the Gaps Within and Between Sectors of Society\"; \"Nanotechnology, Water, and Development\"; and \"Overview and Comparison of Conventional and Nano-Based Water Treatment Technologies\".\n\nConcerns are frequently raised that the claimed benefits of nanotechnology will not be evenly distributed, and that any benefits (including technical and/or economic) associated with nanotechnology will only reach affluent nations. The majority of nanotechnology research and development - and patents for nanomaterials and products - is concentrated in developed countries (including the United States, Japan, Germany, Canada and France). In addition, most patents related to nanotechnology are concentrated amongst few multinational corporations, including IBM, Micron Technologies, Advanced Micro Devices and Intel. This has led to fears that it will be unlikely that developing countries will have access to the infrastructure, funding and human resources required to support nanotechnology research and development, and that this is likely to exacerbate such inequalities.\n\nProducers in developing countries could also be disadvantaged by the replacement of natural products (including rubber, cotton, coffee and tea) by developments in nanotechnology. These natural products are important export crops for developing countries, and many farmers' livelihoods depend on them. It has been argued that their substitution with industrial nano-products could negatively impact the economies of developing countries, that have traditionally relied on these export crops.\n\nIt is proposed that nanotechnology can only be effective in alleviating poverty and aid development \"when adapted to social, cultural and local institutional contexts, and chosen and designed with the active participation by citizens right from the commencement point\" (Invernizzi et al. 2008, p. 132).\n\nRay Kurzweil has speculated in \"The Singularity is Near\" that people who work in unskilled labor jobs for a livelihood may become the first human workers to be displaced by the constant use of nanotechnology in the workplace, noting that layoffs often affect the jobs based around the lowest technology level before attacking jobs with the highest technology level possible. It has been noted that every major economic era has stimulated a global revolution both in the kinds of jobs that are available to people and the kind of training they need to achieve these jobs, and there is concern that the world's educational systems have lagged behind in preparing students for the \"Nanotech Age\".\n\nIt has also been speculated that nanotechnology may give rise to nanofactories which may have superior capabilities to conventional factories due to their small carbon and physical footprint on the global and regional environment. The miniaturization and transformation of the multi-acre conventional factory into the nanofactory may not interfere with their ability to deliver a high quality product; the product may be of even greater quality due to the lack of human errors in the production stages. Nanofactory systems may use precise atomic precisioning and contribute to making superior quality products that the \"bulk chemistry\" method used in 20th century and early 21st currently cannot produce. These advances might shift the computerized workforce in an even more complex direction, requiring skills in genetics, nanotechnology, and robotics.\n"}
{"id": "1174964", "url": "https://en.wikipedia.org/wiki?curid=1174964", "title": "Technological evolution", "text": "Technological evolution\n\nTechnological evolution is a theory of radical transformation of society through technological development. This theory originated with Czech philosopher Radovan Richta.\n\n\"Mankind In Transition; A View of the Distant Past, the Present and the Far Future\", Masefield Books, 1993. Technology (which Richta defines as \"a material entity created by the application of mental and physical effort to nature in order to achieve some value\") evolves in three stages: tools, machine, automation. This evolution, he says, follows two trends:\n\nThe pretechnological period, in which all other animal species remain today aside from some avian and primate species was a non-rational period of the early prehistoric man.\n\nThe emergence of technology, made possible by the development of the rational faculty, paved the way for the first stage: the tool. A tool provides a mechanical advantage in accomplishing a physical task, such as an arrow, plow, or hammer that augments physical labor to more efficiently achieve his objective. Later animal-powered tools such as the plow and the horse, increased the productivity of food production about tenfold over the technology of the hunter-gatherers. Tools allow one to do things impossible to accomplish with one's body alone, such as seeing minute visual detail with a microscope, manipulating heavy objects with a pulley and cart, or carrying volumes of water in a bucket.\n\nThe second technological stage was the creation of the machine. A machine (a powered machine to be more precise) is a tool that substitutes the element of human physical effort, and requires only to control its function. Machines became widespread with the industrial revolution, though windmills, a type of machine, are much older.\n\nExamples of this include cars, trains, computers, and lights. Machines allow humans to\nTremendously exceed the limitations of their bodies. Putting a machine on the farm, a tractor, increased food productivity at least tenfold over the technology of the plow and the horse.\n\nThe third, and final stage of technological evolution is the automation. The automation is a machine that removes the element of human control with an automatic algorithm. Examples of machines that exhibit this characteristic are digital watches, automatic telephone switches, pacemakers, and computer programs.\n\nIt's crucial to understand that the three stages outline the introduction of the fundamental types of technology, and so all three continue to be widely used today. A spear, a plow, a pen, a knife, a glove, a chicken and an optical microscope are all examples of tools.\n\nThe process of technological evolution culminates with the ability to achieve all the material values technologically possible and desirable by mental effort.\n\nAn economic implication of the above idea is that intellectual labour will become increasingly more important relative to physical labour. Contracts and agreements around information will become increasingly more common at the marketplace. Expansion and creation of new kinds of institutes that works with information such as universities, book stores, patent-trading companies, etc. is considered an indication that a civilization is in technological evolution.\n\nThis highlights the importance underlining the debate over intellectual property in conjunction with decentralized distribution systems such as today's internet. Where the price of information distribution is going towards zero with ever more efficient tools to distribute information is being invented. Growing amounts of information being distributed to an increasingly larger customer base as times goes by. With growing disintermediation in said markets and growing concerns over the protection of intellectual property rights it is not clear what form markets for information will take with the evolution of the information age.\n\n"}
{"id": "3784565", "url": "https://en.wikipedia.org/wiki?curid=3784565", "title": "Ternary relation", "text": "Ternary relation\n\nIn mathematics, a ternary relation or triadic relation is a finitary relation in which the number of places in the relation is three. Ternary relations may also be referred to as 3-adic, 3-ary, 3-dimensional, or 3-place.\n\nJust as a binary relation is formally defined as a set of \"pairs\", i.e. a subset of the Cartesian product of some sets \"A\" and \"B\", so a ternary relation is a set of triples, forming a subset of the Cartesian product of three sets \"A\", \"B\" and \"C\".\n\nAn example of a ternary relation in elementary geometry can be given on triples of points, where a triple is in the relation if the three points are collinear. Another geometric example can be obtained by considering triples consisting of two points and a line, where a triple is in the ternary relation if the two points determine (are incident with) the line.\n\nA function in two variables, mapping two values from sets \"A\" and \"B\", respectively, to a value in \"C\" associates to every pair (\"a\",\"b\") in an element \"ƒ\"(\"a\", \"b\") in \"C\". Therefore, its graph consists of pairs of the form . Such pairs in which the first element is itself a pair are often identified with triples. This makes the graph of \"ƒ\" a ternary relation between \"A\", \"B\" and \"C\", consisting of all triples , satisfying , , and \n\nGiven any set \"A\" whose elements are arranged on a circle, one can define a ternary relation \"R\" on \"A\", i.e. a subset of \"A\" = , by stipulating that holds if and only if the elements \"a\", \"b\" and \"c\" are pairwise different and when going from \"a\" to \"c\" in a clockwise direction one passes through \"b\". For example, if \"A\" = { } represents the hours on a clock face, then holds and does not hold.\n\nThe ordinary congruence of arithmetics\nwhich holds for three integers \"a\", \"b\", and \"m\" if and only if \"m\" divides \"a\" − \"b\", formally may be considered as a ternary relation. However, usually, this instead is considered as a family of binary relations between the \"a\" and the \"b\", indexed by the modulus \"m\". For each fixed \"m\", indeed this binary relation has some natural properties, like being an equivalence relation; while the combined ternary relation in general is not studied as one relation.\n\nA \"typing relation\" formula_2 indicates that formula_3 is a term of type formula_4 in context formula_5, and is thus a ternary relation between contexts, terms and types.\n\nA ternary relation formula_6 can be defined in the category of binary relations using composition of relations \"AB\" and inclusion \"AB\" ⊆ \"C\". Within the calculus of relations each relation \"A\" has a converse relation \"A\" and a complement relation formula_7 Using these involutions, Augustus De Morgan and Ernst Schröder showed that formula_6is equivalent to formula_9 and also equivalent to formula_10 The mutual equivalences of these forms, constructed from the ternary are called the Schröder rules.\n\n"}
{"id": "24224534", "url": "https://en.wikipedia.org/wiki?curid=24224534", "title": "World Bank's Inspection Panel", "text": "World Bank's Inspection Panel\n\nThe Inspection Panel is an independent accountability mechanism of the World Bank. It was established in September 1993 by the World Bank Board of Directors, and started operations on August 1, 1994. The Panel provides a forum for people who believe that they may be adversely affected by Bank-financed operations to bring their concerns to the highest decision-making levels of the World Bank. The Panel determines whether the Bank is complying with its own policies and procedures, which are designed to ensure that Bank-financed operations provide social and environmental benefits and avoid harm to people and the environment. The Inspection Panel was the first body to promote accountability among international financial institutions through this community-led, or “bottom-up”, approach which is complementary to the “top-down” forms of accountability, such as evaluation initiated by the World Bank itself. Building on this example, other multilateral and regional financial institutions have established similar accountability mechanisms as part of broader efforts at sustainable and equitable development.\n\nThe Inspection Panel's mandate allows it to investigate projects funded by the IDA and IBRD, both part of the World Bank Group, and to determine whether they are complying with their policies and procedures in the design, appraisal, and implementation of a project. These policies and procedures are not limited to the Bank’s social and environmental safeguard policies, but include other operational policies, bank procedures, and operational directives, as well as other Bank procedural documents.\n\nThe Inspection Panel consists of three members who are appointed by the Board of Directors for non-renewable periods of five years. In addition, an Executive Secretariat was established to assist and support all Panel activities. The Panel is independent of Bank management and is provided with independent resources to discharge its functions.\n\nThe Panel process begins when it receives a request for inspection from a party of two or more Requesters, claiming that the Bank has violated its policies and procedures. Most requests concern some of the Bank’s safeguard policies, such as the policies on environmental assessment, involuntary resettlement, or indigenous people. \n\nOnce the Panel has received and registered a request for inspection, the eligibility phase of the Inspection Process commences. Beginning on the day of registration the World Bank’s management has 21 days to provide the Panel with evidence that it complied or intended to comply with the Bank’s relevant policies and procedures. After receiving management's response, the Panel has 21 business days to determine the eligibility of the request.\n\nOnce it has been determined that the eligibility criteria have been met, and after having reviewed the management response to the request, the Panel may, taking other factors it may have discovered during a field visit into consideration, make a recommendation to investigate. In some cases, the Panel has promoted problem solving between management and the requesters to help mediate less contentious cases and lead to an earlier resolution of community concerns or policy compliance problems.\n\nAn investigation is not automatic, and can only be authorized by the Board of Executive Directors. If the Board approves, the next step is the substantive phase of the inspection process when the Panel evaluates the merits of the request. In the investigation phase, the Panel is focused on fact finding and verification. It visits the borrowing country and meets with the requesters and other affected people, as well as with a broad array of people from whom it can learn in detail about the issues, concerns, the project's status, and potential harmful effects. The investigation phase may take a few months, or longer, in complex cases.\n\nOnce the investigation phase is complete, the Panel submits its Investigation Report to Bank management. The Board meets to consider both the Panel's Investigation Report and management’s recommendations, before deciding whether to approve the recommendations. The Board may ask the Panel to check whether management has made appropriate consultations about the remedial measures with the affected people.\n\nDuring the 1980s the Bank had begun developing and committing itself to operational policies and procedures, including policies on involuntary resettlement (1980), tribal peoples (1982), and environmental assessment (1988). In the late 1980s and early 1990s, however, widespread voices of concern and protest from civil society and project-affected communities questioned the social and environmental impacts of Bank-financed operations. A central element of this critique was that the Bank was not complying with its policy commitments which it had adopted to prevent these very types of adverse social and environmental impacts. Serious debates on these issues also took place within the Bank’s member governments and the Bank itself.\n\nIn June 1992, the international community gathered at the United Nations Conference on Environment and Development in Rio de Janeiro to chart a new cooperative approach to addressing interrelated issues of social development, economic development, and environmental protection. The blueprint for the creation of the Inspection Panel was developed in this larger context as a result of efforts from civil society, governments, and members of the Bank’s Board to establish a new and independent mechanism for greater accountability, participation, and transparency at the World Bank.\n\nThe Panel was officially created by two similar resolutions of the International Bank for Reconstruction and Development (IBRD) and the International Development Agency (IDA) signed by the Board of Executive Directors on September 1, 1993 (Resolution IBRD 93–10 and Resolution IDA 93–6). The Resolution specifies that the Panel has jurisdiction with respect to operations supported by the IBRD and the IDA. In 1996 and 1999, clarifications were added to the Resolution.\n\nThe four units which concur to the improvement of accountability and internal control at the World Bank are (in alphabetical order):\n\n\n\n"}
{"id": "25174608", "url": "https://en.wikipedia.org/wiki?curid=25174608", "title": "Yaakov Teitel", "text": "Yaakov Teitel\n\nYaakov Teitel (, born November 1972) is an American-born Israeli religious nationalist, convicted for killing two people in 2009. Teitel, who had immigrated to Israel in 2000, settling in a West Bank settlement, confessed to planning and committing various acts of terrorism and hate crimes against Palestinians, homosexuals, left-wingers, missionary Christians, and police officers across Israel. Teitel was sentenced to life imprisonment, which he is currently serving.\n\nYaakov Teitel was born in 1972 in Florida, United States, and moved to Norfolk, Virginia, when he was a teenager. His father, Mark, was a retired dentist who had served in the U.S. Navy during the Vietnam War, and his mother, Dianne, was a medical secretary - both of whom became Haredi Jews later in life. Commonly known as Jack, Teitel attended university and received a bachelor's degree in psychology, and in the mid-1990s, he began making regular visits to Israel. He was arrested in August 1997 on suspicion of murdering a Palestinian shepherd, but was released, and the case was eventually closed due to lack of evidence. Teitel subsequently returned to the United States, where he took a computer course with Microsoft, and began working as a computer technician.\n\nIn 1999, Teitel immigrated to Israel and moved to the settlement of Shvut Rachel, becoming an Israeli citizen under the Law of Return in December 2000. The following year, his parents and sister also emigrated to Israel, and settled in Beitar Illit, an Ultra-Orthodox city in the Gush Etzion settlement bloc. In 2002, Teitel met Rivka Pepperman, a British-born dance teacher originally from Broughton Park, Manchester, whose family had moved to Israel in 1981. They married the following year, and had four children. Teitel was summoned by Israel Defense Forces for preliminary checks, but was exempted from conscription due to his advanced age, medical history, and familial state.\n\nTeitel was considered an outcast in the settlement due to his limited proficiency in Hebrew and reclusive behavior. He hardly left his home, and did not regularly participate in services at the local synagogue.\n\nTeitel was arrested in Jerusalem's Har Nof neighborhood on October 7, 2009, while posting flyers praising the 2009 Tel Aviv gay centre shooting. He attempted to throw away a bag he was carrying, but it was retrieved, and was found to contain two handguns. Teitel was then turned over to Shin Bet for interrogation, while special police units raided his home and the homes of his brother-in-law, parents, and mother-in-law. The search of his property revealed a cache of guns, parts used in explosive devices, pellets used to maximize the impact of explosives, binoculars, and flyers containing anti-gay incitement. The explosives were all detonated safely by the police, and a search of his home two days later found a bomb buried in the backyard.\n\nIsraeli police revealed that Teitel, acting alone, confessed to a string of terrorist attacks and attempted attacks. Teitel confessed to having carried out a pipe-bomb attack against Ze'ev Sternhell, as well as the murders of a Palestinian taxi driver and a West Bank shepherd in 1997, a 2006 attempted bombing near the settlement of Eli, three bombings against police targets and a Christian monastery in 2007, and sending a booby-trapped package that injured a teenage boy to the home of a Messianic Jewish family in Ariel. In addition to Teitel's confession, his DNA was found to match DNA retrieved from Sternhell's home after the bombing attack. Teitel also confessed that he had planned to fly a remote-controlled toy plane containing explosives into the Tel Aviv gay pride parade, and that he planned a bombing against a Jerusalem gay bar. He also admitted that he planned to fire an improvised mortar against the Al-Aqsa Mosque, but abandoned the plan over fear of injuring Jewish worshippers on the Temple Mount.\n\nTeitel was also suspected of murdering two police officers in the Jordan Valley, and confessed to the gay centre shooting in Tel Aviv in 2009, though police stated that they were certain he was not involved in either. Following the discoveries, Teitel was held by Shin Bet in special custody for 48 hours, without being allowed to see an attorney or brought before a judge. The Petah Tikva District Court denied appeals from his attorney to meet with him, and subsequently extended both his remand and the special ordnance banning him from meeting his lawyer. All of Teitel's arraignments were held behind closed doors, and a gag order was imposed on the case.\n\nOn 10 November 2009, the Jerusalem District Prosecutor's Office declared that Teitel would be charged for the murder of the two Palestinians, incitement to racism, deploying explosive devices, and sabotage. Teitel's legal defense was provided by \"Honenu\", an organization which provides legal defense for Israelis accused of crimes against Arabs or the Israeli security forces. Honenu founder Shmuel Meidad suggested that Teitel may be mentally unstable, stating that, \"It's my impression that this man is sick, but he's not nearly as sick as the state.\" On 4 May 2010, Jerusalem's district psychiatrist ruled Teitel unfit to stand trial. The evaluation was criticized by Israeli-Arab Knesset members, with Ahmad Tibi stating, \"An Arab who shoots a Jew in Israel is a terrorist, but a Jew who shoots Arabs is insane.\" Following a new psychiatric evaluation, the district court overturned the previous ruling, and declared Teitel fit for trial, on 30 August.\n\nIn February 2012, the Jerusalem District Prosecutor's Office and Teitel's attorneys reached an agreement under which he would confess to two counts of murder. Teitel's attorneys confessed on his behalf, as Teitel himself announced that did not recognize the court's jurisdiction. After the indictment, the defense and prosecution would argue over whether Teitel was aware of his actions and is fit for trial, and the court would deliberate over whether Teitel is accountable for his actions. In January 2013, Teitel was convicted on two counts of murder and two counts of attempted murder, with the court rejecting the defense's arguments that he was legally insane. In April, he was sentenced to two life sentences and 30 years for the murder of two Palestinians, the attempted murder of two others, and other offenses, and ordered to pay mandatory compensation of NIS 650,000 to the families of the victims.\n"}
