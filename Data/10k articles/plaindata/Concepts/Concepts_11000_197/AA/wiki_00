{"id": "25384230", "url": "https://en.wikipedia.org/wiki?curid=25384230", "title": "Association Najdeh", "text": "Association Najdeh\n\nAssociation Najdeh (AN) (جمعية النجدة الاجتماعية in Arabic, Arabic najdeh=\"relief\" or \"aid\") is an NGO involved with development and educational projects in Palestinian refugee camps in Lebanon. It operates more exactly in and around the refugee camps. It defends the Palestinian refugee women who are often victims of discriminations and also participates in different campaigns, in coalitions of local and international organizations, for the right to work in Lebanon and the right of return to Palestine.\n\nThe association Najdeh was founded in 1976 by a group of independent Lebanese women and men to secure immediate income to Palestinian refugee women in particular after the movement of Palestinian family to the camp of Tel-Za'atar. An initial work of embroidery was organized and generated an income, then a day-care center and a workplace for the women was created. Finally, in 1978, the Najdeh association was registered to the Ministry of the Interior as the independent Lebanese social and non-governmental organization (NGO) with headquarters in Beirut. Between 2005 in 2006, the association Najdeh elaborated a Code of conduct, intended to strengthen the role of the civil society institutions. It opposes to the proposition in 2006, of an Islamic constitution in Burj el-Barajneh camp, by institutions with Islamic trend. It has been recently mobilized during the conflict in Nahr el-Bared camps of May, 2007 to bring relief and assistance to residents of the camp. The association Najdeh organized demonstrations in all the regions of the Lebanon, on April 30, 2009, for \"the right to work campaign of the Palestinian refugees in the Lebanon\", in particular with Palestinian-Lebanese coalition's commissions, after having published a report on the contribution of the Palestinian refugees to the Lebanese economy, in January, 2009, thanks to funding from aid agencies (Diakonia and Christian Aid).\n\nThe main purpose of the association Najdeh is to empower women in the Palestinian community because they are a most severely disadvantage component of the Palestinian refugee community. The improvement of the role of the women in the Palestinian community also concerns their economic, social and educational role.\n\nThe association Najdeh organs consists of: a General assembly, an Administrative Bureau elected by this Assembly, members of the Executive Board assigned by this Administrative Bureau.\nThe General assembly is 70% of women and 30% of men. The administrative board has progressed over the last few years from only female members to a fifty-fifty female/male division (6 members). The staff is primarily women and members of the Palestinian refugee community. Eighty percent of the beneficiaries are women and Palestinians, while the remaining 20% are men and individuals of other nationalities.\nThe association Najdeh operates on 26 centers in and around refugee camps. Its programs focus directly on the women and include: vocational training Program, Popular Education and Tutorial Program, Mother and Child Program, Social Affairs, Domestic Violence Program, microcredit Project, and Embroidery.\nIt also defends the rights of the Palestinians by activities of raising awareness on subjects such as the health of the reproduction, the rights of the woman and the rights of the child. The programs of the association would touch up to 10 000 refugees, in all palestinian refugee camps, including camps Burj el-Barajneh, Nahr el-Bared, Shatila, Beddawi, etc.\n\nThe association Najdeh has personal core values that it intend to promote: a community committed to the Palestinian nation, a belief in the importance of human rights, of social justice and of gender equality. As a member of the Humanitarian Accountability Partnership International(HAP International) since August 2009, she believes in the values of PAHs based on transparency and accountability.\n\nThe association Najdeh has several sources of funding. The donations are a big part, their origins are multiple: mainly funds from donor organizations and individuals from Europe, and from some partners present in United States and Canada. The other sources of income are: incomes from the production of embroidery, fees for students in professional training program and those for program of women and children.\n\n\n"}
{"id": "1757881", "url": "https://en.wikipedia.org/wiki?curid=1757881", "title": "Bite the cartridge", "text": "Bite the cartridge\n\nRefusing to \"bite the cartridge\" was a turn of phrase used by the British in India of native Indian soldiers (sepoys) who had mutinied in 1857.\n\nIt derives from the act of biting open a paper cartridge containing gunpowder and musket ball in order to load contemporary rifles, especially the new Pattern 1853 Enfield rifled musket. The phrase is thought by some to have later spawned the more familiar idiom \"bite the bullet\".\n\nOne of the alleged causes of the Indian Rebellion of 1857 were rumours that the grease on these cartridges designed to keep them dry was, variously, pork or beef fat (pork being abhorrent to the Muslims, cows being sacred to the Hindus), thus their refusal to bite them.\n"}
{"id": "1448833", "url": "https://en.wikipedia.org/wiki?curid=1448833", "title": "Cardinal utility", "text": "Cardinal utility\n\nIn economics, a cardinal utility function or scale is a utility index that preserves preference orderings uniquely up to positive affine transformations. Two utility indices are related by an affine transformation if for the value formula_1 of one index \"u\", occurring at any quantity formula_2 of the goods bundle being evaluated, the corresponding value formula_3 of the other index \"v\" satisfies a relationship of the form\n\nfor fixed constants \"a\" and \"b\". Thus the utility functions themselves are related by\n\nThe two indices differ only with respect to scale and origin. Thus if one is concave, so is the other, in which case there is often said to be diminishing marginal utility.\n\nThus the use of cardinal utility imposes the assumption that levels of absolute satisfaction exist, so that the magnitudes of increments to satisfaction can be compared across different situations.\n\nThe idea of cardinal utility is considered outdated except for specific contexts such as decision making under risk, utilitarian welfare evaluations, and discounted utilities for intertemporal evaluations where it is still applied. Elsewhere, such as in general consumer theory, ordinal utility with its weaker assumptions is preferred because results that are just as strong can be derived.\n\nThe first one to theorize about the marginal value of money was Daniel Bernoulli in 1738. He assumed that the value of an additional amount is inversely proportional to the pecuniary possessions which a person already owns. Since Bernoulli tacitly assumed that an interpersonal measure for the utility reaction of different persons can be discovered, he was then inadvertedly using an early conception of cardinality.\n\nBernoulli's imaginary logarithmic utility function and Gabriel Cramer's function were conceived at the time not for a theory of demand but to solve the St. Petersburg's game. Bernoulli assumed that \"a poor man generally obtains more utility than a rich man from an equal gain\" an approach that is more profound that the simple mathematical expectation of money as it involves a law of \"moral expectation\".\n\nEarly theorists of utility considered that it had physically quantifiable attributes. They thought that utility behaved like the magnitudes of distance or time, in which the simple use of a ruler or stopwatch resulted in a distinguishable measure. \"Utils\" was the name actually given to the units in a utility scale.\n\nIn the Victorian era many aspects of life were succumbing to quantification. The theory of utility soon began to be applied to moral-philosophy discussions. The essential idea in utilitarianism is to judge people's decisions by looking at their change in utils and measure whether they are better off. The main forerunner of the utilitarian principles since the end of the 18th century was Jeremy Bentham, who believed utility could be measured by some complex introspective examination and that it should guide the design of social policies and laws. For Bentham a scale of pleasure has as a unit of intensity \"the degree of intensity possessed by that pleasure which is the faintest of any that can be distinguished to be pleasure\"; he also stated that, as these pleasures increase in intensity higher and higher numbers could represent them. In the 18th and 19th centuries utility's measurability received plenty of attention from European schools of political economy, most notably through the work of marginalists (e.g. William Stanley Jevons, Léon Walras, Alfred Marshall). However, neither of them offered solid arguments to support the assumption of measurability. In Jevon's case he added to the later editions of his work a note on the difficulty of estimating utility with accuracy. Walras, too, struggled for many years before he could even attempt to formalize the assumption of measurability. Marshall was ambiguous about the measurability of hedonism because he adhered to its psychological-hedonistic properties but he also argued that it was \"unrealistical\" to do so.\n\nSupporters of cardinal utility theory in the 19th century suggested that market prices reflect utility, although they did not say much about their compatibility (i.e., prices being objective while utility is subjective). Accurately measuring subjective pleasure (or pain) seemed awkward, as the thinkers of the time were surely aware. They renamed utility in imaginative ways such as \"subjective wealth\", \"overall happiness\", \"moral worth\", \"psychic satisfaction\", or \"ophélimité\". During the second half of the 19th century many studies related to this fictional magnitude—utility—were conducted, but the conclusion was always the same: it proved impossible to definitively say whether a good is worth 50, 75, or 125 utils to a person, or to two different people. Moreover, the mere dependence of utility on notions of hedonism led academic circles to be skeptical of this theory.\n\nFrancis Edgeworth was also aware of the need to ground the theory of utility into the real world. He discussed the quantitative estimates that a person can make of his own pleasure or the pleasure of others, borrowing methods developed in psychology to study hedonic measurement: psychophysics. This field of psychology was built on work by Ernst H. Weber, but around the time of World War I, psychologists grew discouraged of it.\n\nIn the late 19th century, Carl Menger and his followers from the Austrian school of economics undertook the first successful departure from measurable utility, in the clever form of a theory of ranked uses. Despite abandoning the thought of quantifiable utility (i.e. psychological satisfaction mapped into the set of real numbers) Menger managed to establish a body of hypothesis about decision-making, resting solely on a few axioms of ranked preferences over the possible uses of goods and services. His numerical examples are \"illustrative of ordinal, not cardinal, relationships\".\n\nAround the turn of the 19th century neoclassical economists started to embrace alternative ways to deal with the measurability issue. By 1900, Pareto was hesitant about accurately measuring pleasure or pain because he thought that such a self-reported subjective magnitude lacked scientific validity. He wanted to find an alternative way to treat utility that did not rely on erratic perceptions of the senses. Pareto's main contribution to ordinal utility was to assume that higher indifference curves have greater utility, but how much greater does not need to be specified to obtain the result of increasing marginal rates of substitution.\n\nThe works and manuals of Vilfredo Pareto, Francis Edgeworth, Irving Fischer, and Eugene Slutsky departed from cardinal utility and served as pivots for others to continue the trend on ordinality. According to Viner, these economic thinkers came up with a theory that explained the negative slopes of demand curves. Their method avoided the measurability of utility by constructing some abstract indifference curve map.\n\nDuring the first three decades of the 20th century, economists from Italy and Russia became familiar with the Paretian idea that utility does not need to be cardinal. According to Schultz, by 1931 the idea of ordinal utility was not yet embraced by American economists. The breakthrough occurred when a theory of ordinal utility was put together by John Hicks and Roy Allen in 1934. In fact pages 54–55 from this paper contain the first use ever of the term 'cardinal utility'. The first treatment of a class of utility functions preserved by affine transformations, though, was made in 1934 by Oskar Lange.\n\nIn 1944 Frank Knight argued extensively for cardinal utility. In the decade of 1960 Parducci studied human judgements of magnitudes and suggested a range-frequency theory. Since the late 20th century economists are having a renewed interest in the measurement issues of happiness. This field has been developing methods, surveys and indices to measure happiness.\n\nSeveral properties of Cardinal utility functions can be derived using tools from measure theory and set theory.\n\nA utility function is considered to be measurable, if the strength of preference or intensity of liking of a good or service is determined with precision by the use of some objective criteria. For example, suppose that eating an apple gives to a person exactly half the pleasure of that of eating an orange. This would be a measurable utility if and only if the test employed for its direct measurement is based on an objective criterion that could let any external observer repeat the results accurately. One hypothetical way to achieve this would be by the use of an hedonometer, which was the instrument suggested by Edgeworth to be capable of registering the height of pleasure experienced by people, diverging according to a law of errors.\n\nBefore the 1930s, the measurability of utility functions was erroneously labeled as cardinality by economists. A different meaning of cardinality was used by economists who followed the formulation of Hicks-Allen. Under this usage, the cardinality of a utility function is simply the mathematical property of uniqueness up to a linear transformation. Around the end of the 1940s, some economists even rushed to argue that von Neumann-Morgenstern axiomatization of expected utility had resurrected measurability.\n\nThe confusion between cardinality and measurability was not to be solved until the works of Armen Alchian, William Baumol, and John Chipman. The title of Baumol's paper, \"The cardinal utility which is ordinal\", expressed well the semantic mess of the literature at the time.\n\nIt is helpful to consider the same problem as it appears in the construction of scales of measurement in the natural sciences. In the case of temperature there are two \"degrees of freedom\" for its measurement - the choice of unit and the zero. Different temperature scales map its intensity in different ways. In the celsius scale the zero is chosen to be the point where water freezes, and likewise, in cardinal utility theory one would be tempted to think that the choice of zero would correspond to a good or service that brings exactly 0 utils. However this is not necessarily true. The mathematical index remains cardinal, even if the zero gets moved arbitrarily to another point, or if the choice of scale is changed, or if both the scale and the zero are changed. Every measurable entity maps into a cardinal function but not every cardinal function is the result of the mapping of a measurable entity. The point of this example was used to prove that (as with temperature) it is still possible to predict something about the combination of two values of some utility function, even if the utils get transformed into entirely different numbers, as long as it remains a linear transformation.\n\nVon Neumann and Morgenstern stated that the question of measurability of physical quantities was dynamic. For instance, temperature was originally a number only up to any monotone transformation, but the development of the ideal-gas-thermometry led to transformations in which the absolute zero and absolute unit were missing. Subsequent developments of thermodynamics even fixed the absolute zero so that the transformation system in thermodynamics consists only of the multiplication by constants. According to Von Neumann and Morgenstern (1944, p. 23) \"For utility the situation seems to be of a similar nature [to temperature]\".\n\nThe following quote from Alchian served to clarify once and for all the real nature of utility functions, emphasizing that they no longer need to be measurable:\n\nIn 1955 Patrick Suppes and Muriel Winet solved the issue of the representability of preferences by a cardinal utility function, and derived the set of axioms and primitive characteristics required for this utility index to work.\n\nSuppose an agent is asked to rank his preferences of and his preferences of . If he finds that he can state, for example, that his degree of preference of exceeds his degree of preference of , we could summarize this information by any triplet of numbers satisfying the two inequalities: and .\n\nIf and were sums of money, the agent could vary the sum of money represented by until he could tell us that he found his degree of preference of over the revised amount equal to his degree of preference of over . If he finds such a , then the results of this last operation would be expressed by any triplet of numbers satisfying the relationships: (a) , and (b) = . Any two triplets obeying these relationships must be related by a linear transformation; they represent utility indices differing only by scale and origin. In this case, \"cardinality\" means nothing more being able to give consistent answers to these particular questions. Note that this experiment does not require measurability of utility. Itzhak Gilboa gives a sound explanation of why measurability can never be attained solely by introspection:\n\nAccording to this view, those situations where a person just cannot tell the difference between and will lead to indifference not because of a consistency of preferences, but because of a misperception of the senses. Moreover, human senses adapt to a given level of stimulation and then register changes from that baseline.\n\nSuppose a certain agent has a preference ordering over random outcomes (lotteries). If the agent can be queried about his preferences, it is possible to construct a cardinal utility function that represents these preferences. This is the core of the Von Neumann–Morgenstern utility theorem.\n\nAmong welfare economists of the utilitarist school it has been the general tendency to take satisfaction (in some cases, pleasure) as the unit of welfare. If the function of welfare economics is to contribute data which will serve the social philosopher or the statesman in the making of welfare judgements, this tendency leads perhaps, to a hedonistic ethics.\n\nUnder this framework, actions (including production of goods and provision of services) are judged by their contributions to the subjective wealth of people. In other words, it provides a way of judging the \"greatest good to the greatest number of persons\". An act that reduces one person's utility by 75 utils while increasing two others' by 50 utils each has increased overall utility by 25 utils and is thus a positive contribution; one that costs the first person 125 utils while giving the same 50 each to two other people has resulted in a net loss of 25 utils.\n\nIf a class of utility functions is cardinal, intrapersonal comparisons of utility differences are allowed. If, in addition, some comparisons of utility are meaningful interpersonally, the linear transformations used to produce the class of utility functions must be restricted across people. An example is cardinal unit comparability. In that information environment, admissible transformations are increasing affine functions and, in addition, the scaling factor must be the same for everyone. This information assumption allows for interpersonal comparisons of utility differences, but utility levels cannot be compared interpersonally because the intercept of the affine transformations may differ across people.\n\n\nThis type of indices involves choices under risk. In this case, , and , are lotteries associated with outcomes. Unlike cardinal utility theory under certainty, in which the possibility of moving from preferences to quantified utility was almost trivial, here it is paramount to be able to map preferences into the set of real numbers, so that the operation of mathematical expectation can be executed. Once the mapping is done, the introduction of additional assumptions would result in a consistent behavior of people regarding fair bets. But fair bets are, by definition, the result of comparing a gamble with an expected value of zero to some other gamble. Although it is impossible to model attitudes toward risk if one doesn't quantify utility, the theory should not be interpreted as measuring strength of preference under certainty.\n\nSuppose that certain outcomes are associated with three states of nature, so that \"x\" is preferred over \"x\" which in turn is preferred over \"x\"; this set of outcomes, , can be assumed to be a calculable money-prize in a controlled game of chance, unique up to one positive proportionality factor depending on the currency unit.\n\nLet and be two lotteries with probabilities \"p\", \"p\", and \"p\" of \"x\", \"x\", and \"x\" respectively being\n\nAssume that someone has the following preference structure under risk:\n\nmeaning that \"L\" is preferred over \"L\". By modifying the values of and in , eventually there will be some appropriate values () for which she is found to be indifferent between it and —for example\n\nExpected utility theory tells us that\n\nand so\n\nIn this example from Majumdar fixing the zero value of the utility index such that the utility of is 0, and by choosing the scale so that the utility of equals 1, gives\n\nModels of utility with several periods, in which people discount future values of utility, need to employ cardinalism in order to have well-behaved utility functions. According to Paul Samuelson the maximization of the discounted sum of future utilities implies that a person can rank utility differences.\n\nSome authors have commented on the misleading nature of the terms \"cardinal utility\" and \"ordinal utility\", as used in economic jargon:\n\nThere remain economists who believe that utility, if it cannot be measured, at least can be approximated somewhat to provide some form of measurement, similar to how prices, which have no uniform unit to provide an actual price level, could still be indexed to provide an \"inflation rate\" (which is actually a level of change in the prices of weighted indexed products). These measures are not perfect but can act as a proxy for the utility. Lancaster's characteristics approach to consumer demand illustrates this point.\n\n\n"}
{"id": "8533426", "url": "https://en.wikipedia.org/wiki?curid=8533426", "title": "Carsten Höller", "text": "Carsten Höller\n\nCarsten Höller (born December 1961) is a German artist. He lives and works in Stockholm, Sweden. Today, he also shares a house in Ghana with colleague Marcel Odenbach.\n\nBorn to German parents working for the European Economic Community, Höller grew up in Brussels. He holds a doctorate in agricultural science, specializing in the area of insects' olfactory communication strategies, from University of Kiel; the title of his dissertation is \"Efficiency Analysis of the Parasitoids of Cereal Aphids\". Only during the late 1980s did he first begin making art. However, he worked as a research entomologist until 1994.\n\nHöller came to prominence in the 1990s alongside a group of artists including Maurizio Cattelan, Douglas Gordon, Pierre Huyghe, Philippe Parreno, Rirkrit Tiravanija, and Andrea Zittel who worked across disciplines to reimagine the experience and the space of art. In his work, Höller creates situations which question familiar forms of perception and allow exhibition visitors to experiment on themselves, often inviting the public's active participation in so-called \"influential environments\". In their form, Höller's works are occasionally reminiscent of scientific laboratory arrangements, allowing the viewer to become the subject of an experiment. His work since the early 1990s has encompassed buildings, vehicles, slides, toys, games, narcotics, animals, performances, lectures, 3D films, flashing lights, mirrors, eyewear and sensory deprivation tanks.\n\nAmong Höller's works is a series of corkscrewing tubular metal slides made from 1998 that is an ongoing project. Not only are slides a practical means of transportation, but the act of sliding down one produces a loss of control, inducing a particular state of mind related to freedom from constraint. His most famous slides include that made for the office of Miuccia Prada in Milan (2000) and the first slides made for the Berlin Biennale in 1998.\n\nHöller's artistic practice reflects the interaction between work and public in various ways, sometimes chemically analyzing the nature of human emotions. His avid interest in the double harks back to the start of his career, when Höller designed a series of works with his then girlfriend, the artist Rosemarie Trockel, actually doubling himself up in another creator. Other examples include an exhibition in which Höller and Maurizio Cattelan presented a series of identical works at two different Paris galleries, removing all differences of style or ownership; and his exhibition \"One Day One Day\" (2003) at Färgfabriken in Stockholm, where two works were shown opposite each other and changed every day without the public’s knowledge. His explorations often involve playful elements such as in \"Sliding Doors\" (2003), a series of electronic sliding doors with a mirrored surface through which the audience passes in a seemingly endless passage. In 2008, Höller installed \"The Revolving Hotel Room\", a hotel room for two, as part of an exhibition at the Guggenheim Museum, New York. At his 2010 show at the Hamburger Bahnhof, visitors could pay 1,000 euros ($1,370) for a night on an exposed circular platform perched above 12 castrated reindeer, 24 canaries, eight mice and two flies. In \"Psycho Tank\", which can be used alone or with others, visitors float weightlessly on the surface of a sensory deprivation pool, providing a strange out-of-body experience Equally encouraging visitors' participation, \"Pill Clock\" (2011) is an aperture which emits a white pill into a growing pile of the same every 15 seconds.\n\nMushrooms became a regular feature of Höller's work from 1994. He has since realized several works with the fly-agaric mushroom, including the \"Mushroom Suitcase\" series (2001/2008) and the \"Upside Down Mushroom Room\" (2000), which was shown in 2000 at Fondazione Prada in Milan and in 2005 at MOCA in Los Angeles. His fly-agaric replicas are large-scale and often spin or hang upside down from the ceiling. The artist has also created photographic works based on the fly-agaric, entitled \"Mushroom Print\" (2003) and \"Soma Series\" (2008). In a series of giant sculptures of funghi – \"Giant Triple Mushrooms\" (2010) –, two quarters of each sculpture replicate the looks of two random fungi; half, a very specific species: the large red-and-white fly agaric fungus, Amanita muscaria, occurring wild in Eurasia. A fungus with psychoactive, hallucinogenic properties, it was used, it is thought, by Siberian shamans as an intoxicant.\n\nAnimals have figured largely in Höller’s work, most prominently at his exhibition \"Soma\" at the Hamburger Bahnhof, where two herds of reindeer were housed in the large main exhibition hall, along with canaries, mice and flies. The project was modeled after a scientific experiment. Other works that include animals include \"House for Pigs and People\", a construction Höller made with Rosemarie Trockel that was exhibited in documenta X, which Höller described as a \"monument of incomprehensibility. The amazement that comes back every time we observe an animal… the pigs are so similar to us, they set off strong biophile sentiments, mainly among children….\" In a conversation with Daniel Birnbaum, Höller continues, \"…It is difficult for us to believe that our consciousness—my consciousness—can correspond to an other’s. Only if others behave like me, am I prepared to accept it—and still consider my own being unique, as do all other humans. Perhaps it is only this uniqueness that a bird doesn’t believe in. Is it missing the process of a feedback with its ‹own›\". Other animal works include \"Loverfinches\" (1992-1994), \"Aquarium\" (1996), \"The Belgian Problem\" (2007), \"Singing Canaries Mobile\" (2009), and, with Rosemarie Trockel, \"Mosquito Bus\" (1996), \"Addina\" (1997), \"Bee House\" (1999), \"Silverfish House\" (1999) and \"Eyeball: a House for Pigeons, People, and Rats\" (2000).\n\nOn 18 June 2014 the 31-meter-high observation tower Vitra Slide Tower in Weil am Rhein was inaugurated.\n\nHöller is represented by Air de Paris, Paris; Massimo de Carlo, Milan; and Gagosian Gallery, London, New York, and Los Angeles.\n\nSlides\nFor the seventh commission in The Unilever Series, in Tate Modern's Turbine Hall, Höller created \"Test Site\", an installment in his ongoing series of slide works which started in 1998. Höller’s interest in slides ranges from their use as a practical and alternative means of transportation and the effect of sliding itself, which involves a loss of control, vertigo and an emotional response from the sliders, often delight. He is also interested in the shape of slides as a counterpoint to rectangular architecture, as in the Turbine Hall.\n\nEarlier slide works include \"Valerio I\" and \"Valerio II\", which were installed at Kunst-Werke in Berlin in 1998 and 1999. Höller described their title, \"The Valerio Phenomenon…supposedly originated at a rock concert in Italy. It’s an interesting example of mass hysteria. A sound technician at a concert disappeared, and someone in the audience, pretending to know his name, shouted 'Valerio!', and more and more people joined in. It was apparently infectious, and it spread from Brindisi to Rimini and other cities. There is something about the sound of this name that makes you want to shout it loud. You feel a little better after you’ve done it, just like after having traveled down a slide.\"\n\nIn an essay about \"Test Site\", Dorothea von Hantelmann compares Höller’s slides to Nietzsche’s ideas regarding art and science as two different powers that inform culture in distinct ways. \"When Nietzsche speaks of art’s potential to create intensities and vital energies, for him these creations are extraordinary—but they need not be true. While the scientist deals with truth and its cognition, the artist, after Nietzche, creates and transforms reality.\"\n\nIn his exhibition \"Experience\" at the New Museum in New York in 2011/2012, a slide was installed inside, spiraling straight through the third and fourth floors of the museum, ejecting the sliders on the second floor.\n\nHouse for Pigs and People\nThe project \"House for Pigs and People\", for documenta X in Kassel in 1997, was one of several collaborations realized by Höller in cooperation with the artist Rosemarie Trockel. It consisted of a box-like house structure with a concrete surface. Inside, the space was divided by a sheet of glass, separating two sections of the house—one side for pigs, the other side for people. The partition was one-way mirror glass, enabling the people to see the pigs, but not the other way around. The piece is a metaphor of ecological and social division, as well as an epistemological critique.\n\nIn a book produced for the project, Höller and Trockel contribute a text that consists of a series of questions involving the relationship between humans and animals. Some of the questions ask: \"Does not human consciousness originate primarily as a project of the sociological conditions of its coming into being? Doesn’t animal consciousness have to be something quite different, something we cannot imagine? Or is there a basic measure of consciousness, which is part of man’s biological makeup and also occurs in animals?\"\n\nThe Double Club\n\"The Double Club\" was a project in the form of a bar, restaurant and nightclub produced in collaboration with Fondazione Prada. It was installed in a warehouse in Islington, London, and was open from 20 November 2008 – 12 July 2009. The club was divided into equal amounts of floor space representing the \"Congo\" and the \"West\". Each division contained exclusively elements from each culture, including the furnishings, wall decors and food.\n\nHöller’s interaction with the culture of the Congo began when he started making regular visits to Kinshasa since 2001, interested in the role of music on public opinion and in turn in effecting politics. In an interview with Hans Ulrich Obrist in 2009, Höller also explains that the impetus for the \"Double Club\" was also drawn from a fascination with the phi phenomenon, \"…phi was of course interesting in a double sense…the jumping to and fro between two points, the eternal back and forth. And in that way this third form arises, which is an illusion and which moves, while the two other forms, by contrast, are fixed.\"\n\nThe phi phenomenon was explored earlier in Höller’s \"Flicker Films\", (2004 and 2005) which took footage from Congolese musicians and dancers presented on overlapping projections, which created a sculptural, quasi-holographic effect.\n\nA second edition of Double Club took place at Miami Art Basel this year where the artist exposes his on going concept on duality and oppositions in the shape of two installations where one side is monochromatic and other, full of neon colours. \nSOMA\nThe exhibition \"Soma\" was installed at the Hamburger Bahnhof in Berlin in 2010. Its main element were 12 reindeer in two pens running the length of the former railway station. Half of the reindeer were fed the fly agaric mushrooms in their food, which are part of their customary diet in the wild, and turn their urine into a hallucinogen. The reindeer urine was collected by handlers and then stored in on-site refrigerators for use. The experiment was extended to canaries, which were housed in two hanging cage pieces, to mice, and to flies. A mushroom-shaped \"Elevator Bed\" was installed in the middle of the space, and visitors could spend the night on the premise for a fee.\n\nThe title \"Soma\" comes from the name of the sacred libation drunk by the Indo-European followers of the Vedic religion, Hinduism's 5,000-year-old parent. Its ancient text, the Rigveda, contains 114 hymns to \"creative juice\", supposed to offer immortality. The recipe was lost, but in the 1960s researcher Robert Wasson hypothesised that soma was based on the fly agaric mushroom.\n\nKilling Children\nHöller’s \"Killing Children\" works are discreet objects made between 1990 and 1994. Some consist of such things as a comforter containing a piece of dried fly agaric titled \"Sucette aux Fausses Oranges\", a trap made from an upside-down playpen baited with a Kinder Schokolade egg attached to fishing wire titled \"Komm Kleines, kriegst was Feines\", a bicycle for children rigged with a fuse, match, and petrol titled \"Bicycle Bomb\", a piece titled \"220 Volt\" consisting of plugs, connecting cable and candy. \"Three Venomous Frogs from Costa Rica in a Bottle\" is a piece that is just that: dangerous frogs submerged in water in a baby’s bottle. The works in this series have been usually installed on bright bubble-gum pink carpet.\n\nCarousels\nHöller’s carousels (sometimes spelled carrousel, with two R’s, intentionally like the original word in French) and amusement park works, are some of his most well-known projects. Dating back to 1998, the key feature with Höller’s carousels are that they are modified in some way than what we expect from an amusement park ride, either most often through speed, or sometimes rotational direction, or surface material, as is the case with his \"Mirror Carousel\" (2005). In his exhibition \"Amusement Park\" at MASS MoCA in North Adams, 2006, the degree of slowness and direction changed every day. A local fairground operator, Art Gillette, engineered the changes.\n\nIn a catalogue essay for Höller’s 2008 exhibition at Kunsthaus Bregenz, Carl Roitmeister writes of the carrousel works, \"This is the age-old artistic ploy of defamiliarizing the commonplace in combination with the age-old artistic ploy represented by the Duchamp-style readymade. But two age-old ploys do not add up to a new one…The title of the carrousel, \"R B Ride\", doesn’t get us anywhere, because R B merely stands for Robles Bouso, the name of the now-defunct Spanish company that made the fairground device. The year of manufacture is 1969, the year of the failed revolution.\" Roitmeister goes on to make comparisons of the failure of the revolution with the action of the carrousel, the revolution of a clock, or time passing, in relation to the fact that the carousels move at a reduced speed, another failure—the failure to produce amusement. He goes further to say that \"the carrousel resembles a huge clock—that is, a time-measuring device rather than a time-diverting machine…\"\n\nFor Höller’s exhibition in 2011 at MACRO, he showed \"Double Carousel with Zöllner Stripes\" (2011). The pair of carousels anthropomorphically suggests a couple, \"I thought it would be nice to have two [merry-go-rounds] interacting as if they were in love or something. To bring people together and then further away ... It is romantic and tragic at the same time.\"\n\nLight Wall and Lichtraum\n\"Light Wall I\" (2000), \"Light Wall II\" (2001), \"Light Wall III\" (2002), \"Light Wall IV\" (2007), constitute a series of variable \"Light Walls\" consisting of at least nine panels. Each panel holds a grid of light bulbs flickering at a frequency of between seven and twelve hertz, combined with a clicking stereo signal that continues back and forth between two audio speakers. This induces optic and acoustic hallucinations: viewers experience modulating fields of color with their eyes open or shut, or perceive themselves or what is said by others around them in altered ways. The incessant turning on and off of the lights—in contrast to strobe lights—primarily induces, due to the slow illumination rate of the filaments, an erratic state of mind caught \"between\" poles of light/dark, awake/asleep, I/other, etc. \"Lichtraum\" (2008), as installed at Kunsthaus Bregenz, consisted of four walls of the exhibition space, each covered by thousands of LED lights. Since given the imposing scale of the surfaces, the lights are always seen out of the corner of the eye, one’s proprioception is affected so that one forms an \"intuited\" revolving.\n\n\n\n"}
{"id": "938069", "url": "https://en.wikipedia.org/wiki?curid=938069", "title": "Cartogram", "text": "Cartogram\n\nA cartogram is a map in which some thematic mapping variable – such as travel time, population, or GNP – is substituted for land area or distance. The geometry or space of the map is distorted, sometimes extremely, in order to convey the information of this alternate variable. They are primarily used to display emphasis and for analysis as nomographs.\n\nTwo common types of cartograms are area and distance cartograms. Cartograms have a fairly long history, with examples from the mid-1800s.\n\nAn area cartogram is sometimes referred to as a \"value-by-area map\" or an \"isodemographic map\", the latter particularly for a \"population cartogram\", which illustrates the relative sizes of the populations of the countries of the world by scaling the area of each country in proportion to its population; the shape and relative location of each country is retained to as large an extent as possible, but inevitably a large amount of distortion results. Other synonyms in use are \"anamorphic map\", \"density-equalizing map\" and \"Gastner map\".\n\nArea cartograms may be contiguous or noncontiguous. The area cartograms shown on this page are all contiguous, while a good example of a noncontiguous cartogram was published in \"The New York Times\". This method of cartogram creation is sometimes referred to as \"the projector method\" or \"scaled-down regions\".\n\nCartograms may be classified also by the properties of shape and topology preservation. Classical area cartograms (shown on this page) are typically distorting the shape of spatial units to some degree, but they are strict at preserving correct neighborhood relationships between them. Scaled-down cartograms (from the NY Times example) are strictly shape-preserving. Another branch of cartograms introduced by Dorling, replaces actual shapes with circles scaled according to the mapped feature. Circles are distributed to resemble the original topology. Demers cartogram is a variation of Dorling cartogram, but it uses rectangles instead of circles, and attempts to retain visual cues at the expense of minimum distance. Schematic maps based on quad trees can be seen as non shape-preserving cartograms with some degree of neighborhood preservation.\n\nA collection of contiguous area cartograms is available at Worldmapper, which was started by a collaborative team of researchers at the Universities of Sheffield and Michigan.\n\nOne of the first cartographers to generate cartograms with the aid of computer visualization was Waldo Tobler of UC Santa Barbara in the 1960s. Prior to Tobler's work, cartograms were created by hand (as they occasionally still are). The National Center for Geographic Information and Analysis located on the UCSB campus maintains an online Cartogram Central with resources regarding cartograms.\n\nA number of software packages generate cartograms. Most of the available cartogram generation tools work in conjunction with other GIS software tools as add-ons or independently produce cartographic outputs from GIS data formatted to work with commonly used GIS products. Examples of cartogram software include ScapeToad, Cart, and the Cartogram Processing Tool (an ArcScript for ESRI's ArcGIS), which all use the Gastner-Newman algorithm. An alternative algorithm, Carto3F, is also implemented as an independent program for non-commercial use on Windows platforms. This program also provides an optimization to the original Dougenik rubber-sheet algorithm.\n\nThe CRAN package recmap provides an implementation of a rectangular cartogram algorithm.\n\nCartograms can also be constructed manually, either by hand or in a computer-assisted environment. Block cartograms are constructed by arranging geometrically regular equal-sized blocks, with the number of blocks allocated to each district proportional to the population variable. Several examples of block cartograms were published during the 2016 U.S. presidential election season by \"The Washington Post\" , the \"FiveThirtyEight\" blog , and the \"Wall Street Journal\" , among others.\n\n\n\n"}
{"id": "43847911", "url": "https://en.wikipedia.org/wiki?curid=43847911", "title": "Child sexual abuse in Nigeria", "text": "Child sexual abuse in Nigeria\n\nChild sexual abuse in Nigeria is an offence under several sections of chapter 21 of the country's criminal code. The age of consent is 18.\n\nUNICEF reported in 2015 that one in four girls and one in ten boys in Nigeria had experienced sexual violence before the age of 18. According to a survey by Positive Action for Treatment Access, over 31.4 percent of girls there said that their first sexual encounter had been rape or forced sex of some kind.\n\nThe Centre for Environment, Human Rights and Development reported that 1,200 girls had been raped in 2012 in Rivers, a coastal state in southeastern Nigeria.\n\nAccording to UNICEF, six out of ten children in Nigeria experience emotional, physical or sexual abuse before the age of 18, with half experiencing physical violence.\n\nConditions that increases the risk of girl-child sexual assault in Nigeria can be found in schools, baby factories and the practice of child labour. Studies conducted in Nigeria disclose that young girls are victims in majority of reported assault cases in hospitals. A four-year review of sexual assault cases at LASUTH that began in 2008 and ended in December 2012, showed that out of a total 287 reported cases of sexual assault, 83% of the victims were below the age of 19. A one-year survey conducted at Enugu State University Teaching Hospital between 2012 and 2013 revealed that 70% of sexual assault victims were under the age of 18. In the Enugu survey, majority of the victims knew their perpetrators and the assault occurred inside uncompleted buildings and the victims or perpetrators residence.\n\nOne of the traditional means of socialization of children is through trading. However, the introduction of young girls into street trading increases the vulnerabilities of the girls to sexual harassment. Sexual abuse of young girls in Nigeria is linked child labour.\n\nReligious and communal stigma associated with surrogacy and adoption has created a rise in baby factories in Nigeria. A large number of female victims in the baby factories are young adolescents. Operators of the baby factories mostly prey on pregnant young girls who are from lower income households, unmarried and are afraid of the public stigma associated with teenage pregnancy. Though, majority of the girls who enter the factory are pregnant some of girls in the factories were kidnapped or bartered to the operators. These girls are then raped solely for the purpose of procreation.\n\n\n"}
{"id": "1316648", "url": "https://en.wikipedia.org/wiki?curid=1316648", "title": "Constructive dilemma", "text": "Constructive dilemma\n\nConstructive dilemma is a valid rule of inference of propositional logic. It is the inference that, if \"P\" implies \"Q\" and \"R\" implies \"S\" and either \"P\" or \"R\" is true, then \"Q or S\" has to be true. In sum, if two conditionals are true and at least one of their antecedents is, then at least one of their consequents must be too. \"Constructive dilemma\" is the disjunctive version of modus ponens, whereas,\ndestructive dilemma is the disjunctive version of \"modus tollens\". The rule can be stated:\n\nwhere the rule is that whenever instances of \"formula_2\", \"formula_3\", and \"formula_4\" appear on lines of a proof, \"formula_5\" can be placed on a subsequent line.\n\nThe \"constructive dilemma\" rule may be written in sequent notation:\n\nwhere formula_7 is a metalogical symbol meaning that formula_5 is a syntactic consequence of formula_2, formula_3, and formula_5 in some logical system;\n\nand expressed as a truth-functional tautology or theorem of propositional logic:\n\nwhere formula_13, formula_14, formula_15 and formula_16 are propositions expressed in some formal system.\n\nThe dilemma derives its name because of the transfer of disjunctive operator.\n"}
{"id": "27313138", "url": "https://en.wikipedia.org/wiki?curid=27313138", "title": "Contour advection", "text": "Contour advection\n\nContour advection is a Lagrangian method \nof simulating the evolution of one or more contours or isolines of\na tracer as it is stirred by a moving fluid.\nConsider a blob of dye injected into a river or stream: to first order it could be modelled by tracking only the motion of its outlines. \nIt is an excellent method for studying chaotic mixing:\neven when advected by smooth or finitely-resolved velocity fields, \nthrough a continuous process of stretching and folding,\nthese contours often develop into intricate fractals. \nThe tracer is typically passive as in \nbut may also be active as in, representing a dynamical property of the fluid such as vorticity.\nAt present, advection of contours is limited to two dimensions,\nbut generalizations to three dimensions are possible.\n\nFirst we need a set of points that accurately define the contour.\nThese points are advected forward using a trajectory\nintegration technique.\nTo maintain its integrity,\npoints must be added to or removed from the curve \nat regular intervals based on some criterion or metric.\nThe most obvious criterion is to maintain the distance between adjacent points\nwithin a certain interval.\nA better method is to use curvature since fewer points are required for\nthe same level of precision.\nThe curvature of a two-dimensional, Cartesian curve is given as:\n\nwhere formula_2 is the radius of curvature \nand formula_3 is the path.\nWe need to keep the fraction of arc traced out between two adjacent points,\nformula_4, where formula_5 is the path difference between them,\nroughly constant\n\nIn,\ncubic spline fitting is used both to calculate the curvature\nand interpolate new points into the contour.\nThe spline, which\nis fitted parametrically,\nreturns a set of second-order derivatives.\n\nA powerful refinement to the technique involves cutting out filaments that have become too\nnarrow to be significant. \nIf the distance method of adding/removing points is used, \nthen it is relatively straight forward \nto check the distances between all combinations of points.\nIf a distance between non-adjacent points is too small, \nthen the two points are separated from their neighbours,\njoined together and their neighbours joined also.\nPoints may then be removed if necessary. \nOnce we allow surgery, we allow multiply connected domains inside the same contour.\nA piece of the contour only one point in length would be removed from the simulation.\nThe most challenging part of the exercise is keeping track of all the points in order\nto reduce the number of distance calculations---see nearest neighbour search.\nIf the curvature method is used, \nthen it may be difficult to recognize when two sections of the contour\nare close enough to apply the surgery because of differing spacing\nin strongly curved versus relatively straight sections.\n\nAdvected contours, e.g. of trace gases (such as ozone) in the stratosphere,\ncan be validated with satellite remote sensing instruments using a method called isoline retrieval.\n"}
{"id": "1500618", "url": "https://en.wikipedia.org/wiki?curid=1500618", "title": "Depressive realism", "text": "Depressive realism\n\nDepressive realism is the hypothesis developed by Lauren Alloy and Lyn Yvonne Abramson that depressed individuals make more realistic inferences than non-depressed individuals. Although depressed individuals are thought to have a negative cognitive bias that results in recurrent, negative automatic thoughts, maladaptive behaviors, and dysfunctional world beliefs, depressive realism argues not only that this negativity may reflect a more accurate appraisal of the world but also that non-depressed individuals' appraisals are positively biased. This theory remains very controversial, as it brings into question the theory underlying cognitive behavioral therapy, which posits that the depressed individual is negatively biased in their perceptions, with the goal of returning them to a more objective state. While some of the evidence currently supports the plausibility of depressive realism, its effect may be restricted to a select few situations.\n\nWhen participants were asked to press a button and rate the control they perceived they had over whether or not a light turned on, depressed individuals made more accurate ratings of control than non-depressed individuals. Among participants asked to complete a task and rate their performance without any feedback, depressed individuals made more accurate self-ratings than non-depressed individuals. For participants asked to complete a series of tasks, given feedback on their performance after each task, and who self-rated their overall performance after completing all the tasks, depressed individuals were again more likely to give an accurate self-rating than non-depressed individuals. When asked to evaluate their performance both immediately and some time after completing a task, depressed individuals made accurate appraisals both immediately before and after time had passed.\n\nIn a functional magnetic resonance imaging study of the brain, depressed patients were shown to be more accurate in their causal attributions of positive and negative social events than non-depressed participants who demonstrated a positive bias. This difference was also reflected in the differential activation of the fronto-temporal network, higher activation for non self-serving attributions in non-depressed participants and for self-serving attributions in depressed patients, and reduced coupling of the dorsomedial prefrontal cortex seed region and the limbic areas when depressed patients made self-serving attributions.\n\nWhen asked to rate both their performance and the performance of others, non-depressed individuals demonstrated positive bias when rating themselves but no bias when rating others. Depressed individuals conversely showed no bias when rating themselves but a positive bias when rating others. \n\nWhen assessing participant thoughts in public versus private settings, the thoughts of non-depressed individuals were more optimistic in public than private, while depressed individuals were less optimistic in public. \n\nWhen asked to rate their performance immediately after a task and after some time had passed, depressed individuals were more accurate when they rated themselves immediately after the task but were more negative after time had passed whereas non-depressed individuals were positive immediately after and some time after.\n\nAlthough depressed individuals make accurate judgments about having no control in situations where they in fact have no control, this appraisal also carries over to situations where they do have control, suggesting that the depressed perspective is not more accurate overall. \n\nWhen studied in real-world settings, depressed individuals are actually less accurate and more overconfident in their predictions about the future than their non-depressed peers. Participants' attributional accuracy may also be more related to their overall attributional style rather than the presence and severity of their depressive symptoms.\n\nSome have argued that the evidence is not more conclusive because no standard for reality exists, the diagnoses are dubious, and the results may not apply to the real world. Because many studies rely on self-report of depressive symptoms and self-reports are known to be biased, the diagnosis of depression in these studies may not be valid, necessitating the use of other objective measures. Due to most of these studies using designs that do not necessarily approximate real-world phenomena, the external validity of the depressive realism hypothesis is unclear. There is also concern that the depressive realism effect is merely a byproduct of the depressed person being in a situation that agrees with their negative bias.\n\n"}
{"id": "143806", "url": "https://en.wikipedia.org/wiki?curid=143806", "title": "Doppelgänger", "text": "Doppelgänger\n\nA doppelgänger (; , literally \"double-goer\") is a non-biologically related look-alike or double of a living person, sometimes portrayed as a ghostly or paranormal phenomenon and usually seen as a harbinger of bad luck. Other traditions and stories equate a doppelgänger with an evil twin. In modern times, the term twin stranger is occasionally used. The word \"doppelgänger\" is often used in a more general and neutral sense, and in slang, to describe any person who physically or behaviorally resembles another person.\n\nThe word \"doppelgänger\" is a loanword from the German \"Doppelgänger\", a compound noun formed by combining the two nouns \"Doppel\" (double) and \"Gänger\" (walker or goer). The singular and plural forms are the same in German, but English usually prefers the plural \"doppelgängers\". The first known use, in the slightly different form \"Doppeltgänger\", occurs in the novel \"Siebenkäs\" (1796) by Jean Paul, in which he explains his newly coined word by a footnote – while actually the word \"Doppelgänger\" also appears, but with a quite different meaning.\n\nLike all nouns in German, the word is written with an initial capital letter. \"Doppelgänger\" and \"Doppelgaenger\" are essentially equivalent spellings, and \"Doppelganger\" is different and would correspond to a different pronunciation. In English, the word should be written with a lower-case letter (doppelgänger) unless it is the first word of a sentence or part of a title. It is further common to drop the umlaut on the letter \"a\", writing (and often pronouncing) \"doppelganger\".\n\nEnglish-speakers have only recently applied this German word to a paranormal concept. Francis Grose's, \"Provincial Glossary\" of 1787 used the term \"fetch\" instead, defined as the \"apparition of a person living.\" Catherine Crowe's book on paranormal phenomena, \"The Night-Side of Nature\" (1848) helped make the German word well-known. However, the concept of alter egos and double spirits has appeared in the folklore, myths, religious concepts, and traditions of many cultures throughout human history.\n\nIn Ancient Egyptian mythology, a \"ka\" was a tangible \"spirit double\" having the same memories and feelings as the person to whom the counterpart belongs. \"The Greek Princess\" presents an Egyptian view of the Trojan War in which a \"ka\" of Helen misleads Paris, helping to stop the war.. This is depicted in Euripides' play \"Helen\". In Norse mythology, a \"vardøger\" is a ghostly double who is seen performing the person's actions in advance. In Finnish mythology, this is called having an \"etiäinen\", \"a firstcomer\". The doppelgänger is a version of the Ankou, a personification of death, in Breton, Cornish, and Norman folklore.\n\nIzaak Walton claimed that English metaphysical poet John Donne saw his wife's doppelgänger in 1612 in Paris, on the same night as the stillbirth of their daughter.\n\nGerman playwright Goethe described an experience in his autobiography \"Dichtung und Wahrheit\" in which he and his double passed one another on horseback. In addition to describing the doppelgänger double as a counterpart to the self, Percy Bysshe Shelley's drama \"Prometheus Unbound\" makes reference to a dead child who \"met his own image walking in the garden\".\nLord Byron uses doppelgänger imagery to explore the duality of human nature.\n\nIn The Devil's Elixir (1815), a man murders the brother and stepmother of his beloved princess, finds his doppelgänger has been sentenced to death for these crimes in his stead, and liberates him, only to have the doppelgänger murder the object of his affection. This was one of E. T. A. Hoffmann's early novels.\n\nFyodor Dostoyevsky's novel \"The Double\" (1846) presents the doppelgänger as an opposite personality who exploits the character failings of the protagonist to take over his life. Charles Williams' \"Descent into Hell\" (1939) has character Pauline Anstruther seeing her own doppelgänger all through her life. Clive Barker's story \"Human Remains\" in his \"Books of Blood\" is a doppelgänger tale, and the doppelgänger motif is a staple of Gothic fiction.\n\nWith the advent of social media, there have been several reported cases of people finding their \"twin stranger\" online, a modern term for a doppelgänger. Twinstrangers.net is a website where users can upload a photo of themselves and facial recognition software attempts to match them with another user of like appearance. The site reports that it has found numerous living doppelgängersincluding three living doppelgängers of its founder Niamh Geaney.\n\nHeautoscopy is a term used in psychiatry and neurology for the hallucination of \"seeing one's own body at a distance\". It can occur as a symptom in schizophrenia and epilepsy, and is considered a possible explanation for doppelgänger phenomena.\n\nCriminologists find a practical application in the concepts of facial familiarity and similarity due to the instances of wrongful convictions based on eyewitness testimony. In one case, a person spent 17 years behind bars persistently denying any involvement with the crime of which he was accused. He was finally released after someone was found who shared a striking resemblance and the same first name.\n\n\n\n"}
{"id": "37436", "url": "https://en.wikipedia.org/wiki?curid=37436", "title": "Emergence", "text": "Emergence\n\nIn philosophy, systems theory, science, and art, emergence is the condition of an entity having properties its parts do not have, due to interactions among the parts.\n\nEmergence plays a central role in theories of integrative levels and of complex systems. For instance, the phenomenon of \"life\" as studied in biology is an emergent property of chemistry, and psychological phenomena emerge from the neurobiological phenomena of living things.\n\nIn philosophy, theories that emphasize emergent properties have been called emergentism. Almost all accounts of emergentism include a form of epistemic or ontological irreducibility to the lower levels.\n\nIn philosophy, emergence is often understood to be a claim about the etiology of a system's properties. An emergent property of a system, in this context, is one that is not a property of any component of that system, but is still a feature of the system as a whole. Nicolai Hartmann, one of the first modern philosophers to write on emergence, termed this \"categorial novum\" (new category).\n\nThis idea of emergence has been around since at least the time of Aristotle.  John Stuart Mill and Julian Huxley are two of many scientists and philosophers who have written on the concept.\n\nThe term \"emergent\" was coined by philosopher G. H. Lewes, who wrote:\n\nEvery resultant is either a sum or a difference of the co-operant forces; their sum, when their directions are the same – their difference, when their directions are contrary. Further, every resultant is clearly traceable in its components, because these are homogeneous and commensurable. It is otherwise with emergents, when, instead of adding measurable motion to measurable motion, or things of one kind to other individuals of their kind, there is a co-operation of things of unlike kinds. The emergent is unlike its components insofar as these are incommensurable, and it cannot be reduced to their sum or their difference.\n\nEconomist Jeffrey Goldstein provided a current definition of emergence in the journal \"Emergence\". Goldstein initially defined emergence as: \"the arising of novel and coherent structures, patterns and properties during the process of self-organization in complex systems\".\n\nSystems scientist Peter Corning described the qualities of Goldstein's definition in more detail:\n\nThe common characteristics are: (1) radical novelty (features not previously observed in systems); (2) coherence or correlation (meaning integrated wholes that maintain themselves over some period of time); (3) A global or macro \"level\" (i.e. there is some property of \"wholeness\"); (4) it is the product of a dynamical process (it evolves); and (5) it is \"ostensive\" (it can be perceived).\n\nPeter Corning suggests a narrower definition, requiring that the components be unlike in kind (following Lewes), and that they involve division of labor between these components. He also says that living systems (like the game of chess), while emergent, cannot be reduced to underlying laws of emergence:\n\nRules, or laws, have no causal efficacy; they do not in fact “generate” anything. They serve merely to describe regularities and consistent relationships in nature. These patterns may be very illuminating and important, but the underlying causal agencies must be separately specified (though often they are not). But that aside, the game of chess illustrates ... why any laws or rules of emergence and evolution are insufficient. Even in a chess game, you cannot use the rules to predict “history” – i.e., the course of any given game. Indeed, you cannot even reliably predict the next move in a chess game. Why? Because the “system” involves more than the rules of the game. It also includes the players and their unfolding, moment-by-moment decisions among a very large number of available options at each choice point. The game of chess is inescapably historical, even though it is also constrained and shaped by a set of rules, not to mention the laws of physics. Moreover, and this is a key point, the game of chess is also shaped by teleonomic, cybernetic, feedback-driven influences. It is not simply a self-ordered process; it involves an organized, “purposeful” activity.\n\nUsage of the notion \"emergence\" may generally be subdivided into two perspectives, that of \"weak emergence\" and \"strong emergence\". In terms of physical systems, weak emergence is a type of emergence in which the emergent property is amenable to computer simulation. This is opposed to the older notion of strong emergence, in which the emergent property cannot be simulated by a computer.\n\nSome common points between the two notions are that emergence concerns new properties produced as the system grows, which is to say ones which are not shared with its components or prior states. Also, it is assumed that the properties are supervenient rather than metaphysically primitive .\n\nWeak emergence describes new properties arising in systems as a result of the interactions at an elemental level. However, it is stipulated that the properties can be determined only by observing or simulating the system, and not by any process of analysis.\n\nBedau notes that weak emergence is not a universal metaphysical solvent, as the hypothesis that consciousness is weakly emergent would not resolve the traditional philosophical questions about the physicality of consciousness. However, Bedau concludes that adopting this view would provide a precise notion that emergence is involved in consciousness, and second, the notion of weak emergence is metaphysically benign.\n\nStrong emergence describes the direct causal action of a high-level system upon its components; qualities produced this way are irreducible to the system's constituent parts . The whole is other than the sum of its parts. An example from physics of such emergence is water, being seemingly unpredictable even after an exhaustive study of the properties of its constituent atoms of hydrogen and oxygen. It follows then that no simulation of the system can exist, for such a simulation would itself constitute a reduction of the system to its constituent parts.\n\nHowever, \"the debate about whether or not the whole can be predicted from the properties of the parts misses the point. Wholes produce unique combined effects, but many of these effects may be co-determined by the context and the interactions between the whole and its environment(s)\" . In accordance with his Synergism Hypothesis, Corning also stated, \"It is the synergistic effects produced by wholes that are the very cause of the evolution of complexity in nature.\" Novelist Arthur Koestler used the metaphor of Janus (a symbol of the unity underlying complements like open/shut, peace/war) to illustrate how the two perspectives (strong vs. weak or holistic vs. reductionistic) should be treated as non-exclusive, and should work together to address the issues of emergence. Further,\n\nThe ability to reduce everything to simple fundamental laws does not imply the ability to start from those laws and reconstruct the universe. The constructionist hypothesis breaks down when confronted with the twin difficulties of scale and complexity. At each level of complexity entirely new properties appear. Psychology is not applied biology, nor is biology applied chemistry. We can now see that the whole becomes not merely more, but very different from the sum of its parts.\n\nThe plausibility of strong emergence is questioned by some as contravening our usual understanding of physics. Mark A. Bedau observes:\n\nAlthough strong emergence is logically possible, it is uncomfortably like magic. How does an irreducible but supervenient downward causal power arise, since by definition it cannot be due to the aggregation of the micro-level potentialities? Such causal powers would be quite unlike anything within our scientific ken. This not only indicates how they will discomfort reasonable forms of materialism. Their mysteriousness will only heighten the traditional worry that emergence entails illegitimately getting something from nothing.\n\nStrong emergence can be criticized for being causally overdetermined. The canonical example concerns emergent mental states (M and M∗) that supervene on physical states (P and P∗) respectively. Let M and M∗ be emergent properties. Let M∗ supervene on base property P∗. What happens when M causes M∗? Jaegwon Kim says:\n\nIn our schematic example above, we concluded that M causes M∗ by causing P∗. So M causes P∗. Now, M, as an emergent, must itself have an emergence base property, say P. Now we face a critical question: if an emergent, M, emerges from basal condition P, why cannot P displace M as a cause of any putative effect of M? Why cannot P do all the work in explaining why any alleged effect of M occurred? If causation is understood as nomological (law-based) sufficiency, P, as M’s emergence base, is nomologically sufficient for it, and M, as P∗’s cause, is nomologically sufficient for P∗. It follows that P is nomologically sufficient for P∗ and hence qualifies as its cause…If M is somehow retained as a cause, we are faced with the highly implausible consequence that every case of downward causation involves overdetermination (since P remains a cause of P∗ as well). Moreover, this goes against the spirit of emergentism in any case: emergents are supposed to make distinctive and novel causal contributions.\n\nIf M is the cause of M∗, then M∗ is overdetermined because M∗ can also be thought of as being determined by P. One escape route that a strong emergentist could take would be to deny downward causation. However, this would remove the proposed reason that emergent mental states must supervene on physical states, which in turn would call physicalism into question, and thus be unpalatable for some philosophers and physicists.\n\nMeanwhile, others have worked towards developing analytical evidence of strong emergence. In 2009, Gu et al. presented a class of physical systems that exhibits non-computable macroscopic properties. More precisely, if one could compute certain macroscopic properties of these systems from the microscopic description of these systems, then one would be able to solve computational problems known to be undecidable in computer science. They concluded that\n\nAlthough macroscopic concepts are essential for understanding our world, much of fundamental physics has been devoted to the search for a `theory of everything', a set of equations that perfectly describe the behavior of all fundamental particles. The view that this is the goal of science rests in part on the rationale that such a theory would allow us to derive the behavior of all macroscopic concepts, at least in principle. The evidence we have presented suggests that this view may be overly optimistic. A `theory of everything' is one of many components necessary for complete understanding of the universe, but is not necessarily the only one. The development of macroscopic laws from first principles may involve more than just systematic logic, and could require conjectures suggested by experiments, simulations or insight.\n\nEmergent structures are patterns that emerge via collective actions of many individual entities. To explain such patterns, one might conclude, per Aristotle, that emergent structures are other than the sum of their parts on the assumption that the emergent order will not arise if the various parts simply interact independently of one another. However, there are those who disagree. According to this argument, the interaction of each part with its immediate surroundings causes a complex chain of processes that can lead to order in some form. In fact, some systems in nature are observed to exhibit emergence based upon the interactions of autonomous parts, and some others exhibit emergence that at least at present cannot be reduced in this way. In particular renormalization are methods in theoretical physics which enables scientists to study systems that are not tractable as the combination of their parts.\n\nThe properties of complexity and organization of any system are considered by Crutchfield to be subjective qualities determined by the observer.\n\nDefining structure and detecting the emergence of complexity in nature are inherently subjective, though essential, scientific activities. Despite the difficulties, these problems can be analysed in terms of how model-building observers infer from measurements the computational capabilities embedded in non-linear processes. An observer’s notion of what is ordered, what is random, and what is complex in its environment depends directly on its computational resources: the amount of raw measurement data, of memory, and of time available for estimation and inference. The discovery of structure in an environment depends more critically and subtly, though, on how those resources are organized. The descriptive power of the observer’s chosen (or implicit) computational model class, for example, can be an overwhelming determinant in finding regularity in data.\n\nOn the other hand, Peter Corning argues \"Must the synergies be perceived/observed in order to qualify as emergent effects, as some theorists claim? Most emphatically not. The synergies associated with emergence are real and measurable, even if nobody is there to observe them.\"\n\nIn religion, emergence grounds expressions of religious naturalism and syntheism in which a sense of the sacred is perceived in the workings of entirely naturalistic processes by which more complex forms arise or evolve from simpler forms. Examples are detailed in \"The Sacred Emergence of Nature\" by Ursula Goodenough & Terrence Deacon and \"Beyond Reductionism: Reinventing the Sacred\" by Stuart Kauffman, both from 2006, and in \"Syntheism – Creating God in The Internet Age\" by Alexander Bard & Jan Söderqvist from 2014. An early argument (1904–05) for the emergence of social formations, in part stemming from religion, can be found in Max Weber's most famous work, \"The Protestant Ethic and the Spirit of Capitalism\".\n\nIn art, emergence is used to explore the origins of novelty, creativity, and authorship. Some art/literary theorists (Wheeler, 2006; Alexander, 2011) have proposed alternatives to postmodern understandings of \"authorship\" using the complexity sciences and emergence theory. They contend that artistic selfhood and meaning are emergent, relatively objective phenomena. Michael J. Pearce has used emergence to describe the experience of works of art in relation to contemporary neuroscience. Practicing artist Leonel Moura, in turn, attrbutes to his \"artbots\" a real, if nonetheless rudimentary, creativity based on emergent principles.\n\nIn international development, concepts of emergence have been used within a theory of social change termed SEED-SCALE to show how standard principles interact to bring forward socio-economic development fitted to cultural values, community economics, and natural environment (local solutions emerging from the larger socio-econo-biosphere). These principles can be implemented utilizing a sequence of standardized tasks that self-assemble in individually specific ways utilizing recursive evaluative criteria.\n\nIn postcolonial studies, the term \"Emerging Literature\" refers to a contemporary body of texts that is gaining momentum in the global literary landscape (v. esp.: J.M. Grassin, ed. \"Emerging Literatures\", Bern, Berlin, etc. : Peter Lang, 1996). By opposition, \"emergent literature\" is rather a concept used in the theory of literature.\n\nAn emergent behavior or emergent property can appear when a number of simple entities (agents) operate in an environment, forming more complex behaviors as a collective. If emergence happens over disparate size scales, then the reason is usually a causal relation across different scales. In other words, there is often a form of top-down feedback in systems with emergent properties. The processes causing emergent properties may occur in either the observed or observing system, and are commonly identifiable by their patterns of accumulating change, generally called 'growth'. Emergent behaviours can occur because of intricate causal relations across different scales and feedback, known as interconnectivity. The emergent property itself may be either very predictable or unpredictable and unprecedented, and represent a new level of the system's evolution. The complex behaviour or properties are not a property of any single such entity, nor can they easily be predicted or deduced from behaviour in the lower-level entities, and might in fact be irreducible to such behavior. The shape and behaviour of a flock of birds or school of fish are good examples of emergent properties.\n\nOne reason emergent behaviour is hard to predict is that the number of interactions between a system components increases exponentially with the number of components, thus allowing for many new and subtle types of behaviour to emerge. Emergence is often a product of particular patterns of interaction. Negative feedback introduces constraints that serve to fix structures or behaviours. In contrast, positive feedback promotes change, allowing local variations to grow into global patterns. Another way in which interactions leads to emergent properties is dual-phase evolution. This occurs where interactions are applied intermittently, leading to two phases: one in which patterns form or grow, the other in which they are refined or removed.\n\nOn the other hand, merely having a large number of interactions is not enough by itself to guarantee emergent behaviour; many of the interactions may be negligible or irrelevant, or may cancel each other out. In some cases, a large number of interactions can in fact hinder the emergence of interesting behaviour, by creating a lot of \"noise\" to drown out any emerging \"signal\"; the emergent behaviour may need to be temporarily isolated from other interactions before it reaches enough critical mass to self-support. Thus it is not just the sheer number of connections between components which encourages emergence; it is also how these connections are organised. A hierarchical organisation is one example that can generate emergent behaviour (a bureaucracy may behave in a way quite different from that of the individual humans in that bureaucracy); but emergent behaviour can also arise from more decentralized organisational structures, such as a marketplace. In some cases, the system has to reach a combined threshold of diversity, organisation, and connectivity before emergent behaviour appears.\n\nUnintended consequences and side effects are closely related to emergent properties. Luc Steels writes: \"A component has a particular functionality but this is not recognizable as a subfunction of the global functionality. Instead a component implements a behaviour whose side effect contributes to the global functionality [...] Each behaviour has a side effect and the sum of the side effects gives the desired functionality\". In other words, the global or macroscopic functionality of a system with \"emergent functionality\" is the sum of all \"side effects\", of all emergent properties and functionalities.\n\nSystems with emergent properties or emergent structures may appear to defy entropic principles and the second law of thermodynamics, because they form and increase order despite the lack of command and central control. This is possible because open systems can extract information and order out of the environment.\n\nEmergence helps to explain why the fallacy of division is a fallacy.\n\nEmergent structures can be found in many natural phenomena, from the physical to the biological domain. For example, the shape of weather phenomena such as hurricanes are emergent structures. The development and growth of complex, orderly crystals, as driven by the random motion of water molecules within a conducive natural environment, is another example of an emergent process, where randomness can give rise to complex and deeply attractive, orderly structures.\n\nIt is useful to distinguish three forms of emergent structures. A \"first-order\" emergent structure occurs as a result of shape interactions (for example, hydrogen bonds in water molecules lead to surface tension). A \"second-order\" emergent structure involves shape interactions played out sequentially over time (for example, changing atmospheric conditions as a snowflake falls to the ground build upon and alter its form). Finally, a \"third-order\" emergent structure is a consequence of shape, time, and heritable instructions. For example, an organism's genetic code affects the form of the organism's systems in space and time.\n\nIn physics, emergence is used to describe a property, law, or phenomenon which occurs at macroscopic scales (in space or time) but not at microscopic scales, despite the fact that a macroscopic system can be viewed as a very large ensemble of microscopic systems.\n\nAn emergent property need not be more complicated than the underlying non-emergent properties which generate it. For instance, the laws of thermodynamics are remarkably simple, even if the laws which govern the interactions between component particles are complex. The term emergence in physics is thus used not to signify complexity, but rather to distinguish which laws and concepts apply to macroscopic scales, and which ones apply to microscopic scales. \n\nHowever, another, perhaps more broadly applicable way to conceive of the emergent divide does involve a dose of complexity insofar as the computational feasibility of going from the microscopic to the macroscopic property tells the 'strength' of the emergence. This is better understood given the following definition of emergence that comes from physics:\n\n\"An emergent behavior of a physical system is a qualitative property that can only occur in the limit that the number of microscopic constituents tends to infinity.\"\n\nSince there are no actually infinite systems in the real world, there is no obvious naturally occurring notion of a hard separation between the properties of the constituents of a system and those of the emergent whole. As discussed below, classical mechanics is thought to be emergent from quantum mechanics, though in principal, quantum dynamics fully describes everything happening at a classical level. However, it would take a computer larger then the size of the universe with more computing time then life time of the universe to describe the motion of a falling apple in terms of the locations of its electrons ; thus we can take this to be a \"strong\" emergent divide. \n\nSome examples include:\n\nTemperature is sometimes used as an example of an emergent macroscopic behaviour. In classical dynamics, a \"snapshot\" of the instantaneous momenta of a large number of particles at equilibrium is sufficient to find the average kinetic energy per degree of freedom which is proportional to the temperature. For a small number of particles the instantaneous momenta at a given time are not statistically sufficient to determine the temperature of the system. However, using the ergodic hypothesis, the temperature can still be obtained to arbitrary precision by further averaging the momenta over a long enough time.\n\nConvection in a liquid or gas is another example of emergent macroscopic behaviour that makes sense only when considering differentials of temperature. Convection cells, particularly Bénard cells, are an example of a self-organizing system (more specifically, a dissipative system) whose structure is determined both by the constraints of the system and by random perturbations: the possible realizations of the shape and size of the cells depends on the temperature gradient as well as the nature of the fluid and shape of the container, but which configurations are actually realized is due to random perturbations (thus these systems exhibit a form of symmetry breaking).\n\nIn some theories of particle physics, even such basic structures as mass, space, and time are viewed as emergent phenomena, arising from more fundamental concepts such as the Higgs boson or strings. In some interpretations of quantum mechanics, the perception of a deterministic reality, in which all objects have a definite position, momentum, and so forth, is actually an emergent phenomenon, with the true state of matter being described instead by a wavefunction which need not have a single position or momentum.\nMost of the laws of physics themselves as we experience them today appear to have emerged during the course of time making emergence the most fundamental principle in the universe and raising the question of what might be the most fundamental law of physics from which all others emerged. Chemistry can in turn be viewed as an emergent property of the laws of physics. Biology (including biological evolution) can be viewed as an emergent property of the laws of chemistry. Similarly, psychology could be understood as an emergent property of neurobiological laws. Finally, free-market theories understand economy as an emergent feature of psychology.\n\nAccording to Laughlin (2005), for many particle systems, nothing can be calculated exactly from the microscopic equations, and macroscopic systems are characterised by broken symmetry: the symmetry present in the microscopic equations is not present in the macroscopic system, due to phase transitions. As a result, these macroscopic systems are described in their own terminology, and have properties that do not depend on many microscopic details. This does not mean that the microscopic interactions are irrelevant, but simply that you do not see them anymore — you only see a renormalized effect of them. Laughlin is a pragmatic theoretical physicist: if you cannot, possibly ever, calculate the broken symmetry macroscopic properties from the microscopic equations, then what is the point of talking about reducibility?\n\nLife is a major source of complexity, and evolution is the major process behind the varying forms of life. In this view, evolution is the process describing the growth of complexity in the natural world and in speaking of the emergence of complex living beings and life-forms, this view refers therefore to processes of sudden changes in evolution.\n\nLife is thought to have emerged in the early RNA world when RNA chains began to express the basic conditions necessary for natural selection to operate as conceived by Darwin: heritability, variation of type, and competition for limited resources. Fitness of an RNA replicator (its per capita rate of increase) would likely be a function of adaptive capacities that were intrinsic (in the sense that they were determined by the nucleotide sequence) and the availability of resources. The three primary adaptive capacities may have been (1) the capacity to replicate with moderate fidelity (giving rise to both heritability and variation of type); (2) the capacity to avoid decay; and (3) the capacity to acquire and process resources. These capacities would have been determined initially by the folded configurations of the RNA replicators (see “Ribozyme”) that, in turn, would be encoded in their individual nucleotide sequences. Competitive success among different replicators would have depended on the relative values of these adaptive capacities.\n\nRegarding causality in evolution Peter Corning observes:\nSynergistic effects of various kinds have played a major causal role in the evolutionary process generally and in the evolution of cooperation and complexity in particular... Natural selection is often portrayed as a “mechanism”, or is personified as a causal agency... In reality, the differential “selection” of a trait, or an adaptation, is a consequence of the functional effects it produces in relation to the survival and reproductive success of a given organism in a given environment. It is these functional effects that are ultimately responsible for the trans-generational continuities and changes in nature.\n\nPer his definition of emergence, Corning also addresses emergence and evolution:\n[In] evolutionary processes, causation is iterative; effects are also causes. And this is equally true of the synergistic effects produced by emergent systems. In other words, emergence itself... has been the underlying cause of the evolution of emergent phenomena in biological evolution; it is the synergies produced by organized systems that are the key.\n\nSwarming is a well-known behaviour in many animal species from marching locusts to schooling fish to flocking birds. Emergent structures are a common strategy found in many animal groups: colonies of ants, mounds built by termites, swarms of bees, shoals/schools of fish, flocks of birds, and herds/packs of mammals.\n\nAn example to consider in detail is an ant colony. The queen does not give direct orders and does not tell the ants what to do. Instead, each ant reacts to stimuli in the form of chemical scent from larvae, other ants, intruders, food and buildup of waste, and leaves behind a chemical trail, which, in turn, provides a stimulus to other ants. Here each ant is an autonomous unit that reacts depending only on its local environment and the genetically encoded rules for its variety of ant. Despite the lack of centralized decision making, ant colonies exhibit complex behavior and have even demonstrated the ability to solve geometric problems. For example, colonies routinely find the maximum distance from all colony entrances to dispose of dead bodies.\n\nIt appears that environmental factors may play a role in influencing emergence. Research suggests induced emergence of the bee species Macrotera portalis. In this species, the bees emerge in a pattern consistent with rainfall. Specifically, the pattern of emergence is consistent with southwestern deserts' late summer rains and lack of activity in the spring.\n\nA broader example of emergent properties in biology is viewed in the biological organisation of life, ranging from the subatomic level to the entire biosphere. For example, individual atoms can be combined to form molecules such as polypeptide chains, which in turn fold and refold to form proteins, which in turn create even more complex structures. These proteins, assuming their functional status from their spatial conformation, interact together and with other molecules to achieve higher biological functions and eventually create an organism. Another example is how cascade phenotype reactions, as detailed in chaos theory, arise from individual genes mutating respective positioning. At the highest level, all the biological communities in the world form the biosphere, where its human participants form societies, and the complex interactions of meta-social systems such as the stock market.\n\nAmong the considered phenomena in the evolutionary account of life, as a continuous history, marked by stages at which fundamentally new forms have appeared - the origin of sapiens intelligence. The emergence of mind and its evolution is researched and considered as a separate phenomenon in a special system knowledge noogenesis\n\nGroups of human beings, left free to each regulate themselves, tend to produce spontaneous order, rather than the meaningless chaos often feared. This has been observed in society at least since Chuang Tzu in ancient China. A classic traffic roundabout is a good example, with cars moving in and out with such effective organization that some modern cities have begun replacing stoplights at problem intersections with traffic circles , and getting better results. Open-source software and Wiki projects form an even more compelling illustration.\n\nEmergent processes or behaviors can be seen in many other places, such as cities, cabal and market-dominant minority phenomena in economics, organizational phenomena in computer simulations and cellular automata. Whenever there is a multitude of individuals interacting, an order emerges from disorder; a pattern, a decision, a structure, or a change in direction occurs.\n\nThe stock market (or any market for that matter) is an example of emergence on a grand scale. As a whole it precisely regulates the relative security prices of companies across the world, yet it has no leader; when no central planning is in place, there is no one entity which controls the workings of the entire market. Agents, or investors, have knowledge of only a limited number of companies within their portfolio, and must follow the regulatory rules of the market and analyse the transactions individually or in large groupings. Trends and patterns emerge which are studied intensively by technical analysts..\n\nThe World Wide Web is a popular example of a decentralized system exhibiting emergent properties. There is no central organization rationing the number of links, yet the number of links pointing to each page follows a power law in which a few pages are linked to many times and most pages are seldom linked to. A related property of the network of links in the World Wide Web is that almost any pair of pages can be connected to each other through a relatively short chain of links. Although relatively well known now, this property was initially unexpected in an unregulated network. It is shared with many other types of networks called small-world networks.\n\nInternet traffic can also exhibit some seemingly emergent properties. In the congestion control mechanism, TCP flows can become globally synchronized at bottlenecks, simultaneously increasing and then decreasing throughput in coordination. Congestion, widely regarded as a nuisance, is possibly an emergent property of the spreading of bottlenecks across a network in high traffic flows which can be considered as a phase transition [see review of related research in ].\n\nAnother important example of emergence in web-based systems is social bookmarking (also called collaborative tagging). In social bookmarking systems, users assign tags to resources shared with other users, which gives rise to a type of information organisation that emerges from this crowdsourcing process. Recent research which analyzes empirically the complex dynamics of such systems has shown that consensus on stable distributions and a simple form of shared vocabularies does indeed emerge, even in the absence of a central controlled vocabulary. Some believe that this could be because users who contribute tags all use the same language, and they share similar semantic structures underlying the choice of words. The convergence in social tags may therefore be interpreted as the emergence of structures as people who have similar semantic interpretation collaboratively index online information, a process called semantic imitation.\n\nEmergent structures appear at many different levels of organization or as spontaneous order. Emergent self-organization appears frequently in cities where no planning or zoning entity predetermines the layout of the city. The interdisciplinary study of emergent behaviors is not generally considered a homogeneous field, but divided across its application or problem domains.\n\nArchitects may not design all the pathways of a complex of buildings. Instead they might let usage patterns emerge and then place pavement where pathways have become worn, such as a desire path.\n\nThe on-course action and vehicle progression of the 2007 Urban Challenge could possibly be regarded as an example of cybernetic emergence. Patterns of road use, indeterministic obstacle clearance times, etc. will work together to form a complex emergent pattern that can not be deterministically planned in advance.\n\nThe architectural school of Christopher Alexander takes a deeper approach to emergence, attempting to rewrite the process of urban growth itself in order to affect form, establishing a new methodology of planning and design tied to traditional practices, an Emergent Urbanism. Urban emergence has also been linked to theories of urban complexity and urban evolution.\n\nBuilding ecology is a conceptual framework for understanding architecture and the built environment as the interface between the dynamically interdependent elements of buildings, their occupants, and the larger environment. Rather than viewing buildings as inanimate or static objects, building ecologist Hal Levin views them as interfaces or intersecting domains of living and non-living systems. The microbial ecology of the indoor environment is strongly dependent on the building materials, occupants, contents, environmental context and the indoor and outdoor climate. The strong relationship between atmospheric chemistry and indoor air quality and the chemical reactions occurring indoors. The chemicals may be nutrients, neutral or biocides for the microbial organisms. The microbes produce chemicals that affect the building materials and occupant health and well being. Humans manipulate the ventilation, temperature and humidity to achieve comfort with the concomitant effects on the microbes that populate and evolve.\n\nEric Bonabeau's attempt to define emergent phenomena is through traffic: \"traffic jams are actually very complicated and mysterious. On an individual level, each driver is trying to get somewhere and is following (or breaking) certain rules, some legal (the speed limit) and others societal or personal (slow down to let another driver change into your lane). But a traffic jam is a separate and distinct entity that emerges from those individual behaviors. Gridlock on a highway, for example, can travel backward for no apparent reason, even as the cars are moving forward.\" He has also likened emergent phenomena to the analysis of market trends and employee behavior.\n\nComputational emergent phenomena have also been utilized in architectural design processes, for example for formal explorations and experiments in digital materiality.\n\nSome artificially intelligent (AI) computer applications utilize emergent behavior for animation. One example is Boids, which mimics the swarming behavior of birds.\n\nIt has been argued that the structure and regularity of language grammar, or at least language change, is an emergent phenomenon . While each speaker merely tries to reach his or her own communicative goals, he or she uses language in a particular way. If enough speakers behave in that way, language is changed . In a wider sense, the norms of a language, i.e. the linguistic conventions of its speech society, can be seen as a system emerging from long-time participation in communicative problem-solving in various social circumstances .\n\nWithin the field of group facilitation and organization development, there have been a number of new group processes that are designed to maximize emergence and self-organization, by offering a minimal set of effective initial conditions. Examples of these processes include SEED-SCALE, Appreciative Inquiry, Future Search, the World Cafe or Knowledge Cafe, Open Space Technology, and others (Holman, 2010).\n\n\n\n"}
{"id": "47451366", "url": "https://en.wikipedia.org/wiki?curid=47451366", "title": "Essential monomorphism", "text": "Essential monomorphism\n\nIn mathematics, specifically category theory, an essential monomorphism is a monomorphism \"f\" in a category \"C\" such that for a morphism \"g\" in \"C\", formula_1 is a monomorphism only when \"g\" is a monomorphism. Essential monomorphisms in a category of modules are those whose image is an essential submodule of the codomain. An injective hull of an object \"X\" is an essential monomorphism from \"X\" to an injective object.\n"}
{"id": "39605149", "url": "https://en.wikipedia.org/wiki?curid=39605149", "title": "Fractional-order system", "text": "Fractional-order system\n\nIn the fields of dynamical systems and control theory, a fractional-order system is a dynamical system that can be modeled by a fractional differential equation containing derivatives of non-integer order. Such systems are said to have \"fractional dynamics\". Derivatives and integrals of fractional orders are used to describe objects that can be characterized by power-law nonlocality, power-law long-range dependence or fractal properties. Fractional-order systems are useful in studying the anomalous behavior of dynamical systems in physics, electrochemistry, biology, viscoelasticity and chaotic systems.\n\nA general dynamical system of fractional order can be written in the form\n\nwhere formula_2 and formula_3 are functions of the fractional derivative operator formula_4 of orders formula_5 and formula_6 and formula_7 and formula_8 are functions of time. A common special case of this is the linear time-invariant (LTI) system in one variable:\n\nThe orders formula_10 and formula_11 are in general complex quantities, but two interesting cases are when the orders are \"commensurate\"\n\nand when they are also \"rational\":\n\nWhen formula_14, the derivatives are of integer order and the system becomes an ordinary differential equation. Thus by increasing specialization, LTI systems can be of general order, commensurate order, rational order or integer order.\n\nBy applying a Laplace transform to the LTI system above, the transfer function becomes\n\nFor general orders formula_10 and formula_11 this is a non-rational transfer function. Non-rational transfer functions cannot be written as an expansion in a finite number of terms (e.g., a binomial expansion would have an infinite number of terms) and in this sense fractional orders systems can be said to have the potential for unlimited memory.\n\nExponential laws are classical approach to study dynamics of population densities, but there are many systems where dynamics undergo faster or slower-than-exponential laws. In such case the anomalous changes in dynamics may be best described by Mittag-Leffler functions.\n\nAnomalous diffusion is one more dynamic system where fractional-order systems play significant role to describe the anomalous flow in the diffusion process.\n\nViscoelasticity is the property of material in which the material exhibits its nature between purely elastic and pure fluid. In case of real materials the relationship between stress and strain given by Hooke's law and Newton's law both have obvious disadvances. So G. W. Scott Blair introduced a new relationship between stress and strain given by\n\nIn chaos theory, it has been observed that chaos occurs in dynamical systems of order 3 or more. With the introduction of fractional-order systems, some researchers study chaos in the system of total order less than 3.\n\nConsider a fractional-order initial value problem:\n\nHere, under the continuity condition on function f, one can convert the above equation into corresponding integral equation.\n\nOne can construct a solution space and define, by that equation, a continuous self-map on the solution space, then apply a fixed-point theorem, to get a fixed-point, which is the solution of above equation.\n\nFor numerical simulation of solution of the above equations, Kai Diethelm has suggested fractional linear multistep Adams–Bashforth method or quadrature methods.\n\n\n\n"}
{"id": "636147", "url": "https://en.wikipedia.org/wiki?curid=636147", "title": "General will", "text": "General will\n\nIn political philosophy, the general will () is the will of the people as a whole. The term was made famous by 18th-century French philosopher Jean-Jacques Rousseau.\n\nThe phrase \"general will,\" as Rousseau used it, occurs in Article Six of the \"Declaration of the Rights of Man and the Citizen\" (French: \"Déclaration des droits de l'Homme et du citoyen\"), composed in 1789 during the French Revolution:The law is the expression of the general will. All citizens have the right to contribute personally, or through their representatives, to its formation. It must be the same for all, whether it protects or punishes. All citizens, being equal in its eyes, are equally admissible to all public dignities, positions, and employments, according to their capacities, and without any other distinction than that of their virtues and their talents.\n\nJames Swenson writes: To my knowledge, the only time Rousseau actually uses the formulation \"expression of the general will\" is in a passage of the \"Discours sur l'économie politique\", whose content renders it little susceptible of celebrity. [...] But it is indeed a faithful summary of his doctrine, faithful enough that commentators frequently adopt it without any hesitation. Among Rousseau's definitions of law, the textually closest variant can be found in a passage of the \"Lettres écrites de la montagne\" summarizing the argument of \"Du contrat social\", in which law is defined as \"a public and solemn declaration of the general will on an object of common interest.\"\n\nAs used by Rousseau, the \"general will\" is considered by some identical to the rule of law, and to Spinoza's \"mens una\".\n\nThe notion of the general will is wholly central to Rousseau's theory of political legitimacy. [...] It is, however, an unfortunately obscure and controversial notion. Some commentators see it as no more than the dictatorship of the proletariat or the tyranny of the urban poor (such as may perhaps be seen in the French Revolution). Such was not Rousseau's meaning. This is clear from the \"Discourse on Political Economy\", where Rousseau emphasizes that the general will exists to protect individuals against the mass, not to require them to be sacrificed to it. He is, of course, sharply aware that men have selfish and sectional interests which will lead them to try to oppress others. It is for this reason that loyalty to the good of all alike must be a supreme (although not exclusive) commitment by everyone, not only if a truly general will is to be heeded but also if it is to be formulated successfully in the first place\".\n\nEarly critics of Rousseau included Benjamin Constant and Georg Wilhelm Friedrich Hegel. Hegel argued that, because it lacked any grounding in an objective ideal of reason, Rousseau's account of the general will ineluctably lead to the Reign of Terror. Constant also blamed Rousseau for the excesses of the French Revolution, and he rejected the total subordination of the citizen-subjects to the determinations of the general will.\n\nIn 1952 Jacob Talmon characterized Rousseau's \"general will\" as leading to a totalitarian democracy because, Talmon argued, the state subjected its citizens to the supposedly infallible will of the majority. Another writer of the period, liberal theorist Karl Popper, also interpreted Rousseau in this way, while Bertrand Russell warned that \"the doctrine of general will ... made possible the mystic identification of a leader with its people, which has no need of confirmation by so mundane an apparatus as the ballot box.\" Other prominent critics include Isaiah Berlin who argued that Rousseau's association of freedom with obedience to the General Will allowed totalitarian leaders to defend oppression in the name of freedom, and made Rousseau \"one of the most sinister and formidable enemies of liberty in the whole history of human thought.\"\n\nSome Rousseau scholars, however, such as his biographer and editor Maurice Cranston, and Ralph Leigh, editor of Rousseau's correspondence, do not consider Talmon's 1950s \"totalitarian thesis\" as sustainable.\n\nSupporters of Rousseau argued that Rousseau was not alone among republican political theorists in thinking that small, homogeneous states were best suited to maintaining the freedom of their citizens. Montesquieu and Machiavelli were also of this opinion. Furthermore, Rousseau envisioned his \"Social Contract\" as part of a projected larger work on political philosophy, which would have dealt with issues in larger states. Some of his later writings, such as his \"Discourse on Political Economy\", his proposals for a Constitution of Poland, and his essay on maintaining perpetual peace, in which he recommends a federated European Union, gave an idea of the future direction of his thought.\n\nHis defenders also argued Rousseau is one of the great prose stylists and because of his penchant for the paradoxical effect obtained by stating something strongly and then going on to qualify or negate it, it is easy to misrepresent his ideas by taking them out of context.\n\nRousseau was also a great synthesizer who was deeply engaged in a dialog with his contemporaries and with the writers of the past, such as the theorists of Natural Law, Hobbes and Grotius. Like \"the body politic\", \"the general will\" was a term of art and was not invented by Rousseau, though admittedly Rousseau did not always go out of his way to explicitly acknowledge his debt to the jurists and theologians who influenced him. Prior to Rousseau, the phrase \"general will\" referred explicitly to the general (as opposed to the particular) will or \"volition\" (as it is sometimes translated) of the Deity. It occurs in the theological writings of Malebranche, who had picked it up from Pascal, and in the writings of Malebranche's pupil, Montesquieu, who contrasted \"volonté particulière\" and \"volonté générale\" in a secular sense in his most celebrated chapter (Chapter XI) of \"De L'Esprit des Lois\" (1748). In his \"Discourse on Political Economy\", Rousseau explicitly credits Diderot's \"Encyclopédie\" article \"Droit Naturel\" as the source of \"the luminous concept\" of the general will, of which he maintains his own thoughts are simply a development. Montesquieu, Diderot, and Rousseau's innovation was to use the term in a secular rather than theological sense.\n\nDiderot on the General Will [emphasis added]:\nEVERYTHING you conceive, everything you contemplate, will be good, great, elevated, sublime, if it accords with \"the general and common interest\". There is no quality essential to your species apart from that which you demand from all your fellow men to ensure your happiness and theirs . . . . [D]o not ever lose sight of it, or else you will find that your comprehension of the notions of goodness, justice, humanity and virtue grow dim. Say to yourself often, “I am a man, and I have no other truly inalienable natural rights except those of humanity.” \nBut, you will ask, in what does this general will reside? Where can I consult it? [...] [The answer is:] In the principles of prescribed law of all civilized nations, in the social practices of savage and barbarous peoples; in the tacit agreements obtaining amongst the enemies of mankind; and even in those two emotions — indignation and resentment — which nature has extended as far as animals to compensate for social laws and public retributions. --Denis Diderot, “\"Droit Naturel\"” article in the \"Encyclopédie\".\nRousseau on the General Will [emphasis added]:AS long as several men assembled together consider themselves as a single body, they have only \"one will\" which is directed towards their common preservation and general well-being. Then, all the animating forces of the state are vigorous and simple, and its principles are clear and luminous; it has no incompatible or conflicting interests; the \"common good\" makes itself so manifestly evident that only common sense is needed to discern it. Peace, unity and equality are the enemies of political sophistication. Upright and simple men are difficult to deceive precisely because of their simplicity; stratagems and clever arguments do not prevail upon them, they are not indeed subtle enough to be dupes. When we see among the happiest people in the world bands of peasants regulating the affairs of state under an oak tree, and always acting wisely, can we help feeling a certain contempt for the refinements of other nations, which employ so much skill and effort to make themselves at once illustrious and wretched?\nA state thus governed needs very few laws [...]\nHowever, when the social tie begins to slacken and the state to weaken, when particular interests begin to make themselves felt and sectional societies begin to exert an influence over the greater society, the \"common interest\" then becomes corrupted and meets opposition, voting is no longer unanimous; the general will is no longer the will of all; contradictions and disputes arise, and even the best opinion is not allowed to prevail unchallenged.\"For this reason the sensible rule for regulating public assemblies is one intended not so much to uphold the general will there, as to ensure that it is always questioned and always responds.\n\n"}
{"id": "23534467", "url": "https://en.wikipedia.org/wiki?curid=23534467", "title": "Grammatical mood", "text": "Grammatical mood\n\nIn linguistics, grammatical mood (also mode) is a grammatical feature of verbs, used for signaling modality.\nThat is, it is the use of verbal inflections that allow speakers to express their attitude toward what they are saying (e.g. a statement of fact, of desire, of command, etc.). The term is also used more broadly to describe the syntactic expression of modality; that is, the use of verb phrases that do not involve inflexion of the verb itself.\n\nMood is distinct from grammatical tense or grammatical aspect, although the same word patterns are used for expressing more than one of these meanings at the same time in many languages, including English and most other modern Indo-European languages. (See tense–aspect–mood for a discussion of this.)\n\nSome examples of moods are indicative, interrogative, imperative, subjunctive, injunctive, optative, and potential. These are all finite forms of the verb. Infinitives, gerunds, and participles, which are non-finite forms of the verb, are not considered to be examples of moods.\n\nSome Uralic Samoyedic languages have more than ten moods; Nenets has as many as sixteen. The original Indo-European inventory of moods consisted of indicative, subjunctive, optative, and imperative. Not every Indo-European language has all of these moods, but the most conservative ones such as Avestan, Ancient Greek, and Sanskrit have them all. English has indicative, imperative, and subjunctive moods; other moods, such as the conditional, do not appear as morphologically distinct forms.\n\nNot all of the moods listed below are clearly conceptually distinct. Individual terminology varies from language to language, and the coverage of (e.g.) the \"conditional\" mood in one language may largely overlap with that of the \"hypothetical\" or \"potential\" mood in another. Even when two different moods exist in the same language, their respective usages may blur, or may be defined by syntactic rather than semantic criteria. For example, the subjunctive and optative moods in Ancient Greek alternate syntactically in many subordinate clauses, depending on the tense of the main verb. The usage of the indicative, subjunctive, and jussive moods in Classical Arabic is almost completely controlled by syntactic context. The only possible alternation in the same context is between indicative and jussive following the negative particle \"lā\".\n\nRealis moods are a category of grammatical moods that indicate that something is actually the case or actually not the case. The most common realis mood is the indicative mood. Some languages have a distinct generic mood for expressing general truths. For other realis moods, see the main Realis mood article.\n\nThe indicative mood, or evidential mood, is used for factual statements and positive beliefs. It is the mood of reality. The indicative mood is the most commonly used mood and is found in all languages. Example: \"Paul is eating an apple\" or \"John eats apples\". All intentions that a particular language does not categorize as another mood are classified as indicative.\n\nIrrealis moods are the set of grammatical moods that indicate that something is not actually the case or a certain situation or action is not known to have happened. They are any verb or sentence mood that are not \"realis\" moods. They may be part of expressions of necessity, possibility, requirement, wish or desire, fear, or as part of counterfactual reasonings, etc.\n\n\"Irrealis\" verb forms are used when speaking of an event which has not happened, is not likely to happen, or is otherwise far removed from the real course of events. For example, in the sentence \"If you had done your homework, you wouldn't have failed the class\", \"had done\" is an \"irrealis\" verb form. \n\nSome languages have distinct grammatical forms that indicate that the event described by a specific verb is an \"irrealis verb\". Many of the Indo-European languages preserve a subjunctive mood that functions as an \"irrealis\". Some also preserve an optative mood that describes events that are wished for or hoped for but not factual.\n\nCommon \"irrealis\" moods are the imperative, the conditional, the subjunctive, the optative, the jussive, and the potential. For other examples, see the main article for each respective mood.\n\nThe subjunctive mood, sometimes called conjunctive mood, has several uses in dependent clauses. Examples include discussing imaginary or hypothetical events and situations, expressing opinions or emotions, or making polite requests (the exact scope is language-specific). A subjunctive mood exists in English, though it is used in English much less than in many other Indo-European languages. In English, this mood has, for some uses, become something of a linguistic fossil. An example of the subjunctive mood is \"I suggest that Paul \"eat\" an apple\". The sentence refers to an event which may or may not take place. Contrast this with the indicative verb of the sentence \"Paul will eat an apple\", in which the verb \"will eat\" states an unambiguous fact. Another way of expressing the suggestion is \"I suggest that Paul should eat an apple\".\n\nOther uses of the subjunctive in English are archaisms, as in \"And if he be not able to bring a lamb, then he shall bring for his trespass...\" (KJV Leviticus 5:7). Statements such as \"I will ensure that he leave immediately\" often sound archaic or overly formal, and have been almost completely supplanted by constructions with the indicative, like \"I will ensure that he leaves immediately\".\n\nSome Germanic languages distinguish between two types of subjunctive moods, for example, the \"Konjunktiv I\" and \"II\" in German or the \"present\" and \"past subjunctive\" in English. Note that the latter distinction is not about the actual time at which something happens (or does not happen).\n\nThe conditional version of “John eats if he is hungry” is (subjunctive part boldfaced):\n\nThe subjunctive mood figures prominently in the grammar of the Romance languages, which require this mood for certain types of dependent clauses. This point commonly causes difficulty for English speakers learning these languages.\n\nIn certain other languages, the dubitative or the conditional moods may be employed instead of the subjunctive in referring to doubtful or unlikely events (see the main article).\n\nThe conditional mood is used for speaking of an event whose realization is dependent upon another condition, particularly, but not exclusively, in conditional sentences. In Modern English, this type of modality is expressed via a periphrastic construction, with the form \"would\" + infinitive, (e.g. \"I would buy\"), and thus is a mood only in the broad sense and not in the more common narrow sense of the term \"mood\". In other languages, verbs have a specific conditional inflection. In German, the conditional mood is identical to one of the two subjunctive moods \"(Konjunktiv II,\" see above).\n\nThe conditional version of \"John eats if he is hungry\" is (conditional part boldfaced):\n\nIn the Romance languages, the conditional form is used primarily in the apodosis (main clause) of conditional clauses, and in a few set phrases where it expresses courtesy or doubt. The main verb in the protasis (dependent clause) is usually in the subjunctive or in the indicative mood. However, this is not a universal trait: among others in German (as above), Finnish and Romanian (even though the last is a Romance language), the conditional mood is used in both the apodosis and the protasis. A further example is the sentence \"I would buy a house if I earned a lot of money\", where in Finnish both clauses have the conditional marker \"-isi-\": \"Ostaisin talon, jos ansaitsisin paljon rahaa\". In Polish (as well as in eastern Slavic languages) the conditional marker \"-by\" also appears twice: \"Kupiłbym dom, gdybym zarabiał dużo pieniędzy\".\n\nBecause English is used as a lingua franca, a common error among second-language speakers is to use \"would\" in both clauses, e.g. *\"I would buy if I would earn...\". \"Would\" can, however, correctly be used after \"if\" in sentences such as \"If you would only tell me what is troubling you, I might be able to help\" (i.e. \"if you were willing to tell me...\").\n\nThe optative mood expresses hopes, wishes or commands and has other uses that may overlap with the subjunctive mood. Few languages have an optative as a distinct mood; some that do are Albanian, Ancient Greek, Kazakh, Japanese, Finnish, Nepali, and Sanskrit.\n\nThe imperative mood expresses direct commands, prohibitions, and requests. In many circumstances, using the imperative mood may sound blunt or even rude, so it is often used with care. Example: \"Paul, do your homework now\". An imperative is used for telling someone to do something without argument. Many languages, including English, use the bare verb stem to form the imperative (such as \"go\", \"run\", \"do\"). Other languages, such as Seri and Latin, however, use special imperative forms. In English, the second person is implied by the imperative except when first-person plural is specified, as in \"Let's go\" (\"Let us go\"). In Romance languages a first person plural exists in the imperative mood: Spanish: \"Vayamos a la playa\"; French: \"Allons à la plage\" (both meaning: Let us go to the beach). The prohibitive mood, the negative imperative may be grammatically or morphologically different from the imperative mood in some languages. It indicates that the action of the verb is not permitted, e.g. \"Don't you go!\" In English, the imperative is sometimes used for forming a conditional sentence: e.g. \"go eastwards a mile, and you'll see it\" means \"if you go eastwards a mile, you will see it\".\n\nThe jussive, similarly to the imperative, expresses orders, commands, exhortations, but particularly to a third person not present. An imperative, in contrast, generally applies to the listener. When a language is said to have a jussive, the jussive forms are different from the imperative ones, but may be the same as the forms called \"subjunctive\" in that language. Latin is an example where the jussive is simply about certain specific uses of the subjunctive. Arabic, however, is an example of a language with distinct subjunctive, imperative and jussive conjugations.\n\nThe potential mood is a mood of probability indicating that, in the opinion of the speaker, the action or occurrence is considered likely. It is used in Finnish, Japanese, in Sanskrit, and in the Sami languages. (In Japanese it is often called something like tentative, since potential is used for referring to a voice indicating capability to perform the action.)\n\nIn Finnish, it is mostly a literary device, as it has virtually disappeared from daily spoken language in most dialects. Its affix is \"-ne-\", as in *\"men\" + \"ne\" + \"e\" → \"mennee\" \"(she/he/it) will probably go\". \nIn English, it is formed by means of the auxiliaries \"may\", \"can\", \"ought\", and \"must\": \"She may go.\".\n\nA few languages use a hypothetical mood, which is used in sentences such as \"you could have cut yourself\", representing something that might have happened but did not.\n\nThe inferential mood is used to report unwitnessed events without confirming them. Often, there is no doubt as to the veracity of the statement (for example, if it were on the news), but simply the fact that the speaker was not personally present at the event forces them to use this mood.\n\nIn the Balkan languages, the same forms used for the inferential mood also function as admiratives. When referring to Balkan languages, it is of<dfn>ten called</dfn> renarrative mood; when referring to Estonian, it is called oblique mood.\n\nThe inferential is usually impossible to be distinguishably translated into English. For instance, indicative Bulgarian \"той отиде (toy otide)\" and Turkish \"o gitti\" will be translated the same as inferential \"той отишъл (toy otishal)\" and \"o gitmiş\" — with the English indicative \"he went\". Using the first pair, however, implies very strongly that the speaker either witnessed the event or is very sure that it took place. The second pair implies either that the speaker did not in fact witness it take place, that it occurred in the remote past or that there is considerable doubt as to whether it actually happened. If it were necessary to make the distinction, then the English constructions \"he must have gone\" or \"he is said to have gone\" would partly translate the inferential.\n\nThe interrogative (or interrogatory) mood is used for asking questions. In English, questions are considered interrogative. Most other languages do not have a special mood for asking questions, but exceptions include Welsh, Nenets and Eskimo languages such as Greenlandic.\n\nLinguistics also differentiate moods into two parental categories that include deontic mood and epistemic mood. Deontic mood describes whether one could or should be able to do something. An example of deontic mood is: She should/may start. On the other hand, epistemic mood describes the chance or possibility of something happening. This would then change our example to: She may have started. To further explain modality, linguists introduce weak mood. A weak deontic mood describes how a course of action is not recommended or is frowned upon. A weak epistemic mood includes the terms perhaps and possibly.\n\nPingelapese is a Micronesian language spoken on the Pingelap atoll and on two of the eastern Caroline Islands, called the high island of Pohnpei. \"e\" and \"ae\" are auxiliary verbs found in Pingelapese. Though seemingly interchangeable, e and \"ae\" are separate phonemes and have different uses. A Pingelapese speaker would choose to use \"e\" when they have a high degree of certainty in what they are saying and \"ae\" when they are less certain. This therefore illustrates that \"e\" and \"ae\" are mood indicators. They have no effect on the direct translation of a sentence, but they are used to alter the mood of the sentence spoken. The following example shows the difference between \"e\" and \"ae\" when applied in the same sentence.\n\n\"Ngaei rong pwa Soahn e laid.\"\n\n‘I heard that John was fishing (I am certain about it).’\n\n\" Ngaei rong pwa Soahn ae laid.\"\n\n‘I heard that John was fishing (but I am not certain about it).’\n\nThe use of \"ae\" instead of \"e\" can also indicate an interrogative sentence. This is a form of non-declarative speech that demonstrates the speaker has no commitment to the statement they are saying. The following sentence is an example.\n\nSoahn ae laid?\n\n‘Does John fish?’\n\nThe language we know as Reo Rapa was not created by the combination of 2 languages, but through the introduction of Tahitian to the Rapa monolingual community. Old Rapa words are still used for the grammar and structure of the sentence of phrase but most common context words were replaced with Tahitian. The Reo Rapa language uses TAM (Tense - Aspect - Mood) in their sentence structure such as the Impertective TAM marker \"/e/\" and the Imperative TAM marker \"/a/\".\n\nFor example:\n\nMortlockese is an Austronesian language made up of eleven dialects over the eleven atolls that make up the Mortlock Islands in Micronesia. Various TAM markers are used in the language. Mood markers include the past tense hortative (marking encouragement or to urge) \"aa\", the hortative \"kɞ\" which denotes a polite tone, \"min\" or \"tin\" to stress the importance of something, and the word \"tɞ\" to denote warning or caution. Each of these markers is used in conjunction with the subject proclitics except for the \"aa\" marker. \n\n\nFrom SIL International:\n"}
{"id": "7421532", "url": "https://en.wikipedia.org/wiki?curid=7421532", "title": "Grasscycling", "text": "Grasscycling\n\nGrasscycling refers to an aerobic (requires air) method of handling grass clippings by leaving them on the lawn when mowing.\n\nThe term is a portmanteau combining \"grass\" and \"recycling\", and had come into use by at least 1990 as part of the push to reduce the huge quantities of clippings going into landfills, up to half of some cities' summertime waste flow, as 1,000 square feet (93 m) of lawn can produce 200 to 500 pounds (90 to 225 kg) of clippings a year.\n\nBecause grass consists largely of water (80% or more), contains little lignin, and has high nitrogen content, grass clippings easily break down during an aerobic process (comparable to composting) and returns the decomposed clippings to the soil within one to two weeks, acting primarily as a fertilizer supplement and, to a much smaller degree, a mulch. Grasscycling can provide 15 to 20% or more of a lawn's yearly nitrogen requirements. Proponents also note that grasscycling reduces the use of plastic bags for collecting yard waste and reduces trips to the curb or landfill to haul waste.\n\nOptimal grasscycle techniques include:\n\nAlthough a mulching mower can make grass clippings smaller, one is not necessary for grasscycling.\n\n\n"}
{"id": "39908986", "url": "https://en.wikipedia.org/wiki?curid=39908986", "title": "Ingemar Lundquist", "text": "Ingemar Lundquist\n\nIngemar Henry Lundquist (born in Stockholm, Sweden, October 19, 1921, died in Carmel Valley Village, California, February 25, 2007) was a prolific inventor and mechanical engineer.\n\nLundquist graduated from the Stockholm Institute of Technology in 1945 with a mechanical engineering degree. He migrated to the United States in 1948 and became an American citizen in 1950.\n\nHe worked for various medical technology companies in the San Francisco Bay Area, including Advanced Cardiovascular Systems and E.P. Technologies.\n\nLundquist had hundreds of inventions, typically working in his garage or basement. He held more than a hundred patents. His inventions included over the wire balloon angioplasty, T.U.N.A., and somnoplasty. He also worked on cardiac stem-cell therapy.\n\n"}
{"id": "855980", "url": "https://en.wikipedia.org/wiki?curid=855980", "title": "Just-world hypothesis", "text": "Just-world hypothesis\n\nThe just-world hypothesis or just-world fallacy is the cognitive bias (or assumption) that a person's actions are inherently inclined to bring morally fair and fitting consequences to that person, to the end of all noble actions being eventually rewarded and all evil actions eventually punished. In other words, the just-world hypothesis is the tendency to attribute consequences to—or expect consequences as the result of—a universal force that restores moral balance. This belief generally implies the existence of cosmic justice, destiny, divine providence, desert, stability, or order, and has high potential to result in fallacy, especially when used to rationalize people's misfortune on the grounds that they \"deserve\" it.\n\nThe hypothesis popularly appears in the English language in various figures of speech that imply guaranteed negative reprisal, such as: \"you got what was coming to you\", \"what goes around comes around\", \"chickens come home to roost\", \"everything happens for a reason\", and \"you reap what you sow\". This hypothesis has been widely studied by social psychologists since Melvin J. Lerner conducted seminal work on the belief in a just world in the early 1960s. Research has continued since then, examining the predictive capacity of the hypothesis in various situations and across cultures, and clarifying and expanding the theoretical understandings of just-world beliefs.\n\nMany philosophers and social theorists have observed and considered the phenomenon of belief in a just world, going back to at least as early as the Pyrrhonist philosopher Sextus Empiricus writing around 180 CE who argued against this belief. Lerner's work made the just-world hypothesis a focus of research in the field of social psychology.\n\nLerner was prompted to study justice beliefs and the just-world hypothesis in the context of social psychological inquiry into negative social and societal interactions. Lerner saw his work as extending Stanley Milgram's work on obedience. He sought to answer the questions of how regimes that cause cruelty and suffering maintain popular support, and how people come to accept social norms and laws that produce misery and suffering.\n\nLerner's inquiry was influenced by repeatedly witnessing the tendency of observers to blame victims for their suffering. During his clinical training as a psychologist, he observed treatment of mentally ill persons by the health care practitioners with whom he worked. Although he knew them to be kindhearted, educated people, they often blamed patients for the patients' own suffering. Lerner also describes his surprise at hearing his students derogate (disparage, belittle) the poor, seemingly oblivious to the structural forces that contribute to poverty. In a study on rewards, he observed that when one of two men was chosen at random to receive a reward for a task, that caused him to be more favorably evaluated by observers, even when the observers had been informed that the recipient of the reward was chosen at random. Existing social psychological theories, including cognitive dissonance, could not fully explain these phenomena. The desire to understand the processes that caused these phenomena led Lerner to conduct his first experiments on what is now called the just-world hypothesis.\n\nIn 1966, Lerner and his colleagues began a series of experiments that used shock paradigms to investigate observer responses to victimization. In the first of these experiments conducted at the University of Kansas, 72 female subjects were made to watch a confederate receiving electrical shocks under a variety of conditions. Initially, subjects were upset by observing the apparent suffering. But as the suffering continued and observers remained unable to intervene, the observers began to derogate the victim. Derogation was greater when the observed suffering was greater. But when subjects were told the victim would receive compensation for her suffering, subjects did not derogate the victim. Lerner and colleagues replicated these findings in subsequent studies, as did other researchers.\n\nTo explain these studies' findings, Lerner theorized that there was a prevalent belief in a just world. A just world is one in which actions and conditions have predictable, appropriate consequences. These actions and conditions are typically individuals' behaviors or attributes. The specific conditions that correspond to certain consequences are socially determined by a society's norms and ideologies. Lerner presents the belief in a just world as functional: it maintains the idea that one can influence the world in a predictable way. Belief in a just world functions as a sort of \"contract\" with the world regarding the consequences of behavior. This allows people to plan for the future and engage in effective, goal-driven behavior. Lerner summarized his findings and his theoretical work in his 1980 monograph \"The Belief in a Just World: A Fundamental Delusion\".\n\nLerner hypothesized that the belief in a just world is crucially important for people to maintain for their own well-being. But people are confronted daily with evidence that the world is not just: people suffer without apparent cause. Lerner explained that people use strategies to eliminate threats to their belief in a just world. These strategies can be rational or irrational. Rational strategies include accepting the reality of injustice, trying to prevent injustice or provide restitution, and accepting one's own limitations. Non-rational strategies include denial, withdrawal, and reinterpretation of the event.\n\nThere are a few modes of reinterpretation that could make an event fit the belief in a just world. One can reinterpret the outcome, the cause, and/or the character of the victim. In the case of observing the injustice of the suffering of innocent people, one major way to rearrange the cognition of an event is to interpret the victim of suffering as deserving. Specifically, observers can blame victims for their suffering on the basis of their behaviors and/or their characteristics. Much psychological research on the belief in a just world has focused on these negative social phenomena of victim blaming and victim derogation in different contexts.\n\nAn additional effect of this thinking is that individuals experience less personal vulnerability because they do not believe they have done anything to deserve or cause negative outcomes. This is related to the self-serving bias observed by social psychologists.\n\nMany researchers have interpreted just-world beliefs as an example of causal attribution. In victim blaming, the causes of victimization are attributed to an individual rather than to a situation. Thus, the consequences of belief in a just world may be related to or explained in terms of particular patterns of causal attribution.\n\nOthers have suggested alternative explanations for the derogation of victims. One suggestion is that derogation effects are based on accurate judgments of a victim's character. In particular, in relation to Lerner's first studies, some have hypothesized that it would be logical for observers to derogate an individual who would allow himself to be shocked without reason. A subsequent study by Lerner challenged this alternative hypothesis by showing that individuals are only derogated when they actually suffer; individuals who agreed to undergo suffering but did not were viewed positively.\n\nAnother alternative explanation offered for the derogation of victims early in the development of the just-world hypothesis was that observers derogate victims to reduce their own feelings of guilt. Observers may feel responsible, or guilty, for a victim's suffering if they themselves are involved in the situation or experiment. In order to reduce the guilt, they may devalue the victim. Lerner and colleagues claim that there has not been adequate evidence to support this interpretation. They conducted one study that found derogation of victims occurred even by observers who were not implicated in the process of the experiment and thus had no reason to feel guilty.\n\nAlternatively, victim derogation and other strategies may only be ways to alleviate discomfort after viewing suffering. This would mean that the primary motivation is not to restore a belief in a just world, but to reduce discomfort caused by empathizing. Studies have shown that victim derogation does not suppress subsequent helping activity and that empathizing with the victim plays a large role when assigning blame. According to Ervin Staub, devaluing the victim should lead to lesser compensation if restoring belief in a just world was the primary motive; instead, there is virtually no difference in compensation amounts whether the compensation precedes or follows devaluation. Psychopathy has been linked to the lack of just-world maintaining strategies, possibly due to dampened emotional reactions and lack of empathy.\n\nAfter Lerner's first studies, other researchers replicated these findings in other settings in which individuals are victimized. This work, which began in the 1970s and continues today, has investigated how observers react to victims of random calamities like traffic accidents, as well as rape and domestic violence, illnesses, and poverty. Generally, researchers have found that observers of the suffering of innocent victims tend to both derogate and blame victims for their suffering. Observers thus maintain their belief in a just world by changing their cognitions about the victims' character.\n\nIn the early 1970s, social psychologists Zick Rubin and Letitia Anne Peplau developed a measure of belief in a just world. This measure and its revised form published in 1975 allowed for the study of individual differences in just-world beliefs. Much of the subsequent research on the just-world hypothesis used these measurement scales.\n\nResearchers have looked at how observers react to victims of rape and other violence. In a formative experiment on rape and belief in a just world by Linda Carli and colleagues, researchers gave two groups of subjects a narrative about interactions between a man and a woman. The description of the interaction was the same until the end; one group received a narrative that had a neutral ending and the other group received a narrative that ended with the man raping the woman. Subjects judged the rape ending as inevitable and blamed the woman in the narrative for the rape on the basis of her behavior, but not her characteristics. These findings have been replicated repeatedly, including using a rape ending and a 'happy ending' (a marriage proposal).\n\nOther researchers have found a similar phenomenon for judgments of battered partners. One study found that observers' labels of blame of female victims of relationship violence increase with the intimacy of the relationship. Observers blamed the perpetrator only in the most significant case of violence, in which a male struck an acquaintance.\n\nResearchers have employed the just-world hypothesis to understand bullying. Given other research on beliefs in a just world, it would be expected that observers would derogate and blame bullying victims, but the opposite has been found: individuals high in just-world belief have stronger anti-bullying attitudes. Other researchers have found that strong belief in a just world is associated with lower levels of bullying behavior. This finding is in keeping with Lerner's understanding of belief in a just world as functioning as a \"contract\" that governs behavior. There is additional evidence that belief in a just world is protective of the well-being of children and adolescents in the school environment, as has been shown for the general population.\n\nOther researchers have found that observers judge sick people as responsible for their illnesses. One experiment showed that persons suffering from a variety of illnesses were derogated on a measure of attractiveness more than healthy individuals were. In comparison to healthy people, victim derogation was found for persons presenting with indigestion, pneumonia, and stomach cancer. Moreover, derogation was found to be higher for those suffering from more severe illnesses, except for those presenting with cancer. Stronger belief in a just world has also been found to correlate with greater derogation of AIDS victims.\n\nMore recently, researchers have explored how people react to poverty through the lens of the just-world hypothesis. Strong belief in a just world is associated with blaming the poor, with weak belief in a just world associated with identifying external causes of poverty including world economic systems, war, and exploitation.\n\nSome research on belief in a just world has examined how people react when they themselves are victimized. An early paper by Dr. Ronnie Janoff-Bulman found that rape victims often blame their own behavior, but not their own characteristics, for their victimization. It was hypothesized that this may be because blaming one's own behavior makes an event more controllable.\n\nThese studies on victims of violence, illness, and poverty and others like them have provided consistent support for the link between observers' just-world beliefs and their tendency to blame victims for their suffering. As a result, the existence of the just-world hypothesis as a psychological phenomenon has become widely accepted.\n\nSubsequent work on measuring belief in a just world has focused on identifying multiple dimensions of the belief. This work has resulted in the development of new measures of just-world belief and additional research. Hypothesized dimensions of just-world beliefs include belief in an unjust world, beliefs in immanent justice and ultimate justice, hope for justice, and belief in one's ability to reduce injustice. Other work has focused on looking at the different domains in which the belief may function; individuals may have different just-world beliefs for the personal domain, the sociopolitical domain, the social domain, etc. An especially fruitful distinction is between the belief in a just world for the self (personal) and the belief in a just world for others (general). These distinct beliefs are differentially associated with positive mental health.\n\nResearchers have used measures of belief in a just world to look at correlates of high and low levels of belief in a just world.\n\nLimited studies have examined ideological correlates of the belief in a just world. These studies have found sociopolitical correlates of just-world beliefs, including right-wing authoritarianism and the protestant work ethic. Studies have also found belief in a just world to be correlated with aspects of religiousness.\n\nStudies of demographic differences, including gender and racial differences, have not shown systematic differences, but do suggest racial differences, with blacks and African Americans having the lowest levels of belief in a just world.\n\nThe development of measures of just-world beliefs has also allowed researchers to assess cross-cultural differences in just-world beliefs. Much research conducted shows that beliefs in a just world are evident cross-culturally. One study tested beliefs in a just world of students in 12 countries. This study found that in countries where the majority of inhabitants are powerless, belief in a just world tends to be weaker than in other countries. This supports the theory of the just-world hypothesis because the powerless have had more personal and societal experiences that provided evidence that the world is not just and predictable.\n\nBelief in unjust world has been linked to increased self-handicapping, criminality, defensive coping, anger and perceived future risk. It may also serve as ego-protective belief for certain individuals by justifying maladaptive behavior.\n\nAlthough much of the initial work on belief in a just world focused on its negative social effects, other research suggests that belief in a just world is good, and even necessary, for mental health. Belief in a just world is associated with greater life satisfaction and well-being and less depressive affect. Researchers are actively exploring the reasons why the belief in a just world might have this relationship to mental health; it has been suggested that such beliefs could be a personal resource or coping strategy that buffers stress associated with daily life and with traumatic events. This hypothesis suggests that belief in a just world can be understood as a positive illusion.\n\nSome studies also show that beliefs in a just world are correlated with internal locus of control. Strong belief in a just world is associated with greater acceptance of and less dissatisfaction with negative events in one's life. This may be one way in which belief in a just world affects mental health. Others have suggested that this relationship holds only for beliefs in a just world for oneself. Beliefs in a just world for others are related instead to the negative social phenomena of victim blaming and victim derogation observed in other studies.\n\nMore than 40 years after Lerner's seminal work on belief in a just world, researchers continue to study the phenomenon. Work continues primarily in the United States, Europe, Australia, and Asia. Researchers in Germany have contributed disproportionately to recent research. Their work resulted in a volume edited by Lerner and German researcher Leo Montada titled \"Responses to Victimizations and Belief in a Just World\".\n\n\n"}
{"id": "20948716", "url": "https://en.wikipedia.org/wiki?curid=20948716", "title": "Kripa (philosophy)", "text": "Kripa (philosophy)\n\nKripa (कृपा) is the concept of divine grace in Hinduism. It is the central tenet of Bhakti Yoga and Bhakti movements, which are seen as reform movements in Hinduism as compared to the Hinduism which finds its origins in the Vedas; though variously it can mean \"grace\", \"mercy\", or \"blessing\", depending upon the context. The Hindi word Kirpala from Sanskrit Kripala means \"kind\" and is used as a given name for males, while \"Kripa\" (Kṛpā), is used as a female given name.\n\nKripa is akin to similar beliefs prevalent in mysticism of all traditions. In Hinduism as well, the bestowal of divine grace or Kripa is considered an event which catapults a devotee or bhakta into a period of intense personal transformation leading to his Moksha.\n\nDevotional or Bhakti literature available throughout India is replete with references to Kripa as the ultimate key towards realizing the spiritual path of self-realization In fact, some like the ancient sage Vasistha, in his classical work Yoga Vasistha, considered it to be the only way to transcend the bondage of lifetimes of Karma. He states to Rama that divine grace or Kripa is the only way to help us go beyond the effects of Prarabdha karma, or collection of all the past Karmas, Sanchita karma chosen to experience during a lifetime.\n\nThe Hindu philosopher Madhvacharya held that grace was not a gift from God, but rather must be earned.\n\nAs Krishna says to Arjuna in the final chapter of the Bhagavad Gita, \"Verse 18.66\", \"Setting aside all meritorious deeds (Dharma), just surrender completely to My will (with firm faith and loving contemplation). I shall liberate you from all sins. Do not fear.\"\n\nSimilarly, Adi Shankaracharya composes his famous verse Bhaja Govindam in 8th century, where he declares:\n<poem>\n</poem>\n\nThe Skanda Purana mentions the grace of a Guru in various places, especially in the Uttarakhand, section \"Guru Strotram\", known as Guru Gita, in the form of a dialogue between Shiva and Uma (Shakti):\n<poem>\n\nBhakti or devotion has often remained an ignored aspect in Indian philosophy, hence its significance was lost to majority of the populace. For some time it was considered befitting only to the lower classes and woman, as only the elite were considered able to comprehend the message of the Vedic tradition. Things changed with the revival and reform movements in Hinduism which brought this aspect of Kripa into a new light and made the divine accessible to all, and not just the preserve of the priestly class.\n\nKripa has been categorized in various ways as Ishwara kripa (grace of God), variously Hari Kripa, Shastra kripa (grace of the Scriptures), Guru kripa (grace of the Guru) and lastly Atma kripa (grace of the Self).\n"}
{"id": "49682781", "url": "https://en.wikipedia.org/wiki?curid=49682781", "title": "List of international and European law on child protection and migration", "text": "List of international and European law on child protection and migration\n\nThe following is a list of international standards that are relevant to the protection of children in migration and mobility. These standards are legally binding conventions, treaties and directives. The international legal framework concerning children in migration and mobility provides safeguards in relation to asylum and international protection, labour regulations, the prevention of sexual exploitation and trafficking in human beings, international standards for migrant workers, child victims of crime and the judiciary, as well as international private law for child protection and family matters.\n\nInternational standards of the United Nations, once adopted by the General Assembly of the United Nations, are open to signature and ratification by UN Member States worldwide. When a national government ratifies a Convention, the standards afforded under the Convention have to be reflected in national law and policy and become thereby applicable law in the country. The same procedure of ratification applies to the Conventions of the Council of Europe and the Hague Conference of Private International Law. In addition to the Member States of these inter-governmental organisations, non-Member States can also accede to the Conventions.\n\nThe UN Convention on the Rights of the Child defines the human rights of children and the correlated obligations of states. It provides also for obligations of parents and caregivers, public authorities, private service providers and the private sector. These rights and obligations can guide caseworkers and case officers in all measures, decisions and considerations for children on the move. The Convention supports caseworkers and officers in navigating the complex body of international, European and national laws. It provides the overarching framework and the strongest point of reference for safeguarding children.\n\nThe Convention is complemented by three Optional Protocols, one on the sale of children, child prostitution and child pornography, another one on the involvement of children in armed conflict, and a third and more recent Protocol on a communications procedure for children. States that have ratified the Convention report to the Committee on the Rights of the Child, the international Treaty Body mandated to monitor the implementation of the Convention by States Parties. The Committee reviews and comments on State Party reports on the implementation of the Convention and develops General Comments, in which it elaborates on specific articles of the Convention.\n\nThe Hague Conference on Private International Law (HCCH) is a global inter-governmental organisation that has developed standards for the transnational cooperation on child protection and family matters. The key themes addressed by the Conventions of the Hague Conference include transnational child protection, inter-country adoption, cross-border parental child abduction as well as matters of parental responsibility and contact involving different countries.\n\nThe Hague Conventions in the area of transnational child protection and family law have several common characteristics. They ensure the automatic mutual recognition of official decisions taken by one Contracting State in other Contracting States. They enable and facilitate the cooperation between the Contracting States, including through the establishment of central authorities and the development of unified procedures. By facilitating practical matters, such as the translation of documents, information exchange and the use of standardised model forms, the Conventions aim to simplify and expedite cross-border procedures and the enforcement of official decisions.\n\nThe Hague Conventions are innovative as they work primarily with the concept of ‘habitual residence’ of the child, rather than ‘nationality’, in order to determine which state has the jurisdiction over a case.\n\nThe UN Refugee Convention and its Protocol regulate the right of persons to seek international protection. Children enjoy special safeguards and have a right to have their asylum application examined individually. Child-specific grounds of persecution need to be considered irrespective of whether the child applies alone or together with a parent or caregiver. The European Union Member States have re-elaborated these standards for the EU context and have adopted a series of Directives regulating the qualification and reception conditions of asylum seekers in the EU as well as asylum procedures and matters of return.\n\nThe European Social Charter sets out rights and freedoms and establishes a supervisory mechanism mandated to monitor how States Parties implement the Charter in practice. The rights afforded under the Charter concern all individuals in their daily lives as they relate, for instance, to housing, health, education, employment, legal and social protection, free movement of persons and non-discrimination. The Charter was first adopted in 1961 and was subsequently revised. The 1996 revised European Social Charter entered into force in 1999.\n\nThe European Committee of Social Rights issued in October 2015 a Statement of interpretation on the rights of refugees under the European Social Charter, which provides guidance on the application of the European Social Charter to refugees and asylum seekers.\n\nThe Council of Europe Convention on the Protection of Children against Sexual Exploitation and Sexual Abuse is the first instrument to establish the various forms of sexual abuse and exploitation of children as criminal offences, including abuse committed in the home or family, with the use of force, coercion or threats. Preventive measures outlined in the Convention include the screening, recruitment and training of persons working in contact with children, making children aware of the risks and teaching them to protect themselves, as well as monitoring measures for offenders and potential offenders.\n\nThe Convention also establishes programmes to support victims, encourages people to report suspected sexual exploitation and abuse, and sets up telephone and internet helplines for children. It ensures that certain types of conduct are classified as criminal offences, such as engaging in sexual activities with a child below the legal age and the sexual exploitation of children in prostitution and pornography. The Convention criminalises the solicitation of children for sexual purposes (grooming) and by travelling sex offenders who can be prosecuted for some offences even when the act is committed abroad. The Convention ensures that child victims are protected during judicial proceedings, for example with regard to their identity and privacy.\n\nThe Council of Europe Convention on Action against Trafficking in Human Beings is a comprehensive treaty mainly focused on the protection of victims of trafficking and the safeguard of their rights. It also aims at preventing trafficking as well as prosecuting traffickers. The Convention applies to all forms of trafficking; whether national or transnational, whether or not related to organised crime and whoever the victim, women, men or children and whatever the form of exploitation, sexual exploitation, forced labour or services, or other.\n\nThe Convention provides for the setting up of an independent monitoring mechanism (GRETA), which monitors the States Parties’ compliance with its provisions.\n\nWithin the European Union, EU Directives are legally binding and constitute EU law that needs to be transposed into the national law of Member States within a prescribed period of time. When countries infringe against or violate European standards that are in force within the country, the European Courts offer the possibility to seek legal remedy and to claim the rights afforded under international or European law.\n\nThe \"Council Directive 2003/9/EC of 27 January 2003 laying down minimum standards for the reception of asylum seekers\" sets out minimum reception standards for asylum applicants. The aim is to ensure that the applicants have a dignified standard of living and that comparable living conditions are afforded to them in all Member States. At the same time, the Directive also limits asylum applicants’ secondary movements.\n\nThe \"Directive 2013/32/EU of the European Parliament and of the Council of 26 June 2013 on common procedures for granting and withdrawing international protection\" repealed Directive 2005/85/EC on minimum standards on procedures for granting and withdrawing refugee status in European Union countries and sets up EU-wide procedures for granting and withdrawing international protection (refugee status and the protection given to people who are not refugees but who would risk serious harm if returned to their country of origin).\n\nThe full name of the Qualification Directive is the \"Directive 2011/95/EU of the European Parliament and of the Council of 13 December 2011 on standards for the qualification of third-country nationals or stateless persons as beneficiaries of international protection, for a uniform status for refugees or for persons eligible for subsidiary protection, and for the content of the protection granted\".\n\nThe Qualification Directive establishes common grounds to grant international protection. Its provisions also foresee a series of rights on protection from non refoulement, residence permits, travel documents, access to employment, access to education, social welfare, healthcare, access to accommodation, access to integration facilities, as well as specific provisions for children and vulnerable persons.\n\nWith the full name \"Regulation (EU) No 604/2013 of the European Parliament and of the Council of 26 June 2013 establishing the criteria and mechanisms for determining the Member State responsible for examining an application for international protection lodged in one of the Member States by a third-country national or a stateless person\".\n\nRegulation (EU) No 604/2013 (Dublin III Council Regulation), replacing Council Regulation (EC) No 343/2003 (Dublin II Regulation), lays down the criteria and mechanisms for determining which EU country is responsible for examining an asylum application.\n\nThe \"Directive 2008/115/EC of the European Parliament and of the Council of 16 December 2008 on common standards and procedures in Member States for returning illegally staying third-country nationals\" establishes common standards and procedures for EU countries, whereby illegally staying non-EU nationals may be removed from their territories. It lays down provisions for terminating illegal stays, detaining non-EU nationals with the aim of removing them and procedural safeguards.\n\nThe purpose of the \"Council Directive 2003/86/EC of 22 September 2003 on the right to family reunification\" is to determine the conditions under which non-EU nationals residing lawfully on the territory of EU countries may exercise the right to family reunification. The Directive aims to establish common rules of law relating to the right to family reunification. The intention is to enable family members of non-EU nationals residing lawfully on the territory of the European Union (EU) to join them in the EU country in which they are residing. The objective is to protect the family unit and to facilitate the integration of nationals of non-member countries. The Directive does not apply to Ireland, Denmark and the United Kingdom. In addition, it does not preclude any more favourable conditions recognised by national legislation.\n\nThe aim of the \"Council Directive 2004/114/EC of 13 December 2004 on the conditions of admission of third-country nationals for the purposes of studies, pupil exchange, unremunerated training or voluntary service\" is to harmonise national legislation relating to the conditions of admission of third-country nationals for the purposes of studies, pupil exchange, unremunerated training or voluntary service.\n\n\"European Parliament and Council Directive 2004/38/EC of 29 April 2004 on the right of citizens of the Union and their family members to move and reside freely within the territory of the Member States\" brings together the piecemeal measures found in the complex body of legislation that had previously governed this matter. The measures are designed, among other things, to encourage citizens to exercise their right to move and reside freely within EU countries, to cut back administrative formalities to the bare essentials, to provide a better definition of the status of family members, to limit the scope for refusing entry or terminating the right of residence and to introduce a new right of permanent residence.\n\n\"Council Regulation (EC) No 2201/2003 of 27 November 2003 concerning jurisdiction and the recognition and enforcement of judgments in matrimonial matters and the matters of parental responsibility\" brings together in a single legal instrument the provisions on divorce and parental responsibility, with a view to facilitating the work of judges and legal practitioners and to regulating the exercise of cross-border rights of access. This regulation represents a major step forward in the fight against abductions of children.\n\n\"Directive 2011/92/EU of the European Parliament and of the Council of 13 December 2011 on combating the sexual abuse and sexual exploitation of children and child pornography, and replacing Council Framework Decision 2004/68/JHA\" is aimed at combating sexual offences committed against children. The Directive covers different aspects such as sanctions, prevention, and assistance for victims. Specific provisions are provided concerning child pornography on the Internet and sex tourism.\n\n\"Directive 2011/36/EU of the European Parliament and of the Council of 5 April 2011 on preventing and combating trafficking in human beings and protecting its victims, and replacing Council Framework Decision 2002/629/JHA.\" This directive establishes rules across the European Union to address trafficking in human beings.\n\n\"Council Directive 2004/81/EC of 29 April 2004 on the residence permit issued to third-country nationals who are victims of trafficking in human beings or who have been the subject of an action to facilitate illegal immigration, who cooperate with the competent authorities\" states that residence permits of temporary duration may be issued to non-EU nationals who are victims of trafficking in human beings or (optionally) the subject of an illegal immigration action. It is hoped that this will encourage them to cooperate with the competent authorities whilst providing them with adequate protection.\n\n\"Directive 2012/29/EU of the European Parliament and of the Council of 25 October 2012 establishing minimum standards on the rights, support and protection of victims of crime, and replacing Council Framework Decision 2001/220/JH\" establishes minimum standards on the rights, support and protection of victims of crime ensures that persons who have fallen victim of crime are recognised, treated with respect and receive proper protection, support and access to justice. The Directive replaces the 2001 Framework Decision on the standing of victims in criminal proceedings and considerably strengthens the rights of victims and their family members to information, support and protection and victims' procedural rights in criminal proceedings. The Directive also requires that the Member States ensure appropriate training on victims' needs for officials who are likely to come into contact with victims and encourage cooperation between Member States and coordination of national services of their actions on victims' rights.\n\n\"Communication from the Commission to the European Parliament and the Council of 6 May 2010 – Action Plan on Unaccompanied Minors (2010 – 2014) COM(2010) 213 final\" provides a common approach to tackling the challenges relating to the arrival in the European Union (EU) of large numbers of unaccompanied minors. The action plan is based on the principle of the best interests of the child.\n\n\"Communication from the Commission to the European Parliament, the Council, the Economic and Social Committee and the Committee of the Regions of 4 May 2011, COM(2011) 248\" presents a set of measures aimed at establishing a comprehensive European migration policy, founded on greater solidarity between Member States and enabling the European Union (EU) to respond better to the challenges presented by migration.\n\n\"Communication from the Commission to the Council, the European Parliament, the European Economic and Social Committee and the Committee of the Regions - migration and development: some concrete orientations COM(2005) 390\" recognises migration as a powerful - though challenging - development vehicle in both the country of origin and destination. As a global phenomenon, it cannot be managed by the EU alone, and to identify common interests and challenges, the EU dialogues with partner countries, including countries of origin and transit.\n\n\"Communication from the Commission to the European Parliament, the Council, the European Economic and Social Committee and the Committee of the Regions of 17 June 2008 – Policy Plan on Asylum: An integrated approach to protection across the EU [COM(2008) 360 final – Not published in the Official Journal].\" is a policy plan that provides the road-map for completing the second phase of the Common European Asylum System (CEAS). It is based on a three-pronged strategy that focuses on the harmonisation of protection standards, practical cooperation and solidarity.\n\nThe \"European Pact on Immigration and Asylum of 24 September 2008\" is intended to be the basis for European Union immigration and asylum policies in a spirit of mutual responsibility and solidarity between Member States and a renewed partnership with non-EU countries.\n\n\"Regulation (EU) No 439/2010 of the European Parliament and of the Council of 19 May 2010 establishing a European Asylum Support Office\" establishes a European Asylum Support Office to strengthen cooperation between the Member States in this area and assist them in coping with crisis situations.\n\n\"Communication from the Commission to the European Parliament, the Council, the European Economic and Social Committee and the Committee of the Regions of 15 February 2011 – An EU Agenda for the Rights of the Child COM(2011) 60 final\" aims at strengthening the promotion and protection of the rights of the child by implementing the principles laid down in the Charter of Fundamental Rights of the European Union (EU) and international standards in this field. It consists of a series of actions intended to foster an increase in the attention paid to the well-being and protection of children in Union policies.\n\nThe \"Communication from the Commission to the European Parliament, the Council, the European Economic and Social Committee and the Committee of the Regions, The EU Strategy towards the Eradication of Trafficking in Human Beings 2012–2016\" sets out measures and actions to support the implementation of the 2011 EU Anti-Trafficking Directive. It is structured around the following priority areas: Identifying, protecting and assisting victims of trafficking; strengthening prevention; increased prosecution; enhanced coordination and cooperation among key actors; and increased knowledge of and effective response to emerging concerns related to all forms of trafficking in human beings.\n\n\"Communication from the Commission to the Council and the European Parliament on combating trafficking in human beings and combating the sexual exploitation of children and child pornography\" introduces effective measures to address the whole trafficking chain of recruiters, transporters, exploiters and clients.\n\nThe \"Council Resolution on the contribution of civil society in finding missing or sexually exploited children\" aims to encourage cooperation between civil society organisations and the competent authorities in finding missing or sexually exploited children.\n\n\"Council Regulation No 2725/2000 of 11 December 2000 concerning the establishment of 'Eurodac' for the comparison of fingerprints for the effective application of the Dublin Convention\" establishes Eurodac, a system for comparing fingerprints of asylum seekers and some categories of illegal immigrants. It facilitates the application of the Dublin II Regulation, which makes it possible to determine the European Union (EU) country responsible for examining an asylum application.\n\nIn the public debate on international migration, the mandates and interests of different agencies and disciplines can sometimes appear to be in conflict. International human rights standards provide the common basis for decision makers. In the context of children, the UN Convention on the Rights of the Child provides the basis for promoting the best interests of children in all contexts and situations.\n\nIn addition to legally binding conventions, treaties and directives, the UN, the Council of Europe and the European Union have developed a large number of political recommendations, regulations and guidelines, which are not legally binding but have nonetheless an important value as they aid in the interpretation and implementation of legal standards.\n\n"}
{"id": "4319584", "url": "https://en.wikipedia.org/wiki?curid=4319584", "title": "Luck egalitarianism", "text": "Luck egalitarianism\n\nLuck egalitarianism is a view about distributive justice espoused by a variety of egalitarian and other political philosophers. According to this view, justice demands that variations in how well off people are should be wholly determined by the responsible choices people make and not to differences in their unchosen circumstances. This expresses the intuition that it is a bad thing for some people to be worse off than others through no fault of their own.\n\nLuck egalitarians therefore distinguish between outcomes that are the result of brute luck (e.g. misfortunes in genetic makeup, or being struck by a bolt of lightning) and those that are the consequence of conscious options (such as career choice or fair gambles). Luck egalitarianism is intended as a fundamental normative idea that might guide our thinking about justice rather than as an immediate policy prescription. The idea has its origin in John Rawls' thought that distributive shares should not be influenced by arbitrary factors. Luck egalitarians disagree among themselves about the proper way to measure how well off people are (for instance, whether we should measure material wealth, psychological happiness or some other factor) and the related issue of how to assess the value of their resources.\n\nMany philosophers think that the term \"luck egalitarianism\" is a misnomer, because many so-called \"luck egalitarians\" (of the 'resourcist' strand at least) do not in fact want to equalize luck or eliminate uncertainty, but instead believe that individuals should be equal in the amount of resources they have when facing luck or uncertainty.\n\nThe position is controversial within some currents of egalitarian thought, and the philosopher Elizabeth S. Anderson has been a vocal critic of it — on the ground that, amongst other things, the fact that something is chosen does not necessarily make it acceptable. An example of this would be a robber offering someone the choice \"Your money or your life,\" which choice some theorists, including Thomas Hobbes (\"Leviathan\" XIV: \"Covenants Extorted by Feare are Valide\") have regarded as presumptively binding. She also claims that luck egalitarianism expresses a demeaning pity towards the disadvantaged, basing their claims to compensation not on equality but inferiority, and excludes many individuals from the social conditions of their freedom simply on the basis that it is judged to be their fault for losing them. Further, it involves the state making highly moralistic and intrusive judgements about the choices that individuals make, and seems to lead to very counter-intuitive conclusions: those who voluntarily enter jobs with higher-than-average risks, or who 'choose' to live in geographical locations prone to natural disasters may make no claim on others if they suffer as a result of it.\n\nSusan Hurley has argued that any attempt to ground egalitarianism in issues concerning luck and responsibility must fail, because there is no non-circular way of specifying an \"egalitarian\" baseline rather than any other baseline. For instance, a luck inegalitarian could believe that the baseline from which we should correct luck is one where huge inequalities exist. Without merely assuming equality, there seems to be no way of coming to prefer one approach over the other.\n\nGlobal luck egalitarianism is a view about distributive justice at the global level associated with cosmopolitan moral theory. It starts from the premise that it is a bad thing for some people to be worse off than others through no fault of their own and applies this intuition across borders. Global luck egalitarians characteristically believe that moral agents may have duties to mitigate the brute luck of distant others. Proponents of this school of thought are amongst others Simon Caney and arguably Charles Beitz; opponents, most of whom reject the above premise either in its entirety or with respect to inequalities in which one party's welfare is at least above some minimum level, include Robert Nozick.\n\nProminent advocates of luck egalitarianism have included Ronald Dworkin, Richard Arneson, Gerald Cohen, John Roemer, Eric Rakowski, and Kok-Chor Tan.\n\n\n"}
{"id": "52076168", "url": "https://en.wikipedia.org/wiki?curid=52076168", "title": "Margaret Noble (artist)", "text": "Margaret Noble (artist)\n\nMargaret Noble (born 1972) is an American conceptual artist, sound artist, installation artist, and electronic music composer.\n\nThe daughter of artist Jill Hosmer, Noble was born in Waco, Texas, and grew up in the City Heights neighborhood of San Diego, moving there in 1982 at the age of 9. Her youth in City Heights has been described as \"dependent on welfare, captivated by hip-hop and dance music, among racially diverse neighbors.\"\n\nShe earned a BA in philosophy from the University of California at San Diego, and an MFA in sound art at the School of the Art Institute of Chicago in 2007.\n\nAs a house music disc jockey, Noble performed at underground clubs internationally and in Chicago, Illinois, where she spent five years as a DJ. In 2007, she moved to San Diego to teach media production at High Tech High School in Point Loma while continuing her sound art practice. \n\nShe collaborated in 2011 with math teacher David Stahnke in the \"Illuminated Mathematics\" project, winning second place in knowledge building and critical thinking, among twelve educators who represented the U.S. at Microsoft's Global Learning Forum. They went on to win first place in \"Knowledge Building and Critical Thinking\" category of the Global Forum Educator Awards. \n\nNoble's \"Frakture,\" a remix of a 1953 vinyl recording of George Orwell's novel \"Nineteen Eighty-Four,\" is an \"eight-track audio collage of analog synthesizer, acoustic drums, recordings of healthcare protests, contemporary political propaganda, emergency alarms, the New York Stock Exchange, dice rolling\" and other sounds. On the recording, Noble reads excerpts from the text of the novel. The first track of \"Frakture\", \"Safer is Better\", won first place in the 2013 Electronic Music Composition Contest from Canada's \"Musicworks\" magazine. \n\nNoble's installation \"44th and Landis\" opened in 2012 at the Museum of Contemporary Art San Diego. The large-scale multimedia art piece combined Victorian-style paper dolls with 1980s urban influences based on her upbringing in San Diego's City Heights neighborhood, and included a performance by Noble. The show's visual centerpiece was a hanging series of 100 paper dolls, along with paper-doll clothing, objects and architecture. \nIn her 2016 interactive piece \"What Lies Beneath\", she worked in sound sculpture, creating a tall wooden box with instructions next to it to raise the lid, which caused sounds of organ pipes, truck brakes and other dissonance to emit. The person interacting with it controlled the sound with the lid, with a \"storm\" inside the box.\n\nHer \"Head in the Sand\" is a wooden box sitting on four legs with a head-sized hole in the top and instructions to visitors to place their heads in the hole and wait. Inside is a chambered light and sound show with soft pastoral sounds, the hole serving as a sanctuary from the art exhibit itself. \"Head in the Sand\" was included in her 2016 exhibition \"Resonating Object,\" an interactive mixture of sound, sculpture and videos, at South Puget Sound Community College in Olympia, Washington. The exhibition also included \"I Long to Be Free From Longing\" and \"Material Shrine for the New Class\", featuring dangling objects the visitor could squeeze to activate different sounds. Her 2014 interactive sound installation \"I Long to Be Free From Longing\" won first place in the 23rd annual Juried Exhibition at the Athenaeum Music & Arts Library in San Diego. \n\nNoble's 2016 sound art installation \"Time Strata\", a public art commission for the Port of San Diego at the Cesar Chavez Park pier, consisted of three sound sculptures made of materials including vintage buoys, hunks of bamboo, bells, stainless steel and harp strings, along with sounds of creatures like snapping shrimp in the water under the pier. Microphones placed around the pier fed the sound into a mixer and then into four digital consoles where participants could sample and alter the sounds.\n\nFor \"The Collector\", Noble worked with puppeteers Animal Cracker Conspiracy and visual directors Bridget Rountree and Iain Gunn, creating a multi-layered soundscape that used animated video, live video projection and puppetry to tell a story of a debt collector. \"Righteous Exploits\", a 2013 experimental performance created with Justin Hudnall, used a combination of live audio and video multimedia and performance art. \n\nHer 2018 installations of \"Resonating Objects\" included \"lawn sprinklers sitting on grass-covered pedestals, playing their percussive, shimmering, water-spraying sounds\", entitled \"I Have Arrived\", which explores the use of expendable resources on lawns, or status symbols. \n\nTwo other pieces are \"Scaled Discords, 2015\", with spinning tops representing \"power structures, resource allocation and racial inequality in America\", and \"A Shit Pile of Lights and Sounds for Your Pleasure\" consisting of \"mash-up of Lite Brite, a Ouija board, and an early Akai sampler\". Noble describes the exhibit as \"a quiet and personal experience for the participant. The sounds work together with the gestures of touching the work, the physical materials of each sculpture’s design and the personal responses of the audience\".\n\nNoble's art has been presented on PBS and reviewed favorably in Art Ltd. Magazine, The San Diego Union-Tribune, and San Francisco Weekly. One critic wrote that \"enlarging the sensorium of art with sound begins with disorder,\" but while visual art may be viewed with \"one or one hundred other hushed-up viewers,\" sound art is more like \"a Fourth-of-July picnic, Charles-Ives polyphony, a resolute disequilibrium\".\n\nNoble's exhibits may not be what people expect art to be. Nathan Barnes of the Gallery at Kenneth J. Minnaert Center for the Arts at South Puget Sound Community College said, \"\"What can art be?\" is a good question to pose to students — and to the larger community. If people are walking through the door thinking art is a drawing or a painting that's rectilinear, this is probably going to blow their minds.\" Molly Gilmore of \"The Olympian\" wrote that the exhibition \"doesn't fit into a neat category of art.\" Reviewer Michael James Rocha said in 2016, \"Artist Margaret Noble isn't afraid to push the boundaries of what's art.\"\n\nSusan Myrland of \"The San Diego Tribune\" asked, \"So why does sonic art still seem unusual 100 years after Luigi Russolo built his first intonarumori, or experimental noise instruments?\" Noble and two of her collaborators on \"Time Strata\" offered these reasons: \"It's hard to exhibit properly. Curators might not know how to assess it, and there aren't many collectors rushing to buy work that they can't see. It's difficult for sound art to break away from the paradigm of musical performance. It requires time and space to fully appreciate. Art galleries are often bare rooms with concrete floors, which means poor acoustics and one sound art piece overlapping another.\"\n\nObserving \"Frakture\", Jennie Punter of \"Musicworks\" wrote of Noble's \"underground club DJ’s flair for performance and a conceptual artist's commitment to the rigorous investigation of ideas\". Mark Jenkins of The Washington Post wrote, \"Her primary goal is audience participation, whether that involves turning a crank or inserting one's head into a box to prompt whooshing sounds. Noble's work completes its circuit when the spectator is, literally or figuratively, inside it.\"\n\nOf the exhibit \"44th and Landis,\" Angela Carone of Public Radio International commented, \"Look close and you'll see the ghosts of \"Ms. Pac-Man\", the labels from Animal Cracker boxes and Laffy Taffy, and, on the seedier side, signage from neighborhood massage parlors. The paper dolls are pint-sized mash-ups of '80s pop culture and Victoriana. They seem to emerge from Noble's childhood dreams as she tried to make sense of both a threatening and exciting environment.\" Drew Snyder noted, \"...there is an excess to the sound collage, a soft but persistent drone of spinning bottles or coins, rolling glass marbles, the eternal creak of a cabinet hinge, the rapt knocking on a door, or the sound of something falling over. These reverberations are strikingly material, a confluence and collision of metal, plastic, wood and glass that mash up and reconfigure what we can imagine as a neighborhood's aural life.\"\n\nReviewing her exhibit titled, \"Now Is Not A Good Time\", Rebecca Romani of \"The Buzz\" wrote in 2018 of its \"intriguing mix\" of sewing and tatting materials and rattlesnake tails powered by tiny batteries. Romani commented, \"It's tempting to read a cautionary tale of watching too much \"Little House On The Prairie\" and the nostalgia that lead us to these current times.\"\n\nReviewer Andrew Nunes described \"Head in the Sand\", which Noble exhibited in \"Resonating Objects\", as a \"claustrophobic challenge\", a wooden box with a hole for a viewer's head. Nunes went on to say Noble fused sculptural sound art and interactivity, \"seeking to go deeper on conceptual levels about human experience and our relationships with technology and each other\".\n\nYvonne Wise, director of waterfront arts and activation at the Port of San Diego, wrote of Noble's art installation \"Time Strata\", \"Temporal works like Noble's prompt new opportunities and formats for engaging in public art, producing interactions that are flexible, dynamic, and socially conscious. In place of monumental permanence emerges a sustained program of placemaking and experiential learning.\"\n\n\n\n\n\n"}
{"id": "5765744", "url": "https://en.wikipedia.org/wiki?curid=5765744", "title": "Mariela Castro", "text": "Mariela Castro\n\nMariela Castro Espín (born 27 July 1962) is the director of the Cuban National Center for Sex Education in Havana and an activist for LGBT rights in Cuba. She is the daughter of former Cuban president Raúl Castro and feminist and revolutionary Vilma Espín, and the niece of former president Fidel Castro.\n\nMariela Castro is the daughter of former Cuban president Raúl Castro and feminist and revolutionary Vilma Espín, and the niece of former president Fidel Castro. She has a brother, Alejandro Castro Espín.\n\nHer group campaigns for effective AIDS prevention as well as recognition and acceptance of LGBT human rights. In 2005, she proposed a project to allow transgender people to receive sex reassignment surgery and change their legal gender. The measure became law in June 2008 which allows sex change surgery for Cubans without charge.\n\nMariela Castro is president of the Cuban Multidisciplinary Centre for the Study of Sexuality, president of the National Commission for Treatment of Disturbances of Gender Identity, member of the Direct Action Group for Preventing, Confronting, and Combatting AIDS, and an executive member of the World Association for Sexual Health (WAS). She is also the director of the journal \"Sexología y Sociedad\", a magazine of Sexology edited by her own National Center for Sex Education (CENESEX).\n\nCastro has published 13 scholarly articles and nine books.\n\nCastro is a sitting member of the National Assembly of People's Power. When the assembly voted in 2014 to ban discrimination on the basis of sexual orientation in employment, Castro opposed the legislation because it did not also include protection on the basis of gender identity, and became possibly the first legislator in the body's history ever to vote against a piece of legislation.\n\nCastro is married to Italian Paolo Titolo, General Manager of Amorim Negócios Internacionais, S.A. in Cuba, and has two children with him, and a daughter from her previous marriage with the Chilean former FPMR member Juan Gutiérrez Fischmann.\n\n\n \n"}
{"id": "539008", "url": "https://en.wikipedia.org/wiki?curid=539008", "title": "Mirror stage", "text": "Mirror stage\n\nThe mirror stage () is a concept in the psychoanalytic theory of Jacques Lacan. The mirror stage is based on the belief that infants recognize themselves in a mirror (literal) or other symbolic contraption which induces apperception (the turning of oneself into an object that can be viewed by the child from outside themselves) from the age of about six months.\n\nInitially, Lacan proposed that the mirror stage was part of an infant's development from 6 to 18 months, as outlined at the Fourteenth International Psychoanalytical Congress at Marienbad in 1936. By the early 1950s, Lacan's concept of the mirror stage had evolved: he no longer considered the mirror stage as a moment in the life of the infant, but as representing a permanent structure of subjectivity, or as the paradigm of \"Imaginary order\". This evolution in Lacan's thinking becomes clear in his later essay titled \"The Subversion of the Subject and the Dialectic of Desire\".\n\nLacan's concept of the mirror stage was strongly inspired by earlier work by psychologist Henri Wallon, who speculated based on observations of animals and humans responding to their reflections in mirrors. Wallon noted that by the age of about six months, human infants and chimpanzees both \"seem\" to recognize their reflection in a mirror. While chimpanzees rapidly lose interest in the discovery, human infants typically become very interested and devote much time and effort to exploring the connections between their bodies and their images. In a 1931 paper, Wallon argued that mirrors helped children develop a sense of self-identity. However, later mirror test research indicates that while toddlers are usually fascinated by mirrors, they do not actually recognize themselves in mirrors until the age of 15 months at the earliest, leading psychoanalytically trained critic Norman N. Holland to declare that \"there is no evidence whatsoever for Lacan's notion of a mirror stage\". Similarly, physician Raymond Tallis notes that a literal interpretation of the Lacanian mirror stage contradicts empirical observations about human identity and personality: \"If epistemological maturation and the formation of a world picture were dependent upon catching sight of oneself in a mirror, then the [mirror stage] theory would predict that congenitally blind individuals would lack selfhood and be unable to enter language, society or the world at large. There is no evidence whatsoever that this implausible consequence of the theory is borne out in practice.\"\n\nWallon's ideas about mirrors in infant development were distinctly non-Freudian and little-known until revived in modified form a few years later by Lacan. As Evans writes, \"Lacan used this observation as a springboard to develop an account of the development of human subjectivity that was inherently, though often implicitly, comparative in nature.\" Lacan attempted to link Wallon's ideas to Freudian psychoanalysis, but was met with indifference from the larger community of Freudian psychoanalysts. Richard Webster explains how the \"complex, and at times impenetrable paper ... appears to have made little or no lasting impression on the psychoanalysts who first heard it. It was not mentioned in Ernest Jones's brief account of the congress and received no public discussion.\"\n\nIn the 1930s, Lacan attended seminars by Alexandre Kojève, whose philosophy was heavily influenced by Hegel. The diachronic structure of the mirror stage theory is influenced by Kojève's interpretation of the Master-slave dialectic. Lacan continued to refine and modify the mirror stage concept through the remainder of his career; see below.\n\nDylan Evans argues that Lacan's earliest versions of the mirror stage, while flawed, can be regarded as a bold pioneering in the field of ethology (the study of animal behavior) and a precursor of both cognitive psychology and evolutionary psychology. In the 1930s, zoologists were increasingly interested in the then-new field of ethology, but not until the 1960s would the larger scientific community believe that animal behavior offered any insights into human behavior.\n\nHowever, Evans also notes that by the 1950s Lacan's mirror stage concept had become abstracted to the point that it no longer required a literal mirror, but could simply be the child's observation of observed behavior in the imitative gestures of another child or elder.\n\nThe child’s initiation into what Jacques Lacan would call the \"mirror stage\" entails a \"libidinal dynamism\" caused by the young child's identification with his own image and creation of what Lacan terms the \"Ideal-I\" or \"Ideal ego.\" This reflexivity inherent in fantasy is apparent in the mirror stage, since to recognize oneself as \"I\" is like recognizing oneself as other (\"yes, that person over there is me\"); this act is thus fundamentally self-alienating. Indeed, for this reason feelings towards the image are mixed, caught between hatred (\"I hate that version of myself because it is so much better than me\") and love (\"I want to be like that image\"). A type of repetition compulsion develops from this vacillation as the attempt to locate a fixed subject proves ever elusive. \"The mirror stage is a drama…which manufactures for the subject, caught up in the lure of spatial identification, the succession of phantasies that extends from a fragmented body-image to a form of its totality.” This misrecognition (seeing an ideal-I where there is a fragmented, chaotic body) subsequently \"characterizes the ego in all its structures.\"\n\nAs Lacan further develops the mirror stage concept, the stress falls less on its historical value and ever more on its structural value. \"Historical value\" refers to the mental development of the child and \"structural value\" to the libidinal relationship with the body image. In Lacan's fourth Seminar, \"La relation d'objet\", he states that \"the mirror stage is far from a mere phenomenon which occurs in the development of the child. It illustrates the conflictual nature of the dual relationship\". The dual relationship (\"relation duelle\") refers not only to the relation between the Ego and the body, which is always characterized by illusions of similarity and reciprocity, but also to the relation between the Imaginary and the Real. The visual identity given from the mirror supplies imaginary \"wholeness\" to the experience of a fragmentary real. See Lacan's paper, \"The Mirror Stage as formative of the function of the \"I\" as revealed in psychoanalytic experience\", the first of his \"Écrits.\"\n\nThe mirror stage describes the formation of the Ego via the process of identification, the Ego being the result of identifying with one's own specular image. At six months the baby still lacks coordination (see Louis Bolk); however, Lacan hypothesized that the baby can recognize itself in the mirror before attaining control over its bodily movements. The child sees its image as a whole, but this contrasts with the lack of coordination of the body and leads the child to perceive a fragmented body. This contrast, Lacan hypothesized, is first felt by the infant as a rivalry with its own image, because the wholeness of the image threatens it with fragmentation; thus the mirror stage gives rise to an aggressive tension between the subject and the image. To resolve this aggressive tension, the subject identifies with the image: this primary identification with the counterpart is what forms the Ego. (Evans, 1996) The moment of identification is to Lacan a moment of jubilation since it leads to an imaginary sense of mastery. (\"Écrits\", \"The Mirror Stage\") Yet, the jubilation may also be accompanied by a depressive reaction, when the infant compares his own precarious sense of mastery with the omnipotence of the mother. (\"La relation d'objet\") This identification also involves the ideal ego which functions as a promise of future wholeness sustaining the Ego in anticipation.\n\nThe mirror stage, Lacan also hypothesized, shows that the Ego is the product of misunderstanding – Lacan's term \"méconnaissance\" implies a false recognition. Additionally, the mirror stage is where the subject becomes alienated from itself, and thus is introduced into the Imaginary order.\n\nThe Mirror Stage has also a significant symbolic dimension. The Symbolic order is present in the figure of the adult who is carrying the infant: the moment after the subject has jubilantly assumed his image as his own, he turns his head toward this adult who represents the big Other, as if to call on him to ratify this image. (Tenth Seminar, \"L'angoisse\", 1962–1963)\n\n\n\n"}
{"id": "43316934", "url": "https://en.wikipedia.org/wiki?curid=43316934", "title": "Mr. Asia Contest", "text": "Mr. Asia Contest\n\nMr. Asia (亞洲先生競選), is a male beauty pageant contest run by Mr. Asia contest Organization based in Hong Kong\n\n\n"}
{"id": "6126807", "url": "https://en.wikipedia.org/wiki?curid=6126807", "title": "Natural design", "text": "Natural design\n\nNatural design is an approach to psychology and biology that holds that concepts such as \"motivation\", \"emotion\", \"inner feeling\", \"development\", \"adaptation\" refer not to down-reductive explanations of things but to up-reductive descriptions of patterns of which those things are part. It has its roots in philosophical behaviorism and the new realism. It also refers to an holistic approach to Design called for by Prof David W. Orr (Professor of Environmental Studies and Politics, Oberlin College USA) and developed for research practice by Prof Seaton Baxter (Emeritus Professor for the Study of Natural Design, Duncan of Jordanstone College of Art and Design, University of Dundee). \n\nNatural design has attribution to the process of natural selection. All species have been designed based on their life situation in order to have more offspring. This resulted as only better designed organisms can be found today because natural selection is only limited by the rapidity of environment change and the capacity of the genes to generate variation. Moreover, the theory of evolution by natural selection can some how explain the behavior of animals.\n\nDarwinism, psychologist who studies in behavior, stated two consequences of studying behavior.\n\n\n\nThese two results provided two competing systems for study of behavior and evolved into ethology and further evolved into comparative psychology.\n\n"}
{"id": "3861711", "url": "https://en.wikipedia.org/wiki?curid=3861711", "title": "Nihoa millerbird", "text": "Nihoa millerbird\n\nThe Nihoa millerbird (\"Acrocephalus familiaris kingi\") is a subspecies of the millerbird. It gets its name from its preferred food, the Miller moth. The long millerbird has dark, sepia-colored feathers, white belly, and dark beak. Its natural geographic range is limited to the tiny island of Nihoa in the Northwestern Hawaiian Islands, and it is hoped that birds translocated to Laysan will help to ensure the survival of the species. The Nihoa millerbird is one of the two endemic birds remaining on Nihoa, the other being the Nihoa finch.\n\nOnly 200– 900 Nihoa millerbirds persist on the island, making the species seriously endangered. It is always at risk of extinction from environmental changes (droughts, fires, insect population irruptions), because flight away from the island would likely prove fatal. The Laysan millerbird, now extinct, was closely related.\n\nThe trinomial commemorates Samuel Wilder King, captain of the Tanager Expedition and later Governor of Hawaii.\n\n24 Nihoa millerbirds were translocated by ship about from Nihoa to Laysan in September 2011 after years of planning with the aim of establishing a second population of the species on Laysan. Mark MacDonald, a graduate student from the University of New Brunswick in Canada, lead a team that was working with the Service to collect information needed for translocations. From July through September 2007, MacDonald and his team captured and banded Nihoa millerbirds, collected body measurements, assessed body fat and breeding condition, identified individual territories and analyzed vocalizations, conducted feeding experiments, collected fecal samples, observed behavior to determine diet composition, noted the presence and abundance of non-native grasshoppers, and sampled the insect community on both Nihoa and Laysan to assess the millerbird’s potential prey base. MacDonald’s study estimated the Nihoa millerbird’s population at approximately 800 individuals – a relatively high number in 40 years of low and fluctuating numbers. He believes that this could be attributed not only to high numbers of birds present during the survey period but also a larger survey area, the use of more experienced observers, or (most likely) the greater visibility of the birds during the late summer, when vegetation cover is most limited.\n\nUsing mist nets, 85 Nihoa millerbirds (60 males and 25 females) were captured and banded. Banding permits identification of previously captured birds and reduces stress that can be caused by multiple captures. Most importantly, however, banding allows individual birds to be identified in the field and enables biologists to identify pairs, map their territories, and track individual survival from year to year through repeat sightings. Photographs and measurements of wing and tail feathers were taken from each individual, as well as small feather samples for genetic analysis. Growth bars visible on the tail feathers can help scientists determine the age of the bird, and comparison of photographs and measurements with results of lab analyses will aid in finding a way to sex Nihoa millerbirds in the field. Development of these methods will ensure that the right numbers of male and female birds are moved to Laysan.\nSeveral Nihoa millerbirds were placed in a temporary enclosure and presented with a selection of island insects. The purpose was to identify millerbird dietary preferences and see if the birds would eat in captivity. Preliminary results showed that the birds fed readily from a plastic container of prey items. Of the choices offered, they left behind only lady bugs, sow-bugs, and ants. One bird was quick to chase down fast-moving cockroaches before taking smaller, slower insects such as spiders and beetles. Another test with a male and female showed that, after a brief adjustment period, the pair fed together without hesitation.\nUsing an iPod and a speaker, the team played millerbird songs within the territories of all 60 banded males and recorded the responses with a microphone. These recordings were used to determine the territories of 20 males and will also be analyzed to determine if differences exist in millerbird songs across Nihoa. Preliminary spectrograph analysis of the recordings shows variety among the songs of male millerbirds, but more research is needed to determine if these differences are significant. Identifying millerbird dialects on such a small spatial scale would be a novel finding and a major accomplishment of the expedition.\n\nThanks to MacDonald and his team, the Service is one step closer to establishing a second population and greatly reducing the risk of extinction for the Nihoa millerbird.\n\nThe birds were provided with different combinations of coloured leg bands to help identification of individual birds.\n\nhttp://www.fws.gov/endangered/news/bulletin-spring2009/research-for-nihoa.html\n"}
{"id": "21544", "url": "https://en.wikipedia.org/wiki?curid=21544", "title": "Nuclear fusion", "text": "Nuclear fusion\n\nIn nuclear physics, nuclear fusion is a reaction in which two or more atomic nuclei are combined to form one or more different atomic nuclei and subatomic particles (neutrons or protons). The difference in mass between the reactants and products is manifested as either the release or absorption of energy. This difference in mass arises due to the difference in atomic \"binding energy\" between the atomic nuclei before and after the reaction. Fusion is the process that powers active or \"main sequence\" stars, or other high magnitude stars.\n\nA fusion process that produces a nucleus lighter than iron-56 or nickel-62 will generally yield a net energy release. These elements have the smallest mass per nucleon and the largest binding energy per nucleon, respectively. Fusion of light elements toward these releases energy (an exothermic process), while a fusion producing nuclei heavier than these elements will result in energy retained by the resulting nucleons, and the resulting reaction is endothermic. The opposite is true for the reverse process, nuclear fission. This means that the lighter elements, such as hydrogen and helium, are in general more fusible; while the heavier elements, such as uranium, thorium and plutonium, are more fissionable. The extreme astrophysical event of a supernova can produce enough energy to fuse nuclei into elements heavier than iron.\n\nIn 1920, Arthur Eddington suggested hydrogen-helium fusion could be the primary source of stellar energy. Quantum tunneling was discovered by Friedrich Hund in 1929, and shortly afterwards Robert Atkinson and Fritz Houtermans used the measured masses of light elements to show that large amounts of energy could be released by fusing small nuclei. Building on the early experiments in nuclear transmutation by Ernest Rutherford, laboratory fusion of hydrogen isotopes was accomplished by Mark Oliphant in 1932. In the remainder of that decade, the theory of the main cycle of nuclear fusion in stars were worked out by Hans Bethe. Research into fusion for military purposes began in the early 1940s as part of the Manhattan Project. Fusion was accomplished in 1951 with the Greenhouse Item nuclear test. Nuclear fusion on a large scale in an explosion was first carried out on November 1, 1952, in the Ivy Mike hydrogen bomb test.\n\nResearch into developing controlled thermonuclear fusion for civil purposes began in earnest in the 1940s, and it continues to this day.\n\nThe release of energy with the fusion of light elements is due to the interplay of two opposing forces: the nuclear force, which combines together protons and neutrons, and the Coulomb force, which causes protons to repel each other. Protons are positively charged and repel each other by the Coulomb force, but they can nonetheless stick together, demonstrating the existence of another, short-range, force referred to as nuclear attraction. Light nuclei (or nuclei smaller than iron and nickel) are sufficiently small and proton-poor allowing the nuclear force to overcome repulsion. This is because the nucleus is sufficiently small that all nucleons feel the short-range attractive force at least as strongly as they feel the infinite-range Coulomb repulsion. Building up nuclei from lighter nuclei by fusion releases the extra energy from the net attraction of particles. For larger nuclei, however, no energy is released, since the nuclear force is short-range and cannot continue to act across longer atomic length scales. Thus, energy is not released with the fusion of such nuclei; instead, energy is required as input for such processes.\n\nFusion powers stars and produces virtually all elements in a process called nucleosynthesis. The Sun is a main-sequence star, and, as such, generates its energy by nuclear fusion of hydrogen nuclei into helium. In its core, the Sun fuses 620 million metric tons of hydrogen and makes 606 million metric tons of helium each second. The fusion of lighter elements in stars releases energy and the mass that always accompanies it. For example, in the fusion of two hydrogen nuclei to form helium, 0.7% of the mass is carried away in the form of kinetic energy of an alpha particle or other forms of energy, such as electromagnetic radiation.\n\nIt takes considerable energy to force nuclei to fuse, even those of the lightest element, hydrogen. When accelerated to high enough speeds, nuclei can overcome this electrostatic repulsion and brought close enough such that the attractive nuclear force is greater than the repulsive Coulomb force. The strong force grows rapidly once the nuclei are close enough, and the fusing nucleons can essentially \"fall\" into each other and the result is fusion and net energy produced. The fusion of lighter nuclei, which creates a heavier nucleus and often a free neutron or proton, generally releases more energy than it takes to force the nuclei together; this is an exothermic process that can produce self-sustaining reactions.\n\nEnergy released in most nuclear reactions is much larger than in chemical reactions, because the binding energy that holds a nucleus together is greater than the energy that holds electrons to a nucleus. For example, the ionization energy gained by adding an electron to a hydrogen nucleus is —less than one-millionth of the released in the deuterium–tritium (D–T) reaction shown in the adjacent diagram. The complete conversion of one gram of matter would release 9×10 joules of energy. Fusion reactions have an energy density many times greater than nuclear fission; the reactions produce far greater energy per unit of mass even though \"individual\" fission reactions are generally much more energetic than \"individual\" fusion ones, which are themselves millions of times more energetic than chemical reactions. Only direct conversion of mass into energy, such as that caused by the annihilatory collision of matter and antimatter, is more energetic per unit of mass than nuclear fusion.\n\nResearch into using fusion for the production of electricity has been pursued for over 60 years. Successful accomplishment of controlled fusion has been stymied by scientific and technological difficulties; nonetheless, important progress has been made. At present, controlled fusion reactions have been unable to produce break-even (self-sustaining) controlled fusion. The two most advanced approaches for it are magnetic confinement (toroid designs) and inertial confinement (laser designs).\n\nWorkable designs for a toroidal reactor that theoretically will deliver ten times more fusion energy than the amount needed to heat plasma to the required temperatures are in development (see ITER). The ITER facility is expected to finish its construction phase in 2019. It will start commissioning the reactor that same year and initiate plasma experiments in 2020, but is not expected to begin full deuterium-tritium fusion until 2027.\n\nThe US National Ignition Facility, which uses laser-driven inertial confinement fusion, was designed with a goal of break-even fusion; the first large-scale laser target experiments were performed in June 2009 and ignition experiments began in early 2011.\n\nAn important fusion process is the stellar nucleosynthesis that powers stars and the Sun. In the 20th century, it was recognized that the energy released from nuclear fusion reactions accounted for the longevity of stellar heat and light. The fusion of nuclei in a star, starting from its initial hydrogen and helium abundance, provides that energy and synthesizes new nuclei as a byproduct of the fusion process. Different reaction chains are involved, depending on the mass of the star (and therefore the pressure and temperature in its core).\n\nAround 1920, Arthur Eddington anticipated the discovery and mechanism of nuclear fusion processes in stars, in his paper \"The Internal Constitution of the Stars\". At that time, the source of stellar energy was a complete mystery; Eddington correctly speculated that the source was fusion of hydrogen into helium, liberating enormous energy according to Einstein's equation \"E = mc\". This was a particularly remarkable development since at that time fusion and thermonuclear energy, and even that stars are largely composed of hydrogen (see metallicity), had not yet been discovered. Eddington's paper, based on knowledge at the time, reasoned that:\n\nAll of these speculations were proven correct in the following decades.\n\nThe primary source of solar energy, and similar size stars, is the fusion of hydrogen to form helium (the proton-proton chain reaction), which occurs at a solar-core temperature of 14 million kelvin. The net result is the fusion of four protons into one alpha particle, with the release of two positrons and two neutrinos (which changes two of the protons into neutrons), and energy. In heavier stars, the CNO cycle and other processes are more important. As a star uses up a substantial fraction of its hydrogen, it begins to synthesize heavier elements. The heaviest elements are synthesized by fusion that occurs as a more massive star undergoes a violent supernova at the end of its life, a process known as supernova nucleosynthesis.\n\nA substantial energy barrier of electrostatic forces must be overcome before fusion can occur. At large distances, two naked nuclei repel one another because of the repulsive electrostatic force between their positively charged protons. If two nuclei can be brought close enough together, however, the electrostatic repulsion can be overcome by the quantum effect in which nuclei can tunnel through coulomb forces.\n\nWhen a nucleon such as a proton or neutron is added to a nucleus, the nuclear force attracts it to all the other nucleons of the nucleus (if the atom is small enough), but primarily to its immediate neighbours due to the short range of the force. The nucleons in the interior of a nucleus have more neighboring nucleons than those on the surface. Since smaller nuclei have a larger surface area-to-volume ratio, the binding energy per nucleon due to the nuclear force generally increases with the size of the nucleus but approaches a limiting value corresponding to that of a nucleus with a diameter of about four nucleons. It is important to keep in mind that nucleons are quantum objects. So, for example, since two neutrons in a nucleus are identical to each other, the goal of distinguishing one from the other, such as which one is in the interior and which is on the surface, is in fact meaningless, and the inclusion of quantum mechanics is therefore necessary for proper calculations.\n\nThe electrostatic force, on the other hand, is an inverse-square force, so a proton added to a nucleus will feel an electrostatic repulsion from \"all\" the other protons in the nucleus. The electrostatic energy per nucleon due to the electrostatic force thus increases without limit as nuclei atomic number grows.\n\nThe net result of the opposing electrostatic and strong nuclear forces is that the binding energy per nucleon generally increases with increasing size, up to the elements iron and nickel, and then decreases for heavier nuclei. Eventually, the binding energy becomes negative and very heavy nuclei (all with more than 208 nucleons, corresponding to a diameter of about 6 nucleons) are not stable. The four most tightly bound nuclei, in decreasing order of binding energy per nucleon, are , , , and . Even though the nickel isotope, , is more stable, the iron isotope is an order of magnitude more common. This is due to the fact that there is no easy way for stars to create through the alpha process.\n\nAn exception to this general trend is the helium-4 nucleus, whose binding energy is higher than that of lithium, the next heaviest element. This is because protons and neutrons are fermions, which according to the Pauli exclusion principle cannot exist in the same nucleus in exactly the same state. Each proton or neutron's energy state in a nucleus can accommodate both a spin up particle and a spin down particle. Helium-4 has an anomalously large binding energy because its nucleus consists of two protons and two neutrons, so all four of its nucleons can be in the ground state. Any additional nucleons would have to go into higher energy states. Indeed, the helium-4 nucleus is so tightly bound that it is commonly treated as a single quantum mechanical particle in nuclear physics, namely, the alpha particle.\n\nThe situation is similar if two nuclei are brought together. As they approach each other, all the protons in one nucleus repel all the protons in the other. Not until the two nuclei actually come close enough for long enough so the strong nuclear force can take over (by way of tunneling) is the repulsive electrostatic force overcome. Consequently, even when the final energy state is lower, there is a large energy barrier that must first be overcome. It is called the Coulomb barrier.\n\nThe Coulomb barrier is smallest for isotopes of hydrogen, as their nuclei contain only a single positive charge. A diproton is not stable, so neutrons must also be involved, ideally in such a way that a helium nucleus, with its extremely tight binding, is one of the products.\n\nUsing deuterium-tritium fuel, the resulting energy barrier is about 0.1 MeV. In comparison, the energy needed to remove an electron from hydrogen is 13.6 eV, about 7500 times less energy. The (intermediate) result of the fusion is an unstable He nucleus, which immediately ejects a neutron with 14.1 MeV. The recoil energy of the remaining He nucleus is 3.5 MeV, so the total energy liberated is 17.6 MeV. This is many times more than what was needed to overcome the energy barrier.\n\nThe reaction cross section σ is a measure of the probability of a fusion reaction as a function of the relative velocity of the two reactant nuclei. If the reactants have a distribution of velocities, e.g. a thermal distribution, then it is useful to perform an average over the distributions of the product of cross section and velocity. This average is called the 'reactivity', denoted <σv>. The reaction rate (fusions per volume per time) is <σv> times the product of the reactant number densities:\n\nIf a species of nuclei is reacting with a nucleus like itself, such as the DD reaction, then the product formula_2 must be replaced by formula_3.\n\nformula_4 increases from virtually zero at room temperatures up to meaningful magnitudes at temperatures of 10–100 keV. At these temperatures, well above typical ionization energies (13.6 eV in the hydrogen case), the fusion reactants exist in a plasma state.\n\nThe significance of formula_4 as a function of temperature in a device with a particular energy confinement time is found by considering the Lawson criterion. This is an extremely challenging barrier to overcome on Earth, which explains why fusion research has taken many years to reach the current advanced technical state.\n\nIf matter is sufficiently heated (hence being plasma), fusion reactions may occur due to collisions with extreme thermal kinetic energies of the particles. Thermonuclear weapons produce what amounts to an uncontrolled release of fusion energy. Controlled thermonuclear fusion energy has yet to be achieved.\n\nInertial confinement fusion (ICF) is a method aimed at releasing fusion energy by heating and compressing a fuel target, typically a pellet containing deuterium and tritium.\n\nInertial electrostatic confinement is a set of devices that use an electric field to heat ions to fusion conditions. The most well known is the fusor. Starting in 1999, a number of amateurs have been able to do amateur fusion using these homemade devices. Other IEC devices include: the Polywell, MIX POPS and Marble concepts.\n\nIf the energy to initiate the reaction comes from accelerating one of the nuclei, the process is called \"beam-target\" fusion; if both nuclei are accelerated, it is \"beam-beam\" fusion.\n\nAccelerator-based light-ion fusion is a technique using particle accelerators to achieve particle kinetic energies sufficient to induce light-ion fusion reactions. Accelerating light ions is relatively easy, and can be done in an efficient manner—requiring only a vacuum tube, a pair of electrodes, and a high-voltage transformer; fusion can be observed with as little as 10 kV between the electrodes. The key problem with accelerator-based fusion (and with cold targets in general) is that fusion cross sections are many orders of magnitude lower than Coulomb interaction cross sections. Therefore, the vast majority of ions expend their energy emitting bremsstrahlung radiation and the ionization of atoms of the target. Devices referred to as sealed-tube neutron generators are particularly relevant to this discussion. These small devices are miniature particle accelerators filled with deuterium and tritium gas in an arrangement that allows ions of those nuclei to be accelerated against hydride targets, also containing deuterium and tritium, where fusion takes place, releasing a flux of neutrons. Hundreds of neutron generators are produced annually for use in the petroleum industry where they are used in measurement equipment for locating and mapping oil reserves.\n\nMuon-catalyzed fusion is a fusion process that occurs at ordinary temperatures. It was studied in detail by Steven Jones in the early 1980s. Net energy production from this reaction has been unsuccessful because of the high energy required to create muons, their short 2.2 µs half-life, and the high chance that a muon will bind to the new alpha particle and thus stop catalyzing fusion.\n\nSome other confinement principles have been investigated.\n\nAntimatter-initialized fusion uses small amounts of antimatter to trigger a tiny fusion explosion. This has been studied primarily in the context of making nuclear pulse propulsion, and pure fusion bombs feasible. This is not near becoming a practical power source, due to the cost of manufacturing antimatter alone.\n\nPyroelectric fusion was reported in April 2005 by a team at UCLA. The scientists used a pyroelectric crystal heated from −34 to 7 °C (−29 to 45 °F), combined with a tungsten needle to produce an electric field of about 25 gigavolts per meter to ionize and accelerate deuterium nuclei into an erbium deuteride target. At the estimated energy levels, the D-D fusion reaction may occur, producing helium-3 and a 2.45 MeV neutron. Although it makes a useful neutron generator, the apparatus is not intended for power generation since it requires far more energy than it produces.\n\nHybrid nuclear fusion-fission (hybrid nuclear power) is a proposed means of generating power by use of a combination of nuclear fusion and fission processes. The concept dates to the 1950s, and was briefly advocated by Hans Bethe during the 1970s, but largely remained unexplored until a revival of interest in 2009, due to the delays in the realization of pure fusion.\nProject PACER, carried out at Los Alamos National Laboratory (LANL) in the mid-1970s, explored the possibility of a fusion power system that would involve exploding small hydrogen bombs (fusion bombs) inside an underground cavity. As an energy source, the system is the only fusion power system that could be demonstrated to work using existing technology. However it would also require a large, continuous supply of nuclear bombs, making the economics of such a system rather questionable.\n\nAt the temperatures and densities in stellar cores the rates of fusion reactions are notoriously slow. For example, at solar core temperature (\"T\" ≈ 15 MK) and density (160 g/cm), the energy release rate is only 276 μW/cm—about a quarter of the volumetric rate at which a resting human body generates heat. Thus, reproduction of stellar core conditions in a lab for nuclear fusion power production is completely impractical. Because nuclear reaction rates depend on density as well as temperature and most fusion schemes operate at relatively low densities, those methods are strongly dependent on higher temperatures. The fusion rate as a function of temperature (exp(−\"E\"/\"kT\")), leads to the need to achieve temperatures in terrestrial reactors 10–100 times higher temperatures than in stellar interiors: \"T\" ≈ 0.1–1.0×10 K.\n\nIn artificial fusion, the primary fuel is not constrained to be protons and higher temperatures can be used, so reactions with larger cross-sections are chosen. Another concern is the production of neutrons, which activate the reactor structure radiologically, but also have the advantages of allowing volumetric extraction of the fusion energy and tritium breeding. Reactions that release no neutrons are referred to as \"aneutronic\".\n\nTo be a useful energy source, a fusion reaction must satisfy several criteria. It must:\n\n\nFew reactions meet these criteria. The following are those with the largest cross sections:\n\nFor reactions with two products, the energy is divided between them in inverse proportion to their masses, as shown. In most reactions with three products, the distribution of energy varies. For reactions that can result in more than one set of products, the branching ratios are given.\n\nSome reaction candidates can be eliminated at once. The D-Li reaction has no advantage compared to p- because it is roughly as difficult to burn but produces substantially more neutrons through - side reactions. There is also a p- reaction, but the cross section is far too low, except possibly when \"T\" > 1 MeV, but at such high temperatures an endothermic, direct neutron-producing reaction also becomes very significant. Finally there is also a p- reaction, which is not only difficult to burn, but can be easily induced to split into two alpha particles and a neutron.\n\nIn addition to the fusion reactions, the following reactions with neutrons are important in order to \"breed\" tritium in \"dry\" fusion bombs and some proposed fusion reactors:\n\nThe latter of the two equations was unknown when the U.S. conducted the Castle Bravo fusion bomb test in 1954. Being just the second fusion bomb ever tested (and the first to use lithium), the designers of the Castle Bravo \"Shrimp\" had understood the usefulness of Lithium-6 in tritium production, but had failed to recognize that Lithium-7 fission would greatly increase the yield of the bomb. While Li-7 has a small neutron cross-section for low neutron energies, it has a higher cross section above 5 MeV. The 15 Mt yield was 150% greater than the predicted 6 Mt and caused unexpected exposure to fallout.\n\nTo evaluate the usefulness of these reactions, in addition to the reactants, the products, and the energy released, one needs to know something about the cross section. Any given fusion device has a maximum plasma pressure it can sustain, and an economical device would always operate near this maximum. Given this pressure, the largest fusion output is obtained when the temperature is chosen so that <σv>/T is a maximum. This is also the temperature at which the value of the triple product \"nT\"τ required for ignition is a minimum, since that required value is inversely proportional to <σv>/T (see Lawson criterion). (A plasma is \"ignited\" if the fusion reactions produce enough power to maintain the temperature without external heating.) This optimum temperature and the value of <σv>/T at that temperature is given for a few of these reactions in the following table.\n\nNote that many of the reactions form chains. For instance, a reactor fueled with and creates some , which is then possible to use in the - reaction if the energies are \"right\". An elegant idea is to combine the reactions (8) and (9). The from reaction (8) can react with in reaction (9) before completely thermalizing. This produces an energetic proton, which in turn undergoes reaction (8) before thermalizing. Detailed analysis shows that this idea would not work well, but it is a good example of a case where the usual assumption of a Maxwellian plasma is not appropriate.\n\nAny of the reactions above can in principle be the basis of fusion power production. In addition to the temperature and cross section discussed above, we must consider the total energy of the fusion products \"E\", the energy of the charged fusion products \"E\", and the atomic number \"Z\" of the non-hydrogenic reactant.\n\nSpecification of the - reaction entails some difficulties, though. To begin with, one must average over the two branches (2i) and (2ii). More difficult is to decide how to treat the and products. burns so well in a deuterium plasma that it is almost impossible to extract from the plasma. The - reaction is optimized at a much higher temperature, so the burnup at the optimum - temperature may be low. Therefore, it seems reasonable to assume the but not the gets burned up and adds its energy to the net reaction, which means the total reaction would be the sum of (2i), (2ii), and (1):\n\nFor calculating the power of a reactor (in which the reaction rate is determined by the D-D step), we count the - fusion energy \"per D-D reaction\" as \"E\" = (4.03 MeV + 17.6 MeV)×50% + (3.27 MeV)×50% = 12.5 MeV and the energy in charged particles as \"E\" = (4.03 MeV + 3.5 MeV)×50% + (0.82 MeV)×50% = 4.2 MeV. (Note: if the tritium ion reacts with a deuteron while it still has a large kinetic energy, then the kinetic energy of the helium-4 produced may be quite different from 3.5 MeV, so this calculation of energy in charged particles is only an approximation of the average.) The amount of energy per deuteron consumed is 2/5 of this, or 5.0 MeV (a specific energy of about 225 million MJ per kilogram of deuterium).\n\nAnother unique aspect of the - reaction is that there is only one reactant, which must be taken into account when calculating the reaction rate.\n\nWith this choice, we tabulate parameters for four of the most important reactions\n\nThe last column is the neutronicity of the reaction, the fraction of the fusion energy released as neutrons. This is an important indicator of the magnitude of the problems associated with neutrons like radiation damage, biological shielding, remote handling, and safety. For the first two reactions it is calculated as (\"E\"-\"E\")/\"E\". For the last two reactions, where this calculation would give zero, the values quoted are rough estimates based on side reactions that produce neutrons in a plasma in thermal equilibrium.\n\nOf course, the reactants should also be mixed in the optimal proportions. This is the case when each reactant ion plus its associated electrons accounts for half the pressure. Assuming that the total pressure is fixed, this means that particle density of the non-hydrogenic ion is smaller than that of the hydrogenic ion by a factor 2/(\"Z\"+1). Therefore, the rate for these reactions is reduced by the same factor, on top of any differences in the values of <σv>/T. On the other hand, because the - reaction has only one reactant, its rate is twice as high as when the fuel is divided between two different hydrogenic species, thus creating a more efficient reaction.\n\nThus there is a \"penalty\" of (2/(Z+1)) for non-hydrogenic fuels arising from the fact that they require more electrons, which take up pressure without participating in the fusion reaction. (It is usually a good assumption that the electron temperature will be nearly equal to the ion temperature. Some authors, however discuss the possibility that the electrons could be maintained substantially colder than the ions. In such a case, known as a \"hot ion mode\", the \"penalty\" would not apply.) There is at the same time a \"bonus\" of a factor 2 for - because each ion can react with any of the other ions, not just a fraction of them.\n\nWe can now compare these reactions in the following table.\n\nThe maximum value of <σv>/T is taken from a previous table. The \"penalty/bonus\" factor is that related to a non-hydrogenic reactant or a single-species reaction. The values in the column \"inverse reactivity\" are found by dividing 1.24 by the product of the second and third columns. It indicates the factor by which the other reactions occur more slowly than the - reaction under comparable conditions. The column \"Lawson criterion\" weights these results with \"E\" and gives an indication of how much more difficult it is to achieve ignition with these reactions, relative to the difficulty for the - reaction. The next-to-last column is labeled \"power density\" and weights the practical reactivity by \"E\". The final column indicates how much lower the fusion power density of the other reactions is compared to the - reaction and can be considered a measure of the economic potential.\n\nThe ions undergoing fusion in many systems will essentially never occur alone but will be mixed with electrons that in aggregate neutralize the ions' bulk electrical charge and form a plasma. The electrons will generally have a temperature comparable to or greater than that of the ions, so they will collide with the ions and emit x-ray radiation of 10–30 keV energy, a process known as Bremsstrahlung.\n\nThe huge size of the Sun and stars means that the x-rays produced in this process will not escape and will deposit their energy back into the plasma. They are said to be opaque to x-rays. But any terrestrial fusion reactor will be optically thin for x-rays of this energy range. X-rays are difficult to reflect but they are effectively absorbed (and converted into heat) in less than mm thickness of stainless steel (which is part of a reactor's shield). This means the bremsstrahlung process is carrying energy out of the plasma, cooling it.\nThe ratio of fusion power produced to x-ray radiation lost to walls is an important figure of merit. This ratio is generally maximized at a much higher temperature than that which maximizes the power density (see the previous subsection). The following table shows estimates of the optimum temperature and the power ratio at that temperature for several reactions:\n\nThe actual ratios of fusion to Bremsstrahlung power will likely be significantly lower for several reasons. For one, the calculation assumes that the energy of the fusion products is transmitted completely to the fuel ions, which then lose energy to the electrons by collisions, which in turn lose energy by Bremsstrahlung. However, because the fusion products move much faster than the fuel ions, they will give up a significant fraction of their energy directly to the electrons. Secondly, the ions in the plasma are assumed to be purely fuel ions. In practice, there will be a significant proportion of impurity ions, which will then lower the ratio. In particular, the fusion products themselves \"must\" remain in the plasma until they have given up their energy, and \"will\" remain some time after that in any proposed confinement scheme. Finally, all channels of energy loss other than Bremsstrahlung have been neglected. The last two factors are related. On theoretical and experimental grounds, particle and energy confinement seem to be closely related. In a confinement scheme that does a good job of retaining energy, fusion products will build up. If the fusion products are efficiently ejected, then energy confinement will be poor, too.\n\nThe temperatures maximizing the fusion power compared to the Bremsstrahlung are in every case higher than the temperature that maximizes the power density and minimizes the required value of the fusion triple product. This will not change the optimum operating point for - very much because the Bremsstrahlung fraction is low, but it will push the other fuels into regimes where the power density relative to - is even lower and the required confinement even more difficult to achieve. For - and -, Bremsstrahlung losses will be a serious, possibly prohibitive problem. For -, p- and p- the Bremsstrahlung losses appear to make a fusion reactor using these fuels with a quasineutral, isotropic plasma impossible. Some ways out of this dilemma are considered—and rejected—in \"fundamental limitations on plasma fusion systems not in thermodynamic equilibrium\". This limitation does not apply to non-neutral and anisotropic plasmas; however, these have their own challenges to contend with.\n\nIn a classical picture, nuclei can be understood as hard spheres that repel each other through the Coulomb force but fuse once the two spheres come close enough for contact. Estimating the radius of an atomic nuclei as about one femtometer, the energy needed for fusion of two hydrogen is:\n\nformula_6\n\nThis would imply that for the core of the sun, which has a Boltzmann distribution with a temperature of around 1.4 keV, the probability hydrogen would reach the threshold is formula_7, that is, fusion would never occur. However, fusion in the sun does occur due to quantum mechanics.\n\nThe probability that fusion occurs is greatly increased compared to the classical picture, thanks to the smearing of the effective radius as the DeBroglie wavelength as well as quantum tunnelling through the potential barrier. To determine the rate of fusion reactions, the value of most interest is the cross section, which describes the probability that particle will fuse by giving a characteristic area of interaction. An estimation of the fusion cross sectional area is often broken into three pieces:\n\nWhere formula_9 is the geometric cross section, is the barrier transparency and is the reaction characteristics of the reaction. \n\nformula_9 is of the order of the square of the de-Broglie wavelength formula_11 where formula_12 is the reduced mass of the system and formula_13 is the center of mass energy of the system. \n\nMore detailed forms of the cross section can be derived through nuclear physics based models and R matrix theory.\n\nThe Naval Research Lab's plasma physics formulary gives the total cross section in barns as a function of the energy (in keV) of the incident particle towards a target ion at rest fit by the formula:\n\nformula_19 with the following coefficient values:\n\nBosch-Hale also reports a R-matrix calculated cross sections fitting observation data with Padé approximants. With energy in units of keV and cross sections in units of millibarn, the astrophysical factor has the form:\n\nformula_20, with the coefficient values: \n\n\n"}
{"id": "13165796", "url": "https://en.wikipedia.org/wiki?curid=13165796", "title": "Ocean heat content", "text": "Ocean heat content\n\nOceanic heat content (OHC) is the heat stored in the ocean. Oceanography and climatology are the science branches which study ocean heat content. Changes in the ocean heat content play an important role in the sea level rise, because of thermal expansion. It is with high confidence that ocean warming accounts for 90% of the energy accumulation from global warming between 1971 and 2010. About one third of that extra heat has been estimated to propagate to depth below 700 meters.\n\nThe areal density of ocean heat content between two depth levels is defined using a definite integral:\n\nformula_1\n\nwhere formula_2 is seawater density, formula_3 is the specific heat of sea water, h2 is the lower depth, h1 is the upper depth, and formula_4 is the temperature profile. In SI units, formula_5 has units of J·m. Integrating this density over an ocean basin, or entire ocean, gives the total heat content, as indicated in the figure to right. Thus, the total heat content is the product of the density, specific heat capacity, and the volume integral of temperature over the three-dimensional region of the ocean in question.\n\nOcean heat content can be estimated using temperature measurements obtained by a Nansen bottle, an ARGO float, or ocean acoustic tomography. The World Ocean Database Project is the largest database for temperature profiles from all of the world’s oceans.\n\nThe upper Ocean heat content in most North Atlantic regions is dominated by heat transport convergence (a location where ocean currents meet), without large changes to temperature and salinity relation.\n\nSeveral studies in recent years have found a multi-decadal oscillation increase in OHC of the deep and upper ocean regions and attribute the heat uptake to anthropogenic warming. Studies based on \"ARGO\" indicate that ocean surface winds, especially the subtropical trade winds in the Pacific Ocean, change ocean heat vertical distribution. This results in changes among ocean currents, and an increase of the subtropical overturning, which is also related to the El Niño and La Niña phenomenon. Depending on stochastic natural variability fluctuations, during La Niña years around 30% more heat from the upper ocean layer is transported into the deeper ocean.\nModel studies indicate that ocean currents transport more heat into deeper layers during La Niña years, following changes in wind circulation. Years with increased ocean heat uptake have been associated with negative phases of the interdecadal Pacific oscillation (IPO). This is of particular interest to climate scientists who use the data to estimate the \"ocean heat uptake\".\n\nA study in 2015 concluded that ocean heat content increases by the Pacific Ocean, were compensated by an abrupt distribution of OHC into the Indian Ocean.\n\n\n"}
{"id": "5634139", "url": "https://en.wikipedia.org/wiki?curid=5634139", "title": "Open mines doctrine", "text": "Open mines doctrine\n\nThe open mines doctrine is a term of real property. Under the open mines doctrine, depletion of natural resources constitutes waste unless consumption of such resources constitutes normal use of the land, as in the case of a life estate in coal mine or a granite quarry. \n\nThe life tenant cannot open the land to search for minerals and other natural resources, but if the quarries or mines were open before the tenant took the life estate, then it is not waste for the life tenant to continue their use. Where the life tenant opens the land for new mines (i.e., voluntary waste) a remainderman can enjoin such.\n\nThe sale of harvestable crops does not constitute waste.\n\n"}
{"id": "43662761", "url": "https://en.wikipedia.org/wiki?curid=43662761", "title": "Partial algebra", "text": "Partial algebra\n\nIn abstract algebra, a partial algebra is a generalization of universal algebra to partial operations.\n\n\nThere is a \"Meta Birkhoff Theorem\" by Andreka, Nemeti and Sain (1982).\n\n"}
{"id": "21147954", "url": "https://en.wikipedia.org/wiki?curid=21147954", "title": "Problem management", "text": "Problem management\n\nProblem management is the process responsible for managing the lifecycle of all problems that happen or could happen in an IT service. The primary objectives of problem management are to prevent problems and resulting incidents from happening, to eliminate recurring incidents, and to minimize the impact of incidents that cannot be prevented. The Information Technology Infrastructure Library defines a problem as the cause of one or more incidents.\n\nProblem Management includes the activities required to diagnose the root cause of incidents identified through the Incident Management process, and to determine the resolution to those problems. It is also responsible for ensuring that the resolution is implemented through the appropriate control procedures, especially Change Management and Release Management.\n\nProblem Management will also maintain information about problems and the appropriate workarounds and resolutions, so that the organization is able to reduce the number and impact of incidents over time. In this respect, Problem Management has a strong interface with Knowledge Management, and tools such as the Known Error Database will be used for both. Although Incident Management and Problem Management are separate processes, they are closely related and will typically use the same tools, and may use similar categorization, impact and priority coding systems. This will ensure effective communication when dealing with related incidents and problems.\n\nProblem Management works together with Incident Management and Change Management to ensure that IT service availability and quality are increased. When incidents are resolved, information about the resolution is recorded. Over time, this information is used to speed up the resolution time and identify permanent solutions, reducing the number and resolution time of incidents. This results in less downtime and less disruption to business critical systems.\n\nProblem Management consists of two major processes:\n\n\nAll the relevant details of the problem must be recorded so that a full historic record exists. This must be date and time stamped to allow suitable control and escalation. A cross-reference must be made to the incident(s) which initiated the \"Problem Record\":\n\nProblems may be categorized according to their severity and priority in the same way as incidents in order to facilitate their tracking, taking the impact of the associated incidents and their frequency of occurrence into account.\nFrom an infrastructure point of view one may ask:\n\nThe result of an investigation for a problem will be a root cause diagnosis or a RCA report. The resolution should be the sum of the appropriate level of resources and skills used to find it.\nThere are a number of useful problem solving techniques that can be used to help diagnosis and resolved problems.\n\n\nThe Pain Value Analysis contains a broader view of the impact of an incident or a problem on the business. Rather than analysing the number of incidents/problems of a particular type in a particular time interval, the technique focus on in-depth analysis of what level of pain has been caused to the business by these incidents/problems. A formula to calculate the level of pain should take into account:\n\nThe Kepner and Tregoe method is used to investigate deeper-rooted problems. They defined the following stages:\n\nPareto Analysis or Pareto chart is a technique for separating important potential causes from trivial issues. The following steps should be taken:\nAfter the investigation is complete and a workaround (or even a permanent solution) has been found, a Known Error Record must be raised and placed in the Known Error Database in order to identify and resolve further similar problems. The main purpose is to restore the affected service as soon as possible with a minimal impact on the business.\n\nA good practice would be to raise a Known Error Record as early in the investigation as possible; once a workaround has been successfully tested or a root cause has been identified.\n\nA good practice is to have a review for all major problems.\nThe review should examine:\n\nThe knowledge learned from the review should be incorporated into a service review with the business customer to ensure that the customer is aware of the actions taken and the plans to prevent future similar incidents from occurring.This helps to improve customer satisfaction and assure the business that Service Operations is handling major incidents responsibly and actively working to prevent their future recurrence..\n\n\n"}
{"id": "26076635", "url": "https://en.wikipedia.org/wiki?curid=26076635", "title": "Race and the War on Drugs", "text": "Race and the War on Drugs\n\nThe War on Drugs is a term for the actions taken and legislation enacted by the [Federal government of the United States|United States government]], intended to reduce or eliminate the production, distribution, and use of illicit drugs. The War on Drugs began during the [[Nixon administration|Nixon Administration]with the goal of reducing the supply of and demand for illegal drugs, though an ulterior, racial motivation has been proposed. The War on Drugs has led to controversial legislation and policies, including [[mandatory minimum]] penalties and stop-and-frisk searches, which have been suggested to be carried out disproportionately against [[minorities]]. The effects of the War on Drugs are contentious, with some suggesting that it has created racial disparities in arrests, prosecutions, imprisonment and rehabilitation. Others have criticized the methodology and conclusions of such studies. In addition to enforcement disparities, some claim that the collateral effects of the War on Drugs have established forms of [[structural violence]], especially for minority communities.\n\nThe [[War on Drugs]] was declared by U.S. President [[Richard Nixon|Richard M. Nixon]] during a Special Message to Congress delivered on June 17, 1971, in response to increasing rates of death due to narcotics. During this announcement, Nixon distinguished between fighting the war on two fronts—the supply front and the demand front. To address the \"supply\" front, Nixon requested funding to train narcotics officers internationally, and proposed various legislation with the intent of disrupting illegal drug manufacturers. The \"demand\" front referred to enforcement and rehabilitation. To that end, Nixon proposed the creation of the Special Action Office of Drug Abuse Prevention, with the goal of coordinating various agencies in addressing demand for illegal drugs. He also requested an additional $105 million for treatment and rehabilitation programs, and additional funding to increase the size and technological capability of the Bureau of Narcotics and Dangerous Drugs.\n\nA former Nixon aide has suggested that the [[War on Drugs]] was racially and politically motivated. However, because he may have been disillusioned with the Nixon administration following the [[Watergate scandal|Watergate]] scandal, the validity of Ehrlichman's claim is disputed.\n\nA number of policies introduced during the War on Drugs have been singled out as particularly racially disproportionate.\n\nThe [[Anti-Drug Abuse Act of 1986]] established a 100:1 sentencing disparity for the possession of crack vs. powder cocaine. Whereas possession of 500 grams of powder cocaine triggered a 5-year [[Mandatory sentencing|mandatory minimum sentence]], possession of 5 grams of crack cocaine triggered the same mandatory minimum penalty. In addition, the [[Anti-Drug Abuse Act of 1986|Anti-Drug Abuse Act of 1988]] established a 1-year mandatory minimum penalty for simple possession of crack cocaine, making crack cocaine the only controlled substance for which a first possession offense triggered a mandatory minimum penalty.\n\nA 1992 study found that, as a result of mandatory minimum sentencing, blacks and Hispanics received more severe sentences than their white counterparts from 1984 through 1990.\n\nIn 1995, the [[United States Sentencing Commission]] delivered a report to Congress concluding that, because 80% of crack offenders were black, the 100:1 disparity disproportionately affected minorities. The Commission recommended that the crack-to-powder sentencing ratio be amended, and that other sentencing guidelines be re-evaluated. These recommendations were rejected by Congress. By contrast, certain authors have pointed out that the [[Congressional Black Caucus]] backed the [[Anti-Drug Abuse Act of 1986]], implying that that law could not be racist.\n\nIn 2010, Congress passed the [[Fair Sentencing Act]], which reduced the sentencing disparity between crack and powder cocaine from 100:1 to 18:1. The mandatory minimum penalty was amended to take effect for possession of crack cocaine in excess of 28 grams.\n\nMandatory minimum penalties have been criticized for failing to apply uniformly to all cases of illegal behavior.\n\nThe [[Supreme Court of the United States|Supreme Court]] ruled in [[Terry v. Ohio]] that a \"stop-and-frisk\" search does not violate the [[Fourth Amendment to the United States Constitution|Fourth Amendment]], so long as the officer executing the search bears a \"reasonable suspicion\" that the person being searched has committed, or is about to commit, a crime. As a result, \"stop-and-frisk\" searches became much more common during the [[War on Drugs]], and were generally conducted in minority communities.\n\n\"Stop-and-frisk\" searches have been criticized for being disproportionately carried out against minorities as a result of racial bias, though empirical literature on this count is inconclusive. Certain authors have found that, even after controlling for location and crime participation rates, African-Americans and Hispanics are stopped more frequently than whites. Others have found that no bias exists, on average, in an officer's decision to stop a citizen, though bias may exist in an officer's decision to frisk a citizen.\n\nStop-and-frisk searches in New York City have declined significantly since 2011. [[File:USA 2009. Percent of adult males incarcerated by race and ethnicity.svg|thumb|400px|2009. Percent of adult males incarcerated by race and ethnicity.]]\n\nA 2015 report conducted by the [[United States Department of Justice|Department of Justice]] found that blacks in Ferguson, Missouri were over twice as likely to be searched during vehicle stops, despite being found in possession of contraband 26% less often than white drivers.\n\nA 2016 report conducted by the [[San Francisco District Attorney's Office]] concluded that racial disparities exist regarding stops, searches, and arrests by the San Francisco Police Department, and that these disparities were especially salient for the black population. Blacks made up almost 42% of all non-consensual searches after a stop, though they accounted for less than 15 percent of all stops in 2015. Blacks held the lowest search \"hit rate\", meaning that contraband was least likely to be found during a search.\n\nA 2016 [[Chicago Police Accountability Task Force]] report found that black and Hispanic drivers were searched by the Chicago Police more than four times more frequently than white drivers, despite white drivers being found with contraband twice as often as black and Hispanic drivers.\n\nA 1995 [[Bureau of Justice Statistics]] report found that from 1991 to 1993, 16% of those who sold drugs were black, but 49% of those arrested for doing so were black.\n\nA 2006 study concluded that blacks were significantly over-represented among those arrested for drug delivery offenses in [[Seattle]]. The same study found that this was a result of law enforcement focusing on crack offenders, on outdoor venues, and dedicating resources to racially heterogeneous neighborhoods.\n\nA 2010 study found little difference across races with regards to the rates of adolescent drug dealing. A 2012 study found that African American youth were less likely than white youth to use or sell drugs, but more likely to be arrested for doing so.\n\nA 2013 study by the [[American Civil Liberties Union]] determined that a black person in the United States was 3.73 times more likely to be arrested for marijuana possession than a white person, even though both races have similar rates of marijuana use. Iowa had the highest racial disparity of the fifty states. Black people in Iowa were arrested for marijuana possession at a rate 8.4 times higher than white people. One factor that may explain the difference in arrest rates between whites and blacks is that blacks are more likely than whites to buy marijuana outdoors, from a stranger, and away from their homes.\n\nA 2015 study concluded that minorities have been disproportionately arrested for drug offenses, and that this difference \"cannot be explained by differences in drug offending, non-drug offending, or residing in the kinds of neighborhoods likely to have heavy police emphasis on drug offending.\"\n\nIn 1998, there were wide racial disparities in arrests, prosecutions, sentencing and deaths. African-Americans, who only comprised 13% of regular drug users, made up for 35% of drug arrests, 55% of convictions, and 74% of people sent to prison for drug possession crimes. Nationwide African-Americans were sent to state prisons for drug offenses 13 times more often than white men.\n\nCrime statistics show that in 1999 in the United States blacks were far more likely to be targeted by law enforcement for drug crimes, and received much stiffer penalties and sentences than whites. A 2000 study found that the disproportionality of black drug offenders in [[Pennsylvania]] prisons was unexplained by higher arrest rates, suggesting the possibility of operative discrimination in sentencing.\n\nA 2008 paper stated that drug use rates among Blacks (7.4%) were comparable to those among Whites (7.2%), meaning that, since there are far more White Americans than Black Americans, 72% of illegal drug users in America are white, while only 15% are black.\n\nAccording to [[Michelle Alexander]], the author of \"[[The New Jim Crow]]\" and a professor of law at Stanford Law School, even though drug trading is done at similar rates all over the U.S., most people arrested for it are colored. Together, African American and Hispanics comprised 58% of all prisoners in 2008, even though African Americans and Hispanics make up approximately one quarter of the US population The majority of prisoners are arrested for drug related crime, and in at least 15 states, 3/4 of them are black or Latino people.\n\nA 2012 report by the [[United States Sentencing Commission]] found that drug sentences for black men were 13.1 percent longer than drug sentences for white men between 2007 and 2009.\n\nProfessor Cathy Schnieder of International Service at American University notes that in 1989, African Americans, representing 12-15 percent of all drug use in the United States, made up 41 percent of all arrests. That is a noted increase from 38 percent in 1988. Whites were 47 percent of those in state-funded treatment centers and made up less than 10 percent of those committed to prison.\n\nSome have suggested that certain [[Supreme Court of the United States|Supreme Court]] rulings related to the War on Drugs have reinforced racially disproportionate treatment.\n\nIn \"United States v. Armstrong\" (1996), the Supreme Court heard the case of Armstrong, a black man charged with conspiring to possess and distribute more than 50 grams of crack cocaine. Facing the District Court, Armstrong claimed that he was singled out for prosecution due to his race and filed a motion for discovery. The District Court granted the motion, and required the government to provide statistics from the prior 3 year on similar crimes, dismissing Armstrong's case when the government refused.\n\nThe government appealed the decision, and the [[United States courts of appeals|U.S. Court of Appeals]] affirmed the dismissal, holding that defendants in selective-prosecution claims need not demonstrate that the government failed to prosecute similarly situated individuals. The case was then sent to the Supreme Court, which reversed the decision and held that defendants must show that the government failed to prosecute similarly situated individuals.\n\nIn \"United States v. Bass\" (2002), the Supreme Court heard a similar case. John Bass was charged with two counts of homicide, and the government sought the death penalty. Bass filed for dismissal, along with a discovery request alleging that death sentences are racially motivated. When the government refused to comply with the discovery request, the District Court dismissed the death penalty notice. Upon appeal, the U.S. Court of Appeals affirmed the dismissal, and the case was sent to the Supreme Court.\n\nReversing the decision, the Supreme Court ruled that a defendant in a selective prosecution case must make a \"credible showing\" of evidence that the prosecution policy in question was intentionally discriminatory. The court ruled that Bass did not do so because he failed to show that similarly situated individuals of different races were treated differently. Specifically, the Court rejected Bass' use of national statistics, holding that they were not representative of similarly situated individuals.\n\nBoth cases have been criticized for perpetuating racially motivated legal standards. It has been suggested that the current standard is impossible to meet for selective prosecution claims, because the relevant data may not exist, and if it does, because the prosecution may have sole access to it.\n\nFelony drug convictions often lead to circumstances that carry negative health-related consequences. Employment opportunities (and associated healthcare benefits), access to public housing and food stamps, and financial support for higher education are all jeopardized, if not eliminated, as a result of such a conviction. In addition, a felony distribution charge often precludes a convict from benefiting from most healthcare programs that receive federal funding.\n\nSome authors have suggested that the [[collateral consequences of criminal conviction]] are more serious than the legal penalties. In many cases, statutes do not require that convicts are informed of these consequences. Many felons cannot be employed by the federal government or work in government jobs, as they do not meet the standards to gain security clearance. Felons convicted of distributing or selling drugs may not enlist in the military.\n\nCertain states are financially incentivized to exclude criminals from access to public housing. All states receive less federal highway funding if they fail to revoke or suspend driver's licenses of drug-related felons.\n\nCollateral consequences, and [[felon disenfranchisement]] in particular, have historically been at least partially racially motivated.\n\nThe War on Drugs has incarcerated high numbers of African-Americans. However, the damage has compounded beyond individuals to affect African-American communities as a whole, with some social scientists suggesting the War on Drugs could not be maintained without societal racism and the manipulation of racial stereotypes.\n\nAfrican-American children are over-represented in [[American juvenile justice system|juvenile hall]] and family court cases, a trend that began during the [[War on Drugs]]. From 1985 to 1999, admissions of blacks under the age of 18 increased by 68%. Some authors posit that this over-representation is because minority juveniles commit crime more often, and commit more serious crimes.\n\nA compounding factor is often the imprisonment of a father. Boys with imprisoned fathers are significantly less likely to develop the skills necessary for success in early education. In addition, African-American youth often turn to gangs to generate income for their families, oftentimes more effectively than at a minimum wage or entry level job. Still, this occurs even as substance abuse, especially marijuana, has largely declined among high school students. In contrast, many black youths drop out of school, are subsequently tried for drug-related crime, and acquire AIDS at disparate levels.\n\nIn addition, the high incarceration rate has led to the juvenile justice system and family courts to use race as a negative heuristic in trials, leading to a reinforcing effect: as more African-Americans are incarcerated, the more the heuristic is enforced in the eyes of the courts. This contributes to yet higher imprisonment rates among African-American children.\n\nHigh numbers of African American arrests and charges of possession show that although the majority of drug users in the United States are white, blacks are the largest group being targeted as the root of the problem. Furthermore, a study by Andrew Golub, Bruce Johnson, and Eloise Dunlap affirms the racial divide in drug arrests, notably marijuana arrests, where blacks with no prior arrests (0.9%) or one prior arrest (4.3%) were nearly twice as likely to be sentenced to jail as their white counterparts (0.4% and 2.3%, respectively). Harboring these emotions can lead to a lack of will to contact the police in case of an emergency by members of African-American communities, ultimately leaving many people unprotected. Disproportionate arrests in African-American communities for drug-related offenses has not only spread fear but also perpetuated a deep distrust for government and what some call racist drug enforcement policy.\n\nAdditionally, a black-white disparity can be seen in probation revocation, where black probationers were revoked at higher rates than white and Hispanic probationers in studies as published under [[Urban Institute|The Urban Institute]].\n\nThe War on Drugs also plays a negative role in the lives of women of color. The number of black women imprisoned in the United States increased at a rate more than twice that of black men, over 800% from 1986 to 1991. During that same period, the percentage of females incarcerated for drug-related offenses more than doubled. In 1989, black and white women had similar levels of drug use during [[pregnancy]]. In spite of this, black women were 10 times as likely as white women to be reported to a child welfare agency for [[prenatal]] drug use. In 1997, of women in state prisons for drug-related crimes, forty-four percent were Hispanic, thirty-nine percent were black, and twenty-three percent were white, quite different from the racial make up shown in percentages of the United States as a whole. Statistics in England, Wales, and Canada are similar. Women of color who are implicated in drug crimes are \"generally poor, uneducated, and unskilled; have impaired mental and physical health; are victims of physical and sexual abuse and mental cruelty; are single mothers with children; lack familial support; often have no prior convictions; and are convicted for a small quantity of drugs\".\n\nAdditionally, these women typically have an economic attachment to, or fear of, male drug traffickers, creating a power paradigm that sometimes forces their involvement in drug-related crimes. Though there are programs to help them, women of color are usually unable to take advantage of social welfare institutions in America due to regulations. For example, women's access to methadone, which suppresses cravings for drugs such as heroin, is restricted by state clinics that set appointment times for women to receive their treatment. If they miss their appointment, (which is likely: drug-addicted women may not have access to transportation and lead chaotic lives), they are denied medical care critical to their recovery. Additionally, while women of color are offered jobs as a form of government support, these jobs often do not have childcare, rendering the job impractical for mothers, who cannot leave their children at home alone.\n\nHowever, with respect to mandatory minimum sentencing, female offenders receive relief almost 20% more often than male offenders. In addition, female offenders, on average, receive lighter sentences than those who commit similar offenses.\n\n\n\n\n\n\n[[Category:Drug policy of the United States]]\n[[Category:Race and crime in the United States]]\n[[Category:Race in the United States]]\n[[Category:Social constructionism]]\n[[Category:Social problems in medicine]]"}
{"id": "11112511", "url": "https://en.wikipedia.org/wiki?curid=11112511", "title": "Radio acoustic sounding system", "text": "Radio acoustic sounding system\n\nA radio acoustic sounding system (RASS) is a system for measuring the atmospheric lapse rate using backscattering of radio waves from an acoustic wave front to measure the speed of sound at various heights above the ground. This is possible because the compression and rarefaction of air by an acoustic wave changes the dielectric properties, producing partial reflection of the transmitted radar signal.\nFrom the speed of sound, the temperature of the air in the planetary boundary layer can be computed.\nThe maximum altitude range of RASS systems is typically 750 meters, although observations have been reported up to 1.2 km in moist air.\n\nThe principle of operation behind RASS is as follows: Bragg scattering occurs when acoustic energy (i.e., sound) is transmitted into the vertical beam of a radar such that the wavelength of the acoustic signal matches the half-wavelength of the radar. As the frequency of the acoustic signal is varied, strongly enhanced scattering of the radar signal occurs when the Bragg match takes place. \n\nWhen this occurs, the Doppler shift of the radar signal produced by the Bragg scattering can be determined, as well as the atmospheric vertical velocity. Thus, the speed of sound as a function of altitude can be measured, from which virtual temperature (TV) profiles can be calculated with appropriate corrections for vertical air motion. The virtual temperature of an air parcel is the temperature that dry air would have if its pressure and density were equal to\nthose of a sample of moist air. As a rule of thumb, an atmospheric vertical velocity of 1 m/s can alter a TV observation by 1.6°C.\n\nRASS can be added to a radar wind profiler or to a sodar system. In the former case, the necessary acoustic subsystems must be added to the radar wind profiler to generate the sound signals and to perform signal processing. When RASS is added to a radar profiler, three or four vertically pointing acoustic sources (equivalent to high quality stereo loud speakers) are placed around the radar wind profiler's antenna, and electronic subsystems are added that include the acoustic power amplifier and the signal generating circuit boards. The acoustic sources are used only to transmit sound into the vertical beam of the radar, and are usually encased in noise suppression enclosures to minimize nuisance effects that may bother nearby neighbors or others in the vicinity of the instrument.\n\nWhen RASS is added to a sodar, the necessary radar subsystems are added to transmit and receive the radar signals and to process the radar reflectivity information. Since the wind data are obtained by the sodar, the radar only needs to sample along the vertical axis. The sodar transducers are used to transmit the acoustic signals that produce the Bragg scattering of the radar signals, which allows the speed of sound to be measured by the radar.\n\nThe vertical resolution of RASS data is determined by the pulse length(s) used by the radar. RASS sampling is usually performed with a 60 to 100 meter pulse length. Because of atmospheric attenuation of the acoustic signals at the RASS frequencies used by boundary layer radar wind profilers, the altitude range that can be sampled is usually 0.1 to 1.5 km, depending on atmospheric conditions (e.g., high wind velocities tend to limit RASS altitude coverage to a few hundred meters because the acoustic signals are blown out of the radar beam).\n"}
{"id": "745202", "url": "https://en.wikipedia.org/wiki?curid=745202", "title": "Reality tunnel", "text": "Reality tunnel\n\nReality tunnel is a theory that, with a subconscious set of mental filters formed from beliefs and experiences, every individual interprets the same world differently, hence \"Truth is in the eye of the beholder\". It is similar to the idea of representative realism, and was coined by Timothy Leary (1920–1996). It was further expanded on by Robert Anton Wilson (1932-2007), who wrote about the idea extensively in his 1983 book \"Prometheus Rising\".\n\nWilson and Leary co-wrote a chapter in Leary's 1988 book \"Neuropolitique\" (a revised edition of the 1977 book \"Neuropolitics\"), in which they explained further: \nThe gene-pool politics which monitor power struggles among terrestrial humanity are transcended in this info-world, i.e. seen as static, artificial charades. One is neither coercively manipulated into another's territorial reality nor forced to struggle against it with reciprocal game-playing (the usual soap opera dramatics). One simply elects, consciously, whether or not to share the other's reality tunnel. \n\nEvery kind of ignorance in the world all results from not realizing that our perceptions are gambles. We believe what we see and then we believe our interpretation of it, we don't even know we are making an interpretation most of the time. We think this is reality. – Robert Anton Wilson \n\nThe idea does not necessarily imply that there is no objective truth; rather that our access to it is mediated through our senses, experience, conditioning, prior beliefs, and other non-objective factors. The implied individual world each person occupies is said to be their reality tunnel. The term can also apply to groups of people united by beliefs: we can speak of the fundamentalist Christian reality tunnel or the ontological naturalist reality tunnel.\n\nA parallel can be seen in the psychological concept of confirmation bias—the human tendency to notice and assign significance to observations that confirm existing beliefs, while filtering out or rationalizing away observations that do not fit with prior beliefs and expectations. This helps to explain why reality tunnels are usually transparent to their inhabitants. While it seems most people take their beliefs to correspond to the \"one true objective reality\", Robert Anton Wilson emphasizes that each person's reality tunnel is their own artistic creation, whether they realize it or not.\n\nWilson—like John C. Lilly and many others—relates that through various techniques one can break down old reality tunnels and impose new reality tunnels by removing old filters and replacing them with new ones, with new perspectives on reality—at will. This is attempted through various processes of deprogramming using neuro-linguistic programming, cybernetics, hypnosis, biofeedback devices, meditation, controlled use of hallucinogens, and forcibly acting out other reality tunnels. Thus, it is believed one's reality tunnel can be widened to take full advantage of human potential and experience reality on more positive levels. Robert Anton Wilson's \"Prometheus Rising\" is (among other things) a guidebook to the exploration of various reality tunnels.\n\nWe don't see things as they are; we see them as we are. – Anaïs Nin\n\nHarvard sociologist Talcott Parsons used the word \"gloss\" to describe how the mind perceives reality. We are taught, he theorised, how to \"put the world together\" by others who subscribe to a consensus reality. \"The curious world of Talcott Parsons was where society was a system, interactive subsystems adhering to a certain set of unwritten rules.\"\n\nThe meme is another source of \"gloss\"; it is \"transmitted from one mind to another through speech, gestures, rituals, or other imitable phenomena.\" Because we're social creatures, there are reasons for us to adopt some social currencies.\n\nIn line with Kantian thought, as well as the work of Norwood Russell Hanson, studies have indeed shown that our brains \"filter\" the data coming from our senses. This \"filtering\" is largely unconscious and may be influenced—more-or-less in many ways, in societies and in individuals—by biology, cultural constructs including education and language (such as memes), life experiences, preferences and mental state, belief systems (e.g. world view, the stock market), momentary needs, pathology, etc.\n\nAn everyday example of such filtering is our ability to follow a conversation, or read, without being distracted by surrounding conversations, once called the cocktail party effect.\n\nIn his 1986 book \"Waking Up\", Charles Tart—an American psychologist and parapsychologist known for his psychological work on the nature of consciousness—introduced the phrase \"consensus trance\" to the lexicon. Tart likened normal waking consciousness to hypnotic trance. He discussed how each of us is from birth inducted to the trance of the society around us. Tart noted both similarities and differences between hypnotic trance induction and consensus trance induction. (See G. I. Gurdjieff).\n\nSome disciplines—Zen for example, and monastic schools such as Sufism—seek to overcome such conditioned realities by returning to less thoughtful and channeled states of mind.\n\nConstructivism is a modern psychological response to reality-tunneling.\nFor Wilson, a fully functioning human ought to be aware of their reality tunnel, and be able to keep it flexible enough to accommodate, and to some degree empathize with, different reality tunnels, different \"game rules\", different cultures... Constructivist thinking is the exercise of metacognition to become aware of our reality tunnels or labyrinths and the elements that \"program\" them. Constructivist thinking should, ideally, decrease the chance that we will confuse our map of the world with the actual world... [This philosophy] is currently expressed in many Eastern consciousness-exploration techniques.\n\nAnother example is Lacan's distinction between \"The Real\" and the \"Symbolic\". Lacan argued that the Real is the imminent unified reality which is mediated through symbols that allow it to be parsed into intelligible and differentiated segments. The symbolic, which is primarily subconscious, is further abstracted into the Imaginary (our actual beliefs and understandings of reality). These two orders ultimately shape the way we come to perceive reality.\n\n\n\n"}
{"id": "177700", "url": "https://en.wikipedia.org/wiki?curid=177700", "title": "Risk aversion", "text": "Risk aversion\n\nIn economics and finance, risk aversion is the behavior of humans (especially consumers and investors), who, when exposed to uncertainty, attempt to lower that uncertainty. It is the hesitation of a person to agree to a situation with an unknown payoff rather than another situation with a more predictable payoff but possibly lower expected payoff. For example, a risk-averse investor might choose to put their money into a bank account with a low but guaranteed interest rate, rather than into a stock that may have high expected returns, but also involves a chance of losing value.\n\nA person is given the choice between two scenarios, one with a guaranteed payoff and one without. In the guaranteed scenario, the person receives $50. In the uncertain scenario, a coin is flipped to decide whether the person receives $100 or nothing. The expected payoff for both scenarios is $50, meaning that an individual who was insensitive to risk would not care whether they took the guaranteed payment or the gamble. However, individuals may have different risk attitudes.\n\nA person is said to be:\n\n\nThe average payoff of the gamble, known as its expected value, is $50. The smallest dollar amount that the individual would accept instead of the bet is called the certainty equivalent, and the difference between the expected value and the certainty equivalent is called the risk premium. For risk-averse individuals, it is positive, for risk-neutral persons it is zero, and for risk-loving individuals their risk premium is negative.\n\nIn expected utility theory, an agent has a utility function \"u\"(\"c\") where \"c\" represents the value that he might receive in money or goods (in the above example \"c\" could be $0 or $40 or $100).\n\nThe utility function \"u\"(\"c\") is defined only up to positive affine transformation – in other words, a constant could be added to the value of \"u\"(\"c\") for all \"c\", and/or \"u\"(\"c\") could be multiplied by a positive constant factor, without affecting the conclusions.\n\nAn agent possesses risk aversion if and only if the utility function is concave. For instance \"u\"(0) could be 0, \"u\"(100) might be 10, \"u\"(40) might be 5, and for comparison \"u\"(50) might be 6.\n\nThe expected utility of the above bet (with a 50% chance of receiving 100 and a 50% chance of receiving 0) is\n\nand if the person has the utility function with \"u\"(0)=0, \"u\"(40)=5, and \"u\"(100)=10 then the expected utility of the bet equals 5, which is the same as the known utility of the amount 40. Hence the certainty equivalent is 40.\n\nThe risk premium is ($50 minus $40)=$10, or in proportional terms\n\nor 25% (where $50 is the expected value of the risky bet: (formula_3). This risk premium means that the person would be willing to sacrifice as much as $10 in expected value in order to achieve perfect certainty about how much money will be received. In other words, the person would be indifferent between the bet and a guarantee of $40, and would prefer anything over $40 to the bet.\n\nIn the case of a wealthier individual, the risk of losing $100 would be less significant, and for such small amounts his utility function would be likely to be almost linear, for instance if u(0) = 0 and u(100) = 10, then u(40) might be 4.0001 and u(50) might be 5.0001.\n\nThe utility function for perceived gains has two key properties: an upward slope, and concavity. (i) The upward slope implies that the person feels that more is better: a larger amount received yields greater utility, and for risky bets the person would prefer a bet which is first-order stochastically dominant over an alternative bet (that is, if the probability mass of the second bet is pushed to the right to form the first bet, then the first bet is preferred). (ii) The concavity of the utility function implies that the person is risk averse: a sure amount would always be preferred over a risky bet having the same expected value; moreover, for risky bets the person would prefer a bet which is a mean-preserving contraction of an alternative bet (that is, if some of the probability mass of the first bet is spread out without altering the mean to form the second bet, then the first bet is preferred).\n\nThere are multiple measures of the risk aversion expressed by a given utility function. Several functional forms often used for utility functions are expressed in terms of these measures.\n\nThe higher the curvature of formula_4, the higher the risk aversion. However, since expected utility functions are not uniquely defined (are defined only up to affine transformations), a measure that stays constant with respect to these transformations is needed. One such measure is the Arrow–Pratt measure of absolute risk aversion (ARA), after the economists Kenneth Arrow and John W. Pratt, also known as the coefficient of absolute risk aversion, defined as\n\nwhere formula_6 and formula_7 denote the first and second derivatives with respect to formula_8 of formula_4.\n\nThe following expressions relate to this term:\n\nThe solution to this differential equation (omitting additive and multiplicative constant terms, which do not affect the behavior implied by the utility function) is:\n\nwhere formula_14 and formula_15.\nNote that when formula_16, this is CARA, as formula_17, and when formula_18, this is CRRA (see below), as formula_19.\nSee \n\nand this can hold only if formula_22. Therefore, DARA implies that the utility function is positively skewed; that is, formula_22. Analogously, IARA can be derived with the opposite directions of inequalities, which permits but does not require a negatively skewed utility function (formula_24). An example of a DARA utility function is formula_25, with formula_26, while formula_27 formula_28, with formula_29 would represent a quadratic utility function exhibiting IARA.\n\nThe Arrow-Pratt measure of relative risk aversion (RRA) or coefficient of relative risk aversion is defined as\n\nLike for absolute risk aversion, the corresponding terms \"constant relative risk aversion\" (CRRA) and \"decreasing/increasing relative risk aversion\" (DRRA/IRRA) are used. This measure has the advantage that it is still a valid measure of risk aversion, even if the utility function changes from risk averse to risk loving as \"c\" varies, i.e. utility is not strictly convex/concave over all \"c\". A constant RRA implies a decreasing ARA, but the reverse is not always true. As a specific example of constant relative risk aversion, the utility function formula_32 implies RRA = 1.\n\nIn intertemporal choice problems, the elasticity of intertemporal substitution often cannot be disentangled from the coefficient of relative risk aversion. The isoelastic utility function\nexhibits constant relative risk aversion with formula_34 and the elasticity of intertemporal substitution formula_35. When formula_36 using l'Hôpital's rule shows that this simplifies to the case of \"log utility,\" \"u\"(\"c\") = log \"c\", and the income effect and substitution effect on saving exactly offset.\n\nA time-varying relative risk aversion can be considered.\n\nThe most straightforward implications of increasing or decreasing absolute or relative risk aversion, and the ones that motivate a focus on these concepts, occur in the context of forming a portfolio with one risky asset and one risk-free asset. If the person experiences an increase in wealth, he/she will choose to increase (or keep unchanged, or decrease) the \"number of dollars\" of the risky asset held in the portfolio if \"absolute\" risk aversion is decreasing (or constant, or increasing). Thus economists avoid using utility functions such as the quadratic, which exhibit increasing absolute risk aversion, because they have an unrealistic behavioral implication.\n\nSimilarly, if the person experiences an increase in wealth, he/she will choose to increase (or keep unchanged, or decrease) the \"fraction\" of the portfolio held in the risky asset if \"relative\" risk aversion is decreasing (or constant, or increasing).\n\nIn one model in monetary economics, an increase in relative risk aversion increases the impact of households' money holdings on the overall economy. In other words, the more the relative risk aversion increases, the more money demand shocks will impact the economy.\n\nIn modern portfolio theory, risk aversion is measured as the additional expected reward an investor requires to accept additional risk. Here risk is measured as the standard deviation of the return on investment, i.e. the square root of its variance. In advanced portfolio theory, different kinds of risk are taken into consideration. They are measured as the n-th root of the n-th central moment. The symbol used for risk aversion is A or A.\n\nThe notion of using expected utility theory to analyze risk aversion has come under criticism from behavioral economics. Matthew Rabin has showed that a risk-averse, expected-utility-maximizing individual who,\n\n\"from any initial wealth level [...] turns down gambles where she loses $100 or gains $110, each with 50% probability [...] will turn down 50-50 bets of losing $1,000 or gaining any sum of money.\"\n\nRabin criticizes this implication of expected utility theory on grounds of implausibility. One solution to the problem observed by Rabin is that proposed by prospect theory and cumulative prospect theory, where outcomes are considered relative to a reference point (usually the status quo), rather than to consider only the final wealth.\n\nAnother limitation is the reflection effect which demonstrates the reversing of risk aversion. This effect was first presented by Kahneman and Tversky as a part of the prospect theory, in the behavioral economics domain.\nThe reflection effect is an identified pattern of opposite preferences between negative prospects as opposed to positive prospects. According to this effect, people tend to avoid risks under the gain domain, and to seek risks under the loss domain. Meaning, no risk aversion is expected under the loss domain. For example, in the gain domain, most people prefer a certain gain of 3000, than a gain of 4000 with a risk of 80 percent. When posing the same problem under the loss domain - with negative values, most people prefer a loss of 4000 with 80 percent chance, over a certain loss of 3000.\n\nThe reflection effect (as well as the certainty effect) is inconsistent with the expected utility hypothesis. It is assumed that the psychological principle which stands behind this kind of behavior is the overweighting of certainty. Meaning, options which are perceived as certain, are over-weighted relative to uncertain options. This pattern is an indication of a risk seeking behavior in negative prospects and eliminates other explanations for the certainty effect such as aversion for uncertainty or variability.\n\nThe initial findings regarding the reflection effect faced criticism regarding its validity, as it was claimed that there are insufficient evidence to support the effect on the individual level. Subsequently, an extensive investigation revealed its possible limitations, suggesting that the effect is most prevalent when either small or large amounts and extreme probabilities are involved.\n\nAttitudes towards risk have attracted the interest of the field of neuroeconomics and behavioral economics. A 2009 study by Christopoulos et al. suggested that the activity of a specific brain area (right inferior frontal gyrus) correlates with risk aversion, with more risk averse participants (i.e. those having higher risk premia) also having higher responses to safer options. This result coincides with other studies, that show that neuromodulation of the same area results in participants making more or less risk averse choices, depending on whether the modulation increases or decreases the activity of the target area.\n\nIn the real world, many government agencies, e.g. Health and Safety Executive, are fundamentally risk-averse in their mandate. This often means that they demand (with the power of legal enforcement) that risks be minimized, even at the cost of losing the utility of the risky activity.\nIt is important to consider the opportunity cost when mitigating a risk; the cost of not taking the risky action. Writing laws focused on the risk without the balance of the utility may misrepresent society's goals. The public understanding of risk, which influences political decisions, is an area which has recently been recognised as deserving focus. In 2007 Cambridge University initiated the Winton Professorship of the Public Understanding of Risk, a role described as outreach rather than traditional academic research by the holder, David Spiegelhalter.\n\nChildren's services such as schools and playgrounds have become the focus of much risk-averse planning, meaning that children are often prevented from benefiting from activities that they would otherwise have had. Many playgrounds have been fitted with impact-absorbing matting surfaces. However, these are only designed to save children from death in the case of direct falls on their heads and do not achieve their main goals. They are expensive, meaning that less resources are available to benefit users in other ways (such as building a playground closer to the child's home, reducing the risk of a road traffic accident on the way to it), and—some argue—children may attempt more dangerous acts, with confidence in the artificial surface. Shiela Sage, an early years school advisor, observes \"Children who are only ever kept in very safe places, are not the ones who are able to solve problems for themselves. Children need to have a certain amount of risk taking ... so they'll know how to get out of situations.\"\n\nA vaccine to protect children against the three common diseases measles, mumps and rubella was developed and recommended for all children in several countries including the UK. However, a controversy arose around fraudulent allegations that it caused autism. This alleged causal link was thoroughly disproved, and the doctor who made the claims was expelled from the General Medical Council. Even years after the claims were disproved, some parents wanted to avert the risk of causing autism in their own children. They chose to spend significant amounts of their own money on alternatives from private doctors. These alternatives carried their own risks which were not balanced fairly, most often that the children were not properly immunized against the more common diseases of measles, mumps and rubella.\n\nMobile phones may carry some small health risk. While most people would accept that unproven risk to gain the benefit of improved communication, others remain so risk averse that they do not. (The COSMOS cohort study continues to study the actual risks of mobile phones.)\n\nOne experimental study with student-subject playing the game of the TV show Deal or No Deal finds that people are more risk averse in the limelight than in the anonymity of a typical behavioral laboratory. In the laboratory treatments, subjects made decisions in a standard, computerized laboratory setting as typically employed in behavioral experiments. In the limelight treatments, subjects made their choices in a simulated game show environment, which included a live audience, a game show host, and video cameras. In line with this, studies on investor behavior find that investors trade more and more speculatively after switching from phone-based to online trading and that investors tend to keep their core investments with traditional brokers and use a small fraction of their wealth to speculate online.\n\n\n"}
{"id": "56538", "url": "https://en.wikipedia.org/wiki?curid=56538", "title": "Rule of thumb", "text": "Rule of thumb\n\nThe English phrase rule of thumb refers to a principle with broad application that is not intended to be strictly accurate or reliable for every situation. It refers to an easily learned and easily applied procedure or standard, based on practical experience rather than theory. This usage of the phrase can be traced back to the seventeenth century.\n\nA modern folk etymology holds that the phrase derives from the maximum width of a stick allowed for wife-beating under English law; this belief may have originated in a rumored statement by the eighteenth-century judge Sir Francis Buller that a man may beat his wife with a stick no wider than his thumb. The rumor produced numerous jokes and satirical cartoons at Buller's expense; however, there is no record that he made such a statement.\n\nThe English jurist William Blackstone wrote in his \"Commentaries on the Laws of England\" of an \"old law\" that once allowed \"moderate\" beatings by husbands, but did not mention thumbs or any specific implements. While wife beating has been officially outlawed for centuries in England and the United States, it continued in practice; several nineteenth-century American court rulings referred to an \"ancient doctrine\" that the judges believed had allowed husbands to physically punish their wives using implements no thicker than their thumbs.\n\nThe exact phrase \"rule of thumb\" first became associated with domestic abuse in the 1970s, after which the spurious legal definition was cited as factual in a number of law journals, and the U.S. Commission on Civil Rights published a report on domestic abuse titled \"Under the Rule of Thumb\" in 1982.\n\nIn English, \"rule of thumb\" refers to an approximate method for doing something, based on practical experience rather than theory. The exact origin of the phrase is uncertain. Its earliest (1685) appearance in print comes from a posthumously published collection of sermons by Scottish preacher James Durham: \"Many profest Christians are like to foolish builders, who build by guess, and by rule of thumb (as we use to speak), and not by Square and Rule\".\n\nThe phrase is also found in Sir William Hope's \"The Compleat Fencing Master\", 1692: \"What he doth, he doth by rule of Thumb, and not by Art\". James Kelly's \"The Complete Collection of Scottish Proverbs\", 1721, includes: \"No Rule so good as Rule of Thumb, if it hit\", meaning a practical approximation.\n\nHistorically, the width of the thumb, or \"thumb's breadth\", was used as the equivalent of an inch in the cloth trade; similar expressions existed in Latin and French as well. The thumb has also been used in brewing beer, to gauge the heat of the brewing vat. Ebenezer Cobham Brewer writes that \"rule of thumb\" means a \"rough measurement\". He says that \"Ladies often measure yard lengths by their thumb. Indeed, the expression 'sixteen nails make a yard' seems to point to the thumb-nail as a standard\" and that \"Countrymen always measure by their thumb\". According to \"Phrasefinder\", \"The phrase joins the whole nine yards as one that probably derives from some form of measurement but which is unlikely ever to be definitively pinned down\".\n\nA modern folk etymology relates the phrase to domestic violence via an alleged rule under English law that allowed for wife beating provided the implement used was a rod or stick no thicker than a man's thumb. While wife beating has been officially outlawed in England (and later, the United States) for centuries, enforcement of the law was inconsistent, and wife beating did continue. However, such a \"rule of thumb\" was never codified in law.\n\nThe English jurist William Blackstone wrote in the late 1700s in his \"Commentaries on the Laws of England\" that by an \"old law\", a husband was justified in using \"moderate correction\" against his wife, but was barred from inflicting serious violence. According to Blackstone, by the late 1600s this custom was in doubt, and a woman was by then allowed \"security of the peace\" against an abusive husband (Blackstone did not mention either thumbs or sticks). Citing Blackstone, the twentieth-century legal scholar William L. Prosser wrote that there was \"probably no truth to the legend\" that a husband was allowed to beat his wife \"with a stick no thicker than his thumb\".\n\nThe association between the thumb and implements of domestic violence can be traced to the year 1782, when the English judge Sir Francis Buller was ridiculed for purportedly stating that a husband could beat his wife, provided he used a stick no wider than his thumb. There is no record of Buller making such a statement; however, the rumor generated much satirical press, with Buller being mocked as \"Judge Thumb\" in published jokes and cartoons.\n\nIn the following century, several court rulings in the United States referred to a supposed common-law doctrine that the judges believed had once allowed wife beating with an implement smaller than a thumb. None of these courts referred to such a doctrine as a \"rule of thumb\" or endorsed such a rule. However, all allowed for some degree of wife beating so long as it did not result in serious injury.\n\nAn 1824 court ruling in Mississippi stated that a man was entitled to enforce \"domestic discipline\" by striking his wife with a whip or stick no wider than the judge's thumb. In a later case in North Carolina (\"State v. Rhodes\", 1868), the defendant was found to have struck his wife \"with a switch about the size of this fingers\"; the judge found the man not guilty due to the switch being smaller than a thumb. The judgement was upheld by the state supreme court, although the later judge stated:\n\nIn 1873, also in North Carolina, the judge in \"State v. Oliver\" ruled, \"We assume that the old doctrine that a husband had the right to whip his wife, provided that he used a switch no larger than his thumb, is not the law in North Carolina\". These latter two cases were cited by the legal scholar Beirne Stedman when he wrote in a 1917 law review article that an \"old common law rule\" had permitted a husband to use \"moderate personal chastisement on his wife\" so long as he used \"a switch no larger than his thumb\".\n\nBy the late 19th century, most American states had outlawed wife beating; some had severe penalties such as forty lashes or imprisonment for offenders. There was a common belief in parts of the United States that a man was permitted to beat his wife with a stick no wider than his thumb; however, this belief was not connected with the phrase \"rule of thumb\" until the 1970s.\n\nIn the 20th century, public concern with the problem of domestic violence declined at first, and then re-emerged in the 1970s along with the resurgent feminist movement. The first recorded link between wife beating and the phrase \"rule of thumb\" appeared in 1976, in a report on domestic violence by women's-rights advocate Del Martin: \n\nWhile Martin appears to have meant the phrase \"rule of thumb\" only as a figure of speech, some feminist writers treated it as a literal reference to an earlier law. The following year, a book on battered women stated: \n\nDespite this erroneous reading of the common law (which is a set of judicial principles rather than a written law with individual sections) the spurious legal doctrine of the \"rule of thumb\" was soon mentioned in a number of law journals. The myth was repeated in a 1982 report by the United States Commission on Civil Rights on domestic abuse titled \"Under the Rule of Thumb\", as well as a later United States Senate report on the Violence Against Women Act.\n\nIn the late 20th century, some efforts were made to discourage the phrase \"rule of thumb\", which was seen as taboo owing to this false origin. Patricia T. O'Conner, former editor of the \"New York Times Book Review\", described it as \"one of the most persistent myths of political correctness\". During the 1990s, several authors, including the conservative social critic Christina Hoff Sommers, wrote about the false etymology of \"rule of thumb\". Sommers analyzed the history of the phrase and its origin in a misunderstanding of Blackstone's commentary. Nonetheless, the myth persisted in some legal sources into the early 2000s.\n\n\n\n"}
{"id": "17228700", "url": "https://en.wikipedia.org/wiki?curid=17228700", "title": "Schematic-driven layout", "text": "Schematic-driven layout\n\nSchematic driven layout is the concept in integrated circuit layout or PCB layout where the EDA software links the schematic and layout databases. It was one of the first big steps forward in layout software from the days when editing tools were simply handling drawn polygons. \n\nSchematic-driven layout allows for several features that make the layout designer's job easier and faster. One of the most important is that changes to the circuit schematic are easily translated to the layout. Another is that the connections between components in the schematic are graphically displayed in the layout ensuring work is correct by construction.\n\n"}
{"id": "22717084", "url": "https://en.wikipedia.org/wiki?curid=22717084", "title": "Semigroup with two elements", "text": "Semigroup with two elements\n\nIn mathematics, a semigroup with two elements is a semigroup for which the cardinality of the underlying set is two. There are exactly five distinct nonisomorphic semigroups having two elements:\nThe semigroups LO and RO are antiisomorphic. O, and are commutative, LO and RO are noncommutative. LO, RO and are bands and also inverse semigroups.\n\nChoosing the set \"A\" = { 1, 2 } as the underlying set having two elements, sixteen binary operations can be defined in \"A\". These operations are shown in the table below. In the table, a matrix of the form\n\nindicates a binary operation on \"A\" having the following Cayley table.\n\nIn this table:\n\nThe Cayley table for the semigroup ({0,1}, formula_1) is given below:\n\nThis is the simplest non-trivial example of a semigroup that is not a group. This semigroup has an identity element, 1, making it a monoid. It is also commutative. It is not a group because the element 0 does not have an inverse, and is not even a cancellative semigroup because we cannot cancel the 0 in the equation 1·0 = 0·0.\n\nThis semigroup arises in various contexts. For instance, if we choose 1 to be the truth value \"true\" and 0 to be the truth value \"false\" and the operation to be the logical connective \"and\", we obtain this semigroup in logic. It is isomorphic to the monoid {0,1} under multiplication. It is also isomorphic to the semigroup\nunder matrix multiplication.\n\nThe Cayley table for the semigroup (Z,+) is given below:\n\nThis group is isomorphic to the cyclic group Z and the symmetric group S.\n\nLet \"A\" be the three-element set {1, 2, 3}. Altogether, a total of 3 = 19683 different binary operations can be defined on \"A\". 113 of the 19683 binary operations determine 24 nonisomorphic semigroups, or 18 non-equivalent semigroups (with equivalence being isomorphism or anti-isomorphism). \n\nAlgorithms and computer programs have been developed for determining nonisomorphic finite semigroups of a given order. These have been applied to determine the nonisomorphic semigroups of small order. The number of nonisomorphic semigroups with \"n\" elements, for \"n\" a nonnegative integer, is listed under in the On-Line Encyclopedia of Integer Sequences. lists the number of non-equivalent semigroups, and the number of associative binary operations, out of a total of \"n\", determining a semigroup.\n\n"}
{"id": "45229676", "url": "https://en.wikipedia.org/wiki?curid=45229676", "title": "Standard Cost Coding System", "text": "Standard Cost Coding System\n\nA Standard Cost Coding System is a system of cost classification that can be used for benchmarking cost and quantities data. In the Norwegian oil and gas industry, NORSOK Z-014 developed as part of the NORSOK standards. ISO is also developing a Standard Cost coding System as an extension of NORSOK Z-014 under ISO 19008.\n\nTypically a Cost Classification system would be used to classify, activities, resources and product structure. In NORSOK Z-014, these classification taxonomies are called Standard Activity Breakdown (SAB), Code of Resources (COR) and Physical Breakdown Structure (PBS).\n"}
{"id": "11277004", "url": "https://en.wikipedia.org/wiki?curid=11277004", "title": "Stoic categories", "text": "Stoic categories\n\nThe term Stoic categories refers to Stoic ideas regarding categories of being: the most fundamental classes of being for all things. The Stoics believed there were four categories (substance, quality, disposition, relative disposition) which were the ultimate divisions. Since we do not now possess even a single complete work by Zeno of Citium, Cleanthes or Chrysippus what we do know must be pieced together from a number of sources: doxographies and the works of other philosophers who discuss the Stoics for their own purposes.\n\nThe present information comes from Plotinus and Simplicius, with additional evidence from Plutarch of Chaeronea and Sextus Empiricus. According to both Plotinus and Simplicius there were four Stoic categories, to wit:\n\n\nA simple example of the Stoic categories in use is provided by Jacques Brunschwig:\nI am a certain lump of matter, and thereby a substance, an existent something (and thus far that is all); I am a man, and this individual man that I am, and thereby qualified by a common quality and a peculiar one; I am sitting or standing, disposed in a certain way; I am the father of my children, the fellow citizen of my fellow citizens, disposed in a certain way in relation to something else.\n\nStoicism, like Aristotelianism is derived from Platonic and Socratic traditions. The Stoics held that all being (ὄντα) — though not all things (τινά) — are corporeal. They accepted the distinction between concrete bodies and abstract ones, but rejected Aristotle's teaching that purely incorporeal being exists. Thus, they accepted Anaxagoras' idea (as did Aristotle) that if an object is hot, it is because some part of a universal heat body had entered the object. But, unlike Aristotle, they extended the idea to cover all accidents. Thus if an object is red, it would be because some part of a universal red body had entered the object.\n\nIn addition, the Stoics differed from Aristotle in their sharp distinction between concrete and abstract terms. Technically speaking all four Stoic categories are of concrete bodies. For Aristotle \"white, whiteness, heat\", and \"hot\" were qualities. For the Stoics, however, quality refers to \"white\", but not \"whiteness; hot\", but not \"heat.\" Furthermore, they believed that there are concrete bodies with no corresponding abstraction, something that makes no sense in Aristotelian terms.\n\nIt was apparent that the mere distinction between concrete substance and concrete quality was not a sufficient basis for logic. Socrates in the Hippias Major had pointed out problems in Anaxagoras' approach, explaining all attributes through their presence in a body in the way one body may be contained in another. In that dialog, Hippias tried to explain beauty to Socrates. Socrates finds fault with his explanations, that beauty is a beautiful maiden, that beauty is gold, that beauty is health, wealth and a long life.\n\nAristotle solved the problem in proposing that accidental attributes are non-substantial beings that inhere in substances. He defines this presence saying \"By being 'present in a subject' I do not mean present as parts are present in a whole, but being incapable of existence apart from the said subject.\" (\"The Categories\" 1 24–26)\n\nSuch incorporeal presence caused problems to the Stoics in saying that the οὐσία of a thing is its matter. It is easy to understand the problem. If there is an insubstantial being, \"in Athens\" somehow present in Socrates, causing him to be substantially present in Athens we seem to be faced with an infinite regression, for there would seem to be an insubstantial Socrates in the insubstantial Athens in Socrates, in Athens, etc. Ultimately, who is to say who is the real Socrates and what is the real Athens? Similar arguments can be made of Aristotle's other categories. Was there an insubstantial \"running\" in Archimedes causing him to run naked through the streets of Syracuse, shouting out his immortal \"Eureka\"? Was there an insubstantial fist in Athena causing her to strike Aphrodite as the Iliad recounts?\nOnce Hera spoke, Athena dashed off in pursuit,\ndelighted in her heart. Charging Aphrodite, \nshe struck her in the chest with her powerful fist. \n\nIt was the effort to solve the problems raised by the Platonists and Peripatetics that led the Stoics to develop their categories, \"somehow disposed\" and \"somehow disposed in relation to something.\" The fact that Stoicism, rather than either Platonism or Aristotelianism became the prominent philosophy of the ancient world is due in part to the approach they took to the problem.\n\nAccording to Stephen Menn the first two categories, substance and quality, were recognized by Zeno. The fourth category \"somehow disposed in relation to something\" seems to have been developed by the time of Aristo. The third category, \"somehow disposed\" is first seen in Chrysippus.\n\nThe need for relative terms, seen in the fourth category \"somehow disposed in relation to something\" is more obvious than the need for the third category \"somehow disposed\" and so it seems to have arisen first.\n\nAristotle had used relative terms in a somewhat general way. \"Those things are called relative, which, being either said to be of something else or related to something else, are explained by reference to that other thing.\" (\"The Categories\" 6 37–38) Thus he says that \"knowledge\" and \"the thing known\" are relatives. One can certainly consider knowledge as something properly existing in its subject. Aristotle himself recognized a much different kind of relationship. \"In respect of relation there is no proper change; for, without changing, a thing will be now greater and now less or equal, if that with which it is compared has changed in quantity.\" (\"Metaphysics\" 1088 33–35) In the first case, a relative term can be said to be something \"in\" its subject. In the second case, it can not. Thus, the need for \"somehow disposed in relation to something\" to explain how one thing can be relative to another without the presence of anything corporeal in a subject.\n\nAccording to Stephen Menn, the third category, \"somehow disposed\" probably was recognized first in relation to the virtues. According to Socrates, virtue was a sort of knowledge. The wise man will act virtuously, since he will see it as the right thing to do. But the ignorant man can not avoid vice. The Stoic position held that a sage will possess all the virtues in their fullness. Aristo had argued that there is really only one virtue differentiated as \"somehow disposed in relation to something.\" This seemed to be too much like the Megarian position. Chrysippus thus came to see the virtues as distinct bodies, inseparable from each other \"somehow disposed\" in themselves and not in relation to something. Thus the need for the third category.\n\nPlotinus criticized both Aristotle's Categories and those of the Stoics. His student Porphyry however defended Aristotle's scheme. He justified this by arguing that they be interpreted strictly as expressions, rather than as metaphysical realities. The approach can be justified, at least in part, by Aristotle's own words in \"The Categories.\" Boethius' acceptance of Porphyry's interpretation led to their being accepted by Scholastic philosophy.\n\nThe Stoic scheme did not fare as well. Plotinus wrote...\n\n\"Besides, if they make life and soul no more than this \"pneuma,\"\nwhat is the import of that repeated qualification of theirs \"in a\ncertain state,\" their refuge when they are compelled to recognize some\nacting principle apart from body? If not every pneuma is a soul, but\nthousands of them soulless, and only the pneuma in this \"certain\nstate\" is soul, what follows? Either this \"certain state,\" this\nshaping or configuration of things, is a real being or it is nothing.\n\nIf it is nothing, only the pneuma exists, the \"certain state\"\nbeing no more than a word; this leads imperatively to the assertion\nthat Matter alone exists, Soul and God mere words, the lowest alone\nis.\n\nIf on the contrary this \"configuration\" is really existent-\nsomething distinct from the underlie or Matter, something residing\nin Matter but itself immaterial as not constructed out of Matter, then\nit must be a Reason-Principle, incorporeal, a separate Nature.\n\n\n"}
{"id": "4081192", "url": "https://en.wikipedia.org/wiki?curid=4081192", "title": "Sustainable habitat", "text": "Sustainable habitat\n\nA sustainable habitat is an ecosystem that produces food and shelter for people and other organisms, without resource depletion and in such a way that no external waste is produced. Thus the habitat can continue into future tie without external infusions of resource. Such a sustainable habitat may evolve naturally or be produced under the influence of man. A sustainable habitat that is created and designed by human intelligence will mimic nature, if it is to be successful. Everything within it is connected to a complex array of organisms, physical resources and functions. Organisms from many different biomes can be brought together to fulfill various \"ecological niches\".\n\nThe term often refers to sustainable s, which typically involve some form of green building or environmental planning.\n\nIn creating the sustainable habitats, environmental scientists, designers, engineers and architects must consider no element as a waste product to be disposed of somewhere off site, but as a nutrient stream for another process to feed on. Researching ways to interconnect waste streams to production creates a more sustainable society by minimizing pollution.\n\n\n"}
{"id": "57254349", "url": "https://en.wikipedia.org/wiki?curid=57254349", "title": "Suyat", "text": "Suyat\n\nSuyat is the modern collective name of the indigenous scripts of various ethno-linguistic groups in the Philippines prior to Spanish colonization in the 16th century up to the independence era in the 21st century. The scripts are highly varied; nonetheless, the term was used by cultural organizations in the Philippines to denote a unified neutral terminology for Philippine indigenous scripts.\n\nSuyat includes the kulitan script of the Kapampangan people, the badlit script of various Visayan ethnic groups, the iniskaya script of the Eskaya people, the baybayin script of the Tagalog people, the buhid/buid script of the Buhid Mangyan people, the hanunó'o/hanunoo script of the Hanuno'o Mangyan people, the apurahuano/tagbanwa script of the Tagbanwa people, the palaw'an/pala'wan script of the Palaw'an people, the kur-itan script of the Ilokano people, and many other indigenous scripts in the Philippines.\n\nIn 1999, four suyat scripts were inscribed in the UNESCO Memory of the World Programme, under the name \"Philippine Paleographs (Hanunoo, Buid, Tagbanua and Pala’wan)\". The four scripts, hanunó'o/hanunoo, buhid/buid, apurahuano/tagbanwa, and palaw'an/pala'wan, were recognized by UNESCO as the only existing suyat scripts still used by certain Philippine communities in their daily lives. UNESCO also recognized that the four scripts, along with thirteen other suyat scripts, have existed within the Philippine archipelago since the 10th century AD. The \"ambahan\" poetry made with the hanunó'o/hanunoo script was also cited. The inscription of the four suyat scripts was the first documentary heritage of the Philippines to be inscribed in the Memory of the World Programme.\n\nThe diversity of suyat scripts have also established various calligraphy techniques and styles in the Philippines. Each suyat script has its own suyat calligraphy, although all suyat calligraphy are collectively called as Filipino suyat calligraphy for the sake of nationalism. Western-alphabet and Arabic calligraphy, however, are not considered as Filipino suyat calligraphy as the alphabets used did not develop indigenously. The variety of suyat scripts is due to four main factors: the alignment of the archipelagic culture with the Indosphere; the alignment of the archipelagic culture with the Sinosphere; the alignment of the archipelagic culture with both Indosphere and Sinosphere; and non-alignment of archipelagic culture to both Indosphere and Sinosphere.\n\nThe \"National Script Act\" has been in Congress since 2011. The bill aims to revive the usage of all known indigenous scripts in the Philippines. The bill also mandates that each community should teach the script respective to the area's ethnic peoples. The bill does not mandate the government to teach a single specific suyat script for all citizens, as that would demean the other suyat scripts of various ethnic peoples.\n\nDue to lack of congressional and senatorial sessions and support, the bill did not pass into law in the 16th Congress. It was refiled again in 2016 under the 17th Congress.\n\n"}
{"id": "1636145", "url": "https://en.wikipedia.org/wiki?curid=1636145", "title": "The Camp of the Saints", "text": "The Camp of the Saints\n\nThe Camp of the Saints () is a 1973 French novel by author and explorer Jean Raspail. The novel depicts the destruction of Western civilization through Third World mass immigration to France and the West. Almost forty years after its initial publication, the novel returned to the bestseller list in 2011. On its publication, the book received praise from some French literary figures, including Anouilh, Bazin, Cau, Clavel, Deón, Dutourd, Fourastié, Maulnier, and Pauwels. In the U.S., it was praised by William F. Buckley and Max Lerner, and criticized by Linda Chavez and the Southern Poverty Law Center.\n\nRaspail has said his inspiration came while at the French Riviera in 1971, as he was looking out at the Mediterranean.\n\nThe name of the book comes from a passage in the Book of Revelation (Revelation 20:7–9) depicting the apocalypse. Satan influences most of the nations of the Earth to gather for one final battle against \"the camp of the saints,\" before being defeated for eternity:\n\nIn Calcutta, India, Catholic priests promote the adoption of Indian children by those back in Belgium as a form of charity. When the Belgian government realizes that the number of Indian children raised in Belgium has reached 40,000 in just five years, an emergency policy attempts to halt the migration. Desperate for the chance to send their children to what they call a \"land of plenty\", a mob of desperate Indians swarms the consulate. As a Belgian aid worker works through the crowd, an Indian gong farmer begs him to take them back to Europe, to which the worker agrees.\n\nThe worker and farmer bring the crowd to the docks, where there are hundreds of ships once owned by European powers, now suited only for river traffic. Nevertheless, the crowd boards, and a hundred ships soon leave for Europe; conditions on board are cramped, unsanitary and miserable, with some passengers publicly fornicating. As the ships pass \"the straits of Ceylon\", helicopters swarm overhead, capturing images of the refugees on board to be published in Europe. Meanwhile, on the Russian Far East, the Soviet troops see masses of Chinese ready to enter Siberia but are reluctant to fight them.\n\nAs the fleet crosses the Indian Ocean, the political situation in France becomes more charged. At a press conference about the crisis, a French official who offers a speech in praise of the refugees is confronted by a journalist who claims he is merely trying to \"feed the invaders\" and demands to know if France will \"have the courage to stand up to\" the migrants when they reach France. The official decries this question as morally offensive and threatens to throw the journalist out when he continues to yell. Other journalists seek to inflame tensions between the French and Africans and Arabs already living in the country. Over time, these journalists begin to write that the migrant fleet is on a mission to \"enrich, cleanse and redeem the Capitalist West\". At the same time as the fleet is praised by those in Paris, the people of Southern France, terrified of the migrants' arrival, flee to the north.\n\nAs the fleet approaches the Suez Canal, Egyptian forces fire a warning shot, causing the fleet to steer south, around the Cape of Good Hope. To the surprise of observers, the apartheid regime of South Africa floats out barges of food and supplies, which the migrants throw overboard. The international press is thrilled, believing the rejection of these supplies to be a political statement against the apartheid South African regime. Western leaders, confident the refugees will accept supplies from their \"more virtuous\" nations, organize a supply mission, funded by governments, charities, rock stars and major churches, to meet the refugees off São Tomé. However, the fleet does not stop for these barges either, and when a worker from the Papal barge attempts to board one of the ships, he is strangled and thrown overboard. The press attempts to contain coverage of the disaster.\n\nWhen the migrants pass through the Strait of Gibraltar, the French president orders troops to the south and addresses the nation of his plan to repel the migrants. However, in the middle of the address, he breaks down, demands the troops simply follow their consciences instead. Most of the troops immediately desert their posts and join the civilians as they flee north, and the south is quickly overrun by the migrants. Some of the last troops to stand their ground take refuge in a small village, along with Calguès, an old man who has chosen to remain at his home, and Hamadura, a Westernized Indian who is terrified of his \"filthy, brutish\" countrymen and prides himself on having more in common with whites than Indians. The troops in this village, total of nineteen Frenchmen and one Indian, surrounded by what they deem \"occupied territory\", remains the last defense of Western values and \"Free France\" against the immigrants.\n\nThe migrants make their way north, having no desire to assimilate to French culture, but continuing to demand a First World standard of living, even as they flout laws, do not produce, and murder French citizens, such as factory bosses and shopkeepers, as well as the ordinary people who do not welcome them. They are also joined by the immigrants who already reside in Europe, as well as various left-wing and anarchist groups. Across the West, more and more migrants arrive and have children, rapidly growing to outnumber whites. In a matter of months, the white West has been overrun and the pro-immigrant governments are established, while the white people are ordered to share their houses and flats with the immigrants. The village containing the troops is bombed flat by airplanes of the new French government, referred to only as the \"Paris Multiracial Commune\". Within a few years, most Western governments have surrendered. The mayor of New York City is made to share Gracie Mansion with three African-American families from Harlem, migrants gather at coastal ports in West Africa and South Asia and swarm into Europe, Australia, and New Zealand, London is taken over by an organization of non-white residents known as the \"Non-European Commonwealth Committee\" which force the British queen to have her son marry a Pakistani woman, millions of black Africans from around the continent gather at the Limpopo River and invade apartheid South Africa, and only one drunken Soviet soldier stands in the way of hundreds of thousands of Chinese peasants as they overrun Siberia.\n\nThe epilogue reveals that the story was written in the last holdout of the Western world, Switzerland, but international pressure from the new governments, isolating it as a rogue state for not opening its borders, and the internal pro-migrant elements, force it to capitulate as well. Mere hours from the border opening, the author dedicates the book to his grandchildren, in the hopes they will grow up in a world where they will not be ashamed of him for writing such a book.\n\nA translation by Norman Shapiro was published by Scribner in 1975 (). It was republished in mass market paperback format by Ace Books in 1977 (), and in softcover format by The Social Contract Press in 1995 (). The novel is also available in Amazon Kindle format.\n\nAccording to historian of literature , \"The Camp of the Saints\" received a positive reception, with most critics focusing on the \"prophetic\" nature of the story. According to Matthew Connelly and Paul Kennedy, \"[s]ixties radicalism still prevailed in Paris... [and] French intellectuals and bureaucrats swiftly dismissed [the book] as a racist tract\". It was praised by Bernard Pivot and conservative intellectuals such as Michel Déon, Jean Cau and Louis Pauwels. After it was translated to English, Max Lerner said that it had \"irresistible pace of skill and narrative\", while Sidney Hook said that it would \"succeed in shocking and challenging the complacent contemporary mind\". In 1975, \"Time\" magazine panned the novel as a \"bilious tirade\" that only required a response because it \"arrives trailing clouds of praise from French savants, including Dramatist Jean Anouilh ('A haunting book of irresistible force and calm logic'), with the imprint of a respected U.S. publisher and a teasing pre-publication ad campaign ('The end of the white world is near')\". \"Kirkus Reviews\" compared the novel to \"Mein Kampf\", while Jeffrey Hart in the \"National Review\" mocked the rejection of the novel by critics - deriding them as \"respectable, comfortable reviewers\" - and lauded the book, stating \"in freer and more intelligent circles in Europe, the book is a sensation and Raspail is a prize-winner [...] his plot is both simple and brilliant\". Syndicated columnist Garry Wills condemned the embrace of the novel by the \"more 'respectable' channels\" of American right-wing media, including Hart, drawing parallels between the \"racial implications\" of the book and the \"National Review\"'s \"overtly racist analysis\" of school integration efforts. In 1983, Linda Chavez called the novel \"a sickening book\", describing it as \"racist, xenophobic and paranoid\". The December 1994 cover story of \"The Atlantic Monthly\" focused on the themes of the novel, analyzing them in the context of international relations, while describing it as \"the most politically incorrect book in France in the second half of the twentieth century\". This was at about the same time that The Social Contract Press chose to bring it back into U.S. publication.\n\nIn 2001, the Southern Poverty Law Center described it as \"widely revered by American white supremacists and is a sort of anti-immigration analog to \"The Turner Diaries\",\" and in October 2015, condemned the novel as \"the favorite racist fantasy of the anti-immigrant movement in the US.\" Ryan Lenz of the SPLC notes that \"[t]he premise of \"Camp of the Saints\" plays directly into that idea of white genocide. It is the idea that through immigration, if it's left unchecked, the racial character and content of a culture can be undermined to the point of oblivion.\"\n\nIn 2002, Lionel Shriver described the novel as \"both prescient and appalling,\" certainly \"racist\" but \"written with tremendous verbal energy and passion.\" Shriver writes that the book \"gives bilious voice to an emotion whose expression is increasingly taboo in the West, but that can grow only more virulent when suppressed: the fierce resentment felt by majority populations when that status seems threatened.\"\n\nWilliam F. Buckley, Jr. praised the book in 2004 as \"a great novel\" that raised questions on how to respond to massive illegal immigration, and in 2014, Mackubin Thomas Owens noted Buckley's praise of it, while remarking that \"Raspail was ahead of his time in demonstrating that Western civilization had lost its sense of purpose and history—its 'exceptionalism'.\" In 2005, the conservative Chilton Williamson praised the book as \"one of the most uncompromising works of literary reaction in the 20th century.\"\n\nThe book returned to the bestseller list, ranking in the top 5 in bookstores in France as of March 2011. It has been referred to often by Steve Bannon, U.S. President Donald Trump's former chief strategist.\n\n"}
{"id": "31593594", "url": "https://en.wikipedia.org/wiki?curid=31593594", "title": "The potlatch among Athabaskan peoples", "text": "The potlatch among Athabaskan peoples\n\nThe traditional potlatch among Athabaskan peoples was a gathering that combined aspects of competition, peacekeeping and a show of wealth.\n\nThe traditional Athabaskan potlatch had \"social, religious and economic significance.\" It was a gathering that combined aspects of competition, peacekeeping and a show of wealth. During a potlatch, members of the society with a surplus of food and supplies provide these for all members of a clan, and in situations with other clans this sharing of resources is either a competitive showing or one of creating loyalties, and sometimes both simultaneously.\n\nTraditionally the village was centered on the chiefs' house, and this is where potlatches were held. This was because the chief had the biggest cache where the food was stored.\n\nThere were many different reasons to hold a potlatch in Athabaskan culture, including the birth of a child, a surplus of food, or a death in the clan. The most elaborate of Athabaskan potlatches was the mortuary or funeral potlatch. This marked \"the separation of the deceased from society and is the last public expression of grief.\" \n\nThere were slight variations in the funeral and mortuary potlatches depending on the status or role of the member of the clan who had died. Different songs and dances were performed for a warrior than for an elder. Because of the tight-knit manner of a group or clan, usually due to extended family ties, the death of an elder, in particular, had a very large effect on the tribe. The corpse would first be dressed by the women of the clan and be prepared, while the mobilizing and putting together of the funeral would be taken care of by the closest male relative of the deceased. The preparations would differ but the proceedings of the funerals themselves were generally similar.\n\nThe potlatch generally consisted of \"the feast, dancing & singing, oratory, and the distribution of gifts\". The feast was provided by a wealthier member of the group to communicate \"sentiment, affection, familiarity and goodwill.\" Dancing and singing were a reciprocation of the guests to the hosts for their generosity. Stories were told in the same manner, and for entertainment. The act of giving out gifts was possibly the most dynamic aspect of the traditional Athabaskan potlatch. This was a generous act of sharing one's wealth with the rest of the tribe, and simultaneously a show of the abundance and superiority of the host.\n\nModern potlatches still contain many of the traditional aspects of sharing food, giving gifts, singing, dancing and telling stories, but now the purpose has changed. Most modern potlatches can be held for similar reasons, such as a birth or a death, but now they are no longer so much a show of wealth, but a celebration to keep the tradition alive.\n\nAs with other aspects of the potlatch, food plays a significant role in the social structure; providing large amounts of food for the guests is symbolic of the host's love and care for his guests, and also for the nourishment he symbolically provides to future generations. Due to the overabundance of food, leftovers are distributed first to elders, then the remainder of the guests. According to Simeone, doing so is \"a reminder of the host's generosity long after the event.\"\n\nAs the potlatch is attended by dozens to hundreds of people from neighboring villages, and is often hosted by one or two people, food and money are given as a gift to the host by family members, to help offset the cost of feeding so many. Several men of the village are tasked with hunting moose specifically for a potlatch; it is not uncommon for three or more moose to be killed to feed the guests over the two or three days feasting. One account of a potlatch in the village of Tetlin claimed 22 moose were killed and butchered in preparation of a particularly large feast. In addition to moose meat, many other types of wild food are harvested or donated, such as beaver, duck, salmon, and berries. Traditional subsistence foods provide a reminder of the relationship between the Athabaskan and the land which has historically sustained them.\n\nBreakfast and lunch are served each day for potlatch attendees and are less formal than the evening meal. Breakfast is offered each morning; eggs, bacon, coffee, and potatoes are common fare. Lunch consists of soup, sandwiches, and tea. The evening meal is scheduled for a specific time and all potlatch attendees are expected to attend. It is the most significant spiritual and social event of the potlatch and is treated as such by the attendees. Large rolls of white butcher paper are rolled out on the floor to serve as place settings and many participants sit next to each other on the floor. Benches and chairs are provided for elders.\n\nMoose is normally cooked by the men of the potlatch. It is served roasted, fried, and as moose head stew, which consists of the meaty portions of the moose's head mixed with vegetables and rice in large stew pots. Grilled and smoked salmon is served, as is soup made from round whitefish. Wild cranberries and blueberries are incorporated into desserts.\n\nThe traditional wild food is supplemented by store-bought items, most notably black loose leaf tea, which was introduced to the Athabaskan by traders in the 1800s and remains a staple among present day potlatches. Bannock, also known as fry bread, rolls, and salads are also served.\n\nThe celebration of dance for the Athabaskan people plays a vital role in the success of a potlatch. Unlike many cultures that have changed their mourning over the course of many generations, the Athabaskan people have stayed strong and true to the old ways of song and dance.\n\nDancing is a very important part of Athabaskan culture, and it is often the focal point of the potlatch, particularly after the evening meal. During potlatches a variety of songs are sung; the first songs sung of the weeklong event are called the mourning songs. Traditionally new songs are song first to give thanks to the hosts that are sponsoring the potlatch, followed by the singing of old songs.\n\nPoldine Carlo, an Athabaskan from Interior Alaska, noted that \"The potlatch usually lasted for a week. The first night we would have a big potlatch at the community hall and then the mourning songs were sung, the new songs first for the ones the potlatch was being given for and then the old, old songs that we have been singing for a long time. The women would stand in line all across the hall and dance.\"\n\nAn excerpt from the book \"Rifles, Blankets, & Beads\" tells of \"sorry songs\" that were sung at a potlatch that took place in Tanacross: \"At one potlatch in Tonacross, for example, a sequence of sorry songs began with one made especially for the person that had just died. It followed by songs for a young man who died in a house fire 10 years before; a boy who drowned in the river while attempting to draw water and for Elisha's father, who died in the 1960s.\"\n\nThe grieving process within the Alaskan Native community is rather unusual, it is shared not just by the family but by both local and neighboring communities. If one member of the family of the deceased does not show up or is late, this is considered to be such a great insult that tribal chiefs have been known to interrupt potlatches in order to publicly shame those who have arrived late or have not communicated with the rest of the community concerning their loss. Grieving is expected to be shared, and thus the burden of grief is lifted from the individual to the group.\n\nDance is the outlet that allows potlatch attendees to vent their grief; \"sorry songs\" give way to \"happy songs\" and dances, such as the calico dance, where colorful bolts of cloth are passed among female participants, who dance in a large circle. Such dances serve to restore harmony among the relationships within the tribe.\n\nA traditional Athabaskan potlatch is concluded with the giving of gifts. Valuable trade items, traditionally dentalium shells, now largely replaced by rifles, blankets, cash, and beaded items, are collected by the host from members of their mother's moiety and are redistributed by the host to members of his father's moiety in exchange for their contributions of celebration and participation in the potlatch. For instance, an exceptional dancer might be rewarded for her abilities, and likewise a grave digger or pallbearer would be compensated at a funeral potlatch.\n\nAt the conclusion of a potlatch, gifts are piled high in the center of the meeting hall and distributed to guests. By distributing guns and blankets to the assembled guests, the host demonstrates his relationship to and his feelings for his paternal relatives and potential marriage partners. Through these distributions the host gains prestige as he symbolically ensures his guests' existence by giving them guns to hunt with and blankets to keep them warm.\n\nBeaded items are also commonly given as gifts at potlatches, including necklaces, moccasins, gloves, vests, and gun cases. Beads, like the dentalium they have come to replace, are symbolic of social relations that, when expressed in the form of necklaces or sashes, literally surround or embrace the individual. Today Tanacross people see dentalium shells not only as symbols of prestige but also as expressions of affection.\n\nWhile rifles, blankets and beaded items are traditional potlatch gifts, they are not the only ones. Other gifts might include furs, afghans, quilts, moose hide jackets, calico, snowshoes, gloves, hats, coffeepots, enameled plates, snow shovels, suitcases, frying pans and many others both practical and symbolic.\n\nDentalium shells and moose hide jackets are often worn by the host while distributing gifts. In some communities, gifts are given out while wearing gloves. If the gloves are kept, it is believed that the wealth will be stored in them, and eventually return to the giver.\n\nOften a potlatch host will fully deplete their savings and give away their entire material wealth. The potlatch is an honorable ceremony, and in giving everything away, the host gains prestige. A rich man who does not share his possessions is, to an Athabaskan, a stingy man who is to be pitied.\n\n\n"}
{"id": "30435332", "url": "https://en.wikipedia.org/wiki?curid=30435332", "title": "Tiger Mask donation phenomenon", "text": "Tiger Mask donation phenomenon\n\nThe Tiger Mask donation phenomenon is a series of donations of \"randoseru\" (school backpacks) and other items to orphanages around Japan. The first donation happened when someone left ten 30,000-yen backpacks at a child guidance center in Gunma Prefecture on Christmas Day in 2010. A note attached to the bags was signed \"Naoto Date\", the real name of the titular character of \"Tiger Mask\", a popular 1960s and 70s manga about a wrestler who fought for orphans, being raised in an orphanage himself. Since the initial donation, copycat donations have appeared around Japan at various facilities for children, ranging from more backpacks to toys, food, and monetary gifts.\n\nMany of the donations have been made under the name of Naoto Date, a character in the manga and anime \"Tiger Mask\". In the manga and anime, Tiger Mask (whose real name was Naoto Date) was a feared heel wrestler in America who was extremely vicious in the ring. However, he became a face after returning to Japan when a young boy said that he wanted to be a villain like Tiger Mask when he grew up. The boy lived in an orphanage, the same one that Tiger Mask grew up in during his childhood. Feeling that he did not want the boy to idolize a villain, Tiger was inspired to be a heroic wrestler and fights for the children in the orphanage.\n\nOn 25 December 2010, in Maebashi City, Gunma Prefecture, a child guidance center was visited by someone leaving ten 30,000-yen (about US$360) backpacks: five black backpacks wrapped with blue ribbons, and five red backpacks wrapped with pink ribbons. A card attached to the bags simply said:\nPlease use these backpacks for the children.\n\n\"(signed)\" Naoto Date \n\nThe backpacks were to be distributed between six orphans from six different orphanages in Gunma.\n\nA second donation was found by a security guard at the Odawara Child Guidance Center in Kanagawa Prefecture on 1 January 2011. Six backpacks, three black and three red, were left. A note left behind stated:\nA New Year's gift — Naoto Date\n<br>\nThe donation of backpacks in Gunma last year moved me deeply. My heart skipped a beat when I heard the news. Thinking I might do something as well, I offer these gifts. May the Tiger Mask movement live on.\nAs with the ten original backpacks in Gunma, they were left with a note signed by Naoto Date. The backpacks were to be distributed to new arrivals at the local orphanages and other children that Odawara Child Guidance center serves.\n\nJapan has around 580 children's centers that house around 30,000 children.\n\nAs of January 11, 2011, over 100 donations inspired by Tiger Mask have been reported across Japan. After the two initial donations, many other orphanages and children's facilities began to receive copycat donations of backpacks. Some have also received toys, food, and money to help pay for new backpacks. In addition, parallels to other anime and manga concerning orphaned children have also crept into the donation phenomenon.\n\nSometime during the night of March 17 - morning of March 18, three cars parked at an evacuation center in Yamadamachi, Iwate had their tanks filled up and two 20-liter containers of heating oil were left outside the center by an unknown person. An evacuee discovered the donation upon starting his car and noticing his fuel gauge go from nearly empty to full. The evacuee noted: \"We had hardly any fuel left. We were in a real pinch. I'm so grateful for the donation. I reckon it was Tiger Mask.\" The evacuation center noted that they would use the heating oil during the night to conserve fuel due to the ongoing crisis. Other child welfare offices and agencies have reported increased, anonymous donations in the aftermath of the disaster.\n\nBecause tax benefits for charitable donations are limited in Japan, a culture of charity donations has never really taken root. For example, the ratio of charitable donations by individuals in Japan is 52.5%, compared with 82.7% for the US. Also, Japanese culture emphasizes modesty, which means that Japanese people are generally reluctant to do good deeds in public view. Therefore, the Tiger Mask movement assuages both cultural inhibitions towards charitable deeds; it gives people in Japan a reason to donate while allowing them to do so anonymously.\n\nSeveral donators left notes purportedly from other fictional characters from franchises outside the Tiger Mask series. Notable characters who \"donated\" include Rei and Kaji from \"Neon Genesis Evangelion\", Haruhi Suzumiya, and Stitch.\n"}
{"id": "1129421", "url": "https://en.wikipedia.org/wiki?curid=1129421", "title": "Use of torture since 1948", "text": "Use of torture since 1948\n\nTorture, the infliction of severe physical or psychological pain upon an individual to extract information or a confession, or as an illicit extrajudicial punishment, is prohibited by international law and is illegal in most countries. However, it is still used by many governments. The subject of this article is the use of torture since the adoption of the 1948 Universal Declaration of Human Rights (UDHR), which prohibited it.\n\nTorture is widely practiced worldwide: Amnesty International received reports of torture or cruel, inhuman or degrading treatment or punishment in more than 150 countries during the four-year period from 1997 to 2000. These accusations concerned acts against political prisoners in 70 countries and other prisoners and detainees in more than 130 countries. State torture has been extensively documented and studied, often as part of efforts at collective memory and reconciliation in societies that have experienced a change in government. Surveys of torture survivors reveal that torture \"is not aimed primarily at the extraction of information ... Its real aim is to break down the victim's personality and identity.\" When applied indiscriminately, torture is used as a tool of repression and deterrence against dissent and community empowerment.\n\nWhile many states use torture, few wish to be described as doing so, either to their own citizens or to international bodies. So a variety of strategies are used to circumvent their legal and humanitarian duties, including plausible deniability, secret police, \"need to know\", denial that certain activities constitute torture, appeal to various laws (national or international), use of a jurisdictional argument, claim of \"overriding need\", the use of torture by proxy, and so on. Almost all regimes and governments engaging in torture (and other crimes against humanity) consistently deny engaging in it, in spite of overwhelming hearsay and physical evidence from the citizens they tortured. Through both denial and avoidance of prosecution, most people ordering or carrying out acts of torture do not face legal consequences for their actions. UN Special Rapporteur for the Commission on Human Rights, Sir Nigel Rodley, believes that \"impunity continues to be the principal cause of the perpetuation and encouragement of human rights violations and, in particular, torture.\"\n\nWhile states, particularly their prisons, law enforcement, military and intelligence apparatus, are major perpetrators of torture, many non-state actors also engage in it. These include paramilitaries and guerrillas, criminal actors such as organized crime syndicates and kidnappers.\n\nA recent approach to interrogations has been to use techniques such as waterboarding, sexual humiliation and sexual abuse, and dogs to intimidate or pressure prisoners in a manner claimed to be legal under national or international law. Electric shock techniques such as the use of stun belts and tasers have been considered appropriate provided that they are used to \"control\" prisoners or suspects, even non-violent ones, rather than to extract information. These techniques have been widely criticized as torture.\n\nWhile methods of torture are often quite crude, a number of new technologies of control have been used by torturers in recent years. The Brazilian government devised a number of new electrical and mechanical means of torture during the military dictatorship from 1964 to 1985, and proceeded to train military officials from other right-wing Latin American countries in their techniques. One is the use of tasers and electro-shock devices now widely sold to prison authorities around the world. Minor refinements of ancient techniques--including tearing out fingernails and toenails with iron tools, burning the soles of the feet with clothes irons, and probing between the toes with electric wood-burning pencils--are also widely applied. Some African nations employ an iron foot-squeezing device patterned after the medieval French boot.\n\nSubstantial cooperation between states in the methods and coordination of torture has been documented. Through the Phoenix Program, the United States helped South Vietnam co-ordinate a system of detention, torture and assassination of suspected members of the National Liberation Front, or Viet Cong. During the 1980s wars in Central America, the U.S. government provided manuals and training on interrogation that extended to the use of torture (see U.S. Army and CIA interrogation manuals). The manuals were also distributed by Special Forces Mobile Training teams to military personnel and intelligence schools in Colombia, Ecuador, El Salvador, Guatemala, and Peru. The manuals have a chapter devoted to \"coercive techniques\".\n\nThe southern cone governments of South America – Chile, Argentina, Uruguay, Bolivia, Paraguay and Brazil – involved in Operation Condor co-ordinated the disappearance, torture and execution of dissidents in the 1970s. Hundreds were killed in coordinated operations, and the bodies of those recovered were often mutilated and showed signs of torture. This system operated with the knowledge and support of the United States government through the State Department, Central Intelligence Agency and the Defense Department.\n\nThe United States government has, at least since the Bush administration, used the tactic of legal rendition in which suspected terrorists were extradited to countries where they were to be prosecuted for crimes allegedly committed. In the \"war on terror\" this has evolved into extraordinary rendition, the delivery of prisoners or others recently captured, including terrorism suspects, to foreign governments known to practice torture are Egypt, Jordan, Morocco, and Afghanistan. Human rights activists have alleged that the practice amounts to kidnapping for the purpose of torture, or torture by proxy. A related practice is the operation of facilities for imprisonment, and it is widely believed torture, in foreign countries. In November 2005, the Washington Post reported —- citing administration sources —- that such facilities are operated by the CIA in Thailand (until 2004), Afghanistan, and several unnamed Eastern European countries. Human Rights Watch reports that planes associated with rendition have landed repeatedly in Poland and Romania.\n\nThe use of torture is geographically widespread. A review by Amnesty International, which did not use the United Nations Convention Against Torture as its definition of torture, of its case files found \"reports of torture or ill-treatment by state officials in more than 150 countries from 1997 to 2000\". These reports described widespread or persistent patterns of abuse in more than 70 countries and torture-related deaths in more than 80.\n\nTorture has been reported in Afghanistan under each of its recent governments. Under Najibullah's Soviet-backed regime, beating and electric shocks were widely reported. After the mujahidin victory, Afghanistan fell into a state of chaos, and, according to Amnesty International, \"Torture of civilians in their homes has become endemic ... In almost every jail run by the armed political groups, torture is reported to be a part of the daily routine\". The Taliban are likewise reported to have engaged in torture. Since the U.S. overthrow of the Taliban, torture has been reported on several occasions, both by Afghan groups and by U.S. troops. In the Herat region, dominated by the warlord Ismail Khan, Human Rights Watch reported extensive torture in 2002. Torture by US troops has been alleged in news reports by the New York Times. In March 2008 the UK Ministry of Defence claimed that they and the Afghan army had uncovered a Taliban torture chamber where two individuals were believed to have been beaten.\n\nUnder Enver Hoxha's People's Socialist Republic of Albania, torture was widely used. Since its fall, Amnesty International has reported police abuses amounting to torture; the government says it has \"made efforts to punish all acts of torture under the Albanian criminal justice system\".\n\nAccording to Pierre Vidal-Naquet in \"Torture; Cancer of Democracy\" and \"Les Damnees de la Terre\" by Franz Fanon, torture was practiced endemically by the French forces, commanded by General Jacques Massu, bringing together the experience of \"Les Paras\" in the Indo-China War and German troops in the French Foreign Legion.\n\nIn Angola's 27-year civil war, according to Amnesty International, \"many were tortured\" by both sides.<ref name=\"E/CN.4/1994/31\">\"Report of the Special Rapporteur on torture and cruel, inhuman or degrading treatment or punishment\", U.N. Commission on Human Rights.</ref> Since that time, AI has also reported that \"unarmed civilians are being extrajudicially executed and tortured\" in Angola's war against Cabindan separatists.\n\nDuring the so-called \"Dirty War\" carried out in the 1970s, in particular, but not only, by the military dictatorship from 1976 to 1983, tens of thousands of Argentines were \"disappeared\" by the junta, many never to be seen again. The National Commission on the Disappearance of Persons concluded:\n\nIn nearly all the cases brought to the attention of the Commission, the victims speak of acts of torture. Torture was an important element in the methodology of repression. Secret torture centres were set up, among other reasons, to enable the carrying out of torture to be carried out undisturbed.\n\nTorture has been used frequently by the Bahraini government in the 20th century. Notable cases include that of Ian Henderson, a former colonial officer employed in Bahrain who was accused by multiple witnesses of torturing prisoners. Adel Flaifel, a notorious security officer identified by many detainees as having overseen torture, was given immunity under Royal Decree 56 of 2002. Between 1980 and 1998, nine people died in detention as a result of torture, with five more dying shortly after being released as a result of injuries sustained from torture. Reports released by Amnesty International and Human Rights Watch in the 1990s point to the widespread use of torture in Bahraini prisons.\n\nDuring the Bahraini uprising, torture was described by many human rights reports as widespread and systematic. Up to 1866 who make up 64% of detainees reported cases of torture. Three government agencies, namely the Ministry of Interior, the National Security Agency and the Bahrain Defence Force, were involved in interrogating detainees in relation to the events of the uprising. The NSA and MoI followed a systematic practice of physical and psychological mistreatment, which in many cases amounted to torture. Only four of the individuals who alleged torture were arrested by the BDF. The Bahrain Independent Commission of Inquiry have attributed the deaths of five individuals to torture.\n\nTorture was used regularly by the Brazilian dictatorship regime from 1964 to 1977 against dissidents. It included torturing their children, some of whom were less than 2 years old at the time.\n\nThe regime of Augusto Pinochet in Chile in the 1970s used torture extensively against political opponents. Chile's National Commission on Political Imprisonment and Torture (Comisión Nacional sobre Prisión Política y Tortura) concluded in 2004 that torture had been a systematically implemented policy of the government, and recommended reparations. The commission heard the testimony of more than 35,000 witnesses, whose testimonies are to be kept secret for fifty years. Among those tortured were future president Michelle Bachelet, who was held along with her mother at the notorious Villa Grimaldi detention center in the capital Santiago.\n\nAlthough torture was outlawed in China in 1996, a UN investigator found torture to still be widespread in 2005, particularly because the narrow definition of the law, leaving a mark, does not comply with the UN definition.\n\nPeople imprisoned by the communist regime are reportedly tortured.\n\nIn the socialist German Democratic Republic of divided Germany, torture and inhumane and degrading treatment were systematically used by security forces, including the Stasi secret police, against suspected opponents of the regime.\n\nDuring the Algerian War of Independence (1954–1962), the French military used torture against the National Liberation Front and the civilian population. The French interrogators were notorious for the use of man-powered electrical generators on suspects: this form of torture was called \"(la) gégène\".\n\nThat France has provided a pivotal role in the evolution of western torture practices is the central thesis of the French film \"Death Squadrons: The French School\" by Monique Robin.\nThe French had themselves developed practices in defence of its declining empire through the 20th century, setting up torture \"universities\" at \"Poulo Condor\" (now \"Côn Sơn\") – an island off Vietnam (then French Indo-China, subsequently taken over by the United States) and at Philippeville (now \"Skikda\") in Algeria.\n\nPolice abuse remains a reality in France today, while France has been condemned by the European Court of Human Rights (ECHR) for the conditions of detention in prisons, including the use of torture on detainees. Although the law and the Constitution prohibits any kind of torture, such practices happen. In 2004, the Inspector General of the National Police received 469 registered complaints about illegitimate police violence during the first 11 months of the year, down from 500 during the same period in 2003. There were 59 confirmed cases of police violence, compared to 65 in the previous year. In April 2004, the ECHR condemned the government for \"inhumane and degrading treatments\" in the 1997 case of a teenager beaten while in police custody. The court ordered the government to pay Giovanni Rivas $20,500 (15,000 euros) in damages and $13,500 (10,000 euros) in court costs. The head of the police station in Saint-Denis, near Paris, has been forced to resign after allegations of rape and other violence committed by the police force under his orders. Nine investigations concerning police abuse in this police station were carried out in 2005 by the IGS inspection of police. These repeated abuses are said to be one of the causes of the 2005 civil unrest. Conditions in detention centers for illegal aliens have also been widely criticized by human rights NGOs. In 2006 a young 20-year-old Serbian girl accused a policeman of attempting to rape her in such a centre in Bobigny, in the suburbs of Paris, the year before.\n\nDuring the Guatemalan civil war and the repression by the army against civilians and suspected opponents of the military dictatorship, murder (even genocide), torture, rape and inhumane and degrading treatment was systematically used by the Guatemalan armed forces and police. There is evidence that the CIA, in anticommunist campaigns during the 1980s, was involved in these tortures (in Latin America the threat of communism was often used as justification for dictatorship during the Cold War). Thousands of victims were tortured and murdered. For example, Dianna Ortiz, an American nun who was teaching poor Mayan children in the Guatemala highlands, claims that U.S. personnel were present in interrogation and torture rooms in Guatemala City in 1989 when she was kidnapped, taken to a secret prison and repeatedly raped and tortured by Guatemalan right-wing forces. Ortiz survived because of her American citizenship. Sister Ortiz chronicled her experiences and recovery in a book, \"The Blindfold's Eyes\". \"There were other people in the clandestine cell, the clandestine prison, as well, and I could hear terrible screams. Many were killed. I saw some bodies. There were children, as well\", wrote Dianna Ortiz.\n\nIndia has not ratified the UN Convention against Torture. Custodial deaths and extrajudicial killings are on the rise. The Asian Centre for Human Rights released its report, Torture in India 2010, at a press conference in New Delhi. The report stated that, taking 2000 as the base year, custodial deaths have decreased by 41.66% government between 2004–2005 to 2007–2008. This includes 70.72% increase of deaths in prison and 12.60% increase while in police custody.\nThe government has stated that it intends to pass the Anti Torture Act 2010 so it can ratify the UN convention against torture. The bill provides up to a 10-year sentence for physical or mental torture by the police.\n\nArticle 38 of the constitution of the Islamic Republic forbids \"all forms of torture for the purpose of extracting confession or acquiring information\" and the \"compulsion of individuals to testify, confess, or take an oath.\" It also states that \"any testimony, confession, or oath obtained under duress is devoid of value and credence.\" The Islamic Republic itself vehemently denies the existence of torture by the government.\n\nNonetheless, human rights groups and observers, such as Amnesty International, the United Nations, and Human Rights Watch, have complained that torture is frequently used on political prisoners in Iran.\n\nA substantial number of Iranians have been tortured and imprisoned by the religious police. Arya Aramnejad, a singer, was jailed for his song \"Ali Barkhiz\" where he denounces the Islamic regime's crimes during the 2009 Ashura protests. During his time in prison, he was reportedly tortured (sexually humiliated – photographed naked, laughed at, obliged to walk barefooted on aids' patients blood). Farzad Kamangar was repeatedly tortured in prison. Amnesty International reports that Kamangar was repeatedly beaten, flogged, and electrocuted, and that he now suffers from spasms in his arms and legs as a result of the torture. After she died in the custody of Iranian officials, Zahra \"Ziba\" Kazemi-Ahmadabadi, an Iranian-Canadian freelance photographer, was found to show obvious signs of torture, including a skull fracture, broken nose, signs of rape and severe abdominal bruising. Ehsan Fatahian, an Iranian Kurdish activist, was tortured for confession before being executed. Zeynab Jalalian, also a Kurdish activist, is currently ill due to prison conditions and torture. She has been sentenced to death. Other notable victims include Behrouz Javid Tehrani, Habibollah Latifi, Houshang Asadi, Saeed Malekpour, Shirkoh (Bahman) Moarefi, Hossein Khezri, and Akbar Mohammadi.\n\nIn a study of torture in Iran published in 1999, Iranian-born political historian Ervand Abrahamian included Iran along with \"Stalinist Russia, Maoist China, and early modern Europe\" of the Inquisition and witch hunts, as societies that \"can be considered to be in a league of their own\" in the systematic use of torture.\n\nTorture techniques used in the Islamic Republic include:\n\nwhipping, sometimes of the back but most often of the feet with the body tied on an iron bed; the qapani; deprivation of sleep; suspension from ceiling and high walls; twisting of forearms until they broke; crushing of hands and fingers between metal presses; insertion of sharp instruments under the fingernails; cigarette burns; submersion under water; standing in one place for hours on end; mock executions; and physical threats against family members. Of these, the most prevalent was the whipping of soles, obviously because it was explicitly sanctioned by the sharia.\n\nChronicle of Higher Education International, reports that the widespread practice of raping women imprisoned for engaging in political protest has been effective in keeping female college students \"less outspoken and less likely to take part\" in political demonstrations. The journal quotes an Iranian college student as saying, \"most of the girls arrested are raped in jail. Families can't cope with that.\"\n\nSeveral bills passed the Iranian Parliament that would have had Iran joining the international convention on banning torture in 2003 when reformists controlled Parliament, but were rejected by the Guardian Council.\n\nThe government headed by Baathist Saddam Hussein made extensive use of torture, including at the notorious Abu Ghraib prison.\n\nThe post-invasion Iraqi government holds thousands of people in prison. After investigating from July to October 2004, Human Rights Watch found that torture was \"routine and commonplace.\" According to their report,\n\nDespite apparently credible claims that people were fed into Saddam Hussein's plastic shredder (most likely within Abu Ghraib) prior to the 2003 invasion of Iraq, no such device was found after the war. In October 1990, it was alleged that Iraqi soldiers had \"thrown babies from incubators\" during the invasion of Kuwait. This story was supposed to have come from the 'eye-witness testimony' of a 15-year-old Kuwaiti girl, Nurse Nayirah. Years later it emerged that she was the daughter of Saud bin Nasir Al-Sabah, Kuwait's ambassador to the United States, and that the story was the creation of the Hill & Knowlton public relations firm employed by the Kuwaitis.\n\nAfter investigation of continued allegations of torture, the Supreme Court ruled in 1999 that all torture - even moderate physical pressure - was illegal. This decision was praised by human-rights organizations. Despite this reform of the law, Amnesty International continues to express concerns to Israel about treatment which amounts to torture, and remains unhappy about the steps taken by Israel to eliminate torture. Amnesty International stated in 2002:\n\nThe human rights group B'Tselem estimated that 85% of all Palestinian detainees suspected of terrorism are subject to prolonged sleep deprivation; prolonged sight deprivation or sensory deprivation; forced, prolonged maintenance of body positions that grow increasingly painful; confinement in tiny, closet-like spaces; exposure to temperature extremes, such as in deliberately overcooled rooms; prolonged toilet and hygiene deprivation; and degrading treatment, such as forcing detainees to eat and use the toilet at the same time. Allegations have been made of frequent beatings. Such acts violate Article 16 of the United Nations Convention Against Torture. In January 2000, B'Tselem claimed that the Israeli General Security Service's (GSS) methods of interrogation amounted to the five techniques: \"[The] GSS used methods comparable to those used by the British in 1971, viz. sleep deprivation, infliction of physical suffering, and sensory isolation. But the GSS used them for much longer periods, so the resulting pain and suffering were substantially greater. In addition, the GSS used direct violence... Thus... in practice, the GSS methods were substantially more severe than those used by the British in 1971...\"\n\nMau Mau inflicted torture and death on 1,819 Kikuyu during their uprising in the 1950s, along with 58 people of European and Asian descent.\n\nSuspected Hezbollah guerrillas, their families and Lebanese civilian internees were previously detained in the South Lebanon Army (SLA) prison at Khiam in the then Israeli-occupied Southern Lebanon. Torture, including electric shock torture, by the SLA was routine. This was detailed after the end of the occupation in 2000, when Lebanese who freed the prisoners found instruments of torture.\n\nIn 2005, Human Rights Watch documented that Nigerian police in the cities of Enugu, Lagos and Kano routinely practice torture. Dozens of witnesses and survivors stepped forward to testify to repeated, severe beatings, abuse of sexual organs, rape, death threats, injury by shooting, and the denial of food and water. These abuses were used in campaigns against common crime.\n\nSystematic torture was used in conjunction with military occupation in an attempt to quell anti-oil protests by the Ogoni people in the Niger Delta, according to a World Council of Churches report.\n\nChristian pastors in Nigeria have been involved in the torturing and killing of children accused of witchcraft. Church pastors, in an effort to distinguish from the competition, establish their credentials by accusing children of witchcraft. When repeatedly asked to comment about the matter, the Church has refused to comment.\n\nFrom 1961 to 1973, the North Vietnamese and Vietcong held hundreds of Americans captive. Hanoi's Ministry of Public Security's Medical Office (MPSMO) was responsible for \"preparing studies and performing research on the most effective Soviet, French, Communist Chinese and other ...techniques...\" of extracting information from POWs. The MPSMO \"...supervised the use of torture and the use of drugs to induce [American] prisoners to cooperate.\" Its functions also \"...included working with Soviet and Communist Chinese intelligence advisors who were qualified in the use of medical techniques for intelligence purposes.\"\n\nSee Con Son Island for accounts of US torture practices.\n\nThe Palestinian Authority has reportedly practiced torture in the occupied territories over the years. Amnesty International found: \"Torture [by the Palestine Authority] of detainees remained widespread. Seven detainees died in custody. Unlawful killings, including possible extrajudicial executions, continued to be reported.\"\n\nIn 1995, Azzam Rahim, a naturalized American citizen, was arrested by the Palestinian Authority in the West Bank. He was subsequently taken to a prison in Jericho where he was tortured and killed. Rahim's family attempted to sue the PA and the Palestinian Liberation Organization, but the Supreme Court ultimately ruled against them.\n\nMore than 100 cases of torture by Palestinian security services were reported in 2010. Joe Stork, deputy Middle East director at Human Rights Watch, said: \"The reports of torture by Palestinian security services keep rolling in. President Abbas and Prime Minister Fayyad are well aware of the situation. They need to reverse this rampant impunity and make sure that those responsible are prosecuted.\"\n\nAt least six Palestinians have died under torture in PA prisons. According to a report by the Arab Organization for Human Rights in Britain, the PA has used torture on a systematic basis for years. Methods include beatings with cables, pulling out nails, suspension from the ceiling, flogging, kicking, cursing, electric shocks, sexual harassment and the threat of rape. The report went on to say \"Every one of those detainees has been subject to humiliating and degrading treatment and stayed in cells for more than 10 days. The analysis shows that an astonishing 95 percent of the detainees were subjected to severe torture, others feeling the detrimental effects on their health for varying periods.\" The \"Shabeh\", which involves detainees being handcuffed and bound in stress positions for longs stretches of time, is the most widely used form of torture.\n\nIn 2012, after allegedly selling a house in Hebron to a Jewish family, Muhammad Abu Shahala was arrested by the Palestinian Authority, tortured into a confession, and sentenced to death.\n\nHuman Rights Watch reported 147 cases of torture by Hamas in the West Bank during 2011 and that none of the perpetrators had been prosecuted \"despite consistent allegations of severe abuse.\" It further stated that \"Some men said they had needed medical care due to torture and sought to obtain medical records as evidence that they had been tortured, but that hospital officials refused to provide them. Hamas’s rival in the West Bank, the Fatah-dominated Palestinian Authority, arrests and detains Palestinians arbitrarily, including Hamas members or sympathizers, and similarly subjects detainees to torture and abuse.\"\n\nIn another report, Human Rights Watch \"documents cases in which [Palestinian] security forces tortured, beat, and arbitrarily detained journalists, confiscated their equipment, and barred them from leaving the West Bank and Gaza.\" HRW also reported an incident in which \"the Hamas Ministry of Interior summoned a journalist who published an article on torture by Hamas authorities in secret detention facilities, threatened to take legal action against him if he did not publish an apology for the article, and warned him to correct his 'biased' reporting.\"\n\nThe Constitution of Russia forbids arbitrary detention, torture and ill-treatment. Part 2 of Article 21 of the Constitution states that \"no one may be subjected to torture, violence or any other harsh or humiliating treatment or punishment…\". However Russian police are regularly observed practicing torture – including beatings, electric shocks, rape, asphyxiation – in interrogating arrested suspects.\n\nTorture and humiliation, or \"dedovshchina\", are also widespread in Russian's military, according to Human Rights Watch. This is essentially the Russian version of bullying or hazing that is practiced in the American military, however it is often much more brutal. Many young men are killed or commit suicide every year because of it. Amnesty International reported on allegations of Chechen locals, that Russian military forces in Chechnya rape and torture local women with electric shocks, when electric wires are connected to the straps of their bra on their chest.\n\nIn the most extreme cases, hundreds of innocent people from the street were arbitrarily arrested, beaten, tortured, and raped by special police forces (\"Red Terror\"). Such incidents took place not only in Chechnya, but also in the Russian towns of Blagoveshensk, Bezetsk, and Nefteyugansk.\n\nSaudi Arabia officially considers torture illegal under Islamic Law; however, it is widely practiced, as in the case of William Sampson. According to a 2003 report by Amnesty International, \"torture and ill-treatment remained rife.\" Hanny Megally, Executive director of the Middle East and North Africa division of Human Rights Watch, stated in 2002 \"The practice of torture in Saudi Arabia is well documented\", According to the Human Rights Watch \"World Report 2003\", \"Torture under interrogation of political prisoners and criminal suspects continued\", and the 2006 report notes that \"Arbitrary detention, mistreatment and torture of detainees, restrictions on freedom of movement, and lack of official accountability remain serious concerns\".\n\nTorture was widely practiced by the brutal Soviet secret police during the Stalinism era to extract (often false) confessions from suspects often called enemies of the people. One of the most prevalent and effective types of torture was sleep deprivation, nicknamed \"conveyor\" due to interrogators replacing one another to keep the inmate from sleeping. The use of torture was authorized by the Central Committee of the Communist Party and personally by Joseph Stalin. During the Doctor's Plot, Stalin ordered falsely accused physicians to be tortured \"to death\". Torture was still used after Stalin by the KGB but not on the same extent and level.\n\nThe Spanish kingdom categorically denies the existence of torture.\n\nHowever, the Spanish authorities consistently fail to implement recommendations by the Council of Europe's Committee for the Prevention of Torture and the UN Committee Against Torture to combat the use of torture in detention. The UN committee expressed its concern \"about the length of judicial procedures and made reference to reports that indicated that five years had sometimes passed between crime and sentence. The Committee warned that this problem reduces the effect of penal action and discourages people to file complaints.\" It further indicated that \"all members of the Committee were also deeply concerned about the legal practice of five days incommunicado detention\" (since October 2003, a reform of the Criminal Procedure Code has extended that period to a maximum of 13 days).\n\nTorture has reportedly been used in the Adra Prison near Damascus. In 2010, the prison held 7,000 prisoners. The Tadmor Prison in Palmyra was known for harsh conditions, extensive human rights abuse, torture and summary executions. It was closed in 2001 and all remaining political detainees were transferred to other prisons in Syria. However, Tadmor Prison was reopened on 15 June 2011 and 350 individuals arrested for participation in anti-regime demonstrations were transferred there for interrogation and detainment.\n\nA number of captured Israelis have been tortured in Syria. This includes Eli Cohen, who was executed in 1965. In 1955, five Israeli soldiers were captured in a covert operation on the Golan Heights and brutally tortured in a Syrian prison. One of the soldiers, Uri Ilan, committed suicide when falsely informed by his captors that his comrades had been killed. Ilan became a symbol of courage and patriotism in Israel. During the Yom Kippur War, many Israeli prisoners said that they had been tortured by Syrians, and one POW, Avraham Lanir, was tortured to death.\n\nDuring the Syrian Civil War, reports have been made of widespread and systematic torture used by Syrian security forces. This includes electrocution, brutal beatings and sexual assault. Amnesty said of the situation : \"Torture and other ill-treatment in Syria form part of a widespread and systematic attack against the civilian population, carried out in an organized manner and as part of state policy and therefore amount to crimes against humanity.\"\n\nIn April 2009 a video emerged of a United Arab Emirates Royal Sheik, Sheik Issa bin Zayed Al Nahyan (a son of Zayed bin Sultan Al Nahyan) directing the torture of an Afghan grain dealer Mohammed Shah Poor The video includes the man being tortured with a cattle prod to his genitals, sand in his mouth and being run over by a Mercedes SUV. A man in a UAE police uniform is seen on the tape tying the victim's arms and legs, and later holding him down. The official response of the UAE government was that Sheik Issa is the man shown in the video but he did nothing wrong. The incidents depicted in the videotapes were not part of a pattern of behaviour, the Ministry of the Interior said.\n\nDuring the Mau Mau uprising of the 1950s, British colonial forces rounded up more than a million of Kikuyu people — most of them innocent victims of collective punishment — and imprisoned them in concentration camps in order to flush out the Mau Mau insurgency. In order to extract information about the insurgency and terrorize the enemy, the British used brutal methods to force Kenyans to confess and repudiate the Mau Mau oath. Under slogans like \"labor and freedom\" and other variations on \"Arbeit macht frei,\" inmates were worked to death as slave labor filling in mass graves under cruel conditions. Some men were anally raped with knives. Some women had their breasts mutilated and cut off. Eyes were gouged out and ears cut off and skin lacerated with coiled barbed wire. Interrogation involved stuffing a detainee’s mouth with mud and stamping on his throat until he passed out or died. Survivors were sometimes burned alive, while British guards and interrogators took joy and laughed. British troops denying access to medical aid to the detainees were widespread. A former British officer described a British detention camp in Kenya in 1954: \"Short rations, overwork, brutality, humiliating and disgusting treatment, flogging — all in violation of the UN Declaration of Human Rights.\" According to Canon Bewes, a British missionary, there was a \"constant stream of reports of brutalities by police, military and home guards. Some of the people had been using castration instruments and two men had died under castration.\"\n\nAmong the detainees who suffered severe mistreatment was Hussein Onyango Obama, the grandfather of U.S. President Barack Obama. According to his widow, British soldiers forced pins into his fingernails and buttocks and squeezed his testicles between metal rods and two others were castrated. One British settler described the typical British interrogation:\n\nOn 22 November 1954, Colonel Arthur Young sent a letter to Governor Evelyn Baring about the \"inhumanity\" of various parts of the security forces amid his investigations of wrongdoing:\n\nIn January 1955, Baring sent a telegram to Alan Lennox-Boyd, the Secretary of State for the Colonies and a cabinet minister, and told them that eight white European officers who had been accused of serious crimes, including accessory to murder, would be given immunity from prosecution. One district officer was accused of the \"beating up and roasting alive of one African\". A Kenyan Regiment Sergeant and a field intelligence assistance had been implicated in the burning of two further suspects \"during screening operations\". \"I had not myself realised until today that the extension of the principle of clemency to all members of the security forces involved so many cases with Europeans as principals,\" wrote Baring.\n\nIn 1956, Baring's administration devised the \"dilution technique\" — a system of assaults and psychological shocks to detainees, to force the compliance of the toughest Mau Mau supporters. Lennox-Boyd was told that one commander, Terrence Gavaghan, had developed the techniques at the Mwea camps in central Kenya - and he needed permission to treat the worst detainees in a \"rough way\". Baring telegrammed the Colonial Secretary in London asking for his approval to use \"overpowering\" force, and the cabinet minister's approval came within weeks. A ministerial delegation saw firsthand prisoners beaten for refusing to don camp clothes. Ringleaders of the \"Mau Mau moan\" - a chant of defiance - were singled out for special punishment. They were beaten and forced to the ground. Once there, a boot was placed on their throat while mud was forced into their mouths. Gavaghan also explained how difficult detainees would be subjected to the \"third degree\". \"The measures adopted were to be kept awake all night, having water thrown at him and to be beaten up on a variety of pretexts.\"\n\nOne Hanslope Park document is a letter between Kenyan Special Branch police officers about treatment of \"fanatical\" detainees at the Mwea camps.\n\nIn June 1957, Eric Griffith-Jones, the attorney general of the British administration in Kenya, wrote to Baring, detailing the way the regime of abuse at the colony's detention camps was being subtly altered. He said that the mistreatment of the detainees is \"distressingly reminiscent of conditions in Nazi Germany or Communist Russia\". Despite this, he said that in order for abuse to remain legal, Mau Mau suspects must be beaten mainly on their upper body, \"vulnerable parts of the body should not be struck, particularly the spleen, liver or kidneys\", and it was important that \"those who administer violence ... should remain collected, balanced and dispassionate\". He also agreed to draft legislation that sanctioned beatings, as long as the abuse was kept secret, and reminded the governor that \"If we are going to sin,\" he wrote, \"we must sin quietly.\"\n\nDuring the Troubles, members of the British Army and the British security forces had routinely used torture on Irish Republican Army (IRA) suspects in Northern Ireland, a part of the United Kingdom.\n\nIn 1971, as part of Operation Demetrius, fourteen arrested men were subjected to a program of \"deep interrogation\" at a secret interrogation centre. The interrogation methods involved sensory deprivation and were referred to as the \"five techniques\". The European Court of Human Rights defined them as \"wall-standing, hooding, subjection to noise, deprivation of sleep, and deprivation of food and drink\". For seven days, when not being interrogated, the detainees were kept hooded and handcuffed in a cold cell and subjected to a continuous loud hissing noise. Here they were forced to stand in a stress position for many hours and were deprived of sleep, food and drink. They were also repeatedly beaten, and some reported being kicked in the genitals, having their heads banged against walls and being threatened with injections. The effect was severe pain, severe physical and mental exhaustion, severe anxiety, depression, hallucinations, disorientation and repeated loss of consciousness.\n\nThe fourteen so-called \"Hooded Men\" were the only detainees subjected to all five techniques together. Some other detainees were subjected to at least one of the five techniques, along with other interrogation methods. These allegedly included waterboarding, electric shocks, burning with matches and candles, forcing internees to stand over hot electric fires while beating them, beating and squeezing of the genitals, inserting objects into the anus, injections, whipping the soles of the feet, and psychological abuse such as Russian roulette.\n\nDetails of the \"deep interrogation\" program became known to the public, sparking outrage. In response, the British Government commissioned an inquiry, under Lord Parker, to look into the five techniques. In 1972 the 'Parker Report' concluded that the five techniques were illegal under domestic law. British Prime Minister, Edward Heath, then announced that the five techniques would no longer be used under his government. However, he said that if a future British government decided to reintroduce them, it would need to be approved by Parliament.\n\nThe Irish Government had begun international legal action against the British Government over the Hooded Men in 1971. In 1976, the European Commission of Human Rights ruled that the program of deep interrogation, using the five techniques, amounted to \"torture\". The case was then referred to the European Court of Human Rights. In 1978 it ruled that the program amounted to \"inhuman and degrading treatment\" which breached the European Convention on Human Rights, but did not amount to torture. In 2014, evidence emerged that the British Government had withheld information from the Court. Following these revelations, the Irish Government announced in December 2014 that it would be asking the Court to review its judgement and acknowledge the five techniques as torture.\n\nThe Court's ruling, that the five techniques did not amount to torture, was later cited by the United States and Israel to justify their own interrogation methods.\n\nOn 23 February 2005, British soldiers were found guilty of abuse of Iraqi prisoners arrested for looting at a British Army camp called Bread Basket, in Basra, during May 2003. The judge at the military court, Judge Advocate Michael Hunter, said of photographs and the soldier's behaviour:\n\nAt the court martial, the prosecution alleged that in giving the order to \"work [the prisoners] hard\", Captain Dan Taylor had broken the Geneva Conventions. Neither Taylor, nor his commanding officer Lt-Col Paterson (who was briefed on the operation \"Ali Baba\" by Taylor), was sanctioned and, indeed, during the period of time between the offence and the trial, both were given promotions. All the leaders of the major British political parties condemned the abuse. Tony Blair, British Prime Minister, declared that the pictures were \"shocking and appalling\". After sentencing, the Chief of the General Staff, General Sir Mike Jackson, made a statement on television and said that he was \"appalled and disappointed\" when he first saw photographs of the Iraqi detainees and that\n\nOn 7 December 2005, the House of Lords reversed the deportations of Muslims convicted on \"evidence procured by torture inflicted by foreign officials\", and cited the 1978 case in ruling that centuries of common law and recent international conventions made torture anathema in the country's courts. Lord Bingham said it was \"clear that from its very earliest days the common law of England set its face firmly against the use of torture\"; Lord Nicholls said \"Torture is not acceptable. This is a bedrock moral principle in this country\"; Lord Hoffman said \"The use of torture is dishonourable. It corrupts and degrades the state which uses it and the legal system which accepts it.\"; Lord Hope said it was \"one of most evil practices known to man\"; Lord Rodgers said \"the unacceptable nature of torture ... has long been unquestioned in this country.\"; Lord Carswell referred to the \"abhorrence felt by civilised nations for the use of torture\"; and Lord Brown said that \"torture is an unqualified evil. It can never be justified. Rather it must always be punished.\".\n\nOn 13 March 2007, the six-month court martial of the seven soldiers – including Colonel Jorge Mendonca and Major Michael Peebles – over the detention of Iraqi prisoners in Basra during May 2003 ended with all but one, Corporal Donald Payne, being acquitted. On 30 April 2007, Payne, Britain's first convicted war criminal found guilty under the provisions of the International Criminal Court Act 2001, who had pleaded guilty to mistreating prisoners, was jailed for a year and dishonourably discharged from the army.\n\nIn March 2008, the Ministry of Defence admitted breaching the human rights of Baha Mousa, who died in British custody in Basra, and of eight other Iraqi men held at the same facility, opening the way for a multimillion-pound compensation package for the relatives of Baha Mousa and the other men injured during illegal interrogations. On 14 May 2008, Defence Secretary Des Browne announced in the House of Commons that there would be a public inquiry into the death of Baha Mousa in which \"no stone [will be left] unturned in investigating his tragic death.\"\n\nOn 26 July 2008, the Joint Committee on Human Rights accused Armed Forces Minister Adam Ingram in 2004 and Lieutenant-General Robin Brims, Commander Field Army, in 2006 of misleading the committee when they declared that conditioning practices (based on the five techniques, banned since their use in Northern Ireland in the 1970s) were not being used. It has now emerged that such techniques were being used by some troops deployed abroad. The BBC reported that \"Labour MP Andrew Dismore, chairman of the committee, said he hoped the public inquiry [into the death of Baha Mousa] would give some indications as to why they were given 'wrong evidence'. Earlier this month, the MoD agreed to pay almost £3m in compensation to Mr Mousa's family and nine Iraqi men after admitting breaching human rights\".\n\nWhile the United States is a party to international conventions against torture, a proponent of human rights treaties and a critic of torture by other countries, torture has taken place within its borders and on its government's behalf outside of its borders.\n\nOn 13 December 1999, NYPD officer Justin Volpe was sentenced to thirty years in prison for sodomizing detainee Abner Louima with the handle of a bathroom plunger.\n\nThe Chicago Police Department's Area 2 unit under Commander Jon Burge repeatedly used electroshock, near-suffocation by plastic bags and excessive beating on suspects in the 1970s and 1980s. The City of Chicago's Office of Professional Standards (OPS) concluded that the physical abuse was systematic and, \"The type of abuse described was not limited to the usual beating, but went into such esoteric areas as psychological techniques and planned torture.\" The Supermax facility at the Maine State Prison has been the scene of video-taped forcible extractions that Lance Tapley in the Portland Phoenix wrote \"look[ed] like torture.\"\n\nIn 2003 and 2004 there was substantial controversy over the \"stress and duress\" methods that were used in the U.S. War on Terrorism that had been sanctioned by the U.S. Executive branch of government at Cabinet level.\n\nAmnesty International and numerous commentators have accused the Military Commissions Act of 2006 of approving a system that uses torture, destroying the mechanisms for judicial review created by the Supreme Court ruling in Hamdan v. Rumsfeld, and creating a parallel legal system below international standards.\n\nIn an interview with the \"Washington Post\", the convening authority of the Guantanamo military commissions, Susan J. Crawford, a retired judge, who was responsible for reviewing practices at the Guantanamo Bay detention camp, said of one Guantanamo Bay detainee, \"his treatment met the legal definition of torture, and that is why I did not refer the case\" for prosecution. The U.S. Government denies that torture is being conducted in the detention camps at Guantanamo Bay.\n\nIt was reported in June 2008 that, according to human rights lawyers, the USA was \"operating floating prisons to house those arrested in its war on terror\":\n\"According to research carried out by Reprieve, the US may have used as many as 17 ships as 'floating prisons' since 2001. Detainees are interrogated aboard the vessels and then rendered to other, often undisclosed, locations, it is claimed. Ships that are understood to have held prisoners include the USS Bataan and USS Peleliu. A further 15 ships are suspected of having operated around the British territory of Diego Garcia in the Indian Ocean, which has been used as a military base by the UK and the Americans.\n\n... The Reprieve study includes the account of a prisoner released from Guantánamo Bay, who described a fellow inmate's story of detention on an amphibious assault ship. 'One of my fellow prisoners in Guantánamo was at sea on an American ship with about 50 others before coming to Guantánamo ... he was in the cage next to me. He told me that there were about 50 other people on the ship. They were all closed off in the bottom of the ship. The prisoner commented to me that it was like something you see on TV. The people held on the ship were beaten even more severely than in Guantánamo.'\"\n\nAfter an investigating visit to Uzbekistan, United Nations Special Rapporteur on Torture Theo van Boven concluded:\n\nForms of torture frequently cited include immersion in boiling water, exposure to extreme heat and cold, \"the use of electric shock, temporary suffocation, hanging by the ankles or wrists, removal of fingernails, punctures with sharp objects, rape, the threat of rape, and the threat of murder of family members.\" (For example, see Muzafar Avazov.)\n\nIn 2003, Britain's Ambassador for Uzbekistan, Craig Murray, said that information was being extracted under extreme torture from dissidents in that country, and that the information was subsequently being used by Britain and other western, democratic countries which disapproved of torture.\n\n"}
{"id": "45479", "url": "https://en.wikipedia.org/wiki?curid=45479", "title": "Utility", "text": "Utility\n\nWithin economics the concept of utility is used to model worth or value, but its usage has evolved significantly over time. The term was introduced initially as a measure of pleasure or satisfaction within the theory of utilitarianism by moral philosophers such as Jeremy Bentham and John Stuart Mill. But the term has been adapted and reapplied within neoclassical economics, which dominates modern economic theory, as a utility function that represents a consumer's preference ordering over a choice set. As such, it is devoid of its original interpretation as a measurement of the pleasure or satisfaction obtained by the consumer from that choice.\n\nConsider a set of alternatives facing an individual, and over which the individual has a preference ordering. A utility function is able to represent those preferences if it is possible to assign a real number to each alternative, in such a way that \"alternative a\" is assigned a number greater than \"alternative b\" if, and only if, the individual prefers \"alternative a\" to \"alternative b\". In this situation an individual that selects the most preferred alternative available is necessarily also selecting the alternative that maximises the associated utility function.\n\nGérard Debreu precisely defined the conditions required for a preference ordering to be representable by a utility function. For a finite set of alternatives these require only that the preference ordering is complete (so the individual is able to determine which of any two alternatives is preferred, or that they are equally preferred), and that the preference order is transitive.\n\nUtility is usually applied by economists in such constructs as the indifference curve, which plot the combination of commodities that an individual or a society would accept to maintain a given level of satisfaction. Utility and indifference curves are used by economists to understand the underpinnings of demand curves, which are half of the supply and demand analysis that is used to analyze the workings of goods markets.\n\nIndividual utility and social utility can be construed as the value of a utility function and a social welfare function respectively. When coupled with production or commodity constraints, under some assumptions these functions can be used to analyze Pareto efficiency, such as illustrated by Edgeworth boxes in contract curves. Such efficiency is a central concept in welfare economics.\n\nIn finance, utility is applied to generate an individual's price for an asset called the indifference price. Utility functions are also related to risk measures, with the most common example being the entropic risk measure.\n\nIt was recognized that utility could not be measured or observed directly, so instead economists devised a way to infer underlying relative utilities from observed choice. These 'revealed preferences', as they were named by Paul Samuelson, were revealed e.g. in people's willingness to pay: Utility is taken to be correlative to Desire or Want. It has been already argued that desires cannot be measured directly, but only indirectly, by the outward phenomena to which they give rise: and that in those cases with which economics is chiefly concerned the measure is found in the price which a person is willing to pay for the fulfillment or satisfaction of his desire.\n\nThere has been some controversy over the question whether the utility of a commodity can be measured or not. At one time, it was assumed that the consumer was able to say exactly how much utility he got from the commodity. The economists who made this assumption belonged to the 'cardinalist school' of economics. Today utility functions, expressing utility as a function of the amounts of the various goods consumed, are treated as either \"cardinal\" or \"ordinal\", depending on whether they are or are not interpreted as providing more information than simply the rank ordering of preferences over bundles of goods, such as information on the strength of preferences.\n\nWhen cardinal utility is used, the magnitude of utility differences is treated as an ethically or behaviorally significant quantity. For example, suppose a cup of orange juice has utility of 120 utils, a cup of tea has a utility of 80 utils, and a cup of water has a utility of 40 utils. With cardinal utility, it can be concluded that the cup of orange juice is better than the cup of tea by exactly the same amount by which the cup of tea is better than the cup of water. Formally speaking, this means that if one has a cup of tea, she would be willing to take any bet with a probability, p, greater than .5 of getting a cup of juice, with a risk of getting a cup of water equal to 1-p. One cannot conclude, however, that the cup of tea is two thirds as good as the cup of juice, because this conclusion would depend not only on magnitudes of utility differences, but also on the \"zero\" of utility. For example, if the \"zero\" of utility was located at -40, then a cup of orange juice would be 160 utils more than zero, a cup of tea 120 utils more than zero.\n\nNeoclassical economics has largely retreated from using cardinal utility functions as the basis of economic behavior. A notable exception is in the context of analyzing choice under conditions of risk (see below).\n\nSometimes cardinal utility is used to aggregate utilities across persons, to create a social welfare function.\n\nWhen ordinal utilities are used, differences in utils (values taken on by the utility function) are treated as ethically or behaviorally meaningless: the utility index encodes a full behavioral ordering between members of a choice set, but tells nothing about the related \"strength of preferences\". In the above example, it would only be possible to say that juice is preferred to tea to water, but no more.\n\nOrdinal utility functions are unique up to increasing monotone (or monotonic) transformations. For example, if a function formula_1 is taken as ordinal, it is equivalent to the function formula_2, because taking the 3rd power is an increasing monotone transformation (or monotonic transformation). This means that the ordinal preference induced by these functions is the same (although they are two different functions). In contrast, cardinal utilities are unique only up to increasing linear transformations, so if formula_1 is taken as cardinal, it is not equivalent to formula_2.\n\nAlthough preferences are the conventional foundation of microeconomics, it is often convenient to represent preferences with a utility function and analyze human behavior indirectly with utility functions. Let \"X\" be the consumption set, the set of all mutually-exclusive baskets the consumer could conceivably consume. The consumer's utility function formula_5 ranks each package in the consumption set. If the consumer strictly prefers \"x\" to \"y\" or is indifferent between them, then formula_6.\n\nFor example, suppose a consumer's consumption set is \"X\" = {nothing, 1 apple,1 orange, 1 apple and 1 orange, 2 apples, 2 oranges}, and its utility function is \"u\"(nothing) = 0, \"u\"(1 apple) = 1, \"u\"(1 orange) = 2, \"u\"(1 apple and 1 orange) = 4, \"u\"(2 apples) = 2 and \"u\"(2 oranges) = 3. Then this consumer prefers 1 orange to 1 apple, but prefers one of each to 2 oranges.\n\nIn micro-economic models, there are usually a finite set of L commodities, and a consumer may consume an arbitrary amount of each commodity. This gives a consumption set of formula_7, and each package formula_8 is a vector containing the amounts of each commodity. In the previous example, we might say there are two commodities: apples and oranges. If we say apples is the first commodity, and oranges the second, then the consumption set formula_9 and \"u\"(0, 0) = 0, \"u\"(1, 0) = 1, \"u\"(0, 1) = 2, \"u\"(1, 1) = 4, \"u\"(2, 0) = 2, \"u\"(0, 2) = 3 as before. Note that for \"u\" to be a utility function on \"X\", it must be defined for every package in \"X\".\n\nA utility function formula_5 represents a preference relation formula_11 on X iff for every formula_12, formula_13 implies formula_14. If u represents formula_11, then this implies formula_11 is complete and transitive, and hence rational.\n\nIn financial applications, e.g. portfolio optimization, an investor chooses financial portfolio which maximizes his/her own utility function, or, equivalently, minimizes his/her risk measure. For example, modern portfolio theory selects variance as a measure of risk; other popular theories are expected utility theory and prospect theory. To determine specific utility function for any given investor, one could design a questionnaire procedure with questions in the form: How much would you pay for \"x%\" chance of getting \"y\"? Revealed preference theory suggests a more direct approach: observe a portfolio \"X*\" which an investor currently holds, and then find a utility function/risk measure such that \"X*\" becomes an optimal portfolio.\n\nIn order to simplify calculations, various alternative assumptions have been made concerning details of human preferences, and these imply various alternative utility functions such as:\n\n\nMost utility functions used in modeling or theory are well-behaved. They are usually monotonic and quasi-concave. However, it is possible for preferences not to be representable by a utility function. An example is lexicographic preferences which are not continuous and cannot be represented by a continuous utility function.\n\nThe expected utility theory deals with the analysis of choices among risky projects with multiple (possibly multidimensional) outcomes.\n\nThe St. Petersburg paradox was first proposed by Nicholas Bernoulli in 1713 and solved by Daniel Bernoulli in 1738. D. Bernoulli argued that the paradox could be resolved if decision-makers displayed risk aversion and argued for a logarithmic cardinal utility function. (Analyses of international survey data in the 21st century have shown that insofar as utility represents happiness, as in utilitarianism, it is indeed proportional to log income.)\n\nThe first important use of the expected utility theory was that of John von Neumann and Oskar Morgenstern, who used the assumption of expected utility maximization in their formulation of game theory.\n\nVon Neumann and Morgenstern addressed situations in which the outcomes of choices are not known with certainty, but have probabilities attached to them.\n\nA notation for a \"lottery\" is as follows: if options A and B have probability \"p\" and 1 − \"p\" in the lottery, we write it as a linear combination:\n\nMore generally, for a lottery with many possible options:\n\nwhere formula_19.\n\nBy making some reasonable assumptions about the way choices behave, von Neumann and Morgenstern showed that if an agent can choose between the lotteries, then this agent has a utility function such that the desirability of an arbitrary lottery can be calculated as a linear combination of the utilities of its parts, with the weights being their probabilities of occurring.\n\nThis is called the \"expected utility theorem\". The required assumptions are four axioms about the properties of the agent's preference relation over 'simple lotteries', which are lotteries with just two options. Writing formula_20 to mean 'A is weakly preferred to B' ('A is preferred at least as much as B'), the axioms are:\n\n\nAxioms 3 and 4 enable us to decide about the relative utilities of two assets or lotteries.\n\nIn more formal language: A von Neumann–Morgenstern utility function is a function from choices to the real numbers:\nwhich assigns a real number to every outcome in a way that captures the agent's preferences over simple lotteries. Under the four assumptions mentioned above, the agent will prefer a lottery formula_42 to a lottery formula_43 if and only if, for the utility function characterizing that agent, the expected utility of formula_42 is greater than the expected utility of formula_43:\n\nOf all the axioms, independence is the most often discarded. A variety of generalized expected utility theories have arisen, most of which drop or relax the independence axiom.\n\nCastagnoli and LiCalzi and Bordley and LiCalzi (2000) provided another interpretation for Von Neumann and Morgenstern's theory. Specifically for any utility function, there exists a hypothetical reference lottery with the expected utility of an arbitrary lottery being its probability of performing no worse than the reference lottery. Suppose success is defined as getting an outcome no worse than the outcome of the reference lottery. Then this mathematical equivalence means that maximizing expected utility is equivalent to maximizing the probability of success. In many contexts, this makes the concept of utility easier to justify and to apply. For example, a firm's utility might be the probability of meeting uncertain future customer expectations.\n\nAn indirect utility function gives the optimal attainable value of a given utility function, which depends on the prices of the goods and the income or wealth level that the individual possesses.\n\nOne use of the indirect utility concept is the notion of the utility of money. The (indirect) utility function for money is a nonlinear function that is bounded and asymmetric about the origin. The utility function is concave in the positive region, reflecting the phenomenon of diminishing marginal utility. The boundedness reflects the fact that beyond a certain point money ceases being useful at all, as the size of any economy at any point in time is itself bounded. The asymmetry about the origin reflects the fact that gaining and losing money can have radically different implications both for individuals and businesses. The non-linearity of the utility function for money has profound implications in decision making processes: in situations where outcomes of choices influence utility through gains or losses of money, which are the norm in most business settings, the optimal choice for a given decision depends on the possible outcomes of all other decisions in the same time-period.\n\nCambridge economist Joan Robinson famously criticized utility for being a circular concept: \"Utility is the quality in commodities that makes individuals want to buy them, and the fact that individuals want to buy commodities shows that they have utility\" Robinson also pointed out that because the theory assumes that preferences are fixed this means that utility is not a testable assumption. This is so because if we take changes in peoples' behavior in relation to a change in prices or a change in the underlying budget constraint we can never be sure to what extent the change in behavior was due to the change in price or budget constraint and how much was due to a change in preferences. This criticism is similar to that of the philosopher Hans Albert who argued that the ceteris paribus conditions on which the marginalist theory of demand rested rendered the theory itself an empty tautology and completely closed to experimental testing. In essence, demand and supply curve (theoretical line of quantity of a product which would have been offered or requested for given price) is purely ontological and could never been demonstrated empirically.\n\nAnother criticism comes from the assertion that neither cardinal nor ordinal utility is empirically observable in the real world. In the case of cardinal utility it is impossible to measure the level of satisfaction \"quantitatively\" when someone consumes or purchases an apple. In case of ordinal utility, it is impossible to determine what choices were made when someone purchases, for example, an orange. Any act would involve preference over a vast set of choices (such as apple, orange juice, other vegetable, vitamin C tablets, exercise, not purchasing, etc.).\n\nOther questions of what arguments ought to enter into a utility function are difficult to answer, yet seem necessary to understanding utility. Whether people gain utility from coherence of wants, beliefs or a sense of duty is key to understanding their behavior in the utility organon. Likewise, choosing between alternatives is itself a process of determining what to consider as alternatives, a question of choice within uncertainty.\n\nAn evolutionary psychology perspective is that utility may be better viewed as due to preferences that maximized evolutionary fitness in the ancestral environment but not necessarily in the current one.\n\n\n\n"}
{"id": "2850867", "url": "https://en.wikipedia.org/wiki?curid=2850867", "title": "Years of Lead", "text": "Years of Lead\n\nThe Years of Lead were a political phenomenon related to the Cold War that was characterized by left- and right-wing terrorism and the putative strategy of tension, beginning in Italy and later spreading to the rest of Europe. In Italy terrorist groups include the Red Brigades and Ordine Nuovo, in Germany there was the Red Army Faction and in France there was \"Action directe\". In Belgium, the \"années de plomb\" or \"Bloody Eighties\", refers to the Brabant massacres which resulted in 28 deaths. The far-right movement Westland New Post (WNP) has been suspected in this campaign. Attacks by the Communist Combatant Cells, a short-lived group, killed two. Many of the right-wing groups involved have been linked to Operation Gladio. The liberation of James L. Dozier signaled the end of the Years of Lead.\n\n\nFrench \"Les années de plomb\", Italian \"Anni di piombo\", and Portuguese \"Anos de chumbo\", all meaning \"Years of Lead\", are also the French, Italian and Portuguese titles of the 1981 West German film \"Die bleierne Zeit\" (literally: \"The Period of Lead\") about the RAF period in Germany (English titles: \"The German Sisters\" (UK); \"Marianne and Juliane\" (USA)).\n\n"}
