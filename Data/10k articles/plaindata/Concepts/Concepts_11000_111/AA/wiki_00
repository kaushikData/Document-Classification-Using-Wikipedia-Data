{"id": "40428751", "url": "https://en.wikipedia.org/wiki?curid=40428751", "title": "Algorave", "text": "Algorave\n\nAn algorave is an event where people dance to music generated from algorithms, often using live coding techniques, and short for \"algorithmic rave.\" Alex McLean of Slub and Nick Collins coined the word \"algorave\", with the first event to be held under that name taking place in 2012. It has since become a movement, with algoraves taking place around the world.\n\nAn algorave is an event where people dance to music generated from algorithms, often using live coding techniques. Algoraves can include a range of styles, including a complex form of minimal techno, and the movement has been described as a meeting point of hacker philosophy, geek culture, and clubbing. \n\nAlthough live coding is commonplace, any algorithmic music is welcome which is \"wholly or predominantly characterised by the emission of a succession of repetitive conditionals\", which is a corruption of the definition of rave music (“wholly or predominantly characterised by the emission of a succession of repetitive beats”) in the UK's Criminal Justice Act. Although algorave musicians have been compared with DJs, they are in fact live musicians or improvisers, creating music live, usually by writing or modifying code, rather than mixing recorded music.\n\nAt an algorave the computer musician may not be the main point of focus for the audience and instead attention may be centered on a screen that displays live coding, that is the process of writing source code, so the audience can not just dance or listen to the music generated by the source code but also to see the process of programming.\n\nAlgorithmic approaches have long been applied in electronic dance music from the 1970s when Brian Eno established randomised musical practises which evolved into generative music over the course of his long career. This in turn influenced rave culture and techno of the 1990s by Farmers Manual, Autechre, and Aphex Twin. The Anti EP was an explicit response to the Criminal Justice and Public Order Act 1994 - specifically the track \"flutter\" as a means of creating \"non-repetitive beats\" at raves which had been outlawed by the wording of the Act. The snare rush famously featured on the Girl/Boy EP of 1996 is an earlier form of digital algorhythmic coding and featured in drum and bass influenced electronic music of the early to mid 1990s, this approach later evolving into glitch music. Traditional use of algorithms include Maypole dancing, where they are applied to the dance itself as a form of Algorithmic Choreography and bell-ringing. The first self-proclaimed \"algorave\" was held in London as a warmup concert for the SuperCollider Symposium 2012. However the name was first coined in 2011, after live coders Nick Collins and Alex McLean tuned into a happy hardcore pirate radio station on the way to a performance in the UK. Since then, algorave has been growing into an international movement, with algoraves having been held mainly in Europe and Asia; and few events in Australia and North America.\n\nAlgorave can also be considered an international music movement with a community of electronic musicians, visual artists and developing technologies. See .\n\n"}
{"id": "518397", "url": "https://en.wikipedia.org/wiki?curid=518397", "title": "Angle of repose", "text": "Angle of repose\n\nThe angle of repose, or critical angle of repose, of a granular material is the steepest angle of descent or dip relative to the horizontal plane to which a material can be piled without slumping. At this angle, the material on the slope face is on the verge of sliding. The angle of repose can range from 0° to 90°. The morphology of the material affects the angle of repose; smooth, rounded sand grains cannot be piled as steeply as can rough, interlocking sands. The angle of repose can also be affected by additions of solvents. If a small amount of water is able to bridge the gaps between particles, electrostatic attraction of the water to mineral surfaces will increase the angle of repose, and related quantities such as the soil strength.\n\nWhen bulk granular materials are poured onto a horizontal surface, a conical pile will form. The internal angle between the surface of the pile and the horizontal surface is known as the angle of repose and is related to the density, surface area and shapes of the particles, and the coefficient of friction of the material. Material with a low angle of repose forms flatter piles than material with a high angle of repose.\n\nThe term has a related usage in mechanics, where it refers to the maximum angle at which an object can rest on an inclined plane without sliding down. This angle is equal to the arctangent of the coefficient of static friction \"μ\" between the surfaces.\n\nThe angle of repose is sometimes used in the design of equipment for the processing of particulate solids. For example, it may be used to design an appropriate hopper or silo to store the material, or to size a conveyor belt for transporting the material. It can also be used in determining whether or not a slope (of a stockpile, or uncompacted gravel bank, for example) will likely collapse; the talus slope is derived from angle of repose and represents the steepest slope a pile of granular material will take. This angle of repose is also crucial in correctly calculating stability in vessels.\n\nIt is also commonly used by mountaineers as a factor in analysing avalanche danger in mountainous areas.\n\nThere are numerous methods for measuring angle of repose and each produces slightly different results. Results are also sensitive to the exact methodology of the experimenter. As a result, data from different labs are not always comparable. One method is the triaxial shear test, another is the direct shear test.\n\nIf the coefficient of static friction is known of a material, then a good approximation of the angle of repose can be made with the following function. This function is somewhat accurate for piles where individual objects in the pile are minuscule and piled in random order.\n\nwhere, \"μ\" is the coefficient of static friction, and \"θ\" is the angle of repose.\n\nThe measured angle of repose may vary with the method used.\n\nThis method is appropriate for fine-grained, non-cohesive materials with individual particle size less than 10 mm. The material is placed within a box with a transparent side to observe the granular test material. It should initially be level and parallel to the base of the box. The box is slowly tilted until the material begins to slide in bulk, and the angle of the tilt is measured.\n\nThe material is poured through a funnel to form a cone. The tip of the funnel should be held close to the growing cone and slowly raised as the pile grows, to minimize the impact of falling particles. Stop pouring the material when the pile reaches a predetermined height or the base a predetermined width. Rather than attempt to measure the angle of the resulting cone directly, divide the height by half the width of the base of the cone. The inverse tangent of this ratio is the angle of repose.\n\nThe material is placed within a cylinder with at least one transparent end. The cylinder is rotated at a fixed speed and the observer watches the material moving within the rotating cylinder. The effect is similar to watching clothes tumble over one another in a slowly rotating clothes dryer. The granular material will assume a certain angle as it flows within the rotating cylinder. This method is recommended for obtaining the dynamic angle of repose, and may vary from the static angle of repose measured by other methods.\n\nHere is a list of various materials and their angle of repose. All measurements are approximated.\n\nDifferent supports will modify the shape of the pile, in the illustrations below sand piles, though angles of repose remain the same.\n\nThe larvae of the antlions and the unrelated wormlions Vermileonidae trap small insects such as ants by digging conical pits in loose sand, such that the slope of the walls is effectively at the critical angle of repose for the sand. They achieve this by flinging the loose sand out of the pit and permitting the sand to settle at its critical angle of repose as it falls back. Thus, when a small insect, commonly an ant, blunders into the pit, its weight causes the sand to collapse below it, drawing the victim toward the center where the predator that dug the pit lies in wait under a thin layer of loose sand. The larva assists this process by vigorously flicking sand out from the center of the pit when it detects a disturbance. This undermines the pit walls and causes them to collapse toward the center. The sand that the larva flings also pelts the prey with so much loose, rolling material as to prevent it from getting any foothold on the easier slopes that the initial collapse of the slope has presented. The combined effect is to bring the prey down to within grasp of the larva, which then can inject venom and digestive fluids.\n\nThe angle of repose plays a part in several topics of technology and science, including:\n"}
{"id": "45001834", "url": "https://en.wikipedia.org/wiki?curid=45001834", "title": "Ataptatanu", "text": "Ataptatanu\n\nAtaptatanū (Sanskrit:अतप्ततनू) – \"tapa\" (तप) means – 'to burn', 'heat up'; \"atapta\" (अतप्त) – means - 'not heated', 'cool', and \"tanū\" (तनू) – means - 'body', 'the physical self'; \"ataptatanū\" means – 'he whose body or mass is not prepared in fire', 'raw' \n\nThe compound word, \"ataptatanū\", appears in the below cited mantra of the IX Mandala of the Rig Veda. In a sukta addressed to Pavmāna Somo Devatā, Rishi Pavitra prays:-\n\nIn this mantra, \"ataptatanūh\", refers to the one who has not subjected himself to the heat of tapas, \"tadāmah\" refers to one who is raw and who therefore, \"aśunate\" - cannot experience the highest bliss because his body is not yet properly prepared to receive the knowledge he seeks.\n\nIn his Satyarth Prakash (Light of Truth), Swami Dayananda Saraswati explains that \"ataptatanū\" does not refer to branding with fire of one’s body which fact is clarified by Rishi Pavitra in the subsequent mantra which reads:-\n\nand which means –\n\nTapa or \"tapasya\", is practical discipline; according to the Bhagavada Gita (XVII.14), revering the gods, the twice-born, elders, teachers and wise men, purity, celibacy, and non-violence these are the \"tapasya\" of the body, of speech and of mind. The finishing phase of a scholar’s higher education was called \"tapasya\" in the time of Krishna. Gandhi considered \"tapasya\" to be the test of love, \"ahimsa\", self-suffering and self-sacrifice, essentials in the quest for truth.\n"}
{"id": "44466971", "url": "https://en.wikipedia.org/wiki?curid=44466971", "title": "Biased random walk on a graph", "text": "Biased random walk on a graph\n\nIn network science, a biased random walk on a graph is a time path process in which an evolving variable jumps from its current state to one of various potential new states; unlike in a pure random walk, the probabilities of the potential new states are unequal.\n\nBiased random walks on a graph provide an approach for the structural analysis of undirected graphs in order to extract their symmetries when the network is too complex or when it is not large enough to be analyzed by statistical methods. The concept of biased random walks on a graph has attracted the attention of many researchers and data companies over the past decade especially in the transportation and social networks.\n\nThere have been written many different representations of the biased random walks on graphs based on the particular purpose of the analysis. A common representation of the mechanism for undirected graphs is as follows:\n\nOn an undirected graph, a walker takes a step from the current node, formula_1 to node formula_2 Assuming that each node has an attribute formula_3 the probability of jumping from node formula_4 to formula_5 is given by:\n\nwhere formula_7 represents the topological weight of the edge going from formula_4 to formula_2\n\nIn fact, the steps of the walker are biased by the factor of formula_10 which may differ from one node to another.\n\nDepending on the network, the attribute formula_10 can be interpreted differently. It might be implied as the attraction of a person in a social network, it might be betweenness centrality or even it might be explained as an intrinsic characteristic of a node. In case of a fair random walk on graph formula_10 is one for all the nodes.\n\nIn case of shortest paths random walks formula_13 is the total number of the shortest paths between all pairs of nodes that pass through the node formula_14. In fact the walker prefers the nodes with higher betweenness centrality which is defined as below:\n\nBased on the above equation, the recurrence time to a node in the biased walk is given by:\n\nVariety of applications by using biased random walks on graphs have been developed; control of diffusion, advertisement of products on social networks, explaining dispersal and population redistribution of animals and micro-organisms, community detections, wireless networks, search engines and so on.\n\n\n"}
{"id": "53009580", "url": "https://en.wikipedia.org/wiki?curid=53009580", "title": "Brown–Gitler spectrum", "text": "Brown–Gitler spectrum\n\nIn topology, a discipline within mathematics, the Brown–Gitler spectrum is a spectrum whose cohomology is a certain cyclic module over the Steenrod algebra.\n\nBrown–Gitler spectra are defined by the isomorphism:\n\nThe concept was introduced by mathematicians Edgar H. Brown and Samuel Gitler in a 1973 paper.\n\nIn topology, Brown–Gitler spectrum is related to the concepts of Segal conjecture and Burnside ring.\n\nBrown–Gitler spectra have had many important applications in homotopy theory.\n"}
{"id": "43559559", "url": "https://en.wikipedia.org/wiki?curid=43559559", "title": "Bóndi", "text": "Bóndi\n\nBóndi (also húsbóndi, (pl.) bændr in Old Norse) was the Norse core of society, formed by farmers and craftsmen in the Scandinavian Viking Age, and constituted a widespread middle class. They were free men and enjoyed rights such as the use of weapons and the privilege to join the Thing as farm owners landlords.\n\nThe profile is specified in \"Rígsthula\", a Scandinavian legend describing the god Ríg lying with three couples to procreate and give birth to the three social classes: thralls, karls (or bændr) and earls. The poem describes the image and behavior as it should be, and the type of work expected at each.\n\nThe Norse mythology cites Karl as a result of the illicit relationship between god Heimdal and mortal Amma. Karl and his wife Snor would be progenitors of the peasants and freemen. The odalsbóndi (owner with hereditary possessions) could give up some of their land to other karls in exchange for loyalty and unconditional support whenever necessary. However, the term \"karl\" in some ancient writings denoting a free man status of low social class without access to family ties with upper castes or royalty.\n\nThe bóndi had parallel activities; were great sailors, merchants, and Vikings; in areas further north also hunters and fishermen. With their snekke for war and knarr to trade, Vikings virtually dominating the seas in northern Europe. Sometimes hackers and other traders, according to circumstances. It is in 873, despite the mutual distrust between the Vikings and the Carolingian Empire, both parts came to an agreement that the merchants could cross borders in order to buy and sell goods in good will. Birka and Hedeby become two important enclaves and trade routes of the time.\n\nIn the icelandic Commonwealth, a bóndi figure was subject to the authority of a goði so their rights as free men were subject by law to a minimum in properties (a cow, a boat or a network for each family member) and establish a formal relationship with the goði, thereafter a bóndi formally considered a follower and his vote as \"Þingmaðr\" (\"men of thing\") was influenced by the will of goði in the Althing. These conditions were regulated by the Icelandic law collected in the \"Grágás\".\n"}
{"id": "2047150", "url": "https://en.wikipedia.org/wiki?curid=2047150", "title": "Coincidence point", "text": "Coincidence point\n\nIn mathematics, a coincidence point (or simply coincidence) of two mappings is a point in their common domain having the same image.\n\nFormally, given two mappings\nwe say that a point \"x\" in \"X\" is a \"coincidence point\" of \"f\" and \"g\" if \"f\"(\"x\") = \"g\"(\"x\").\n\nCoincidence theory (the study of coincidence points) is, in most settings, a generalization of fixed point theory, the study of points \"x\" with \"f\"(\"x\") = \"x\". Fixed point theory is the special case obtained from the above by letting \"X = Y\" and taking \"g\" to be the identity mapping.\n\nJust as fixed point theory has its fixed-point theorems, there are theorems that guarantee the existence of coincidence points for pairs of mappings. Notable among them, in the setting of manifolds, is the Lefschetz coincidence theorem, which is typically known only in its special case formulation for fixed points.\n\nCoincidence points, like fixed points, are today studied using many tools from mathematical analysis and topology. An equaliser is a generalization of the coincidence set.\n"}
{"id": "3999072", "url": "https://en.wikipedia.org/wiki?curid=3999072", "title": "Congruence bias", "text": "Congruence bias\n\nCongruence bias is a type of cognitive bias similar to confirmation bias. Congruence bias occurs due to people's overreliance on directly testing a given hypothesis as well as neglecting indirect testing.\n\nSuppose that, in an experimental setting, a subject is presented with two buttons and told that pressing one of those buttons, but not the other, will open a door. The subject adopts the hypothesis that the button on the left opens the door in question. A direct test of this hypothesis would be pressing the button on the left; an indirect test would be pressing the button on the right. The latter is still a valid test because once the result of the door's remaining closed is found, the left button is proven to be the desired button. (This example is parallel to Bruner, Goodnow, and Austin's example in the psychology classic, \"A Study of Thinking\".)\n\nIt is possible to take this idea of direct and indirect testing and apply it to more complicated experiments in order to explain the presence of a congruence bias in people. In an experiment, a subject will test his own usually naive hypothesis again and again instead of trying to disprove it.\n\nThe classic example of subjects' congruence bias was discovered by Peter Wason (1960, 1968). Here, the experimenter gave subjects the number sequence \"2, 4, 6\", telling the subjects that this sequence followed a particular rule and instructing subjects to find the rule underlying the sequence logic. Subjects provided their own number sequences as tests to see if they could ascertain the rule dictating which numbers could be included in the sequence and which could not. Most subjects respond to the task by quickly deciding that the underlying rule is \"numbers ascending by 2\", and provide as tests only sequences concordant with this rule, such as \"3, 5, 7,\" or even \"pi plus 2, plus 4, plus 6\". Each of these sequences follows the underlying rule the experimenter is thinking of, though \"numbers ascending by 2\" is not the actual criterion being used. However, because subjects succeed at repeatedly testing the same singular principle, they naively believe their chosen hypothesis is correct. When a subject offers up to the experimenter the hypothesis \"numbers ascending by 2\" only to be told he is wrong, much confusion usually ensues. At this point, many subjects attempt to change the wording of the rule without changing its meaning, and even those who switch to indirect testing have trouble letting go of the \"+ 2\" convention, producing potential rules as idiosyncratic as \"the first two numbers in the sequence are random, and the third number is the second number plus two\". Many subjects never realize that the actual rule the experimenter was using was simply just to list ascending numbers, because of the subjects' inability to consider indirect tests of their hypotheses.\n\nWason attributed this failure of subjects to an inability to consider alternative hypotheses, which is the root of the congruence bias. Jonathan Baron explains that subjects could be said to be using a \"congruence heuristic\", wherein a hypothesis is tested only by thinking of results that would be found if that hypothesis is true. This heuristic, which many people seem to use, ignores alternative hypotheses.\n\nBaron suggests the following heuristics to avoid falling into the congruence bias trap:\n\n\n"}
{"id": "770084", "url": "https://en.wikipedia.org/wiki?curid=770084", "title": "Digital painting", "text": "Digital painting\n\nDigital painting is an emerging art form in which traditional painting techniques such as watercolor, oils, impasto, etc. are applied using digital tools by means of a computer, a digitizing tablet and stylus, and software. Traditional painting is painting with a physical medium as opposed to a more modern style like digital. Digital painting differs from other forms of digital art, particularly computer-generated art, in that it does not involve the computer rendering from a model. The artist uses painting techniques to create the digital painting directly on the computer. All digital painting programs try to mimic the use of physical media through various brushes and paint effects. Included in many programs are brushes that are digitally styled to represent the traditional style like oils, acrylics, pastels, charcoal, pen and even media such as airbrushing. There are also certain effects unique to each type of digital paint which portray the realistic effects of say watercolor on a digital 'watercolor' painting. In most digital painting programs, the users can create their own brush style using a combination of texture and shape. This ability is very important in bridging the gap between traditional and digital painting.\n\nDigital painting thrives mostly in production art. It is most widely used in conceptual design for film, television and video games. Digital painting software such as Corel Painter, Adobe Photoshop, ArtRage, GIMP, Krita and openCanvas give artists a similar environment to a physical painter: a canvas, painting tools, mixing palettes, and a multitude of color options.\nThere are various types of digital painting, including impressionism, realism, and watercolor. There are both benefits and drawbacks of digital painting. While digital painting allows the artist the ease of working in an organized, mess-free environment, some argue there will always be more control for an artist holding a physical brush in their hand. Some artists believe there is something missing from digital painting, such as the character that is unique to every physically made object. Many artists post blogs and comment on the various differences between digitally created work and traditionally created artwork.\n\nThe main difference between digital and traditional painting is the non-linear process. That is, an artist can often arrange his painting in layers that can be edited independently. Also, the ability to undo and redo strokes frees the artist from a linear process. But digital painting is limited in how it employs the techniques and study of a traditional painter because of the surface differences and lack of physicality. The digital artist has at his disposal several tools not available to the traditional painter. Some of these include: a virtual palette consisting of millions of colors, almost any size canvas or media, and the ability to take back mistakes, as well as erasers, pencils, spray cans, brushes, combs, and a variety of 2D and 3D effect tools. A graphics tablet allows the artist to work with precise hand movements simulating a real pen and drawing surface. Even the traditional surface has changed for digital painting. Instead of a canvas or sketchbook, artists would use a mouse or tablet to display strokes that would appear with the touch of a pen to the tablet’s surface, or a click of pen. Tablets can be pressure sensitive, allowing the artist to vary the intensity of the chosen media on the screen. There are tablets with over two thousand different levels of pressure sensitivity.\n\nThe earliest graphical manipulation program was called Sketchpad. Created in 1963 by Ivan Sutherland, a grad student at MIT, Sketchpad allowed the user to manipulate objects on a CRT (cathode ray tube). Sketchpad eventually led to the creation of the Rand Tablet for work on the GRAIL project in 1968, and the very first tablet was created. Other early tablets, or digitizers, like the ID (intelligent digitizer) and the BitPad were commercially successful and used in CAD (Computer Aided Design) programs. Modern day tablets are the tools of choice by digital painters. WACOM is the industry leader in tablets which can range in size from 4” x 6” all the way to 12” x 19” and are less than an inch thick. Other brands of graphic tablets are Aiptek, Monoprice, Hanvon, Genius, Adesso, Trust, Manhattan, Vistablet, DigiPro, etc.\nAll these graphic tablets have the basic functions of a mouse, so they can be used as a mouse, not only in graphic editors but also as a replacement for a mouse, and they are compatible with practically all Windows and Macintosh software.\n\nThe idea of using a tablet to communicate directions to a computer has been an idea since 1968 when the RAND (Research and Development) company out of Santa Monica, developed the RAND tablet that was used to program. \nDigitizers were popularized in the mid 1970s and early 1980s by the commercial success of the ID (Intelligent Digitizer) and BitPad manufactured by the Summagraphics Corp. These digitizers were used as the input device for many high-end CAD (Computer Aided Design) systems as well as bundled with PC's and PC based CAD software like AutoCAD.\n\nThe first commercial program that allowed users to design, draw, and manipulate objects was the program MacPaint. This program’s first version was introduced on January 22, 1984 on the Apple Lisa. The ability to freehand draw and create graphics with this program made it the top program of its kind during 1984. The earlier versions of the program were called MacSketch and LisaSketch, and the last version of MacPaint was MacPaint 2.0 released in 1998.\nMuch of MacPaint's universal success was attributed to the release of the first Macintosh computer which was equipped with one other program called MacWrite. It was the first personal computer with a graphical user interface and lost much of the bulky size of its predecessor, the Lisa. The Macintosh was available at about $2500 and the combination of a smaller design made the computer a hit, exposing the average computer user to the graphical possibilities of the included MacPaint.\n\nAnother early image manipulation program was Adobe Photoshop. It was first called Display and was created in 1987 by Thomas Knoll at the University of Michigan as monochrome picture display program. With help from his brother John, the program was turned into an image editing program called Imagepro, but later changed to Photoshop. The Knolls agreed on a deal with Adobe systems and Apple, and Photoshop 1.0 was released in 1991 for Macintosh. Adobe systems had previously release Adobe Illustrator 1.0 in 1986 on the Apple Macintosh. These two programs, Adobe Photoshop and Adobe Illustrator are currently two of the top programs used in the productions of digital paintings. Illustrator introduced the uses of Bezier curves which allowed the user to be incredibly detailed in their vector drawings.\n\nIn 1988, Craig Hickman created a paint program called Kid Pix, which made it easier for children to use MacPaint. The program was originally created in black in white, and after several revisions was released in color in 1991. Kid Pix was one of the first commercial programs to integrate color and sound in a creative format. While the Kid Pix was intentionally created for children, it became a useful tool for introducing adults to the computer as well.\n\nIn recent years there has been a growth in the websites which support painting digitally online. \nThe user is still drawing digitally with the use of software: often the software is on the server of the website which is being used. However, with the emergence of html5, some programs now partly use the client's web browser to handle some of the processing. The range of tools and brushes can be more limited than free standing software. Speed of response, quality of colour and the ability to save to a file or print are similar in either media.\n\n\n"}
{"id": "1274285", "url": "https://en.wikipedia.org/wiki?curid=1274285", "title": "Earth orbit rendezvous", "text": "Earth orbit rendezvous\n\nEarth orbit rendezvous (EOR) is a potential methodology for conducting round trip human flights to the Moon, involving the use of space rendezvous to assemble, and possibly fuel, components of a translunar vehicle in low Earth orbit. It was considered and ultimately rejected in favor of lunar orbit rendezvous (LOR) for NASA's Apollo Program of the 1960s and 1970s. Three decades later, it was planned to be used for Project Constellation, until that program’s cancellation in October 2010.\n\nThe Agena target vehicle (ATV) was used for testing Earth orbit rendezvous in the NASA Gemini Program. Gemini 6 and Gemini 7 rendezvoused in orbit in 1965, but without Agena. Next, Gemini 8 successfully docked with the Agena on March 16, 1966. The Agena-Gemini rendezvous also achieved other objectives in later Gemini launches, including docked orbital maneuvering (Gemini 10 and Gemini 11), inspection of the abandoned Gemini 8 ATV (Gemini 10) and space walks (Gemini 12).\n\nThe EOR proposal for Apollo consisted of using a series of small rockets half the size of a Saturn V to put different components of a spacecraft to go to the Moon in orbit around the Earth, then assemble them in orbit. Experiments of Project Gemini involving docking with the Agena target vehicle were designed partly to test the feasibility of this program.\n\nIn the end, NASA employed the Lunar Orbit Rendezvous for the Apollo Program: a Saturn V would simultaneously lift both the Apollo Command and Lunar Modules into low Earth orbit, and then the Saturn V third stage would fire again (Trans-lunar injection) to send both spacecraft to the Moon.\n\nThis mode had been revived for Project Constellation as the Earth Departure Stage (EDS) and Altair (LSAM), which would be launched into low Earth orbit on the Ares V rocket. The EDS and Altair would be met by the separately launched Orion (CEV). Once joined in low Earth orbit, the three would then travel out to the Moon and the Orion/Altair combination would fly a lunar orbit rendezvous flight pattern.\n"}
{"id": "340314", "url": "https://en.wikipedia.org/wiki?curid=340314", "title": "Edict of toleration", "text": "Edict of toleration\n\nAn edict of toleration is a declaration, made by a government or ruler and states, that members of a given religion will not be persecuted for engaging in their religious practices and traditions. The edict implies tacit acceptance of the religion rather than its endorsement by the ruling power.\n\n\n\n\n\n\n\n\n"}
{"id": "1478035", "url": "https://en.wikipedia.org/wiki?curid=1478035", "title": "Flagship species", "text": "Flagship species\n\nIn conservation biology, a flagship species is a species chosen to raise support for biodiversity conservation in a given place or social context. Definitions have varied, but they have tended to focus on the strategic goals and the socio-economic nature of the concept, to support the marketing of a conservation effort. The species need to be popular, to work as symbols or icons, and to stimulate people to provide money or support.\n\nSpecies selected since the idea was developed in 1980s include widely recognised and charismatic species like the black rhinoceros, the Bengal tiger, and the Asian elephant. More locally significant species like the Chesapeake blue crab and the Pemba flying fox have suited a cultural and social context.\n\nUtilizing a flagship species has limitations. It can skew management and conservation priorities, which may conflict. Stakeholders may be negatively affected if the flagship species is lost. The use of a flagship may have limited effect, and the approach may not protect the species from extinction: all of the top ten charismatic groups of animal including tigers, lions, elephants and giraffes are endangered.\n\nThe term flagship is linked to the metaphor of representation. In its popular usage, flagships are viewed as ambassadors or icons for a conservation project or movement. The geographer Maan Barua noted that metaphors influence what people understand and how they act; that mammals are disproportionately chosen; and that biologists need to come to grips with language to improve the public's knowledge of conservation. Several definitions have been advanced for the flagship species concept and for some time there has been confusion even in the academic literature. Most of the latest definitions focus on the strategic, socio-economic, and marketing character of the concept.\n\n\nThe flagship species concept appears to have become popular around the mid 1980s within the debate on how to prioritise species for conservation. The first widely available references to use the flagship concept applied it to both neotropical primates and African elephants and rhinos, in the mammal-centric approach that still dominates how the concept is used. The use of flagship species has been dominated by large bodied animals, especially mammals, although members of other taxonomic groups have occasionally been used.\n\nFlagship species projects have sometimes been successful in saving the species and its habitat, as with the American bald eagle and the manatee.\n\nChosen flagship species include the Bengal tiger (\"Panthera tigris\"), the giant panda (\"Ailuropoda melanoleuca\"), the Golden lion tamarin (\"Leontopithecus rosalia\"), the African elephant (\"Loxodonta sp.\") and Asian elephant (\"Elephas maximus\"). However, because flagship species are selected according to the audience they are hoping to influence, these species can also belong to traditionally uncharismatic groups, if the cultural and social content is right. Less charismatic but locally significant species include the use of the Pemba flying fox as a flagship in Tanzania, and of the Chesapeake blue crab as a flagship in the USA.\n\nSome flagship species are keystone species, like the African lion, a top predator: it used to control the populations of large herbivores, protecting ecosystems across the entire landscape. However, the lion's ability to serve as a keystone species is decreasing as its population and range decline. The WWF uses flagship species as one of its species classification categories, along with keystone and indicator species. It chooses between these when selecting a priority species to represent the conservation threats facing a certain region.\n\nFlagship species can represent an environmental feature (e.g. a species or ecosystem), cause (e.g. climate change or ocean acidification), organization (e.g. NGO or government department) or geographic region (e.g. state or protected area).\n\nFlagship species can be selected according to many different methodologies, such as social marketing, environmental economics, and conservation biology, depending on what is valued by the audience they try to target, and the goals of the project, such as conservation awareness, fundraising, ecotourism promotion, community-based conservation, and promotion of funded research. This is illustrated by the differences in recommendations made for flagship species selection targeting different target audiences such as local communities and tourists.\n\nThe use of flagship species has some limitations:\n\nLeaving aside the impact on other species, charisma does not seem to protect even charismatic species against extinction. All ten of the most charismatic groups of animal identified in a 2018 study, namely tiger, lion, elephant, giraffe, leopard, panda, cheetah, polar bear, wolf, and gorilla, are currently endangered; only the giant panda shows a demographic growth from an extremely small population. The researchers suggest that the widespread use of images of these animals has given the public the impression that the animals are abundant, obscuring their high risk of imminent extinction. They note that this remains true despite the intense focus of conservation efforts on these particular species. A major challenge for the utilization of several flagship species in non-Western contexts is that they may come into conflict with local communities, thereby jeopardizing well-intended conservation actions. This has been termed 'flagship mutiny', and is exemplified by the Asian elephant in countries where there is human-elephant conflict.\n\nConservation flagships can be used at broader levels, for example as ecosystems like coral reefs, rainforests or protected areas like the Serengeti or Yellowstone. Some recent initiatives have developed flagships based on the conservation value of particular areas or species. Examples of these are the EDGE project run by the Zoological Society of London and the Hotspots run by Conservation International.\nMore recently, work in microbiology has started to use flagship species in a distinct way. This work relates to the biogeography of micro-organisms and uses particular species because \"eyecatching \"flagships\" with conspicuous size and/or morphology are the best distribution indicators\".\n\n\n"}
{"id": "8301203", "url": "https://en.wikipedia.org/wiki?curid=8301203", "title": "Frenemy", "text": "Frenemy\n\n\"Frenemy\" (less commonly spelled \"frienemy\") is an oxymoron and a portmanteau of \"friend\" and \"enemy\" that refers to \"a person with whom one is friendly, despite a fundamental dislike or rivalry\" or \"a person who combines the characteristics of a friend and an enemy\". The term is used to describe personal, geopolitical and commercial relationships both among individuals and groups or institutions. The word has appeared in print as early as 1953 in an article titled \"Howz about calling the Russians our Frienemies?\" by the American gossip columnist Walter Winchel in the Nevada State Journal.\n\nA \"Businessweek\" article stated that frenemies in the workplace are common, due to increasingly informal environments and the \"abundance of very close, intertwined relationships that bridge people's professional and personal lives ... [while] it certainly wasn't unheard of for people to socialize with colleagues in the past, the sheer amount of time that people spend at work now has left a lot of people with less time and inclination to develop friendships outside of the office.\"\n\nSigmund Freud said of himself that “an intimate friend and a hated enemy have always been indispensable to my emotional life...not infrequently…friend and enemy have coincided in the same person”.\n\n\n\n"}
{"id": "1261336", "url": "https://en.wikipedia.org/wiki?curid=1261336", "title": "Golden trout", "text": "Golden trout\n\nThe California golden trout, or simply the golden trout (\"Oncorhynchus mykiss aguabonita\"), is a subspecies of the rainbow trout native to California. The golden trout is native to Golden Trout Creek (tributary to the Kern River), Volcano Creek (tributary to Golden Trout Creek), and the South Fork Kern River. It is the state fish of California.\n\nThe California golden trout is closely related to two other rainbow trout subspecies. The Little Kern golden trout (\"O. m. whitei\"), found in the Little Kern River basin, and the Kern River rainbow trout (\"O. m. gilberti\"), found in the Kern River system. Together, these three trout form what is sometimes referred to as the \"golden trout complex\".\n\nOriginally the golden trout was described as a subspecies of the salmon species, with a name \"Salmo mykiss agua-bonita\", and it is still often considered a subspecies (now called \"Oncorhynchus mykiss aguabonita\") along with several other rainbow trout subspecies commonly known as redband trout.\n\nFishBase and the Catalog of Fishes however now (2014) list \"O. aguabonita\" as an independent species rather than as subspecies of \"O. mykiss\". Likewise, while ITIS lists \"O. m. whitei\" and \"O. m. gilberti\" as subspecies of \"O. mykiss\", \"O. aguabonita\" instead is listed as a full species.\n\nThe golden trout has golden flanks with red, horizontal bands along the lateral lines on each side and about 10 dark, vertical, oval marks (called \"parr marks\") on each side. Dorsal, lateral and anal fins have white leading edges. In their native habitat, adults range from long. Fish over are considered large. Golden trout that have been transplanted to lakes have been recorded up to .\n\nThe golden trout should be distinguished from the similarly named golden rainbow trout, also known as the palomino trout. The golden rainbow is a color variant of the rainbow trout.\n\nThe golden trout is commonly found at elevations from to above sea level, and is native only to California's southern Sierra Nevada mountains. Outside of its native range in California, Golden trout are more often found in cirques and creeks in wilderness areas around 10,500–12,000\"+, often beyond 12,500\"+ passes that are not passable without crampons, ice axes, and ropes until after the Fourth of July. Their preferred water temperature is but they can tolerate temperatures in degraded streams on the Kern Plateau as high as so long as those waters cool during the night. The only other species of fish indigenous to the native range of California golden trout is the Sacramento sucker (\"Catostomus occidentalis occidentalis\").\n\nThe Wyoming Game & Fish Department state record golden trout measured and weighed , caught in Cook Lake, Wyoming in 1948. The IGFA \"All-Tackle Length Record\" for \"O. m. aguabonita\" measured caught in Golden Lake, Wyoming in 2012.\n\n\"O. m. aguabonita\" is native to the southern Sierra Nevada, including the upper reach and tributaries of the South Fork of the Kern River, and Golden Trout Creek and its tributaries. It has been introduced in hundreds of lakes and streams outside the native range, though most of these populations did not last or hybridized with cutthroat trout and other subspecies of rainbow trout. Distribution data that may be incomplete or inaccurate includes the Canadian province of Alberta, and the US states of Arizona, California, Colorado, Idaho, Montana, New Mexico, Nevada, Oregon, Utah, Washington, and Wyoming.\n\nIn 1892 the California golden trout was originally described by David Starr Jordan, the first President of Stanford University, as \"Salmo mykiss agua-bonita\". The fish was named after the Agua Bonita Waterfall where the first specimens were collected, at the mouth of Volcano Creek, at the creek's confluence with the Kern River. A century later they were listed as \"Oncorhynchus mykiss aguabonita\" in Behnke's \"Native trout of western North America\".\n\nIn 1904 Stewart Edward White communicated to his friend President Theodore Roosevelt, that overfishing could lead to extinction of the golden trout. In White's novel \"The Mountains\", he wrote about the threatened golden trout on California’s Kern Plateau. Roosevelt shared White’s concern and, through U.S. Fish Commissioner George M. Bowers, dispatched biologist Barton Warren Evermann of the U.S. Bureau of Fisheries to study the situation. In 1906 Evermann published \"The Golden Trout of the Southern High Sierras\". Based on morphology, Evermann accurately described four forms of this native fish: \"Salmo roosevelti\" from Golden Trout (Volcano) Creek, \"Salmo aguabonita\" from nearby South Fork of the Kern River, \"Salmo whitei\" (named in recognition of Stewart Edward White) from the Little Kern River, and \"Salmo gilberti\", the Kern River rainbow.\n\nGenetic studies have since clarified three groups of trout native to the Kern River: California golden trout (\"O. m. aguabonita\") native to the South Fork Kern River and Golden Trout Creek (tributary to the Kern River mainstem but the historic course of the South Fork Kern River and now only separated from it by a lava flow and ridge of sediment), Little Kern River golden trout (\"O. m. whitei\"), and Kern River rainbow trout (\"O. m. gilberti\").\n\nYears of overexploitation, mismanagement and competition with exotic species have brought golden trout to the brink of being designated as \"threatened\". Introduced brook trout (\"Salvelinus fontinalis\") outcompete them for food, introduced brown trout (\"Salmo trutta\") prey on them and introduced rainbow trout (\"O. mykiss\") hybridize with them, damaging the native gene pool through introgression. Populations have been in steady decline for decades.\n\nIn 1978 the Golden Trout Wilderness was established within Inyo National Forest and Sequoia National Forest, protecting the upper watersheds of the Kern River and South Fork Kern River.\n\nIn September 2004, the California Department of Fish and Game signed an agreement with federal agencies to work on restoring back-country habitat, heavily damaged by overgrazing from cattle and sheep, as part of a comprehensive conservation strategy.\n\nThe US Endangered Species Act (USESA) designated the subspecies \"O. m. whitei\" as LT, or Listed Threatened, since 1978, under the name \"Oncorhynchus aguabonita whitei\".\n\nNatureServe has designated the following NatureServe Conservation Status for the three subspecies:\n\n\nThe American Fisheries Society has designated all three subspecies as Threatened since August 2008.\n\nFor sportfishing, the California golden trout underwent many twentieth century translocations into multiple Western states and established populations survive in California, Idaho, Montana, Utah, Washington, Colorado, and Wyoming. Populations in the high-elevation lakes in the Ruby Mountains, Nevada, have died out. The current status in other states where the California golden trout were planted (Arizona, Nevada, New Mexico and Oregon) lacks documentation.\n\nWhen Colonel Chuck Yeager introduced one of his commanding officers, General Irving \"Twig\" Branch, to the Sierra Nevada populations of golden trout, Branch ordered Yeager and Bud Anderson to introduce the species to the mountain streams of New Mexico. However, the New Mexico populations have also died out.\n\nIn his second autobiography, \"Press On\", Yeager details his annual fishing trips to catch golden trout which he extols as one of the best game fish and best eating fish to be found.\n"}
{"id": "4157971", "url": "https://en.wikipedia.org/wiki?curid=4157971", "title": "Goodness", "text": "Goodness\n\nGoodness may refer to:\n\n\n"}
{"id": "1118323", "url": "https://en.wikipedia.org/wiki?curid=1118323", "title": "Grievance", "text": "Grievance\n\nIn general, grievance (from class. lat. \"gravis\": heavy) is a wrong or hardship suffered, real or supposed, which forms legitimate grounds of complaint. In the past, the word meant oppressive state of things.\n\nThe revolt of English barons in the early thirteenth century which led to the Magna Carta of 1215 was partly motivated by grievances against abuses by King John. This right to Petition the king, for grievances, was affirmed in the Bill of Rights 1689\n\nThe United States Declaration of Independence is mainly an enumeration of the colonists' grievances against King George III.\n\n"}
{"id": "8026722", "url": "https://en.wikipedia.org/wiki?curid=8026722", "title": "Hiroshima National Peace Memorial Hall for the Atomic Bomb Victims", "text": "Hiroshima National Peace Memorial Hall for the Atomic Bomb Victims\n\nHiroshima National Peace Memorial Hall for the Atomic Bomb Victims is one of the National Memorial Halls in Hiroshima, Japan.\n\nThe Hall was founded by the Japanese national government to mourn the atomic bomb victims in 2002. It was designed by Kenzo Tange. There is another National Peace Memorial Hall for the Atomic Bomb Victims in Nagasaki built for the same purpose.\n\nThe Hall is in Hiroshima Peace Memorial Park near Hiroshima Peace Memorial across the \"Motoyasu River\" by \"Motoyasu Bridge\". The Hall curators are collecting atomic bomb memories and stories from the survivors to mourn the victims, as the survivors are aging. They are also collecting names and photographs of atomic bomb victims for the same purpose and for the same reason. From the collection, they are developing a project to \"read the stories of the atomic bombing\".\n\nAdmission is free of charge.\n\n\n"}
{"id": "7051566", "url": "https://en.wikipedia.org/wiki?curid=7051566", "title": "Idios kosmos", "text": "Idios kosmos\n\nIdios kosmos comes from Greek and means private world. It exists with, and is opposite to, koinos kosmos (shared world). \"Idios kosmos\" is the view of the world that is developed from personal experience and knowledge and is therefore unique; however, it can be difficult to tell the difference between it and \"koinos kosmos\".\n\nThe two phrases come from the Diels-Kranz fragment B89 of Heraclitus: ὁ Ἡράκλειτός φησι τοῖς ἐγρηγορόσιν ἕνα καὶ κοινὸν κόσμον εἶναι τῶν δὲ κοιμωμένων ἕκαστον εἰς ἴδιον ἀποστρέφεσθαι (\"Heraclitus said that the waking have one common world, but the sleeping turn aside each into a world of his own.\")\n\nThe idea of \"idios kosmos\" is an important part of Philip K. Dick's views on schizophrenia, as expressed in his 1964 essay \"Schizophrenia & 'The Book of Changes'\", drawing on personal experience with the I Ching.\n\n\n"}
{"id": "25940209", "url": "https://en.wikipedia.org/wiki?curid=25940209", "title": "Jacek Tylicki", "text": "Jacek Tylicki\n\nJacek Tylicki (born 1951 in Sopot, Poland) is a Polish artist who settled in New York City in 1982. Tylicki works in the field of land art, installation art, and site-specific art. His conceptual projects often raise social and environmental issues.\n\nStarting in 1973, Tylicki began sending sheets of canvas or paper into the wind, rivers, or forests and leaving them for a long while in a natural environment, thus forcing upon nature an attitude previously reserved to the artist: the creation of forms. The project is often called \"natural art\".\nIn the years 1974–1990, he initiated the idea of an anonymous artist by issuing a periodical called \"Anonymous Artists\" where artists could present their art without revealing their own names.\n\nIn 1985 he created an installation called \"Chicken Art\". Tylicki transformed the Now Gallery in Manhattan to a hen house in which live chickens watched realistic paintings of chickens, chicks and roosters hanging on the gallery walls. Tylicki declared: \n\nAnother installation was the \"Free Art\", where Tylicki invited well-known artists, including Mark Kostabi and Rodney Greenblat, to give away their art to the public for free. Video and photography play an important role in his work as a record of its elusiveness and transience.\n\n\n\n"}
{"id": "29793734", "url": "https://en.wikipedia.org/wiki?curid=29793734", "title": "Joy Bale Boone", "text": "Joy Bale Boone\n\nJoy Bale Boone (October 29, 1912 – October 3, 2002) was an American poet best known for her devotion to the arts. She was also active in the women's liberation movement throughout her life. Although she was born in Chicago, Illinois Boone spent most of her life in Kentucky.\n\nBoone became interested in poetry at a very young age. As a young girl, she attended the Chicago Latin School and then went on to Roycemore School for girls. Boone received inspiration as a young girl from poet Harriet Monroe, who lived just a few blocks away from her as a child. Bale Boone came to Kentucky to begin her career in writing after she met her husband, Shelby Garnett Bale. The two met in Chicago while Garnett Bale was attending medical school at Northwestern University. They were married in 1934. In the first few years of their marriage, the couple lived in both New York and Louisville while Garnett Bale finished his residencies.\n\nIn 1944, Boone formed the League of Women Voters in Hardin County, Kentucky. and served as its first president. Bale Boone's first job in Kentucky came in 1945 as a book reviewer for the Louisville Courier-Journal. In 1964, Boone went on to found the literary magazine \"Approaches\". She held the position of editor of the magazine until 1975. She was also the editor for the 1964 and 1967 \"Contemporary Poetry\" collections. Bale Boone has had many individual poems published, but her most significant work was \"The Storm's Eye: A Narrative in Verse Celebrating Cassius Marcellus Clay, Man of Freedom 1810–1903.\" Her two collections of poetry include: \"Never Less Than Love\" (1972) and \"Even Without Love\" (1992). Boone received the Distinguished Kentuckian Award from KET in 1974. She also received the Sullivan Award from the University of Kentucky in 1969. Finally, in 1997, Boone was honored by being named the Poet Laureate of Kentucky.\n\nBoone spent most her life in Elizabethtown, Kentucky with her first husband, physician Shelby Garnett Bale. The couple had four sons and two daughters. Shelby Garnett Bale (Senior) died in 1972. In 1975, Boone married George Street Boone of Elkton. After marrying, she spent many years residing in Elkton, Kentucky where she continued to write and actively serve the state of Kentucky through the arts.\n\nAfter suffering from an illness for some time, Boone died in Glasgow, Kentucky on Tuesday, October 3, 2002, at the age of 89.\n\nBoone dedicated her life to the arts. Throughout her life, she served on numerous committees and boards in hopes that more people would have the opportunity to experience the arts in the way that she had. She served as President of the Friends of Kentucky Libraries; in this role, she spearheaded the creation of the bookmobile, which is still used today to deliver books to those who are unable to come to the library. She served on many other boards and committees, these include: the Kentucky Educational Television Advisory Board, Kentucky Council on Higher Education (now the Kentucky Council on Postsecondary Education), Editorial Board of the University Press of Kentucky, the Kentucky Humanities Council, chair of the Robert Penn Warren Committee at Western Kentucky University, board member the Robert Penn Warren Circle at Duke University, director of the Thomas Clark Foundation of the University Press of Kentucky, and the Gaines Center for the Humanities at the University of Kentucky.\n"}
{"id": "12461938", "url": "https://en.wikipedia.org/wiki?curid=12461938", "title": "Langkawi Declaration", "text": "Langkawi Declaration\n\nThe Langkawi Declaration on the Environment was a declaration issued by the assembled Heads of Government of the Commonwealth of Nations on the issue of environmental sustainability. It was issued on October 21, 1989 at Langkawi, Malaysia, during the tenth Commonwealth Heads of Government Meeting (CHOGM).\n\nThe declaration covers a wide range of topics related to the environment, blaming 'past neglect in managing the natural environment and resources'. It lists what the Heads of Governments perceived to be the main environmental problems: the greenhouse effect, damage to the ozone layer, acid rain, marine pollution, land degradation, and species extinction. These, the declaration affirmed, were issues that transcended national borders, and hence required the involvement of international organisations, such as the Commonwealth, to coordinate strategies to solve them.\n\nA key agreement in the formulation of the agreement was the pledge by developed countries not to connect future international development aid to commitment to environmental sustainability or introduce trade barriers. This, the developing countries argued, would prevent economic growth (described as a 'compelling necessity'), and hence reduce their ability to develop sustainable natural environments. In exchange, the developing countries conceded to the Commonwealth's developed members (particularly Australia, Canada, New Zealand, and the United Kingdom), their interest in protecting the environment.\n\nAmongst the commitments made by members in the Langkawi Declaration were:\n\n"}
{"id": "91591", "url": "https://en.wikipedia.org/wiki?curid=91591", "title": "Linearity", "text": "Linearity\n\nLinearity is the property of a mathematical relationship or function which means that it can be graphically represented as a straight line. Examples are the relationship of voltage and current across a resistor (Ohm's law), or the mass and weight of an object. Proportionality implies linearity, but linearity does not imply proportionality.\n\nIn mathematics, a linear map or linear function \"f\"(\"x\") is a function that satisfies the following two properties:\n\n\nThe homogeneity and additivity properties together are called the superposition principle. It can be shown that additivity implies homogeneity in all cases where α is rational; this is done by proving the case where α is a natural number by mathematical induction and then extending the result to arbitrary rational numbers. If \"f\" is assumed to be continuous as well, then this can be extended to show homogeneity for any real number α, using the fact that rationals form a dense subset of the reals.\n\nIn this definition, \"x\" is not necessarily a real number, but can in general be a member of any vector space. A more specific definition of linear function, not coinciding with the definition of linear map, is used in elementary mathematics.\n\nThe concept of linearity can be extended to linear operators. Important examples of linear operators include the derivative considered as a differential operator, and many constructed from it, such as del and the Laplacian. When a differential equation can be expressed in linear form, it is generally straightforward to solve by breaking the equation up into smaller pieces, solving each of those pieces, and summing the solutions.\n\nLinear algebra is the branch of mathematics concerned with the study of vectors, vector spaces (also called linear spaces), linear transformations (also called linear maps), and systems of linear equations.\n\nThe word linear comes from the Latin word \"linearis\", which means \"pertaining to or resembling a line\". For a description of linear and nonlinear equations, see \"linear equation\". Nonlinear equations and functions are of interest to physicists and mathematicians because they can be used to represent many natural phenomena, including chaos.\n\nIn a different usage to the above definition, a polynomial of degree 1 is said to be linear, because the graph of a function of that form is a line.\n\nOver the reals, a linear equation is one of the forms:\n\nwhere \"m\" is often called the slope or gradient; \"b\" the y-intercept, which gives the point of intersection between the graph of the function and the \"y\"-axis.\n\nNote that this usage of the term \"linear\" is not the same as in the section above, because linear polynomials over the real numbers do not in general satisfy either additivity or homogeneity. In fact, they do so if and only if . Hence, if , the function is often called an affine function (see in greater generality affine transformation).\n\nIn Boolean algebra, a linear function is a function formula_2 for which there exist formula_3 such that\n\nA Boolean function is linear if one of the following holds for the function's truth table:\n\nAnother way to express this is that each variable always makes a difference in the truth value of the operation or it never makes a difference.\n\nNegation, Logical biconditional, exclusive or, tautology, and contradiction are linear functions.\n\nIn physics, \"linearity\" is a property of the differential equations governing many systems; for instance, the Maxwell equations or the diffusion equation.\n\nLinearity of a differential equation means that if two functions \"f\" and \"g\" are solutions of the equation, then any linear combination is, too.\n\nIn instrumentation, linearity means that for every change in the variable you are observing, you get the same change in the output of the measurement apparatus - this is highly desirable in scientific work. In general, instruments are close to linear over a useful certain range, and most useful within that range. In contrast, human senses are highly nonlinear- for instance, the brain totally ignores incoming light unless it exceeds a certain absolute threshold number of photons.\n\nIn electronics, the linear operating region of a device, for example a transistor, is where a dependent variable (such as the transistor collector current) is directly proportional to an independent variable (such as the base current). This ensures that an analog output is an accurate representation of an input, typically with higher amplitude (amplified). A typical example of linear equipment is a high fidelity audio amplifier, which must amplify a signal without changing its waveform. Others are linear filters, linear regulators, and linear amplifiers in general.\n\nIn most scientific and technological, as distinct from mathematical, applications, something may be described as linear if the characteristic is approximately but not exactly a straight line; and linearity may be valid only within a certain operating region—for example, a high-fidelity amplifier may distort a small signal, but sufficiently little to be acceptable (acceptable but imperfect linearity); and may distort very badly if the input exceeds a certain value, taking it away from the approximately linear part of the transfer function.\n\nFor an electronic device (or other physical device) that converts a quantity to another quantity, Bertram S. Kolts writes:\n\nThere are three basic definitions for integral linearity in common use: independent linearity, zero-based linearity, and terminal, or end-point, linearity. In each case, linearity defines how well the device's actual performance across a specified operating range approximates a straight line. Linearity is usually measured in terms of a deviation, or non-linearity, from an ideal straight line and it is typically expressed in terms of percent of full scale, or in ppm (parts per million) of full scale. Typically, the straight line is obtained by performing a least-squares fit of the data. The three definitions vary in the manner in which the straight line is positioned relative to the actual device's performance. Also, all three of these definitions ignore any gain, or offset errors that may be present in the actual device's performance characteristics.\n\nMany times a device's specifications will simply refer to linearity, with no other explanation as to which type of linearity is intended. In cases where a specification is expressed simply as linearity, it is assumed to imply independent linearity.\n\nIndependent linearity is probably the most commonly used linearity definition and is often found in the specifications for DMMs and ADCs, as well as devices like potentiometers. Independent linearity is defined as the maximum deviation of actual performance relative to a straight line, located such that it minimizes the maximum deviation. In that case there are no constraints placed upon the positioning of the straight line and it may be wherever necessary to minimize the deviations between it and the device's actual performance characteristic.\n\nZero-based linearity forces the lower range value of the straight line to be equal to the actual lower range value of the device's characteristic, but it does allow the line to be rotated to minimize the maximum deviation. In this case, since the positioning of the straight line is constrained by the requirement that the lower range values of the line and the device's characteristic be coincident, the non-linearity based on this definition will generally be larger than for independent linearity.\n\nFor terminal linearity, there is no flexibility allowed in the placement of the straight line in order to minimize the deviations. The straight line must be located such that each of its end-points coincides with the device's actual upper and lower range values. This means that the non-linearity measured by this definition will typically be larger than that measured by the independent, or the zero-based linearity definitions. This definition of linearity is often associated with ADCs, DACs and various sensors.\n\nA fourth linearity definition, absolute linearity, is sometimes also encountered. Absolute linearity is a variation of terminal linearity, in that it allows no flexibility in the placement of the straight line, however in this case the gain and offset errors of the actual device are included in the linearity measurement, making this the most difficult measure of a device's performance. For absolute linearity the end points of the straight line are defined by the ideal upper and lower range values for the device, rather than the actual values. The linearity error in this instance is the maximum deviation of the actual device's performance from ideal.\n\nIn military tactical formations, \"linear formations\" were adapted from phalanx-like formations of pike protected by handgunners towards shallow formations of handgunners protected by progressively fewer pikes. This kind of formation would get thinner until its extreme in the age of Wellington with the 'Thin Red Line'. It would eventually be replaced by skirmish order at the time of the invention of the breech-loading rifle that allowed soldiers to move and fire independently of the large-scale formations and fight in small, mobile units.\n\nLinear is one of the five categories proposed by Swiss art historian Heinrich Wölfflin to distinguish \"Classic\", or Renaissance art, from the Baroque. According to Wölfflin, painters of the fifteenth and early sixteenth centuries (Leonardo da Vinci, Raphael or Albrecht Dürer) are more linear than \"painterly\" Baroque painters of the seventeenth century (Peter Paul Rubens, Rembrandt, and Velázquez) because they primarily use outline to create shape. Linearity in art can also be referenced in digital art. For example, hypertext fiction can be an example of nonlinear narrative, but there are also websites designed to go in a specified, organized manner, following a linear path.\n\nIn music the linear aspect is succession, either intervals or melody, as opposed to simultaneity or the vertical aspect.\n\nIn measurement, the term \"linear foot\" refers to the number of feet in a straight line of material (such as lumber or fabric) generally without regard to the width. It is sometimes incorrectly referred to as \"lineal feet\"; however, \"lineal\" is typically reserved for usage when referring to ancestry or heredity. The words \"linear\" & \"lineal\" \nboth descend from the same root meaning, the Latin word for line, which is \"linea\".\n\n"}
{"id": "49454822", "url": "https://en.wikipedia.org/wiki?curid=49454822", "title": "Matrix field", "text": "Matrix field\n\nIn abstract algebra, a matrix field is a field with matrices as elements. In field theory we come across two type of fields: finite field and infinite field. There are several examples of matrix fields of finite and infinite order. In general, corresponding to each field of numbers there is a matrix field. \n\nThere is a finite matrix field of order \"p\" for each positive prime \"p\". One can find several finite matrix field of order p for any given positive prime p. In general, corresponding to each finite field there is a matrix field. However any two finite fields of equal order are algebraically equivalent. The elements of a finite field can be represented by matrices. In this way one can construct a finite matrix field.\n\nContrary to the general case for matrix multiplication, multiplication is commutative in a matrix field (if the usual operations are used). Since addition and multiplication of matrices have all needed properties for field operations except for commutativity of multiplication, one way to verify if a set of matrices is a field with the usual operations of matrix sum and multiplication is to check wether \n\n\nHowever, it may be possible to construct fields of matrices using other operations -- in which case it would be necessary to verify all properties of the operations in the set in order to guarantee that it is a field.\n\n1. The set of all diagonal matrices of order n over the field of rational (real or complex) numbers is a matrix field of infinite order under addition and multiplication of matrices.\n\n2. The set of all diagonal matrices of order two over the field of integers modulo \"p\" (a positive prime) forms a finite matrix field of order \"p\" under addition and multiplication of matrices modulo \"p\".\n\n3. (a negative example) It does not suffice for multiplication to commute in order to obtain a field of matrices. Take the set of all formula_1 matrices of the form\nwith formula_3 -- that is, matrices filled with zeroes except for the formula_4-th row, which is filled with the same real constant formula_5.\nThese matrices are commutative for multiplication:\n\nHowever, there is no neutral element for multiplication, since this needs to be the identity matrix, and it is not included in the set, so this is \"not\" a matrix field.\n\n4. The set of all matrices of order two of the form\nover the real numbers (or over any other field), with the usual operations of matrix sum and matrix multiplication.\nThe zero matrix (formula_8) and the identity matrix (formula_9) are included. Summing two of these matrices results in another matrix of the same form:\nThe same is true for multiplication:\nFinally, multiplication is commutative for these matrices:\n\n5. As a particular case of example 4, taking formula_13 and formula_14 ranging over the reals, one obtains the field of matrices of the form\nwhich is isomorphic to the field formula_16 of the complex numbers: formula_17 corresponds to the real part of the number, while formula_18 correspods to the imaginary part. So the number formula_19, for example, would be represented as\nOne can easily verify, for example, that formula_21:\nand also, by computing a matrix exponential, that Euler's identity, formula_23 is also valid:\n6. As another particular case of example 4, a field of matrices \nwith formula_26 square-free and formula_14 rationals is isomorphic to the quadratic field formula_28.\n\n"}
{"id": "49220613", "url": "https://en.wikipedia.org/wiki?curid=49220613", "title": "Meson bomb", "text": "Meson bomb\n\nThe meson bomb was a proposed nuclear weapon that would derive its destructive force from meson interactions with fissionable material like uranium. The idea behind the bomb was rejected by most scientists, but during the Cold War, American intelligence managed to trick the Soviet Union into conducting research on this topic, which resulted in several years of wasted labor by one of the Soviet nuclear weapon research bureaus.\n\nMesons (hadronic subatomic particles composed of one quark and one antiquark, bound together by the strong interaction) were proposed to form a nuclear weapon as early as the 1940s. Early speculation suggested that the resulting bomb would be the most powerful nuclear weapon yet to have been developed. American physicist Ernest Lawrence used the potential military applications of mesons to obtain funding for its synchrocyclotron built between 1940 and 1946 at the University of California, Berkeley. Soon, however, the scientific consensus was that construction of such a bomb would be impossible; and in 1968 physicist M. Stanley Livingston wrote that \"no responsible scientists would attempt to justify support in this field with predictions of an 'anti-matter engine,' or a super 'meson bomb,' or a 'hyper-drive' for spaceships.\"\n\nIn the 1960s, however, the Soviet Union became convinced that the United States had already developed the meson bomb, and devoted considerable resources to developing its own. In 1994, Russian physicist and chief constructor of nuclear weapons bureau KB-11 (Design Bureau No. 11, set up in 1946) Arkadiy Brish stated that their work on the meson bomb was prompted by intelligence. Even though physicists Yuliy Borisovich Khariton and Yakov Borisovich Zel'dovich agreed that the reports were nonsensical, the directors of the Soviet atomic bomb project decided to pursue this line of research, which resulted in several years of fruitless labor. Brish attributed this to American disinformation.\n\n"}
{"id": "2145170", "url": "https://en.wikipedia.org/wiki?curid=2145170", "title": "Multicategory", "text": "Multicategory\n\nIn mathematics (especially category theory), a multicategory is a generalization of the concept of category that allows morphisms of multiple arity. If morphisms in a category are viewed as analogous to functions, then morphisms in a multicategory are analogous to functions of several variables. Multicategories, are also sometimes called operads, or colored operads.\n\nA (non-symmetric) multicategory consists of\nAdditionally, there are composition operations: Given a sequence of sequences formula_4 of objects, a sequence formula_5 of objects, and an object \"Z\": if\nthen there is a composite morphism formula_9 from formula_10 to \"Z\". This must satisfy certain axioms:\n\nA \"comcategory\" (co-multi-category) is a totally ordered set O of objects, a set A of \"multiarrows\" with two functions\n\nformula_21\n\nformula_22\n\nwhere O is the set of all finite ordered sequences of elements of O. The dual image of a multiarrow f may be summarized\n\nformula_23\n\nA comcategory C also has a \"multiproduct\" with the usual character of a composition operation. C is said to be associative if there holds a \"multiproduct axiom\" in relation to this operator.\n\nAny multicategory, symmetric \"or\" non-symmetric, together with a total-ordering of the object set, can be made into an equivalent comcategory.\n\nA \"multiorder\" is a comcategory satisfying the following conditions.\n\n\nMultiorders are a generalization of partial orders (posets), and were first introduced (in passing) by Tom Leinster.\n\nThere is a multicategory whose objects are (small) sets, where a morphism from the sets \"X\", \"X\", ..., and \"X\" to the set \"Y\" is an \"n\"-ary function,\nthat is a function from the Cartesian product \"X\" × \"X\" × ... × \"X\" to \"Y\".\n\nThere is a multicategory whose objects are vector spaces (over the rational numbers, say), where a morphism from the vector spaces \"X\", \"X\", ..., and \"X\" to the vector space \"Y\" is a multilinear operator, that is a linear transformation from the tensor product \"X\" ⊗ \"X\" ⊗ ... ⊗ \"X\" to \"Y\".\n\nMore generally, given any monoidal category C, there is a multicategory whose objects are objects of C, where a morphism from the C-objects \"X\", \"X\", ..., and \"X\" to the C-object \"Y\" is a C-morphism from the monoidal product of \"X\", \"X\", ..., and \"X\" to \"Y\".\n\nAn operad is a multicategory with one unique object; except in degenerate cases, such a multicategory does not come from a monoidal category.\n\nExamples of multiorders include \"pointed multisets\" , \"integer partitions\" , and \"combinatory separations\" . The triangles (or compositions) of any multiorder are morphisms of a (not necessarily associative) category of \"contractions\" and a comcategory of \"decompositions\". The contraction category for the multiorder of \"multimin partitions\" is the simplest known category of multisets.\n\nMulticategories are often incorrectly considered to belong to higher category theory, as their original application was the observation that the operators and identities satisfied by higher categories are the objects and multiarrows of a multicategory. The study of n-categories was in turn motivated by applications in algebraic topology and attempts to describe the homotopy theory of higher dimensional manifolds. However it has mostly grown out of this motivation and is now also considered to be part of pure mathematics.\n\nThe correspondence between contractions and decompositions of triangles in a multiorder allows one to construct an associative algebra called its \"incidence algebra\". Any element that is nonzero on all unit arrows has a compositional inverse, and the \"Möbius function\" of a multiorder is defined as the compositional inverse of the zeta function (constant-one) in its incidence algebra.\n\nMulticategories were first introduced under that name by Jim Lambek in \"Deductive systems and categories II\" (1969) He mentions (p.108) that he was \"told that multicategories have also been studied by [Jean] Benabou and [Pierre] Cartier\", and indeed Leinster opines that \"the idea might have occurred to anyone who knew what both a category and a multilinear map were\".\n"}
{"id": "5644212", "url": "https://en.wikipedia.org/wiki?curid=5644212", "title": "North east down", "text": "North east down\n\nNorth east down (NED), also known as local tangent plane (LTP), is a geographical coordinate system for representing state vectors that is commonly used in aviation. It consists of three numbers: one represents the position along the northern axis, one along the eastern axis, and one represents vertical position. Down is chosen as opposed to up in order to comply with the right-hand rule. The origin of this coordinate system is usually chosen to be a point on the surface of the geoid below the aircraft's center of gravity. However, care must be taken since, if the aircraft is accelerating (turning or accelerating linearly), then the NED coordinates are no longer inertial coordinates.\n\nNED coordinates are similar to ECEF in that they're Cartesian, however they can be more convenient due to the relatively small numbers involved, and also because of the intuitive axes. NED and ECEF coordinates can be related with the following formula:\n\nwhere formula_2 is a 3D position in a NED system, formula_3 is the corresponding ECEF position, formula_4 is the reference ECEF position (where the local tangent plane originates), and formula_5 is a rotation matrix whose columns are the north, east, and down axes. formula_5 may be defined conveniently from the latitude formula_7 and longitude formula_8 corresponding to formula_4:\n\n"}
{"id": "39771519", "url": "https://en.wikipedia.org/wiki?curid=39771519", "title": "Nothing to Hide (book)", "text": "Nothing to Hide (book)\n\nNothing to Hide: The False Tradeoff Between Privacy and Security is a book written by Daniel J. Solove regarding the nothing to hide argument regarding privacy. It was published by Yale University Press in 2011.\n\nThe book, written for a general audience, includes some material that had been adapted by law review articles written by Solove. Raymond G. Kessler wrote in the \"Law and Politics Book Review\" that \"the average reader may find some discussions of the law difficult to follow.\" The book has twenty one chapters in four parts. The parts are \"How We Should Assess and Balance the Values of Privacy and Security\", \"How the Law Should Address Matters of National Security\", \"How the Constitution Should Protect Privacy\" and \"How the Law Should Cope With Changing Technology\".\n\nTony Doyle wrote in a book review published in the \"Journal of Value Inquiry\" that \"Overwhelmingly \"Nothing to Hide\" is a carefully argued, hysteria-free book\" and that the author \"makes a strong case for the profound social value of privacy, the siege it is currently under, and how to preserve it.\" Woodrow Hartzog wrote in the \"Michigan Law Review\" that the thesis of his book review is \"Solove's polemic is a strong and desperately needed collection of frames that counterbalances the \"nothing to hide\" argument and other refrains so often used in privacy disputes.\" J.M. Keller wrote in \"\" that the book is \"Interesting for those with a passing interest in privacy, security, or legal rhetoric, yet well researched enough to recommend to privacy and security scholars.\"\n\nDelmus E. Williams of the \"Journal of Academic Librarianship\" argues that while Solove \"writes well\" and \"offers solid support for those who would argue that we need more protection for personal privacy\" he argues that the book \"does not claim to a balanced, and thoughtful readers will want to find other sources if they hope to understand counter arguments.\" Raymond G. Kessler wrote in the \"Law and Politics Book Review\" that \"Being so short, some readers might find it wanting in details, examples and explanations. However, this work is one very, very, good place to start.\"\n\n"}
{"id": "851554", "url": "https://en.wikipedia.org/wiki?curid=851554", "title": "PDCA", "text": "PDCA\n\nPDCA (plan–do–check–act or plan–do–check–adjust) is an iterative four-step management method used in business for the control and continuous improvement of processes and products. It is also known as the Deming circle/cycle/wheel, the Shewhart cycle, the control circle/cycle, or plan–do–study–act (PDSA). Another version of this PDCA cycle is OPDCA. The added \"O\" stands for \"observation\" or as some versions say: \"Observe the current condition.\" This emphasis on observation and current condition has currency with the literature on lean manufacturing and the Toyota Production System. The PDCA cycle, with Ishikawa’s changes, can be traced back to S. Mizuno of the Tokyo Institute of Technology in 1959.\n\nThe planning phase involves assessing a current process, or a new process, and figuring out how it can be improved upon. Knowing what types of outputs are desired helps to develop a plan to fix or improve the process. It is often easier to plan smaller changes during this phase of the plan so that they can be easily monitored and the outputs are more predictable. Establish the objectives and necessary to deliver results in accordance with the expected output (the target or goals)\n\nThe do phase allows the plan from the previous step to be enacted. Small changes are usually tested, and data is gathered to see how effective the change is.\n\nDuring the check phase, the data and results gathered from the do phase are evaluated. Data is compared to the expected outcomes to see any similarities and differences. The testing process is also evaluated to see if there were any changes from the original test created during the planning phase. If the data is placed in a chart it can make it easier to see any trends if the PDCA cycle is conducted multiple times. This helps to see what changes work better than others, and if said changes can be improved as well.\n\nExample: Gap analysis, or Appraisals\n\nIf the check phase shows that the plan phase which was implemented in do phase is an improvement to the prior standard (baseline), then that becomes the new standard (baseline) for how the organization should act going forward (new standards are thus said to be \"enACTed\"). Instead, if the check phase shows that the plan phase which was implemented in do phase is not an improvement, then the existing standard (baseline) will remain in place. In either case, if the check phase showed something different than expected (whether better or worse), then there is some more learning to be done... and that will suggest potential future PDCA cycles. Note that some who teach PDCA assert that the act phase involves making adjustments or corrective actions, but generally it would be counter to PDCA thinking to propose and decide upon alternative changes without using a proper plan phase, or to make them the new standard (baseline) without going through do and check steps.\n\nThe adjust phase is the alternative version of the act phase. Once PDCA has been run multiple times, the process generally has enough information for it to be considered a new standard. This is usually completed in the act phase. The adjust phase allows the process to continue to be monitored after the changes have been implemented and fix them accordingly. Doing this lets the PDCA cycle truly be for continuous improvement instead of changing a process and letting it become inefficient again.\n\nPDCA was made popular by W. Edwards Deming, who is considered by many to be the father of modern quality control; however, he always referred to it as the \"Shewhart cycle\". Later in Deming's career, he modified PDCA to \"Plan, Do, Study, Act\" (PDSA) because he felt that \"check\" emphasized inspection over analysis. The PDSA cycle was used to create the model of know-how transfer process, and other models.\n\nThe concept of PDCA is based on the scientific method, as developed from the work of Francis Bacon (\"Novum Organum\", 1620). The scientific method can be written as \"hypothesis–experiment–evaluation\" or as \"plan–do–check\". Walter A. Shewhart described manufacture under \"control\"—under statistical control—as a three-step process of specification, production, and inspection. He also specifically related this to the scientific method of hypothesis, experiment, and evaluation. Shewhart says that the statistician \"must help to change the demand [for goods] by showing [...] how to close up the tolerance range and to improve the quality of goods.\" Clearly, Shewhart intended the analyst to take action based on the conclusions of the evaluation. According to Deming, during his lectures in Japan in the early 1950s, the Japanese participants shortened the steps to the now traditional \"plan, do, check, act\". Deming preferred \"plan, do, study, act\" because \"study\" has connotations in English closer to Shewhart's intent than \"check\".\n\nA fundamental principle of the scientific method and PDCA is iteration—once a hypothesis is confirmed (or negated), executing the cycle again will extend the knowledge further. Repeating the PDCA cycle can bring its users closer to the goal, usually a perfect operation and output.\n\nAnother fundamental function of PDCA is the \"hygienic\" separation of each phase, for if not properly separated measurements of effects due to various simultaneous actions (causes) risk becoming confounded.\n\nPDCA (and other forms of scientific problem solving) is also known as a system for developing critical thinking. At Toyota this is also known as \"Building people before building cars\". Toyota and other lean manufacturing companies propose that an engaged, problem-solving workforce using PDCA in a culture of critical thinking is better able to innovate and stay ahead of the competition through rigorous problem solving and the subsequent innovations.\n\nDeming continually emphasized iterating towards an improved system, hence PDCA should be repeatedly implemented in spirals of increasing knowledge of the system that converge on the ultimate goal, each cycle closer than the previous. One can envision an open coil spring, with each loop being one cycle of the scientific method, and each complete cycle indicating an increase in our knowledge of the system under study. This approach is based on the belief that our knowledge and skills are limited, but improving. Especially at the start of a project, key information may not be known; the PDCA—scientific method—provides feedback to justify guesses (hypotheses) and increase knowledge. Rather than enter \"analysis paralysis\" to get it perfect the first time, it is better to be approximately right than exactly wrong. With improved knowledge, one may choose to refine or alter the goal (ideal state). The aim of the PDCA cycle is to bring its users closer to whatever goal they choose.\n\nWhen PDCA is used for complex projects or products with a certain controversy, checking with external stakeholders should happen before the Do stage, since changes to projects and products that are already in detailed design can be costly; this is also seen as Plan-Check-Do-Act.\n\nRate of change, that is, rate of improvement, is a key competitive factor in today's world. PDCA allows for major \"jumps\" in performance (\"breakthroughs\" often desired in a Western approach), as well as kaizen (frequent small improvements). In the United States a PDCA approach is usually associated with a sizable project involving numerous people's time, and thus managers want to see large \"breakthrough\" improvements to justify the effort expended. However, the scientific method and PDCA apply to all sorts of projects and improvement activities.\n\n"}
{"id": "23577", "url": "https://en.wikipedia.org/wiki?curid=23577", "title": "Partial function", "text": "Partial function\n\nIn mathematics, a partial function from \"X\" to \"Y\" (sometimes written as or ) is a function , for some proper subset \"X\" of \"X\". It generalizes the concept of a function by not forcing \"f\" to map \"every\" element of \"X\" to an element of \"Y\" (only some proper subset \"X\" of \"X\"). If , then \"f\" is called a total function and is equivalent to a function. Partial functions are often used when the exact domain, \"X\", is not known (e.g. many functions in computability theory).\n\nSpecifically, we will say that for any , either:\n\nFor example, we can consider the square root function restricted to the integers\n\nThus \"g\"(\"n\") is only defined for \"n\" that are perfect squares (). So, , but \"g\"(26) is undefined.\n\nThere are two distinct meanings in current mathematical usage for the notion of the domain of a partial function. Most mathematicians, including recursion theorists, use the term \"domain of \"f\"\" for the set of all values \"x\" such that \"f\"(\"x\") is defined (\"X<nowiki>'</nowiki>\" above). But some, particularly category theorists, consider the domain of a partial function \"f\":\"X\" → \"Y\" to be \"X\", and refer to \"X<nowiki>'</nowiki>\" as the domain of definition. Similarly, the term range can refer to either the codomain or the \"image\" of a function.\n\nA partial function is said to be injective or surjective when the total function given by the restriction of the partial function to its domain of definition is injective or surjective respectively. A partial function may be both injective and surjective (and thus bijective).\n\nBecause a function is trivially surjective when restricted to its image, the term partial bijection denotes a partial function which is injective.\n\nAn injective partial function may be inverted to an injective partial function, and a partial function which is both injective and surjective has an injective function as inverse. Furthermore, a total function which is injective may be inverted to an injective partial function.\n\nThe notion of transformation can be generalized to partial functions as well. A partial transformation is a function , where both \"A\" and \"B\" are subsets of some set \"X\".\n\nTotal function is a synonym for function. The use of the adjective \"total\" is to suggest that it is a special case of a partial function (specifically, a total function with domain \"X\" is a special case of a partial function over \"X\"). The adjective will typically be used for clarity in contexts where partial functions are common, for example in computability theory.\n\nThe set of all partial functions from a set \"X\" to a set \"Y\", denoted by , is the union of all total functions defined on subsets of \"X\" with same codomain \"Y\":\nthe latter also written as formula_5. In finite case, its cardinality is\nbecause any partial function can be extended to a total function by any fixed value \"c\" not contained in \"Y\", so that the codomain is }, an operation which is injective (unique and invertible by restriction).\n\nThe first diagram above represents a partial function that is \"not\" a total function since the element 1 in the left-hand set is not associated with anything in the right-hand set. Whereas, the second diagram represents a total function since every element on the left-hand set is associated with exactly one element in the right hand set.\n\nConsider the natural logarithm function mapping the real numbers to themselves. The logarithm of a non-positive real is not a real number, so the natural logarithm function doesn't associate any real number in the codomain with any non-positive real number in the domain. Therefore, the natural logarithm function is not a total function when viewed as a function from the reals to themselves, but it is a partial function. If the domain is restricted to only include the positive reals (that is, if the natural logarithm function is viewed as a function from the positive reals to the reals), then the natural logarithm is a total function.\n\nSubtraction of natural numbers (non-negative integers) can be viewed as a partial function:\n\nIt is defined only when formula_10.\n\nIn denotational semantics a partial function is considered as returning the bottom element when it is undefined.\n\nIn computer science a partial function corresponds to a subroutine that raises an exception or loops forever. The IEEE floating point standard defines a not-a-number value which is returned when a floating point operation is undefined and exceptions are suppressed, e.g. when the square root of a negative number is requested.\n\nIn a programming language where function parameters are statically typed, a function may be defined as a partial function because the language's type system cannot express the exact domain of the function, so the programmer instead gives it the smallest domain which is expressible as a type and contains the true domain.\n\nIn category theory, when considering the operation of morphism composition in concrete categories, the composition operation formula_11 is a total function if and only if formula_12 has one element. The reason for this is that two morphisms formula_13 and formula_14 can only be composed as formula_15 if formula_16, that is, the codomain of formula_17 must equal the domain of formula_18.\n\nThe category of sets and partial functions is equivalent to but not isomorphic with the category of pointed sets and point-preserving maps. One textbook notes that \"This formal completion of sets and partial maps by adding “improper,” “infinite” elements was reinvented many times, in particular, in topology (one-point compactification) and in theoretical computer science.\"\n\nThe category of sets and partial bijections is equivalent to its dual. It is the prototypical inverse category.\n\nPartial algebra generalizes the notion of universal algebra to partial operations. An example would be a field, in which the multiplicative inversion is the only proper partial operation (because division by zero is not defined).\n\nThe set of all partial functions (partial transformations) on a given base set, \"X\", forms a regular semigroup called the semigroup of all partial transformations (or the partial transformation semigroup on \"X\"), typically denoted by formula_19. The set of all partial bijections on \"X\" forms the symmetric inverse semigroup.\n\nCharts in the atlases which specify the structure of manifolds and fiber bundles are partial functions. In the case of manifolds, the domain is the point set of the manifold. In the case of fiber bundles, the domain is the total space of the fiber bundle. In these applications, the most important construction is the transition map, which is the composite of one chart with the inverse of another. The initial classification of manifolds and fiber bundles is largely expressed in terms of constraints on these transition maps.\n\nThe reason for the use of partial functions instead of total functions is to permit general global topologies to be represented by stitching together local patches to describe the global structure. The \"patches\" are the domains where the charts are defined.\n\n\n"}
{"id": "23483", "url": "https://en.wikipedia.org/wiki?curid=23483", "title": "Philosophy of perception", "text": "Philosophy of perception\n\nThe philosophy of perception is concerned with the nature of perceptual experience and the status of perceptual data, in particular how they relate to beliefs about, or knowledge of, the world. Any explicit account of perception requires a commitment to one of a variety of ontological or metaphysical views. Philosophers distinguish internalist accounts, which assume that perceptions of objects, and knowledge or beliefs about them, are aspects of an individual's mind, and externalist accounts, which state that they constitute real aspects of the world external to the individual. The position of naïve realism—the 'everyday' impression of physical objects constituting what is perceived—is to some extent contradicted by the occurrence of perceptual illusions and hallucinations and the relativity of perceptual experience as well as certain insights in science. Realist conceptions include phenomenalism and direct and indirect realism. Anti-realist conceptions include idealism and skepticism.\n\nWe may categorize perception as \"internal\" or \"external\".\n\nThe philosophy of perception is mainly concerned with exteroception.\n\nAn object at some distance from an observer will reflect light in all directions, some of which will fall upon the corneae of the eyes, where it will be focussed upon each retina, forming an image. The disparity between the electrical output of these two slightly different images is resolved either at the level of the lateral geniculate nucleus or in a part of the visual cortex called 'V1'. The resolved data is further processed in the visual cortex where some areas have specialised functions, for instance area V5 is involved in the modelling of motion and V4 in adding colour. The resulting single image that subjects report as their experience is called a 'percept'. Studies involving rapidly changing scenes show the percept derives from numerous processes that involve time delays. Recent fMRI studies show that dreams, imaginings and perceptions of things such as faces are accompanied by activity in many of the same areas of brain as are involved with physical sight. Imagery that originates from the senses and internally generated imagery may have a shared ontology at higher levels of cortical processing.\n\nSound is analyzed in term of pressure waves sensed by the cochlea in the ear. Data from the eyes and ears is combined to form a 'bound' percept. The problem of how this is produced, known as the binding problem.\n\nPerception is analyzed as a cognitive process in which information processing is used to transfer information into the mind where it is related to other information. Some psychologists propose that this processing gives rise to particular mental states (cognitivism) whilst others envisage a direct path back into the external world in the form of action (radical behaviourism). Behaviourists such as John B. Watson and B.F. Skinner have proposed that perception acts largely as a process between a stimulus and a response but have noted that Gilbert Ryle's \"ghost in the machine of the brain\" still seems to exist. \"The objection to inner states is not that they do not exist, but that they are not relevant in a functional analysis\". This view, in which experience is thought to be an incidental by-product of information processing, is known as epiphenomenalism.\n\nContrary to the behaviouralist approach to understanding the elements of cognitive processes, gestalt psychology sought to understand their organization as a whole, studying perception as a process of figure and ground.\n\nImportant philosophical problems derive from the epistemology of perception—how we can gain knowledge via perception—such as the question of the nature of qualia. Within the biological study of perception naive realism is unusable. However, outside biology modified forms of naive realism are defended. Thomas Reid, the eighteenth-century founder of the Scottish School of Common Sense, formulated the idea that sensation was composed of a set of data transfers but also declared that there is still a direct connection between perception and the world. This idea, called direct realism, has again become popular in recent years with the rise of postmodernism.\n\nThe succession of data transfers involved in perception suggests that sense data are somehow available to a perceiving subject that is the substrate of the percept. Indirect realism, the view held by John Locke and Nicolas Malebranche, proposes that we can only be aware of mental representations of objects. however this may imply an infinite regress (a perceiver within a perceiver within a perceiver...), though a finite regress is perfectly possible. It also assumes that perception is entirely due to data transfer and information processing, an argument that can be avoided by proposing that the percept does not depend wholly upon the transfer and rearrangement of data. This still involves basic ontological issues of the sort raised by Leibniz Locke, Hume, Whitehead and others, which remain outstanding particularly in relation to the binding problem, the question of how different perceptions (e.g. color and contour in vision) are \"bound\" to the same object when they are processed by separate areas of the brain.\n\nIndirect realism (representational views) provides an account of issues such as perceptual contents, qualia, dreams, imaginings, hallucinations, illusions, the resolution of binocular rivalry, the resolution of multistable perception, the modelling of motion that allows us to watch TV, the sensations that result from direct brain stimulation, the update of the mental image by saccades of the eyes and the referral of events backwards in time. Direct realists must either argue that these experiences do not occur or else refuse to define them as perceptions.\n\nIdealism holds that reality is limited to mental qualities while skepticism challenges our ability to know anything outside our minds. One of the most influential proponents of idealism was George Berkeley who maintained that everything was mind or dependent upon mind. Berkeley's idealism has two main strands, phenomenalism in which physical events are viewed as a special kind of mental event and subjective idealism. David Hume is probably the most influential proponent of skepticism.\n\nA fourth theory of perception in opposition to naive realism, enactivism, attempts to find a middle path between direct realist and indirect realist theories, positing that cognition arises as a result of the dynamic interplay between an organism's sensory-motor capabilities and its environment. Instead of seeing perception as a passive process determined entirely by the features of an independently existing world, enactivism suggests that organism and environment are structurally coupled and co-determining. The theory was first formalized by Francisco Varela, Evan Thompson, and Eleanor Rosch in \"The Embodied Mind\".\n\nAn aspect of perception that is common to both realists and anti-realists is the idea of mental or perceptual space. David Hume concluded that things appear extended because they have attributes of colour and solidity. A popular modern philosophical view is that the brain cannot contain images so our sense of space must be due to the actual space occupied by physical things. However, as René Descartes noticed, perceptual space has a projective geometry, things within it appear as if they are viewed from a point. The phenomenon of perspective was closely studied by artists and architects in the Renaissance, who relied mainly on the 11th century polymath, Alhazen (Ibn al-Haytham), who affirmed the visibility of perceptual space in geometric structuring projections. Mathematicians now know of many types of projective geometry such as complex Minkowski space that might describe the layout of things in perception (see Peters (2000)) and it has also emerged that parts of the brain contain patterns of electrical activity that correspond closely to the layout of the retinal image (this is known as retinotopy). How or whether these become conscious experience is still unknown (see McGinn (1995)).\n\n\n"}
{"id": "25916521", "url": "https://en.wikipedia.org/wiki?curid=25916521", "title": "Plasma (physics)", "text": "Plasma (physics)\n\nPlasma () is one of the four fundamental states of matter, and was first described by chemist Irving Langmuir in the 1920s. Plasma can be artificially generated by heating or subjecting a neutral gas to a strong electromagnetic field to the point where an ionised gaseous substance becomes increasingly electrically conductive, and long-range electromagnetic fields dominate the behaviour of the matter.\n\nPlasma and ionised gases have properties and display behaviours unlike those of the other states, and the transition between them is mostly a matter of nomenclature and subject to interpretation. Based on the surrounding environmental temperature and density, partially ionised or fully ionised forms of plasma may be produced. Neon signs and lightning are examples of partially ionised plasma. The Earth's ionosphere is a plasma and the magnetosphere contains plasma in the Earth's surrounding space environment. The interior of the Sun is an example of fully ionised plasma, along with the solar corona and stars.\n\nPositive charges in ions are achieved by stripping away electrons orbiting the atomic nuclei, where the total number of electrons removed is related to either increasing temperature or the local density of other ionised matter. This also can be accompanied by the dissociation of molecular bonds, though this process is distinctly different from chemical processes of ion interactions in liquids or the behaviour of shared ions in metals. The response of plasma to electromagnetic fields is used in many modern technological devices, such as plasma televisions or plasma etching.\n\nPlasma may be the most abundant form of ordinary matter in the universe, although this hypothesis is currently tentative based on the existence and unknown properties of dark matter. Plasma is mostly associated with stars, extending to the rarefied intracluster medium and possibly the intergalactic regions.\n\nThe word \"plasma\" comes or 'jelly', and describes the behaviour of the Ionised atomic nuclei and the electrons within the surrounding region of the plasma. Very simply, each of these nuclei are suspended in a movable sea of electrons. Plasma was first identified in a Crookes tube, and so described by Sir William Crookes in 1879 (he called it \"radiant matter\"). The nature of this \"cathode ray\" matter was subsequently identified by British physicist Sir J.J. Thomson in 1897. \n\nThe term \"plasma\" was coined by Irving Langmuir in 1928. Lewi Tonks and Harold Mott-Smith, both of whom worked with Irving Langmuir in the 1920s, recall that Langmuir first used the word \"plasma\" in analogy with blood. Mott-Smith recalls, in particular, that the transport of electrons from thermionic filaments reminded Langmuir of \"the way blood plasma carries red and white corpuscles and germs.\" \n\nLangmuir described the plasma he observed as follows:\n\nPlasma is a state of matter in which an ionised gaseous substance becomes highly electrically conductive to the point that long-range electric and magnetic fields dominate the behaviour of the matter. The plasma state can be contrasted with the other states: solid, liquid, and gas.\n\nPlasma is an electrically neutral medium of unbound positive and negative particles (i.e. the overall charge of a plasma is roughly zero). Although these particles are unbound, they are not \"free\" in the sense of not experiencing forces. Moving charged particles generate an electric current within a magnetic field, and any movement of a charged plasma particle affects and is affected by the fields created by the other charges. In turn this governs collective behaviour with many degrees of variation. Three factors define a plasma:\n\n\nPlasma temperature is commonly measured in kelvins or electronvolts and is, informally, a measure of the thermal kinetic energy per particle. High temperatures are usually needed to sustain ionisation, which is a defining feature of a plasma. The degree of plasma ionisation is determined by the electron temperature relative to the ionization energy (and more weakly by the density), in a relationship called the Saha equation. At low temperatures, ions and electrons tend to recombine into bound states—atoms—and the plasma will eventually become a gas.\n\nIn most cases the electrons are close enough to thermal equilibrium that their temperature is relatively well-defined, even when there is a significant deviation from a Maxwellian energy distribution function, for example, due to UV radiation, energetic particles, or strong electric fields. Because of the large difference in mass, the electrons come to thermodynamic equilibrium amongst themselves much faster than they come into equilibrium with the ions or neutral atoms. For this reason, the ion temperature may be very different from (usually lower than) the electron temperature. This is especially common in weakly ionised technological plasmas, where the ions are often near the ambient temperature.\n\nFor plasma to exist, ionisation is necessary. The term \"plasma density\" by itself usually refers to the \"electron density\", that is, the number of free electrons per unit volume. The degree of ionisation of a plasma is the proportion of atoms that have lost or gained electrons, and is controlled by the electron and ion temperatures and electron-ion vs electron-neutral collision frequencies. The degree of ionisation, formula_1, is defined as formula_2, where formula_3 is the number density of ions and formula_4 is the number density of neutral atoms. The \"electron density\" is related to this by the average charge state formula_5 of the ions through formula_6, where formula_7 is the number density of electrons.\n\nIn a plasma, the electron-ion collision frequency formula_8 is much greater than the electron-neutral collision frequency formula_9. Therefore, with a weak degree of ionization formula_1, the electron-ion collision frequency can equal the electron-neutral collision frequency: formula_11 is the limit separating a plasma from being partially or fully ionized.\n\n\n\nMost of \"technological\" (engineered) plasmas are weakly ionized gases.\n\nBased on the relative temperatures of the electrons, ions and neutrals, plasmas are classified as \"thermal\" or \"non-thermal\" (also referred to as \"cold plasmas\"). \n\n\n\nA particular and unusual case of \"inverse\" nonthermal plasma is the very high temperature plasma produced by the Z machine, where ions are much hotter than electrons.\n\nSince plasmas are very good electrical conductors, electric potentials play an important role. The average potential in the space between charged particles, independent of how it can be measured, is called the \"plasma potential\", or the \"space potential\". If an electrode is inserted into a plasma, its potential will generally lie considerably below the plasma potential due to what is termed a Debye sheath. The good electrical conductivity of plasmas makes their electric fields very small. This results in the important concept of \"quasineutrality\", which says the density of negative charges is approximately equal to the density of positive charges over large volumes of the plasma (formula_6), but on the scale of the Debye length there can be charge imbalance. In the special case that \"double layers\" are formed, the charge separation can extend some tens of Debye lengths.\n\nThe magnitude of the potentials and electric fields must be determined by means other than simply finding the net charge density. A common example is to assume that the electrons satisfy the Boltzmann relation:\n\nDifferentiating this relation provides a means to calculate the electric field from the density:\n\nIt is possible to produce a plasma that is not quasineutral. An electron beam, for example, has only negative charges. The density of a non-neutral plasma must generally be very low, or it must be very small, otherwise, it will be dissipated by the repulsive electrostatic force.\n\nIn astrophysical plasmas, Debye screening prevents electric fields from directly affecting the plasma over large distances, i.e., greater than the Debye length. However, the existence of charged particles causes the plasma to generate, and be affected by, magnetic fields. This can and does cause extremely complex behaviour, such as the generation of plasma double layers, an object that separates charge over a few tens of Debye lengths. The dynamics of plasmas interacting with external and self-generated magnetic fields are studied in the academic discipline of magnetohydrodynamics.\n\nPlasma with a magnetic field strong enough to influence the motion of the charged particles is said to be magnetized. A common quantitative criterion is that a particle on average completes at least one gyration around the magnetic field before making a collision, i.e., formula_18, where formula_19 is the \"electron gyrofrequency\" and formula_20 is the \"electron collision rate\". It is often the case that the electrons are magnetized while the ions are not. Magnetized plasmas are \"anisotropic\", meaning that their properties in the direction parallel to the magnetic field are different from those perpendicular to it. While electric fields in plasmas are usually small due to the high conductivity, the electric field associated with a plasma moving in a magnetic field is given by formula_21 (where formula_22 is the electric field, formula_23 is the velocity, and formula_24 is the magnetic field), and is not affected by Debye shielding.\n\nPlasma is often called the \"fourth state of matter\" after solid, liquids and gases, despite plasma typically being an ionised gas. It is distinct from these and other lower-energy states of matter. Although it is closely related to the gas phase in that it also has no definite form or volume, it differs in a number of ways, including the following:\n\nPlasmas are by far the most common phase of ordinary matter in the universe, both by mass and by volume.\n\nAbove the Earth's surface, the ionosphere is a plasma, and the magnetosphere contains plasma. Within our Solar System, interplanetary space is filled with the plasma expelled via the solar wind, extending from the Sun's surface out to the heliopause. Furthermore, all the distant stars, and much of interstellar space or intergalactic space is also likely filled with plasma, albeit at very low densities. Astrophysical plasmas are also observed in Accretion disks around stars or compact objects like white dwarfs, neutron stars, or black holes in close binary star systems. Plasma is associated with ejection of material in astrophysical jets, which have been observed with accreting black holes or in active galaxies like M87's jet that possibly extends out to 5,000 light-years.\n\nPlasmas can appear in nature in various forms and locations, which can be usefully broadly summarised in the following Table:\n\nAlthough the underlying equations governing plasmas are relatively simple, plasma behaviour is extraordinarily varied and subtle: the emergence of unexpected behaviour from a simple model is a typical feature of a complex system. Such systems lie in some sense on the boundary between ordered and disordered behaviour and cannot typically be described either by simple, smooth, mathematical functions, or by pure randomness. The spontaneous formation of interesting spatial features on a wide range of length scales is one manifestation of plasma complexity. The features are interesting, for example, because they are very sharp, spatially intermittent (the distance between features is much larger than the features themselves), or have a fractal form. Many of these features were first studied in the laboratory, and have subsequently been recognized throughout the universe. Examples of complexity and complex structures in plasmas include:\n\nStriations or string-like structures, also known as Birkeland currents, are seen in many plasmas, like the plasma ball, the aurora, lightning, electric arcs, solar flares, and supernova remnants. They are sometimes associated with larger current densities, and the interaction with the magnetic field can form a magnetic rope structure. High power microwave breakdown at atmospheric pressure also leads to the formation of filamentary structures. (See also Plasma pinch)\n\nFilamentation also refers to the self-focusing of a high power laser pulse. At high powers, the nonlinear part of the index of refraction becomes important and causes a higher index of refraction in the center of the laser beam, where the laser is brighter than at the edges, causing a feedback that focuses the laser even more. The tighter focused laser has a higher peak brightness (irradiance) that forms a plasma. The plasma has an index of refraction lower than one, and causes a defocusing of the laser beam. The interplay of the focusing index of refraction, and the defocusing plasma makes the formation of a long filament of plasma that can be micrometers to kilometers in length. One interesting aspect of the filamentation generated plasma is the relatively low ion density due to defocusing effects of the ionised electrons. (See also Filament propagation)\n\nThe strength and range of the electric force and the good conductivity of plasmas usually ensure that the densities of positive and negative charges in any sizeable region are equal (\"quasineutrality\"). A plasma with a significant excess of charge density, or, in the extreme case, is composed of a single species, is called a non-neutral plasma. In such a plasma, electric fields play a dominant role. Examples are charged particle beams, an electron cloud in a Penning trap and positron plasmas.\n\nA dusty plasma contains tiny charged particles of dust (typically found in space). The dust particles acquire high charges and interact with each other. A plasma that contains larger particles is called grain plasma. Under laboratory conditions, dusty plasmas are also called \"complex plasmas\".\n\nImpermeable plasma is a type of thermal plasma which acts like an impermeable solid with respect to gas or cold plasma and can be physically pushed. Interaction of cold gas and thermal plasma was briefly studied by a group led by Hannes Alfvén in 1960s and 1970s for its possible applications in insulation of fusion plasma from the reactor walls. However, later it was found that the external magnetic fields in this configuration could induce kink instabilities in the plasma and subsequently lead to an unexpectedly high heat loss to the walls.\nIn 2013, a group of materials scientists reported that they have successfully generated stable impermeable plasma with no magnetic confinement using only an ultrahigh-pressure blanket of cold gas. While spectroscopic data on the characteristics of plasma were claimed to be difficult to obtain due to the high pressure, the passive effect of plasma on synthesis of different nanostructures clearly suggested the effective confinement. They also showed that upon maintaining the impermeability for a few tens of seconds, screening of ions at the plasma-gas interface could give rise to a strong secondary mode of heating (known as viscous heating) leading to different kinetics of reactions and formation of complex nanomaterials.\n\nTo completely describe the state of a plasma, all of the\nparticle locations and velocities that describe the electromagnetic field in the plasma region would need to be written down.\nHowever, it is generally not practical or necessary to keep track of all the particles in a plasma.\nTherefore, plasma physicists commonly use less detailed descriptions, of which\nthere are two main types:\n\nFluid models describe plasmas in terms of smoothed quantities, like density and averaged velocity around each position (see Plasma parameters). One simple fluid model, magnetohydrodynamics, treats the plasma as a single fluid governed by a combination of Maxwell's equations and the Navier–Stokes equations. A more general description is the two-fluid plasma picture, where the ions and electrons are described separately. Fluid models are often accurate when collisionality is sufficiently high to keep the plasma velocity distribution close to a Maxwell–Boltzmann distribution. Because fluid models usually describe the plasma in terms of a single flow at a certain temperature at each spatial location, they can neither capture velocity space structures like beams or double layers, nor resolve wave-particle effects.\n\nKinetic models describe the particle velocity distribution function at each point in the plasma and therefore do not need to assume a Maxwell–Boltzmann distribution. A kinetic description is often necessary for collisionless plasmas. There are two common approaches to kinetic description of a plasma. One is based on representing the smoothed distribution function on a grid in velocity and position. The other, known as the particle-in-cell (PIC) technique, includes kinetic information by following the trajectories of a large number of individual particles. Kinetic models are generally more computationally intensive than fluid models. The Vlasov equation may be used to describe the dynamics of a system of charged particles interacting with an electromagnetic field.\nIn magnetized plasmas, a gyrokinetic approach can substantially reduce the computational expense of a fully kinetic simulation.\n\nMost artificial plasmas are generated by the application of electric and/or magnetic fields through a gas. Plasma generated in a laboratory setting and for industrial use can be generally categorized by:\n\nJust like the many uses of plasma, there are several means for its generation, however, one principle is common to all of them: there must be energy input to produce and sustain it. For this case, plasma is generated when an electric current is applied across a dielectric gas or fluid (an electrically non-conducting material) as can be seen in the adjacent image, which shows a discharge tube as a simple example (DC used for simplicity).\n\nThe potential difference and subsequent electric field pull the bound electrons (negative) toward the anode (positive electrode) while the cathode (negative electrode) pulls the nucleus. As the voltage increases, the current stresses the material (by electric polarization) beyond its dielectric limit (termed strength) into a stage of electrical breakdown, marked by an electric spark, where the material transforms from being an insulator into a conductor (as it becomes increasingly ionised). The underlying process is the Townsend avalanche, where collisions between electrons and neutral gas atoms create more ions and electrons (as can be seen in the figure on the right). The first impact of an electron on an atom results in one ion and two electrons. Therefore, the number of charged particles increases rapidly (in the millions) only \"after about 20 successive sets of collisions\", mainly due to a small mean free path (average distance travelled between collisions).\n\nWith ample current density and ionisation, this forms a luminous electric arc (a continuous electric discharge similar to lightning) between the electrodes. Electrical resistance along the continuous electric arc creates heat, which dissociates more gas molecules and ionises the resulting atoms (where degree of ionisation is determined by temperature), and as per the sequence: solid-liquid-gas-plasma, the gas is gradually turned into a thermal plasma. A thermal plasma is in thermal equilibrium, which is to say that the temperature is relatively homogeneous throughout the heavy particles (i.e. atoms, molecules and ions) and electrons. This is so because when thermal plasmas are generated, electrical energy is given to electrons, which, due to their great mobility and large numbers, are able to disperse it rapidly and by elastic collision (without energy loss) to the heavy particles.\n\nBecause of their sizable temperature and density ranges, plasmas find applications in many fields of research, technology and industry. For example, in: industrial and extractive metallurgy, surface treatments such as plasma spraying (coating), etching in microelectronics, metal cutting and welding; as well as in everyday vehicle exhaust cleanup and fluorescent/luminescent lamps, fuel ignition, while even playing a part in supersonic combustion engines for aerospace engineering.\n\n\n\nA world effort was triggered in the 1960s to study magnetohydrodynamic converters in order to bring MHD power conversion to market with commercial power plants of a new kind, converting the kinetic energy of a high velocity plasma into electricity with no moving parts at a high efficiency. Research was also conducted in the field of supersonic and hypersonic aerodynamics to study plasma interaction with magnetic fields to eventually achieve passive and even active flow control around vehicles or projectiles, in order to soften and mitigate shock waves, lower thermal transfer and reduce drag.\n\nSuch ionized gases used in \"plasma technology\" (\"technological\" or \"engineered\" plasmas) are usually \"weakly ionized gases\" in the sense that only a tiny fraction of the gas molecules are ionized. These kinds of weakly ionized gases are also nonthermal \"cold\" plasmas. In the presence of magnetics fields, the study of such magnetized nonthermal weakly ionized gases involves resistive magnetohydrodynamics with low magnetic Reynolds number, a challenging field of plasma physics where calculations require dyadic tensors in a 7-dimensional phase space. When used in combination with a high Hall parameter, a critical value triggers the problematic electrothermal instability which limited these technological developments.\n\nPlasmas are the object of study of the academic field of \"plasma science\" or \"plasma physics\", including sub-disciplines such as space plasma physics. It currently involves the following fields of active research and features across many journals, whose interest includes:\n\n\n\n\n"}
{"id": "253786", "url": "https://en.wikipedia.org/wiki?curid=253786", "title": "Price point", "text": "Price point\n\nPrice points are prices at which demand for a given product is supposed to stay relatively high.\n\nIntroductory microeconomics depicts a demand curve as downward-sloping to the right and either linear or gently convex to the origin. The downwards slope generally holds, but the model of the curve is only piecewise true, as price surveys indicate that demand for a product is not a linear function of its price and not even a smooth function. Demand curves resemble a series of waves rather than a straight line. \n\nThe diagram shows price points at the points labeled A, B, and C. When a vendor increases a price beyond a price point (say to a price slightly above \"price point B\"), sales volume decreases by an amount more than proportional to the price increase. This decrease in quantity-demanded more than offsets the additional revenue from the increased unit-price. As a result, total revenue (price multiplied by quantity-demanded) decreases when a firm raises its price beyond a price point. Technically, the price elasticity of demand is low (inelastic) at a price lower than the price point (steep section of the demand curve), and high (elastic) at a price higher than a price point (gently sloping part of the demand curve). Firms commonly set prices at existing price-points as a marketing strategy.\n\nThere are three main reasons for price points to appear:\n\n\nIn relation to customary price points, oligopolies can also generate price points. Such price points do not necessarily result from collusion, but as an emergent property of oligopolies: when all firms sell at the same price, any firm which attempts to raise its selling price will experience a decrease in sales and revenues (preventing firms from raising prices unilaterally); on the other hand, any firm in an oligopoly which lowers its prices will mostly likely be matched by competitors, resulting in small increases in sales but decreases in revenues (for all the firms in that market). This effect can potentially produce a kinked demand-curve where the kink lies at the point of the current price-level in the market.\nThese results depend on the elasticity of the demand curve and on the properties of each market.\n\n\n"}
{"id": "7247062", "url": "https://en.wikipedia.org/wiki?curid=7247062", "title": "Psychical nomadism", "text": "Psychical nomadism\n\nPsychical nomadism is a philosophical term that refers to the practice of taking as one needs from any moral, religious, political, ethical, or whatever system, and leaving behind the parts of that system found to be unappealing. \n\nIt is one of the main characteristics of the Temporary Autonomous Zone by Hakim Bey, but the notion was previously discussed by Gilles Deleuze and Félix Guattari in \"Nomadology and the War Machine\", by Jean-François Lyotard in \"Driftworks\" and by various authors in the \"Oasis\" issue of \"Semiotext(e)\".\n\nPsychic nomadism facilitates the construction of ad hoc reference frames in which to situate the temporary actions required by TAZ.\n\nBey in his essay explains why he chose the name:\n\n\"\"We use the term \"psychic nomadism\" here rather than \"urban nomadism,\" \"nomadology,\" \"driftwork,\" etc., simply in order to garner all these concepts into a single loose complex, to be studied in light of the coming- into-being of the TAZ.\"\"\nHe states that there is a paradox where our modern society’s false unity blurs all cultural diversity and any place is as good as another.\n\nBey describes psychic nomadism's tactical qualities along with Deleuze and Guattari's sensibilities about the war machine:\n\n\"“These nomads practice the razzia, they are corsairs, they are viruses; they have both need and desire for TAZs, camps of black tents under the desert stars, interzones, hidden fortified oases along secret caravan routes, 'liberated' bits of jungle and bad-land, no-go areas, black markets, and underground bazaars.”\"\nBey also discusses these nomads in terms of the Internet and cyberspace. His poetry foreshadows ideas that appear in CAE’s \"The Electronic Disturbance\" and later in electronic civil disobedience. With the words “cyberspace” and “hallucination” used interchangeably, we can see William Gibson's cyberpunk novel \"Neuromancer\" (1984) being combined with Deleuze and Guattari.\n\"“These nomads chart their course by strange stars, which might be luminous clusters of data in cyberspace, or perhaps hallucinations. Lay down a map of the land; over that, set a map of political change; over that, a map of the Net, especially the counter-Net with its emphasis on clandestine information-flow and logisitics - and finally, over all, the 1:1 map of the creative imagination, aesthetics, values. The resultant grid comes to life, animated by unexpected eddies and surges of energy, coagulations of light, secret tunnels, and surprises.”\"\n\n\n"}
{"id": "7980471", "url": "https://en.wikipedia.org/wiki?curid=7980471", "title": "Rape", "text": "Rape\n\nRape is a type of sexual assault usually involving sexual intercourse or other forms of sexual penetration carried out against a person without that person's consent. The act may be carried out by physical force, coercion, abuse of authority, or against a person who is incapable of giving valid consent, such as one who is unconscious, incapacitated, has an intellectual disability or is below the legal age of consent. The term \"rape\" is sometimes used interchangeably with the term \"sexual assault.\"\n\nThe rate of reporting, prosecuting and convicting for rape varies between jurisdictions. Internationally, the incidence of rapes recorded by the police during 2008 ranged, per 100,000 people, from 0.2 in Azerbaijan to 92.9 in Botswana with 6.3 in Lithuania as the median. Worldwide, rape is primarily committed by males. Rape by strangers is usually less common than rape by people the victim knows, and male-on-male and female-on-female prison rapes are common and may be the least reported forms of rape.\n\nWidespread and systematic rape (e.g., war rape) and sexual slavery can occur during international conflict. These practices are crimes against humanity and war crimes. Rape is also recognized as an element of the crime of genocide when committed with the intent to destroy, in whole or in part, a targeted ethnic group.\n\nPeople who have been raped can be traumatized and develop posttraumatic stress disorder. Serious injuries can result along with the risk of pregnancy and sexually transmitted infections. A person may face violence or threats from the rapist, and, in some cultures, from the victim's family and relatives.\n\nThe term \"rape\" originates from the Latin \"rapere\" (supine stem \"raptum\"), \"to snatch, to grab, to carry off\". Since the 14th century, the term has come to mean \"to seize and take away by force\". In Roman law, the carrying off of a woman by force, with or without intercourse, constituted \"raptus\". In Medieval English law the same term could refer to either kidnapping or rape in the modern sense of \"sexual violation\". The original meaning of \"carry off by force\" is still found in some phrases, such as \"rape and pillage\", or in titles, such as the stories of the Rape of the Sabine Women and The Rape of Europa or the poem \"The Rape of the Lock\", which is about the theft of a lock of hair.\n\nRape is defined in most jurisdictions as sexual intercourse, or other forms of sexual penetration, committed by a perpetrator against a victim without their consent. The definition of rape is inconsistent between governmental health organizations, law enforcement, health providers, and legal professions. It has varied historically and culturally. Originally, \"rape\" had no sexual connotation and is still used in other contexts in English. In Roman law, it or \"raptus\" was classified as a form of \"crimen vis\", \"crime of assault\". \"Raptus\" described the abduction of a woman against the will of the man under whose authority she lived, and sexual intercourse was not a necessary element. Other definitions of rape have changed over time.\n\nUntil 2012, the Federal Bureau of Investigation (FBI) considered rape a crime solely committed by men against women. In 2012, they changed their definition from \"The carnal knowledge of a female forcibly and against her will\" to \"The penetration, no matter how slight, of the vagina or anus with any body part or object, or oral penetration by a sex organ of another person, without the consent of the victim.\" The previous definition, which had remained unchanged since 1927, was considered outdated and narrow. The updated definition includes recognizing any gender of victim and perpetrator and that rape with an object can be as traumatic as penile/vaginal rape. The bureau further describes instances when the victim is unable to give consent because of mental or physical incapacity. It recognizes that a victim can be incapacitated by drugs and alcohol and unable to give valid consent. The definition does not change federal or state criminal codes or impact charging and prosecution on the federal, state or local level; it rather means that rape will be more accurately reported nationwide.\n\nHealth organizations and agencies have also expanded rape beyond traditional definitions. The World Health Organization (WHO) defines rape as a form of sexual assault, while the Centers for Disease Control and Prevention (CDC) includes rape in their definition of sexual assault; they term rape a form of sexual violence. The CDC lists other acts of coercive, non-consensual sexual activity that may or may not include rape, including drug-facilitated sexual assault, acts in which a victim is made to penetrate a perpetrator or someone else, intoxication where the victim is unable to consent (due to incapacitation or being unconscious), non-physically forced penetration which occurs after a person is pressured verbally (by intimidation or misuse of authority to force to consent), or completed or attempted forced penetration of a victim via unwanted physical force (including using a weapon or threatening to use a weapon).\n\nSome countries or jurisdictions differentiate between rape and sexual assault by defining rape as involving penile penetration of the vagina, or solely penetration involving the penis, while other types of non-consensual sexual activity are called sexual assault. Scotland, for example, emphasizes penile penetration, requiring that the sexual assault must have been committed by use of a penis to qualify as rape. The 1998 International Criminal Tribunal for Rwanda defines rape as \"a physical invasion of a sexual nature committed on a person under circumstances which are coercive\". In other cases, the term \"rape\" has been phased out of legal use in favor of terms such as \"sexual assault\" or \"criminal sexual conduct\".\n\nVictims of rape or sexual assault come from a wide range of genders, ages, sexual orientations, ethnicitities, geographical locations, cultures and degrees of impairment or disability. Incidences of rape are classified into a number of categories, and they may describe the relationship of the perpetrator to the victim and the context of the sexual assault. These include date rape, gang rape, marital rape, incestual rape, child sexual abuse, prison rape, acquaintance rape, war rape and statutory rape. Forced sexual activity can be committed over a long period of time with little to no physical injury.\n\nLack of consent is key to the definition of rape. Consent is affirmative \"informed approval, indicating a freely given agreement\" to sexual activity. It is not necessarily expressed verbally, and may instead be overtly implied from actions, but the absence of objection does not constitute consent. Lack of consent may result from either forcible compulsion by the perpetrator or an inability to consent on the part of the victim (such as people who are asleep, intoxicated or otherwise mentally compromised). Sexual intercourse with a person below the age of consent, i.e., the age at which legal competence is established, is referred to as statutory rape.\n\nDuress is the situation when the person is threatened by force or violence, and may result in the absence of an objection to sexual activity. This can lead to the presumption of consent. Duress may be actual or threatened force or violence against the victim or someone close to the victim. Even blackmail may constitute duress. Abuse of power may constitute duress. For instance, in the Philippines, a man commits rape if he engages in sexual intercourse with a woman \"By means of fraudulent machination or grave abuse of authority\". The International Criminal Tribunal for Rwanda in its landmark 1998 judgment used a definition of rape which did not use the word 'consent': \"a physical invasion of a sexual nature committed on a person \"under circumstances which are coercive\".\"\n\nMarital rape, or spousal rape, is non-consensual sex in which the perpetrator is the victim's spouse. It is a form of partner rape, domestic violence, and sexual abuse. Once widely accepted or ignored by law, spousal rape is now denounced by international conventions and is increasingly criminalized. Still, in many countries, spousal rape either remains legal, or is illegal but widely tolerated and accepted as a husband's prerogative. In 2006, the UN Secretary-General's \"In-depth study on all forms of violence against women\" stated that (pg 113): \"Marital rape may be prosecuted in at least 104 states. Of these, 32 have made marital rape a specific criminal offence, while the remaining 74 do not exempt marital rape from general rape provisions. Marital rape is not a prosecutable offense in at least 53 States. Four States criminalize marital rape only when the spouses are judicially separated. Four States are considering legislation that would allow marital rape to be prosecuted.\" Since 2006, several other states have outlawed marital rape (for example Thailand in 2007). In the US, the criminalization of marital rape started in the mid-1970s and in 1993 North Carolina became the last state to make marital rape illegal. In many countries, it is not clear if marital rape may or may not be prosecuted under ordinary rape laws. In the absence of a spousal rape law, it may be possible to bring prosecution for acts of forced sexual intercourse inside marriage by prosecuting, through the use of other criminal offenses (such as assault based offenses), the acts of violence or criminal threat that were used to obtain submission.\n\nConsent may be complicated by law, language, context, culture and sexual orientation. Studies have shown that men consistently perceive women's actions as more sexual than they intend. In addition, verbalized 'no' to sex may be interpreted as 'keep trying', or even 'yes' by offenders. Some may believe that when injuries are not visible, the woman must have consented. If a man solicits sex from another man, the pursuer may be regarded as virile.\n\nThe WHO states that the principal factors that lead to the perpetration of sexual violence against women, including rape, are:\n\n\nNo single facet explains the motivation for rape; the underlying motives of rapists can be multi-faceted. Several factors have been proposed: anger, power, sadism, sexual gratification, or evolutionary proclivities. However, some factors have significant causal evidence supporting them. American clinical psychologist David Lisak, co-author of a 2002 study of undetected rapists, says that compared with non-rapists, both undetected and convicted rapists are measurably more angry at women and more motivated by a desire to dominate and control them, are more impulsive, disinhibited, anti-social, hypermasculine, and less empathic.\n\nSexual aggression is often considered a masculine identity characteristic of manhood in some male groups and is significantly correlated to the desire to be held higher in esteem among male peers. Sexually aggressive behavior among young men has been correlated with gang or group membership as well as having other delinquent peers. Gang rape is often perceived by male perpetrators as a justified method of discouraging or punishing what they consider as immoral behavior among women for example wearing short skirts or visiting bars. In some areas in Papua New Guinea, women can actually be punished by public gang rape usually through permission by elders.\n\nOne metric used by the WHO to determine the severity of global rates of coercive, forced sexual activity was the question \"Have you ever\nbeen forced to have sexual intercourse against your will?\" Asking this question produced higher positive response rates than being asked, whether they had ever been abused or raped.\n\nThe WHO report describes the consequences of sexual abuse:\n \n\nFrequently, victims may not recognize what happened to them was rape. Some may remain in denial for years afterwards. Confusion over whether or not their experience constitutes rape is typical, especially for victims of psychologically coerced rape. Women may not identify their victimization as rape for many reasons such as feelings of shame, embarrassment, non-uniform legal definitions, reluctance to define the friend/partner as a rapist, or because they have internalized victim-blaming attitudes. The public perceives these behaviors as 'counterintuitive' and therefore, as evidence of a dishonest woman.\n\nDuring the assault, a person will respond with fight, flight, freeze, friend (sometimes called fawn), or flop. Victims may react in ways they did not anticipate. After the rape, they may be uncomfortable/frustrated with and not understand their reactions. Most victims respond by 'freezing up' or becoming compliant and cooperative during the rape. These are common survival responses of all mammals. This can cause confusion for others and the person assaulted. An assumption is that someone being raped would call for help or struggle. A struggle would result in torn clothes or injuries.\n\nDissociation can occur during the assault. Memories may be fragmented especially immediately afterwards. They may consolidate with time and sleep. A man or boy who is raped may be stimulated and even ejaculate during the experience of the rape. A woman or girl may orgasm during a sexual assault. This may become a source of shame and confusion for those assaulted along with those who were around them.\n\nTrauma symptoms may not show until years after the sexual assault occurred. Immediately following a rape, the survivor may react outwardly in a wide range of ways, from expressive to closed down; common emotions include distress, anxiety, shame, revulsion, helplessness, and guilt. Denial is not uncommon.\n\nIn the weeks following the rape, the survivor may develop symptoms of post traumatic stress syndrome and may develop wide array of psychosomatic complaints. PTSD symptoms include re-experiencing of the rape, avoiding things associated with the rape, numbness, and increased anxiety and startle response. The likelihood of sustained severe symptoms is higher if the rapist confined or restrained the person, if the person being raped believed the rapist would kill them, the person who was raped was very young or very old, and if the rapist was someone they knew. The likelihood of sustained severe symptoms is also higher if people around the survivor ignore (or are ignorant of) the rape or blame the rape survivor.\n\nMost people recover from rape in three to four months, but many have persistent PTSD that may manifest in anxiety, depression, substance abuse, irritability, anger, flashbacks, or nightmares. In addition, rape survivors may have long term generalised anxiety disorder, may develop one or more specific phobias, major depressive disorder, and may experience difficulties with resuming their social life, and with sexual functioning. People who have been raped are at higher risk of suicide.\n\nMen experience similar psychological effects of being raped, but they are less likely to seek counseling.\n\nAnother effect of rape and sexual assault is the stress created in those who study rape or counsel the survivors. This is called vicarious traumatization.\n\nThe presence or absence of physical injury may be used to determine whether a rape has occurred. Those who have experienced sexual assault yet have no physical trauma may be less inclined to report to the authorities or to seek health care.\n\nWhile penetrative rape generally does not involve the use of a condom, in some cases a condom is used. This significantly reduces the likelihood of pregnancy and disease transmission, both to the victim and to the rapist. Rationales for condom use include: avoiding contracting infections or diseases (particularly HIV), especially in cases of rape of sex workers or in gang rape (to avoid contracting infections or diseases from fellow rapists); eliminating evidence, making prosecution more difficult (and giving a sense of invulnerability); giving the appearance of consent (in cases of acquaintance rape); and thrill from planning and the use of the condom as an added prop. Concern for the victim is generally not considered a factor.\n\nThose who have been raped have relatively more reproductive tract infections than those not been raped. The HIV virus can be transmitted through rape. Acquiring AIDS through rape puts people risk of suffering psychological problems. Acquiring HIV through rape may lead to the in behaviors that create risk of injecting drugs. Acquiring sexually transmitted infections increases the risk of acquiring HIV.\nThe belief that having sex with a virgin can cure HIV/AIDS exists in parts of Africa. This leads to the rape of girls and women. The claim that the myth drives either HIV infection or child sexual abuse in South Africa is disputed by researchers Rachel Jewkes and Helen Epstein.\n\nSociety's treatment of victims has the potential to exacerbate their trauma. People who have been raped or sexually assaulted are sometimes blamed and considered responsible for the crime. This refers to the just world fallacy and rape myth acceptance that certain victim behaviors (such as being intoxicated, flirting or wearing sexually provocative clothing) may encourage rape. In many cases, victims are said to have \"asked for it\" because of not resisting their assault or violating female gender expectations. A global survey of attitudes toward sexual violence by the Global Forum for Health Research shows that victim-blaming concepts are at least partially accepted in many countries. Women who have been raped are sometimes deemed to have behaved improperly. Usually, these are cultures where there is a significant social divide between the freedoms and status afforded to men and women.\n\n\"Rape victims are blamed more when they resist the attack later in the rape encounter rather than earlier (Kopper, 1996), which seems to suggest the stereotype that these women are engaging in token resistance (Malamuth & Brown, 1994; Muehlenhard & Rogers, 1998) or leading the man on because they have gone along with the sexual experience thus far. Finally, rape victims are blamed more when they are raped by an acquaintance or a date rather than by a stranger (e.g., Bell, Kuriloff, & Lottes, 1994; Bridges, 1991; Bridges & McGr ail, 1989; Check & Malamuth, 1983; Kanekar, Shaherwalla, Franco, Kunju, & Pinto, 1991; L'Armand & Pepitone, 1982; Tetreault & Barnett, 1987), which seems to evoke the stereotype that victims really want to have sex because they know their attacker and perhaps even went out on a date with him. The underlying message of this research seems to be that when certain stereotypical elements of rape are in place, rape victims are prone to being blamed.\"\n\nCommentators state: \"individuals may endorse rape myths and at the same time recognize the negative effects of rape.\" A number of gender role stereotypes can play a role in rationalization of rape. These include the idea that power is reserved to men whereas women are meant for sex and objectified, that women want forced sex and to be pushed around, and that male sexual impulses and behaviors are uncontrollable and must be satisfied.\n\nFor females, victim-blaming correlates with fear. Many rape victims blame themselves. Female jurors might look at the woman on the witness stand and believe she had done something to entice the defendant. In Chinese culture, victim blaming often is associated with the crime of rape, as women are expected to resist rape using physical force. Thus, if rape occurs, it is considered to be at least partly the women’s fault and her virtue is called into question.\n\nIn many cultures, those who are raped have a high risk of suffering additional violence or threats of violence after the rape. This can be perpetrated by the rapist, friends, or relatives of the rapist. The intent can be to prevent the victim from reporting the rape. Other reasons for threats against the those assaulted is to punish them for reporting it, or of forcing them to withdraw the complaint. The relatives of the person who has been raped may wish to prevent \"bringing shame\" to the family and may also threaten them. This is especially the case in cultures where female virginity is highly valued and considered mandatory before marriage; in extreme cases, rape victims are killed in honor killings.\n\nIn the US, victims' rights include the right to have a victims advocate preside over every step of the medical/legal exam to ensure sensitivity towards victims, provide emotional support, and minimize the risk of re-traumatization. Victims are to be informed of this immediately by law enforcement or medical service providers. Emergency rooms of many hospitals employ sexual assault nurse/forensic examiners (SAN/FEs) with specific training to care for those who have experienced a rape or sexual assault. They are able to conduct a focused medical-legal exam. If such a trained clinician is not available, the emergency department has a sexual assault protocol that has been established for treatment and the collection of evidence. Staff are also trained to explain the examinations in detail, the documentation and the rights associated with the requirement for informed consent. Emphasis is placed on performing the examinations at a pace that is appropriate for the person, their family, their age, and their level of understanding. Privacy is recommended to prevent self-harm.\n\nMany rapes do not result in serious injury. The first medical response to sexual assault is a complete assessment. This general assessment will prioritize the treatment of injuries by the emergency room staff. Medical personnel involved are trained to assess and treat those assaulted or follow protocols established to ensure privacy and best treatment practices. Informed consent is always required prior to treatment unless the person who was assaulted is unconscious, intoxicated or does not have the mental capacity to give consent. Priorities governing the physical exam are the treatment of serious life-threatening emergencies and then a general and complete assessment. Some physical injuries are readily apparent such as, bites, broken teeth, swelling, bruising, lacerations and scratches. In more violent cases, the victim may need to have gunshot wounds or stab wounds treated. The loss of consciousness is relevant to the medical history. If abrasions are found, immunization against tetanus is offered if 5 years have elapsed since the last immunization.\n\nAfter the general assessment and treatment of serious injuries, further evaluation may include the use of additional diagnostic testing such as x-rays, CT or MRI image studies and blood work. The presence of infection is determined by sampling of body fluids from the mouth, throat, vagina, perineum, and anus.\n\nVictims have the right to refuse any evidence collection. Victims advocates ensure the victims' wishes are respected by hospital staff. After the physical injuries are addressed and treatment has begun, then forensic examination proceeds along with the gathering of evidence that can be used to identify and document the injuries. Such evidence-gathering is only done with the complete consent of the patient or the caregivers of the patient. Photographs of the injuries may be requested by staff. At this point in the treatment, if a victims' advocate had not been requested earlier, experienced social support staff are made available to the patient and family.\n\nIf the patient or the caregivers, (typically parents) agree, the medical team utilizes standardized sampling and testing usually referred to a forensic evidence kit or \"rape kit\". The patient is informed that submitting to the use of the rape kit does not obligate them to file criminal charges against the perpetrator. The patient is discouraged from bathing or showering for the purpose of obtaining samples from their hair. Evidence gathered within the past 72 hours is more likely to be valid. The sooner that samples are obtained after the assault, the more likely that evidence is present in the sample and provide valid results. Once the injuries of the patient have been treated and she or he is stabilized, the sample gathering will begin. Staff will encourage the presence of a rape/sexual assault counselor to provide an advocate and reassurance.\n\nDuring the medical exam, evidence of bodily secretions is assessed. Dried semen that is on clothing and skin can be detected with a fluorescent lamp. Notes will be attached to those items on which semen has been found. These specimens are marked, placed in a paper bag, and be marked for later analysis for the presence of seminal vesicle-specific antigen.\n\nThough technically, medical staff are not part of the legal system, only trained medical personnel can obtain evidence that is admissible during a trial. The procedures have been standardized. Evidence is collected, signed, and locked in a secure place to guarantee that legal evidence procedures are maintained. This is known as the chain of evidence and is a legal term that describes a carefully monitored procedure of evidence collection and preservation. Maintaining the Chain of evidence from the medical examination, testing and tissue sampling from its origin of collection to court allows the results of the sampling to be admitted as evidence. The use of photography is often used for documentation.\n\nSome physical effects of the rape are not immediately apparent. Follow up examinations also assess the patient for tension headaches, fatigue, sleep pattern disturbances, gastrointestinal irritability, chronic pelvic pain, menstrual pain or irregularity, pelvic inflammatory disease, multiple yeast infections, sexual dysfunction, premenstrual distress, fibromyalgia, vaginal discharge, vaginal itching, burning during urination, and generalized vaginal pain.\n\nWomen are typically offered contraceptive medications because about 5% of male-on-female rapes result in pregnancy.\n\nAn internal pelvic exam is not recommended for sexually immature or prepubescent girls due to the probability that internal injuries do not exist in this age group. An internal exam may be recommended if significant bloody discharge is observed, though. A complete pelvic exam for rape (anal or vaginal) is conducted. An oral exam is done if there have been injuries to the mouth, teeth, gums or pharynx. Though the patient may have no complaints about genital pain signs of trauma can still be assessed. Prior to the complete bodily and genital exam, the patient is asked to undress, standing on a white sheet that collects any debris that may be in the clothing. The clothing and sheet are properly bagged and labeled along with other samples that can be removed from the body or clothing of the patient. Samples of fibers, mud, hair, leaves are gathered if present. Samples of fluids are collected to determine the presence of the perpetrator's saliva and semen that may be present in the patients mouth, vagina or rectum. Sometimes the victim has scratched the perpetrator in defense and fingernail scrapings can be collected.\n\nInjuries to the genital areas can be swelling, lacerations, and bruising. Common genital injuries are anal injury, labial abrasions, hymenal bruising, tears of the posterior fourchette and fossa. Bruises, tears, abrasions, inflammation and lacerations may be visible. If a foreign object was used during the assault, x-ray visualization will identify retained fragments. Genital injuries are more prevalent in post-menopausal women and prepubescent girls. Internal injuries to the cervix and vagina can be visualized using colposcopy. Using colposcopy has increased the detection of internal trauma from six percent to fifty-three percent. Genital injuries to children who have been raped or sexually assaulted differ in that the abuse may be on-going or it happened in the past after the injuries heal. Scarring is one sign of the sexual abuse of children.\n\nSeveral studies have explored the association between skin color and genital injury among rape victims. Many studies found a difference in rape-related injury based on race, with more injuries being reported for white females and males than for black females and males. This may be because the dark skin color of some victims obscures bruising. Examiners paying attention to victims with darker skin, especially the thighs, labia majora, posterior fourchette, and fossa navicularis, can help remedy this.\n\nThe presence of a sexually contracted infection can not be confirmed after rape because it cannot be detected until 72 hours afterwards.\n\nThe person who was raped may already have a sexually transmitted bacterial, viral and other infections and if diagnosed, it is treated. Prophylactic antibiotic treatment for vaginitis, gonorrhea, trichomoniasis and chlamydia may be done. Chlamydial and gonococcal infections in women are of particular concern due to the possibility of ascending infection. Immunization against hepatitis B is often considered. After prophylactic treatment is initiated, further testing is done to determine what other treatments may be necessary for other infections transmitted during the assault. These are:\n\n\nTreatment may include the administration of zidovudine/lamivudine, tenofovir/emtricitabine, ritonavir/lopinavir. Information regarding other treatment options are available from the CDC.\n\nThe transmission of HIV is frequently a major concern of the patient. Prophylactic treatment for HIV is not necessarily administered. Routine treatment for HIV after rape or sexual assault is controversial due to the low risk of infection after one sexual assault. Transmission of HIV after one exposure to penetrative anal sex is estimated to be 0.5 to 3.2 percent. Transmission of HIV after one exposure to penetrative vaginal intercourse is 0.05 to 0.15 percent. HIV can also be contracted through the oral route but is considered rare. Other recommendations are that the patient be treated prophylactically for HIV if the perpetrator is found to be infected.\n\nTesting at the time of the initial exam does not typically have forensic value if patients are sexually active and have an STI since it could have been acquired prior to the assault. Rape shield laws protect the person who was raped and who has positive test results. These laws prevent having such evidence used against someone who was raped. Someone who was raped may be concerned that a prior infection may suggest sexual promiscuity. There may, however, be situations in which testing has the legal purpose, as in cases where the threat of transmission or actual transmission of an STI was part of the crime. In nonsexually active patients, an initial, baseline negative test that is followed by a subsequent STI could be used as evidence, if the perpetrator also had an STI.\n\nTreatment failure is possible due to the emergence of antibiotic-resistant strains of pathogens.\n\nPsychiatric and emotional consequences can be apparent immediately after rape and it may be necessary to treat these very early in the evaluation and treatment. Other treatable emotional and psychiatric disorders may not become evident after the rape. These can be eating disorders, anxiety, fear, intrusive thoughts, fear of crowds, avoidance, anger, depression, humiliation, post-traumatic stress disorder (PTSD) hyperarousal, sexual disorders,(including fear of engaging in sexual activity), mood disorders, suicidal ideation, borderline personality disorder, nightmares, fear of situations that remind the patient of the rape and fear of being alone, agitation, numbness and emotional distance. Victims are able to receive help by using a telephone hotline, counseling, or shelters. Recovery from sexual assault is a complicated and controversial concept, but support groups, usually accessed by organizations are available to help in recovery. Professional counseling and on-going treatment by trained health care providers is often sought by the victim.\n\nThere are clinicians who are specially trained in the treatment of those who have experienced rape and sexual assault/abuse. Treatment can be lengthy and be challenging for both the counselor and the patient. Several treatment options exist and vary by accessibility, cost, or whether or not insurance coverage exists for the treatment. Treatment also varies dependent upon the expertise of the counselor—some have more experience and or have specialized in the treatment of sexual trauma and rape. To be the most effective, a treatment plan should developed based upon the struggles of the patient and not necessarily based upon the traumatic experience. An effective treatment plan will take the following into consideration: current stressors, coping skills, physical health, interpersonal conflicts, self-esteem, family issues, involvement of the guardian, and the presence of mental health symptoms.\nThe degree of success for emotional and psychiatric treatments is often dependent upon the terminology used in the treatment, i.e. redefining the event and experience. Labels used like \"rape victim\" and \"rape survivor\" to describe the new identities of women who have been raped suggest that the event is the dominant and controlling influence on her life. These may have an effect on supportive personnel. The consequences of using these labels needs to be assessed. Positive outcomes of emotional and psychiatric treatment for rape exist; these can be an improved self-concept, the recognition of growth, and implementing new coping styles.\n\nA perpetrator found guilty by the court is often required to receive treatment. There are many options for treatment, some more successful than others. The psychological factors that motivated the convicted perpetrator are complex but treatment can still be effective. A counselor will typically evaluate disorders that are currently present in the offender. Investigating the developmental background of the offender can help explain the origins of the abusive behavior occurred in the first place. Emotional and psychological treatment has the purpose of identifying predictors of recidivism, or the potential that the offender will commit rape again. In some instances neurological abnormalities have been identified in the perpetrators, and in some cases they have themselves experienced past trauma. Adolescents and other children can be the perpetrators of rape, although this is uncommon. In this instance, appropriate counseling and evaluation are usually conducted.\n\nShort-term treatment with a benzodiazepine may help with anxiety and antidepressants may be helpful for symptoms of post traumatic stress disorder, depression and panic attacks.\n\nAs sexual violence affects all parts of society, the response to sexual violence is comprehensive. The responses can be categorized as: individual approaches, health care responses, community-based efforts and actions to prevent other forms of sexual violence.\n\nSexual assault may be prevented by secondary school, college, and workplace education programs. At least one program for fraternity men produced \"sustained behavioral change.\" With regard to campus sexual assault, nearly two thirds of students reported knowing victims of rape and over half reported knowing perpetrators of sexual assault in one study; one in ten reported knowing a victim of rape and nearly one in four reported knowing a victim of alcohol-facilitated rape.\n\nInternational Crime on Statistics and Justice by the United Nations Office on Drugs and Crime (UNODC) find that worldwide, most victims of rape are women and most perpetrators male. Rapes against women are rarely reported to the police and the number of female rape victims is significantly underestimated. Southern Africa, Oceania and North America report the highest numbers of rape.\nThe humanitarian news organization IRIN claims that an estimated \"500,000 rapes are committed annually in South Africa once called 'the world's rape capital.' The country has some of the highest incidences of child sexual abuse in the world with more than 67,000 cases of rape and sexual assaults against children reported in 2000, with welfare groups believing that unreported incidents could be up to 10 times higher. Current data suggest that the incidence of rape has risen significantly in India.\n\nMost rape research and reports of rape are limited to male-female forms of rape. Research on male-on-male and female-on-male rape is rare. Fewer than one in ten male-male rapes are reported. As a group, males who have been raped by either gender often get little services and support, and legal systems are often ill-equipped to deal with this type of crime. Instances in which the perpetrator is female, are not clear and lead to the denial of women being sexual aggressors. This could obscure the dimensions of the problem. Research also suggests that men with sexually aggressive peers have a higher chance of reporting coercive or forced sexual intercourse outside gang circles than men without such sexually aggressive peers.\n\nFBI sex offense victims in 2012:\n\n\nFBI convicted sex offenders in 2012:\n\n\nStatistics maintained by the CDC include:\n\n\nRisk factors vary among different ethnicities. About one third of African American adolescent females report encountering some form of sexual assault including rape. One in three Native American women will experience sexual assault, more than twice the national average for American women.\n\nMore than 250,000 cases of rape or attempted rape were recorded by police annually in 65 countries in 2001 through 2002. In 2007, 40% of the 90,427 forcible rapes reported were cleared by arrest or \"exceptional means.\" Exceptional refers to situations where the person refuses to provide information or assistance necessary to obtain an arrest, the defendant dies before being arrested, or the defendant cannot be extradited from another state.\n\nForty-three percent of high school and young college men reported being coerced into sexual behavior and, of those, 95% reported that a female was the aggressor.\n\nIn 2005, sexual violence, and rape in particular, was considered the most under-reported violent crime in Great Britain. The number of reported rapes in Great Britain is lower than both incidence and prevalence rates.\n\nThe legal requirements for reporting rape vary by jurisdiction—each US state may have different requirements. New Zealand has less stringent limits.\n\nIn Italy, a 2006 National Statistic Institute survey on sexual violence against women found that 91.6% of women who suffered this did not report it to the police.\n\nIn the United Kingdom, In 1970 there was a 33% rate of conviction, while by 1985 there was a 24 per cent conviction rate for rape trials in the UK, by 2004 the conviction rate reached 5%. At that time the government report has expressed documented the year-on-year increase in attrition of reported rape cases, and pledged to address this \"justice gap\". According to Amnesty International Ireland had the lowest rate of conviction for rape, (1%) among 21 European states, in 2003. In America as of 2012, there exists a noticeable discrepancy in conviction rates among women of various ethnic identities; an arrest was made in just 13% of the sexual assaults reported by American Indian women, compared with 35% for black women and 32% for whites.\n\nJudicial bias due to rape myths and preconceived notions about rape is a salient issue in rape conviction, but voir dire intervention may be used to curb such bias.\n\nA false accusation of rape is the reporting of a rape where no rape has occurred. It is difficult to assess the true prevalence of false rape allegations, but it's generally agreed by scholars that rape accusations are false about 2% to 10% of the time. In most cases, a false accusation will not name a specific suspect.\n\nEight percent of 2,643 sexual assault cases were classified as false reports by the police in one study. The researchers noted that much of these classifications were based on the personal judgments and biases of the police investigators and were made in violation of official criteria for establishing a false allegation. Closer analysis of this category applying the Home Office counting rules for establishing a false allegation, which requires \"strong evidential grounds\" of a false allegation or a \"clear and credible\" retraction by the complainant, reduced the percentage of false reports to 3%. The researchers concluded that \"one cannot take all police designations at face value\" and that \"[t]here is an over-estimation of the scale of false allegations by both police officers and prosecutors\".\n\nAnother large-scale study was conducted in Australia, with the 850 rapes reported to the Victoria police between 2000 and 2003 (Heenan & Murray, 2006). Using both quantitative and qualitative methods, the researchers examined 812 cases and found 15.1% of complaints were withdrawn, 46.4% were marked \"no further police action\", and 2.1% of the total were \"clearly\" classified by police as false reports. The researchers noted that where the police found a case to be a false allegation but didn't want to pursue charges against the accuser, they marked it as \"no further police action\" instead. All of these complainants were then charged or threatened with charges for filing a false police report.\n\nThe Crown Prosecution Service (CPS) analyzed every rape complaint made over a 17-month period and found that \"the indication is that it is therefore extremely rare that a suspect deliberately makes a false allegation of rape or domestic violence purely out of malice.\".\nFBI reports consistently put the number of \"unfounded\" rape accusations around 8%. The unfounded rate is higher for forcible rape than for any other Index crime. The average rate of unfounded reports for Index crimes is 2%. \"Unfounded\" is not synonymous with false allegation. Bruce Gross of the Forensic Examiner described it as meaningless, saying a report could be marked as unfounded if there is no physical evidence or the alleged victim did not sustain any physical injuries.\n\nOther studies have suggested that the rate of false allegations in America may be higher. A nine-year study by Eugene J. Kanin of Purdue University in a small metropolitan area in the Midwestern United States claimed that 41% of rape accusations were false. However David Lisak, an associate professor of psychology and director of the Men's Sexual Trauma Research Project at the University of Massachusetts Boston states that \"Kanin's 1994 article on false allegations is a provocative opinion piece, but it is not a scientific study of the issue of false reporting of rape\". He further states that Kanin's study has significantly poor systematic methodology and had no independent definition of a false report. Instead, Kanin classified reports that the police department classified as false also as false. The criterion for falsehood was simply a denial of a polygraph test of the accuser. A 1998 report by the National Institute of Justice found that DNA evidence excluded the primary suspect in 26% of rape cases and concluded that this \"strongly suggests that postarrest and postconviction DNA exonerations are tied to some strong, underlying systemic problems that generate erroneous accusations and convictions\". However, this study also noted that analyzed samples involved a specific subset of rape cases (e.g. those where \"there is no consent defense\").\n\nA 2010 study by David Lisak, Lori Gardinier and other researchers published in the journal of \"Violence against Women\" found that out of 136 cases reported in a ten-year period, 5.9% were found likely to be false.\n\nVirtually all societies have had a concept of the crime of rape. Although what constituted this crime has varied by historical period and culture, the definitions tended to focus around an act of forced vaginal intercourse perpetrated through physical violence or imminent threat of death or severe bodily injury, by a man, on a woman or a girl, not his wife. The actus reus of the crime, was, in most societies, the insertion of the penis into the vagina. The way sexuality was conceptualized in many societies rejected the very notion that a woman could force a man into sex — women were often seen as passive while men were deemed to be assertive and aggressive. Sexual penetration of a male by another male fell under the legal domain of sodomy.\n\nRape laws existed to protect virginal daughters from rape. In these cases, a rape done to a woman was seen as an attack on the estate of her father because she was his property and a woman's virginity being taken before marriage lessened her value; if the woman was married, the rape was an attack on the husband because it violated his property. The rapist was either subject to payment (see wreath money) or severe punishment. The father could rape or keep the rapist's wife or make the rapist marry his daughter. A man could not be charged with raping his wife since she was his property. Thus, marital rape was allowed. Author Winnie Tomm stated, \"By contrast, rape of a single woman without strong ties to a father or husband caused no great concern.\" An incident could be excluded from the definition of rape due to the relation between the parties, such as marriage, or due to the background of the victim. In many cultures forced sex on a prostitute, slave, war enemy, member of a racial minority, etc., was not rape.\n\nFrom the classical antiquity of Greece and Rome into the Colonial period, rape along with arson, treason and murder was a capital offense. \"Those committing rape were subject to a wide range of capital punishments that were seemingly brutal, frequently bloody, and at times spectacular.\" In the 12th century, kinsmen of the victim were given the option of executing the punishment themselves. \"In England in the early fourteenth century, a victim of rape might be expected to gouge out the eyes and/or sever the offender's testicles herself.\" Despite the harshness of these laws, actual punishments were usually far less severe: in late Medieval Europe, cases concerning rapes of marriageable women, wives, widows, or members of the lower class were rarely brought forward, and usually ended with only a small monetary fine or a marriage between the victim and the rapist. \n\nIn ancient Greece and Rome, both male-on-female and male-on-male concepts of rape existed. Roman laws allowed three distinct charges for the crime: \"stuprum\", unsanctioned sexual intercourse (which, in the early times, also included adultery); \"vis\", a physical assault for purpose of lust; and \"iniuria\", a general charge denoting any type of assault upon person. Aforementioned \"Lex Iulia\" specifically criminalized \"per vim stuprum\", unsanctioned sexual intercourse by force. The former two were public criminal charges which could be brought whenever the victim was a woman or a child of either gender, but only if the victim was a freeborn Roman citizen (\"ingenuus\"), and carried potential sentence of death or exile. \"Iniuria\" was a civil charge that demanded monetary compensation, and had a wider application (for example, it could have been brought in case of sexual assault on a slave by a person other than their owner.) Augustus Caesar enacted reforms for the crime of rape under the assault statute \"Lex Iulia de vi publica\", which bears his family name, \"Iulia\". It was under this statute rather than the adultery statute of \"Lex Iulia de adulteriis\" that Rome prosecuted this crime. Rape was made into a \"public wrong\" (\"iniuria publica\") by the Roman Emperor Constantine.\n\nIn contrast to the modern understanding of the subject, Romans drew clear distinctions between \"active\" (penetrative) and \"passive\" (receptive) partners, and all these charges implied penetration by the assailant (which necessarily ruled out the possibility of female-on-male or female-on-female rape.) It is not clear which (if any) of these charges applied to assaults upon an adult male, though such an assault upon a citizen was definitely seen as a grave insult (within Roman culture, an adult male citizen could not possibly consent to the receptive role in a sexual intercourse without a severe loss of status.) The law known as Lex Scantinia covered at least some forms of male-on-male \"stuprum\", and Quintillian mentions a fine of 10,000 sesterces – about 10 years worth of a Roman legionnaire's pay – as a normal penalty for \"stuprum\" upon an \"ingenuus\". However, its text is lost and its exact provisions are no longer known.\n\nEmperor Justinian continued the use of the statute to prosecute rape during the sixth century in the Eastern Roman Empire. By late antiquity, the general term \"raptus\" had referred to abduction, elopement, robbery, or rape in its modern meaning. Confusion over the term led ecclesiastical commentators on the law to differentiate it into \"raptus seductionis\" (elopement without parental consent) and \"raptus violentiae\" (ravishment). Both of these forms of \"raptus\" had a civil penalty and possible excommunication for the family and village receiving the abducted woman, although \"raptus violentiae\" also incurred punishments of mutilation or death.\n\nIn the United States, a husband could not be charged with raping his wife until 1979. In the 1950s, in some states in the US, a white woman having consensual sex with a black man was considered rape. Prior to the 1930s, rape was considered a sex crime that was always committed by men and always done to women. From 1935–1965, a shift from labeling rapists as criminals to believing them to be mentally ill \"sexual psychopaths\" began making its way into popular opinion. Men caught for committing rape were no longer sentenced to prison but admitted to mental health hospitals where they would be given medication for their illness. Because only men deemed insane were the ones considered to have committed rape, no one considered the everyday person to be capable of such violence.\n\nTransitions in women's roles in society were also shifting, causing alarm and blame towards rape victims. Because women were becoming more involved in the public (i.e. searching for jobs rather than being a housewife), some people claimed that these women were \"loose\" and looking for trouble. Giving up the gender roles of mother and wife was seen as defiant against traditional values while immersing themselves within society created the excuse that women would \"not [be] entitled to protection under the traditional guidelines for male-female relationships\".\n\nUntil the 19th century, many jurisdictions required ejaculation for the act to constitute the offense of rape. Acts other than vaginal intercourse did not constitute rape in common law countries and in many other societies. In many cultures, such acts were illegal, even if they were consensual and performed between married couples (see sodomy laws). In England, for example, the Buggery Act 1533, which remained in force until 1828, provided for the death penalty for \"buggery\". Many countries criminalized \"non-traditional\" forms of sexual activity well into the modern era: notably, in the US state of Idaho, sodomy between consensual partners was punishable by a term of five years to life in prison as late as 2003, and this law was only ruled to be inapplicable to married couples in 1995. Today, in many countries, the definition of the actus reus has been extended to all forms of penetration of the vagina and anus (e.g. penetration with objects, fingers or other body parts) as well as insertion of the penis in the mouth.\n\nIn 1998, Judge Navanethem Pillay of the International Criminal Tribunal for Rwanda said: \"From time immemorial, rape has been regarded as spoils of war. Now it will be considered a war crime. We want to send out a strong message that rape is no longer a trophy of war.\" Rape, in the course of war, dates back to antiquity, ancient enough to have been mentioned in the Bible. The Israelite, Persian, Greek and Roman armies reportedly engaged in war rape. The Mongols, who established the Mongol Empire across much of Eurasia, caused much destruction during their invasions. Contemporary documents say that after a conquest, the Mongol soldiers looted, pillaged and raped. According to Rogerius of Apulia, a monk who survived the Mongol invasion of Hungary, the Mongol warriors \"found pleasure\" in humiliating local women.\n\nThe systematic rape of as many as 80,000 women by the Japanese soldiers during the six weeks of the Nanking Massacre is an example of such atrocities. During World War II an estimated 200,000 Korean and Chinese women were forced into prostitution in Japanese military brothels, as so-called \"comfort women\". French Moroccan troops known as Goumiers committed rapes and other war crimes after the Battle of Monte Cassino. \"(See Marocchinate.)\" French women in Normandy complained about rapes during the liberation of Normandy. Soldiers raping women and girls was common in many areas occupied by the Red Army. A female Soviet war correspondent described what she had witnessed: \"The Russian soldiers were raping every German female from eight to eighty. It was an army of rapists.\" According to German historian Miriam Gebhardt, as many as 190,000 women were raped by U.S. soldiers in Germany.\n\nAccording to researcher and author Krisztián Ungváry, some 38,000 civilians were killed during the Siege of Budapest: about 13,000 from military action and 25,000 from starvation, disease and other causes. Included in the latter figure are about 15,000 Jews, largely victims of executions by Hungarian Arrow Cross Party militia. When the Soviets finally claimed victory, they initiated an orgy of violence, including the wholesale theft of anything they could lay their hands on, random executions and mass rape. An estimated 50,000 women and girls were raped, although estimates vary from 5,000 to 200,000. Hungarian girls were kidnapped and taken to Red Army quarters, where they were imprisoned, repeatedly raped and sometimes murdered.\n\nIn the United States, before and during the American Civil War when chattel slavery was widespread, the law focused primarily on rape as it pertained to black men raping white women. The penalty for such a crime in many jurisdictions was death or castration. The rape of a black woman, by any man, was considered legal. As early as the 19th century, American women were criticized if they \"stray[ed] out of a [dependent] position...fought off [an] attacker...[or] behaved in too self reliant a manner...\" in which case \"the term rape no longer applied\". \n\n\n"}
{"id": "19991106", "url": "https://en.wikipedia.org/wiki?curid=19991106", "title": "Redistributive change", "text": "Redistributive change\n\nRedistributive change is a legal theory of economic justice in the context of U.S. law that promotes the recognition of poverty as a classification, like race, ethnicity, gender, and religion, that should likewise draw extra scrutiny from the courts in matters pertaining to civil rights.\n\nThe theory was discussed in academia in the wake of \"Goldberg v. Kelly\", a 1970 U.S. Supreme Court case, which decided that due process, such as a notice and a fair hearing, were required when dealing with the deprivation of a government benefit (such as a medical license) or an entitlement (such as welfare payments). However, attempts to promote redistributive change through the courts gained no traction, and the result of \"Goldberg v. Kelly\" was, thus, limited in scope. \n\nOne of the goals, in light of \"Brown v. Board of Education\", was to promote equality in school funding, but this was specifically rejected by the Supreme Court in \"San Antonio Independent School District v. Rodriguez\" (1973), which ruled that there was no inherent right to education in the United States.\n\nA discussion among two law professors and Illinois State Senator and law lecturer Barack Obama on the topic of civil rights aired on Chicago Public Radio's Odyssey program in 2001. Obama declared in the discussion that redistributive change needs to come through legislation, not the courts, and lamented that the civil rights movement failed to pursue political means to bring such a change about. As a result of Obama's candidacy in the 2008 U.S. Presidential election, the matter became a campaign issue during the final week of the election, fueling a charge made by his opponent, U.S. Senator John McCain, that Obama was a closet socialist.\n\n"}
{"id": "4501180", "url": "https://en.wikipedia.org/wiki?curid=4501180", "title": "Robert Hansen", "text": "Robert Hansen\n\nRobert Christian Hansen (February 15, 1939 – August 21, 2014), known in the media as the \"Butcher Baker\", was an American serial killer. Between 1971 and 1983, Hansen abducted, raped, and murdered at least 17 women in and around Anchorage, Alaska, hunting them down in the wilderness with a Ruger Mini-14 and a knife. He was arrested and convicted in 1983 and was sentenced to 461 years and a life sentence without the possibility of parole.\n\nRobert Hansen was born in Estherville, Iowa, in 1939. He was the son of a Danish immigrant and followed in his father's footsteps as a baker. In his youth, he was skinny and painfully shy, afflicted with a stutter and severe acne that left him permanently scarred. Shunned by the attractive girls in school, he grew up hating them and nursing fantasies of cruel revenge. Throughout childhood and adolescence, Hansen was described as being quiet and a loner and he had an unhealthy relationship with his domineering father. He started to practice both hunting and archery and often found refuge in these pastimes.\n\nIn 1957, Hansen enlisted in the United States Army Reserve and served for one year before being discharged. He later worked as an assistant drill instructor at a police academy in Pocahontas, Iowa. There, he began a relationship with a younger woman. He married her in the summer of 1960.\n\nOn December 7, 1960, he was arrested for burning down a Pocahontas County Board of Education school bus garage, for which he served 20 months of a three-year prison sentence in Anamosa State Penitentiary. His wife filed for divorce while he was incarcerated. Over the next few years, he was jailed several times for petty theft. In 1967, he moved to Anchorage, Alaska, with his second wife, whom he had married in 1963 and with whom he had two children. In Anchorage, he was well liked by his neighbors and set several local hunting records.\n\nOn June 13, 1983, 17-year-old Cindy Paulson escaped from Hansen, while he was trying to load her into his Piper Super Cub. She told police he had offered her $200 to perform oral sex but that, when she got into the car, he pulled a gun on her and drove her to his home in Muldoon. There, he held her captive, torturing, raping and sexually assaulting her. She mentioned that, after he chained her by the neck to a post in the house's basement, Hansen took a nap on a nearby couch.\n\nWhen he awoke, he put her in his car and took her to Merrill Field airport, where he told her that he intended to \"take her out to his cabin\" (a shack in the Knik River area of the Matanuska Valley accessible only by boat or bush plane). Paulson, crouched in the back seat of the car with her wrists cuffed in front of her body, waited to escape until Hansen was busy loading the airplane's cockpit. While Hansen's back was turned, Paulson crawled out of the back seat, opened the driver's side door and ran toward nearby Sixth Avenue.\n\nShe later told police that she had left her blue sneakers on the passenger side floor of the sedan's backseat, as evidence that she had been in the car. Hansen panicked and chased her, but Paulson made it to Sixth Avenue first and managed to flag down a passing truck. The driver, Robert Yount, alarmed by her disheveled appearance, stopped and picked her up. He drove her to the Mush Inn, where she jumped out of the truck and ran inside. While she pleaded with the clerk to phone her boyfriend at the Big Timber Motel, the truck driver continued on to work, where he called the police to report the barefoot, handcuffed woman.\n\nWhen Anchorage Police Department officers arrived at the Mush Inn, they were told that the young woman had taken a cab to the Big Timber Motel. APD officers arrived at Room 110 of the Big Timber Motel and found Cindy Paulson, still handcuffed and alone. She was taken to APD headquarters, where she described the perpetrator. Hansen, when questioned by APD officers, denied the accusation, stating that Paulson was just trying to cause some trouble because he would not pay her extortion demands. Although Hansen had several prior run-ins with the law, his meek demeanour and humble occupation as a baker, along with a strong alibi from his friend John Henning, kept him from being considered as a serious suspect and the case went cold.\n\nDetective Glenn Flothe of the Alaska State Troopers had been part of a team investigating the discovery of several bodies in and around Anchorage, Seward and the Matanuska-Susitna Valley area. The first of the bodies was found by construction workers near Eklutna Road. The body, dubbed \"Eklutna Annie\" by investigators, has never been identified. Later that year, the body of Joanna Messina was discovered in a gravel pit near Seward and in 1982, the remains of 23-year-old Sherry Morrow were discovered, in a shallow grave near the Knik River. Flothe now had three bodies and what looked like one killer.\n\nHe contacted Federal Bureau of Investigation Special Agent Roy Hazelwood and requested help with a criminal psychological profile, based on the three recovered bodies. Hazelwood thought that the killer would be an experienced hunter with low self-esteem, have a history of being rejected by women and would feel compelled to keep \"souvenirs\" of his murders, such as a victim's jewelry. He also suggested that the assailant might stutter. Using this profile, Flothe investigated possible suspects until he reached Hansen, who fit the profile and owned a plane.\n\nSupported by Paulson's testimony and Hazelwood's profile, Flothe and the APD secured a warrant to search Hansen's plane, cars and home. On October 27, 1983, investigators uncovered jewelry belonging to some of the missing women, as well as an array of firearms in a corner hideaway of Hansen's attic. Also found was an aviation map with little \"x\" marks on it, hidden behind Hansen's headboard.\n\nWhen confronted with the evidence found in his home, Hansen denied it as long as he could but he eventually began to blame the women and tried to justify his motives. Eventually, confessing to each item of evidence as it was presented to him, he admitted to a spree of attacks against Alaskan women starting in 1971. Hansen's earliest victims were young women, usually between 16 and 19 and not sex workers, unlike the victims who led to his discovery.\n\nHansen is known to have raped and assaulted over 30 Alaskan women. He is also responsible for murdering at least 17, ranging in age from 16 to 41. They were:\n\nOf these 17 women, Hansen was only formally charged with the murder of four - Sherry Morrow, Joanna Messina, Eklutna Annie and Paula Goulding. He was also charged with the kidnapping and rape of Cindy Paulson.\n\nOnce arrested, Hansen was charged with assault, kidnapping, multiple weapons offenses, and theft and insurance fraud. The last charge was related to a claim filed with the insurance company over alleged theft of some trophies, whose funds he used to purchase the Super Cub. At trial, he claimed he later recovered the trophies in his backyard but forgot to inform the insurer.\n\nOnly after ballistics tests returned a match between bullets found at the crime scenes and Hansen's rifle, did he enter into a plea bargain. He pleaded guilty to the four homicides the police had evidence for (Morrow, Messina, Goulding and Eklutna Annie) and provided details about his other victims, in return for serving his sentence in a federal prison, along with no publicity in the press. Another condition of the plea bargain was his participation in deciphering the markings on his aviation map and locating his victims' bodies. He confirmed the police theory of how the women were abducted, adding that he would sometimes let a potential victim go if she convinced him that she wouldn't report him to police. He indicated that he began killing in the early 1970s.\n\nHe showed investigators 17 grave sites, in and around Southcentral Alaska, 12 of which were unknown to investigators. There remained marks on his map that he refused to give up, including three in Resurrection Bay, near Seward (authorities suspect two of these marks belong to the graves of Mary Thill and Megan Emrick, whom Hansen has denied killing). The remains of 12 (of a probable 21) victims were exhumed by the police and returned to their families. Hansen was sentenced by jury to 461 years plus life in prison, without the possibility of parole. He was first imprisoned at the United States Penitentiary, Lewisburg in Lewisburg, Pennsylvania.\n\nIn 1988, he was returned to Alaska and briefly incarcerated at Lemon Creek Correctional Center in Juneau. He was also imprisoned at Spring Creek Correctional Center in Seward until May 2014, when he was transported to the Anchorage Correctional Complex for health reasons.\n\nHansen died at the age of 75, at Alaska Regional Hospital in Anchorage on August 21, 2014, due to undisclosed, lingering health conditions.\n\n\nDocumentaries:\n\nTV series:\n\n\n"}
{"id": "46745277", "url": "https://en.wikipedia.org/wiki?curid=46745277", "title": "Robert Singleton (activist)", "text": "Robert Singleton (activist)\n\nRobert Singleton, also known as Bob Singleton (born 1936), is a civil rights activist. He and his wife Helen Singleton are known for being part of the Freedom Rides together in Jackson, Mississippi. He currently teaches economics at Loyola Marymount University, where he has been for over twenty years.\n\nRobert was born and raised in Philadelphia. He joined the army and served in Europe. When he came back from war, he went to school at the University of California, Los Angeles. This is where studied economics and also went for his undergraduate. In 1964, he received his Ph.D. and then went on to teach at UCLA for a few years. Robert began his activism at UCLA when he ran for president of their chapter for the NAACP. During his time on campus, he became inspired by the speakers that came and spoke, including James Farmer, Malcolm X, James Baldwin, and Langston Hughes, each of whom helped him realize the injustices in America. He first joined the sit-ins, where he met other activists and helped them organize events where they would find food and clothing, get sharecroppers to vote, and organize the freedom rider to Jackson, Mississippi. When he returned, the NAACP had been kicked out of the school, so it was replaced with a CORE group, of which he was chair.\n\nRobert Singleton and his wife are most known for the Freedom Rides that they took part of in Jackson, Mississippi. The south ignored the ruling that the U.S Supreme Court made about public transportation not being segregated because segregation was unconstitutional. As a result, they protested against segregation on public transportation. On July 30, 1961, when they got to Mississippi, Robert, his wife, and the other riders were arrested and sent to Parchmen Penitentiary. The conditions in the prison were harsh. They were put in maximum security, separated from each other, and placed next to the gas chambers. \"Three months after the freedom rides they were to desegregate public transportation. Robert and Helen Singleton were among the few married couples who participated in the Freedom Rides.\n"}
{"id": "15291970", "url": "https://en.wikipedia.org/wiki?curid=15291970", "title": "Schur–Weyl duality", "text": "Schur–Weyl duality\n\nSchur–Weyl duality is a mathematical theorem in representation theory that relates irreducible finite-dimensional representations of the general linear and symmetric groups. It is named after two pioneers of representation theory of Lie groups, Issai Schur, who discovered the phenomenon, and Hermann Weyl, who popularized it in his books on quantum mechanics and classical groups as a way of classifying representations of unitary and general linear groups.\n\nSchur–Weyl duality can be proven using the double centralizer theorem.\n\nSchur–Weyl duality forms an archetypical situation in representation theory involving two kinds of symmetry that determine each other. Consider the tensor space\n\nThe symmetric group \"S\" on \"k\" letters acts on this space (on the left) by permuting the factors,\n\nThe general linear group \"GL\" of invertible \"n\"×\"n\" matrices acts on it by the simultaneous matrix multiplication,\n\nThese two actions commute, and in its concrete form, the Schur–Weyl duality asserts that under the joint action of the groups \"S\" and \"GL\", the tensor space decomposes into a direct sum of tensor products of irreducible modules for these two groups that determine each other,\n\nThe summands are indexed by the Young diagrams \"D\" with \"k\" boxes and at most \"n\" rows, and representations formula_5 of \"S\" with different \"D\" are mutually non-isomorphic, and the same is true for representations formula_6 of \"GL\".\n\nThe abstract form of the Schur–Weyl duality asserts that two algebras of operators on the tensor space generated by the actions of \"GL\" and \"S\" are the full mutual centralizers in the algebra of the endomorphisms formula_7\n\nSuppose that \"k\" = 2 and \"n\" is greater than one. Then the Schur–Weyl duality is the statement that the space of two-tensors decomposes into symmetric and antisymmetric parts, each of which is an irreducible module for \"GL\":\n\nThe symmetric group \"S\" consists of two elements and has two irreducible representations, the trivial representation and the sign representation. The trivial representation of \"S\" gives rise to the symmetric tensors, which are invariant (i.e. do not change) under the permutation of the factors, and the sign representation corresponds to the skew-symmetric tensors, which flip the sign.\n\n\n"}
{"id": "53654611", "url": "https://en.wikipedia.org/wiki?curid=53654611", "title": "Seiko Mikami", "text": "Seiko Mikami\n\nSeiko Mikami (Kanji:) was a Japanese artist known for her large-scale interactive art installations. Mikami was born in Shizuoka, Japan, in 1961, and she stepped into the art scene in the mid-1980s with large-scale art that studied information society and the human body. After moving to the United States in 1991, she studied computer science at the New York Institute of Technology, which lead her art to focus on the interaction between electronics and the human perception. She became a professor in the Department of Information Deisgn at Tama University in 2000. Mikami died from cancer on January 2, 2015.\n\nUnveiled in 2004, a collaboration between Mikami and the architect Sota Ichikawa resulted in \"Gravicells\", an interactive installation originally housed at the Yamaguchi Center for Arts and Media (YCAM). The piece serves as a means for encountering the existence of gravity in a manner one could not experience in his or her daily life, and how the world, our bodies, and our perception are all influenced by gravity in complex and almost delicate ways. When the space is encountered by multiple participants, the visual interactions of the projection between them helps the users achieve a sense of each other. This artwork thus functions as almost an external human sensory function, encouraging awareness of ourselves and others in ways not previously approached.\n\nIn the center of the installation space rests a six-by-six-meter paneled floor; these panels are laced with sensors that can detect your weight, tilt, and velocity. This information is continuously collected and analyzed, and is then translated into deformations in the lines projected on the floor near the visitor. GPS data of the location in which the installation is housed also affects the projected environment and the lines will change according to shifts in the location's gravity.\n\nOne of Mikami's most notable works, \"Desire of Codes\", was commissioned by and created at YCAM. Exploring the individual’s existence in public and in private, \"Desire of Codes\" emphasizes the increasingly less blurred line between the individual as defined by information and the individual as flesh and blood. Housed in a space normally devoted to theatrical performances, this interactive piece consists of three parts:\n\n\nIn addition to these main three parts, the soundscape of the area in which the work resides is also altered: the noise generated by visitors as well as the artwork’s own mechanical sounds are mixed and played back, which further emphasizes the indistinction between one's own encoded existence and physical reality.\n"}
{"id": "25779828", "url": "https://en.wikipedia.org/wiki?curid=25779828", "title": "Self-esteem functions", "text": "Self-esteem functions\n\nSelf-esteem can be defined as how favorably individuals evaluate themselves. According to Leary, self-esteem is the relationship between one’s real self and one’s ideal self, feeding off of favorable behaviors. It refers to an individual's sense of his or her value or worth, or the extent to which a person values, approves of, appreciates, prizes, or likes him or herself. Self-esteem is generally considered the evaluative component of the self-concept, a broader representation of the self that includes cognitive and behavioral aspects as well as evaluative or affective ones. There are several different proposals as to the functions of self-esteem. One proposal includes to satisfy the inherent need to feel good about one self. Another one would be to minimize social rejection . Self-esteem is also known as A way for a person to remain dominant in relationships (Barkow, 1980). Self-esteem is known to protect people from fear that has potential of arising from the prospect of death- terror management theory. Self-esteem helps motivate people to achieve their goals- high self-esteem leading to coping in situations and low self-esteem leading to avoidance.\n\nThe sociometer theory was developed by Mark Leary (1999) to explain the functions of self-esteem. A sociometer is a measure of how desirable one would be to other people - this is influenced by one’s self-esteem. They suggested that self-esteem has evolved to monitor one’s social acceptance and is used as a gauge for avoiding social devaluation and rejection.\nThe sociometer theory is strongly grounded in evolutionary theories which suggest that survival depends on social acceptance for reasons such as protection, reciprocal behaviours and most importantly reproduction. The monitoring of one's acceptance via self-esteem is therefore crucial in order to achieve these kinds of social interactions and be better able to compete for the social benefits of them.\n\nKirkpatrick and Ellis (2003) expanded on Leary’s work and suggested that the sociometer’s function was not only to ensure that an individual was not excluded from their social group but also to rate the strength of the social group compared to other groups. Leary and his colleagues stated that a sociometer is a measure of how a person is desirable by the people and this is oftentimes influenced through a person's self-esteem.\n\nSelf-determination theory (SDT) states that man is born with an intrinsic motivation to explore, absorb and master his surroundings and that true high self-esteem is reported when the basic psychological nutrients, or needs, of life (relatedness, competency and autonomy) are in balance Nayler, C. (2005) Theories of Self Esteem. Positive Psychology.\n\nThe ethological perspective (Barkow, 1980) suggests that self-esteem is an adaptation that has evolved for the purpose of maintaining dominance in relationships. It is said that human beings have evolved certain mechanisms for monitoring dominance in order to facilitate reproductive behaviours such attaining a mate. Because attention and favorable reactions from others were associated with being dominant, feelings of self-esteem have also become associated with social approval and deference. From this perspective, the motive to evaluate oneself positively in evolutionary terms is to enhance one’s relative dominance (Leary, 1999).\n\nLeary et al. (2001) tested the idea of dominance and social acceptance on self-esteem. Trait self-esteem appeared to be related to the degree to which participants felt accepted by specific people in their lives, but not to the degree to which participants thought those individuals perceived them as dominant. Acceptance and dominance appeared to have independent effects on self-esteem.\n\nThe terror management theory, developed by Sheldon Solomon at al. (1991), which in relation to self-esteem states that having self-esteem helps protect individuals from the fear they experience at the prospect of their own death. It is suggested that people are constantly searching for ways to enhance their self-esteem in order to quell unconscious death anxiety. This internalisation of cultural values is also a key factor in Terror Management Theory (TMT) in which self-esteem is seen as a culturally based construction derived from integrating specific contingencies valued by society into one's own ‘worldview’. High self-esteem promotes positive affect and personal growth, psychological well-being and coping as a buffer against anxiety in the knowledge of our eventual certain death, and reduces defensive anxiety related behaviour. Nayler, C. (2005) Theories of Self Esteem. Positive Psychology. Terror management theory, based primarily on the writings of Ernest Becker (1962, 1971, 1973, 1975) and Otto\nRank (1936, 1941), posits that self-esteem is sought because it provides protection against the fear of death (Greenberg, Pyszczynski,\n& Solomon, 1986; Solomon, Greenberg, & Pyszczynski,1991a). From this perspective, the fear of death is rooted\nin an instinct for self-preservation that humans share with other species. Jones, E. McGreggor, H. Pyszczynski, T. Simon, L. & Solomon, S. (1997). Terror Management\nTheory and Self –Esteem Reduces Mortality Salience Effects. Personality and Social Psychology 72, 24-36\n\nSome researchers believe that having a high self-esteem facilitates goal achievement.\nBednar, Wells, and Peterson (1989) proposed that self-esteem is a form of subjective feedback about the adequacy of the self. This feedback (self-esteem) is positive when the individual copes well with circumstances and is negative when he or she avoids threats. In turn, self-esteem affects subsequent goal achievement; high self-esteem increases coping, and low self-esteem leads to further avoidance (Leary, 1999).\n\nIllusion of control is the tendency for human beings to believe they can control, or at least influence, outcomes that they demonstrably have no influence over, a mindset often seen in those who gamble (Langer, 1975). However, for individuals who are not gamblers Taylor and Brown (1988) suggest it may serve to be a function of self-esteem. Belief that there is a level of control over the situation a person is in, may lead to an increased level of motivation and performance in a self-regulating manner. In other words, one will work harder to become successful if they believe they have control over their success. A high self-esteem would be needed for this belief of control and so the need for a sense of control may be a function of self-esteem.\nWhen applying sociometer theory, it suggests that the illusion of control is an adaptive response in order to self-regulate behaviour to cultural norms and thereby provide an individual with an increased level of self-esteem. In social psychology, the illusion of control is grouped with two other concepts and termed as the ‘positive illusions’. Refer to link for a blog entry on illusion of control. http://interaction-dynamics.com/blog/tag/illusion-of-control/.\n\n"}
{"id": "275871", "url": "https://en.wikipedia.org/wiki?curid=275871", "title": "Signal", "text": "Signal\n\nIn communication systems, signal processing, and electrical engineering, a signal is a function that \"conveys information about the behavior or attributes of some phenomenon\". A signal may also be defined as an \"observable change in a quantifiable entity\". In the physical world, any quantity exhibiting variation in time or variation in space (such as an image) is potentially a signal that might provide information on the status of a physical system, or convey a message between observers, among other possibilities. The \"IEEE Transactions on Signal Processing\" states that the term \"signal\" includes audio, video, speech, image, communication, geophysical, sonar, radar, medical and musical signals. In a later development, a signal is redefined as an \"observable change in a quantifiable entity\"; here, anything which is only a function of space, such as an image, is excluded from the category of signals. Also, it is stated that a signal may or may not contain any information.\n\nIn nature, signals can take the form of any action by one organism able to be perceived by other organisms, ranging from the release of chemicals by plants to alert nearby plants of the same type of a predator, to sounds or motions made by animals to alert other animals of the presence of danger or of food. Signaling occurs in organisms all the way down to the cellular level, with cell signaling. Signaling theory, in evolutionary biology, proposes that a substantial driver for evolution is the ability for animals to communicate with each other by developing ways of signaling. In human engineering, signals are typically provided by a sensor, and often the original form of a signal is converted to another form of energy using a transducer. For example, a microphone converts an acoustic signal to a voltage waveform, and a speaker does the reverse.\n\nThe formal study of the information content of signals is the field of information theory. The information in a signal is usually accompanied by noise. The term \"noise\" usually means an undesirable random disturbance, but is often extended to include unwanted signals conflicting with the desired signal (such as crosstalk). The prevention of noise is covered in part under the heading of signal integrity. The separation of desired signals from a background is the field of signal recovery, one branch of which is estimation theory, a probabilistic approach to suppressing random disturbances.\n\nEngineering disciplines such as electrical engineering have led the way in the design, study, and implementation of systems involving transmission, storage, and manipulation of information. In the latter half of the 20th century, electrical engineering itself separated into several disciplines, specialising in the design and analysis of systems that manipulate physical signals; electronic engineering and computer engineering as examples; while design engineering developed to deal with functional design of user–machine interfaces.\n\nDefinitions specific to sub-fields are common. For example, in information theory, a \"signal\" is a codified message, that is, the sequence of states in a communication channel that encodes a message. In the context of signal processing, signals are analog and digital representations of analog physical quantities.\n\nIn terms of their spatial distributions, signals may be categorized as point source signals (PSSs) and distributed source signals (DSSs).\n\nIn a communication system, a \"transmitter\" encodes a \"message\" to create a signal, which is carried to a \"receiver\" by the communications channel. For example, the words \"Mary had a little lamb\" might be the message spoken into a telephone. The telephone transmitter converts the sounds into an electrical signal. The signal is transmitted to the receiving telephone by wires; at the receiver it is reconverted into sounds.\n\nIn telephone networks, signaling, for example common-channel signaling, refers to phone number and other digital control information rather than the actual voice signal.\n\nSignals can be categorized in various ways. The most common distinction is between discrete and continuous spaces that the functions are defined over, for example discrete and continuous time domains. Discrete-time signals are often referred to as \"time series\" in other fields. Continuous-time signals are often referred to as \"continuous signals\".\n\nA second important distinction is between discrete-valued and continuous-valued. Particularly in digital signal processing, a digital signal may be defined as a sequence of discrete values, typically associated with an underlying continuous-valued physical process. In digital electronics, digital signals are the continuous-time waveform signals in a digital system, representing a bit-stream.\n\nAnother important property of a signal is its entropy or information content.\n\nTwo main types of signals encountered in practice are \"analog\" and \"digital\". The figure shows a digital signal that results from approximating an analog signal by its values at particular time instants. Digital signals are \"quantized\", while analog signals are continuous.\n\nAn analog signal is any continuous signal for which the time varying feature (variable) of the signal is a representation of some other time varying quantity, i.e., \"analogous\" to another time varying signal. For example, in an analog audio signal, the instantaneous voltage of the signal varies continuously with the pressure of the sound waves. It differs from a digital signal, in which the continuous quantity is a representation of a sequence of discrete values which can only take on one of a finite number of values. The term analog signal usually refers to electrical signals; however, mechanical, pneumatic, hydraulic, human speech, and other systems may also convey or be considered analog signals.\nAn analog signal uses some property of the medium to convey the signal's information. For example, an aneroid barometer uses rotary position as the signal to convey pressure information. In an electrical signal, the voltage, current, or frequency of the signal may be varied to represent the information.\nAny information may be conveyed by an analog signal; often such a signal is a measured response to changes in physical phenomena, such as sound, light, temperature, position, or pressure. The physical variable is converted to an analog signal by a transducer. For example, in sound recording, fluctuations in air pressure (that is to say, sound) strike the diaphragm of a microphone which induces corresponding fluctuations in the current produced by a coil in an electromagnetic microphone, or the voltage produced by a condenser microphone. The voltage or the current is said to be an \"analog\" of the sound.\n\nA digital signal is a signal that is constructed from a discrete set of waveforms of a physical quantity so as to represent a sequence of discrete values. A \"logic signal\" is a digital signal with only two possible values, and describes an arbitrary bit stream. Other types of digital signals can represent three-valued logic or higher valued logics.\n\nAlternatively, a digital signal may be considered to be the sequence of codes represented by such a physical quantity. The physical quantity may be a variable electric current or voltage, the intensity, phase or polarization of an optical or other electromagnetic field, acoustic pressure, the magnetization of a magnetic storage media, etcetera. Digital signals are present in all digital electronics, notably computing equipment and data transmission.\nWith digital signals, system noise, provided it is not too great, will not affect system operation whereas noise always degrades the operation of analog signals to some degree.\n\nDigital signals often arise via sampling of analog signals, for example, a continually fluctuating voltage on a line that can be digitized by an analog-to-digital converter circuit, wherein the circuit will read the voltage level on the line, say, every 50 microseconds and represent each reading with a fixed number of bits. The resulting stream of numbers is stored as digital data on a discrete-time and quantized-amplitude signal. Computers and other digital devices are restricted to discrete time.\n\nOne of the fundamental distinctions between different types of signals is between continuous and discrete time. In the mathematical abstraction, the domain of a continuous-time (CT) signal is the set of real numbers (or some interval thereof), whereas the domain of a discrete-time (DT) signal is the set of integers (or some interval). What these integers represent depends on the nature of the signal; most often it is time.\n\nIf for a signal, the quantities are defined only on a discrete set of times, we call it a discrete-time signal. A simple source for a discrete time signal is the sampling of a continuous signal, approximating the signal by a sequence of its values at particular time instants.\n\nA discrete-time real (or complex) signal can be seen as a function from (a subset of) the set of integers (the index labeling time instants) to the set of real (or complex) numbers (the function values at those instants).\n\nA continuous-time real (or complex) signal is any real-valued (or complex-valued) function which is defined at every time \"t\" in an interval, most commonly an infinite interval.\n\nIf a signal is to be represented as a sequence of numbers, it is impossible to maintain exact precision - each number in the sequence must have a finite number of digits. As a result, the values of such a signal belong to a finite set; in other words, it is quantized. Quantization is the process of converting a continuous analog audio signal to a digital signal with discrete numerical values.\n\nSignals in nature can be converted to electronic signals by various sensors. Some examples are:\n\n\nOther examples of signals are the output of a thermocouple, which conveys temperature information, and the output of a pH meter which conveys acidity information.\n\nA typical role for signals is in signal processing. A common example is signal transmission between different locations. The embodiment of a signal in electrical form is made by a transducer that converts the signal from its original form to a waveform expressed as a current (\"I\") or a voltage (\"V\"), or an electromagnetic waveform, for example, an optical signal or radio transmission. Once expressed as an electronic signal, the signal is available for further processing by electrical devices such as electronic amplifiers and electronic filters, and can be transmitted to a remote location by electronic transmitters and received using electronic receivers.\n\nIn Electrical engineering programs, a class and field of study known as \"signals and systems\" (S and S) is often seen as the \"cut class\" for EE careers, and is dreaded by some students as such. Depending on the school, undergraduate EE students generally take the class as juniors or seniors, normally depending on the number and level of previous linear algebra and differential equation classes they have taken.\n\nThe field studies input and output signals, and the mathematical representations between them known as systems, in four domains: Time, Frequency, \"s\" and \"z\". Since signals and systems are both studied in these four domains, there are 8 major divisions of study. As an example, when working with continuous time signals (\"t\"), one might transform from the time domain to a frequency or \"s\" domain; or from discrete time (\"n\") to frequency or \"z\" domains. Systems also can be transformed between these domains like signals, with continuous to \"s\" and discrete to \"z\".\n\nAlthough S and S falls under and includes all the topics covered in this article, as well as Analog signal processing and Digital signal processing, it actually is a subset of the field of Mathematical modeling. The field goes back to RF over a century ago, when it was all analog, and generally continuous. Today, software has taken the place of much of the analog circuitry design and analysis, and even continuous signals are now generally processed digitally. Ironically, digital signals also are processed continuously in a sense, with the software doing calculations between discrete signal \"rests\" to prepare for the next input/transform/output event.\n\nIn past EE curricula S and S, as it is often called, involved circuit analysis and design via mathematical modeling and some numerical methods, and was updated several decades ago with Dynamical systems tools including differential equations, and recently, Lagrangians. The difficulty of the field at that time included the fact that not only mathematical modeling, circuits, signals and complex systems were being modeled, but physics as well, and a deep knowledge of electrical (and now electronic) topics also was involved and required.\n\nToday, the field has become even more daunting and complex with the addition of circuit, systems and signal analysis and design languages and software, from MATLAB and Simulink to NumPy, VHDL, PSpice, Verilog and even Assembly language. Students are expected to understand the tools as well as the mathematics, physics, circuit analysis, and transformations between the 8 domains.\n\nBecause mechanical engineering topics like friction, dampening etc. have very close analogies in signal science (inductance, resistance, voltage, etc.), many of the tools originally used in ME transformations (Laplace and Fourier transforms, Lagrangians, sampling theory, probability, difference equations, etc.) have now been applied to signals, circuits, systems and their components, analysis and design in EE. Dynamical systems that involve noise, filtering and other random or chaotic attractors and repellors have now placed stochastic sciences and statistics between the more deterministic discrete and continuous functions in the field. (Deterministic as used here means signals that are completely determined as functions of time).\n\nEE taxonomists are still not decided where S&S falls within the whole field of signal processing vs. circuit analysis and mathematical modeling, but the common link of the topics that are covered in the course of study has brightened boundaries with dozens of books, journals, etc. called Signals and Systems, and used as text and test prep for the EE, as well as, recently, computer engineering exams.\n\n\n"}
{"id": "19435781", "url": "https://en.wikipedia.org/wiki?curid=19435781", "title": "Solar Collector (sculpture)", "text": "Solar Collector (sculpture)\n\nSolar Collector is an interactive light art installation in Cambridge, Ontario, Canada. It was commissioned by the Region of Waterloo and designed by Gorbet Design Inc. The sculpture consists of twelve aluminum shafts lined with solar panels and high-intensity LEDs. Each shaft is arranged to optimize its exposure to the sun at a particular time of year. The shortest, most acutely angled shaft is perpendicular to the sun at the Summer solstice (June 21) and the tallest shaft is perpendicular to the sun at the Winter solstice (December 21). Energy collected and stored during the day powers a nightly show. The general public interacts with the sculpture via its website where they design patterns and routines for the LEDs which are combined to create the nightly shows.\n\n\n\n"}
{"id": "49680984", "url": "https://en.wikipedia.org/wiki?curid=49680984", "title": "Statistical disclosure control", "text": "Statistical disclosure control\n\nStatistical disclosure control (SDC), also known as statistical disclosure limitation (SDL) or disclosure avoidance, is a technique used in data-driven research to ensure no person or organization is identifiable from the results of an analysis of survey or administrative data, or in the release of microdata. The purpose of SDC is to protect the confidentiality of the respondents and subjects of the research.\n\nThere are two main approaches to SDC: \"principles-based\" and \"rules-based.\" In principles-based systems, disclosure control attempts to uphold a specific set of fundamental principles—for example, \"no person should be identifiable in released microdata\". Rules-based systems, in contrast, are evidenced by a specific set of rules that a person performing disclosure control follows, after which the data are presumed to be safe to release. Using this taxonomy, proposed by Ritchie and Elliot in 2013, disclosure control based on differential privacy can be seen as a principles-based approach, whereas controls based on de-identification, such as the US Health Insurance Portability and Accountability Act's Privacy Rule's Safe Harbor method for de-identifying protected health information can be seen as a rule-based system.\n\nMany kinds of social, economic and health research use potentially sensitive data as a basis for their research, such as survey or Census data, tax records, health records, educational information, etc. Such information is usually given in confidence, and, in the case of administrative data, not always for the purpose of research.\n\nResearchers are not usually interested in information about one single person or business; they are looking for trends among larger groups of people. However, the data they use is, in the first place, linked to individual people and businesses, and SDC ensures that these cannot be identified from published data, no matter how detailed or broad.\n\nIt is possible that at the end of data analysis, the researcher somehow singles out one person or business through their research. For example, a researcher may identify the exceptionally good or bad service in a geriatric department within a hospital in a remote area, where only one hospital provides such care. In that case, the data analysis 'discloses' the identity of the hospital, even if the dataset used for analysis was properly anonymised or de-identified.\n\nStatistical disclosure control will identify this disclosure risk and ensure the results of the analysis are altered to protect confidentiality. It requires a balance between protecting confidentiality and ensuring the results of the data analysis are still useful for statistical research.\n\nIn rules-based SDC, a rigid set of rules is used to determine whether or not the results of data analysis can be released. The rules are applied consistently, which makes it obvious what kinds of output are acceptable. However, because the rules are inflexible, either disclosive information may still slip through, or the rules are overrestrictive and may only allow for results that are too broad for useful analysis to be published.\n\nThe Northern Ireland Statistics and Research Agency uses a rules-based approach to releasing statistics and research results.\n\nIn principles-based SDC, both the researcher and the output checker are trained in SDC. They receive a set of rules, which are rules-of-thumb rather than hard rules as in rules-based SDC. This means that in principle, any output may be approved or refused. The rules-of-thumb are a starting point for the researcher and explain from the beginning which outputs would be deemed safe and non-disclosive, and which outputs are unsafe. It is up to the researcher to prove that any 'unsafe' outputs are non-disclosive, but the checker has the final say. Since there are no hard rules, this requires specialist knowledge on disclosure risks from both the researcher and the checker. It encourages the researcher to produce safe results in the first place. However, this also means that the outcome may be inconsistent and uncertain. It requires extensive training and a high understanding of statistics and data analysis.\n\nThe UK Data Service employs a principles-based approach to statistical disclosure control.\n\nMany contemporary statistical disclosure control techniques, such as generalization and cell suppression, have been shown to be vulnerable to attack by a hypothetical data intruder. For example, Cox showed in 2009 that Complementary cell suppression typically leads to \"over-protected\" solutions because of the need to suppress both primary and complementary cells, and even then can lead to the compromise of sensitive data when exact intervals are reported.\n\n"}
{"id": "1745325", "url": "https://en.wikipedia.org/wiki?curid=1745325", "title": "Survivorship bias", "text": "Survivorship bias\n\nSurvivorship bias or survival bias is the logical error of concentrating on the people or things that made it past some selection process and overlooking those that did not, typically because of their lack of visibility. This can lead to false conclusions in several different ways. It is a form of selection bias.\n\nSurvivorship bias can lead to overly optimistic beliefs because failures are ignored, such as when companies that no longer exist are excluded from analyses of financial performance. It can also lead to the false belief that the successes in a group have some special property, rather than just coincidence (correlation proves causality). For example, if three of the five students with the best college grades went to the same high school, that can lead one to believe that the high school must offer an excellent education. This could be true, but the question cannot be answered without looking at the grades of all the other students from that high school, not just the ones who \"survived\" the top-five selection process\n\nIn finance, survivorship bias is the tendency for failed companies to be excluded from performance studies because they no longer exist. It often causes the results of studies to skew higher because only companies which were successful enough to survive until the end of the period are included. For example, a mutual fund company's selection of funds today will include only those that are successful now. Many losing funds are closed and merged into other funds to hide poor performance. In theory, 90% of extant funds could truthfully claim to have performance in the first quartile of their peers, if the peer group includes funds that have closed. \n\nIn 1996, Elton, Gruber, and Blake showed that survivorship bias is larger in the small-fund sector than in large mutual funds (presumably because small funds have a high probability of folding). They estimate the size of the bias across the U.S. mutual fund industry as 0.9% per annum, where the bias is defined and measured as:\n\nAdditionally, in quantitative backtesting of market performance or other characteristics, survivorship bias is the use of a current index membership set rather than using the actual constituent changes over time. Consider a backtest to 1990 to find the average performance (total return) of S&P 500 members who have paid dividends within the previous year. To use the current 500 members only and create a historical equity line of the total return of the companies that met the criteria would be adding survivorship bias to the results. S&P maintains an index of healthy companies, removing companies that no longer meet their criteria as a representative of the large-cap U.S. stock market. Companies that had healthy growth on their way to inclusion in the S&P 500 would be counted as if they were in the index during that growth period, which they were not. Instead there may have been another company in the index that was losing market capitalization and was destined for the S&P 600 Small-cap Index that was later removed and would not be counted in the results. Using the actual membership of the index and applying entry and exit dates to gain the appropriate return during inclusion in the index would allow for a bias-free output.\n\nMichael Shermer in \"Scientific American\" and Larry Smith of the University of Waterloo have described how advice about commercial success distorts perceptions of it by ignoring all of the businesses and college dropouts that failed. Journalist and author David McRaney observes that the \"advice business is a monopoly run by survivors. When something becomes a non-survivor, it is either completely eliminated, or whatever voice it has is muted to zero\".\n\nIn his book \"The Black Swan\", financial writer Nassim Taleb called the data obscured by survivorship bias \"silent evidence.\"\n\nDiogenes was asked concerning paintings of those who had escaped shipwreck: \"Look, you who think the gods have no care of human things, what do you say to so many persons preserved from death by their especial favour?\", to which Diogenes replied: \"Why, I say that their pictures are not here who were cast away, who are by much the greater number.\" \n\nSusan Mumm has described how survival bias leads historians to study organisations that are still in existence more than those which have closed. This means large, successful organisations such as the Women's Institute, which were well organised and still have accessible archives for historians to work from, are studied more than smaller charitable organisations, even though these may have done a great deal of work.\n\nA commonly held opinion in many populations is that machinery, equipment, and goods manufactured in previous generations often is better built and lasts longer than similar contemporary items. (This perception is reflected in the common expression \"They don't make 'em [them] like they used to\"). Again, because of the selective pressures of time and use, it is inevitable that only those items which were built to last will have survived into the present day. Therefore, most of the old machinery still seen functioning well in the present day must necessarily have been built to a standard of quality necessary to survive. All of the machinery, equipment, and goods that have failed over the intervening years are no longer visible to the general population as they have been junked, scrapped, recycled, or otherwise disposed of. \n\nThough survivorship bias may explain a significant portion of the common perception that older manufacturing processes were more rigorous, there are other processes that may explain that perception, such as planned obsolescence and overengineering. It is difficult to directly compare and determine whether manufacturing has become overall better or worse. Manufactured goods are constantly changing, the same items are rarely built for more than a single generation, and even the raw materials change from one era to the next. Capabilities and processes in materials science, technology, manufacturing, and testing have all advanced immensely since the 20th century, undoubtedly raising the potential for similar increases in durability, but pressures on production costs and time have also increased, resulting in manufacturing shortcuts that often result in less durable products. Overall, the contemporary consumer probably has access to and experiences a much wider range of product durability than past generations. Again, bias arises from the fact that historical goods of poor quality are no longer visible, and only the best produced items of the past survive to today.\n\nJust as new buildings are being built every day and older structures are constantly torn down, the story of most civil and urban architecture involves a process of constant renewal, renovation, and revolution. Only the most (subjectively, but popularly determined) beautiful, most useful, and most structurally sound buildings survive from one generation to the next. This creates another selection effect where the ugliest and weakest buildings of history have long been eradicated from existence and thus the public view, and so it leaves the visible impression, seemingly correct but factually flawed, that all buildings in the past were both more beautiful and better built.\n\nWhether it be movie stars, or athletes, or musicians, or CEOs of multibillion-dollar corporations who dropped out of school, popular media often tells the story of the determined individual who pursues their dreams and beats the odds. There is much less focus on the many people that may be similarly skilled and determined but fail to ever find success because of factors beyond their control or other (seemingly) random events. This creates a false public perception that anyone can achieve great things if they have the ability and make the effort. The overwhelming majority of failures are not visible to the public eye, and only those who survive the selective pressures of their competitive environment are seen regularly.\n\nDuring World War II, the statistician Abraham Wald took survivorship bias into his calculations when considering how to minimize bomber losses to enemy fire. Researchers from the Center for Naval Analyses had conducted a study of the damage done to aircraft that had returned from missions, and had recommended that armor be added to the areas that showed the most damage. Wald noted that the study only considered the aircraft that had \"survived\" their missions—the bombers that had been shot down were not present for the damage assessment. The holes in the returning aircraft, then, represented areas where a bomber could take damage and still return home safely. Wald proposed that the Navy reinforce areas where the returning aircraft were unscathed, since those were the areas that, if hit, would cause the plane to be lost. His work is considered seminal in the then-fledgling discipline of operational research.\n\nIn a study performed in 1987 it was reported that cats who fall from less than six stories, and are still alive, have greater injuries than cats who fall from higher than six stories. It has been proposed that this might happen because cats reach terminal velocity after righting themselves at about five stories, and after this point they relax, leading to less severe injuries in cats who have fallen from six or more stories.\n\nIn 2008, \"The Straight Dope\" newspaper column proposed that another possible explanation for this phenomenon would be survivorship bias. Cats that die in falls are less likely to be brought to a veterinarian than injured cats, and thus many of the cats killed in falls from higher buildings are not reported in studies of the subject.\n\nTropical vines and lianas are often viewed as macro-parasites of trees that reduce host tree survival. The proportion of trees infested with lianas was observed to be much greater in shade-tolerant, heavy wooded, slow-growing tree species while light-demanding, lighter wooded and fast-growing species are often liana free. Such observations led to the expectation that lianas have stronger negative effects on shade-tolerant species . However, further investigations revealed that liana infestation is far more harmful to light-demanding fast-growing tree species where liana infestation greatly decreases survival such that the observable sample is biased towards those that survived and are liana-free . Hence, the observable sample of trees with lianas in their crown is skewed due to survivorship bias.\n\nLarge groups of organisms called clades that survive a long time are subject to various survivorship biases such as the \"push of the past\", generating the illusion that clades in \"general\" tend to originate with a high rate of diversification that then slows through time.\n\nSurvivorship bias (or survivor bias) is a statistical artifact in applications outside finance, where studies on the remaining population are fallaciously compared with the historic average despite the survivors having unusual properties. Mostly, the unusual property in question is a track record of success (like the successful funds).\n\nFor example, the parapsychology researcher Joseph Banks Rhine believed he had identified the few individuals from hundreds of potential subjects who had powers of ESP. His calculations were based on the improbability of these few subjects guessing the Zener cards shown to a partner by chance.\n\nA major criticism which surfaced against his calculations was the possibility of unconscious survivorship bias in subject selections. He was accused of failing to take into account the large effective size of his sample (all the people he rejected as not being \"strong telepaths\" because they failed at an earlier testing stage). Had he done this he might have seen that, from the large sample, one or two individuals would probably achieve the track record of success he had found purely by chance.\n\nWriting about the Rhine case in \"Fads and Fallacies in the Name of Science\", Martin Gardner explained that he did not think the experimenters had made such obvious mistakes out of statistical naïveté, but as a result of subtly disregarding some poor subjects. He said that, without trickery of any kind, there would always be some people who had improbable success, if a large enough sample were taken. To illustrate this, he speculates about what would happen if one hundred professors of psychology read Rhine's work and decided to make their own tests; he said that survivor bias would winnow out the typical failed experiments, but encourage the lucky successes to continue testing. He thought that the common null hypothesis (of no result) would not be reported, but:\nHe concludes:\n\nIf enough scientists study a phenomenon, some will find statistically significant results by chance, and these are the experiments submitted for publication. Additionally, papers showing positive results may be more appealing to editors. This problem is known as \"positive results bias\", a type of publication bias. To combat this, some editors now call for the submission of \"negative\" scientific findings, where \"nothing happened\".\n\nSurvivorship bias is one of the issues discussed in the provocative 2005 paper \"Why Most Published Research Findings Are False\".\n\nSurvivorship bias can raise truth-in-advertising problems when the success rate advertised for a product or service is measured with respect to a population whose makeup differs from that of the target audience whom the company offering that product or service targets with advertising claiming that success rate. These problems become especially significant when\n\nFor example, the advertisements of online dating service eHarmony.com pass this test because they fail the first two prongs but not the third: They claim a success rate significantly higher than that of competing services while generally not disclosing that the rate is calculated with respect to a viewership subset who possess traits that increase their likelihood of finding and maintaining relationships and lack traits that pose obstacles to their doing so (1), and the company deliberately selects for these traits by administering a lengthy pre-screening process designed to reject prospective customers who lack the former traits or possess the latter ones (2), but the company does not charge a fee for administration of its pre-screening test, with the effect that its prospective customers face no \"downside risk\" other than losing the time and expending the effort involved in completing the pre-screening process (negating 3).\n\n"}
{"id": "2249594", "url": "https://en.wikipedia.org/wiki?curid=2249594", "title": "The Nature of the Judicial Process", "text": "The Nature of the Judicial Process\n\nThe Nature of the Judicial Process is a legal classic written by Associate Justice of the United States Supreme Court, and New York Court of Appeals Chief Justice Benjamin N. Cardozo in 1921. It was compiled from The Storrs Lectures delivered at Yale Law School earlier that year.\n\nThe central question of \"The Nature of the Judicial Process\" is how judges should decide cases. Cardozo’s answer is that judges should do what they have always done in the Anglo-American legal tradition, namely, follow and apply the law in easy cases, and make new law in hard cases by balancing competing considerations, including the paramount value of social welfare. Cardozo identifies four leading methods of legal analysis: (1) the method of logic (or “analogy,” or “philosophy”), which seeks to extend legal principles in ways that preserve logical consistency; (2) the method of history (or “evolution”), which adverts to the historical origins of the legal rule or concept; (3) the method of custom (or “tradition”), which views social customs as helpful guides to community values and settled expectations; and (4) the method of sociology, which looks to considerations of reason, justice, utility, and social welfare. Each of these methods may have their “preponderating value” in particular cases. No simple test or rigid formula can decide which method should prevail in a given case. But in difficult cases where a legal rule is outmoded or the law contains “gaps” that must be filled, judges should frankly play the role of legislators and let “the welfare of society fix the path.”\n\nCardozo admits that there are risks in judicial lawmaking. To minimize these, he points to a number of factors that significantly limit judicial discretion. First, judges may make new law only “interstitially,” that is, when the law contains gaps or a legal rule is clearly obsolete. Second, judges in their exercise of judicial review should never strike down a law unless it is “so plainly arbitrary and oppressive that right-minded men and women could not reasonably regard” it otherwise. Finally, when judges invoke norms such as “reason,” “justice,” or “social advantage” when employing the method of sociology, they should look to community standards rather than to their own personal values. In the Anglo-American system of law, Cardozo remarks, a judge “is not a knight-errant, roaming at will in pursuit of his own ideal of beauty or of goodness.” In the final balance, a judge’s freedom to innovate is insignificant “compared with the bulk and pressure of the rules that hedge him on every side.”\n\nIn claiming that judges do and must make law, Cardozo was siding with Oliver Wendell Holmes, Jr., Roscoe Pound, John Chipman Gray, and other American “proto-realists” of his day who were challenging the traditional “oracular” or “mechanical” or “formalist” view of judicial reasoning. On that view, judges never make law, they simply discover pre-existing law and apply it. According to strict formalists, there are no hard cases where the law is silent, or ambiguous, or vague, or contradictory, or couched in broad generalities. Rather, the law is clear, consistent, and complete; all legal questions have a single correct answer; and judges are (in Blackstone's phrase) “living oracles” who deduce inexorable legal conclusions from indisputable legal axioms.\n\nFormalism was not Cardozo’s only target in \"The Nature of the Judicial Process\". He also attacked radical critics of formalism, such as John Chipman Gray, who claimed that judges have immense freedom and rejected the very idea of law as a set of binding rules. Gray and other proto-realists of the time tried to demystify law and view it with hard-headed pragmatism. They argued that since judges are the ultimate arbiters of law, “law” in the final analysis is whatever judges say it is (or what they predictably will say it is in the future). Cardozo argued that this isn’t an “analysis” of law, but a denial that any true law exists. The proto-realists confuse right with power. Judges may have the power to ignore settle legal standards, but they do not have the right. Moreover, the attempt to identify law with judicial rulings ignores the fact that the great majority of legal questions have clear, uncontroversial answers that guide everyday conduct and are never litigated in courts.\n\n\"The Nature of the Judicial Process\" established Cardozo “as one of the leading jurists of his time” and “has become a classic of legal education.\" Its continuing appeal is due, in part, to its self-effacing tone, its lapidary prose, and its attempt to strike a happy medium between legal formalism and radical realist theories that wholly reject traditional views of law, legal reasoning, judicial restraint, and the rule of law.\n\nThe great success of Cardozo’s \"The Nature of the Judicial Process\" created demand for further reflections on the law. In two later works, \"The Growth of Law\" (1924) and \"The Paradoxes of Legal Science\" (1927), Cardozo refined, deepened, and to some extent modified the views of law laid out in \"The Nature of the Judicial Process\".\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "31480", "url": "https://en.wikipedia.org/wiki?curid=31480", "title": "Theosophy (Boehmian)", "text": "Theosophy (Boehmian)\n\nTheosophy, also known as Christian theosophy and Boehmian theosophy, refers to a range of positions within Christianity which focus on the attainment of direct, unmediated knowledge of the nature of divinity and the origin and purpose of the universe.\nThey have been characterized as mystical and occultist philosophies. Theosophy is considered part of Western esotericism, which believes that hidden knowledge or wisdom from the ancient past offers a path to enlightenment and salvation.\n\nThe foundation of Christian theosophy is usually attributed to the German philosopher Jakob Böhme.\n\nIn 1875, the term \"theosophy\" was adopted and revived by the Theosophical Society, an esoteric organisation which spawned a spiritual movement also called Theosophy. In the twentieth century, theosophy became the object of study for various scholars of Western esotericism.\n\nTheosophy comes from the Greek ' (), which combines ' (θεός), \"God\" and \"\" (), \"wisdom\". Its etymological meaning is thus \"wisdom of God.\" \n\nThe term \"theosophia\" appeared (in both Greek and Latin) in the works of early church fathers, as a synonym for theology: the \"theosophoi\" are \"those knowing divine things\". The term however acquired various other meanings throughout its history. The adjective \"theosophos\" (θεόσοφος) \"wise in divine things\" was applied by Iamblichus to the gymnosophists (Γυμνοσοφισταί), i.e. the Indian yogis or sadhus.\n\nScholars of esotericism such as Godwin and Faivre differentiated the tradition of religious illumination from the religious system established in the late nineteenth century by Helena Blavatsky by referring to the latter with a capital letter as Theosophy, and the former with a lower-case letter as theosophy. Followers of Blavatsky's movement are known as Theosophists, while adherents of the older tradition are termed theosophers. Causing some confusion was the fact that a few Theosophists — such as C. C. Massey — were also theosophers.\n\nAntoine Faivre suggested that it be called \"Boehmian theosophy\".\n\nThe term theosophy was used as a synonym for theology as early as the 3rd century CE. The 13th century work \"Summa philosophiae\" attributed to Robert Grosseteste made a distinction between theosophers and theologians. In Summa, theosophers were described as authors only inspired by the holy books, while theologians like Pseudo-Dionysius the Areopagite and Origen were described as persons whose task was to explain theosophy. Therefore, the terms were the opposite of the present-day meaning.\n\nDuring the Renaissance, use of the term diverged to refer to gnostic knowledge that offers the individual enlightenment and salvation through a knowledge of the bonds that are believed to unite her or him to the world of divine or intermediary spirits. Christian theosophy arose in Germany in the 16th century. Inspired to a considerable extent by the works of Paracelsus (1493–1541). The term had not yet reached a settled meaning, however, as the mid-16th century \"Theosophia\" by Johannes Arboreus provided a lengthy exposition that included no mention of esotericism.\n\nIn the seventeenth and eighteenth centuries, Christian theosophy and Pietism arose in response to the orthodoxy of the Lutheran Reformation.\n\nThe work of the 17th-century German Christian mystic Jakob Boehme (1575–1624) strongly contributed to spread the use of the word \"theosophy\", even though Boehme rarely used the word in his writings. It is on account of the title of some of his works, but these titles appear to have been chosen more by the editors than by Boehme himself. Moreover, Boehme gave the word \"theosophy\" a limited meaning, making it clear that he was not conflating nature with God. Boehme's work has been described as being \"foundational\" for Christian theosophy. There were relatively few theosophers in the 17th century, but many of them were prolific. Outside of Germany, there were also theosophers from Holland, England, and France. This group is represented by Jan Baptist van Helmont (1618–1699), Robert Fludd (1574–1637), John Pordage (1608–1681), Jane Leade (1623–1704), Henry More (1614–1687), Pierre Poiret (1646–1719), and Antoinette Bourignon (1616–1680). Theosophers of this period often inquired into nature using a method of interpretation founded upon a specific myth or revelation, applying active imagination in order to draw forth symbolic meanings and further their pursuit of knowledge toward a complete understanding of these mysteries. In Athanasius Kircher's \"Oedipus Aegyptiacus\" (1652), Kircher assigned the word theosophy to the metaphysics adhered to in ancient Egypt, and to Neo-Platonism, and thus he gave once again the word one of its most generally accepted meanings, that of divine metaphysics.\n\nIn the 18th century, the word theosophy came into more widespread use among some philosophers. However, the term \"theosophy\" was still \"practically absent\" throughout the entire eighteenth century in dictionaries and encyclopedias, where it only appeared more and more frequently beginning in the second half of the nineteenth century. Theosophers themselves used the word theosophy sparingly, at least up until the middle of the nineteenth century. Johann Jakob Brucker (1696–1770) included a long chapter on theosophy in his monumental work (1741). He included theosophers alongside other currents in esotericism in what was then a standard reference in the history of philosophy. By the 18th century, the word \"theosophy\" was often used in conjunction with The term \"theosophy\" is more properly reserved for the reverse process of contemplating the divine in order to discover the content of the concrete universe.\n\nIn England, Robert Hindmarsh, a printer with a Methodist background, formed a \"Theosophical Society\" in 1783, for translating, printing and distributing the writings of Swedenborg. This society was renamed in 1785 as \"The British Society for the Propagation of the Doctrines of the New Church\", consisting of Swedenborgian based beliefs. In France, Denis Diderot gave the word \"\" more attention than other encyclopedias of this period by including an article on it in his \"Encyclopédie\", published during the French Enlightenment. The article dealt mostly with Paracelsus and essentially plagiarized Brucker's \"Historia\".\n\nGroups such as the Martinist Order founded by Papus in 1891, followed the theosophical current closely linked to the Judeo-Christian-Islamic tradition and Western esotericism. Theosophers outside of the initiate societies included people such as Vladimir Solovyov (1853–1900), whose views have been described as follows: \"although empiricism and rationalism rest on false principles, their respective objective contents, external experience, qua the foundation of natural science, and logical thought, qua the foundation of pure philosophy, are to be synthesized or encompassed along with mystical knowledge in 'integral knowledge,' what Solovyov terms 'theosophy.'\"\n\nFaivre stated that \"Theosophy is a gnosis that has a bearing not only on the salvific relations the individual maintains with the divine world, but also on the nature of God Himself, or of divine persons, and on the natural universe, the origin of that universe, the hidden structures that constitute it in its actual state, its relationship to mankind, and its final ends.\"\n\nTheosophy actually designates a specific flow of thought or tradition within the modern study of esotericism. Thus, it follows the path starting from the more modern period of the 15th century onward. Faivre describes the \"theosophic current\" or theosophy as a single esoteric current among seven other esoteric currents in early modern Western thought (i.e., alchemy, astrology, Neo-Alexandrian Hermeticism, Christian Kabbalah, Paracelsism (i.e., the studying of the \"prognostications\" of Paracelsus), philosophia occulta and Rosicrucianism). \n\nFaivre noted that there are \"obvious similarities\" between earlier theosophy and modern Theosophy as both play an important part in Western esotericism and both claim to deal with wisdom from a gnostic perspective. But he says there are also differences, since they do not actually rely on the same reference works; and their style is different. The referential corpus of earlier theosophy \"belongs essentially to the Judeo-Christian type\", while that of modern Theosophy \"reveals a more universal aspect\". Although there are many differences between Christian theosophy and the Theosophical movement begun by Helena Blavatsky, the differences \"are not important enough to cause an insurmountable barrier\".\n\nTheosophers engage in analysis of the universe, humanity, divinity, and the reciprocal effects of each on the other. The starting point for theosophers may be knowledge of external things in the world or inner experiences and the aim of the theosopher is to discover deeper meanings in the natural or divine realm. Antoine Faivre notes, \"the theosophist dedicates his energy to inventing (in the word's original sense of 'discovering') the articulation of all things visible and invisible, by examining both divinity and nature in the smallest detail.\" The knowledge that is acquired through meditation is believed to change the being of the meditator.\n\nFaivre identified three characteristics of theosophy. The three characteristics of theosophy are listed below.\n\n\"Theosophy\":\n\nThe scholar of esotericism Wouter Hanegraaff described Christian theosophy as \"one of the major currents in the history of Western esotericism\". \n\nChristian theosophy is an under-researched area; a general history of it has never been written. The French scholar Antoine Faivre had a specific interest in the theosophers and illuminists of the eighteenth and nineteenth centuries. He wrote his doctoral thesis on Karl von Eckartshausen and Christian theosophy.\nScholars of esotericism have argued that Faivre's definition of Western esotericism relies on his own specialist focus on Christian theosophy, Renaissance Hermeticism, and Romantic \"Naturphilosophie\" and therefore creates an \"ideal\" type of esotericism that does not suit all esoteric currents.\n\n"}
{"id": "12528280", "url": "https://en.wikipedia.org/wiki?curid=12528280", "title": "Transferable utility", "text": "Transferable utility\n\nTransferable utility is a concept in cooperative game theory and in economics. Utility is \"transferable\" if one player can losslessly transfer part of its utility to another player. Such transfers are possible if the players have a common currency that is valued equally by all. Note that being able to transfer cash payoffs does not imply that utility is transferable: wealthy and poor players may derive a different utility from the same amount of money.\n\nTransferable utility is assumed in many cooperative games, where the payoffs are not given for individual players, but only for coalitions. In this case the assumption implies that irrespective of the division of the coalitional payoff, members of the coalition enjoy the same total utility.\n"}
{"id": "32169", "url": "https://en.wikipedia.org/wiki?curid=32169", "title": "Unified Modeling Language", "text": "Unified Modeling Language\n\nThe Unified Modeling Language (UML) is a general-purpose, developmental, modeling language in the field of software engineering, that is intended to provide a standard way to visualize the design of a system.\n\nThe creation of UML was originally motivated by the desire to standardize the disparate notational systems and approaches to software design. It was developed by Grady Booch, Ivar Jacobson and James Rumbaugh at Rational Software in 1994–1995, with further development led by them through 1996.\n\nIn 1997 UML was adopted as a standard by the Object Management Group (OMG), and has been managed by this organization ever since. In 2005 UML was also published by the International Organization for Standardization (ISO) as an approved ISO standard. Since then the standard has been periodically revised to cover the latest revision of UML.\n\nUML has been evolving since the second half of the 1990s and has its roots in the object-oriented programming methods developed in the late 1980s and early 1990s. The timeline (see image) shows the highlights of the history of object-oriented modeling methods and notation.\n\nIt is originally based on the notations of the Booch method, the object-modeling technique (OMT) and object-oriented software engineering (OOSE), which it has integrated into a single language.\n\nRational Software Corporation hired James Rumbaugh from General Electric in 1994 and after that the company became the source for two of the most popular object-oriented modeling approaches of the day: Rumbaugh's object-modeling technique (OMT) and Grady Booch's method. They were soon assisted in their efforts by Ivar Jacobson, the creator of the object-oriented software engineering (OOSE) method, who joined them at Rational in 1995.\n\nUnder the technical leadership of those three (Rumbaugh, Jacobson and Booch), a consortium called the UML Partners was organized in 1996 to complete the \"Unified Modeling Language (UML)\" specification, and propose it to the Object Management Group (OMG) for standardisation. The partnership also contained additional interested parties (for example HP, DEC, IBM and Microsoft). The UML Partners' UML 1.0 draft was proposed to the OMG in January 1997 by the consortium. During the same month the UML Partners formed a group, designed to define the exact meaning of language constructs, chaired by Cris Kobryn and administered by Ed Eykholt, to finalize the specification and integrate it with other standardization efforts. The result of this work, UML 1.1, was submitted to the OMG in August 1997 and adopted by the OMG in November 1997.\n\nAfter the first release a task force was formed to improve the language, which released several minor revisions, 1.3, 1.4, and 1.5.\n\nThe standards it produced (as well as the original standard) have been noted as being ambiguous and inconsistent.\n\nAs with database Chen, Bachman, and ISO ER diagrams, class models are specified to use \"look-across\" cardinalities, even though several authors (Merise, Elmasri & Navathe amongst others) prefer same-side or \"look-here\" for roles and both minimum and maximum cardinalities. Recent researchers (Feinerer, Dullea et al.) have shown that the \"look-across\" technique used by UML and ER diagrams is less effective and less coherent when applied to \"n\"-ary relationships of order strictly greater than 2.\n\nFeinerer says: \"Problems arise if we operate under the look-across semantics as used for UML associations. Hartmann investigates this situation and shows how and why different transformations fail.\", and: \"As we will see on the next few pages, the look-across interpretation introduces several difficulties which prevent the extension of simple mechanisms from binary to \"n\"-ary associations.\"\n\nUML 2.0 major revision replaced version 1.5 in 2005, which was developed with an enlarged consortium to improve the language further to reflect new experience on usage of its features.\n\nAlthough UML 2.1 was never released as a formal specification, versions 2.1.1 and 2.1.2 appeared in 2007, followed by UML 2.2 in February 2009. UML 2.3 was formally released in May 2010. UML 2.4.1 was formally released in August 2011. UML 2.5 was released in October 2012 as an \"In progress\" version and was officially released in June 2015. Formal version 2.5.1 was adopted in December 2017.\n\nThere are four parts to the UML 2.x specification:\n\n\nThe current versions of these standards are:\n\nIt continues to be updated and improved by the revision task force, who resolve any issues with the language.\n\nUML offers a way to visualize a system's architectural blueprints in a diagram, including elements such as:\n\n\nAlthough originally intended for object-oriented design documentation, UML has been extended to a larger set of design documentation (as listed above), and been found useful in many contexts.\n\nUML is not a development method by itself; however, it was designed to be compatible with the leading object-oriented software development methods of its time, for example OMT, Booch method, Objectory and especially RUP that it was originally intended to be used with when work began at Rational Software.\n\nIt is important to distinguish between the UML model and the set of diagrams of a system. A diagram is a partial graphic representation of a system's model. The set of diagrams need not completely cover the model and deleting a diagram does not change the model. The model may also contain documentation that drives the model elements and diagrams (such as written use cases).\n\nUML diagrams represent two different views of a system model:\n\n\nUML models can be exchanged among UML tools by using the XML Metadata Interchange (XMI) format.\n\nIn UML, one of the key tools for behavior modelling is the use-case model, caused by OOSE. Use cases are a way of specifying required usages of a system. Typically, they are used to capture the requirements of a system, that is, what a system is supposed to do.\n\nUML 2 has many types of diagrams, which are divided into two categories. Some types represent \"structural\" information, and the rest represent general types of \"behavior\", including a few that represent different aspects of \"interactions\". These diagrams can be categorized hierarchically as shown in the following class diagram:\n\nThese diagrams may all contain comments or notes explaining usage, constraint, or intent.\n\nStructure diagrams emphasize the things that must be present in the system being modeled. Since structure diagrams represent the structure, they are used extensively in documenting the software architecture of software systems. For example, the component diagram describes how a software system is split up into components and shows the dependencies among these components.\n\nBehavior diagrams emphasize what must happen in the system being modeled. Since behavior diagrams illustrate the behavior of a system, they are used extensively to describe the functionality of software systems. As an example, the activity diagram describes the business and operational step-by-step activities of the components in a system.\n\nInteraction diagrams, a subset of behavior diagrams, emphasize the flow of control and data among the things in the system being modeled. For example, the sequence diagram shows how objects communicate with each other regarding a sequence of messages.\n\nThe Object Management Group (OMG) has developed a metamodeling architecture to define the UML, called the Meta-Object Facility. MOF is designed as a four-layered architecture, as shown in the image at right. It provides a meta-meta model at the top, called the M3 layer. This M3-model is the language used by Meta-Object Facility to build metamodels, called M2-models.\n\nThe most prominent example of a Layer 2 Meta-Object Facility model is the UML metamodel, which describes the UML itself. These M2-models describe elements of the M1-layer, and thus M1-models. These would be, for example, models written in UML. The last layer is the M0-layer or data layer. It is used to describe runtime instances of the system.\n\nThe meta-model can be extended using a mechanism called stereotyping. This has been criticised as being insufficient/untenable by Brian Henderson-Sellers and Cesar Gonzalez-Perez in \"Uses and Abuses of the Stereotype Mechanism in UML 1.x and 2.0\".\n\nUML has been marketed for many contexts.\n\nIt has been treated, at times, as a design silver bullet, which leads to problems. UML misuse includes overuse (designing every part of the system with it, which is unnecessary) and assuming that novices can design with it.\n\nIt is considered a large language, with many constructs. Some people (including Jacobson) feel that UML's size hinders learning (and therefore, using) it.\n\n\n"}
{"id": "8344429", "url": "https://en.wikipedia.org/wiki?curid=8344429", "title": "Value stream mapping", "text": "Value stream mapping\n\nValue-stream mapping is a lean-management method for analyzing the current state and designing a future state for the series of events that take a product or service from its beginning through to the customer with reduced lean wastes as compared to current map. A value stream focuses on areas of a firm that add value to a product or service, whereas a value chain refers to all of the activities within a company. At Toyota, it is known as \"material- and information-flow mapping\".\n\nThe purpose of value stream mapping is to identify and remove or reduce \"waste\" in value streams, thereby increasing the efficiency of a given value stream. Waste removal is intended to increase productivity by creating leaner operations which in turn make waste and quality problems easier to identify.\n\nDaniel T. Jones (1995) identifies seven commonly accepted types of waste. These terms are updated from the Toyota production system (TPS)'s original nomenclature:\n\nMonden (1994) identifies three types of operations:\n\nValue-stream mapping has supporting methods that are often used in Lean environments to analyze and design flows at the system level (across multiple processes).\n\nAlthough value-stream mapping is often associated with manufacturing, it is also used in logistics, supply chain, service related industries, healthcare, software development, product development, and administrative and office processes.\n\nIn a build-to-the-standard form, Shigeo Shingo suggests that the value-adding steps be drawn across the centre of the map and the non–value-adding steps be represented in vertical lines at right angles to the value stream. Thus, the activities become easily separated into the value stream, which is the focus of one type of attention, and the 'waste' steps, another type. He calls the value stream the process and the non-value streams the operations. The thinking here is that the non–value-adding steps are often preparatory or tidying up to the value-adding step and are closely associated with the person or machine/workstation that executes that value-adding step. Therefore, each vertical line is the 'story' of a person or workstation whilst the horizontal line represents the 'story' of the product being created.\n\nValue stream mapping is a recognised method used as part of Six Sigma methodologies.\n\nThe success of Lean in manufacturing and production has led to an interest in its adoption in software development. However, it was noted that the current literature on adoption of Lean in software development had a disconnect between the high-level principles and the concrete practices related to lean and agile software development. The literature also had a limited focus on wastes that were literally mapped from the categories identified for manufacturing. This was ignoring the transformation that lean thinking has itself undergone and moved away from the focus on \"removal of waste\" to \"creating and delivering value\". The use of value stream mapping as suggested by the pioneer authors of the field Womack and Jones was identified as the missing link in the current literature on lean in software development.\n\nValue-stream mapping analyzes both material (artifact) and information flow. The following two resources exemplify the use of VSM in the context of software process improvement in industrial settings:\n\n\nHines and Rich (1997) defined seven value stream mapping tools they are:\n\n\n"}
{"id": "26743030", "url": "https://en.wikipedia.org/wiki?curid=26743030", "title": "Virion Screen Project", "text": "Virion Screen Project\n\nVirion is a screen based digital art exhibition that links to public sites across Brisbane, focused in the Kelvin Grove Urban Screen Network. The Virion Screen Project will be held from the 19 July – 1 of August.\n\nDeveloped by Rachael Parsons, Virion will explore the internet’s potential to support a democratic, networked arts culture, by employing an open curatorial process that allows members of the general public to actively contribute to the exhibitions’ content and construction. Virion seeks to maximize & diversify Brisbane audiences’ exposure to new media practices by screening works in non-traditional & traditional locations each creating a unique viewing experience. Virion strives to make vital connections between contemporary art, the general public and the local community and environment.\n\nUsing the Virion Website , internet users are invited to upload an image, short video work or sound work that will be exhibited during the exhibition both on selected screen sites and the Virion website. The exhibition is open to all users, there is no selection criteria (the exception being explicit material that may be inappropriate for public spaces) and each work is given equal showing time. Users are able to select the screen they want their work to be shown on . The ability to upload content will remain open during the exhibition period. Virion provides artists the opportunity to display their work across a network of public screens and access a diverse audience. The exhibition is open to all users from professional and emerging artists to people experimenting with cameras and scanners. Submissions may be in the form of digital stills or video files up to 1 minute each. Throughout the exhibition the audience is invited to respond and participate by contributing further content to the show. Sites will be updated progressively to include all work. Online Artist submissions are now open. To be included in the opening of the exhibition submissions must be received by July 14 .\n\nEach screen will play a compilation of diverse images and video that represent a wide & integrated range of local and international art practices and styles. Screens are located across a range of public, gallery and institutional sites to offer unique viewing experiences and to maximize & diversify Brisbane audiences’ exposure to new media practices.\n\nThe KGUV Screens Network is a unique and innovative method of sharing visual art and creative industries content out into the broader community, through a network of linked screens in Kelvin Grove and QUT. Screens are located at AXIOM Estate Agents, The Exchange, Blue Lotus, Urban Dental,Health Stream Fitness Club, QUT Health Clinics – Podiatry and Optometry, Queensland Academy for Creative Industries and the Creative Industries Precinct.\n\nThe Edge is The State Library of Queensland's newest initiative. It is a place for young Queenslanders; a place for experimentation and creativity, giving contemporary tools to young people to allow them to explore critical ideas, green initiatives, new design practices and media making. The Edge's physical component is at Brisbane's Cultural Centre at South Bank, housed in the former Cultural Centre Auditorium building and associated spaces. It's a state-of-the-art facility that provides creative spaces and innovative programs; a welcoming place, buzzing with people inventing, creating, presenting and meeting peers.\n\nH-Block Gallery, located at the centre of QUT's visual arts faculty, facilitates a program of exhibitions showcasing the works of undergraduate and postgraduate students, staff and emerging artists from QUT.\n\n"}
