{"id": "5260416", "url": "https://en.wikipedia.org/wiki?curid=5260416", "title": "1989 riots in Argentina", "text": "1989 riots in Argentina\n\nThe 1989 riots were a series of riots and related episodes of looting in stores and supermarkets in Argentina, during the last part of the presidency of Raúl Alfonsin, between May and June 1989. The riots were caused by the rampant hyperinflation and food shortage, and were associated with legal protests and demonstrations.\n\nThe first riots started in Rosario, the third-largest city in the country, when people demanded supermarkets to give away food; they quickly spread to other cities, specially in Greater Buenos Aires. The national government established a state of emergency. More than 40 people were arrested, and there were 14 dead (20 according to unofficial reports). Eventually President Alfonsín resigned, and president elect Carlos Menem took office six months in advance, in July.\nIn August 1988 the Alfonsín government launched a new economic plan, called \"Plan Primavera\", intended to contain inflation. It included price controls, negotiated with 53 leading companies, and exchange rate controls, the freezing of state workers' wages, and negotiations with the labour unions. The plan soon proved a failure. Interest rates rose uncontrollably, and the Central Bank's foreign currency reserves were depleted, as it sold U.S. dollars to preserve the value of the Argentine austral. The economic establishment withdrew deposits from the banks, withheld the dollars brought in by exports, and purposefully delayed the paying of taxes.\nDuring May 1989, the exchange rate (while fixed in theory) rose from 80 to 200 australes to the dollar. In Rosario, the inflation rate of May reached 96.5%. There was a shortage of basic products in supermarkets and stores, and their price tags were updated several times in the same day.\n\nThe results of the general elections held on 14 May 1989 were overwhelmingly favourable to the Justicialist Party. The volatile situation prompted talks about the possibility of anticipating the assumption of the president elect. In Rosario, mayor Horacio Usandizaga resigned, fulfilling a promise that he would leave office if Menem got elected.\n\nWednesday, 24 May was a bank holiday. The first isolated riots erupted in Rosario and Córdoba. On 28 May the president announced an emergency economic plan. That night the riots and episodes of looting became generalized in Rosario, especially in the southern neighbourhoods, where most of the larger supermarkets were concentrated at the time, and the next day they spread to the nearby industrial corridor and to other cities, accompanied in certain cases by road blockades and \"cacerolazos\".\nThe rioters broke into supermarkets, food stores and smaller businesses, in groups of varying size (as small as 20 people and as large as 1,000). In most cases they were young, and included a significant proportion of women and children, who doubled as willing human shields against the police. Though most were motivated by hunger and took only food, many also stole cash registers, furniture, refrigerators, etc. Common robbers as well as ostensibly middle-class people could be found among the crowd, as well as people who loaded stolen merchandise on cars and other motor vehicles. The violence was directed at the businesses, not the people, although there were some isolated incidents of owners being hurt or hurting others when trying to defend their shops, and attacks on some police stations.\nPolice action was rather passive during the first two days, which contributed to the generalization of the riots. Whether they were overwhelmed by its massive dimensions is a matter of discussion; some hypotheses point to orders from a faction of the provincial government. Some neighbours claimed that the police was merely \"guarding\" the robbers, as the security forces only shot some rounds into the air and few arrests were made.\n\nThis changed on 29 May, when the president declared a state of emergency for 30 days. The city was militarized and divided into three operational areas. School classes were suspended, banks were closed, public transportation was shut down, and a curfew was imposed.\n\nBy the beginning of June the riots ceased, as the situation was controlled by the security forces and the municipal and national governments began to deliver food assistance. The national government also ordered the creation of hundreds of soup kitchens.\nThe beginning of 1990 saw a new, albeit much smaller wave of riots, mainly February and March, in Rosario and Greater Buenos Aires. The economic crisis had not abated, and many businesses had resumed operating with physical barriers. The riots were contained quickly, again with delivery of food assistance to the poorer neighbourhoods.\n"}
{"id": "52982000", "url": "https://en.wikipedia.org/wiki?curid=52982000", "title": "Access to public information in Albania", "text": "Access to public information in Albania\n\nAccess to public information and freedom of information (FOI) refer to the right to access information held by public bodies also known as \"right to know\". Access to public information is considered of fundamental importance for the effective functioning of democratic systems, as it enhances governments' and public officials' accountability, boosting people participation and allowing their informed participation into public life. The fundamental premise of the right to access public information is that the information held by governmental institutions is in principle public and may be concealed only on the basis of legitimate reasons which should be detailed in the law.\n\nIn Albania the right to access to public information is guaranteed by the Constitution and by the Law on the Right to Information approved in 2014 and providing for a sound legal framework regulating access to public information. However, despite some significant progress toward securing the right to information over the last years, notable weakness remains in the functioning of the system and in the implementation of the law. Access is still a challenge in a country with a strong culture of secrecy and confidentiality.\n\nAlbania provides both constitutional and legal guarantees of the right to information. Efforts to establish a constitutional and legal framework securing citizens' right to access public information date back to the early 1990s during Albania's transition from a Communist country to a multi-party democratic system.\n\nThe Constitution, effective since 1998, enshrined this right in Article 23 and provides for the right of every person to access information held by state bodies and to attend public meetings. In 2004, the parliament passed the Law 8503, the \"Right to Information about Official Documents law\", becoming the first country in the region to adopt a legislation on freedom of information. Currently, access to information is regulated by the Law on the Right to Information approved in 2014, which is an upgrade of previous laws and regulations.\n\nAlbania has also committed itself to realize the right to access public information through several international agreements. In 2002, Albania ratified the Convention on access to Information, Public Participation in Decision-Making and Access to Justice in Environmental Matters (Aarhus Convention) which requires the adoption of laws on access to environmental information. Moreover, as a member of the Council of Europe, Albania has committed to comply with a 2002 recommendation on access to official document.\n\nThe new Right to Information Law of Albania has been assessed by many experts as one of the most important steps towards transparency and accountability, bringing Albanian legislation closer with the best international standards.\n\nThe new law provided greater access for the public to official documents, and sets forth a system of penalties for public officials who refused to disclose information. The new legislation includes also an array of new concepts, such as the possibility of reclassifying secret documents, the release of partial information and the use of ICTs to make information held py public bodies available and accessible to the public. The law also establishes the obligation to appoint coordinators for access to information by every public authority charged with the task of supervising the authority’s responses to information requests and created the institution of a Commissioner for the Right to Information and Protection of Personal Data charged with supervising and monitoring compliance with the law and appeals bodies and procedures in cases of refusal or partial disclosure. Under the new law the Commissioner has the faculty to use disciplinary sanctions against those violating the requirements established in the law. The sanction system for failure to respect the right to information has been strengthened with the introduction of heavy administrative sanctions for officials violating the law.\n\nFurthermore, the new has broadened the scope of the definition of the term “public information” defined as any data registered in any form and format, maintained by a public authority as well as the definition of the term “public authority” which now encompasses commercial companies where the state hold the majority of shares, as well as any legal entity exercising public functions. Proactivity of publication for certain categories of information has been introduced.\n\nThe law establishes that requests for information of public interest can be made orally or in writing. The provision of information is free of charge: the law only enables public bodies to charge the costs for photocopying; no charge can be applied to electronic delivery.\n\nPublic bodies are obliged to give an answer within 10 days from the submission of the request (under the previous legal framework it was 40). In case the request is rejected, the applicant has the right to appeal to the Commissioner and then to the courts.\n\nExceptions to the right to information are established by the law, including for reasons of national security and international and intergovernmental relations.\n\nDespite the improvements introduced by the new legal framework, the effectiveness of its implementation still remains to be seen. According to the journalist Rudina Hoxha, one of the problem is that the two institutions that should monitor the application of the law, meaning the Information and Data Protection Commissioner and the Ombudsman, have in fact a counseling character and even though by the law they are entitled to apply sanctions, this happens very rarely.\n\nAccording to a 2016 report by the NGO \"Mjaft Movement\", Albanian public institutions comply with their obligations to provided public information under FOI requests only in 42% of the cases. To test and monitor the application of the new law, the organisation submitted 230 requests for information to different public bodies, including central institutions, local ones, justice institutions and public universities. Out of 230 requests, only 98 replies were obtained. Only 80 of these answers disclosed the required information. The remaining 18 were cases of incomplete information provision.\n\nThe report found out that Albanian municipalities were the most problematic and opaque institutions in terms of applying the FOI law. The main problem concerned non-compliance with the obligation to appoint the coordinator on the right to information and great lack of transparency on budget and financial issues.\n\n"}
{"id": "1009767", "url": "https://en.wikipedia.org/wiki?curid=1009767", "title": "Acquiescence", "text": "Acquiescence\n\nIn law, acquiescence occurs when a person knowingly stands by without raising any objection to the infringement of his or her rights, while someone else unknowingly and without malice aforethought acts in a manner inconsistent with their rights. As a result of acquiescence, the person whose rights are infringed may lose the ability to make a legal claim against the infringer, or may be unable to obtain an injunction against continued infringement. The doctrine infers a form of \"permission\" that results from silence or passiveness over an extended period of time.\n\nAlthough not typically not found in statutory law, the doctrine of acquiescence is well-supported by case law. One common context in which acquiescence is raised is when there is a dispute or disagreement over the location of a property line, followed by an extended period of time during which the parties respect a property line. Even if it is later discovered that the actual property line was in a different location, the long-term acquiescence to the incorrectly placed line may result in its becoming enforceable as the legal property line.\n\nAn example of the law of acquiescence occurred in a dispute between the State of Georgia and the State of South Carolina, in which the Supreme Court of the United States held that Georgia could no longer make any claim to an island in the Savannah River, despite the 1787 Treaty of Beaufort's assignment to the contrary. The court said that Georgia had knowingly allowed South Carolina to join the island as a peninsula to its own coast by dumping sand from dredging, and to then levy property taxes on it for decades. Georgia thereby lost the island-turned-peninsula by its own acquiescence, even though the treaty had given it all of the islands in the river.\n\nDoctrines similar to acquiescence include:\n\n\n"}
{"id": "47950379", "url": "https://en.wikipedia.org/wiki?curid=47950379", "title": "Arditi–Ginzburg equations", "text": "Arditi–Ginzburg equations\n\nThe Arditi–Ginzburg equations describe ratio dependent predator–prey dynamics. Where \"N\" is the population of a prey species and \"P\" that of a predator, the population dynamics are described by the following two equations:\n\nHere \"f\"(\"N\") captures any change in the prey population not due to predator activity including inherent birth and death rates. The per capita effect of predators on the prey population (the harvest rate) is modeled by a function \"g\" which is a function of the ratio \"N\"/\"P\" of prey to predators. Predators receive a reproductive payoff, \"e,\" for consuming prey, and die at rate \"u\". Making predation pressure a function of the ratio of prey to predators contrasts with the prey dependent Lotka–Volterra equations, where the effect of predators on the prey population is simply a function of the magnitude of the prey population \"g\"(\"N\"). Because the number of prey harvested by each predator decreases as predators become more dense, ratio dependent predation represents an example of a trophic function. Ratio dependent predation may account for heterogeneity in large-scale natural systems in which predator efficiency decreases when prey is scarce. The merit of ratio dependent versus prey dependent models of predation has been the subject of much controversy, especially between the biologists Lev R. Ginzburg and Peter A. Abrams. Ginzburg purports that ratio dependent models more accurately depict predator-prey interactions while Abrams maintains that these models make unwarranted complicating assumptions.\n\n"}
{"id": "37671", "url": "https://en.wikipedia.org/wiki?curid=37671", "title": "Aristocracy", "text": "Aristocracy\n\nAristocracy (Greek ἀριστοκρατία \"aristokratía\", from ἄριστος \"\" \"excellent\", and κράτος, kratos 'rule') is a form of government that places strength in the hands of a small, privileged ruling class. The term derives from the Greek \"aristokratia\", meaning \"rule of the best-born\".\nThe term is synonymous with hereditary government, and hereditary succession is its primary philosophy, after which the hereditary monarch appoints officers as they see fit. At the time of the word's origins in ancient Greece, the Greeks conceived it as rule by the best qualified citizens—and often contrasted it favourably with monarchy, rule by an individual. In later times, aristocracy was usually seen as rule by a privileged group, the aristocratic class, and has since been contrasted with democracy. The idea of hybrid forms which have aspects of both aristocracy and democracy are in use in the parliament form.\n\nThe concept evolved in Ancient Greece, whereby a council of leading citizens was commonly empowered and contrasted with representative democracy, in which a council of citizens was appointed as the \"senate\" of a city state or other political unit. The Greeks did not like the concept of monarchy, and as their democratic system fell, aristocracy was upheld.\nIn the 1651 book \"Leviathan\", Thomas Hobbes describes an aristocracy as a commonwealth in which the representative of the citizens is an assembly by part only. It is a system in which only a small part of the population represents the government; \"certaine men distinguished from the rest\". Modern depictions of aristocracy tend to regard it not as the ancient Greek concept of rule by the best, but more as an oligarchy or plutocracy—rule by the few or the wealthy.\n\nThe concept of aristocracy per Plato, has an ideal state ruled by the philosopher king. Plato describes these \"philosopher kings\" as \"those who love the sight of truth\" (Republic 475c) and supports the idea with the analogy of a captain and his ship or a doctor and his medicine. According to him, sailing and health are not things that everyone is qualified to practice by nature. A large part of the Republic then addresses how the educational system should be set up to produce these philosopher kings.\n\n"}
{"id": "43453855", "url": "https://en.wikipedia.org/wiki?curid=43453855", "title": "Center for the Study of Democracy (St. Mary's College of Maryland)", "text": "Center for the Study of Democracy (St. Mary's College of Maryland)\n\nThe Center for the Study of Democracy is a research and education institute at St. Mary's College of Maryland that focuses on the study of the history of emerging democracy in St. Mary's City, Maryland, the site of Maryland'a first colonial capital and the location of many firsts in the development of democratic rights in North America; this work is done in conjunction with studies of modern democracies. \n\nThe mission of the Center for the Study of Democracy is to draw historical lessons and also inspiration from history, in order to increase understanding of the processes and principles that lead to the constructive maintenance and enhancement of democracy in the United States and around the world. \n\nIt does so by drawing on in-depth historical research, in conjunction with in-depth research on modern democracy-related issues and events, as they both relate to the process of democratization (establishing and improving democracy in all of its processes). \n\nThe Center also sponsors numerous ongoing public forums and debates and seminars on these issues, many of which are covered by the media. \n\nThe debates also often include hosting and moderating major political leaders or policy leaders facing off against their opponents. \n\nThe Center for the Study of Democracy also strives to better understand historic setbacks and inconsistencies in the democratization process of historic Maryland and the United States, in order to better understand how these were eventually overcome; taking lessons from history that can then be applied to study of how democracy might be more constructively furthered today. The center Works in particular to increase understanding of how democracy may be enhanced in spite of the many obstacles that democratization often faces. \n\nThe Center is jointly run by the Public Honors College, St. Mary's College of Maryland and its partner institution and neighbor, Historic St. Mary's City, one of the nation's preeminent historical and archeological research institutions, \n\nThe Center for the Study of Democracy was established in 2002. Such notable people as former U.S. District Court judge Thomas Penfield Jackson, former National Security Adviser Anthony Lake former state Senator J. Frank Raley Jr. and former Maryland Governor William Donald Schaefer were very involved in its founding as well as being advisory board members. \n\nSo was former Washington Post Editor in Chief Benjamin C. Bradlee.\n\nAll of these men also served for years on the Board of Trustees of St. Mary's College of Maryland.\n\nThe Center for the Study of Democracy hosts over 25 educational events and or / or public forums per year. \n\nThe Center hosts forums for state and national politicians on various key issues related to democratic governance, human rights, inclusion, lawmaking and security as it relates to democracy.\n\nThe center sponsors and also directly provides many academic and also public lectures on both the historical and modern process of democratization. Learning from history (both recent history and older, foundational history) in order to better understand how democracy takes hold, and may be enhanced and spread constructively and effectively.\n\nThe center provides a minor in Democracy Studies at the St. Mary's College of Maryland. It also hosts numerous seminars and forums available to students. \n\nIt also provides special visiting lecturers and scholars, as well as providing special lectures to other college classes and programs.\n\nThe center also hosts the Patuxent Defense Forum, a gathering of high level experts on international security, including representatives from the academic community, foreign policy leadership, defense industry and military leadership. The purpose of the forum is to increase understanding related to the processes that lead to constructive maintenance and enhancement of democracy from a strategic defense point of view.\n\nThe Center also sponsors and hosts public lectures on areas of study related to its mission: \n\n\nThe center supports and engages in research focused on examining the birth and emergence of various aspects of democracy in the earlier history of the state of Maryland and also the wider English speaking world. It then applies lessons and perspectives related to this history to modern day issues tied to the furtherance of democracy, both in the United States and in newly emerging democracies around the world.\n\nExtensive research is also done on issues related to the emergence of democracy in countries where it never previously existed or persisted; enhancement and furtherance of democracy in developed nations as well as furtherance of minority and women's participation in democracy worldwide.\n\nOn Maryland and Washington, D.C. public radio, the Center for the Study of Democracy is often sought out for expert commentary.\n\nThe Institution is both publicly and privately funded. It is nonprofit and under the joint operation of the Public Honors College, St. Mary's College of Maryland (a Maryland state public school) and Historic St. Mary's City Maryland (a state research, historic interpretation and educational agency). \n\nThe organization has been funded by the National Endowment for the Humanities. It also has a history of gaining funding from other major granting institutions.\n\nThe current director of the Center is Dr. Michael J.G. Cain, a professor of Political Science at St. Mary's College of Maryland. \n\nThe prior director was Todd Eberly, also a professor at St. Mary's College of Maryland as well as being a noted Maryland political commentator. Eberly is often heard on several radio shows and is often quoted by the Washington Post, as well as writing his own columns.\n\n\n\n"}
{"id": "26569682", "url": "https://en.wikipedia.org/wiki?curid=26569682", "title": "Clockwise", "text": "Clockwise\n\nTwo-dimensional rotation can occur in two possible directions. A clockwise (typically abbreviated as CW) motion is one that proceeds in the same direction as a clock's hands: from the top to the right, then down and then to the left, and back up to the top. The opposite sense of rotation or revolution is (in North American English) counterclockwise (CCW) or (in Commonwealth English) anticlockwise (ACW).\n\nBefore clocks were commonplace, the terms \"sunwise\" and \"deasil\", \"deiseil\" and even \"deocil\" from the Scottish Gaelic language and from the same root as the Latin \"dexter\" (\"right\") were used for clockwise. \"Widdershins\" or \"withershins\" (from Middle Low German \"weddersinnes\", \"opposite course\") was used for counterclockwise.\nThe terms clockwise and counterclockwise can only be applied to a rotational motion once a side of the rotational plane is specified, from which the rotation is observed. For example, the daily rotation of the Earth is clockwise when viewed from above the South Pole, and counterclockwise when viewed from above the North Pole (considering \"above a point\" to be defined as \"farther away from the center of earth and on the same ray\").\n\nClocks traditionally follow this sense of rotation because of the clock's predecessor: the sundial. Clocks with hands were first built in the Northern Hemisphere (see \"Clock\"), and they were made to work like horizontal sundials. In order for such a sundial to work north of the equator during spring and summer, and north of the Tropic of Cancer the whole year, the noon-mark of the dial must be placed northward of the pole casting the shadow. Then, when the Sun moves in the sky (from east to south to west), the shadow, which is cast on the sundial in the opposite direction, moves with the same sense of rotation (from west to north to east). This is why hours must be drawn in horizontal sundials in that manner, and why modern clocks have their numbers set in the same way, and their hands moving accordingly. For a vertical sundial (such as those placed on the walls of buildings, the dial being \"below\" the post), the movement of the sun is from right to top to left, and, accordingly, the shadow moves from left to down to right, i.e., counterclockwise. This effect is caused by the plane of the dial having been rotated through the plane of the motion of the sun and thus the shadow is observed from the other side of the dial's plane and is observed as moving in the opposite direction. Some clocks were constructed to mimic this. The best-known surviving example is the astronomical clock in the Münster Cathedral, whose hands move counterclockwise.\n\nOccasionally, clocks whose hands revolve counterclockwise are nowadays sold as a novelty. Historically, some Jewish clocks were built that way, for example in some synagogue towers in Europe such as the Jewish Town Hall in Prague, to accord with right-to-left reading in the Hebrew language. In 2014 under Bolivian president Evo Morales, the clock outside the Legislative Assembly in Plaza Murillo, La Paz, was shifted to counterclockwise motion to promote indigenous values.\n\nTypical nuts, screws, bolts, bottle caps, and jar lids are tightened (moved away from the observer) clockwise and loosened (moved towards the observer) counterclockwise in accordance with the right-hand rule.\n\nTo apply the right-hand rule, place one's loosely clenched right hand above the object with the thumb pointing in the direction one wants the screw, nut, bolt, or cap ultimately to move, and the curl of the fingers, from the palm to the tips, will indicate in which way one needs to turn the screw, nut, bolt or cap to achieve the desired result. Almost all threaded objects obey this rule except for a few left-handed exceptions described below.\n\nThe reason for the clockwise standard for most screws and bolts is that supination of the arm, which is used by a right-handed person to tighten a screw clockwise, is generally stronger than pronation used to loosen. \n\nSometimes the opposite (left-handed, counterclockwise, reverse) sense of threading is used for a special reason. A thread might need to be left-handed to prevent operational stresses from loosening it. For example, some older cars and trucks had right-handed lug nuts on the right wheels and left-handed lug nuts on the left wheels, so that, as the vehicle moved forward, the lug nuts tended to tighten rather than loosen. For bicycle pedals, the one on the left must be reverse-threaded to prevent it unscrewing during use. Similarly, the flyer whorl of a spinning wheel uses a left-hand thread to keep it from loosening. A turnbuckle has right-handed threads on one end and left-handed threads on the other. Some gas fittings are left-handed to prevent disastrous misconnections: oxygen fittings are right-handed, but acetylene, propane, and other flammable gases are unmistakably distinguished by left-handed fittings.\n\nIn trigonometry and mathematics in general, plane angles are conventionally measured counterclockwise, starting with 0° or 0 radians pointing directly to the right (or east), and 90° pointing straight up (or north). However, in navigation, compass headings increase clockwise around the compass face, starting with 0° at the top of the compass (the northerly direction), with 90° to the right (east).\n\nA circle defined parametrically in a positive Cartesian plane by the equations and is traced counterclockwise as the angle \"t\" increases in value, from the right-most point at . An alternative formulation with sin and cos swapped gives a clockwise trace from the upper-most point.\n\nIn general, most card games, board games, parlor games and multiple team sports play in a clock-wise turn rotation in Western Countries and Latin America with a notable resistance to playing in the opposite direction (counter-clockwise). Traditionally (and still continued for the most part) turns pass counter-clockwise in many Asian countries. In Western countries when speaking and discussion activities take part in a circle, turns tend to naturally pass in a clockwise motion even though there is no obligation to do so. Curiously, unlike with games, there is usually no objection when the activity uncharacteristically begins in a counter-clockwise motion.\n\nNotably, the game of baseball is played counter-clockwise.\n\nMost left-handed people prefer to draw circles and circulate in buildings clockwise, while most right-handed people prefer to draw circles and circulate in buildings counterclockwise. This is believed to result from dominant brain hemispheres, though some attribute it to muscle mechanics.\n\n"}
{"id": "14532984", "url": "https://en.wikipedia.org/wiki?curid=14532984", "title": "Coincidence detection in neurobiology", "text": "Coincidence detection in neurobiology\n\nCoincidence detection in the context of neurobiology is a process by which a neuron or a neural circuit can encode information by detecting the occurrence of temporally close but spatially distributed input signals. Coincidence detectors influence neuronal information processing by reducing temporal jitter, reducing spontaneous activity, and forming associations between separate neural events. This concept has led to a greater understanding of neural processes and the formation of computational maps in the brain.\n\nCoincidence detection relies on separate inputs converging on a common target. Consider a basic neural circuit with two input neurons, A and B, that have excitatory synaptic terminals converging on a single output neuron, C (Fig. 1). If each input neuron's EPSP is subthreshold for an action potential at C, then C will not fire unless the two inputs from A and B are temporally close together. Synchronous arrival of these two inputs may push the membrane potential of a target neuron over the threshold required to create an action potential. If the two inputs arrive too far apart, the depolarization of the first input may have time to drop significantly, preventing the membrane potential of the target neuron from reaching the action potential threshold. This example incorporates the principles of spatial and temporal summation. Furthermore, coincidence detection can reduce the jitter formed by spontaneous activity. While random sub-threshold stimulations by neuronal cells may not often fire coincidentally, coincident synaptic inputs derived from a unitary external stimulus will ensure that a target neuron fires as a result of the stimulus.\n\nThe above description applies well to feedforward inputs to neurons, which provide inputs from either sensory nerves or lower-level regions in the brain. About 90% of interneural connections are, however, not feedforward but predictive (or modulatory, or attentional) in nature. These connections receive inputs mainly from nearby cells in the same layer as the receiving cell, and also from distant connections which are fed through Layer 1. The dendrites which receive these inputs are quite distant from the cell body, and therefore they exhibit different electrical and signal-processing behaviour compared with the proximal (or feedforward) dendrites described above.\n\nIn a short section (perhaps 40 µm long) of distral dendrite, the reaction to activations coming in on synapses to the dendritic spines acts to raise the overall local potential with each incoming signal. This rising potential acts against a background of decay in the potential back to the resting level. If sufficient signals are received within a short period of time (i.e. before the overall voltage decays to background), the voltage of the segment will rise above a threshold, giving rise to a non-linear dendritic spike, which travels, effectively undiminished, all the way to the cell body, and which causes it to become partially depolarised.\n\nThis is perhaps the most important form of dendritic coincidence detection in the brain. The more easily understood proximal activation acts over much longer time periods, and is thus much less sensitive to the time factor in coincidence detection.\n\nCoincidence detection has been shown to be a major factor in sound localization along the azimuth plane in several organisms. In 1948, Lloyd A. Jeffress proposed that some organisms may have a collection of neurons that receive auditory input from each ear. The neural pathways to these neurons are called delay lines. Jeffress claimed that the neurons that the delay lines link act as coincidence detectors by firing maximally when receiving simultaneous inputs from both ears. When a sound is heard, sound waves may reach the ears at different times. This is referred to as the interaural time difference (ITD). Due to differing lengths and a finite conduction speed within the axons of the delay lines, different coincidence detector neurons will fire when sound comes from different positions along the azimuth. Jeffress' model proposes that two signals even from an asynchronous arrival of sound in the cochlea of each ear will converge synchronously on a coincidence detector in the auditory cortex based on the magnitude of the ITD (Fig. 2). Therefore, the ITD should correspond to an anatomical map that can be found within the brain. Masakazu Konishi's study on barn owls shows that this is true. Sensory information from the hair cells of the ears travels to the ipsilateral nucleus magnocellularis. From here, the signals project ipsilaterally and contralaterally to two nucleus laminari. Each nucleus laminaris contains coincidence detectors that receive auditory input from the left and the right ear. Since the ipsilateral axons enter the nucleus laminaris dorsally while the contralateral axons enter ventrally, sounds from various positions along the azimuth correspond directly to stimulation of different depths of the nucleus laminaris. From this information, a neural map of auditory space was formed. The function of the nucleus laminaris parallels that of the medial superior olive in mammals.\n\nIn 1949, Donald Hebb postulated that synaptic efficiency will increase through repeated and persistent stimulation of a postsynaptic cell by a presynaptic cell. This is often informally summarized as \"cells that fire together, wire together\". The theory was validated in part by the discovery of long-term potentiation. Studies of LTP on multiple presynaptic cells stimulating a postsynaptic cell uncovered the property of associativity. A weak neuronal stimulation onto a pyramidal neuron may not induce long-term potentiation. However, this same stimulation paired with a simultaneous strong stimulation from another neuron will strengthen both synapses. This process suggests that two neuronal pathways converging on the same cell may both strengthen if stimulated coincidentally.\n\nLTP in the hippocampus requires a prolonged depolarization that can expel the Mg block of postsynaptic NMDA receptors. The removal of the Mg block allows the flow of Ca into the cell. A large elevation of calcium levels activate protein kinases that ultimately increase the number of postsynaptic AMPA receptors. This increases the sensitivity of the postsynaptic cell to glutamate. As a result, both synapses strengthen. The prolonged depolarization needed for the expulsion of Mg from NMDA receptors requires a high frequency stimulation. Associativity becomes a factor because this can be achieved through two simultaneous inputs that may not be strong enough to activate LTP by themselves.\n\nBesides the NMDA-receptor based processes, further cellular mechanisms allow of the association between two different input signals converging on the same neuron, in a defined timeframe. Upon a simultaneous increase in the intracellular concentrations of cAMP and Ca, a transcriptional coactivator called TORC1 (CRTC1) becomes activated, that converts the temporal coincidence of the two second messengers into long term changes such as LTP. This cellular mechanism, through calcium-dependent adenylate cyclase activation, might also account for the detection of the repetitive stimulation of a given synapse.\n\nAdenylyl cyclase (also commonly known as adenyl cyclase and adenylate cyclase) has been implicated in memory formation as a coincidence detector\n\nLong-term depression also works through associative properties although it is not always the reverse process of LTP. LTD in the cerebellum requires a coincident stimulation of parallel fibers and climbing fibers. Glutamate released from the parallel fibers activates AMPA receptors which depolarize the postsynaptic cell. The parallel fibers also activate metabotropic glutamate receptors that release the second messengers IP and DAG. The climbing fibers stimulate a large increase in postsynaptic Ca levels when activated. The Ca, IP, and DAG work together in a signal transduction pathway to internalize AMPA receptors and decrease the sensitivity of the postsynaptic cell to glutamate.\n\n\n\nry lo\n"}
{"id": "3466370", "url": "https://en.wikipedia.org/wiki?curid=3466370", "title": "Common assault", "text": "Common assault\n\nCommon assault was an offence under the common law of England, and has been held now to be a statutory offence in the United Kingdom of Great Britain and Northern Ireland. It is committed by a person who causes another person to apprehend the immediate use of unlawful violence by the defendant. It was thought to include battery but it does not. In England and Wales, the penalty and mode of trial for this offence is now provided section 39 of the Criminal Justice Act 1988, and it has been held that the offence should be alleged as contrary to the statute because of this. It was also held that common assault and battery are two distinct offences, so that a charge that the accused \"assaulted and battered\" another person would be bad for duplicity (the rule against charging more than one offence in a single count).\n\nSection 39 of the Criminal Justice Act 1988 provides:\n\nOn 13th September 2018, the Emergency Workers (Offences) Act 2018 received Royal Assent. This added a subsection which states any common assault and/or battery on an emergency services worker is also now indictable and therefore subject to a maximum of 12 months if tried on indictment\n\nSection 39 of the Criminal Justice Act 1988 does not contain a definition of the expression \"common assault\" that appears there. What the offence actually consists of must be determined by reference to case law.\n\nA person commits an assault if they perform an act (which does not for this purpose include a mere omission to act) by which they intentionally or recklessly causes another person to apprehend immediate unlawful violence.\n\nBoth in the common law and under statute, the \"actus reus\" of a common assault is committed when one person causes another to apprehend or fear that force is about to be used to cause some degree of personal contact and possible injury. There must be some quality of reasonableness to the apprehension on the part of the victim. If the physical contact is everyday social behaviour such as a handshake or friendly pat on the back, this is acceptable even though the victim may have a phobia although, if the defendant is aware of the psychological difficulty, this may be converted into an assault if the intention is to exploit the condition and embarrass the victim. More generally, if the defendant threatens injury tomorrow, the victim has the opportunity to take avoiding action. Thus, what is threatened must be capable of being carried out immediately. This would exclude a conditional threat. For example, if the defendant says that he would beat the living daylights out of you but for the presence of a police officer watching them both, the victim is supposed to understand that there is no immediate danger (\"cf.\" \"Tuberville v Savage\"'s \"If it were not assize time I would not take such language from you\"). But inequality in size can be disregarded so if a very small person threatens a very large person and it is obvious that the risk of any real injury from this attack is remote, the large person may nevertheless feel some degree of apprehension. Normally, both the one making the threat and the victim must be physically present because, otherwise, there would be no immediate danger. However, if a mobile phone is used to transmit the threat (whether orally or by SMS) and, from the words used, the victim reasonably understands that an attack is imminent, this may constitute an assault.\n\nIn \"Fagan v. Metropolitan Police Commissioner\" a police officer ordered the defendant to park his car and he reluctantly complied. In doing so, he accidentally drove the car on to the policeman’s foot and, when asked to remove the car, said \"Fuck you, you can wait\" and turned off the ignition. Because of the steel toe cap in his boot, the policeman's foot was not in actual danger, but the Divisional Court held that this could constitute an assault. Albeit accidentally, the driver had caused his car to rest on the officer's foot. This \"actus reus\" was a continuing act and the \"mens rea\" was formed during the relevant time (see concurrence). Whether realistically or not, the officer apprehended the possibility of injury so the offence was complete.\n\nIn \"R v. Ireland\", it was found that causing a person to apprehend violence can be committed by way of action or words. \"Words\" can also mean that otherwise threatening actions are rendered not capable of being an assault, as in the case of \"Tuberville v. Savage\". In that case, the plaintiff told the defendant (while putting his hand on his sword) that he would \"not\" stab him, because the circuit judge was visiting town for the local assizes. On that basis, the defendant was deemed to have known that he was not about to be injured, and it was held that no assault had been committed by the plaintiff (which would otherwise have justified the defendant's allegedly pre-emptive strike).\n\nThe \"immediacy\" requirement has been the subject of some debate. The leading case, again, is \"R v. Ireland\". Therein, the House of Lords held that the making of silent telephone calls could amount to an assault if it caused the victim to believe that physical violence might be used against him in the immediate future. One example of \"immediacy\" adopted by the House in that case was that a man who said, \"I will be at your door in a minute or two,\" might (in the circumstances where those words amounted to a threat) be guilty of an assault.\n\nSee also \"R v. Constanza\".\n\nThe \"mens rea\" is that this fear must have been caused either intentionally or recklessly. A battery is committed when the threatened force actually results in contact to the other and that contact was caused either intentionally or recklessly.\n\nSelf-defence is available when reasonable force is used to prevent harm to self or another.\nPrevention of a greater crime or with the purpose of aiding a lawful arrest is also known as The Public Defence. The Private Defence or defence of property may be also be used as an argument. \nThese arguments are not strictly defences but justifications for a certain level of force.\n\nThe original effect of sections 39 and 40 of the Criminal Justice Act 1988 was that common assault was not available as an alternative verdict under section 6(3) of the Criminal Law Act 1967.\n\nCommon assault is now available as an alternative verdict under section 6(3) of the Criminal Law Act 1967, by virtue of section 6(3A) of that Act (which was inserted by section 11 of the Domestic Violence, Crime and Victims Act 2004).\n\nIn \"DPP v. Taylor\" and \"DPP v. Little\" it was held that common assault is a statutory offence, contrary to section 39 of the Criminal Justice Act 1988. This decision was criticised and in \"Haystead v DPP\" the Divisional court expressed the \"obiter\" opinion that common assault remains a common law offence.\n\nIn England and Wales, it is a summary offence. However, where section 40 of the Criminal Justice Act 1988 applies, it can be an additional charge on an indictment. It is usually tried summarily.\n\nHowever it is tried, it is punishable with imprisonment for a term not exceeding six months, or a fine not exceeding level 5 on the standard scale, or both.\n\nSee Crown Prosecution Service Sentencing Manual for case law on sentencing. Relevant cases are:\n\nIn England and Wales, section 29(1)(c) of the Crime and Disorder Act 1998 (c.37) creates the distinct offence of racially or religiously aggravated common assault.\n\nThis is the least serious assault. It is not at all uncommon for more serious assault charges to be reduced to common assault in \"plea-bargaining\" by prosecutors to avoid the additional expense of a Crown Court trial should the defendant elect for same. In real terms, the degree of fear or the level of injury required for a conviction can be unproven. No injury is required to prove battery.\n\n\n"}
{"id": "26967147", "url": "https://en.wikipedia.org/wiki?curid=26967147", "title": "Comparison of Adobe Flex charts", "text": "Comparison of Adobe Flex charts\n\nThe following comparison of Adobe Flex charts provides charts classification, compares Flex chart products for different chart type availability and for different visual features like 3D versions of charts.\n\nThis gallery shows:\n\n"}
{"id": "216147", "url": "https://en.wikipedia.org/wiki?curid=216147", "title": "Conservation agriculture", "text": "Conservation agriculture\n\nConservation agriculture (CA) can be defined by a statement given by the Food and Agriculture Organization of the United Nations as “a concept for resource-saving agricultural crop production that strives to achieve acceptable profits together with high and sustained production levels while concurrently conserving the environment” (FAO 2007).\n\nAgriculture according to the New Standard Encyclopedia is “one of the most important sectors in the economies of most nations” (New Standard 1992). At the same time conservation is the use of resources in a manner that safely maintains a resource that can be used by humans. Conservation has become critical because the global population has increased over the years and more food needs to be produced every year (New Standard 1992). Sometimes referred to as \"agricultural environmental management\", conservation agriculture may be sanctioned and funded through conservation programs promulgated through agricultural legislation, such as the U.S. Farm Bill.\n\nThe Food and Agriculture Organization of the United Nations (FAO) has determined that CA has three key principles that producers (farmers) can proceed through in the process of CA. These three principles outline what conservationists and producers believe can be done to conserve what we use for a longer period of time.\n\nThe first key principle in CA (Conservation Agriculture) is practicing minimum soil disturbance which is essential to maintaining minerals within the soil, stopping erosion, and preventing water loss from occurring within the soil. In the past agriculture has looked at soil tillage as a main process in the introduction of new crops to an area. It was believed that tilling the soil would increase fertility within the soil through mineralization that takes place in the soil. Also tilling of soil can cause severe erosion and crusting which leads to a decrease in soil fertility. Today tillage is seen as destroying organic matter that can be found within the soil cover. No-till farming has caught on as a process that can save soil organic levels for a longer period and still allow the soil to be productive for longer periods (FAO 2007). Additionally, the process of tilling can increase time and labor for producing that crop. Minimum soil disturbance also reduce destruction of soil micro and macro-organism habitats that is common in conventional ploughing practices. \n\nWhen no-till practices are followed, the producer sees a reduction in production cost for a certain crop. Tillage of the ground requires more money in order to fuel tractors or to provide feed for the animals pulling the plough. The producer sees a reduction in labor because he or she does not have to be in the fields as long as a conventional farmer.\n\nThe second key principle in CA is much like the first in dealing with protecting the soil. The principle of managing the top soil to create a permanent organic soil cover can allow for growth of organisms within the soil structure. This growth will break down the mulch that is left on the soil surface. The breaking down of this mulch will produce a high organic matter level which will act as a fertilizer for the soil surface. If CA practices were used done for many years and enough organic matter was being built up at the surface, then a layer of mulch would start to form. This layer helps prevent soil erosion from taking place and ruining the soil's profile or layout. The presence of mulching also reduce the velocity of runoff and the impact of rain drops thus reducing soil erosion and runoff.\n\nAccording to the article “The role of conservation agriculture and sustainable agriculture”, the layer of mulch that is built up over time will become like a buffer zone between soil and mulch and this will help reduce wind and water erosion. With this comes the protection of the soil's surface when rain falls on the ground. Land that is not protected by a layer of mulch is left open to the elements (Hobbs et al. 2007). This type of ground cover also helps keep the temperature and moisture levels of the soil at a higher level rather than if it was tilled every year (FAO 2007).\n\nThe third principle is the practicing diverse crop rotations or crop interactions. According to an article published in the \"Physiological Transactions of the Royal Society\" called “The role of conservation agriculture and sustainable agriculture,” crop rotation can be used best as a disease control against other preferred crops (Hobbs et al. 2007). This process will not allow pests such as insects and weeds to be set into a rotation with specific crops. Rotational crops will act as a natural insecticide and herbicide against specific crops. Not allowing insects or weeds to establish a pattern will help to eliminate problems with yield reduction and infestations within fields (FAO 2007). Crop rotation can also help build up soil infrastructure. Establishing crops in a rotation allows for an extensive buildup of rooting zones which will allow for better water infiltration (Hobbs et al. 2007).\n\nOrganic molecules in the soil break down into phosphates, nitrates and other beneficial elements which are thus better absorbed by plants. Plowing increases the amount of oxygen in the soil and increases the aerobic processes, hastening the breakdown of organic material. Thus more nutrients are available for the next crop but, at the same time, the soil is depleted more quickly of its nutrient reserves.\n\nIn conservation agriculture there are many examples that can be looked towards as a way of farming and at the same time conserving. These practices are well known by most producers. The process of no-till is one that follows the first principle of CA, causing minimal mechanical soil disturbance. No-till also brings other benefits to the producer . According to the FAO, tillage is one of the most “energy consuming” processes that can be used: It requires a lot of labor, time, and fuel to till. Producers can save 30% to 40% of time and labor by practicing the no-till process. (FAO 3020)\n\nBesides conserving the soil, there are other examples of how CA is used. According to an article in \"Science\" called “Farming and the Fate of Wild Nature” there are two more kinds of CA . The practice of wildlife-friendly farming and land sparing are ideas for producers who are looking to practice better conservation towards biodiversity (Green, et al. 2005).\n\nWildlife-friendly farming is a practice of setting aside land that will not be developed by the producer (farmer). This land will be set aside so that biodiversity has a chance to establish itself in areas with agricultural fields. At the same time, the producer is attempting to lower the amount of fertilizer and pesticides used on the fields so that organisms and microbial activity have a chance to establish themselves in the soil and habitat (Green, et al. 2005). But as in all systems, not all can be perfect. To create a habitat suitable for biodiversity something has to be reduced, and as in this case for agriculture farmers, yields can be reduced. This is where the second idea of land sparing can be looked on as an alternative manner.\n\nLand sparing is another way that producer and conservationist can be on the same page. Land sparing advocates for the land that is being used for agricultural purposes to continue to produce crops at increased yield. With an increase in yield on all land that is in use, other land can be set aside for conservation and production for biodiversity. Agricultural land stays in production but would have to increase its yield potential to keep up with demand. Land that is not being put into agriculture would be used for conserving biodiversity (Green, et al. 2005). In fact, data from the Food and Agriculture Organization shows that between 1961 and 2012, the amount of arable land needed to produce the same amount of food declined by 68 percent worldwide.\n\nIn the field of CA there are many benefits that both the producer and conservationist can obtain.\n\nOn the side of the conservationist, CA can be seen as beneficial because there is an effort to conserve what people use every day. Since agriculture is one of the most destructive forces against biodiversity, CA can change the way humans produce food and energy. With conservation come environmental benefits of CA. These benefits include less erosion possibilities, better water conservation, improvement in air quality due to lower emissions being produced, and a chance for larger biodiversity in a given area.\n\nOn the side of the producer and/or farmer, CA can eventually do all that is done in conventional agriculture, and it can conserve better than conventional agriculture. CA according to Theodor Friedrich, who is a specialist in CA, believes “Farmers like it because it gives them a means of conserving, improving, and making more efficient use of their natural resources\" (FAO 2006). Producers will find that the benefits of CA will come later rather than sooner. Since CA takes time to build up enough organic matter and have soils become their own fertilizer, the process does not start to work overnight. But if producers make it through the first few years of production, results will start to become more satisfactory.\n\nCA is shown to have even higher yields and higher outputs than conventional agriculture once it has been established over long periods. Also, a producer has the benefit of knowing that the soil in which his crops are grown is a renewable resource. According to New Standard Encyclopedia, soils are a renewable resource, which means that whatever is taken out of the soil can be put back over time (New Standard 1992). As long as good soil upkeep is maintained, the soil will continue to renew itself. This could be very beneficial to a producer who is practicing CA and is looking to keep soils at a productive level for an extended time.\n\nThe farmer and/or producer can use this same land in another way when crops have been harvested. The introduction of grazing livestock to a field that once held crops can be beneficial for the producer and also the field itself. Livestock manure can be used as a natural fertilizer for a producer’s field which will then be beneficial for the producer the next year when crops are planted once again. The practice of grazing livestock using CA helps the farmer who raises crops on that field and the farmer who raises the livestock that graze off that field. Livestock produce compost or manure which are a great help in generating soil fertility (Pawley W.H. 1963). The practices of CA and grazing livestock on a field for many years can allow for better yields in the following years as long as these practices continue to be followed.\nThe FAO believes that there are three major benefits from CA: \n\nAs in any other business, producers and conservationists are always looking towards the future. In this case CA is a very important process to be looked at for future generation. There are many organizations that have been created to help educate and inform producers and conservationists in the world of CA. These organizations can help to inform, conduct research, and buy land in order to preserve animals and plants (New Standard 1992).\n\nAnother way in which CA is looking to the future is through prevention. According to the \"European Journal of Agronomy\" producers are looking for ways to reduce leaching problems within their fields. These producers are using the same principles within CA, in that they are leaving cover over their fields in order to save fields from erosion and leaching of chemicals (Kirchmann & Thorvaldsson 2000). Processes and studies like this are allowing for a better understanding of how to conserve what we are using and finding ways to put back something that may have been lost before.\n\nIn the same journal article is presented another way in which producers and conservationists are looking towards the future. Circulation of plant nutrients can be a vital part for conserving the future. An example of this would be the use of animal manure. This process has been used for quite some time now, but the future is looking towards ways to handle and conserve nutrients within manure for a longer time. But besides animal waste, food and urban waste are also being looked towards as a way to use growth within CA (Kirchmann & Thorvaldsson 2000). Turning these products from waste to being used to grow crops and improve yields is something that would be beneficial for conservationists and producers.\n\nAgri-environment schemes\n\nIn 1992, ‘agri-environment schemes’ became compulsory for all European Union Member States. In the following years the main purpose of these schemes changed slightly. \nInitially, they sought to protect threatened habitats, but gradually shifted their focus to the prevention of the loss of wildlife from agricultural landscapes. Most recently, the schemes are placing more emphasis on improving the services that the land can provide to humans (e.g. pollination). Overall, farmers involved in the scheme aim to practice environmentally friendlier farming techniques such as: reducing the use of pesticides, managing or altering their land to increase more wildlife friendly habitats (e.g. increasing areas of trees and bushes), reducing irrigation, conserving soil, and organic farming. \nAs the changes in practices that ensure the protection of the environment are costly to farmers, the EU developed agri-environment schemes to financially compensate individual farmers for applying these changes and therefore increased the implementation of conservation agriculture. \nThe schemes are voluntary for farmers. Once joined, they commit to a minimum of five years during which they have to adopt various sustainable farming techniques.\nAccording to the Euro-stat website, in 2009 the agricultural area enrolled in agri-environment schemes covered 38.5 million hectares (20.9% of agricultural land in the 27 member states of the EU at the time) (Agri-environmental indicator 2015). The European Commission spent a total of €3.23 billion on agri-environment schemes in 2012, significantly exceeding the cost of managing special sites of conservation (Natura 2000) that year, which came to a total of €39.6 million (Batáry et al. 2015). \nThere are two main types of agri-environment schemes which have shown different outcomes. Out-of-production schemes tend to be used in extensive farming practices (where the farming land is widespread and less intensive farming is practiced), and focus on improving or setting land aside that will not be used for the production of food, for example, the addition of wildflower strips.\nIn-production schemes (used for a smaller scale, but more intensively farmed land) focus on the sustainable management of arable crops or grassland, for example reduction of pesticides, reduction of grassland mowing, and most commonly, organic farming. In a 2015 review of studies examining the effects of the two schemes, it was found that out-of-production schemes had a higher success rate at enhancing the number of thriving species around the land. \nThe reason behind this is thought to be the scheme’s focus on enhancing specific species by providing them with more unaltered habitats, which results in more food resources for the specific species.\nOn the other hand, in-production schemes attempt to enhance the quality of the land in general, and are thus less species specific. \nBased on the findings, the reviewers suggest that schemes which more specifically target the declining groups of species, may be more effective. The findings and the targets will be implemented between 2015 and 2020, so that by 2025, the effectiveness of these schemes can be re-assessed and will have increased significantly (Batáry et al. 2015).\n\nAs much as conservation agriculture can benefit the world, there are some problems that come with it. There are many reasons why conservation agriculture cannot always be a win-win situation.\nThere are not enough people who can financially turn from conventional farming to conservation. The process of CA takes time; when a producer first becomes a conservationist, the results can be a financial loss to them(in most cases, the investment and policy generally exist). CA is based upon establishing an organic layer and producing its own fertilizer and this may take time. It can be many years before a producer will start to see better yields than he/she has had previously. Another financial undertaking is purchasing of new equipment. When starting to use CA, a producer may have to buy new planters or drills in order to produce effectively. These financial tasks are ones that may impact whether or not a producer decides to switch to CA or not.\n\nWith the struggle to adapt comes the struggle to make CA grow across the globe. CA has not spread as quickly as most conservationists would like. The reason for this is because there is not enough pressure for producers in places such as North America to change their way of living to a more conservationist outlook. But in the tropics there is more pressure to change to conservation areas because of the limited resources that are available. Places like Europe have also started to catch onto the ideas and principles of CA, but still nothing much is being done to change due to there being a minimal amount of pressure for people to change their ways of living (FAO 2006).\n\nWith CA comes the idea of producing enough food. With cutting back in fertilizer, not tilling the ground, and other processes comes the responsibility to feed the world. According to the Population Reference Bureau, there were around 6.08 billion people on Earth in the year 2000. By 2050 there will be an estimated 9.1 billion people. With this increase comes the responsibility for producers to increase food supply using the same or less land than we use today. Problems arise in the fact that if CA farms do not produce as much as conventional farms, this leaves the world with less food for more people.\n\n\n\n"}
{"id": "435754", "url": "https://en.wikipedia.org/wiki?curid=435754", "title": "Control chart", "text": "Control chart\n\nControl charts, also known as Shewhart charts (after Walter A. Shewhart) or process-behavior charts, are a statistical process control tool used to determine if a manufacturing or business process is in a state of control.\n\nIf analysis of the control chart indicates that the process is currently under control (i.e., is stable, with variation only coming from sources common to the process), then no corrections or changes to process control parameters are needed or desired. In addition, data from the process can be used to predict the future performance of the process. If the chart indicates that the monitored process is not in control, analysis of the chart can help determine the sources of variation, as this will result in degraded process performance. A process that is stable but operating outside desired (specification) limits (e.g., scrap rates may be in statistical control but above desired limits) needs to be improved through a deliberate effort to understand the causes of current performance and fundamentally improve the process.\n\nThe control chart is one of the seven basic tools of quality control. Typically control charts are used for time-series data, though they can be used for data that have logical comparability (i.e. you want to compare samples that were taken all at the same time, or the performance of different individuals); however the type of chart used to do this requires consideration.\n\nThe control chart was invented by Walter A. Shewhart working for Bell Labs in the 1920s. The company's engineers had been seeking to improve the reliability of their telephony transmission systems. Because amplifiers and other equipment had to be buried underground, there was a stronger business need to reduce the frequency of failures and repairs. By 1920, the engineers had already realized the importance of reducing variation in a manufacturing process. Moreover, they had realized that continual process-adjustment in reaction to non-conformance actually increased variation and degraded quality. Shewhart framed the problem in terms of Common- and special-causes of variation and, on May 16, 1924, wrote an internal memo introducing the control chart as a tool for distinguishing between the two. Shewhart's boss, George Edwards, recalled: \"Dr. Shewhart prepared a little memorandum only about a page in length. About a third of that page was given over to a simple diagram which we would all recognize today as a schematic control chart. That diagram, and the short text which preceded and followed it set forth all of the essential principles and considerations which are involved in what we know today as process quality control.\" Shewhart stressed that bringing a production process into a state of statistical control, where there is only common-cause variation, and keeping it in control, is necessary to predict future output and to manage a process economically.\n\nShewhart created the basis for the control chart and the concept of a state of statistical control by carefully designed experiments. While Shewhart drew from pure mathematical statistical theories, he understood that data from physical processes typically produce a \"normal distribution curve\" (a Gaussian distribution, also commonly referred to as a \"bell curve\"). He discovered that observed variation in manufacturing data did not always behave the same way as data in nature (Brownian motion of particles). Shewhart concluded that while every process displays variation, some processes display controlled variation that is natural to the process, while others display uncontrolled variation that is not present in the process causal system at all times.\n\nIn 1924, or 1925, Shewhart's innovation came to the attention of W. Edwards Deming, then working at the Hawthorne facility. Deming later worked at the United States Department of Agriculture and became the mathematical advisor to the United States Census Bureau. Over the next half a century, Deming became the foremost champion and proponent of Shewhart's work. After the defeat of Japan at the close of World War II, Deming served as statistical consultant to the Supreme Commander for the Allied Powers. His ensuing involvement in Japanese life, and long career as an industrial consultant there, spread Shewhart's thinking, and the use of the control chart, widely in Japanese manufacturing industry throughout the 1950s and 1960s.\n\nA control chart consists of:\n\nThe chart may have other optional features, including:\n(n.b., there are several rule sets for detection of signal, this is just one set. The rule set should be clearly stated.)\n\n\nIf the process is in control (and the process statistic is normal), 99.7300% of all the points will fall between the control limits. Any observations outside the limits, or systematic patterns within, suggest the introduction of a new (and likely unanticipated) source of variation, known as a special-cause variation. Since increased variation means increased quality costs, a control chart \"signaling\" the presence of a special-cause requires immediate investigation.\n\nThis makes the control limits very important decision aids. The control limits provide information about the process behavior and have no intrinsic relationship to any specification targets or engineering tolerance. In practice, the process mean (and hence the centre line) may not coincide with the specified value (or target) of the quality characteristic because the process design simply cannot deliver the process characteristic at the desired level.\n\nControl charts limit specification limits or targets because of the tendency of those involved with the process (e.g., machine operators) to focus on performing to specification when in fact the least-cost course of action is to keep process variation as low as possible. Attempting to make a process whose natural centre is not the same as the target perform to target specification increases process variability and increases costs significantly and is the cause of much inefficiency in operations. Process capability studies do examine the relationship between the natural process limits (the control limits) and specifications, however.\n\nThe purpose of control charts is to allow simple detection of events that are indicative of actual process change. This simple decision can be difficult where the process characteristic is continuously varying; the control chart provides statistically objective criteria of change. When change is detected and considered good its cause should be identified and possibly become the new way of working, where the change is bad then its cause should be identified and eliminated.\n\nThe purpose in adding warning limits or subdividing the control chart into zones is to provide early notification if something is amiss. Instead of immediately launching a process improvement effort to determine whether special causes are present, the Quality Engineer may temporarily increase the rate at which samples are taken from the process output until it is clear that the process is truly in control. Note that with three-sigma limits, common-cause variations result in signals less than once out of every twenty-two points for skewed processes and about once out of every three hundred seventy (1/370.4) points for normally distributed processes. The two-sigma warning levels will be reached about once for every twenty-two (1/21.98) plotted points in normally distributed data. (For example, the means of sufficiently large samples drawn from practically any underlying distribution whose variance exists are normally distributed, according to the Central Limit Theorem.)\n\nShewhart set \"3-sigma\" (3-standard deviation) limits on the following basis.\n\n\nShewhart summarized the conclusions by saying:\n\"... the fact that the criterion which we happen to use has a fine ancestry in highbrow statistical theorems does not justify its use. Such justification must come from empirical evidence that it works. As the practical engineer might say, the proof of the pudding is in the eating.\"\nAlthough he initially experimented with limits based on probability distributions, Shewhart ultimately wrote:\n\"Some of the earliest attempts to characterize a state of statistical control were inspired by the belief that there existed a special form of frequency function\" f \"and it was early argued that the normal law characterized such a state. When the normal law was found to be inadequate, then generalized functional forms were tried. Today, however, all hopes of finding a unique functional form\" f \"are blasted.\"\nThe control chart is intended as a heuristic. Deming insisted that it is not a hypothesis test and is not motivated by the Neyman–Pearson lemma. He contended that the disjoint nature of population and sampling frame in most industrial situations compromised the use of conventional statistical techniques. Deming's intention was to seek insights into the cause system of a process \"...under a wide range of unknowable circumstances, future and past...\" He claimed that, under such conditions, \"3-sigma\" limits provided \"... a rational and economic guide to minimum economic loss...\" from the two errors:\n\n\nThe sample size plays a critical role in the overall performance of any control chart. Many articles have studied the influence of the sample size on the performance of the control charts. It is found that the best sample size of the X bar & R and X bar & S charts is n = 3 for many tested cases.\n\nAs for the calculation of control limits, the standard deviation (error) required is that of the common-cause variation in the process. Hence, the usual estimator, in terms of sample variance, is not used as this estimates the total squared-error loss from both common- and special-causes of variation.\n\nAn alternative method is to use the relationship between the range of a sample and its standard deviation derived by Leonard H. C. Tippett, as an estimator which tends to be less influenced by the extreme observations which typify special-causes.\n\nThe most common sets are:\n\n\nThere has been particular controversy as to how long a run of observations, all on the same side of the centre line, should count as a signal, with 6, 7, 8 and 9 all being advocated by various writers.\n\nThe most important principle for choosing a set of rules is that the choice be made before the data is inspected. Choosing rules once the data have been seen tends to increase the Type I error rate owing to testing effects suggested by the data.\n\nIn 1935, the British Standards Institution, under the influence of Egon Pearson and against Shewhart's spirit, adopted control charts, replacing \"3-sigma\" limits with limits based on percentiles of the normal distribution. This move continues to be represented by John Oakland and others but has been widely deprecated by writers in the Shewhart–Deming tradition.\n\nWhen a point falls outside the limits established for a given control chart, those responsible for the underlying process are expected to determine whether a special cause has occurred. If one has, it is appropriate to determine if the results with the special cause are better than or worse than results from common causes alone. If worse, then that cause should be eliminated if possible. If better, it may be appropriate to intentionally retain the special cause within the system producing the results.\n\nEven when a process is \"in control\" (that is, no special causes are present in the system), there is approximately a 0.27% probability of a point exceeding \"3-sigma\" control limits. So, even an in control process plotted on a properly constructed control chart will eventually signal the possible presence of a special cause, even though one may not have actually occurred. For a Shewhart control chart using \"3-sigma\" limits, this \"false alarm\" occurs on average once every 1/0.0027 or 370.4 observations. Therefore, the \"in-control average run length\" (or in-control ARL) of a Shewhart chart is 370.4.\n\nMeanwhile, if a special cause does occur, it may not be of sufficient magnitude for the chart to produce an immediate \"alarm condition\". If a special cause occurs, one can describe that cause by measuring the change in the mean and/or variance of the process in question. When those changes are quantified, it is possible to determine the out-of-control ARL for the chart. \n\nIt turns out that Shewhart charts are quite good at detecting large changes in the process mean or variance, as their out-of-control ARLs are fairly short in these cases. However, for smaller changes (such as a \"1-\" or \"2-sigma\" change in the mean), the Shewhart chart does not detect these changes efficiently. Other types of control charts have been developed, such as the EWMA chart, the CUSUM chart and the real-time contrasts chart, which detect smaller changes more efficiently by making use of information from observations collected prior to the most recent data point.\n\nMany control charts work best for numeric data with Gaussian assumptions. The real-time contrasts chart was proposed to monitor process with complex characteristics, e.g. high-dimensional, mix numerical and categorical, missing-valued, non-Gaussian, non-linear relationship.\n\nSeveral authors have criticised the control chart on the grounds that it violates the likelihood principle. However, the principle is itself controversial and supporters of control charts further argue that, in general, it is impossible to specify a likelihood function for a process not in statistical control, especially where knowledge about the cause system of the process is weak.\n\nSome authors have criticised the use of average run lengths (ARLs) for comparing control chart performance, because that average usually follows a geometric distribution, which has high variability and difficulties.\n\nSome authors have criticized that most control charts focus on numeric data. Nowadays, process data can be much more complex, e.g. non-Gaussian, mix numerical and categorical, or be missing-valued. Moreover, control charts are widely applied in areas far beyond manufacturing and production, for example to monitor health care system surveillance, signal and image processing, duration of contract labour strikes and call centre waiting times. Traditional statistical process control procedures assume that the process distribution is normal. Unfortunately, this assumption is very often not met in many practical situations, see and . The actual false alarm rate of traditional process control procedures could be substantially different than the desired false alarm rate. In this context, nonparametric control charts are appealing.\n\nSome practitioners also recommend the use of Individuals charts for attribute data, particularly when the assumptions of either binomially distributed data (p- and np-charts) or Poisson-distributed data (u- and c-charts) are violated. Two primary justifications are given for this practice. First, normality is not necessary for statistical control, so the Individuals chart may be used with non-normal data. Second, attribute charts derive the measure of dispersion directly from the mean proportion (by assuming a probability distribution), while Individuals charts derive the measure of dispersion from the data, independent of the mean, making Individuals charts more robust than attributes charts to violations of the assumptions about the distribution of the underlying population. It is sometimes noted that the substitution of the Individuals chart works best for large counts, when the binomial and Poisson distributions approximate a normal distribution. i.e. when the number of trials for p- and np-charts or for u- and c-charts.\n\nCritics of this approach argue that control charts should not be used when their underlying assumptions are violated, such as when process data is neither normally distributed nor binomially (or Poisson) distributed. Such processes are not in control and should be improved before the application of control charts. Additionally, application of the charts in the presence of such deviations increases the type I and type II error rates of the control charts, and may make the chart of little practical use.\n\n\n\n"}
{"id": "505760", "url": "https://en.wikipedia.org/wiki?curid=505760", "title": "Counter-Enlightenment", "text": "Counter-Enlightenment\n\nThe Counter-Enlightenment was a term that some 20th-century commentators have used to describe multiple strains of thought that arose in the late-18th and early-19th centuries in opposition to the 18th-century Enlightenment. \n\nThough the first known use of the term in English was in 1949 and there were several uses of it, including one by German philosopher Friedrich Nietzsche, Counter-Enlightenment is usually associated with Isaiah Berlin, who is often credited for re-inventing it. The starting point of discussion on this concept in English started with Isaiah Berlin's 1973 Essay, \"The Counter-Enlightenment\"\".\" He published widely about the Enlightenment and its challengers and did much to popularise the concept of a Counter-Enlightenment movement that he characterized as relativist, anti-rationalist, vitalist, and organic, which he associated most closely with German Romanticism. \n\nDespite criticism of the Enlightenment has emerged as a widely discussed topic in twentieth-century thought, the term 'Counter-Enlightenment' was underdeveloped. It was first mentioned briefly in English in William Barrett's 1949 article \"Art, Aristocracy and Reason\" in \"Partisan Review.\" He used the term again in his 1958 book on existentialism, \"Irrational Man,\" however, his comment on Enlightenment criticism was very limited. In Germany, the expression \"Gegen-Aukflärung\" has a longer history. It was probably coined by Friedrich Nietzsche in \"Nachgelassene Fragmente\" in 1877. \n\nLewis White Beck used this term in his \"Early German Philosophy\" (1969), a book about Counter-Enlightenment in Germany. Beck claims that there is a counter-movement arising in Germany in reaction to Frederick II's secular authoritarian state. On the other hand, Johann Georg Hamann and his fellow philosophers believe that a more organic conception of social and political life, a more vitalistic view of nature, and an appreciation for beauty and the spiritual life of man have been neglected by the eighteenth century.\n\nIsaiah Berlin established this term's place in the history of ideas. He used it to refer to a movement that arose primarily in late 18th- and early 19th-century Germany against the rationalism, universalism and empiricism, which are commonly associated with the Enlightenment. Berlin's essay \"The Counter-Enlightenment\" was first published in 1973, and later reprinted in a collection of his works, \"Against the Current\", in 1981. The term has been more widely used since.\n\nBerlin argues that, while there were opponents of the Enlightenment outside of Germany (e.g. Joseph de Maistre) and before the 1770s (e.g. Giambattista Vico), Counter-Enlightenment thought did not start until the Germans 'rebelled against the dead hand of France in the realms of culture, art and philosophy, and avenged themselves by launching the great counter-attack against the Enlightenment.' This German reaction to the imperialistic universalism of the French Enlightenment and Revolution, which had been forced on them first by the francophile Frederick II of Prussia, then by the armies of Revolutionary France and finally by Napoleon, was crucial to the shift of consciousness that occurred in Europe at this time, leading eventually to Romanticism. The consequence of this revolt against the Enlightenment was pluralism. The opponents to the Enlightenment played a more crucial role than its proponents, some of whom were monists, whose political, intellectual and ideological offspring have been terreur and totalitarianism.\n\nIn his book \"Enemies of the Enlightenment\" (2001), historian Darrin McMahon extends the Counter-Enlightenment back to pre-Revolutionary France and down to the level of 'Grub Street,' thereby marking a major advance on Berlin's intellectual and Germanocentric view. McMahon focuses on the early opponents to the Enlightenment in France, unearthing a long-forgotten 'Grub Street' literature in the late 18th and early 19th centuries aimed at the \"philosophes\". He delves into the obscure world of the 'low Counter-Enlightenment' that attacked the \"encyclopédistes\" and fought to prevent the dissemination of Enlightenment ideas in the second half of the century. Many people from earlier times attacked the Enlightenment for undermining religion and the social and political order. It later became a major theme of conservative criticism of the Enlightenment. After the French Revolution, it appeared to vindicate the warnings of the \"anti-philosophes\" in the decades prior to 1789.\n\nCardiff University professor Graeme Garrard claims that historian William R. Everdell was the first to situate Rousseau as the \"founder of the Counter-Enlightenment\" in his 1971 dissertation and in his 1987 book, \"Christian Apologetics in France, 1730–1790: The Roots of Romantic Religion\". In his 1996 article, \"the Origin of the Counter-Enlightenment: Rousseau and the New Religion of Sincerity\", in the \"American Political Science Review\" (Vol. 90, No. 2), Arthur M. Melzer corroborates Everdell's view in placing the origin of the Counter-Enlightenment in the religious writings of Jean-Jacques Rousseau, further showing Rousseau as the man who fired the first shot in the war between the Enlightenment and its opponents. Graeme Garrard follows Melzer in his \"Rousseau's Counter-Enlightenment\" (2003). This contradicts Berlin's depiction of Rousseau as a \"philosophe\" (albeit an erratic one) who shared the basic beliefs of his Enlightenment contemporaries. But similar to McMahon, Garrard traces the beginning of Counter-Enlightenment thought back to France and prior to the German \"Sturm und Drang\" movement of the 1770s. Garrard's book \"Counter-Enlightenments\" (2006) broadens the term even further, arguing against Berlin that there was no single 'movement' called 'The Counter-Enlightenment'. Rather, there have been many Counter-Enlightenments, from the middle of the 18th century to 20th-century Enlightenment among critical theorists, postmodernists and feminists. The Enlightenment has opponents on all points of its ideological compass, from the far left to the far right, and all points in between. Each of the Enlightenment's challengers depicted it as they saw it or wanted others to see it, resulting in a vast range of portraits, many of which are not only different but incompatible.\n\nThe idea of Counter-Enlightenment has evolved in the following years. The historian James Schmidt questioned the idea of 'Enlightenment' and therefore of the existence of a movement opposing it. As the conception of 'Enlightenment' has become more complex and difficult to maintain, so has the idea of the 'Counter-Enlightenment'. Advances in Enlightenment scholarship in the last quarter-century have challenged the stereotypical view of the 18th century as an 'Age of Reason', leading Schmidt to speculate on whether the Enlightenment might not actually be a creation of its opponents, but the other way round. The fact that the term 'Enlightenment' was first used in 1894 in English to refer to a historical period supports the argument that it was a late construction projected back onto the 18th century.\n\nBy the mid-1790s, the Reign of Terror during the French Revolution fueled a major reaction against the Enlightenment. Many leaders of the French Revolution and their supporters made Voltaire and Rousseau, as well as Marquis de Condorcet's ideas of reason, progress, anti-clericalism, and emancipation central themes to their movement. It led to an unavoidable backlash to the Enlightenment as there were people oppose to the revolution. Many counter-revolutionary writers, such as Edmund Burke, Joseph de Maistre and Augustin Barruel, asserted an intrinsic link between the Enlightenment and the Revolution. They blamed the Enlightenment for undermining traditional beliefs that sustained the ancien regime. As the Revolution became increasingly bloody, the idea of 'Enlightenment' was discredited, too. Hence, the French Revolution and its aftermath have attributed to the development of Counter-Enlightenment thought. \n\nEdmund Burke was among the first of the Revolution's opponents to relate the \"philosophes\" to the instability in France in the 1790s. His \"Reflections on the Revolution in France\" (1790) refers the Enlightenment as the principle cause of the French revolution. In Burke's opinion, the \"philosophes\" provided the revolutionary leaders with the theories on which their political schemes were based on. \n\nAugustin Barruel's Counter-Enlightenment ideas were well developed before the revolution. He worked as an editor for the anti-\"philosophes\" literary journal, \"\". Barruel argues in his \"Memoirs Illustrating the History of Jacobinism\" (1797) that the Revolution was the consequence of a conspiracy of \"philosophes\" and freemasons. \n\nIn \"Considerations on France\" (1797), Joseph de Maistre interprets the Revolution as divine punishment for the sins of the Enlightenment. According to him, \"the revolutionary storm is an overwhelming force of nature unleashed on Europe by God that mocked human pretensions.\"\n\nIn the 1770s, the \"'Sturm und Drang\"' movement started in Germany. It questioned some key assumptions and implications of the \"Auflärung\" and the term 'Romanticism' was first coined. Many early Romantic writers such as Chateaubriand, Federich von Hardenberg (Novalis) and Samuel Taylor Coleridge inherited the Counter-Revolutionary antipathy towards the philosophes. All three directly blamed the \"philosophes\" in France and the \"Aufklärer\" in Germany for devaluing beauty, spirit and history in favour of a view of man as a soulless machine and a view of the universe as a meaningless, disenchanted void lacking richness and beauty. One particular concern to early Romantic writers was the allegedly anti-religious nature of the Enlightenment since the \"philosophes\" and \"Aufklarer\" were generally deists, opposed to revealed religion. Some historians, such as Hamann, nevertheless contend that this view of the Enlightenment as an age hostile to religion is common ground between these Romantic writers and many of their conservative Counter-Revolutionary predecessors. However, not many have commented on the Enlightenment, except for Chateaubriand, Novalis, and Coleridge, since the term itself did not exist at the time and most of their contemporaries ignored it.\nThe philosopher Jacques Barzun argues that Romanticism has its roots in the Enlightenment. It was not anti-rational, but rather balanced rationality against the competing claims of intuition and the sense of justice. This view is expressed in Goya's \"Sleep of Reason\", in which the nightmarish owl offers the dozing social critic of \"Los Caprichos,\" a piece of drawing chalk. Even the rational critic is inspired by irrational dream-content under the gaze of the sharp-eyed lynx. Marshall Brown makes much the same argument as Barzun in \"Romanticism and Enlightenment\", questioning the stark opposition between these two periods.\n\nBy the middle of the 19th century, the memory of the French Revolution was fading and so was the influence of Romanticism. In this optimistic age of science and industry, there were few critics of the Enlightenment, and few explicit defenders. Friedrich Nietzsche is a notable and highly influential exception. After an initial defence of the Enlightenment in his so-called 'middle period' (late-1870s to early 1880s), Nietzsche turned vehemently against it.\n\nIn the intellectual discourse of the mid-20th century, two concepts emerged simultaneously in the West: enlightenment and totalitarianism. After World War II, the former re-emerged as a key organizing concept in social and political thought and the history of ideas. The Counter-Enlightenment literature blaming the 18th-century trust in reason for 20th-century totalitarianism also resurged along with it. The \"locus classicus\" of this view is Max Horkheimer and Theodor Adorno's \"Dialectic of Enlightenment\" (1947), which traces the degeneration of the general concept of enlightenment from ancient Greece (epitomized by the cunning 'bourgeois' hero Odysseus) to 20th-century fascism. They mentioned little about Soviet communism, only referring to it as a regressive totalitarianism that \"clung all too desperately to the heritage of bourgeois philosophy\".\n\nThe authors take 'enlightenment' as their target including its 18th-century form – which we now call 'The Enlightenment'. They claim it is epitomized by the Marquis de Sade. However, there were philosophers rejecting Adorno and Horkheimer’s claim that Sade's moral skepticism is actually coherent, or that it reflects Enlightenment thought.\n\nMany postmodern writers and feminists (e.g. Jane Flax) have made similar arguments. They regard \"the\" Enlightenment conception of reason as totalitarian, and as not having been enlightened enough since. For Adorno and Horkheimer, though it banishes myth it falls back into a further myth, that of individualism and formal (or mythic) equality under instrumental reason.\n\nMichel Foucault, for example, argued that attitudes towards the \"insane\" during the late-18th and early 19th centuries show that supposedly enlightened notions of humane treatment were not universally adhered to, but instead, the Age of Reason had to construct an image of \"Unreason\" against which to take an opposing stand. Berlin himself, although no postmodernist, argues that the Enlightenment's legacy in the 20th century has been monism (which he claims favours political authoritarianism), whereas the legacy of the Counter-Enlightenment has been pluralism (associates with liberalism). These are two of the 'strange reversals' of modern intellectual history.\n\nWhat seems to unite all of the Enlightenment's disparate critics from 18th-century religious opponents, counter-revolutionaries, to Romantics, to 20th-century conservatives, feminists, critical theorists and environmentalists is a rejection of what they consider to be the Enlightenment's perversion of reason: the distorted conceptions of reason associate with the Enlightenment in favour of a more restricted view of the nature, scope and limits of human rationality. Debates have occurred over the scope, meaning and application of reason, not over whether it is good or bad, desirable or undesirable, essential or inessential per se. Some charge that the Enlightenment inflated the power and scope of reason, while others claim that it narrowed it.\n\n\n"}
{"id": "41283739", "url": "https://en.wikipedia.org/wiki?curid=41283739", "title": "Creative synthesis", "text": "Creative synthesis\n\nThe principle of creative synthesis was first mentioned by Wilhelm Wundt in 1862. He wanted to identify the different elements of conscious and to see what laws govern the connections of these different elements. It started with the fact that colors, touches, and the spoken were not seen as the decoding of stimuli or the reception and storage of the things that are received into the brain from the external world. Wundt believed that instead, these factors are seen as the brain's subjective reactions to external stimuli that enter into our sensory systems. This is the concept of creative synthesis.\n\nThis theory shifted towards the emphasis on principles concerned with emotion, motivation, and volition as it had matured. These three ideas compete with one another, with the idea of creative synthesis at the center. This relates to the fact that Wundt viewed the mind as \"active, creative, dynamic, and volitional.\" Volitional acts are creative but they are not free. This viewpoint could be assumed due to Wundt's deterministic view. Behind every volitional action that occurs, there were mental laws that acted on the contents of the consciousness. The shift in the goal-directed activity may have occurred, but it was already determined to change from the original plan.\nThe sensory organs can be described endlessly in physics and other sciences, but these descriptions do not include explanations of the psychological qualities that are experienced. Qualities such as \"sweet\", \"heavy\", \"painful\" or \"dark blue\" are ones that can only be studied in a brain that is still able to react to experiences around it.\nA key feature of creative synthesis is that mental capacities are more than the sum of their parts. In all psychical combinations, the product is more than the sum of their different parts that are combined; what occurs is a new creation altogether. By this, it is meant that they are generative (creative) in every aspect. There is a real novelty and creativity in higher cognitive operations.\n\nThere is a two-stage process for consciousness. The first is a large-capacity short-term memory, which was sometimes referred to as the Blickfield. The second is a narrow-capacity focus of selection attention, or apperception, under voluntary control. The second moves through the first. Wundt's main difference between his position and that of empiricists was that he emphasized the role of attention. When someone pays attention to elements, these elements can be arranged and rearranged according to that person's will. This is how things that have not actually been experienced, can result in the brain as if they had. Wundt believed that creative synthesis was entwined with all acts of apperception. It was believed by Wundt that this apperceptive process was important for normal cognitive functioning.\n\nThe creative synthesis principle was continually being expanded Factors regarding this are:\n\n\nThe principle of contrasts is the idea that opposite experiences intensify one another. For example, a pleasant experience always seems more pleasant if it follows one that is interpreted as painful. A theory that is somewhat similar to this is the principle toward the development of opposites. This is the phenomena that occurs after a prolonged experience of one type, there becomes an increasing tendency to seek out the opposite experience\n\nA major manifestation of creative synthesis is the concept of heterogony of ends. Heterogony of ends is the development of new motives during the series of events. There is an exchange of motives in our immediate experience and it defines the social behaviors that are engaged in as well as, the cognitive reaction to the world that occurs. An example of this would be going to the store with the intention of buying food and to come home to make dinner. But after getting to the store, you run into a friend that you have not seen in a long time, who you have missed. This adds a new set of motives to the pre-existing or original motives. There is almost always something that happens that changes a person's entire motivational pattern.\n"}
{"id": "1744868", "url": "https://en.wikipedia.org/wiki?curid=1744868", "title": "Damping ratio", "text": "Damping ratio\n\nDamping is an influence within or upon an oscillatory system that has the effect of reducing, restricting or preventing its oscillations. In physical systems, damping is produced by processes that dissipate the energy stored in the oscillation. Examples include viscous drag in mechanical systems, resistance in electronic oscillators, and absorption and scattering of light in optical oscillators. Damping not based on energy loss can be important in other oscillating systems such as those that occur in biological systems and bikes.\n\nThe damping ratio is a dimensionless measure describing how oscillations in a system decay after a disturbance. Many systems exhibit oscillatory behavior when they are disturbed from their position of static equilibrium. A mass suspended from a spring, for example, might, if pulled and released, bounce up and down. On each bounce, the system tends to return to its equilibrium position, but overshoots it. Sometimes losses (e.g. frictional) damp the system and can cause the oscillations to gradually decay in amplitude towards zero or attenuate. The damping ratio is a measure describing how rapidly the oscillations decay from one bounce to the next.\n\nThe damping ratio is a system parameter, denoted by ζ (zeta), that can vary from undamped (ζ=0), underdamped (ζ<1) through critically damped (ζ=1) to overdamped (ζ>1).\n\nThe behaviour of oscillating systems is often of interest in a diverse range of disciplines that include control engineering, chemical engineering, mechanical engineering, structural engineering, and electrical engineering. The physical quantity that is oscillating varies greatly, and could be the swaying of a tall building in the wind, or the speed of an electric motor, but a normalised, or non-dimensionalised approach can be convenient in describing common aspects of behavior.\n\nDepending on the amount of damping present, a system exhibits different oscillatory behaviors.\n\nThe \"damping ratio\" is a parameter, usually denoted by ζ (zeta), that characterizes the frequency response of a second-order ordinary differential equation. It is particularly important in the study of control theory. It is also important in the harmonic oscillator.\n\nThe damping ratio provides a mathematical means of expressing the level of damping in a system relative to critical damping. For a damped harmonic oscillator with mass \"m\", damping coefficient \"c\", and spring constant \"k\", it can be defined as the ratio of the damping coefficient in the system's differential equation to the critical damping coefficient:\n\nwhere the system's equation of motion is\n\nand the corresponding critical damping coefficient is\n\nor\nwhere\n\nThe damping ratio is dimensionless, being the ratio of two coefficients of identical units.\n\nUsing the natural frequency of a harmonic oscillator formula_7 and the definition of the damping ratio above, we can rewrite this as:\n\nThis equation can be solved with the approach.\n\nwhere \"C\" and \"s\" are both complex constants, with \"s\" satisfying\n\nTwo such solutions, for the two values of \"s\" satisfying the equation, can be combined to make the general real solutions, with oscillatory and decaying properties in several regimes:\n\n\nThe Q factor, damping ratio ζ, and exponential decay rate α are related such that\n\nWhen a second-order system has formula_18 (that is, when the system is underdamped), it has two complex conjugate poles that each have a real part of formula_19; that is, the decay rate parameter formula_20 represents the rate of exponential decay of the oscillations. A lower damping ratio implies a lower decay rate, and so very underdamped systems oscillate for long times. For example, a high quality tuning fork, which has a very low damping ratio, has an oscillation that lasts a long time, decaying very slowly after being struck by a hammer.\n\nFor underdamped vibrations, the damping ratio is also related to the logarithmic decrement formula_21 via the relation\nwhere formula_23 and formula_24 are the vibration amplitudes at two successive peaks of the decaying vibration.\n"}
{"id": "1190842", "url": "https://en.wikipedia.org/wiki?curid=1190842", "title": "Decision analysis", "text": "Decision analysis\n\nDecision analysis (DA) is the discipline comprising the philosophy, methodology, and professional practice necessary to address important decisions in a formal manner. Decision analysis includes many procedures, methods, and tools for identifying, clearly representing, and formally assessing important aspects of a decision, for prescribing a recommended course of action by applying the maximum expected utility action axiom to a well-formed representation of the decision, and for translating the formal representation of a decision and its corresponding recommendation into insight for the decision maker and other stakeholders.\n\nGraphical representation of decision analysis problems commonly use framing tools, influence diagrams and decision trees. Such tools are used to represent the alternatives available to the decision maker, the uncertainty they involve, and evaluation measures representing how well objectives would be achieved in the final outcome. Uncertainties are represented through probabilities. The decision maker's attitude to risk is represented by utility functions and their attitude to trade-offs between conflicting objectives can be expressed using multi-attribute value functions or multi-attribute utility functions (if there is risk involved). In some cases, utility functions can be replaced by the probability of achieving uncertain aspiration levels. Decision analysis advocates choosing that decision whose consequences have the maximum expected utility (or which maximize the probability of achieving the uncertain aspiration level). Such decision analytic methods are used in a wide variety of fields, including business (planning, marketing, negotiation), environmental remediation, health care, research, and management, energy, exploration, litigation and dispute resolution, etc.\n\nFraming is the front end of decision analysis, which focuses on developing an opportunity statement (what & why), boundary conditions, success measures, decision hierarchy, strategy table, action items. The frame may lead to developing of an influence diagram for more complex analyses and is useful in developing a quantitative model when needed. \n\nDecision analysis is used by major corporations to make multibillion-dollar capital investments and can be used to make more complex but personal decisions like planning retirement or planning a vacation. \n\nIn 2010, Chevron won the Decision Analysis Society Practice Award for its use of decision analysis in all major decisions. In a video detailing Chevron's use of decision analysis, Chevron Vice Chairman George Kirkland notes that \"decision analysis is a part of how Chevron does business for a simple, but powerful, reason: it works.\"\n\nDecision analysis, a prescriptive approach, especially concerned with quantitatively dealing with uncertainties (prescriptive decision-making researches how optimal decisions could be made, while descriptive decision-making targets to explain how people actually make decisions, regardless of decision quality), is found to be in fact rarely used in the decision-making of individuals. The hiatus between prescriptive decision analysis and descriptive approaches is greater in high-stakes decisions, made under time pressure.\nDecision analysts argue that it is not their aim to study the flaws in the way people actually make decisions. Studies have demonstrated the utility of decision analysis in creating decision-making algorithms that are superior to \"unaided intuition\".\n\nCritics cite the phenomenon of paralysis by analysis as one possible consequence of over-reliance on decision analysis in organizations (the expense of decision analysis is in itself a factor in the analysis). Strategies are available to reduce such risk.\n\nThe term \"decision analytic\" has often been reserved for decisions that do not appear to lend themselves to mathematical optimization methods. Methods like applied information economics, however, attempt to apply more rigorous quantitative methods even to these types of decisions.\n\nThere is some confusion in that decision analysis is all about quantitative methods but in reality, many decisions and strategy decisions may be developed solely using framing methods without or with little quantitative methods required.\n\n\n"}
{"id": "48901154", "url": "https://en.wikipedia.org/wiki?curid=48901154", "title": "Double Fourier sphere method", "text": "Double Fourier sphere method\n\nIn mathematics, the double Fourier sphere (DFS) method is a simple technique that transforms a function defined on the surface of the sphere to a function defined on a rectangular domain while preserving periodicity in both the longitude and latitude directions.\n\nFirst, a function formula_1 on the sphere is written as formula_2 using spherical coordinates, i.e.,\n\nThe function formula_4 is formula_5-periodic in formula_6, but not periodic in formula_7. The periodicity in the latitude direction has been lost. To recover it, the function is \"doubled up” and a related function on formula_8 is defined as\n\nwhere formula_10 and formula_11 for formula_12. The new function formula_13 is formula_5-periodic in formula_6 and formula_7, and is constant along the lines formula_17 and formula_18, corresponding to the poles.\n\nThe function formula_13 can be expanded into a double Fourier series\n\nThe DFS method was proposed by Merilees and developed further by Steven Orszag. The DFS method has been the subject of relatively few investigations since (a notable exception is Fornberg's work), perhaps due to the dominance of spherical harmonics expansions. Over the last fifteen years it has begun to be used for the computation of gravitational fields near black holes and to novel space-time spectral analysis.\n"}
{"id": "4241160", "url": "https://en.wikipedia.org/wiki?curid=4241160", "title": "ESeL", "text": "ESeL\n\neSeL is an art platform in Vienna, Austria. Founded 1998 by Lorenz Seidler, it provides a weekly newsletter \"eSeL Mehl\", various mailing lists, a photo-archive and an event-database. The label \"eSeL\" is also serving as a nickname for the artist Lorenz Seidler (originating in the phonetic pronunciation of his initials), who also is initiating, curating and conducting various art projects. eSeL's offices are located at Museumsquartier in Vienna.\n\nThe label eSeL (also serving as a nickname of founder Lorenz Seidler) represents the crossover of the roles of artist, curator, online medium and infrastructure-provider in contemporary artistic practice in the new media genres.\n\n\"Esel\" means \"donkey\" in German. The label originates in the phonetic pronunciation of its founder's initials and soon became a standalone brand, expanding its animal analogies into a multitude of animals-participants. Qualities attributed to the eponymous animal are consciously matching with the initiative's characteristics: stubbornness (autonomy and independence), grey fur (searching for options beyond the polarity of \"right/wrong\" or \"good/bad\"), long ears (overhearing insider's information), asininity (asking alleged “stupid” questions).\n\neSeL aims at revealing and changing conventions and coherence in the art field, putting a special focus on structural implications and representation in the media sphere.\n\n\"Kunst kommt von Kommunizieren\" (\"art derives from communication\") is eSeL's slogan - Contributions of the term \"art\" are shown as a constant process of (transitory) quality agreements.\n\nThe eSeL-initiative grew from an independent online medium for of art information and event listings to an institutional carrier and server for art activities in Vienna, combined with focused interventions through projects, performances, and exhibitions.\n\nReaching a young audience beyond the art circles, eSeL's information services deliberately feature upcoming initiatives next to well-established art positions from all fields of artistic practice in Vienna (Fine Arts, performance, dance, art-in-public space, film, media art, music and civil society's artistic activities, inviting participation by new artists as well as the audience).\n\nThe website www.eSeL.at offers a selected database/calendar covering art events in Vienna and surrounding areas, a weekly newsletter (\"eSeL Mehl\") as well as a photo archive.. The calendar eSeL.at is divided into daily overviews by date and into the categories \"eSeL Neugierde\" (editorial recommendation), \"hAmSteR Events\" (scene), „Maultier Kunst\" (fine art), \"Uhu Diskurs\" (discourse / mediation) , \"Ameisen Urbanismus\" (Architecture / City), \"Nerz Techleben\" (Internet / Technology), \"Flimmer Ratte\" (Film / Video Art), \"Kanari Klangwelten\" (Sound Art / Music), \"Tauben Loge\" (Performance / Choreography ), \"Pudel Design\" (design), \"Public Access\" (society / participation), \"Eselchen Kinderprogramm\" (children / family) and „nicht in Wien“ (not in Vienna). \n\nThe addition of Social Media functionality to the eSeL-Website (\"eSeL 2.0.\") was awarded the second prize at the \"IG Kultur Wien Innovation Award 2010\".\n\nThe newsletter „eSeL Mehl“ is sent out every Thursday and offers an excerpt from the eSeL.at event database for the next seven days. The newsletter reaches over 10,000 subscribers weekly (February 2017). Each issue contains a photo selection compiled by Lorenz \"eSeL\" Seidler from the photographic documentation of current exhibitions and art events.\n\nThe \"eSeL RECEPTION\" at Q21 at the MuseumsQuartier supplements esel.at since 2011 as an \"open office\" with an \"analogue\" information portal and with an exhibition and workshop area. It offers, among other things, a library with exhibition catalogs as well as a moderated flyer box. The eSeL reception serves as a low-threshold mediation of art-happenings for the visitors of the MuseumsQuartier and as a venue for events such as the series \"Kunst & SpieleN\".\n\n\nGroup exhibitions:\n\nPerformances:\n\nInfrastructure projects:\n\n\n\n"}
{"id": "5139914", "url": "https://en.wikipedia.org/wiki?curid=5139914", "title": "EVA Conferences", "text": "EVA Conferences\n\nThe Electronic Visualisation and the Arts conferences (EVA Conferences for short, aka Electronic Information, the Visual Arts and Beyond) are a series of international interdisciplinary conferences mainly in Europe, but also elsewhere in the world, for people interested in the application of information technology to the cultural and especially the visual arts field, including art galleries and museums. \n\nStarted in London (UK), there are now EVA conferences in Berlin (Germany), Florence (Italy), Jerusalem (Israel), St Petersburg (formerly in Moscow, Russia), Australasia (first time in Canberra, Australia in 2016) and other major cities. The first EVA Conference was held at Imperial College, London in 1990, organised by the founders James Hemsley, Kirk Martinez, and Anthony Hamber.\n\nThe conferences have been overseen by EVA Conferences International, based in London. Conference proceedings are published. In addition, two collected volumes of revised papers are available.\n\nThe EVA London conference, founded in 1990 by James Hemsley, is now organised through the Computer Arts Society (CAS), a Specialist Group of the British Computer Society (BCS) each July at the BCS London office.\n\nSome V&A Digital Futures events organised by the Victoria and Albert Museum are held in conjunction with EVA London. In 2016, it hosted an event for the Lumen Prize, an annual award for digital art. The proceedings have published through the BCS \"Electronic Workshops in Computing\" (eWiC) series since 2008 and are indexed by DBLP.\n\n\n"}
{"id": "15907019", "url": "https://en.wikipedia.org/wiki?curid=15907019", "title": "Ecological design", "text": "Ecological design\n\nEcological design is defined by Sim Van der Ryn and Stuart Cowan as \"any form of design that minimizes environmentally destructive impacts by integrating itself with living processes.\" Ecological design is an integrative ecologically responsible design discipline. \n\nIt helps connect scattered efforts in green architecture, sustainable agriculture, ecological engineering, ecological restoration and other fields. The “eco” prefix was used to ninety sciences including eco-city, eco-management, eco-technique, eco-tecture. It was first used by John Button in 1998. The inchoate developing nature of ecological design was referred to the “adding in “of environmental factor to the design process, but later it was focused on the details of eco-design practice such as product system or individual product or industry as a whole.\nBy including life cycle models through energy and materials flow, ecological design was related to the new interdisciplinary subject of industrial ecology. Industrial ecology meant a conceptual tool emulating models derived from natural ecosystem and a frame work for conceptualizing environmental and technical issues.\n\nLiving organisms exist in various systems of balanced symbiotic relationships. The ecological movement of the late twentieth-century is based on understanding that disruptions in these relationships has led to serious breakdown of natural ecosystems. In human history, technological means have resulted in growth of human populations through fire, implements and weapons. This dramatic increase in explosive population contributed the introduction of mechanical energies in machine production and there have been improvements in mechanized agriculture, manufactured chemical fertilizers and general health measures. Although the earlier invention inclined energy adjusting the ecological balance, population growth following the industrial revolution led to abnormal ecological change.\n\nSince the Industrial Revolution, many propositions in the design field were raised with unsustainable design principles. The architect-designer Victor Papanek suggested that industrial design has murdered by creating new species of permanent garbage and by choosing materials and processes that pollute the air. For these issues, R. Buckminster Fuller, who was invited as University Professor at Southern Illinois University in Carbondale in 1960s, demonstrated how design could play a central role in identifying major world problems between 1965 and 1975. That included following contents:\nIn the 1992 conference, ‘The Agenda 21: The Earth Summit Strategy to Save Our Planet”, a proposition was put forward that our world is on a path of energy production and consumption that cannot be sustained. The report drew attention to Individuals and groups around the world who have a set of principles to develop strategies for change that might be effective in world economics and trade policies, and the design professions will play a role in it. Namely, those meant that design profession becomes not what new products to make, but how to reinvent design culture likely to be realized. He noted designers firstly have to realize that design has historically been a dependent, contingent practice rather than one based on necessity. The design theorist, Clive Dilnot noted design becomes once again a means of ordering the world rather than merely of shaping products. As a broader approach, the conference of ‘Agenda 21: The Earth Summit Strategy to Save Our Planet’ 1992, emphasized that designers should challenge for facing human problems. These problems were mentioned to six themes: quality of life, efficient use of natural resources, protecting the global commons, managing human settlements, the use of chemicals and the management of human industrial waste, and fostering sustainable economic growth on a global scale.\n\n\nThere are some clothing companies that are using several ecological design methods to change the future of the textile industry into a more environmentally friendly one. Recycling used clothing to minimize the use of resources, using biodegradable textile materials to reduce the impact on the environment, and using plant dyes instead of poisonous chemicals to improve the appearance of fabric.\n\n\n"}
{"id": "6947256", "url": "https://en.wikipedia.org/wiki?curid=6947256", "title": "Evolution (TV series)", "text": "Evolution (TV series)\n\nEvolution is a 2001 documentary series by the American broadcaster Public Broadcasting Service (PBS) and WGBH on evolutionary biology, from the producers of \"NOVA\".\nThe spokespeople for the series were Jane Goodall (overall spokesperson), Kenneth R. Miller and Stephen Jay Gould (science spokespeople), Eugenie C. Scott (education spokesperson), Arthur Peacocke and Arnold Thomas (religious spokespeople). The series was narrated by the Irish actor Liam Neeson.\n\nThe series was accompanied by a book by the popular science writer Carl Zimmer \"\". An extensive website provides teaching resources for each episode's material, including \"The Mating Game\", further looks at Charles Darwin, and an interactive history of speciation in the invented \"pollencreeper\" birds.\n\nThe episode \"What about God?\" features discussion of the issues of evolution and creationism at Wheaton College, an Evangelical Protestant college that teaches evolution but has in the past restricted professors from taking a stance on the literal versus the allegorical interpretations of Adam and Eve in the Genesis account of creation.\n\n\nTV critic Julie Salamon, writing in \"The New York Times\", said that \"[a] powerful sense of drama, discovery and intellectual enthusiasm runs through this rich eight-hour series ... The series covers an enormous amount of ground but doesn't leave you feeling swamped.\"\n\nBeing made and broadcast in the country where creation-evolution controversy is strongest, the last episode \"What About God?\" focused on religion, and \"through personal stories of students and teachers, it offers the view that they are compatible\". Phina Borgeson, Faith Network Director of the National Center for Science Education, provided a Congregational Study Guide for Evolution. Conversely, the Discovery Institute's Center for the Renewal of Science and Culture produced a website to refute the documentary and started a petition it called A Scientific Dissent From Darwinism to show that there were \"scientists that dispute the claims\".\n\n"}
{"id": "369981", "url": "https://en.wikipedia.org/wiki?curid=369981", "title": "Glossary of tensor theory", "text": "Glossary of tensor theory\n\nThis is a glossary of tensor theory. For expositions of tensor theory from different points of view, see:\n\n\nFor some history of the abstract theory see also Multilinear algebra.\n\n\nThe earliest foundation of tensor theory – tensor index notation.\n\n\nThe components of a tensor with respect to a basis is an indexed array. The \"order\" of a tensor is the number of indices needed. Some texts may refer to the tensor order using the term \"degree\" or \"rank\".\n\n\nThe rank of a tensor is the minimum number of rank-one tensor that must be summed to obtain the tensor. A rank-one tensor may be defined as expressible as the outer product of the number of nonzero vectors needed to obtain the correct order.\n\n\nA \"dyadic\" tensor is a tensor of order two, and may be represented as a square matrix. In contrast, a \"dyad\" is specifically a dyadic tensor of rank one.\n\n\nThis notation is based on the understanding that whenever a term in an expression contains a repeated index letter, the default interpretation is that the product is summed over all permitted values of the index. For example, if \"a\" is a matrix, then under this convention \"a\" is its trace. The Einstein convention is widely used in physics and engineering texts, to the extent that if summation is not to be applied, it is normal to note that explicitly.\n\n\n\n\n\nThe classical interpretation is by components. For example, in the differential form \"adx\" the components \"a\" are a covariant vector. That means all indices are lower; contravariant means all indices are upper.\n\n\nThis refers to any tensor that has both lower and upper indices.\n\nCartesian tensor\n\nCartesian tensors are widely used in various branches of continuum mechanics, such as fluid mechanics and elasticity. In classical continuum mechanics, the space of interest is usually 3-dimensional Euclidean space, as is the tangent space at each point. If we restrict the local coordinates to be Cartesian coordinates with the same scale centered at the point of interest, the metric tensor is the Kronecker delta. This means that there is no need to distinguish covariant and contravariant components, and furthermore there is no need to distinguish tensors and tensor densities. All Cartesian-tensor indices are written as subscripts. Cartesian tensors achieve considerable computational simplification at the cost of generality and of some theoretical insight.\n\n\n\n\n\n\nThis avoids the initial use of components, and is distinguished by the explicit use of the tensor product symbol.\n\nIf \"v\" and \"w\" are vectors in vector spaces \"V\" and \"W\" respectively, then\n\nis a tensor in\n\nThat is, the ⊗ operation is a binary operation, but it takes values into a fresh space (it is in a strong sense \"external\"). The ⊗ operation is a bilinear map; but no other conditions are applied to it.\n\nA pure tensor of \"V\" ⊗ \"W\" is one that is of the form \"v\" ⊗ \"w\"\n\nIt could be written dyadically \"ab\", or more accurately \"ab\" e ⊗ f, where the e are a basis for \"V\" and the f a basis for \"W\". Therefore, unless \"V\" and \"W\" have the same dimension, the array of components need not be square. Such \"pure\" tensors are not generic: if both \"V\" and \"W\" have dimension greater than 1, there will be tensors that are not pure, and there will be non-linear conditions for a tensor to satisfy, to be pure. For more see Segre embedding.\n\nIn the tensor algebra \"T\"(\"V\") of a vector space \"V\", the operation formula_3 becomes a normal (internal) binary operation. A consequence is that \"T\"(\"V\") has infinite dimension unless \"V\" has dimension 0. The free algebra on a set \"X\" is for practical purposes the same as the tensor algebra on the vector space with \"X\" as basis.\n\nThe wedge product is the anti-symmetric form of the ⊗ operation. The quotient space of \"T\"(\"V\") on which it becomes an internal operation is the \"exterior algebra\" of \"V\"; it is a graded algebra, with the graded piece of weight \"k\" being called the \"k\"-th exterior power of \"V\".\n\nThis is the invariant way of constructing polynomial algebras.\n\n\n\n\n\n\n\n\n\n\n\nThis is an operation on fields, that does not always produce a field.\n\n\n\nA representation of a Clifford algebra which gives a realisation of a Clifford algebra as a matrix algebra.\n\n\nThese are the derived functors of the tensor product, and feature strongly in homological algebra. The name comes from the torsion subgroup in abelian group theory.\n\n\n\n\nThese are \"highly\" abstract approaches used in some parts of geometry.\n\nSee:\n\n\n\n\n\n\n\n\n"}
{"id": "35176447", "url": "https://en.wikipedia.org/wiki?curid=35176447", "title": "Golem effect", "text": "Golem effect\n\nThe Golem effect is a psychological phenomenon in which lower expectations placed upon individuals either by supervisors or the individual themselves lead to poorer performance by the individual. This effect is mostly seen and studied in educational and organizational environments. It is a form of self-fulfilling prophecy.\n\nThe effect is named after the golem, a clay creature that was given life by Rabbi Loew of Prague in Jewish mythology. According to the legend, the golem was originally created to protect the Jews of Prague; however, over time, the golem grew more and more corrupt to the point of spiraling violently out of control and had to be destroyed. The effect was named after the golem legend in 1982 by Babad, Inbar, and Rosenthal because it \"represent[s] the concerns of social scientists and educators, which are focused on the negative effects of self-fulfilling prophecies\".\n\nThe Golem effect has very similar underlying principles to its theoretical counterpart, the Pygmalion effect. Robert Rosenthal and Lenore Jacobson's \"Pygmalion in the Classroom\" and further experiments have shown that expectations of supervisors or teachers affect the performance of their subordinates or students. The most thoroughly studied situations of this effect are classrooms. When arbitrarily informed that a particular student is \"bright\" or \"dull\", not only will the supervisor's behavior change to favor the \"bright\" students (as indicated by more praise or attention), the students themselves will exhibit behaviors in line with their labels (such as the \"bright\" students leaning more forward in their chairs relative to the \"dull\" students). While the Pygmalion effect and the majority of studies focus on the positive side of this phenomenon, the Golem effect is the negative corollary. Supervisors with negative expectations will produce behaviors that impair the performance of their subordinates while the subordinates themselves produce negative behaviors. This mechanism is an example of a self-fulfilling prophecy: the idea that self-held beliefs can come true in reality. When both supervisor and subordinate notice the low performance, the negative expectations are confirmed and the belief is reinforced.\n\nUp until Babad, Inbar, and Rosenthal, studies on teacher/supervisor expectancy and its effect on performance had primarily focused on the Pygmalion effect. Babad actually investigated the effect in his 1977 paper looking at developmentally challenged students but his 1982 paper is considered the seminal Golem effect article due to its more generalizable student population. As opposed to other past teacher-student expectancy studies, the authors asked their teachers to nominate three high-expectancy and three low-expectancy students out of each class instead of just high-expectancy nominations and a control group. In addition to replicating the findings of previous Pygmalion effect studies, the authors found support for the Golem effect. Teachers who were susceptible to biasing information treated their low-expectancy students more dogmatically than their high-expectancy students. Consequently, low-expectancy students performed worse than their high-expectancy counterparts. Teachers who were not susceptible to bias did not show any distinctions in behavior between high and low-expectancy students.\n\nAlthough the majority of research looking at the Golem effect has focused on educational contexts, the effect has also been studied in the workplace. A study by Schrank that predated the Rosenthal and Jacobson article looked at US Air Force Academy airmen. The author induced a \"labeling effect\" by randomly assigning incoming freshmen to one of five class sections supposedly designating ability levels. McNatt performed a meta-analysis on studies with workplace samples and found that the Golem (and Pygmalion) effects still hold true to around the same magnitude at the workplace as they do in the classroom. Furthermore, the Golem effect can influence entire organizations, not just supervisors and their direct subordinates.\n\nDavidson and Eden suggested there are two different types of Golem effects: absolute and relative. The absolute Golem effect occurs when the individuals who are identified as the low tier of their group are in fact underqualified for their group. For any given normal distribution of students or employees, this may be the case; there will be some individuals who do not meet the performance standards of the group. However, the more potentially dangerous type of effect is the relative Golem effect. In this case, the entire population is qualified to be in the group. However, because there will always be a \"lower tier\" even for a group of individuals who meet all of the performance standards of the group, the Golem effect could potentially degrade the performance of even highly skilled individuals. Davidson and Eden suggested a number of \"de-Golemization\" efforts such as convincing the group that the initial performance measures underestimate true potential in order to reduce this threat.\n\nAlthough the consequences of the Pygmalion/Golem effects are well documented, the mechanisms behind them are more disputed among researchers. Both effects have been argued to stem from Victor Vroom's expectancy theory. This theory posits that people are more likely to perform behaviors that they believe they have a high expectation of performing successfully. In relation to the Golem effect, when expectations are set low by the supervisor, subordinates do not require as much effort to successfully reach their performance expectation, which consequently results in lower performance. Rowe and O'Brian argued that the Golem effect was a result of transaction cost and agency theories. They posit that because teachers monitor their classes for opportunistic behaviors, some students may take such monitoring as a sign that the teacher doesn't trust them and, in turn, engage in opportunistic behavior because it is expected of them. Although there have been proposed models of self-fulfilling prophecies including the Pygmalion/Golem effect, no model has been empirically tested. This lack of research is especially glaring considering the golem effect is heavily involved with other established motivational theories and organizational behavior concepts such as self-efficacy, leader-member exchange, and transformational leadership.\n\nThere is currently a relative paucity of research that directly addresses Golem effects, and an even lesser body that measures and examines it. There are a multitude of reasons cited for this scarcity, but the most common reason involves the ethical concerns raised in examining negative and potentially harmful phenomenon. Specifically, the concern arises in trying to operationalize negative expectancies in individuals, which will theoretically result in their lower performance. The worry then is the possible harmful, lingering effects on research participants beyond the study due to this manipulation. These effects could originate either from the participant having the knowledge that they performed worse than others, were unwittingly manipulated to perform worse, or were viewed negatively by a superior in the research paradigm; on the other hand, participants in a position of superiority that were manipulated to have negative expectancies may feel guilty about treating others differently following the experiment. Whatever the exact effect may be, these concerns have resulted in many researchers only making passive mention of the Golem effect in studies or ignoring it entirely.\n\nHowever, there is reason to believe that the apprehension towards conducting Golem studies may not be entirely well founded, as evidenced by several studies that have successfully set out to measure the effect explicitly. For example, Feldman & Prohaska used confederate subordinates to elicit negative expectations from subjects acting as students or teachers; in doing so, the ethical concerns of subjects having to be direct \"victims\" of the Golem effect were avoided. Oz & Eden designed a study in which military squad leaders were differentiated by treatment and control conditions. In the treatment condition, squad leaders' perceptions were manipulated as to believe that low scores on a physical fitness test were not indicative of a subordinate's ineptitude, whereas the control condition involved no manipulation. Thus, the Golem effect was measured indirectly through theoretically creating a \"buffer\" from the effect in the treatment condition. In this manner, the experimenters never actually created the Golem effect in their participants; rather, they measured a naturally occurring Golem effect in comparison to a \"Golem treatment\" group.\n\nThere is evidence that even studies that directly generate and measure Golem effects in participants are still very viable and can be passed by ethics boards and other regulatory bodies. Reynolds designed a study in which support instructors for an introductory management course were led to believe they were assigned either lower-performing or higher-performing students based on a pretest, although the actual assignment was completely random and arbitrary. He was able to demonstrate the Golem effect from this manipulation on a posttest (in which the \"lower-performing\" students actually performed worse, and the \"higher-performing students\" performed better), showing that it is very feasible to design studies that measure the effect in a more direct and controlled fashion without being shut down by regulatory bodies. However, studies such as this are regrettably still extremely scarce in current Golem research.\n\nCompared to the Golem effect, the Pygmalion effect enjoys a far greater body of literature; this is most likely due to the fact that this research is free from the ethical challenges of examining Golem effects. More specific and involved discussion of the Pygmalion effect is beyond the scope of this article, but several recent studies on this effect are worth mentioning in regards to their implications for the Golem effect and future research. Although the Pygmalion effect has been studied in great detail using experimental and quasi-experimental designs, due to the methodological issues surrounding the Golem effect, most of the conclusions drawn on the Golem effect have been from correlational data from Pygmalion studies.\n\nRecent research has explored the Pygmalion effect in cultural settings not previously studied. For example, a recent study examined how Japanese humanitarian aid workers stationed in different countries across the world perceived and interacted with the local organizations that they consulted with. The researchers found support for the notion that when the aid workers held more positive perceptions of their local colleagues, higher levels of organizational performance were observed. Such findings raise the question of whether Golem effects would also be observed in such multicultural settings and provides ample opportunity for future research inquiry.\n\nMany modern organizations are starting to face a new challenge in superior/subordinate relationships: the older employee reporting to a younger supervisor. This particular situation is expected to occur more and more as the baby boomer generation reaches retirement age. As such, there is great opportunity for research in examining the effects of older employees' expectations and perceptions of younger supervisors, a phenomenon that has been labeled as Reverse Pygmalion. A reverse Pygmalion effect is not synonymous with the Golem effect. In both the regular Pygmalion and Golem effects, the expectations of the supervisor have an effect on the performance of the subordinate while in the reverse Pygmalion and Golem effects, the expectations of the subordinate have an effect on the performance of the supervisor. To date, there has been little research on the subject; however, one study found that, compared to younger workers, older workers with younger supervisors expected less out of their supervisors, and consequently rated their leadership behaviors as lower than in other conditions. While this study refers to the effect studied as Reverse Pygmalion, it appears to be also lending credence to the possibility of a Reverse Golem effect existing, in that subordinates' negative expectations of supervisors may consequently influence supervisor behavior in a negative fashion. This would be an example of the \"reverse\" phenomenon due to the fact that the typical Golem effect runs in the direction of supervisor's expectation down to subordinates' behavior. However, more research is clearly needed to fully and rigorously test such speculation.\n\nLastly, there have been concerns raised about the Pygmalion effect possibly being an artifact of interpersonal contrast effects; by experimentally focusing high expectations on a treatment group, the control group (which typically receives no manipulation in Pygmalion studies) naturally is perceived with lower expectations. As such, the perceived difference between individuals becomes the driving force instead of high expectations alone. However, Eden demonstrated that this concern was not supported through manipulating entire groups (in this case, separate military squads randomly receiving Pygmalion versus control status); he found that the Pygmalion effect was still observed beyond the scope of any contrast effects as evidenced by higher mean performances of groups with leaders that received Pygmalion manipulation when compared to controls. It would be of significant value to the Golem research literature to see whether Golem effects are also unaffected by interpersonal contrast effects through similar group study designs.\n\nThe Golem effect has many implications for various organizational settings, from schools to sports to multimillion-dollar corporations. Public education systems are likely to be very familiar with Golem effects in the form of controversy surrounding tracking systems, which have been almost completely abandoned in education today due to their inefficacy and detrimental effects. While tracking systems varied widely from school to school, the message conveyed to many students placed in remedial tracks was that of low expectations, which, in line with Golem research, led to poorer performance and behaviors. There is also great relevance for the Golem effect in sports, where a coach (a superior) must frequently gauge his or her outward displays of expectations towards individual teammates (the subordinate) in order to ensure that he/she is not sending negative messages. Such negative messages have the possibility to affect players' performance significantly. As such, it is far more ideal for coaches to engender high expectations towards all team members in order to harness the power of the Pygmalion effect.\n\nFinally, there is something to be said about Golem effects towards disenfranchised and stigmatized demographics in society such as the homeless, intellectually disabled, and other groups often looked down upon. Due to the low expectations often cast upon individuals in these groups by society as a whole, there is reason to believe that such individuals suffer from Golem effects in a truly significant and crippling manner. However, there is great hope for such trends to be stopped or even reversed, as evidenced by multiple government and non-profit programs aimed at recognizing and empowering these individuals to succeed in the modern workforce.\n\n"}
{"id": "820253", "url": "https://en.wikipedia.org/wiki?curid=820253", "title": "Helmholtz decomposition", "text": "Helmholtz decomposition\n\nIn physics and mathematics, in the area of vector calculus, Helmholtz's theorem, also known as the fundamental theorem of vector calculus, states that any sufficiently smooth, rapidly decaying vector field in three dimensions can be resolved into the sum of an irrotational (curl-free) vector field and a solenoidal (divergence-free) vector field; this is known as the Helmholtz decomposition or Helmholtz representation. It is named after Hermann von Helmholtz.\n\nAs an irrotational vector field has a scalar potential and a solenoidal vector field has a vector potential, the Helmholtz decomposition states that a vector field (satisfying appropriate smoothness and decay conditions) can be decomposed as the sum of the form formula_1, where is a scalar field, called scalar potential, and is a vector field, called a vector potential.\n\nLet formula_2 be a vector field on a bounded domain formula_3, which is twice continuously differentiable, and let formula_4 be the surface that encloses the domain formula_5. Then formula_2 can be decomposed into a curl-free component and a divergence-free component:\n\nwhere\n\nand formula_9 is the nabla operator with respect to formula_10, not formula_11 .\n\nIf formula_12 and is therefore unbounded, and formula_2 vanishes faster than formula_14 as formula_15, then the second component of both scalar and vector potential are zero. That is,\n\nSuppose we have a vector function formula_17 of which we know the curl, formula_18, and the divergence, formula_19, in the domain and the fields on the boundary. Writing the function using delta function in the form\n\nwhere formula_21is the Laplace operator, we have\n\nwhere we have used the “double cross” vector identity:\n\nderivation/integration with respect to formula_24by formula_25and in the last line, linearity of function arguments:\n\nThen using the vectorial identities\n\nwe get\n\nThanks to the divergence theorem the equation can be rewritten as\n\nwith outward surface normal formula_30.\n\nDefining\n\nwe finally obtain\n\nformula_34 is the Green's function for the Laplacian, and in a more general setting it should be replaced by the appropriate Green's function - for example, in two dimensions it should be replaced by formula_35. For higher dimensional generalization, see the discussion of Hodge decomposition below.\n\nNote that in the theorem stated here, we have imposed the condition that if formula_2 is not defined on a bounded domain, then formula_2 shall decay faster than formula_14. Thus, the Fourier Transform of formula_2, denoted as formula_40, is guaranteed to exist. We apply the convention\n\nThe Fourier transform of a scalar field is a scalar field, and the Fourier transform of a vector field is a vector field of same dimension.\n\nNow consider the following scalar and vector fields:\n\nHence\n\nThe term \"Helmholtz theorem\" can also refer to the following. Let be a solenoidal vector field and \"d\" a scalar field on which are sufficiently smooth and which vanish faster than at infinity. Then there exists a vector field such that\n\nif additionally the vector field vanishes as , then is unique.\n\nIn other words, a vector field can be constructed with both a specified divergence and a specified curl, and if it also vanishes at infinity, it is uniquely specified by its divergence and curl. This theorem is of great importance in electrostatics, since Maxwell's equations for the electric and magnetic fields in the static case are of exactly this type. The proof is by a construction generalizing the one given above: we set\n\nwhere formula_46 represents the Newtonian potential operator. (When acting on a vector field, such as , it is defined to act on each component.)\n\nThe Hodge decomposition is closely related to the Helmholtz decomposition, generalizing from vector fields on R to differential forms on a Riemannian manifold \"M\". Most formulations of the Hodge decomposition require \"M\" to be compact. Since this is not true of R, the Hodge decomposition theorem is not strictly a generalization of the Helmholtz theorem. However, the compactness restriction in the usual formulation of the Hodge decomposition can be replaced by suitable decay assumptions at infinity on the differential forms involved, giving a proper generalization of the Helmholtz theorem.\n\nThe Helmholtz decomposition can also be generalized by reducing the regularity assumptions (the need for the existence of strong derivatives). Suppose is a bounded, simply-connected, Lipschitz domain. Every square-integrable vector field has an orthogonal decomposition:\n\nwhere is in the Sobolev space of square-integrable functions on whose partial derivatives defined in the distribution sense are square integrable, and , the Sobolev space of vector fields consisting of square integrable vector fields with square integrable curl.\n\nFor a slightly smoother vector field , a similar decomposition holds:\n\nwhere .\n\nA terminology often used in physics refers to the curl-free component of a vector field as the longitudinal component and the divergence-free component as the transverse component. This terminology comes from the following construction: Compute the three-dimensional Fourier transform of the vector field F. Then decompose this field, at each point k, into two components, one of which points longitudinally, i.e. parallel to k, the other of which points in the transverse direction, i.e. perpendicular to k. So far, we have\n\nNow we apply an inverse Fourier transform to each of these components. Using properties of Fourier transforms, we derive:\n\nSince formula_55 and formula_56,\n\nwe can get\n\nso this is indeed the Helmholtz decomposition.\n\n\n"}
{"id": "2383366", "url": "https://en.wikipedia.org/wiki?curid=2383366", "title": "Hexis", "text": "Hexis\n\nHexis () is a relatively stable arrangement or disposition, for example a person's health or knowledge or character. It is an Ancient Greek word, important in the philosophy of Aristotle, and because of this it has become a traditional word of philosophy. It stems from a verb related to possession or \"having\", and Jacob Klein, for example, translates it as \"possession\". It is more typically translated in modern texts occasionally as \"state\" (e.g., H. Rackham), but more often as \"disposition\". Joe Sachs translates it as \"active condition\", in order to make sure that \"hexis\" is not confused with passive conditions of the soul, such as feelings and impulses or mere capacities that belong to us by nature. Sachs points to Aristotle's own distinction, explained for example in \"Categories\" 8b, which distinguishes the word \"diathesis\", normally uncontroversially translated as disposition. In this passage, \"diathesis\" only applies to passive and shallow dispositions that are easy to remove and change, such as being hot or cold, while \"hexis\" is reserved for deeper and more active dispositions, such as properly getting to know something in a way that it will not be easily forgotten. Another common example of a human \"hexis\" in Aristotle is health (\"hugieia\", or sometimes \"eu(h)exia\", in Greek) and in cases where \"hexis\" is discussed in the context of health, it is sometimes translated as \"constitution\".\n\nApart from needing to be relatively stable or permanent, in contexts concerning humans (such as knowledge, health, and good character) \"hexis\" is also generally understood to be contrasted from other dispositions, conditions and habits, by being \"acquired\" by some sort of training or other habituation.\n\nAccording to Plotinus, virtue is a hexis of the soul that is not primarily related to praxis and habituation; hexis is a quality of being in an active state of possession that intellectualizes the soul in permanent contemplation of the intelligible world (Enn. VI.8.5.3–37).\n\nOther uses also occur, for example it is sometimes translated as \"habit\", based upon the classical translation from Greek to Latin \"habitus\", which also comes from a verb indicating having.\n\nBeing in a truly fixed state, as opposed to being stable, is not implied in the original Aristotelian usage of this word. He uses the example of \"health\" being a \"hexis\".\n\nSo according to Aristotle, a \"hexis\" is a type of \"disposition\" (diathesis) which he in turn describes in the same as follows...\n\nAnd specifically it is the type of disposition \"in virtue of which (\"kath' ho\") the thing which is disposed is disposed well or badly, and either independently or in relation to something else\".\n\nThe wording \"in virtue of which\" was also described in the same passage...\n\nIn Aristotle then, a \"hexis\" is an arrangement of parts such that the arrangement might have excellence, being well arranged, or in contrast, might be badly arranged. Also see Aristotle's Categories viii where a hexis (\"habit\" in the translation of Edghill) is contrasted with a disposition (\"diathesis\") in terms of it being more permanent and less easy to change. The example given is \"knowledge\" (\"epistemē\").\n\nIn perhaps the most important case, Aristotle contrasted \"hexis\" with \"energeia\" (in the sense of activity or operation) at Nicomachean Ethics I.viii.1098b33 and Eudemian Ethics II.i.1218b. The subject here was \"eudaimonia\", the proper aim of human life, often translated as \"happiness\" and \"hexis\" is contrasted with \"energeia\" (ἐνέργεια) in order to show the correctness of a proposed definition of \"eudaimonia\" - \"activity (ἐνέργεια) in conformity with virtue\"\n\nHappiness then, is an \"energeia\", but virtue of character (often translated as \"moral virtue\") is made up of \"hexeis\". Happiness is said to deserve honoring like the divine if it actually achieved, while virtue of character, being only a potential achievement, deserves praise but is lower.\n\n...and defined in the Strong's concordance...\n\n"}
{"id": "24317242", "url": "https://en.wikipedia.org/wiki?curid=24317242", "title": "Hong Kong units of measurement", "text": "Hong Kong units of measurement\n\nHong Kong has three main systems of units of measurement in current use:\n\n\nIn 1976 the Hong Kong Government started the conversion to the metric system, and as of 2012 measurements for government purposes, such as road signs, are almost always in metric units. However, all three systems are officially permitted for trade, and in the wider society a mixture of all three systems prevails.\n\nThe Chinese system's most commonly used units were 里 (li), 丈 (tseung/cheung), 尺 (tsek/chek), 寸 (tsun/chun), 分 (fen/fan) in descending scale order. These units are now rarely used in daily life, the Imperial and metric systems being preferred.\n\nThe Imperial system's units are written with the same basic Chinese characters as the Chinese system. In order to distinguish between the units of the two systems, the units can be prefixed with \"ying\" () for the British Imperial system and \"wa\" () for the Chinese system. In writing, derived characters are often used, with an additional 口(mouth) radical to the left of the original Chinese character, for writing Imperial units. The most commonly used units are the mile or \"li\" (), the yard or \"ma\" (), the foot or \"chek\" (), and the inch or \"tsun\" ().\n\nChinese, Imperial and metric weight units are all used in Hong Kong. The choice of system depends on the type of goods and their origins. Metric is used for all official purposes, for example the Post Office and Road signs. Packaged food weights and volumes may be given using any of the three systems of units.\n\nTraditional weights are still \"de facto\" standard in certain areas. For example, vegetables, meats, and Chinese medicines are usually measured in Chinese units, while some fruits are normally measured using the Imperial system.\n\nPrecious metals (gold, silver and platinum) are traded in the Chinese troy weight system, which differs from other goods.\n\nThe traditional measure of flat area is in square feet () of the Imperial system. Apartment or office size is generally still given in square feet. Traditionally, the measurement of agricultural plots and fields are conducted in 畝 (mau) of the Chinese system.\n\nIn Chinese system, the measurement of volume of rice is 斗 (tau) but it is replaced by packaged rice in weight. The volume of water and fuel is the litre. The gallon (加侖, ka-lun) of the Imperial system is still occasionally used.\n\nTime measurement follows the international system. Gregorian calendar is usually used, but the Chinese calendar also plays a very important role in everyday life and in telling the dates of traditional festivals.\n\nIn the following table, multiple Chinese names are listed in the order of usage frequency.\n\nThe kilojoule or kilocalorie is the unit used for the measurement of energy in food. The British thermal unit (BTU) is still used to measure the output of air conditioners.\n\nHorsepower (馬力) is still the dominant measurement for the power of cars . The Chinese counting word 匹 (\"pat\") is the unit to describe it in Cantonese.\n\n"}
{"id": "55604987", "url": "https://en.wikipedia.org/wiki?curid=55604987", "title": "Human rights and encryption", "text": "Human rights and encryption\n\nHuman rights applied to encryption is an important concept for freedom of expression as encryption is a technical resource of implementation of basic human rights.\n\nWith the evolution of the digital age, application of freedom of speech becomes more controversial as new means of communication and restrictions arise including government control or commercial methods putting personal information to danger. From a human rights perspective, there is a growing awareness that encryption is an important piece of the puzzle for realizing a free, open and trustworthy Internet.\n\nHuman rights are moral principles or norms that describe certain standards of human behavior, and are regularly protected as legal rights in municipal and international law. They are commonly understood as inalienable fundamental rights \"to which a person is inherently entitled simply because she or he is a human being\", and which are \"inherent in all human beings\" regardless of their nation, location, language, religion, ethnic origin or any other status. They are applicable everywhere and at every time in the sense of being universal, and they are egalitarian in the sense of being the same for everyone.\n\nCryptography is a long-standing subject in the field of mathematics, computer science and engineering. It can generally be defined as \"the protection of information and computation using mathematical techniques.\" In the OECD Guidelines, Encryption and cryptography are defined as follows: \"Encryption\" means the transformation of data by the use of cryptography to produce unintelligible data (encrypted data) to ensure its confidentiality. Cryptography\" means the discipline which embodies principles, means, and methods for the transformation of data in order to hide its information content, establish its authenticity, prevent its undetected modification, prevent its repudiation, and/or prevent its unauthorized use. encryption and cryptography are \"often used synonymously, although \"cryptographic\" has a broader technical meaning. For example, a digital signature is \"cryptographic\" but arguably it is not technically \"encryption\"\".\n\nThe human rights aspects related to the availability and use of a technology of particular significance for the field of information and communication is recognised in many places. Freedom of expression is recognized as a human right under article 19 of the Universal Declaration of Human Rights and recognized in international human rights law in the International Covenant on Civil and Political Rights (ICCPR). Article 19 of the UDHR states that \"everyone shall have the right to hold opinions without interference\" and \"everyone shall have the right to freedom of expression; this right shall include freedom to seek, receive and impart information and ideas of all kinds, regardless of frontiers, either orally, in writing or in print, in the form of art, or through any other media of his choice\".\n\nSince the 1970s, the availability of digital computing and the invention of public key cryptography has made encryption more widely available. Previously, strong versions of encryption were the domain of nation state actors. However, since the year 2000, cryptographic techniques have been widely deployed by a variety of actors to ensure personal, commercial and public sector protection of information and communication. Cryptographic techniques are also used to protect anonymity of communicating actors and protecting privacy more generally. The availability and use of encryption continues to lead to complex, important and highly contentious legal policy debates. There are government statements and proposals on the need to curtail such usage and deployment in view of the potential hurdles it could present for access by government agencies. The rise of commercial services offering end-to-end encryption and the calls for restrictions and solutions in view of law enforcement access are pushing towards more and more debates around the use of encryption and the legal status of the deployment of cryptography more generally.\n\nEncryption, as defined above, refers to a subset of cryptographic techniques for the protection of information and computation. The normative value of encryption, however, is not fixed but varies with the type of cryptographic method that is used or deployed and for which purposes. Traditionally, encryption (cypher) techniques were used to ensure the confidentiality of communications and prevent access to information and communications by others than intended recipients. Cryptography can also ensure the authenticity of communicating parties and the integrity of communications contents, providing a key ingredient for enabling trust in the digital environment.\n\nThere is a growing awareness within human rights that encryption plays an important role in realizing a free, open and trustworthy Internet. UN Special Rapporteur on the promotion and protection of the right to freedom of opinion and expression David Kaye observed, during the Human Rights Council in June 2015, that encryption and anonymity deserve a protected status under the rights to privacy and freedom of expression:\n\n\"Encryption and anonymity, today's leading vehicles for online security, provide individuals with a means to protect their privacy, empowering them to browse, read, develop and share opinions and information without interference and enabling journalists, civil society organizations, members of ethnic or religious groups, those persecuted because of their sexual orientation or gender identity, activists, scholars, artists and others to exercise the rights to freedom of opinion and expression.\"\n\nTwo types of encryption in media and communication can be distinguished:\n\nAmongst the most widely deployed cryptographic techniques is securing the communications channel between internet users and specific service providers from man-in-the-middle attacks, access by unauthorized third parties. These cryptographic techniques must be run jointly by a user and the service provider to work. This means that they require service providers, such as an online news publisher or a social network, to actively integrate them into service design and implementation. Users cannot deploy these techniques unilaterally; their deployment is contingent on active participation by the service provider. The TLS protocol, which becomes visible to the normal internet user through the HTTPS header, is widely used for securing online commerce, e-government services and health applications as well as devices that make up networked infrastructures, e.g., routers, cameras. However, although the standard has been around since 1990, the wider spread and evolution of the technology has been slow. As with other cryptographic methods and protocols, the practical challenges related to proper, secure and (wider) deployment are significant and have to be considered. Many service providers still do not implement TLS or do not implement it well.\n\nIn the context of wireless communications, the use of cryptographic techniques that protect communications from third parties are also important. Different standards have been developed to protect wireless communications: 2G, 3G and 4G standards for communication between mobile phones, base stations and base stations controllers; standards to protect communications between mobile devices and wireless routers ('WLAN'); and standards for local computer networks. One common weakness in these designs is that the transmission points of the wireless communication can access all communications e.g., the telecommunications provider. This vulnerability is exacerbated when wireless protocols only authenticate user devices, but not the wireless access point.\n\nWhether the data is stored on a device, or on a local server as in the cloud, there is also a distinction between 'at rest'. Given the vulnerability of cellphones to theft for instance, particular attention may be given to limiting service provided access. This does not exclude the situation that the service provider discloses this information to third parties like other commercial entities or governments. The user needs to trust the service provider to act in its interests. The possibility that a service provider is legally compelled to hand over user information or to interfere with particular communications with particular users, remains.\n\nThere are services that specifically market themselves with claims not to have access to the content of their users' communication. Service Providers can also take measures that restrict their ability to access information and communication, further increasing the protection of users against access to their information and communications. The integrity of these Privacy Enhancing Technologies (PETs), depends on delicate design decisions as well as the willingness of the service provider to be transparent and accountable. For many of these services, the service provider may offer some additional features (besides the ability to communicate), for example contact list management—meaning that they can observe who is communicating with whom—but take technical measures so that they cannot read the contents of the messages. This has potentially negative implications for users, for instance, since the service provider has to take action to connect users who want to communicate using the service, it will also have the power to prevent users from communicating in the first place.\n\nFollowing the discovery of vulnerabilities, there is a growing awareness that there needs to be more investment in the auditing of widely used code coming out of the free and open software community. The pervasiveness of business models that depend on collection and processing of user data can be an obstacle for adopting cryptographic mechanisms for protecting information at rest. As Bruce Schneier, has stated:\n\n\"[s]urveillance is the business model of the Internet. This has evolved into a shockingly extensive, robust, and profitable surveillance architecture. You are being tracked pretty much everywhere you go on the Internet, by many companies and data brokers: ten different companies on one site, a dozen on another.\" Cryptographic methods play a key role in online identity management.\n\nDigital credential systems can be used to allow anonymous yet authenticated and accountable transactions between users and service providers, and can be used to build privacy preserving identity management systems.\n\nThe Internet allows end-users to develop applications and uses of the network without having to coordinate with the relevant internet service providers. Many of the available encryption tools are not developed or offered by traditional service providers or organizations but by experts in the free and open software (FOSS) and Internet engineering communities. A major focus of these initiatives is to produce Privacy Enhancing Technologies (PETs) that can be unilaterally or collaboratively deployed by interested users who are ready, willing, and able to look after their own privacy interests when interacting with service providers. These PETs include standalone encryption applications as well as browser add-ons that help maintain the confidentiality of web-based communications or permit anonymous access to online services. Technologies such as keystroke loggers can intercept content as it is entered before encryption is applied, thereby falling short of offering protection. Hacking into information systems and devices to access data at or after the moment of decryption may have the same effect.\n\nMulti-party computation (MPC) techniques are an example of Collaboration|collaborative solutions that allow parties, e.g. NGOs with sensitive data, to do data analytics without revealing their datasets to each other. All of these designs leverage encryption to provide privacy and security assurances in the absence of a trustworthy centralized authority.\n\nThere are many developments in the implementations of crypto-currencies using blockchain protocols. These systems can have many benefits and these protocols can also be useful for novel forms of contracts and electronic attestation, useful aids when legal infrastructure are not readily available. As to the protection of privacy related to payments, it is a common misconception that the cryptographic techniques that are used in Bitcoin ensure anonymous payments. The only protection offered by Bitcoin is pseudonymity.\n\nThe availability of metadata (the information relating to a user's information and communications behavior) can pose a particular threat to users including information that can be observed by service providers through the provisioning of services: when, how frequently, how long, and with whom users are communicating. Metadata can also be used to track people geographically and can interfere with their ability to communicate anonymously. As noted by the Berkman Center report, metadata is generally not encrypted in ways that make it inaccessible for governments, and accordingly \"provides an enormous amount of surveillance data that was unavailable before [internet communication technologies] became widespread.\" In order to minimize exposure of meaningful metadata, encryption tools may need to be used in combination with technologies that provide communication anonymity.\nThe Onion Router, most commonly known as Tor, offers the ability to access websites and online services anonymously. Tor requires a community of volunteers to run intermediary proxies which channel a user's communication with a website so that third parties cannot observe who the user is communicating with. Through the use of encryption, each proxy is only aware of part of the communication path meaning that none of the proxies can by itself infer both the user and the website she is visiting. Besides protecting anonymity, Tor is also useful when the user's ISP blocks access to content. This is similar as the protection that can be offered by a VPN. Service providers, such as websites, can block connections that come from the Tor network. Because certain malicious traffic may reach service providers as Tor traffic and because Tor traffic may also interfere with the business models, service providers may have an incentive to do so. This interference can prevent users from using the most effective means to protect their anonymity online. The Tor browser allows users to obfuscate the origin and end-points of their communications when they communicate on the internet.\n\nObfuscation, the automated generation of \"fake\" signals that are indistinguishable from users' actual online activities, providing users with a noisy \"cover\" under which their real information and communication behavior remains unobservable. Obfuscation has received more attention as a method to protect users online recently. TrackMeNot is an obfuscation tool for search engine users: the plugin sends fake search queries to the search engine, affecting the ability of the search engine provider to build an accurate profile of the user. Although TrackMeNot and other search obfuscation tools have been found to be vulnerable to certain attacks that allow search engines to distinguish between user-generated and computer-generated queries, further advances in obfuscation are likely to play a positive role in protecting users when disclosure of information is inevitable, as in the case of search or location-based services.\n\nRecent incidents of terrorism have led to further calls for restrictions on encryption. Even though, in the interest of public safety, there are many proposals to interfere with the free deployment of strong encryption, these proposals do not hold against close scientific scrutiny. These proposals side-step a more fundamental point, related to what is at stake for users. More advanced security measures seem necessary for governments, considering the existing threat landscape for users of digital communications and computing.\n\nWhile many governments consider that encryption techniques could present a hurdle in the investigation of crime and the protection of national security, certain countries, such as Germany or the Netherlands have taken a strong position against restrictions on encryption on the Internet. In 2016, the Ministers of the Interior of France and Germany have jointly stated the need to work on solutions for the challenges law enforcement can face as a result of end-to-end encryption, in particular when offered from a foreign jurisdiction. In a joint statement, the European Agency for Network and Information Security (ENISA) and Europol have also taken a stance against the introduction of backdoors in encryption products. In addition, restrictions would have serious detrimental effects on cyber security, trade and e-commerce.\n\nPrivacy and data protection legislation is closely related to the protection of human rights. There are now more than 100 countries with data protection laws. One of the key principles for the fair and lawful processing of personal information regulated by data protection laws is the principle of security. This principle implies that proper security measures are taken to ensure the protection of personal data against unlawful access by others than intended recipients. The European Union General Data Protection Regulation, which was adopted in 2016 and will enter in to force in 2018, contains an advanced set of rules with respect to the security of personal data.\n\nEncryption can be a safeguard against personal data breaches for the UN, as it can facilitate the implementation of privacy and data protection by design. Cryptography has also been an essential ingredient for establishing the conditions for e-Commerce over the Internet. The OECD Principles were adopted to ensure that national cryptography policy would not interfere with trade and to ensure the conditions for international developments in e-Commerce.\n\nThe policy debate about encryption has a significant international dimension because of the international nature of the communications networks and the Internet as well as trade, globalization and the national security dimensions. The OECD adopted a Recommendation Concerning Guidelines for Cryptography Policy on 27 March 1997. There are three components to this policy intervention of the OECD, which is primarily aimed at its Member Countries: a recommendation of the OECD Council, Guidelines for Cryptography Policy (as an Annex to the Recommendation) and a Report on Background and Issues of Cryptography Policy to explain the context for the Guidelines and the basic issues involved in the cryptography law and policy debate. The Principle most explicit about the connection to human rights is Principle 5 on the Protection of Privacy and Personal Data: \"The fundamental rights of individuals to privacy, including secrecy of communications and protection of personal data, should be respected in national cryptography policies and in the implementation and use of cryptographic methods.\"\n\nUNESCO, after consulting stakeholders, identified encryption as a relevant element for policy on privacy and freedom of expression. The Keystones Report (2015) articulates that \"\"to the extent that our data can be considered representative of ourselves, encryption has a role to play in protecting who we are, and in preventing abuse of user content. It also allows for greater protection of privacy and anonymity in transit by ensuring that the contents (and sometimes also the metadata) of communications are only seen by the intended recipient.\"\" The report recognizes \"the role that anonymity and encryption can play as enablers of privacy protection and freedom of expression\", and proposes that UNESCO facilitate dialogue on these issues.\n\nThe Necessary and Proportionate Principles developed and adopted by civil society actors stipulates the protection of the integrity of communications systems as one of its 13 principles. The principles themselves do not provide for explicit guidance on specific cryptographic policy issues such as backdoors or restrictions on the deployment of encryption. The guidance that is offered by the OECD principles and the recent positions of the UN Rapporteur on Encryption state the importance of encryption for the protection of human rights. While it does not give a definitive answer to the question of whether a mandate for encryption backdoors is to be considered incompatible with international law, it does point in that direction. Generally, the available guidance at the international level clarifies that when limitations are imposed on encryption, relevant human rights guarantees have to be strictly observed.\n\nThere has been a broad, active and contentious policy debate on encryption in the USA since the 1990s beginning with the 'Crypto Wars'. This involved the adoption of the Communications Assistance for Law Enforcement Act (CALEA), containing requirements for telecommunications providers and equipment manufacturers to ensure the possibility of effective wiretapping. It also involved a debate over existing export controls on strong encryption products (considering their classification as munition) and a criminal investigation into cryptographic email software developer and activist Phil Zimmermann. The case was dropped and the general debate resolved after the liberalization of export controls on most commercial products with strong encryption features and the transfer of these items from the U.S.A. Munitions List (USML), administered by the Department of State, to the Commerce Control List (CCL), administered by the Department of Commerce. The USA Department of Commerce maintains some controls over items on the CCL, including registration, technical reviews and reporting obligations, and continues to impose licensing and other requirements for sensitive encryption items and sales of such items to foreign governments.\n\nThe debate ignited after the Edward Snowden revelations and the well-documented increase in deployed encryption measures by Internet services, device makers and users, as well as a concerted call from the technical community and civil society to increase encryption use and security to address mass surveillance practices. The increased adoption of encryption by the industry has been received critically by certain government actors, the FBI in particular. This led to the widely reported FBI–Apple encryption dispute over the possibility to gain access to information on the iPhone in assistance to law enforcement. In 2016, several bills were introduced in the US Congress that would place new limits encryption under USA law. The USA's legal system promotes and requires security measures to be implemented in the relevant contexts, including cryptographic methods of various kinds, to ensure security in commerce and trade. Relevant laws are the Federal Information Security Modernization Act (FISMA) of 2014, the Gramm-Leach-Bliley Act, the Health Insurance Portability and Accountability Act (HIPAA) and also the Federal Trade Commission Act. These acts contain security requirements and thereby indirectly require or stimulate the use of encryption in certain circumstances. Finally, many state breach notification laws treat encrypted data as a safe harbor by exempting firms that have encrypted data from notice obligations.\n\nConstitutional considerations and human rights play a role of significance in the USA debate about the legal treatment of encryption methods. Restrictions on distribution of cryptographic protocols, and the publication of cryptographic methods are considered an interference with the First Amendment, the USA constitutional safeguard protecting freedom of expression. The USA has particularly active and strongly developed civil society actors involved in cryptographic policy and practice.\n\nThe United States of America is a primary site for cryptology research and engineering, development and implementation of cryptographic service innovations. There is an active community of Non-Governmental Organizations engaged in the national and international debate on encryption policy. The predominant interferences with strong encryption that take place or are being considered take place in the field of national security, law enforcement and foreign affairs. In this area and in answering the contentious question of whether and how lawful access to specific communications could be ensured, the US Government has internationally explained its policy as one aiming to ensure that 'responsibly deployed encryption' helps to \"\"secure many aspects of our daily lives, including our private communications and commerce\",\" but also \"to ensure that malicious actors can be held to account without weakening our commitment to strong encryption\".\n\nAs part of the global debate on encryption in the late 1990s, a debate also took place in Germany about the need and legitimacy of imposing a general ban on the encryption of communications because of the impact on criminal investigations. There were profound doubts concerning the constitutional legitimacy as well as concerns about negative factual consequences of such a ban. In qualitative terms, a number of fundamental rights are considered to be affected by restrictions on encryption: the secrecy of telecommunications, expressions of the general right of personality and, indirectly, all communicative freedoms that are exercisable over the Internet. The Federal Government set key points in 1999 for the German cryptographic policy which should especially provide confidence in the security of encryption instead of restricting it. Besides the statements of the German Minister of the Interior towards possible future restrictions, Germany aligns with the position of the UN Special Rapporteur David Kaye and adopts policies of non-restriction or comprehensive protection and only adopts restrictions on a case-specific basis. In November 2015 governmental representatives as well as representatives of the private sector signed a \"Charter to strengthen the trusted communication \"(Charta zur Stärkung der vertrauenswürdigen Kommunikation) together, in which they stated: \"We want to be Encryption Site No. 1 in the world\". The German government has also used its foreign policy to promote international privacy standards. In particular, Germany, in a joint effort with Brazil, committed itself in the Human Rights Council for the appointment of an UN Special Rapporteur on Privacy. There are multiple examples of how there have been efforts by the government to implement encryption policy. They range from informal actions, to laws and regulations: The IT Security Act in 2015, the 'De-Mail' law. There are also several sector-specific rules for encryption and information security in Germany, like the Telecommunications Act (TKG). The German Constitutional Court has also provided valuable input for the international legal handling of encryption techniques with the IT basic right, with which, the constitutional court recognizes that parts of one's personality go into IT systems and therefore the applied protection has to travel with it.\n\nThere are a number of limitations on the free deployment of encryption by electronic communications services despite the fact that Indian law and policy promotes and requires the implementation of strong encryption as a security measure, such as in banking, ecommerce and by organizations handling sensitive personal information. There is notable legal uncertainty about the precise legal scope of these license requirements and to what extent they could have legal effect on (the use of or deployment of ) services by the end-users of covered services. The encryption debate ignited publicly in India in 2008 after the Government published a draft proposal with a number of envisioned limitations on the use of encryption. The policy, issued under Section 84A of the Indian Information Technology (Amendment) Act, 2008 was short-lived, but worries remain about the lack of safeguards for privacy and freedom of expression that the draft illustrated. In response to the outcry, the Indian government first exempted \"mass use encryption products, which are currently being used in web applications, social media sites, and social media applications such as Whatsapp, Facebook, Twitter etc.\" Soon thereafter, it withdrew the proposed policy and a new policy has not been made public yet.\n\nSection 84A of the Indian Information Technology (Amendment) Act, 2008 empowers the government to formulate rules on modes of encryption for the electronic medium. Legal commentators have noted the lack of transparency about what types of encryption use and deployment are permitted and required under Indian law, especially in the field of electronic communications services. Thus, the Central Indian Government has, in theory, a broad exclusive monopoly over electronic communications which includes the privilege to provide telecommunication and Internet services in India.\n\nAfter the Edward Snowden revelations in 2013, Brazil was at the forefront of a global coalition promoting the right to privacy at the UN and condemning USA mass surveillance. In recent events, Brazil has demonstrated diverse aims when it comes to the use and implementation of encryption. On the one side, the country is a leader in providing a legal framework of rules for the Internet. But it has also taken several measures that may be seen to restrict the dissemination of encryption technology. In 2015, in a process that was open for public comments and discussions, Brazil's legislator drafted a new privacy bill (\"proteção de dados pessoais\"), which was sent to Brazil's Federal Congress on 13 May 2016 and came into force as Bill 5276 of 2016. It regulates and protects personal data and privacy, including online practices and includes provisions for more secure methods such as encryption on the treatment of personal data. The law also addresses security issues and a duty for companies to report any attacks and security breaches. With the Marco Civil (2014), that introduces principles like neutrality, the Brazilian Civil Rights Framework for the Internet, Brazil was one of the first countries to ever introduce a law, that aims at combining all Internet rules in one bundle. Brazil has a well-established e-government model: The Brazilian Public Key Infrastructure (Infraestrutura de Chaves Públicas Brasileira – ICP-Brasil). Since 2010 ICP-Brasil certificates can be partly integrated in Brazilian IDs, which can then be used for several services like tax revenue service, judicial services or bank related services. In practice, the ICP-Brasil digital certificate acts as a virtual identity that enables secure and unique identification of the author of a message or transaction made in an electronic medium such as the web. Brazilian courts have taken a stance against encryption in private messaging services by repeatedly ordering the blocking of the messaging service WhatsApp. Since it switched to a full end-to-end encryption, the service has been periodically blocked as a result of a court order in an attempt to make the company comply with demands for information.\n\nThe African (Banjul) Charter on Human and People's Rights, was adopted in the context of the African Union in 1981. A Protocol to the Charter, establishing the African Court on Human and Peoples' Rights was adopted in 1998 and came into effect in 2005. In the area of information policy, the African Union has adopted the African Union Convention on Cyber Security and Personal Data Protection. The provisions on personal data protection in this Convention generally follow the European model for the protection of data privacy and contains a number of provisions on the security of personal data processing. A civil society initiative has adopted a specific African Declaration on Internet Rights and Freedoms \"to help shape approaches to Internet policy-making and governance across the continent\".\n\nDifferent countries in the North-African region have not seen a significant rise in legal actions aiming at the suppression of encryption in the transformations that started in 2011. Although legislation often dates back to before the transformations, the enforcement has become stricter since then. No difference in the position towards cryptography can be seen between the countries that had successful revolutions and went through regime changes and those that didn't.\n\nTunisia has several laws that limit online anonymity. Articles 9 and 87 of the 2001 Telecommunication Code ban the use of encryption and provide a sanction of up to five years in prison for the unauthorized sale and use of such techniques.\n\nIn Algeria, users have legally needed authorization for the use of cryptographic technology from the relevant telecommunications authority ARPT (Autorité de Régulation de la Poste et des Télécommunications) since 2012.\n\nIn Egypt, Article 64 of the 2003 Telecommunication Regulation Law states that the use of encryption devices is prohibited without the written consent of the NTRA, the military, and national security authorities.\n\nIn Morocco, the import and export of cryptographic technology, be it soft- or hardware, requires a license from the government. The relevant law No. 53-05 (Loi n° 53-05 relative à l'échange électronique de données juridiques) went into effect in December 2007.\n\nThere are no specific provisions in effect in countries in the East-African region restricting the use of encryption technology. As in other African countries, the main reason given for State surveillance is the prevention of terroristic attacks. Kenya with its proximity to Somalia, has cited this threat for adopting restrictive actions. The country has recently fast-tracked a Computer and Cybercrime Law, to be adopted in the end of 2016. In Uganda a number of laws and ICT policies have been passed over the past three years, none of them however deal with encryption. In 2016, following the Presidential Elections, the Ugandan government shut down social networks such as Twitter, Facebook and WhatsApp.\n\nWest-African countries neither limit the import or export of encryption technology, nor its use, most national and foreign companies still rely on the use of VPNs for their communication. Ghana recently introduced a draft law aiming at intercepting electronic and postal communications of citizens, to aid crime prevention. Section 4(3) of the proposed bill gives the government permission to intercept anyone's communication upon only receiving oral order from a public officer. Recently the Nigerian Communications Commission has drafted a bill regarding Lawful Interception of Communications Regulations. If passed, the bill allows the interception of all communication without judicial oversight or court order and forces mobile phone companies to store voice and data communication for three years. Furthermore, the draft plans to give the National Security Agency a right to ask for a key to decrypt all encrypted communication.\n\nUsers in South Africa are not prohibited from using encryption. The provision of such technology, however, is strictly regulated by the Electronic Communications and Transactions Act, 2002.\n\nCountries in Central Africa, like the Democratic Republic of Congo, the Central African Republic, Gabon and Cameroon do not yet have a well-developed legal framework addressing Internet policy issues. The Internet remains a relatively unregulated sphere.\n\nWhile a very broad range of human rights is touched upon by digital technologies, the human rights to freedom of expression (Art. 19 International Covenant on Civil and Political Rights [ICCPR]) and the right to private life (Art. 17 ICCPR) are of particular relevance to the protection of cryptographic methods. Unlike the Universal Declaration of Human Rights (UDHR) which is international 'soft law', the ICCPR is a legally binding international treaty.\n\nRestrictions on the right to freedom of expression are only permitted under the conditions of Article 19, paragraph 3. Restrictions shall be provided for by law and they shall be necessary (a) for the respect of the rights or reputations of others or (b) for the protection of national security or of public order or of public health or morals. A further possibility for restriction is set out in Art. 20 ICCPR, In the context of limitations on cryptography, restrictions will most often be based on Article 19 (3)(b), i.e. risks for national security and public order. This raises the complex issue of the relation, and distinction, between security of the individual, e.g. from interference with personal electronic communications, and national security. The right to privacy protects against 'arbitrary or unlawful interference' with one's privacy, one's family, one's home and one's correspondence. Additionally, Article 17(1) of the ICCPR protects against 'unlawful attacks' against one's honor and reputation. The scope of Article 17 is broad. Privacy can be understood as the right to control information about one's self. The possibility to live one's life as one sees fit, within the boundaries set by the law, effectively depends on the information which others have about us and use to inform their behavior towards us. That is part of the core justification for protecting privacy as a human right.\n\nIn addition to the duty to not infringe these rights, States have a positive obligation to effectively ensure the enjoyment of freedom of expression and privacy of every individual under their jurisdiction. These rights may conflict with other rights and interests, such as dignity, equality or life and security of an individual or legitimate public interests. In these cases, the integrity of each right or value must be maintained to the maximum extent, and any limitations required for balancing have to be in law, necessary and proportionate (especially least restrictive) in view of a legitimate aim (such as the rights of others, public morals and national security).\n\nEncryption supports this mode of communication by allowing people to protect the integrity, availability and confidentiality of their communications. The requirement of uninhibited communications is an important precondition for freedom of communication, which is acknowledged by constitutional courts e.g. US Supreme Court and the German Bundesverfassungsgericht as well as the European Court of Human Rights. More specifically, meaningful communication requires people's ability to freely choose the pieces of information and develop their ideas, the style of language and select the medium of communication according to their personal needs. Uninhibited communication is also a precondition for autonomous personal development. Human beings grow their personality by communicating with others. UN's first Special Rapporteur on Privacy, professor Joe Cannataci, stated that \"privacy is not just an enabling right as opposed to being an end in itself, but also an essential right which enables the achievement of an over-arching fundamental right to the free, unhindered development of one's personality\". In case such communication is inhibited, the interaction is biased because a statement does not only reflect the speaker's true (innermost) personal views but can be unduly influenced by considerations that should not shape communication in the first place. Therefore, the process of forming one's personality through social interaction is disrupted. In a complex society freedom of speech does not become reality when people have the right to speak. A second level of guarantees need to protect the precondition of making use of the right to express oneself. If there is the risk of surveillance the right to protect one freedom of speech by means of encryption has to be considered as one of those second level rights. Thus, restriction of the availability and effectiveness of encryption as such constitutes an interference with the freedom of expression and the right to privacy as it protects private life and correspondence. Therefore, it has to be assessed in terms of legality, necessity and purpose.\n\nFreedom of expression and the right to privacy (including the right to private communications) materially protect a certain behavior or a personal state. It is well established in fundamental rights theory that substantive rights have to be complemented by procedural guaranties to be effective. Those procedural guarantees can be rights such as the right to an effective remedy. However, it is important to acknowledge that those procedural rights must, similar to the substantive rights, be accompanied by specific procedural duties of governments without which the rights would erode. The substantial rights have to be construed in a way that they also contain the duty to make governance systems transparent, at least to the extent that allows citizens to assess who made a decision and what measures have been taken. In this aspect, transparency ensures accountability. It is the precondition to know about the dangers for fundamental rights and make use of the respective freedoms.\n\nThe effectuation of human rights protection requires the involvement of service providers. These service providers often act as intermediaries facilitating expression and communication of their users of different kinds. In debates about cryptographic policy, the question of lawful government access – and the conditions under which such access should take place in order to respect human rights – has a vertical and national focus. Complexities of jurisdiction in lawful government access are significant and present a still unsolved puzzle. In particular, there has been a dramatic shift from traditional lawful government access to digital communications through the targeting of telecommunications providers with strong local connections, to access through targeting over-the-top services with fewer or loose connections to the jurisdictions in which they offer services to users. In which cases such internationally operating service providers should (be able to) hand over user data and communications to local authorities. The deployment of encryption by service providers is a further complicating factor.\n\nFrom the perspective of service providers, it seems likely that cryptographic methods will have to be designed to account for only providing user data on the basis of valid legal process in certain situations. In recent years, companies and especially online intermediaries have found themselves increasingly in the focus of the debate on the implementation of human rights. Online intermediaries not only have a role of intermediaries between content providers and users but also one of \"Security Intermediaries\" in various aspects. Their practices and defaults as regards encryption are highly relevant to the user's access to and effective usage of those technologies. Since a great amount of data is traveling through their routers and is stored in their clouds, they offer ideal points of access for the intelligence community and non-state actors. Thus, they also, perhaps involuntarily, function as an interface between the state and the users in matters of encryption policy. The role has to be reflected in the human rights debate as well, and it calls for a comprehensive integration of security of user information and communication in the emerging Internet governance model of today.\n\nUNESCO is working on promoting the use of legal assessments based on human rights in cases of interference with the freedom to use and deploy cryptographic methods. The concept of Internet Universality, developed by UNESCO, including its emphasis on openness, accessibility to all, and multi-stakeholder participation. While these minimal requirements and good practices can be based on more abstract legal analysis, these assessments have to be made in specific contexts. Secure authenticated access to publicly available content, for instance, is a safeguard against many forms of public and private censorship and limits the risk of falsification. One of the most prevalent technical standards that enables secure authenticated access is TLS. Closely related to this is the availability of anonymous access to information. TOR is a system that allows the practically anonymous retrieval of information online. Both aspects of access to content directly benefit the freedom of thought and expression. The principle of legal certainty is vital to every juridical process that concerns cryptographic methods or practices. The principle is essential to any forms of interception and surveillance, because it can prevent unreasonable fears of surveillance, such as when the underlying legal norms are drafted precisely. Legal certainty may avert chilling effects by reducing an inhibiting key factor for the exercise of human rights, for UNESCO. Continuous innovation in the field ofcryptography and setting and spreading new technical standards is therefore essential. Cryptographic standards can expire quickly as computing power increases . UNESCO has outlined that education and continuous modernization of cryptographic techniques are important.\n\nThe impact of human rights can only be assessed by analyzing the possible limitations that states can set for those freedoms. UNESCO states that national security can be a legitimate aim for actions that limit freedom of speech and the right to privacy, but it calls for actions that are necessary and proportional. \"UNESCO considers an interference with the right to encryption as a guarantee enshrined in the freedom of expression and in privacy as being especially severe if:\n\n• It affects the ability of key service providers in the media and communications landscape to protect their users' information and communication through secure cryptographic methods and protocols, thereby constituting the requirement of uninhibited communications for users of networked communication services and technologies.\n\n• The state reduces the possibility of vulnerable communities and/or structurally important actors like journalists to get access to encryption;\n\n• Mere theoretical risks and dangers drive restrictions to the relevant fundamental rights under the legal system of a state;\n\n• The mode of state action, e.g. if restrictions on fundamental rights are established through informal and voluntary arrangements, lead to unaccountable circumvention or erosion of the security of deployed cryptographic methods and technologies.\"\n\n"}
{"id": "7739729", "url": "https://en.wikipedia.org/wiki?curid=7739729", "title": "Influencer marketing", "text": "Influencer marketing\n\nInfluencer marketing (also influence marketing) is a form of marketing in which focus is placed on influential people rather than the target market as a whole on social media. It identifies the individuals that have influence over potential customers, and orients marketing activities around these influencers\n\nInfluencer content may be framed as testimonial advertising where they play the role of a potential buyer themselves, or they may be third parties. These third parties exist either in the supply chain (retailers, manufacturers, etc.) or may be so-called value-added influencers (such as journalists, academics, industry analysts, professional advisers, and so on).\n\nIn the United States, influence marketing is treated by the Federal Trade Commission as a form of paid endorsement, governed under the rules for native advertising; the agency applies established truth-in-advertising standards to such advertising and establishes requirements for disclosure on the part of endorsers (influencers).\n\nOther countries' media-regulatory bodies, such as Australia's, have created guidelines around influencer marketing following the decision of the FTC. Most countries have not created a regulatory framework for influencer marketing.\n\nMost discussion on the generic topic of social influence centres on compliance and persuasion in a social environment. In the context of influencer marketing, influence is less about argument and coercion to a particular point of view and more about loose interactions between various parties in a community. Influence is often equated to advocacy, but may also be negative, and is thus related to concepts of promoters and detractors.\nThe idea of a \"two-step flow of communication\" was introduced in \"The People's Choice\" (Paul Lazarsfeld, Bernard Berelson, and Hazel Gaudet, a 1940 study on the decision making process of voters). This idea was further developed in \"Personal Influence\" (Lazarsfeld, Elihu Katz 1955) and \"The Effects of Mass Communication\" (Joseph Klapper 1960).\n\nInfluencer marketing tends to be broken into two sub-practices: earned influencer marketing and paid influencer marketing. Earned marketing stems from unpaid or preexisting relationships with influencers or third party content that is promoted by the influencer to further their own personal social growth. Paid influencer marketing campaigns can take the form of sponsorship, pre-roll advertising or testimonial messaging and can appear at any point in the content. Budgets vary widely and are usually based on audience reach. Most influencers are paid upfront before a marketing campaign while others are paid after the execution of the marketing campaign. \n\nAs a company's brands evolve in terms of marketing, the cost in relation to the possible benefits (i.e., purchase) it can receive is very important. The airing a television spot has a high cost, conversely, working with an influencer has a negligible cost. If an influencer has 200,000 followers on their social media site, and a company gives them a product specifically as a marketing tool, which they are to expose to their audience, the company's financial outlay, by comparison would be negligible. The company will have spent less (the cost of the product), but exposed to a more focused group of followers (and theorefore potential purchasers) of the public figure. \n\nAs more people use the internet, more people are using online websites to make their purchases. This increase in the number of online purchases forces some companies to invest more resources in their general advertising - on the internet, and in particular, social networks. Marketing through social networks allows for an instantaneous purchase process; a person can see the item and typically be connected to an online retailer immediately. This decrease between lag time - from seeing the promoted item and being redirected to the product - is more effective for spontaneous purchases.\n\nOften, the influencer's social media site is on Instagram.\n\nSome marketers use influencer marketing to establish credibility in the market, others to create social conversations around their brand, others yet to drive online or in-store sales of their products. The influencer marketer can also take to marketing diversified products and services leveraging, leveraging upon the credibility earned over time. The value which influencer marketing creates can be measured in several ways. Some marketers measure earned media value, others track impressions, and others track cost per action.\n\nInfluencer marketing derives its value from three sources:\n\nInfluencing factors may vary. Sellers traditionally target influential people who are easy to identify, such as the press, industry analysts and high profile executives. For most business-to-business (B2B) purchases, influential people could include shoppers and retail staff. In B2B transactions, the highest value of the influential community can be wide and varied and includes consultants, government-backed regulators, financiers and user communities.\n\nInfluencer marketing, in a commercial context, comprises four main activities:\n\n\nInfluencer marketing is continuously improved by constant evaluation done simultaneously with the four main activities.\n\nInfluencer marketing is not synonymous with word of mouth marketing (WOM), but influence may be communicated this way. WOM is a central to the mechanics of influencer marketing.\n\nThere is no consensus of what an 'influencer' is. One writer defines influencers as; \"a range of third parties who exercise influence over the organization and its potential customers\"., whilst another defines an influencer as being; \"(a)...third party who significantly shapes the customer's purchasing decision, but may never be accountable for it.\". Yet, still another says influencers are 'activists, are well-connected, have impact, have active minds, and are trendsetters'. though this set of attributes is aligned specifically to consumer markets.\n\nExactly what is included in influencer marketing depends on the context (retail or B2B) and the medium of influence transmission (online or offline, or both), as company's are increasingly eager to identify and work with influencers.\n\nBusiness is working harder and paying more to pursue people who are trying to watch and listen less to its messages. Targeting influencers is seen as a method of increasing the reach of marketing messages, in order to counteract the growing tendency of prospective customers to ignore marketing.\n\nThe first step in influencer marketing is to identify influencers. Influencers are specific to discrete market segments, and are used as conduits to the entire target segment. While there are lists of generic influencers (such as the Time 100) they have limited use in marketing programmes targeted at specific segments.\n\nMarket research techniques can be used to identify influencers, using pre-defined criteria to determine the extent and type of influence.\n\n\nIn a study of what traits are associated with the top influencers, one researcher found 4 archetypes of influencers\nMalcolm Gladwell notes that “the success of any kind of social epidemic is heavily dependent on the involvement of people with a particular and rare set of social gifts”, and identified 3 different types of influencer: \n\n\nConnectors network across a variety of people, and thus have a wide reach. They are essential for word of mouth communication. \n\nMavens look to consume information and share it with others, they are extremely insightful with regards to trends.\nSalesmen are 'charismatic persuaders'. Their source of influence leans toward the tendency of others to attempt to imitate their behaviour. These 3 groups are responsible for the 'generation, communication and adoption' of messages.\n\nCurrently, most of the material on influencers focuses on consumer markets, rather than business-to-business influencers. A key distinction between consumer and business markets is that most of the focus in consumer markets is on consumer influencers themselves, primarily because word of mouth communication is prevalent in consumer environments. In business marketing, influencers are people that affect a sale, but are typically removed from the actual purchase decision. Consultants, analysts, journalists, academics, regulators, standards bodies are examples of business influencers.\n\nNot all business influencers are equal. Some have more influence than others, and some mechanism of ranking is required, to distinguish between key influencers and less impactful people. One model for ranking business influencers looks at various criteria:\n\n\nVarious companies, worldwide have developed their own proprietary methodologies for identifying and targeting influencers for a market (or market sector).\n\nInfluencers can also be defined by the number of followers they have. \"Top-influencers\", influencers with large followings, become celebrities and generate a strong reach . However, \"micro-influencers\" are more available to their audiences, create more focused, retainable and attainable content and in the long-run generate higher engagement rates. \n\nUnder United States law, those who endorse products or service, and/or provide testimonials for products or services, are obligated to disclose brand information to their target audience and follow other relevant guidelines. However, that doesn’t stop some content providers, and the like, from being deceptive on their opinion to ensure they make money. This often makes them subject to backlash from consumers who spend money on products that fail to uphold their intended purpose. \n\nIn 2014, YouTube Beauty Guru Nicole Guerriero was criticized in a series of hateful comments on her Instagram page. Fans were outraged after purchasing Bellami Hair extensions and believing that the hair company that sponsored Guerriero was misleading consumers into thinking their hair extensions were of the same quality of those sent in PR packages to promote on YouTube and Instagram by influencers. This prompted Guerriero to upload a video on YouTube addressing the concerns people had with the product. \n\nA large influencer sponsor in 2018 across all social media platforms has been the brand Zaful, a clothing company based in China. The company promises cheap discounted clothing along with free shipping. A large of amount of complaints have been about Zaful by consumers on YouTube, Instagram, Facebook, and Twitter. Consumers claim that the company uses images of clothing from other brands deceiving people into purchasing items. Quality and shipping issues are other complaints often made by Zaful consumers. The company also falls under the category of fast fashion where sweatshops are used to create clothing to ensure it can be sold to consumers for a low price.\n\nIn November of 2018, YouTube make-up artist James Charles announced he would be releasing an eyeshadow palette in collaboration with Morphe. Weeks before the release, he uploaded a video to his channel swatching the colours. A YouTuber by the name of John Kuckian then released a video calling the swatch a scam and calling out Charles for editing the video in a such a way that misleads consumers, not giving them a fair idea of what the pigment truly looks like on skin. Reactions to the video were mixed with people coming to Charles’ defense calling Kuckian a liar and an attention whore and others sending hateful tweets to Charles directly.\n\nIn a recent interview with supermodel Ashley Graham, Kim Kardashian admitted to being offered one million dollars to post an image of herself on Instagram in the clothing in a brand she chose not to disclose the information on. Kardashian explained that she turned the deal down because this brand tends to steal ideas from her husband’s clothing line Yeezy. She also said that although it was a lot of money to turn down a post, she was content in her decision. She also claimed the company was notorious for making money off she and her sisters red carpet or everyday looks.\n\nBrand deals are likely to break and sponsors are just as likely to pull out of backings if an influencer is under scrutiny. Making money is the primary goal and if a brand risks being tarnished at the hands of controversy, businesses are quick to pull the plug. High profile actors, influencers, YouTubers and others in the field of entertainment have lost brand deals based on inappropriate comments, poor judgement, and a lack of knowledge. \n\nOnline Beauty Guru Laura Lee lost a makeup line in the works backed by beauty supply store Ulta because of racist tweets of hers that surfaced back in 2012 where she used the N-word along with other racial slurs. Along with the halt put onto her makeup line, she lost famous beauty community sponsors Morphe Brushes and Diff Eyewear. In 2017, highest ranked YouTuber in the world PewDiePie was also under fire for using the n-word during one of his videogame live streams. Since then, YouTube has dropped Ad Sense from his videos, a source of income, and they cancelled the second season of his YouTube Red Show. Disney, a sponsor, also parted ways with the YouTuber. Although influencers often apologize for their mishaps, it is unheard of for brand deals and sponsorships to continue their relationship with those they worked with prior to the controversy. \n\nFake influencers have been around as long as their real counterparts have. All measurements used to determine the veracity of an influencer account can be fabricated. One social media site has shut down third-party sites and apps which provide paid services to individual accounts for buying followers, likes, comments and more. Despite these shutdowns, self-hosted automation apps such as FollowLiker continued to operate.\n\nA marketing agency recently conducted an experiment to test whether fake influencer accounts can profit. The company created 2 fictitious social media influencer accounts - with their followera built-up with bought followers and engagement (likes and comments) and applied to campaigns on popular influencer marketing websites. They published their experiment online with step-by-step explanation of how the accounts were created, and the brands which sponsored them.\n\nInfluencer marketing works so well because of social media. When it comes to ads, everyone ignores or fast forwards them on TV, and a majority completely bypasses them online with tools like AdBlock. \n\nAn analysis of more than 7,000 UK influencers, showed that about half the followers of influencers with up to 20,000 followers are \"low-quality\", due to the inclusion of mass followers, bots, and other suspicious accounts amongst their followers’. More than 4 of 10 engagements with this group of influencers are \"non-authentic\".\n\nA study of UK influencers which looked at almost 700,000 posts from the first half of 2018, found 12% of UK influencers had bought fake followers. \n\nWeb services can be used to trawl social media sites for users that exert influence in their respective communities. The social influencer marketing firm then asks those influencers to try client products or services and discuss them on their respective social networks. Clients can then observe, through an enhanced digital dashboard, with metrics that measure the dissemination of brand mentions across numerous web platforms.\n\nare at least 70 companies offering online influence measurement. Advocates of this online-only approach claim that online activity reflects (or pre-empts) the trends in offline transactions. For example, Razorfish released one of the first social influencer marketing reports, entitled Fluent. The report discusses many theories surrounding social marketing, including the importance of the push/pull dynamic and online consumer empowerment, authenticity and importance of buzz marketing.\n\nIn addition, online activity can be a core part of offline decision making, as consumers research products and review sites.\n\nCritics of this online-only approach argue that only researching online sources misses critical influential individuals and inputs. They note that much influential exchange of information occurs in the offline world, and is not captured in online media. Indeed, the majority of consumer exchanges occurs face-to-face, not in an online environment, as evidenced by Carl. He notes that \"an overwhelming majority of word-of-mouth (WOM) episodes (nearly 80%) ... occur in face-to-face interpersonal settings, while online WOM accounted for only seven to ten percent of the reported (WOM) episodes.\"\n\nCarl concludes that \"The majority of the WOM action still seems to be happening in the offline world. These findings are especially provocative since they emerge at a time when more and more organizations are paying attention to how their brands are discussed online and recent academic research has focused on online WOM. Thus it is important for organizations to keep both online and offline conversations on their radar screen.\"\n\nKeller Fay announced in 2007 that \"While experts have previously estimated that 80% of marketing-relevant word of mouth takes place “offline” (i.e., face-to-face or via telephone), the new results indicate that this figure is even higher – 92%.\"\n\nMore recently, Nate Elliott at Forrester observed that \"the huge majority of users influence each other face to face rather than through social online channels like blogs and social networks.\"\n\nWith any marketing strategy, risks are involved, and there have been reports of brands dropping their influencers because of the controversies that surround them. One such influencer is famous YouTuber PewDiePie, whose use of antisemitic and racist comments lead to canceled deals from Disney and widespread backlash.\n\nSources of influencers can be varied. Marketers traditionally target influencers that are easy to identify, such as press, industry analysts and high profile executives. For most B2C purchases, however, influencers might include people known to the purchaser and the retailer staff. In higher-value business-to-business transactions the community of influencers may be wide and varied, and include consultants, government-backed regulators, financiers and user communities.\n\nForrester analyst Michael Speyer notes that, for small and medium-size businesses, \"IT sales are influenced by many parties, including peers, consultants, bloggers, and technology resellers\". He advises that \"Vendors need to identify and characterize the influencers in their market. This requires a comprehensive influencer identification program and the establishment of criteria for ranking influencer impact on the decision process.\"\n\nAs well as a variety of influencer sources, influencers can play a variety of roles at different times in a decision process. This idea has been developed in influencer marketing by Brown and Hayes. They map out how and when particular types of influencer affect the decision process. This then enables marketers to selectively target influencers depending on their individual profile of influence.\n\nIn order to achieve the purpose of the business clients, influencers should deliver a change in attitude towards the client's brand or product. The change of the attitude from the viewer takes places from the contents that influencer produces. The change itself is believed as a psychological process in human mind. The psychological process can be explained under the RACE model. RACE stands for reach, act convert and engage. These four steps are designed to help brands engage with their customers throughout the customer lifecycle.\n\n\n"}
{"id": "53392628", "url": "https://en.wikipedia.org/wiki?curid=53392628", "title": "Land Reform (Scotland) Act 2016", "text": "Land Reform (Scotland) Act 2016\n\nThe Land Reform (Scotland) Act 2016 is an Act of the Scottish Parliament which continues the process of land reform in Scotland following the Community Empowerment (Scotland) Act 2015. It is notable for granting Scottish ministers the power to force the sale of private land to community bodies to further sustainable development in the absence of a willing seller.\n\nUnder the provisions of the act there is to be a ‘Land Rights and Responsibilities Statement’, setting out the Scottish Government’s objectives for land reform, a Scottish Land Commission is to take forward the land reform process, preparing a strategic plan, for the approval of Scottish ministers. One of the new land commissioners is to be a Tenant Farming Commissioner who must be neither an agricultural landlord nor a tenant but who is to be responsible for reviewing issues relating to tenant farming.\n\nA further provision is the creation of the Community Right to Buy for Sustainable Development. This permits Scottish ministers to approve the purchase of privately owned land by a community body with a registered interest. Unlike the Community Right to Buy established by the Land Reform (Scotland) Act 2003 and extended by the Community Empowerment (Scotland) Act 2015, the Community Right to Buy for Sustainable Development does not require a willing seller but allows ministers to compel landowners to sell if they decide that the sale will further sustainable development in the area. In this respect it is similar to the Crofting Community Right to Buy of the 2003 Act which allows crofting communities to purchase croft land and the Community Right to Buy abandoned or derelict land of the 2015 Act, neither of which require a willing seller. Community bodies may also register an interest in allowing a 3rd party to purchase land on the same basis. \n\nOther provisions of the act include new regulations to require persons who control land to be identified, with information obtained to appear in the Land Register of Scotland; the removal of sporting rights exemption from rates, which are to be re-valued; and further powers for Scottish Natural Heritage to control deer management. It also makes provision for notice and consultation where core paths are to be amended.\n\n"}
{"id": "17368856", "url": "https://en.wikipedia.org/wiki?curid=17368856", "title": "Linear continuum", "text": "Linear continuum\n\nIn the mathematical field of order theory, a continuum or linear continuum is a generalization of the real line.\nFormally, a linear continuum is a linearly ordered set \"S\" of more than one element that is densely ordered, i.e., between any two distinct elements there is another (and hence infinitely many others), and which \"lacks gaps\" in the sense that every non-empty subset with an upper bound has a least upper bound. More symbolically:\n\nA set has the least upper bound property, if every nonempty subset of the set that is bounded above has a least upper bound. Linear continua are particularly important in the field of topology where they can be used to verify whether an ordered set given the order topology is connected or not.\n\nUnlike the standard real line, a linear continuum may be bounded on either side: for example, any (real) closed interval is a linear continuum.\n\n\nExamples in addition to the real numbers:\n\nThis map is known as the projection map. The projection map is continuous (with respect to the product topology on \"I\" × \"I\") and is surjective. Let \"A\" be a nonempty subset of \"I\" × \"I\" which is bounded above. Consider \"π\"(\"A\"). Since \"A\" is bounded above, \"π\"(\"A\") must also be bounded above. Since, \"π\"(\"A\") is a subset of \"I\", it must have a least upper bound (since \"I\" has the least upper bound property). Therefore, we may let \"b\" be the least upper bound of \"π\"(\"A\"). If \"b\" belongs to \"π\"(\"A\"), then \"b\" × \"I\" will intersect \"A\" at say \"b\" × \"c\" for some \"c\" ∈ \"I\". Notice that since \"b\" × \"I\" has the same order type of \"I\", the set (\"b\" × \"I\") ∩ \"A\" will indeed have a least upper bound \"b\" × \"c\"', which is the desired least upper bound for \"A\".\n\nIf \"b\" does not belong to \"π\"(\"A\"), then \"b\" × 0 is the least upper bound of \"A\", for if \"d\" < \"b\", and \"d\" × \"e\" is an upper bound of \"A\", then \"d\" would be a smaller upper bound of \"π\"(\"A\") than \"b\", contradicting the unique property of \"b\".\n\n\n\n\nEven though linear continua are important in the study of ordered sets, they do have applications in the mathematical field of topology. In fact, we will prove that an ordered set in the order topology is connected if and only if it is a linear continuum (notice the 'if and only if' part). We will prove one implication, and leave the other one as an exercise. (Munkres explains the second part of the proof )\n\nTheorem\n\nLet \"X\" be an ordered set in the order topology. If \"X\" is connected, then \"X\" is a linear continuum.\n\n\"Proof:\"\n\nSuppose, x is in X and y is in X where x < y. If there exists no z in X such that x < z < y, consider the sets:\n\nA = (−∞, y)\n\nB = (x, +∞)\n\nThese sets are disjoint (If a is in A, a < y so that if a is in B, a > x and a < y which is impossible by hypothesis), nonempty (x is in A and y is in B) and open (in the order topology) and their union is X. This contradicts the connectedness of X.\n\nNow we prove the least upper bound property. If C is a subset of X that is bounded above and has no least upper bound, let D be the union of all open rays of the form (b, +∞) where b is an upper bound for C. Then D is open (since it is the union of open sets), and closed (if 'a' is not in D, then a < b for all upper bounds b of C so that we may choose q > a such that q is in C (if no such q exists, a is the least upper bound of C), then an open interval containing a, may be chosen that doesn't intersect D). Since D is nonempty (there is more than one upper bound of D for if there was exactly one upper bound s, s would be the least upper bound. Then if b and b are two upper bounds of D with b < b, b will belong to D), D and its complement together form a separation on X. This contradicts the connectedness of X.\n\n1. Since the ordered set:\n\nA = (−∞, 0) U (0,+∞)\n\nis not a linear continuum, it is disconnected.\n\n2. By applying the theorem just proved, the fact that R is connected follows. In fact any interval (or ray) in R is also connected.\n\n3. The set of integers is not a linear continuum and therefore cannot be connected.\n\n4. In fact, if an ordered set in the order topology is a linear continuum, it must be connected. Since any interval in this set is also a linear continuum, it follows that this space is locally connected since it has a basis consisting entirely of connected sets.\n\n5. For an example of a topological space that is a linear continuum, see long line.\n\n"}
{"id": "42812528", "url": "https://en.wikipedia.org/wiki?curid=42812528", "title": "Michigan Studies of Leadership", "text": "Michigan Studies of Leadership\n\nThe Michigan Leadership Studies were the well-known series of leadership studies that commenced at the University of Michigan in the 1950s by Rensis Likert, with the objective of identifying the principles and types of leadership styles that led to greater productivity and enhanced job satisfaction among workers. The studies identified two broad leadership styles – an employee orientation and a production orientation. They also identified three critical characteristics of effective leaders – task-oriented behavior, relationship-oriented behavior and participative leadership. The studies concluded that an employee orientation rather than a production orientation, coupled with general instead of close supervision, led to better results. The Michigan leadership studies, along with the Ohio State University studies that took place in the 1940s, are two of the best-known behavioral leadership studies and continue to be cited to this day.\n\n\n"}
{"id": "1505572", "url": "https://en.wikipedia.org/wiki?curid=1505572", "title": "Moral courage", "text": "Moral courage\n\nMoral courage is the courage to take action for moral reasons despite the risk of adverse consequences.\n\nCourage is required to take action when one has doubts or fears about the consequences. Moral courage therefore involves deliberation or careful thought. Reflex action or dogmatic fanaticism do not involve moral courage because such impulsive actions are not based upon moral reasoning.\n\nMoral courage may also require physical courage when the consequences are punishment or other bodily peril.\n\nMoral courage has been seen as the exemplary modernist form of courage.\n\nParenting with the incorporation of moral courage can have an effect on the gender roles and self-expression of young adolescents. For example, young girls who conform to society's interpretation of women being passive.<ref name=\"Parenting/Gender Roles\"></ref> Both the parents and children engage in moral courage from different standpoints. The development of moral courage within parenting looks at not only on the parent's passed down moral values but the children's autonomy on how to perceive and practice their moral values. Those who incorporate the practice of their morals values into their everyday lives engage in moral courage to protect those values as well. Therefore, this parenting approach with practice of moral courage demonstrates a relationship between how parents morally raise their children and how the children choose to act based on their learned moral values.\n\nIn a workplace, LGBT employees can experience behaviors or actions of discriminating nature as well as violent hate crimes. As sexual and gender identity minorities, the LGBT employees express a need for straight or heterosexual allies in the workplace to go to as a resource.\n\nA research study was performed using qualitative research methods to analyze the process of how and individuals become LGBT allies. The study mentions how human resources development play a role in this matter. Human resources development professionals have the responsibility and the power to incorporate what LGBT employees desire in a workplace: inclusion, safety, and equal treatment with heterosexual employees.\n\nMoral courage distinguishes the abilities of a heterosexual LGBT ally and a human resources development professional. While heterosexual LGBT allies, acting on their moral values provide support and stand up for their LGBT colleagues, human resource development professionals and their positions in the workplace translates their values into action which translates into movement for change regarding LGBT discrimination.\n\n"}
{"id": "4398216", "url": "https://en.wikipedia.org/wiki?curid=4398216", "title": "Mount Penglai", "text": "Mount Penglai\n\nPenglai (Traditional Chinese: 蓬萊仙島 (\"lit. Penglai Immortal Island))\" is a legendary land of Chinese mythology. It is known in Japanese mythology as Hōrai.\n\nAccording to the \"Classic of Mountains and Seas\", the mountain is said to be on an island in the eastern end of Bohai Sea, along with four other islands where the immortals lived, called Fāngzhàng (), Yíngzhōu (), Dàiyú (), and Yuánjiāo ().\n\nVarious theories have been offered over the years as to the \"real\" location of these places, including Japan, Nam-Hae (南海), Geo-Je (巨濟), Jejudo (濟州島) south of the Korean Peninsula, and Taiwan. Penglai, Shandong exists, but its claimed connection is as the site of departures for those leaving for the island rather than the island itself.\n\nIn Chinese mythology, the mountain is often said to be the base for the Eight Immortals, or at least where they travel to have a banquet, as well as the magician Anqi Sheng. Supposedly, everything on the mountain seems white, while its palaces are made from gold and platinum, and jewelry grows on trees.\n\nThere is no pain and no winter; there are rice bowls and wine glasses that never become empty no matter how much people eat or drink from them; and there are magical fruits growing in Penglai that can heal any disease, grant eternal youth, and even raise the dead.\n\nHistorically, Qin Shi Huang, in search of the elixir of life, made several attempts to find the island where the mountain is located, to no avail. Legends tell that Xu Fu, one servant sent to find the island, found Japan instead, and named Mount Fuji as Penglai.\n\nThe presentation of Mt. Hōrai in Lafcadio Hearn's \"\", is somewhat different from the earlier idyllic Chinese myth. This version, which does not truly represent the Japanese views of Horai in the Meiji and preceding Tokugawa periods, rejects much of the fantastic and magical properties of Hōrai. In this version of the myth, Hōrai is not free from sorrow or death, and the winters are bitterly cold. Hearn's conception of Hōrai holds that there are no magical fruits that cure disease, grant eternal youth or raise the dead, and no rice bowls or wine glasses that never become empty.\n\nHearn's incarnation of the myth of Hōrai focuses more on the atmosphere of the place, which is said to be made up not of air but of \"quintillions of quintillions\" of souls. Breathing in these souls is said to grant one all of the perceptions and knowledge of these ancient souls. The Japanese version also holds that the people of Hōrai are small fairies, and they have no knowledge of great evil, and so their hearts never grow old.\n\nIn the \"Kwaidan\", there is some indication that the Japanese hold such a place to be merely a fantasy. It is pointed out that \"Hōrai is also called Shinkiro, which signifies Mirage—the Vision of the Intangible\".\n\nYet uses of Mount Hōrai in Japanese literature and art of the Tokugawa period (1615–1868) reveal a very different view than Hearn's Victorian-influenced interpretation.\n\n\n"}
{"id": "738178", "url": "https://en.wikipedia.org/wiki?curid=738178", "title": "Orbital spaceflight", "text": "Orbital spaceflight\n\nAn orbital spaceflight (or orbital flight) is a spaceflight in which a spacecraft is placed on a trajectory where it could remain in space for at least one orbit. To do this around the Earth, it must be on a free trajectory which has an altitude at perigee (altitude at closest approach) above ; this is, by at least one convention, the boundary of space. To remain in orbit at this altitude requires an orbital speed of ~7.8 km/s. Orbital speed is slower for higher orbits, but attaining them requires greater delta-v.\n\nDue to atmospheric drag, the lowest altitude at which an object in a circular orbit can complete at least one full revolution without propulsion is approximately .\n\nThe expression \"orbital spaceflight\" is mostly used to distinguish from sub-orbital spaceflights, which are flights where the apogee of a spacecraft reaches space, but the perigee is too low.\n\nOrbital spaceflight from Earth has only been achieved by launch vehicles that use rocket engines for propulsion. To reach orbit, the rocket must impart to the payload a delta-v of about 9.3–10 km/s. This figure is mainly (~7.8 km/s) for horizontal acceleration needed to reach orbital speed, but allows for atmospheric drag (approximately 300 m/s with the ballistic coefficient of a 20 m long dense fueled vehicle), gravity losses (depending on burn time and details of the trajectory and launch vehicle), and gaining altitude.\n\nThe main proven technique involves launching nearly vertically for a few kilometers while performing a gravity turn, and then progressively flattening the trajectory out at an altitude of 170+ km and accelerating on a horizontal trajectory (with the rocket angled upwards to fight gravity and maintain altitude) for a 5-8 minute burn until orbital velocity is achieved. Currently, 2-4 stages are needed to achieve the required delta-v. Most launches are by expendable launch systems.\n\nThe Pegasus rocket for small satellites instead launches from an aircraft at an altitude of 12 km.\n\nThere have been many proposed methods for achieving orbital spaceflight that have the potential of being much more affordable than rockets. Some of these ideas such as the space elevator, and rotovator, require new materials much stronger than any currently known. Other proposed ideas include ground accelerators such as launch loops, rocket assisted aircraft/spaceplanes such as Reaction Engines Skylon, scramjet powered spaceplanes, and RBCC powered spaceplanes. Gun launch has been proposed for cargo.\n\nFrom 2015 SpaceX have demonstrated significant progress in their more incremental approach to reducing the cost of orbital spaceflight. Their potential for cost reduction comes mainly from pioneering propulsive landing with their reusable rocket booster stage as well as their Dragon capsule, but also includes reuse of the other components such as the payload fairings and the use of 3D printing of a superalloy to construct more efficient rocket engines, such as their SuperDraco. The initial stages of these improvements could reduce the cost of an orbital launch by an order of magnitude.\n\nAn object in orbit at an altitude of less than roughly 200 km is considered unstable due to atmospheric drag. For a satellite to be in a stable orbit (i.e. sustainable for more than a few months), 350 km is a more standard altitude for low Earth orbit. For example, on 1 February 1958 the Explorer 1 satellite was launched into an orbit with a perigee of . It remained in orbit for more than 12 years before its atmospheric reentry over the Pacific Ocean on 31 March 1970.\n\nHowever, the exact behaviour of objects in orbit depends on altitude, their ballistic coefficient, and details of space weather which can affect the height of the upper atmosphere.\n\nThere are three main 'bands' of orbit around the Earth: low Earth orbit (LEO), medium Earth orbit (MEO) and geostationary orbit (GEO).\n\nDue to orbital mechanics, orbits are in a particular, largely fixed plane around the Earth, which coincides with the center of the Earth, and may be tilted with respect to the equator. The Earth rotates about its axis within this orbit, and the relative motion of the spacecraft and the movement of the Earth's surface determines the position that the spacecraft appears in the sky from the ground, and which parts of the Earth are visible from the spacecraft.\n\nBy dropping a vertical down to the Earth's surface it is possible to calculate a ground track that shows which part of the Earth a spacecraft is immediately above, and this is useful for helping to visualise the orbit.\n\nIn spaceflight, an orbital maneuver is the use of propulsion systems to change the orbit of a spacecraft. For spacecraft far from Earth—for example those in orbits around the Sun—an orbital maneuver is called a \"deep-space maneuver (DSM)\".\n\nReturning spacecraft (including all potentially manned craft) have to find a way of slowing down as much as possible while still in higher atmospheric layers and avoid hitting the ground (lithobraking) or burning up. For many orbital space flights, initial deceleration is provided by the retrofiring of the craft's rocket engines, perturbing the orbit (by lowering perigee down into the atmosphere) onto a suborbital trajectory. Many spacecraft in low-Earth orbit (e.g., nanosatellites or spacecraft that have run out of station keeping fuel or are otherwise non-functional) solve the problem of deceleration from orbital speeds through using atmospheric drag (aerobraking) to provide initial deceleration. In all cases, once initial deceleration has lowered the orbital perigee into the mesosphere, all spacecraft lose most of the remaining speed, and therefore kinetic energy, through the atmospheric drag effect of aerobraking.\n\nIntentional aerobraking is achieved by orienting the returning space craft so as to present the heat shields forward toward the atmosphere to protect against the high temperatures generated by atmospheric compression and friction caused by passing through the atmosphere at hypersonic speeds. The thermal energy is dissipated mainly by compression heating the air in a shockwave ahead of the vehicle using a blunt heat shield shape, with the aim of minimising the heat entering the vehicle.\n\nSub-orbital space flights, being at a much lower speed, do not generate anywhere near as much heat upon re-entry.\n\nEven if the orbiting objects are expendable, most space authorities are pushing toward controlled re-entries to minimize hazard to lives and property on the planet.\n\n\n"}
{"id": "1191544", "url": "https://en.wikipedia.org/wiki?curid=1191544", "title": "Pacers–Pistons brawl", "text": "Pacers–Pistons brawl\n\nThe Pacers–Pistons brawl (colloquially known as the Malice at the Palace) was an altercation that occurred in a National Basketball Association (NBA) game between the Indiana Pacers and the Detroit Pistons on November 19, 2004, at The Palace of Auburn Hills in Auburn Hills, Michigan. The Associated Press (AP) called it \"the most infamous brawl in NBA history\", while the media has dubbed it the \"worst night in NBA history\". \n\nWith 45.9 seconds left in the game, Pistons center Ben Wallace went up for a layup, but was fouled by Pacers small forward Ron Artest. Furious for being fouled when the game had already been decided, Wallace pushed Artest. A fight broke out on the court between several players. After the fight was broken up, a fan threw a drink from the stands at Artest while he was lying on the scorer's table. Artest immediately charged after the fan, sparking a massive brawl between players and spectators that stretched from the seats down to the court and lasted several minutes.\n\nAfter the game, the NBA suspended nine players for a total of 146 games, which led to $11 million in salary being lost by the players. Five players were also charged with assault, and eventually sentenced to a year of probation and community service. Five fans also faced criminal charges and were banned from attending Pistons home games for life. The fight also led the NBA to increase security between players and fans, and to limit the sale of alcohol in games.\n\nThe meeting was the first between the two teams since the previous season's , which the Pistons won in six games en route to their first NBA title since the of the late 1980s and early 1990s. This caused the game to receive much hype from the media and fans. Having won two games in a row, the Pacers came into the game with a 6–2 record, while the Pistons, the defending champions, began their season 4–3. The game was televised nationally on ESPN, as well as on the Pacers' and Pistons' local broadcast affiliates, Fox Sports Midwest and WDIV (Detroit's NBC affiliate), respectively.\n\nThe game, like many previous meetings between the two teams, was dominated by defense. The Pacers got off to a quick start, opening up a 20-point lead with seven minutes to go before halftime. The Pistons managed to cut into the lead, trailing by 16 points by halftime. The Pistons opened the third quarter with a 9–2 run, but the Pacers ended it with a buzzer-beating three-pointer and a layup from Jamaal Tinsley heading into the fourth quarter. Richard Hamilton and Lindsey Hunter started the last quarter with consecutive three-point field goals, as the Pistons cut into the lead again. But Stephen Jackson's back-to-back field goals pushed the lead back to 93–79 with 3:52 remaining, essentially putting the Pistons away. Despite the lopsided score near the end of the game, most key players on both teams remained in the game.\n\nThe Pacers were led by the 24-point effort of Ron Artest, who scored 17 in the first quarter. Jermaine O'Neal notched a double-double with 20 points and 13 rebounds. Tinsley had 13 points, eight assists and a career-high eight steals. Hamilton led the Pistons with 20 points. Rasheed Wallace and Ben Wallace both recorded a double-double.\n\nThe brawl began with 45.9 seconds remaining in the game, when Indiana led 97–82. Pistons center Ben Wallace was fouled from behind by Pacers small forward Ron Artest, now known as Metta World Peace, who slapped him across the back of the head during a layup attempt. Wallace later said that Artest had warned him he would be hit. Wallace responded by shoving Artest in the face with both hands, causing players from both teams to quickly get in between them as they attempted to keep Wallace and Artest separated.\n\nPistons coach Larry Brown was not yet very concerned, because fights in the NBA rarely lasted for more than a few seconds. During the altercation, Artest lay down on the scorer's table to relax while putting on a headset to speak with Pacers radio broadcaster Mark Boyle. The microphone was not live. Boyle recalled that the broadcasting team knew Artest's personality and \"there was no way we were going to put an open mic in front of [him] in that situation\". Pacers president Donnie Walsh later stated that Artest was following advice he had received on how to calm down and avoid trouble in a volatile situation. After unsuccessfully attempting to break up the confrontation, referees prepared to eject various players before the game resumed. Sportscaster Mike Breen, calling the game for ESPN, believed Wallace would be ejected, while Bill Walton was of the opinion that Stephen Jackson should be ejected as well, for shouting at the Pistons players and aggravating the situation. However, Breen expressed concern that, if ejected, Wallace would have to walk past the Pacers bench, which could have triggered another incident.\n\nNinety seconds after Wallace shoved Artest, most of both teams' players and coaches were huddled at midcourt, attempting to calm down Wallace (Tayshaun Prince was the only player on either team to not leave the bench during the entire incident; others became automatically eligible for one-game suspensions). While Artest was lying on the table, Wallace threw a towel at him, causing Artest to briefly stand up before being held back by coaches. A spectator, John Green, then threw a plastic cup of Diet Coke at Artest, hitting him in the chest. Artest jumped off the table, ran into the stands, and grabbed a man, Michael Ryan, who he mistakenly believed was responsible. Boyle stood up to try and hold back Artest and was trampled in the effort, suffering five fractured vertebrae and a gouge on his head. Jackson followed Artest into the stands and punched a fan, William Paulson, in the face in retaliation for the man throwing another drink in Artest's face while he was being restrained by other spectators. Pacers players Eddie Gill, David Harrison, Reggie Miller (who was not dressed for the game due to injury), Fred Jones, and Jamaal Tinsley, the Pistons' Rasheed Wallace, and numerous personnel (including Pistons radio analyst and former player Rick Mahorn) also quickly entered the stands to retrieve Artest and Jackson, and to break up the fighting. Green punched Artest twice in the head from behind, as did Ben Wallace's brother, David, to Jones. More fans then began throwing drinks and other objects, while a number of fans spilled out onto the court.\n\nAs Artest walked out of the stands, he was confronted by two more fans, Alvin \"A.J.\" Shackleford and Charlie Haddad, who ran onto the court. Artest punched Shackleford in the face, causing Haddad to intervene by pushing away Artest, before both fans fell over. While Haddad was on the floor, Anthony Johnson struck him in the back of the head. As Haddad stood up, Jermaine O'Neal punched him in the jaw after a running start, while slipping in liquid and falling backwards, causing witnesses Scot Pollard, ESPN sideline reporter Jim Gray, and a Pistons executive, Tom Wilson, to briefly fear that O'Neal would kill Haddad. O'Neal later claimed that Haddad had been asked to leave the arena earlier that night, and was well-known to security because of claims that he wanted to fight an NBA player in order to receive compensation. William Wesley, Austin Croshere, and Miller pulled Artest away from the fans, but the scene became chaotic as outnumbered arena security struggled to reestablish order. Although Auburn Hills police had plans to handle many disorders, and had three officers in the arena, they were unprepared for players entering the stands.\n\nPacers coach Rick Carlisle said after the game, \"I felt like I was fighting for my life out there.\" One reporter who attempted to stop Tinsley from entering the stands recalled that the player \"went through me like I was butter\", and then NBA Commissioner David Stern, watching the game on TV, recalled that he said, \"Holy [\"mouths a swear word\"].\" (O'Neal later said, \"As bad as it looked on TV, it was at least 20 times worse in person.\") Pacers assistant coach Chuck Person compared the situation to being \"trapped in a gladiator-type scene where the fans were the lions and we were just trying to escape with our lives. That's how it felt. That there was no exit. That you had to fight your way out.\" Players' children and others in the audience cried from fear and shock. Derrick Coleman stood near Brown and Brown's ball boy son to protect them. The remaining seconds of the game were called off and the Pacers were awarded the 97–82 win. Fans booed the Indiana players as they were escorted from the court by officials and security, and continued to throw beverages and other objects (including a folding chair that nearly hit O'Neal) at them as they walked under the tunnel to the locker room. A replay on ESPN's broadcast showed Larry Brown attempting to talk to the fans over the loudspeaker of the arena in an attempt to stop the debacle, but Brown threw the microphone down in disgust after the microphone failed to work. No players from either team spoke to the media before leaving the arena. Eventually, police officers were able to swarm the arena, threatening to handcuff those who wouldn't leave. Nine spectators were injured, and two were taken to the hospital.\n\nIn the Pacers' locker room, O'Neal and Carlisle nearly got into a fight over the coaches trying to restrain the players when they were defending themselves. Artest asked Jackson whether he thought the players would get in trouble. Jackson responded, \"We'll be lucky if we have a freaking job.\" The conversation convinced an amazed Jackson and Pollard that Artest \"wasn't in his right mind, to ask that question\". Auburn Hills police entered the locker room to make arrests, but the team rushed Artest onto the bus and refused to take him off. The police decided to protect the Pacers as they left the arena and to later contact the team after reviewing game film. Outside of the arena, there were dozens of police cruisers lining the roads out of the parking lots.\n\nOn November 20, 2004, the NBA suspended Artest, Jackson, O'Neal, and Wallace indefinitely, saying that their actions were \"shocking, repulsive, and inexcusable\". The following day, the NBA announced that nine players would be suspended for a total of what eventually became 146 games–137 games for Pacers players and nine games for Pistons players. David Harrison was also seen fighting with fans, but the NBA stated that he wouldn't be suspended because \"the incident occurred as the players were attempting to leave the floor\".\n\nArtest was given the longest suspension; he was suspended for the remainder of the 2004–05 NBA season, a suspension that eventually totaled 86 games (73 regular season and 13 subsequent playoff games), the longest suspension for an on-court incident in NBA history. The players suspended also lost in excess of $11 million in salary due to the suspensions, with Artest alone losing almost $5 million.\n\nIn the week following the announcement of the suspensions, the players' union appealed the suspensions of Artest, Jackson, and O'Neal, saying they thought that commissioner Stern had \"exceeded his authority\". (Jackson felt that despite losing millions the players were fortunate, however, as Stern could have expelled them from the league.) A federal arbitrator upheld the full length of all suspensions, except that of O'Neal, which was reduced to 15 games. However, the NBA appealed the decision of the arbitrator to reduce O'Neal's suspension in federal court, and on December 24, a judge issued a temporary injunction allowing O'Neal to play, until a full hearing was held on the NBA's appeal.\n\nO'Neal played in two more games before the NBA's case was brought before the U.S. District Court in Brooklyn, New York on December 30. The NBA argued that under the terms of the Collective Bargaining Agreement (CBA), Stern had absolute authority to hand out suspensions and hear appeals for all on-court incidents. But the judge ruled that because O'Neal's behavior was an off-court incident, arbitration was allowed under the CBA, and thus the arbitrator was within his rights to reduce the suspension. Despite O'Neal's successful appeal, no further appeals were made to reduce Artest's and Jackson's suspensions.\n\nGreen was identified by county prosecutor David Gorcyca, who had been his neighbor. On November 30, Palace Sports and Entertainment, the owner of the Pistons, banned Green and Haddad from attending any events at Palace properties (including the Palace of Auburn Hills and the DTE Energy Music Theatre), revoked their season tickets and issued them refunds. Green had several previous criminal convictions, including counterfeiting, carrying a concealed weapon, felony assault, and three drunken driving convictions, and he was on court-ordered probation from a DUI conviction at the time of the brawl.\n\nOn December 8, 2004, five Indiana players and five fans (John Ackerman, John Green, Bryant Jackson, William Paulson, and David Wallace, Ben Wallace's brother) were charged for assault and battery. Player O'Neal (who also threw usher Melvin Kendziorski onto the scorer's table when attempting to enter the stands) and spectator Green, who Gorcyca said \"single-handedly incited\" the brawl by throwing a cup of liquid at Artest, were charged with two counts, and Artest, Harrison, Jackson, and Anthony Johnson were charged with one count each. Three fans, including David Wallace, received one count of the same charge; two fans (Haddad and Shackleford) who entered the court during the fight were charged with trespassing, and Bryant Jackson, who had prior criminal convictions, was charged with felony assault for throwing a chair. All of the fans involved were banned from attending Pistons games.\n\nOn March 29, 2005, Bryant Jackson pleaded no contest to a felony assault charge for throwing the chair, and on May 3, 2005, he was sentenced to two years' probation and ordered to pay $6,000 in restitution. David Wallace was also convicted, and sentenced to one year of probation and community service for punching Pacers guard Fred Jones from behind.\n\nAll five players who were charged pleaded no contest to the charges. On September 23, 2005, Artest, Jackson, and O'Neal were all sentenced to one year on probation, 60 hours of community service, a $250 fine, and anger management counseling. A week later, Harrison received the same sentence, and on October 7, 2005, Johnson, the last player to be charged, received a similar sentence (he was ordered to serve 100 hours of community service).\n\nOn March 27, 2006, a jury found Green guilty on one count of assault and battery for punching Artest in the stands, but acquitted him of an assault charge for throwing the cup. On May 1, 2006, Green was sentenced to 30 days in jail and two years' probation. On November 7, 2006, the Pistons issued a letter to Green informing him that he was banned for life from attending any Pistons home games under orders from the NBA (although the ban does not extend to other events at the Palace of Auburn Hills).\n\nSeveral NBA players and coaches said the brawl was the worst fight they had ever seen. Pacers fans began to refer to the team as \"the Thugs\". Pistons CEO Tom Wilson later stated that Artest's action took away physical barriers, such as tables and benches, that normally separate fans and players, and \"Indianapolis Star\" reporter Mark Montieth claimed that \"in a way, [Artest] provoked [the forthcoming assault] passively by lying down\". In the post-game commentary on ESPN's \"NBA Shootaround\", ESPN's studio analysts laid the blame on the Pistons' fans instead of on the players. John Saunders of ESPN referred to the fans as \"a bunch of punks\". Tim Legler said that \"the fans crossed the line\". Stephen A. Smith stated that \"They should be ashamed of themselves and some of them (the fans) should be arrested as far as I'm concerned\". Their commentary prompted ESPN vice president Mark Shapiro to place calls to host Saunders, as well as analysts Legler, Smith, and Greg Anthony. Shapiro felt their commentary was biased. The following Tuesday, Shapiro stated, \"I wish the studio hadn't laid the blame solely on the backs of the fans Friday night.\"\n\nA significant portion of media criticism was directed at the Pistons fans and 46% of the voters in the ESPN SportsNation poll believed that the fans were to blame for the incident. Other commentators said that Artest and the other players involved were to blame.\n\nThe Pacers and Pistons played for the first time after the brawl on December 25 at the Conseco Fieldhouse in Indianapolis. The Pistons won 98–93 without any incidents. Neither Artest nor Jackson played due to their suspensions; O'Neal played in his first game back after the arbitrator reduced his suspension to 15 games. Some NBA teams immediately increased protection of players and arenas, the NFL reminded teams of existing security procedures, and on February 17, 2005, the NBA imposed new security guidelines for all NBA arenas. The new policies included a size limit of for alcohol purchases and a hard cap of two alcoholic beverage purchases for any individual person, as well as a ban of alcohol sales after the end of the third quarter. They also later ordered that each team put at least three security guards between the players and the fans.\n\nOn March 25, 2005, the Pacers played at The Palace for the first time since the brawl. The game was delayed 90 minutes after a series of bomb threats were aimed at the Pacers locker room, but the game eventually started after no explosives were found. Two of the key figures in the original incident missed the game, as Artest was still suspended and O'Neal had an injured shoulder. In the game, the Pacers stopped the Pistons' twelve-game winning streak with a 94–81 win.\nIn the playoffs, Detroit entered as the second seed of the Eastern Conference, and Indiana as the sixth. After the Pistons defeated the Philadelphia 76ers in five games, and the Pacers upset the third seed Boston Celtics in seven games, the two teams met in the second round. Although the Pacers went ahead two games to one, the Pistons clinched the series in six games with three straight wins. After eliminating Indiana, Detroit defeated the Miami Heat in the Eastern Conference Finals in seven games, then advanced to the NBA Finals, where they lost to the San Antonio Spurs in seven games.\n\nAfter serving his suspension of the rest of the , Ron Artest returned to the Pacers at the beginning of the . After playing 16 games he demanded to be traded, and the Pacers put him on the injured list. The Pacers' Walsh said that Artest's demands were \"the last straw\", and many Pacers players who had fought in the brawl to help their teammate felt betrayed. Jackson later said, \"I put my career on the line for him, going into the stands and fighting ... I lost $3 million [but] there was no 'thank you' or nothing.\" After more than a month of inactivity, Indiana traded Artest to the Sacramento Kings for Peja Stojaković. Artest faced Ben Wallace for the first time after the fight in November 2006, and finally made his return to Detroit in January 2007. During the Kings' 91–74 loss to the Pistons, Artest was booed constantly, but there were no unusual incidents. After a year's stop with the Houston Rockets in the 2008–09 season, Artest signed with the Los Angeles Lakers. After winning his first NBA championship in , Artest apologized to Jackson and other Pacers for being \"so young and egotistical\", stating \"sometimes I feel like a coward when I see those guys. I'm on the Lakers, but I had a chance to win with you guys. I feel almost like a coward.\" On September 16, 2011, Artest legally changed his name to Metta World Peace.\n\nDuring the , only one of the nine players who were suspended after the brawl was still with his original team: Ben Wallace, who signed with the Chicago Bulls as a free agent in 2006, later traded to the Cleveland Cavaliers, and rejoined the Pistons on August 7, 2009. Most of the players involved were traded to other teams, and since then, all of the players involved in the brawl have retired, with Artest being the last to do so in 2017.\nThe Pistons advanced to four straight Eastern Conference Finals after the brawl, and six straight overall, making them the first team since the Los Angeles Lakers in the 1980s to advance to six straight conference finals though they only won the championship once in that streak. However, after losing to the Pistons in the 2005 playoffs, the Pacers failed to finish above .500 until the and missed the playoffs for five straight seasons from 2006 through 2010. Many Pacers from the 2004–05 season believe that the brawl and its consequences ruined a potential championship team, with Artest as the cause. The Pacers have attempted to rebuild by obtaining \"character guys\" as players.\n\nOn November 19, 2009, John Green, one of the fans who helped begin the brawl, appeared on \"ESPN First Take\", where he talked about the incident and the changes he had made since then. Green recounted that he had an alcohol problem at the time and had since made an effort to deal with that. He also said that Ron Artest had apologized to him several months earlier, and wished to work together in some type of community service in Detroit.\n\n"}
{"id": "2903306", "url": "https://en.wikipedia.org/wiki?curid=2903306", "title": "Paper negative", "text": "Paper negative\n\nThe paper negative process consists of using a negative printed on paper (either photographically or digitally) to create the final print of a photograph, as opposed to using a modern negative on a film base of cellulose acetate. The plastic acetate negative (which is what modern films produce) enables the printing of a very sharp image intended to be as close a representation of the actual subject as is possible. By using a negative based on paper instead, there is the possibility of creating a more ethereal image, simply by using a paper with a very visible grain, or by drawing on the paper or distressing it in some way. \n\nOne of the original forms of photography was based on the paper negative process. William Henry Fox Talbot's paper negative process, which was used to create his work \"The Pencil of Nature\", used a negative created on paper treated with silver salts, which was exposed in a camera obscura to create the negative and then contact printed on a similar paper to produce a positive image. \n\nWhen Talbot created this process it was intended to be a way to reproduce nature as accurately as possible (hence the name of his work, \"The Pencil of Nature\"). Through the years afterwards, however, better and more accurate ways of producing exact replicas of nature were developed, and these processes relegated the paper negative process to obsolescence. \n\nThe process of the paper negative is still relevant, though, in the realm of alternative-process photography. Photographers employing alternative processes reject the idea of the exact replica of nature and seek to use the inherent inexactness of antiquated processes to create a more personal and emotional image. The paper negative is an extremely versatile process that allows all manner of reworking and retouching of an image, and is the perfect medium to bridge the gap between camera operator and artist.\n\n\n\n"}
{"id": "143135", "url": "https://en.wikipedia.org/wiki?curid=143135", "title": "Parity (mathematics)", "text": "Parity (mathematics)\n\nIn mathematics, parity is the property of an integer's inclusion in one of two categories: even or odd. An integer is even if it is evenly divisible by two and odd if it is not even. For example, 6 is even because there is no remainder when dividing it by 2. By contrast, 3, 5, 7, 21 leave a remainder of 1 when divided by 2. Examples of even numbers include −4, 0, 82 and 178. In particular, zero is an even number. Some examples of odd numbers are −5, 3, 29, and 73. \n\nA formal definition of an even number is that it is an integer of the form \"n\" = 2\"k\", where \"k\" is an integer; it can then be shown that an odd number is an integer of the form \"n\" = 2\"k\" + 1. It is important to realize that the above definition of parity applies only to integer numbers, hence it cannot be applied to numbers like 1/2, 4.201. See the section \"Higher mathematics\" below for some extensions of the notion of parity to a larger class of \"numbers\" or in other more general settings.\n\nThe sets of even and odd numbers can be defined as following:\n\n\nA number (i.e., integer) expressed in the decimal numeral system is even or odd according to whether its last digit is even or odd.\nThat is, if the last digit is 1, 3, 5, 7, or 9, then it is odd; otherwise it is even. The same idea will work using any even base.\nIn particular, a number expressed in the binary numeral system is odd if its last digit is 1 and even if its last digit is 0.\nIn an odd base, the number is even according to the sum of its digits – it is even if and only if the sum of its digits is even.\n\nThe following laws can be verified using the properties of divisibility. They are a special case of rules in modular arithmetic, and are commonly used to check if an equality is likely to be correct by testing the parity of each side. As with ordinary arithmetic, multiplication and addition are commutative and associative in modulo 2 arithmetic, and multiplication is distributive over addition. However, subtraction in modulo 2 is identical to addition, so subtraction also possesses these properties, which is not true for normal integer arithmetic.\n\n\n\nThe structure ({even, odd}, +, ×) is in fact a field with just two elements.\n\nThe division of two whole numbers does not necessarily result in a whole number. \nFor example, 1 divided by 4 equals 1/4, which is neither even \"nor\" odd, since the concepts even and odd apply only to integers.\nBut when the quotient is an integer, it will be even if and only if the dividend has more factors of two than the divisor.\n\nThe ancient Greeks considered 1, the monad, to be neither fully odd nor fully even. Some of this sentiment survived into the 19th century: Friedrich Wilhelm August Fröbel's 1826 \"The Education of Man\" instructs the teacher to drill students with the claim that 1 is neither even nor odd, to which Fröbel attaches the philosophical afterthought,\n\nInteger coordinates of points in Euclidean spaces of two or more dimensions also have a parity, usually defined as the parity of the sum of the coordinates. For instance, the face-centered cubic lattice and its higher-dimensional generalizations, the \"D\" lattices, consist of all of the integer points whose sum of coordinates is even. This feature manifests itself in chess, where the parity of a square is indicated by its color: bishops are constrained to squares of the same parity; knights alternate parity between moves. This form of parity was famously used to solve the mutilated chessboard problem: if two opposite corner squares are removed from a chessboard, then the remaining board cannot be covered by dominoes, because each domino covers one square of each parity and there are two more squares of one parity than of the other.\n\nThe parity of an ordinal number may be defined to be even if the number is a limit ordinal, or a limit ordinal plus a finite even number, and odd otherwise.\n\nLet \"R\" be a commutative ring and let \"I\" be an ideal of \"R\" whose index is 2. Elements of the coset formula_3 may be called even, while elements of the coset formula_4 may be called odd.\nAs an example, let be the localization of Z at the prime ideal (2). Then an element of \"R\" is even or odd if and only if its numerator is so in Z.\n\nThe even numbers form an ideal in the ring of integers, but the odd numbers do not — this is clear from the fact that the identity element for addition, zero, is an element of the even numbers only. An integer is even if it is congruent to 0 modulo this ideal, in other words if it is congruent to 0 modulo 2, and odd if it is congruent to 1 modulo 2.\n\nAll prime numbers are odd, with one exception: the prime number 2. All known perfect numbers are even; it is unknown whether any odd perfect numbers exist.\n\nGoldbach's conjecture states that every even integer greater than 2 can be represented as a sum of two prime numbers.\nModern computer calculations have shown this conjecture to be true for integers up to at least 4 × 10, but still no general proof has been found.\n\nThe parity of a permutation (as defined in abstract algebra) is the parity of the number of transpositions into which the permutation can be decomposed. For example (ABC) to (BCA) is even because it can be done by swapping A and B then C and A (two transpositions). It can be shown that no permutation can be decomposed both in an even and in an odd number of transpositions. Hence the above is a suitable definition. In Rubik's Cube, Megaminx, and other twisting puzzles, the moves of the puzzle allow only even permutations of the puzzle pieces, so parity is important in understanding the configuration space of these puzzles.\n\nThe Feit–Thompson theorem states that a finite group is always solvable if its order is an odd number. This is an example of odd numbers playing a role in an advanced mathematical theorem where the method of application of the simple hypothesis of \"odd order\" is far from obvious.\n\nThe parity of a function describes how its values change when its arguments are exchanged with their negations. An even function, such as an even power of a variable, gives the same result for any argument as for its negation. An odd function, such as an odd power of a variable, gives for any argument the negation of its result when given the negation of that argument. It is possible for a function to be neither odd nor even, and for the case \"f\"(\"x\") = 0, to be both odd and even. The Taylor series of an even function contains only terms whose exponent is an even number, and the Taylor series of an odd function contains only terms whose exponent is an odd number.\n\nIn combinatorial game theory, an \"evil number\" is a number that has an even number of 1's in its binary representation, and an \"odious number\" is a number that has an odd number of 1's in its binary representation; these numbers play an important role in the strategy for the game Kayles. The parity function maps a number to the number of 1's in its binary representation, modulo 2, so its value is zero for evil numbers and one for odious numbers. The Thue–Morse sequence, an infinite sequence of 0's and 1's, has a 0 in position \"i\" when \"i\" is evil, and a 1 in that position when \"i\" is odious.\n\nIn information theory, a parity bit appended to a binary number provides the simplest form of error detecting code. If a single bit in the resulting value is changed, then it will no longer have the correct parity: changing a bit in the original number gives it a different parity than the recorded one, and changing the parity bit while not changing the number it was derived from again produces an incorrect result. In this way, all single-bit transmission errors may be reliably detected. Some more sophisticated error detecting codes are also based on the use of multiple parity bits for subsets of the bits of the original encoded value.\n\nIn wind instruments with a cylindrical bore and in effect closed at one end, such as the clarinet at the mouthpiece, the harmonics produced are odd multiples of the fundamental frequency. (With cylindrical pipes open at both ends, used for example in some organ stops such as the open diapason, the harmonics are even multiples of the same frequency for the given bore length, but this has the effect of the fundamental frequency being doubled and all multiples of this fundamental frequency being produced.) See harmonic series (music).\n\nIn some countries, house numberings are chosen so that the houses on one side of a street have even numbers and the houses on the other side have odd numbers.\nSimilarly, among United States numbered highways, even numbers primarily indicate east-west highways while odd numbers primarily indicate north-south highways. Among airline flight numbers, even numbers typically identify eastbound or northbound flights, and odd numbers typically identify westbound or southbound flights.\n"}
{"id": "2045417", "url": "https://en.wikipedia.org/wiki?curid=2045417", "title": "Petrov classification", "text": "Petrov classification\n\nIn differential geometry and theoretical physics, the Petrov classification (also known as Petrov–Pirani–Penrose classification) describes the possible algebraic symmetries of the Weyl tensor at each event in a Lorentzian manifold.\n\nIt is most often applied in studying exact solutions of Einstein's field equations, but strictly speaking the classification is a theorem in pure mathematics applying to any Lorentzian manifold, independent of any physical interpretation. The classification was found in 1954 by A. Z. Petrov and independently by Felix Pirani in 1957.\n\nWe can think of a fourth rank tensor such as the Weyl tensor, \"evaluated at some event\", as acting on the space of bivectors at that event like a linear operator acting on a vector space:\n\nThen, it is natural to consider the problem of finding eigenvalues formula_2 and eigenvectors (which are now referred to as eigenbivectors) formula_3 such that\n\nIn (four-dimensional) Lorentzian spacetimes, there is a six-dimensional space of antisymmetric bivectors at each event. However, the symmetries of the Weyl tensor imply that any eigenbivectors must belong to a four-dimensional subset.\nThus, the Weyl tensor (at a given event) can in fact have \"at most four\" linearly independent eigenbivectors.\n\nJust as in the theory of the eigenvectors of an ordinary linear operator, the eigenbivectors of the Weyl tensor can occur with various multiplicities. Just as in the case of ordinary linear operators, any multiplicities among the eigenbivectors indicates a kind of \"algebraic symmetry\" of the Weyl tensor at the given event. Just as you would expect from the theory of the eigenvalues of an ordinary linear operator on a four-dimensional vector space, the different types of Weyl tensor (at a given event) can be determined by solving a characteristic equation, in this case a quartic equation.\n\nThese eigenbivectors are associated with certain null vectors in the original spacetime, which are called the principal null directions (at a given event).\nThe relevant multilinear algebra is somewhat involved (see the citations below), but the resulting classification theorem states that there are precisely six possible types of algebraic symmetry. These are known as the Petrov types:\n\n\nThe possible transitions between Petrov types are shown in the figure, which can also be interpreted as stating that some of the Petrov types are \"more special\" than others. For example, type I, the most general type, can \"degenerate\" to types II or D, while type II can degenerate to types III, N, or D.\n\nDifferent events in a given spacetime can have different Petrov types. A Weyl tensor that has type I (at some event) is called algebraically general; otherwise, it is called algebraically special (at that event). Type O spacetimes are said to be conformally flat.\n\nThe Newman–Penrose formalism is often used in practice for the classification. Consider the following set of bivectors:\n\nThe Weyl tensor can be expressed as a combination of these bivectors through\n\nwhere the formula_9 are the Weyl scalars and c.c. is the complex conjugate. The six different Petrov types are distinguished by which of the Weyl scalars vanish. The conditions are\n\n\nGiven a metric on a Lorentzian manifold formula_16, the Weyl tensor formula_17 for this metric may be computed. If the Weyl tensor is \"algebraically special\" at some formula_18, there is a useful set of conditions, found by Lluis (or Louis) Bel and Robert Debever, for determining precisely the Petrov type at formula_19. Denoting the Weyl tensor components at formula_19 by formula_21 (assumed non-zero, i.e., not of type O), the Bel criteria may be stated as:\n\n\nwhere formula_25 is necessarily null and unique (up to scaling).\n\n\nwhere formula_25 is necessarily null and unique (up to scaling).\n\n\nwhere formula_25 is necessarily null and unique (up to scaling).\n\n\nand\n\nwhere <math> \"See chapters 4, 26\"\n"}
{"id": "42562075", "url": "https://en.wikipedia.org/wiki?curid=42562075", "title": "Plantation Act 1740", "text": "Plantation Act 1740\n\nThe Plantation Act 1740 (referring to colonies) or the Naturalization Act 1740 are common names used for an act of the British Parliament (13 Geo. 2 c.7) that was officially titled \"An Act for Naturalizing such foreign Protestants and others therein mentioned, as are settled or shall settle in any of His Majesty's Colonies in America\". The act became effective 1 June 1740 and allowed any Protestant alien residing in any of their American colonies for seven years, without being absent from that colony for more than two months, would be deemed to be one of \"his Majesty’s natural-born subjects of this Kingdom.\" The act also required making specific declarations concerning royal allegiance and succession, profession of the Christian faith, and the payment of two shillings. Compared to other alternatives available at the time, the act provided a cheap and easy method of imperial naturalization, and the length of residency was not unreasonable.\n\nReflecting the situation in Britain, the necessary profession of Christian faith was distinctly Protestant and specifically Anglican. The act was not intended for conscientious Roman Catholics, and they were referred to in the law as Papists. Exceptions however for non-conforming religious scruples and conscience were made, in the case of Quakers and Jews. Adherents of both faiths were allowed to dispense with the Sacramental test, with the former being allowed to affirm the oaths, and the latter being relieved of the obligation to repeat the words 'Upon the true faith of a Christian', at the end of the Oath of Abjuration required by the Act of Settlement since 1701.\n\nThe Plantation Act was enacted to systematize naturalization procedures in all localities as well as to encourage immigration to the American colonies. The act provided a workable naturalization procedure by empowering colonial courts to administer the oath of allegiance to aliens. The secretaries of Colonies were required, under penalty, to send annual lists of such persons to the Commissioners of Trade and Plantations. The act endowed colonial courts with the responsibility of deciding when alien petitioners had fulfilled the statutory requirements for imperial citizenship. In proprietary colonies, the judges were appointed by colonial governors and thus represented the royal or proprietary interest. However, by delegating authority to colonial officials, Parliament subjected naturalization to the pressure of the same local interest groups that increasingly defied the governor's efforts to implement royal and proprietary instructions on other issues. Despite the penalties imposed under the act, only six Secretaries of the thirteen American Colonies and one in the West Indies submitted the mandated lists.\n\nIn England during the seventeenth and eighteenth centuries, several basic statutes applied to naturalization, but these statutes would not include many aliens in the colonies who were considered an integral part of the colonial communities. Over the same time period, the American provinces (except New Hampshire) passed their own naturalization laws, which granted citizenship to people living within their province. These laws were based more on local colonial interpretations of community citizenship than on stricter considerations in the mother-country. but they gave no rights beyond their borders. This situation was initially accepted as positive and workable in England, but proved to be too inconsistent with and broader imperial intentions as time passed.\n\nBritain began withdrawing support to promote immigration to the colonies at the end of the Seven Years' War. Colonial implementation both before and after the Plantation Act tended to ignore, evade, and reshape English regulations, thus vitiating many policy decisions and generating a chaotic array of citizenship procedures. The Navigation Acts were a particular target for evasion, since they were seen as acting more as restraint on both colonial naturalization and its peoples' ability to take part fully in the economy. Colonial naturalization of aliens was outright forbidden in December 1773, under any conditions. A ban on royal land grants, initiated earlier in 1773, was made final in February 1774.\n\nThe colonial naturalization laws and the Plantation Act, during its nearly 30-year utilization would define British North America as a refuge and land of opportunity, The prohibition on naturalizing foreigners under the act was considered intolerable, and would be included, \"inter alia\", in the grievances listed in the Declaration of Independence, though some consider that surprising. Despite being a British law, the Plantation Act \"was the model upon which the first U.S. naturalization act, with respect to time, oath of allegiance, process of swearing before a judge, and the like, was clearly based.\"\n"}
{"id": "4401265", "url": "https://en.wikipedia.org/wiki?curid=4401265", "title": "Principal type", "text": "Principal type\n\nIn type theory, a type system is said to have the principal type property if, given a term and an environment, there exists a principal type for this term in this environment, i.e. a type such that all other types for this term in this environment are an instance of the principal type.\n\nThe principal type property is a desirable one for a type system, as it provides a way to type expressions in a given environment with a type which encompasses all of the expressions' possible types, instead of having several incomparable possible types. Type inference for systems with the principal type property will usually attempt to infer the principal type.\n\nFor instance, the ML system has the principal type property and principal types for an expression can be computed by Robinson's unification algorithm, which is used by the Hindley–Milner type inference algorithm. However, many extensions to the type system of ML, such as polymorphic recursion, can make the inference of the principal type undecidable. Other extensions, such as Haskell's generalized algebraic data types, destroy the principal type property of the language, requiring the use of type annotations or the compiler to \"guess\" the intended type from among several options.\n\nThe principal type property should not be confused with the principal typing property which requires that, given a term, there exist a typing (i.e. a pair with a context and a type) which is an instance of all possible typings of the term.\n"}
{"id": "3910890", "url": "https://en.wikipedia.org/wiki?curid=3910890", "title": "Projective identification", "text": "Projective identification\n\nProjective identification is a term introduced by Melanie Klein to describe the process whereby in a close relationship, as between mother and child, lovers, or therapist and patient, parts of the self may in unconscious fantasy be thought of as being forced into the other person.\n\nWhile based on Freud's concept of psychological projection, projective identification represents a step beyond. In R.D. Laing's words, \"The one person does not use the other merely as a hook to hang projections on. He/she strives to find in the other, or to induce the other to become, the very embodiment of projection\". Feelings which can not be consciously accessed are defensively projected into another person in order to evoke the thoughts or feelings projected.\n\nProjective identification may be used as a type of defense, a means of communicating, a primitive form of relationship, or a route to psychological change; used for ridding the self of unwanted parts or for controlling the other's body and mind.\n\nThough a difficult concept for the conscious mind to come to terms with, since its primitive nature makes its operation or interpretation seem more like magic or art than science, projective identification is nonetheless a powerful tool of interpersonal communication.\n\nThe recipient of the projection may suffer a loss of both identity and insight as they are caught up in and manipulated by the other person's fantasy. One therapist, for example, describes how \"I felt the progressive extrusion of his internalized mother into me, not as a theoretical construct but in actual experience. The intonation of my voice altered, became higher with the distinctly \"Ur-mutter\" quality.\" If the projection can be accepted and understood, however, much insight into the projector will be obtained.\n\nProjective identification differs from simple projection in that projective identification can become a self-fulfilling prophecy, whereby a person, believing something false about another, influences or coerces that other person to carry out that precise projection. In extreme cases, the recipient may lose any sense of their real self and become reduced to the passive carrier of outside projections, as if possessed by them.\n\nThe objects (feelings, attitudes) extruded in projective identification are of various kinds – both good and bad, ideal and abjected.\n\nHope may be projected by a client into their therapist, when they can no longer consciously feel it themselves; equally, it may be a fear of (psychic) dying which is projected.\n\nAggression may be projected, leaving the projector's personality diminished and reduced; alternatively it may be desire, leaving the projector feeling asexual.\n\nThe good/ideal parts of the personality may be projected, leading to dependence upon the object of identification; equally it may be jealousy or envy that are projected, perhaps by the therapist into the client.\n\nProjective identification may take place with varying degrees of intensity.\n\nIn narcissism, extremely powerful projections may take place and obliterate the distinction between self and other.\n\nIn less disturbed personalities, projective identification is not only a way of getting rid of feelings but also of getting help with them.\n\nIn an emotionally balanced person, projective identification may act as a bridge to empathy and intuitive understanding.\n\nVarious types of projective identification have been distinguished over the years:\n\nA division has also been made between normal projective identification and pathological projective identification, where what is projected is splintered into minute pieces before the projection takes place.\n\nAs with transference and countertransference, projective identification can be a potential key to therapeutic understanding, especially where the therapist is able to tolerate and contain the unwanted, negative aspects of the patient's self over time.\n\nTransactional analysis emphasizes the need for the therapist's Adult to remain uncontaminated, if the experience of the client's projective identification is to be usefully understood.\n\nRelationship problems have been linked to the way there can be a division of emotional labour in a couple, by way of projective identification, with one partner carrying projected aspects of the other for them. Thus one partner may carry all the aggression or all the competence in the relationship, the other all the vulnerability.\n\nJungians describe the resultant dynamics as characterising a so-called \"wounded couple\" – projective identification ensuring that each carries the most ideal or the most primitive parts of their counterpart. The two partners may initially have been singled out for that very readiness to carry parts of each other's self; but the projected inner conflicts/division then come to be replicated in the partnership itself.\n\nConscious resistance to such projective identification may produce on the one side guilt for refusing to enact the projection, on the other bitter rage at the thwarting of the projection.\n\n\n\nR. M. Young, Benign and virulent projective identification\n\nWynn Schwartz Demystifying Projective Identification\n"}
{"id": "6875096", "url": "https://en.wikipedia.org/wiki?curid=6875096", "title": "Rights of Englishmen", "text": "Rights of Englishmen\n\nThe rights of Englishmen are the perceived traditional rights of citizens of England. In the 18th century, some of the colonists who objected to British rule in the British colonies in North America argued that their traditional rights as Englishmen were being violated. The colonists wanted and expected the rights that they (or their forebears) had previously enjoyed in England: a local, representative government, with regards to judicial matters (some colonists were being sent back to England for trials) and particularly with regards to taxation. Belief in these rights subsequently became a widely-accepted justification for the American Revolution.\n\nThe American colonies had since the 17th century been fertile ground for liberalism within the center of European political discourse. However, as the ratification of the Declaration of Independence approached, the issue among the colonists of which particular rights were significant became divisive. George Mason, one of the Founding Fathers of the United States, stated that \"We claim nothing but the liberty and privileges of Englishmen in the same degree, as if we had continued among our brethren in Great Britain.\"\n\nIn the tradition of Whig history, Judge William Blackstone called them \"The absolute rights of every Englishman\", and explained how they had been established slowly over centuries of English history, in his book on \"Fundamental Laws of England\", which was the first part of his influential \"Commentaries on the Laws of England\". They were certain basic rights that all subjects of the English monarch were understood to be entitled to, such as those expressed in Magna Carta since 1215, the Petition of Right in 1628, the Habeas Corpus Act 1679 and the Bill of Rights 1689.\n\nIn a legal case in 1608 that came to be known as \"Calvin's Case\", or the \"Case of the Postnati\", the Law Lords decided in 1608 that Scotsmen born after King James I united Scotland and England (the \"postnati\") had all the rights of Englishmen. This decision would have a subsequent effect on the concept of the \"rights of Englishmen\" in British America.\n\nSome scholars believed that the case did not fit British America's situation, and thus reasoned that the 18th-century colonists could \"claim all the rights and protections of English citizenship.\" The legal apologists for the American Revolution even claimed they had \"\"improved\" on the rights of Englishmen\" by creating additional, purely American rights.\n\nOwing to its inclusion in the standard legal treatises of the 19th century, \"Calvin's Case\" was well known in the early judicial history of the United States. Consideration of the case by the United States Supreme Court and by state courts transformed it into a rule regarding American citizenship and solidified the concept of \"jus soli\" – the right by which nationality or citizenship can be recognised to any individual born in the territory of the related state – as the primary determining factor controlling the acquisition of citizenship by birth.\n\nThe Supreme Court Justice Joseph P. Bradley asserted that the \"rights of Englishmen\" were a foundation of American law in his dissenting opinion on the Slaughter-House Cases, the first Supreme Court interpretation of the Fourteenth Amendment to the United States Constitution, in 1873.\n\n\n"}
{"id": "33629240", "url": "https://en.wikipedia.org/wiki?curid=33629240", "title": "Section Nine of the Constitution of South Africa", "text": "Section Nine of the Constitution of South Africa\n\nSection Nine of the Constitution of South Africa guarantees equality before the law and freedom from discrimination to the people of South Africa. This equality right is the first right listed in the Bill of Rights. It prohibits both discrimination by the government and discrimination by private persons; however, it also allows for affirmative action to be taken to redress past unfair discrimination.\n\nUnder the heading \"Equality\", the section states:\n\n"}
{"id": "528602", "url": "https://en.wikipedia.org/wiki?curid=528602", "title": "Semigroupoid", "text": "Semigroupoid\n\nIn mathematics, a semigroupoid (also called semicategory, naked category or precategory) is a partial algebra that satisfies the axioms for a small category, except possibly for the requirement that there be an identity at each object. Semigroupoids generalise semigroups in the same way that small categories generalise monoids and groupoids generalise groups. Semigroupoids have applications in the structural theory of semigroups.\n\nFormally, a \"semigroupoid\" consists of:\n\nsuch that the following axiom holds:\n\n"}
{"id": "11722806", "url": "https://en.wikipedia.org/wiki?curid=11722806", "title": "Social procedure", "text": "Social procedure\n\nThe term social procedure is sometimes applied to any of the procedures carried out by people in various areas of society, such as legislative assemblies, judicial systems, and resource arbiters, such as banks or other lending organizations. It has been described as social software and indeed does resemble software. \n\nSocial procedures include as a subset procedures which are at the foundation of our society, such as procedural law. Similarly many social procedures are explicitly designed to ensure fair treatment to individuals or corporations, as official records of parliamentary debates show. \n\nMultiple social procedures can serve to achieve the same end. Justice, as an example, in the U.S. achieved through the use of 2 adversarial lawyers, a neutral judge, and a jury of peers. In Canada, the judge is less neutral. This subtle difference in programming has an effect on the justice outcome.\n\nThough game theory is a highly technical subject with no special attention to sports, an example of a social procedure designed to help make a sporting system more fair is the Major League Baseball Draft whereby the teams which performed the worst in the last season get the first choice of players for the new season.\n\nThere is a project called A Formal Analysis of Social Procedures underway at the Tilburg Center for Logic and Philosophy of Science (TiLPS) under the direction of E. Pacuit. This interdisciplinary project looks at social procedures undertaken by rational and not-so-rational people, and the complex phenomena arising when the people involved in such procedures interact.\n\n\n"}
{"id": "50332796", "url": "https://en.wikipedia.org/wiki?curid=50332796", "title": "Stabbing of Adele Morales by Norman Mailer", "text": "Stabbing of Adele Morales by Norman Mailer\n\nDuring a November 1960 party celebrating his mayoral candidacy, American public intellectual Norman Mailer twice stabbed his wife Adele Morales with a pen-knife in a drunken altercation, nearly taking her life. The incident, though by many accounts swept under the rug by Mailer and his associates, had a lasting impact on his public and critical legacy and persona.\n\nOn the night of November 19, 1960, Mailer and his wife Adele Morales hosted a party, intended to launch his proposed New York mayoral campaign, at the Upper West Side apartment the couple shared with their two young daughters. Mailer had enlisted his well-connected friend, journalist George Plimpton, to attract figures from the city's \"power structure\", he hoped to unite at his party this elite echelon with the \"disenfranchised\" population he saw as his natural constituency—having written of the \"courage\" of hoodlums in his 1957 essay \"The White Negro\"—into a voting base that would propel him to office.\n\nThough David Rockefeller and the Aga Khan declined the invitation, the party's approximately two hundred guests included the poet Allen Ginsberg, as well as several \"derelicts, cut-throats and bohemians\"—many of them homeless—whom Mailer had recruited off the street. This produced an atmosphere characterized by later commentators as at best, \"legendarily tetchy\" and at worst, \"the most dangerous evening I've ever spent in my life\" (from publisher Barney Rosset, a guest at the party).\n\nFights broke out throughout the course of the night and tensions ran high—by some accounts, Norman Mailer at one point divided guests \"on opposite sides of the room according to whether he considered them 'for' or 'against' him\"—with a near-incoherent Mailer eventually leaving the apartment to seek trouble elsewhere. Morales later recalled that \"he was down in the street punching people….He didn't know what his name was, he was so out of it\".\n\nWhen Mailer returned at 4:30 a.m. to find all of the guests departed but for the \"five or six\" who remained in the dining room and Morales getting ready for bed, the altercation broke out. The enraged Mailer burst into the room and Morales taunted his heterosexual masculinity and made a disparaging reference to his mistress. Mailer rushed at her, stabbing her with a rusty two-and-a-half inch penknife, once in the back and once through her breast, puncturing her cardiac sac and narrowly missing her heart. Mailer addressed the shocked guests standing over Morales' prostrate body, saying \"Don't touch her. Let the bitch die\". Morales was rushed downstairs to the apartment of novelist Doc Hume and from there in a taxi cab to University Hospital for emergency surgery.\n\nWhile she remained in critical condition, Morales initially told doctors that she \"had fallen on some glass\", denying any wrongdoing on the part of Mailer, who had come to the hospital later that night to \"lecture Adele's surgeon on the likely dimensions of her wound\". Mailer appeared the next day in a scheduled interview on \"The Mike Wallace Show\", where he spoke of the knife as a symbol of manhood and continued to plug his mayoral bid.\n\nTwo days later, in the hospital's intensive care unit, Morales admitted to police that Mailer had stabbed her; he was arrested at the hospital and involuntarily committed for seventeen days to Bellevue Hospital for psychiatric evaluation by a judge, who pronounced him \"both homicidal and suicidal\". Mailer maintained his sanity, responding \"It is very important to me not to be sent to some mental institution. I'm a sane man. If this happens, for the rest of my life, my work will be considered as the work of a man with a disordered mind\".\n\nThough Morales divorced him in 1962, she refused to press charges, citing a desire to protect their children. He was indicted by a grand jury on charges of felony assault but after pleading guilty to a reduced charge received probation and later a suspended sentence.\n\nThe reaction to the incident in the literary community to which Mailer and Morales belonged has been judged by many observers to be remarkably mild. As Mailer later noted, his friends \"closed ranks\" behind him. He remarked to \"New York Magazine\" in 1983 that \"the reactions were subtle as hell. Five degrees less warmth than I was accustomed to. Not fifteen degrees less—five.\" Many of his counterparts saw the assault as an artistic, even literary act; James Baldwin a writer and friend of Mailer's, characterized it as an attempt to free himself from \"the spiritual prison he had created with his fantasies of becoming a politician,\" \"like burning down the house in order to, at last, be free of it\". Diana Trilling later recalled being told by her husband, critic Lionel Trilling, that the stabbing was a \"Dostoyevskian ploy\" allowing Mailer to \"test the limits of evil in himself.\"\n\nThe attack was, according to many observers, entirely consistent with Mailer's public image, founded on bombastic machismo and an existentially tinged inclination toward norm-defying violence. The incident quickly became a focal point for criticism for Mailer's feminist contemporaries, particularly feminist writer Kate Millett in her 1970 work \"Sexual Politics\", who paralleled the attack with themes of sexual violence they found throughout his work. Nine years later Mailer launched a second mayoral campaign, received five percent of the votes cast and enjoyed the foundational support of prominent feminists Bella Abzug and Gloria Steinem.\n\nMailer long remained publicly blasé about the assault. In 1971 he made a dismissive remark during his appearance on \"The Dick Cavett Show\", \"We all know that I stabbed my wife many years ago. We all know that\". Mailer's admission that the stabbing was \"the one act I can look back on and regret for the rest of my life\" in a 2000 interview, forty years after the fact, marked his first public expression of remorse.\n"}
{"id": "43405497", "url": "https://en.wikipedia.org/wiki?curid=43405497", "title": "United Nations Evaluation Group", "text": "United Nations Evaluation Group\n\nThe United Nations Evaluation Group (UNEG) is a platform for the different United Nations (UN) evaluation offices to discuss evaluation issues and to share evaluation knowledge. It suggests norms and standards for all evaluation offices in the UN and delivers thematic reports concerning monitoring and evaluation. It has 45 members and 3 observers listed below.\n\nUNEG was created in January 1984 with the name ‘Inter-Agency Working Group on Evaluation’ (IAWG). It was part of the UN consultative group on programme questions (CCPOQ). It is a group of heads of UN evaluation offices which meet to discuss system wide evaluation issues. Its genesis owes much to the work of the Joint Inspection Unit (JIU) in particular Inspector Earl Sohm, JIU/REP/85/10 and JIU/REP/85/11.\n\nMuch of its initial work was on designing, testing and introducing an M&E system for UN operational activities in particular technical assistance projects. This was done by 1986. Most of UN operational activities were then funded by the UN Development Programme (UNDP) which provided the secretariat and the leadership for the Group. \n\nPolicy and Strategy evaluation were paid insufficient attention initially. {An evaluation of the IAWG in the year 2000 pointed out that it needed to return to its proactive function with specific deliverables that could be used by the UN evaluation system in their work. The recommendations of the report highlighted the need for structured interaction among UNEG members, annual work planning, work groups to move the agenda forward and developing a website to enhance knowledge sharing. In 2003, the IAWG changed its name to the United Nations Evaluation Group (UNEG).\n\nThe mission on UNEG is to promote and support the independence, credibility and usefulness of the evaluation units in the UN system. According to its Strategy 2014-2019, UNEG's work is focused on four strategic objectives:\n\n1) Evaluation functions and products of UN entities meet the UNEG Norms and Standards for evaluation;\n\n2) UN entities and partners use evaluation in support of accountability and programme learning;\n\n3) Evaluation informs UN system-wide initiatives and emerging demands; and\n\n4) UNEG benefits from and contributes to an enhanced global evaluation profession.\n\nUNEG currently has 46 members and 3 observers. Membership reserved for the units responsible for evaluation in the UN system, including the specialized agencies, funds, programmes and affiliated organizations. These units should have, or aspire to have, the required professional knowledge, experience and responsibility for evaluation as defined by the updated UNEG Norms and Standards for Evaluation (2016).\n\nMembers:\n\nObservers:\n\n\n"}
{"id": "238966", "url": "https://en.wikipedia.org/wiki?curid=238966", "title": "Warrior", "text": "Warrior\n\nA warrior is a person specializing in combat or warfare, especially within the context of a tribal or clan-based warrior culture society that recognizes a separate warrior class or caste. \n\nWarriors seem to have been present in the earliest pre-state societies. Along with hunting, war was considered to be a definitive male activity. No matter the pretext for combat, it seemed to have been a rite of passage for a boy to become a man. Warriors took upon costumes and equipment that seemed to have a symbolic significance; combat itself would be preceded by ritual or sacrifice. Men of fighting age often lived apart in order to encourage bonding, and would ritualise combat in order to demonstrate individual prowess among one another. Most of the basic weapons used by warriors appeared before the rise of most hierarchical systems. Bows and arrows, clubs, spears, and other edged weapons were in widespread use. However with the new findings of metallurgy, the aforementioned weapons had grown in effectiveness. \n\nWhen the first hierarchical systems evolved 5000 years ago, the gap between the rulers and the ruled had increased. Making war to extend the outreach of their territories, rulers often forced men from lower orders of society into the military role. This had been the first use of professional soldiers —a distinct difference from the warrior communities.\nThe warrior ethic in many societies later became the preserve of the ruling class. Egyptian pharaohs would depict themselves in war chariots, shooting at enemies, or smashing others with clubs. Fighting was considered a prestigious activity, but only when associated with status and power. European mounted knights would often feel contempt for the foot soldiers recruited from lower classes. Even in meso American societies of pre-Columbian America, the elite aristocratic soldiers remained separated from the lower classes of stone-throwers. \n\nIn contrast to the belief of the caste and clan based warrior who saw war as a place to attain valor and glory, warfare was a practical matter that could change the course of history. History always showed that men of lower orders who, provided that they were practically organized and equipped, almost always outfought warrior elites through an individualistic and humble approach to war. This was the approach of the Roman legions who had only the incentive of promotion, as well as a strict level of discipline. When Europe's standing armies of the 17th and 18th centuries developed, discipline was at the core of their training. Officers had the role of transforming men that they viewed as lower class to become reliable fighting men. \n\nInspired by the Ancient Greek ideals of the 'citizen soldier', many European societies during the Renaissance began to incorporate conscription and raise armies from the general populace. A change in attitude was noted as well, as officers were told to treat their soldiers with moderation and respect. For example, men who fought in the American Civil War often elected their own officers. With the mobilization of citizens in the armies sometimes reaching the millions, societies often made efforts in order to maintain or revive the warrior spirit. This trend continues to the modern day. \nDue to the heroic connotations of the term \"warrior\", this metaphor is especially popular in publications advocating or recruiting for a country's military.\n\nWhile the warrior class in tribal societies is typically all-male, there are some exceptions on record where women (typically unmarried, young women) formed part of the warrior class, particularly in pre-modern Japan.\n\nA purported group of fighting women is the legendary Amazons, recorded in Classical Greek mythology. Similarly, the Valkyries are depicted in Norse mythology, particularly the Icelandic Etta. During the Viking Age a type of female warrior was the \"skjaldmær\", or shieldmaiden. Hard historical evidence of non-mythological female warrior classes have been harder to come by, but some studies have been done (e.g. Birka warrior). However, groups of female warriors typically belong in folkelore and mythology, rather than in reality where there were only exceptional cases of women engaging directly in combat roles.\n\nA 2017 study led by Charlotte Hedenstierna-Jonson produced DNA results confirming the remains excavated in Birka, Sweden, were a female warrior. However, prominent historian and viking specialists, such as Judith Jesch, have disputed the findings, calling their thinking \"sloppy\" and citing issues of academic validity, including referential errors, a lack of involvement from linguistics experts, and no physical evidence that the skeleton in question actually engaged in any battle. Meanwhile, archaeologist Anna Kjellström, who worked with Hedenstierna-Jonson on the initial study, voiced her own doubts claiming it was clear the \"material and the contextual information given... did not match the data\".\n\nMany women not only fought on the field but led entire hosts of men within Pictish, Brythonic, and Irish tribes in Pre-Christian culture. Boudicca of the Iceni is a famous example of a female leader of warriors, who rebelled against Roman rule in Britain. Tomoe Gozen is celebrated in Japanese history as a woman samurai General in the 12th Century. Joan of Arc, nicknamed \"The Maid of Orléans\" is considered a heroine of France for her role during the Lancastrian phase of the Hundred Years' War. These women survive in few historical testimonies like those of the Byzantine Empire.\n\n\n"}
{"id": "273831", "url": "https://en.wikipedia.org/wiki?curid=273831", "title": "Weber–Fechner law", "text": "Weber–Fechner law\n\nThe Weber–Fechner law refers to two related laws in the field of psychophysics, known as Weber's law and Fechner's law. Both laws relate to human perception, more specifically the relation between the \"actual\" change in a physical stimulus and the \"perceived\" change. This includes stimuli to all senses: vision, hearing, taste, touch, and smell.\n\nBoth Weber's law and Fechner's law were formulated by Gustav Theodor Fechner (1801–1887). They were first published in 1860 in the work \"Elemente der Psychophysik\" (Elements of psychophysics). This publication was the first work ever in this field, and where Fechner coined the term \"psychophysics\" to describe the interdisciplinary study of how humans perceive physical magnitudes.\n\nErnst Heinrich Weber (1795–1878) was one of the first people to approach the study of the human response to a physical stimulus in a quantitative fashion. Fechner was a student of Weber and named his first law in honor of his mentor, since it was Weber who had conducted the experiments needed to formulate the law.\n\nFechner formulated several versions of the law, all stating the same thing. One formulation states:\n\n\"Simple differential sensitivity is inversely proportional to the size of the components of the difference; relative differential sensitivity remains the same regardless of size.\"\n\nWhat this means is that the perceived change in stimuli is proportional to the initial stimuli.\n\nWeber's law also incorporates the Just Noticeable Difference (JND). This is the smallest change in stimuli that can be perceived. As stated above, the JND is proportional to the initial stimuli. Fechner found that the JND is constant for any sense.\n\nAlthough Weber's law includes a statement of the proportionality of a perceived change to initial stimuli, Weber only refers to this as a rule of thumb regarding human perception. It was Fechner who formulated this statement as a mathematical expression referred to as Weber contrast. \n\nWeber contrast is not part of Weber's law.\n\nFechner noticed in his own studies that different individuals have different sensitivity to certain stimuli. For example, the ability to perceive differences in light intensity could be related to how good that individual's vision is. He also noted that the human sensitivity to stimuli changes depends on which sense is affected. He used this to formulate another version of Weber's law that he named \"the Massformel\", the \"measurement formula\". Fechner's law states that the subjective sensation is proportional to the logarithm of the stimulus intensity. According to this law, human perceptions of sight and sound work as follows: Perceived loudness/brightness is proportional to logarithm of the actual intensity measured with an accurate nonhuman instrument.\n\nThe relationship between stimulus and perception is logarithmic. This logarithmic relationship means that if a stimulus varies as a geometric progression (i.e., multiplied by a fixed factor), the corresponding perception is altered in an arithmetic progression (i.e., in additive constant amounts). For example, if a stimulus is tripled in strength (i.e., 3 x 1), the corresponding perception may be two times as strong as its original value (i.e., 1 + 1). If the stimulus is again tripled in strength (i.e., 3 x 3 x 1), the corresponding perception will be three times as strong as its original value (i.e., 1 + 1 + 1). Hence, for multiplications in stimulus strength, the strength of perception only adds. The mathematical derivations of the torques on a simple beam balance produce a description that is strictly compatible with Weber's law.\n\nFechner's law is a mathematical derivation of Weber's law.\n\nIntegrating the mathematical expression for Weber's law gives:\n\nwhere formula_6 is the constant of integration and \"ln\" is the natural logarithm.\n\nTo solve for formula_6, assume that the perceived stimuli becomes zero at some threshold stimuli formula_8. Using this as a constraint, set formula_9 and formula_10. This gives:\n\nSubstituting formula_6 in the integrated expression for Weber's law, the expression can be written as:\n\nThe constant k is sense-specific and must be determined depending on the sense and type of stimuli.\n\nWeber and Fechner conducted research on differences in light intensity and the perceived difference in weight. Other sense modalities provide only mixed support for either Weber's law or Fechner's law.\n\nWeber found that the just noticeable difference (JND) between two weights was approximately proportional to the weights. Thus, if the weight of 105 g can (only just) be distinguished from that of 100 g, the JND (or differential threshold) is 5 g. If the mass is doubled, the differential threshold also doubles to 10 g, so that 210 g can be distinguished from 200 g. In this example, a weight (any weight) seems to have to increase by 5% for someone to be able to reliably detect the increase, and this minimum required fractional increase (of 5/100 of the original weight) is referred to as the \"Weber fraction\" for detecting changes in weight. Other discrimination tasks, such as detecting changes in brightness, or in tone height (pure tone frequency), or in the length of a line shown on a screen, may have different Weber fractions, but they all obey Weber's law in that observed values need to change by at least some small but constant proportion of the current value to ensure human observers will reliably be able to detect that change.\n\nFechner did not conduct any experiments on how perceived heaviness increased with the mass of the stimulus. Instead, he assumed that all JNDs are subjectively equal, and argued mathematically that this would produce a logarithmic relation between the stimulus intensity and the sensation. These assumptions have both been questioned. \nFollowing the work of S. S. Stevens, many researchers came to believe in the 1960s that the power law was a more general psychophysical principle than Fechner's logarithmic law. But in 1963 Donald Mackay showed and in 1978 John Staddon demonstrated with Stevens' own data, that the power law is the result of logarithmic input and output processes.\n\nWeber's law does not quite hold for loudness. It is a fair approximation for higher intensities, but not for lower amplitudes.\n\nWeber's law does not hold at perception of higher intensities. Intensity discrimination improves at higher intensities. The first demonstration of the phenomena was presented by Riesz in 1928, in Physical Review. This deviation of the Weber's law is known as the \"near miss\" of the Weber's law. This term was coined by McGill and Goldberg in their paper of 1968 in Perception & Psychophysics. Their study consisted of intensity discrimination in pure tones. Further studies have shown that the near miss is observed in noise stimuli as well. Jesteadt et al. (1977) demonstrated that the near miss holds across all the frequencies, and that the intensity discrimination is not a function of frequency, and that the change in discrimination with level can be represented by a single function across all frequencies.\n\nThe eye senses brightness approximately logarithmically over a moderate range (but more like a power law over a wider range), and stellar magnitude is measured on a logarithmic scale.\nThis magnitude scale was invented by the ancient Greek astronomer Hipparchus in about 150 B.C. He ranked the stars he could see in terms of their brightness, with 1 representing the brightest down to 6 representing the faintest, though now the scale has been extended beyond these limits; an increase in 5 magnitudes corresponds to a decrease in brightness by a factor of 100.\nModern researchers have attempted to incorporate such perceptual effects into mathematical models of vision.\n\nPerception of Glass patterns and mirror symmetries in the presence of noise follows Weber's law in the middle range of regularity-to-noise ratios (\"S\"), but in both outer ranges, sensitivity to variations is disproportionally lower. As Maloney, Mitchison, & Barlow (1987) showed for Glass patterns, and as van der Helm (2010) showed for mirror symmetries, perception of these visual regularities in the whole range of regularity-to-noise ratios follows the law \"p\" = \"g\"/(2+1/\"S\") with parameter \"g\" to be estimated using experimental data.\n\nActivation of neurons by sensory stimuli in many parts of the brain is by a proportional law: neurons change their spike rate by about 10–30%, when a stimulus (e.g. a natural scene for vision) has been applied. However, as Scheler (2017) showed,\nthe population distribution of the intrinsic excitability or gain of a neuron is a heavy tail distribution, more precisely a lognormal shape, which is equivalent to a logarithmic coding scheme. Neurons may therefore spike with 5–10 fold different mean rates. Obviously, this increases the dynamic range of a neuronal population, while stimulus-derived changes remain small and linear proportional.\n\nThe Weber–Fechner law has been applied in other fields of research than just the human senses.\n\nPsychological studies show that it becomes increasingly difficult to discriminate between two numbers as the difference between them decreases. This is called the \"distance effect\". This is important in areas of magnitude estimation, such as dealing with large scales and estimating distances. It may also play a role in explaining why consumers neglect to shop around to save a small percentage on a large purchase, but will shop around to save a large percentage on a small purchase which represents a much smaller absolute dollar amount.\n\nIt has been hypothesized that dose–response relationships can follow Weber's Law which suggests this law – which is often applied at the sensory level – originates from underlying chemoreceptor responses to cellular signaling dose relationships within the body. Dose response can be related to the Hill equation, which is closer to a power law.\n\nThere is a new branch of the literature on public finance hypothesizing that the Weber–Fechner law can explain the increasing levels of public expenditures in mature democracies. Election after election, voters demand more public goods to be effectively impressed; therefore, politicians try to increase the magnitude of this \"signal\" of competence – the size and composition of public expenditures – in order to collect more votes.\n\n\n\n"}
{"id": "44974523", "url": "https://en.wikipedia.org/wiki?curid=44974523", "title": "Women's police station", "text": "Women's police station\n\nWomen's police stations (also units or offices) – , – are police stations specializing in crimes with female victims. They were first introduced in 1985 in Brazil and are numerous in Latin America. According to \"Latin American Perspectives\", the first women's police station was opened in Sao Paulo, Brazil and \"In the first six months of operation, the DDM processed 2,083 reports.\"\n\nOfficers at these stations are only allowed to respond to certain crimes, such as psychological violence, domestic violence, family violence, as well as specific types of threats and sexual violence. Some units offer financial help, counseling, and medical care for women who are having trouble.\n\nIn India, a study found \"the establishment of 188 women's police stations resulted in a 23 percent increase in reporting of crimes against women and children and a higher conviction rate between 2002 and 2004\".\n\nWomen's police stations are located in mostly Latin American countries where rates of rape and violence against women are high. Americasquarterly.org states, \"Femicide—the killing of women—has reached alarming levels in Latin America. The most recent region-wide statistics available, from 2003, show that seven Latin American countries score among the worst 10 nations when measuring the rate of femicide per one million women in 40 countries.\" Women's police stations are also in Ghana, India, Pakistan, Kosovo, Liberia, Nicaragua, Peru, Sierra Leone, South Africa, Uganda and Uruguay. A policewoman at a station in Pakistan states, \"Even if a woman is being beaten and psychologically tortured, she's told to consider her husband's honor and not go to the police station.\" Some women in Latin America do not even know their rights, Endvawow.org states, \"Only in Brazil had a majority of women surveyed receiving training or information about their rights one or more times (by any source): 54% in Brazil, 42% in Nicaragua, 34% in Peru, and 23% in Ecuador.\" According to Hautzinger in her article \"Criminalising Male Violence in Brazil's Women's Police Stations\", in Salvador, Brazil in regular police stations in spousal violence cases less than 2% actually went to court and the punishments the men did get were very minor. Endvawnow states that women police stations are an important first step for crimes to enter the justice system.\n\nWomen's police stations have greatly expanded since 1985. Endvawnow.org states, \"In 2010, there were 475 WPS in Brazil, 34 in Ecuador, 59 in Nicaragua, and 27 in Peru.\" In Santos' article \"EN-GENDERING THE POLICE\" states, \"They [women's police stations] expanded victims' citizenship rights, allowing them to denounce a violence that not long ago was invisible and considered a private matter. In 2000, for example, 310,058 complaints of violence against women were registered in the women's police stations of Sao Paulo.\" Language barriers and the inability to get to a station is still a problem. According to Endvawnow.org, women's police stations are located in more populated areas making it hard for women in rural areas to get to them and women who do not speak the same language as the policewomen can not communicate effectively. Endvawnow.org also states \"It was also found that poor and less educated women are sometimes ignored in the WPS. Also, despite psychological violence being illegal in all four countries, operators frequently prioritise those cases in which women have severe visible physical injuries, and may resist accepting complaints of psychological violence.\"\n\n"}
