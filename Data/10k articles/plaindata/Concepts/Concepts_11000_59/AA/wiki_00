{"id": "52715832", "url": "https://en.wikipedia.org/wiki?curid=52715832", "title": "Anglophone problem (Cameroon)", "text": "Anglophone problem (Cameroon)\n\nThe Anglophone Problem, as it is commonly referred to in Cameroon, is a socio-political issue rooted in Cameroon's colonial legacies from the Germans, British, and the French.\n\nThe issue classically and principally opposes many Cameroonians from the Northwest and Southwest regions, many of whom consider themselves anglophones, to the Cameroon government. This is based on the fact that these two regions (formally British Southern Cameroons) were controlled by Britain as a mandated and trust territory of the League of Nations and the United Nations respectively.\" \n\nWhile many Northwesterners and Southwesterners believe there is an anglophone problem, some do not. In fact, the term \"anglophone\" today creates a lot of controversy, as many former French-speaking Cameroonians who are either bilingual or speak only English (most of whom have gone through the English sub-system of education) consider themselves as anglophones. The root of the Anglophone problem in Cameroon can be traced back to the Foumban Conference of 1961 that united the two territories, with different colonial legacies, into one state. The Anglophone Problem is increasingly dominating the political agenda of Cameroon. This problem has led to arguments and actions (protests, strikes, etc.) that argue for federalism or separation from the union by the Anglophones. Failure to address the Anglophone Problem threatens Cameroon's ability to create national unity between the two groups of people.\n\nThe roots of the Anglophone problem can be traced back to World War I, when Cameroon was known as German Kamerun. The German Empire first gained influence in Cameroon in 1845 when Alfred Saker of the Baptist Missionary Society introduced a mission station. In 1860, German merchants established a factory: the Woermann Company. On July 5, 1884, local tribes provided the Woermann Company with rights to control the Kamerun River, consequently setting the foundation for the later German colonization of Kamerun. In 1916, during World War I, France and Britain joined forces to attack and seize German Kamerun. Later, the Treaty of Versailles would award France and Britain mandates over Cameroon as punishment of the Germans who lost the war. Most of German Kamerun was given to the French, over 167,000 square miles of territory. The British were given Northern Cameroons, about 17,500 square miles of territory and Southern Cameroons, 16,580 square miles. Each colonizer would later influence the colonies with their European languages and cultures, thus rendering them as Anglophones and Francophones. The large difference in awarded territory has resulted in present-day Cameroon having a huge majority Francophone population and a very small minority Anglophone population.\n\nFollowing World War II, a wave of independence flowed rapidly throughout Africa. The United Nations obliged that Britain and France relinquish their colonies and guide them towards independence. There were three political options for British Southern Cameroons. They could become independent by uniting with Nigeria or with French Cameroun. No option of self-determination by becoming independent was given. The most desired option was independence with the least popular being unification with French Cameroun. However, during the British Plebiscite of 1961, the British argued that Southern Cameroons was not economically viable enough to sustain itself as an independent nation and could only survive by joining with Nigeria or La République du Cameroun (the Republic of Cameroon). Though documents on the United Nations' \"Non-Self-Governing Territories\" state, \"integration should be the result of the freely expressed wishes of the territory's peoples\", the United Nations would later reject Southern Cameroons' appeal to have independence as a sovereign nation placed on the ballot. The plebiscite questions were: \nThe United Nations documents defined the basis of integration as: \"Integration with an independent State should be on the basis of complete equality between the peoples of the erstwhile Non-Self-Governing Territory and those of the independent country with which it is integrated. The peoples of both territories should have equal status and rights of citizenship... at all levels in the executive, legislative and judicial organs of government.\" With this promise in mind, on February 1961, British Northern Cameroons voted to join Nigeria, while British Southern Cameroons voted to join La République du Cameroun.\n\nThe purpose of the Foumban Constitutional Conference was to create a constitution for the new Federal state of British Southern Cameroon and La République du Cameroun. The conference brought together representatives from La République du Cameroun, including Amadou Ahidjo, their president, with representatives from Southern Cameroons. Two weeks before the Foumban Conference, there were reports that more than one hundred people were killed by terrorists in Loum, Bafang, Ndom, and Douala. The reports worried unification advocates who wanted British Cameroon to unify with French Cameroun. For the conference, the location of Foumban had been carefully chosen to make Ahidjo, appear as if he had everything under control. Mr. Mbile, a Southern Cameroonian representative at the conference noted, \"Free from all the unrest that had scared Southern Cameroonians, the Francophone authorities had picked the place deliberately for the occasion. The entire town had been exquisitely cleaned up and houses splashed with whitewash. Food was good and receptions lavish. The climate in Foumban real or artificial went far to convince us that despite the stories of 'murder and fire,' there could be at least this island of peace, east of the Mungo.\"\n\nBefore the Foumban Conference, all the parties in Southern Cameroons, the Native Authority Councils and the traditional leaders attended the Bamenda Conference. This conference decided on a common proposal to present when negotiations with La République du Cameroun arrived. Among many things, the Bamenda Conference agreed on a non-centralized federation to ensure there was a distinction between the powers of the states and the powers of the federation. Most of the proposals from the Bamenda Conference were ignored by Ahidjo. Some of these proposals included having a bicameral legislature and decentralizing power, but instead a unicameral system was established with a centralized system of power.\n\nAt the Foumban conference, Ahidjo presented delegates with a draft constitution. By the end of the conference, instead of creating an entirely new constitution, the contributions of the Southern Cameroons delegates were reflected in suggestions made to the draft initially presented to them. John Ngu Foncha and Ahidjo intended for the Foumban Constitutional Conference to be brief, however delegates left the three day conference with the impression that there would be sequential conferences to continue the drafting of the constitution. Mbile later noted, \"We may have done more if we had spent five months instead of five days in writing our constitution at Foumban.\" The Constitution for the new Federal Republic was agreed in Yaoundé in August 1961, between Ahidjo and Foncha, pending approval by the House of Assembly of the two states. In the end, the West Cameroon House of Assembly never ratified the Constitution. However, on October 1, 1961, the Federal Republic of Cameroon nevertheless came to fruition.\n\nOn May 6, 1972, Ahidjo announces his decision to convert the Federal Republic into a unitary state, on the provision that the idea was supported via referendum. This suggestion violated the articles in the Foumban document that read: 'any proposal for the revision of the present constitution, which impairs the unity and integrity of the Federation shall be inadmissible,' and 'proposals for revision shall be adopted by simple majority vote of the members of the Federal Assembly, provided that such majority includes a majority of the representatives ... of each of the Federated States,'... not through referendum. Such violations easily allowed for the passing of the referendum that turned the Federal Republic into the United Republic of Cameroon. Taking into account these actions, the evidence shows that the Francophone's intentions may have not been to form a federal state, but rather to annex Southern Cameroons and not treat them as equals. In 1984, Ahidjo's successor, Paul Biya, replaced the name \"United Republic of Cameroon\" with \"La République du Cameroun,\" the same name the francophone Cameroon had before federation talks. With changes in the Constitution of 1996, reference to the existence of a territory called the British Southern Cameroons that had a \"functioning self-government and recognized international boundaries\" was essentially erased.\n\nDespite the non-acknowledgement/denial of the Anglophone problem from Francophone government leaders, there exists a discontent by Anglophones, both young and old, as to how Anglophones are treated. This discontent presents itself in calls for federation or separation with movements that are garnering strength. At the core of Anglophone grievances is the loss of the former West Cameroons as a \"distinct community defined by differences in official language and inherited colonial traditions of education, law, and public administration.\" On 22 December 2016, in a letter to Paul Biya, the Anglophone Archbishops of Southern Cameroons define the Anglophone problem as follows:\n\nMovements which advocate the separation of English-speaking Cameroon from French-speaking Cameroun exist, led by the Cameroon Action Group, the Southern Cameroons Youth League, the Southern Cameroons National Council, the Southern Cameroon Peoples Organization and the Ambazonia Movement.\n\nAdvocates of Federation want a return to the constitution agreed upon in the 1961 Foumban Conference that acknowledges the history and culture of the two regions while giving equal power to the two. This federation had been dismantled on 20 May 1972 by the larger French-speaking Cameroon and extended the latter's executive power throughout West Cameroon. Federation advocates include the instrumental Consortium of the leaders of three Cameroon-based trade unions: Lawyers, Teachers, and Transporters. It also includes some Cameroonians in the diaspora led by a well organized US-based Anglophone Action Group, Inc. (AAG). AAG was one of the first groups in the diaspora to endorse the Cameroon-based Consortium as a peaceful alternative to achieving a return to the pre-1972 federated system. Opponents of federation include the ruling Cameroon Peoples Democratic Movement.\n\nUnitarianism do not want Federation or Separation, but rather a decentralized unitary government; whereas, now the government is highly centralized in power. This violates the tenets of the 1996 Constitution as decentralization has yet to be implemented.\n\nIn March 1990, the Social Democratic Front (SDF) led by John Fru Ndi, was founded on the perception of widespread Anglophone alienation. The SDF was the first major opposition party to the People's Democratic Movement, led by Paul Biya.\n\nBelow are various reasons that Anglophones feel marginalized, systemically, by the government. \n\n\n, the Anglophone problem is still on-going. It has spiraled into violence with police officers and gendarmes shooting dead several civilians. Official sources have put the number at 17 dead, but local individuals and groups have talked of 50 or more. Radical members of some secessionist groups have killed several police officers and gendarmes. 15,000 refugees have fled Southern Cameroons into neighboring Nigeria, with the UNHCR expecting that number to grow to 40,000 if the situation continues.\n\nWithout clearly acknowledging the existence of the Anglophone problem, the President of Cameroon has attempted to appease tensions by making a number of announcements:\nSeveral separatist or secessionist groups have emerged or become more prominent as a result of the harsh response by the government to the Anglophone problem. These groups desire to see Southern Cameroons completely separate from \"La République du Cameroun\" and form its own state, sometimes referred to as \"Ambazonia\". Some groups such as the \"Southern Cameroon Ambazonia United Front\" (SCACUF) are using diplomatic means in an attempt to gain independence for the Anglophone regions, whereas other groups have begun to employ armed confrontation with artisan weapons against the deployed gendarmes and soldiers in those regions.\n\n"}
{"id": "6055770", "url": "https://en.wikipedia.org/wiki?curid=6055770", "title": "Basic hostility", "text": "Basic hostility\n\nBasic hostility is a psychological concept first described by psychoanalyst Karen Horney. Horney described it as a bad attitude which child develops as a result of \"basic evil\". Horney generally defines \"basic evil\" as \"invariably the lack of genuine warmth and affection\". Basic evil includes all range of inappropriate parental behavior – from lack of affection to abuse\n\n\nAccording to Horney, some children find Basic Hostility to be an aggressive coping strategy and continue using it to deal with life's problems.\n\n\n"}
{"id": "514534", "url": "https://en.wikipedia.org/wiki?curid=514534", "title": "Canonical transformation", "text": "Canonical transformation\n\nIn Hamiltonian mechanics, a canonical transformation is a change of canonical coordinates that preserves the form of Hamilton's equations. This is sometimes known as form invariance. It need not preserve the form of the Hamiltonian itself. Canonical transformations are useful in their own right, and also form the basis for the Hamilton–Jacobi equations (a useful method for calculating conserved quantities) and Liouville's theorem (itself the basis for classical statistical mechanics).\n\nSince Lagrangian mechanics is based on generalized coordinates, transformations of the coordinates do not affect the form of Lagrange's equations and, hence, do not affect the form of Hamilton's equations if we simultaneously change the momentum by a Legendre transformation into\n\nTherefore, coordinate transformations (also called point transformations) are a \"type\" of canonical transformation. However, the class of canonical transformations is much broader, since the old generalized coordinates, momenta and even time may be combined to form the new generalized coordinates and momenta. Canonical transformations that do not include the time explicitly are called restricted canonical transformations (many textbooks consider only this type).\n\nFor clarity, we restrict the presentation here to calculus and classical mechanics. Readers familiar with more advanced mathematics such as cotangent bundles, exterior derivatives and symplectic manifolds should read the related symplectomorphism article. (Canonical transformations are a special case of a symplectomorphism.) However, a brief introduction to the modern mathematical description is included at the end of this article.\n\nBoldface variables such as represent a list of generalized coordinates that need not transform like a vector under rotation, e.g.,\n\nA dot over a variable or list signifies the time derivative, e.g.,\n\nThe dot product notation between two lists of the same number of coordinates is a shorthand for the sum of the products of corresponding components, e.g.,\n\nThe dot product (also known as an \"inner product\") maps the two coordinate lists into one variable representing a single numerical value.\n\nThe functional form of Hamilton's equations is\n\nBy definition, the transformed coordinates have analogous dynamics\n\nwhere is a new Hamiltonian (sometimes called the Kamiltonian) that must be determined.\n\nIn general, a transformation does not preserve the form of Hamilton's equations. For time independent transformations between and we may check if the transformation is restricted canonical, as follows. Since restricted transformations have no explicit time dependence (by definition), the time derivative of a new generalized coordinate is\n\nwhere is the Poisson bracket.\n\nWe also have the identity for the conjugate momentum \"P\"\n\nIf the transformation is canonical, these two must be equal, resulting in the equations\n\nThe analogous argument for the generalized momenta \"P\" leads to two other sets of equations\n\nThese are the direct conditions to check whether a given transformation is canonical.\n\nThe direct conditions allow us to prove Liouville's theorem, which states that the \"volume\" in phase space is conserved under canonical transformations, i.e.,\n\nBy calculus, the latter integral must equal the former times the Jacobian \n\nwhere the Jacobian is the determinant of the matrix of partial derivatives, which we write as\n\nExploiting the \"division\" property of Jacobians yields\n\nEliminating the repeated variables gives\n\nApplication of the direct conditions above yields .\n\nTo \"guarantee\" a valid transformation between and , we may resort to an indirect generating function approach. Both sets of variables must obey Hamilton's principle. That is the Action Integral over the Lagrangian formula_16 and formula_17 respectively, obtained by the Hamiltonian via (\"inverse\") Legendre transformation, both must be stationary (so that one can use the Euler–Lagrange equations to arrive at equations of the above-mentioned and designated form; as it is shown for example here):\n\nOne way for both variational integral equalities to be satisfied is to have\n\nLagrangians are not unique: one can always multiply by a constant and add a total time derivative and yield the same equations of motion (see for reference: ).\n\nIn general, the scaling factor is set equal to one; canonical transformations for which are called extended canonical transformations. is kept, otherwise the problem would be rendered trivial and there would be not much freedom for the new canonical variables to differ from the old ones.\n\nHere is a generating function of one old canonical coordinate ( or ), one new canonical coordinate ( or ) and (possibly) the time . Thus, there are four basic types of generating functions (although it should be noted that mixtures of these four types can exist), depending on the choice of variables. As will be shown below, the generating function will define a transformation from old to new canonical coordinates, and any such transformation is guaranteed to be canonical.\n\nThe type 1 generating function depends only on the old and new generalized coordinates\nTo derive the implicit transformation, we expand the defining equation above\n\nSince the new and old coordinates are each independent, the following equations must hold\n\nThese equations define the transformation as follows. The \"first\" set of equations\n\ndefine relations between the new generalized coordinates and the old canonical coordinates . Ideally, one can invert these relations to obtain formulae for each as a function of the old canonical coordinates. Substitution of these formulae for the coordinates into the \"second\" set of equations\n\nyields analogous formulae for the new generalized momenta in terms of the old canonical coordinates . We then invert both sets of formulae to obtain the \"old\" canonical coordinates as functions of the \"new\" canonical coordinates . Substitution of the inverted formulae into the final equation \nyields a formula for as a function of the new canonical coordinates .\n\nIn practice, this procedure is easier than it sounds, because the generating function is usually simple. For example, let \nThis results in swapping the generalized coordinates for the momenta and vice versa \nand . This example illustrates how independent the coordinates and momenta are in the Hamiltonian formulation; they are equivalent variables.\n\nThe type 2 generating function depends only on the old generalized coordinates and the new generalized momenta\nwhere the formula_29 terms represent a Legendre transformation to change the right-hand side of the equation below. To derive the implicit transformation, we expand the defining equation above\n\nSince the old coordinates and new momenta are each independent, the following equations must hold\n\nThese equations define the transformation as follows. The \"first\" set of equations\n\ndefine relations between the new generalized momenta and the old canonical coordinates . Ideally, one can invert these relations to obtain formulae for each as a function of the old canonical coordinates. Substitution of these formulae for the coordinates into the \"second\" set of equations\n\nyields analogous formulae for the new generalized coordinates in terms of the old canonical coordinates . We then invert both sets of formulae to obtain the \"old\" canonical coordinates as functions of the \"new\" canonical coordinates . Substitution of the inverted formulae into the final equation \nyields a formula for as a function of the new canonical coordinates .\n\nIn practice, this procedure is easier than it sounds, because the generating function is usually simple. For example, let \nwhere is a set of functions. This results in a point transformation of the generalized coordinates\n\nThe type 3 generating function depends only on the old generalized momenta and the new generalized coordinates \nwhere the formula_38 terms represent a Legendre transformation to change the left-hand side of the equation below. To derive the implicit transformation, we expand the defining equation above\n\nSince the new and old coordinates are each independent, the following equations must hold\n\nThese equations define the transformation as follows. The \"first\" set of equations\n\ndefine relations between the new generalized coordinates and the old canonical coordinates . Ideally, one can invert these relations to obtain formulae for each as a function of the old canonical coordinates. Substitution of these formulae for the coordinates into the \"second\" set of equations\n\nyields analogous formulae for the new generalized momenta in terms of the old canonical coordinates . We then invert both sets of formulae to obtain the \"old\" canonical coordinates as functions of the \"new\" canonical coordinates . Substitution of the inverted formulae into the final equation \nyields a formula for as a function of the new canonical coordinates .\n\nIn practice, this procedure is easier than it sounds, because the generating function is usually simple.\n\nThe type 4 generating function formula_44 depends only on the old and new generalized momenta\nwhere the formula_46 terms represent a Legendre transformation to change both sides of the equation below. To derive the implicit transformation, we expand the defining equation above\n\nSince the new and old coordinates are each independent, the following equations must hold\n\nThese equations define the transformation as follows. The \"first\" set of equations\n\ndefine relations between the new generalized momenta and the old canonical coordinates . Ideally, one can invert these relations to obtain formulae for each as a function of the old canonical coordinates. Substitution of these formulae for the coordinates into the \"second\" set of equations\n\nyields analogous formulae for the new generalized coordinates in terms of the old canonical coordinates . We then invert both sets of formulae to obtain the \"old\" canonical coordinates as functions of the \"new\" canonical coordinates . Substitution of the inverted formulae into the final equation \nyields a formula for as a function of the new canonical coordinates .\n\nMotion itself (or, equivalently, a shift in the time origin) is a canonical transformation. If formula_52 and formula_53, then Hamilton's principle is automatically satisfied\n\nsince a valid trajectory formula_55 should always satisfy Hamilton's principle, regardless of the endpoints.\n\nIn mathematical terms, canonical coordinates are any coordinates on the phase space (cotangent bundle) of the system that allow the canonical one-form to be written as\n\nup to a total differential (exact form). The change of variable between one set of canonical coordinates and another is a canonical transformation. The index of the generalized coordinates is written here as a \"superscript\" (formula_57), not as a \"subscript\" as done above (formula_58). The superscript conveys the contravariant transformation properties of the generalized coordinates, and does \"not\" mean that the coordinate is being raised to a power. Further details may be found at the symplectomorphism article.\n\nThe first major application of the canonical transformation was in 1846, by Charles Delaunay, in the study of the Earth-Moon-Sun system. This work resulted in the publication of a pair of large volumes as \"Mémoires\" by the French Academy of Sciences, in 1860 and 1867.\n\n\n"}
{"id": "5825131", "url": "https://en.wikipedia.org/wiki?curid=5825131", "title": "Centre for Humanitarian Dialogue", "text": "Centre for Humanitarian Dialogue\n\nThe Centre for Humanitarian Dialogue, otherwise known as the Henry Dunant Centre for Humanitarian Dialogue, or HD, is a private diplomacy organisation based in Switzerland that assists in mediation between conflicting parties to prevent or end armed conflicts.\nFounded in 1999, the aim of the organisation is to promote and facilitate dialogue among the leadership of the main belligerents.\n\nIt also conducts research and analysis on mediation and peacemaking in support of its operational work to improve international efforts to secure and sustain peace. To do so, HD opens channels of communication and mediates between parties in conflict, as well as facilitates dialogue and provides support to the broader mediation and peacebuilding community. HD will facilitate dialogue in both confidential settings as well as public ones.\n\nIt is headquartered in Geneva, which is also the location of its Middle East and North Africa programme. HD has regional offices in Africa and Asia.\n\nThe initial intention of HD was to explore new concepts of humanitarian innovation by serving as a venue for dialogue on humanitarian issues - where discreet discussions could take place among those who could have a practical impact on humanitarian policy and practice.\nThe organisation evolved this approach to include negotiations in support of humanitarian objectives and aimed to create space for humanitarian activities in conflict environments. It quickly broadened, at the behest of conflicting parties in Aceh, to include the resolution of the conflict through mediation and conflict prevention.\n\nIn July 2015, in recognition of its important role in the mediation of armed conflicts, HD was granted a special status by the Federal Council of Switzerland. Through this status, HD was awarded certain privileges and immunities \nintended to enable its peacemaking efforts worldwide.\n\nThe Founding Executive Director of HD was Martin Griffiths. He led the organization for more than 10 years, from 1999 to July 2010, when he stepped down from his position. Griffiths was replaced for a brief period (July 2010 to March 2011) by Angelo Gnaedinger, former Director General of the ICRC and the then HD Regional Director for the Middle East. Following this transitional period, David Harland was appointed as HD's new Executive Director in April 2011.\n\nHD aims to bring parties together through mediation and dialogue on issues of common concern in conflict zones.\n\nMore specifically, this includes the following activities:\n\n\nHD has been involved in peacemaking activities in the Philippines, Sudan, Syria, Tunisia, Kenya, Libya, the Central African Republic, Nigeria, Senegal, Liberia, Somalia, Mali, Indonesia (Aceh), Timor Leste, Burundi, Nepal, and Ukraine, as well as in a large number of confidential operational projects.\n\nIn Tunisia, HD supported an informal dialogue process between the country’s main political parties which culminated in July 2014 with the signing of a Charter of Honour on the fair, transparent and democratic conduct of elections.\n\nIn Libya, HD helped drafting the Humanitarian Appeal for Benghazi, a document signed by 76 “sons and daughters of Benghazi” including tribal leaders, parliamentarians, lawyers, judges and former military commanders who commit themselves and call upon others to improve the access for humanitarian workers, to respect international humanitarian law and establish a system of transitional justice for their hometown.\n\nIn the Philippines, HD is a member of the International Contact Group which has been supporting the Government and the Moro Islamic Liberation Front in a peace process which aimed to end decades of conflict in the southern Philippines. This process culminated in signing of historic agreements, the Framework Agreement on the Bangsamoro in 2012 and the Comprehensive Agreement on the Bangsamoro in 2014.\n\nIn Kenya, HD supported a peace process between local communities in the Rift Valley of Kenya. This area had been the epicentre of post-election violence in 2007-2008. The process culminated in the signing of a peace agreement, the Nakuru County Peace Accord in August 2012.\n\nIn northern Mali, HD facilitated and moderated the meeting which led to the signing of the Ouagadagou Declaration by the six political and military movements of Azawad on August 28, 2014. The purpose of the declaration was to put an end to hostilities in northern Mali and to establish a political and legal status for Azawad.\n\nIn Nigeria, HD supported an inter-communal dialogue process with five local government areas (LGAs) from southern Kaduna State, which culminated in the Kafanchan Peace Declaration in March 2016. It is a commitment to non-violent conflict resolution by communities from five local government areas (LGAs) in southern Kaduna State.\n\nHD seeks to improve the practice of mediation and strengthen capacity within the community of mediators.\n\nSupport activities include:\nHD collaborates with the United Nations, regional organisations such as ECOWAS, governments, and civil society.\nHD has launched a series of publications including the Mediation Practice Series, which seeks to provide mediation practitioners with insight into how challenges have been addressed by others in order to help them prepare for the demands of mediation. The Oslo Forum Papers aim to advance thinking and debate on issues linked to armed conflict mediation and international peacemaking.\n\nHD seeks to promote the sharing of experiences within the community of mediators and peacemakers at The Oslo Forum. Launched in 2003, the Oslo Forum is an initiative led by the Norwegian Ministry of Foreign Affairs and HD to improve practice in conflict mediation, and to enhance mediation as a profession. The Oslo Forum features an annual global event in Oslo, as well as regional forums in Africa and Asia.\n\nThe organisation was founded to pursue Henry Dunant’s vision of a world more humane. The aim, through mediation and dialogue, is to reduce the suffering caused by armed conflict in the world – where possible, to prevent such conflict; otherwise to help resolve it; or to mitigate its consequences.\n\nHD embraces a set of values that foster integrity, professionalism and respect for diversity in all areas of its work. It subscribes to the core humanitarian principles of humanity, neutrality, impartiality and operational independence and are committed to respecting international principles in relation to human rights and humanitarian affairs.\n\nHD receives a combination of project earmarked contributions and unearmarked grants from approximately 25 different governments and multilateral institutions as well as a small number of foundations and private philanthropists. In 2015, HD's annual income was 27 million Swiss Francs.\n\n"}
{"id": "897134", "url": "https://en.wikipedia.org/wiki?curid=897134", "title": "Cheating", "text": "Cheating\n\nCheating is the receiving of a reward for ability or finding an easy way out of an unpleasant situation by dishonest means. It is generally used for the breaking of rules to gain unfair advantage in a competitive situation. This broad definition will necessarily include acts of bribery, cronyism, nepotism, sleaze and any situation where individuals are given preference using inappropriate criteria. The rules infringed may be explicit, or they may be from an unwritten code of conduct based on morality, ethics or custom, making the identification of cheating conduct a potentially subjective process. Cheating can refer specifically to infidelity. Someone who is known for cheating is referred to as a \"cheat\" in British English, and a \"cheater\" in American English. A person described as a \"cheat\" doesn't necessarily cheat all the time, but rather, relies on deceiving tactics to the point of acquiring a reputation for it.\n\nAcademic cheating is a significantly common occurrence in high schools and colleges in the United States. Statistically, 64% of public high school students admit to serious test cheating. 58% say they have plagiarized. 95% of students admit to some form of cheating. This includes tests, copying homework, and papers. Only 50% of private school students, however, admit to this. The report was made in June 2005 by Rutgers University professor Donald McCabe for The Center for Academic Integrity. The findings were corroborated in part by a Gallup survey. In McCabe's 2001 of 4500 high school students, \"74% said they cheated on a test, 72% cheated on a written work, and 97% reported to at least had copied someone's homework or peeked at someone's test. 1/3 reported to have repeatedly cheated.\" The new revolution in high-tech digital info contributes enormously to the new wave in cheating: online term-paper mills sell formatted reports on practically any topic; services exist to prepare any kind of homework or take online tests for students, despite the fact that this phenomenon, and these websites, are well known to educators ; MP3 players can hold digitalized notes; graphing calculators store formulas to solve math problems.\n\nThe Chinese civil service examinations, the main route to career success for literate men in imperial China, was bedeviled for centuries by rampant cheating and examiner-bribery, as detailed in books like the Ming-dynasty \"Book of Swindles\".\n\nCheating in sports is the intentional breaking of rules in order to obtain an advantage over the other teams or players. Sports are governed by both customs and explicit rules regarding acts which are permitted and forbidden at the event and away from it. Forbidden acts frequently include performance-enhancing drug taking (known as \"doping\"), using equipment that does not conform to the rules or altering the condition of equipment during play, and deliberate harassment or injury to competitors.\n\nHigh-profile examples of alleged doping cheating include Lance Armstrong's use of steroids in professional cycling - particularly controversial as it is widely suspected that a high percentage of professional cyclists are using prohibited substances - Ben Johnson's disqualification following the 100 metres final at the 1988 Summer Olympics, and admissions of steroid use by former professional baseball players after they have retired, such as José Canseco and Ken Caminiti. A famous sporting scandal involving cheating via harassment and injury occurred in 1994 in figure skating when Tonya Harding's ex-husband, Jeff Gillooly, and her bodyguard Shawn Eckhardt, hired Shane Stant to break Nancy Kerrigan's leg to remove her from the year's competitions and prevent her from competing with Harding. One of the most famous instances of cheating involving a prohibited player action occurred during the 1986 FIFA World Cup quarter-final, when Diego Maradona used his hand to punch the ball into the goal of England goalkeeper Peter Shilton. Using the hand or arm by anyone other than a goalkeeper is illegal according to the rules of association football.\n\nIllegally altering the condition of playing equipment is frequently seen in bat sports such as baseball and cricket, which are heavily dependent on equipment condition. For example, in baseball, a pitcher using a doctored baseball (e.g. putting graphite or Vaseline on the baseball), or a batter using a corked bat are some examples of this. Tennis and golf are also no strangers to equipment cheating, with players being accused of using rackets of illegal string tension, or golf clubs of illegal weight, size, or make. Equipment cheating can also occur via the use of external aids in situations where equipment is prohibited - such as in American football via the use of stickum on the hands of receivers, making the ball easier to catch. An example of this is Hall of Famer Jerry Rice, who admitted to regularly & illegally using \"stickum\" throughout his career, calling into question the integrity of his receiving records.\n\nAthletic cheating is a widespread problem. For example, in professional bodybuilding, cheating is now estimated to be so universal that it is now considered impossible to engage in professional competition without cheating and the use of supposedly banned substances; bodybuilders who refuse to take banned substances now compete in natural bodybuilding leagues.\n\nCheating may also be seen in coaching. One of the most common forms of this is the use of bribery and kickbacks in the player recruitment process. Such practices are widespread all across athletics, and are particularly visible in college sports recruitment. Another common form of cheating in coaching is profiteering in association with gamblers and match fixing (see also the section below on cheating in the gambling industry). The most famous coach of the University of Nevada, Las Vegas Runnin' Rebels basketball team, Jerry Tarkanian, was accused of both recruitment fraud and gambling fraud over the course of his career and was the subject of intense NCAA scrutiny. Another form of this involves a team coach or other manager undertaking corporate espionage or another form of prohibited spying in order to obtain details about other teams' strategies and tactics. The 2007 New England Patriots videotaping controversy, in which the New England Patriots were found to have videotaped an opposing team from an unapproved location while trying to obtain defensive signals. As was the Pittsburgh Steelers use of, at the time legal, performance enhancers. However, there was cheating proven by the Denver Broncos during their back to back titles in the late 90's to circumvent the league's salary cap and obtain and retain players that they would otherwise not have been able to. Circumvention of rules governing conduct and procedures of a sport can also be considered cheating. During the 2007 Formula One Season, driver Fernando Alonso was labelled a \"cheat\" for exchanging confidential information between the teams of Scuderia Ferrari and Mclaren, a form of collusion. \n\nAn example of cheating via judging collusion occurred in the 2002 Winter Olympics figure skating scandal when the Russian team was awarded a gold medal over the Canadian team in an alleged vote-swapping judging deal; the Canadian team's silver medals were eventually upgraded to gold at a second awards ceremony and the French judge was suspended for misconduct. The head of the French skating federation was later also suspended, and both were banned from the 2006 Olympic Games. The International Skating Union modified its judging system as a result of this case. \n\nCheating is also used to refer to movements in strength training that transfer weight from an isolated or fatigued muscle group to a different or fresh muscle group. This allows the cheater to move an initial greater weight (if the cheating continues through an entire training set) or to continue exercising past the point of muscular exhaustion (if the cheating begins part way through the set). As strength training is not a sport, cheating has no rule-based consequences, but can result in injury or a failure to meet training goals. This is because each exercise is designed to target specific muscle groups and if the proper form is not used the weight can be transferred away from the targeted group.\n\nIn video games, cheating can take the form of secret access codes in single-player games (such as the Konami code) which unlock a bonus for the player when entered, hacks and exploits which give players an unfair advantage in online multiplayer games and single-player modes, or unfair collusion between players in online games (such as a player who spectates a match, removing limitations such as \"fog of war\", and reports on enemy positions to game partners).\n\nAttitudes towards cheating vary. Using exploits in single-player modes is usually considered to be simply another form of exploring the game's content unless the player's accomplishments are to be submitted competitively, and is common in single-player games with a high difficulty level; however, cheating in multiplayer modes is considered immoral and harshly condemned by fair players and developers alike. On one hand, cheating allows casual players to complete games at much-accelerated speed, which can be helpful in some cinematic or one-player games, which can take a subjectively long time to finish, as is typical of the Role-Playing Game (RPG) genre. While this may be seen as a hasty advantage causing no damage to anyone, in a multi-player game such as MMORPGs the repercussions of cheating are much more damaging, breaking the risk/reward curve of the game and causing fair players to lose online matches and/or character development. Cheating in those types of games is generally prohibited - though often widespread anyway. In many circles, the purchasing of items or currency from sources outside the game is also considered to be cheating. The Terms of Service from many games where this is possible, directly prohibits this activity. One area where there is little consensus as of yet involves modern Free-to-play business models which support and are supported by the exchange of real-world money for in-game services, items, and advantages. Games that grant excessive advantages only available to paying customers may be criticized as being 'Pay to win' - sometimes considered a form of \"cheating\" that is actually legitimatized by the system - whilst games that limit real-money purchases to cosmetic changes are generally accepted as fair.\n\nAnother form of video game cheating is when a player does things to interact with game objects that are unforeseen by the programmers and break the intended function or reward system of the object. This can involve the way enemies are encountered, objectives met, items used, or any other game object that contains a software bug. One common example is the exploitation of errors in an enemy's pathfinding; if a player can cause an enemy to become \"stuck\" in a given terrain feature, that player can then usually dispatch the enemy from a distance without risk, even if much stronger, and achieve greater rewards than the player is intended to be able to at that level of progression. Another example was common in early first-person shooter games and involved skipping a weapon's reload timer by quickly switching weapons back and forth without actually reloading the weapons; resulting in what was effectively instant reloading. It also can be accomplished through means of altered game files are substituted for the normal files, or image graphics changed to permit greater visibility of the targets, etc. - for example, replacing the colors on a dark-colored enemy intended to blend in with the background with a bright color permitting instant visibility and targeting. Generally speaking, there is often some concern that this is not truly cheating, as it is the fault of the programmers that such an exploit exists in the first place. However, technically, as with live sports, it is cheating if the player is not playing the game in a formally approved manner, breaking unwritten rules. In some cases, this behavior is directly prohibited by the Terms of Service of the game.\n\nThe wagering of money on an event extends the motivation for cheating beyond directly participating competitors. As in sport and games, cheating in gambling is generally related to directly breaking rules or laws, or misrepresenting the event being wagered on, or interfering in the outcome.\nA boxer who takes a dive, a casino which plays with secretly loaded dice, a rigged roulette wheel or slot machine, or a doctored deck of cards, are generally regarded as cheating, because it has misrepresented the likelihood of the game's outcomes beyond what is reasonable to expect a bettor to protect himself against. However, for a bookmaker to flatter a horse in order to sell bets on it at shorter odds may be regarded as salesmanship rather than cheating, since bettors can counter this by informing themselves and by exercising skepticism.\nDoping a horse is a clear example of cheating by interfering with the instruments of the event under wager. Again, not all interference is cheating; spending money to support the health and well-being of a horse one has wagered on is not in itself generally regarded as cheating, nor is improving the morale of a sportsman one has backed by cheering for them. Generally, interference is more likely to be regarded as cheating if it diminishes the standard of a sporting competition, damages a participant, or modifies the apparatus of the event or game.\n\nIn the world of gambling, knowing a secret which is not priced into the odds gives a significant advantage, which may give rise to a perception of cheating. However, legal systems do not regard secretly making use of knowledge in this way as criminal deception in itself. This is in contrast to the financial world, where people with certain categories of relationship to a company are restricted from transacting, which would constitute the crime of insider trading. This may be because of a stronger presumption of equality between investors, or it may be because a company employee who also trades in the company's stock has a conflict of interest, and has thus misrepresented himself the company.\nAn advantage player typically uses mental, observational or technical skills to choose when and how much to bet, and neither interferes with the instruments of the game nor breaks any of its rules. Representatives of the casino industry have claimed that all advantage play is cheating, but this point of view is reflected neither among societies in general nor in legislation. As of 2010, the only example anywhere of a type of advantage play being unlawful is for an advantage player to use an auxiliary device in the U.S. State of Nevada, whose legislation is uniquely influenced by large casino corporations. Nonetheless it remains a widely held principle that the law should not impose any restraint over the method by which a player arrives at a playing or betting decision from information held by him lawfully and which he is not debarred from under the rules of the game. In \"hole carding\", a casino player tries to catch sight of the front of cards which are dealt face-down according to the rules.\nOne way of cheating and profiting through gambling is to bet against yourself and then intentionally lose. This is known as throwing a game or taking a dive. Illegal gamblers will at times pay sports players to lose so that they may profit from the otherwise unexpected loss. An especially notorious case is the Black Sox Scandal, when eight players of the 1919 Chicago White Sox took payment from gamblers and intentionally played poorly. Another happened in boxing when Jake LaMotta famously took a dive against Billy Fox in order to obtain his entry to a championship match against Marcel Cerdan, a deal offered by the mobsters who controlled professional boxing.\n\nVarious regulations exist to prevent unfair competitive advantages in business and finance, for example competition law, or the prohibition of insider trading.\n\nThe most extreme forms of cheating (e.g. attempting to gain money through outright deceit rather than providing a service) are referred to as fraud.\n\n\n"}
{"id": "11716544", "url": "https://en.wikipedia.org/wiki?curid=11716544", "title": "Classical fluid", "text": "Classical fluid\n\nClassical fluids are systems of particles which retain a definite volume, and are at sufficiently high temperatures (compared to their Fermi energy) that quantum effects can be neglected. A system of hard spheres, interacting only by hard collisions (e.g., billiards, marbles), is a model classical fluid. Such a system is well described by the Percus-Yevik equation. Common liquids, e.g., liquid air, gasoline etc., are essentially mixtures of classical fluids. Electrolytes, molten salts, salts dissolved in water, are classical charged fluids. A classical fluid when cooled undergoes a freezing transition. On heating it undergoes an evaporation transition and becomes a classical gas that obeys Boltzmann statistics.\nA system of charged classical particles moving in a uniform positive neutralizing background is known as a one-component plasma (OCP). This is well described by the Hyper-netted chain equation (see CHNC). \nAn essentially very accurate way of determining the properties of classical fluids is provided by the method of molecular dynamics.\nAn electron gas confined in a metal is NOT a classical fluid, whereas a very high-temperature plasma of electrons could behave as a classical fluid. Such non-classical Fermi systems, i.e., quantum fluids, can be studied using quantum Monte Carlo methods, Feynman path integral equation methods, and approximately via CHNC integral-equation methods.\n"}
{"id": "22539338", "url": "https://en.wikipedia.org/wiki?curid=22539338", "title": "Communication rights", "text": "Communication rights\n\nCommunication rights involve freedom of opinion and expression, democratic media governance, media ownership and media control, participation in one's own culture, linguistic rights, rights to education, privacy, assemble, and self-determination. They are also related inclusion and exclusion, quality and accessibility to means of communication.\n\nA \"right to communicate\" and \"communication rights\" are closely related, but not identical. The former is more associated with the New World Information and Communication Order debate, and points to the need for a formal legal acknowledgment of such a right, as an overall framework for more effective implementation. The latter emphasizes the fact that an array of international rights underpinning communication already exists, but many are often ignored and require active mobilization and assertion.\n\nThe concept of the right to communicate began in 1969 with Jean D’Arcy, a pioneer in French and European television in the 1950s and by 1969 Director of the United Nations Radio and Visual Services Division, where he was involved in international policy discussions arising out of the recent innovations in satellite global communications. He recognized that the communication rights relating to freedom of expression embodied in the U. N. Universal Declaration of Human Rights (UDHR) adopted in 1948 would need to be re-examined in the context of global, interactive communication between individuals and communities. He called for the need for the recognition of a human right to communicate that would encompass earlier established rights. He thus was the first to link communication and \"universal\" human rights. His call was taken up by academics, policy experts, and public servants who evolved into the Right to Communicate Group, the many non-governmental and civil society organisations that made up the Platform for Co-operation on Communication and Democratisation, and the Communication Rights in the Information Society (CRIS) Campaign.\n\nThe first broad-based debate on media and communication globally, limited mainly to governments, ran for a decade from the mid-1970s. Governments of the South, by then a majority in the UN, began voicing demands in UNESCO concerning media concentration, the flow of news, and ‘cultural imperialism’. The MacBride Report (1981) studied the problem, articulating a general ‘right to communicate’. The debate was compromised, however, by Cold War rhetoric, and fell apart after the US and the UK pulled out of UNESCO.\n\nThe \"MacBride Report\" became unavailable until the World Association for Christian Communication (WACC) sponsored its republication in 1988. WACC held the secretariat of the CRIS Campaign 2000–05.\n\nInterest in the right to communicate languished during the 1980s as there was no mass movement to promote it for the simple reason few people had direct experience with interactive communication over global electronic networks. This situation changed dramatically in the 1990s with a cluster of innovations that included the Internet, the World Wide Web, search engines, availability of personal computers, and social networking. As more people participated in interactive communication and the many challenges it raised in regard to such communication rights as free of speech, privacy, and freedom of information, they began to develop a growing consciousness of the importance of such rights to their ability to communicate.\n\nA result of this growing communicative consciousness is a renewed research interest in and political advocacy for a right to communicate (see references). From the 1990s onwards, NGOs and activists became increasingly active in a variety of communication issues, from community media, to language rights, to copyright, to Internet provision and free and open source software. These coalesced in a number of umbrella groups tackling inter-related issues from which the pluralistic notion of communication rights began to take shape, this time from the ground up.\n\nEach Pillar [of Communication Rights] relates to a different domain of social existence, experience and practice, in which communication is a core activity and performs key functions. The for the four [pillars is,] that each involves a \"relatively autonomous sphere of social action\", yet \"depends on the others\" for achieving its ultimate goal - they are necessary interlocking blocks in the struggle to achieve communication rights. Action can be coherently pursued under, each, often in collaboration with other social actors concerned with the area more generally; while bridges can and must be built to the other areas if the goal is to be achieved.\n\n\"The role of communication and media in exercising democratic political participation in society.\"\n\n\"The terms and means by which knowledge generated by society is communicated, or blocked, for use by different groups.\"\n\n\"The exercise of civil rights relating to the processes of communication in society.\"\n\n\"The communication of diverse cultures, cultural forms and identities at the individual and social levels.\"\n\nA ‘right to communicate’ and ‘communication rights’ are closely related, but not identical, in their history and usage. In the Cold War tensions of the 1970s and 1980s, the former became associated with the New World Information and Communication Order (NWICO) debate, thus, efforts within UNESCO to formulate such a right were abandoned. The latter emphasizes the fact that an array of international rights underpinning communication already exists, but many are too often ignored and require active mobilisation and assertion. While some, especially within the mass media sector, still see the right to communicate as a \"code word\" for state censorship,the technological innovations in interactive electronic, global communication of recent decades are seen by others as challenging the traditional mass media structures and formulations of communication rights values arising from them, thereby renewing the need to re-consider the need for a right to communicate.\n\n \n\n\n"}
{"id": "19260818", "url": "https://en.wikipedia.org/wiki?curid=19260818", "title": "Covariance group", "text": "Covariance group\n\nIn physics, a covariance group is a group of coordinate transformations between frames of reference (see for example Ryckman (2005)). A frame of reference provides a set of coordinates for an observer moving with that frame to make measurements and define physical quantities. The covariance principle states the laws of physics should transform from one frame to another covariantly, that is, according to a representation of the covariance group.\n\nSpecial relativity considers observers in inertial frames, and the covariance group consists of rotations, velocity boosts, and the parity transformation. It is denoted as O(1,3) and is often referred to as Lorentz group.\n\nFor example, the Maxwell equation with sources,\n\ntransforms as a four-vector, that is, under the (1/2,1/2) representation of the O(1,3) group.\n\nThe Dirac equation,\n\ntransforms as a bispinor, that is, under the (1/2,0)⊕(0,1/2) representation of the O(1,3) group.\n\nThe covariance principle, unlike the relativity principle, does not imply that the equations are invariant under transformations from the covariance group. In practice the equations for electromagnetic and strong interactions \"are\" invariant, while the weak interaction is not invariant under the parity transformation. For example, the Maxwell equation \"is\" invariant, while the corresponding equation for the weak field explicitly contains left currents and thus is not invariant under the parity transformation.\n\nIn general relativity the covariance group consists of all arbitrary (invertible and differentiable) coordinate transformations.\n\n\n"}
{"id": "46911592", "url": "https://en.wikipedia.org/wiki?curid=46911592", "title": "Crown rabbi (Iberia)", "text": "Crown rabbi (Iberia)\n\nIn the Iberian peninsula, the crown rabbi (Spanish: ' or Old Portuguese: ' (\"chief rabbi\") ) was a secular, administrative post occupied by a member of the Jewish community for the benefit of the governing state, and existed in the kingdoms of Castile, Aragon, Navarre and Portugal as far back as the 13th century, and is referred to as crown rabbi by historians in English, as well as by court rabbi and other terms.\n\nIn Spanish this position was known as \"\" or , which can be translated literally as \"chief rabbi\" or \"court rabbi\", respectively, and which is referred to in some English sources as \"crown rabbi\" and in others as \"court rabbi\". In Sicily (part of Aragon) the position was known locally as the \"dayyan kalali\" and in Portuguese as \"arrabi-mor\". The derivation of arrabi mor is through a very unusual, three-language merger of parts in Judaeo-Portuguese, from Hebrew \"rabi\" (noun, \"rabbi\") preceded by Arabic definite article \"ar\" (\"the\", from \"al\" + initial \"r-\" consonsant), and Portuguese \"mor\" (\"adj.\", \"chief\", in normal postposition).\n\nThe concept of an official rabbi performing administrative duties and acting as an intermediary existed as far back as the 13th century in the kingdoms of Castile, Aragon, and Portugal and elsewhere in the Iberian peninsula. \n\nThe crown rabbi was one of the chief ways for the kingdoms in the peninsula to exert power over their Jewish communities. Those officials fulfilling this position often acquired significant secular power over their communities, and sometimes over provinces or even kingdoms.\n\nIn Castile, the Court Rabbinate extended as an institution from 1255 until Expulsion in 1492. They were often laymen, not rabbis, and had near dictatorial authority of their flock. They presided in appeals cases and international synods, and might also be a court physician, as well as tax collector over both the Jewish as well as the Christian community. The last one to hold the office of crown rabbi of Castile was Abraham Seneor who became a \"converso\" rather than be expelled.\n\nIn 1386 in the Kingdom of Aragon for example, King John I in the context of a time of political reform, issued edicts defining the functions and duties of the as intermediary between the power of the kingdom and that of the \"aljama\", or Jewish community. There were various requirements as to the good character and faith of the person holding this charge, as well as a requirement that he live among the entourage of the Court, and thus away from his community, and in constant contact with the Christian majority population. His powers and authorities over the of Castile, economic, judicial, and otherwise, were specified.\n\nIn Portugal, the \"arrabi\" was a Jewish official who acted as a private municipal judge in a locality, chosen from among the community. \n\nThe term \"arrabi\" is attested from the late 12th century in Latin and Portuguese under Afonso III, and is mentioned in a judicial sense in municipal legislation documents. Sometimes it appears as \"Rabi\". Documents from Lisbon, Leiria and elsewhere suggest that there was one \"arrabi\" per community, who was an outpost of royal authority, parallel to and separate from the traditional rabbi who tended to their flock's religious and spiritual needs.\n\nPresiding above the \"arrabis\" was a high functionary of the crown known as the \"arrabi mor\" (or arrabi-môr; \"chief rabbi\") and reporting to the King. Besides supervising the administration of justice, he also was in charge of fiscal administration and presided over the \"ouvidores\" (auditors) of the kingdom. \n\nThe position of \"arrabi mor\" emerged in Portugal as a result of efforts begun in the 12th century to centralize the legal and fiscal system in the country. By the late 13th century this effort extended to all of Portuguese Jewry, as manifested by the creation of a network of Jewish officials in each locality. The head of this network was the \"arrabi mor\" (chief rabbi) who acted as the royal tax collector similar to the position of the \"almoxarife mayor\" (chief financial administrator) in Castile. Under him were seven officials also called \"arrabis\" or \"ouvidores\" (auditors) who were responsible for taxes in their region (\"arrabiado\"); the local \"arrabis\" were assigned to individual communities after the model in Castile and Aragon.\n\nThe high post tended to be filled by wealthy Jews, and the post was handed down and controlled by family dynasties. The first \"arrabi mor\" mentioned was Don Judah in the 13th century under King Dinis, followed by his son Guedelha. The main duties were judicial, and fiscal. Judicially, the decisions of the \"arrabi mor\" concerning matters in the Jewish community were final, per a decree by Afonso III in 1266, and he was responsible only for the highest issues, as the simpler suits and appeals were judged by the local \"arrabis\".\n\nA powerful \"arrabi-mor\" could sometimes influence the laws of the kingdom in favor of the \"aljama\". Such a man was Moses Navarro under King John I of Portugal. Following the carnage and forced conversions in the 1391 massacre of Jews in Seville and its aftermath in other kingdoms of the Iberian peninsula, the devastation threatened to spill across the border into Portugal, but Moses Navarro exercised his power and influence with the monarch and his knowledge of edicts from the Vatican by Popes Boniface IX and Clement VI friendly to Jews to prevent any harm from coming to Portuguese Jewry. King John upon hearing of the edicts, immediately promulgated a law on July 17, 1392 prohibiting any persecution, which was obeyed gladly by his subjects due to the extent of his popularity in the land. As a result, Portugal became a safe haven for Jews escaping persecution in Spain.\n\n\n"}
{"id": "27420250", "url": "https://en.wikipedia.org/wiki?curid=27420250", "title": "Declarator", "text": "Declarator\n\nA declarator in Scottish law is a form of legal action by which some right of property, servitude, or status (or some other inferior right or interest) is sought to be judicially declared.\n\n"}
{"id": "6053993", "url": "https://en.wikipedia.org/wiki?curid=6053993", "title": "Distributive category", "text": "Distributive category\n\nIn mathematics, a category is distributive if it has finite products and finite coproducts such that for every choice of objects formula_1, the canonical map\n\nis an isomorphism, and for all objects formula_3, the canonical map formula_4 is an isomorphism (where 0 denotes the initial object). Equivalently. if for every object formula_3 the endofunctor formula_6 defined by formula_7 preserves coproducts up to isomorphisms formula_8. It follows that formula_8 and aforementioned canonical maps are equal for each choice of objects. \n\nIn particular, if the functor formula_6 has a right adjoint (i.e., if the category is cartesian closed), it necessarily preserves all colimits, and thus any cartesian closed category with finite coproducts (i.e., any bicartesian closed category) is distributive.\n\nThe category of sets is distributive. Let , , and be sets. Then\nwhere formula_12 denotes the coproduct in Set, namely the disjoint union, and formula_13 denotes a bijection. In the case where , , and are finite sets, this result reflects the distributive property: the above sets each have cardinality formula_14.\n\nThe category Grp is not distributive, even though it has both products and coproducts.\n\nAn even simpler category that has both products and coproducts but is not distributive is the category of pointed sets.\n\n"}
{"id": "37237272", "url": "https://en.wikipedia.org/wiki?curid=37237272", "title": "Eocyte hypothesis", "text": "Eocyte hypothesis\n\nThe Eocyte hypothesis is a biological classification that indicates eukaryotes emerged within the prokaryotic Crenarchaeota (formerly known as eocytes), a phylum within the archaea. This hypothesis was originally proposed by James A. Lake and colleagues in 1984 based on the discovery that the shapes of ribosomes in the Crenarchaeota and eukaryotes are more similar to each other than to either bacteria or the second major kingdom of archaea, the Euryarchaeota.\nThe eocyte hypothesis gained considerable attention after its introduction due to the interest in determining the origin of the eukaryotic cell. This hypothesis has primarily been in contrast with the three-domain system introduced by Carl Woese in 1977. Additional evidence supporting the eocyte hypothesis was published in the 1980s, but despite fairly unequivocal evidence, support waned in favor of the three-domain system.\n\nWith advancements in genomics, the eocyte hypothesis experienced a revival beginning in the mid-2000s. As more archaeal genomes were sequenced, numerous genes coding for eukaryotic traits have been discovered in various archaean phyla, seemingly providing support for the eocyte hypothesis. Proteomics based research has also found supporting data with the use of elongation factor 1-α (eEF-1), a common housekeeping protein, to compare structural homology between eukaryotic and archaean lineages. Furthermore, other proteins have been sequenced through proteomics with homologous structures in heat shock proteins found in both eukaryotes and archaea. The structure of these heat shock proteins were identified though X-ray crystallography to find the three dimensional structure of the proteins. These proteins however have differing purposes as the eukaryote heat shock protein is a part of the T-complex while the archaeal heat shock protein is a molecular chaperone. This creates an issue with the sequence homology that has been seen between 70 kilodalton heat shock proteins in eukaryotes and gram negative bacteria.\n\nIn addition to a Crenarchaeal origin of eukaryotes, some studies have suggested that eukaryotes may also have originated in the Thaumarchaeota. A superphylum - TACK - has been proposed that includes the Thaumarchaeota, Crenarchaeota, and other groups of archaea, so that this superphylum may be related to the origin of eukaryotes. It is seen that eukaryotes share a large number of proteins with members of the TACK superphylum and that these complex archaea may have had rudimentary phagocytosis abilities to engulf bacteria.\n\nAs a result of metagenomic analysis of material found nearby hydrothermal vents, another superphylum -- Asgard—has been named and proposed to be more closely related to the original eukaryote and a sister group to TACK more recently. \nThe eocyte tree root may be located in the RNA World, that is the root organism may have been a Ribocyte (aka Ribocell). For cellular DNA and DNA handling an \"out of virus\" scenario has been proposed, i. e. string genetic information in DNA may have been an invention performed by viruses later handed over to Ribocytes twice, once transforming them into bacteria and once transforming them into archaea. \nAlthough archaeal viruses aren't as studied as bacterial phages, it is thought that dsDNA viruses lead to the incorporation of the viral genome into archaeal genomes. The transduction of genetic material through a viral vector lead to an increase in complexity in the pre-eukaryotic cells. All these findings do not change the eocyte tree as given here in principle, but zoom into a higher resolution of it.\n\nDue to the similarities found between eukaryotes and both archaea and bacteria, it is thought that a major source of the genetic variation is through horizontal gene transfer. The horizontal gene transfer is the reason for why archaeal sequences are found in bacteria and bacterial sequences are found in archaea. This could be the reasoning for why elongation factors found in archaea and eukaryotes are so similar, the data currently out is obscured as horizontal gene transfer, vertical gene transfer, or endosymbiosis could be behind the gene sequence similarity. The Eocyte Hypothesis also has troubles due to the endosymbiotic theory and how the archaea were able to phagocytize the bacteria for the formation of membrane bound organelles. It is thought that these ancestral prokaryotes began to have ectosymbiotic relationships with other prokaryotes and slowly engulfed these symbiotes through cell membrane protrusions.\n\nAlthough there has been data that attempts to confirm the relationship between eukaryotes and Chrenarcheota through the analysis of elongation factors, more recent experimentation with elongation factors provides data that disproves the eocyte hypothesis. Hasegawa et al. uses these elongation factors to help solidy that eukaryotes and archaebacteria are more closely related than archaebacteria and eubacteria that is explained in this 2 tree system.\n\nA competing hypothesis is that prokaryotes evolved towards thriving in higher temperatures to evade viruses through the thermoreductive hypothesis, however this does not account for eukaryotes arise and only takes into consideration the prokaryotic origins. However decrease in complexity from a more complex origin is the basis of reductive evolution where a commensal relationship occurs, while this reduction explained in the thermoreduction hypothesis uses a parasitic relationship with viruses to explain the movement of complex pre-eukaryotes to a more harsh environment; that being ocean floor hydrothermal vents.\n"}
{"id": "53768121", "url": "https://en.wikipedia.org/wiki?curid=53768121", "title": "Eroseanna Robinson", "text": "Eroseanna Robinson\n\nEroseanna “Sis” Robinson was an American social worker, track star, activist and member of the Peacemakers who organized for desegregation and against the U.S. military in the 1950s and 1960s. In particular, she was an advocate of nonviolent resistance strategies. Robinson went on hunger strike or risked violence and arrest multiple times, but nonetheless won various victories for equality. Born in 1924, Robinson died at age in 52 in 1976.\n\nIn addition to her activism, Robinson was a successful track runner. These two interests collided when she was chosen to represent the United States in a track meet against Russia and refused to participate because she felt she was being used as a “political pawn” by appearing alongside white athletes, giving the international community the false impression that white and black people were treated equally inside of the United States.\n\nIn 1952, Eroseanna Robinson worked at a community center in Cleveland when she decided that she would help to desegregate a public skating facility called Skateland. In trying to accomplish this goal, she pursued a non-violent plan. In her first attempt to desegregate Skateland, she brought children—two black children and one white child—from the community center at which she worked to skate. White teenagers harassed the children. Over the next few days, Robinson returned with friends who supported her cause but was continually tripped and physically assaulted by white customers at Skateland. At one point, Robinson required urgent medical attention after she was shoved, causing her to break an arm. Skateland’s management and security teams did not intervene on Robinson’s behalf.\n\nIn early 1960, Robinson held another nonviolent protest by refusing to pay federal taxes as a way of showing her lack of support for the United States military. Her refusal to pay taxes caused her to be sentenced to a year of imprisonment, but she used the process as an opportunity to engage in nonviolent protest. When she was sentenced, Robinson has to be carried into the courtroom on a stretcher because she refused to walk. Once she was in prison, she held a three-month fast. She was force fed through a tube and then released nine months before her sentence was complete.\n\nIn 1961, Eroseanna Robinson travelled along Route 40 in Maryland with Wally and Juanita Nelson. The three decided to stop for dinner in Elkton, Maryland, but a diner refused to serve them. The trio refused to leave until police came and arrested them, putting them in county jail. In jail, they refused to appear in court or eat. Their story was picked up by local newspapers which dubbed them the “Elkton Three.” Since the trio refused to cooperate with the court proceedings, they were given $50 fines and released. Their case sparked a local movement for desegregation that ended up gaining then Maryland Governor Millar Tawes’ attention and led to eventual desegregation of restaurants along Route 40.\n\nAlthough not widely known amongst other African American civil rights activists, Robinson was instrumental in integrating several public spaces across the country in the early days of the Civil Rights Movement. She is perhaps most remembered for her methods of nonviolent resistance, which were deployed in sit-ins and other pro-integration demonstrations throughout the latter part of the Civil Rights Movement.\n\nHer work to integrate restaurants along Maryland’s Route 40 had international ramifications, as foreign diplomats would often travel the road in trips from Washington, D.C.. to New York City. In 1958, Kenyan politician Komla Gbedema was denied service at a restaurant on Route 40, which embarrassed the Eisenhower administration. With the slogan “49 Miles of Highway and No Place to Stop and Eat,” Robinson, the Nelsons, and others were able to integrate these Maryland restaurants, which at the time hurt the United State’s international reputation. During several of Robinson’s hunger strikes, she also garnered headlines in African American publications, who lauded her relentless commitment to achieve racial justice.\n\nRobinson stands as an icon of the war tax resistance movement. The National War Tax Resistance Coordinating Committee, a grassroots movement with the aim of educating American taxpayers of their rights to resist making tax contributions to war efforts, touts Robinson as a foundational figure in their movement.\n\nSocial work, as a profession, has a history of leadership within social justice movements that dates back to the work of activists like Robinson. Today, social workers carry on this legacy of activism through grassroots movements such as RISE.\n"}
{"id": "48515699", "url": "https://en.wikipedia.org/wiki?curid=48515699", "title": "Food waste recycling in Hong Kong", "text": "Food waste recycling in Hong Kong\n\nFood waste recycling is a process to convert food waste into useful materials and products for achieving sustainability of the environment. Food waste is defined as all inedible and edible parts of food that creates preceding and succeeding food processing, production and consumption. Greenhouse gases, especially methane can be reduced by food waste recycling. Food waste recycling can also alleviate the saturation of landfill sites in Hong Kong.\n\nThe amount of food waste accounts for 38% of the municipal solid waste in Hong Kong. According to the statistics published by the Environmental Protection Department, Hong Kong generates approximately 3,648 tonnes of food waste each day. About one-third of the food waste comes from the commercial and industrial sectors while the remaining part is from households. The situation of food waste disposal in Hong Kong has become more serious in recent years. The amount of food waste from the commercial and industry sectors increased from 400 tonnes in 2002 to 1033 tonnes in 2014.\n\nFood waste recycling typically involves a three-step strategy that includes separation, collection and recycling. These three steps are interdependent and equally important in efficiently generating useful resources from food waste recycling.\n\nThe collected food waste is separated into two categories: pre-consumer food waste and post-consumer food waste. Pre-consumer food waste includes animal food waste, vegetable food waste, and waste from industrially-processed food. Post-consumer food waste refers to leftover food, such as from an unfinished restaurant order. Source separation also involves removing any food packaging and utensils.\n\nFood waste vehicles, managed by the government, collect and deliver food waste to governmental recycling facilities. These recycling facilities, known as Organic Waste Treatment Facilities (OWTFs), will be spread across Hong Kong's districts. The government plans to construct 5 to 6 OWTF networks, with a goal of recycling 1300 to 1500 tonnes of food waste between 2014 and 2024. The first facility is planned to be built in Siu Ho Wan to serve Lantau Island and districts nearby. The second and third are projected to be built in Sha Ling and Shek Kong so as to serve the New Territories and West Kowloon.\n\nThe Environmental Protection Department also plans to construct two Organic Resources Recovery Centres (ORRCs) to collect the food waste and turn it into compost for recovery. These two centres will collectively be able to treat 500 tonnes of organic waste and divert 200 to 300 tonnes of waste from the landfills daily. The first Centre will also be located in Siu Ho Wan. The construction works commenced in December 2014 and it is expected to be commissioned in early 2018.\n\nTwo technologies are used to turn food waste into biogas and useful products. The first technology is a low-carbon method that processes 100 to 300 of tonnes food waste per day. By applying this technology, it produces a source of the renewable energy biogas. It is estimated that the first Centre will be able to provide 14 million kWh of electricity to Hong Kong's power grid every year, which can power 3,000 households. The second technology converts food waste into succinic acid by using enzymes and bacteria. The residues can be processed to be animal feed, fertilizers and environmental-friendly cleansing products.\n\nFood waste recycling partnership scheme is introduced by the Environmental Protection Department in 2009. The scheme was designed to promote a good practice on food waste management and enhance people’s experiences of separating and recycling food waste sources. The scheme collaborated with commercial and industrial sectors. Some members from the Hong Kong Government and that of the commercial and industrial sectors formed a working group for planning and managing the operation of the project. There are about 20 to 30 public and private organizations that participate in the scheme every year. In the project, all the participants will have a chance to practice separation process of food waste sources. The collection process of separated food waste will be carried out by the Environmental Protection Department and the Kowloon Bay Pilot Composting Plant will be responsible to the recycling part. The scheme also consists of discussion sessions for the participating parties to share their experience of recycling food waste. All participants will receive a commendation certificate at the end of the project.\n\nFrom November 2012 to July 2014, The Hong Kong Housing Authority implemented a trial scheme, named as Food Waste Recycling Projects in Housing Estates, with an aim to promote waste recycling in housing estates. The scheme involved 14 estates with about 3200 households. There were two modes adopted in the scheme. One is to convert food waste to fish grain by transporting them to a Central Food Waste Recycling Plant. The other one is to convert food waste into compost for farm by using micro-organisms.\n\nOn 31 March 2012, The Environmental Protection Department introduced a recycling scheme on Cheung Chau and Lamma Island. The purpose of the scheme is to promote and encourage food waste recycling on both islands. Food waste collected from shops and restaurants would be transported to the food waste treatment facilities on the islands. The food waste would be converted to organic compost.\n\nHong Kong Organic Waste Recycling Centre (HKOWRC) was established in 2011 as the first organic waste management consulting firm in Hong Kong. It combines local and foreign technologies to provide one-stop organic recycling services to customers. The assistance in collection of food waste solves problems of companies in immature recycling. Through commitment to different types of people, HKOWRC can promote and provide waste management training for particular customers.\n\nThe objectives of the collection and recycling routine are to educate the general public in cherishing food as well as making good use of resources. The service encourages two-way cooperation for food waste recycling. HKOWRC aims to provide 24-hour recycling services particularly to livestock breeders, farmers, catering industry, schools and large-scale housing estates. For most of the cases, the recycled food waste is to be used by the service targets again for other purposes, therefore avoiding wastage of resources. For livestock breeders and farmers, the recycled food waste is converted to animal feeds and fertilizers that contain richer nutritional values than the conventional ones. It possibly gives higher yield. For catering industry, the recycled food waste is converted to crops which are grown from organic compost from HKOWRC for latest organic food at a special prize. For schools and housing estates, it aims to raise the awareness of food waste reduction when the recycling service reveals the huge amount of food wastes accumulated.\n\n\n"}
{"id": "39871498", "url": "https://en.wikipedia.org/wiki?curid=39871498", "title": "Force chain", "text": "Force chain\n\nIn the study of the physics of granular materials, a force chain consists of a set of particles within a compressed granular material that are held together and jammed into place by a network of mutual compressive forces.\n\nBetween these chains are regions of low stress whose grains are shielded for the effects of the grains above by vaulting and arching. A set of interconnected force chains is known as a force network.\n\nForce networks are an emergent phenomenon that are created by the complex interaction of the individual grains of material and the patterns of pressure applied within the material. Force chains can be shown to have fractal properties.\n\nForce chains have been investigated both experimentally, through the construction of specially instrumented physical models, and through computer simulation.\n\n"}
{"id": "14384856", "url": "https://en.wikipedia.org/wiki?curid=14384856", "title": "Fourth International Conference on Environmental Education", "text": "Fourth International Conference on Environmental Education\n\nThe Tbilisiplus30 or the Fourth International Conference on Environmental Education was held at the Centre for Environment Education, Ahmedabad, India between November 24, 2007 and November 28, 2007.\n\nThe conference was the fourth in the series of Conferences on environmental education held since the first international conference in Tbilisi (former USSR). The second conference was organised in 1977 in Moscow; and the third conference was held in Thessaloniki in 1997. The United Nations has declared the decade 2005 to 2014 as the \"Decade of Education for Sustainable Development\" (DESD). This conference underlined the key role of education in achieving sustainable development. The participants and delegates from countries across the globe came together to bridge the gap between environmental education and Education for Sustainable Development. They examined the development of environmental education since the first conference, thirty years ago, and set a global agenda for the DESD. This will be a platform for sharing practices and ideas on initiatives in environmental education throughout the world.\n\nThere was a significant amount of participation in workshops on topics including \"Education for Sustainable Development\" and \"Teacher Education,\" research for DESD, \"DESD Monitoring and Evaluation,\" \"ESD and Media,\" \"Man and Biosphere Reserves\" and \"World Heritage Sites\" as learning sites for environmental development, \"Floods and Disaster Reduction\" and \"Education for Sustainable Consumption\".\n"}
{"id": "41197", "url": "https://en.wikipedia.org/wiki?curid=41197", "title": "Front-to-back ratio", "text": "Front-to-back ratio\n\nIn telecommunication, the term front-to-back ratio (\"also known as front-to-rear ratio\") can mean:\n\nThe ratio compares the antenna gain in a specified direction, \"i.e.\", azimuth, usually that of maximum gain, to the gain in a direction 180° from the specified azimuth. A front-to-back ratio is usually expressed in dB. \n\nIn point-to-point microwave antennas, a \"high performance\" antenna usually has a higher front to back ratio than other antennas. For example, an unshrouded 38 GHz microwave dish may have a front to back ratio of 64 dB, while the same size reflector equipped with a shroud would have a front to back ratio of 70 dB. Other factors affecting the front to back ratio of a parabolic microwave antenna include the material of the dish and the precision with which the reflector itself was formed.\n\nIn other electrical engineering the front to back ratio is a ratio of parameters used to characterize rectifiers or other devices, in which electric current, signal strength, resistance, or other parameters, in one direction is compared with that in the opposite direction. \n"}
{"id": "39226029", "url": "https://en.wikipedia.org/wiki?curid=39226029", "title": "HCS clustering algorithm", "text": "HCS clustering algorithm\n\nThe HCS (Highly Connected Subgraphs) clustering algorithm (also known as the HCS algorithm , and other names such as Highly Connected Clusters/Components/Kernels) is an algorithm based on graph connectivity for Cluster analysis, by first representing the similarity data in a similarity graph, and afterwards finding all the highly connected subgraphs as clusters. The algorithm does not make any prior assumptions on the number of the clusters. This algorithm was published by Erez Hartuv and Ron Shamir in 1998.\n\nThe HCS algorithm gives clustering solution, which is inherently meaningful in the application domain, since each solution cluster must have diameter 2 while a union of two solution clusters will have diameter 3.\n\nThe goal of cluster analysis is to group elements into disjoint subsets, or clusters, based on similarity between elements, so that elements in the same cluster are highly similar to each other (homogeneity), while elements from different clusters have low similarity to each other (separation). Similarity graph is one of the models to represent the similarity between elements, and in turn facilitate generating of clusters. To construct a similarity graph from similarity data, represent elements as vertices, and elicit edges between vertices when the similarity value between them is above some threshold.\n\nIn the similarity graph, the more edges exist for a given number of vertices, the more similar such a set of vertices are between each other. In other words, if we try to disconnect a similarity graph by removing edges, the more edges we need to remove before the graph becomes disconnected, the more similar the vertices in this graph. Minimum cut is a minimum set of edges without which the graph will become disconnected.\n\nHCS clustering algorithm finds all the subgraphs with n vertices such that the minimum cut of those subgraphs contain more than n/2 edges, and identifies them as clusters. Such a subgraph is called a Highly Connected Subgraph (HCS). Single vertices are not considered clusters and are grouped into a singletons set S.\n\nGiven a similarity graph G(V,E), HCS clustering algorithm will check if it is already highly connected, if yes, returns G, otherwise uses the minimum cut of G to partition G into two subgraphs H and H', and recursively run HCS clustering algorithm on H and H'.\n\nThe following animation shows how the HCS clustering algorithm partitions a similarity graph into three clusters.\n\n1 function HCS(G(V,E)) \n2 if G is highly connected\n3 then return (G)\n4 else\n5 (H1,H2,C) ← MINIMUMCUT(G)\n6 HCS(H1)\n7 HCS(H2)\n8 end if\n9 end\nThe step of finding the minimum cut on graph G is a subroutine that can be implemented using different algorithms for this problem. See below for an example algorithm for finding minimum cut using randomization.\n\nThe running time of the HCS clustering algorithm is bounded by N x f(n,m). f(n,m) is the time complexity of computing a minimum cut in a graph with n vertices and m edges, and N is the number of clusters found. In many applications N < < n.\n\nFor fast algorithms for finding a minimum cut in an unweighted graph: \n\nThe clusters produced by the HCS clustering algorithm possess several properties, which can demonstrate the homogeneity and the separation of the solution.\n\nTheorem 1 The diameter of every highly connect graph is at most two.\n\n\"Proof:\" We know the edges of minimum cut must be greater or equal than the minimum degree of the graph. If the graph G is highly connected, then the edges of the minimum cut must be greater than the number of vertices divided by 2. So the degree of vertices in the highly connected graph G must be greater than half the vertices. Therefore, for any two vertices in this graph G, there must be at least one common neighbor, as the distance between them is two.\n\nTheorem 2 (a) The number of edges in a highly connected subgraph is quadratic. (b) The number of edges removed by each iteration of the HCS algorithm is at most linear.\n\n\"Proof:\" (For a) From Theorem 1 we know every vertex must have more than half of the total vertices as neighbors. Therefore, the total number of edges in a highly connect subgraph must be at least (n/2) x n x 1/2, where we sum all the degrees of each vertex and divide by 2.\n\n(For b) Each iteration HCS algorithm will separate a graph containing n vertices into two subgraphs, so the number of edges between those two components is at most n/2.\n\nTheorem 1 and 2a provide a strong indication to the homogeneity, as the only better possibility in terms of the diameter is that every two vertices of a cluster are connected by an edge, which is both too stringent and also a NP-hard problem.\n\nTheorem 2b also indicates separation since the number of edges removed by each iteration of the HCS algorithm is at most linear in the size of the underlying subgraph, contrast to the quadratic number of edges within final clusters.\n\nSingletons adoption: Elements left as singletons by the initial clustering process can be \"adopted\" by clusters based on similarity to the cluster. If the maximum number of neighbors to a specific cluster is large enough, then it can be added to that cluster.\n\nRemoving Low Degree Vertices: When the input graph has vertices with low degrees, it is not worthy to run the algorithm since it is computationally expensive and not informative. Alternatively, a refinement of the algorithm can first remove all vertices with a degree lower than certain threshold.\n\n\n\n"}
{"id": "1291083", "url": "https://en.wikipedia.org/wiki?curid=1291083", "title": "Harlan Lane", "text": "Harlan Lane\n\nHarlan Lane (born 19 August 1936) is an American psychologist. Lane is the Matthews Distinguished University Professor of Psychology at Northeastern University in Boston, Massachusetts, in the United States, and founder of the Center for Research in Hearing, Speech, and Language . His research is focused on speech, Deaf culture, and sign language. \n\nLane was born in Brooklyn, New York. Remaining in New York City for college, he obtained both a B.S. and an M.S. in Psychology from Columbia University in 1958. He subsequently received a PhD. in Psychology from Harvard (1960) and a \"Doc. des Lettres\" from the Sorbonne (1973). In 1991, Lane received a MacArthur Foundation Fellowship.\n\nLane, a hearing man, has become an often controversial spokesman for the Deaf community and critic of cochlear implants. He has written extensively on the social construction of disability and states that \"Unless Deaf people challenge the culturally determined meanings of \"deaf\" and \"disability\" with at least as much vigor as the technologies of normalization seek to institutionalize those meanings, the day will continue to recede in which Deaf children and adults live the fullest lives and make the fullest contribution to our diverse society.\" In recognition of his research and advocacy regarding these issues, Lane has received the Distinguished Service Award from the National Association of the Deaf (United States), the International Social Merit Award from the World Federation of the Deaf, and numerous other awards.\n\nHe is \"Commandeur de l'Ordre des Palmes Académiques\", the highest level of the academic honor given out by the French government.\n\n\n"}
{"id": "39936262", "url": "https://en.wikipedia.org/wiki?curid=39936262", "title": "Homology (psychology)", "text": "Homology (psychology)\n\nHomology in psychology, as in biology, refers to a relationship between characteristics that reflects the characteristics' origins in either evolution or development. Homologous behaviors can theoretically be of at least two different varieties. As with homologous anatomical characteristics, behaviors present in different species can be considered homologous if they are likely present in those species because the behaviors were present in a common ancestor of the two species. Alternatively, in much the same way as reproductive structures (e.g., the penis and the clitoris) are considered homologous because they share a common origin in embryonic tissues, behaviors—or the neural substrates associated with those behaviors—can also be considered homologous if they share common origins in development.\n\nBehavioral homologies have been considered since at least 1958, when Konrad Lorenz studied the evolution of behavior. More recently, the question of behavioral homologies has been addressed by philosophers of science such as Marc Ereshefsky, psychologists such as Drew Rendall, and neuroscientists such as Georg Striedter and Glenn Northcutt. It is debatable whether the concept of homology is useful in developmental psychology.\n\nFor example, D. W. Rajecki and Randall C. Flanery, using data on humans and on nonhuman primates, argue that patterns of behaviour in dominance hierarchies are homologous across the primates.\n"}
{"id": "20562825", "url": "https://en.wikipedia.org/wiki?curid=20562825", "title": "How to Break a Terrorist", "text": "How to Break a Terrorist\n\nHow to Break a Terrorist: The US Interrogators Who Used Brains, Not Brutality, to Take Down the Deadliest Man in Iraq is a book written by an American airman who played a key role in tracking down Abu Musab al-Zarqawi.\n\nThe interrogator who wrote the book published it under the pen name Matthew Alexander, for security reasons. The author wrote the book as a pseudonymous officer in the US Air Force who had served for fourteen years. Alexander's real name has been sealed by court order by the District of Columbia District Court. Alexander makes television appearances under his pseudonym.\nAccording to Jeff Stein, writing in the \"Washington Post\", the author's real name was Anthony Camerino, a Major in the United States Air Force Reserve.\nIn an op-ed published in the \"Washington Post\" he wrote that after his arrival in Iraq in 2006 he found:\n\nThe author has stated that when the Pentagon vetted the book they initially made 93 redactions.\n\nAlexander is an outspoken opponent of torture. He refutes the effectiveness of torture, citing its negative long-term effects such as recruiting for Al-Qaeda. He also argues that torture is contrary to the American principles of freedom, liberty, and justice, and that should they resort to torture, American interrogators become the enemy they serve to defeat. Similar arguments have been made by other former interrogators from the U.S. military, FBI, and the CIA, including Colonel Steven Kleinman. In an interview with human rights lawyer Scott Horton for Harper's Magazine, Alexander said\n\nAlexander described Marc Thiessen's 2010 book \"Courting Disaster\", which defended the use of enhanced interrogation techniques, as 'a literary defense of war criminals'.\n"}
{"id": "18575557", "url": "https://en.wikipedia.org/wiki?curid=18575557", "title": "Informatization", "text": "Informatization\n\nInformatization or informatisation refers to the extent by which a geographical area, an economy or a society is becoming information-based, i.e. the increase in size of its information labor force. Usage of the term was inspired by Marc Porat’s categories of ages of human civilization: the Agricultural Age, the Industrial Age and the Information Age (1978). Informatization is to the Information Age what industrialization was to the Industrial Age. It has been stated that:\n\nThe term has mostly been used within the context of national development. Everett Rogers (2000) defines informatization as the process through which new communication technologies are used as a means for furthering development as a nation becomes more and more an information society. However, some observers, such as Alexander Flor (1986) have cautioned about the negative impact of informatization on traditional societies.\n\nRecently, the technological determinism dimension has been highlighted in informatization. Randy Kluver of Texas A&M University defines informatization as the process primarily by which information technologies, such as the World Wide Web and other communication technologies, have transformed economic and social relations to such an extent that cultural and economic barriers are minimized. Kluver expands the concept to encompass the civic and cultural arenas. He believes that it is a process whereby information and communication technologies shape cultural and civic discourse.\n\nG. Wang describes the same phenomenon (1994) which she calls \"informatization\" as a \"process\" of change that features (a) the use of informatization and IT (information technologies) to such an extent that they become the dominant forces in commanding economic, political, social and cultural development; and (b) unprecedented growth in the speed, quantity, and popularity of information production and distribution.\"\n\nThe term informatisation was coined by Simon Nora and Alain Minc in their publication \"L'Informatisation de la société: Rapport à M. le Président de la République\" which was translated in English in 1980 as \"The Computerization of Society: A report to the President of France\". (SAOUG) However, in an article published in 1987 Minc preferred to use informatisation and not computerization.\n\nAfter the 1978 publication the concept was adopted in French, German and English subject literatures and was broadened to include more aspects than only computers and telecommunications (SAOUG).\n\nInformatization has many far-reaching consequences in society. Kim (2004) observes that these include repercussions in economics, politics and other aspects of modern living. In the economic sphere, for example, information is viewed as a focal resource for development, replacing the centrality of labor and capital during the industrial age. In the political arena, there are increased opportunities for participative democracy with the advent of information and communication technology (ICT) that provide easy access to information on varied social and political issues.\n\nIndustrialization propelled transformation of the economic system from agricultural age to modernized economies, and so informatization ushered the industrial age into an information-rich economy. Unlike the agricultural and industrial ages where economics refers to optimization of scarce resources, the information age deals with maximization of abundant resources. Alexander Flor (2008) wrote that informatization gives rise to information-based economies and societies wherein information naturally becomes a dominant commodity or resource. The accumulation and efficient use of knowledge has played a central role in the transformation of the economy (Linden 2004).\n\nOver the years, globalization and informatization have \"redefined industries, politics, cultures, and perhaps the underlying rules of social order\" (Friedman 1999). Although they explain different phenomena, their social, political, economic, and cultural functions remarkably overlaps. \"Although globalization ultimately refers to the integration of economic institutions, much of this integration occurs through the channels of technology. Although international trade is not a new phenomenon, the advent of communications technologies has accelerated the pace and scope of trade\" (Kluver).\n\nKim (2004) proposed to measure the informatization in a country using a composite measure made up of the following extraneous variables: Education, R&D Expenditure, Agricultural Sector and Intellectual Property. Kim also relates increasing democracy as evidence of social informatization. It supposedly take into consideration the three approaches to conceptualizing informatization namely the economic, technological, and stock. Each can be measured with economic data (e.g. GDP), ICT data (e.g. number of computers per population), and amount of information (e.g. number of published technological journals) respectively.\n\nSuch composite measure is similar to the World Bank's Knowledge Assessment Methodology (KAM) Variables (2008) which are clustered into: overall performance of the economy, economic incentive and institutional regime, innovation system, education and human resources, and information and communication technology.\n\nThe measurement for the level of informatization is an ongoing area of development. Among the issues are the ambiguity of the definition of “information” and whether this entity can be quantifiable in contrast to the tangible products of industrialization.\n\nTaylor and Zang (2007) explored the issues behind the limitations of current theoretical models in terms of quantifying the positive impacts of ICT projects, and provided critiques of the information indicators used to gauge and justify informatization projects.\n\nInternational organizations such as the United Nations, through its World Summit on the Information Society (WSIS) and International Telecommunication Union (ITU); and Organisation for Economic Co-operation and Development (OECD) also recognize this challenge and have initiated efforts to improve the methodologies for measuring an “information society”.\n\nInformatization is recognized by states as important to national development. Some states have created laws implementing or regulating informatization.\n\nIn Russia the State Duma enacted the \"Federal Law on Information, Informatization, and the Protection of Information\" on January 25, 1995. It was signed into law by President Boris Yeltsin on February 20, 1995.\n\nAzerbaijan had a \"Law on Information, Informatization and Protection of Information\" in 1998.\n\n\n\n"}
{"id": "14511671", "url": "https://en.wikipedia.org/wiki?curid=14511671", "title": "Interpretation (logic)", "text": "Interpretation (logic)\n\nAn interpretation is an assignment of meaning to the symbols of a formal language. Many formal languages used in mathematics, logic, and theoretical computer science are defined in solely syntactic terms, and as such do not have any meaning until they are given some interpretation. The general study of interpretations of formal languages is called formal semantics.\n\nThe most commonly studied formal logics are propositional logic, predicate logic and their modal analogs, and for these there are standard ways of presenting an interpretation. In these contexts an interpretation is a function that provides the extension of symbols and strings of symbols of an object language. For example, an interpretation function could take the predicate \"T\" (for \"tall\") and assign it the extension {\"a\"} (for \"Abraham Lincoln\"). Note that all our interpretation does is assign the extension {a} to the non-logical constant \"T\", and does not make a claim about whether \"T\" is to stand for tall and 'a' for Abraham Lincoln. Nor does logical interpretation have anything to say about logical connectives like 'and', 'or' and 'not'. Though \"we\" may take these symbols to stand for certain things or concepts, this is not determined by the interpretation function.\n\nAn interpretation often (but not always) provides a way to determine the truth values of sentences in a language. If a given interpretation assigns the value True to a sentence or theory, the interpretation is called a model of that sentence or theory.\n\nA formal language consists of a possibly infinite set of \"sentences\" (variously called \"words\" or \"formulas\") built from a fixed set of \"letters\" or \"symbols\". The inventory from which these letters are taken is called the \"alphabet\" over which the language is defined. To distinguish the strings of symbols that are in a formal language from arbitrary strings of symbols, the former are sometimes called \"well-formed formulæ\" (wff). The essential feature of a formal language is that its syntax can be defined without reference to interpretation. For example, we can determine that (\"P\" or \"Q\") is a well-formed formula even without knowing whether it is true or false.\n\nA formal language formula_1 can be defined with the\nalphabet formula_2, and with a word being in formula_1 if it begins with formula_4 and is composed solely of the symbols formula_4 and formula_6.\n\nA possible interpretation of formula_1 could assign the decimal digit '1' to formula_4 and '0' to formula_6. Then formula_10 would denote 101 under this interpretation of formula_1.\n\nIn the specific cases of propositional logic and predicate logic, the formal languages considered have alphabets that are divided into two sets: the logical symbols (logical constants) and the non-logical symbols. The idea behind this terminology is that \"logical\" symbols have the same meaning regardless of the subject matter being studied, while \"non-logical\" symbols change in meaning depending on the area of investigation.\n\nLogical constants are always given the same meaning by every interpretation of the standard kind, so that only the meanings of the non-logical symbols are changed. Logical constants include quantifier symbols ∀ (\"all\") and ∃ (\"some\"), symbols for logical connectives ∧ (\"and\"), ∨ (\"or\"), ¬ (\"not\"), parentheses and other grouping symbols, and (in many treatments) the equality symbol =.\n\nMany of the commonly studied interpretations associate each sentence in a formal language with a single truth value, either True or False. These interpretations are called \"truth functional\"; they include the usual interpretations of propositional and first-order logic. The sentences that are made true by a particular assignment are said to be \"satisfied\" by that assignment.\n\nNo sentence can be made both true and false by the same interpretation, but it is possible that the truth value of the same sentence can be different under different interpretations. A sentence is \"consistent\" if it is true under at least one interpretation; otherwise it is \"inconsistent\". A sentence φ is said to be \"logically valid\" if it is satisfied by every interpretation (if φ is satisfied by every interpretation that satisfies ψ then φ is said to be a \"logical consequence\" of ψ).\n\nSome of the logical symbols of a language (other than quantifiers) are truth-functional connectives that represent truth functions — functions that take truth values as arguments and return truth values as outputs (in other words, these are operations on truth values of sentences).\n\nThe truth-functional connectives enable compound sentences to be built up from simpler sentences. In this way, the truth value of the compound sentence is defined as a certain truth function of the truth values of the simpler sentences. The connectives are usually taken to be logical constants, meaning that the meaning of the connectives is always the same, independent of what interpretations are given to the other symbols in a formula.\n\nThis is how we define logical connectives in propositional logic:\n\nSo under a given interpretation of all the sentence letters Φ and Ψ (i.e., after assigning a truth-value to each sentence letter), we can determine the truth-values of all formulas that have them as constituents, as a function of the logical connectives. The following table shows how this kind of thing looks. The first two columns show the truth-values of the sentence letters as determined by the four possible interpretations. The other columns show the truth-values of formulas built from these sentence letters, with truth-values determined recursively.\n\nNow it is easier to see what makes a formula logically valid. Take the formula \"F\": (Φ ∨ ¬Φ). If our interpretation function makes Φ True, then ¬Φ is made False by the negation connective. Since the disjunct Φ of \"F\" is True under that interpretation, \"F\" is True. Now the only other possible interpretation of Φ makes it False, and if so, ¬Φ is made True by the negation function. That would make \"F\" True again, since one of \"F\"s disjuncts, ¬Φ, would be true under this interpretation. Since these two interpretations for \"F\" are the only possible logical interpretations, and since \"F\" comes out True for both, we say that it is logically valid or tautologous.\n\nAn \"interpretation of a theory\" is the relationship between a theory and some subject matter when there is a many-to-one correspondence between certain elementary statements of the theory, and certain statements related to the subject matter. If every elementary statement in the theory has a correspondent it is called a \"full interpretation\", otherwise it is called a \"partial interpretation\".\n\nThe formal language for propositional logic consists of formulas built up from propositional symbols (also called sentential symbols, sentential variables, and propositional variables) and logical connectives. The only non-logical symbols in a formal language for propositional logic are the propositional symbols, which are often denoted by capital letters. To make the formal language precise, a specific set of propositional symbols must be fixed.\n\nThe standard kind of interpretation in this setting is a function that maps each propositional symbol to one of the truth values true and false. This function is known as a \"truth assignment\" or \"valuation\" function. In many presentations, it is literally a truth value that is assigned, but some presentations assign truthbearers instead.\n\nFor a language with \"n\" distinct propositional variables there are 2 distinct possible interpretations. For any particular variable \"a\", for example, there are 2=2 possible interpretations: 1) \"a\" is assigned T, or 2) \"a\" is assigned F. For the pair \"a\", \"b\" there are 2=4 possible interpretations: 1) both are assigned T, 2) both are assigned F, 3) \"a\" is assigned T and \"b\" is assigned F, or 4) \"a\" is assigned F and \"b\" is assigned T.\n\nGiven any truth assignment for a set of propositional symbols, there is a unique extension to an interpretation for all the propositional formulas built up from those variables. This extended interpretation is defined inductively, using the truth-table definitions of the logical connectives discussed above.\n\nUnlike propositional logic, where every language is the same apart from a choice of a different set of propositional variables, there are many different first-order languages. Each first-order language is defined by a signature. The signature consists of a set of non-logical symbols and an identification of each of these symbols as a constant symbol, a function symbol, or a predicate symbol. In the case of function and predicate symbols, a natural number arity is also assigned. The alphabet for the formal language consists of logical constants, the equality relation symbol =, all the symbols from the signature, and an additional infinite set of symbols known as variables.\n\nFor example, in the language of rings, there are constant symbols 0 and 1, two binary function symbols + and ·, and no binary relation symbols. (Here the equality relation is taken as a logical constant.)\n\nAgain, we might define a first-order language L, as consisting of individual symbols a, b, and c; predicate symbols F, G, H, I and J; variables x, y, z; no function letters; no sentential symbols.\n\nGiven a signature σ, the corresponding formal language is known as the set of σ-formulas. Each σ-formula is built up out of atomic formulas by means of logical connectives; atomic formulas are built from terms using predicate symbols. The formal definition of the set of σ-formulas proceeds in the other direction: first, terms are assembled from the constant and function symbols together with the variables. Then, terms can be combined into an atomic formula using a predicate symbol (relation symbol) from the signature or the special predicate symbol \"=\" for equality (see the section \"Interpreting equality\" below). Finally, the formulas of the language are assembled from atomic formulas using the logical connectives and quantifiers.\n\nTo ascribe meaning to all sentences of a first-order language, the following information is needed.\nAn object carrying this information is known as a structure ( signature σ), or σ-structure, or \"L\"-structure, or as a \"model\".\n\nThe information specified in the interpretation provides enough information to give a truth value to any atomic formula, after each of its free variables, if any, has been replaced by an element of the domain. The truth value of an arbitrary sentence is then defined inductively using the T-schema, which is a definition of first-order semantics developed by Alfred Tarski. The T-schema interprets the logical connectives using truth tables, as discussed above. Thus, for example, is satisfied if and only if both φ and ψ are satisfied.\n\nThis leaves the issue of how to interpret formulas of the form and . The domain of discourse forms the range for these quantifiers. The idea is that the sentence is true under an interpretation exactly when every substitution instance of φ(\"x\"), where \"x\" is replaced by some element of the domain, is satisfied. The formula is satisfied if there is at least one element \"d\" of the domain such that φ(\"d\") is satisfied.\n\nStrictly speaking, a substitution instance such as the formula φ(\"d\") mentioned above is not a formula in the original formal language of φ, because \"d\" is an element of the domain. There are two ways of handling this technical issue. The first is to pass to a larger language in which each element of the domain is named by a constant symbol. The second is to add to the interpretation a function that assigns each variable to an element of the domain. Then the T-schema can quantify over variations of the original interpretation in which this variable assignment function is changed, instead of quantifying over substitution instances.\n\nSome authors also admit propositional variables in first-order logic, which must then also be interpreted. A propositional variable can stand on its own as an atomic formula. The interpretation of a propositional variable is one of the two truth values \"true\" and \"false.\"\n\nBecause the first-order interpretations described here are defined in set theory, they do not associate each predicate symbol with a property(or relation), but rather with the extension of that property (or relation). In other words, these first-order interpretations are extensional not intensional.\n\nAn example of interpretation formula_12 of the language L described above is as follows.\n\nIn the interpretation formula_12 of L:\n\nAs stated above, a first-order interpretation is usually required to specify a nonempty set as the domain of discourse. The reason for this requirement is to guarantee that equivalences such as\nwhere \"x\" is not a free variable of φ, are logically valid. This equivalence holds in every interpretation with a nonempty domain, but does not always hold when empty domains are permitted. For example, the equivalence\nfails in any structure with an empty domain. Thus the proof theory of first-order logic becomes more complicated when empty structures are permitted. However, the gain in allowing them is negligible, as both the intended interpretations and the interesting interpretations of the theories people study have non-empty domains.\n\nEmpty relations do not cause any problem for first-order interpretations, because there is no similar notion of passing a relation symbol across a logical connective, enlarging its scope in the process. Thus it is acceptable for relation symbols to be interpreted as being identically false. However, the interpretation of a function symbol must always assign a well-defined and total function to the symbol.\n\nThe equality relation is often treated specially in first order logic and other predicate logics. There are two general approaches.\n\nThe first approach is to treat equality as no different than any other binary relation. In this case, if an equality symbol is included in the signature, it is usually necessary to add various axioms about equality to axiom systems (for example, the substitution axiom saying that if \"a\" = \"b\" and \"R\"(\"a\") holds then \"R\"(\"b\") holds as well). This approach to equality is most useful when studying signatures that do not include the equality relation, such as the signature for set theory or the signature for second-order arithmetic in which there is only an equality relation for numbers, but not an equality relation for set of numbers.\n\nThe second approach is to treat the equality relation symbol as a logical constant that must be interpreted by the real equality relation in any interpretation. An interpretation that interprets equality this way is known as a \"normal model\", so this second approach is the same as only studying interpretations that happen to be normal models. The advantage of this approach is that the axioms related to equality are automatically satisfied by every normal model, and so they do not need to be explicitly included in first-order theories when equality is treated this way. This second approach is sometimes called \"first order logic with equality\", but many authors adopt it for the general study of first-order logic without comment.\n\nThere are a few other reasons to restrict study of first-order logic to normal models. First, it is known that any first-order interpretation in which equality is interpreted by an equivalence relation and satisfies the substitution axioms for equality can be cut down to an elementarily equivalent interpretation on a subset of the original domain. Thus there is little additional generality in studying non-normal models. Second, if non-normal models are considered, then every consistent theory has an infinite model; this affects the statements of results such as the Löwenheim–Skolem theorem, which are usually stated under the assumption that only normal models are considered.\n\nA generalization of first order logic considers languages with more than one \"sort\" of variables. The idea is different sorts of variables represent different types of objects. Every sort of variable can be quantified; thus an interpretation for a many-sorted language has a separate domain for each of the sorts of variables to range over (there is an infinite collection of variables of each of the different sorts). Function and relation symbols, in addition to having arities, are specified so that each of their arguments must come from a certain sort.\n\nOne example of many-sorted logic is for planar Euclidean geometry. There are two sorts; points and lines. There is an equality relation symbol for points, an equality relation symbol for lines, and a binary incidence relation \"E\" which takes one point variable and one line variable. The intended interpretation of this language has the point variables range over all points on the Euclidean plane, the line variable range over all lines on the plane, and the incidence relation \"E\"(\"p\",\"l\") holds if and only if point \"p\" is on line \"l\".\n\nA formal language for higher-order predicate logic looks much the same as a formal language for first-order logic. The difference is that there are now many different types of variables. Some variables correspond to elements of the domain, as in first-order logic. Other variables correspond to objects of higher type: subsets of the domain, functions from the domain, functions that take a subset of the domain and return a function from the domain to subsets of the domain, etc. All of these types of variables can be quantified.\n\nThere are two kinds of interpretations commonly employed for higher-order logic. \"Full semantics\" require that, once the domain of discourse is satisfied, the higher-order variables range over all possible elements of the correct type (all subsets of the domain, all functions from the domain to itself, etc.). Thus the specification of a full interpretation is the same as the specification of a first-order interpretation. \"Henkin semantics\", which are essentially multi-sorted first-order semantics, require the interpretation to specify a separate domain for each type of higher-order variable to range over. Thus an interpretation in Henkin semantics includes a domain \"D\", a collection of subsets of \"D\", a collection of functions from \"D\" to \"D\", etc. The relationship between these two semantics is an important topic in higher order logic.\n\nThe interpretations of propositional logic and predicate logic described above are not the only possible interpretations. In particular, there are other types of interpretations that are used in the study of non-classical logic (such as intuitionistic logic), and in the study of modal logic.\n\nInterpretations used to study non-classical logic include topological models, Boolean-valued models, and Kripke models. Modal logic is also studied using Kripke models.\n\nMany formal languages are associated with a particular interpretation that is used to motivate them. For example, the first-order signature for set theory includes only one binary relation, ∈, which is intended to represent set membership, and the domain of discourse in a first-order theory of the natural numbers is intended to be the set of natural numbers.\n\nThe intended interpretation is called the \"standard model\" (a term introduced by Abraham Robinson in 1960). In the context of Peano arithmetic, it consists of the natural numbers with their ordinary arithmetical operations. All models that are isomorphic to the one just given are also called standard; these models all satisfy the Peano axioms. There are also non-standard models of the (first-order version of the) Peano axioms, which contain elements not correlated with any natural number.\n\nWhile the intended interpretation can have no explicit indication in the strictly formal syntactical rules, it naturally affects the choice of the formation and transformation rules of the syntactical system. For example, primitive signs must permit expression of the concepts to be modeled; sentential formulas are chosen so that their counterparts in the intended interpretation are meaningful declarative sentences; primitive sentences need to come out as true sentences in the interpretation; rules of inference must be such that, if the sentence formula_16 is directly derivable from a sentence formula_17, then formula_18 turns out to be a true sentence, with meaning implication, as usual. These requirements ensure that all provable sentences also come out to be true.\n\nMost formal systems have many more models than they were intended to have (the existence of non-standard models is an example). When we speak about 'models' in empirical sciences, we mean, if we want reality to be a model of our science, to speak about an \"intended model\". A model in the empirical sciences is an \"intended factually-true descriptive interpretation\" (or in other contexts: a non-intended arbitrary interpretation used to clarify such an intended factually-true descriptive interpretation.) All models are interpretations that have the same domain of discourse as the intended one, but other assignments for non-logical constants.\n\nGiven a simple formal system (we shall call this one formula_19) whose alphabet α consists only of three symbols formula_20 and whose formation rule for formulas is:\n\nThe single axiom schema of formula_19 is:\n\nA formal proof can be constructed as follows:\n\nIn this example the theorem produced \" formula_30 \" can be interpreted as meaning \"One plus three equals four.\" A different interpretation would be to read it backwards as \"Four minus three equals one.\"\n\nThere are other uses of the term \"interpretation\" that are commonly used, which do not refer to the assignment of meanings to formal languages.\n\nIn model theory, a structure \"A\" is said to interpret a structure \"B\" if there is a definable subset \"D\" of \"A\", and definable relations and functions on \"D\", such that \"B\" is isomorphic to the structure with domain \"D\" and these functions and relations. In some settings, it is not the domain \"D\" that is used, but rather \"D\" modulo an equivalence relation definable in \"A\". For additional information, see Interpretation (model theory).\n\nA theory \"T\" is said to interpret another theory \"S\" if there is a finite extension by definitions \"T\"′ of \"T\" such that \"S\" is contained in \"T\"′.\n\n\n"}
{"id": "12714466", "url": "https://en.wikipedia.org/wiki?curid=12714466", "title": "Involution (philosophy)", "text": "Involution (philosophy)\n\nIn philosophy, involution refers to a situation in which a process or object is ontologically \"turned in\" upon itself.\n\nIn meta-ethics, involution of values is the extension of an otherwise convenient and decent system of values to its inevitable logical conclusion, with an indecent or inconvenient result.\n\n\n"}
{"id": "57158826", "url": "https://en.wikipedia.org/wiki?curid=57158826", "title": "Journal of Family Violence", "text": "Journal of Family Violence\n\nThe Journal of Family Violence is a quarterly peer-reviewed scientific journal dedicated to the study of family violence. It was established in 1986 and is published by Springer Science+Business Media. The editor-in-chief is Rebecca J. Macy (UNC School of Social Work). According to the \"Journal Citation Reports\", the journal has a 2016 impact factor of 0.871.\n"}
{"id": "38164457", "url": "https://en.wikipedia.org/wiki?curid=38164457", "title": "Levering Act", "text": "Levering Act\n\nThe Levering Act was a law enacted by the U.S. state of California in 1950. It required state employees to subscribe to a loyalty oath that specifically disavowed radical beliefs. It was aimed in particular at employees of the University of California. Several teachers lost their positions when they refused to sign loyalty oaths.\n\nBeginning with the onset of the Cold War in the years following World War II, government officials at all levels of government in the United States feared Soviet infiltration that might influence public opinion and frustrate the efforts of the United States to counter Soviet influence. Several laws passed and programs established during the Truman administration enhanced the federal government's authority to investigate those suspected of disloyalty and, in particular, to prevent their employment by the federal government. Individual states enacted similar anti-subversion statutes.\n\nIn the late 1940s, California state employees were already required to take a general oath indicating support for the Constitutions of California and the U.S., though the requirement did not extend to employees of the quasi-independent University of California. That would require legislation to enhance the state's authority over employees of the state university. Senator Jack B. Tenney, chairman of the legislature's Committee on Un-American Activities, submitted several loyalty oath bills along with a dozen other anti-subversive proposals. In response, Robert Sproul, president of the University of California, decided on his own initiative to forestall legislative action by requiring university employees to take such an oath. It read:\n\nThe California Constitution specified that no oath other than the basic statement of loyalty to the state and federal constitutions could be required of state employees. The Levering Act, named for Harold K. Levering, the Republican legislator who drafted it and managed its passage in the course of 1949-50, was designed to remedy that by classifying public employees as civil defense workers and using that as a rationale for requiring the new oath. The Levering Act required all employees of the state of California to take the new anti-radical loyalty oath. \n\nThe California State Federation of Teachers said in 1950:\nRepublican Governor Earl Warren initially opposed the legislation. The University's Regents fired 31 tenured professors who refused to sign the oath on grounds of academic freedom. Warren decided to support the oath during his 1950 campaign for re-election. \n\nIn October 1952, the California Supreme Court reinstated university teachers who had been fired by the university before the Act's passage for refusing to sign the oath required by the University Regents. The court found that the Regents had exceeded their authority in imposing the oath as a condition of employment. The 18 teachers whose dismissals were at issue needed to take the oath required by the Levering Act in order to be reinstated. The case was brought by Stanley Weigel, a Republican, later member of the national committee of the ACLU and Kennedy appointment to the federal bench. \n\nIn 1953, the Supreme Court of the United States declined to hear an appeal by one of the dismissed teachers, Professor Leonard T. Pockman of San Francisco State College. The order the court issue said that the case involved no substantial federal question. \n\nIn 1967, the California Supreme Court ruled in a 6-1 decision that the Levering Act was unconstitutional. Suits on the part of individuals went on for years. Albert E. Monroe won some of the benefits he lost upon his 1950 dismissal in 1972. \n\nSuch oaths have occasionally been a point of controversy. In 2008, a Quaker teacher was fired by California State University East Bay because she edited her loyalty oath by writing \"non-violently\" in front of \"support and defend [the U.S. and state Constitutions] against all enemies, foreign and domestic.\" The office of the California Attorney General said that \"as a general matter, oaths may be modified to conform with individual values\", suggesting that the teacher's modification was acceptable.\n\n\n"}
{"id": "1967935", "url": "https://en.wikipedia.org/wiki?curid=1967935", "title": "Mark Felt", "text": "Mark Felt\n\nWilliam Mark Felt Sr. (August 17, 1913 – December 18, 2008) was a Federal Bureau of Investigation (FBI) special agent and Associate Director, the Bureau's second-highest-ranking post, from May 1972 until his retirement from the FBI in June 1973. During his time as Associate Director, Felt served as an anonymous informant, nicknamed \"Deep Throat\", to reporters Bob Woodward and Carl Bernstein of \"The Washington Post\". He provided them with critical information about the Watergate scandal, which ultimately led to the resignation of President Richard Nixon in 1974. \n\nThough Felt's identity as Deep Throat was strongly suspected by some in Washington, D.C., including Nixon himself, it generally remained a secret for 30 years. In 2005, Felt finally acknowledged that he was Deep Throat, after being persuaded by his daughter to reveal his identity.\n\nFelt worked in several FBI field offices prior to his promotion to the Bureau's headquarters. In 1980, he was convicted of having violated the civil rights of people thought to be associated with members of the Weather Underground, by ordering FBI agents to break into their homes and search the premises as part of an attempt to prevent bombings. He was ordered to pay a fine, but was pardoned by President Ronald Reagan during his appeal.\n\nFelt published two memoirs: \"The FBI Pyramid\" in 1979 (updated in 2006), and \"A G-Man's Life\", written with John O'Connor, in 2006. In 2012, the FBI released Felt's personnel file, covering the period from 1941 to 1978. It also released files pertaining to an extortion threat made against Felt in 1956.\n\nBorn on August 17, 1913, in Twin Falls, Idaho, Felt was the son of Rose R. Dygert and Mark Earl Felt, a carpenter and building contractor. His paternal grandfather was a Free Will Baptist minister. His maternal grandparents were born in Canada and Scotland. Through his maternal grandfather, Felt was descended from Revolutionary War general Nicholas Herkimer of New York.\n\nAfter graduating from Twin Falls High School in 1931, Felt attended the University of Idaho in Moscow. He was a member and president of the Gamma Gamma chapter of the Beta Theta Pi fraternity. He received a Bachelor of Arts degree in 1935.\n\nFelt then went to Washington, D.C., to work in the office of Democratic U.S. Senator James P. Pope. In 1938, Felt married Audrey Robinson of Gooding, Idaho, whom he had known when they were students at the University of Idaho. She had come to Washington to work at the Bureau of Internal Revenue. Their wedding was officiated by the chaplain of the United States House of Representatives, the Rev. Sheara Montgomery. Audrey died in 1984; she and Felt had two children, Joan and Mark.\n\nFelt stayed on with Pope's successor in the Senate, David Worth Clark (D-Idaho). He attended the George Washington University Law School at night, earning his law degree in 1940, and was admitted to the District of Columbia Bar in 1941.\n\nUpon graduation, Felt took a position at the Federal Trade Commission but did not enjoy his work. His workload was very light, and he was assigned to investigate whether a toilet paper brand, called \"Red Cross\", was misleading consumers into thinking it was endorsed by the American Red Cross. Felt wrote in his memoir:\nMy research, which required days of travel and hundreds of interviews, produced two definite conclusions:\n\n1. Most people \"did\" use toilet tissue.\n\n2. Most people \"did not\" appreciate being asked about it.\n\nThat was when I started looking for other employment.\n\nHe applied for a job with the FBI in November 1941 and was accepted. His first day at the Bureau was January 26, 1942.\n\nFBI Director J. Edgar Hoover often moved Bureau agents around so they would have wide experience in the field. This was typical of other agencies and corporations of the time. Felt observed that Hoover \"wanted every agent to get into any field office at any time. Since he [Hoover] had never been transferred and did not have a family, he had no idea of the financial and personal hardship involved.\"\n\nAfter completing 16 weeks of training at the FBI Academy in Quantico, Virginia, and FBI Headquarters in Washington, Felt was assigned to Texas, spending three months each in the field offices in Houston and San Antonio. He returned to FBI Headquarters, where he was assigned to the Espionage Section of the Domestic Intelligence Division, tracking down spies and saboteurs during World War II. He worked on the Major Case Desk. His most notable work was on the \"Peasant\" case. Helmut Goldschmidt, operating under the codename \"Peasant\", was a German agent in custody in England. Under Felt's direction, his German masters were informed that \"Peasant\" had made his way to the United States, and thus were fed disinformation on Allied plans.\n\nThe Espionage Section was abolished in May 1945 after V-E Day. After the war, Felt was assigned to the Seattle field office. After two years of general work, he spent two years as a firearms instructor and was promoted from agent to supervisor. Upon passage of the Atomic Energy Act and the creation of the United States Atomic Energy Commission, the Seattle office became responsible for completing background checks of workers at the Hanford plutonium plant near Richland, Washington. Felt oversaw those investigations. In 1954, Felt returned briefly to Washington as an inspector's aide. Two months later, he was sent to New Orleans as Assistant Special Agent-in-Charge of the field office. When he was transferred to Los Angeles fifteen months later, he held the same rank there.\n\nIn 1956, Felt was transferred to Salt Lake City and promoted to Special Agent-in-Charge. The Salt Lake City office included Nevada within its purview, and Felt oversaw some of the Bureau's earliest investigations into organized crime, assessing the mob's operations in the Reno and Las Vegas casinos. (It was Hoover's, and therefore the Bureau's, official position at the time that there was no such thing as the Mob.) In February 1958, Felt was assigned to Kansas City, Missouri (which he dubbed \"the Siberia of field offices\" in his memoir), where he directed further investigations of organized crime. By this time, Hoover had come to believe in organized crime, in the wake of the famous Apalachin, New York, conclave of underworld bosses in November 1957.\n\nFelt returned to Washington, D.C., in September 1962. As assistant to the bureau's assistant director in charge of the Training Division, Felt helped oversee the FBI Academy. In November 1964, he was promoted to an Assistant Director of the Bureau, as Chief Inspector of the Bureau and Head of the Inspection Division. This division oversaw compliance with Bureau regulations and conducted internal investigations.\n\nOn July 1, 1971, Felt was promoted by Hoover to Deputy Associate Director, assisting Associate Director Clyde Tolson. Hoover's right-hand man for decades, Tolson was in failing health and unable to carry out his duties. Richard Gid Powers wrote that Hoover installed Felt to rein in William C. Sullivan's domestic spying operations, as Sullivan had been engaged in secret unofficial work for the White House. In his memoir, Felt quoted Hoover as having said, \"I need someone who can control Sullivan. I think you know he has been getting out of hand.\" In his book, \"The Bureau\", Ronald Kessler said that Felt \"managed to please Hoover by being tactful with him and tough on agents.\" Curt Gentry described Felt as \"the director's latest fair-haired boy\", who had \"no inherent power\" in his new post, the real number three being John P. Mohr.\n\nAmong the criminal groups that the FBI investigated in the early 1970s was the Weather Underground. Their case was dismissed by the court because it concluded that the FBI had conducted illegal activities, including unauthorized wiretaps, break-ins, and mail interceptions. The lead federal prosecutor on the case, William C. Ibershof, claims that Mark Felt and Attorney General John N. Mitchell initiated these illegal activities that tainted the investigation.\n\nHoover died in his sleep and was found on the morning of May 2, 1972. Tolson was nominally in charge until the next day, when Nixon appointed L. Patrick Gray III as Acting FBI Director. Tolson submitted his resignation, which Gray accepted. Felt succeeded to Tolson's post as Associate Director, the number-two job in the Bureau. Felt served as an honorary pallbearer at Hoover's funeral.\nOn the day of Hoover's death, Hoover's secretary for five decades, Helen Gandy, began destroying his files. She turned over twelve boxes of the \"Official/Confidential\" files to Felt on May 4, 1972. This consisted of 167 files and 17,750 pages, many of them containing derogatory information about individuals whom Hoover had investigated. He used his information as power over them. Felt stored the files in his office.\n\nThe existence of such files had long been rumored. Gray told the press that afternoon that \"there are no dossiers or secret files. There are just general files and I took steps to preserve their integrity.\" Felt earlier that day had told Gray, \"Mr. Gray, the Bureau doesn't have any secret files\", and later accompanied Gray to Hoover's office. They found Gandy boxing up papers. Felt said Gray \"looked casually at an open file drawer and approved her work\", though Gray would later deny he looked at anything. Gandy retained Hoover's \"Personal File\" and destroyed it.\n\nWhen Felt was called to testify in 1975 by the U.S. House about the destruction of Hoover's papers, he said, \"There's no serious problems if we lose some papers. I don't see anything wrong and I still don't.\" At the same hearing, Gandy claimed that she had destroyed Hoover's personal files only after receiving Gray's approval. In a letter submitted to the committee in rebuttal of Gandy's testimony, Gray vehemently denied ever giving such permission. Both Gandy's testimony and Gray's letter were included in the committee's final report.\n\nIn his memoir, Felt expressed mixed feelings about Gray. He was the first person appointed as head of the FBI who had no experience in the agency, but he had experience in the Navy. While noting Gray did work hard, Felt was critical of how often he was away from FBI headquarters. Gray lived in Stonington, Connecticut, and commuted to Washington. He also visited all of the Bureau's field offices except Honolulu. His frequent absences led to the nickname \"Three-Day Gray\". These absences, combined with Gray's hospitalization and recuperation from November 20, 1972, to January 2, 1973, meant that Felt was effectively in charge for much of his final year at the Bureau. Bob Woodward wrote \"Gray got to be director of the FBI and Felt did the work.\" Felt wrote in his memoir:\n\nThe record amply demonstrates that President Nixon made Pat Gray the Acting Director of the FBI because he wanted a politician in J. Edgar Hoover's position who would convert the Bureau into an adjunct of the White House machine.\n\nGray's defenders would later argue that Gray had practiced a management style that was different from that of Hoover. Gray's program of field office visits was something that Hoover had not done since his early years as director; some believed that Gray's visits helped raise the morale of the field agents. Gray's leadership style seemed to continue what he had learned in the US Navy, in which the executive officer concentrates on the basic operation of the ship, while the captain concentrates on its position and heading. Felt believed Gray's methods were an unnecessary distraction from the work of the FBI and showed a lack of leadership. He believed that he was not the only career manager at the FBI who disapproved of Gray's methods, particularly among those who had served under Hoover.\n\nAs Associate Director, Felt saw everything compiled on Watergate before it was given to Gray. The Agent in Charge, Charles Nuzum, sent his findings to Investigative Division Head Robert Gebhardt, who passed the information on to Felt. From the day of the break-in, June 17, 1972, until the FBI investigation was mostly completed in June 1973, Felt was the key control point for FBI information. He had been among the first to learn of the investigation, being informed the morning of June 17. Ronald Kessler, who spoke to former Bureau agents, reported that throughout the investigation, they \"were amazed to see material in Woodward and Bernstein's stories lifted almost verbatim from their reports of interviews a few days or weeks earlier\".\n\nBob Woodward first describes his source, nicknamed \"Deep Throat\", in \"All the President's Men\", as a \"source in the Executive Branch who had access to information at CRP (the Committee to Re-elect the President, Nixon's 1972 campaign organization), as well as at the White House.\" In the book, Deep Throat is described as an \"incurable gossip\" who was \"in a unique position to observe the Executive Branch\", a man \"whose fight had been worn out in too many battles\". Woodward had known the source before Watergate and had discussed politics and government with him.\n\nIn 2005, Woodward wrote that he first met Felt at the White House in 1969 or 1970. Woodward was working as an aide to Admiral Thomas Hinman Moorer, Chairman of the Joint Chiefs of Staff, and was delivering papers to the White House Situation Room. In his book \"The Secret Man\", Woodward described Felt as a \"tall man with perfectly combed gray hair ... distinguished looking\" with a \"studied air of confidence, even what might be called a command presence\". They stayed in touch and spoke on the telephone several times. When Woodward started working at the \"Washington Post\", he phoned Felt on several occasions to ask for information for articles in the paper. Felt's information, taken on a promise that Woodward would never reveal its origin, was a source for a few stories, notably for an article on May 18, 1972, about Arthur Bremer, who shot George Wallace.\n\nWhen the Watergate story broke, Woodward called on Felt. The senior manager advised Woodward on June 19 that E. Howard Hunt, who had ties to Nixon, was involved; the telephone number of his White House office had been listed in the address book of one of the burglars. Initially, Woodward's source was known at the \"Post\" as \"My Friend\". \"Post\" editor Howard Simons tagged him as \"Deep Throat\", after the widely known porno film \"Deep Throat\". According to Woodward, Simons thought of the term because Felt had been providing information on a deep background basis.\n\nWhen Felt revealed his role in 2005, it was noted that \"My Friend\" has the same initial letters as \"Mark Felt\". Woodward's notes from interviewing Felt were marked \"M.F.\", which Woodward says was \"not very good tradecraft.\"\n\nWoodward explained that when he wanted to meet Deep Throat, he would move a flowerpot with a red flag on his apartment balcony; he lived at number 617, Webster House, 1718 P Street, Northwest. On occasions when Deep Throat wanted a meeting, he would circle the page number on page twenty of Woodward's copy of \"The New York Times\" (delivered to his building) and draw clock hands to signal the hour. Adrian Havill questioned these claims in his 1993 biography of Woodward and Bernstein. He said Woodward's balcony faced an interior courtyard and was not visible from the street. Woodward said that the courtyard had been bricked in since he lived there. Havill also said \"The Times\" was not delivered in copies marked by apartment, but Woodward and a former neighbor disputed this claim.\n\nWoodward said:\nDays after the break-in, Nixon and White House chief of staff H. R. Haldeman talked about putting pressure on the FBI to slow down the investigation. The District of Columbia police had called in the FBI because they found the burglars had wiretapping equipment. Wiretapping is a crime investigated by the FBI. Haldeman told President Nixon on June 23, 1972, that Felt would \"want to cooperate because he's ambitious.\" These tapes were not declassified and revealed for some time.\n\nHaldeman later initially suspected lower-level FBI agents, including Angelo Lano, of speaking to the \"Post\". But in a taped conversation on October 19, 1972, Haldeman told the president that sources had said that Felt was speaking to the press.\n\nYou can't say anything about this because it will screw up our source and there's a real concern. Mitchell is the only one who knows about this and he feels strongly that we better not do anything because ... if we move on him, he'll go out and unload everything. He knows everything that's to be known in the FBI. He has access to absolutely everything.\n\nHaldeman also said that he had spoken to White House counsel John W. Dean about punishing Felt, but Dean said Felt had committed no crime and could not be prosecuted.\n\nWhen Acting FBI Director Gray returned from his sick leave in January 1973, he confronted Felt about being the source for Woodward and Bernstein. Gray said he had defended Felt to Attorney General Richard G. Kleindienst: \"You know, Mark, Dick Kleindienst told me I ought to get rid of you. He says White House staff members are concerned that you are the FBI source of leaks to Woodward and Bernstein\". Felt replied, \"Pat, I haven't leaked anything to anybody.\" Gray told Felt:\n\nI told Kleindienst that you've worked with me in a very competent manner and I'm convinced that you are completely loyal. I told him I was not going to move you out. Kleindienst told me, \"Pat, I love you for that.\"\n\nOn February 17, 1973, Nixon nominated Gray as Hoover's permanent replacement as Director. Until then, Gray had been in limbo as Acting Director. In another taped conversation on February 28, Nixon spoke to Dean about Felt's acting as an informant, and mentioned that he had never met him. Gray was forced to resign on April 27, after it was revealed that he had destroyed a file that had been in the White House safe of E. Howard Hunt. Gray recommended Felt as his successor.\n\nThe day Gray resigned, Kleindienst spoke to Nixon, urging him to appoint Felt as head of the FBI. Nixon instead appointed William Ruckelshaus as Acting Director. Stanley Kutler reported that Nixon said, \"I don't want him. I can't have him. I just talked to Bill Ruckelshaus and Bill is a Mr. Clean and I want a fellow in there that is not part of the old guard and that is not part of that infighting in there.\" On another White House tape, from May 11, 1973, Nixon and White House Chief of Staff, Alexander M. Haig, spoke of Felt leaking material to \"The New York Times\". Nixon said, \"he's a bad guy, you see.\" He said that William Sullivan had told him of Felt's ambition to be Director of the Bureau.\n\nFelt called his relationship with Ruckelshaus \"stormy.\" In his memoir, Felt describes Ruckelshaus as a \"security guard sent to see that the FBI did nothing which would displease Mr. Nixon.\"\n\nIn mid-1973, \"The New York Times\" published a series of articles about wiretaps that had been ordered by J. Edgar Hoover during his tenure at the FBI. Ruckelshaus believed that the information must have come from someone at the FBI.\n\nIn June 1973, Ruckelshaus received a call from someone claiming to be a \"New York Times\" reporter, telling him that Felt was the source of this information. On June 21, Ruckelshaus met privately with Felt and accused him of leaking information to \"The New York Times\", a charge that Felt adamantly denied. Ruckelshaus told Felt to \"sleep on it\" and let him know the next day what he wanted to do. Felt resigned from the Bureau the next day, June 22, 1973, ending his thirty-one year career.\n\nIn a 2013 interview, Ruckelshaus noted the possibility that the original caller was a hoax. He said that he considered Felt's resignation \"an admission of guilt\" anyway.\n\nRuckelshaus, who had served only as Acting Director, was replaced several weeks later by Clarence M. Kelley, who had been nominated by Nixon as FBI Director and confirmed by the U.S. Senate.\n\nIn the early 1970s, Felt had supervised Operation COINTELPRO, initiated by Hoover in the 1950s. This period of FBI history has generated great controversy for its abuses of private citizens' rights. The FBI was spying on, infiltrating, and disrupting the Civil Rights Movement, Anti-War Movement, Black Panthers, and other New Left groups. By 1972 Felt was heading the investigation into the Weather Underground, which had planted bombs at the Capitol, the Pentagon, and the State Department building. Felt, along with Edward S. Miller, authorized FBI agents to break into homes secretly in 1972 and 1973, without a search warrant, on nine separate occasions. These kinds of FBI operations were known as \"black bag jobs.\" The break-ins occurred at five addresses in New York and New Jersey, at the homes of relatives and acquaintances of Weather Underground members. They did not contribute to the capture of any fugitives. The use of \"black bag jobs\" by the FBI was declared unconstitutional by the United States Supreme Court in the \"Plamondon\" case, 407 U.S. 297 (1972).\n\nThe Church Committee of Congress revealed the FBI's illegal activities, and many agents were investigated. In 1976, Felt publicly stated he had ordered break-ins, and recommended against punishment of individual agents who had carried out orders. Felt also stated that Patrick Gray had also authorized the break-ins, but Gray denied this. Felt said on the CBS television program \"Face the Nation\" he would probably be a \"scapegoat\" for the Bureau's work. \"I think this is justified and I'd do it again tomorrow,\" he said on the program. While admitting the break-ins were \"extralegal\", he justified them as protecting the \"greater good.\" Felt said:\n\nTo not take action against these people and know of a bombing in advance would simply be to stick your fingers in your ears and protect your eardrums when the explosion went off and then start the investigation.\n\nGriffin B. Bell, the Attorney General in the Jimmy Carter administration, directed investigation of these cases. On April 10, 1978, a federal grand jury charged Felt, Miller, and Gray with conspiracy to violate the constitutional rights of American citizens by searching their homes without warrants.\n\nThe indictment charged violations of Title 18, Section 241 of the United States Code and stated Felt and the others:\n\nDid unlawfully, willfully, and knowingly combine, conspire, confederate, and agree together and with each other to injure and oppress citizens of the United States who were relatives and acquaintances of the Weatherman fugitives, in the free exercise and enjoyments of certain rights and privileges secured to them by the Constitution and the laws of the United States of America.\n\nFelt told his biographer Ronald Kessler: I was shocked that I was indicted. You would be too, if you did what you thought was in the best interests of the country and someone on technical grounds indicted you.\n\nFelt, Gray, and Miller were arraigned in Washington, DC on April 20. Seven hundred current and former FBI agents were outside the courthouse applauding the \"Washington Three\", as Felt referred to himself and his colleagues in his memoir. Gray's case did not go to trial and was dropped by the government for lack of evidence, on December 11, 1980.\n\nFelt and Miller attempted to plea bargain with the government, willing to agree to a misdemeanor guilty plea to conducting searches without warrants—a violation of . The government rejected the offer in 1979. After eight postponements, the case against Felt and Miller went to trial in the United States District Court for the District of Columbia on September 18, 1980. On October 29, former President Richard M. Nixon appeared as a rebuttal witness for the defense. He testified that in authorizing the Bureau to conduct break-ins to gather foreign intelligence information \"he was acting on precedents established by a number of Presidential directives dating to 1939.\" It was Nixon's first courtroom appearance since his resignation in 1974. Nixon also contributed money to Felt's defense fund, since Felt's legal expenses were running over $600,000 by then. Also testifying were former Attorneys General Mitchell, Kleindienst, Herbert Brownell Jr., Nicholas Katzenbach, and Ramsey Clark, all of whom said warrantless searches in national security matters were commonplace and understood not to be illegal. Mitchell and Kleindienst denied they had authorized any of the break-ins at issue in the trial. (The Bureau used a national security justification for the searches because it alleged the Weather Underground was in the employ of Cuba.)\n\nThe jury returned guilty verdicts on November 6, 1980. Although the charge carried a maximum sentence of 10 years in prison, Felt was fined $5,000 and Miller was fined $3,500. Writing an OpEd piece in \"The New York Times\" a week after the conviction, attorney Roy Cohn claimed that Felt and Miller were being used as scapegoats by the Carter administration and it was an unfair prosecution. Cohn wrote the break-ins were the \"final dirty trick\" of the Nixon administration, and there had been no \"personal motive\" to their actions. \"The New York Times\" praised the convictions, saying \"the case has established that zeal is no excuse for violating the Constitution.\"\n\nFelt and Miller appealed their verdicts.\n\nIn a phone call on January 30, 1981, Edwin Meese encouraged President Ronald Reagan to issue a pardon. After further encouragement from Felt's former colleagues, President Reagan pardoned Felt and Miller. The pardon was signed on March 26, but was not announced to the public until April 15, 1981.\n\nIn the pardon, Reagan wrote:\n\nNixon sent Felt and Miller bottles of champagne with the note \"Justice ultimately prevails.\" \"The New York Times\" disapproved in an editorial, saying that the United States \"deserved better than a gratuitous revision of the record by the President.\" Felt and Miller said they would seek repayment of their legal fees from the government.\n\nThe prosecutor at the trial, John W. Nields Jr., said, \"I would warrant that whoever is responsible for the pardons did not read the record of the trial and did not know the facts of the case.\" Nields also complained that the White House did not consult with the prosecutors in the case, which was contrary to the usual practice when a pardon was under consideration.\n\nFelt said, \n\nI feel very excited and just so pleased that I can hardly contain myself. I am most grateful to the President. I don't know how I'm ever going to be able to thank him. It's just like having a heavy burden lifted off your back. This case has been dragging on for five years.\n\nAt a press conference the day of the announcement, Miller said, \"I certainly owe the Gipper one.\" Carter Attorney General Griffin Bell said he did not object to the pardons, as the convictions had upheld constitutional principles.\n\nDespite their pardons, Felt and Miller won permission from the United States Court of Appeals for the District of Columbia Circuit to appeal their convictions so as to remove it from their record and to prevent it from being used in civil suits by victims of the break-ins they had ordered. Ultimately, the court restored Felt's law license in 1982, based on Reagan's pardon. In June 1982, Felt and Miller testified before the Senate Judiciary Committee's security and terrorism subcommittee. They said that the restrictions placed on the FBI by Attorney General Edward H. Levi were threatening the country's safety.\n\nHis daughter Joan graduated from high school in Kansas City during his assignment there and attended the University of Kansas for two years before transferring to Stanford in California to study drama. When she was an undergraduate, Felt finally settled in Alexandria, Virginia, when he took his post at the FBI Academy.\n\nPrior to the Watergate scandal, Felt had become estranged from Joan. They had been close during her childhood, but after she graduated from Stanford, she had gone to Chile under a Fulbright scholarship to continue her studies. While there, she became friends with Marxist revolutionary Andrés Pascal Allende, nephew of future president Salvador Allende. When she returned home, her political views had shifted to the extreme left, putting her in conflict with her conservative father.\n\nShe earned her master's degree in Spanish at Stanford, and then joined a hippie community in the Santa Cruz Mountains. Felt and his wife went to visit her once and were appalled at her counterculture lifestyle and use of drugs; he was reminded of members of the militant Weather Underground that the FBI had been prosecuting. Joan's friends were equally shocked that her father was an FBI agent. Following their visit, Joan cut off most contact with her parents. As a result, and combined with the fact that she did not follow the news, she was unaware of her father's legal problems that arose from the Watergate scandal.\n\nOver the years, the stress of following her husband's career as well as from the separation from her daughter, combined with Felt's prosecution had taken their toll on Audrey. During Felt's time in Seattle in 1954, Audrey suffered a nervous breakdown. She developed a dependency on alcohol and had been taking antidepressants for years. She had also been hospitalized several times for various ailments. When Felt was put on trial in 1980, she attended the first day, but did not return because she was unable to bear it. In 1984, she committed suicide using Felt's revolver. Felt and his son Mark Jr., an officer in the United States Air Force, decided to keep this a secret and told Joan that her mother had died of a heart attack. Joan did not learn the truth about her mother until 2001.\n\nMeanwhile, Joan had become an adherent of Adi Da, who had founded a new religious movement in San Francisco called Adidam, and she was living in Santa Rosa. She had borne three sonsLudi (later Will), Rob, and Nick, the latter two from another Adidam devotee whom she never marriedbut her parents had only met Ludi during their visit in 1974. After Audrey's death, Felt began making yearly visits to see Joan and his grandsons, and they also came to visit him and his new girlfriend, who lived in the same apartment complex.\n\nIn 1990, Felt permanently moved to Santa Rosa, leaving behind his entire life in Alexandria. He bought a house where he lived with Joan, and took care of the boys while she worked, teaching at Sonoma State University and Santa Rosa Junior College. He suffered a stroke before 1999, as reported by Kessler in his book \"The Bureau.\" According to Kessler's book, in the summer of 1999, Woodward showed up unexpectedly at the Santa Rosa home and took Felt to lunch.\n\nJoan, who was caring for her father, told Kessler that her father had greeted Woodward like an old friend. Their meeting appeared to be more of a celebration than an interview. \"Woodward just showed up at the door and said he was in the area,\" Joan Felt was quoted as saying in Kessler's book, which was published in 2002. \"He came in a white limousine, which parked at a schoolyard about ten blocks away. He walked to the house. He asked if it was okay to have a martini with my father at lunch, and I said it would be fine.\"\n\nFelt published his memoir \"The FBI Pyramid: From the Inside\" in 1979. It was co-written with Hoover biographer Ralph de Toledano, though the latter's name appears only in the copyright notice. Toledano in 2005 wrote that the volume was \"largely written by me since his original manuscript read like \"The Autocrat of the Breakfast-Table\".\" Toledano said: Felt swore to me that he was not Deep Throat, and that he had never leaked information to the Woodward-Bernstein team or anyone else. The book was published and bombed.\n\nIn his memoir, Felt strongly defended Hoover and his tenure as Director; he condemned the criticisms of the Bureau made in the 1970s by the Church Committee and civil libertarians. He also denounced the treatment of Bureau agents as criminals and said the Freedom of Information Act and Privacy Act of 1974 served only to interfere with government work and helped criminals. (He opens the book with the sentence, \"The Bill of Rights is not a suicide pact\", Justice Robert H. Jackson's comment in his dissent to \"Terminiello v. City of Chicago\", 337 U.S. 1 (1949)).\n\n\"Library Journal\" wrote in its review that \"at one time Felt was assumed to be Watergate's 'Deep Throat'; in this interesting but hardly sensational memoir, he makes it clear that that honor, if honor it be, lies elsewhere.\" \"The New York Times Book Review\" was highly critical of the book, saying Felt \"seeks to perpetuate a view of Hoover and the FBI that is no longer seriously peddled even on the backs of cereal boxes\". It said the book contained \"a disturbing number of factual errors\". Curt Gentry said that Felt was \"the keeper of the Hoover flame.\"\n\nKessler said in his book that the measures Woodward took to conceal his meeting with Felt lent \"credence\" to the notion that Felt was Deep Throat. Woodward confirmed that Felt was Deep Throat in 2005. \"There are plenty of people claiming they knew Deep Throat was actually former FBI man Mark Felt ... On May 3, 2002, PAGE SIX reported that Ronald Kessler, author of \"The Bureau: The Secret History of the FBI\", says that all the evidence points to former top FBI official W. Mark Felt.\"\n\nThe identity of Deep Throat was debated for more than three decades, and Felt was frequently mentioned as a possibility. An October 1990 \"Washingtonian\" magazine article about \"Washington secrets\" listed the 15 most prominent Deep Throat candidates, including Felt.\n\nJack Limpert published evidence as early as 1974 that Felt was the informant. On June 25 of that year, a few weeks after \"All the President's Men\" was published, \"The Wall Street Journal\" ran an editorial, \"If You Drink Scotch, Smoke, Read, Maybe You're Deep Throat.\" It began \"W. Mark Felt says he isn't now, nor has he ever been Deep Throat.\" The \"Journal\" quoted Felt saying the character was a \"composite\" and \"I'm just not that kind of person.\" In 1975, George V. Higgins wrote: \"Mark Felt knows more reporters than most reporters do, and there are some who think he had a \"Washington Post\" alias borrowed from a dirty movie.\" During a grand jury investigation in 1976, Felt was called to testify. The prosecutor, J. Stanley Pottinger, Assistant Attorney General for Civil Rights, discovered that Felt was \"Deep Throat\", but the secrecy of the proceedings protected the information from being public.\n\nIn 1992, James Mann, who had been a reporter at \"The Washington Post\" in 1972 and worked with Woodward, wrote a piece for \"The Atlantic Monthly\", saying the source had to have been within the FBI. He noted Felt as a possibility, but said he could not confirm this.\n\nAlexander P. Butterfield, the White House aide best known for revealing Nixon's taping system, told the \"Hartford Courant\" in 1995, \"I think it was a guy named Mark Felt.\" In July 1999, Felt was identified as Deep Throat by the \"Hartford Courant\", citing Chase Culeman-Beckman, a nineteen-year-old from Port Chester, New York. Culeman-Beckman said Jacob Bernstein, the son of Carl Bernstein and Nora Ephron, had told him the name at summer camp in 1988, and that Jacob claimed he had been told by his father. Felt said to the \"Courant\", \"No, it's not me. I would have done better. I would have been more effective. Deep Throat didn't exactly bring the White House crashing down, did he?\" Bernstein said his son didn't know. \"Bob and I have been wise enough never to tell our wives, and we've certainly never told our children.\" (Bernstein reiterated on June 2, 2005, on the \"Today Show\" that his wife had never known.)\n\nLeonard Garment, President Nixon's former law partner who became White House counsel after John W. Dean's resignation, ruled Felt out as Deep Throat in his 2000 book \"In Search of Deep Throat\". Garment wrote:\n\nThe Felt theory was a strong one ... Felt had a personal motive for acting. After the death of J. Edgar Hoover ... Felt thought he was a leading candidate to succeed Hoover ... The characteristics were a good fit. The trouble with Felt's candidacy was that Deep Throat in \"All the President's Men\" simply did not sound to me like a career FBI man.\n\nGarment said the information leaked to Woodward was inside White House information to which Felt would not have had access. \"Felt did not fit.\" (Once the secret was revealed, it was noted Felt did have access to such information because the Bureau's agents were interviewing high-ranking White House officials.)\n\nIn 2002, the \"San Francisco Chronicle\" profiled Felt. Noting his denial in \"The FBI Pyramid\", the paper wrote:\n\nCuriously, his son—American Airlines pilot Mark Felt—now says that shouldn't be read as a definitive denial, and that he plans to answer the question once-and-for-all in a second memoir. The excerpt of the working draft obtained by the \"Chronicle\" has Felt still denying he's Throat but providing a rationale for why Throat did the right thing.\n\nIn February 2005, reports surfaced that Woodward had prepared Deep Throat's obituary because he was near death. Chief Justice William H. Rehnquist was battling cancer at the time (he would die in September 2005), and there was speculation that Rehnquist might have been Deep Throat. Rehnquist was Assistant Attorney General of the Office of Legal Counsel, from 1969 to 1971, and then served on the Supreme Court until his death in 2005.\n\n\"Vanity Fair\" magazine revealed that Felt was Deep Throat on May 31, 2005, when it published an article (eventually appearing in the July issue of the magazine) on its website by John D. O'Connor, an attorney acting on Felt's behalf. Felt said, \"I'm the guy they used to call Deep Throat.\" After the \"Vanity Fair\" story broke, Benjamin C. Bradlee, the editor of the \"Washington Post\" during Watergate, confirmed that Felt was Deep Throat. According to the \"Vanity Fair\" article, Felt was persuaded to come out by his family. They hoped to capitalize on the book deals and other lucrative opportunities which Felt would be offered in order to help pay for his grandchildren's education. His family was unaware that he was Deep Throat for many years. They realized the truth after his retirement, when they became aware of his close friendship with Bob Woodward.\n\nNixon's Chief Counsel Charles Colson, who served prison time for his actions in the Nixon White House, said Felt had violated \"his oath to keep this nation's secrets\". A \"Los Angeles Times\" editorial argued that this argument was specious, \"as if there's no difference between nuclear strategy and rounding up hush money to silence your hired burglars.\" Ralph de Toledano, who co-wrote Felt's 1979 memoir, said Mark Felt Jr. had approached him in 2004 to buy Toledano's half of the copyright. Toledano agreed to sell but was never paid. He attempted to rescind the deal, threatening legal action. A few days before the \"Vanity Fair\" article was released, Toledano finally received a check.\n\nHe later said: \"I had been gloriously and illegally deceived, and Deep Throat was, in characteristic style, back in business—which given his history of betrayal, was par for the course.\"\n\nAfter the revelation, publishers were interested in signing Felt to a book deal. Weeks later, PublicAffairs Books announced that it signed a deal with Felt. Its CEO was a \"Washington Post\" reporter and editor during the Watergate era. The new book was to include material from Felt's 1979 memoir, plus an update. The new volume was scheduled for publication in early 2006. Felt sold the movie rights to his story to Universal Pictures for development by Tom Hanks's production company, Playtone. The book and movie deals were valued at US $1 million. A film based on those rights, \"\", in which Felt is portrayed by Liam Neeson, was released in 2017.\n\nIn mid-2005, Woodward published an account of his contacts with Felt, \"The Secret Man: The Story of Watergate's Deep Throat\" ().\n\nPublic response to Felt and his actions has varied widely since these revelations. In the immediate aftermath, Felt's family called him an \"American hero\", suggesting that he leaked information for moral or patriotic reasons. G. Gordon Liddy, who was convicted of burglary in the Watergate scandal, said Felt should have gone to the grand jury rather than leak.\n\nSpeculation about Felt's motives for leaking has varied widely. Some suggested that it was revenge for Nixon's choosing Gray over Felt to replace Hoover as FBI Director. Others suggest Felt acted out of institutional loyalty to the FBI.\n\nPolitical scientist George Friedman argued: \n\nThe \"Washington Post\" created a morality play about an out-of-control government brought to heel by two young, enterprising journalists and a courageous newspaper. That simply wasn't what happened. Instead, it was about the FBI using \"The Washington Post\" to leak information to destroy the president, and \"The Washington Post\" willingly serving as the conduit for that information while withholding an essential dimension of the story by concealing Deep Throat's identity.\n\nIn his 2012 book \"Leak: Why Mark Felt Became Deep Throat\", Max Holland argued that Felt leaked the information in an attempt to become head of the FBI. Holland said that Felt wanted to create the perception that Gray \"could not control the FBI\". This could result in Nixon's firing Gray, leaving Felt as the obvious choice to run the agency. Holland said this plan (if it was one) backfired as Nixon and his team found out that Felt was the leaker.\n\nFelt died at home, in his sleep, on December 18, 2008. He was 95 years old and his death was attributed to heart failure.\n\n"}
{"id": "54017105", "url": "https://en.wikipedia.org/wiki?curid=54017105", "title": "Medusa complex", "text": "Medusa complex\n\nMedusa complex is a psychological complex revolving around the petrification or freezing of human emotion, and drawing on the classical myth of the Medusa.\n\nThe term Medusa Complex was coined in 1948 by Gaston Bachelard to cover the feeling of petrification induced by the threat of the parental gaze. A mute, paralysed fury responds to the danger of the obliteration of an individual consciousness by an external Other (and perhaps by the corresponding internalised desire to obliterate the subjectivity of others in turn).\n\nLater writers have developed Bachelard's idea in various ways.\n\n"}
{"id": "54964081", "url": "https://en.wikipedia.org/wiki?curid=54964081", "title": "Melvyn R. Leventhal", "text": "Melvyn R. Leventhal\n\nMelvyn Rosenman Leventhal (born March 18, 1943) is an American attorney known primarily for his work as a community organizer and lawyer in the 1960s–1970s Civil Rights Movement, and for being the husband of author Alice Walker for ten years, and part of the first legally married interracial couple in the history of Mississippi.\n\nBorn and raised in Brooklyn, New York City, Leventhal attended a yeshiva elementary school, and Brooklyn Technical H.S. When he was nine years old, his parents divorced, and he and his siblings were split up, with the father taking Leventhal's older brother to live with him. Leventhal recalled that he rarely saw his father after that, and that on one occasion, when Leventhal was a teen-ager, he took a younger sibling to see their father who \"slammed the door in our face\". In Leventhal's formative years he was greatly influenced by Judaism's emphasis on community service and recalls in particular being \"outraged and disgusted by the way white people treated Jackie Robinson\". He resolved to fight injustice, and in pursuit of this, after receiving his undergraduate degree from New York University's Washington Square College in 1964, he received his J.D. from the New York University School of Law in 1967.\n\nAs a young lawyer, Leventhal worked in Mississippi for the NAACP Legal Defense Fund, and from this work he formed the first interracial law partnership in the history of the state, with Reuben V. Anderson, Fred L. Banks Jr., and John A. Nichols. Anderson and Banks would go on to become the first two African American justices of the Mississippi Supreme Court. \n\nThrough his work, Leventhal met Alice Walker, who came to trust and admire him due to his willingness to endanger his own social status and well-being by standing up to bigotry. On March 17, 1967, Leventhal and Walker married in New York, in a civil ceremony performed by Family Court Judge Justine W. Polier. The marriage was at that time illegal in Walker's home state of Georgia. When the couple returned to Mississippi in July 1967, they became \"the first legally married interracial couple in the state\". Walker and Leventhal had one child, Rebecca Walker, and divorced in 1976.\n\nWhile in Law School, during spring, summer and winter recesses, Leventhal worked as a student volunteer at LDF's offices in Jackson, Mississippi under the supervision of Marian Wright Edelman. This included serving as LDF's liaison to Dr. Martin Luther King, Jr. during the June, 1966 Meredith March Against Fear, from Memphis, Tennessee to Jackson, Mississippi.\n\nFrom 1969 through 1974 Leventhal served as LDF's lead counsel in Mississippi. He represented plaintiffs in approximately 75 lawsuits filed throughout the state to eliminate segregation and discrimination in public schools and to eliminate discrimination in employment, public accommodations, housing and in the provision of municipal services (e.g., street paving, street lighting and fire protection). After Leventhal moved back to New York in 1974, he was, until 1978, an LDF staff attorney litigating cases brought in Mississippi and throughout the United States. Leventhal’s ten-year career at the LDF was highlighted by three landmark cases:\n\nLeventhal also testified before the United States Senate's Select Committee on Equal Educational Opportunity (Chair: Hon. Walter Mondale) in 1970, on the progress of school desegregation in Mississippi.\n\nLeventhal returned to live in New York in 1974, and eventually remarried. Between 1979 and 1984, Leventhal served first as the Assistant Attorney General of New York, in charge of the Consumer Frauds and Protection Bureau and then as the Deputy First Assistant Attorney General of New York and Chief of the Litigation Bureau. Leventhal has argued two cases before the Supreme Court of the United States, \"Norwood v. Harrison\", 413 U.S. 455 (1973) (argued in 1972) and \"Blum v. Stenson\", 465 U.S. 886 (1984) (argued in 1983).\n"}
{"id": "8294411", "url": "https://en.wikipedia.org/wiki?curid=8294411", "title": "Morphogram", "text": "Morphogram\n\nA morphogram is the representation of a morpheme by a grapheme based solely on its meaning. Kanji and hanja are two writing systems that make use of morphograms, where Chinese characters were borrowed to represent native morphemes because of their meanings. Thus, a single character can represent a variety of morphemes which originally all had the same meaning. An example of this in Japanese would be the grapheme 東 [east], which can be read as \"higashi\" or \"azuma\", in addition to its logographic representation of the morpheme \"tō\". Additionally, in Japanese, the logographic (Chinese-derived) reading is called the \"on\", and the morphographic reading (native Japanese) is called the \"kun\".\n\n\n"}
{"id": "29780013", "url": "https://en.wikipedia.org/wiki?curid=29780013", "title": "Motion chart", "text": "Motion chart\n\nA motion chart is a dynamic bubble chart which allows efficient and interactive exploration and visualization of longitudinal multivariate Data. Motion Charts provide mechanisms for mapping ordinal, nominal and quantitative variables onto time, 2D coordinate axes, size, colors, glyphs and appearance characteristics, which facilitate the interactive display of multidimensional and temporal data.\n\nIn general, charts, graphs and plots provide the means for summarizing quantitative and qualitative data using diverse graphical representations. The main limitations of such static types of data exploratory and visualization are the low number of variables that can be shown simultaneously on the chart. Many classical data visualization techniques have limitations in terms of the volume, properties or complexity of the dataset. For instance, Scatter plots require bivariate data. Many datasets include multiple measurements like time, space, demographic, phenotypic and functional recording. For instance, the annual US Housing Price Index dataset includes dozens of variable including location (State and US region), year, unemployment rate, state population, percent subprime loans, etc.\n\nMotion charts provide a dynamic data visualization paradigm that facilitates the representation and understanding of large and multivariate data. Using the familiar 2D Bubble charts, motion Charts enable the display of large multivariate data with thousands of data points and allow for interactive visualization of the data using additional dimensions like time, the size of the blobs, and color) to show different characteristics of the data. \n\nThe central object of a motion chart is a blob (or bubble), which is a solid object homeomorphic to a disc. Blobs have 3 important characteristics – size, position and appearance. Using variable mapping, motion charts allow control over the appearance of the blobs at different time points. This mechanism enhances the dynamic appearance of the data in the motion chart and facilitates the visual inspection of associations, patterns and trends in multivariate datasets.\n\nThere are several web-based motion charts graphical data visualization tools including Many-Eyes, Gapminder, Google Motion Charts, nanobi analytics, motion-chart by amCharts and SOCR Motion Charts. These graphical resources allow users to interactively explore relationships and trends of data with temporal characteristics.\n\n"}
{"id": "146103", "url": "https://en.wikipedia.org/wiki?curid=146103", "title": "Nonlinear system", "text": "Nonlinear system\n\nIn mathematics and science, a nonlinear system is a system in which the change of the output is not proportional to the change of the input. Nonlinear problems are of interest to engineers, biologists, physicists, mathematicians, and many other scientists because most systems are inherently nonlinear in nature. Nonlinear dynamical systems, describing changes in variables over time, may appear chaotic, unpredictable, or counterintuitive, contrasting with much simpler linear systems.\n\nTypically, the behavior of a nonlinear system is described in mathematics by a nonlinear system of equations, which is a set of simultaneous equations in which the unknowns (or the unknown functions in the case of differential equations) appear as variables of a polynomial of degree higher than one or in the argument of a function which is not a polynomial of degree one.\nIn other words, in a nonlinear system of equations, the equation(s) to be solved cannot be written as a linear combination of the unknown variables or functions that appear in them. Systems can be defined as nonlinear, regardless of whether known linear functions appear in the equations. In particular, a differential equation is \"linear\" if it is linear in terms of the unknown function and its derivatives, even if nonlinear in terms of the other variables appearing in it.\n\nAs nonlinear dynamical equations are difficult to solve, nonlinear systems are commonly approximated by linear equations (linearization). This works well up to some accuracy and some range for the input values, but some interesting phenomena such as solitons, chaos, and singularities are hidden by linearization. It follows that some aspects of the dynamic behavior of a nonlinear system can appear to be counterintuitive, unpredictable or even chaotic. Although such chaotic behavior may resemble random behavior, it is in fact not random. For example, some aspects of the weather are seen to be chaotic, where simple changes in one part of the system produce complex effects throughout. This nonlinearity is one of the reasons why accurate long-term forecasts are impossible with current technology.\n\nSome authors use the term nonlinear science for the study of nonlinear systems. This is disputed by others:\n\nIn mathematics, a linear map (or \"linear function\") formula_1 is one which satisfies both of the following properties:\n\nAdditivity implies homogeneity for any rational \"α\", and, for continuous functions, for any real \"α\". For a complex \"α\", homogeneity does not follow from additivity. For example, an antilinear map is additive but not homogeneous. The conditions of additivity and homogeneity are often combined in the superposition principle\n\nAn equation written as\n\nis called linear if formula_1 is a linear map (as defined above) and nonlinear otherwise. The equation is called \"homogeneous\" if formula_7.\n\nThe definition formula_5 is very general in that formula_9 can be any sensible mathematical object (number, vector, function, etc.), and the function formula_1 can literally be any mapping, including integration or differentiation with associated constraints (such as boundary values). If formula_1 contains differentiation with respect to formula_9, the result will be a differential equation.\n\nNonlinear algebraic equations, which are also called \"polynomial equations\", are defined by equating polynomials to zero. For example,\n\nFor a single polynomial equation, root-finding algorithms can be used to find solutions to the equation (i.e., sets of values for the variables that satisfy the equation). However, systems of algebraic equations are more complicated; their study is one motivation for the field of algebraic geometry, a difficult branch of modern mathematics. It is even difficult to decide whether a given algebraic system has complex solutions (see Hilbert's Nullstellensatz). Nevertheless, in the case of the systems with a finite number of complex solutions, these systems of polynomial equations are now well understood and efficient methods exist for solving them.\n\nA nonlinear recurrence relation defines successive terms of a sequence as a nonlinear function of preceding terms. Examples of nonlinear recurrence relations are the logistic map and the relations that define the various Hofstadter sequences. Nonlinear discrete models that represent a wide class of nonlinear recurrence relationships include the NARMAX (Nonlinear Autoregressive Moving Average with eXogenous inputs) model and the related nonlinear system identification and analysis procedures. These approaches can be used to study a wide class of complex nonlinear behaviors in the time, frequency, and spatio-temporal domains.\n\nA system of differential equations is said to be nonlinear if it is not a linear system. Problems involving nonlinear differential equations are extremely diverse, and methods of solution or analysis are problem dependent. Examples of nonlinear differential equations are the Navier–Stokes equations in fluid dynamics and the Lotka–Volterra equations in biology.\n\nOne of the greatest difficulties of nonlinear problems is that it is not generally possible to combine known solutions into new solutions. In linear problems, for example, a family of linearly independent solutions can be used to construct general solutions through the superposition principle. A good example of this is one-dimensional heat transport with Dirichlet boundary conditions, the solution of which can be written as a time-dependent linear combination of sinusoids of differing frequencies; this makes solutions very flexible. It is often possible to find several very specific solutions to nonlinear equations, however the lack of a superposition principle prevents the construction of new solutions.\n\nFirst order ordinary differential equations are often exactly solvable by separation of variables, especially for autonomous equations. For example, the nonlinear equation\n\nhas formula_15 as a general solution (and also \"u\" = 0 as a particular solution, corresponding to the limit of the general solution when \"C\" tends to infinity). The equation is nonlinear because it may be written as\n\nand the left-hand side of the equation is not a linear function of \"u\" and its derivatives. Note that if the \"u\" term were replaced with \"u\", the problem would be linear (the exponential decay problem).\n\nSecond and higher order ordinary differential equations (more generally, systems of nonlinear equations) rarely yield closed-form solutions, though implicit solutions and solutions involving nonelementary integrals are encountered.\n\nCommon methods for the qualitative analysis of nonlinear ordinary differential equations include:\n\n\nThe most common basic approach to studying nonlinear partial differential equations is to change the variables (or otherwise transform the problem) so that the resulting problem is simpler (possibly even linear). Sometimes, the equation may be transformed into one or more ordinary differential equations, as seen in separation of variables, which is always useful whether or not the resulting ordinary differential equation(s) is solvable.\n\nAnother common (though less mathematic) tactic, often seen in fluid and heat mechanics, is to use scale analysis to simplify a general, natural equation in a certain specific boundary value problem. For example, the (very) nonlinear Navier-Stokes equations can be simplified into one linear partial differential equation in the case of transient, laminar, one dimensional flow in a circular pipe; the scale analysis provides conditions under which the flow is laminar and one dimensional and also yields the simplified equation.\n\nOther methods include examining the characteristics and using the methods outlined above for ordinary differential equations.\n\nA classic, extensively studied nonlinear problem is the dynamics of a pendulum under the influence of gravity. Using Lagrangian mechanics, it may be shown that the motion of a pendulum can be described by the dimensionless nonlinear equation\n\nwhere gravity points \"downwards\" and formula_18 is the angle the pendulum forms with its rest position, as shown in the figure at right. One approach to \"solving\" this equation is to use formula_19 as an integrating factor, which would eventually yield\n\nwhich is an implicit solution involving an elliptic integral. This \"solution\" generally does not have many uses because most of the nature of the solution is hidden in the nonelementary integral (nonelementary unless formula_21).\n\nAnother way to approach the problem is to linearize any nonlinearities (the sine function term in this case) at the various points of interest through Taylor expansions. For example, the linearization at formula_22, called the small angle approximation, is\n\nsince formula_24 for formula_25. This is a simple harmonic oscillator corresponding to oscillations of the pendulum near the bottom of its path. Another linearization would be at formula_26, corresponding to the pendulum being straight up:\n\nsince formula_28 for formula_29. The solution to this problem involves hyperbolic sinusoids, and note that unlike the small angle approximation, this approximation is unstable, meaning that formula_30 will usually grow without limit, though bounded solutions are possible. This corresponds to the difficulty of balancing a pendulum upright, it is literally an unstable state.\n\nOne more interesting linearization is possible around formula_31, around which formula_32:\n\nThis corresponds to a free fall problem. A very useful qualitative picture of the pendulum's dynamics may be obtained by piecing together such linearizations, as seen in the figure at right. Other techniques may be used to find (exact) phase portraits and approximate periods.\n\n\n\n\n"}
{"id": "1047138", "url": "https://en.wikipedia.org/wiki?curid=1047138", "title": "Novus homo", "text": "Novus homo\n\nHomo novus (or: \"novus homo\", Latin for \"new man\"; plural homines novi) was the term in ancient Rome for a man who was the first in his family to serve in the Roman Senate or, more specifically, to be elected as consul. When a man entered public life on an unprecedented scale for a high communal office, then the term used was novus civis (plural: novi cives) or \"new citizen\".\n\nIn the Early Republic, tradition held that both Senate membership and the consulship were restricted to patricians. When plebeians gained the right to this office during the Conflict of the Orders, all newly elected plebeians were naturally \"novi homines\". With time, \"novi homines\" became progressively rarer as some plebeian families became as entrenched in the Senate as their patrician colleagues. By the time of the First Punic War, it was already a sensation that \"novi homines\" were elected in two consecutive years (Gaius Fundanius Fundulus in 243 BC and Gaius Lutatius Catulus in 242 BC). In 63 BC, Cicero became the first \"novus homo\" in more than thirty years.\n\nBy the Late Republic, the distinction between the orders became less important. The consuls came from a new elite, the \"nobiles\" (noblemen), an artificial aristocracy of all who could demonstrate direct descent in the male line from a consul.\n\n\nThe literary theme of \"Homo novus\", or \"how the lowly born but inherently worthy man may properly rise to eminence in the world\" was the \"topos\" of Seneca's influential Epistle XLIV. At the endpoint of Late Antiquity, it was likewise a subject in Boethius' \"Consolation of Philosophy\" (iii, vi). In the Middle Ages Dante's \"Convivio\" (book IV) and Petrarch's \"De remediis utriusque fortunae\" (I.16; II.5) take up the subject, and Chaucer's Wife of Bath's Tale.\n\nIn its Christian renderings, the theme suggested a tension in the \"scala naturae\" or great chain of being, one that was produced through the agency of Man's free will.\n\nThe theme came naturally to Renaissance humanists who were often \"homines novi\" rising by their own wits in a network of noble courts that depended on the highly literate new men to run increasingly complicated chancelries and create the cultural propaganda that was a contemporary vehicle for noble fame, and that consequently offered a kind of intellectual \"cursus honorum\". In the fifteenth century Buonaccorso da Montemagno's \"Dialogus de vera nobilitate\" treated of the \"true nobility\" inherent in the worthy individual; Poggio Bracciolini also wrote at length \"De nobilitate\", stressing the Renaissance view of human responsibility and effectiveness that are at the heart of Humanism: \"sicut virtutis ita et nobilitatis sibi quisque existit auctor et opifex\".\n\nBriefer summaries of the theme were to be found in Francesco Patrizi, \"De institutionae republicae\" (VI.1), and in Rodrigo Sánchez de Arévalo's encyclopedic \"Speculum vitae humanae\". In the sixteenth century these and new texts came to be widely printed and distributed. Sánchez de Arévalo's \"Speculum\" was first printed at Rome, 1468, and there are more than twenty fifteenth-century printings; German, French and Spanish translations were printed. The characters of Baldassare Castiglione's The Book of the Courtier (1528) discuss the requirement that a \"cortegiano\" be noble (I.XIV-XVI). This was translated into French, Spanish, English, Latin and other languages. Jerónimo Osório da Fonseca's \"De nobilitate\" (Lisbon 1542, and seven reprintings in the sixteenth century), stressing \"propria strennuitas\" (\"one's own determined striving\") received an English translation in 1576.\n\nThe Roman figure most often cited as an \"exemplum\" is Gaius Marius, whose speech of self-justification was familiar to readers from the set-piece in Sallust's \"Bellum Iugurthinum\", 85; the most familiar format in the Renaissance treatises is a dialogue that contrasts the two sources of nobility, with the evidence weighted in favour of the \"new man\".\n\n\n\n"}
{"id": "9952783", "url": "https://en.wikipedia.org/wiki?curid=9952783", "title": "PGA Tour Golf Team Challenge", "text": "PGA Tour Golf Team Challenge\n\nPGA Tour Golf Team Challenge is a trackball-based golf arcade game series manufactured by Global VR of San Jose, California.\n\nBased on the PC version of EA Sports' \"PGA Tour\" game, the game is run from a computer within the cabinet which has an Intel Pentium 4 processor and Nvidia GeForce video card.\n\nThe current - and final - edition of the PGA series is titled PGA Tour Golf ‘Team Challenge’. A player can select from a number of real PGA pro golfers & PGA tour courses in either 9 or 18 holes, against up to 3 other players or in team play of up to 4 players per team. The game has 3 main modes of play;\n\n‘Play Golf’ mode (does not require a players’ card or online capabilities) has 13 eighteen-hole courses but if the machine is online/tournament enabled, there’s a 14th ‘bonus’ course which changes every month. There are training courses and a driving range here as well as a front 9 ‘course’ and back 9 ‘course’ made up of various Fantasy course holes.\n\n‘World Tour’ mode provides access to all 24 courses each with two hidden/secret skill-shots which unlock customizable items for your created golfer. This mode keeps track of your personal best course scores and once all 24 courses are complete, a ranking will be assigned using your combined average score - Rookie, Amateur, Scratch, Club Pro, Pro, Champion & Legend. You can continue to improve upon your scores to gain a higher ranking.\n\n‘Tournaments’ run weekly featuring one of the 24 courses within the game. Weather conditions and pin placements change with each day of the tournament. Tournament Leaderboards are displayed during the machine’s ‘attract mode’ screens.\n\nOnline play (Tournaments and World Tour) requires the Global VR 2nd generation \"Smart Card\" reader (recognizable by two LED lights on the front) and accompanying Players’ Card Format for saving created golfers as well as World Tour stats & unlockable items. The original magnetic stripe card reader from the first three editions of PGA Tour Golf (lacks LED lights on the front) is not compatible with ‘Challenge’ and ‘Team Challenge’ editions.\n\n\nNote: Some courses only available to tournament (online) enabled machines.\n\nFantasy Courses\n\nReal Courses\n\n\nSince the game has the PC version of Tiger Woods Golf embedded within, pros who were unlicensed for the coin-op version can be substituted in place of an existing golfer with simple modifications within the game’s files (Tiger Woods, Jack Nicklaus, Ben Hogan, Adam Scott and Jesper Parnevik for example).\n\nThe series originally launched with a Beige ‘DFI’ computer within. As the series continued, Global VR provided operators with new software on compact discs (to be installed on the computer’s hard drive) featuring the latest edition of PGA Tour Golf. The hardware demands for each new edition increased and Global VR provided a computer upgrade option; the superior black ‘Everlast’ computer. Many operators passed on spending on hardware upgrades so subsequent editions of the series may have seen noticeable lag in gameplay as a result.\n\nThe DFI computer has a nb32 motherboard with maximum capable processor upgrade of a 2.8 Ghz Intel Pentium 4 SL7EY 512/400 MHz socket 478N.\n\nThe Everlast computer has a PS35-BL motherboard with a maximum capable processor upgrade of a 3.4 Ghz Intel Pentium 4 SL7PP 865/875 socket 478.\n\nBoth computers require an AGP (not PCI-e) type video card - Nvidia GeForce 5700 - 9600 is recommended.\n\nAlso, both computers will perform better if the onboard memory is increased to 1 GB of RAM or more.\n\nThe compatible memory for a DFI is SDRAM pc133 pc100. 512MB per memory stick. There are 3 slots for memory on a DFI motherboard for up to 1.5GB of memory.\n"}
{"id": "24562", "url": "https://en.wikipedia.org/wiki?curid=24562", "title": "Pareto principle", "text": "Pareto principle\n\nThe Pareto principle (also known as the 80/20 rule, the law of the vital few, or the principle of factor sparsity) states that, for many events, roughly 80% of the effects come from 20% of the causes. Management consultant Joseph M. Juran suggested the principle and named it after Italian economist Vilfredo Pareto, who noted the 80/20 connection while at the University of Lausanne in 1896, as published in his first work, \"Cours d'économie politique\". Essentially, Pareto showed that approximately 80% of the land in Italy was owned by 20% of the population.\n\nIt is an axiom of business management that \"80% of sales come from 20% of clients\". Richard Koch authored the book, \"The 80/20 Principle,\" which illustrated some practical applications of the Pareto principle in business management and life.\n\nMathematically, the 80/20 rule is roughly followed by a power law distribution (also known as a Pareto distribution) for a particular set of parameters, and many natural phenomena have been shown empirically to exhibit such a distribution.\n\nThe Pareto principle is only tangentially related to Pareto efficiency. Pareto developed both concepts in the context of the distribution of income and wealth among the population.\n\nThe original observation was in connection with population and wealth. Pareto noticed that approximately 80% of Italy's land was owned by 20% of the population. He then carried out surveys on a variety of other countries and found to his surprise that a similar distribution applied.\n\nA chart that gave the inequality a very visible and comprehensible form, the so-called \"champagne glass\" effect, was contained in the 1992 United Nations Development Program Report, which showed that distribution of global income is very uneven, with the richest 20% of the world's population controlling 82.7% of the world's income.\nThe Pareto principle also could be seen as applying to taxation. In the US, the top 20% of earners have paid roughly 80-90% of Federal income taxes in 2000 and 2006, and again in 2018.\n\nIn computer science the Pareto principle can be applied to optimization efforts. For example, Microsoft noted that by fixing the top 20% of the most-reported bugs, 80% of the related errors and crashes in a given system would be eliminated. Lowell Arthur expressed that \"20 percent of the code has 80 percent of the errors. Find them, fix them!\" It was also discovered that in general the 80% of a certain piece of software can be written in 20% of the total allocated time. Conversely, the hardest 20% of the code takes 80% of the time. This factor is usually a part of COCOMO estimating for software coding.\n\nIt has been inferred the Pareto principle applies to athletic training, where roughly 20% of the exercises and habits have 80% of the impact and the trainee should not focus so much on a varied training. This does not necessarily mean that having a healthy diet or going to the gym are not important, but they are not as significant as the key activities. It is also important to note this 80/20 rule has yet to be scientifically tested in controlled studies with regards to athletic training.\n\nIn baseball, the Pareto principle has been perceived in Wins Above Replacement (an attempt to combine multiple statistics to determine a player's overall importance to a team). \"15% of the all the players last year produced 85% of the total wins with the other 85% of the players creating 15% of the wins. The Pareto Principle holds up pretty soundly when it is applied to baseball...\" Unfortunately a brief look at this article's methods shows that the choice of alternate and probably better indicators of performance yields drastically different results that fail to support the Pareto Principle for this context. Thus this probably entails an example of cherry-picking.\n\nOccupational health and safety professionals use the Pareto principle to underline the importance of hazard prioritization. Assuming 20% of the hazards account for 80% of the injuries, and by categorizing hazards, safety professionals can target those 20% of the hazards that cause 80% of the injuries or accidents. Alternatively, if hazards are addressed in random order, a safety professional is more likely to fix one of the 80% of hazards that account only for some fraction of the remaining 20% of injuries.\n\nAside from ensuring efficient accident prevention practices, the Pareto principle also ensures hazards are addressed in an economical order as the technique ensures the resources used are best used to prevent the most accidents.\n\nIn engineering control theory, such as for electromechanical energy converters, the 80/20 principle applies to optimization efforts.\n\nThe law of the few can be also seen in betting, where it is said that with 20% effort you can match the accuracy of 80% of the bettors.\n\nIn the systems science discipline, Joshua M. Epstein and Robert Axtell created an agent-based simulation model called Sugarscape, from a decentralized modeling approach, based on individual behavior rules defined for each agent in the economy. Wealth distribution and Pareto's 80/20 principle became emergent in their results, which suggests the principle is a collective consequence of these individual rules.\n\nThe Pareto principle has many applications in quality control. It is the basis for the Pareto chart, one of the key tools used in total quality control and Six Sigma techniques. The Pareto principle serves as a baseline for ABC-analysis and XYZ-analysis, widely used in logistics and procurement for the purpose of optimizing stock of goods, as well as costs of keeping and replenishing that stock.\n\nIn health care in the United States, in one instance 20% of patients have been found to use 80% of health care resources., \n\nSome cases of super-spreading conform to the 20/80 rule, where approximately 20% of infected individuals are responsible for 80% of transmissions, although super-spreading can still be said to occur when super-spreaders account for a higher or lower percentage of transmissions. In epidemics with super-spreading, the majority of individuals infect relatively few secondary contacts.\n\nThe Dunedin Study has found 80% of crimes are committed by 20% of criminals. This statistic has been used to support both stop-and-frisk policies and broken windows policing, as catching those criminals committing minor crimes will supposedly net many criminals wanted for (or who would normally commit) larger ones. Once again, however, data from the same Dunedin study provide other results that nowhere near approximate the 80-20 principle; choose a different indicator and the 80% may become 36%. Thus support for the applicability of the Pareto Principle again seems to require cherry-picking.\n\nThe idea has a rule of thumb application in many places, but it is commonly misused. For example, it is a misuse to state a solution to a problem \"fits the 80/20 rule\" just because it fits 80% of the cases; it must also be that the solution requires only 20% of the resources that would be needed to solve all cases. Additionally, it is a misuse of the 80/20 rule to interpret a small number of categories or observations.\n\nThis is a special case of the wider phenomenon of Pareto distributions. If the Pareto index α, which is one of the parameters characterizing a Pareto distribution, is chosen as α = log5 ≈ 1.16, then one has 80% of effects coming from 20% of causes.\n\nIt follows that one also has 80% of that top 80% of effects coming from 20% of that top 20% of causes, and so on. Eighty percent of 80% is 64%; 20% of 20% is 4%, so this implies a \"64/4\" law; and similarly implies a \"51.2/0.8\" law. Similarly for the bottom 80% of causes and bottom 20% of effects, the bottom 80% of the bottom 80% only cause 20% of the remaining 20%. This is broadly in line with the world population/wealth table above, where the bottom 60% of the people own 5.5% of the wealth, approximating to a 64/4 connection.\n\nThe 64/4 correlation also implies a 32% 'fair' area between the 4% and 64%, where the lower 80% of the top 20% (16%) and upper 20% of the bottom 80% (also 16%) relates to the corresponding lower top and upper bottom of effects (32%). This is also broadly in line with the world population table above, where the second 20% control 12% of the wealth, and the bottom of the top 20% (presumably) control 16% of the wealth.\n\nThe term 80/20 is only a shorthand for the general principle at work. In individual cases, the distribution could just as well be, say, nearer to 80/20 or 70/30. There is no need for the two numbers to add up to the number 100, as they are measures of different things, (e.g., 'number of customers' vs 'amount spent'). However, each case in which they do not add up to 100%, is equivalent to one in which they do. For example, as noted above, the \"64/4 law\" (in which the two numbers do not add up to 100%) is equivalent to the \"80/20 law\" (in which they do add up to 100%). Thus, specifying two percentages independently does not lead to a broader class of distributions than what one gets by specifying the larger one and letting the smaller one be its complement relative to 100%. Thus, there is only one degree of freedom in the choice of that parameter.\n\nAdding up to 100 leads to a nice symmetry. For example, if 80% of effects come from the top 20% of sources, then the remaining 20% of effects come from the lower 80% of sources. This is called the \"joint ratio\", and can be used to measure the degree of imbalance: a joint ratio of 96:4 is very imbalanced, 80:20 is significantly imbalanced (Gini index: 76%), 70:30 is moderately imbalanced (Gini index: 28%), and 55:45 is just slightly imbalanced (Gini index 14%).\n\nThe Pareto principle is an illustration of a \"power law\" relationship, which also occurs in phenomena such as brush fires and earthquakes.\nBecause it is self-similar over a wide range of magnitudes, it produces outcomes completely different from Normal or Gaussian distribution phenomena. This fact explains the frequent breakdowns of sophisticated financial instruments, which are modeled on the assumption that a Gaussian relationship is appropriate to, for example, stock price movements.\n\nUsing the \"\"A\" : \"B\"\" notation (for example, 0.8:0.2) and with \"A\" + \"B\" = 1, inequality measures like the Gini index (G) \"and\" the Hoover index (H) can be computed. In this case both are the same.\n\nThe Theil index is an entropy measure used to quantify inequalities. The measure is 0 for 50:50 distributions and reaches 1 at a Pareto distribution of 82:18. Higher inequalities yield Theil indices above 1.\n\n\nParetoRule.cf : The Pareto Rule\n\n"}
{"id": "84617", "url": "https://en.wikipedia.org/wiki?curid=84617", "title": "Peacekeeping", "text": "Peacekeeping\n\nPeacekeeping refers to activities intended to create conditions that favour lasting peace. Research generally finds that peacekeeping reduces civilian and battlefield deaths and reduces the risk of renewed warfare.\n\nWithin the United Nations (UN) group of nation-state governments and organisations, there is a general understanding that at the international level, peacekeepers monitor and observe peace processes in post-conflict areas, and may assist ex-combatants in implementing peace agreement commitments that they have undertaken. Such assistance may come in many forms, including confidence-building measures, power-sharing arrangements, electoral support, strengthening the rule of law, and economic and social development. Accordingly, the UN peacekeepers (often referred to as Blue Berets or Blue Helmets because of their light blue berets or helmets) can include soldiers, police officers, and civilian personnel.\n\nThe United Nations is not the only organisation to implement peacekeeping missions. Non-UN peacekeeping forces include the NATO mission in Kosovo (with United Nations authorisation) and the Multinational Force and Observers on the Sinai Peninsula or the ones organised by the European Union (like EUFOR RCA, with UN authorisation) and the African Union (like the African Union Mission in Sudan). The Nonviolent Peaceforce is one NGO widely considered to have expertise in general peacemaking by non-governmental volunteers or activists.\n\nThere are a range of various types of operations encompassed in peacekeeping. In Page Fortna's book \"Does Peacekeeping Work?\", for instance, she distinguishes four different types of peacekeeping operations. Importantly, these types of missions and how they are conducted are heavily influenced by the mandate in which they are authorized. Three of Fortna's four types are consent-based missions, i.e. so-called \"Chapter VI\" missions, with the fourth being a \"Chapter VII\" Mission. \nChapter VI missions are consent based, therefore they require the consent of the belligerent factions involved in order to operate. Should they lose that consent, Peacekeepers would be compelled to withdraw. Chapter VII missions, by contrast, do not require consent, though they may have it. If consent is lost at any point, Chapter VII missions would not be required to withdraw.\n\nDuring the Cold War, peacekeeping was primarily interpositional in nature—thus being referred to as traditional peacekeeping. UN Peacekeepers were deployed in the aftermath of interstate conflict in order to serve as a buffer between belligerent factions and ensure compliance with the terms of an established peace agreement. Missions were consent-based, and more often than not observers were unarmed—such was the case with UNTSO in the Middle East and UNCIP in India and Pakistan. Others were armed—such as UNEF-I, established during the Suez Crisis. They were largely successful in this role.\n\nIn the post-Cold War era, the United Nations has taken on a more nuanced, multidimensional approach to Peacekeeping. In 1992, in the aftermath of the Cold War, then Secretary-General Boutros Boutros-Ghali put together a report detailing his ambitious concepts for the United Nations and Peacekeeping at large. The report, titled \"An Agenda for Peace\", described a multi-faceted and interconnected set of measures he hoped would lead to effective use of the UN in its role in post-Cold War international politics. This included the use of preventative diplomacy, peace-enforcement, peace-making, peace-keeping and post-conflict reconstruction.\n\nIn \"The UN Record on Peacekeeping Operations\", Michael Doyle and Nicolas Sambanis summarise Boutros Boutros’ report as preventative diplomacy, confidence-building measures such as fact-finding missions, observer mandates, and the potential deployment of UN mandated forces as a preventative measure in order to diminish the potential for violence or the danger of violence occurring and thus increasing the prospect for lasting peace. Their definitions are as follows:\n\n \nNot all international peacekeeping forces have been directly controlled by the United Nations. In 1981, an agreement between Israel and Egypt formed the Multinational Force and Observers which continues to monitor the Sinai Peninsula.\n\nThe African Union (AU) is working on building an African Peace and Security Architecture that fulfils the mandate to enforce peace and security on the continent. In cases of genocide or other serious human-rights violations, an AU-mission could be launched even against the wishes of the government of the country concerned, as long as it is approved by the AU General Assembly. The establishment of the African Peace and Security Architecture (APSA) which includes the African Standby Force (ASF) is planned earliest for 2015.\n\nUnarmed Civilian Peacekeeping (UCP) are civilian personnel that carry out non-violent, non-interventionist and impartial set of tactics in order to protect civilians in conflict zones from violence in addition to supporting additional efforts to build a lasting peace. While the term UCP is not entirely ubiquitous among non-governmental agencies (NGOs) in the field: many utilize similar techniques and desire shared outcomes for peace; such as accompaniment, presence, rumour control, community security meetings, the securing of safe passage, and monitoring.\n\nUnited Nations Peacekeeping started in 1948 when the United Nations Security Council authorised the deployment of UN unarmed military observers to the Middle East in order to monitor the armistice agreement that was signed between Israel and its Arab neighbours in the wake of the Arab-Israeli War. This operation was called the United Nations Truce Supervision Organization (UNTSO) and is still in operation today. With the passage of resolution 73 (1949) by the Security Council in August 1949, UNTSO was given the task of fulfilling four Armistice Agreements between the state of Israel and the Arab states which had participated in the war. Thus, UNTSO's operations were spread through five states in the region—Israel, Egypt, Jordan, Lebanon and the Syrian Arab Republic.\n\nIn the wake of independence in India and Pakistan in August 1947 and the subsequent bloodshed that followed the Security Council adopted resolution 39 (1948) in January 1948 in order to create the United Nations Commission for India and Pakistan (UNCIP), with the purpose of mediating the dispute between India and Pakistan over Kashmir and the fighting related to it. This operation was non-interventionist in nature and was additionally tasked with supervision of a ceasefire signed by Pakistan and India in the state of Jammu and Kashmir. With the passage of the Karachi agreement in July 1949, UNCIP would supervise a ceasefire line that would be mutually overseen by UN unarmed military observers and local commanders from each side in the dispute. UNCIP's mission in the region continues to this day, now under the operational title of the United Nations Military Observer Group in India and Pakistan (UNMOGIP).\n\nSince then, sixty-nine peacekeeping operations have been authorised and have deployed to various countries all over the world. The great majority of these operations have begun in the post-Cold War world. Between 1988 and 1998 thirty-five UN operations had been established and deployed. This signified a substantial increase when compared with the periods between 1948 and 1978; which saw the creation and deployment of only thirteen UN Peacekeeping operations and zero between 1978 and 1988.\n\nArmed intervention first came in the form of UN involvement in the wake of the Suez Crisis in 1956. United Nations Emergency Force (UNEF-1), which existed from November 1956 to June 1967 was essentially the first ever United Nations peacekeeping force. It was given the mandate of ensuring the cessation of hostilities between Egypt, the United Kingdom, France, and Israel in addition to overseeing the withdrawal of French, Israeli and British troops from Egyptian territory. Upon completion of said withdrawal, UNEF would serve as a buffer force between Egyptian and Israeli forces in order to supervise conditions of the ceasefire and contribute to a lasting peace.\n\nShortly thereafter, the United Nations Operation in the Congo (ONUC), was deployed in 1960. This operation involved upwards of 20,000 military personnel at its peak, and resulted in the death of 250 UN personnel, including then Secretary-General Dag Hammarskjold. ONUC was meant to ensure the withdrawal of Belgian forces in the Congo, who had reinserted themselves after Congolese independence in the wake of a revolt carried out by the Force Publique (FP), in order to protect Belgian citizens and economic interests. ONUC was also tasked with establishing and maintaining law and order (helping to end the FP revolt and ethnic violence) as well as provide technical assistance and training to Congolese security forces. An additional function was added to ONUC's mission, in which the force was tasked with maintaining the territorial integrity and political independence of the Congo—resulting from the secession of the mineral-rich provinces of Katanga and South Kasai. The UN forces there, somewhat controversially, more or less became an arm of the Congolese government at the time and helped to forcefully end the secession of both provinces.\n\nThroughout the 1960s and 1970s the UN created multiple short-term missions all over the world including the Mission of the Representative of the Secretary-General in the Dominican Republic (DOMREP), the UN Security Force in West New Guinea (UNSF), the UN Yemen Observation Mission (UNYOM), in conjunction with more long-term operations such as the UN Peacekeeping Force in Cyprus (UNFICYP), the UN Emergency Force II (UNEF II), the UN Disengagement Observer Force (UNDOF) and the UN Interim Force in Lebanon (UNIFIL).\n\nExperiences of peacekeeping during the Yugoslav Wars, especially failures such as the Srebrenica Massacre, led, in part, to the United Nations Peacebuilding Commission, which works to implement stable peace through some of the same civic functions that peacekeepers also work on, such as elections. The Commission currently works with six countries, all in Africa. In 2013 the U.N. Security Council unanimously passed Resolution 2122, which among other things calls for stronger measures regarding women's participation in conflict and post-conflict processes such as peace talks, gender expertise in peacekeeping missions, improved information about the impact of armed conflict on women, and more direct briefing to the Council on progress in these areas. Also in 2013, the Committee on the Elimination of Discrimination against Women (CEDAW), a UN women's rights committee, said in a general recommendation that states that have ratified the UN Women's Rights Convention are obliged to uphold women's rights before, during, and after conflict when they are directly involved in fighting, and/or are providing peacekeeping troops or donor assistance for conflict prevention, humanitarian aid or post-conflict reconstruction.\nThe Committee also stated that ratifying states should exercise due diligence in ensuring that non-state actors, such as armed groups and private security contractors, be held accountable for crimes against women.\n\nOne of the findings of Page Fortna about where peacekeepers go is that “peacekeeping is a matter of supply and demand” From the supply side, she observes that there is unlikely a Peacekeeping mission in civil wars on countries close to one of the members of the Security Council. From the demand side, there is diverse evidence that peacekeeping missions are deployed in the countries who need it the most, this is where the risk of a recurring war is high.\n\nThe United Nations Charter stipulates that to assist in maintaining peace and security around the world, all member states of the UN should make available to the Security Council necessary armed forces and facilities. Since 1948, about 130 nations have contributed military and civilian police personnel to peace operations. While detailed records of all personnel who have served in peacekeeping missions since 1948 are not available, it is estimated that up to one million soldiers, police officers and civilians have served under the UN flag in the last 56 years. As of March 2008, 113 countries were contributing a total 88,862 military observers, police, and troops.\n\nDespite the large number of contributors, the greatest burden continues to be borne by a core group of developing countries. The ten largest troop (including police and military experts) contributing countries to UN peacekeeping operations as of May, 2017 were Ethiopia (8229), India (7665), Pakistan (7135), Bangladesh (6958), Rwanda (6256), Nepal (5158), Burkina Faso (2969), Senegal (2847), Ghana (2751), Indonesia (2719).\n\nAs of March 2008, in addition to military and police personnel, 5,187 international civilian personnel, 2,031 UN Volunteers, and 12,036 local civilian personnel worked in UN peacekeeping missions.\n\nAs of 30 June 2014, 3,243 people from over 100 countries have been killed while serving on peacekeeping missions. Many of those came from India (157), Nigeria (142), Pakistan (136), Ghana (132), Canada (121), France (110) and the United Kingdom (103). Thirty percent of the fatalities in the first 55 years of UN peacekeeping occurred between 1993 and 1995.\n\nDeveloping nations tend to participate in peacekeeping more than developed countries. This may be due in part because forces from smaller countries avoid evoking thoughts of imperialism. The rate of reimbursement by the UN for troop contributing countries per peacekeeper per month include: $1,028 for pay and allowances; $303 supplementary pay for specialists; $68 for personal clothing, gear and equipment; and $5 for personal weaponry. This can be a significant source of revenue for a developing country. By providing important training and equipment for the soldiers as well as salaries, UN peacekeeping missions allow them to maintain larger armies than they otherwise could. About 4.5% of the troops and civilian police deployed in UN peacekeeping missions come from the European Union and less than one percent from the United States.\n\nSecurity Council Resolution 1325 was the first major step taken by the UN to include women as active and equal actors in “the prevention and resolution of conflicts, peace negotiations, peace-building, peacekeeping, humanitarian response and in post-conflict reconstruction and stresses the importance of their equal participation and full involvement in all efforts for the maintenance and promotion of peace and security”. A critique of this resolution is that UNSCR 1325 proposes the implementing gender mainstreaming, however the progress that has been accomplished in this area has focused on women, rather than on assessing the impacts of planned action on both men and women. In 2010, a comprehensive 10-year impact study was conducted to assess the success of this resolution and found that there was limited success with the implementation, particularly in the increasing women's participation in peace negotiations and peace agreements, and sexual and gender-based violence has continued to be prevalent, despite efforts to reduce it.\n\nIn 2013 the U.N. Security Council unanimously passed Resolution 2122, which among other things calls for stronger measures regarding women's participation in conflict and post-conflict processes such as peace talks, gender expertise in peacekeeping missions, improved information about the impact of armed conflict on women, and more direct briefing to the Council on progress in these areas. Also in 2013, the Committee on the Elimination of Discrimination against Women (CEDAW), a UN women's rights committee, said in a general recommendation that states that have ratified the UN Women's Rights Convention are obliged to uphold women's rights before, during, and after conflict when they are directly involved in fighting, and/or are providing peacekeeping troops or donor assistance for conflict prevention, humanitarian aid or post-conflict reconstruction The Committee also stated that ratifying states should exercise due diligence in ensuring that non-state actors, such as armed groups and private security contractors, be held accountable for crimes against women.\n\nAs of July 2016, women serve in every UN peacekeeping mission either as troops, police, or civilian staff. In 1993, women made up 1% of deployed uniformed personnel. In 2014, out of approximately 125,000 peacekeepers, women constitute 3% of military personnel and 10% of police personnel in UN Peacekeeping missions, as well as 29% of international and 17% of national staff in peacekeeping and special political missions. In 2016, five women were leading peacekeeping missions as Special Representatives of the Secretary-General.\n\nWhile much has been written about Peacekeeping and what Peacekeepers do, very little empirical research has taken place in order to identify the manner in which Peacekeepers can have an impact in a post-conflict environment. Columbia University Professor, Virginia Page Fortna attempts to lay out four causal mechanisms through which peacekeepers have the opportunity to lay the groundwork for a lasting peace. Fortna's four mechanisms are as follows:\nFortna argues that peacekeepers have a positive impact on the peace process, despite often being sent to places where peace is most difficult to achieve. Peacekeeping is often looked at by detractors as ineffective, or unnecessary. Peace prevails when belligerents already have a vested interest in sustaining peace and therefore it could be argued that Peacekeepers play only a minor role in creating a strong foundation for enduring peace. Yet these causal reasons illustrate the important roles that Peacekeepers play in ensuring that peace lasts, especially when contrasted against situations in which belligerents are left to their own devices. These causal reasons thus illustrate the need for Peacekeeping and lay a foundation for the manner in which Peacekeeping operations can have a substantive impact on the post-conflict environment.\n\nIn order to change the incentives for war and make peace more appealing the UN can provide a military force by way of an enforcement mandate which provides deterrence to would-be spoilers. They can monitor the situation making the potential for surprise attack by one of the belligerents less likely to occur or by making it more difficult to carry out such an attack. A lightly-armed observer mission can also serve as an early-warning force or “tripwire” for the aforementioned enforcement mission. Aid and recognition provided to the belligerents by the international community should be made conditional and based on compliance with objectives laid out in the negotiating process. And lastly, peace dividends should be provided in the forms of jobs, public works and other benefits.\n\nTo reduce uncertainty and fear the UN Peacekeeping force can monitor the aforementioned compliance, facilitate communication between belligerents in order to ease security dilemma concerns thus reassuring belligerents that the other side will not renege, and allow for belligerents to signal their legitimate intentions for peace to the other side. That is to say, provide a meaningful pathway for communication between both sides to make their intentions known and credible.\n\nPrevention and control of potential accidents that may derail the peace process can be achieved by the peacekeeping force by deterring rogue groups. Belligerent forces are often undisciplined without a strong central source of command and control, therefore while a peace is being negotiated there is potential for a rogue group on one side to renege and spoil the peace process. UN forces can serve to prevent this. Additionally, the UN force can serve as a moderator and make communication easy between both parties and bring in political moderates from either side. By providing law and order UN peacekeeping forces can temporarily replace a state's security forces and prevent a bias overreaction to an alleged violation by one side which could in turn result in escalation and a renewal in the violence.\n\nPrevention of political abuse can be achieved through the reformation of institutions associated with the government. Training and monitoring the security forces (e.g. army or police) help to make them an unbiased protector of the people rather than a weapon of suppression for the ruling government. Hopefully this training can bring trust by the people for the security establishment. UN forces can also run and monitor elections in order to ensure a fair process. In other cases, the UN may provide a neutral interim government to administer the country during a transitional period wherein the associated government institutions are being retrained, reformed or better developed. Lastly, military groups such as armed rebels can be encouraged to put down their weapons and transformed into political organisations using appropriate non-violent means to mete out their grievances and compete in the election cycle. This is especially important as many of these groups serve as the chief opposition to a given government, but lack the means or know-how to operate effectively as political organisations.\n\nDifferent peacekeeping missions take place as a result of different causal mechanisms. More military deterrence and enforcement are meant for those missions operating under the auspices of Chapter VII, while Chapter VI missions are meant to serve more as monitoring forces and interpositional operations are meant to target and prevent potential political abuse—these are primarily multidimensional missions and are heavily involved in the post-conflict political situation.\n\nAccording to a 2014 survey of the academic literature, \"there is considerable evidence that [United Nations peacekeeping operations] are effective in maintaining peace.\" According to Fortna, there is strong evidence that the presence of peacekeepers significantly reduces the risk of renewed warfare; more peacekeeping troops contribute to fewer battlefield deaths; and more peacekeeping troops contribute to fewer civilian deaths. A study by political scientists at Uppsala University and Peace Research Institute Oslo estimates that an ambitious UN peacekeeping policy with a doubled peacekeeping operation and strong mandates would \"reduce the global incidence of armed conflict by two thirds relative to a no-PKO scenario.\" According to Fordham University political scientist Anjali Dayal, \"Scholars have found that peacekeeping keeps wars from bleeding across borders. Having more peacekeepers on the ground also seems to correspond with fewer civilians targeted with violence. And peace operations at times have successfully served as transitional authorities, handing power back to local authorities, although this is decreasingly true.\" There is also evidence that the promise to deploy peacekeepers can help international organizations bring combatants to the negotiation table and increase the likelihood that they will agree to a cease-fire.\n\nBy controlling for specific factors that affect where peacekeepers are deployed and what the potential chances for peace are, Page Fortna's statistical research shows that there is a statistically significant impact on lasting peace when peacekeepers are deployed. Despite the fact that peacekeepers are sent to locations where peace is least likely to succeed, Fortna finds that conservative estimates suggest that the presence of UN peacekeepers diminishes the risk for renewed violence by at least 55%-60%; with less conservatives estimates upwards of 75%-85%. Additionally, her analysis concludes that there is little difference in the effectiveness between Chapter VI consent-based missions and Chapter VII enforcement missions. Indeed, enforcement missions only remain effective if the UN peacekeeping force can prove and sustain their credibility in the use of force. This stresses the importance of a UN mission maintaining the consent of the peacekept. Ultimately, Fortna finds that peacekeeping is an effective tool for ensuring a lasting peace; especially compared to situations in which belligerents' are left to their own devices. Utilising the previously mentioned causal mechanisms for peacekeeping, a UN peacekeeping force can have a substantial and substantive impact on sustaining a lasting peace. Having a relative consensus of the positive impact of peacekeeping for ensuring a lasting peace, Fortna and Howard suggest that the literature is moving towards the study of i) the effectiveness of the types of peace-keepers, ii) the transitional administrations, iii) the links between peacekeeping and democratisation, and iv) the perspectives of the “peacekept\".\n\nDoyle and Sambanis' analysis finds that lasting peace is more likely after non-ethnic wars in countries with a relatively high level of development in addition to whether or not UN peacekeeping forces and financial assistance are available. They conclude that in the short run lasting peace is more dependent on a robust UN deployment coupled with low levels of hostility between belligerents. They note that increased economic capacity can provide an incentive not to renew hostilities. In the long run, however, economic capacity matters far more whereas the degree of hostility between belligerents is less important. As successful as UN deployments can be, they have inadequately spurred independent economic development within the countries where they have intervened. Thus, the UN plays a strong, but indirect role and success in lasting peace is predicated on the development of institutions that support peace, rather than serving as a deterrent for renewed war.\n\nOther scholarly analyses show varying success rates for peacekeeping missions, with estimate ranging from 31 percent to 85 percent.\n\nThere are many factors that can have a negative impact on lasting peace such as hidden information about the relative strength possessed by the belligerents; a rebel group's involvement in illicit financing through means such as through the export of diamonds and other minerals; participation in the trafficking of drugs, weapons and human beings; whether or not military victory was achieved by one side; the length of the war as well as how costly it was; commitment problems and security dilemma spirals experienced by both sides; whether a cease-fire or treaty signed by the belligerents; lack of transparency in the motives and actions carried out by belligerents in the immediate aftermath of the conflict; extremist spoilers; participants in the conflict that may benefit from its continuation; indivisibility and more.\n\nPerhaps one of the most statistically significant contributors to a lasting peace is whether or not military victory was achieved by one side. According to Fortna's research, civil wars in which one side wins, resulting in a cease-fire or truce, have an approximately 85%-90% lower chance of renewed war. Moreover, peace treaties further reduce the risk by 60%-70%.\n\nIf a group is funded by drugs, diamonds or other illicit trade then there is a substantial increase in the chance of renewed violence—100%-250%-- which is to say that in such circumstances war is two to three-and-a-half times more likely to begin again. While Fortna finds that wars which involve many factions are less likely to resume, Doyle and Sambanis find the opposite.\n\nCostly wars and wars fought along identity lines both provide varied chances of the renewal of violence. While longer wars and peace established by treaty (especially those attained by military victory) can reduce the chances of another war.\n\nSome commentators have highlighted the potential to leverage peacekeeping operations as a mechanism for advancing military normalisation. Michael Edward Walsh and Jeremy Taylor have argued that Japan's peacekeeping operations in South Sudan provide those promoting Japan's military normalisation with \"a unique opportunity to further erode the country’s pacifist constitution.\" \"Unable to accept the full weight of modern peacekeeping operations without fundamental political, legal, and social changes,\" they conclude that \"Japan’s peacekeepers remain ill-prepared to tackle many serious contingencies requiring use of deadly force.\" For this reason, they suggest that Japan's continued participation in UN peacekeeping operations might force policy changes that ultimately push the country toward \"a tipping point from which the normalisation of Japan’s military (will be) the only outcome.\"\n\nDiana Muir Appelbaum, has expressed concern that the creation of a military in Fiji for the purpose of serving in international peacekeeping missions, has produced a military powerful enough to stage four coups d’état (1987, 1999–2000, 2006, and 2009) and to rule Fiji as a military dictatorship for over two decades. However, a 2018 study published in the Journal of Peace Research, found that countries where militaries are highly dependent on the funds they receive from UN peacekeeping were less likely to experience coups d’états than comparable countries less dependent on such funds.\n\nStudies of peacekeeping soldiers show both positive and negative effects. A study of 951 US Army soldiers assigned to Bosnia revealed that 77% reported some positive consequences, 63% reported a negative consequence, and 47% reported both. The peacekeepers are exposed to danger caused by the warring parties and often in an unfamiliar climate. This gives rise to different mental health problems, suicide, and substance abuse as shown by the percentage of former peacekeepers with those problems. Having a parent in a mission abroad for an extended period is also stressful to the peacekeepers' families.\n\nAnother viewpoint raises the problem that the peacekeeping may soften the troops and erode their combat ability, as the mission profile of a peacekeeping contingent is totally different from the profile of a unit fighting an all-out war.\n\nSince the 1990s, UN Peacekeepers have been the subject of numerous accusations of abuse ranging from rape and sexual assault, to pedophilia and human trafficking. Complaints have arisen from Cambodia, East Timor and West Africa. In Bosnia-Herzegovina prostitution associated with trafficked women skyrocketed and often operated just beyond the gates of U.N. compounds. David Lamb, a regional human rights officer in Bosnia from 2000 to 2001 claimed “The sex slave trade in Bosnia largely exists because of the U.N. peacekeeping operation. Without the peacekeeping presence, there would have been little or no forced prostitution in Bosnia.” In addition, hearing held by the U.S. House of Representatives in 2002 found that members of SFOR were frequenting Bosnian brothels and engaging in sex with trafficked women and underage girls.\n\nReporters witnessed a rapid increase in prostitution in Cambodia, Mozambique, Bosnia, and Kosovo after UN and, in the case of the latter two, NATO peacekeeping forces moved in. In the 1996 UN study called \"The Impact of Armed Conflict on Children\", former first lady of Mozambique Graça Machel documented: \"In 6 out of 12 country studies on sexual exploitation of children in situations of armed conflict prepared for the present report, the arrival of peacekeeping troops has been associated with a rapid rise in child prostitution\".\n\nGita Sahgal spoke out in 2004 with regard to the fact that prostitution and sex abuse crops up wherever humanitarian intervention efforts are set up. She observed that the \"issue with the UN is that peacekeeping operations unfortunately seem to be doing the same thing that other militaries do. Even the guardians have to be guarded\".\n\nAn investigation by Prince Zeid Ra’ad Zeid Al-Hussein, then Permanent Representative of Jordan to the United Nations, in 2006 resulted in a comprehensive report which detailed some of this abuse in detail— particularly that which occurred in the Democratic Republic of Congo. Sexual exploitation frequently came in the form of prostitution, wherein some money (an average of $1-$3 per encounter) was exchanged for sex. In other instances food, or jobs were utilized to ply women for sex. Other young women reported of “rape disguised as prostitution”, whereabouts Peacekeepers would rape them and were then given some money or food in order to make the act seem consensual. Between May and September 2004, there were seventy-two allegations of sexual exploitation—68 against military and 4 against civilian personnel. By the end of 2004 there would be a total of 105 allegations. The majority of these allegations were in regards to sex with person under the age of 18 years (45 percent) and sex with adult prostitutes (31 percent). Rape and sexual assault made up approximately 13 and 5 percent respectively, with the remaining 6 percent of allegations relating to other forms of sexual exploitation. Most of the allegations were against peacekeepers from Pakistan, Uruguay, Morocco, Tunisia, South Africa, and Nepal.\n\nUruguayan President Jose Mujica apologized to Haitian President Michel Martelly over the alleged rape of an 18-year-old Haitian man by Uruguayan UN peacekeeping troops. Martelly said \"a collective rape carried out against a young Haitian\" would not go unpunished. Four soldiers suspected of being involved in the rape have been detained.\n\nIn July 2007 the United Nations Department of Peacekeeping Operations (DPKO) confined an entire contingent of 734 Moroccans in the Ivory Coast in the wake of allegations that some had sexually abused underage girls. In the following years, there were 80 investigations carried out by the UN Office of Internal Oversight Services (OIOS). In 2013, allegations were levelled on personnel from France, Gabon, and Burundi operating in the Central African Republic. These include accusations of sexual abuse and exploitation of at least 108 from Kemo Prefecture and that the vast majority of the cases involved minors. In 2016, more allegations of abuse were levelled on Peacekeepers operating in the Democratic Republic of Congo's eastern province of North Kivu. Tanzania and the UN opened a joint inquiry into the alleged abuse, which involved Tanzanian troops. There have been 18 reports of sexual abuse, eight of which involved minors. Sixteen Tanzanian soldiers, a Malawian and a South African are implicated in the accusations. The UN reported in March 2016 that there was a large increase in allegations; which involved troops from twenty one countries. Most of the allegations involved troops from African countries including: Cameroon, Congo, Tanzania, Benin, Burkina Faso, Burundi, Ghana, Madagascar, Niger, Nigeria, Rwanda, Senegal and Togo.\n\nSignificant scientific evidence, first reported by the Associated Press, and later the \"New York Times\", \"Al Jazeera\", and ABC News has shown that Nepalese Peacekeeping troops stationed at a remote base in Mirebalais, Haiti, triggered a deadly cholera epidemic that has ravaged the country since October 2010. Cholera is a waterborne disease that causes diarrhoea and vomiting, and it can kill in a matter of hours if patients do not receive rehydration intervention. As of July 2012, Haiti's cholera epidemic was the worst in the world: about 7,500 had died and about 585,000 Haitians (about 1 in every 20 Haitians) had become ill with the disease.\n\nAccording to the UN-appointed Independent Panel of Experts on the Cholera Outbreak in Haiti, the conditions at the Peacekeeping base were unsafe, and allowed contamination of Haiti's river systems in at least two ways: \"The construction of the water pipes in the main toilet/showering area [was] haphazard, with significant potential for cross-contamination...especially from pipes that run over an open drainage ditch that runs throughout the camp and flows directly into the Meye Tributary System\". Additionally, the Independent Panel reported that on a regular basis black water waste from the Mirebalais base and two other bases was deposited in an open, unfenced septic pit that was susceptible to flooding and would overflow into the Meye Tributary during rainfall.\n\nIn November 2011, over 5,000 victims of the cholera epidemic filed a claim with the UN's internal claims mechanism seeking redress in the form of clean water and sanitation infrastructure necessary to control the epidemic, compensation for individual losses, and an apology. In July 2012, 104 Members of the United States Congress signed a letter affirming that the \"actions of the UN\" had brought cholera to Haiti and that the UN should \"confront and ultimately eliminate cholera\". In 2013 the UN rejected the claim and the victims' lawyers have pledged to sue the UN in court.\n\nThere is a notable intermingling of varied cultures when it comes to peacekeeping. From the vast number of troops, police and personnel that are brought together from various contributing countries to the oftentimes challenging ethnic regions which peacekeeping forces are often deployed. Because of these varied cultures, complicated cultural interactions take place which not only affect mission effectiveness, but can also lead to friction with the population the peacekeepers are meant to be assisting.\n\nIn most cases prior to 1988, specific countries often provided peacekeepers. At that point, only twenty six countries had sent personnel to participate in peacekeeping deployments. Today, that number has risen to more than eighty. This results in an extremely heterogeneous group. Thus, UN Peacekeeping deployments must not only contend with language complications, but also myriad cultural and social differences that can create operational difficulties that are hard to overcome. These difference can create problems with regard to interactions (whether personal or between institutions/units), misunderstandings, inadvertent offensive behaviour and prejudices that may be associated with a particular contingent from a given country.\n\nIn terms of operations, effectiveness can be hindered by the varying tactics, techniques and procedures employed by the military or police personnel that are a part of a given deployment. Because UN forces are cobbled together from so many different sources, there is a discrepancy in capabilities, training, equipment, standards and procedures. Moreover, substantial differences exist in the form of command and control between contributing members personnel. In addition, some nations may not wish to be subordinated to another, complicating unity of command. This can lead to deep-seated divisions between contingents within the UN force that results in a lack of mutual support between units in the field. This can be demonstrated in the experiences of UN peacekeeping forces deployed to East Timor, where the Australians engaged in a robust operation that maximised force protection in contrast to a pro-active heart and minds approach utilised by Great Britain's Ghurka personnel.\n\nMaintaining the consent of the peacekept is an important facet of modern peacekeeping. Notably in Bosnia, Somalia and Rwanda, fundamental principles of retaining that consent was ignored on the grounds of a humanitarian intervention—reflecting the nature of an Article VII intervention. Yet in order to stress and maintain the legitimacy of an intervention it is important that the UN's forces continue to enjoy the consent of the population and government of the country to which they were deployed. This means making the peacekept feel a part of the process in addition to important cultural knowledge of the area in which peacekeepers are operating, in order to reduce friction and provide for a successful operation.\n\nThere has been little study on the interaction of cultures that exist within a peacekeeping force and the population within which they operate. However, in 1976 Galtun and Hveem studied Norwegian personnel who participated in UNEF-1 (in Gaza) and ONUC (Congo). They posited that knowledge of the culture and an understanding of the inhabitants in a given country were not only necessary, but crucial for the success of the mission. They found that personnel from the Norwegian contingent wanted greater insight into the conflict and the culture in which they operated. They also wanted more robust training with regard to working with people from other countries. Yet the study revealed the troops received very little from briefings and that the majority of the information regarding the conflict was gained through the news, reading books or speaking with other UN personnel—rather than any established UN training program.\n\nSimilarly, a study conducted on the relations between members of UNIFIL and local population in Lebanon, carried out by Heiberg and Holst, all but confirmed the findings. In their example, they found that the countries that were able to integrate more fully with the population and show a depth of knowledge about the local culture were more successful, while those that were ambitious, but less integrated into the local scene found themselves far removed from the individuals with which they were supposed to be engaged with, and their success, or lack thereof, illustrated this.Only the Italian contingent of some 2,200 people operated as part of the local environment and became an active element in restoring normal living conditions. Its soldiers were provided with the training required to acquaint them with the cultural, political and social situation of the people among whom they worked. Operating in a sector that contained approximately 600,000 inhabitants, mostly Shi'ites, the Italians carefully nurtured contact with the ordinary citizens and the political leaders in their area... While the Americans thought they were becoming involved in Lebanese politics, they entered into Lebanese culture and history with little or no understanding of the way things worked-- or didn't work... Most Americans did not understand the subtleties of short-term alliances, the length of memories and blood feuds, the strength of \"aln\" [kin] in Arab culture nor the nuances of religious differences.This illustrates the importance of understanding the significance that culture plays in the conduct of successful peacekeeping operations. However, despite the existence of a UN training manual that attempts to advise peacekeepers on necessary techniques, there is no unifying doctrine, or standardised procedure among peacekeeping contingents, which will ultimately hinder the potential for success.\n\nThroughout the duration of the Cold War external intervention and mediation in civil conflicts took on a state-centric mechanism in which sovereignty was inviolable. Rarely did the international community intervene in internal conflicts involving a state's government and domestic belligerents that opposed it. Since the end of the Cold War, however, that has changed. Today, mediation by international actors in civil conflict rest on a standardised resolution mechanism that accords broadly equal standing to all factions within a conflict, and attempts to reach a settlement accepted by all.\n\nThe end of the Cold War presented an opportunity to reshape the international system. This opportunity was afforded to the Cold War's victor's-- that is to say—the United States and other western capitalist states governed by liberal-democratic values that put a premium on basic human rights and democratization. In the preceding decades the state was the only entity to receive special status. While there were exceptions, such as groups struggling against colonial powers, the state possessed the ultimate degree of legitimacy. As a result, the international community rarely meddled with the internal machinations of a given country. Sovereignty was not to be violated and this was a system which benefited both superpowers, their allies, as well as third world governments.\n\nNow, however, with legitimacy being extended to non-state actors, as well as the opportunity for a minority to secede from a given state and form a new country there has been a dramatic shift in the international status quo. Moreover, the international community's model for conflict resolution is heavily influenced by academic thought developed in western countries. This model encourages intervening in civil wars in order to stop political violence and come to a negotiated settlement which often involves democratising efforts. Critics such as Christopher Clapham and David Shearer, argue that this intervention can provide mechanisms for continued conflict to the detriment of the civilian population. Clapham's argument is principally in relation to the situation in Rwanda leading up to the genocide, whereas Shearer focuses on the negative aspects of intervention, primarily regarding Sierra Leone, which prevents total victory by one side and results in the creation of asymmetries between belligerents which opens the door for continued bloodshed.\n\nIn Rwanda, third-party attempts at a negotiated settlement between the Hutu and Tutsi afforded an opportunity for Hutu extremists to prepare for the killing of Hutu moderates and the genocide of the Tutsi. The international community, led by regional states from the Organisation of African Unity, sought to negotiate a settlement and find a solution for the ongoing ethnic violence between Hutu and Tutsi via the Arusha Peace Process. This process lasted just over a year, included substantial international involvement, and incorporated many regional actors such as Tanzania (host of the process), Burundi, Uganda and Zaire.\n\nWhile the Rwandan Patriotic Front (RPF) was a major beneficiary of the Arusha accords and was able to redress many of its grievances, many of the gains that it made could have been achieved through military action. Arusha, according to Clapham, affected the relative power of the participants in the two following ways: a ceasefire which froze the distribution of territorial control at a particular point and secondly the importance it ascribed to the participants of the negotiations. Meaning that it froze the conflict and prevented continued territorial gains being made by the RPF, in addition to designating the degree of importance with regard to the factions within the negotiations. A faction's importance was weighted not on their relative popularity or military strength, but on artificial weight assigned by the mediators. Thus, the entire process served to undermine the RPF's position while stalling their hitherto successful military campaign, while allowing Hutu extremists to prepare for a genocide.\n\nShearer argues that modern strategies that rely solely on consent-based negotiations are severely limited and that victory by military means should not be ignored. He states that a shift in battlefield fortunes can often bring one belligerent to the negotiation table and will likewise moderate their demands.\n\nConsent is of great importance when it comes to negotiation and mediation. The current international system and the conflict resolution model which the international community has utilised most since the end of the Cold War puts a premium on consent. But Shearer asks that if a belligerent uses negotiations and cease-fires as a method of delay in order to allow them to reposition military forces and continue fighting, then should consent-based strategies still be pursued, regardless of the potential for lengthening a conflict and the associated human cost?\n\nAccording to the empirical analysis cited by Shearer, past civil wars with negotiated settlements have had little success. He cites a study from John Stedman that notes between 1900 and 1980 85% of civil wars were solved by one side winning outright (this excludes colonial wars). 15% percent ended as a result of negotiation. Additionally, Roy Licklider's study supports these conclusions by noting the following:\"From 1945 to 1989, 58 out of a total of 93 civil conflicts, as he categorised them, were settled in some form, while the remainder continued. However, only 14 (or 24 percent) of those settled were solved by negotiation. The others (76 percent) ended with military victories. Additionally, fighting resumed in seven of the 14 conflict which were initially ended by negotiation. The overall success rate of negotiated settlements, therefore, was around 12 percent out of the internal wars that ended.\"In Sierra Leone the Revolutionary United Front, led by Foday Sankoh, fought an ongoing and bloody civil war with the government from 1991 to 1996. The conflict attracted little international attention, but managed to devastate the country and destroy its economy. Neither belligerent was willing to concede or compromise on their demands, despite multiple attempts at a negotiated settlement. It was not until the intervention of the private military corporation Executive Outcomes and a reversal in the RUF's battlefield fortunes that Sankoh would come to the table.\n\nIn the aftermath the RUF was a depleted threat, civilians were able to return from refugee camps and begin rebuilding their lives. But the peace was fragile and negotiations were ongoing. The RUF was reluctant to put down their arms, concerned over potential retribution at the hands of army units and civilian militias alike. There was a planned deployment of UN peacekeepers meant to ease these concerns and help with the transition to peace, but things began to unravel. International contributors began to shy away from further peacekeeping initiatives; such as an expensive and open-ended mission in a strategically unimportant country. As a result, the UN's intervention force was slow to come to fruition and then came to a halt completely when Sankoh argued the size of the contingent of 740 UN peacekeepers was too large.\n\nThe UN refused to engage without total consent from both parties, thus preventing the deployment of a peacekeeping force. This consent-based approach, Shearer argues, illustrates the limits the UN can play in the volatile and fragile state of affairs that exist during and after civil wars. \"In Sierra Leone, it meant that an important component needed to shore up the peace-building process was absent. It also meant that Sankoh was dictating terms.\" This consent-based approach effectively allowed the leadership of a brutal rebel group to hinder the potential for peace.\n\nThe situation was exacerbated by the fact that the newly elected President of Sierra Leone terminated the Executive Outcomes contract undermining his hard power advantage. Things were further inflamed when disaffected officers of the army overthrew the government in 1997. The war quickly renewed. A small UN force of monitors was deployed to observe the security situation. UNOMSIL, as it was called, was deployed between July 1998 and October 1999, but was forced to withdraw from the country when the RUF took the country's capitol.\n\nUNAMSIL was eventually formed and deployed in 1999, authorised under a Chapter VII mandate, it was meant to enforce the Lome agreements. However, violence would continue. From the outset the RUF was beyond uncooperative and once the ECOMOG contingent withdrew, the RUF attacked UN forces, eventually taking hundreds hostage. This led to an unexpected backlash from the international community that the RUF did not anticipate. Its leadership had expected the international community to cut and run, as it had done in Somalia and earlier when UNOMSIL fled Freetown. Instead, with British support, an aggressive campaign was waged against the RUF. UNAMSIL's mandate was expanded and its manpower enlarged. By late 2000 and early 2001 the RUF's military strength had been severely depleted. Thus the Abuja agreements were signed and UNAMSIL fulfilled its mandate in December 2005. While Sierra Leone is at peace today and the UN's mission can be deemed a success, the way in which the situation developed illustrates Shearer's point: that a consent-based approach focused on negotiation that encompasses all belligerents' interest may not necessarily lead to success. As we see, fighting continued despite the presence of UNOMSIL. Indeed, even after UNOMSIL was replaced by a more robust force under a Chapter VII mandate in the form of UNAMSIL the violence continued. It was not until the British intervened militarily and substantially degraded the RUF's capability to sustain the conflict, as Executive Outcomes had done years prior, did the RUF finally come to the negotiating table and allow for the establishment of peace.\n\nSome authors question the idea of international interventions at all. In a 2005 working paper for the Center for Global Development, Jeremy Weinstein of Stanford University provides a theory of “autonomous recovery,” in which states can achieve sustainable peace without international intervention. Using case studies of Uganda, Eritrea, and Somalia, Weinstein demonstrates how states can develop effective institutions out of warfare. This method has cost and benefits that must be weighed against the potential outcome of international intervention. External intervention can stop mass atrocities, but also stop institutional change. Autonomous recovery elevates the strongest leader, but also rewards the strongest fighters who may be less inclined to share power. Furthermore, intervention depends on external influence while autonomous recovery is based on internal factors. The conclusions of his argument could suggest intervention is not ideal policy, but Weinstein argues the international community's “responsibility to protect” doctrine has moral importance for intervention and the conditions for “autonomous recovery” are very rare. Weinstein argues the fundamental challenge is how to incentivise good governance and assistance to rebel groups without disrupting the connection of citizens to rulers in terms of revenue collection that enables accountability.\n\nAlthough acknowledging a number of practical and moral reasons for peacekeeping operations, James Fearon and David Laitin assert that they have a tendency under some circumstances to become tangled with state-building efforts. In weak states facing successful guerrilla campaigns, peacekeepers face pressures to build state institutional and administrative capacity in order to achieve lasting peace. These pressures can lead to mission creep beyond the original purview of the peacekeeping operation; without engaging in state-building, the peacekeepers risk allowing the peacekept country to revert to violence following their exit. Thus, Fearon and Laitin advocate for the greater integration of state-building in peacekeeping efforts through a new framework of \"neotrusteeship,\" which would see foreign powers exercising a great deal of control over a weak state's domestic affairs in order to ensure the prevention of future violence.\n\nA growing critique of peacekeeping is the lack of engagement between the peacekeeping officials and the local populace. As Séverine Autesserre outlines in a 2015 Foreign Policy article, this creates an environment where the peacekeeping officials develop plans to ‘keep’ the peace, but they are disconnected from reality, having the opposite effect on the ground. Additionally, it creates a reinforcement mechanism for the peacekeeping officials, because the officials on the ground report that their plan was successfully implemented, but, in reality, it had adverse effects. If the situation on the ground turns into another outbreak of violence, the local populace will be blamed.\n\nThis criticism is similar to the critic levelled at development in developing countries by authors such as James C. Scott, James Ferguson, and L. Lohman. Although peacekeeping and development are two different things, the logic behind the criticism is the same. The third-party officials-whether they are peacekeepers or agents of development-are isolated from the general populace, believing they know what is best, and refusing to gather information from a ground level. This is not out of maliciousness or imperialism, but out of a legitimate belief that they, as educated officials with access to other experts and who are well versed in development and peacekeeping literature, know what is best.\n\nIn response to criticism, particularly of the cases of sexual abuse by peacekeepers, the UN has taken steps toward reforming its operations. The Brahimi Report was the first of many steps to recap former peacekeeping missions, isolate flaws, and take steps to patch these mistakes to ensure the efficiency of future peacekeeping missions. The UN has vowed to continue to put these practices into effect when performing peacekeeping operations in the future. The technocratic aspects of the reform process have been continued and revitalised by the DPKO in its \"Peace Operations 2010\" reform agenda. This included an increase in personnel, the harmonisation of the conditions of service of field and headquarters staff, the development of guidelines and standard operating procedures, and improving the partnership arrangement between the Department of Peacekeeping Operations (DPKO) and the United Nations Development Programme (UNDP), African Union, and European Union. A 2008 capstone doctrine entitled \"United Nations Peacekeeping Operations: Principles and Guidelines\" incorporates and builds on the Brahimi analysis.\n\nOne of the main issues that the Brahimi report identifies is the lack of coordination and planning of the Peacekeeping Operations. Also, the difference between the objectives of the Peacekeeping Operations and the resources destined to fund the missions. Therefore, the report asks the Security Council to make clear the goals and the resources to accomplish them. According to Fearon and Laitin, the Brahimi Report provides a political instrument for the secretary-general to negotiate with the Security Council the goals, the troops, and the resources need it to the operations. This instrument tries to avoid the cases of underfunding presented in Missions such as in Bosnia, Somalia, and Sierra Leone.\n\nChristine Gray analyses the issues of implementing the recommendations of the Brahimi Report. She explains the difficulty in implementing these recommendations. In particular, in reducing the gap between the mandates of Security Council and the actual resources devoted to implementing them.\n\n\n\n"}
{"id": "2160442", "url": "https://en.wikipedia.org/wiki?curid=2160442", "title": "Rodomontade", "text": "Rodomontade\n\nRodomontade \"\\rod-uh-muhn-TADE; roh-duh-muhn-TAHD\\\" is a mass noun meaning boastful talk or behavior. The term is a reference to Rodomonte, a character in Italian Renaissance epic poems \"Orlando innamorato\" and its sequel \"Orlando furioso\".\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "29807596", "url": "https://en.wikipedia.org/wiki?curid=29807596", "title": "Secondary consciousness", "text": "Secondary consciousness\n\nSecondary consciousness is an individual's accessibility to their history and plans. The ability allows its possessors to go beyond the limits of the remembered present of primary consciousness. Primary consciousness can be defined as simple awareness that includes perception and emotion. As such, it is ascribed to most animals. By contrast, secondary consciousness depends on and includes such features as self-reflective awareness, abstract thinking, volition and metacognition. The term was coined by Gerald Edelman.\n\nSince Descartes's proposal of dualism, it became a general consensus that the mind had become a matter of philosophy and that science was not able to penetrate the issue of consciousness- that consciousness was outside of space and time. However, over the last 20 years, many scholars have begun to move toward a science of consciousness. Such notable neuroscientists that have led the move to neural correlates of the self and of consciousness are Antonio Damasio and Gerald Edelman. Damasio has demonstrated that emotions and their biological foundation play a critical role in high level cognition, and Edelman has created a framework for analyzing consciousness through a scientific outlook. The current problem consciousness researchers face involves explaining how and why consciousness arises from neural computation. In his research on this problem, Edelman has developed a theory of consciousness, in which he has coined the terms primary consciousness and secondary consciousness. Despite being an often criticized theory, Edelman’s theory of consciousness is regarded as the most neurobiologically sound and accurate description of consciousness to date. The author puts forward the belief that consciousness is a particular kind of brain process; linked and integrated, yet complex and differentiated.\n\nEdelman argues that the evolutionary emergence of consciousness depended on the natural selection of neural systems that gave rise to consciousness, but not on selection for consciousness itself. He is noted for his theory of neuronal group selection, also known as Neural Darwinism, which displays the belief that consciousness is the product of natural selection. He believes consciousness is not something separate from the real world, thus the attempt to eliminate Descartes’ \"dualism\" as a possible consideration. He also rejects theories based on the notion that the brain is a computer or an instructional system. Instead, he suggests that the brain is a selectional system, one in which large numbers of variant circuits are generated epigenetically. He claims the potential connectivity in the neural net \"far exceeds the number of elementary particles in the universe\" \n\nDynamic core hypothesis \n\nEdelman elaborates on the dynamic core hypothesis (DCH), which describes the thalamocortical region- the region believed to be the integration center of consciousness. The DCH reflects the use and disuse of interconnected neuronal networks during stimulation of this region. It has been shown through computer models that neuronal groups existing in the cerebral cortex and thalamus interact in the form of synchronous oscillation. The interaction between distinct neuronal groups forms the \"dynamic core\" and may help explain the nature of conscious experience.\n\nEdelman integrates the DCH hypothesis into Neural Darwinism, in which metastable interactions in the thalamocortical region cause a process of selectionism through re-entry, a host of internal feedback loops. \"Re-entry\", as Edelman states, \"provides the critical means by which the activities of distributed multiple brain areas are linked, bound, and then dynamically altered in time during perceptual categorization. Both diversity and re-entry are necessary to account for the fundamental properties of conscious experience.\" These re-entrant signals are reinforced by areas Edelman calls \"degenerate\". Degeneracy doesn't imply deterioration, but instead redundancy as many areas in the brain handle the same or similar tasks. With this brain structure emerging in early humans, selection could favor certain brains and pass their patterns down the generations. Habits once erratic and highly individual ultimately became the social norm.\n\nWhile animals with primary consciousness have long-term memory, they lack explicit narrative, and, at best, can only deal with the immediate scene in the remembered present. While they still have an advantage over animals lacking such ability, evolution has brought forth a growing complexity in consciousness, particularly in mammals. Animals with this complexity are said to have secondary consciousness.\nSecondary consciousness is seen in animals with semantic capabilities, such as the four great apes. It is present in its richest form in the human species, which is unique in possessing complex language made up of syntax and semantics. In considering how the neural mechanisms underlying primary consciousness arose and were maintained during evolution, it is proposed that at some time around the divergence of reptiles into mammals and then into birds, the embryological development of large numbers of new reciprocal connections allowed rich re-entrant activity to take place between the more posterior brain systems carrying out perceptual categorization and the more frontally located systems responsible for value-category memory. The ability of an animal to relate a present complex scene to its own previous history of learning conferred an adaptive evolutionary advantage. At much later evolutionary epochs, further re-entrant circuits appeared that linked semantic and linguistic performance to categorical and conceptual memory systems. This development enabled the emergence of secondary consciousness.\n\nSelf-recognition\n\nFor the advocates of the idea of a secondary consciousness, self-recognition serves as a critical component and a key defining measure. What is most interesting then, is the evolutionary appeal that arises with the concept of self-recognition. In non-human species and in children, the \"mirror test\" has been used as an indicator of self-awareness. In these experiments, subjects are placed in front of a mirror and provided with a mark that cannot be seen directly but is visible in the mirror.\n\nThere have been numerous findings in the past 30 years which display fairly clear evidence of possessors of self-recognition including the following animals:\nIt should be mentioned that even in the chimpanzee, the species most studied and with the most convincing findings, clear-cut evidence of self-recognition is not obtained in all individuals tested. Occurrence is about 75% in young adults and considerably less in young and old individuals. For Monkeys, non-primate mammals, and in a number of bird species, exploration of the mirror and social displays were observed. However, hints at mirror-induced self-directed behavior have been obtained.\n\nIt was recently thought that self-recognition was restricted to mammals with large brains and highly evolved social cognition but absent from animals without a neocortex. However, in a recent study, an investigation of self-recognition in corvids was carried out, and significant result quantified the ability of self-recognition in the magpie. Mammals and birds inherited the same brain components from their last common ancestor nearly 300 million years ago, and have since independently evolved and formed significantly different brain types. The results of the mirror and mark tests showed that neocortex-less magpies are capable of understanding that a mirror image belongs to their own body. The findings show that magpies respond in the mirror and mark test in a manner similar to apes, dolphins and elephants. This is a remarkable capability that, although not fully concrete in its determination of self-recognition, is at least a prerequisite of self-recognition. This is not only of interest regarding the convergent evolution of social intelligence; it is also valuable for an understanding of the general principles that govern cognitive evolution and their underlying neural mechanisms. The magpies were chosen to study based on their empathy/ lifestyle, a possible precursor for their ability of self-awareness.\n\nMany researchers of consciousness have looked upon such types of research in animals as significant and interesting approaches. Ursula Voss of the Universität Bonn believes that the theory of protoconsciousness may serve as adequate explanation for self-recognition found in this bird species, as they would develop secondary consciousness during REM sleep. She added that many types of birds have very sophisticated language systems. Don Kuiken of the University of Alberta finds such research interesting as well as if we continue to study consciousness with animal models (with differing types of consciousness), we would be able to separate the different forms of reflectiveness found in today’s world.\n\nIn the last couple of decades, dream research has begun to focus on the field of consciousness. Through lucid dreaming, NREM sleep, REM sleep, and waking states, many dream researchers are attempting to scientifically explore consciousness. When exploring consciousness through the concept of dreams, many researchers believe the general characteristics that constitute primary and secondary consciousness remain intact:\n\"Primary consciousness is a state in which you have no future or past, a state of just being…. no executive ego control in your dreams, no planning, things just happen to you, you just are in a dream. Yet, everything feels real…Secondary is based on language, has to do with self-reflection, it has to do with forming abstractions, and that is dependent of language. Only animals with language have secondary consciousness\".\n\nThere have been studies used to determine what parts of the brain are associated with lucid dreaming, NREM sleep, REM sleep and waking states. The goal of these studies is often to seek physiological correlates of dreaming and apply them in the hopes of understanding relations to consciousness.\n\nSome notable, albeit criticized findings include the functions of the prefrontal cortex that are most relevant to the self-conscious awareness that is lost in sleep, commonly termed as ‘executive’ functions. These include self-observation, planning, prioritizing and decision-making abilities, which are, in turn, based upon more basic cognitive abilities such as attention, working memory, temporal memory and behavioral inhibition Some experimental data which display differences between the self-awareness experienced in waking and its diminution in dreaming can be explained by deactivation of the dorsolateral prefrontal cortex during REM sleep. It has been proposed that deactivation results from a direct inhibition of the dorsolateral prefrontal cortical neurons by acetylcholine, the release of which is enhanced during REM sleep.\n\nExperiments and studies have been taken out to test neural correlations of lucid dreams with consciousness in dream research. Although there are many difficulties in conducting lucid dreaming research (e.g. number of lucid subjects, ‘type’ of lucidity achieved, etc.), there have been studies with significant results.\n\nIn one study, researchers sought physiological correlates of lucid dreaming. They showed that the unusual combination of hallucinatory dream activity and wake-like reflective awareness and agentive control experienced in lucid dreams is paralleled by significant changes in electrophysiology. Participants were recorded using 19-channel Electroencephalography (EEG), and 3 achieved lucidity in the experiment. Differences between REM sleep and lucid dreaming were most prominent in the 40-Hz frequency band. The increase in 40-Hz power was especially strong at frontolateral and frontal sites. Their findings include the indication that 40-Hz activity holds a functional role in the modulation of conscious awareness across different conscious states. Furthermore, they termed lucid dreaming as a hybrid state, or that lucidity occurs in a state with features of both REM sleep and waking. In order to move from non-lucid REM sleep dreaming to lucid REM sleep dreaming, there must be a shift in brain activity in the direction of waking.\nOther well-known contributing scholars involved with lucid dream research and consciousness, yet primarily based in fields such as psychology and philosophy include:\n\nThe theory of protoconsciousness, developed by Allan Hobson, a creator of the Activation-synthesis hypothesis, has been developed through dream research and involves the idea of a secondary consciousness. Hobson suggests that brain states underlying waking and dreaming cooperate and that their functional interplay is crucial to the optimal functioning of both. Ultimately, he proposes the idea that REM sleep provides opportunities to the brain to prepare itself for its main integrative functions, including secondary consciousness, which would explain the developmental and evolutionary considerations to be taken with birds. This functional interplay which occurs during REM sleep constitutes a ‘proto-conscious’ state which preludes consciousness and can develop and maintain higher order consciousness.\n\nAs the activation-synthesis hypothesis has evolved, it has metamorphosed into the three-dimensional framework known as the AIM model. The AIM model describes a method of mapping conscious states onto an underlying physiological state space. The AIM model relates not just to wake/sleep states of consciousness, but to all states of consciousness. By choosing activation, input source, and mode of neuromodulation as the three dimensions, the proposers believe to have selected \"how much information is being processed by the brain (A), what information is being processed (I), and how it is being processed (M).\n\nHobson, Schott, and Stickgold propose three aspects of the AIM model:\n\n\nSecondary consciousness, as it remains a controversial topic, has received often contrasting findings and beliefs regarding lucid dreaming as a model, which entails the true difficulty in understanding consciousness.\n\nThe most common of recent criticisms include:\n\n"}
{"id": "23332738", "url": "https://en.wikipedia.org/wiki?curid=23332738", "title": "Self-image", "text": "Self-image\n\nSelf-image is the mental picture, generally of a kind that is quite resistant to change, that depicts not only details that are potentially available to objective investigation by others (height, weight, hair color, etc.), but also items that have been learned by that person about themself, either from personal experiences or by internalizing the judgments of others.\n\nSelf-image may consist of four types:\n\nThese four types may or may not be an accurate representation of the person. All, some or none of them may be true.\n\nA more technical term for self-image that is commonly used by social and cognitive psychologists is self-schema. Like any schema, self-schemas store information and influence the way we think and remember. For example, research indicates that information which refers to the self is preferentially encoded and recalled in memory tests, a phenomenon known as \"self-referential encoding\". Self-schemas are also considered the traits people use to define themselves, they draw information about the self into a coherent scheme.\n\nPoor self-image may be the result of accumulated criticisms that the person collected as a child which have led to damaging their own view of themselves. Children in particular are vulnerable to accepting negative judgments from authority figures because they have yet to develop competency in evaluating such reports. Also, adolescents are highly targeted to suffer from poor body image issues. Individuals that already exhibit a low-sense of self-worth may be vulnerable to develop social disorders.\n\nNegative self-images can arise from a variety of factors. A prominent factor, however, is personality type. Perfectionists, high achievers, and those with \"type A\" personalities seem to be prone to having negative self-images. This is because such people constantly set the standard for success high above a reasonable, attainable level. Thus, they are constantly disappointed in this \"failure.\"\n\nAnother factor that contributes to a negative self-image is the beauty values of the society in which a person lives. In the American society, a popular beauty ideal is thinness. Oftentimes, girls feel that they do not measure up to society's \"thin\" standards, which leads to them having a negative self-image.\n\nWhen people are in the position of evaluating others, self-image maintenance processes can lead to a more negative evaluation depending on the self-image of the evaluator. That is to say stereotyping and prejudice may be the way individuals maintain their self-image. When individuals evaluate a member of a stereotyped group, they are less likely to evaluate that person negatively if their self-images had been bolstered through a self-affirmation procedure, and they are more likely to evaluate that person stereotypically if their self-images have been threatened by negative feedback. Individuals may restore their self-esteem by derogating the member of a stereotyped group.\n\nFein and Spencer (1997) conducted a study on Self-image Maintenance and Discriminatory Behavior. This study showed evidence that increased prejudice can result from a person's need to redeem a threatened positive perception of the self. The aim of the study was to test whether a particular threat to the self would instigate increased stereotyping and lead to actual discriminatory behavior or tendencies towards a member of a \"negatively\" stereotyped group. \nThe study began when Fein and Spencer gave participants an ostensible test of intelligence. Some of them received negative feedback, and others, positive and supportive feedback. In the second half of the experiment, the participants were asked to evaluate another person who either belonged to a \"negatively stereotyped group\", or one who did not. \nThe results of the experiment proved that the participants who had previously received unfavorable comments on their test, evaluated the target of the \"negatively stereotyped group\" in a more antagonistic or opposing way, than the participants who were given excellent reports on their intelligence test. They concluded that the negative feedback on the test threatened the participants' self-image and they evaluated the target in a more negative manner, all in efforts to restore their own self-esteem.\n\nA present study extends the studies of Fein and Spencer in which the principal behavior examined was avoidance behavior. In the study, Macrae et al. (2004) found that participants that had a salient negative stereotype of \"skinheads\" attached, physically placed themselves further from a skinhead target compared to those in which the stereotype was not as apparent. Therefore, greater salience of a negative stereotype led participants to show more stereotype-consistent behavior towards the target.\n\nResidual self-image is the concept that individuals tend to think of themselves as projecting a certain physical appearance, or certain position of social entitlement, or lack thereof. The term was used at least as early as 1968, but was popularized in fiction by the \"Matrix\" series, where persons who existed in a digitally created world would subconsciously maintain the physical appearance that they had become accustomed to projecting.\n\nVictims of abuse and manipulation often get trapped into a self-image of victimisation. The psychological profile of victimisation includes a pervasive sense of helplessness, passivity, loss of control, pessimism, negative thinking, strong feelings of self-guilt, shame, self-blame and depression. This way of thinking can lead to hopelessness and despair.\n\nSelf-image disparity was found to be positively related to chronological age (CA) and intelligence, two factors thought to increase concomitantly with maturity: Capacity for guilt and ability for cognitive differentiation. However, males had larger self-image disparities than females, Caucasians had larger disparities and higher ideal self images than African Americans, and socioeconomic status (SES) affected self-images differentially for the 2nd and 5th graders.\n\nA child's self-awareness of who they are differentiates into three categories around the age of five: their social self, academic persona, and physical attributes. Several ways to strengthen a child's self-image include communication, reassurance, support of hobbies, and finding good role models.\n\nWhen does a child become aware that the image in a mirror is their own? Research was done on 88 children between 3 and 24 months. Their behaviors were observed before a mirror. The results indicated that children's awareness of self-image followed three major age-related sequences: \n\nRegular practice of endurance exercise was related to a more favourable self-image. There was a strong association between participation in sports and the type of personality that tends to be resistant to drug and alcohol addiction. Physical exercise was further significantly related to scores for physical and psychological well-being. Adolescents who engaged regularly in physical activity were characterised by lower anxiety-depression scores, and displayed much less social behavioural inhibition than their less active counterparts.\n\nIt is likely that discussion of recreational or exercise involvement may provide a useful point of entry for facilitating dialogue among adolescents about concerns relating to body image and self-esteem. In terms of psychotherapeutic applications, physical activity has many additional rewards for adolescents. It is probable that by promoting physical fitness, increased physical performance, lessening body mass and promoting a more favourable body shape and structure, exercise will provide more positive social feedback and recognition from peer groups, and this will subsequently lead to improvement in an individual's self-image.\n\nDoes self-image threatening feedback make perceivers more likely to activate stereotypes when confronted by members of a minority group? Participants in Study 1 saw an Asian American or European American woman for several minutes, and participants in Studies 2 and 3 were exposed to drawings of an African American or European American male face for fractions of a second. These experiments found no evidence of automatic stereotype activation when perceivers were cognitively busy and when they had not received negative feedback. When perceivers had received negative feedback, however, evidence of stereotype activation emerged even when perceivers were cognitively busy.\n\nA magazine survey that included items about body image, self-image, and sexual behaviors was completed by 3,627 women. The study found that overall self-image and body image are significant predictors of sexual activity. Women more satisfied with body image reported more sexual activity, orgasm, and initiating sex, greater comfort undressing in front of their partner, having sex with the lights on, trying new sexual behaviors (e.g. anal sex), and pleasing their partner sexually than those dissatisfied. Positive body image was inversely related to self-consciousness and importance of physical attractiveness, and positively related to relationships with others and overall satisfaction. Body image was predictive only of one's comfort undressing in front of partner and having sex with lights on. Overall satisfaction was predictive of frequency of sex, orgasm, and initiating sex, trying new sexual behaviors, and confidence in giving partner sexual pleasure.\n\nOne hundred and ten heterosexual individuals (67 men; 43 women) responded to questions related to penis size and satisfaction. Men showed significant dissatisfaction with penile size, despite perceiving themselves to be of average size. Importantly, there were significant relationships between penile dissatisfaction and comfort with others seeing their penis, and with likelihood of seeking medical advice with regard to penile and/or sexual function. Given the negative consequences of low body satisfaction and the importance of early intervention in sexually related illnesses (e.g., testicular cancer), it is imperative that attention be paid to male body dissatisfaction.\n"}
{"id": "23517808", "url": "https://en.wikipedia.org/wiki?curid=23517808", "title": "Social cleansing", "text": "Social cleansing\n\nSocial cleansing () is class-based killing that consists of elimination of members of society considered \"undesirable,\" including but not limited to the homeless, criminals, street children, the elderly, sex workers, and sexual minorities. This phenomenon is caused by a combination of economic and social factors, but killings are notably present in regions with high levels of poverty and disparities of wealth. Perpetrators are usually of the same community as the victims and are often motivated by the idea that the victims are a drain on the resources of society. Efforts by national and local governments to stop these killings have been largely ineffective, and the government and police forces are often involved in the killings, especially in South America.\n\nIn African countries, social cleansing almost always takes the form of witch hunting, which is most common in areas with poor economic circumstances. Several social and economic theories exist as to why such circumstances have arisen and led to accusations of witchcraft, including warfare, natural disasters, unequal patterns of development, and larger forces of globalization. Most scholars agree that the cause of social cleansing efforts is a result of \"interaction of economic conditions and cultural factors.\" All of these theories must be linked to larger societal trends, including the devaluation and social marginalization of women as well as the placement of blame on individuals for their own economic misfortunes in lieu of recognition of global and local economic forces at play. However, several scholars have emphasized the outside groups and circumstances related to these killings to dispute the idea that they are simply a cultural norm.\n\nIn many countries, income disparities have led to social tensions and a climate of \"mutual suspicion\". The wealthy and powerful are perceived as having obtained their wealth through \"evil arts\", while the economically disadvantaged are accused of responsibility for misfortunes of the community. There is also evidence that the causes of social cleansing are linked to globalization and economic liberalization, \"to the extent that it has stripped entire populations of their means of subsistence, torn communities apart, deepened economic inequalities and forced people to compete for diminishing resources.\" Many African communities have been destabilized as communal lands have been privatized, local currency has been devalued, and public services have been eliminated.\nSometimes these larger economic trends have been linked to more specific events. For example, in Southern Zimbabwe, violent wars led certain areas to be neglected in development efforts, leading to a lack of resources and increasing disparities of wealth in these areas. In Tanzania, scholars have found positive correlations between extreme rainfall (both floods and droughts) and large negative income shocks and famine. These periods have been statistically linked to increases in murder of witches.\n\nSeveral cultural explanations for social cleansing in Africa are related to religion. One that has been offered by scholars is the presence of Pentecostalists, whose focus on the occult has been spread by the media and increased social anxiety. Pentecostalists have been recorded as preaching connections between illness and the devil, which has combined potently with existing indigenous beliefs, most notably in Kenya, Ghana, Nigeria, and Tanzania. In Tanzania, a positive correlation exists between witch killing and areas where populations practice traditional religions, where belief in witchcraft is strong. While scholars have suggested that the presence of these beliefs is important because it demonstrates that perpetrators generally do believe that their victims are practicing witchcraft, they also recognize that populations of traditional religious beliefs are often also of low socio-economic status, which supports their assertion that poverty is still the primary factor in motivating killings.\n\nThe most widespread myth about social cleansing in Latin America is that these killings are all related to drug use. However, the phenomenon is larger than the drug problem and is related to state ideology, a culture of violence, and inequitable wealth distribution. Within Colombia specifically, economic factors account for many of the reasons behind these killings, but such factors are additionally \"aggravated by external political and economic pressures from the United States\".\n\nLatin America has an extremely large number of individuals living below the poverty line, and these individuals are largely blamed for their impoverished state. Many of these individuals are in critical poverty, meaning that they do not even have the ability to secure food and shelter. This critical poverty is connected to inflation rates that has led the cost of living to increase and the minimum salary to be hardly adequate for survival. Since the 1990s, the gap between the rich and poor has widened, and funds for welfare programs and social services has decreased while funding for security forces to protect \"the haves from the have-nots\" have tripled in Colombia specifically.\n\nLatin America's history has long been plagued by political violence, which over time has morphed into class-based violence. Despite mostly formally democratic governments, the \"legacy of authoritarianism\" lingers, and the presence of \"armed actors\" is prevalent as a result of a long history of violence between military, paramilitary, and guerilla groups. The presence of this culture of violence has had various effects on the underclass in countries in Latin America. The military and especially the police have been known to use violence to harm citizens rather than protect them. Private \"vigilante\" security forces have likewise used violence against the poor with the idea of promoting law and order, especially in Colombia, Guatemala, and Peru. Though many guerilla groups are much less violent than when they originally emerged, they are a presence and an additional source of violence, especially in Colombia. Tension between political groups has led these guerilla groups, the government, and vigilante actors to suspect peasants of working with their enemy and to intimidate them into leaving land in the countryside for the city. Other poor rural residents have been forced to leave due to general violence or lack of public services. Violence at the local level is also extremely common by organized criminals such as street gangs, drug bosses, vigilante justice groups, and local civil patrols. When poor residents are forced to move to the city, they often must turn to prostitution, crime, or begging in inner-city ghettos, which puts them in an extremely vulnerable position in the presence of these violent groups. Men in particular become even more entrenched in the culture of violence as many join gangs to escape \"social exclusion and economic disadvantage\" and establish a sense of identity and masculinity. Finally, violence exists at a level even smaller than the community—the home. Children are often victims of \"physical, mental, and sexual abuse by adults member of their own families.\" In Guatemala specifically, social cleansing occurs with the \"backdrop of genocide,\" and homicide rates are still extremely high after \"three decades of armed conflict\" during the Guatemalan Civil War. Violence experienced across the region has led to an erosion of social capital, which was described by Colombians as including \"'social mistrust,' 'lack of unity,' 'fear' and 'lack of social institutions.'\" \n\nSpain, during times before colonization, demonstrates societal patterns that shaped life in Spain in terms of prejudice and discrimination. The discriminatory practice of Spanish legislation led to a certain caste system, pertaining to those with or without honor. Moreover one's reputation and the way one was treated was based on aspects such as honor, legitimacy, and the limpieza de sangre, a prejudicial marker that indicates one’s purity of blood, relating to their family timeline and deciding how society was to treat them.  “Historic meanings of honor included those cultural specific ways that Spaniards had always rationalized discrimination due to defects in birth (illegitimate, nonnoble), religion (non-Catholic), and race (nonwhite).” This discriminatory institution based on these concepts of honor, limpieza, and legitimacy, is expressed through the legislation of Spanish government at the time. An example of this is was in 1414 when Pope Benedict XIII approved the constitution of Spanish College’s San Bartolomé, which linked these concepts through the school's acceptances. Those excepted into the school had to prove they had pure blood, rather than Jewish, Moorish, or heretical heritage.\n\nThe concept of \"limpieza de sangre,\" or purity of blood, was used in societies of Spain and Portugal, originating from Iberian culture, where reputation was inherited by one's ancestors. Had there been someone found to be a race such as Jewish, a converted Jewish, or Muslim in their family timeline, it was said to be a stain on their ancestry. Consequently, one's own ancestry determined their reputation and social standing, impacting other aspects such as access to education, career, and marriage for further generations. This became known as the culture of honor, which resided in such reputational ancestry that came to define how individuals were respected.\n\nLimpieza de sangre affected life for every individual in the Spanish and Portuguese colonies, including the degree to exclusion and racial discrimination. When Europe came to colonize the \"New World\" these ethics of honor and limpieza de sangre implied that those with blood lacking purity to European standards was inferior. The Europeans were at the \"apex\" of social structure and everyone who was not identical in blood was inferior. As colonists, the presumed inferiors were the colonized and thus, due to this culture of honor, they became victims of this discrimination. While over time assimilation and miscegenation and admixture complicated this concept of social cleansing, the ideals of \"purity of blood\" prevailed, and elite was considered to be of European and Christian origins.\n\nThe conquest of indigenous people in Latin America strengthened these ideals. The vulnerability of one's identity in colonial America gave way to ones defense of honor, except for the elites and those in power, typically the colonists. As the degree of honor perceived by individuals was reputational, people felt the need to be confirmed by society or from those in good social standing through submission into the given standards that one's place holds. The pursuit of this honor leads to many disputes, as well as the fear of being rejected by society and losing one's place.\n\nDespite these long-lived standards, during post colonialism, the ideals of Latin America changed with independence and the growth of democratic values. With this, the culture of honor and respect in those with pure bloodlines changed. People began to socially include those who were previously seen as inferior.\n\nThe most common murder victim in Brazil is a young, black male living in a favela, or a Brazilian slum. These young men typically are, or are assumed to be, gang members and criminals. Violence and murder are most common in areas that are economically disadvantaged and socially marginalized. Women are often targets by association, though the effects of social cleansing and violence against women are largely absent in existing research. Killings are often in public places, with victims being beaten or shot in the street. Police groups sometimes simply enter the community in a large armoured vehicle called a \"caveirão\" and start shooting. This vehicle contributes to the \"anonymity and impunity for the perpetrators.\"\n\nSocial cleansing in Brazil is the result of a \"murky symbiosis [that] has developed between the official security forces and paramilitary and vigilante-type actors\" that carry out \"law enforcement against the 'marginal classes.'\" State actors often act against the poor \"as a form or result of exclusion and oppression.\" Gangs serve as a scapegoat for the levels of violence and lack of security in many communities. Private groups and some gangs perpetrate killings in attempts to take policing into their own hands. While some killings are a result of groups attempting to punish criminals for misdemeanors, others are a result of perceived threats of poor citizens, such as members of workers' movements.\n\nIn the 1980s, \"social cleansing\" groups started to be created. Their main mission was \"to make justice\" by killing all non-appropriate people in social terms, like prostitutes, street-living people, transvestites, and drug addicts. \n\nVictims of social cleansing in Colombia are members of society who are considered \"undesirable\" and \"disposable.\" They are economically disadvantaged, usually live on the streets, and are considered to be a burden on society, \"the cause of the country's problems, rather than a consequence of them.\"\n\nOne of these groups is street children, who are without homes due to abuse, forced displacement, or the death of their parents. Death rates for street children have been as high as six to eight children per day. Children are often shot in their sleep or stabbed to death on the streets or in the police station. A 1993 case in which a 9-year-old girl was strangled to death in Bogota brought attention to this problem and led to nominal reforms. The National Police targets street children specifically under the assumption that they are drug users and criminals. This is to some extent true, as many use drugs to relieve pain and avoid hunger and must shoplift to survive. Despite living in conditions of extreme vulnerability to \"aggression and danger,\" the National Police poses the greatest threat to street children's survival, as they drive them off the streets and target them in social cleansing.\n\nPoor criminals, drug users, and drug dealers are also common targets. From 1988 to 1993, these individuals collectively comprised 56% of social cleansing victims. These individuals are often victims of physical and sexual abuse by the police and vigilante groups known as \"comas\". One common method of killing these individuals in the city of Bogota is the Choachí run, in which victims are taken to the top of a mountain in the town of Choachí, executed, and thrown off the mountain. In some cases, they are released to attempt escape, but die by falling down the mountain or being shot at as they run. A similar method is known as \"the ride,\" in which victims are forced into a vehicle, killed, and left in a desolate area. Death squads have employed other means for killing suspected criminals, such as murdering them and then cutting off their hands and putting them in small boxes in public spaces to intimidate other criminals. Another group has been known to shoot victims and then cast their bodies into the municipal stadium.\n\nSex workers and sexual minorities are treated quite similarly in regards to social cleansing, as both are hard to identify and victims of heavy discrimination despite the fact that both homosexuality and prostitution are legal. Many female sex workers are forced into the line of work due to poverty and domestic violence. Both male and female sex workers are often harassed by the police, and males specifically are demanded to pay a \"tax\" where \"failure to pay results in beatings or imprisonment.\" In an upside-down system, encounters with dishonest police are preferable to honest police, as the dishonest will accept bribes, while the honest are more likely to kill. Sexual minorities are particularly difficult to identify, because not only do some male sex workers participate in gay sex out of economic necessity, but victims are only considered homosexual if they were dressed as females at the time of death.\n\nAnother group includes those individuals in the most extreme form of poverty. These individuals subsist by asking for money and/or collecting garbage. The police have been known to kill these victims in especially cruel ways, such as pouring gasoline on them and setting them on fire. It is also notable that at least 14 destitute individuals have been killed by security guards at one Colombian university for the use of their bodies as cadavers in the medical school.\n\nThe National Police has played a large role in carrying out class-based killings in Colombia. The police created the term \"disposable\" (Spanish: \"desechable\") to define economically disadvantaged people who are considered to have no value to society. Whether directly or through \"paramilitary clients,\" the National Police was responsible for 74% of deaths related to social cleansing in 1992. Motivations include \"security, aesthetics, economic well-being, morals, and religion.\" In regards to safety and economic well-being, rationales include the idea that the poor are or look like criminals and decrease public safety and drive customers away from businesses. Moral arguments include protection from homosexuals and prostitutes.\n\nThe distinction between death squads and paramilitary groups and the National Police is not always clear. Not only are policemen often members of these groups, but these groups typically enjoy the protection of the police. Other members include businessmen, industrialists, guerillas, and soldiers. Death squads emerged in the late 1970s, one of the first being the Black Hand, a group that murdered suspected criminals. Their reasoning behind these killings is the flawed legal system, which convicts less than 3% of criminals. Death squads and other groups believe it necessary to step in where the legal system has failed by eliminating these suspected criminals. As of 1995, there were no less than 40 of these squads operating in Colombia.\n\nThe role of the national government in Colombia has largely been complicit cooperation with the National Police. By failing to deal with crime and then also effectively granting impunity to police and military groups, the state has allowed safety issues to be addressed with violence and has perpetuated a \"cycle of crime, lack of public safety, violent response, and impunity\" due to \"terrifying inefficiency and unwillingness to hold people accountable for their acts\". While the government at least tries to protect street children through programs to put them in state-run homes, these programs often do not align with their actual needs and have largely failed. There have been limited attempts to protect those in poverty in the legal system, and one case of harassment against the poor resulted in a judge calling for their equal treatment and compensation from the offending policemen. However, the decision was not enforced.\n\nSocial cleansing and gang killings make up a large portion of the homicides in Guatemala. Since gangs typically make no effort to cover up crimes and leave bodies at the place of death, \"signs of torture,\" as well as location of the body, \"serve as indicators of the existence of social cleansing,\" according to Elizabeth Sanford. According to a study by the Human Rights Ombudsman, \"the increase in the number of women killed whose bodies bore marks of torture and other sadistic abuse accounted for 40 percent of the total increase in female murders in 2005.\" Though female victims account for 10% of all homicides, over 18% of cadavers with signs of torture indicating social cleansing were female. Furthermore, 2% of female victims of homicide are prostitutes, a common victim group of social cleansing efforts. Furthermore, young and destitute male gang members, especially those blamed for homicide of females, have been common victims of social cleansing.\nThe most common form of killing, indicated by the 305 cadavers found in 2005, is strangulation. Other common methods included victims being beaten, shot in the head, bound by their hands and feet, and in the case of female victims, sexual abuse. Victims are abducted, taken to a different location, are tortured and killed, and finally have their body dumped in a different location.\n\nPerpetrators include the Guatemalan government as well as private groups either directly or indirectly complicit with the state. Social cleansing efforts are targeted against gangs and other perceived and actual criminals, who are blamed for the high rates of homicide. The perpetrators intend to both exterminate victims and intimidate other members of the target group. Intimidation is carried out both through torture tactics used as well as propaganda including flyers and stickers that support social cleansing as a \"method of social control.\" Because perpetrators are directly or indirectly tied to the state, they naturally have impunity.\n\nThe use of \"social cleansing\" efforts to eliminate criminals and other persons deemed to be socially dangerous has its origins during the period of military dictatorship and civil war (1954-1996). During the 1960s and 70s, many state-operated paramilitary front organizations (so-called \"death squads\") emerged with the express purpose of exterminating suspected communists and other enemies of the state. These groups included the MANO, NOA, CADEG, 'Ojo por Ojo' and others. While nominally employed against political targets, the use of \"death squads\" came to be seen by the Guatemalan police forces (specifically the National Police) as a crime fighting tool, particularly after the election of Col. Arana Osorio in 1970 and the subsequent \"state of siege\". One early example of the use of \"death squads\" against non-political targets was a phantom organization called the 'Avenging Vulture', which specifically targeted criminals.\n\nThe most common victims of social cleansing efforts in Tanzania are elderly women, the majority of whom are of low socioeconomic status, but several groups of people who are considered burdens to the community, such as children, the sick, infants, and the handicapped, are also victims. These people are usually accused of witchcraft following deaths or other misfortunes in society and tend to flee, choosing homelessness over death. Those who do not flee successfully are killed violently in their homes. Sometimes those considered burdens are simply reduced to zero consumption and are starved to death. This occurs particularly among infants, who have no ability to flee or attempt to provide for themselves.\n\nVictims are typically killed by members of their own families, who blame them for economic suffering and household misfortune. Accusations and subsequent killings are often incited by death or illness in the family or the family's livestock. However, general misfortune in the form of \"failed crops, lost jobs, and bad dreams also arouse suspicion.\" While often accusations are raised to the effect of creating a scapegoat, not all forms of social cleansing are connected to witch hunting. The extreme scarcity theory suggests that some families to drive out or starve unproductive family members to provide more nutrients for other members. Many of these perpetrators are young, unemployed men who see the elderly as a burden on their potential for success. Another key perpetrator of social cleansing in Tanzania are the Sungusungu, councils of male elders that operate under the premise of promoting village security. These groups formed under the premise that the government was not able to prevent crimes such as theft, and they serve as a form of vigilante justice.\n\nAlthough the Tanzanian government has made public witchcraft accusations illegal, the efforts to stop them have been unsuccessful. Conviction levels are extremely low, as \"only seven of 1,622 individuals arrested in connection with witch killings during the 1970s and 1980s were convicted, and since then the conviction rate as apparently fallen even lower,\" according to Edward Miguel. The perception of the government and police force as unable to control crime has led groups such as Sungusungu to take matters into their own hands, though studies suggest that the police may sometimes be involved in witch killings.\n\n\n"}
{"id": "1968559", "url": "https://en.wikipedia.org/wiki?curid=1968559", "title": "Substitution principle (sustainability)", "text": "Substitution principle (sustainability)\n\nThe substitution principle in sustainability is the maxim that processes, services and products should, wherever possible, be replaced with alternatives which have a lower impact on the environment. An example of a strong, hazard-based interpretation of the principle in application to chemicals is: \"that hazardous chemicals should be systematically substituted by less hazardous alternatives or preferably alternatives for which no hazards can be identified\".\n\nThe principle has historically been promoted by environmental groups. The concept is becoming increasingly mainstream, being a key concept in green chemistry and a central element of EU REACH regulation. Critics of the principle claim it is very difficult to implement in reality, especially in terms of legislation.\n\nNonetheless, the concept is an important one and a key driver behind identifying Substances of Very High Concern in REACH and the development of hazardous substance lists such as the SIN List and the ETUC Trade Union Priority List. EU-funded projects such as SubsPort are under development to aid the identification and development of safer substitutes for hazardous chemicals.\n\n"}
{"id": "10446290", "url": "https://en.wikipedia.org/wiki?curid=10446290", "title": "Sustainable regional development", "text": "Sustainable regional development\n\nSustainable regional development is the application of sustainable development at a regional, rather than local, national or global level. It differs to regional development per se, as the latter is a term used more generally to describe economic development that emphasises the alleviation of regional disparities. While regional development has an economic and equity emphasis, sustainable regional development seeks to incorporate ecological concerns.\n\nSustainable regional development has particular currency in Australia, where the Institute for Sustainable Regional Development has been established (1997) for the purpose of developing integrated, multi- and inter-disciplinary strategies for environmental and socio-economic change in regional Australia.\n"}
{"id": "1063436", "url": "https://en.wikipedia.org/wiki?curid=1063436", "title": "T-schema", "text": "T-schema\n\nThe T-schema (\"truth schema\"; not to be confused with 'Convention T') is used to give an inductive definition of truth which lies at the heart of any realisation of Alfred Tarski's semantic theory of truth. Some authors refer to it as the \"Equivalence Schema\", a synonym introduced by Michael Dummett.\n\nThe T-schema is often expressed in natural language, but it can be formalized in many-sorted predicate logic or modal logic; such a formalisation is called a \"T-theory.\" T-theories form the basis of much fundamental work in philosophical logic, where they are applied in several important controversies in analytic philosophy.\n\nAs expressed in semi-natural language (where 'S' is the name of the sentence abbreviated to S):\n'S' is true if and only if S\n\nExample: 'snow is white' is true if and only if snow is white.\n\nBy using the schema one can give an inductive definition for the truth of compound sentences. Atomic sentences are assigned truth values disquotationally. For example, the sentence \"'Snow is white' is true\" becomes materially equivalent with the sentence \"snow is white\", i.e. 'snow is white' is true if and only if snow is white. The truth of more complex sentences is defined in terms of the components of the sentence:\n\nJoseph Heath points out that \"The analysis of the truth predicate provided by Tarski's Schema T is not capable of handling all occurrences of the truth predicate in natural language. In particular, Schema T treats only “freestanding” uses of the predicate—cases when it is applied to complete sentences.\" He gives as \"obvious problem\" the sentence:\nHeath argues that analyzing this sentence using T-schema generates the sentence fragment—“everything that Bill believes”—on the righthand side of the Logical biconditional.\n\n\n"}
{"id": "32300415", "url": "https://en.wikipedia.org/wiki?curid=32300415", "title": "The Politics of Nonviolent Action", "text": "The Politics of Nonviolent Action\n\nThe Politics of Nonviolent Action is a three-volume political science book by Gene Sharp, originally published in the United States in 1973. Sharp is one of the most influential theoreticians of nonviolent action, and his publications have been influential in struggles around the world. This book contains his foundational analyses of the nature of political power, and of the methods and dynamics of nonviolent action. It represents a \"thorough revision and rewriting\" of the author's 1968 doctoral thesis at Oxford University. The book has been reviewed in professional journals and newspapers, and is mentioned on many contemporary websites. It has been fully translated into Italian and partially translated into several other languages.\n\nThe three volumes or \"parts\" of \"The Politics of Nonviolent Action\" contain a total of 14 chapters, as well as a preface by the author, and an introduction by Thomas C. Schelling. Each part begins with an introduction by the author. The first volume or \"part\" addresses the theory of power that implicitly or explicitly underlies nonviolent action; Volume 2 offers a detailed analysis of the \"methods\" of nonviolent action; and the Volume 3 analyzes the \"dynamics\" of nonviolent action.\n\nChapter 1, \"The Nature and Control of Political Power\", explains that, \nalthough rarely articulated, there are \"basically... two views of the nature of power.\" The \"monolith theory\" views people as dependent upon the good will of their governments, whereas nonviolent action is grounded in the converse \"pluralistic-dependency theory\" that views government as \"dependent on the people's good will, decisions and support,\" a view that Sharp argues is \"sounder and more accurate.\" Sharp argues that \"political power is not intrinsic to the power-holder,\" but flows from outside sources that include perceptions of authority, available human resources; skills and knowledge; material resources; and intangible psychological and ideological factors. These sources all depend upon obedience, which arises for \"various and multiple\" reasons that include habit, fear of sanctions, perceived moral obligation, psychological identification with the ruler, zones of indifference, and absence of self-confidence among subjects. Obedience is essentially voluntary, and consent can be withdrawn.\n\nNext, \"Nonviolent Action: An Active Technique of Struggle\" (chapter 2) explains that nonviolent action may be used for a diverse mixture of motives that are religious, ethical, moral, or based on expediency. \"Passivity, submission, cowardice [have] nothing to do with the nonviolent technique,\" which is correctly understood as \"\"one\" type of \"active\" response.\" Nonviolence has suffered scholarly neglect. Nonviolence may involve both \"acts of omission\" and \"acts of commission\", does not rely solely on persuading the opponent, and \"does not depend on the assumption that man is inherently 'good'.\" These and other characteristics of nonviolence are explained and illustrates through examples from ancient Rome, colonial United States, Tsarist Russia, Soviet Russia, Nazi Germany, Latin America, India, Czeschoslavakia, and the Southern United States.\n\nVolume 2 (chapters 3 to 8) contains a detailed listing and description of specific methods of nonviolent action, such as boycotts, strikes, and sit-ins. Such a listing, Sharp says, \"may assist actionists in the selection of methods most appropriate for use in a particular situation... [or] give researchers and persons evaluating the political potentialities of the nonviolent technique a greater grasp of its armory of methods of struggle.\" Nearly 200 methods are listed in the table of contents, and Sharp groups them into three broad categories, \"protest and persuasion\" (ch. 3), \"noncooperation\" (chs. 4-7), and \"intervention\" (ch. 8), in terms of how they relate to the dynamics of nonviolent action (Vol. 3). These categories \"ought not to be regarded as rigid, but simply as generally valid.\" The methods are summarized in the adjacent table.\n\nThe third volume focuses on the dynamics of nonviolent action, which always \"involves continuous change in the various influences and forces which operate in that process and are constantly influencing each other. No discussion in static terms... can be valid.\" It opens with Chapter 9, \"Laying the Groundwork for Nonviolent Action\", with subsections addressing such issues as casting off fear, the social sources of power changes, leadership needs, openness and secrecy, investigation, negotiations, generating \"cause-consciousness.\" It also describes key elements of nonviolent strategy and tactics, pertaining to issues such as initiative, timing, numbers and strength, psychological elements, application of an Indirect approach, the choice of weapons (as described in Vol. 2), and the issuance of an ultimatum.\n\nChapter 10 describes how the onset of nonviolent action is likely to bring various types of oppression, and reviews examples and approaches for withstanding increasing repression, which is imperative, because \"without willingness to face repression... the nonviolent action movement cannot hope to succeed.\" Chapter 11 describes methods for maintaining the nonviolent group's solidarity, such as \"Maintaining rapport\" through regular mass meetings. Chapter 11 also extensively analyzes the threats against and needs for ongoing adherence to nonviolent discipline, \"in order to bring into operation the changes that will alter relationships and achieve [the] objectives,\" even as \"the opponent... tries to provoke them to commit violence - with which he could deal more effectively.\"\n\nChapter 12 covers \"political \"jiu-jitsu\"... one of the special processes by which nonviolent action deals with violent repression.\" More specifically:\n\nBy combining nonviolent discipline with solidarity and persistence in struggle, the nonviolent actionists cause the violence of the opponent's repression to be exposed in the worst possible light. This, in turn, may lead to shifts in opinion and then to shifts in power relationships favorable to the nonviolent group. These shifts result from withdrawal of support for the opponent and the grant of support to the nonviolent actionists. \n\nThis chapter provides numerous historical examples of such political \"jiu-jitsu\", and analyzes such factors as the impact of third party opinion and international indignation, arousing dissent and opposition in the opponent's own camp, and increasing support and participation from the grievance group.\n\n\"Three Ways Success May Be Achieved\" (Chapter 13) describes and analyzes \"conversion\", \"accommodation\", and \"nonviolent coercion\". These represent \"three broad processes, or mechanisms, by which the complicated forces utilized and produced by nonviolent action influence the opponent and his capacity for action and thereby perhaps bring success to the cause of the grievance group\": \n\nIn \"conversion\" the opponent has been inwardly changed so that he wants to make the changes desired by the nonviolent actionists. In \"accommodation\", the opponent does not agree with the changes... and he could continue the struggle... but... has concluded that it is best to grant some or all of the demands... In \"nonviolent coercion\" the opponent has not changed his mind on the issues and wants to \"continue\" the struggle, but is \"unable\" to do so; the sources of his power and means of control have been taken away from him without the use of violence . This may have been done by the nonviolent group or by the opposition and noncooperation among his own group (as, mutiny of his troops), or some combination of these. \n\nFinally, \"The Redistribution of Power\" (Chapter 14) describes how using the nonviolent technique is likely to affect the nonviolent group, and the distribution of power between the contenders and in the larger society or system. Such effects may include the ending of submissiveness, increases in hope, effects on aggression, masculinity, crime and violence, increased group unity, and the decentralization of power. \"Nonviolent action appears by its very nature to contribute to the diffusion of effective power throughout the society\" due in part to the enhanced self-reliance of those using the technique.\n\nReviews have appeared in the \n\"Armed Forces & Society\",\n\"International Organization\",\n\"Social Forces\", \n\"Social Work\",\n\"The Annals of the American Academy of Political and Social Science\",\n\"Ethics\",\n\"American Journal of Sociology\",\n\"The Journal of Developing Areas\",\n\"The Western Political Quarterly\",\n\"Political Theory\",\nThe \"Bay State Banner\",\nand elsewhere.\n\nIn \"Armed Forces & Society\", Kenneth Boulding described the book as \"monumental,\" writing that \"there are some works which bear the unmistakable stamp of the classic... and this work is a good candidate.\" Sharp, he said, \"has been called the \"Machiavelli of nonviolence\" and the \"Clausewitz of nonviolent warfare\" [and] the comparisons are by no means unjust.\" The book \"reveals a large but previously mostly unnoticed segment of human action relationship which would very properly be described as 'nonviolent warfare.'\" Boulding asserted several parallels with a seminal work in his own field, economics, explaining that\nthis volume... reminds one of <nowiki>[</nowiki>Adam Smith's<nowiki>]</nowiki> \"The Wealth of Nations\". There is a single theme of immense importance to society played in innumerable variations throughout the whole work. There is a wealth of historical illustration and detail. There is a distinct view of society as a whole seen perhaps from a somewhat unfamiliar angle. And there is a wholly honorable passion for human betterment through intellectual clarification.\n\nBoulding stated that if a key word for economics is \"exchange,\" then the key word for nonviolent action is \"disobedience\" - \"One might almost call Gene Sharp's book, therefore... the discovery of disobedience, especially of large-scale disobedience... nonviolent action is concerned with the institutionalization of a threat-defiance system.\" A key to this process, Boulding argues, is the \"dynamics of legitimacy... [the] public denial of the legitimacy of some command,\" which Sharp \"hints at many times,\" although Sharp \"never quite works [it] out in detail.\"\nBoulding described Part II as \"in some ways... the meatiest and richest part of the work,\" although he noted that Sharp's examples are drawn from \"quite restricted range\" of human history:\n\nThere must be many examples from Chinese history; Latin America is hardly mentioned; and the European middle ages, with its extraordinary phenomenon of the \"Truce of God,\" receives hardly a mention. Nevertheless, Sharp's examples are broad and wide and illustrate the universality and significance of this phenomenon, which, simply because it has not had a name, has been grossly neglected by conventional historians.\n\nBoulding also reported some ways that Sharp's theoretical analysis seemed to be deficient, \"even in terms of what might be called 'classical' or Gandhian theories of nonviolence.\" The book's analysis\n\nneglects the importance of \"Satyagraha\" or \"Truth-grasping,\" that is, the appeal of nonviolent action to some objective truth, even an objective moral truth, as the basic source of its legitimacy. In this sense nonviolent action is closer to the spirit of science than it is to the spirit of war, in that it is concerned... that truth should prevail no matter who wins. Sharp, perhaps in too great a reaction to the accusations of sentimental pacifism sometimes brought against nonviolence, has stressed the conflictual aspects of it perhaps to the exclusion of its integrative aspects.\"\nIn \"International Organization\", Bleicher's 21-page review stated that \"What Professor Sharp... has demonstrated is that our understanding of the dependency of governments upon the\ncontinuing consent of the governed can be translated into the development of nonviolent action as a strategy of change that is effective outside of established institutional arrangements and yet operates without the use of force.\" He wrote that Sharp \n\nhas exposed the inadequacy of assuming the monolithic character of the nation state in international relations theory. Recognition of nonviolent action as a tool in the hands of governments and citizens to influence the policies of other nations and of international organizations calls for a fundamental re-evaluation of the critical parameters in the study and conduct of international relations.\n\nBleicher stated that the \"full utilization of this new understanding requires an expansion of the horizons of scholars and policy makers in the international arena, the collection and application of new data in the evaluation of international relations, and the development of new theoretical constructs.\" He warned that if we do not develop a better understanding of phenomenon related to nonviolence, we face the danger that we will be progressively less able to... design policies and institutions that can cope with the future.\"\n\nIn \"Political Theory\", Carl J. Friedrich wrote that Sharp considers his view of power as \"much more original than it is,\" and that the reviewer [Friedrich] found it \"exasperating to try and follow arguments with the drift of which he definitely sympathized, except for their alleged novelty.\" In particular, Sharp does not\n\nrelate [his view of power] to such classics as Charles Merriam's magisterial treatment of power, or even Bertrand Russell's journalistic book on the subject. He seems unaware of the reviewer's [Friedrich's] analysis of many years ago, in which the distinction between two views of power, and the dependence of power on the cooperation of those over whom it is wielded, was analytically developed, and its root in the classics was shown.\n\nFriedrich also stated that a \"fundamental weakness\" of Sharp's argument was his understanding of violence \"as physical violence,\" since \"some of the most vicious forms of coercion are psychic.\" Furthermore \"According to Sharp, violence by definition excludes demolition and destruction of things, such as machinery, buildings and the like. Hence, according to him, much sabotage is not violent; clearly at this point Sharp deviates markedly from popular usage.\" Due to the \"illusory\" nature of the distinction based on physical violence alone, Sharp is often \"confused,\" although \"many thoughtful arguments are offered.\" Friedrich concluded that \"the topic of how to avoid violence in political conflicts is an important one, the treatment given here is learned, but not very clear, and the results not conclusive,\" but that the book was a \"timely one\" that he hoped it will \"lead to further more searching studies.\"\n\nIn \"The Western Political Quarterly\", H. L. Nieburg wrote that he \"would like to see the work cut down to 125 pages and published in paperback as a token of new life for insurgency politics. But, this... should not deter one from the duty to welcome a monumental, competent, and sometimes exciting, work of scholarship.\"\n\nIn \"Social Work\", Harry Specht stated that \"Sharp has performed a useful service for students of community organizing by producing an encyclopedic description of nonviolent action,\" and that \"by shear weight of detail, the reader comes to recognize that nonviolent action has been far more pervasive than many assume.\" Specht stated that the books flaws included \"repetition and excessive detail,\" and that the book \"seems to imply that nonviolent action is usually undertaken by the oppressed against the state and that it is usually in the cause of positive social change. But... for example, I have just read of two massive nonviolent demonstrations in Boston, one for and the other against integration of the public schools.\" Specht described an \"absence of a clear theoretical framework... it does not illuminate such central questions as... Why is nonviolent action used in some cases and not in others? Why does it work in some cases and not in others?\" The book is also \"rich with writings on social movements and thin on theorists such as Kenneth Boulding, Amitai Etzioni, Jerome Skolnick, and Ralf Dahrendorf - who have dealt with conflict and violence.\" Still, \"Sharp's work is an impressive accomplishment that will be welcomed as an important addition to the literature of community organizing.\"\n\nThe book has been mentioned in various other publications, including \n\"Utne\",\nthe \"American Conservative\",\nthe CNN website,\nand elsewhere.\n\nThe \"Politics of Nonviolent Action\" originally appeared in 3 volumes in English in 1973, and has subsequently been translated fully or partially into several other languages. The English language edition was published by Porter Sargent in 3 volumes entitled: 1. \"Power and struggle\", 2. \"The methods of nonviolent action\", and 3. \"The dynamics of nonviolent action\". The respective citations of the 3-volume set and of each individual volume are:\n\n\nIn 2013, Sharp published a 143-page English language condensation:\n\nFull or partial non-English translations have appeared in languages that include Arabic, Dutch, Italian, Polish, and Spanish:\n\n"}
{"id": "255446", "url": "https://en.wikipedia.org/wiki?curid=255446", "title": "Thermodynamic potential", "text": "Thermodynamic potential\n\nA thermodynamic potential (in fact, rather energy than potential) is a scalar quantity used to represent the thermodynamic state of a system. The concept of thermodynamic potentials was introduced by Pierre Duhem in 1886. Josiah Willard Gibbs in his papers used the term \"fundamental functions\". One main thermodynamic potential that has a physical interpretation is the internal energy . It is the energy of configuration of a given system of conservative forces (that is why it is called potential) and only has meaning with respect to a defined set of references (or data). Expressions for all other thermodynamic energy potentials are derivable via Legendre transforms from an expression for . In thermodynamics, external forces, such as gravity, are typically disregarded when formulating expressions for potentials. For example, while all the working fluid in a steam engine may have higher energy due to gravity while sitting on top of Mount Everest than it would at the bottom of the Mariana Trench, the gravitational potential energy term in the formula for the internal energy would usually be ignored because \"changes\" in gravitational potential within the engine during operation would be negligible. In a large system under even homogeneous external force, like the earth atmosphere under gravity, the intensive parameters (formula_1) should be studied locally having even in equilibrium different values in different places far from each other (see thermodynamic models of troposphere].\n\nFive common thermodynamic potentials are:\n\nwhere = temperature, = entropy, = pressure, = volume. The Helmholtz free energy is in ISO/IEC standard called Helmholtz energy or Helmholtz function. It is often denoted by the symbol , but the use of is preferred by IUPAC, ISO and IEC. is the number of particles of type in the system and is the chemical potential for an -type particle. For the sake of completeness, the set of all are also included as natural variables, although they are sometimes ignored.\n\nThese five common potentials are all energy potentials, but there are also entropy potentials. The thermodynamic square can be used as a tool to recall and derive some of the potentials.\n\nJust as in mechanics, where potential energy is defined as capacity to do work, similarly different potentials have different meanings:\nFrom these meanings (which actually apply in specific conditions, e.g. constant pressure, temperature, etc), we can say that is the energy added to the system, is the total work done on it, is the non-mechanical work done on it, and is the sum of non-mechanical work done on the system and the heat given to it.\nThermodynamic potentials are very useful when calculating the equilibrium results of a chemical reaction, or when measuring the properties of materials in a chemical reaction. The chemical reactions usually take place under some constraints such as constant pressure and temperature, or constant entropy and volume, and when this is true, there is a corresponding thermodynamic potential that comes into play. Just as in mechanics, the system will tend towards lower values of potential and at equilibrium, under these constraints, the potential will take on an unchanging minimum value. The thermodynamic potentials can also be used to estimate the total amount of energy available from a thermodynamic system under the appropriate constraint.\n\nIn particular: (see principle of minimum energy for a derivation)\n\n\nThe variables that are held constant in this process are termed the natural variables of that potential. The natural variables are important not only for the above-mentioned reason, but also because if a thermodynamic potential can be determined as a function of its natural variables, all of the thermodynamic properties of the system can be found by taking partial derivatives of that potential with respect to its natural variables and this is true for no other combination of variables. On the converse, if a thermodynamic potential is not given as a function of its natural variables, it will not, in general, yield all of the thermodynamic properties of the system.\n\nNotice that the set of natural variables for the above four potentials are formed from every combination of the - and - variables, excluding any pairs of conjugate variables. There is no reason to ignore the − conjugate pairs, and in fact we may define four additional potentials for each species. Using IUPAC notation in which the brackets contain the natural variables (other than the main four), we have:\n\nIf there is only one species, then we are done. But, if there are, say, two species, then there will be additional potentials such as formula_2 and so on. If there are dimensions to the thermodynamic space, then there are unique thermodynamic potentials. For the most simple case, a single phase ideal gas, there will be three dimensions, yielding eight thermodynamic potentials.\n\nThe definitions of the thermodynamic potentials may be differentiated and, along with the first and second laws of thermodynamics, a set of differential equations known as the \"fundamental equations\" follow. (Actually they are all expressions of the same fundamental thermodynamic relation, but are expressed in different variables.) By the first law of thermodynamics, any differential change in the internal energy of a system can be written as the sum of heat flowing into the system and work done by the system on the environment, along with any change due to the addition of new particles to the system:\n\nwhere is the infinitesimal heat flow into the system, and is the infinitesimal work done by the system, is the chemical potential of particle type and is the number of type particles. (Note that neither nor are exact differentials. Small changes in these variables are, therefore, represented with rather than .)\n\nBy the second law of thermodynamics, we can express the internal energy change in terms of state functions and their differentials. In case of reversible changes we have:\n\nwhere\nand is volume, and the equality holds for reversible processes.\n\nThis leads to the standard differential form of the internal energy in case of a quasistatic reversible change:\n\nSince , and are thermodynamic functions of state, the above relation holds also for arbitrary non-reversible changes. If the system has more external variables than just the volume that can change, the fundamental thermodynamic relation generalizes to:\n\nHere the are the generalized forces corresponding to the external variables .\n\nApplying Legendre transforms repeatedly, the following differential relations hold for the four potentials:\n\nNote that the infinitesimals on the right-hand side of each of the above equations are of the natural variables of the potential on the left-hand side.\nSimilar equations can be developed for all of the other thermodynamic potentials of the system. There will be one fundamental equation for each thermodynamic potential, resulting in a total of fundamental equations.\n\nThe differences between the four thermodynamic potentials can be summarized as follows:\n\nWe can use the above equations to derive some differential definitions of some thermodynamic parameters. If we define to stand for any of the thermodynamic potentials, then the above equations are of the form:\n\nwhere and are conjugate pairs, and the are the natural variables of the potential . From the chain rule it follows that:\n\nWhere is the set of all natural variables of except . This yields expressions for various thermodynamic parameters in terms of the derivatives of the potentials with respect to their natural variables. These equations are known as \"equations of state\" since they specify parameters of the thermodynamic state. If we restrict ourselves to the potentials , , and , then we have:\n\nwhere, in the last equation, is any of the thermodynamic potentials , , , and formula_17 are the set of natural variables for that potential, excluding . If we use all potentials, then we will have more equations of state such as\n\nand so on. In all, there will be  equations for each potential, resulting in a total of equations of state. If the  equations of state for a particular potential are known, then the fundamental equation for that potential can be determined. This means that all thermodynamic information about the system will be known, and that the fundamental equations for any other potential can be found, along with the corresponding equations of state.\n\nAgain, define and to be conjugate pairs, and the to be the natural variables of some potential . We may take the \"cross differentials\" of the state equations, which obey the following relationship:\n\nFrom these we get the Maxwell relations. There will be of them for each potential giving a total of equations in all. If we restrict ourselves the , , , \n\nUsing the equations of state involving the chemical potential we get equations such as:\n\nand using the other potentials we can get equations such as:\n\nAgain, define and to be conjugate pairs, and the to be the natural variables of the internal energy.\nSince all of the natural variables of the internal energy are extensive quantities\n\nit follows from Euler's homogeneous function theorem that the internal energy can be written as:\n\nFrom the equations of state, we then have:\n\nSubstituting into the expressions for the other main potentials we have:\n\nAs in the above sections, this process can be carried out on all of the other thermodynamic potentials. Note that the Euler integrals are sometimes also referred to as fundamental equations.\n\nDeriving the Gibbs–Duhem equation from basic thermodynamic state equations is straightforward. Equating any thermodynamic potential definition with its Euler integral expression yields:\n\nDifferentiating, and using the second law:\n\nyields:\n\nWhich is the Gibbs–Duhem relation. The Gibbs–Duhem is a relationship among the intensive parameters of the system. It follows that for a simple system with components, there will be independent parameters, or degrees of freedom. For example, a simple system with a single component will have two degrees of freedom, and may be specified by only two parameters, such as pressure and volume for example. The law is named after Josiah Willard Gibbs and Pierre Duhem.\n\nChanges in these quantities are useful for assessing the degree to which a chemical reaction will proceed. The relevant quantity depends on the reaction conditions, as shown in the following table. denotes the change in the potential and at equilibrium the change will be zero.\n\nMost commonly one considers reactions at constant and , so the Gibbs free energy is the most useful potential in studies of chemical reactions.\n\n\n\n\n"}
{"id": "32018959", "url": "https://en.wikipedia.org/wiki?curid=32018959", "title": "Time's Up (artist group)", "text": "Time's Up (artist group)\n\nTime's Up \nis an interactive media and machine art group from Linz, Austria. Founded in 1996, the group has developed interactive media and mechanical art works exhibited throughout Europe and Australasia.\n\nTime's Up runs Dorkbot Linz and is involved in the Funkfeuer community, the local artists \"Freie Szene\" and educational institutions.\n\nTime's Up has shown a large number of works as individual objects and as complete environments internationally. The exhibitions fall into four main phases:\nThe group has been involved in an extensive network of European projects, working closely with groups such as FoAM, M-ITI and ATOL . Matt Heckert worked closely with the group in the first year, members of the group have worked closely with Chico MacMurtrie within the Amorphic Robot Works.\nThe Anchortronic series of residencies culminated in the release of the same named DVD on the Dutch-German label Staalplaat.\nThe book \"PARN: Physical and Alternate Reality Narratives\" was published in 2012 at the end of the project with the same name. \nThe books \"Futurish\" and \"Turtles and Dragons\" were created using the book sprint methodology and published in short runs on demand.\n\n"}
{"id": "35658939", "url": "https://en.wikipedia.org/wiki?curid=35658939", "title": "Verification and validation of computer simulation models", "text": "Verification and validation of computer simulation models\n\nVerification and validation of computer simulation models is conducted during the development of a simulation model with the ultimate goal of producing an accurate and credible model. \"Simulation models are increasingly being used to solve problems and to aid in decision-making. The developers and users of these models, the decision makers using information obtained from the results of these models, and the individuals affected by decisions based on such models are all rightly concerned with whether a model and its results are \"correct\". This concern is addressed through verification and validation of the simulation model.\n\nSimulation models are approximate imitations of real-world systems and they never exactly imitate the real-world system. Due to that, a model should be verified and validated to the degree needed for the models intended purpose or application.\n\nThe verification and validation of simulation model starts after functional specifications have been documented and initial model development has been completed. Verification and validation is an iterative process that takes place throughout the development of a model.\n\nIn the context of computer simulation, verification of a model is the process of confirming that it is correctly implemented with respect to the conceptual model (it matches specifications and assumptions deemed acceptable for the given purpose of application).\nDuring verification the model is tested to find and fix errors in the implementation of the model.\nVarious processes and techniques are used to assure the model matches specifications and assumptions with respect to the model concept. \nThe objective of model verification is to ensure that the implementation of the model is correct.\n\nThere are many techniques that can be utilized to verify a model.\nIncluding, but not limited to, have the model checked by an expert, making logic flow diagrams that include each logically possible action, examining the model output for reasonableness under a variety of settings of the input parameters, and using an interactive debugger.\nMany software engineering techniques used for software verification are applicable to simulation model verification.\n\nValidation checks the accuracy of the model's representation of the real system. Model validation is defined to mean \"substantiation that a computerized model within its domain of applicability possesses a satisfactory range of accuracy consistent with the intended application of the model\". A model should be built for a specific purpose or set of objectives and its validity determined for that purpose.\n\nThere are many approaches that can be used to validate a computer model. The approaches range from subjective reviews to objective statistical tests. One approach that is commonly used is to have the model builders determine validity of the model through a series of tests.\n\nNaylor and Finger [1967] formulated a three-step approach to model validation that has been widely followed:\n\nStep 1. Build a model that has high face validity.\n\nStep 2. Validate model assumptions.\n\nStep 3. Compare the model input-output transformations to corresponding input-output transformations for the real system.\n\nA model that has face validity appears to be a reasonable imitation of a real-world system to people who are knowledgeable of the real world system. Face validity is tested by having users and people knowledgeable with the system examine model output for reasonableness and in the process identify deficiencies. An added advantage of having the users involved in validation is that the model's credibility to the users and the user's confidence in the model increases. Sensitivity to model inputs can also be used to judge face validity. For example, if a simulation of a fast food restaurant drive through was run twice with customer arrival rates of 20 per hour and 40 per hour then model outputs such as average wait time or maximum number of customers waiting would be expected to increase with the arrival rate.\n\nAssumptions made about a model generally fall into two categories: structural assumptions about how system works and data assumptions.\n\nAssumptions made about how the system operates and how it is physically arranged are structural assumptions. For example, the number of servers in a fast food drive through lane and if there is more than one how are they utilized? Do the servers work in parallel where a customer completes a transaction by visiting a single server or does one server take orders and handle payment while the other prepares and serves the order. Many structural problems in the model come from poor or incorrect assumptions. If possible the workings of the actual system should be closely observed to understand how it operates. The systems structure and operation should also be verified with users of the actual system.\n\nThere must be a sufficient amount of appropriate data available to build a conceptual model and validate a model. Lack of appropriate data is often the reason attempts to validate a model fail. Data should be verified to come from a reliable source. A typical error is assuming an inappropriate statistical distribution for the data. The assumed statistical model should be tested using goodness of fit tests and other techniques. Examples of goodness of fit tests are the Kolmogorov–Smirnov test and the chi-square test. Any outliers in the data should be checked.\n\nThe model is viewed as an input-output transformation for these tests. The validation test consists of comparing outputs from the system under consideration to model outputs for the same set of input conditions. Data recorded while observing the system must be available in order to perform this test. The model output that is of primary interest should be used as the measure of performance. For example, if system under consideration is a fast food drive through where input to model is customer arrival time and the output measure of performance is average customer time in line, then the actual arrival time and time spent in line for customers at the drive through would be recorded. The model would be run with the actual arrival times and the model average time in line would be compared with the actual average time spent in line using one or more tests.\n\nStatistical hypothesis testing using the t-test can be used as a basis to accept the model as valid or reject it as invalid.\n\nThe hypothesis to be tested is\nversus\n\nThe test is conducted for a given sample size and level of significance or α. To perform the test a number \"n\" statistically independent runs of the model are conducted and an average or expected value, E(Y), for the variable of interest is produced. Then the test statistic, \"t\" is computed for the given α, \"n\", E(Y) and the observed value for the system μ\n\nIf \nreject H, the model needs adjustment.\n\nThere are two types of error that can occur using hypothesis testing, rejecting a valid model called type I error or \"model builders risk\" and accepting an invalid model called Type II error, β, or \"model user's risk\". The level of significance or α is equal the probability of type I error. If α is small then rejecting the null hypothesis is a strong conclusion. For example, if α = 0.05 and the null hypothesis is rejected there is only a 0.05 probability of rejecting a model that is valid. Decreasing the probability of a type II error is very important. The probability of correctly detecting an invalid model is 1 - β. The probability of a type II error is dependent of the sample size and the actual difference between the sample value and the observed value. Increasing the sample size decreases the risk of a type II error.\n\nA statistical technique where the amount of model accuracy is specified as a range has recently been developed. The technique uses hypothesis testing to accept a model if the difference between a model's variable of interest and a system's variable of interest is within a specified range of accuracy. A requirement is that both the system data and model data be approximately Normally Independent and Identically Distributed (NIID). The t-test statistic is used in this technique. If the mean of the model is μ and the mean of system is μ then the difference between the model and the system is D = μ - μ. The hypothesis to be tested is if D is within the acceptable range of accuracy. Let L = the lower limit for accuracy and U = upper limit for accuracy. Then\n\nversus\n\nis to be tested.\n\nThe operating characteristic (OC) curve is the probability that the null hypothesis is accepted when it is true. The OC curve characterizes the probabilities of both type I and II errors. Risk curves for model builder's risk and model user's can be developed from the OC curves. Comparing curves with fixed sample size tradeoffs between model builder's risk and model user's risk can be seen easily in the risk curves. If model builder's risk, model user's risk, and the upper and lower limits for the range of accuracy are all specified then the sample size needed can be calculated.\n\nConfidence intervals can be used to evaluate if a model is \"close enough\" to a system for some variable of interest. The difference between the known model value, μ, and the system value, μ, is checked to see if it is less than a value small enough that the model is valid with respect that variable of interest. The value is denoted by the symbol ε. To perform the test a number, \"n\", statistically independent runs of the model are conducted and a mean or expected value, E(Y) or μ for simulation output variable of interest Y, with a standard deviation \"S\" is produced. A confidence level is selected, 100(1-α). An interval, [a,b], is constructed by\n\nwhere \nis the critical value from the t-distribution for the given level of significance and n-1 degrees of freedom.\n\nIf statistical assumptions cannot be satisfied or there is insufficient data for the system a graphical comparisons of model outputs to system outputs can be used to make a subjective decisions, however other objective tests are preferable.\n\nDocuments and standards involving verification and validation of computational modeling and simulation are developed by the American Society of Mechanical Engineers (ASME) Verification and Validation (V&V) Committee. ASME V&V 10 provides guidance in assessing and increasing the credibility of computational solid mechanics models through the processes of verification, validation, and uncertainty quantification. ASME V&V 10.1 provides a detailed example to illustrate the concepts described in ASME V&V 10. ASME V&V 20 provides a detailed methodology for validating computational simulations as applied to fluid dynamics and heat transfer. Soon to be published in 2018, ASME V&V 40 provides a framework for establishing model credibility requirements for computational modeling, and presents examples specific in the medical device industry. \n\n"}
{"id": "363699", "url": "https://en.wikipedia.org/wiki?curid=363699", "title": "Worldshop", "text": "Worldshop\n\nWorldshops, world shops or Fair Trade Shops are specialized retail outlets offering and promoting Fair Trade products. Worldshops also typically organize various educational Fair Trade activities and play an active role in trade justice and other North-South political campaigns.\n\nWorldshops are often not-for-profit organizations and run by locally based volunteer networks.\n\nAlthough the movement emerged in Europe and a vast majority of worldshops are still based on the continent, worldshops can also be found today in North America, Australia, Israel and New Zealand.\n\nThe start of the movement is usually attributed to the first worldshop in Europe, which was founded by Oxfam in 1959. The Oxfam shop sold Chinese bric-a-brac that had been sourced from Chinese refugees that had escaped the Communist revolution to Hong Kong. However, some sources credit the first fair trade shop that had been opened in the US in 1958, selling Puerto Rican needlework.\n\nThe shops were not called worldshops at that time, however. Alternative trading organisations imported various third world goods, such as cane sugar starting in the 1960s, and still continuing today. These goods were sold in \"third world shops\" or \"developing country shops\" (the actual name differing from country to country). The most active organisations were operating in the United Kingdom and the Netherlands. The term \"worldshop\" came into existence in the 1990s. In 1994, worldshops organised themselves under the auspices of NEWS, the Network of European Worldshops, who now falls under the World Fair Trade Organization. The Fairtrade label, which is used on fair trade products, has its roots in the 1980s worldshop movement.\n\nWorldshops' aim is to make trade as direct and fair with the trading partners as possible. Usually, this means a producer in a developing country and consumers in industrialized countries. The worldshops' target is to pay the producers a fair price that guarantees subsistence and positive social development. They often cut out any intermediaries in the import chain.\n\n"}
