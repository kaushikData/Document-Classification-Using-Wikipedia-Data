{"id": "53106462", "url": "https://en.wikipedia.org/wiki?curid=53106462", "title": "Access to public information in Croatia", "text": "Access to public information in Croatia\n\nAccess to public information and freedom of information (FOI) refer to the right of access to information held by public bodies also known as \"right to know\". Access to public information is considered of fundamental importance for the effective functioning of democratic systems, as it enhances governments' and public officials' accountability, boosting people participation and allowing their informed participation into public life. The fundamental premise of the right of access to public information is that the information held by governmental institutions is in principle public and may be concealed only on the basis of legitimate reasons which should be detailed in the law. \nIn the course of EU accession negotiations Croatia harmonised its media legislation to European standards. This process touched also legislation on access to public information which has been amended to reflect European and international standards.\nCroatia thus adopted its law on the Right of Access to Public Information in 2013, after a decade of advocacy, campaigns, and public discussions led by civil society organisations. However, despite the improvements of the legal framework regulating access to public information, problems remain in the implementation, especially for journalists willing to request and obtain information from the government.\nIn Croatia, access to public information has become a constitutional right with the 2010 amendments of the Constitution. It is regulated by the Law on the Right to Access Information adopted by the Croatian Parliament in 2013. The Law also regulates the re-use of information held by public authorities. The Law complies with the Directive 2003/98/EC of the European Parliament and of the Council on the re-use of public sector information and with Regulation 1049/2001 of the European Parliament and of the Council of 30 May 2001 regarding public access to European Parliament, Council and Commission documents.\nAccording to the Law, the right of access to information encompasses the right of the beneficiaries, i.e. any local or foreign natural person or legal entity, to seek and acquire information, as well as the obligation of public authorities to guarantee access to requested information, regardless of the request.\nThe Croatian law on Access to public information is in informed by four principles:\n\nPublic bodies are obliged to publish on the Internet the following relevant information: laws and other regulations in their field of activity, including draft proposals of laws; general acts and decisions affecting the interests of beneficiaries; annual plans, programmes, strategies and financial reports referring to the work of public authority bodies; information on budget, financing sources and subsidies; information on their internal organisation; notes and conclusions from official sessions; information on public procurement and tenders; information on the way of exercising rights of access to and re-use of information, including contact details of the Information Commissioner and the fee required to access and re-use of information.\nFor the purpose of ensuring the right of access to information, the Law establishes that public bodies are obliged to appoint an Information Commissioner, a special official in charge of resolving the issues emerging from the exercise of the right of access to information. Specifically, the Information Commissioner shall conduct the tasks connected to disclosure of public information in accordance with the Law, including providing the necessary assistance to applicants, improving the manner of processing, classification and safe-keeping of the information, and maintaining the Information Commissioner Register.\n\nPublic authority bodies may restrict access to information if:\nMoreover, restrictions may be operated when there can be reasonable doubts that disclosing the requested information might prevent the efficiency, independence or impartiality of ongoing proceedings or the execution of court orders and sentence.\nThe public authority body in charge of acting upon the request of access to information is obliged to conduct the Proportionality Interest Test in order to reach the decision about disclosure. Proportionality Test and Public Interest Test refer to the assessment of proportionality between reasons for granting access to information and reasons for imposing restrictions and granting access to information only when the public interest prevails. If, on the basis of the Test, the public interest prevails over the damage caused to other protected interests, the information shall be disclosed.\nPublic authorities are bound to grant access to information by timely publishing the information on their work in an accessible manner, i.e. on the webpage, or in the Official Gazette and the Central catalogue of officials documents of the Republic of Croatia, etc. Information can be provided directly, or in writing, or by giving insight into documents and making copies of the documents containing the requested information, or by delivering copies of the requested information.\nApplicants can submit the request either orally or in written form. The submitter of the request is not obliged to mention any reason for requesting access to information.\nAccess to public information does not require paying any administrative and court fees. The beneficiaries might only be called to pay for the actual costs of providing the information requested.\nAuthorities are obliged to issue their decisions within 15 days from the day of submitting a request. This deadline can be prolonged by other 15 days in case of complex requests (e.g. when the information must be sought outside the offices of the public authority concerned, when a single request contains a request for different information, or when the situation requires to conduct the Proportionality Test and Public Interest Test, etc.) \nIf the public authority does not hold the information, it is obliged to transfer the request to the body that might have it and notify the submitter thereof.\nAgainst the Decision taken by the public authority, the applicant may file a Complaint to the Commissioner within 15 days since the Decision has been delivered. \nNo complaint may be filed against the decision issued by the Commission, but an administrative dispute may be initiated before the High Administrative Court of the Republic of Croatia.\nThe Croatian law on access to public information regulates also the right to re-use information for commercial or non-commercial purposes.\n\nThe Croatian law on access to public information is quite advanced and in line with international standards and best practices. However, in the country a culture of secrecy persists. and the law, by itself, has not changed this and so far it has not raised the level of general transparency in Croatian society and institutions.\nOne of the main problem affecting the realization of the right of access to public information is the lack of adequate resources allocated to the office of the Commissioner. According to the 2015 Report on the implementation of the law presented to the Parliament, an increase in the resources for the functioning of the Commissioner’s Office if compared with previous years, they are still too limited to allow for the full application of the law. Over the course of 2015, the number of cases dealt with by the Commissioner’s Office has increased. However, the report showed that, due to a lack of staff and resources, not all the complaints received during 2015 have been solved by the Commissioner’s Office.\nAnother problem concerns the limited application of proactive disclosure, which - according to the Commissioner Anamarija Musa - together with re-use of information, a milestone of access to public information in the 21st century. Also, according to Commissioner Musa, the implementation of the law is particularly problematic at regional and local level and when it comes to requesting information to private entities providing a public service or to enterprises where the state holds the majority of capital shares. Moreover, Commissioner Musa deems particularly concerning the fact that access to public information requests are regularly ignored, so that the two thirds of the overall complaints received by the Commissioner are caused by this reason.\nIn Croatia, the web platform imamopravoznati.org, developed using the Alavetely software, has been launched to facilitate citizens' exercise of the right of access to public information. It allows to submit requests for public information to Croatian public authorities, and track their answers. Historic requests, any correspondence between the applicant and public authorities, are archived and publicly online.\n\n \n"}
{"id": "2355653", "url": "https://en.wikipedia.org/wiki?curid=2355653", "title": "Agent of influence", "text": "Agent of influence\n\nAn agent of influence is an agent of some stature who uses his or her position to influence public opinion or decision making to produce results beneficial to the country whose intelligence service operates the agent. Agents of influence are often the most difficult agents to detect, as there is seldom material evidence that connects them with a foreign power, but they can be among the most effective means of influencing foreign opinion and actions as they hold considerable credibility among the target audience. Most commonly they serve the interests of a foreign power in one of three ways: either as a controlled agent directly recruited and controlled by a foreign power; as a \"trusted contact\" that consciously collaborates to advance foreign interests but are not directly recruited or controlled by a foreign power; or as a \"useful idiot\" that is completely unaware of how their actions further the interests of a foreign power.\n\nThe term \"agent of influence\" is often used to describe both individuals and organizations engaged in influence operations. Individuals engaged in this type of influence operation may serve in the fields of journalism, government, art, labor, academia, or a number of other professional fields. Cultural opinion makers, nationalists, and religious leaders have also been targeted to serve as individual agents of influence.\n\nIn addition to individual agents of influence, front organizations can serve the interests of a foreign power in this capacity. Some Cold War examples of front organizations serving as agents of influence, focusing largely on the Soviet side, were many \"peace\" groups: the Christian Peace Conference, the International Organization of Journalists, the World Federation of Scientific Workers, the World Federation of Trade Unions, the International Institute for Peace, and the World Peace Council. When individuals join such organizations in good faith but are in fact serving the interests of a foreign elite, their affiliation becomes infiltration, and cumulatively the organization serves as an agent of influence.\n\n\nThe primary characteristic that distinguishes agents of influence from spies is the lack of absolute control exercised by the foreign power on an agent of influence. According to Angelo Codevilla, the work of an agent of influence \"can be far more valuable, subtle, and dangerous than that of a mere spy\". As witnessed in the Cold War through \"fellow travelers\", the best agents of influence were those whose interests paralleled that of the aggressor's and needed little if any coordination. A foreign power can rarely exercise complete control over an agent of influence, as these agents possess their own preferences and motivations; the most proven way to cultivate the desired results is for a foreign power to choose and develop an agent of influence whose interests already align with their own. Overlooking an agent of influence's different motivations can have negative consequences, as witnessed in World War I, when German political warfare strategists sent Vladimir Lenin back to St. Petersburg in an effort to foster domestic instability and get Russia out of the war in 1917. Since Lenin had different motivations and interests than the German government at the time, he acted in a manner not suited to German interests, and grew so powerful that his party was instrumental to bringing down Imperial Germany.\n\nExcessive efforts to control or exploit agents of influence can also have negative consequences. Such agents are best seen as strategic or tactical allies, and efforts to exercise too much control over them may result in the loss of an influence asset. Excessive exploitation of these agents can lead to their exposure by forcing them to take questionably one-sided positions, as witnessed in the exposure of Norwegian Arne Treholt. Because these agents exercise influence, their positions and opinions are not wholly secret, but the level to which they coordinate activities with a hostile power is likely to be kept secret.\n\nAgents of influence are most effective because they bring with them a sense of credibility among the target audience, and they use this credibility to convey a story or manipulate a situation in favor of the foreign power with which they share common preferences and motivations. This credibility makes agents of influence so effective that, according to Angelo Codevilla, using these agents is an act of war \"in the same sense that armies crashing across border or airplanes dropping bombs are acts of war because their results can be as intrusive or conclusive as the results of armies or bombs.\"\n\nIndividuals operating as an agent of influence may serve in the fields of journalism, government, art, labor, academia, or a number of other professional fields. Cultural opinion makers, nationalists, and religious leaders have also been targeted to serve as individual agents of influence. The following are some notable individuals that have been accused of being foreign agents of influence. The list is not exhaustive but is meant to show the wide range in which such agents can operate. As previously noted, proving someone is an agent of influence is among the most difficult endeavors, even for the most skilled counterintelligence officers.\n\n\nIn addition to individual agents of influence, front organizations can serve the interests of a foreign power in this capacity. When individuals join such organizations in good faith but are in fact serving the interests of a foreign elite, their affiliation becomes infiltration, and cumulatively the organization serves as an agent of influence. It is important to note, however, that not all front organizations focus exclusively on influence operations, as some have more specific objectives (intelligence collection, etc.). The Cold War is a recent example of increased use of not only front organizations, but of front organizations being used as agents of influence to alter the target nation's belief system and policies on the international stage.\n\nThe use of organizations as agents of influence during the Cold War is a recent example that serves to illustrate how frequently front organizations were used in an attempt to alter the perceptions and actions of a foreign nation and its public. A Communist front organization is an organization identified to be a front organization under the effective control of a Communist party, the Communist International or other Communist organizations. Lenin originated the idea in his manifesto of 1902, \"What Is to Be Done?\". Since the party was illegal in Russia, he proposed to reach the masses through \"a large number of other organizations intended for wide membership and, which, therefore, can be as loose and as public as possible.\" Generally called \"mass organizations\" by the Communists themselves, these groups were prevalent from the 1920s through the 1950s, with their use accelerating during the Popular Front period of the 1930s.\n\nStarting in 1939, Attorney General Biddle began compiling a list of Fascist and Communist front organizations. It was called \"Attorney General's List of Subversive Organizations\" (AGLOSO), but was not at first made public. Political pressures from Congress forced President Harry S. Truman to act. Truman's Attorney General Tom C. Clark expanded the list, which was officially authorized by presidential Executive Order 9835 in 1947 and was administered by the new Loyalty Review Board. The Board became part of the Civil Service Commission. The list was used by federal agencies to screen appointments during the Truman Administration. The program investigated over 3 million government employees, of whom 300 were dismissed as security risks. Adverse decisions could be appealed to the Loyalty Review Board, a government agency set up by President Truman.\n\nThe Loyalty Review Board publicized the previously secret Attorney General's list in March 1948 as a \"List of Communist classified organizations.\" The list gave the name and date founded, and (for active groups) the headquarters, and chief officers. \n\nIn 1955, SSIS published a list of what it described as the 82 most active and typical sponsors of communist fronts in the United States; some of those named had literally dozens of affiliations with groups that had either been cited as Communist fronts or had been labelled \"subversive\" by either the subcommittee or the House Committee on Un-American Activities.\n\n\n"}
{"id": "3556316", "url": "https://en.wikipedia.org/wiki?curid=3556316", "title": "Apples and oranges", "text": "Apples and oranges\n\nA comparison of apples and oranges occurs when two items or groups of items are compared that cannot be practically compared.\n\nThe idiom, \"comparing apples and oranges\", refers to the apparent differences between items which are popularly thought to be incomparable or incommensurable, such as apples and oranges. The idiom may also be used to indicate that a false analogy has been made between two items, such as where an \"apple\" is faulted for not being a good \"orange\".\n\nThe idiom is not unique to English. In Quebec French, it may take the form \"comparer des pommes avec des oranges\" (to compare apples and oranges), while in European French the idiom says \"comparer des pommes et des poires\" (to compare apples and pears). In Latin American Spanish, it is usually \"comparar papas y boniatos\" (comparing potatoes and sweet potatoes) or commonly for all varieties of Spanish \"comparar peras con manzanas\" (comparing pears and apples). In some other languages the term for 'orange' derives from 'apple', suggesting not only that a direct comparison between the two is possible, but that it is implicitly present in their names. Fruit other than apples and oranges can also be compared; for example, apples and pears are compared in Danish, Dutch, German, Spanish, Swedish, Croatian, Czech, Romanian, Hungarian, Italian, Slovene, Luxembourgish, Serbian, and Turkish. In fact, in the Spanish-speaking world, a common idiom is \"sumar peras con manzanas\", that is, \"to add pears and apples\"; the same thing applies in Italian (\"sommare le mele con le pere\") and Romanian (\"a aduna merele cu perele\"). In Portuguese, the expression is \"comparar laranjas com bananas\" (compare orange to banana). In Czech, the idiom \"míchat jablka s hruškami\" literally means 'to mix apples and pears'.\n\nSome languages use completely different items, such as the Serbian \"Поредити бабе и жабе\" (comparing grandmothers and toads), or the Romanian \"baba şi mitraliera\" (the grandmother and the machine gun); \"vaca şi izmenele\" (the cow and the longjohns); or \"țiganul şi carioca\" (the gypsy and the marker), or the Welsh \"mor wahanol â mêl a menyn\" (as different as honey and butter), while some languages compare dissimilar properties of dissimilar items. For example, an equivalent Danish idiom, \"Hvad er højest, Rundetårn eller et tordenskrald?\" means \"What is highest, the Round Tower or a thunderclap?\", referring to the size of the former and the sound of the latter. In Russian, the phrase \"сравнивать тёплое с мягким\" (to compare warm and soft) is used. In Argentina, a common question is \"¿En qué se parecen el amor y el ojo del hacha?\" (What do love and the eye of an axe have in common?) and emphasizes dissimilarity between two subjects; in Colombia, a similar (though more rude) version is common: \"confundir la mierda con la pomada\" (to confuse shit with ointment). In Polish, the expression \"co ma piernik do wiatraka?\" is used, meaning \"What has (is) gingerbread to a windmill?\". In Chinese, a phrase that has the similar meaning is 风马牛不相及 (fēng mǎ niú bù xiāng jí), literally meaning \"horses and cattles won't mate with each other\", and later used to describe things that are totally unrelated and incomparable.\n\nA number of more exaggerated comparisons are sometimes made, in cases in which the speaker believes the two objects being compared are radically different. For example, \"oranges with orangutans\", \"apples with dishwashers\", and so on. In English, different fruits, such as pears, plums, or lemons are sometimes substituted for oranges in this context.\n\nSometimes the two words sound similar, for example, Romanian \"merele cu perele\" (apples and pears) and the Hungarian \"szezont a fazonnal\" (the season with the fashion).\n\nAt least two tongue-in-cheek scientific studies have been conducted on the subject, each of which concluded that apples can be compared with oranges fairly easily and on a low budget and the two fruits are quite similar.\n\nThe first study, conducted by Scott A. Sandford of the NASA Ames Research Center, used infrared spectroscopy to analyze both apples and oranges. The study, which was published in the satirical science magazine \"Annals of Improbable Research\", concluded: \"[...] the comparing apples and oranges defense should no longer be considered valid. This is a somewhat startling revelation. It can be anticipated to have a dramatic effect on the strategies used in arguments and discussions in the future.\"\n\nA second study, written by Stamford Hospital's surgeon-in-chief James Barone and published in the \"British Medical Journal,\" noted that the phrase \"apples and oranges\" was appearing with increasing frequency in the medical literature, with some notable articles comparing \"Desflurane and propofol\" and \"Salmeterol and ipratropium\" with \"apples and oranges\". The study also found that both apples and oranges were sweet, similar in size, weight, and shape, that both are grown in orchards, and both may be eaten, juiced, and so on. The only significant differences found were in terms of seeds (the study used seedless oranges), the involvement of Johnny Appleseed, and color.\n\nThe \"Annals of Improbable Research\" subsequently noted that the \"earlier investigation was done with more depth, more rigour, and, most importantly, more expensive equipment\" than the \"British Medical Journal\" study.\n\nOn April Fools' Day 2014, \"The Economist\" compared worldwide production of apples and oranges from 1983 to 2013, however noted them to be \"unrelated variables\".\n\nWhile references to comparing apples and oranges are often a rhetorical device, references to adding apples and oranges are made in the case of teaching students the proper uses of units. Here, the admonition not to \"add apples and oranges\" refers to the requirement that two quantities with different units may not be combined by addition, although they may always be combined in ratio form by multiplication, so that multiplying ratios of apples and oranges is allowed. Similarly, the concept of this distinction is often used metaphorically in elementary algebra.\n\nThe admonition is really more of a mnemonic, since in general counts of objects have no intrinsic unit and, for example, a number count of apples may be dimensionless or have dimension \"fruit\"; in either of these two cases, apples and oranges may indeed be added.\n\n"}
{"id": "113147", "url": "https://en.wikipedia.org/wiki?curid=113147", "title": "Barbarian", "text": "Barbarian\n\nA barbarian is a human who is perceived to be either uncivilized or primitive. The designation is usually applied as generalization based on a popular stereotype; barbarians can be any member of a nation judged by some to be less civilized or orderly (such as a tribal society), but may also be part of a certain \"primitive\" cultural group (such as nomads) or social class (such as bandits) both within and outside one's own nation. Alternatively, they may instead be admired and romanticised as noble savages. In idiomatic or figurative usage, a \"barbarian\" may also be an individual reference to a brutal, cruel, warlike, and insensitive person.\n\nThe term originates from the (\"barbaros\" pl. βάρβαροι \"barbaroi\"), which in turn originates from the languages of early Anatolian nations that were heard by the Greeks as \"bar... bar...\" In Ancient Greece, the Greeks used the term towards those who did not speak Greek and follow classical Greek customs. In Ancient Rome, the Romans used the term towards non-Romans such as the Germanics, Celts, Gauls, Iberians, Thracians, Illyrians, Berbers, Parthians, and Sarmatians. In the early modern period and sometimes later, the Byzantine Greeks used it for the Turks, in a clearly pejorative manner.\n\nThe Ancient Greek name βάρβαρος (\"barbaros\"), \"barbarian\", was an antonym for πολίτης (\"politēs\"), \"citizen\" (from πόλις – \"polis\", \"city-state\"). The earliest attested form of the word is the Mycenaean Greek , \"pa-pa-ro\", written in Linear B syllabic script.\n\nThe Greeks used the term \"barbarian\" for all non-Greek-speaking peoples, including the Egyptians, Persians, Medes and Phoenicians, emphasizing their otherness. According to Greek writers, this was because the language they spoke sounded to Greeks like gibberish represented by the sounds \"bar..bar..;\" the alleged root of the word βάρβαρος, which is an echomimetic or onomatopoeic word. However, in various occasions, the term was also used by Greeks, especially the Athenians, to deride other Greek tribes and states (such as Epirotes, Eleans, Macedonians, Boeotians and Aeolic-speakers) but also fellow Athenians, in a pejorative and politically motivated manner. Of course, the term also carried a cultural dimension to its dual meaning. The verb (\"barbarízō\") in ancient Greek meant to behave or talk like a barbarian, or to hold with the barbarians.\n\nPlato (\"Statesman\" 262de) rejected the Greek–barbarian dichotomy as a logical absurdity on just such grounds: dividing the world into Greeks and non-Greeks told one nothing about the second group, yet Plato used the term barbarian frequently in his seventh letter. In Homer's works, the term appeared only once (\"Iliad\" 2.867), in the form () (\"of incomprehensible speech\"), used of the Carians fighting for Troy during the Trojan War. In general, the concept of \"barbaros\" did not figure largely in archaic literature before the 5th century BC. Still it has been suggested that \"barbarophonoi\" in the \"Iliad\" signifies not those who spoke a non-Greek language but simply those who spoke Greek badly.\n\nA change occurred in the connotations of the word after the Greco-Persian Wars in the first half of the 5th century BC. Here a hasty coalition of Greeks defeated the vast Persian Empire. Indeed, in the Greek of this period 'barbarian' is often used expressly to refer to Persians, who were enemies of the Greeks in this war.\n\nThe Romans used the term \"barbarus\" for uncivilised people, opposite to Greek or Roman, and in fact, it became a common term to refer to all foreigners among Romans after Augustus age (as, among the Greeks, after the Persian wars, the Persians), including the Germanic peoples, Persians, Gauls, Phoenicians and Carthaginians.\n\nThe Greek term \"barbaros\" was the etymological source for many words meaning \"barbarian\", including English \"barbarian\", which was first recorded in 16th century Middle English.\n\nA word \"barbara-\" is also found in the Sanskrit of ancient India, with the primary meaning of \"stammering\" implying someone with an unfamiliar language. The Greek word \"barbaros\" is related to Sanskrit \"barbaras\" (stammering). This Indo-European root is also found in Latin \"balbus\" for \"stammering\" and Czech blblati \"to stammer\".\n\nIn Aramaic, Old Persian and Arabic context, the root refers to \"babble confusedly\". It appears as \"barbary\" or in Old French \"barbarie\", itself derived from the Arabic \"Barbar\", \"Berber\", which is an ancient Arabic term for the North African inhabitants west of Egypt. The Arabic word might be ultimately from Greek \"barbaria\".\n\nThe \"Oxford English Dictionary\" defines five meanings of the noun \"barbarian\", including an obsolete Barbary usage.\nThe \"OED\" \"barbarous\" entry summarizes the semantic history. \"The sense-development in ancient times was (with the Greeks) 'foreign, non-Hellenic,' later 'outlandish, rude, brutal'; (with the Romans) 'not Latin nor Greek,' then 'pertaining to those outside the Roman Empire'; hence 'uncivilized, uncultured,' and later 'non-Christian,' whence 'Saracen, heathen'; and generally 'savage, rude, savagely cruel, inhuman.'\"\n\nGreek attitudes towards \"barbarians\" developed in parallel with the growth of chattel slavery - especially in Athens. Although the enslavement of Greeks for non-payment of debts continued in most Greek states, Athens banned this practice under Solon in the early 6th century BC. Under the Athenian democracy established ca. 508 BC, slavery came into use on a scale never before seen among the Greeks. Massive concentrations of slaves worked under especially brutal conditions in the silver mines at Laureion in south-eastern Attica after the discovery of a major vein of silver-bearing ore there in 483 BC, while the phenomenon of skilled slave craftsmen producing manufactured goods in small factories and workshops became increasingly common.\n\nFurthermore, slave-ownership no longer became the preserve of the rich: all but the poorest of Athenian households came to have slaves in order to supplement the work of their free members. The slaves of Athens that had \"barbarian\" origins were coming especially from lands around the Black Sea such as Thrace and Taurica (Crimea), while Lydians, Phrygians and Carians came from Asia Minor. Aristotle (\"Politics\" 1.2–7; 3.14) characterises barbarians as slaves by nature.\n\nFrom this period, words like \"barbarophonos\", cited above from Homer, came into use not only for the sound of a foreign language but also for foreigners who spoke Greek improperly. In the Greek language, the word \"logos\" expressed both the notions of \"language\" and \"reason\", so Greek-speakers readily conflated speaking poorly with stupidity.\n\nFurther changes occurred in the connotations of \"barbari\"/\"barbaroi\" in Late Antiquity, when bishops and \"catholikoi\" were appointed to sees connected to cities among the \"civilized\" \"gentes barbaricae\" such as in Armenia or Persia, whereas bishops were appointed to supervise entire peoples among the less settled.\n\nEventually the term found a hidden meaning through the folk etymology of Cassiodorus (c. 485 – c. 585). He stated that the word \"barbarian\" was \"made up of \"barba\" (beard) and \"rus\" (flat land); for barbarians did not live in cities, making their abodes in the fields like wild animals\".\n\nFrom classical origins the Hellenic stereotype of barbarism evolved: barbarians are like children, unable to speak or reason properly, cowardly, effeminate, luxurious, cruel, unable to control their appetites and desires, politically unable to govern themselves. Writers voiced these stereotypes with much shrillness - Isocrates in the 4th century B.C., for example, called for a war of conquest against Persia as a panacea for Greek problems.\n\nHowever, the disparaging Hellenic stereotype of barbarians did not totally dominate Hellenic attitudes. Xenophon (died 354 B.C.), for example, wrote the \"Cyropaedia\", a laudatory fictionalised account of Cyrus the Great, the founder of the Persian Empire, effectively a utopian text. In his \"Anabasis\", Xenophon's accounts of the Persians and other non-Greeks who he knew or encountered show few traces of the stereotypes.\n\nIn Plato's \"Protagoras\", Prodicus of Ceos calls \"barbarian\" the Aeolian dialect that Pittacus of Mytilene spoke.\n\nThe renowned orator Demosthenes (384–322 B.C.) made derogatory comments in his speeches, using the word \"barbarian\".\n\nIn the Bible's New Testament, St. Paul (from Tarsus) - lived about A.D. 5 to about A.D. 67) uses the word \"barbarian\" in its Hellenic sense to refer to non-Greeks (\"Romans 1:14\"), and he also uses it to characterise one who merely speaks a different language (\"1 Corinthians 14:11\").\n\nAbout a hundred years after Paul's time, Lucian – a native of Samosata, in the former kingdom of Commagene, which had been absorbed by the Roman Empire and made part of the province of Syria – used the term \"barbarian\" to describe himself. Because he was a noted satirist, this could have indicated self-deprecating irony. It might also have suggested descent from Samosata's original Semitic population – who were likely called \"barbarians by later Hellenistic, Greek-speaking settlers\", and might have eventually taken up this appellation themselves.\n\nThe term retained its standard usage in the Greek language throughout the Middle Ages; Byzantine Greeks used it widely until the fall of the Eastern Roman Empire, (later named the Byzantine Empire) in the 15th century (1453 with the fall of capital city Constantinople}.\n\nCicero (106-43 BC) described the mountain area of inner Sardinia as \"a land of barbarians\", with these inhabitants also known by the manifestly pejorative term \"latrones mastrucati\" (\"thieves with a rough garment in wool\").\nThe region, still known as \"Barbagia\" (in Sardinian \"Barbàgia\" or \"Barbàza\"), preserves this old \"barbarian\" designation in its name – but it no longer consciously retains \"barbarian\" associations: the inhabitants of the area themselves use the name naturally and unaffectedly.\n\nThe statue of the \"Dying Galatian\" provides some insight into the Hellenistic perception of and attitude towards \"Barbarians\". Attalus I of Pergamon (ruled 241-197 BC) commissioned (220s BC) a statue to celebrate his victory (ca 232 BC) over the Celtic Galatians in Anatolia (the bronze original is lost, but a Roman marble copy was found in the 17th century). The statue depicts with remarkable realism a dying Celt warrior with a typically Celtic hairstyle and moustache. He sits on his fallen shield while a sword and other objects lie beside him. He appears to be fighting against death, refusing to accept his fate.\n\nThe statue serves both as a reminder of the Celts' defeat, thus demonstrating the might of the people who defeated them, and a memorial to their bravery as worthy adversaries. As H. W. Janson comments, the sculpture conveys the message that \"they knew how to die, barbarians that they were\".\n\nThe Greeks admired Scythians and Galatians as heroic individuals – and even (as in the case of Anacharsis) as philosophers – but they regarded their culture as barbaric. The Romans indiscriminately characterised the various Germanic tribes, the settled Gauls, and the raiding Huns as barbarians, and subsequent classically oriented historical narratives depicted the migrations associated with the end of the Western Roman Empire as the \"barbarian invasions\".\n\nThe Romans adapted the term in order to refer to anything that was non-Roman. The German cultural historian Silvio Vietta points out that the meaning of the word \"barbarous\" has undergone a semantic change in modern times, after Michel de Montaigne used it to characterize the activities of the Spaniards in the New World – supposedly representatives of the \"higher\" European culture – as \"barbarous,\" in a satirical essay published in the year 1580. It was not the supposedly \"uncivilized\" Indian tribes who were \"barbarous\", but the conquering Spaniards. Montaigne argued that Europeans noted the barbarism of other cultures but not the crueler and more brutal actions of their own societies, particularly (in his time) during the so-called religious wars. In Montaigne's view, his own people – the Europeans – were the real \"barbarians\". In this way, the Eurocentric argument was turned around and applied to the European invaders. With this shift in meaning, a whole literature arose in Europe that characterized the indigenous Indian peoples as innocent, and the militarily superior Europeans as \"barbarous\" intruders invading a paradisical world.\n\nHistorically, the term \"barbarian\" has seen widespread use, in English. Many peoples have dismissed alien cultures and even rival civilizations, because they were unrecognizably strange. For instance, the nomadic steppe peoples north of the Black Sea, including the Pechenegs and the Kipchaks, were called barbarians by the Byzantines.\n\nThe \"Berbers\" of North Africa were among the many peoples called \"Barbarian\" by the Romans; in their case, the name remained in use, having been adopted by the Arabs (see Berber etymology) and is still in use as the name for the non-Arabs in North Africa (though not by themselves). The geographical term Barbary or Barbary Coast, and the name of the Barbary pirates based on that coast (and who were not necessarily Berbers) were also derived from it.\n\nThe term has also been used to refer to people from Barbary, a region encompassing most of North Africa. The name of the region, \"Barbary,\" comes from the Arabic word \"Barbar,\" possibly from the Latin word \"barbaricum,\" meaning \"land of the barbarians.\"\n\nMany languages define the \"Other\" as those who do not speak one's language; Greek \"barbaroi\" was paralleled by Arabic \"ajam\" \"non-Arabic speakers; non-Arabs; (especially) Persians.\"\n\nThe term \"Barbarian\" in traditional Chinese culture had several aspects. For one thing, Chinese has more than one historical \"barbarian\" exonym. Several historical Chinese characters for non-Chinese peoples were graphic pejoratives, the character for the Yao people, for instance, was changed from \"yao\" 猺 \"jackal\" to \"yao\" 瑤 \"precious jade\" in the modern period. The original Hua–Yi distinction between \"Chinese\" and \"barbarian\" was based on culture and power but not on race.\n\nHistorically, the Chinese used various words for foreign ethnic groups. They include terms like 夷 \"Yi\", which is often translated as \"barbarians.\" Despite this conventional translation, there are also other ways of translating \"Yi\" into English. Some of the examples include \"foreigners,\" \"ordinary others,\" \"wild tribes,\" \"uncivilized tribes,\" and so forth.\n\nChinese historical records mention what may now perhaps be termed \"barbarian\" peoples for over four millennia, although this considerably predates the Greek language origin of the term \"barbarian\", at least as is known from the thirty-four centuries of written records in the Greek language. The sinologist Herrlee Glessner Creel said, \"Throughout Chinese history \"the barbarians\" have been a constant motif, sometimes minor, sometimes very major indeed. They figure prominently in the Shang oracle inscriptions, and the dynasty that came to an end only in 1912 was, from the Chinese point of view, barbarian.\"\n\nShang dynasty (1600–1046 BC) oracles and bronze inscriptions first recorded specific Chinese exonyms for foreigners, often in contexts of warfare or tribute. King Wu Ding (r. 1250–1192 BC), for instance, fought with the Guifang 鬼方, Di 氐, and Qiang 羌 \"barbarians.\"\n\nDuring the Spring and Autumn period (771–476 BC), the meanings of four exonyms were expanded. \"These included Rong, Yi, Man, and Di—all general designations referring to the barbarian tribes.\" These \"Siyi\" 四夷 \"Four Barbarians\", most \"probably the names of ethnic groups originally,\" were the Yi or Dongyi 東夷 \"eastern barbarians,\" Man or Nanman 南蠻 \"southern barbarians,\" Rong or Xirong 西戎 \"western barbarians,\" and Di or Beidi 北狄 \"northern barbarians.\" The Russian anthropologist Mikhail Kryukov concluded.\nThe Chinese classics use compounds of these four generic names in localized \"barbarian tribes\" exonyms such as \"west and north\" \"Rongdi\", \"south and east\" \"Manyi\", \"Nanyibeidi\" \"barbarian tribes in the south and the north,\" and \"Manyirongdi\" \"all kinds of barbarians.\" Creel says the Chinese evidently came to use \"Rongdi\" and \"Manyi\" \"as generalized terms denoting 'non-Chinese,' 'foreigners,' 'barbarians',\" and a statement such as \"the Rong and Di are wolves\" (\"Zuozhuan\", Min 1) is \"very much like the assertion that many people in many lands will make today, that 'no foreigner can be trusted'.\"\n\nThis word \"Yi\" has both specific references, such as to \"Huaiyi\" 淮夷 peoples in the Huai River region, and generalized references to \"barbarian; foreigner; non-Chinese.\" \"Lin Yutang's Chinese-English Dictionary of Modern Usage\" translates \"Yi\" as \"Anc[ient] barbarian tribe on east border, any border or foreign tribe.\" The sinologist Edwin G. Pulleyblank says the name \"Yi\" \"furnished the primary Chinese term for 'barbarian',\" but \"Paradoxically the Yi were considered the most civilized of the non-Chinese peoples.\n\nSome Chinese classics romanticize or idealize barbarians, comparable to the western noble savage construct. For instance, the Confucian \"Analects\" records:\nThe translator Arthur Waley noted that, \"A certain idealization of the 'noble savage' is to be found fairly often in early Chinese literature\", citing the \"Zuo Zhuan\" maxim, \"When the Emperor no longer functions, learning must be sought among the 'Four Barbarians,' north, west, east, and south.\" Professor Creel said,\nFrom ancient to modern times the Chinese attitude toward people not Chinese in culture—\"barbarians\"—has commonly been one of contempt, sometimes tinged with fear ... It must be noted that, while the Chinese have disparaged barbarians, they have been singularly hospitable both to individuals and to groups that have adopted Chinese culture. And at times they seem to have had a certain admiration, perhaps unwilling, for the rude force of these peoples or simpler customs.\n\nIn a somewhat related example, Mencius believed that Confucian practices were universal and timeless, and thus followed by both Hua and Yi, \"Shun was an Eastern barbarian; he was born in Chu Feng, moved to Fu Hsia, and died in Ming T'iao. King Wen was a Western barbarian; he was born in Ch'i Chou and died in Pi Ying. Their native places were over a thousand \"li\" apart, and there were a thousand years between them. Yet when they had their way in the Central Kingdoms, their actions matched like the two halves of a tally. The standards of the two sages, one earlier and one later, were identical.\"\n\nThe prominent (121 CE) \"Shuowen Jiezi\" character dictionary, defines \"yi\" 夷 as \"men of the east” 東方之人也. The dictionary also informs that \"Yi\" is not dissimilar from the \"Xia\" 夏, which means Chinese. Elsewhere in the \"Shuowen Jiezi\", under the entry of \"qiang\" 羌, the term \"yi\" is associated with benevolence and human longevity. \"Yi\" countries are therefore virtuous places where people live long lives. This is why Confucius wanted to go to \"yi\" countries when the \"dao\" could not be realized in the central states.\n\nSome Chinese characters used to transcribe non-Chinese peoples were graphically pejorative ethnic slurs, where the insult derived not from the Chinese word but from the character used to write it. Take for instance, the Written Chinese transcription of \"Yao\" \"the Yao people\", who primarily live in the mountains of southwest China and Vietnam. When 11th-century Song Dynasty authors first transcribed the exonym \"Yao\", they insultingly chose \"yao\" 猺 \"jackal\" from a lexical selection of over 100 characters pronounced \"yao\" (e.g., 腰 \"waist\", 遙 \"distant\", 搖 \"shake\"). During a series of 20th-century Chinese language reforms, this graphic pejorative 猺 (written with the 犭\"dog/beast radical\") \"jackal; the Yao\" was replaced twice; first with the invented character \"yao\" 傜 (亻\"human radical\") \"the Yao\", then with \"yao\" 瑤 (玉 \"jade radical\") \"precious jade; the Yao.\" Chinese orthography (symbols used to write a language) can provide unique opportunities to write ethnic insults logographically that do not exist alphabetically. For the Yao ethnic group, there is a difference between the transcriptions \"Yao\" 猺 \"jackal\" and \"Yao\" 瑤 \"jade\" but none between the romanizations \"Yao\" and \"Yau\".\n\nAccording to the archeologist William Meacham, it was only by the time of the late Shang dynasty that one can speak of \"Chinese,\" \"Chinese culture,\" or \"Chinese civilization.\" \"There is a sense in which the traditional view of ancient Chinese history is correct (and perhaps it originated ultimately in the first appearance of dynastic civilization): those on the fringes and outside this esoteric event were \"barbarians\" in that they did not enjoy (or suffer from) the fruit of civilization until they were brought into close contact with it by an imperial expansion of the civilization itself.\"\nIn a similar vein, Creel explained the significance of Confucian \"li\" \"ritual; rites; propriety\".\nThe fundamental criterion of \"Chinese-ness,\" anciently and throughout history, has been cultural. The Chinese have had a particular way of life, a particular complex of usages, sometimes characterized as \"li\". Groups that conformed to this way of life were, generally speaking, considered Chinese. Those that turned away from it were considered to cease to be Chinese. ... It was the process of acculturation, transforming barbarians into Chinese, that created the great bulk of the Chinese people. The barbarians of Western Chou times were, for the most part, future Chinese, or the ancestors of future Chinese. This is a fact of great importance. ... It is significant, however, that we almost never find any references in the early literature to physical differences between Chinese and barbarians. Insofar as we can tell, the distinction was purely cultural.\nDikötter says,\nThought in ancient China was oriented towards the world, or \"tianxia\", \"all under heaven.\" The world was perceived as one homogenous unity named \"great community\" (\"datong\") The Middle Kingdom [China], dominated by the assumption of its cultural superiority, measured outgroups according to a yardstick by which those who did not follow the \"Chinese ways\" were considered \"barbarians.\" A Theory of \"using the Chinese ways to transform the barbarian\" as strongly advocated. It was believed that the barbarian could be culturally assimilated. In the Age of Great Peace, the barbarians would flow in and be transformed: the world would be one. \n\nAccording to the Pakistani academic M. Shahid Alam, \"The centrality of culture, rather than race, in the Chinese world view had an important corollary. Nearly always, this translated into a civilizing mission rooted in the premise that 'the barbarians could be culturally assimilated'\"; namely \"laihua\" 來化 \"come and be transformed\" or \"Hanhua\" 漢化 \"become Chinese; be sinicized.\"\n\nTwo millennia before the French anthropologist Claude Lévi-Strauss wrote \"The Raw and the Cooked\", the Chinese differentiated \"raw\" and \"cooked\" categories of barbarian peoples who lived in China. The \"shufan\" 熟番 \"cooked [food eating] barbarians\" are sometimes interpreted as Sinicized, and the \"shengfan\" 生番 \"raw [food eating] barbarians\" as not Sinicized.\nThe \"Liji\" gives this description.\nThe people of those five regions – the Middle states, and the [Rong], [Yi] (and other wild tribes around them) – had all their several natures, which they could not be made to alter. The tribes on the east were called [Yi]. They had their hair unbound, and tattooed their bodies. Some of them ate their food without its being cooked with fire. Those on the south were called Man. They tattooed their foreheads, and had their feet turned toward each other. Some of them ate their food without its being cooked with fire. Those on the west were called [Rong]. They had their hair unbound, and wore skins. Some of them did not eat grain-food. Those on the north were called [Di]. They wore skins of animals and birds, and dwelt in caves. Some of them did not eat grain-food.\n\nDikötter explains the close association between nature and nurture. \"The \"shengfan\", literally 'raw barbarians', were considered savage and resisting. The \"shufan\", or 'cooked barbarians', were tame and submissive. The consumption of raw food was regarded as an infallible sign of savagery that affected the physiological state of the barbarian.\"\n\nSome Warring States period texts record a belief that the respective natures of the Chinese and the barbarian were incompatible. Mencius, for instance, once stated: \"I have heard of the Chinese converting barbarians to their ways, but not of their being converted to barbarian ways.\" Dikötter says, \"The nature of the Chinese was regarded as impermeable to the evil influences of the barbarian; no retrogression was possible. Only the barbarian might eventually change by adopting Chinese ways.\"\n\nHowever, different thinkers and texts convey different opinions on this issue. The prominent Tang Confucian Han Yu, for example, wrote in his essay \"Yuan Dao\" the following: \"When Confucius wrote the \"Chunqiu\", he said that if the feudal lords use Yi ritual, then they should be called Yi; If they use Chinese rituals, then they should be called Chinese.\" Han Yu went on to lament in the same essay that the Chinese of his time might all become Yi because the Tang court wanted to put Yi laws above the teachings of the former kings. Therefore, Han Yu's essay shows the possibility that the Chinese can lose their culture and become the uncivilized outsiders, and that the uncivilized outsiders have the potential to become Chinese.\n\nAfter the Song Dynasty, many of China's rulers in the north were of Inner Asia ethnicities, such as Qidan, Ruzhen, and Mongols of the Liao, Jin and Yuan Dynasties, the latter ended up ruling over the entire China. Hence, the historian John King Fairbank wrote, \"the influence on China of the great fact of alien conquest under the Liao-Jin-Yuan dynasties is just beginning to be explored.\" During the Qing Dynasty, the rulers of China adopted Confucian philosophy and Han Chinese institutions to show that the Manchu rulers had received the Mandate of Heaven to rule China. At the same time, they also tried to retain their own indigenous culture. Due to the Manchus' adoption of Han Chinese culture, most Han Chinese (though not all) did accept the Manchus as the legitimate rulers of China. Similarly, according to Fudan University historian Yao Dali, even the supposedly \"patriotic\" hero Wen Tianxiang of the late Song and early Yuan period did not believe the Mongol rule to be illegitimate. In fact, Wen was willing to live under Mongol rule as long as he was not forced to be a Yuan dynasty official, out of his loyalty to the Song dynasty. Yao explains that Wen chose to die in the end because he was forced to become a Yuan official. So, Wen chose death due to his loyalty to his dynasty, not because he viewed the Yuan court as a non-Chinese, illegitimate regime and therefore refused to live under their rule. Yao also says that many Chinese who were living in the Yuan-Ming transition period also shared Wen's beliefs of identifying with and putting loyalty towards one's dynasty above racial/ethnic differences. Many Han Chinese writers did not celebrate the collapse of the Mongols and the return of the Han Chinese rule in the form of the Ming dynasty government at that time. Many Han Chinese actually chose not to serve in the new Ming court at all due to their loyalty to the Yuan. Some Han Chinese also committed suicide on behalf of the Mongols as a proof of their loyalty. We should note that the founder of the Ming Dynasty, Zhu Yuanzhang, also indicated that he was happy to be born in the Yuan period and that the Yuan did legitimately receive the Mandate of Heaven to rule over China. On a side note, one of his key advisors, Liu Ji, generally supported the idea that while the Chinese and the non-Chinese are different, they are actually equal. Liu was therefore arguing against the idea that the Chinese were and are superior to the \"Yi.\"\n\nThese things show that many times, pre-modern Chinese did view culture (and sometimes politics) rather than race and ethnicity as the dividing line between the Chinese and the non-Chinese. In many cases, the non-Chinese could and did become the Chinese and vice versa, especially when there was a change in culture.\n\nAccording to the historian Frank Dikötter, \"The delusive myth of a Chinese antiquity that abandoned racial standards in favour of a concept of cultural universalism in which all barbarians could ultimately participate has understandably attracted some modern scholars. Living in an unequal and often hostile world, it is tempting to project the utopian image of a racially harmonious world into a distant and obscure past.\"\n\nThe politician, historian, and diplomat K. C. Wu analyzes the origin of the characters for the \"Yi\", \"Man\", \"Rong\", \"Di\", and \"Xia\" peoples and concludes that the \"ancients formed these characters with only one purpose in mind—to describe the different ways of living each of these people pursued.\" Despite the well-known examples of pejorative exonymic characters (such as the \"dog radical\" in Di), he claims there is no hidden racial bias in the meanings of the characters used to describe these different peoples, but rather the differences were \"in occupation or in custom, not in race or origin.\" K. C. Wu says the modern character 夷 designating the historical \"Yi peoples,\" composed of the characters for 大 \"big (person)\" and 弓 \"bow\", implies a big person carrying a bow, someone to perhaps be feared or respected, but not to be despised. However, differing from K. C. Wu, the scholar Wu Qichang believes that the earliest oracle bone script for \"yi\" 夷 was used interchangeably with \"shi\" 尸 \"corpse\". The historian John Hill explains that \"Yi\" \"was used rather loosely for non-Chinese populations of the east. It carried the connotation of people ignorant of Chinese culture and, therefore, 'barbarians'.\"\n\nChristopher I. Beckwith makes the extraordinary claim that the name \"barbarian\" should only be used for Greek historical contexts, and is inapplicable for all other \"peoples to whom it has been applied either historically or in modern times.\" Beckwith notes that most specialists in East Asian history, including him, have translated Chinese exonyms as English \"\"barbarian\".\" He believes that after academics read his published explanation of the problems, except for direct quotations of \"earlier scholars who use the word, it should no longer be used as a term by any writer.\"\n\nThe first problem is that, \"it is impossible to translate the word \"barbarian\" into Chinese because the concept does not exist in Chinese,\" meaning a single \"completely generic\" loanword from Greek \"barbar-\". \"Until the Chinese borrow the word \"barbarian\" or one of its relatives, or make up a new word that explicitly includes the same basic ideas, they cannot express the idea of the 'barbarian' in Chinese.\". The usual Standard Chinese translation of English \"barbarian\" is \"yemanren\" (), which Beckwith claims, \"actually means 'wild man, savage'. That is very definitely not the same thing as 'barbarian'.\" Despite this semantic hypothesis, Chinese-English dictionaries regularly translate \"yemanren\" as \"barbarian\" or \"barbarians.\" Beckwith concedes that the early Chinese \"apparently disliked foreigners in general and looked down on them as having an inferior culture,\" and pejoratively wrote some exonyms. However, he purports, \"The fact that the Chinese did not \"like\" foreigner Y and occasionally picked a transcriptional character with negative meaning (in Chinese) to write the sound of his ethnonym, is irrelevant.\"\n\nBeckwith's second problem is with linguists and lexicographers of Chinese. \"If one looks up in a Chinese-English dictionary the two dozen or so partly generic words used for various foreign peoples throughout Chinese history, one will find most of them defined in English as, in effect, 'a kind of barbarian'. Even the works of well-known lexicographers such as Karlgren do this.\"\nAlthough Beckwith does not cite any examples, the Swedish sinologist Bernhard Karlgren edited two dictionaries: \"Analytic Dictionary of Chinese and Sino-Japanese\" (1923) and \"Grammata Serica Recensa\" (1957). Compare Karlgrlen's translations of the \"siyi\" \"four barbarians\":\nThe \"Sino-Tibetan Etymological Dictionary and Thesaurus\" Project includes Karlgren's \"GSR\" definitions. Searching the STEDT Database finds various \"a kind of\" definitions for plant and animal names (e.g., \"you\" 狖 \"a kind of monkey,\" but not one \"a kind of barbarian\" definition. Besides faulting Chinese for lacking a general \"barbarian\" term, Beckwith also faults English, which \"has no words for the many foreign peoples referred to by one or another Classical Chinese word, such as 胡 \"hú\", 夷 \"yí\", 蠻 \"mán\", and so on.\"\n\nThe third problem involves Tang Dynasty usages of \"fan\" \"foreigner\" and \"lu\" \"prisoner\", neither of which meant \"barbarian.\" Beckwith says Tang texts used \"fan\" 番 or 蕃 \"foreigner\" (see \"shengfan\" and \"shufan\" above) as \"perhaps the only true generic at any time in Chinese literature, was practically the opposite of the word \"barbarian\". It meant simply 'foreign, foreigner' without any pejorative meaning.\" In modern usage, \"fan\" 番 means \"foreigner; barbarian; aborigine\". The linguist Robert Ramsey illustrates the pejorative connotations of \"fan\".\nThe word \"Fān\" was formerly used by the Chinese almost innocently in the sense of 'aborigines' to refer to ethnic groups in South China, and Mao Zedong himself once used it in 1938 in a speech advocating equal rights for the various minority peoples. But that term has now been so systematically purged from the language that it is not to be found (at least in that meaning) even in large dictionaries, and all references to Mao's 1938 speech have excised the offending word and replaced it with a more elaborate locution, \"Yao, Yi, and Yu.\"\nThe Tang Dynasty Chinese also had a derogatory term for foreigners, \"lu\" () \"prisoner, slave, captive\". Beckwith says it means something like \"those miscreants who should be locked up,\" therefore, \"The word does not even mean 'foreigner' at all, let alone 'barbarian'.\"\n\nChristopher I. Beckwith's 2009 \"The Barbarians\" epilogue provides many references, but overlooks H. G. Creel's 1970 \"The Barbarians\" chapter. Creel descriptively wrote, \"Who, in fact, were the barbarians? The Chinese have no single term for them. But they were all the non-Chinese, just as for the Greeks the barbarians were all the non-Greeks.\" Beckwith prescriptively wrote, \"The Chinese, however, have still not yet borrowed Greek \"barbar\"-. There is also no single native Chinese word for 'foreigner', no matter how pejorative,\" which meets his strict definition of \"barbarian.\".\n\nIn the Tang Dynasty houses of pleasure, where drinking games were common, small puppets in the aspect of Westerners, in a ridiculous state of drunkenness, were used in one popular permutation of the drinking game; so, in the form of blue-eyed, pointy nosed, and peak-capped barbarians, these puppets were manipulated in such a way as to occasionally fall down: then, whichever guest to whom the puppet pointed after falling was then obliged by honor to empty his cup of Chinese wine.\n\nWhen Europeans came to Japan, they were called , literally \"Barbarians from the South\", because the Portuguese ships appeared to sail from the South. The Dutch, who arrived later, were also called either \"nanban\" or , literally meaning \"Red Hair.\"\n\nIn the ancient Indian epic Mahabharata, the Sanskrit word \"barbara-\" meant \"stammering, wretch, foreigner, sinful people, low and barbarous\".\n\nAccording to Romila Thapar, the Indo-Aryan semi-nomadic people viewed the indigenous people as barbarians when they arrived. Indo-Aryan used the term \"mleccha\" in referring to people \"outside the caste system and ritual ambience.\" \n\nIn Mesoamerica the Aztec civilization used the word \"Chichimeca\" to denominate a group of nomadic hunter-gatherer tribes that lived on the outskirts of the Triple Alliance's Empire, in the north of Modern Mexico, and whom the Aztec people saw as primitive and uncivilized. One of the meanings attributed to the word \"Chichimeca\" is \"dog people\".\n\nThe Incas of South America used the term \"puruma auca\" for all peoples living outside the rule of their empire (see Promaucaes).\n\nThe British, and later the white colonial settlers of the United States, referred to Native Americans as \"savages.\"\n\nThe entry of \"barbarians\" into mercenary service in a metropole repeatedly occurs in history as a standard way in which peripheral peoples from and beyond frontier regions relate to \"civilised\" imperial powers as part of a (semi-)foreign militarised proletariat.\nExamples include:\n\nItalians in the Renaissance often called anyone who lived outside of their country a barbarian. As an example, there is the last chapter of The Prince by Niccolò Machiavelli, \"Exhortatio ad Capesendam Italiam in Libertatemque a Barbaris Vinsicandam\" (in English: Exhortation to take Italy and free her from the barbarians) in which he appeals to Lorenzo de' Medici, Duke of Urbino to unite Italy and stop the \"barbarian invasions\" led by other European rulers, such as Charles VIII and Louis XII, both of France, and Ferdinand II of Aragon.\n\nSpanish sea captain Francisco de Cuellar, who sailed with the Spanish Armada in 1588, used the term 'savage' ('salvaje') to describe the Irish people.\n\nIn her 1916 anti-war pamphlet \"The Crisis of German Social Democracy\", Marxist theorist Rosa Luxemburg writes: Bourgeois society stands at the crossroads, either transition to Socialism or regression into Barbarism.Luxemburg attributed it to Friedrich Engels, though – as shown by Michael Löwy – Engels had not used the term \"Barbarism\" but a less resounding formulation: \"If the whole of modern society is not to perish, a revolution in the mode of production and distribution must take place.\" The case has been made that Luxemburg had remembered a passage from \"The Erfurt Program\", written in 1892 by Karl Kautsky, and mistakenly attributed it to Engels:As things stand today capitalist civilization cannot continue; we must either move forward into socialism or fall back into barbarism.Luxemburg went on to explain what she meant by \"Regression into Barbarism\": \"A look around us at this moment [i.e., 1916 Europe] shows what the regression of bourgeois society into Barbarism means. This World War is a regression into Barbarism. The triumph of Imperialism leads to the annihilation of civilization. At first, this happens sporadically for the duration of a modern war, but then when the period of unlimited wars begins it progresses toward its inevitable consequences. Today, we face the choice exactly as Friedrich Engels foresaw it a generation ago: either the triumph of Imperialism and the collapse of all civilization as in ancient Rome, depopulation, desolation, degeneration – a great cemetery. Or the victory of Socialism, that means the conscious active struggle of the International Proletariat against Imperialism and its method of war.\"\n\n\"Socialism or Barbarism\" became, and remains, an often quoted and influential concept in Marxist literature. \"Barbarism\" is variously interpreted as meaning either a technologically advanced but extremely exploitative and oppressive society (e.g. a victory and world domination by Nazi Germany and its Fascist allies); a collapse of technological civilization due to Capitalism causing a Nuclear War or ecological disaster; or the one form of barbarism bringing on the other.\n\nThe Internationalist Communist Tendency considers \"Socialism or Barbarism\" to be a variant of the earlier \"Liberty or Death\", used by revolutionaries of different stripes since the late 18th century \n\nModern popular culture contains such fantasy barbarians as Conan the Barbarian. In such fantasy, the negative connotations traditionally associated with \"Barbarian\" are often inverted. For example, \"The Phoenix on the Sword\" (1932), the first of Robert E. Howard's \"Conan\" series, is set soon after the \"Barbarian\" protagonist had forcibly seized the turbulent kingdom of Aquilonia from King Numedides, whom he strangled upon his throne. The story is clearly slanted to imply that the kingdom greatly benefited by power passing from a decadent and tyrannical hereditary monarch to a strong and vigorous Barbarian usurper.\n\n\n\n\n\nNotes\nBibliography\n\n"}
{"id": "42394652", "url": "https://en.wikipedia.org/wiki?curid=42394652", "title": "Beijing Students' Autonomous Federation", "text": "Beijing Students' Autonomous Federation\n\nThe Beijing Students' Autonomous Federation was a self-governing student organization, representing multiple Beijing universities, and acting as the student protesters' principle decision-making body during the Tiananmen Square protests of 1989. Student protesters founded the Federation in opposition to the official, government-supported student organizations, which they believed were undemocratic. Although the Federation made several demands of the government during the protests and organized multiple demonstrations in the Square, its primary focus was to obtain government recognition as a legitimate organization. By seeking this recognition, the Federation directly challenged the Chinese Communist Party's authority. After failing to achieve direct dialogue with the government, the Federation lost support from student protesters, and its central leadership role within the Tiananmen Square protests.\n\nAfter former General Secretary Hu Yaobang's death on April 15, students mobilized spontaneously both to mourn Hu's passing and to demand democratic reform in China. On April 19, at Peking University (Beida), a meeting was anonymously organized to discuss the ongoing protests in the Square, as well as the prospect of forming an autonomous student organization. The meeting, in essence, was a \"democracy salon\"—an unofficial student discussion group that students at Beida had founded by the former Beida physics graduate student Liu Gang months before Hu Yaobang's death. The salon decided that an autonomous organization was necessary to coordinate student protesters on multiple Beijing campuses. However, for fear of punishment by the government, few at the meeting were willing to speak out. Those who did, including history student Wang Dan, became the leaders of the newly formed Beijing Students' Autonomous Federation.\n\nAnother aim of the new Federation was the rejection of the official student organizations. According to one student announcement, \"the leadership of the original union is inept, has sold out the students' interests…and is completely unable to represent the students' wishes.\" In this way, the student protesters saw the Federation as representing the wishes of the entire student body. The Federation planned to seek legitimacy by strict observance of democratic policies such as elections and group decision-making. The students hoped these methods would ensure the organization's unity of leadership, and would effectively contrast with the lack of transparency they perceived in the Communist Party. On May 23, the Beijing Students' Autonomous Federation established officially at Liu Gang's residence near Yuanmingyuan, during the first meeting, the students had elected Zhou Yongjun as the first chairman of the Beijing Students' Autonomous Federation.\n\nOn April 26, \"People's Daily\" published the editorial \"It is Necessary to Take a Clear-Cut Stand Against Turmoil\", attributing the protests to \"a small minority\" attempting to \"poison people's minds\" and \"create national turmoil.\" On the same day, the Beijing Students' Autonomous Federation was officially established. At their meeting, around 2,000 students elected a seven-person committee to lead the Federation. Concerned by the April 26 editorial, the Federation decided that, as an illegal organization, it needed to reinforce its legitimacy by showing its popular support. To achieve this goal, the Federation organized a demonstration to take place on April 27.\n\nBetween April 16 and 26, the government had dismissed the autonomous students' demands for recognition and dialogue. The government, unwilling to accept the legitimacy of the independent student unions, had attempted instead to arrange talks with individual student leaders. Although the student protesters believed they were acting patriotically and morally, the Communist leadership saw the students' demands as a threat to political and social order. As a result, the students had difficulty obtaining their goal of direct dialogue with the government.\n\nAfter the April 27 student demonstrations, the government held its first dialogue with the Federation leaders on April 29. However, the government avoided direct contact with the Federation, instead inviting student leaders to meet on an individual basis. While some students saw the government's offer as a victory, others felt that they should only attend the dialogue as recognized representatives of the Federation. Wuer Kaixi, a Federation leader, attended the dialogue as a \"private individual\", but decided to leave midway through to protest the government's lack of recognition of the Federation. Dissatisfied with the dialogue, Wuer Kaixi later described it as a \"trick of the government to destroy the student solidarity.\" During the meeting, State Council spokesman Yuan Mu insisted that the Communist Party and the student protesters shared the same goals.\n\nOn May 1, the Federation rejected the legitimacy of the April 29 dialogue in a news conference. The following day, the Federation presented a twelve-point petition, including a demand that future government efforts at dialogue be sincere. Although the government rejected this petition, the petition represented a compromise on the students' part, as Amnesty International researcher Corinna-Barbara Francis suggests. That is, instead of demanding explicit government recognition, the students only asked for dialogue \"on the basis of full equality between the two parties.\" By May 29, the students' demands were reduced to two: denunciation of the April 26 editorial by the government and acknowledgement of the federation's democratic nature.\n\nOn May 4, the Federation successfully organized a demonstration of over 100,000 protesters in Tiananmen Square, marking the 70-year anniversary of the 1919 May 4 Movement. Also on May 4, the Federation decided to end the boycott on class attendance it had begun on April 24. This sudden and unexpected end to the boycott, and loss of momentum in student-government negotiations, resulted in a loss of student enthusiasm for the protests. As student leader Chai Ling noted \"the movement dwindled to a low point as more and more students returned to classes.\" In an attempt to reinvigorate the movement, Chai Ling and other students started a hunger strike on the Square. Although the Federation initially opposed the hunger strikers during its May 12 meeting, it eventually decided to support individual strikers, but avoided official endorsement of the Hunger Strike Group. As the protests continued throughout May and into June, the Hunger Strike Group would take control of events in the Square, and would largely displace the authority previously held by the Beijing Students' Autonomous Federation.\n\nSpeaking with Party General Secretary Zhao Ziyang on May 4, Premier Li Peng voiced his concern over the ongoing protests in the Square. While Zhao Ziyang contended that the April 26 editorial had encouraged student protesters, Li Peng contended that this was too simple an explanation for the growth of the protest movement, suggesting that the April 26 editorial \"did not accuse the vast majority of students of creating turmoil.\" Li Peng also opposed to the student protester's demands for negotiation. Voicing concern over the rising prominence of the \"illegal student organizations,\" Li objected to the Federation's desire \"to negotiate with the Party and government as equals\" and saw the students' twelve-point petition as a \"threat.\" In his conversation with Zhao, Li also asserted that the Federation's primary goal was to \"negate the leadership of the CCP and negate the entire socialist system.\"\n\nOn May 13, after the hunger striker's announced their plan, Yan Mingfu, Director of the Party's United Front Work Department, met with a number of intellectuals and student protesters, including Wang Dan, Chai Ling, and Wuer Kaixi. At the meeting, Yan Mingfu suggested that if the student protesters stopped their hunger strike and instead submitted their \"demands and suggestions through proper channels,\" he could assure them that \"the door to dialogue\" would remain open. However, when he later briefed Zhao Ziyang on the meeting, Yan Mingfu shared his unease at the divisions he observed among the student protesters: \"the AFS [the Federation], the Dialogue Delegation, and representatives of the hunger strikers…are in disagreement among themselves…I'm not sure any of them truly represents the hunger strikers or can exert any influence on them.\"\n\n\n"}
{"id": "7527647", "url": "https://en.wikipedia.org/wiki?curid=7527647", "title": "Binaural fusion", "text": "Binaural fusion\n\nBinaural fusion or binaural integration is a cognitive process that involves the \"fusion\" of different auditory information presented binaurally, or to each ear. In humans, this process is essential in understanding speech as one ear may pick up more information about the speech stimuli than the other.\n\nThe process of binaural fusion is important for computing the location of sound sources in the horizontal plane (sound localization), and it is important for sound segregation. Sound segregation refers the ability to identify acoustic components from one or more sound sources. The binaural auditory system is highly dynamic and capable of rapidly adjusting tuning properties depending on the context in which sounds are heard. Each eardrum moves one-dimensionally; the auditory brain analyzes and compares movements of both eardrums to extract physical cues and synthesize auditory objects.\n\nWhen stimulation from a sound reaches the ear, the eardrum deflects in a mechanical fashion, and the three middle ear bones (ossicles) transmit the mechanical signal to the cochlea, where hair cells transform the mechanical signal into an electrical signal. The auditory nerve, also called the cochlear nerve, then transmits action potentials to the central auditory nervous system.\n\nIn binaural fusion, inputs from both ears integrate and fuse to create a complete auditory picture at the brainstem. Therefore, the signals sent to the central auditory nervous system are representative of this complete picture, integrated information from both ears instead of a single ear.\n\nThe binaural squelch effect is a result of nuclei of the brainstem processing timing, amplitude, and spectral differences between the two ears. Sounds are integrated and then separated into auditory objects. For this effect to take place, neural integration from both sides is required.\n\nAs sound travels into the inner eardrum of vertebrate mammals, it encounters the hair cells that line the basilar membrane of the cochlea in the inner ear. The cochlea receives auditory information to be binaurally integrated. At the cochlea, this information is converted into electrical impulses that travel by means of the cochlear nerve, which spans from the cochlea to the ventral cochlear nucleus, which is located in the pons of the brainstem. The lateral lemniscus projects from the cochlear nucleus to the superior olivary complex (SOC), a set of brainstem nuclei that consists primarily of two nuclei, the medial superior olive (MSO) and the lateral superior olive (LSO), and is the major site of binaural fusion. The subdivision of the ventral cochlear nucleus that concerns binaural fusion is the anterior ventral cochlear nucleus (AVCN). The AVCN consists of spherical bushy cells and globular bushy cells and can also transmit signals to the medial nucleus of the trapezoid body (MNTB), whose neuron projects to the MSO. Transmissions from the SOC travel to the inferior colliculus (IC) via the lateral lemniscus. At the level of the IC, binaural fusion is complete. The signal ascends to the thalamocortical system, and sensory inputs to the thalamus are then relayed to the primary auditory cortex.\n\nThe ear functions to analyze and encode a sound’s dimensions. Binaural fusion is responsible for avoiding the creation of multiple sound images from a sound source and its reflections. The advantages of this phenomenon are more noticeable in small rooms, decreasing as the reflective surfaces are placed farther from the listener.\n\nThe central auditory system converges inputs from both ears (inputs contain no explicit spatial information) onto single neurons within the brainstem. This system contains many subcortical sites that have integrative functions. The auditory nuclei collect, integrate, and analyze afferent supply, the outcome is a representation of auditory space. The subcortical auditory nuclei are responsible for extraction and analysis of dimensions of sounds.\n\nThe integration of a sound stimulus is a result of analyzing frequency (pitch), intensity, and spatial localization of the sound source. Once a sound source has been identified, the cells of lower auditory pathways are specialized to analyze physical sound parameters. Summation is observed when the loudness of a sound from one stimulus is perceived as having been doubled when heard by both ears instead of only one. This process of summation is called binaural summation and is the result of different acoustics at each ear, depending on where sound is coming from.\n\nThe cochlear nerve spans from the cochlea of the inner ear to the ventral cochlear nuclei located in the pons of the brainstem, relaying auditory signals to the superior olivary complex where it is to be binaurally integrated.\n\nThe MSO contains cells that function in comparing inputs from the left and right cochlear nuclei. The tuning of neurons in the MSO favors low frequencies, whereas those in the LSO favor high frequencies.\n\nGABA receptors in the LSO and MSO are involved in balance of excitatory and inhibitory inputs. The GABA receptors are coupled to G proteins and provide a way of regulating synaptic efficacy. Specifically, GABA receptors modulate excitatory and inhibitory inputs to the LSO. Whether the GABA receptor functions as excitatory or inhibitory for the postsynaptic neuron, depends on the exact location and action of the receptor.\n\nSound localization is the ability to correctly identify the directional location of sounds. A sound stimulus localized in the horizontal plane is called azimuth; in the vertical plane it is referred to as elevation. The time, intensity, and spectral differences in the sound arriving at the two ears are used in localization. Localization of low frequency sounds is accomplished by analyzing interaural time difference (ITD). Localization of high frequency sounds is accomplished by analyzing interaural level difference (ILD).\n\nAction potentials originate in the hair cells of the cochlea and propagate to the brainstem; both the timing of these action potentials and the signal they transmit provide information to the SOC about the orientation of sound in space. The processing and propagation of action potentials is rapid, and therefore, information about the timing of the sounds that were heard, which is crucial to binaural processing, is conserved. Each eardrum moves in one dimension, and the auditory brain analyzes and compares the movements of both eardrums in order to synthesize auditory objects. This integration of information from both ears is the essence of binaural fusion. The binaural system of hearing involves sound localization in the horizontal plane, contrasting with the monaural system of hearing, which involves sound localization in the vertical plane.\n\nThe primary stage of binaural fusion, the processing of binaural signals, occurs at the SOC, where afferent fibers of the left and right auditory pathways first converge. This processing occurs because of the interaction of excitatory and inhibitory inputs in the LSO and MSO. The SOC processes and integrates binaural information, in the form of ITD and ILD, entering the brainstem from the cochleae. This initial processing of ILD and ITD is regulated by GABA receptors.\n\nThe auditory space of binaural hearing is constructed based on the analysis of differences in two different binaural cues in the horizontal plane: sound level, or ILD, and arrival time at the two ears, or ITD, which allow for the comparison of the sound heard at each eardrum. ITD is processed in the MSO and results from sounds arriving earlier at one ear than the other; this occurs when the sound does not arise from directly in front or directly behind the hearer. ILD is processed in the LSO and results from the shadowing effect that is produced at the ear that is farther from the sound source. Outputs from the SOC are targeted to the dorsal nucleus of the lateral lemniscus as well as the IC.\n\nLSO neurons are excited by inputs from one ear and inhibited by inputs from the other, and are therefore referred to as IE neurons. Excitatory inputs are received at the LSO from spherical bushy cells of the ipsilateral cochlear nucleus, which combine inputs coming from several auditory nerve fibers. Inhibitory inputs are received at the LSO from globular bushy cells of the contralateral cochlear nucleus.\n\nMSO neurons are excited bilaterally, meaning that they are excited by inputs from both ears, and they are therefore referred to as EE neurons. Fibers from the left cochlear nucleus terminate on the left of MSO neurons, and fibers from the right cochlear nucleus terminate on the right of MSO neurons. Excitatory inputs to the MSO from spherical bushy cells are mediated by glutamate, and inhibitory inputs to the MSO from globular bushy cells are mediated by glycine. MSO neurons extract ITD information from binaural inputs and resolve small differences in the time of arrival of sounds at each ear. Outputs from the MSO and LSO are sent via the lateral lemniscus to the IC, which integrates the spatial localization of sound. In the IC, acoustic cues have been processed and filtered into separate streams, forming the basis of auditory object recognition.\n\nCurrent research is being performed on the dysfunction of binaural fusion in individuals with autism. The neurological disorder autism is associated with many symptoms of impaired brain function, including the degradation of hearing, both unilateral and bilateral. Individuals with autism who experience hearing loss maintain symptoms such as difficulty listening to background noise and impairments in sound localization. Both the ability to distinguish particular speakers from background noise and the process of sound localization are key products of binaural fusion. They are particularly related to the proper function of the SOC, and there is increasing evidence that morphological abnormalities within the brainstem, namely in the SOC, of autistic individuals are a cause of the hearing difficulties. The neurons of the MSO of individuals with autism display atypical anatomical features, including atypical cell shape and orientation of the cell body as well as stellate and fusiform formations. Data also suggests that neurons of the LSO and MNTB contain distinct dysmorphology in autistic individuals, such as irregular stellate and fusiform shapes and a smaller than normal size. Moreover, a significant depletion of SOC neurons is seen in the brainstem of autistic individuals. All of these structures play a crucial role in the proper functioning of binaural fusion, so their dysmorphology may be at least partially responsible for the incidence of these auditory symptoms in autistic patients.\n\n"}
{"id": "14619041", "url": "https://en.wikipedia.org/wiki?curid=14619041", "title": "Camiola", "text": "Camiola\n\nCamiola Turinga was from Messina and was known as a virtuous woman. She was the daughter of Lawrence of Thuringia. She lived in the beginning of the fourteenth century, during the time of the reign of Frederick III of Sicily. When her husband and parents died she inherited much wealth and was known for handling this large fortune in a moral fashion.\n\nBoccaccio, as a medieval historian and early Renaissance archaeologist, describes the events that happened that lead up to her becoming a widow when she was still a young lady. He says in his \"De mulieribus claris\" in the next to the last biography that when king Frederick III of Sicily died, his eldest son Peter took over. Right after that time Godfrey of Squillace, commander for king Robert of Sicily, attacked the town of Lipari. The townspeople were overwhelmed and practically wiped out. Peter then gathered together an army of mercenaries and volunteer auxiliary troops. He put them into the command of a Count John of Chiaamonte. John's mission was to bring much needed supplies for the starving people of Lipari.\n\nWhen Godfrey learned that John's army outnumbered his own, he abandoned his camp and left all his supplies there. John then came upon these supplies and took back to the people of Lipari. Then with overconfidence John challenged Godfrey to battle. Godfrey accepted the challenge. He prepared and reinforced his troops overnight and was fully prepared in the morning. John, being overconfident, did not expect Godfrey to attack, assuming instead that he Godfrey would retreat.\n\nGodfrey's men attacked the Sicilians with much vigor. The Sicilians under John's command were stunned with the onslaught. Hand-to-hand combat took place on the Sicilian ships. The Sicilians ultimately lost confidence, since they were not fully prepared for such an attack. They then took whatever ships were still seaworthy and hightailed it. Many of the Sicilian ships were sunk and victory was within grasp for Godfrey. Even though some of John's ships were able to get away, John himself along with some princes and missionaries, were captured. The town of Lipari surrendered and sued for peace.\n\nThe prisoners were then taken to Naples and jailed. Among the prisoners was Roland of Sicily, an illegitimate son of Frederick III of Sicily. Ransom was paid for all the prisoners, and they were all released, except for one - Roland. King Peter hated his brother and all the others under John's command because of their incompetence and how they handled the attack of Godfrey.\n\nNow enters Camiola. She remembers him when they both were children. Rolan was in prison with no hope of getting out. She saw that Peter and his other brothers were willing to let him rot in jail and wished to do something about this. It turned out that there was no legal way of getting him out - except marrying him! She sent secretly to Roland a proposition on this matter. Roland jumped at the opportunity: anything to be able to get out of jail. In fact, that is all he really wanted and would say and promise anything just to get out. Camiola then legally drew up all the necessary documents and arranged for his release. She had to pay two thousand ounces of silver for his release, which was half her wealth. She handed over the agreed amount to gain his release.\n\nRoland was returned to Messina a free man. Immediately he acted like there was no arrangement of a marriage and had no appreciation for his release. Camiola was totally taken off guard because of his attitude and confronted him. She first diplomatically tried an arrangement for the marriage to take place, however Roland would have nothing to do with it. Ultimately he was brought up in front of an ecclesiastical judge. She proved to the judge that she had a \nlegal and binding signed contract for marriage in return for Roland's release out of prison. She showed to the judge that Roland agreed to this and in return was to marry her for his release out of an otherwise hopeless situation of being imprisoned for the rest of his life.\n\nUpon the judge making the decision that Rolan was wrong in not fulfilling his obligation, he admitted to his mistake however still did not want to marry her. His brothers and friends then got involved and convinced Roland that it was the right thing to do. Roland then decided he would marry her and\ncame to her with this proposal with his friends. She however at this point gave a lengthy speech. From that time forward, she could not be swayed by either the plebs or reprimands from her high moral standards which she had already shown. Roland, however, was scorned by everyone from then on, including his immediate family.\n\n\nThe English playwright Philip Massinger based one of his best characters of \"The Maid of Honour\" on Boccaccio's heroine Camiola.\n"}
{"id": "13529329", "url": "https://en.wikipedia.org/wiki?curid=13529329", "title": "Categories (Peirce)", "text": "Categories (Peirce)\n\nOn May 14, 1867, the 27-year-old Charles Sanders Peirce, who eventually founded pragmatism, presented a paper entitled \"\" to the American Academy of Arts and Sciences. Among other things, this paper outlined a theory of predication involving three universal categories that Peirce continued to apply in philosophy and elsewhere for the rest of his life. In the categories one will discern, concentrated, the pattern which one finds formed by the three grades of clearness in \"\" (1878 foundational paper for pragmatism), and in numerous other three-way distinctions in his work.\n\nIn Aristotle's logic, categories are adjuncts to reasoning that are designed to resolve equivocations, ambiguities that make expressions or signs recalcitrant to being ruled by logic. Categories help the reasoner to render signs ready for the application of logical laws. An equivocation is a variation in meaning — a manifold of sign senses — such that, as Aristotle put it about names in the opening of \"\" (1.11–12), \"Things are said to be named ‘equivocally’ when, though they have a common name, the definition corresponding with the name differs for each\". So Peirce's claim that three categories are sufficient amounts to an assertion that all manifolds of meaning can be unified in just three steps.\n\nThe following passage is critical to the understanding of Peirce's Categories:\nI will now say a few words about what you have called Categories, but for which I prefer the designation Predicaments, and which you have explained as predicates of predicates.\n\nThat wonderful operation of hypostatic abstraction by which we seem to create \"entia rationis\" that are, nevertheless, sometimes real, furnishes us the means of turning predicates from being signs that we think or think \"through\", into being subjects thought of. We thus think of the thought-sign itself, making it the object of another thought-sign.\n\nThereupon, we can repeat the operation of hypostatic abstraction, and from these second intentions derive third intentions. Does this series proceed endlessly? I think not. What then are the characters of its different members?\n\nMy thoughts on this subject are not yet harvested. I will only say that the subject concerns Logic, but that the divisions so obtained must not be confounded with the different Modes of Being: Actuality, Possibility, Destiny (or Freedom from Destiny).\n\nOn the contrary, the succession of Predicates of Predicates is different in the different Modes of Being. Meantime, it will be proper that in our system of diagrammatization we should provide for the division, whenever needed, of each of our three Universes of modes of reality into Realms for the different Predicaments. (Peirce 1906).\nThe first thing to extract from this passage is the fact that Peirce's Categories, or \"Predicaments\", are predicates of predicates. Meaningful predicates have both \"extension\" and \"intension\", so predicates of predicates get their meanings from at least two sources of information, namely, the classes of relations and the qualities of qualities to which they refer. Considerations like these tend to generate hierarchies of subject matters, extending through what is traditionally called the \"logic of second intentions\", or what is handled very roughly by \"second order logic\" in contemporary parlance, and continuing onward through higher intensions, or \"higher order logic\" and \"type theory\".\n\nPeirce arrived at his own system of three categories after a thoroughgoing study of his predecessors, with special reference to the categories of Aristotle, Kant, and Hegel. The names that he used for his own categories varied with context and occasion, but ranged from reasonably intuitive terms like \"quality\", \"reaction\", and \"representation\" to maximally abstract terms like \"firstness\", \"secondness\", and \"thirdness\", respectively. Taken in full generality, \"nth\"-ness can be understood as referring to those properties that all \"n\"-adic relations have in common. Peirce's distinctive claim is that a type hierarchy of three levels is generative of all that we need in logic.\n\nPart of the justification for Peirce's claim that three categories are both necessary and sufficient appears to arise from mathematical ideas about the reducibility of \"n\"-adic relations. According to Peirce's Reduction Thesis, (a) triads are necessary because genuinely triadic relations cannot be completely analyzed in terms of monadic and dyadic predicates, and (b) triads are sufficient because there are no genuinely tetradic or larger polyadic relations—all higher-arity \"n\"-adic relations can be analyzed in terms of triadic and lower-arity relations. Others, notably Robert Burch (1991) and Joachim Hereth Correia and Reinhard Pöschel (2006), have offered proofs of the Reduction Thesis.\n\nThere have been proposals by Donald Mertz, Herbert Schneider, Carl Hausman, and Carl Vaught to augment Peirce's threefolds to fourfolds; and one by Douglas Greenlee to reduce them to twofolds.\n\nPeirce introduces his Categories and their theory in \"On a New List of Categories\" (1867), a work which is cast as a Kantian deduction and is short but dense and difficult to summarize. The following table is compiled from that and later works.\n\n\n\n"}
{"id": "5961", "url": "https://en.wikipedia.org/wiki?curid=5961", "title": "Cognitive psychology", "text": "Cognitive psychology\n\nCognitive psychology is the study of mental processes such as \"attention, language use, memory, perception, problem solving, creativity, and thinking\". Much of the work derived from cognitive psychology has been integrated into various other modern disciplines such as Cognitive Science and of psychological study, including educational psychology, social psychology, personality psychology, abnormal psychology, developmental psychology, and economics.\n\nPhilosophically, ruminations of the human mind and its processes have been around since the times of the ancient Greeks. In 387 BCE, Plato is known to have suggested that the brain was the seat of the mental processes. In 1637, René Descartes posited that humans are born with innate ideas, and forwarded the idea of mind-body dualism, which would come to be known as substance dualism (essentially the idea that the mind and the body are two separate substances). From that time, major debates ensued through the 19th century regarding whether human thought was solely experiential (empiricism), or included innate knowledge (nativism). Some of those involved in this debate included George Berkeley and John Locke on the side of empiricism, and Immanuel Kant on the side of nativism.\n\nWith the philosophical debate continuing, the mid to late 19th century was a critical time in the development of psychology as a scientific discipline. Two discoveries that would later play substantial roles in cognitive psychology were Paul Broca's discovery of the area of the brain largely responsible for language production, and Carl Wernicke's discovery of an area thought to be mostly responsible for comprehension of language. Both areas were subsequently formally named for their founders and disruptions of an individual's language production or comprehension due to trauma or malformation in these areas have come to commonly be known as Broca's aphasia and Wernicke's aphasia.\n\nFrom the 1920s to the 1950s, the main approach to psychology was behaviorism. Initially, its adherents viewed mental events such as thoughts, ideas, attention, and consciousness as unobservables, hence outside the realm of a science of psychology. One pioneer of cognitive psychology, who worked outside the boundaries (both intellectual and geographical) of behaviorism was Jean Piaget. From 1926 to the 1950s and into the 1980s, he studied the thoughts, language, and intelligence of children and adults.\n\nIn the mid-20th century, three main influences arose that would inspire and shape cognitive psychology as a formal school of thought:\n\nUlric Neisser put the term \"cognitive psychology\" into common use through his book \"Cognitive Psychology\", published in 1967. Neisser's definition of \"cognition\" illustrates the then-progressive concept of cognitive processes:\n\nThe term \"cognition\" refers to all processes by which the sensory input is transformed, reduced, elaborated, stored, recovered, and used. It is concerned with these processes even when they operate in the absence of relevant stimulation, as in images and hallucinations. ... Given such a sweeping definition, it is apparent that cognition is involved in everything a human being might possibly do; that every psychological phenomenon is a cognitive phenomenon. But although cognitive psychology is concerned with all human activity rather than some fraction of it, the concern is from a particular point of view. Other viewpoints are equally legitimate and necessary. Dynamic psychology, which begins with motives rather than with sensory input, is a case in point. Instead of asking how a man's actions and experiences result from what he saw, remembered, or believed, the dynamic psychologist asks how they follow from the subject's goals, needs, or instincts.\n\nThe main focus of cognitive psychologists is on the mental processes that affect behavior. Those processes include, but are not limited to, the following:\n\nThe psychological definition of attention is \"a state of focused awareness on a subset of the available perceptual information\". A key function of attention is to identify irrelevant data and filter it out, enabling significant data to be distributed to the other mental processes. For example, the human brain may simultaneously receive auditory, visual, olfactory, taste, and tactile information. The brain is able to handle only a small subset of this information, and this is accomplished through the attentional processes.\n\nAttention can be divided into two major attentional systems: exogenous control and endogenous control Exogenous control works from bottom-up and is responsible for orienting reflex, and pop-out effects. Endogenous control works top-down and is the more deliberate attentional system, responsible for divided attention and conscious processing.\n\nOne major focal point relating to attention within the field of cognitive psychology is the concept of divided attention. A number of early studies dealt with the ability of a person wearing headphones to discern meaningful conversation when presented with different messages into each ear; this is known as the dichotic listening task. Key findings involved an increased understanding of the mind's ability to both focus on one message, while still being somewhat aware of information being taken in from the ear not being consciously attended to. E.g., participants (wearing earphones) may be told that they will be hearing separate messages in each ear and that they are expected to attend only to information related to basketball. When the experiment starts, the message about basketball will be presented to the left ear and non-relevant information will be presented to the right ear. At some point the message related to basketball will switch to the right ear and the non-relevant information to the left ear. When this happens, the listener is usually able to repeat the entire message at the end, having attended to the left or right ear only when it was appropriate. The ability to attend to one conversation in the face of many is known as the cocktail party effect.\n\nOther major findings include that participants can't comprehend both passages, when shadowing one passage, they can't report content of the unattended message, they can shadow a message better if the pitches in each ear are different. However, while deep processing doesn't occur, early sensory processing does. Subjects did notice if the pitch of the unattended message changed or if it ceased altogether, and some even oriented to the unattended message if their name was mentioned.\n\nThe two main types of memory are short-term memory and long-term memory; however, short-term memory has become better understood to be working memory. Cognitive psychologists often study memory in terms of working memory.\n\nThough working memory is often thought of as just short-term memory, it is more clearly defined as the ability to remember information in the face of distraction. The famously known capacity of memory of 7 plus or minus 2 is a combination of both memory in working memory and long term memory.\n\nOne of the classic experiments is by Ebbinghaus, who found the serial position effect where information from the beginning and end of list of random words were better recalled than those in the center. This primacy and recency effect varies in intensity based on list length. Its typical U-shaped curve can be disrupted by an attention-grabbing word; this is known as the Von Restorff effect.\n\nMany models of working memory have been made. One of the most regarded is the Baddeley and Hitch model of working memory. It takes into account both visual and auditory stimuli, long-term memory to use as a reference, and a central processor to combine and understand it all.\n\nA large part of memory is forgetting, and there is a large debate among psychologists of decay theory versus interference theory.\n\nModern conceptions of memory are usually about long-term memory and break it down into three main sub-classes. These three classes are somewhat hierarchical in nature, in terms of the level of conscious thought related to their use.\n\nPerception involves both the physical senses (sight, smell, hearing, taste, touch, and proprioception) as well as the cognitive processes involved in interpreting those senses. Essentially, it is how people come to understand the world around them through interpretation of stimuli. Early psychologists like [[Edward Titchener|Edward B. Titchener]] began to work with perception in their [[Structuralism (psychology)|structuralist]] approach to psychology. [[Structuralism (psychology)|Structuralism]] dealt heavily with trying to reduce human thought (or \"consciousness,\" as Titchener would have called it) into its most basic elements by gaining understanding of how an individual perceives particular stimuli.\n\nCurrent perspectives on [[perception]] within cognitive psychology tend to focus on particular ways in which the human mind interprets stimuli from the senses and how these interpretations affect behavior. An example of the way in which modern psychologists approach the study of [[perception]] is the research being done at the Center for Ecological Study of Perception and Action at the University of Connecticut (CESPA). One study at CESPA concerns ways in which individuals perceive their physical environment and how that influences their navigation through that environment.\n\nPsychologists have had an interest in the cognitive processes involved with [[language]] that dates back to the 1870s, when [[Carl Wernicke]] proposed a model for the mental processing of language. Current work on [[language]] within the field of cognitive psychology varies widely. Cognitive psychologists may study [[language acquisition]], individual components of [[language]] formation (like [[phoneme]]s), how language use is involved in [[Mood (psychology)|mood]], or numerous other related areas.\n[[File:BrocasAreaSmall.png|thumbnail|Broca's and Wernicke's areas of the brain, which are critical in language]]\n\nSignificant work has been done recently with regard to understanding the timing of [[language acquisition]] and how it can be used to determine if a child has, or is at risk of, developing a [[learning disability]]. A study from 2012, showed that while this can be an effective strategy, it is important that those making evaluations include all relevant information when making their assessments. Factors such as individual variability, [[socioeconomic status]], [[Short-term memory|short-term]] and [[long-term memory]] capacity, and others must be included in order to make valid assessments.\n\n[[Metacognition]], in a broad sense, is the thoughts that a person has about their own thoughts. More specifically, metacognition includes things like:\nMuch of the current study regarding metacognition within the field of cognitive psychology deals with its application within the area of education. Being able to increase a student's metacognitive abilities has been shown to have a significant impact on their learning and study habits. One key aspect of this concept is the improvement of students' ability to set goals and self-regulate effectively to meet those goals. As a part of this process, it is also important to ensure that students are realistically evaluating their personal degree of knowledge and setting realistic goals (another metacognitive task).\n\nCommon phenomena related to metacognition include:\n\nModern perspectives on cognitive psychology generally address cognition as a [[dual process theory]], expounded upon by [[Daniel Kahneman]] in 2011. Kahneman differentiated the two styles of processing more, calling them intuition and reasoning. Intuition (or system 1), similar to associative reasoning, was determined to be fast and automatic, usually with strong emotional bonds included in the reasoning process. Kahneman said that this kind of reasoning was based on formed habits and very difficult to change or manipulate. Reasoning (or system 2) was slower and much more volatile, being subject to conscious judgments and attitudes.\n\nFollowing the cognitive revolution, and as a result of many of the principle discoveries to come out of the field of cognitive psychology, the discipline of cognitive therapy evolved. [[Aaron T. Beck]] is generally regarded as the father of cognitive therapy. His work in the areas of recognition and treatment of depression has gained worldwide recognition. In his 1987 book titled \"Cognitive Therapy of Depression\", Beck puts forth three salient points with regard to his reasoning for the treatment of depression by means of therapy or therapy and antidepressants versus using a pharmacological-only approach:\n1. Despite the prevalent use of antidepressants, the fact remains that not all patients respond to them. Beck cites (in 1987) that only 60 to 65% of patients respond to antidepressants, and recent [[Meta-analysis|meta-analyses]] (a statistical breakdown of multiple studies) show very similar numbers.<br>2. Many of those who do respond to antidepressants end up not taking their medications, for various reasons. They may develop side-effects or have some form of personal objection to taking the drugs.<br>3. Beck posits that the use of [[Psychotropic|psychotropic drugs]] may lead to an eventual breakdown in the individual's [[Coping (psychology)|coping mechanisms]]. His theory is that the person essentially becomes reliant on the medication as a means of improving mood and fails to practice those coping techniques typically practiced by healthy individuals to alleviate the effects of depressive symptoms. By failing to do so, once the patient is weaned off of the antidepressants, they often are unable to cope with normal levels of depressed mood and feel driven to reinstate use of the antidepressants.\n\nMany facets of modern social psychology have roots in research done within the field of cognitive psychology. [[Social cognition]] is a specific sub-set of social psychology that concentrates on processes that have been of particular focus within cognitive psychology, specifically applied to human interactions. [[Gordon Moskowitz|Gordon B. Moskowitz]] defines social cognition as \"... the study of the mental processes involved in perceiving, attending to, remembering, thinking about, and making sense of the people in our social world\".\n\nThe development of multiple [[social information processing (theory)|social information processing]] (SIP) models has been influential in studies involving aggressive and anti-social behavior. Kenneth Dodge's SIP model is one of, if not the most, empirically supported models relating to aggression. Among his research, Dodge posits that children who possess a greater ability to process social information more often display higher levels of socially acceptable behavior. His model asserts that there are five steps that an individual proceeds through when evaluating interactions with other individuals and that how the person interprets cues is key to their reactionary process.\n\nMany of the prominent names in the field of developmental psychology base their understanding of development on cognitive models. One of the major paradigms of developmental psychology, the [[Theory of Mind]] (ToM), deals specifically with the ability of an individual to effectively understand and attribute cognition to those around them. This concept typically becomes fully apparent in children between the ages of 4 and 6. Essentially, before the child develops ToM, they are unable to understand that those around them can have different thoughts, ideas, or feelings than themselves. The development of ToM is a matter of [[metacognition]], or thinking about one's thoughts. The child must be able to recognize that they have their own thoughts and in turn, that others possess thoughts of their own.<br>\n\nOne of the foremost minds with regard to developmental psychology, Jean Piaget, focused much of his attention on cognitive development from birth through adulthood. Though there have been considerable challenges to parts of his [[Piaget's theory of cognitive development|stages of cognitive development]], they remain a staple in the realm of education. Piaget's concepts and ideas predated the cognitive revolution but inspired a wealth of research in the field of cognitive psychology and many of his principles have been blended with modern theory to synthesize the predominant views of today.\n\nModern theories of education have applied many concepts that are focal points of cognitive psychology. Some of the most prominent concepts include:\n\nCognitive therapeutic approaches have received considerable attention in the treatment of personality disorders in recent years. The approach focuses on the formation of what it believes to be faulty schemata, centralized on judgmental biases and general cognitive errors.\n\nThe line between cognitive psychology and [[cognitive science]] can be blurry. The differentiation between the two is best understood in terms of cognitive psychology's relationship to [[applied psychology]], and the understanding of psychological phenomena. Cognitive psychologists are often heavily involved in running psychological experiments involving human participants, with the goal of gathering information related to how the human mind takes in, processes, and acts upon inputs received from the outside world. The information gained in this area is then often used in the applied field of clinical psychology.\n\nCognitive science is better understood as predominantly concerned with gathering data through research. Cognitive science envelopes a much broader scope, which has links to philosophy, linguistics, anthropology, neuroscience, and particularly with artificial intelligence. It could be said that cognitive science provides the database of information that fuels the theory from which cognitive psychologists operate. Cognitive scientists' research sometimes involves non-human subjects, allowing them to delve into areas which would come under ethical scrutiny if performed on human participants. I.e., they may do research implanting devices in the brains of rats to track the firing of neurons while the rat performs a particular task. Cognitive science is highly involved in the area of artificial intelligence and its application to the understanding of mental processes.\n\nIn the early years of cognitive psychology, [[Behaviorism|behaviorist]] critics held that the empiricism it pursued was incompatible with the concept of internal mental states. [[Cognitive neuroscience]], however, continues to gather evidence of direct correlations between physiological brain activity and putative mental states, endorsing the basis for cognitive psychology.\n\nSome observers have suggested that as cognitive psychology became a movement during the 1970s, the intricacies of the phenomena and processes it examined meant it also began to lose cohesion as a field of study. In \"Psychology: Pythagoras to Present\", for example, John Malone writes: \"Examinations of late twentieth-century textbooks dealing with \"cognitive psychology\", \"human cognition\", \"cognitive science\" and the like quickly reveal that there are many, many varieties of cognitive psychology and very little agreement about exactly what may be its domain.\"  This misfortune produced competing models that questioned information-processing approaches to cognitive functioning such as [[Naturalistic decision-making|Decision Making]] and [[Center for Advanced Study in the Behavioral Sciences|Behavioral Science]].\n\n\n\n[[Category:Cognition]]\n[[Category:Behavioural sciences]]\n[[Category:Cognitive psychology| ]]"}
{"id": "1905931", "url": "https://en.wikipedia.org/wiki?curid=1905931", "title": "Common-mode rejection ratio", "text": "Common-mode rejection ratio\n\nIn electronics, The common mode rejection ratio (CMRR) of a differential amplifier (or other device) is a metric used to quantify the ability of the device to reject common-mode signals, i.e., those that appear simultaneously and in-phase on both inputs. An ideal differential amplifier would have infinite CMRR, however this is not achievable in practice. A high CMRR is required when a differential signal must be amplified in the presence of a possibly large common-mode input, such as strong electromagnetic interference (EMI). An example is audio transmission over balanced line in sound reinforcement or recording.\n\nIdeally, a differential amplifier takes the voltages, formula_1 and formula_2 on its two inputs and produces an output voltage formula_3, where formula_4 is the differential gain. However, the output of a real differential amplifier is better described as\nwhere formula_6 is the common-\"mode gain\", which is typically much smaller than the differential gain.\n\nThe CMRR is defined as the ratio of the powers of the differential gain over the common-mode gain, measured in positive decibels (thus using the 20 log rule):\n\nAs differential gain should exceed common-mode gain, this will be a positive number, and the higher the better.\n\nThe CMRR is a very important specification, as it indicates how much of the common-mode signal will appear in your measurement. The value of the CMRR often depends on signal frequency as well, and must be specified as a function thereof.\n\nIt is often important in reducing noise on transmission lines. For example, when measuring the resistance of a thermocouple in a noisy environment, the noise from the environment appears as an offset on both input leads, making it a common-mode voltage signal. The CMRR of the measurement instrument determines the attenuation applied to the offset or noise.\n\n\n"}
{"id": "12207392", "url": "https://en.wikipedia.org/wiki?curid=12207392", "title": "Compact complement topology", "text": "Compact complement topology\n\nIn mathematics, the compact complement topology is a topology defined on the set formula_1 of real numbers, defined by declaring a subset formula_2 open if and only if it is either empty or its complement formula_3 is compact in the standard Euclidean topology on formula_1.\n"}
{"id": "44294098", "url": "https://en.wikipedia.org/wiki?curid=44294098", "title": "Comparison diagram", "text": "Comparison diagram\n\nComparison diagram or comparative diagram is a general type of diagram, in which a comparison is made between two or more objects, phenomena or groups of data. A comparison diagram or can offer qualitative and/or quantitative information. This type of diagram can also be called comparison chart or comparison chart. The diagram itself is sometimes referred to as a cluster diagram.\n\nA comparison diagram is a general type of diagram, meaning a class of specific diagrams and charts, in which a comparison is made between two or more objects, phenomena or groups of data. They are a tool for visual comparison.\n\nWhen it comes to comparing data, five basic types of comparison can be determined.\n\nComparison diagrams can be used in research projects, to give an overview of existing possibilities and to validate models. It can be used in decision making in presenting alternatives for further selection. And it can be used in education to show the variety in a specific population.\n\nComparison charts originate from the late 18th century and early 19th century. One of its roots are the 18th century nautical chart, which could offer a comparison of shore or coastal profiles. These were made popular by the English cartographer and a publisher of maps William Faden (1749–1836).\n\nAnother root of comparison diagrams are the earliest thematic maps. Late 18th century August Friedrich Wilhelm Crome published one of the first economic thematic map. His \"Groessen Karte von Europa\" from 1785 compared the sizes of all then existing European countries. This work inspired later scientist, such as Alexander von Humboldt in Germany, and Charles Dupin in France. in their works. In France in 1872 Charles Louis de Fourcroy presented a similar diagram, which he named \"Table Poléométrique\" (Poleometric Table).\n\nEarly 19th century, Alexander von Humboldt was one of the first to picture various cross sections of mountains, including for example the \"limit of perpetual snows at different latitudes,\" or the different kinds of vegetation on different heights. Much of this work was published in the 1914 \"Atlas géographique et physique des régions équinoxiales du nouveau continent.\" In 1805 Von Humboldt had published a map (see image), entitled \"Ideen zu einer Geographie der Pflanzen nebst einem Naturgemälde der Tropenländer\" (Ideas for a geography of plants, together with a nature paintings of the tropics) in which he made a comparison of the different types of plans in the tropics, and the heights on which they grew.\n\nIn the 1810s the first formal comparative mountains charts emerged. Early examples are:\n\nAnother popular subject became the comparative views of the lengths of the principal rivers in one country or from all over the world. In 1822 William Home Lizars presented a map, entitled \"Comparative View of the Lengths of the Principal Rivers of Scotland.\" And in 1826 \nAnthony Finley (1790–1840) published a \"Comparative Map of the Principle Rivers of the World.\" \n\nOther types of comparison charts would soon appear. For example, the \"Comprehensive atlas: geographical, historical & commercial,\" published by William D. Ticknor in 1835 contained a series of different comparison diagrams and charts on:\nOther popular themes in 19th century comparison diagrams were the sizes of lakes, the lengths of waterfalls, and the sizes of Islands.\n\nOne of the first maps to compare the lengths of rivers was the \"Map of the Principal Rivers Throughout the World. Comparative Lengths of the Principal Rivers throughout the World.\" by the political economist Henry Charles Carey and Isaac Lea published their 1822 \"A Complete Historical, Chronological, and Geographical American Atlas.\" In this maps the lengths of the rivers, were presented in a bar chart with horizontal bars. In 1826 the same data was presented in a bar chart with vertical bars, entitled \"Table of the comparative lengths of the principal Rivers throughout the World\" (see image). \n\nIn his \"Graphic methods for presenting facts.\" Willard C. Brinton was one of the first to theorize about the existence and role or comparison in statistical graphics. He stipulated that \"the graphic method lends itself admirably to use in making comparisons. It is surprising how much clearer even simple comparisons of only two or three items will appear when their numerical value is put in graphic form rather than in figures.\" Brinton showed over a dozen different types of diagrams, which could make simple comparison, and devoted another chapter on comparisons over time.\n\nThere are different types of comparison diagrams called comparison diagram/chart in theory and practice, such as\n\n"}
{"id": "290749", "url": "https://en.wikipedia.org/wiki?curid=290749", "title": "Conch", "text": "Conch\n\nConch ()\nis a common name that is applied to a number of different medium to large-sized shells. The term generally applies to large snails whose shell has a high spire and a noticeable siphonal canal (in other words, the shell comes to a noticeable point at both ends).\n\nIn North America, a conch is often identified as a queen conch, found off the coast of Florida. Queen conchs are valued for fish bait, and are also known as seafood.\n\nThe group of conchs that are sometimes referred to as \"true conchs\" are marine gastropod molluscs in the family Strombidae, specifically in the genus Strombus and other closely related genera. For example, see \"Lobatus gigas\", the queen conch, and \"Laevistrombus canarium\", the dog conch.\n\nMany other species are also often called \"conch\", but are not at all closely related to the family Strombidae, including \"Melongena\" species (family Melongenidae), and the horse conch \"Triplofusus papillosus\" (family Fasciolariidae). Species commonly referred to as conchs also include the sacred chank or more correctly shankha shell (\"Turbinella pyrum\") and other \"Turbinella\" species in the family Turbinellidae. \n\nThe English word \"conch\" is attested in Middle English, coming from Latin \"concha\" (shellfish, mussel), which in turn comes from Greek \"konchē\" (same meaning) ultimately from PIE root \"*konkho-\", cognate with Sanskrit \"śaṅkha.\"\n\nThe meat of conchs is eaten raw in salads, or cooked, as in burgers, chowders, fritters, and gumbos. All parts of the conch meat are edible. \n\nConch is most indigenous to the Bahamas, and is typically served in fritter, salad, and soup forms. In addition to the Bahamas, conch is also eaten in the West Indies (Jamaica in particular); locals in Jamaica eat conch in soups, stews and curries. Restaurants all over the islands serve this particular meat. In the Dominican Republic, Grenada, and Haiti, conch is commonly eaten in curries or in a spicy soup. It is locally referred to as \"lambi\". In the Turks and Caicos Islands, the Annual Conch Festival is held in November each year, located at the Three Queens Bar/Restaurant in Blue Hills. Local restaurateurs compete for the best and most original conch dishes, that are then judged by international chefs. Free sampling of the dishes follows the judging; along with those festivities, other competitions, events, and music performances occur well into the evening. In Puerto Rico, conch is served as a ceviche, often called \"ensalada de carrucho\" (conch salad), consisting of raw conch marinated in lime juice, olive oil, vinegar, garlic, green peppers, and onions. It is also used to fill empanadas.\n\nIn Panama, conch is known as \"cambombia\" and is often served as a ceviche known as \"ceviche de cambombia\" consisting of raw conch marinated in lime juice, chopped onions, finely chopped habaneros, and often vinegar.\n\nConch is very popular in Italy and among Italian Americans. Called \"scungille\" (pl. \"scungilli\") is eaten in a variety of ways, but most often in salads or cooked in a sauce for pasta. It is often included as one of the dishes prepared for the Feast of the Seven Fishes. \n\nIn East Asian cuisines, this seafood is often cut into thin slices and then steamed or stir-fried.\n\nConch shells can be used as wind instruments. They are prepared by cutting a hole in the spire of the shell near the apex, and then blowing into the shell as if it were a trumpet, as in blowing horn. Sometimes a mouthpiece is used, but some shell trumpets are blown without one. Pitch is adjusted by moving one's hand in and out of the aperture; the deeper the hand, the lower the note.\n\nVarious species of large marine gastropod shells can be turned into \"blowing shells\", but some of the best-known species used are the sacred chank or shankha \"Turbinella pyrum\", the Triton's trumpet \"Charonia tritonis\", and the queen conch \"Strombus gigas\".\n\nMany different kinds of mollusks can produce pearls. Pearls from the queen conch, \"L. gigas\", are rare and have been collectors' items since Victorian times. Conch pearls occur in a range of hues, including white, brown and orange, with many intermediate shades, but pink is the colour most associated with the conch pearl, such that these pearls are sometimes referred to simply as \"pink pearls\". In some gemological texts, non-nacreous gastropod pearls used to be referred to as \"calcareous concretions\" because they were \"porcellaneous\" (shiny and ceramic-like in appearance), rather than \"nacreous\" (with a pearly lustre), sometimes known as \"orient\". The GIA and CIBJO now simply use the term \"pearl\"—or, where appropriate, the more descriptive term \"non-nacreous pearl\"—when referring to such items, and under Federal Trade Commission rules, various mollusc pearls may be referred to as \"pearls\" without qualification.\n\nAlthough non-nacreous, the surface of fine conch pearls has a unique and attractive appearance of its own. The microstructure of conch pearls comprises partly aligned bundles of microcrystalline fibres that create a shimmering, slightly iridescent effect known as \"flame structure\". The effect is a form of chatoyancy, caused by the interaction of light rays with the microcrystals in the pearl's surface, and it somewhat resembles moiré silk.\n\n\nThe Moche people of ancient Peru worshipped the sea and often depicted conch shells in their art.\n\nQuetzalcoatl, the Mexican god of wind and learning, wears around his neck the \"wind breastplate\" ehecailacocozcatl, \"the spirally voluted wind jewel\" made of a conch shell.\n\nBuddhism has also incorporated the conch shell, \"Turbinella pyrum\", as one of the Eight Auspicious Symbols.\n\nA \"shankha\" shell (the shell of a \"Turbinella pyrum\", a species in the gastropod family \"Turbinellidae\") is often referred to in the West as a conch shell, or a chank shell. This shell is used as an important ritual object in Hinduism. The shell is used as a ceremonial trumpet, as part of religious practices, for example \"puja\". The chank trumpet is sounded during worship at specific points, accompanied by ceremonial bells and singing. As it is an auspicious instrument, it is often played in a \"Lakshmi puja\" in temple or at home.\n\nIn the story of Dhruva, the divine conch plays a special part. The warriors of ancient India blew conch shells to announce battle, as is described in the beginning of the war of Kurukshetra, in the Mahabharata, the famous Hindu epic.\n\nThe god of preservation, Vishnu, is said to hold a special conch, Panchajanya, that represents life, as it has come out of life-giving waters.There is an interesting story behind the origin of a \"the shankha\" or conch shell.According to Hindu mythology, Devas (Gods) and Asuras (Demons) once decided to churn the ocean in order to get a special divine nectar. This divine nectar also known as \"Amrit\" was known to give immortality to whoever drank it. All the Gods were on one side of it and the Demons were on the other end. The Samudra Manthan produced a number of things from the Ocean. One of the first things to come out of it was lethal poison called Halahala.Everyone was terrified as the poison was potent enough to destroy entire creation. So they went to Lord Shiva for protection and he consumed the poison to safeguard the universe. Lord Shiva took the poison in his mouth but did not swallow it. Later some additional objects came out of the ocean like Lakshmi (goddess of prosperity and beauty), Goddess of wine, Moon, divine Nymphs like Rambha- and Menakha, Uchhaishravas the divine seven headed White horse, Kaustubha a jewel, Parijata the celestial tree, Surabhi the cow of plenty, Airavata a white elephant, Dhanus a mighty bow and many more such things were produced. \"Shankha\" or conch shell also was one of divine objects that was obtained from Samudra manthan.\n\nAlso, the sound of the conch is believed to drive away the evil spirits.\nThe blowing of the conch or \"the \"shankha\"\" needs a tremendous power and respiratory capacity. Hence, blowing it daily helps keep the lungs healthy.\n\nA newly wed Bengali bride wears bangles called Shakha Paula, made from coral and conch shell powder. They have been a part of Bengali custom and tradition. It is believed that in ancient era, the Bengali farming community resided near the river. They collected conch shells and powdered to create bangles. They also used red coral for the bangles. They gifted these beautiful bangles to their wives as they could not afford ivory bangles.They were also known as poor man's ivory as they were cheap substitute for ivory bangles.\n\n\n\n"}
{"id": "40309259", "url": "https://en.wikipedia.org/wiki?curid=40309259", "title": "Cybernetic art", "text": "Cybernetic art\n\nCybernetic art is contemporary art that builds upon the legacy of Cybernetics, where feedback involved in the work take precedence over traditional aesthetic and material concerns.\n\nNicolas Schöffer's \"CYSP I\" (1956) was perhaps the first artwork to explicitly employ cybernetic principles (CYSP is an acronym that joins the first two letters of the words \"CYbernetic\" and \"SPatiodynamic\"). The artist Roy Ascott elaborated an extensive theory of cybernetic art in \"Behaviourist Art and the Cybernetic Vision\" (Cybernetica, Journal of the International Association for Cybernetics (Namur), Volume IX, No.4, 1966; Volume X No.1, 1967) and in \"The Cybernetic Stance: My Process and Purpose\" (Leonardo Vol 1, No 2, 1968). Art historian Edward A. Shanken has written about the history of art and cybernetics in essays including \"Cybernetics and Art: Cultural Convergence in the 1960s\" and \"From Cybernetics to Telematics: The Art, Pedagogy, and Theory of Roy Ascott\"(2003), which traces the trajectory of Ascott's work from cybernetic art to telematic art (art using computer networking as its medium, a precursor to net.art.)\n\nAudio feedback and the use of tape loops, sound synthesis and computer generated compositions reflected a cybernetic awareness of information, systems and cycles. Such techniques became widespread in the 1960s in the music industry. The visual effects of electronic feedback became a focus of artistic research in the late 1960s, when video equipment first reached the consumer market. Steina and Woody Vasulka, for example, used \"all manner and combination of audio and video signals to generate electronic feedback in their respective of corresponding media.\"\n\nWith related work by Edward Ihnatowicz, Wen-Ying Tsai and cybernetician Gordon Pask and the animist kinetics of Robert Breer and Jean Tinguely, the 1960s produced a strain of cyborg art that was very much concerned with the shared circuits within and between the living and the technological. A line of cyborg art theory also emerged during the late 1960s. Writers like Jonathan Benthall and Gene Youngblood drew on cybernetics and cybernetic. The most substantial contributors here were the British artist and theorist Roy Ascott with his essay \"Behaviourist Art and the Cybernetic Vision\" in the journal Cybernetica (1976), and the American critic and theorist Jack Burnham. In \"Beyond Modern Sculpture\" from 1968 he builds cybernetic art into an extensive theory that centers on art's drive to imitate and ultimately reproduce life.\n\nLeading art theorists and historians in this field include Christiane Paul, Frank Popper, Christine Buci-Glucksmann, Dominique Moulon, Robert C. Morgan, Roy Ascott, Margot Lovejoy, Edmond Couchot, Fred Forest and Edward A. Shanken.\n\n\n"}
{"id": "41963234", "url": "https://en.wikipedia.org/wiki?curid=41963234", "title": "Diplomatic gift", "text": "Diplomatic gift\n\nA diplomatic gift is a gift given by a diplomat, politician or leader when visiting a foreign country. Usually the gift is reciprocated by the host. The use of diplomatic gifts dates back to the ancient world and givers have competed to outdo each other in the lavishness of their gifts. Examples include silks given to the West by the Byzantines in the early Middle Ages, the luxury book, and panda diplomacy by the Chinese in the twentieth century.\n\nIn 757 Byzantine emperor Constantine V gave Pippin III of Francia a mechanical organ intended to indicate the superiority of Byzantine technology.\n\nAfter the Congress of Vienna (1814–15), Rundell, Bridge, and Rundell, goldsmiths to the British royal family and government, prepared 22 snuff-boxes to a value of 1000 guineas each to be given as diplomatic gifts.\n\nDiplomatic gifts have the potential to seal international friendships, but also to be rebuffed, to seem mismatched, or to accidentally send the wrong message. Taiwan rejected the People's Republic of China's offer of a Panda and the gifts given by British Prime Minister Gordon Brown to President Obama in 2009 were widely seen as more thoughtful than those given by Obama to Brown. A 2012 gift of a \"British\" table tennis table to President Obama seemed ideal until it was revealed that it was designed in Britain but made in China, evoking worries about the decline of British manufacturing industry.\n\nDiplomatic gifts take diverse forms:\n\n"}
{"id": "2679811", "url": "https://en.wikipedia.org/wiki?curid=2679811", "title": "Electrogravitic tensor", "text": "Electrogravitic tensor\n\nIn general relativity, the gravitoelectric tensor or tidal tensor is one of the pieces in the Bel decomposition of the Riemann tensor. It is physically interpreted as giving the tidal stresses on small bits of a material object (which may also be acted upon by other physical forces), or the tidal accelerations of a small cloud of test particles in a vacuum solution or electrovacuum solution.\n\n"}
{"id": "46237919", "url": "https://en.wikipedia.org/wiki?curid=46237919", "title": "Energy for Sustainable Development", "text": "Energy for Sustainable Development\n\nEnergy for Sustainable Development is a peer-reviewed academic journal covering research on energy-related aspects of sustainable development. It is published by Elsevier and the editor-in-chief is Daniel B. Jones. According to the \"Journal Citation Reports\", the journal has a 2013 impact factor of 2.360, ranking it 37th out of 83 journals in the category \"Energy & Fuels\".\n"}
{"id": "20719145", "url": "https://en.wikipedia.org/wiki?curid=20719145", "title": "Exhausted combination doctrine", "text": "Exhausted combination doctrine\n\nThe exhausted combination doctrine, also referred to as the doctrine of the Lincoln Engineering case, is the doctrine of U.S. patent law that when an inventor invents a new, unobvious device and seeks to patent not merely the new device but also the combination of the new device with a known, conventional device with which the new device cooperates in the conventional and predictable way in which devices of those types have previously cooperated, the combination is unpatentable as an \"exhausted combination\" or \"old combination\". The doctrine is also termed the doctrine of the \"Lincoln Engineering\" case because the United States Supreme Court explained the doctrine in its decision in \"Lincoln Engineering Co. v. Stewart-Warner Corp.\"\n\nIn \"Lincoln Engineering\", the inventor invented a new and improved coupling device to attach a nozzle to a grease gun. The patent, however, claimed the whole combination of grease gun, nozzle, and coupling. The Supreme Court stated that \"the improvement of one part of an old combination gives no right to claim that improvement in combination with other old parts which perform no new function in the combination\". It then concluded that the inventor's \"effort, by the use of a combination claim, to extend the monopoly of his invention of an improved form of chuck or coupler to old parts or elements having no new function when operated in connection with the coupler renders the claim void.\" \n\nThis way of claiming an invention was termed “overclaiming,” because it inflated the royalty base for licensing and potentially effectuated a tie-in by means of which the patentee required users, for example, to purchase not only the couplings but the whole grease gun as well in order to use the invention.\n\nThe Federal Circuit held in 1984 that this doctrine is outdated and no longer reflects the law. In effect, the Federal Circuit overruled the Supreme Court on this point—or claimed that the passage of the 1952 patent recodification law had done so. \n\nIn its decision in \"Quanta Computer, Inc. v. LG Electronics, Inc.\", however, the Supreme Court seems to have assumed without any discussion that its old precedents such as \"Lincoln Engineering\" (uncited in the \"Quanta\" opinion) are still in force, as least with regard to the exhaustion doctrine. In \"Quanta\", the Court considered the sale of a patented microprocessor to \"exhaust\" not only the patent on the microprocessor but the patent on a conventional personal computer (PC) containing the microprocessor, since the PC patent had essentially the same inventive concept (or departure from the prior art) as the microprocessor patent.\n\nAfter 1984 it appeared that it was possible to obtain patents on old combinations, for example not only a new motor but also an otherwise conventional disc drive containing the new motor. It has also been held that the sale of the motor, in such a case, does not exhaust the patent on the disc drive containing the new motor. Hence, a purchaser of the motor who incorporated it into a disk drive would infringe the disk drive patent. It is now uncertain whether such patents are valid. In any event, the \"Quanta\" decision appears to hold that the exhaustion doctrine shields from infringement liability the foregoing motor purchaser that incorporates the motor into a disk drive.\n\nClaiming a computer-related advance as an exhausted combination may provide a way to prevent the claimed advance from being classified as nonstatutory subject matter under section 101 of the US patent law. Placing a process that fails the machine-or-transformation test in a machine environment may overcome the absence of implementation by a specific machine, as required by In re Bilski and the Supreme Court decisions on which it is based. (The successfulness of this expedient depends on acceptance of the Federal Circuit's abolition of the exhausted combination doctrine.)\n\nFor example, the form of the processes claimed in \"Diamond v. Diehr\", \"Parker v. Flook\", and \"Gottschalk v. Benson\" may appropriately be compared. In \"Diehr\", the claim is to “a method of operating a rubber-molding press” and the claim contains at least minimal references to the press and other apparatus. In \"Flook\", the claim is to a \"method for updating the value of at least one alarm limit,\" where an “’alarm limit’ is a number.” The claim does not say anything about a reaction vessel or even temperature measuring devices. In \"Benson\", the claim is to “a data processing method for converting binary coded decimal number representations into binary number representations.” One claim mentions a reentrant shift register and the other claim mentions no apparatus at all. In \"Flook\", the claim could have instead been to “a method of operating a hydrocracking plant wherein hydrocarbon feedstock is placed into a chemical reactor, heat is applied, etc.” The claim, although to an exhausted combination, would have required apparatus as did that in the \"Diehr\" case. Similarly, the claim in \"Benson\" could have been to a method of operating a telephone switch box or perhaps even a method of providing binary-coded-decimal numerical signals to a binary-coded operating device. Again, by providing a mechanical environment, even though it was an exhausted combination, the claims drafter might have avoided the holding of nonstatutory subject matter. It is possible that careful claims drafting techniques will succeed in elevating form over substance, to avoid the impact of the machine-or-transformation test.\n\nThe preceding analysis may have been overtaken by the Supreme Court's 2014 decision in \"Alice v. CLS Bank\". In that case the Court confirmed and extended the legal analysis of its prior decisions in \"Parker v. Flook\" and \"Mayo v. Prometheus\" to claimed inventions involving implementations of computer algorithms, methods of doing business, and other methods of organizing human activity. The form of analysis that these cases dictate is that the presence of a machine, in particular a programmed digital computer, is not enough without more to assure patent eligibility. Rather, the implementation of the underlying idea must embody an \"inventive concept.\" The inventive concept must provide \"something extra\" that extends beyond the algorithm or other idea, if an otherwise patent-ineligible claim is to be saved from patent ineligibility--according to these cases and recent lower court decisions that follow in their wake.\n\n"}
{"id": "364332", "url": "https://en.wikipedia.org/wiki?curid=364332", "title": "Experimental analysis of behavior", "text": "Experimental analysis of behavior\n\nThe experimental analysis of behavior (EAB) is school of thought in psychology founded on B. F. Skinner's philosophy of radical behaviorism and defines the basic principles used in applied behavior analysis (ABA). A central principle was the inductive, data-driven examination of functional relations, as opposed to the kinds of hypothetico-deductive learning theory that had grown up in the comparative psychology of the 1920–1950 period. Skinner's approach was characterized by empirical observation of measurable behavior which could be predicted and controlled. It owed its early success to the effectiveness of Skinner's procedures of operant conditioning, both in the laboratory and in behavior therapy.\n\nIn classical or respondent conditioning, a relatively neutral stimulus (\"conditioned stimulus\") comes to signal the occurrence of a biologically significant stimulus (\"unconditioned stimulus\") such as food or pain. This typically done by repeatedly pairing the two stimuli, as in Pavlov's experiments with dogs, where a bell was followed by food. As a result, the conditioned stimulus yields a \"conditioned response\" that is usually similar to the \"unconditioned response\" elicited by unconditioned stimulus (e.g., salivation in Pavlov's dogs).\n\nOperant conditioning (also, \"instrumental conditioning\") is a learning process in which behavior is sensitive to, or controlled by its consequences. For example, behavior that is followed by reward (reinforcement) becomes more probable, whereas behavior that is followed by punishment becomes less probable. Many variations and details of this process may be found in the main article.\n\nThe most commonly used tool in animal behavioral research is the operant conditioning chamber—also known as a Skinner Box. The chamber is an enclosure designed to hold a test animal (often a rodent, pigeon, or primate). The interior of the chamber contains some type of device that serves the role of \"discriminative stimuli\", at least one mechanism to measure the subject's \"behavior\" as a rate of response—such as a lever or key-peck switch—and a mechanism for the delivery of \"consequences\"—such as a food pellet dispenser or a token reinforcer such as an LED light.\n\nOf historical interest is the cumulative recorder, an instrument used to record the responses of subjects graphically. Traditionally, its graphing mechanism has consisted of a rotating drum of paper equipped with a marking needle. The needle would start at the bottom of the page and the drum would turn the roll of paper horizontally. Each subject response would result in the marking needle moving vertically along the paper one tick. This makes it possible for the rate of response to be calculated by finding the slope of the graph at a given point. For example, a regular rate of response would cause the needle to move vertically at a regular rate, resulting in a straight diagonal line rising towards the right. An accelerating or decelerating rate of response would lead to a quadratic (or similar) curve. For the most part, cumulative records are no longer graphed using rotating drums, but are recorded electronically instead.\n\nLaboratory methods employed in the experimental analysis of behavior are based upon B.F. Skinner's philosophy of radical behaviorism, which is premised upon:\n\nThe idea that Skinner's position is anti-theoretical is probably inspired by the arguments he put forth in his article \"Are Theories of Learning Necessary?\" However, that article did not argue against the use of theory as such, only against certain theories in certain contexts. Skinner argued that many theories did not explain behavior, but simply offered another layer of structure that itself had to be explained in turn. If an organism is said to have a drive, which causes its behavior, what then causes the drive? Skinner argued that many theories had the effect of halting research or generating useless research.\n\nSkinner's work did have a basis in theory, though his theories were different from those that he criticized. Mecca Chiesa notes that Skinner's theories are inductively derived, while those that he attacked were deductively derived. The theories that Skinner opposed often relied on mediating mechanisms and structures—such as a mechanism for memory as a part of the mind—which were not measurable or observable. Skinner's theories form the basis for two of his books: \"Verbal Behavior\", and \"Science and Human Behavior\". These two texts represent considerable theoretical extensions of his basic laboratory work into the realms of political science, linguistics, sociology and others.\n\n\n"}
{"id": "19509", "url": "https://en.wikipedia.org/wiki?curid=19509", "title": "Finitary relation", "text": "Finitary relation\n\nIn mathematics, a finitary relation has a finite number of \"places\". In set theory and logic, a \"relation\" is a property that assigns truth values to formula_1-tuples of individuals. Typically, the property describes a possible connection between the components of a formula_1-tuple. For a given set of formula_1-tuples, a truth value is assigned to each formula_1-tuple according to whether the property does or does not hold.\n\nAn example of a \"ternary relation\" (i.e., between three individuals) is: \"formula_5 was introduced to formula_6 by formula_7\", where formula_8 is a 3-tuple of persons; for example, \"Beatrice Wood was introduced to Henri-Pierre Roché by Marcel Duchamp\" is true, while \"Karl Marx was introduced to Friedrich Engels by Queen Victoria\" is false.\n\n\"Relation\" is formally defined in the next section. In this section we introduce the concept of a relation with a familiar everyday example. Consider the relation involving three roles that people might play, expressed in a statement of the form \"\"X\" thinks that \"Y\" likes \"Z\" \". The facts of a concrete situation could be organized in a table like the following:\n\nEach row of the table records a fact or makes an assertion of the form \"\"X\" thinks that \"Y\" likes \"Z\" \". For instance, the first row says, in effect, \"Alice thinks that Bob likes Denise\". The table represents a relation \"S\" over the set \"P\" of people under discussion:\n\nThe data of the table are equivalent to the following set of ordered triples:\n\nIt is usual to write \"S\"(Alice, Bob, Denise) to say the same thing as the first row of the table. The relation \"S\" is a \"ternary\" relation, since there are \"three\" items involved in each row. The relation itself is a mathematical object defined in terms of concepts from set theory (i.e., the relation is a subset of the Cartesian product on {Person X, Person Y, Person Z}), that carries all of the information from the table in one neat package. Mathematically, then, a relation is simply an \"ordered set\".\n\nThe table for relation \"S\" is an extremely simple example of a relational database. The theoretical aspects of databases are the specialty of one branch of computer science, while their practical impacts have become all too familiar in our everyday lives. Computer scientists, logicians, and mathematicians, however, tend to see different things when they look at these concrete examples and samples of the more general concept of a relation.\n\nFor one thing, databases are designed to deal with empirical data, and experience is always finite, whereas mathematics at the very least concerns itself with potential infinity. This difference in perspective brings up a number of ideas that may be usefully introduced at this point, if by no means covered in depth.\n\nThe variable formula_1 giving the number of \"places\" in the relation, 3 for the above example, is a non-negative integer, called the relation's \"arity\", \"adicity\", or \"dimension\". A relation with formula_1 places is variously called a formula_1\"-ary\", a formula_1\"-adic\", or a formula_1\"-dimensional\" relation. Relations with a finite number of places are called \"finite-place\" or \"finitary\" relations. It is possible to generalize the concept to include \"infinitary\" relations between infinitudes of individuals, for example infinite sequences; however, in this article only finitary relations are discussed, which will from now on simply be called relations.\n\nSince there is only one 0-tuple, the so-called empty tuple ( ), there are only two zero-place relations: the one that always holds, and the one that never holds. They are sometimes useful for constructing the base case of an induction argument. One-place relations are called unary relations. For instance, any set (such as the collection of Nobel laureates) can be viewed as a collection of individuals having some property (such as that of having been awarded the Nobel prize). Two-place relations are called binary relations or, in the past, \"dyadic relations\". Binary relations are very common, given the ubiquity of relations such as:\n\nA formula_1\"-ary\" relation is a straightforward generalization of a binary relation.\n\nThe simpler of the two definitions of \"k\"-place relations encountered in mathematics is:\n\nDefinition 1. A relation \"L\" over the sets \"X\", …, \"X\" is a subset of their Cartesian product, written \"L\" ⊆ \"X\" × … × \"X\".\n\nRelations are classified according to the number of sets in the defining Cartesian product, in other words, according to the number of terms following \"L\". Hence:\nRelations with more than four terms are usually referred to as \"k\"-ary or \"n\"-ary, for example, \"a 5-ary relation\". A \"k\"-ary relation is simply a set of \"k\"-tuples.\n\nThe second definition makes use of an idiom that is common in mathematics, stipulating that \"such and such is an \"n\"-tuple\" in order to ensure that such and such a mathematical object is determined by the specification of \"n\" component mathematical objects. In the case of a relation \"L\" over \"k\" sets, there are \"k\" + 1 things to specify, namely, the \"k\" sets plus a subset of their Cartesian product. In the idiom, this is expressed by saying that \"L\" is a (\"k\" + 1)-tuple.\n\nDefinition 2. A relation \"L\" over the sets \"X\", …, \"X\" is a (\"k\" + 1)-tuple \"L\" = (\"X\", …, \"X\", \"G\"(\"L\")), where \"G\"(\"L\") is a subset of the Cartesian product \"X\" × … × \"X\". \"G\"(\"L\") is called the \"graph\" of \"L\".\n\nElements of a relation are more briefly denoted by using boldface characters, for example, the constant element a = (a, …, a) or the variable element x = (\"x\", …, \"x\").\n\nA statement of the form \"a is in the relation \"L\" \" or \"a satisfies \"L\" \" is taken to mean that a is in \"L\" under the first definition and that a is in \"G\"(\"L\") under the second definition.\n\nThe following considerations apply under either definition:\n\nAs a rule, whatever definition best fits the application at hand will be chosen for that purpose, and anything that falls under it will be called a relation for the duration of that discussion. If it becomes necessary to distinguish the two definitions, an entity satisfying the second definition may be called an \"embedded\" or \"included\" relation.\n\nIf \"L\" is a relation over the domains \"X\", …, \"X\", it is conventional to consider a sequence of terms called \"variables\", \"x\", …, \"x\", that are said to \"range over\" the respective domains.\n\nLet a Boolean domain B be a two-element set, say, B = {0, 1}, whose elements can be interpreted as logical values, typically 0 = false and 1 = true. The characteristic function of the relation \"L\", written \"ƒ\" or χ(\"L\"), is the Boolean-valued function \"ƒ\" : \"X\" × … × \"X\" → B, defined in such a way that \"ƒ\"(formula_23) = 1 just in case the \"k\"-tuple formula_23 is in the relation \"L\". Such a function can also be called an indicator function, particularly in probability and statistics, to avoid confusion with the notion of a characteristic function in probability theory.\n\nIt is conventional in applied mathematics, computer science, and statistics to refer to a Boolean-valued function like \"ƒ\" as a \"k\"-place predicate. From the more abstract viewpoint of formal logic and model theory, the relation \"L\" constitutes a \"logical model\" or a \"relational structure\" that serves as one of many possible interpretations of some \"k\"-place predicate symbol.\n\nBecause relations arise in many scientific disciplines as well as in many branches of mathematics and logic, there is considerable variation in terminology. This article treats a relation as the set-theoretic extension of a relational concept or term. A variant usage reserves the term \"relation\" to the corresponding logical entity, either the logical comprehension, which is the totality of intensions or abstract properties that all of the elements of the relation in extension have in common, or else the symbols that are taken to denote these elements and intensions. Further, some writers of the latter persuasion introduce terms with more concrete connotations, like \"relational structure\", for the set-theoretic extension of a given relational concept.\n\nThe logician Augustus De Morgan, in work published around 1860, was the first to articulate the notion of relation in anything like its present sense. He also stated the first formal results in the theory of relations (on De Morgan and relations, see Merrill 1990). Charles Sanders Peirce restated and extended De Morgan's results.\n\nIn the 19th century Peirce, Gottlob Frege, Georg Cantor, Richard Dedekind, and others advanced the theory of relations. Many of their ideas, especially on relations called orders, were summarized in Principles of Mathematics (1903) by Bertrand Russell. Russell and A. N. Whitehead made free use of these results in their \"Principia Mathematica\".\n\n\n"}
{"id": "352714", "url": "https://en.wikipedia.org/wiki?curid=352714", "title": "Germ (mathematics)", "text": "Germ (mathematics)\n\nIn mathematics, the notion of a germ of an object in/on a topological space is an equivalence class of that object and others of the same kind which captures their shared local properties. In particular, the objects in question are mostly functions (or maps) and subsets. In specific implementations of this idea, the sets or maps in question will have some property, such as being analytic or smooth, but in general this is not needed (the maps or functions in question need not even be continuous); it is however necessary that the space on/in which the object is defined is a topological space, in order that the word \"local\" have some sense.\n\nThe name is derived from \"cereal germ\" in a continuation of the sheaf metaphor, as a germ is (locally) the \"heart\" of a function, as it is for a grain.\n\nGiven a point \"x\" of a topological space X, and two maps formula_1 (where \"Y\" is any set), then formula_2 and formula_3 define the same germ at \"x\" if there is a neighbourhood \"U\" of \"x\" such that restricted to \"U\", \"f\" and \"g\" are equal; meaning that formula_4 for all \"u\" in \"U\".\n\nSimilarly, if \"S\" and \"T\" are any two subsets of X, then they define the same germ at \"x\" if there is again a neighbourhood \"U\" of \"x\" such that\n\nIt is straightforward to see that \"defining the same germ\" at \"x\" is an equivalence relation (be it on maps or sets), and the equivalence classes are called germs (map-germs, or set-germs accordingly). The equivalence relation is usually written\n\nGiven a map \"f\" on \"X\", then its germ at \"x\" is usually denoted [\"f\" ]. Similarly, the germ at \"x\" of a set \"S\" is written [\"S\"]. Thus,\n\nA map germ at \"x\" in \"X\" which maps the point \"x\" in \"X\" to the point \"y\" in \"Y\" is denoted as\n\nWhen using this notation, \"f\" is then intended as an entire equivalence class of maps, using the same letter \"f\" for any representative map.\n\nNotice that two sets are germ-equivalent at \"x\" if and only if their characteristic functions are germ-equivalent at \"x\":\n\nMaps need not be defined on all of \"X\", and in particular they don't need to have the same domain. However, if \"f\" has domain \"S\" and \"g\" has domain \"T\", both subsets of \"X\", then \"f\" and \"g\" are germ equivalent at \"x\" in \"X\" if first \"S\" and \"T\" are germ equivalent at \"x\", say formula_10 and then moreover formula_11, for some smaller neighbourhood \"V\" with formula_12. This is particularly relevant in two settings: \n\nIf \"f\" and \"g\" are germ equivalent at \"x\", then they share all local properties, such as continuity, differentiability etc., so it makes sense to talk about a \"differentiable or analytic germ\", etc. Similarly for subsets: if one representative of a germ is an analytic set then so are all representatives, at least on some neighbourhood of \"x\".\n\nAlgebraic structures on the target \"Y\" are inherited by the set of germs with values in \"Y\". For instance, if the target \"Y\" is a group (mathematics), then it makes sense to multiply germs: to define [\"f\"][\"g\"], first take representatives \"f\" and \"g\", defined on neighbourhoods \"U\" and \"V\" respectively, and define [\"f\"][\"g\"] to be the germ at \"x\" of the map \"fg\" (which is defined on formula_13). In the same way, if \"Y\" is an abelian group, vector space, or ring, then so is the set of germs.\n\nThe set of germs at \"x\" of maps from \"X\" to \"Y\" does not have a useful topology, except for the discrete one. It therefore makes little or no sense to talk of a convergent sequence of germs. However, if \"X\" and \"Y\" are manifolds, then the spaces of jets formula_14 (finite order Taylor series at \"x\" of map(-germs)) do have topologies as they can be identified with finite-dimensional vector spaces.\n\nThe idea of germ is behind the definition of sheaves and presheaves. A presheaf formula_15 of abelian groups on a topological space \"X\" assigns an abelian group formula_16 to each open set \"U\" in \"X\". Typical examples of abelian groups here are: real valued functions on \"U\", differential forms on \"U\", vector fields on \"U\", holomorphic functions on \"U\" (when \"X\" is a complex space), constant functions on \"U\" and differential operators on \"U\".\n\nIf formula_17 then there is a restriction map formula_18 satisfying certain compatibility conditions. For a fixed \"x\", one says that elements formula_19 and formula_20 are equivalent at \"x\" if there is a neighbourhood formula_21 of \"x\" with res(\"f\") = res(\"g\") (both elements of formula_22). The equivalence classes form the stalk formula_23 at \"x\" of the presheaf formula_15. This equivalence relation is an abstraction of the germ equivalence described above.\n\nInterpreting germs through sheaves also gives a general explanation for the presence of algebraic structures on sets of germs. The reason is because formation of stalks preserves finite limits. This implies that if \"T\" is a Lawvere theory and a sheaf \"F\" is a \"T\"-algebra, then any stalk \"F\" is also a \"T\"-algebra.\n\nIf formula_25 and formula_26 have additional structure, it is possible to define subsets of the set of all maps from \"X\" to \"Y\" or more generally sub-presheaves of a given presheaf formula_15 and corresponding germs: \"some notable examples follow\".\n\n\n\n\n\n\nThe stalk of a sheaf formula_15 on a topological space formula_25 at a point formula_44 of formula_25 is commonly denoted by formula_46 As a consequence germs, being stalks of sheaves of various kind of functions, borrow this scheme of notation:\n\nFor germs of sets and varieties, the notation is not so well established: some notations found in literature include:\n\n\nThe key word in the applications of germs is locality: \"all local properties of a function at a point can be studied by analyzing its germ\". They are a generalization of Taylor series, and indeed the Taylor series of a germ (of a differentiable function) is defined: you only need local information to compute derivatives.\n\nGerms are useful in determining the properties of dynamical systems near chosen points of their phase space: they are one of the main tools in singularity theory and catastrophe theory.\n\nWhen the topological spaces considered are Riemann surfaces or more generally analytic varieties, germs of holomorphic functions on them can be viewed as power series, and thus the set of germs can be considered to be the analytic continuation of an analytic function.\n\nAs noted earlier, sets of germs may have algebraic structures such as being rings. In many situations, rings of germs are not arbitrary rings but instead have quite specific properties.\n\nSuppose that \"X\" is a space of some sort. It is often the case that, at each \"x\" ∈ \"X\", the ring of germs of functions at \"x\" is a local ring. This is the case, for example, for continuous functions on a topological space; for \"k\" times differentiable, smooth, or analytic functions on a real manifold (when such functions are defined); for holomorphic functions on complex manifold; and for regular functions on an algebraic variety. The property that rings of germs are local rings is axiomatized by the theory of locally ringed spaces.\n\nThe types of local rings that arise, however, depend closely on the theory under consideration. The Weierstrass preparation theorem implies that rings of germs of holomorphic functions are Noetherian rings. It can also be shown that these are regular rings. On the other hand, let formula_69 be the ring of germs at the origin of smooth functions on R. This ring is local but not Noetherian. To see why, observe that the maximal ideal \"m\" of this ring consists of all germs which vanish at the origin, and the power \"m\" consists of those germs whose first \"k\" − 1 derivatives vanish. If this ring were Noetherian, then the Krull intersection theorem would imply that a smooth function whose Taylor series vanished would be the zero function. But this is false, as can be seen by considering\nThis ring is also not a unique factorization domain. This is because all UFDs satisfy the ascending chain condition on principal ideals, but there is an infinite ascending chain of principal ideals\nThe inclusions are strict because \"x\" is in the maximal ideal \"m\".\n\nThe ring formula_72 of germs at the origin of continuous functions on R even has the property that its maximal ideal \"m\" satisfies \"m\" = \"m\". Any germ \"f\" ∈ \"m\" can be written as\nwhere sgn is the sign function. Since |\"f\"| vanishes at the origin, this expresses \"f\" as the product of two functions in \"m\", whence the conclusion. This is related to the setup of almost ring theory.\n\n\n\n"}
{"id": "56797070", "url": "https://en.wikipedia.org/wiki?curid=56797070", "title": "Grossman model of health demand", "text": "Grossman model of health demand\n\nThe Grossman model of health demand is a model for studying the demand for health and medical care outlined by Michael Grossman in a monograph in 1972 entitled: \"The demand for health: A theoretical and empirical investigation\". The model based demand for medical care on the interaction between a demand function for health and a production function for health. Andrew Jones, Nigel Rice, and Paul Contoyannis call the model the \"founding father of demand for health models\".\n\nIn this model, health is a durable capital good which is inherited and depreciates over time. Investment in health takes the form of medical care purchases and other inputs and depreciation is interpreted as natural deterioration of health over time. In the model, health enters the utility function directly as a good people derive pleasure from and indirectly as an investment which makes more healthy time available for market and non-market activities.\n\nThe model creates a dynamic system of equations which can be cast as an optimization problem where utility is optimized over gross investment in health in each period, consumption of medical care, and time inputs in the gross investment function in each period. In this way, the length of life of the agent is partially endogenous to the model.\n\nDynamic optimization problems are often optimized using comparative statics, setting partial derivatives of the outcome function of interest, in this case the utility function, equal to zero. When the partial derivative of the utility function with respect to health consumption is assumed to equal zero, the resulting sub-model is the investment model. Solutions to the problem of this sub-model generally show that the rate of return on health capital must equal the opportunity cost of said capital. Thus, increases in the depreciation rate over time cause the optimal stock of health to decrease. If the marginal efficiency of capital curve is inelastic, gross investment grows over time. In practical terms, this model thus predicts that older people will have more sick time and time spent on increasing health and have higher medical expenditures than younger people. Another implication is that since increases in wages shift the marginal efficiency of capital curve to the right and increases the curve's slope, an increase in wage will increase the demand for health capital.\n\nThe Grossman model was extended in a number of directions. Among the first to include uncertainty in the model were Charles Phelps and Maureen Cropper.\n\nThe relationship between education and health was expanded in the model by Isaac Ehrlich. Regarding the relationship between education and medical care demand, one important question is whether the marginal efficiency of capital elasticity with respect to education is less than or greater than one. If the curve is elastic (elasticity greater than one), education will increase medical care demand. On the other hand, if the curve is inelastic, education will decrease medical care demand.\n\nJan Acton expanded the time constraint by including travel and waiting time in health care.\n\nImportant empirical studies into these components include the RAND Health Insurance Study led by Joseph Newhouse in the 1970s, 1980s, and 1990s. This study sought to estimate income, money price, and time price elasticities of demand for medical care. Another important study was the Oregon Health Insurance in the late 2000s and also estimated the role of public health provision on healthcare demand and was led by Katherine Baicker and Amy Finkelstein.\n\nIn the model, health is neither a pure investment good nor a pure consumption good, but health stock benefits the agent in two ways, directly increasing utility and second by increasing healthy time available for other activities. One early criticism is that framing health as a dicotomous concept is intuitively wrong in that health is simultaneously both and health provides both alternatives simultaneously.\n\n"}
{"id": "45486119", "url": "https://en.wikipedia.org/wiki?curid=45486119", "title": "Ground segment", "text": "Ground segment\n\nA ground segment consists of all the ground-based elements of a spacecraft system used by operators and support personnel, as opposed to the space segment and user segment. The ground segment enables management of a spacecraft, and distribution of payload data and telemetry among interested parties on the ground. The primary elements of a ground segment include:\nThese elements are present in nearly all space missions, whether commercial, military, or scientific. They may be located together or separated geographically, and they may be operated by different parties. Some elements may support multiple spacecraft simultaneously.\n\nGround stations provide radio interfaces between the space and ground segments for telemetry, tracking, and command (TT&C), as well as payload data transmission and reception. Tracking networks, such as NASA's Near Earth Network and Space Network, may handle communications with multiple spacecraft through time-sharing. \n\nGround station equipment may be monitored and controlled remotely, often via serial and/or IP interfaces. There are typically backup stations from which radio contact can be maintained if there is a problem at the primary ground station which renders it unable to operate, such as a natural disaster. Such contingencies are considered in a Continuity of Operations plan.\n\nSignals to be uplinked to a spacecraft must first be extracted from ground network packets, encoded to baseband, and modulated, typically onto an intermediate frequency (IF) carrier, before being up-converted to the assigned radio frequency (RF) band. The RF signal is then amplified to high power and carried via waveguide to an antenna for transmission. In colder climates, electric heaters or hot air blowers may be necessary to prevent ice or snow buildup on the parabolic dish.\n\nReceived (\"downlinked\") signals are passed through a low-noise amplifier (often located in the antenna hub to minimize the distance the signal must travel) before being down-converted to IF; these two functions may be combined in a low-noise block downconverter. The IF signal is then demodulated, and the data stream extracted via bit and frame synchronization and decoding. Data errors, such as those caused by signal degradation, are identified and corrected where possible. The decoded data stream is then packetized or saved to files for transmission on the ground network. Ground stations may temporarily store received telemetry for later playback to control centers.\n\nA single spacecraft may make use of multiple RF bands for different telemetry, command, and payload data streams, depending on bandwidth and other requirements.\n\nThe timing of passes, when a line of sight exists to the spacecraft, is determined by the location of ground stations, and by the characteristics of the spacecraft orbit or trajectory. The Space Network uses geostationary relay satellites to extend pass opportunities over the horizon.\n\nGround stations must track spacecraft in order to point their antennas properly, and must account for Doppler shifting of RF frequencies due to the motion of the spacecraft. Ground stations may also perform automated ranging; ranging tones may be multiplexed with command and telemetry signals. Ground station tracking and ranging data are passed to the control center along with spacecraft telemetry.\n\nMission control centers issue commands, data uploads, and software updates to spacecraft, and process, analyze, and distribute telemetry. For manned spacecraft, mission control manages voice and video communications with the crew. Control centers may also be responsible for configuration management and data archival. As with ground stations, there are typically backup control facilities available to support continuity of operations. \n\nControl centers use telemetry to determine the status of a spacecraft and its systems. Housekeeping, diagnostic, science, and other types of telemetry may be carried on separate virtual channels. Flight control software performs the initial processing of received telemetry, including:\n\nA spacecraft database is called on to provide information on telemetry frame formatting, the positions and frequencies of parameters within frames, and their associated mnemonics, calibrations, and soft and hard limits. The contents of this database—especially calibrations and limits—may be updated periodically to maintain consistency with flight software and operating procedures; these can change during the life of a mission in response to upgrades, hardware degradation in the space environment, and changes to mission parameters.\n\nCommands sent to spacecraft are formatted according to the spacecraft database, and are validated against the database before being transmitted via a ground station. Commands may be issued manually in real time, or they may be part of automated or semi-automated procedures. Typically, commands successfully received by the spacecraft are acknowledged in telemetry, and a command counter is maintained on the spacecraft and at the ground to ensure synchronization. In certain cases, closed-loop control may be performed. Commanded activities may pertain directly to mission objectives, or they may be part of housekeeping. Commands (and telemetry) may be encrypted to prevent unauthorized access to the spacecraft or its data.\n\nSpacecraft procedures are often developed and tested against a spacecraft simulator prior to use with the actual spacecraft.\n\nControl centers may be continuously or regularly staffed by flight controllers. Staffing is typically greatest during the early phases of a mission, and during critical procedures and periods. Increasingly commonly, control centers for unmanned spacecraft may be set up for \"lights-out\" (or automated) operation, as a means of controlling costs. Flight control software will typically generate alerts regarding significant events – both planned and unplanned – in the ground or space segment that may require operator action.\n\nMission control centers may rely on \"offline\" (i.e., non-real-time) data processing subsystems to handle analytical tasks such as:\n\nDedicated physical spaces may be provided in the control center for certain mission support roles, such as flight dynamics and network control, or these roles may be handled via remote terminals outside the control center. As on-board computing power and flight software complexity have increased, there is a trend toward performing more automated data processing on board the spacecraft.\n\nGround networks handle data transfer and voice communication between different elements of the ground segment. These networks often combine LAN and WAN elements, for which different parties may be responsible. Geographically separated elements may be connected via leased lines or virtual private networks. The design of ground networks is driven by requirements on reliability, bandwidth, and security.\n\nReliability is a particularly important consideration for critical systems, with uptime and mean time to recovery being of paramount concern. As with other aspects of the spacecraft system, redundancy of network components is the primary means of achieving the required system reliability.\n\nSecurity considerations are vital to protect space resources and sensitive data. WAN links often incorporate encryption protocols and firewalls to provide information and network security. Antivirus software and intrusion detection systems provide additional security at network endpoints.\n\nRemote terminals are interfaces on ground networks, separate from the mission control center, which may be accessed by payload controllers, telemetry analysts, instrument and science teams, and support personnel, such as system administrators and software development teams. They may be receive-only, or they may transmit data to the ground network.\n\nSatellite terminals used by service customers, including ISPs and end users, are collectively called the \"user segment\", and are typically distinguished from the ground segment.\n\nSpace vehicles and their interfaces are assembled and tested at integration and test (I&T) facilities. I&T provides an opportunity to fully test communications with, and behavior of, the spacecraft prior to launch.\n\nVehicles are delivered to space via launch facilities, which handle the logistics of rocket launches. Launch facilities are typically connected to the ground network to relay telemetry prior to and during launch. The launch vehicle itself is sometimes said to constitute a \"transfer segment\", which may be considered distinct from both the space and ground segments.\n\nCosts associated with the establishment and operation of a ground segment are highly variable, and depend on accounting methods. According to a study by Delft University of Technology, the ground segment contributes approximately 5% to the total cost of a space system. According to a report by the RAND Corporation on NASA small spacecraft missions, operation costs alone contribute 8% to the lifetime cost of a typical mission, with integration and testing making up a further 3.2%, ground facilities 2.6%, and ground systems engineering 1.1%.\n\nGround segment cost drivers include requirements placed on facilities, hardware, software, network connectivity, security, and staffing. Ground station costs in particular depend largely on the required transmission power, RF band(s), and the suitability of preexisting facilities. Control centers may be highly automated as a means of controlling staffing costs.\n\n"}
{"id": "19593167", "url": "https://en.wikipedia.org/wiki?curid=19593167", "title": "Heat", "text": "Heat\n\nIn thermodynamics, heat is energy transferred from one system to another as a result of thermal interactions. The amount of heat transferred in any process can be defined as the total amount of transferred energy excluding any macroscopic work that was done and any transfer of part of the object itself. When two systems with different temperatures are put in contact, heat flows spontaneously from the hotter to the colder system. Transfer of energy as heat can occur through direct contact, through a barrier that is impermeable to matter (as in conduction), by radiation between separated bodies, by way of an intermediate fluid (as in convective circulation), or by a combination of these. By contrast to work, heat involves the stochastic (random) motion of particles (such as atoms or molecules) that is equally distributed among all degrees of freedom, while work is confined to one or more specific degrees of freedom such as those of the center of mass. \n\nLike thermodynamic work, heat is a property of a process, not a property of a system. Energy exchanged as heat (a process function) changes the internal energy (a state function) of each system by equal and opposite amounts. This is to be distinguished from the common conception of heat as a property of high-temperature systems. \n\nAlthough heat flows from a hotter body to a cooler one, it is possible to construct a heat pump or refrigeration system that does work to increase the difference in temperature between two systems. In contrast, a heat engine reduces an existing temperature difference to do work on another system.\n\nAs a form of energy, the SI unit of heat is the joule (J). The conventional symbol used to represent the amount of heat exchanged in a thermodynamic process is . \nHeat is measured by its effect on the states of interacting bodies, for example, by the amount of ice melted or a change in temperature. The quantification of heat via the temperature change of a body is called calorimetry.\n\nAs a form of energy, heat has the unit joule (J) in the International System of Units (SI). However, in many applied fields in engineering the British thermal unit (BTU) and the calorie are often used. The standard unit for the rate of heat transferred is the watt (W), defined as one joule per second.\n\nUse of the symbol for the total amount of energy transferred as heat is due to Rudolf Clausius in 1850:\n\nHeat released by a system into its surroundings is by convention a negative quantity ( < 0); when a system absorbs heat from its surroundings, it is positive ( > 0). Heat transfer rate, or heat flow per unit time, is denoted by formula_1. This should not be confused with a time derivative of a function of state (which can also be written with the dot notation) since heat is not a function of state. \nHeat flux is defined as rate of heat transfer per unit cross-sectional area (units watts per square metre).\n\nIn 1856, Rudolf Clausius, referring to closed systems, in which transfers of matter do not occur, defined the \"second fundamental theorem\" (the second law of thermodynamics) in the mechanical theory of heat (thermodynamics): \"if two transformations which, without necessitating any other permanent change, can mutually replace one another, be called equivalent, then the generations of the quantity of heat \"Q\" from work at the temperature \"T\", has the \"equivalence-value\":\"\n\nIn 1865, he came to define the entropy symbolized by \"S\", such that, due to the supply of the amount of heat \"Q\" at temperature \"T\" the entropy of the system is increased by\n\nIn a transfer of energy as heat without work being done, there are changes of entropy in both the surroundings which lose heat and the system which gains it. The increase, , of entropy in the system may be considered to consist of two parts, an increment, that matches, or 'compensates', the change, , of entropy in the surroundings, and a further increment, that may be considered to be 'generated' or 'produced' in the system, and is said therefore to be 'uncompensated'. Thus\n\nThis may also be written\n\nThe total change of entropy in the system and surroundings is thus\n\nThis may also be written\n\nIt is then said that an amount of entropy has been transferred from the surroundings to the system. Because entropy is not a conserved quantity, this is an exception to the general way of speaking, in which an amount transferred is of a conserved quantity.\n\nFrom the second law of thermodynamics follows that in a spontaneous transfer of heat, in which the temperature of the system is different from that of the surroundings:\n\nFor purposes of mathematical analysis of transfers, one thinks of fictive processes that are called \"reversible\", with the temperature of the system being hardly less than that of the surroundings, and the transfer taking place at an imperceptibly slow rate.\n\nFollowing the definition above in formula (1), for such a fictive reversible process, a quantity of transferred heat (an inexact differential) is analyzed as a quantity , with (an exact differential):\n\nThis equality is only valid for a fictive transfer in which there is no production of entropy, that is to say, in which there is no uncompensated entropy.\n\nIf, in contrast, the process is natural, and can really occur, with irreversibility, then there is entropy production, with . The quantity was termed by Clausius the \"uncompensated heat\", though that does not accord with present-day terminology. Then one has\n\nThis leads to the statement\n\nwhich is the second law of thermodynamics for closed systems.\n\nIn non-equilibrium thermodynamics that approximates by assuming the hypothesis of local thermodynamic equilibrium, there is a special notation for this. The transfer of energy as heat is assumed to take place across an infinitesimal temperature difference, so that the system element and its surroundings have near enough the same temperature . Then one writes\n\nwhere by definition\n\nThe second law for a natural process asserts that\n\nFor a closed system (a system from which no matter can enter or exit), one version of the first law of thermodynamics states that the change in internal energy of the system is equal to the amount of heat supplied to the system minus the amount of work done by system on its surroundings. The foregoing sign convention for work is used in the present article, but an alternate sign convention, followed by IUPAC, for work, is to consider the work performed on the system by its surroundings as positive. This is the convention adopted by many modern textbooks of physical chemistry, such as those by Peter Atkins and Ira Levine, but many textbooks on physics define work as work done by the system.\n\nThis formula can be re-written so as to express a definition of quantity of energy transferred as heat, based purely on the concept of adiabatic work, if it is supposed that is defined and measured solely by processes of adiabatic work:\n\nThe work done by the system includes boundary work (when the system increases its volume against an external force, such as that exerted by a piston) and other work (e.g. shaft work performed by a compressor fan), which is called isochoric work:\n\nIn this Section we will neglect the \"other-\" or isochoric work contribution.\n\nThe internal energy, , is a state function. In cyclical processes, such as the operation of a heat engine, state functions of the working substance return to their initial values upon completion of a cycle.\n\nThe differential, or infinitesimal increment, for the internal energy in an infinitesimal process is an exact differential . The symbol for exact differentials is the lowercase letter .\n\nIn contrast, neither of the infinitesimal increments nor in an infinitesimal process represents the state of the system. Thus, infinitesimal increments of heat and work are inexact differentials. The lowercase Greek letter delta, , is the symbol for inexact differentials. The integral of any inexact differential over the time it takes for a system to leave and return to the same thermodynamic state does not necessarily equal zero.\n\nAs recounted below, in the section headed Entropy, the second law of thermodynamics observes that if heat is supplied to a system in which no irreversible processes take place and which has a well-defined temperature , the increment of heat and the temperature form the exact differential\n\nand that , the entropy of the working body, is a function of state. Likewise, with a well-defined pressure, , behind the moving boundary, the work differential, , and the pressure, , combine to form the exact differential\n\nwith the volume of the system, which is a state variable. In general, for homogeneous systems,\n\nAssociated with this differential equation is that the internal energy may be considered to be a function of its natural variables and . The internal energy representation of the fundamental thermodynamic relation is written\n\nIf is constant\n\nand if is constant\n\nwith the enthalpy defined by\n\nThe enthalpy may be considered to be a function of its natural variables and . The enthalpy representation of the fundamental thermodynamic relation is written\n\nThe internal energy representation and the enthalpy representation are partial Legendre transforms of one another. They contain the same physical information, written in different ways. Like the internal energy, the enthalpy stated as a function of its natural variables is a thermodynamic potential and contains all thermodynamic information about a body.\n\nIf a quantity of heat is added to a body while it does expansion work on its surroundings, one has\n\nIf this is constrained to happen at constant pressure with , the expansion work done by the body is given by ; recalling the first law of thermodynamics, one has\n\nConsequently, by substitution one has\n\nIn this scenario, the increase in enthalpy is equal to the quantity of heat added to the system. Since many processes do take place at constant pressure, or approximately at atmospheric pressure, the enthalpy is therefore sometimes given the misleading name of 'heat content'. It is sometimes also called the heat function.\n\nIn terms of the natural variables of the state function , this process of change of state from state 1 to state 2 can be expressed as\n\nIt is known that the temperature is identically stated by\n\nConsequently,\n\nIn this case, the integral specifies a quantity of heat transferred at constant pressure.\n\nAs a common noun, English \"heat\" or \"warmth\" (just as French \"chaleur\", German \"Wärme\", Latin \"calor\", Greek θάλπος, etc.) refers to (the human perception of) either thermal energy or temperature. Speculation on thermal energy or \"heat\" as a separate form of matter has a long history, see caloric theory, phlogiston and fire (classical element).\n\nThe modern understanding of thermal energy originates with Thompson's 1798 mechanical theory of heat (\"An Experimental Enquiry Concerning the Source of the Heat which is Excited by Friction\"), postulating a mechanical equivalent of heat.\nA collaboration between Nicolas Clément and Sadi Carnot (\"Reflections on the Motive Power of Fire\") in the 1820s had some related thinking near the same lines. In 1845, Joule published a paper entitled \"The Mechanical Equivalent of Heat\", in which he specified a numerical value for the amount of mechanical work required to \"produce a unit of heat\".\nThe theory of classical thermodynamics matured in the 1850s to 1860s. John Tyndall's \"Heat Considered as Mode of Motion\" (1863) was instrumental in popularising the idea of heat as motion to the English-speaking public.\nThe theory was developed in academic publications in French, English and German. From an early time, the French technical term \"chaleur\" used by Carnot was taken as equivalent to the English \"heat\" and German \"Wärme\" (lit. \"warmth\", the equivalent of \"heat\" would be German \"Hitze\").\n\nThe process function was introduced by Rudolf Clausius in 1850. \nClausius described it with the German compound \"Wärmemenge\", translated as \"amount of heat\".\n\nJames Clerk Maxwell in his 1871 \"Theory of Heat\" outlines four stipulations for the definition of heat:\n\nThe process function is referred to as \"Wärmemenge\" by Clausius, or as \"amount of heat\" in translation.\nUse of \"heat\" as an abbreviated of the specific concept of \"amount of heat being transferred\" led to some terminological confusion by the early 20th century. The generic meaning of \"heat\", even in classical thermodynamics, is just \"thermal energy\".\nSince the 1920s, it has been recommended practice to use enthalpy to refer to the \"heat content at constant volume\", and to thermal energy when \"heat\" in the general sense is intended, while \"heat\" is reserved for the very specific context of the transfer of thermal energy between two systems.\nLeonard Benedict Loeb in his \"Kinetic Theory of Gases\" (1927) makes a point of using \"quanitity of heat\" \nor \"heat–quantity\" when referring to :\nA frequent definition of heat is based on the work of Carathéodory (1909), referring to processes in a closed system.\n\nThe internal energy of a body in an arbitrary state can be determined by amounts of work adiabatically performed by the body on its surroundings when it starts from a reference state . Such work is assessed through quantities defined in the surroundings of the body. It is supposed that such work can be assessed accurately, without error due to friction in the surroundings; friction in the body is not excluded by this definition. The adiabatic performance of work is defined in terms of adiabatic walls, which allow transfer of energy as work, but no other transfer, of energy or matter. In particular they do not allow the passage of energy as heat. According to this definition, work performed adiabatically is in general accompanied by friction within the thermodynamic system or body. On the other hand, according to Carathéodory (1909), there also exist non-adiabatic, \"diathermal\" walls, which are postulated to be permeable only to heat.\n\nFor the definition of quantity of energy transferred as heat, it is customarily envisaged that an arbitrary state of interest is reached from state by a process with two components, one adiabatic and the other not adiabatic. For convenience one may say that the adiabatic component was the sum of work done by the body through volume change through movement of the walls while the non-adiabatic wall was temporarily rendered adiabatic, and of isochoric adiabatic work. Then the non-adiabatic component is a process of energy transfer through the wall that passes only heat, newly made accessible for the purpose of this transfer, from the surroundings to the body. The change in internal energy to reach the state from the state is the difference of the two amounts of energy transferred.\n\nAlthough Carathéodory himself did not state such a definition, following his work it is customary in theoretical studies to define heat, , to the body from its surroundings, in the combined process of change to state from the state , as the change in internal energy, , minus the amount of work, , done by the body on its surrounds by the adiabatic process, so that .\n\nIn this definition, for the sake of conceptual rigour, the quantity of energy transferred as heat is not specified directly in terms of the non-adiabatic process. It is defined through knowledge of precisely two variables, the change of internal energy and the amount of adiabatic work done, for the combined process of change from the reference state to the arbitrary state . It is important that this does not explicitly involve the amount of energy transferred in the non-adiabatic component of the combined process. It is assumed here that the amount of energy required to pass from state to state , the change of internal energy, is known, independently of the combined process, by a determination through a purely adiabatic process, like that for the determination of the internal energy of state above. The rigour that is prized in this definition is that there is one and only one kind of energy transfer admitted as fundamental: energy transferred as work. Energy transfer as heat is considered as a derived quantity. The uniqueness of work in this scheme is considered to guarantee rigor and purity of conception. The conceptual purity of this definition, based on the concept of energy transferred as work as an ideal notion, relies on the idea that some frictionless and otherwise non-dissipative processes of energy transfer can be realized in physical actuality. The second law of thermodynamics, on the other hand, assures us that such processes are not found in nature.\n\nBefore the rigorous mathematical definition of heat based on Carathéodory's 1909 paper, \nhistorically, heat, temperature, and thermal equilibrium were presented in thermodynamics textbooks as jointly primitive notions. Carathéodory introduced his 1909 paper thus: \"The proposition that the discipline of thermodynamics can be justified without recourse to any hypothesis that cannot be verified experimentally must be regarded as one of the most noteworthy results of the research in thermodynamics that was accomplished during the last century.\" Referring to the \"point of view adopted by most authors who were active in the last fifty years\", Carathéodory wrote: \"There exists a physical quantity called heat that is not identical with the mechanical quantities (mass, force, pressure, etc.) and whose variations can be determined by calorimetric measurements.\" James Serrin introduces an account of the theory of thermodynamics thus: \"In the following section, we shall use the classical notions of \"heat\", \"work\", and \"hotness\" as primitive elements, ... That heat is an appropriate and natural primitive for thermodynamics was already accepted by Carnot. Its continued validity as a primitive element of thermodynamical structure is due to the fact that it synthesizes an essential physical concept, as well as to its successful use in recent work to unify different constitutive theories.\" This traditional kind of presentation of the basis of thermodynamics includes ideas that may be summarized by the statement that heat transfer is purely due to spatial non-uniformity of temperature, and is by conduction and radiation, from hotter to colder bodies. It is sometimes proposed that this traditional kind of presentation necessarily rests on \"circular reasoning\"; against this proposal, there stands the rigorously logical mathematical development of the theory presented by Truesdell and Bharatha (1977).\n\nThis alternative approach to the definition of quantity of energy transferred as heat differs in logical structure from that of Carathéodory, recounted just above.\n\nThis alternative approach admits calorimetry as a primary or direct way to measure quantity of energy transferred as heat. It relies on temperature as one of its primitive concepts, and used in calorimetry. It is presupposed that enough processes exist physically to allow measurement of differences in internal energies. Such processes are not restricted to adiabatic transfers of energy as work. They include calorimetry, which is the commonest practical way of finding internal energy differences. The needed temperature can be either empirical or absolute thermodynamic.\n\nIn contrast, the Carathéodory way recounted just above does not use calorimetry or temperature in its primary definition of quantity of energy transferred as heat. The Carathéodory way regards calorimetry only as a secondary or indirect way of measuring quantity of energy transferred as heat. As recounted in more detail just above, the Carathéodory way regards quantity of energy transferred as heat in a process as primarily or directly defined as a residual quantity. It is calculated from the difference of the internal energies of the initial and final states of the system, and from the actual work done by the system during the process. That internal energy difference is supposed to have been measured in advance through processes of purely adiabatic transfer of energy as work, processes that take the system between the initial and final states. By the Carathéodory way it is presupposed as known from experiment that there actually physically exist enough such adiabatic processes, so that there need be no recourse to calorimetry for measurement of quantity of energy transferred as heat. This presupposition is essential but is explicitly labeled neither as a law of thermodynamics nor as an axiom of the Carathéodory way. In fact, the actual physical existence of such adiabatic processes is indeed mostly supposition, and those supposed processes have in most cases not been actually verified empirically to exist.\n\nReferring to conduction, Partington writes: \"If a hot body is brought in conducting contact with a cold body, the temperature of the hot body falls and that of the cold body rises, and it is said that a \"quantity of heat\" has passed from the hot body to the cold body.\"\n\nReferring to radiation, Maxwell writes: \"In Radiation, the hotter body loses heat, and the colder body receives heat by means of a process occurring in some intervening medium which does not itself thereby become hot.\"\n\nMaxwell writes that convection as such \"is not a purely thermal phenomenon\". In thermodynamics, convection in general is regarded as transport of internal energy. If, however, the convection is enclosed and circulatory, then it may be regarded as an intermediary that transfers energy as heat between source and destination bodies, because it transfers only energy and not matter from the source to the destination body.\n\nIn accordance with the first law for closed systems, energy transferred solely as heat leaves one body and enters another, changing the internal energies of each. Transfer, between bodies, of energy as work is a complementary way of changing internal energies. Though it is not logically rigorous from the viewpoint of strict physical concepts, a common form of words that expresses this is to say that heat and work are interconvertible.\n\nCyclically operating engines, that use only heat and work transfers, have two thermal reservoirs, a hot and a cold one. They may be classified by the range of operating temperatures of the working body, relative to those reservoirs. In a heat engine, the working body is at all times colder than the hot reservoir and hotter than the cold reservoir. In a sense, it uses heat transfer to produce work. In a heat pump, the working body, at stages of the cycle, goes both hotter than the hot reservoir, and colder than the cold reservoir. In a sense, it uses work to produce heat transfer.\n\nIn classical thermodynamics, a commonly considered model is the heat engine. It consists of four bodies: the working body, the hot reservoir, the cold reservoir, and the work reservoir. A cyclic process leaves the working body in an unchanged state, and is envisaged as being repeated indefinitely often. Work transfers between the working body and the work reservoir are envisaged as reversible, and thus only one work reservoir is needed. But two thermal reservoirs are needed, because transfer of energy as heat is irreversible. A single cycle sees energy taken by the working body from the hot reservoir and sent to the two other reservoirs, the work reservoir and the cold reservoir. The hot reservoir always and only supplies energy and the cold reservoir always and only receives energy. The second law of thermodynamics requires that no cycle can occur in which no energy is received by the cold reservoir. Heat engines achieve higher efficiency when the difference between initial and final temperature is greater.\n\nAnother commonly considered model is the heat pump or refrigerator. Again there are four bodies: the working body, the hot reservoir, the cold reservoir, and the work reservoir. A single cycle starts with the working body colder than the cold reservoir, and then energy is taken in as heat by the working body from the cold reservoir. Then the work reservoir does work on the working body, adding more to its internal energy, making it hotter than the hot reservoir. The hot working body passes heat to the hot reservoir, but still remains hotter than the cold reservoir. Then, by allowing it to expand without doing work on another body and without passing heat to another body, the working body is made colder than the cold reservoir. It can now accept heat transfer from the cold reservoir to start another cycle.\n\nThe device has transported energy from a colder to a hotter reservoir, but this is not regarded as by an inanimate agency; rather, it is regarded as by the harnessing of work . This is because work is supplied from the work reservoir, not just by a simple thermodynamic process, but by a cycle of thermodynamic operations and processes, which may be regarded as directed by an animate or harnessing agency. Accordingly, the cycle is still in accord with the second law of thermodynamics. The efficiency of a heat pump is best when the temperature difference between the hot and cold reservoirs is least.\n\nFunctionally, such engines are used in two ways, distinguishing a target reservoir and a resource or surrounding reservoir. A heat pump transfers heat, to the hot reservoir as the target, from the resource or surrounding reservoir. A refrigerator transfers heat, from the cold reservoir as the target, to the resource or surrounding reservoir. The target reservoir may be regarded as leaking: when the target leaks hotness to the surroundings, heat pumping is used; when the target leaks coldness to the surroundings, refrigeration is used. The engines harness work to overcome the leaks.\n\nAccording to Planck, there are three main conceptual approaches to heat. One is the microscopic or kinetic theory approach. The other two are macroscopic approaches. One is the approach through the law of conservation of energy taken as prior to thermodynamics, with a mechanical analysis of processes, for example in the work of Helmholtz. This mechanical view is taken in this article as currently customary for thermodynamic theory. The other macroscopic approach is the thermodynamic one, which admits heat as a primitive concept, which contributes, by scientific induction to knowledge of the law of conservation of energy. This view is widely taken as the practical one, quantity of heat being measured by calorimetry.\n\nBailyn also distinguishes the two macroscopic approaches as the mechanical and the thermodynamic. The thermodynamic view was taken by the founders of thermodynamics in the nineteenth century. It regards quantity of energy transferred as heat as a primitive concept coherent with a primitive concept of temperature, measured primarily by calorimetry. A calorimeter is a body in the surroundings of the system, with its own temperature and internal energy; when it is connected to the system by a path for heat transfer, changes in it measure heat transfer. The mechanical view was pioneered by Helmholtz and developed and used in the twentieth century, largely through the influence of Max Born. It regards quantity of heat transferred as heat as a derived concept, defined for closed systems as quantity of heat transferred by mechanisms other than work transfer, the latter being regarded as primitive for thermodynamics, defined by macroscopic mechanics. According to Born, the transfer of internal energy between open systems that accompanies transfer of matter \"cannot be reduced to mechanics\". It follows that there is no well-founded definition of quantities of energy transferred as heat or as work associated with transfer of matter.\n\nNevertheless, for the thermodynamical description of non-equilibrium processes, it is desired to consider the effect of a temperature gradient established by the surroundings across the system of interest when there is no physical barrier or wall between system and surroundings, that is to say, when they are open with respect to one another. The impossibility of a mechanical definition in terms of work for this circumstance does not alter the physical fact that a temperature gradient causes a diffusive flux of internal energy, a process that, in the thermodynamic view, might be proposed as a candidate concept for transfer of energy as heat.\n\nIn this circumstance, it may be expected that there may also be active other drivers of diffusive flux of internal energy, such as gradient of chemical potential which drives transfer of matter, and gradient of electric potential which drives electric current and iontophoresis; such effects usually interact with diffusive flux of internal energy driven by temperature gradient, and such interactions are known as cross-effects.\n\nIf cross-effects that result in diffusive transfer of internal energy were also labeled as heat transfers, they would sometimes violate the rule that pure heat transfer occurs only down a temperature gradient, never up one. They would also contradict the principle that all heat transfer is of one and the same kind, a principle founded on the idea of heat conduction between closed systems. One might to try to think narrowly of heat flux driven purely by temperature gradient as a conceptual component of diffusive internal energy flux, in the thermodynamic view, the concept resting specifically on careful calculations based on detailed knowledge of the processes and being indirectly assessed. In these circumstances, if perchance it happens that no transfer of matter is actualized, and there are no cross-effects, then the thermodynamic concept and the mechanical concept coincide, as if one were dealing with closed systems. But when there is transfer of matter, the exact laws by which temperature gradient drives diffusive flux of internal energy, rather than being exactly knowable, mostly need to be assumed, and in many cases are practically unverifiable. Consequently, when there is transfer of matter, the calculation of the pure 'heat flux' component of the diffusive flux of internal energy rests on practically unverifiable assumptions. This is a reason to think of heat as a specialized concept that relates primarily and precisely to closed systems, and applicable only in a very restricted way to open systems.\n\nIn many writings in this context, the term \"heat flux\" is used when what is meant is therefore more accurately called diffusive flux of internal energy; such usage of the term \"heat flux\" is a residue of older and now obsolete language usage that allowed that a body may have a \"heat content\".\n\nIn the kinetic theory, heat is explained in terms of the microscopic motions and interactions of constituent particles, such as electrons, atoms, and molecules. The immediate meaning of the kinetic energy of the constituent particles is not as heat. It is as a component of internal energy. \nIn microscopic terms, heat is a transfer quantity, and is described by a transport theory, not as steadily localized kinetic energy of particles. Heat transfer arises from temperature gradients or differences, through the diffuse exchange of microscopic kinetic and potential particle energy, by particle collisions and other interactions. An early and vague expression of this was made by Francis Bacon. Precise and detailed versions of it were developed in the nineteenth century.\n\nIn statistical mechanics, for a closed system (no transfer of matter), heat is the energy transfer associated with a disordered, microscopic action on the system, associated with jumps in occupation numbers of the energy levels of the system, without change in the values of the energy levels themselves. It is possible for macroscopic thermodynamic work to alter the occupation numbers without change in the values of the system energy levels themselves, but what distinguishes transfer as heat is that the transfer is entirely due to disordered, microscopic action, including radiative transfer. A mathematical definition can be formulated for small increments of quasi-static adiabatic work in terms of the statistical distribution of an ensemble of microstates.\n\nQuantity of heat transferred can be measured by calorimetry, or determined through calculations based on other quantities.\n\nCalorimetry is the empirical basis of the idea of quantity of heat transferred in a process. The transferred heat is measured by changes in a body of known properties, for example, temperature rise, change in volume or length, or phase change, such as melting of ice.\n\nA calculation of quantity of heat transferred can rely on a hypothetical quantity of energy transferred as adiabatic work and on the first law of thermodynamics. Such calculation is the primary approach of many theoretical studies of quantity of heat transferred.\n\nThe discipline of heat transfer, typically considered an aspect of mechanical engineering and chemical engineering, deals with specific applied methods by which thermal energy in a system is generated, or converted, or transferred to another system. Although the definition of heat implicitly means the transfer of energy, the term \"heat transfer\" encompasses this traditional usage in many engineering disciplines and laymen language.\n\n\"Heat transfer\" is generally described as including the mechanisms of heat conduction, heat convection, thermal radiation, but may include mass transfer and heat in processes of phase changes.\n\nConvection may be described as the combined effects of conduction and fluid flow. From the thermodynamic point of view, heat flows into a fluid by diffusion to increase its energy, the fluid then transfers (advects) this increased internal energy (not heat) from one location to another, and this is then followed by a second thermal interaction which transfers heat to a second body or system, again by diffusion. This entire process is often regarded as an additional mechanism of heat transfer, although technically, \"heat transfer\" and thus heating and cooling occurs only on either end of such a conductive flow, but not as a result of flow. Thus, conduction can be said to \"transfer\" heat only as a net result of the process, but may not do so at every time within the complicated convective process.\n\nIn an 1847 lecture entitled \"On Matter, Living Force, and Heat\", James Prescott Joule characterized the terms latent heat and sensible heat as components of heat each affecting distinct physical phenomena, namely the potential and kinetic energy of particles, respectively. \nHe described latent energy as the energy possessed via a distancing of particles where attraction was over a greater distance, i.e. a form of potential energy, and the sensible heat as an energy involving the motion of particles, i.e. kinetic energy.\n\nLatent heat is the heat released or absorbed by a chemical substance or a thermodynamic system during a change of state that occurs without a change in temperature. Such a process may be a phase transition, such as the melting of ice or the boiling of water. \n\n\"Heat capacity\" is a measurable physical quantity equal to the ratio of the heat added to an object to the resulting temperature change. The \"molar heat capacity\" is the heat capacity per unit amount (SI unit: mole) of a pure substance, and the \"specific heat capacity\", often called simply \"specific heat\", is the heat capacity per unit mass of a material. Heat capacity is a physical property of a substance, which means that it depends on the state and properties of the substance under consideration.\n\nThe specific heats of monatomic gases, such as helium, are nearly constant with temperature. Diatomic gases such as hydrogen display some temperature dependence, and triatomic gases (e.g., carbon dioxide) still more.\n\nBefore the development of the laws of thermodynamics, heat was measured by changes in the states of the participating bodies.\n\nSome general rules, with important exceptions, can be stated as follows.\n\nIn general, most bodies expand on heating. In this circumstance, heating a body at a constant volume increases the pressure it exerts on its constraining walls, while heating at a constant pressure increases its volume.\n\nBeyond this, most substances have three ordinarily recognized states of matter, solid, liquid, and gas. Some can also exist in a plasma. Many have further, more finely differentiated, states of matter, such as for example, glass, and liquid crystal. In many cases, at fixed temperature and pressure, a substance can exist in several distinct states of matter in what might be viewed as the same 'body'. For example, ice may float in a glass of water. Then the ice and the water are said to constitute two phases within the 'body'. Definite rules are known, telling how distinct phases may coexist in a 'body'. Mostly, at a fixed pressure, there is a definite temperature at which heating causes a solid to melt or evaporate, and a definite temperature at which heating causes a liquid to evaporate. In such cases, cooling has the reverse effects.\n\nAll of these, the commonest cases, fit with a rule that heating can be measured by changes of state of a body. Such cases supply what are called \"thermometric bodies\", that allow the definition of empirical temperatures. Before 1848, all temperatures were defined in this way. There was thus a tight link, apparently logically determined, between heat and temperature, though they were recognized as conceptually thoroughly distinct, especially by Joseph Black in the later eighteenth century.\n\nThere are important exceptions. They break the obviously apparent link between heat and temperature. They make it clear that empirical definitions of temperature are contingent on the peculiar properties of particular thermometric substances, and are thus precluded from the title 'absolute'. For example, water contracts on being heated near 277 K. It cannot be used as a thermometric substance near that temperature. Also, over a certain temperature range, ice contracts on heating. Moreover, many substances can exist in metastable states, such as with negative pressure, that survive only transiently and in very special conditions. Such facts, sometimes called 'anomalous', are some of the reasons for the thermodynamic definition of absolute temperature.\n\nIn the early days of measurement of high temperatures, another factor was important, and used by Josiah Wedgwood in his pyrometer. The temperature reached in a process was estimated by the shrinkage of a sample of clay. The higher the temperature, the more the shrinkage. This was the only available more or less reliable method of measurement of temperatures above 1000 °C. But such shrinkage is irreversible. The clay does not expand again on cooling. That is why it could be used for the measurement. But only once. It is not a thermometric material in the usual sense of the word.\n\nNevertheless, the thermodynamic definition of absolute temperature does make essential use of the concept of heat, with proper circumspection.\n\nAccording to Denbigh (1981), the property of hotness is a concern of thermodynamics that should be defined without reference to the concept of heat. Consideration of hotness leads to the concept of empirical temperature. All physical systems are capable of heating or cooling others. With reference to hotness, the comparative terms hotter and colder are defined by the rule that heat flows from the hotter body to the colder.\n\nIf a physical system is inhomogeneous or very rapidly or irregularly changing, for example by turbulence, it may be impossible to characterize it by a temperature, but still there can be transfer of energy as heat between it and another system. If a system has a physical state that is regular enough, and persists long enough to allow it to reach thermal equilibrium with a specified thermometer, then it has a temperature according to that thermometer. An empirical thermometer registers degree of hotness for such a system. Such a temperature is called empirical. For example, Truesdell writes about classical thermodynamics: \"At each time, the body is assigned a real number called the \"temperature\". This number is a measure of how hot the body is.\"\n\nPhysical systems that are too turbulent to have temperatures may still differ in hotness. A physical system that passes heat to another physical system is said to be the hotter of the two. More is required for the system to have a thermodynamic temperature. Its behavior must be so regular that its empirical temperature is the same for all suitably calibrated and scaled thermometers, and then its hotness is said to lie on the one-dimensional hotness manifold. This is part of the reason why heat is defined following Carathéodory and Born, solely as occurring other than by work or transfer of matter; temperature is advisedly and deliberately not mentioned in this now widely accepted definition.\n\nThis is also the reason that the zeroth law of thermodynamics is stated explicitly. If three physical systems, \"A\", \"B\", and \"C\" are each not in their own states of internal thermodynamic equilibrium, it is possible that, with suitable physical connections being made between them, \"A\" can heat \"B\" and \"B\" can heat \"C\" and \"C\" can heat \"A\". In non-equilibrium situations, cycles of flow are possible. It is the special and uniquely distinguishing characteristic of internal thermodynamic equilibrium that this possibility is not open to thermodynamic systems (as distinguished amongst physical systems) which are in their own states of internal thermodynamic equilibrium; this is the reason why the zeroth law of thermodynamics needs explicit statement. That is to say, the relation 'is not colder than' between general non-equilibrium physical systems is not transitive, whereas, in contrast, the relation 'has no lower a temperature than' between thermodynamic systems in their own states of internal thermodynamic equilibrium is transitive. It follows from this that the relation 'is in thermal equilibrium with' is transitive, which is one way of stating the zeroth law.\n\nJust as temperature may be undefined for a sufficiently inhomogeneous system, so also may entropy be undefined for a system not in its own state of internal thermodynamic equilibrium. For example, 'the temperature of the solar system' is not a defined quantity. Likewise, 'the entropy of the solar system' is not defined in classical thermodynamics. It has not been possible to define non-equilibrium entropy, as a simple number for a whole system, in a clearly satisfactory way.\n\n\n\n"}
{"id": "10069680", "url": "https://en.wikipedia.org/wiki?curid=10069680", "title": "Hot and cold cognition", "text": "Hot and cold cognition\n\nHot cognition is a hypothesis on motivated reasoning in which a person's thinking is influenced by their emotional state. Put simply, hot cognition is cognition coloured by emotion.<ref name=\"Brand1985/1986\"></ref> Hot cognition contrasts with cold cognition, which implies cognitive processing of information that is independent of emotional involvement. Hot cognition is proposed to be associated with cognitive and physiological arousal, in which a person is more responsive to environmental factors. As it is automatic, rapid and led by emotion, hot cognition may consequently cause biased and low-quality decision making. Hot cognition may arise, with varying degrees of strength, in politics, religion, and other sociopolitical contexts because of moral issues, which are inevitably tied to emotion. Hot cognition was initially proposed in 1963 by Robert P. Abelson. This idea became popular in the 1960s and the 1970s. An example of a biased decision caused by hot cognition would be a juror disregarding evidence because of an attraction to the defendant.\n\nDecision making with cold cognition is more likely to involve logic and critical analysis. Therefore, when an individual engages in a task when displaying cold cognition, the stimuli is likely to be emotionally neutral and the \"outcome of the test is not motivationally relevant\" to the individual. An example of a critical decision using cold cognition would be concentrating on the evidence before drawing a conclusion.\n\nHot and cold cognition form a dichotomy within executive functioning. Executive functioning has long been considered as a domain general cognitive function, but there has been support for separation into \"hot\" affective aspects and \"cold\" cognitive aspects. It is recognized that executive functioning spans across a number of cognitive tasks including working memory, cognitive flexibility and reasoning in active goal pursuit. The distinction between hot and cool cognition implies that executive function may operate differently in different contexts. The distinction has been applied to research in cognitive psychology, developmental psychology, clinical psychology, social psychology, neuropsychology, and other areas of study in psychology.\n\nPerformance on hot and cold tasks improves most rapidly during the preschool years, but continues into adolescence. This co-occurs with both structural and functional development associated with the prefrontal cortex. Specific areas within the PFC are thought to be associated with both hot and cold cognition. Hot cognition is likely to be utilized during tasks that require the regulation of emotion or motivation, as well as reevaluating the motivational significance of a stimulus. The ventral and medial areas of the prefrontal cortex (VM-PFC) are implicated during these tasks. Cold cognition is thought to be associated with executive functions elicited by abstract, deconceptualized tasks, such as card sorting. The area of the brain that is utilized for these tasks is the dorsolateral prefrontal cortex (DL-PFC). It is between the ages of 3 years and 5 years that the most significant change in task completion is seen. Age-related trends have been observed in tasks used to measure hot cognition, as well as cold cognition. However, the age at which children reach adult-like functioning varies. It appears as though children take longer to fully develop hot executive functioning than cold. This lends support to the idea that hot cognition may follow a separate, and perhaps delayed, developmental trajectory as opposed to cold cognition. Further research done on these neurological areas suggests there may be some plasticity during the development of both hot and cold cognition. While the preschool years are ones of extreme sensitivity to the development of prefrontal cortex, a similar period is found in the transition into adolescence. This gives rise to the idea that there may be a possible time window for intervention training, which would improve cognitive abilities and executive functioning in children and adolescents.\n\nThis section explains the most common tasks that are used to measure hot and cold cognitive functioning. The cool tasks are neutrally affective and measure executive function abilities such as cognitive flexibility and working memory. In other words, there is nothing to be gained or lost by performing these tasks. The hot tasks also measure executive function, but these tasks result in emotionally significant consequences.\n\nIn the Iowa gambling task participants are initially given $2,000 facsimile dollars and asked to win as much money as possible. They are presented with four decks of cards that represent either a gain or loss in money. One card from each deck is drawn at a time. Consistently choosing a card from the advantageous decks results in a net gain, whereas choosing from a disadvantageous deck results in a net loss. Each card from the disadvantageous deck offers a higher reward than the advantageous deck, but also a higher and more variable loss.\n\nStudies have been conducted on the concept of delay of gratification to test whether or not people are capable of waiting to receive a reward in order to increase the value of the reward. In these experiments, participants can choose to either take the reward they are immediately presented with or can choose to wait a period of time to then receive a higher valued reward. Hot cognition would motivate people to immediately satisfy their craving for the present reward rather than waiting for a better reward.\n\nThe influence that beliefs can have on logical reasoning may vary as a result of emotions during cognitive processes. When presented with neutral content, this will typically lead to the exhibition of the belief-bias effect. In contrast, content that is emotionally charged will result in a diminished likelihood of beliefs having an influence. The impact of negative emotions demonstrates the capability they have for altering the process underlying logical reasoning. There is an interaction that occurs between emotions and beliefs that interferes with the ability that an individual has to reason.\n\nThe cool tasks are neutrally affective and measure executive function abilities such as cognitive flexibility and working memory. In other words, there is nothing to be gained or lost by performing these tasks. The hot tasks also measure executive function, but these tasks result in emotionally significant consequences.\n\nIn this task an array of items is presented to participants. The position of these items then randomly changes from trial to trial. Participants are instructed to point to one of these items, but then asked to not point to that same item again. In order to perform well on this task, participants must remember what item they pointed to and use this information to decide on subsequent responses.\n\nThe Wisconsin Card Sort Task requires participants to sort stimulus cards that differ in dimensions (shape, colour, or number). However, they are not told how to sort them. The only feedback they receive is whether or not a match is correct. Participants must discover the rule according to dimension. Once the participant matches a certain number of correct cards, the dimension changes and they must rediscover the new rule. This requires participants to remember the rule they were using and cognitively change the rule by which they use to sort.\n\nParticipants are required to sort stimulus cards based on either shape or colour. They are first instructed to sort based on one dimension (colour) in a trial, and then it switches to the other (shape) in the following trial. \"Switch\" trials are also used where the participant must change back and forth between rules within a single trial. Unlike the WCST, the rule is explicitly stated and does not have to be inferred. The task measures how flexible participants are to changing rules. This requires participants to shift between dimensions of sorting.\n\nResearch has demonstrated emotional manipulations on decision making processes. Participants who are induced with enthusiasm, anger or distress (different specific emotions) responded in different ways to the risky-choice problems, demonstrating that hot cognition, as an automatic process, affects decision making differently. Another example of hot cognition is a better predictor of negative emotional arousal as compared to cold cognition when they have a personal investment, such as wanting your team to win. In addition, hot cognition changes the way people use decision-making strategies, depending on the type of mood they are in, positive or negative. When people are in a positive mood, they tend to use compensatory, holistic strategies. This leads to a shallow and broad processing of information. In a negative mood people employ non-compensatory, narrow strategies which leads to a more detail-oriented and thorough processing of information. In the study participants were shown movie clips in order to induce a mood of happiness, anger or sadness and asked to complete a decision-making task. Researchers found that participants in the negative mood condition used more non-compensatory, specific decision-making techniques by focusing on the details of the situation. Participants in the positive mood condition used more compensatory, broad decision making techniques by focusing on the bigger picture of the situation. Also, hot cognition has been implicated in automatic processing and autobiographical memory. Furthermore, hot cognition extends outside the laboratory as exhibited in political process and criminal judgments. When police officers were induced with sadness they were more likely to think the suspect was guilty. However, if police officers were induced with anger there was no difference in judgments. There are also clinical implications for understanding certain disorders. Patients diagnosed with anorexia nervosa went through intervention training, which included hot cognition as a part of emotional processing development, did not show any improvement after this training. In another clinical population, those diagnosed with bipolar disorder exaggerated their perception of negative feedback and were less likely to adjust their decision making process in the face of risky-choices (gambling tasks).\n"}
{"id": "5382986", "url": "https://en.wikipedia.org/wiki?curid=5382986", "title": "Hydrogen storage", "text": "Hydrogen storage\n\nMethods of hydrogen storage for subsequent use span many approaches including high pressures, cryogenics, and chemical compounds that reversibly release H upon heating. Underground hydrogen storage is useful to provide grid energy storage for intermittent energy sources, like wind power, as well as providing fuel for transportation, particularly for ships and airplanes.\n\nMost research into hydrogen storage is focused on storing hydrogen as a lightweight, compact energy carrier for mobile applications.\n\nLiquid hydrogen or slush hydrogen may be used, as in the Space Shuttle. However liquid hydrogen requires cryogenic storage and boils around 20.268 K (−252.882 °C or −423.188 °F). Hence, its liquefaction imposes a large energy loss (as energy is needed to cool it down to that temperature). The tanks must also be well insulated to prevent boil off but adding insulation increases cost. Liquid hydrogen has less energy density \"by volume\" than hydrocarbon fuels such as gasoline by approximately a factor of four. This highlights the density problem for pure hydrogen: there is actually about 64% more hydrogen in a liter of gasoline (116 grams hydrogen) than there is in a liter of pure liquid hydrogen (71 grams hydrogen). The carbon in the gasoline also contributes to the energy of combustion.\n\nCompressed hydrogen, by comparison, is stored quite differently. Hydrogen gas has good energy density by weight, but poor energy density by volume versus hydrocarbons, hence it requires a larger tank to store. A large hydrogen tank will be heavier than the small hydrocarbon tank used to store the same amount of energy, all other factors remaining equal. Increasing gas pressure would improve the energy density by volume, making for smaller, but not lighter container tanks (see hydrogen tank). Compressed hydrogen costs 2.1% of the energy content to power the compressor. Higher compression without energy recovery will mean more energy lost to the compression step. Compressed hydrogen storage can exhibit very low permeation.\n\nCompressed hydrogen is a storage form where hydrogen gas is kept under pressures to increase the storage density. Compressed hydrogen in hydrogen tanks at 350 bar (5,000 psi) and 700 bar (10,000 psi) is used for hydrogen tank systems in vehicles, based on type IV carbon-composite technology. Car manufacturers have been developing this solution, such as Honda or Nissan.\n\nBMW has been working on liquid hydrogen tanks for cars, producing for example the BMW Hydrogen 7. Japan have liquid hydrogen (LH2) storage at a tanker port in Kobe, and are anticipated to receive the first shipment of liquid hydrogen via LH2 carrier in 2020. Hydrogen is liquified by reducing its temperature to -253°C, similar to liquified natural gas (LNG) which is stored at -162°C. A potential efficiency loss of 12.79% can be achieved, or 4.26kWh/kg out of 33.3kWh/kg.\n\nHydrogen storage technologies can be divided into physical storage, where hydrogen molecules are stored (including pure hydrogen storage via compression and liquefaction), and chemical storage, where hydrides are stored.\n\nChemical storage could offer high storage performance due to the strong binding of hydrogen and the high storage densities. However, the regeneration of storage material is still an issue. A large number of chemical storage systems are under investigation, which involve hydrolysis reactions, hydrogenation/dehydrogenation reactions, ammonia borane and other boron hydrides, ammonia, and alane etc. Storage in hydrocarbons may also be successful in overcoming the issue with low density. For example, supercritical hydrogen at 30 °C and 500 bar only has a density of 15.0 mol/L while methanol has a density of 49.5 mol H/L methanol and saturated dimethyl ether at 30 °C and 7 bar has a density of 42.1 mol H/L dimethyl ether. These liquids would use much smaller, cheaper, safer storage tanks. The most promising chemical approach is electrochemical hydrogen storage, as the release of hydrogen can be controlled by the applied electricity. Most of the materials listed below can be directly used for electrochemical hydrogen storage.\n\nMetal hydrides, such as MgH, NaAlH, LiAlH, LiH, LaNiH, TiFeH and palladium hydride, with varying degrees of efficiency, can be used as a storage medium for hydrogen, often reversibly. Some are easy-to-fuel liquids at ambient temperature and pressure, others are solids which could be turned into pellets. These materials have good energy density, although their specific energy is often worse than the leading hydrocarbon fuels.\n\nMost metal hydrides bind with hydrogen very strongly. As a result, high temperatures around 120 °C (248 °F) – 200 °C (392 °F) are required to release their hydrogen content. This energy cost can be reduced by using alloys which consists of a strong hydride former and a weak one such as in LiNH, LiBH and NaBH. These are able to form weaker bonds, thereby requiring less input to release stored hydrogen. However, if the interaction is too weak, the pressure needed for rehydriding is high, thereby eliminating any energy savings. The target for onboard hydrogen fuel systems is roughly <100 °C for release and <700 bar for recharge (20–60 kJ/mol H).\n\nAn alternative method for reducing dissociation temperatures is doping with activators. This has been successfully used for aluminium hydride but its complex synthesis makes it undesirable for most applications as it is not easily recharged with hydrogen.\n\nCurrently the only hydrides which are capable of achieving the 9 wt% gravimetric goal for 2015 (see chart above) are limited to lithium, boron and aluminium based compounds; at least one of the second-row elements or Al must be added. Research is being done to determine new compounds which can be used to meet these requirements.\n\nProposed hydrides for use in a hydrogen economy include simple hydrides of magnesium or transition metals and complex metal hydrides, typically containing sodium, lithium, or calcium and aluminium or boron. Hydrides chosen for storage applications provide low reactivity (high safety) and high hydrogen storage densities. Leading candidates are lithium hydride, sodium borohydride, lithium aluminium hydride and ammonia borane. A French company McPhy Energy is developing the first industrial product, based on magnesium hydride, already sold to some major clients such as Iwatani and ENEL.\n\n\"New Scientist\" reported that Arizona State University is investigating using a borohydride solution to store hydrogen, which is released when the solution flows over a catalyst made of ruthenium. Researchers at University of Pittsburgh and Georgia Tech performed extensive benchmarking simulations on mixtures of several light metal hydrides to predict possible reaction thermodynamics for hydrogen storage.\n\nThe Italian catalyst manufacturer Acta has proposed using hydrazine as an alternative to hydrogen in fuel cells. As the hydrazine fuel is liquid at room temperature, it can be handled and stored more easily than hydrogen. By storing it in a tank full of a double-bonded carbon-oxygen carbonyl, it reacts and forms a safe solid called hydrazone. By then flushing the tank with warm water, the liquid hydrazine hydrate is released. Hydrazine breaks down in the cell to form nitrogen and hydrogen which bonds with oxygen, releasing water. Silicon hydrides and germanium hydrides are also candidates of hydrogen storage materials, as they can subject to energetically favored reaction to form covalently bonded dimers with loss of a hydrogen molecule. \n\nCarbohydrates (polymeric CHO) releases H in a bioreformer mediated by the enzyme cocktail—cell-free synthetic pathway biotransformation. Carbohydrate provides high hydrogen storage densities as a liquid with mild pressurization and cryogenic constraints: It can also be stored as a solid powder. Carbohydrate is the most abundant renewable bioresource in the world.\n\nIn May 2007 biochemical engineers from the Virginia Polytechnic Institute and State University and biologists and chemists from the Oak Ridge National Laboratory announced a method of producing high-yield pure hydrogen from starch and water. In 2009, they demonstrated to produce nearly 12 moles of hydrogen per glucose unit from cellulosic materials and water. Thanks to complete conversion and modest reaction conditions, they propose to use carbohydrate as a high energy density hydrogen carrier with a density of 14.8 wt%.\n\nAn alternative to hydrides is to use regular hydrocarbon fuels as the hydrogen carrier. Then a small hydrogen reformer would extract the hydrogen as needed by the fuel cell. However, these reformers are slow to react to changes in demand and add a large incremental cost to the vehicle powertrain.\n\nDirect methanol fuel cells do not require a reformer, but provide a lower energy density compared to conventional fuel cells, although this could be counterbalanced with the much better energy densities of ethanol and methanol over hydrogen. Alcohol fuel is a renewable resource.\n\nSolid-oxide fuel cells can operate on light hydrocarbons such as propane and methane without a reformer, or can run on higher hydrocarbons with only partial reforming, but the high temperature and slow startup time of these fuel cells are problematic for automotive applications.\n\nAluminum has been proposed as an energy storage method by a number of researchers. hydrogen can be extracted from aluminum by reacting it with water. To react with water, however, aluminum must be stripped of its natural oxide layer, a process which requires pulverization, chemical reactions with caustic substances, or alloys. The byproduct of the reaction to create hydrogen is aluminum oxide, which can be recycled back into aluminum with the Hall–Héroult process, making the reaction theoretically renewable.\n\nUnsaturated organic compounds can store huge amounts of hydrogen. These \"Liquid Organic Hydrogen Carriers\" (LOHC) are hydrogenated for storage and dehydrogenated again when the energy/hydrogen is needed. Research on LOHC was concentrated on cycloalkanes at an early stage, with its relatively high hydrogen capacity (6-8 wt %) and production of CO-free hydrogen. Heterocyclic aromatic compounds (or N-Heterocycles) are also appropriate for this task. A compound that stands in the focus of the current LOHC research is N-ethylcarbazole (NEC) but many others do exist. More recently dibenzyltoluene, which is already industrially used as a heat transfer fluid in industry, was identified as potential LOHC. With a wide liquid range between -39 °C (melting point) and 390 °C (boiling point) and a hydrogen storage density of 6.2 wt% dibenzyltoluene is ideally suited as LOHC material. More recently, formic acid (FA) has been suggested as a promising hydrogen storage material with a 4.4wt% hydrogen capacity.\n\nUsing LOHCs relatively high gravimetric storage densities can be reached (about 6 wt-%) and the overall energy efficiency is higher than for other chemical storage options such as producing methane from the hydrogen.\nCycloalkanes reported as LOHC include cyclohexane, methyl-cyclohexane and decalin. The dehydrogenation of cycloalkanes is highly endothermic (63-69 kJ/mol H), which means this process requires high temperature. Dehydrogenation of decalin is the most thermodynamically favored among the three cycloalkanes, and methyl-cyclohexane is second because of the presence of the methyl group. Research on catalyst development for dehydrogenation of cycloalkanes has been carried out for decades. Nickel (Ni), Molybdenum (Mo) and Platinum (Pt) based catalysts are highly investigated for dehydrogenation. However, coking is still a big challenge for catalyst’s long-term stability.\nBoth hydrogenation and dehydrogenation of LOHCs requires catalysts. It was demonstrated that replacing hydrocarbons by hetero-atoms, like N, O etc. improves reversible de/hydrogenation properties. The temperature required for hydrogenation and dehydrogenation of drops significantly with increasing numbers of heteroatoms. Among all the N-heterocycles, the saturated-unsaturated pair of dodecahydro-N-ethylcarbazole (12H-NEC) and NEC has been considered as a promising candidate for hydrogen storage with a fairly large hydrogen content (5.8wt%). The figure on the top right shows dehydrogenation and hydrogenation of the 12H-NEC and NEC pair. The standard catalyst for NEC to 12H-NEC is Ru and Rh based. The selectivity of hydrogenation can reach 97% at 7 MPa and 130 °C-150 °C. Although N-Heterocyles can optimize the unfavorable thermodynamic properties of cycloalkanes, a lot of issues remain unsolved, such as high cost, high toxicity and kinetic barriers etc.\nIn 2006 researchers of EPFL, Switzerland, reported the use of formic acid as a hydrogen storage material. Carbon monoxide free hydrogen has been generated in a very wide pressure range (1–600 bar). A homogeneous catalytic system based on water-soluble ruthenium catalysts selectively decompose HCOOH into H and CO in aqueous solution. This catalytic system overcomes the limitations of other catalysts (e.g. poor stability, limited catalytic lifetimes, formation of CO) for the decomposition of formic acid making it a viable hydrogen storage material. And the co-product of this decomposition, carbon dioxide, can be used as hydrogen vector by hydrogenating it back to formic acid in a second step. The catalytic hydrogenation of CO has long been studied and efficient procedures have been developed. Formic acid contains 53 g L hydrogen at room temperature and atmospheric pressure. By weight, pure formic acid stores 4.3 wt% hydrogen. Pure formic acid is a liquid with a flash point 69 °C (cf. gasoline −40 °C, ethanol 13 °C). 85% formic acid is not flammable.\n\nAmmonia (NH) releases H in an appropriate catalytic reformer. Ammonia provides high hydrogen storage densities as a liquid with mild pressurization and cryogenic constraints: It can also be stored as a liquid at room temperature and pressure when mixed with water. Ammonia is the second most commonly produced chemical in the world and a large infrastructure for making, transporting, and distributing ammonia exists. Ammonia can be reformed to produce hydrogen with no harmful waste, or can mix with existing fuels and under the right conditions burn efficiently. Since there is no carbon in ammonia, no carbon by-products are produced; thereby making this possibility a \"carbon neutral\" option for the future. Pure ammonia burns poorly at the atmospheric pressures found in natural gas fired water heaters and stoves. Under compression in an automobile engine it is a suitable fuel for slightly modified gasoline engines. Ammonia is the suitable alternative fuel because it has 18.6 MJ/kg energy density at NTP and carbon-free combustion byproducts. However, ammonia is a toxic gas at normal temperature and pressure and has a potent odor.\n\nIn 2018, researchers at CSIRO in Australia powered a Toyota Mirai and Hyundai Nexo with hydrogen separated from ammonia using a membrane technology. \n\nIn September 2005 chemists from the Technical University of Denmark announced a method of storing hydrogen in the form of ammonia saturated into a salt tablet. They claim it will be an inexpensive and safe storage method.\n\nPrior to 1980, several compounds were investigated for hydrogen storage including complex borohydrides, or aluminohydrides, and ammonium salts. These hydrides have an upper theoretical hydrogen yield limited to about 8.5% by weight. Amongst the compounds that contain only B, N, and H (both positive and negative ions), representative examples include: amine boranes, boron hydride ammoniates, hydrazine-borane complexes, and ammonium octahydrotriborates or tetrahydroborates. Of these, amine boranes (and especially ammonia borane) have been extensively investigated as hydrogen carriers. During the 1970s and 1980s, the U.S. Army and Navy funded efforts aimed at developing hydrogen/deuterium gas-generating compounds for use in the HF/DF and HCl chemical lasers, and gas dynamic lasers. Earlier hydrogen gas-generating formulations used amine boranes and their derivatives. Ignition of the amine borane(s) forms boron nitride (BN) and hydrogen gas. In addition to ammonia borane\n(HBNH), other gas-generators include diborane diammoniate, HB(NH)BH.\n\nIn 2007 Dupont and others reported hydrogen-storage materials based on imidazolium ionic liquids. Simple alkyl(aryl)-3-methylimidazolium N-bis(trifluoromethanesulfonyl)imidate salts that possess very low vapour pressure, high density, and thermal stability and are not inflammable can add reversibly 6–12 hydrogen atoms in the presence of classical Pd/C or Ir0 nanoparticle catalysts and can be used as alternative materials for on-board hydrogen-storage devices. These salts can hold up to 30 g L of hydrogen at atmospheric pressure.\n\nIn 2006 researchers of University of Windsor reported on reversible hydrogen storage in a non-metal phosphonium borate frustrated Lewis pair:\n\nThe phosphino-borane on the left accepts one equivalent of hydrogen at one atmosphere and 25 °C and expels it again by heating to 100 °C. The storage capacity is 0.25 wt% still rather below the 6 to 9 wt% required for practical use.\n\nResearch has proven that graphene can store hydrogen efficiently. After taking up hydrogen, the substance becomes graphane. After tests, conducted by dr André Geim at the University of Manchester, it was shown that not only can graphene store hydrogen easily, it can also release the hydrogen again, after heating to 450 °C.\n\nMetal-organic frameworks represent another class of synthetic porous materials that store hydrogen and energy at the molecular level. MOFs are highly crystalline inorganic-organic hybrid structures that contain metal clusters or ions (secondary building units) as nodes and organic ligands as linkers. When guest molecules (solvent) occupying the pores are removed during solvent exchange and heating under vacuum, porous structure of MOFs can be achieved without destabilizing the frame and hydrogen molecules will be adsorbed onto the surface of the pores by physisorption. Compared to traditional zeolites and porous carbon materials, MOFs have very high number of pores and surface area which allow higher hydrogen uptake in a given volume. Thus, research interests on hydrogen storage in MOFs have been growing since 2003 when the first MOF-based hydrogen storage was introduced. Since there are infinite geometric and chemical variations of MOFs based on different combinations of SBUs and linkers, many researches explore what combination will provide the maximum hydrogen uptake by varying materials of metal ions and linkers.\n\nIn 2006, chemists at UCLA and the University of Michigan have achieved hydrogen storage concentrations of up to 7.5 wt% in MOF-74 at a low temperature of 77 K. In 2009, researchers at University of Nottingham reached 10 wt% at 77 bar (1,117 psi) and 77 K with MOF NOTT-112. Most articles about hydrogen storage in MOFs report hydrogen uptake capacity at a temperature of 77K and a pressure of 1 bar because these conditions are commonly available and the binding energy between hydrogen and the MOF at this temperature is large compared to the thermal vibration energy. Varying several factors such as surface area, pore size, catenation, ligand structure, and sample purity can result in different amounts of hydrogen uptake in MOFs.\n\nCella Energy technology is based around the encapsulation of hydrogen gas and nano-structuring of chemical hydrides in small plastic balls, at room temperature and pressure.\n\nIn this case hydrogen remains in physical forms, i.e., as gas, supercritical fluid, adsorbate, or molecular inclusions. Theoretical limitations and experimental results are considered \nconcerning the volumetric and gravimetric capacity of glass microvessels, microporous, and nanoporous media, as well as safety and refilling-time demands.\n\nActivated carbons are highly porous amorphous carbon materials with high apparent surface area. Hydrogen physisorption can be increased in these materials by increasing the apparent surface area and optimizing pore diameter to around 7 Å. These materials are of particular interest due to the fact that they can be made from waste materials, such as cigarette butts which have shown great potential as precursor materials for high-capacity hydrogen storage materials.\n\nCryo-compressed storage of hydrogen is the only technology that meets 2015 DOE targets for volumetric and gravimetric efficiency (see \"CcH2\" on slide 6 in ).\n\nFurthermore, another study has shown that cryo-compressed exhibits interesting cost advantages: ownership cost (price per mile) and storage system cost (price per vehicle) are actually the lowest when compared to any other technology (see third row in slide 13 of ). For example, a cryo-compressed hydrogen system would cost $0.12 per mile (including cost of fuel and every associated other cost), while conventional gasoline vehicles cost between $0.05 and $0.07 per mile.\n\nLike liquid storage, cryo-compressed uses cold hydrogen (20.3 K and slightly above) in order to reach a high energy density. However, the main difference is that, when the hydrogen would warm-up due to heat transfer with the environment (\"boil off\"), the tank is allowed to go to pressures much higher (up to 350 bars versus a couple of bars for liquid storage). As a consequence, it takes more time before the hydrogen has to vent, and in most driving situations, enough hydrogen is used by the car to keep the pressure well below the venting limit.\n\nConsequently, it has been demonstrated that a high driving range could be achieved with a cryo-compressed tank : more than were driven with a full tank mounted on an hydrogen-fueled engine of Toyota Prius. Research is still on its way in order to study and demonstrate the full potential of the technology.\n\nAs of 2010, the BMW Group has started a thorough component and system level validation of cryo-compressed vehicle storage on its way to a commercial product.\n\nHydrogen carriers based on nanostructured carbon (such as carbon buckyballs and nanotubes) have been proposed. However, since Hydrogen usually amounts up to ~3.0-7.0 wt% at 77K which is far from the value set by US department of Energy (6 wt% at nearly ambient conditions), it makes carbon materials poor candidates for hydrogen storage.\n\nH caged in a clathrate hydrate was first reported in 2002, but requires very high pressures to be stable. In 2004, researchers from Delft University of Technology and Colorado School of Mines showed solid H-containing hydrates could be formed at ambient temperature and 10s of bar by adding small amounts of promoting substances such as THF. These clathrates have a theoretical maximum hydrogen densities of around 5 wt% and 40 kg/m.\n\nA team of Russian, Israeli and German scientists have collaboratively developed an innovative technology based on glass capillary arrays for the safe infusion, storage and controlled release of hydrogen in mobile applications. The C.En technology has achieved the United States Department of Energy (DOE) 2010 targets for on-board hydrogen storage systems.\nDOE 2015 targets can be achieved using flexible glass capillaries and cryo-compressed method of hydrogen storage.\n\nHollow glass microspheres (HGM) can be utilized for controlled storage and release of hydrogen.\n\nUnlike mobile applications, hydrogen density is not a huge problem for stationary applications. As for mobile applications, stationary applications can use established technology:\n\nUnderground hydrogen storage is the practice of hydrogen storage in underground caverns, salt domes and depleted oil and gas fields. Large quantities of gaseous hydrogen have been stored in underground caverns by ICI for many years without any difficulties. The storage of large quantities of liquid hydrogen underground can function as grid energy storage. The round-trip efficiency is approximately 40% (vs. 75-80% for pumped-hydro (PHES)), and the cost is slightly higher than pumped hydro, if only a limited number of hours of storage is required. Another study referenced by a European staff working paper found that for large scale storage, the cheapest option is hydrogen at €140/MWh for 2,000 hours of storage using an electrolyser, salt cavern storage and combined-cycle power plant. The European project Hyunder indicated in 2013 that for the storage of wind and solar energy an additional 85 caverns are required as it cannot be covered by PHES and CAES systems. A German case study on storage of hydrogen in salt caverns found that if the German power surplus (7% of total variable renewable generation by 2025 and 20% by 2050) would be converted to hydrogen and stored underground, these quantities would require some 15 caverns of 500,000 cubic metres each by 2025 and some 60 caverns by 2050 – corresponding to approximately one third of the number of underground gas caverns currently operated in Germany. In the US, Sandia Labs are conducting research into the storage of hydrogen in depleted oil and gas fields, which could easily absorb large amounts of renewably produced hydrogen as there are some 2.7 million depleted wells in existence.\n\nPower to gas is a technology which converts electrical power to a gas fuel. There are two methods: the first is to use the electricity for water splitting and inject the resulting hydrogen into the natural gas grid; the second, less efficient method is used to convert carbon dioxide and hydrogen to methane, (see natural gas) using electrolysis and the Sabatier reaction. A third option is to combine the hydrogen via electrolysis with a source of carbon (either carbon dioxide or carbon monoxide from biogas, from industrial processes or via direct air-captured carbon dioxide) via biomethanation, where biomethanogens (archaea) consume carbon dioxide and hydrogen and produce methane within an anaerobic environment. This process is highly efficient, as the archaea are self-replicating and only require low-grade (60°C) heat to perform the reaction.\n\nAnother process has also been achieved by SoCalGas to convert the carbon dioxide in raw biogas to methane in a single electrochemical step, representing a simpler method of converting excess renewable electricity into storable natural gas. \n\nThe UK has completed surveys and is preparing to start injecting hydrogen into the gas grid as the grid previously carried 'town gas' which is a 50% hydrogen-methane gas formed from coal. Auditors KPMG found that converting the UK to hydrogen gas could be £150bn to £200bn cheaper than rewiring British homes to use electric heating powered by lower-carbon sources.\n\nExcess power or off peak power generated by wind generators or solar arrays can then be used for load balancing in the energy grid. Using the existing natural gas system for hydrogen, Fuel cell maker Hydrogenics and natural gas distributor Enbridge have teamed up to develop such a power to gas system in Canada.\n\nPipeline storage of hydrogen where a natural gas network is used for the storage of hydrogen. Before switching to natural gas, the German gas networks were operated using towngas, which for the most part (60-65%) consisted of hydrogen. The storage capacity of the German natural gas network is more than 200,000 GW·h which is enough for several months of energy requirement. By comparison, the capacity of all German pumped storage power plants amounts to only about 40 GW·h. The transport of energy through a gas network is done with much less loss (<0.1%) than in a power network (8%). The use of the existing natural gas pipelines for hydrogen was studied by NaturalHy\n\nTargets were set by the FreedomCAR Partnership in January 2002 between the United States Council for Automotive Research (USCAR) and U.S. DOE (Targets assume a 5-kg H storage system). The 2005 targets were not reached in 2005. The targets were revised in 2009 to reflect new data on system efficiencies obtained from fleets of test cars. The ultimate goal for volumetric storage is still above the theoretical density of liquid hydrogen.\n\nIt is important to note that these targets are for the hydrogen storage system, not the hydrogen storage material. System densities are often around half those of the working material, thus while a material may store 6 wt% H, a working system using that material may only achieve 3 wt% when the weight of tanks, temperature and pressure control equipment, etc., is considered.\n\nIn 2010, only two storage technologies were identified as having the potential to meet DOE targets: MOF-177 exceeds 2010 target for volumetric capacity, while cryo-compressed H exceeds more restrictive 2015 targets for both gravimetric and volumetric capacity (see slide 6 in ).\n\n\n"}
{"id": "342684", "url": "https://en.wikipedia.org/wiki?curid=342684", "title": "Inclusion–exclusion principle", "text": "Inclusion–exclusion principle\n\nIn combinatorics (combinatorial mathematics), the inclusion–exclusion principle is a counting technique which generalizes the familiar method of obtaining the number of elements in the union of two finite sets; symbolically expressed as\nwhere \"A\" and \"B\" are two finite sets and |\"S\"| indicates the cardinality of a set \"S\" (which may be considered as the number of elements of the set, if the set is finite). The formula expresses the fact that the sum of the sizes of the two sets may be too large since some elements may be counted twice. The double-counted elements are those in the intersection of the two sets and the count is corrected by subtracting the size of the intersection.\n\nThe principle is more clearly seen in the case of three sets, which for the sets \"A\", \"B\" and \"C\" is given by \nThis formula can be verified by counting how many times each region in the Venn diagram figure is included in the right-hand side of the formula. In this case, when removing the contributions of over-counted elements, the number of elements in the mutual intersection of the three sets has been subtracted too often, so must be added back in to get the correct total.\n\nGeneralizing the results of these examples gives the principle of inclusion–exclusion. To find the cardinality of the union of sets:\n\n\nThe name comes from the idea that the principle is based on over-generous \"inclusion\", followed by compensating \"exclusion\".\nThis concept is attributed to Abraham de Moivre (1718); but it first appears in a paper of Daniel da Silva (1854), and later in a paper by J. J. Sylvester (1883). Sometimes the principle is referred to as the formula of Da Silva, or Sylvester due to these publications. The principle is an example of the sieve method extensively used in number theory and is sometimes referred to as the \"sieve formula\", though Legendre already used a similar device in a sieve context in 1808.\n\nAs finite probabilities are computed as counts relative to the cardinality of the probability space, the formulas for the principle of inclusion–exclusion remain valid when the cardinalities of the sets are replaced by finite probabilities. More generally, both versions of the principle can be put under the common umbrella of measure theory.\n\nIn a very abstract setting, the principle of inclusion–exclusion can be expressed as the calculation of the inverse of a certain matrix. This inverse has a special structure, making the principle an extremely valuable technique in combinatorics and related areas of mathematics. As Gian-Carlo Rota put it:\n\n\"One of the most useful principles of enumeration in discrete probability and combinatorial theory is the celebrated principle of inclusion–exclusion. When skillfully applied, this principle has yielded the solution to many a combinatorial problem.\"\n\nIn its general form, the principle of inclusion–exclusion states that for finite sets , one has the identity\n\nThis can be compactly written as\nor\nSubstitute\n\non the right hand side of (***). Notice that formula_7 appears once on both sides of (***). So we must show that for all formula_8 with formula_9, the terms formula_10 cancel out on the right hand side of (***). For that purpose, take a fixed formula_8 such that formula_9 and take an arbitrary fixed formula_13 such that formula_14.\n\nNotice that formula_5 must be a set for each positive or negative appearance of formula_10 on the right hand side of (***) that is obtained by way of the multiset formula_17 such that formula_18. Now each appearance of formula_10 on the right hand side of (***) that is obtained by way of formula_17 such that formula_5 is a set that contains formula_22 cancels out with the one that is obtained by way of the corresponding formula_17 such that formula_5 is a set that does not contain formula_22. This gives the desired result.\n\nThe inclusion–exclusion principle is widely used and only a few of its applications can be mentioned here.\n\nA well-known application of the inclusion–exclusion principle is to the combinatorial problem of counting all derangements of a finite set. A \"derangement\" of a set \"A\" is a bijection from \"A\" into itself that has no fixed points. Via the inclusion–exclusion principle one can show that if the cardinality of \"A\" is \"n\", then the number of derangements is [\"n\"! / \"e\"] where [\"x\"] denotes the nearest integer to \"x\"; a detailed proof is available here and also see the examples section above.\n\nThe first occurrence of the problem of counting the number of derangements is in an early book on games of chance: \"Essai d'analyse sur les jeux de hazard\" by P. R. de Montmort (1678 – 1719) and was known as either \"Montmort's problem\" or by the name he gave it, \"\"problème des rencontres\".\" The problem is also known as the \"hatcheck problem.\"\n\nThe number of derangements is also known as the subfactorial of \"n\", written !\"n\". It follows that if all bijections are assigned the same probability then the probability that a random bijection is a derangement quickly approaches 1/\"e\" as \"n\" grows.\n\nThe principle of inclusion–exclusion, combined with De Morgan's law, can be used to count the cardinality of the intersection of sets as well. Let formula_26 represent the complement of \"A\" with respect to some universal set \"A\" such that formula_27 for each \"k\". Then we have\n\nthereby turning the problem of finding an intersection into the problem of finding a union.\n\nThe inclusion exclusion principle forms the basis of algorithms for a number of NP-hard graph partitioning problems, such as graph coloring.\n\nA well known application of the principle is the construction of the chromatic polynomial of a graph.\n\nThe number of perfect matchings of a bipartite graph can be calculated using the principle.\n\nGiven finite sets \"A\" and \"B\", how many surjective functions (onto functions) are there from \"A\" to \"B\"? Without any loss of generality we may take \"A\" = {1,2...,\"k\"} and \"B\" = {1,2...,\"n\"}, since only the cardinalities of the sets matter. By using \"S\" as the set of all functions from \"A\" to \"B\", and defining, for each \"i\" in \"B\", the property \"P\" as \"the function misses the element \"i\" in \"B\"\" (\"i\" is not in the image of the function), the principle of inclusion–exclusion gives the number of onto functions between \"A\" and \"B\" as:\n\nA permutation of the set \"S\" = {1,2...,\"n\"} where each element of \"S\" is restricted to not being in certain positions (here the permutation is considered as an ordering of the elements of \"S\") is called a \"permutation with forbidden positions\". For example, with \"S\" = {1,2,3,4}, the permutations with the restriction that the element 1 can not be in positions 1 or 3, and the element 2 can not be in position 4 are: 2134, 2143, 3124, 4123, 2341, 2431, 3241, 3421, 4231 and 4321. By letting \"A\" be the set of positions that the element \"i\" is not allowed to be in, and the property \"P\" to be the property that a permutation puts element \"i\" into a position in \"A\", the principle of inclusion–exclusion can be used to count the number of permutations which satisfy all the restrictions.\n\nIn the given example, there are 12 = 2(3!) permutations with property \"P\", 6 = 3! permutations with property \"P\" and no permutations have properties \"P\" or \"P\" as there are no restrictions for these two elements. The number of permutations satisfying the restrictions is thus:\nThe final 4 in this computation is the number of permutations having both properties \"P\" and \"P\". There are no other non-zero contributions to the formula.\n\nThe Stirling numbers of the second kind, \"S\"(\"n\",\"k\") count the number of partitions of a set of \"n\" elements into \"k\" non-empty subsets (indistinguishable \"boxes\"). An explicit formula for them can be obtained by applying the principle of inclusion–exclusion to a very closely related problem, namely, counting the number of partitions of an \"n\"-set into \"k\" non-empty but distinguishable boxes (ordered non-empty subsets). Using the universal set consisting of all partitions of the \"n\"-set into \"k\" (possibly empty) distinguishable boxes, \"A\", \"A\", ..., \"A\", and the properties \"P\" meaning that the partition has box \"A\" empty, the principle of inclusion–exclusion gives an answer for the related result. Dividing by \"k\"<nowiki>!</nowiki> to remove the artificial ordering gives the Stirling number of the second kind:\n\nA rook polynomial is the generating function of the number of ways to place non-attacking rooks on a \"board B\" that looks like a subset of the squares of a checkerboard; that is, no two rooks may be in the same row or column. The board \"B\" is any subset of the squares of a rectangular board with \"n\" rows and \"m\" columns; we think of it as the squares in which one is allowed to put a rook. The coefficient, \"r\"(\"B\") of \"x\" in the rook polynomial \"R\"(\"x\") is the number of ways \"k\" rooks, none of which attacks another, can be arranged in the squares of \"B\". For any board \"B\", there is a complementary board \"B' \" consisting of the squares of the rectangular board that are not in \"B\". This complementary board also has a rook polynomial \"R\"(\"x\") with coefficients \"r\"(\"B\"').\n\nIt is sometimes convenient to be able to calculate the highest coefficient of a rook polynomial in terms of the coefficients of the rook polynomial of the complementary board. Without loss of generality we can assume that \"n\" ≤ \"m\", so this coefficient is \"r\"(\"B\"). The number of ways to place \"n\" non-attacking rooks on the complete \"n\" × \"m\" \"checkerboard\" (without regard as to whether the rooks are placed in the squares of the board \"B\") is given by the falling factorial:\nLetting \"P\" be the property that an assignment of \"n\" non-attacking rooks on the complete board has a rook in column \"i\" which is not in a square of the board \"B\", then by the principle of inclusion–exclusion we have:\n\nEuler's totient or phi function, \"φ\"(\"n\") is an arithmetic function that counts the number of positive integers less than or equal to \"n\" that are relatively prime to \"n\". That is, if \"n\" is a positive integer, then φ(\"n\") is the number of integers \"k\" in the range 1 ≤ \"k\" ≤ \"n\" which have no common factor with \"n\" other than 1. The principle of inclusion–exclusion is used to obtain a formula for φ(\"n\"). Let \"S\" be the set {1,2...,\"n\"} and define the property \"P\" to be that a number in \"S\" is divisible by the prime number \"p\", for 1 ≤ \"i\" ≤ \"r\", where the prime factorization of\n\nThen,\n\nIn many cases where the principle could give an exact formula (in particular, counting prime numbers using the sieve of Eratosthenes), the formula arising doesn't offer useful content because the number of terms in it is excessive. If each term individually can be estimated accurately, the accumulation of errors may imply that the inclusion–exclusion formula isn't directly applicable. In number theory, this difficulty was addressed by Viggo Brun. After a slow start, his ideas were taken up by others, and a large variety of sieve methods developed. These for example may try to find upper bounds for the \"sieved\" sets, rather than an exact formula.\n\nLet \"A\", ..., \"A\" be arbitrary sets and \"p\", ..., \"p\" real numbers in the closed unit interval <nowiki>[</nowiki>0,1<nowiki>]</nowiki>. Then, for every even number \"k\" in {0, ..., \"n\"}, the indicator functions satisfy the inequality:\n\nChoose an element contained in the union of all sets and let formula_36 be the individual sets containing it. (Note that \"t\" > 0.) Since the element is counted precisely once by the left-hand side of equation (), we need to show that it is counted precisely once by the right-hand side. On the right-hand side, the only non-zero contributions occur when all the subsets in a particular term contain the chosen element, that is, all the subsets are selected from formula_36. The contribution is one for each of these sets (plus or minus depending on the term) and therefore is just the (signed) number of these subsets used in the term. We then have: \n\nBy the binomial theorem,\n\nUsing the fact that formula_40 and rearranging terms, we have\n\nand so, the chosen element is counted only once by the right-hand side of equation ().\n\nAn algebraic proof can be obtained using indicator functions (characteristic functions of subsets of a set). The indicator function of a subset \"S\" of a set \"X\" is a function\n\ndefined as\n\nIf formula_44 and formula_45 are two subsets of formula_46, then \n\nLet \"A\" denote the union formula_48 of the sets \"A\", ..., \"A\". To prove the inclusion–exclusion principle in general, we first verify the identity\n\nfor indicator functions, where\n\nThe following function is identically zero\n\nbecause: if \"x\" is not in \"A\", then all factors are 0 − 0 = 0; and otherwise, if \"x\" does belong to some \"A\", then the corresponding \"m\"th factor is 1 − 1 = 0. By expanding the product on the left-hand side, equation (∗) follows.\n\nTo prove the inclusion–exclusion principle for the cardinality of sets, sum the equation (∗) over all \"x\" in the union of \"A\", ..., \"A\". To derive the version used in probability, take the expectation in (∗). In general, integrate the equation (∗) with respect to \"μ\". Always use linearity in these derivations.\n\n\n"}
{"id": "41431266", "url": "https://en.wikipedia.org/wiki?curid=41431266", "title": "Ishvaratva", "text": "Ishvaratva\n\nIshvaratva in Sanskrit language is an abstract noun meaning 'godhood', it also means divinity.\n\nPurushottama (the Lord) conceals and also manifests the qualities at His will, He conceals his qualities like \"Ananda\" ('bliss') and \"Ishvaratva\" ('Lordship') in the \"Jivas\" ('Individual Souls') and also conceals His quality of Consciousness in this material world.\n\nThe \"Chidabhasa\" which constitutes \"Ishvaratva\" is almost an exact likeness of true consciousness on account of its being associated with \"Prakrti\" in equilibrium and consequently unperturbed by the \"gunas\" in action. He is \"Saguna Brahman\" whilst true consciousness is \"Nirguna Brahman\".\n\n\"Ishvaratva\" is only from the standpoint of \"Jivatva\". Both, \"Ishvaratva\" and \"Jivatva\", are the apparent modifications of the Atman or Brahman. Though of mutually opposed qualities they are denoted by word \"tvam\", the \"Atman\" as qualified by the mental states such as 'waking', 'dream' and 'dreamless sleep\". The Mahavakya, Tat Tvam Asi affirms the identity between Brahman, Jiva and Ishvara (Vivekachudamani 243-244).\n\nSelf-luminosity means being directly cognizable without dependence on anything else; and being different from that is \"hetu\" ('proximal or concomitant cause'). The assumed difference between Brahman that is cognized and the Brahman that cognizes is imaginary (\"kalpanika\") because in reality there is no difference. The assumed difference between Brahman on the one hand and \"Jiva\" and \"Ishvara\" on the other is not based on luminosity but on other \"dharmas\" (\"jivatva\" and \"ishvaratva\") (Advaita-siddhi 22-23).\n\n\"Ishvaratva\" is due to the \"Upadhi\" of \"Avidya\". By the \"Upadhis\" that are \"avidyatmaka\", \"attatvika\" and \"kalpanika\" by creating divisions in the divisionless and partless Brahman when in reality no divisions whatsoever exist. Sankara in his Bhashya on Brahma Sutra 2.1.14 explains that name and form constitute the seeds of the entire expanse of phenomenal existence, and which are conjured up by nescience. The omniscient God i.e. Brahman, who diversifies the seed (Shvetashvatara Upanishad VI.12), who manifests names and forms (Chandogya Upanishad VI.iii.2) and creates all forms, gives them names (and entering into them) (Taittirya Aranyaka III.xii.7), is different from them.\n\nThe sage of the Mandukya Upanishad partitioning the symbol Aum in three different morae adds a fourth mora-less part corresponding to which there are three different states of consciousness, corresponding to which, again, are different kinds of soul and posits \"the four states of consciousness – wakefulness, the dream, sleep and a fourth name-less state of consciousness (turiya) while teaching that there is an aspect of the Godhead corresponding to these states of consciousness, the last alone being ultimately real. The Absolute of philosophy surpasses even such a theological conception as that of God.\" It is only to those who regard the Universal Being as immanent in their own Selves, to them belongs eternal happiness, to no one else (Shvetashvatara Upanishad VI.12).\n"}
{"id": "17524", "url": "https://en.wikipedia.org/wiki?curid=17524", "title": "Language", "text": "Language\n\nLanguage is a system that consists of the development, acquisition, maintenance and use of complex systems of communication, particularly the human ability to do so; and a language is any specific example of such a system.\n\nThe scientific study of language is called linguistics. Questions concerning the philosophy of language, such as whether words can represent experience, have been debated at least since Gorgias and Plato in ancient Greece. Thinkers such as Rousseau have argued that language originated from emotions while others like Kant have held that it originated from rational and logical thought. 20th-century philosophers such as Wittgenstein argued that philosophy is really the study of language. Major figures in linguistics include Ferdinand de Saussure and Noam Chomsky.\n\nEstimates of the number of human languages in the world vary between 5,000 and 7,000. However, any precise estimate depends on a partly arbitrary distinction between languages and dialects. Natural languages are spoken or signed, but any language can be encoded into secondary media using auditory, visual, or tactile stimuli – for example, in whistling, signed, or braille. This is because human language is modality-independent. Depending on philosophical perspectives regarding the definition of language and meaning, when used as a general concept, \"language\" may refer to the cognitive ability to learn and use systems of complex communication, or to describe the set of rules that makes up these systems, or the set of utterances that can be produced from those rules. All languages rely on the process of semiosis to relate signs to particular meanings. Oral, manual and tactile languages contain a phonological system that governs how symbols are used to form sequences known as words or morphemes, and a syntactic system that governs how words and morphemes are combined to form phrases and utterances.\n\nHuman language has the properties of productivity and displacement, and relies entirely on social convention and learning. Its complex structure affords a much wider range of expressions than any known system of animal communication. Language is thought to have originated when early hominins started gradually changing their primate communication systems, acquiring the ability to form a theory of other minds and a shared intentionality. This development is sometimes thought to have coincided with an increase in brain volume, and many linguists see the structures of language as having evolved to serve specific communicative and social functions. Language is processed in many different locations in the human brain, but especially in Broca's and Wernicke's areas. Humans acquire language through social interaction in early childhood, and children generally speak fluently by approximately three years old. The use of language is deeply entrenched in human culture. Therefore, in addition to its strictly communicative uses, language also has many social and cultural uses, such as signifying group identity, social stratification, as well as social grooming and entertainment.\n\nLanguages evolve and diversify over time, and the history of their evolution can be reconstructed by comparing modern languages to determine which traits their ancestral languages must have had in order for the later developmental stages to occur. A group of languages that descend from a common ancestor is known as a language family. The Indo-European family is the most widely spoken and includes languages as diverse as English, Russian and Hindi; the Sino-Tibetan family includes Mandarin, Bodo and the other Chinese languages, and Tibetan; the Afro-Asiatic family includes Arabic, Somali, and Hebrew; the Bantu languages include Swahili, and Zulu, and hundreds of other languages spoken throughout Africa; and the Malayo-Polynesian languages include Indonesian, Malay, Tagalog, and hundreds of other languages spoken throughout the Pacific. The languages of the Dravidian family, spoken mostly in Southern India, include Tamil Telugu and Kannada. Academic consensus holds that between 50% and 90% of languages spoken at the beginning of the 21st century will probably have become extinct by the year 2100.\n\nThe English word \"language\" derives ultimately from Proto-Indo-European \"\" \"tongue, speech, language\" through Latin \"lingua\", \"language; tongue\", and Old French \"language\". The word is sometimes used to refer to codes, ciphers, and other kinds of artificially constructed communication systems such as formally defined computer languages used for computer programming. Unlike conventional human languages, a formal language in this sense is a system of signs for encoding and decoding information. This article specifically concerns the properties of natural human language as it is studied in the discipline of linguistics.\n\nAs an object of linguistic study, \"language\" has two primary meanings: an abstract concept, and a specific linguistic system, e.g. \"French\". The Swiss linguist Ferdinand de Saussure, who defined the modern discipline of linguistics, first explicitly formulated the distinction using the French word \"langage\" for language as a concept, \"langue\" as a specific instance of a language system, and \"parole\" for the concrete usage of speech in a particular language.\n\nWhen speaking of language as a general concept, definitions can be used which stress different aspects of the phenomenon. These definitions also entail different approaches and understandings of language, and they also inform different and often incompatible schools of linguistic theory. Debates about the nature and origin of language go back to the ancient world. Greek philosophers such as Gorgias and Plato debated the relation between words, concepts and reality. Gorgias argued that language could represent neither the objective experience nor human experience, and that communication and truth were therefore impossible. Plato maintained that communication is possible because language represents ideas and concepts that exist independently of, and prior to, language.\n\nDuring the Enlightenment and its debates about human origins, it became fashionable to speculate about the origin of language. Thinkers such as Rousseau and Herder argued that language had originated in the instinctive expression of emotions, and that it was originally closer to music and poetry than to the logical expression of rational thought. Rationalist philosophers such as Kant and Descartes held the opposite view. Around the turn of the 20th century, thinkers began to wonder about the role of language in shaping our experiences of the world – asking whether language simply reflects the objective structure of the world, or whether it creates concepts that it in turn imposes on our experience of the objective world. This led to the question of whether philosophical problems are really firstly linguistic problems. The resurgence of the view that language plays a significant role in the creation and circulation of concepts, and that the study of philosophy is essentially the study of language, is associated with what has been called the linguistic turn and philosophers such as Wittgenstein in 20th-century philosophy. These debates about language in relation to meaning and reference, cognition and consciousness remain active today.\n\nOne definition sees language primarily as the mental faculty that allows humans to undertake linguistic behaviour: to learn languages and to produce and understand utterances. This definition stresses the universality of language to all humans, and it emphasizes the biological basis for the human capacity for language as a unique development of the human brain. Proponents of the view that the drive to language acquisition is innate in humans argue that this is supported by the fact that all cognitively normal children raised in an environment where language is accessible will acquire language without formal instruction. Languages may even develop spontaneously in environments where people live or grow up together without a common language; for example, creole languages and spontaneously developed sign languages such as Nicaraguan Sign Language. This view, which can be traced back to the philosophers Kant and Descartes, understands language to be largely innate, for example, in Chomsky's theory of Universal Grammar, or American philosopher Jerry Fodor's extreme innatist theory. These kinds of definitions are often applied in studies of language within a cognitive science framework and in neurolinguistics.\n\nAnother definition sees language as a formal system of signs governed by grammatical rules of combination to communicate meaning. This definition stresses that human languages can be described as closed structural systems consisting of rules that relate particular signs to particular meanings. This structuralist view of language was first introduced by Ferdinand de Saussure, and his structuralism remains foundational for many approaches to language.\n\nSome proponents of Saussure's view of language have advocated a formal approach which studies language structure by identifying its basic elements and then by presenting a formal account of the rules according to which the elements combine in order to form words and sentences. The main proponent of such a theory is Noam Chomsky, the originator of the generative theory of grammar, who has defined language as the construction of sentences that can be generated using transformational grammars. Chomsky considers these rules to be an innate feature of the human mind and to constitute the rudiments of what language is. By way of contrast, such transformational grammars are also commonly used to provide formal definitions of language are commonly used in formal logic, in formal theories of grammar, and in applied computational linguistics. In the philosophy of language, the view of linguistic meaning as residing in the logical relations between propositions and reality was developed by philosophers such as Alfred Tarski, Bertrand Russell, and other formal logicians.\n\nYet another definition sees language as a system of communication that enables humans to exchange verbal or symbolic utterances. This definition stresses the social functions of language and the fact that humans use it to express themselves and to manipulate objects in their environment. Functional theories of grammar explain grammatical structures by their communicative functions, and understand the grammatical structures of language to be the result of an adaptive process by which grammar was \"tailored\" to serve the communicative needs of its users.\n\nThis view of language is associated with the study of language in pragmatic, cognitive, and interactive frameworks, as well as in sociolinguistics and linguistic anthropology. Functionalist theories tend to study grammar as dynamic phenomena, as structures that are always in the process of changing as they are employed by their speakers. This view places importance on the study of linguistic typology, or the classification of languages according to structural features, as it can be shown that processes of grammaticalization tend to follow trajectories that are partly dependent on typology. In the philosophy of language, the view of pragmatics as being central to language and meaning is often associated with Wittgenstein's later works and with ordinary language philosophers such as J.L. Austin, Paul Grice, John Searle, and W.O. Quine.\n\nA number of features, many of which were described by Charles Hockett and called design features set human language apart from other known systems of communication, such as those used by non-human animals.\n\nCommunication systems used by other animals such as bees or apes are closed systems that consist of a finite, usually very limited, number of possible ideas that can be expressed. In contrast, human language is open-ended and productive, meaning that it allows humans to produce a vast range of utterances from a finite set of elements, and to create new words and sentences. This is possible because human language is based on a dual code, in which a finite number of elements which are meaningless in themselves (e.g. sounds, letters or gestures) can be combined to form an infinite number of larger units of meaning (words and sentences). However, one study has demonstrated that an Australian bird, the chestnut-crowned babbler, is capable of using the same acoustic elements in different arrangements to create two functionally distinct vocalizations. Additionally, pied babblers have demonstrated the ability to generate two functionally distinct vocalisations composed of the same sound type, which can only be distinguished by the number of repeated elements.\n\nSeveral species of animals have proved to be able to acquire forms of communication through social learning: for instance a bonobo named Kanzi learned to express itself using a set of symbolic lexigrams. Similarly, many species of birds and whales learn their songs by imitating other members of their species. However, while some animals may acquire large numbers of words and symbols, none have been able to learn as many different signs as are generally known by an average 4 year old human, nor have any acquired anything resembling the complex grammar of human language.\n\nHuman languages also differ from animal communication systems in that they employ grammatical and semantic categories, such as noun and verb, present and past, which may be used to express exceedingly complex meanings. Human language is also unique in having the property of recursivity: for example, a noun phrase can contain another noun phrase (as in \"<nowiki>the chimpanzee]'s lips]</nowiki>\") or a clause can contain another clause (as in \"<nowiki>[I see [the dog is running</nowiki>\"). Human language is also the only known natural communication system whose adaptability may be referred to as \"modality independent\". This means that it can be used not only for communication through one channel or medium, but through several. For example, spoken language uses the auditive modality, whereas sign languages and writing use the visual modality, and braille writing uses the tactile modality.\n\nHuman language is also unique in being able to refer to abstract concepts and to imagined or hypothetical events as well as events that took place in the past or may happen in the future. This ability to refer to events that are not at the same time or place as the speech event is called \"displacement\", and while some animal communication systems can use displacement (such as the communication of bees that can communicate the location of sources of nectar that are out of sight), the degree to which it is used in human language is also considered unique.\n\nTheories about the origin of language differ in regard to their basic assumptions about what language is. Some theories are based on the idea that language is so complex that one cannot imagine it simply appearing from nothing in its final form, but that it must have evolved from earlier pre-linguistic systems among our pre-human ancestors. These theories can be called continuity-based theories. The opposite viewpoint is that language is such a unique human trait that it cannot be compared to anything found among non-humans and that it must therefore have appeared suddenly in the transition from pre-hominids to early man. These theories can be defined as discontinuity-based. Similarly, theories based on the generative view of language pioneered by Noam Chomsky see language mostly as an innate faculty that is largely genetically encoded, whereas functionalist theories see it as a system that is largely cultural, learned through social interaction.\n\nChomsky is one prominent proponent of a discontinuity-based theory of human language origins. He suggests that for scholars interested in the nature of language, \"talk about the evolution of the language capacity is beside the point.\" Chomsky proposes that perhaps \"some random mutation took place [...] and it reorganized the brain, implanting a language organ in an otherwise primate brain.\" Though cautioning against taking this story literally, Chomsky insists that \"it may be closer to reality than many other fairy tales that are told about evolutionary processes, including language.\"\n\nContinuity-based theories are held by a majority of scholars, but they vary in how they envision this development. Those who see language as being mostly innate, for example psychologist Steven Pinker, hold the precedents to be animal cognition, whereas those who see language as a socially learned tool of communication, such as psychologist Michael Tomasello, see it as having developed from animal communication in primates: either gestural or vocal communication to assist in cooperation. Other continuity-based models see language as having developed from music, a view already espoused by Rousseau, Herder, Humboldt, and Charles Darwin. A prominent proponent of this view is archaeologist Steven Mithen. Stephen Anderson states that the age of spoken languages is estimated at 60,000 to 100,000 years and that: Researchers on the evolutionary origin of language generally find it plausible to suggest that language was invented only once, and that all modern spoken languages are thus in some way related, even if that relation can no longer be recovered ... because of limitations on the methods available for reconstruction.\n\nBecause language emerged in the early prehistory of man, before the existence of any written records, its early development has left no historical traces, and it is believed that no comparable processes can be observed today. Theories that stress continuity often look at animals to see if, for example, primates display any traits that can be seen as analogous to what pre-human language must have been like. And early human fossils can be inspected for traces of physical adaptation to language use or pre-linguistic forms of symbolic behaviour. Among the signs in human fossils that may suggest linguistic abilities are: the size of the brain relative to body mass, the presence of a larynx capable of advanced sound production and the nature of tools and other manufactured artifacts.\n\nIt was mostly undisputed that pre-human australopithecines did not have communication systems significantly different from those found in great apes in general. However, a 2017 study on Ardipithecus ramidus challenges this belief. Scholarly opinions vary as to the developments since the appearance of the genus \"Homo\" some 2.5 million years ago. Some scholars assume the development of primitive language-like systems (proto-language) as early as \"Homo habilis\" (2.3 million years ago) while others place the development of primitive symbolic communication only with \"Homo erectus\" (1.8 million years ago) or \"Homo heidelbergensis\" (0.6 million years ago), and the development of language proper with Anatomically Modern \"Homo sapiens\" with the Upper Paleolithic revolution less than 100,000 years ago.\n\nThe study of language, linguistics, has been developing into a science since the first grammatical descriptions of particular languages in India more than 2000 years ago, after the development of the Brahmi script. Modern linguistics is a science that concerns itself with all aspects of language, examining it from all of the theoretical viewpoints described above.\n\nThe academic study of language is conducted within many different disciplinary areas and from different theoretical angles, all of which inform modern approaches to linguistics. For example, descriptive linguistics examines the grammar of single languages, theoretical linguistics develops theories on how best to conceptualize and define the nature of language based on data from the various extant human languages, sociolinguistics studies how languages are used for social purposes informing in turn the study of the social functions of language and grammatical description, neurolinguistics studies how language is processed in the human brain and allows the experimental testing of theories, computational linguistics builds on theoretical and descriptive linguistics to construct computational models of language often aimed at processing natural language or at testing linguistic hypotheses, and historical linguistics relies on grammatical and lexical descriptions of languages to trace their individual histories and reconstruct trees of language families by using the comparative method.\n\nThe formal study of language is often considered to have started in India with Pāṇini, the 5th century BC grammarian who formulated 3,959 rules of Sanskrit morphology. However, Sumerian scribes already studied the differences between Sumerian and Akkadian grammar around 1900 BC. Subsequent grammatical traditions developed in all of the ancient cultures that adopted writing.\n\nIn the 17th century AD, the French Port-Royal Grammarians developed the idea that the grammars of all languages were a reflection of the universal basics of thought, and therefore that grammar was universal. In the 18th century, the first use of the comparative method by British philologist and expert on ancient India William Jones sparked the rise of comparative linguistics. The scientific study of language was broadened from Indo-European to language in general by Wilhelm von Humboldt. Early in the 20th century, Ferdinand de Saussure introduced the idea of language as a static system of interconnected units, defined through the oppositions between them.\n\nBy introducing a distinction between diachronic and synchronic analyses of language, he laid the foundation of the modern discipline of linguistics. Saussure also introduced several basic dimensions of linguistic analysis that are still fundamental in many contemporary linguistic theories, such as the distinctions between syntagm and paradigm, and the Langue-parole distinction, distinguishing language as an abstract system (\"langue\"), from language as a concrete manifestation of this system (\"parole\").\n\nIn the 1960s, Noam Chomsky formulated the generative theory of language. According to this theory, the most basic form of language is a set of syntactic rules that is universal for all humans and which underlies the grammars of all human languages. This set of rules is called Universal Grammar; for Chomsky, describing it is the primary objective of the discipline of linguistics. Thus, he considered that the grammars of individual languages are only of importance to linguistics insofar as they allow us to deduce the universal underlying rules from which the observable linguistic variability is generated.\n\nIn opposition to the formal theories of the generative school, functional theories of language propose that since language is fundamentally a tool, its structures are best analyzed and understood by reference to their functions. Formal theories of grammar seek to define the different elements of language and describe the way they relate to each other as systems of formal rules or operations, while functional theories seek to define the functions performed by language and then relate them to the linguistic elements that carry them out. The framework of cognitive linguistics interprets language in terms of the concepts (which are sometimes universal, and sometimes specific to a particular language) which underlie its forms. Cognitive linguistics is primarily concerned with how the mind creates meaning through language.\n\nSpeaking is the default modality for language in all cultures. The production of spoken language depends on sophisticated capacities for controlling the lips, tongue and other components of the vocal apparatus, the ability to acoustically decode speech sounds, and the neurological apparatus required for acquiring and producing language. The study of the genetic bases for human language is at an early stage: the only gene that has definitely been implicated in language production is FOXP2, which may cause a kind of congenital language disorder if affected by mutations.\n\n The brain is the coordinating center of all linguistic activity; it controls both the production of linguistic cognition and of meaning and the mechanics of speech production. Nonetheless, our knowledge of the neurological bases for language is quite limited, though it has advanced considerably with the use of modern imaging techniques. The discipline of linguistics dedicated to studying the neurological aspects of language is called neurolinguistics.\n\nEarly work in neurolinguistics involved the study of language in people with brain lesions, to see how lesions in specific areas affect language and speech. In this way, neuroscientists in the 19th century discovered that two areas in the brain are crucially implicated in language processing. The first area is Wernicke's area, which is in the posterior section of the superior temporal gyrus in the dominant cerebral hemisphere. People with a lesion in this area of the brain develop receptive aphasia, a condition in which there is a major impairment of language comprehension, while speech retains a natural-sounding rhythm and a relatively normal sentence structure. The second area is Broca's area, in the posterior inferior frontal gyrus of the dominant hemisphere. People with a lesion to this area develop expressive aphasia, meaning that they know what they want to say, they just cannot get it out. They are typically able to understand what is being said to them, but unable to speak fluently. Other symptoms that may be present in expressive aphasia include problems with fluency, articulation, word-finding, word repetition, and producing and comprehending complex grammatical sentences, both orally and in writing. Those with this aphasia also exhibit ungrammatical speech and show inability to use syntactic information to determine the meaning of sentences. Both expressive and receptive aphasia also affect the use of sign language, in analogous ways to how they affect speech, with expressive aphasia causing signers to sign slowly and with incorrect grammar, whereas a signer with receptive aphasia will sign fluently, but make little sense to others and have difficulties comprehending others' signs. This shows that the impairment is specific to the ability to use language, not to the physiology used for speech production.\n\nWith technological advances in the late 20th century, neurolinguists have also incorporated non-invasive techniques such as functional magnetic resonance imaging (fMRI) and electrophysiology to study language processing in individuals without impairments.\n\nSpoken language relies on human physical ability to produce sound, which is a longitudinal wave propagated through the air at a frequency capable of vibrating the ear drum. This ability depends on the physiology of the human speech organs. These organs consist of the lungs, the voice box (larynx), and the upper vocal tract – the throat, the mouth, and the nose. By controlling the different parts of the speech apparatus, the airstream can be manipulated to produce different speech sounds.\n\nThe sound of speech can be analyzed into a combination of segmental and suprasegmental elements. The segmental elements are those that follow each other in sequences, which are usually represented by distinct letters in alphabetic scripts, such as the Roman script. In free flowing speech, there are no clear boundaries between one segment and the next, nor usually are there any audible pauses between words. Segments therefore are distinguished by their distinct sounds which are a result of their different articulations, and they can be either vowels or consonants. Suprasegmental phenomena encompass such elements as stress, phonation type, voice timbre, and prosody or intonation, all of which may have effects across multiple segments.\n\nConsonants and vowel segments combine to form syllables, which in turn combine to form utterances; these can be distinguished phonetically as the space between two inhalations. Acoustically, these different segments are characterized by different formant structures, that are visible in a spectrogram of the recorded sound wave (See illustration of Spectrogram of the formant structures of three English vowels). Formants are the amplitude peaks in the frequency spectrum of a specific sound.\n\nVowels are those sounds that have no audible friction caused by the narrowing or obstruction of some part of the upper vocal tract. They vary in quality according to the degree of lip aperture and the placement of the tongue within the oral cavity. Vowels are called \"close\" when the lips are relatively closed, as in the pronunciation of the vowel (English \"ee\"), or \"open\" when the lips are relatively open, as in the vowel (English \"ah\"). If the tongue is located towards the back of the mouth, the quality changes, creating vowels such as (English \"oo\"). The quality also changes depending on whether the lips are rounded as opposed to unrounded, creating distinctions such as that between (unrounded front vowel such as English \"ee\") and (rounded front vowel such as German \"ü\").\n\nConsonants are those sounds that have audible friction or closure at some point within the upper vocal tract. Consonant sounds vary by place of articulation, i.e. the place in the vocal tract where the airflow is obstructed, commonly at the lips, teeth, alveolar ridge, palate, velum, uvula, or glottis. Each place of articulation produces a different set of consonant sounds, which are further distinguished by manner of articulation, or the kind of friction, whether full closure, in which case the consonant is called \"occlusive\" or \"stop\", or different degrees of aperture creating \"fricatives\" and \"approximants\". Consonants can also be either \"voiced or unvoiced\", depending on whether the vocal cords are set in vibration by airflow during the production of the sound. Voicing is what separates English in \"bus\" (unvoiced sibilant) from in \"buzz\" (voiced sibilant).\n\nSome speech sounds, both vowels and consonants, involve release of air flow through the nasal cavity, and these are called \"nasals\" or \"nasalized\" sounds. Other sounds are defined by the way the tongue moves within the mouth: such as the l-sounds (called \"laterals\", because the air flows along both sides of the tongue), and the r-sounds (called \"rhotics\") that are characterized by how the tongue is positioned relative to the air stream.\n\nBy using these speech organs, humans can produce hundreds of distinct sounds: some appear very often in the world's languages, whereas others are much more common in certain language families, language areas, or even specific to a single language.\n\nWhen described as a system of symbolic communication, language is traditionally seen as consisting of three parts: signs, meanings, and a code connecting signs with their meanings. The study of the process of semiosis, how signs and meanings are combined, used, and interpreted is called semiotics. Signs can be composed of sounds, gestures, letters, or symbols, depending on whether the language is spoken, signed, or written, and they can be combined into complex signs, such as words and phrases. When used in communication, a sign is encoded and transmitted by a sender through a channel to a receiver who decodes it.\nSome of the properties that define human language as opposed to other communication systems are: the arbitrariness of the linguistic sign, meaning that there is no predictable connection between a linguistic sign and its meaning; the duality of the linguistic system, meaning that linguistic structures are built by combining elements into larger structures that can be seen as layered, e.g. how sounds build words and words build phrases; the discreteness of the elements of language, meaning that the elements out of which linguistic signs are constructed are discrete units, e.g. sounds and words, that can be distinguished from each other and rearranged in different patterns; and the productivity of the linguistic system, meaning that the finite number of linguistic elements can be combined into a theoretically infinite number of combinations.\n\nThe rules by which signs can be combined to form words and phrases are called syntax or grammar. The meaning that is connected to individual signs, morphemes, words, phrases, and texts is called semantics. The division of language into separate but connected systems of sign and meaning goes back to the first linguistic studies of de Saussure and is now used in almost all branches of linguistics.\n\nLanguages express meaning by relating a sign form to a meaning, or its content. Sign forms must be something that can be perceived, for example, in sounds, images, or gestures, and then related to a specific meaning by social convention. Because the basic relation of meaning for most linguistic signs is based on social convention, linguistic signs can be considered arbitrary, in the sense that the convention is established socially and historically, rather than by means of a natural relation between a specific sign form and its meaning.\n\nThus, languages must have a vocabulary of signs related to specific meaning. The English sign \"dog\" denotes, for example, a member of the species \"Canis familiaris\". In a language, the array of arbitrary signs connected to specific meanings is called the lexicon, and a single sign connected to a meaning is called a lexeme. Not all meanings in a language are represented by single words. Often, semantic concepts are embedded in the morphology or syntax of the language in the form of grammatical categories.\n\nAll languages contain the semantic structure of predication: a structure that predicates a property, state, or action. Traditionally, semantics has been understood to be the study of how speakers and interpreters assign truth values to statements, so that meaning is understood to be the process by which a predicate can be said to be true or false about an entity, e.g. \"<nowiki>[x [is y]]\" or \"[x [does y]]</nowiki>\". Recently, this model of semantics has been complemented with more dynamic models of meaning that incorporate shared knowledge about the context in which a sign is interpreted into the production of meaning. Such models of meaning are explored in the field of pragmatics.\n\nDepending on modality, language structure can be based on systems of sounds (speech), gestures (sign languages), or graphic or tactile symbols (writing). The ways in which languages use sounds or signs to construct meaning are studied in phonology. The study of how humans produce and perceive vocal sounds is called phonetics. In spoken language, meaning is produced when sounds become part of a system in which some sounds can contribute to expressing meaning and others do not. In any given language, only a limited number of the many distinct sounds that can be created by the human vocal apparatus contribute to constructing meaning.\n\nSounds as part of a linguistic system are called phonemes. Phonemes are abstract units of sound, defined as the smallest units in a language that can serve to distinguish between the meaning of a pair of minimally different words, a so-called minimal pair. In English, for example, the words \"bat\" and \"pat\" form a minimal pair, in which the distinction between and differentiates the two words, which have different meanings. However, each language contrasts sounds in different ways. For example, in a language that does not distinguish between voiced and unvoiced consonants, the sounds and (if they both occur) could be considered a single phoneme, and consequently, the two pronunciations would have the same meaning. Similarly, the English language does not distinguish phonemically between aspirated and non-aspirated pronunciations of consonants, as many other languages like Korean and Hindi do: the unaspirated in \"spin\" and the aspirated in \"pin\" are considered to be merely different ways of pronouncing the same phoneme (such variants of a single phoneme are called allophones), whereas in Mandarin Chinese, the same difference in pronunciation distinguishes between the words 'crouch' and 'eight' (the accent above the á means that the vowel is pronounced with a high tone).\n\nAll spoken languages have phonemes of at least two different categories, vowels and consonants, that can be combined to form syllables. As well as segments such as consonants and vowels, some languages also use sound in other ways to convey meaning. Many languages, for example, use stress, pitch, duration, and tone to distinguish meaning. Because these phenomena operate outside of the level of single segments, they are called suprasegmental. Some languages have only a few phonemes, for example, Rotokas and Pirahã language with 11 and 10 phonemes respectively, whereas languages like Taa may have as many as 141 phonemes. In sign languages, the equivalent to phonemes (formerly called cheremes) are defined by the basic elements of gestures, such as hand shape, orientation, location, and motion, which correspond to manners of articulation in spoken language.\n\nWriting systems represent language using visual symbols, which may or may not correspond to the sounds of spoken language. The Latin alphabet (and those on which it is based or that have been derived from it) was originally based on the representation of single sounds, so that words were constructed from letters that generally denote a single consonant or vowel in the structure of the word. In syllabic scripts, such as the Inuktitut syllabary, each sign represents a whole syllable. In logographic scripts, each sign represents an entire word, and will generally bear no relation to the sound of that word in spoken language.\n\nBecause all languages have a very large number of words, no purely logographic scripts are known to exist. Written language represents the way spoken sounds and words follow one after another by arranging symbols according to a pattern that follows a certain direction. The direction used in a writing system is entirely arbitrary and established by convention. Some writing systems use the horizontal axis (left to right as the Latin script or right to left as the Arabic script), while others such as traditional Chinese writing use the vertical dimension (from top to bottom). A few writing systems use opposite directions for alternating lines, and others, such as the ancient Maya script, can be written in either direction and rely on graphic cues to show the reader the direction of reading.\n\nIn order to represent the sounds of the world's languages in writing, linguists have developed the International Phonetic Alphabet, designed to represent all of the discrete sounds that are known to contribute to meaning in human languages.\n\nGrammar is the study of how meaningful elements called \"morphemes\" within a language can be combined into utterances. Morphemes can either be \"free\" or \"bound\". If they are free to be moved around within an utterance, they are usually called \"words\", and if they are bound to other words or morphemes, they are called affixes. The way in which meaningful elements can be combined within a language is governed by rules. The rules for the internal structure of words are called morphology. The rules of the internal structure of phrases and sentences are called \"syntax\".\n\nGrammar can be described as a system of categories and a set of rules that determine how categories combine to form different aspects of meaning. Languages differ widely in whether they are encoded through the use of categories or lexical units. However, several categories are so common as to be nearly universal. Such universal categories include the encoding of the grammatical relations of participants and predicates by grammatically distinguishing between their relations to a predicate, the encoding of temporal and spatial relations on predicates, and a system of grammatical person governing reference to and distinction between speakers and addressees and those about whom they are speaking.\n\nLanguages organize their parts of speech into classes according to their functions and positions relative to other parts. All languages, for instance, make a basic distinction between a group of words that prototypically denotes things and concepts and a group of words that prototypically denotes actions and events. The first group, which includes English words such as \"dog\" and \"song\", are usually called nouns. The second, which includes \"run\" and \"sing\", are called verbs. Another common category is the adjective: words that describe properties or qualities of nouns, such as \"red\" or \"big\". Word classes can be \"open\" if new words can continuously be added to the class, or relatively \"closed\" if there is a fixed number of words in a class. In English, the class of pronouns is closed, whereas the class of adjectives is open, since an infinite number of adjectives can be constructed from verbs (e.g. \"saddened\") or nouns (e.g. with the -like suffix, as in \"noun-like\"). In other languages such as Korean, the situation is the opposite, and new pronouns can be constructed, whereas the number of adjectives is fixed.\n\nWord classes also carry out differing functions in grammar. Prototypically, verbs are used to construct predicates, while nouns are used as arguments of predicates. In a sentence such as \"Sally runs\", the predicate is \"runs\", because it is the word that predicates a specific state about its argument \"Sally\". Some verbs such as \"curse\" can take two arguments, e.g. \"Sally cursed John\". A predicate that can only take a single argument is called \"intransitive\", while a predicate that can take two arguments is called \"transitive\".\n\nMany other word classes exist in different languages, such as conjunctions like \"and\" that serve to join two sentences, articles that introduce a noun, interjections such as \"wow!\", or ideophones like \"splash\" that mimic the sound of some event. Some languages have positionals that describe the spatial position of an event or entity. Many languages have classifiers that identify countable nouns as belonging to a particular type or having a particular shape. For instance, in Japanese, the general noun classifier for humans is \"nin\" (人), and it is used for counting humans, whatever they are called:\n\nFor trees, it would be:\n\nIn linguistics, the study of the internal structure of complex words and the processes by which words are formed is called morphology. In most languages, it is possible to construct complex words that are built of several morphemes. For instance, the English word \"unexpected\" can be analyzed as being composed of the three morphemes \"un-\", \"expect\" and \"-ed\".\n\nMorphemes can be classified according to whether they are independent morphemes, so-called roots, or whether they can only co-occur attached to other morphemes. These bound morphemes or affixes can be classified according to their position in relation to the root: \"prefixes\" precede the root, suffixes follow the root, and infixes are inserted in the middle of a root. Affixes serve to modify or elaborate the meaning of the root. Some languages change the meaning of words by changing the phonological structure of a word, for example, the English word \"run\", which in the past tense is \"ran\". This process is called \"ablaut\". Furthermore, morphology distinguishes between the process of inflection, which modifies or elaborates on a word, and the process of derivation, which creates a new word from an existing one. In English, the verb \"sing\" has the inflectional forms \"singing\" and \"sung\", which are both verbs, and the derivational form \"singer\", which is a noun derived from the verb with the agentive suffix \"-er\".\n\nLanguages differ widely in how much they rely on morphological processes of word formation. In some languages, for example, Chinese, there are no morphological processes, and all grammatical information is encoded syntactically by forming strings of single words. This type of morpho-syntax is often called isolating, or analytic, because there is almost a full correspondence between a single word and a single aspect of meaning. Most languages have words consisting of several morphemes, but they vary in the degree to which morphemes are discrete units. In many languages, notably in most Indo-European languages, single morphemes may have several distinct meanings that cannot be analyzed into smaller segments. For example, in Latin, the word \"bonus\", or \"good\", consists of the root \"bon-\", meaning \"good\", and the suffix -\"us\", which indicates masculine gender, singular number, and nominative case. These languages are called \"fusional languages\", because several meanings may be fused into a single morpheme. The opposite of fusional languages are agglutinative languages which construct words by stringing morphemes together in chains, but with each morpheme as a discrete semantic unit. An example of such a language is Turkish, where for example, the word \"evlerinizden\", or \"from your houses\", consists of the morphemes, \"ev-ler-iniz-den\" with the meanings \"house-plural-your-from\". The languages that rely on morphology to the greatest extent are traditionally called polysynthetic languages. They may express the equivalent of an entire English sentence in a single word. For example, in Persian the single word \"nafahmidamesh\" means \"I didn't understand it\" consisting of morphemes \"na-fahm-id-am-esh\" with the meanings, \"negation.understand.past.I.it\". As another example with more complexity, in the Yupik word \"tuntussuqatarniksatengqiggtuq\", which means \"He had not yet said again that he was going to hunt reindeer\", the word consists of the morphemes \"tuntu-ssur-qatar-ni-ksaite-ngqiggte-uq\" with the meanings, \"reindeer-hunt-future-say-negation-again-third.person.singular.indicative\", and except for the morpheme \"tuntu\" (\"reindeer\") none of the other morphemes can appear in isolation.\n\nMany languages use morphology to cross-reference words within a sentence. This is sometimes called \"agreement\". For example, in many Indo-European languages, adjectives must cross-reference the noun they modify in terms of number, case, and gender, so that the Latin adjective \"bonus\", or \"good\", is inflected to agree with a noun that is masculine gender, singular number, and nominative case. In many polysynthetic languages, verbs cross-reference their subjects and objects. In these types of languages, a single verb may include information that would require an entire sentence in English. For example, in the Basque phrase \"ikusi nauzu\", or \"you saw me\", the past tense auxiliary verb \"n-au-zu\" (similar to English \"do\") agrees with both the subject (you) expressed by the \"n\"- prefix, and with the object (me) expressed by the – \"zu\" suffix. The sentence could be directly transliterated as \"see you-did-me\"\n\nAnother way in which languages convey meaning is through the order of words within a sentence. The grammatical rules for how to produce new sentences from words that are already known is called syntax. The syntactical rules of a language determine why a sentence in English such as \"I love you\" is meaningful, but \"*love you I\" is not. Syntactical rules determine how word order and sentence structure is constrained, and how those constraints contribute to meaning. For example, in English, the two sentences \"the slaves were cursing the master\" and \"the master was cursing the slaves\" mean different things, because the role of the grammatical subject is encoded by the noun being in front of the verb, and the role of object is encoded by the noun appearing after the verb. Conversely, in Latin, both \"Dominus servos vituperabat\" and \"Servos vituperabat dominus\" mean \"the master was reprimanding the slaves\", because \"servos\", or \"slaves\", is in the accusative case, showing that they are the grammatical object of the sentence, and \"dominus\", or \"master\", is in the nominative case, showing that he is the subject.\n\nLatin uses morphology to express the distinction between subject and object, whereas English uses word order. Another example of how syntactic rules contribute to meaning is the rule of inverse word order in questions, which exists in many languages. This rule explains why when in English, the phrase \"John is talking to Lucy\" is turned into a question, it becomes \"Who is John talking to?\", and not \"John is talking to who?\". The latter example may be used as a way of placing special emphasis on \"who\", thereby slightly altering the meaning of the question. Syntax also includes the rules for how complex sentences are structured by grouping words together in units, called phrases, that can occupy different places in a larger syntactic structure. Sentences can be described as consisting of phrases connected in a tree structure, connecting the phrases to each other at different levels. To the right is a graphic representation of the syntactic analysis of the English sentence \"the cat sat on the mat\". The sentence is analyzed as being constituted by a noun phrase, a verb, and a prepositional phrase; the prepositional phrase is further divided into a preposition and a noun phrase, and the noun phrases consist of an article and a noun.\n\nThe reason sentences can be seen as being composed of phrases is because each phrase would be moved around as a single element if syntactic operations were carried out. For example, \"the cat\" is one phrase, and \"on the mat\" is another, because they would be treated as single units if a decision was made to emphasize the location by moving forward the prepositional phrase: \"[And] on the mat, the cat sat\". There are many different formalist and functionalist frameworks that propose theories for describing syntactic structures, based on different assumptions about what language is and how it should be described. Each of them would analyze a sentence such as this in a different manner.\n\nLanguages can be classified in relation to their grammatical types. Languages that belong to different families nonetheless often have features in common, and these shared features tend to correlate. For example, languages can be classified on the basis of their basic word order, the relative order of the verb, and its constituents in a normal indicative sentence. In English, the basic order is SVO: \"The snake(S) bit(V) the man(O)\", whereas for example, the corresponding sentence in the Australian language Gamilaraay would be \"d̪uyugu n̪ama d̪ayn yiːy\" (snake man bit), SOV. Word order type is relevant as a typological parameter, because basic word order type corresponds with other syntactic parameters, such as the relative order of nouns and adjectives, or of the use of prepositions or postpositions. Such correlations are called implicational universals. For example, most (but not all) languages that are of the SOV type have postpositions rather than prepositions, and have adjectives before nouns.\n\nAll languages structure sentences into Subject, Verb, and Object, but languages differ in the way they classify the relations between actors and actions. English uses the nominative-accusative word typology: in English transitive clauses, the subjects of both intransitive sentences (\"I run\") and transitive sentences (\"I love you\") are treated in the same way, shown here by the nominative pronoun \"I\". Some languages, called ergative, Gamilaraay among them, distinguish instead between Agents and Patients. In ergative languages, the single participant in an intransitive sentence, such as \"I run\", is treated the same as the patient in a transitive sentence, giving the equivalent of \"me run\". Only in transitive sentences would the equivalent of the pronoun \"I\" be used. In this way the semantic roles can map onto the grammatical relations in different ways, grouping an intransitive subject either with Agents (accusative type) or Patients (ergative type) or even making each of the three roles differently, which is called the tripartite type.\n\nThe shared features of languages which belong to the same typological class type may have arisen completely independently. Their co-occurrence might be due to universal laws governing the structure of natural languages, \"language universals\", or they might be the result of languages evolving convergent solutions to the recurring communicative problems that humans use language to solve.\n\nWhile humans have the ability to learn any language, they only do so if they grow up in an environment in which language exists and is used by others. Language is therefore dependent on communities of speakers in which children learn language from their elders and peers and themselves transmit language to their own children. Languages are used by those who speak them to communicate and to solve a plethora of social tasks. Many aspects of language use can be seen to be adapted specifically to these purposes. Due to the way in which language is transmitted between generations and within communities, language perpetually changes, diversifying into new languages or converging due to language contact. The process is similar to the process of evolution, where the process of descent with modification leads to the formation of a phylogenetic tree.\n\nHowever, languages differ from biological organisms in that they readily incorporate elements from other languages through the process of diffusion, as speakers of different languages come into contact. Humans also frequently speak more than one language, acquiring their first language or languages as children, or learning new languages as they grow up. Because of the increased language contact in the globalizing world, many small languages are becoming endangered as their speakers shift to other languages that afford the possibility to participate in larger and more influential speech communities.\n\nThe semantic study of meaning assumes that meaning is in a relation between signs and meanings that are firmly established through social convention. However, semantics does not study the way in which social conventions are made and affect language. Rather, when studying the way in which words and signs are used, it is often the case that words have different meanings, depending on the social context of use. An important example of this is the process called deixis, which describes the way in which certain words refer to entities through their relation between a specific point in time and space when the word is uttered. Such words are, for example, the word, \"I\" (which designates the person speaking), \"now\" (which designates the moment of speaking), and \"here\" (which designates the position of speaking). Signs also change their meanings over time, as the conventions governing their usage gradually change. The study of how the meaning of linguistic expressions changes depending on context is called pragmatics. Deixis is an important part of the way that we use language to point out entities in the world. Pragmatics is concerned with the ways in which language use is patterned and how these patterns contribute to meaning. For example, in all languages, linguistic expressions can be used not just to transmit information, but to perform actions. Certain actions are made only through language, but nonetheless have tangible effects, e.g. the act of \"naming\", which creates a new name for some entity, or the act of \"pronouncing someone man and wife\", which creates a social contract of marriage. These types of acts are called speech acts, although they can also be carried out through writing or hand signing.\n\nThe form of linguistic expression often does not correspond to the meaning that it actually has in a social context. For example, if at a dinner table a person asks, \"Can you reach the salt?\", that is, in fact, not a question about the length of the arms of the one being addressed, but a request to pass the salt across the table. This meaning is implied by the context in which it is spoken; these kinds of effects of meaning are called conversational implicatures. These social rules for which ways of using language are considered appropriate in certain situations and how utterances are to be understood in relation to their context vary between communities, and learning them is a large part of acquiring communicative competence in a language.\n\nAll healthy, normally developing human beings learn to use language. Children acquire the language or languages used around them: whichever languages they receive sufficient exposure to during childhood. The development is essentially the same for children acquiring sign or oral languages. This learning process is referred to as first-language acquisition, since unlike many other kinds of learning, it requires no direct teaching or specialized study. In \"The Descent of Man\", naturalist Charles Darwin called this process \"an instinctive tendency to acquire an art\".\n\nFirst language acquisition proceeds in a fairly regular sequence, though there is a wide degree of variation in the timing of particular stages among normally developing infants. From birth, newborns respond more readily to human speech than to other sounds. Around one month of age, babies appear to be able to distinguish between different speech sounds. Around six months of age, a child will begin babbling, producing the speech sounds or handshapes of the languages used around them. Words appear around the age of 12 to 18 months; the average vocabulary of an eighteen-month-old child is around 50 words. A child's first utterances are holophrases (literally \"whole-sentences\"), utterances that use just one word to communicate some idea. Several months after a child begins producing words, he or she will produce two-word utterances, and within a few more months will begin to produce telegraphic speech, or short sentences that are less grammatically complex than adult speech, but that do show regular syntactic structure. From roughly the age of three to five years, a child's ability to speak or sign is refined to the point that it resembles adult language. Studies published in 2013 have indicated that unborn fetuses are capable of language acquisition to some degree.\n\nAcquisition of second and additional languages can come at any age, through exposure in daily life or courses. Children learning a second language are more likely to achieve native-like fluency than adults, but in general, it is very rare for someone speaking a second language to pass completely for a native speaker. An important difference between first language acquisition and additional language acquisition is that the process of additional language acquisition is influenced by languages that the learner already knows.\n\nLanguages, understood as the particular set of speech norms of a particular community, are also a part of the larger culture of the community that speaks them. Languages differ not only in pronunciation, vocabulary, and grammar, but also through having different \"cultures of speaking.\" Humans use language as a way of signalling identity with one cultural group as well as difference from others. Even among speakers of one language, several different ways of using the language exist, and each is used to signal affiliation with particular subgroups within a larger culture. Linguists and anthropologists, particularly sociolinguists, ethnolinguists, and linguistic anthropologists have specialized in studying how ways of speaking vary between speech communities.\n\nLinguists use the term \"varieties\" to refer to the different ways of speaking a language. This term includes geographically or socioculturally defined dialects as well as the jargons or styles of subcultures. Linguistic anthropologists and sociologists of language define communicative style as the ways that language is used and understood within a particular culture.\n\nBecause norms for language use are shared by members of a specific group, communicative style also becomes a way of displaying and constructing group identity. Linguistic differences may become salient markers of divisions between social groups, for example, speaking a language with a particular accent may imply membership of an ethnic minority or social class, one's area of origin, or status as a second language speaker. These kinds of differences are not part of the linguistic system, but are an important part of how people use language as a social tool for constructing groups.\n\nHowever, many languages also have grammatical conventions that signal the social position of the speaker in relation to others through the use of registers that are related to social hierarchies or divisions. In many languages, there are stylistic or even grammatical differences between the ways men and women speak, between age groups, or between social classes, just as some languages employ different words depending on who is listening. For example, in the Australian language Dyirbal, a married man must use a special set of words to refer to everyday items when speaking in the presence of his mother-in-law. Some cultures, for example, have elaborate systems of \"social deixis\", or systems of signalling social distance through linguistic means. In English, social deixis is shown mostly through distinguishing between addressing some people by first name and others by surname, and in titles such as \"Mrs.\", \"boy\", \"Doctor\", or \"Your Honor\", but in other languages, such systems may be highly complex and codified in the entire grammar and vocabulary of the language. For instance, in languages of east Asia such as Thai, Burmese, and Javanese, different words are used according to whether a speaker is addressing someone of higher or lower rank than oneself in a ranking system with animals and children ranking the lowest and gods and members of royalty as the highest.\n\nThroughout history a number of different ways of representing language in graphic media have been invented. These are called writing systems.\n\nThe use of writing has made language even more useful to humans. It makes it possible to store large amounts of information outside of the human body and retrieve it again, and it allows communication across distances that would otherwise be impossible. Many languages conventionally employ different genres, styles, and registers in written and spoken language, and in some communities, writing traditionally takes place in an entirely different language than the one spoken. There is some evidence that the use of writing also has effects on the cognitive development of humans, perhaps because acquiring literacy generally requires explicit and formal education.\n\nThe invention of the first writing systems is roughly contemporary with the beginning of the Bronze Age in the late 4th millennium BC. The Sumerian archaic cuneiform script and the Egyptian hieroglyphs are generally considered to be the earliest writing systems, both emerging out of their ancestral proto-literate symbol systems from 3400–3200 BC with the earliest coherent texts from about 2600 BC. It is generally agreed that Sumerian writing was an independent invention; however, it is debated whether Egyptian writing was developed completely independently of Sumerian, or was a case of cultural diffusion. A similar debate exists for the Chinese script, which developed around 1200 BC. The pre-Columbian Mesoamerican writing systems (including among others Olmec and Maya scripts) are generally believed to have had independent origins.\n\nAll languages change as speakers adopt or invent new ways of speaking and pass them on to other members of their speech community. Language change happens at all levels from the phonological level to the levels of vocabulary, morphology, syntax, and discourse. Even though language change is often initially evaluated negatively by speakers of the language who often consider changes to be \"decay\" or a sign of slipping norms of language usage, it is natural and inevitable.\n\nChanges may affect specific sounds or the entire phonological system. Sound change can consist of the replacement of one speech sound or phonetic feature by another, the complete loss of the affected sound, or even the introduction of a new sound in a place where there had been none. Sound changes can be \"conditioned\" in which case a sound is changed only if it occurs in the vicinity of certain other sounds. Sound change is usually assumed to be \"regular\", which means that it is expected to apply mechanically whenever its structural conditions are met, irrespective of any non-phonological factors. On the other hand, sound changes can sometimes be \"sporadic\", affecting only one particular word or a few words, without any seeming regularity. Sometimes a simple change triggers a chain shift in which the entire phonological system is affected. This happened in the Germanic languages when the sound change known as Grimm's law affected all the stop consonants in the system. The original consonant * became /b/ in the Germanic languages, the previous * in turn became /p/, and the previous * became /f/. The same process applied to all stop consonants and explains why Italic languages such as Latin have \"p\" in words like pater\" and pisces\", whereas Germanic languages, like English, have father\" and fish\".\n\nAnother example is the Great Vowel Shift in English, which is the reason that the spelling of English vowels do not correspond well to their current pronunciation. This is because the vowel shift brought the already established orthography out of synchronization with pronunciation. Another source of sound change is the erosion of words as pronunciation gradually becomes increasingly indistinct and shortens words, leaving out syllables or sounds. This kind of change caused Latin \"mea domina\" to eventually become the French \"madame\" and American English \"ma'am\".\n\nChange also happens in the grammar of languages as discourse patterns such as idioms or particular constructions become grammaticalized. This frequently happens when words or morphemes erode and the grammatical system is unconsciously rearranged to compensate for the lost element. For example, in some varieties of Caribbean Spanish the final /s/ has eroded away. Since Standard Spanish uses final /s/ in the morpheme marking the second person subject \"you\" in verbs, the Caribbean varieties now have to express the second person using the pronoun \"tú\". This means that the sentence \"what's your name\" is \"¿como te llamas?\" in Standard Spanish, but in Caribbean Spanish. The simple sound change has affected both morphology and syntax. Another common cause of grammatical change is the gradual petrification of idioms into new grammatical forms, for example, the way the English \"going to\" construction lost its aspect of movement and in some varieties of English has almost become a full-fledged future tense (e.g. \"I'm gonna\").\n\nLanguage change may be motivated by \"language internal\" factors, such as changes in pronunciation motivated by certain sounds being difficult to distinguish aurally or to produce, or through patterns of change that cause some rare types of constructions to drift towards more common types. Other causes of language change are social, such as when certain pronunciations become emblematic of membership in certain groups, such as social classes, or with ideologies, and therefore are adopted by those who wish to identify with those groups or ideas. In this way, issues of identity and politics can have profound effects on language structure.\n\nOne important source of language change is contact and resulting diffusion of linguistic traits between languages. Language contact occurs when speakers of two or more languages or varieties interact on a regular basis. Multilingualism is likely to have been the norm throughout human history and most people in the modern world are multilingual. Before the rise of the concept of the ethno-national state, monolingualism was characteristic mainly of populations inhabiting small islands. But with the ideology that made one people, one state, and one language the most desirable political arrangement, monolingualism started to spread throughout the world. Nonetheless, there are only 250 countries in the world corresponding to some 6000 languages, which means that most countries are multilingual and most languages therefore exist in close contact with other languages.\n\nWhen speakers of different languages interact closely, it is typical for their languages to influence each other. Through sustained language contact over long periods, linguistic traits diffuse between languages, and languages belonging to different families may converge to become more similar. In areas where many languages are in close contact, this may lead to the formation of language areas in which unrelated languages share a number of linguistic features. A number of such language areas have been documented, among them, the Balkan language area, the Mesoamerican language area, and the Ethiopian language area. Also, larger areas such as South Asia, Europe, and Southeast Asia have sometimes been considered language areas, because of widespread diffusion of specific areal features.\n\nLanguage contact may also lead to a variety of other linguistic phenomena, including language convergence, borrowing, and relexification (replacement of much of the native vocabulary with that of another language). In situations of extreme and sustained language contact, it may lead to the formation of new mixed languages that cannot be considered to belong to a single language family. One type of mixed language called pidgins occurs when adult speakers of two different languages interact on a regular basis, but in a situation where neither group learns to speak the language of the other group fluently. In such a case, they will often construct a communication form that has traits of both languages, but which has a simplified grammatical and phonological structure. The language comes to contain mostly the grammatical and phonological categories that exist in both languages. Pidgin languages are defined by not having any native speakers, but only being spoken by people who have another language as their first language. But if a Pidgin language becomes the main language of a speech community, then eventually children will grow up learning the pidgin as their first language. As the generation of child learners grow up, the pidgin will often be seen to change its structure and acquire a greater degree of complexity. This type of language is generally called a creole language. An example of such mixed languages is Tok Pisin, the official language of Papua New-Guinea, which originally arose as a Pidgin based on English and Austronesian languages; others are Kreyòl ayisyen, the French-based creole language spoken in Haiti, and Michif, a mixed language of Canada, based on the Native American language Cree and French.\n\n\"SIL Ethnologue\" defines a \"living language\" as \"one that has at least one speaker for whom it is their first language\". The exact number of known living languages varies from 6,000 to 7,000, depending on the precision of one's definition of \"language\", and in particular, on how one defines the distinction between languages and dialects. As of 2016, \"Ethnologue\" cataloged 7,097 living human languages. The \"Ethnologue\" establishes linguistic groups based on studies of mutual intelligibility, and therefore often includes more categories than more conservative classifications. For example, the Danish language that most scholars consider a single language with several dialects is classified as two distinct languages (Danish and Jutish) by the \"Ethnologue\".\n\nAccording to the \"Ethnologue\", 389 languages (nearly 6%) have more than a million speakers. These languages together account for 94% of the world's population, whereas 94% of the world's languages account for the remaining 6% of the global population. To the right is a table of the world's 10 most spoken languages with population estimates from the \"Ethnologue\" (2009 figures).\n\nThere is no clear distinction between a language and a dialect, notwithstanding a famous aphorism attributed to linguist Max Weinreich that \"a language is a dialect with an army and navy\". For example, national boundaries frequently override linguistic difference in determining whether two linguistic varieties are languages or dialects. Hakka, Cantonese and Mandarin are, for example, often classified as \"dialects\" of Chinese, even though they are more different from each other than Swedish is from Norwegian. Before the Yugoslav civil war, Serbo-Croatian was considered a single language with two dialects, but now Croatian and Serbian are considered different languages and employ different writing systems. In other words, the distinction may hinge on political considerations as much as on cultural differences, distinctive writing systems, or degree of mutual intelligibility.\n\nThe world's languages can be grouped into language families consisting of languages that can be shown to have common ancestry. Linguists recognize many hundreds of language families, although some of them can possibly be grouped into larger units as more evidence becomes available and in-depth studies are carried out. At present, there are also dozens of language isolates: languages that cannot be shown to be related to any other languages in the world. Among them are Basque, spoken in Europe, Zuni of New Mexico, Purépecha of Mexico, Ainu of Japan, Burushaski of Pakistan, and many others.\n\nThe language family of the world that has the most speakers is the Indo-European languages, spoken by 46% of the world's population. This family includes major world languages like English, Spanish, Russian, and Hindustani (Hindi/Urdu). The Indo-European family achieved prevalence first during the Eurasian Migration Period (c. 400–800 AD), and subsequently through the European colonial expansion, which brought the Indo-European languages to a politically and often numerically dominant position in the Americas and much of Africa. The Sino-Tibetan languages are spoken by 20% of the world's population and include many of the languages of East Asia, including Hakka, Mandarin Chinese, Cantonese, and hundreds of smaller languages.\n\nAfrica is home to a large number of language families, the largest of which is the Niger-Congo language family, which includes such languages as Swahili, Shona, and Yoruba. Speakers of the Niger-Congo languages account for 6.9% of the world's population. A similar number of people speak the Afroasiatic languages, which include the populous Semitic languages such as Arabic, Hebrew language, and the languages of the Sahara region, such as the Berber languages and Hausa.\n\nThe Austronesian languages are spoken by 5.5% of the world's population and stretch from Madagascar to maritime Southeast Asia all the way to Oceania. It includes such languages as Malagasy, Māori, Samoan, and many of the indigenous languages of Indonesia and Taiwan. The Austronesian languages are considered to have originated in Taiwan around 3000 BC and spread through the Oceanic region through island-hopping, based on an advanced nautical technology. Other populous language families are the Dravidian languages of South Asia (among them Kannada Tamil and Telugu), the Turkic languages of Central Asia (such as Turkish), the Austroasiatic (among them Khmer), and Tai–Kadai languages of Southeast Asia (including Thai).\n\nThe areas of the world in which there is the greatest linguistic diversity, such as the Americas, Papua New Guinea, West Africa, and South-Asia, contain hundreds of small language families. These areas together account for the majority of the world's languages, though not the majority of speakers. In the Americas, some of the largest language families include the Quechumaran, Arawak, and Tupi-Guarani families of South America, the Uto-Aztecan, Oto-Manguean, and Mayan of Mesoamerica, and the Na-Dene, Iroquoian, and Algonquian language families of North America. In Australia, most indigenous languages belong to the Pama-Nyungan family, whereas New Guinea is home to a large number of small families and isolates, as well as a number of Austronesian languages.\n\nLanguage endangerment occurs when a language is at risk of falling out of use as its speakers die out or shift to speaking another language. Language loss occurs when the language has no more native speakers, and becomes a \"dead language\". If eventually no one speaks the language at all, it becomes an \"extinct language\". While languages have always gone extinct throughout human history, they have been disappearing at an accelerated rate in the 20th and 21st centuries due to the processes of globalization and neo-colonialism, where the economically powerful languages dominate other languages.\n\nThe more commonly spoken languages dominate the less commonly spoken languages, so the less commonly spoken languages eventually disappear from populations. The total number of languages in the world is not known. Estimates vary depending on many factors. The consensus is that there are between 6,000 and 7,000 languages spoken as of 2010, and that between 50–90% of those will have become extinct by the year 2100. The top 20 languages, those spoken by more than 50 million speakers each, are spoken by 50% of the world's population, whereas many of the other languages are spoken by small communities, most of them with less than 10,000 speakers.\n\nThe United Nations Educational, Scientific and Cultural Organization (UNESCO) operates with five levels of language endangerment: \"safe\", \"vulnerable\" (not spoken by children outside the home), \"definitely endangered\" (not spoken by children), \"severely endangered\" (only spoken by the oldest generations), and \"critically endangered\" (spoken by few members of the oldest generation, often semi-speakers). Notwithstanding claims that the world would be better off if most adopted a single common \"lingua franca\", such as English or Esperanto, there is a consensus that the loss of languages harms the cultural diversity of the world. It is a common belief, going back to the biblical narrative of the tower of Babel in the Old Testament, that linguistic diversity causes political conflict, but this is contradicted by the fact that many of the world's major episodes of violence have taken place in situations with low linguistic diversity, such as the Yugoslav and American Civil War, or the genocide of Rwanda, whereas many of the most stable political units have been highly multilingual.\n\nMany projects aim to prevent or slow this loss by revitalizing endangered languages and promoting education and literacy in minority languages. Across the world, many countries have enacted specific legislation to protect and stabilize the language of indigenous speech communities. A minority of linguists have argued that language loss is a natural process that should not be counteracted, and that documenting endangered languages for posterity is sufficient.\n\n"}
{"id": "1646993", "url": "https://en.wikipedia.org/wiki?curid=1646993", "title": "Law on Freedom of Conscience and Religious Associations", "text": "Law on Freedom of Conscience and Religious Associations\n\nThe Law on Freedom of Conscience and Religious Associations (Russian: «Закон О свободе совести и о религиозных объединениях»), also known as the 1997 Law (Russian: «Закон 1997 года») is a Russian law passed and signed by President Boris Yeltsin on September 26, 1997.\n\nThe law redefined the state's relationship with religion, as Soviet premier Mikhail Gorbachev had defined in on the Law of the Russian Soviet Federative Socialist Republic on Freedom of Worship passed on October 25, 1990, commonly known as the 1990 Law. After the fall of Communism, Gorbachev had given much-needed breathing room to the practice of religion in Russia, whose culture's heart is Eastern Orthodoxy, but had also opened the door indescriminately and generally to the practice of religion. The Russian Orthodox Church believed that a new law was needed to preserve Russia against what they considered to be the corruption of Orthodoxy.\n\nThe law was formulated and pushed by the Russian Orthodox Church, secular nationalists, and communists alike, with such determination that though Yeltsin vetoed the bill once, he could not legitimately do so a second time.\n\nWritten in the law was the upholding of separation of church and state, as well as an interdiction against a state religion. With that in mind, the following definitions and regulations are given:\n\n\n\n\n\n\nReligion under the new law became nearly as regulated as it had been in Soviet times, though without the official communist hostility. It did accomplish some expulsion of Western religious work, though it left room for some foreign churches to legitimately register. There had been on the order of 16,000 registered organizations before the passage of the law, and by 2004 there were 22,000. By regulating on grounds common among new, foreign organizations, it made it difficult for them to take root, and it succeeded in promoting and securing a privileged place for the Russian Orthodox Church.\n\nOn June 22, 2005, debate of the Parliamentary Assembly of the Council of Europe (PACE) concluded, that the 1997 Law \"creates a complex form of categorization of religions which has led to various forms of discrimination and to the stigmatization of 'non-traditional' religions.\"\n\n"}
{"id": "1663139", "url": "https://en.wikipedia.org/wiki?curid=1663139", "title": "Legislative intent", "text": "Legislative intent\n\nIn law, the legislative intent of the legislature in enacting legislation may sometimes be considered by the judiciary when interpreting the law (see judicial interpretation). The judiciary may attempt to assess legislative intent where legislation is ambiguous, or does not appear to directly or adequately address a particular issue, or when there appears to have been a legislative drafting error.\n\nThe courts have repeatedly held that when a statute is clear and unambiguous, the inquiry into legislative intent ends at that point. It is only when a statute could be interpreted in more than one fashion that legislative intent must be inferred from sources other than the actual text of the statute.\n\nCourts frequently look to the following sources in attempting to determine the goals and purposes that the legislative body had in mind when it passed the law:\n\n\nCourts in the United States and elsewhere have developed a number of principles for handling such evidence of legislative intent; as an example, many courts have suggested that the comments of those opposing a bill under consideration should be treated with skepticism, on the principle that opponents of a bill may often exaggerate its practical consequences. \n\nOne early example of an important Supreme Court case which relied on legislative intent was \"Johnson v. Southern Pacific Co.\" (1904) 196 U.S. 1, where the court decided that a man may sue the railroad for failing to have an automatic coupler since the legislature was attempting to remedy the problem of multiple injuries by railroad coupling.\n\nOthers, most notably United States Supreme Court Justice Antonin Scalia, have objected generally to the use of such evidence, rather than reliance on the literal language of the statute, arguing that such evidence of \"legislative intent\" is often created by proponents of a bill to persuade a court to interpret the statute in a way that they were not able to persuade the legislative body to adopt when passing the bill.\n\nThese principles of legislative intent often overlap with those principles of statutory construction that courts have developed to interpret ambiguous or incomplete legislation. As an example, the principle that courts should not interpret a statute to produce absurd or unintended results will often be informed by evidence of what the proponents of a bill stated about the objectives to be achieved by the statute.\n\n"}
{"id": "4775804", "url": "https://en.wikipedia.org/wiki?curid=4775804", "title": "Leonidas C. Dyer", "text": "Leonidas C. Dyer\n\nLeonidas Carstarphen Dyer (June 11, 1871 – December 15, 1957) was an American politician, reformer, civil rights activist, and military officer who served 11 terms in the U.S. Congress as a Republican Representative from Missouri from 1911 to 1933. In 1898, enrolling in the U.S. Army as a private, Dyer served notably in the Spanish–American War; and was promoted to Colonel at the war's end.\n\nWorking as an attorney in St. Louis, Dyer started an anti-usury campaign and was elected to Congress as a Republican in 1910. As a progressive reformer, Dyer authored an anti-usury law in 1914 that limited excessive loan rates by bank lenders in the nation's capital, then still governed by Congress.\n\nHorrified by the race riots in Saint Louis and East Saint Louis in 1917 and the high rate of reported lynchings in the South, in 1918 Dyer was notable for proposing the Dyer Anti-Lynching Bill. In 1920, the Republican Party supported such legislation in its platform from the National Convention. In January 1922, Dyer's bill was passed by the House, which approved it by a wide margin due to \"insistent countrywide demand\". The bill was defeated by the white Democratic voting bloc of the South in filibusters in the Senate in December 1922, in 1923 and 1924.\n\nIn 1919, Dyer authored the motor-vehicle theft law, which made transporting stolen automobiles across state lines a federal crime. By 1956, the FBI reported that the law had enabled the recovery of cars worth more than $212 million. In terms of Prohibition, Rep. Dyer voted against various anti-liquor laws, including the Eighteenth Amendment.\n\nDyer served in Congress from the 62nd Congress to the 72nd Congress. He was defeated for re-election during the 1930s of the Great Depression. President Franklin D. Roosevelt's programs to put people to work and extend social welfare during the Depression helped attract voters to Democratic candidates.\n\nDyer was born near Warrenton in Warren County, Missouri, the son of James Coleman Dyer and Martha E. (Camp) Dyer. His father's family had roots in Virginia, where his uncle David Patterson Dyer was born; he was elected as a Republican Congressman from Missouri (1869–71).\n\nLeonidas attended common schools and Central Wesleyan College. He studied law at Washington University in St. Louis and was admitted to the bar in 1893.\n\nWhen the Spanish–American War began, Dyer joined the United States Army and served in combat during the Santiago campaign as a private in 1898. He was promoted to colonel during the war, and served as a member of the staff of Herbert S. Hadley, future Governor of Missouri.\n\nAfter the war, the young Dyer served as assistant circuit attorney in St. Louis, where he championed an anti-usury reform campaign that eventually gained national attention. Dyer successfully represented a railroad clerk who was being charged 34% monthly (\"408% annual\") interest on a $100 loan after having paid $480 interest in 14 months. None of the interest payment to the money lender was used to pay off the principal. The money lender, in front of Att. Dyer, tore up the railroad workers loan. Dyer organized a group of wealthy merchants in St. Louis who through investigations were able to keep interest rates low in Missouri.\n\nIn 1910, Dyer successfully ran and was elected Congressman to the U.S. House of Representatives. His long career in Congress having begun in 1911, Rep. Dyer was repeatedly re-elected. His time in Congress was briefly interrupted between 1914 and 1915 due to a dispute over 1912 election results, but he was reelected in 1914.\n\nDyer was defeated for re-election from his district in 1932, 1934 and 1936, and decided to retire from politics. Dyer represented the 12th District of Missouri, which had a majority African-American population. They were disappointed by the Republican failure to pass an anti-lynching bill during the 1920s, and attracted to Democratic candidates during the Great Depression, after Franklin D. Roosevelt had started some of his work and welfare programs. Dyer followed Harry Coudrey, also a Republican.\n\nRep. Dyer continued his anti-usury campaign in 1914 by authoring a law that prevented banks from charging excessive interest rates on loans in Washington D.C., which was then governed by Congress. Rep. Dyer believed that money lenders went after financially vulnerable people, authorizing loan contracts for unnecessary purposes. Dyer stated that usury was \"an ancient moral crime against the poor and helpless.\" He advocated for each state to pass similar anti-usury laws.\n\nOn March 29, 1916, Rep. Dyer spoke before a Senate Committee advocating H.R. 10484, to fund a U.S. Postal pneumatic tube service in St. Louis. Under the existing service, U.S. mail was transported by compressed air vacuum tubes in the St. Louis area. Dyer asked the committee to extend the pneumatic tube service from 2 to 5 miles; at a cost of $50,000. According to Dyer, the tube extension would promote business and private citizens in East St. Louis by reducing delivery time 11 hours and 50 minutes. By comparison, the city of Boston had 8 miles of U.S. Postal pneumatic tube service.\n\nIn May 1917, a riot broke out in Saint Louis, where mobs of ethnic white men attacked black workers, strikebreakers who had been brought in to replace American Federation of Labor strikers. In July 1917, mob violence broke out in East St. Louis against blacks, also against a background of competition over jobs. Two white police officers were killed early in the confrontation. In retaliation, ethnic white mobs killed 35 blacks, mutilated the bodies and threw them into the Mississippi River. White rioters openly targeted and lynched several blacks. Those who attempted to stop the lynchings were threatened by the white mob with physical violence. As blacks fled into St. Louis, white rioters threatened to kill them upon their return. The white Illinois National guard brought into quell the riot, participated in the violence against blacks or did nothing to stop the violence. One black child was shot and thrown into a burning building, while white prostitutes openly attacked black women. After the riots, of the 134 persons indicted, only 9 whites who were put on trial went to prison while 12 indicted blacks who went to trial were imprisoned. Nearly 1/3 of the total 134 persons indicted were black. The conviction rate, mathematically, was more than doubled for blacks then for whites.\n\nDyer was distressed by such mob violence, with its disregard for the courts and the \"rule of law\". His district in St. Louis had mostly African-American residents and he wanted to protect his constituents and other citizens. Many people from his district had migrated to St. Louis from the South, in the exodus known as the Great Migration. They settled in St. Louis where industrialization had led to a strong economy and an increase in jobs. The economy had also attracted numerous immigrants from Europe and competition for work was high. Dyer also knew of the continuing high rate of lynchings, mostly of blacks by whites in the South. Working with W.E.B. Du Bois and Walter White of the National Association for the Advancement of Colored People, who had been working on a national anti-lynching campaign, Dyer helped develop and agreed to sponsor anti-lynching legislation. \nCalling for an end to mob violence, on April 1, 1918, Dyer introduced the Dyer Anti-Lynching Bill, which would have made lynching a federal crime. In his speech, he anticipated some members likely objections about the federal government sponsoring \"social\" legislation, and noted that lynching violated individuals' rights under the 14th Amendment. In addition, he noted that Congress had passed child labor laws and the Prohibition amendment. He said:\n\nIf Congress has felt its duty to do these things, why should it not also assume jurisdiction and enact laws to protect the lives of citizens of the United States against lynch law and mob violence? Are the rights of property, or what a citizen shall drink, or the ages and conditions under which children shall work, any more important to the Nation than life itself?\n\nBlack leaders in the North had insisted that the Republican Party National platform for the presidential election of 1920 include support for anti-lynching legislation. After the election, the black community complained when months passed without Harding's getting a bill introduced and passed by Congress.\n\nDyer introduced a revised version of the bill in the House of Representatives in 1921. Due to \"insistent country-wide demand,\" it passed on January 22, 1922. The first such federal legislation to gain House passage in the twentieth century, it would have enabled the federal government to prosecute the crime. Southern authorities seldom did so. In the South, most blacks had been disfranchised from 1890–1911 by constitutional changes and discriminatory legislation after southern Democrats regained power in the state legislatures. Unable to vote, blacks were disqualified from serving on juries or holding any political office; they had virtually no political power within the official system. In the few cases that came to trial, all-white juries generally never convicted a white man of lynching a black.\n\nThe Republican President Warren G. Harding spoke in favor of Dyer's anti-lynching bill at an appearance in Birmingham, Alabama. With high interest in the bill across the country, it passed the House on January 26, 1922, with the help of Liberal Republicans and eight Democratic Representatives, and went on to the Senate. President Harding stated he would sign the bill if it was passed by the Senate\n\nProponents of Dyer's anti-lynching bill believed that lynching and mob violence took away African-American citizens' rights under the Fourteenth Amendment. These rights included a speedy and fair trial by an impartial jury. Other citizen rights included the right to be informed of the nature of the crime accused, the ability to have witnesses in the defense's favor, and to be represented by council in court. Many blacks felt betrayed by the Republicans due to the bill's slow process to the Senate. A silent protest march by many blacks took place in front of the Capital grounds and White House in 1922 while the bill's constitutionality was being contemplated. A protest sign read, \"Congress discusses constitutionality while the smoke of burning bodies darkens the heavens.\"\n\nAfter Dyer's bill reached the Senate and received a favorable report from the Judiciary Committee, some Republican senators, including most prominently William Borah, an otherwise progressive Senator from Idaho, spoke against it. Borah was concerned about issues of state sovereignty and believed that the bill was not constitutional. He was especially concerned about the clause that provided for federal authorities to punish state officials \"remiss in the suppression of lynchings.\"\n\nA prolonged filibuster by Southern white Democrats prevented consideration of the bill and defeated it. After the Democrats had held up voting on all the national business in the Senate for a week in December 1922 by their filibuster, the Republicans realized they could not overcome the tactic and finally conceded defeat on Dyer's bill. Senator Lee S. Overman of North Carolina told the \"New York Times\" that the \"good negroes of the South did not want the legislation for 'they do not need it'.\"\n\nFollowing the defeat of his first bill in the Senate in 1922, Dyer tried unsuccessfully two more times to get it passed by the Senate. Some of the bill's opponents claimed that the threat of lynching protected white women from sexual advances from black men. The studies by the journalist Ida B. Wells in the late 1890s had shown that black lynch victims were accused or rape or attempted rape only one third of the time. Rather, the murders of blacks were an extreme form of white extrajudicial punishment and community control, often targeting blacks who were economic competitors with whites, who were trying to advance in society, who were in debt to landowners (settlement season for sharecroppers was a time of high rates of lynchings in rural areas), or those who failed to \"stay in their place\". In 1919, according to the Pittsburgh \"Gazette Times\", many Southerners viewed the practice of lynching as a sporting event.\n\nIn 1923, to gain national support for his anti-lynching bill, which was to be heard again that year in the Senate, Dyer toured the western United States to generate public support. His motto for his anti-lynching campaign was \"We have just begun to fight.\" (This was the statement made famous by John Paul Jones.) Dyer attracted mixed black and white audiences in Denver, Portland, Los Angeles, Omaha, and Chicago. He thanked the National Association for the Advancement of Colored People (NAACP) for supporting his bill and praised their continuing to publicize the terrible human toll of lynching in the United States. In Chicago, 4000 people attended his anti-lynching rally. Dyer's campaign received positive coverage by the white mainstream press, which helped strengthen an anti-lynching movement in the West.\n\nThe national attention received by Dyer's anti-lynching bill and speaking campaign may have helped reduce lynchings in the South. Lynchings per year dropped in four years, from 60 in 1918 to 57 in 1922. More significantly, the Great Migration was underway, and black workers by the tens of thousands were leaving the South for Northern and Midwestern industrial cities, for jobs, education, and a chance to escape Jim Crow laws and violence. By 1934, when the Costigan-Wagner anti-lynching bill was introduced, lynchings had dropped to 15 per year. In 1935 and 1938, Senator Borah repeated his constitutional arguments against the bill; he added that he believed such legislation was no longer needed, because the rate of lynchings had fallen so dramatically. By 1940, 1.5 million blacks had left the South in the Great Migration. Another five million left from 1940–1970.\nThe political power of the white Democrats in the South came from their having disfranchised most blacks from 1890–1910. The South was essentially a one-party, Democratic region in which only whites voted and held office, well into the 1960s, but Congressional representation was based on the total population. The situation of disfranchisement did not change markedly until passage in the 1960s of federal civil rights legislation that protected and enforced the constitutional rights of voting and citizenship for African Americans and other minorities.\n\nFrom 1882–1968, \"... nearly 200 anti-lynching bills were introduced in Congress, and three passed the House. Seven presidents between 1890 and 1952 petitioned Congress to pass a federal law.\" None was approved by the Senate because of the powerful opposition of the Southern Democratic voting bloc. In June 2005, through passing a bipartisan resolution sponsored by senators Mary Landrieu of Louisiana and George Allen of Virginia, the US Senate officially apologized for not having passed an anti-lynching law \"when it was most needed.\"\n\nIn 1919, Rep. Dyer authored an anti-crime law that made transporting stolen cars across state borders a federal crime, to be prosecuted by federal law enforcement. In 1956, the FBI Director J. Edgar Hoover said that the law had led to the recovery of 227,752 stolen automobiles worth $212,679,296.\n\nIn December 1922, Rep. Dyer had traveled to the Philippines, then a U.S. territory, according to the 1898 Treaty of Paris. On December 22, Rep. Dyer spoke before the Philippine Senate in Manila, stating that he believed the Philippines would be independent by the next Congress. Rep. Dyer stated he favored Philippine independence. He said that the US would always \"be proud of the Philippines and what we have accomplished here for the Filipinos and for the American people.\" The Philippines were granted commonwealth status in 1935 and finally given independence in 1946.\n\nThe New York Curb Exchange (NYCE) on April 10, 1929, had received a letter written by Rep. Dyer that demanded he be returned money after he had bought and sold at a loss Canadian whiskey company Hiram Walker stock. Rep. Dyer contended he did not know that company made liquor, a contraband product in the U.S. during Prohibition and was forced to sell at a loss. Rep. Dyer believed Hiram Walker and other company liquor stocks had been sold on the NYCE without acknowledgement that these were whiskey companies.\n\nDyer had voted against the Prohibition Eighteenth Amendment, the Volstead Act, the Volstead Act over-riding Presidential veto, and the Jones law. These laws authorized federal enforcement and essentially prohibited the sale of liquor in the United States. Saint Louis had a large beer-brewing industry, and before Prohibition, Missouri was the second-largest wine-producing state in the country. Both industries had been started and developed by German immigrants to the state. Prohibition would seriously damage the economies of Dyer's major city and state.\n\nRep. Dyer served 11 terms in office for Missouri's 12th District. During his second term in office, on June 19, 1914 Dyer was suspended from taking his seat in the House of Representatives due to contested voting election returns in 1912 in Missouri's 12th District. According to the \"New York Times\", Dyer had nothing to do with the voting fraud. The House voted to unseat Dyer, who was active in legislative work, by a 147 to 98 vote. During the vote to oust Dyer, 22 Representatives voted \"present\", rather than give a vote for or against. By a 126 to 108 vote to replace Dyer, the House seated a Democrat, Michael J. Gill, who took the oath of office.\n\nGill served in the House from June 19, 1914, until March 3, 1915. He was defeated by Dyer in the 1914 elections. \n\nRep. Dyer's voted 1556 times out of 2,035 Congressional roll calls. He missed voting 482 times, or 28%, in the Congressional time frame starting on April 5, 1911, and ending on March 1, 1933. The two time periods when Rep. Dyer missed voting 80% of the time were April–June 1912, and October–December 1922.\n\nDyer ran unsuccessfully for reelection in 1932, 1934 and 1936, during the Great Depression. Black voters had been disappointed that the Republicans had failed to deliver on their promise to pass an anti-lynching law, part of the national platform in 1920, and by President Hoover's approach to dealing with economic problems. The administration of President Franklin D. Roosevelt attracted many voters to Democratic candidates because he was putting people to work through the Works Progress Administration and providing social aid programs.\n\nAfter three successive defeats, Dyer retired from politics and returned to private law practice as an attorney.\n\nDyer died in Saint Louis, Missouri on December 15, 1957, at the age of 86. Dyer was buried in the Oak Grove Cemetery in Saint Louis.\n\n"}
{"id": "55223808", "url": "https://en.wikipedia.org/wiki?curid=55223808", "title": "Miscarriage and grief", "text": "Miscarriage and grief\n\nMiscarriage and grief are both an event and subsequent process of grieving that develops in response to a miscarriage. Almost all those experiencing a miscarriage experience grief. This event is often considered to be identical to the loss of a child and has been described as traumatic. \"Devastation\" is another descriptor of miscarriage. Grief differs from the emotion sadness. Sadness is an emotion along with grief, on the other hand, is a response to the loss a of the bond or affection was formed and is a process rather than one single emotional response. Grief is not equivalent to depression. Grief also has physical, cognitive, behavioral, social, cultural, and philosophical dimensions. Bereavement and mourning refer to the ongoing state of loss, and grief is the reaction to that loss. Emotional responses may be bitterness, anxiety, anger, surprise, fear, and disgust and blaming others; these responses may persist for months. Self-esteem can be diminished as another response to miscarriage. Not only does miscarriage tend to be a traumatic event, women describe their treatment afterwards to be worse than the miscarriage itself.\n\nA miscarriage can often be \"heart-breaking\". A miscarriage can effect the women, husband, partner, siblings, grandparents, the whole family system and friends. Almost all those experiencing a miscarriage go through a grieving process. Serious emotional impact is usually experienced immediately after the miscarriage. Some may go through the same loss when an ectopic pregnancy is terminated. In some, the realization of the loss can take weeks. Providing family support to those experiencing the loss can be challenging because some find comfort in talking about the miscarriage while others may find the event painful to discuss. The father of the baby can have the same sense of loss. Expressing feelings of grief and loss can sometimes be harder for men. Some women are able to begin planning their next pregnancy after a few weeks of having the miscarriage. For others, planning another pregnancy can be difficult. Organizations exist that provide information and counselling to help those who have had a miscarriage. Some women have a higher risk of developing prolonged grief and complicated grief than others.\n\nMiscarriage has an emotional effect and can also lead to psychological disorders. One discorder that can develop is primary maternal preoccupation. This is defined as a \" ...'special psychiatric condition' in which the pregnant woman identifies with her baby, highlights the crisis a woman faces when the baby with whom she is preoccupied and identified dies...\" Grieving manifests itself differently for each woman after miscarriage. It may often go unrecognized. The grief that follows a miscarriage resembles, but is not the same as, the grief experienced after the loss of a family member. Disbelief, depression, anger, and yearning, are described as being a part of the normal grieving process. These reactions remain from three to nine months after the loss. Forty-one percent of parents experience a normal, expected decline in grief in the first two years while 59% were delayed in the resolution of their grief.\n\nGrieving can create feelings of loneliness.\nThis grieving has been called a type of psychological trauma. Other serious consequences can develop including depression, anxiety disorder, post-traumatic stress disorder, and somatoform disorder. These responses all are associated with grieving after a miscarriage. Some women are able to complete the grieving process a few weeks after the miscarriage and start anticipating their next pregnancy. Planning another pregnancy is traumatic for others. The impact of a miscarriage can be \"crippling\" psychologically. Anger can be directed toward those who have had successful pregnancies and children. A woman can grieve the \"loss of a future child\" and question her own role as a mother. They may blame themselves or their partner for the miscarriage.\n\nUnsuccessful attempts to become pregnant through in vitro fertilization (IVF) can also illicit a similar grief response in women. Those experiencing a late miscarriage may have more significant distress compared to those who have experienced a miscarriage in the first trimester. Even depression can occur.\n\n\"Women today...are aught in a unique historical moment: technology encourages them to form emotional attachments to their pregnancies, but society has not developed traditions to cushion the shock when those attachmets are shattered.\"\n\nDescriptions of the miscarriage are expressed in non-clinical terms by those who have experienced the event.\n\nMiscarriage has been found to be a traumatic event and a major loss for women. Pregnancy loss, including induced abortion is a risk factor for mental illness. The impact of miscarriage can be underestimated. The trauma can be compounded if the miscarriage was accompanied by visible and relatively large amounts of blood loss.\nThe trauma of miscarriage can be compounded if the miscarriage was accompanied by visible and relatively large amounts of blood loss.\nBipolar disorders are associated with miscarriage. Depression and bilpolar disorder becomes evident after a miscarriage in 43% of women. Some women are more likely to experience complicated and prolonged grief than others.\n\nWomen experiencing miscarriage are at risk for grief reactions, anxiety or depression. Obsessiveness regarding the miscarriage can develop. Primary maternal preoccupation is also considered a consequence of miscarriage. This condition can occur if a woman who develop a close bond \"with her baby\" experiences the loss of the pregnancy.\n\nDifferent grieving \"styles\" can exist and vary between individuals. There can be a complete avoidance of dealing with the memories of the miscarriage and there can be an \"obsessive\" concentration on an event in the miscarriage. This is in contrast with the expected ability to \"reminisce about the loss of a loved one\". Complicated grief differs from the more common form of grief that occurs after a miscarriage. The grieving process associated with other events such as the loss of a spouse or parent is expected to decline in predictable and steady rate. This not true for those experiencing grief after a miscarriage because only 41% follow the expected decline in grief while most, 59% do not fit this pattern.\n\nMiscarriage is associated with post traumatic stress disorder.\nRisks for developing PTSD after miscarriage are: emotional pain, expressions of emotion, and low levels of social support. Even if relatively low levels of stress occur after the miscarriage, symptoms of PTSD including flashbacks, intrusive thoughts, dissociation and hyperarousal can later develop. The effects of stress can complicate miscarriage. Miscarriage is a stressful event and because stress is a risk factor for subsequent miscarriage, its presence can become part of cycle that continues. Lower stress levels are associated with more favorable outcomes in future pregnancies while higher stress levels increase the risk.\n\nPhysical recovery from miscarriage can have an effect on emotional disturbances. The body has to recover from the sudden pregnancy loss. In some instances ffatigue is present. Insomnia can be a problem. The miscarriage is very upsetting to the family and can generate very strong emotions. Some women may feel that the miscarriage occurred because they somehow had caused it. Others may blame the father or partner for the miscarriage. Coping with a miscarriage can very greatly between women and families. Some find it difficult to talk about the miscarriage. The narratives of women tend to coincide with quantified and measurable effects. Some women engage in activities that are believed to aid in recovery such as therapy, religion and art.\nCounseling can be offered but effective interventions to assist in recovery have been difficult to identify due to the reports of efficacy and ineffective counseling. Comparisons are hard to make. Despite the lack of studies that describe effective interventions for those with grief after a miscarriage, some clinicians still offer counselling and follow-up to help women recover and adapt to the loss.\n\nRecommendations to help recover from the event include:\n\nGenerally, the impact of experiencing miscarriage is underestimated. Other methods used to promote recovery are be relaxation techniques, guided imagery, and \"thought-stopping\". Even Gestalt role-playing has been used. Some women can \"emotionally relocate the child\", redefine a relationship \"with the missing child\", and engage in \"continuing the bond\" to incorporate the loss into their life experiences.\n\nWomen who have miscarried report that they were dissatisfied with the care they received from physicians and nurses. One observer highlights the insensitivity of some health care providers when they approach the grieving mother \"...by playing down her emotion as somehow an irrational response...\" Clinicians may not recognize the psychological impact of the miscarriage and can \"expect parents to move on with their lives.\"\n\nSince the experiences of women can vary so widely, sensitive nursing care afterward is appropriate.\n\nOne emotional response to miscarriage is the strong apprehension that can develop anticipating a subsequent pregnancy. Procreation abilities may also be questioned by the woman. Significant distress can develop in the other children in the family when they think a sibling has died. They may regard this as a baby they did not get to meet. They can also experience grief and guilt find it difficult to express these emotions to their parents. The siblings may feel a need to act as if everything is the same and that they are unaffected in an attempt to protect their parents from their own feelings. Children can also need help, understanding and the ability to make sense of the event.\n\nRituals that recognize the loss can be important in coping. Family and friends often conduct a memorial or burial service. Hospitals also can provide support and help memorialize the event. Depending on locale others desire to have a private ceremony. The religious faith of the woman and family impacts the grief process. Conversely, the lack of recognition that the miscarriage has occurred by family and friends can be troubling and add to the trauma of the event.\nGrieving after the loss of a child through miscarriage in other cultures can vary from western culture. An individual’s culture plays a large role in determining an inappropriate pattern of grief, and it is appropriate to take into account cultural norms before reaching a complicated grief diagnosis. There are cultural differences in emotional levels, how these are expressed and how long they are expressed. External symptoms of grief differ in non-Western cultures, presenting increased somatization. Narratives by Swedish women include their own perception of losing a child. Investigations describe their grief over their miscarriage: \"When miscarriage occurs it is not a gore, an embryo, or a fetus they lose, it is their child. They feel that they are the cause of the miscarriage through something they have done, eaten, or thought. They feel abandonment and they grieve for their profound loss; they are actually in bereavement.\" Native American women have cut their long hair following the death of a family member. The narratives of women tend to coincide with quantified and measurable effects. In women who are induced to have an abortion, an identical grieving process can occur. The emotional responses to a spontaneous abortion (miscarriage) and an elective abortion are sometimes identical. Spanish women experience grief in much the same way in the rest of Western culture. Some women find online forums helpful.\n\n\n\n"}
{"id": "14921500", "url": "https://en.wikipedia.org/wiki?curid=14921500", "title": "Nunatak hypothesis", "text": "Nunatak hypothesis\n\nIn biogeography, particularly phytogeography, the nunatak hypothesis about the origin of a biota in formerly glaciated areas is the idea that some or many species have survived the inhospitable period on icefree land such as nunataks. Its antithesis is the \"tabula rasa hypothesis\", which posits that all species have immigrated into completely denuded land after the retreat of glaciers.\n\nBy the mid-20th Century, the nunatak hypothesis was widely accepted among biologists working on the floras of Greenland and Scandinavia. However, while modern geology has established the presence of icefree areas during the last glacial maximum in both Greenland and Scandinavia, molecular techniques have revealed limited between-region genetic differentiation in many Arctic taxa, strongly suggesting a general capacity for long-distance dispersal among polar organisms. This does not directly disprove glacial survival. But it makes it less necessary as an explanation. Moreover, populations that survived on icefree land have probably in most cases been genetically flooded by postglacial immigrants.\n"}
{"id": "15077184", "url": "https://en.wikipedia.org/wiki?curid=15077184", "title": "Peace in Islamic philosophy", "text": "Peace in Islamic philosophy\n\nThe Arabic word \"salaam\" (سلام) (\"peace\") originates from the same root as the word \"Islam\". One Islamic interpretation is that individual personal peace is attained by utterly submitting to Allah. \n\nThe ideal society according to the Qur’an is Dar as-Salam, literally, \"the house of peace\" of which it intones: \"And Allah invites to the 'abode of peace' and guides whom He pleases into the right path.\" \n\nAccording to Ibn Hajar al-Haythami, there will be an era in which justice, plenty, abundance, well-being, security, peace, and brotherhood will prevail among humanity, and one in which people will experience love, self-sacrifice, tolerance, compassion, mercy, and loyalty. Muhammad said that this blessed period will be experienced through the mediation of the Mahdi, who will come in the end times to save the world from chaos, injustice, and moral collapse. He will eradicate godless ideologies and bring an end to the prevailing injustice. Moreover, he will make religion like it was in the days of Muhammad, cause the Qur'an's moral teachings to prevail among humanity, and establish peace and well-being throughout the world.\n\n\n"}
{"id": "24910", "url": "https://en.wikipedia.org/wiki?curid=24910", "title": "Product topology", "text": "Product topology\n\nIn topology and related areas of mathematics, a product space is the Cartesian product of a family of topological spaces equipped with a natural topology called the product topology. This topology differs from another, perhaps more obvious, topology called the box topology, which can also be given to a product space and which agrees with the product topology when the product is over only finitely many spaces. However, the product topology is \"correct\" in that it makes the product space a categorical product of its factors, whereas the box topology is too fine; this is the sense in which the product topology is \"natural\".\n\nGiven \"X\" such that\n\nis the Cartesian product of the topological spaces \"X\", indexed by formula_2, and the canonical projections \"p\" : \"X\" → \"X\", the product topology on \"X\" is defined to be the coarsest topology (i.e. the topology with the fewest open sets) for which all the projections \"p\" are continuous. The product topology is sometimes called the Tychonoff topology.\n\nThe open sets in the product topology are unions (finite or infinite) of sets of the form formula_3, where each \"U\" is open in \"X\" and \"U\" ≠ \"X\" for only finitely many \"i\". In particular, for a finite product (in particular, for the product of two topological spaces), the set of all Cartesian products between one basis elements from each \"X\" gives a basis for the product topology of formula_4. That is, for a finite product, the set of all formula_3, where formula_6 is an element of the (chosen) basis of formula_7, is a basis for the product topology of formula_4.\n\nThe product topology on \"X\" is the topology generated by sets of the form \"p\"(\"U\"), where \"i\" is in \"I \" and \"U\" is an open subset of \"X\". In other words, the sets {\"p\"(\"U\")} form a subbase for the topology on \"X\". A subset of \"X\" is open if and only if it is a (possibly infinite) union of intersections of finitely many sets of the form \"p\"(\"U\"). The \"p\"(\"U\") are sometimes called open cylinders, and their intersections are cylinder sets.\n\nIn general, the product of the topologies of each \"X\" forms a basis for what is called the box topology on \"X\". In general, the box topology is finer than the product topology, but for finite products they coincide.\n\nIf one starts with the standard topology on the real line R and defines a topology on the product of \"n\" copies of R in this fashion, one obtains the ordinary Euclidean topology on R.\n\nThe Cantor set is homeomorphic to the product of countably many copies of the discrete space {0,1} and the space of irrational numbers is homeomorphic to the product of countably many copies of the natural numbers, where again each copy carries the discrete topology.\n\nSeveral additional examples are given in the article on the initial topology.\n\nThe product space \"X\", together with the canonical projections, can be characterized by the following universal property: If \"Y\" is a topological space, and for every \"i\" in \"I\", \"f\" : \"Y\" → \"X\" is a continuous map, then there exists \"precisely one\" continuous map \"f\" : \"Y\" → \"X\" such that for each \"i\" in \"I\" the following diagram commutes:\n\nThis shows that the product space is a product in the category of topological spaces. It follows from the above universal property that a map \"f\" : \"Y\" → \"X\" is continuous if and only if \"f\" = \"p\" o \"f\" is continuous for all \"i\" in \"I\". In many cases it is easier to check that the component functions \"f\" are continuous. Checking whether a map \"f\" : \"Y\" → \"X\" is continuous is usually more difficult; one tries to use the fact that the \"p\" are continuous in some way.\n\nIn addition to being continuous, the canonical projections \"p\" : \"X\" → \"X\" are open maps. This means that any open subset of the product space remains open when projected down to the \"X\". The converse is not true: if \"W\" is a subspace of the product space whose projections down to all the \"X\" are open, then \"W\" need not be open in \"X\". (Consider for instance \"W\" = R \\ (0,1).) The canonical projections are not generally closed maps (consider for example the closed set formula_9 whose projections onto both axes are R \\ {0}).\n\nThe product topology is also called the \"topology of pointwise convergence\" because of the following fact: a sequence (or net) in \"X\" converges if and only if all its projections to the spaces \"X\" converge. In particular, if one considers the space \"X\" = R of all real valued functions on \"I\", convergence in the product topology is the same as pointwise convergence of functions.\n\nAny product of closed subsets of \"X\" is a closed set in \"X\".\n\nAn important theorem about the product topology is Tychonoff's theorem: any product of compact spaces is compact. This is easy to show for finite products, while the general statement is equivalent to the axiom of choice.\n\n\nThe axiom of choice is equivalent to the statement that the product of a collection of non-empty sets is non-empty. The proof is easy enough: one needs only to pick an element from each set to find a representative in the product. Conversely, a representative of the product is a set which contains exactly one element from each component.\n\nThe axiom of choice occurs again in the study of (topological) product spaces; for example, Tychonoff's theorem on compact sets is a more complex and subtle example of a statement that is equivalent to the axiom of choice.\n\n"}
{"id": "1797188", "url": "https://en.wikipedia.org/wiki?curid=1797188", "title": "Pulation square", "text": "Pulation square\n\nIn category theory, a branch of mathematics, a pulation square (also called a Doolittle diagram) is a diagram that is simultaneously a pullback square and a pushout square. It is a self-dual concept.\n\n"}
{"id": "14434839", "url": "https://en.wikipedia.org/wiki?curid=14434839", "title": "Rashid Nugmanov", "text": "Rashid Nugmanov\n\nRashid Nugmanov (also written Rachid Nougmanov; ; born March 19, 1954 in Alma-Ata, Kazakhstan) is a Kazakh film director, dissident, political activist and founder of the cinema movement.\n\nAfter graduating in 1977 from the Architectural Institute in Alma-Ata, Nugmanov enrolled at the prestigious Moscow State Film Institute (VGIK), the world's first institute of cinematography in 1984. His directorial debut, \"The Needle\", premiered in September 1988 at the \"Golden Duke\" Festival in Odessa, where it won the Un Certain Regard prize. Starring popular Soviet rock musician Viktor Tsoi, it was one of the first films to break the taboo against talking about drug addiction in the Soviet Union. The film was released in the USSR in February 1989 with 1,000 prints in circulation and became a box office hit viewed by over 30 million cinemagoers. The film was also a critical success, winning First Prize at the Nuremberg Film Festival and initiating the \"Kazakh New Wave\". He declared, in 1990, the motto of the New Wave of Kazakh cinema: \"We demand no unified philosophy nor uniform artistic views on art. We are unified, instead, in our freedom and love of art\". Nugmanov served as President of the Union of Kazakh Filmmakers from 1989 until 1992, when he wrote, directed and produced \"The Wild East\", a post-apocalyptic punk samurai Ostern which attracted international acclaim at film festivals in Venice, Los Angeles, and Tokyo, and was awarded the Prix Special du Jury in Valenciennes, France. The film marked the end of both the Kazakh New Wave and Nugmanov's active directorial career, although he continued to write screenplays throughout the 1990s.\n\nNugmanov moved to Paris, France in 1993 and currently serves as the General Director of the International Freedom Network, a London-based think tank created to foster democracy in the former Soviet Union. A harsh critic of the political regime of Nursultan Nazarbaev, which he has decried as a mafia, Nugmanov has been responsible for the international relations of dissident organisations including the Forum for Democratic Forces of Kazakhstan and Central Asia, Republican People's Party of Kazakhstan, Democratic Choice of Kazakhstan, and For a Just Kazakhstan.\n\n\n"}
{"id": "26329", "url": "https://en.wikipedia.org/wiki?curid=26329", "title": "Responsible government", "text": "Responsible government\n\nResponsible government is a conception of a system of government that embodies the principle of parliamentary accountability, the foundation of the Westminster system of parliamentary democracy. Governments (the equivalent of the executive branch) in Westminster democracies are responsible to parliament rather than to the monarch, or, in a colonial context, to the imperial government, and in a republican context, to the president, either in full or in part. If the parliament is bicameral, then the government is responsible first to the parliament's lower house, which is more representative than the upper house, as it has more members and they are always directly elected.\n\nResponsible government of parliamentary accountability manifests itself in several ways. Ministers account to Parliament for their decisions and for the performance of their departments. This requirement to make announcements and to answer questions in Parliament means that ministers must have the privileges of the \"floor\", which are only granted to those who are members of either house of Parliament. Secondly, and most importantly, although ministers are officially appointed by the authority of the head of state and can theoretically be dismissed at the pleasure of the sovereign, they concurrently retain their office subject to their holding the confidence of the lower house of Parliament. When the lower house has passed a motion of no confidence in the government, the government must immediately resign or submit itself to the electorate in a new general election.\n\nLastly, the head of state is in turn required to effectuate their executive power only through these responsible ministers. They must never attempt to set up a \"shadow\" government of executives or advisors and attempt to use them as instruments of government, or to rely upon their \"unofficial\" advice. They are bound to take no decision or action that is put into effect under the color of their executive power without that action being as a result of the counsel and advisement of their responsible ministers. Their ministers are required to counsel them (i.e., explain to them and be sure they understand any issue that they will be called upon to decide) and to form and have recommendations for them (i.e., their advice or advisement) to choose from, which are the ministers' formal, reasoned, recommendations as to what course of action should be taken.\n\nAn exception to this is Israel, which operates under a simplified version of the Westminster system.\n\nIn the Canadian system, responsible government was developed between 1846 and 1850, with the executive Council formulating policy with the assistance of the legislative branch the legislature voted approval or disapproval, and the appointed governor enacted those policies that it had approved. It was a transition from the older system whereby the governor took advice from an executive Council, and use the legislature chiefly to raise money. After the formation of elected legislative assemblies starting with Nova Scotia in 1758, governors and their executive councils did not require the consent of elected legislators in order to carry out all their roles. It was only in the decades leading up to Canadian Confederation in 1867 that the governing councils of those British North American colonies became responsible to the elected representatives of the people.\n\nResponsible government was a major element of the gradual development of Canada towards independence. The concept of responsible government is associated in Canada more with self-government than with parliamentary accountability; hence there is the notion that the Dominion of Newfoundland \"gave up responsible government\" when it suspended its self-governing status in 1933, as a result of financial problems. It did not regain responsible government until it became a province of Canada in 1948.\n\nIn the aftermath of the American Revolution, the British government became more sensitive to unrest in its remaining colonies with large populations of European-descended colonists. Elected assemblies were introduced to both Upper Canada and Lower Canada with the Constitutional Act of 1791. Many reformers thought that these assemblies should have some control over the executive power, leading to political unrest between the governors and assemblies in both Upper and Lower Canada. The Lieutenant Governor of Upper Canada Sir Francis Bond Head wrote in one dispatch to London that if responsible government were implemented \"Democracy, in the worst possible Form, will prevail in our Colonies.\" \n\nAfter the 1837 Lower Canada Rebellion led by Louis-Joseph Papineau, and the 1837–1838 Upper Canada Rebellion led by William Lyon Mackenzie, Lord Durham was appointed governor general of British North America and had the task of examining the issues and determining how to defuse tensions. In his report, one of his recommendations was that colonies which were developed enough should be granted \"responsible government\". This term specifically meant the policy that British-appointed governors should bow to the will of elected colonial assemblies.\n\nThe first instance of responsible government in the British Empire outside of the United Kingdom itself was achieved by the colony of Nova Scotia in January–February 1848 through the efforts of Joseph Howe. The plaque in the Nova Scotia House of Assembly erected by the Historic Sites and Monuments Board of Canada reads:\nFirst Responsible Government in the British Empire.<br>\nThe first Executive Council chosen exclusively from the party having a majority in the representative branch of a colonial legislature was formed in Nova Scotia on 2 February 1848. Following a vote of want of confidence in the preceding Council, James Boyle Uniacke, who had moved the resolution, became Attorney General and leader of the Government. Joseph Howe, the long-time campaigner for this \"Peaceable Revolution\", became Provincial Secretary. Other members of the Council were Hugh Bell, Wm. F. Desbarres, Lawrence O.C. Doyle, Herbert Huntingdon, James McNab, Michael Tobin, and George R. Young.\n\nThe colony of New Brunswick soon followed in May 1848 when Lieutenant Governor Edmund Walker Head brought in a more balanced representation of Members of the Legislative Assembly to the Executive Council and ceded more powers to that body.\n\nIn the Province of Canada, responsible government was introduced with the ministry of Louis-Hippolyte LaFontaine and Robert Baldwin in spring 1848; it was put to the test in 1849, when Reformers in the legislature passed the Rebellion Losses Bill. This was a law that provided compensation to French-Canadians who suffered losses during the Rebellions of 1837–1838 in Lower-Canada. \nThe Governor General, Lord Elgin, had serious misgivings about the bill but nonetheless assented to it despite demands from the Tories that he refuse to do so. Elgin was physically assaulted by an English-speaking mob for this, and the Montreal Parliament building was burned to the ground in the ensuing riots. Nonetheless, the Rebellion Losses Bill helped entrench responsible government into Canadian politics.\n\nIn time, the granting of responsible government became the first step on the road to complete independence. Canada gradually gained greater and greater autonomy over a considerable period of time through inter imperial and commonwealth diplomacy, including the British North America Act of 1867, the Statute of Westminster of 1931, and even as late as the patriation of the Constitution Act in 1982 (see Constitution of Canada).\n\nWhile the various colonies in Australia were either sparsely populated or penal settlements or both, executive power was in the hands of the Governors, who, because of the great distance from their superiors in London and the resulting very slow communication, necessarily exercised vast powers.\nHowever, the early colonists, coming mostly from the United Kingdom, were familiar with the Westminster system and made efforts to reform it to increase the opportunity for ordinary men to participate.\n\nThe Governors and London therefore set in motion a gradual process of establishing a Westminster system in the colonies, not so fast as to get ahead of population or economic growth, nor so slow as to provoke clamouring for revolutionary change as happened in America. Initially, this took the form of appointed or partially elected Legislative Councils. Then, during the 1850s, all Australian colonies except Western Australia, along with New Zealand, established both representative and responsible government; Western Australia did the same in 1890.\n\nThe Cape Colony, in Southern Africa, was under responsible self-government from 1872 until 1910 when it became the Cape Province of the new Union of South Africa.\n\nUnder its previous system of representative government, the Ministers of the Cape Government reported directly to the British Imperial Governor, and not to the locally elected representatives in the Cape Parliament. Among Cape citizens of all races, growing anger at their powerlessness in influencing unpopular imperial decisions had repeatedly led to protests and rowdy political meetings – especially during the early \"Convict Crisis\" of the 1840s.\nA popular political movement for responsible government soon emerged, under local leader John Molteno. A protracted struggle was then conducted over the ensuing years as the movement (known informally as \"the responsibles\") grew increasingly powerful, and used their parliamentary majority to put pressure on the British Governor, withholding public finances from him, and conducting public agitations. Not everyone favoured responsible government though, and pro-imperial press outlets even accused the movement of constituting \"crafts and assaults of the devil\".\n\nSupporters believed that the most effective means of instituting responsible government was simply to change the section of the constitution which prevented government officials from being elected to parliament or members of parliament from serving in executive positions. The conflict therefore centred on the changing of this specific section. \"Although responsible government merely required an amendment to s.79 of the constitution, it transpired only after nearly twenty years in 1872 when the so-called \"responsibles\" under Molteno were able to command sufficient support in both houses to secure the passage of the necessary bill.\" Finally, with a parliamentary majority and with the Colonial Office and new Governor Henry Barkly won over, Molteno instituted responsible government, making the Ministers directly responsible to the Cape Parliament, and becoming the Cape's first Prime Minister.\n\nThe ensuing period saw an economic recovery, a massive growth in exports and an expansion of the colony's frontiers. Despite political complications that arose from time to time (such as an ill-fated scheme by the British Colonial Office to enforce a confederation in Southern Africa in 1878, and tensions with the Afrikaner-dominated Government of Transvaal over trade and railroad construction), economic and social progress in the Cape Colony continued at a steady pace until a renewed attempt to extend British control over the hinterland caused the outbreak of the Anglo-Boer Wars in 1899.\n\nAn important feature of the Cape Colony under responsible government was that it was the only state in southern Africa (and one of very few in the world at the time) to have a non-racial system of voting.\n\nLater however – following the South Africa Act 1909 to form the Union of South Africa – this multi-racial universal suffrage was steadily eroded, and eventually abolished by the Apartheid government in 1948.\n\n\nIn the early 1860s, the Prussian Prime Minister Otto von Bismarck was involved in a bitter dispute with the Liberals, who sought to institute a system of responsible government modeled on that of Britain. Bismarck, who strongly opposed that demand, managed to deflect the pressure by embarking energetically and successfully on the unification of Germany. The Liberals, who were also strong German nationalists, backed Bismarck's unification efforts and tacitly accepted that the Constitution of Imperial Germany, crafted by Bismarck, did not include a responsible government – the Chancellor being accountable solely to the emperor and needing no parliamentary confidence. Germany gained a responsible government only with the Weimar Republic and more securely with the creation of the German Federal Republic. Historians account the lack of responsible government in the formative decades of united Germany as one of the factors contributing to the prolonged weakness of German democratic institutions, lasting also after such a government was finally instituted.\n\n\n\n"}
{"id": "44120129", "url": "https://en.wikipedia.org/wiki?curid=44120129", "title": "Revolving rivers", "text": "Revolving rivers\n\nRevolving rivers are a surprising, uncommon way of sand pile growth that can be found in a few sands around the world, but has been studied in detail only for one Cuban sand from a place called Santa Teresa (Pinar del Rio province).\n\nWhen pouring \"revolving\" sand on a flat surface from a fixed position, the growth of a conical pile does not occur by the common avalanche mechanism, where sand slides down the pile in a more or less random fashion. What happens in that a relatively thin \"river\" of flowing sand travels from the pouring point at the apex of the pile to its base, while the rest of the sand at the surface is static. In addition, the river \"revolves\" around the pile either in clockwise or counter-clockwise directions (looking from top) depending on the initial conditions of the experiment. Actually the river constitutes the \"cutting edge\" of a layer of sand that deposits as a helix on the conical pile, and makes it grow.\nFor small sandpiles, rivers are continuous, but they become intermittent\nfor larger piles.\n\nThe phenomenon was observed first by E. Altshuler at the University of Havana in 1995, but at the time he assumed that it was well known, and temporarily forgot about it. In 2000, being at the University of Houston, he told K. E. Bassler, who showed a vivid interest in the matter. Embarrassingly enough, Altshuler was unable to demonstrate it before Bassler using a random sand from Houston, so he had to send him a video from Cuba after his return to the island.\n\nOnce the existence of the strange phenomenon was confirmed for everyone, E. Altshuler and a number of collaborators performed a systematic study in Havana, which was then jointly published with Bassler.\nFurther work has been done to understand in more detail the\nphenomenon, and it has been found in other sands from different parts of the world. \nHowever, the connection between the physical, chemical (and possibly biological) properties of the grains in a specific sand, the nature of the inter-grain interactions, and the emergence of the revolving rivers is still an open question.\n\nSand from Santa Teresa is made of almost pure silicon dioxide grains with an average grain size of 0.2 mm approximately and no visible special features regarding grain shape. But in spite of its apparent simplicity, many puzzles still remain. For example, after many experiments one batch of sand may stop showing revolving rivers (just as singing sand eventually stops singing), which suggests that the decay is connected to certain properties of the surface of the grains that degrade by continued friction.\n\nVideos of the effect are available on YouTube.\n"}
{"id": "2610305", "url": "https://en.wikipedia.org/wiki?curid=2610305", "title": "Savage Worlds", "text": "Savage Worlds\n\nSavage Worlds is a generic role-playing game and miniatures wargame written by Shane Lacy Hensley and published by Pinnacle Entertainment Group. The game emphasizes speed of play and reduced preparation over realism or detail. The game received the 2003 Origin Gamers' Choice Award for best role-playing game.\n\nAlthough \"Savage Worlds\" is a generic rules system, Pinnacle has released \"Savage Settings\" - campaign settings or modules designed specifically for the \"Savage Worlds\" rules. These have included \"Evernight\", \"50 Fathoms\", \"Necessary Evil\", \"Rippers\", and \"Low Life\". Pinnacle has also published setting books based on the company's earlier lines, including \"Deadlands: Reloaded\" as well as the \"Tour of Darkness,\" \"Necropolis,\" and \"Weird War II\" settings based on the \"Weird Wars\" line.\n\nBeginning with \"50 Fathoms\", the majority of settings released by Pinnacle feature a concept known as a \"Plot Point Campaign\". In such campaigns, a series of loosely defined adventure scenarios are presented. A main storyline is presented as a series of \"Plot Points\" and additional side-quests (or \"Savage Tales\") expand the scope of the campaign. This format allow a group of characters to explore the game universe while playing through (or disregarding) the main storyline in a manner similar to that of role-playing video games.\n\nA licensing system is in place for electronic and book publishers to release material for the \"Savage Worlds\" game. Such \"Savaged!\" licensees are allowed to use the \"Savage Worlds\" mascot \"Smiling Jack\" as a logo on their products. Multiple PDF adventure scenarios are available using this licensing system, as well as setting related supplements like the \"Vampire Earth RPG Sourcebook\" and the \"Shaintar Player's Guide\".\n\nPlayer characters are built using a point allocation system, though game masters are encouraged to design non-player characters to the needs of the game rather than to fit the system. Characters in \"Savage Worlds\" are composed of a variety of statistics. These include Race, Traits, Edges, Hindrances and sometimes Powers.\n\nA character's race usually refers to his or her species, which may grant or impose modifiers to characteristics. In some settings (such as the \"Pirates of the Spanish Main\" RPG) this may instead refer to nationality, which typically does not. Nationality based differences may occur in campaigns where certain Skill specializations, Edges and Hindrances are affected by cultural or technological differences or are included to add flavor to a character. For instance, in \"Deadlands: Reloaded\" a non-Chinese character may learn Chinese Martial Arts but cannot acquire and use its Chi-based Powers. In \"Weird War II\" American, British, or French soldiers have special Edges and Hindrances to reflect their different national and military cultures.\n\nA character's traits are characteristics that are rated by a single polyhedral die. The more sides the trait is rated in, the better the character is at the trait - ranging from a 4-sided die (d4 - the lowest) to a 12-sided die (d12 - the highest). So a character with a Strength trait of a ten-sided die (d10) is stronger than a character whose Strength trait is rated with a six-sided die (d6). Traits are divided into Attributes, which are inherent, and Skills, which are learned.\n\nThe five Attributes used in \"Savage Worlds\" are Agility (Physical Precision and Speed), Smarts (Mental Power), Spirit (Willpower), Strength (Physical Power) and Vigor (Physical Health). Attributes start at Level 1 (d4) and cost one point per additional level; Level 1 (d4) in an Attribute would cost nothing and Level 5 (d12) would cost four points. The number of points assigned to spend on Attributes is usually 5 points, but can be more in certain gameworlds.\n\nAttributes are also used to set the point cost of skills in that Attribute's group. The player can buy levels in a Skill at cost as long as its level is lower than its controlling Attribute. The point cost doubles if the Skill level exceeds the controlling Attribute. For instance, Healing is a Smarts-based skill. If a character had a Smarts of Level 1 (4-sided die, or d4) and wants to buy the Healing skill at Level 2 (six-sided die, or d6), it will cost 3 points - one point for Healing at Level 1 (d4) and two points for Healing at Level 2 (d6). If they had a Smarts of Level 2 (or d6) it would only have cost 2 points - one point for Level 1 (d4) and another for Level 2 (d6). The number of points assigned to spend on Skills is usually 15 points, but can be more in certain gameworlds.\n\nIn addition to Attributes a character has the following derived statistics: Pace (ground speed), Parry (the ability to defend one's self), Toughness (resistance to damage) and Charisma (presence and charm). Some setting supplements add a fifth derived statistic such as Reason (Problem Solving), Sanity (Mental Health) or Grit (Mental Strength) to reflect the special needs of the gameworld.\n\nLike in the FUDGE and FATE systems the skills are broad and allow the character to use them for a variety of related tasks. For instance, a character with Fighting would not just be skilled in fighting with their bare hands or with melee weapons. They might also be able to identify and counter an opponent's fighting style, know the name and reputation of a skilled fighter they meet, figure out the nationality and rank of a soldier by their uniform and insignia, or locate and hire a mercenary or bodyguard. Healing could be used to diagnose an illness, identify medicinal herbs or pharmaceutical drugs, find a healer or medical specialist, or prevent a disease outbreak in an encampment by organizing sanitation protocols.\n\nCharacters are also customized with advantages and disadvantages known as Edges and Hindrances. Edges and Hindrances, unlike Traits, are not rated with dice. Edges (character advantages) cost points, are based on their character Rank (Novice, Seasoned, Veteran, Heroic, or Legendary), and are unlocked as the character levels up. They are also grouped by Type, which may - depending on the campaign or world - affect their availability. Beginning Edges can only be granted at character creation. Social Edges affect interaction skills. Combat Edges affect the character's fighting skills and Leadership Edges affect group or massed combat. Professional Edges are related to the character's job or role and affects their career skills. Power, Weird, or Wild Card Edges are supernatural, paranormal, or superhuman advantages and grant bonuses to Powers; they may not be available in mundane game worlds. Hindrances (character disadvantages) grant points and are ranked as Minor (which grants a character point) or Major (which grants two character points).\n\nSome gameworlds have the option of granting superhuman abilities to characters -usually with a magical, mystical, technological, psionic, racial, or mutant origin. They are ranked like Edges (Novice, Seasoned, Veteran, Heroic, or Legendary) and can be expanded by leveling up.\n\nDice are rolled to determine the outcome of character actions and interactions in the game. Usually a trait die is rolled against a target number of four. If the roll equals or exceeds the target number, the action succeeds; otherwise it fails.\n\nIf a player rolls the highest number possible on a given die (such as an 8 on an eight-sided die), the die may be re-rolled and its result added to the initial roll. This is known as \"Acing\". A die may continue to Ace as long as the highest die number is rolled.\n\nPlayer characters and significant non-player characters are known as \"Wild Cards\". Wild Cards get to roll a second die, known as a \"Wild Die\", alongside their trait rolls. This roll may Ace as normal. The player of the Wild Card uses the higher of the two rolls (trait die or Wild Die) to determine the actual result of the roll. In addition Wild Cards also receive a number of Bennies (Slang for benefits, also called poker chips in Dead Lands) per session. These can be traded in to reduce or negate damage from a given attack, or to reroll a trait die, and are used as rewards for good play.\n\nCombat initiative is determined by a standard deck of playing cards (with two jokers); characters act in sequence according to the fall of the cards from highest to lowest. Ties are broken by suit (in order from best to worst, spades, hearts, diamonds, clubs). Jokers beat all other cards and additionally give bonuses on rolls made in the round one receives them. The deck is shuffled at the end of every round in which a joker was dealt.\n\nAny player that receives a Joker during initiative may take his action at any time during the round. So if he wishes to act first, or in response to another PC or NPC acting he may at any point.\n\nIn 1997, Pinnacle published \"Deadlands: the Great Rail Wars\", a miniature wargame set in the \"Weird West\" world of Hensley's Deadlands role-playing game. The rules were a greatly simplified version of the full \"Deadlands\" system, focused on single-figure skirmishes. In 2003 the rules from \"The Great Rail Wars\" were revised and expanded into a generic, simple but complete role-playing system and retitled \"Savage Worlds\". At Origins 2003, \"Savage Worlds\" was awarded the Gamer's Choice Award in the Roleplaying Game category. The main rulebook was revised and released as a PDF format eBook in late 2004, with a print version following in early 2005. The same year, Great White Games began releasing rules expansions in the form of several PDF format genre toolkit books. Self-contained miniature skirmish games based upon the \"Savage Worlds\" engine were also released in print and PDF form.\n\n\"Deadlands Reloaded\", a version of the classic Pinnacle game using the \"Savage Worlds\" rules, was released in May 2006. In late 2005, Pinnacle entered into an agreement with WizKids to publish self-contained RPGs set in the worlds of \"Pirates\", \"Rocketmen\", and \"MageKnight\" using the \"Savage Worlds\" rules. Of the three licenses, only \"The Pirates of the Spanish Main RPG\" saw release, and was published in April 2007. Pinnacle released another licensed game, \"The Savage World of Solomon Kane\", in 2007.\n\nIn October 2007, Pinnacle released the \"Savage Worlds Explorer's Edition\", a digest size paperback edition of the rules. It featured the revisions to melee damage rules first introduced in \"Deadlands Reloaded\", as well as new chase rules, and was released at Origins 2007. At that event, \"Deadlands Reloaded\" won the Origins Award in the category of Best Roleplaying Game Supplement.\n\nIn August 2011, Pinnacle released \"Savage Worlds Deluxe\", a hardcover and expanded version of the rules found in the Explorer's Edition.\n\nIn August 2012, Pinnacle released the digest size paperback edition of the Deluxe rules, \"Savage Worlds Deluxe Explorer's Edition\".\n\nIn 2015 Pinnacle announced a series of supplements converting \"Rifts\" to the Savage Worlds system.\n\nIn 2018 Pinnacle announced a new edition: Savage Worlds Adventure Edition (#SWADE). funded in six minutes.\n\n\nSavage Worlds Adventure Edition Kickstarter Page\n\n\n"}
{"id": "55701343", "url": "https://en.wikipedia.org/wiki?curid=55701343", "title": "Screen memory", "text": "Screen memory\n\nA screen memory is a distorted memory, generally of a visual rather than verbal nature, deriving from childhood. The term was coined by Sigmund Freud, and the concept was the subject of his 1899 paper, \"Screen Memories\".\n\nFreud was struck by the presence, in himself and in other adults, of vivid but bland memories standing from early childhood; and he came to believe that their strength and their preservation both derived from their association with other, less innocent infantile occurrences. As he concluded in his 1899 paper, “The falsified memory is the first that we become aware of….the essential elements of an experience are represented in memory by the inessential elements of the same experience”.\n\nLater writers have emphasised the element of psychological trauma underpinning the screen memory, as well as the way it can encapsulate in miniature the core conflicts of childhood.\n\nThe construction of the screen memory turns on the balance between memory and denial. The blocking of an unpleasant event, thought or perception is facilitated if some harmless, but associated object can be substituted for the unpleasantness itself. The ego searches for memories that can serve as “screens” for the unpleasantness behind, which is thereby removed from consciousness.\n\n\n\n\n"}
{"id": "4952620", "url": "https://en.wikipedia.org/wiki?curid=4952620", "title": "Tarski's axiomatization of the reals", "text": "Tarski's axiomatization of the reals\n\nIn 1936, Alfred Tarski set out an axiomatization of the real numbers and their arithmetic, consisting of only the 8 axioms shown below and a mere four primitive notions: the set of reals denoted R, a binary total order over R, denoted by infix <, a binary operation of addition over R, denoted by infix +, and the constant 1.\n\nThe literature occasionally mentions this axiomatization but never goes into detail, notwithstanding its economy and elegant metamathematical properties. This axiomatization appears little known, possibly because of its second-order nature. Tarski's axiomatization can be seen as a version of the more usual definition of real numbers as the unique Dedekind-complete ordered field; it is however made much more concise by using unorthodox variants of standard algebraic axioms and other subtle tricks (see e.g. axioms 4 and 5, which combine together the usual four axioms of abelian groups).\n\nThe term \"Tarski's axiomatization of real numbers\" also refers to the theory of real closed fields, which Tarski showed completely axiomatizes the first-order theory of the structure 〈R, +, ·, <〉.\n\n\"Axioms of order\" (primitives: R, <):\n\n\n\n\nTo clarify the above statement somewhat, let \"X\" ⊆ R and \"Y\" ⊆ R. We now define two common English verbs in a particular way that suits our purpose:\n\nAxiom 3 can then be stated as:\n\n\"Axioms of addition\" (primitives: R, <, +):\n\n\n\n\n\"Axioms for one\" (primitives: R, <, +, 1):\n\n\n\nThese axioms imply that R is a linearly ordered abelian group under addition with distinguished element 1. R is also Dedekind-complete and divisible.\n\nTarski stated, without proof, that these axioms gave a total ordering. The missing component was supplied in 2008 by Stefanie Ucsnay.\n\nThis axiomatization does not give rise to a first-order theory, because the formal statement of axiom 3 includes two universal quantifiers over all possible subsets of R. Tarski proved these 8 axioms and 4 primitive notions independent.\n\nTarski sketched the (nontrivial) proof of how these axioms and primitives imply the existence of a binary operation called multiplication and having the expected properties, so that R is a complete ordered field under addition and multiplication. This proof builds crucially on the integers with addition being an abelian group and has its origins in Eudoxus' definition of magnitude.\n"}
{"id": "267800", "url": "https://en.wikipedia.org/wiki?curid=267800", "title": "Tit for tat", "text": "Tit for tat\n\nTit for tat is an English saying meaning \"equivalent retaliation\". It is also a highly effective strategy in game theory for the iterated prisoner's dilemma. The strategy was first introduced by Anatol Rapoport in Robert Axelrod's two tournaments, held around 1980. Notably, it was (on both occasions) both the simplest strategy and the most successful in direct competition.\n\nThe phrase originally came from another phrase \"tip for tap\", first used in 1558.\n\nAn agent using this strategy will first cooperate, then subsequently replicate an opponent's previous action. If the opponent previously was cooperative, the agent is cooperative. If not, the agent is not. This is similar to superrationality and reciprocal altruism in biology.\n\nThe success of the tit-for-tat strategy, which is largely cooperative despite that its name emphasizes an adversarial nature, took many by surprise. Arrayed against strategies produced by various teams it won in two competitions. After the first competition, new strategies formulated specifically to combat tit-for-tat failed due to their negative interactions with each other; a successful strategy other than tit-for-tat would have had to be formulated with both tit-for-tat and itself in mind.\n\nThis result may give insight into how groups of animals (and particularly human societies) have come to live in largely (or entirely) cooperative societies, rather than the individualistic \"red in tooth and claw\" way that might be expected from individuals engaged in a Hobbesian state of nature. This, and particularly its application to human society and politics, is the subject of Robert Axelrod's book \"The Evolution of Cooperation\".\n\nMoreover, the tit-for-tat strategy has been of beneficial use to social psychologists and sociologists in studying effective techniques to reduce conflict. Research has indicated that when individuals who have been in competition for a period of time no longer trust one another, the most effective competition reverser is the use of the tit-for-tat strategy. Individuals commonly engage in behavioral assimilation, a process in which they tend to match their own behaviors to those displayed by cooperating or competing group members. Therefore, if the tit-for-tat strategy begins with cooperation, then cooperation ensues. On the other hand, if the other party competes, then the tit-for-tat strategy will lead the alternate party to compete as well. Ultimately, each action by the other member is countered with a matching response, competition with competition and cooperation with cooperation.\n\nIn the case of conflict resolution, the tit-for-tat strategy is effective for several reasons: the technique is recognized as \"clear\", \"nice\", \"provocable\", and \"forgiving\". Firstly, It is a \"clear\" and recognizable strategy. Those using it quickly recognize its contingencies and adjust their behavior accordingly. Moreover, it is considered to be \"nice\" as it begins with cooperation and only defects in following competitive move. The strategy is also \"provocable\" because it provides immediate retaliation for those who compete. Finally, it is \"forgiving\" as it immediately produces cooperation should the competitor make a cooperative move.\n\nThe implications of the tit-for-tat strategy have been of relevance to conflict research, resolution and many aspects of applied social science.\n\nWhile Axelrod has empirically shown that the strategy is optimal in some cases of direct competition, two agents playing tit for tat remain vulnerable. A one-time, single-bit error in either player's interpretation of events can lead to an unending \"death spiral\": if one agent defects and the opponent cooperates, then both agents will end up alternating cooperate and defect, yielding a lower payoff than if both agents were to continually cooperate. This situation frequently arises in real world conflicts, ranging from schoolyard fights to civil and regional wars. The reason for these issues is that tit for tat is not a subgame perfect equilibrium, except under knife-edge conditions on the discount rate.\nWhile this subgame is not directly reachable by two agents playing tit for tat strategies, a strategy must be a Nash equilibrium in all subgames to be subgame perfect. Further, this subgame may be reached if any noise is allowed in the agents' signaling. A subgame perfect variant of tit for tat known as \"contrite tit for tat\" may be created by employing a basic reputation mechanism.\n\nTit for two tats could be used to mitigate this problem; see the description below. \"Tit for tat with forgiveness\" is a similar attempt to escape the death spiral. When the opponent defects, a player employing this strategy will occasionally cooperate on the next move anyway. The exact probability that a player will respond with cooperation depends on the line-up of opponents.\n\nFurthermore, the tit-for-tat strategy is not proved optimal in situations short of total competition. For example, when the parties are friends it may be best for the friendship when a player cooperates at every step despite occasional deviations by the other player. Most situations in the real world are less competitive than the total competition in which the tit-for-tat strategy won its competition.\n\nTit for two tats is similar to tit for tat in that it is nice, retaliating, forgiving and non-envious, the only difference between the two being how forgiving the strategy is.\n\nIn a tit for tat strategy, once an opponent defects, the tit for tat player immediately responds by defecting on the next move. This has the unfortunate consequence of causing two retaliatory strategies to continuously defect against each other resulting in a poor outcome for both players. A tit for two tats player will let the first defection go unchallenged as a means to avoid the \"death spiral\" of the previous example. If the opponent defects twice in a row, the tit for two tats player will respond by defecting.\n\nThis strategy was put forward by Robert Axelrod during his second round of computer simulations at RAND. After analyzing the results of the first experiment, he determined that had a participant entered the tit for two tats strategy it would have emerged with a higher cumulative score than any other program. As a result, he himself entered it with high expectations in the second tournament. Unfortunately, owing to the more aggressive nature of the programs entered in the second round, which were able to take advantage of its highly forgiving nature, tit for two tats did significantly worse (in the game-theory sense) than tit for tat.\n\nBitTorrent peers use tit-for-tat strategy to optimize their download speed. More specifically, most BitTorrent peers use a variant of Tit for two Tats which is called \"regular unchoking\" in BitTorrent terminology. BitTorrent peers have a limited number of upload slots to allocate to other peers. Consequently, when a peer's upload bandwidth is saturated, it will use a tit-for-tat strategy. Cooperation is achieved when upload bandwidth is exchanged for download bandwidth. Therefore, when a peer is not uploading in return to our own peer uploading, the BitTorrent program will \"choke\" the connection with the uncooperative peer and allocate this upload slot to a hopefully more cooperating peer. \"Regular unchoking\" corresponds very strongly to always cooperating on the first move in prisoner's dilemma. Periodically, a peer will allocate an upload slot to a randomly chosen uncooperative peer (\"unchoke\"). This is called \"optimistic unchoking\". This behavior allows searching for more cooperating peers and gives a second chance to previously non-cooperating peers. The optimal threshold values of this strategy are still the subject of research.\n\nStudies in the prosocial behaviour of animals have led many ethologists and evolutionary psychologists to apply tit-for-tat strategies to explain why altruism evolves in many animal communities. Evolutionary game theory, derived from the mathematical theories formalised by von Neumann and Morgenstern (1953), was first devised by Maynard Smith (1972) and explored further in bird behaviour by Robert Hinde. Their application of game theory to the evolution of animal strategies launched an entirely new way of analysing animal behaviour.\n\nReciprocal altruism works in animal communities where the cost to the benefactor in any transaction of food, mating rights, nesting or territory is less than the gains to the beneficiary. The theory also holds that the act of altruism should be reciprocated if the balance of needs reverse. Mechanisms to identify and punish \"cheaters\" who fail to reciprocate, in effect a form of tit for tat, are important to regulate reciprocal altruism. For example, tit-for-tat is suggested to be the mechanism of cooperative predator inspection behavior in guppies.\n\nThe tit-for-tat inability of either side to back away from conflict, for fear of being perceived as weak or as cooperating with the enemy, has been the source of many conflicts throughout history.\n\nHowever, the tit for tat strategy has also been detected by analysts in the spontaneous non-violent behaviour, called \"live and let live\" that arose during trench warfare in the First World War. Troops dug in only a few hundred feet from each other would evolve an unspoken understanding. If a sniper killed a soldier on one side, the other could expect an equal retaliation. Conversely, if no one was killed for a time, the other side would acknowledge this implied \"truce\" and act accordingly. This created a \"separate peace\" between the trenches.\n\n"}
{"id": "4095924", "url": "https://en.wikipedia.org/wiki?curid=4095924", "title": "Value (ethics)", "text": "Value (ethics)\n\nIn ethics, value denotes the degree of importance of some thing or action, with the aim of determining what actions are best to do or what way is best to live (normative ethics), or to describe the significance of different actions. Value systems are proscriptive and prescriptive beliefs, they affect ethical behavior of a person or are the basis of their intentional activities. Often primary values are strong and secondary values are suitable for changes, What makes an action valuable may in turn depend on the ethical values of the objects it increases, decreases or alters. An object with \"ethic value\" may be termed an \"ethic or philosophic good\" (noun sense).\n\nValues can be defined as broad preferences concerning appropriate courses of actions or outcomes. As such, values reflect a person's sense of right and wrong or what \"ought\" to be. \"Equal rights for all\", \"Excellence deserves admiration\", and \"People should be treated with respect and dignity\" are representatives of values. Values tend to influence attitudes and behavior and these types include ethical/moral values, doctrinal/ideological (religious, political) values, social values, and aesthetic values. It is debated whether some values that are not clearly physiologically determined, such as altruism, are intrinsic, and whether some, such as acquisitiveness, should be classified as vices or virtues.\n\nEthical value may be regarded as a study under ethics, which, in turn, may be grouped as philosophy. Similarly, \"ethical value\" may be regarded as a subgroup of a broader field of philosophic value sometimes referred to as axiology. Ethical value denotes something's degree of importance, with the aim of determining what action or life is best to do, or at least attempt to describe the value of different actions. \n\nThe study of ethical value is also included in value theory. In addition, values have been studied in various disciplines: anthropology, behavioral economics, business ethics, corporate governance, moral philosophy, political sciences, social psychology, sociology and theology.\n\n\"Ethical value\" is sometimes used synonymously with goodness. However, goodness has many other meanings and may be regarded as more ambiguous.\n\nPersonal values exist in relation to cultural values, either in agreement with or divergence from prevailing norms. A culture is a social system that shares a set of common values, in which such values permit social expectations and collective understandings of the good, beautiful and constructive. Without normative personal values, there would be no cultural reference against which to measure the virtue of individual values and so cultural identity would disintegrate.\n\nPersonal values provide an internal reference for what is good, beneficial, important, useful, beautiful, desirable and constructive. Values are one of the factors that generate behaviour and influence the choices made by an individual.\n\nValues may help common human problems for survival by comparative rankings of value, the results of which provide answers to questions of why people do what they do and in what order they choose to do them. Moral, religious, and personal values, when held rigidly, may also give rise to conflicts that result from a clash between differing world views.\n\nOver time the public expression of personal values that groups of people find important in their day-to-day lives, lay the foundations of law, custom and tradition. Recent research has thereby stressed the \"implicit nature of value communication\". Consumer behavior research proposes there are six internal values and three external values. They are known as List of Values (LOV) in management studies. They are self respect, warm relationships, sense of accomplishment, self-fulfillment, fun and enjoyment, excitement, sense of belonging, being well respected, and security. From a functional aspect these values are categorized into three and they are interpersonal relationship area, personal factors, and non-personal factors. From an ethnocentric perspective, it could be assumed that a same set of values will not reflect equally between two groups of people from two countries. Though the core values are related, the processing of values can differ based on the cultural identity of an individual.\n\nIndividual cultures emphasize values which their members broadly share. Values of a society can often be identified by examining the level of honor and respect received by various groups and ideas. In the United States of America, for example, top-level professional athletes receive more respect (measured in terms of monetary payment) than university professors. Another example is that certain voters (taken from surveys) in the United States would not willingly elect an atheist as president, suggesting that believing in a God is a generally shared value.\n\nValues clarification differs from cognitive moral education:\n\nValues relate to the norms of a culture, but they are more global and intellectual than norms. Norms provide rules for behavior in specific situations, while values identify what should be judged as good or evil. While norms are standards, patterns, rules and guides of expected behavior, values are abstract concepts of what is important and worthwhile. Flying the national flag on a holiday is a norm, but it reflects the value of patriotism. Wearing dark clothing and appearing solemn are normative behaviors to manifest respect at a funeral. Different cultures represent values differently and to different levels of emphasis. \"Over the last three decades, traditional-age college students have shown an increased interest in personal well-being and a decreased interest in the welfare of others.\" Values seemed to have changed, affecting the beliefs, and attitudes of the students.\n\nMembers take part in a culture even if each member's personal values do not entirely agree with some of the normative values sanctioned in that culture. This reflects an individual's ability to synthesize and extract aspects valuable to them from the multiple subcultures they belong to.\n\nIf a group member expresses a value that seriously conflicts with the group's norms, the group's authority may carry out various ways of encouraging conformity or stigmatizing the non-conforming behavior of that member. For example, imprisonment can result from conflict with social norms that the state has established as law.\n\nFurthermore, institutions in the global economy can genuinely respect values which are of three kinds based on a \"triangle of coherence\". In the first instance, a value may come to expression within the World Trade Organization (WTO), as well as (in the second instance) within the United Nations – particularly in the Educational, Scientific and Cultural Organization (UNESCO) – providing a framework for global legitimacy through accountability. In the third instance, the expertise of member-driven international organizations and civil society depends on the incorporation of flexibility in the rules, to preserve the expression of identity in a globalized world..\n\nNonetheless, in warlike economic competition, differing views may contradict each other, particularly in the field of culture. Thus audiences in Europe may regard a movie as an artistic creation and grant it benefits from special treatment, while audiences in the United States may see it as mere entertainment, whatever its artistic merits. EU policies based on the notion of \"cultural exception\" can become juxtaposed with the policy of \"cultural specificity\" on the liberal Anglo-Saxon side. Indeed, international law traditionally treats films as property and the content of television programs as a service. Consequently, cultural interventionist policies can find themselves opposed to the Anglo-Saxon liberal position, causing failures in international negotiations.\n\nValues are generally received through cultural means, especially diffusion and transmission or socialization from parents to children. Parents in different cultures have different values. For example, parents in a hunter–gatherer society or surviving through subsistence agriculture value practical survival skills from a young age. Many such cultures begin teaching babies to use sharp tools, including knives, before their first birthdays. Italian parents value social and emotional abilities and having an even temperament. Spanish parents want their children to be sociable. Swedish parents value security and happiness. Dutch parents value independence, long attention spans, and predictable schedules. American parents are unusual for strongly valuing intellectual ability, especially in a narrow \"book learning\" sense. The Kipsigis people of Kenya value children who are not only smart, but who employ that intelligence in a responsible and helpful way, which they call \"ng'om\". Luos of Kenya value education and pride which they call \"nyadhi\".\n\nFactors that influence the development of cultural values are summarized below.\n\nThe Inglehart–Welzel cultural map of the world is a two-dimensional cultural map showing the cultural values of the countries of the world along two dimensions: The \"traditional versus secular-rational values\" reflect the transition from a religious understanding of the world to a dominance of science and bureaucracy. The second dimension named \"survival values versus self-expression values\" represents the transition from industrial society to post-industrial society.\n\nCultures can be distinguished as tight and loose in relation to how much they adhere to social norms and tolerates deviance. Tight cultures are more restrictive, with stricter disciplinary measures for norm violations while loose cultures have weaker social norms and a higher tolerance for deviant behavior. A history of threats, such as natural disasters, high population density, or vulnerability to infectious diseases, is associated with greater tightness. It has been suggested that tightness allows cultures to coordinate more effectively to survive threats. \n\nStudies in evolutionary psychology have led to similar findings. The so-called regality theory finds that war and other perceived collective dangers have a profound influence on both the psychology of individuals and on the social structure and cultural values. A dangerous environment leads to a hierarchical, authoritarian, and warlike culture, while a safe and peaceful environment fosters an egalitarian and tolerant culture.\n\nRelative values differ between people, and on a larger scale, between people of different cultures. On the other hand, there are theories of the existence of \"absolute values\", which can also be termed \"noumenal values\" (and not to be confused with mathematical absolute value). An absolute value can be described as philosophically absolute and independent of individual and cultural views, as well as independent of whether it is known or apprehended or not. Ludwig Wittgenstein was pessimistic towards the idea that an elucidation would ever happen regarding the absolute values of actions or objects; \"\"we can speak as much as we want about \"life\" and \"its meaning,\" and believe that what we say is important. But these are no more than expressions and can never be facts, resulting from a tendency of the mind and not the heart or the will\".\n\nPhilosophic value may be split into \"instrumental value\" and \"intrinsic values\". An instrumental value is worth having as a means towards getting something else that is good (e.g., a radio is instrumentally good in order to hear music). An intrinsically valuable thing is worth for itself, not as a means to something else. It is giving value intrinsic and extrinsic properties.\n\nAn \"ethic good\" with \"instrumental value\" may be termed an ethic mean, and an \"ethic good\" with \"intrinsic value\" may be termed an end-in-itself. An object may be both a mean and end-in-itself.\n\nIntrinsic and instrumental goods are not mutually exclusive categories. Some objects are both good in themselves, and also good for getting other objects that are good. \"Understanding science\" may be such a good, being both worthwhile in and of itself, and as a means of achieving other goods. In these cases, the sum of instrumental (specifically the all instrumental value) and intrinsic value of an object may be used when putting that object in value systems, which is a set of consistent values and measures.\n\nThe \"intensity\" of philosophic value is the degree it is generated or carried out, and may be regarded as the prevalence of the good, the object having the value.\n\nIt should not be confused with the amount of value per object, although the latter may vary too, e.g. because of instrumental value conditionality. For example, taking a fictional life-stance of accepting waffle-eating as being the end-in-itself, the intensity may be the speed that waffles are eaten, and is zero when no waffles are eaten, e.g. if no waffles are present. Still, each waffle that had been present would still have value, no matter if it was being eaten or not, independent on intensity.\n\n\"Instrumental value conditionality\" in this case could be exampled by every waffle not present, making them less valued by being far away rather than easily accessible.\n\nIn many life stances it is the product of value and intensity that is ultimately desirable, i.e. not only to generate value, but to generate it in large degree. Maximizing lifestances have the highest possible intensity as an imperative.\n\nThere may be a distinction between positive and negative philosophic or ethic value. While positive ethic value generally correlates with something that is pursued or maximized, negative ethic value correlates with something that is avoided or minimized.\n\nNegative value may be both intrinsic negative value and/or instrumental negative value.\n\nA \"protected value\" (also sacred value) is one that an individual is unwilling to trade off no matter what the benefits of doing so may be. For example, some people may be unwilling to kill another person, even if it means saving many others individuals. Protected values tend to be \"intrinsically good\", and most people can in fact imagine a scenario when trading off their most precious values would be necessary. If such trade-offs happen between two competing protected values such as killing a person and defending your family they are called \"tragic trade-offs.\"\n\nProtected values have been found to be play a role in protracted conflicts (e.g., the Israeli-Palestinian conflict) because they can hinder businesslike (<nowiki>\"utilitarian\"</nowiki>) negotiations. A series of experimental studies directed by Scott Atran and Ángel Gómez among combatants on the ISIS frontline in Iraq and with ordinary citizens in Western Europe suggest that commitment to sacred values motivate the most \"devoted actors\" to make the costliest sacrifices, including willingness to fight and die, as well as a readiness to forsake close kin and comrades for those values if necessary. From the perspective of utilitarianism, protected values are biases when they prevent utility from being maximized across individuals.\n\nAccording to Jonathan Baron and Mark Spranca, protected values arise from norms as described in theories of deontological ethics (the latter often being referred to in context with Immanuel Kant). The protectedness implies that people are concerned with their participation in transactions rather than just the consequences of it.\n\nA \"value system\" is a set of consistent values used for the purpose of ethical or ideological integrity.\n\nAs a member of a society, group or community, an individual can hold both a personal value system and a communal value system at the same time. In this case, the two value systems (one personal and one communal) are externally consistent provided they bear no contradictions or situational exceptions between them.\n\nA value system in its own right is internally consistent when\n\nConversely, a value system by itself is internally inconsistent if:\n\nAbstract exceptions serve to reinforce the ranking of values. Their definitions are generalized enough to be relevant to any and all situations. Situational exceptions, on the other hand, are ad hoc and pertain only to specific situations. The presence of a type of exception determines one of two more kinds of value systems:\n\nThe difference between these two types of systems can be seen when people state that they hold one value system yet in practice deviate from it, thus holding a different value system. For example, a religion lists an absolute set of values while the practice of that religion may include exceptions.\n\nImplicit exceptions bring about a third type of value system called a formal value system. Whether idealized or realized, this type contains an implicit exception associated with each value: \"as long as no higher-priority value is violated\". For instance, a person might feel that lying is wrong. Since preserving a life is probably more highly valued than adhering to the principle that lying is wrong, lying to save someone’s life is acceptable. Perhaps too simplistic in practice, such a hierarchical structure may warrant explicit exceptions.\n\nAlthough sharing a set of common values, like hockey is better than baseball or ice cream is better than fruit, two different parties might not rank those values equally. Also, two parties might disagree as to certain actions are right or wrong, both in theory and in practice, and find themselves in an ideological or physical conflict. Ethonomics, the discipline of rigorously examining and comparing value systems, enables us to understand politics and motivations more fully in order to resolve conflicts.\n\nAn example conflict would be a value system based on individualism pitted against a value system based on collectivism. A rational value system organized to resolve the conflict between two such value systems might take the form below. Note that added exceptions can become recursive and often convoluted.\n\nPhilosophical value is distinguished from economic value, since it is independent on some other desired condition or commodity. The economic value of an object may rise when the exchangeable desired condition or commodity, e.g. money, become high in supply, and vice versa when supply of money becomes low.\n\nNevertheless, economic value may be regarded as a result of philosophical value. In the subjective theory of value, the personal philosophic value a person puts in possessing something is reflected in what economic value this person puts on it. The limit where a person considers to purchase something may be regarded as the point where the \"personal philosophic value\" of possessing something exceeds the personal philosophic value of what is given up in exchange for it, e.g. money. In this light, everything can be said to have a \"personal economic value\" in contrast to its \"societal economic value.\"\n\n\n"}
{"id": "42296094", "url": "https://en.wikipedia.org/wiki?curid=42296094", "title": "Wilhelm Mautner", "text": "Wilhelm Mautner\n\nWilhelm Mautner (1889-1944) was born in Vienna. He was an Austrian-German economist and attorney-in-fact of the Rotterdamse Bank who spent a part of his life in the Netherlands. Mautner was born Jewish. He was also an art collector.\n\nAfter a short professional career in banking and industry, he studied economics in Paris, Vienna and Tübingen, where he finished his studies and wrote a thesis about Bolshevism in 1919. In the same year, he moved to the Netherlands.\n\nFrom 1929 until 1944 he lived in Amsterdam. Mautner never married and had no children. On 22 July 1941, he drew up a will in which he appointed his brother in Ohio in the United States as his sole heir. During the war Mautner attempted unsuccessfully to escape from the Nazi regime. Mautner tried to get an exit visa for the United States with the help of his brother, who lived in New York. In December 1943 Mautner was removed from his home in Amsterdam during a raid and transported to Westerbork transit camp and from there to Theresienstadt concentration camp. Mautner died on or around 29 September 1944 in Auschwitz concentration camp.\n\nMautner collected paintings. His collection also included some Dutch old masters. He continued purchasing works of art during the war. At that time he was in frequent contact with Max Jakob Friedlander. From 1941, he was no longer able to do so in his own name because of his Jewish origins. He gave works of art to various people for safekeeping. In February 1942, Mautner was ordered to move to Tugelaweg in Amsterdam as a result of the forced relocation of Jews. He sent 15 paintings for safe-keeping with the collector Dr J. van Dongen on the city's Museumplein. The latter kept all of these items until after the war. Other works, including a Brueghel and a Timmermann, were sold under duress in 1943 by Mautner. Various of Mautner's works of art and other possessions were recovered for his heirs, by custodians of his estate after the war. The 15 paintings in J.A. van Dongen's custody were probably bought by van Dongen with the consent of Mautner's heirs.\nThree of the paintings that Mautner sold under duress are:\n\nThe latter two were acquired by the Sonderauftrag Linz and returned to the Netherlands after the war, where they became part of the NK-Collection of the Stichting Nederlands Kunstbezit.\n\nHe also sold 12 paintings, primarily 19th century, to W. Kadzik from Vienna. Mautner could not act as a seller, due to his Jewish origin, so he asked his friend Hans Alfred Wetzlar to fulfill the formalities. It is unknown where the works are.\n\n\n"}
{"id": "13798788", "url": "https://en.wikipedia.org/wiki?curid=13798788", "title": "Ziran", "text": "Ziran\n\nZiran is a key concept in Daoism that literally means \"self so; so of its own; so of itself\" and thus \"naturally; natural; spontaneously; freely; in the course of events; of course; doubtlessly\". This Chinese word is a two-character compound of \"zi\" (自) \"nose; self; oneself; from; since\" and \"ran\" (然) \"right; correct; so; yes\", which is used as a \"-ran\" suffix marking adjectives or adverbs (roughly corresponding to English \"-ly\"). In Chinese culture, the nose (or zi) is a common metaphor for a person's point of view.\n\nThe word 'ziran' first occurs in the Daodejing (17, 23, 25, 51) and refers to the structure of Dao, which cannot be referred back to anything else. It is generally accepted that the philosopher Laozi, author of the Daodejing, coined the term. Ziran is a central concept of Daoism, closely tied to the practice of wuwei, detached or effortless action. Ziran refers to a state of \"as-it-isness,\" the most important quality for anyone following Daoist beliefs. To become nearer to a state of ziran, one must become separate from unnatural influences and returned to an entirely natural, spontaneous state. Ziran is related to developing an \"altered sense of human nature and of nature per se\". When it comes to sensibility of Taoism, the moral import can be most found in ziran.\n\nZiran has been interpreted and reinterpreted in a great number of ways over time. Most commonly it has been seen as a model that was followed by the Dao, Heaven, Earth, and Man in turn, based on the traditional translation and interpretation of Chapter 25 of the Daodejing. Wang's more modern translation eliminates the logical flaw that arises when one considers that to model oneself after another entity may be to become less natural, to lose the 'as-it-isness' that ziran refers to. Wang reinterprets the words of Chapter 25 to be instructions to follow the model set by Earth's being Earth, by Heaven's being Heaven, and by the Dao being the Dao; each behaving perfectly in accordance with ziran. This interpretation reaffirms that the base nature of the Dao is one of complete naturalness.\n\nWing-Chuek Chan provides another translation of 'ziran:' \"It is so by virtue of its own\". This brings up ziran's link to another Daoist belief, specifically that the myriad things exist because of the qualities that they possess, not because they were created by any being to fulfill a purpose or goal. The only thing that a being must be when it exists in accordance with ziran is ultimately natural, unaffected by artificial influences.\n\nZiran and Tianran are related concepts. Tianran refers to a thing created by heaven that is ultimately untouched by human influence, a thing fully characterized by ziran. The two terms are sometimes interchangeably used. It can be said that by gaining ziran, a person grows nearer to a state of tianran.\n\nZiran can also be looked at from under Buddha's influence, \"non-substantial\". It is then believed to mean 'having no nature of its own'. In this aspect it is seen as a synonym of real emptiness.\n\nD. T. Suzuki, in a brief article penned in 1959, makes the suggestion of \"ziran\" as an aesthetic of action: \"Living is an act of creativity demonstrating itself. Creativity is objectively seen as necessity, but from the inner point of view of Emptiness it is 'just-so-ness,' (ziran). It literally means 'byitself-so-ness,' implying more inner meaning than 'spontaneity' or 'naturalness'\".\n\n"}
{"id": "2321181", "url": "https://en.wikipedia.org/wiki?curid=2321181", "title": "Élan vital", "text": "Élan vital\n\nÉlan vital () is a term coined by French philosopher Henri Bergson in his 1907 book \"Creative Evolution\", in which he addresses the question of self-organisation and spontaneous morphogenesis of things in an increasingly complex manner. \"Elan vital\" was translated in the English edition as \"vital impetus\", but is usually translated by his detractors as \"vital force\". It is a hypothetical explanation for evolution and development of organisms, which Bergson linked closely with consciousness – with the intuitive perception of experience and the flow of inner time.\n\nDistant anticipations of Bergson can be found in the work of the pre-Christian Stoic philosopher Posidonius, who postulated a \"vital force\" emanated by the sun to all living creatures on the Earth's surface, and in that of Zeno of Elea. The concept of \"élan vital\" is also similar to Arthur Schopenhauer's concept of the will-to-live.\n\nIt was believed by some that this essence (\"élan vital\") could be harvested and embedded into an inanimate substance and activated with electricity, perhaps taking literally another of Bergson's metaphorical descriptions, the \"current of life\".\n\nThe French philosopher Gilles Deleuze attempted to recoup the novelty of Bergson's idea in his book \"Bergsonism\", though the term itself underwent substantial changes by Deleuze. No longer considered a mystical, elusive force acting on brute matter, as it was in the vitalist debates of the late 19th century, \"élan vital\" in Deleuze's hands denotes an internal force, a substance in which the distinction between organic and inorganic matter is indiscernible, and the emergence of life undecidable.\n\nThe notion of \"élan vital\" also had considerable influence on the psychiatrist and phenomenologist Eugène Minkowski and his own concept of a \"personal élan\" – the element which keeps us in touch with a feeling of life (and is lost in autism).\n\nThe French army incorporated the doctrine of \"élan vital\" into its thinking during the leadup to the First World War by arguing that the spirit of individual soldiers was more important for victory than weapons.\n\n"}
