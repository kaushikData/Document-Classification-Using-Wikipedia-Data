{"id": "58263133", "url": "https://en.wikipedia.org/wiki?curid=58263133", "title": "Ableism in Canada", "text": "Ableism in Canada\n\nAbleism in Canada refers to a set of discourses, behaviours and structures which express feelings of anxiety, fear, hostility and antipathy towards disabled people in Canada. \n\nThe specific types of discrimination that have occurred or are still occurring in Canada include the ability to access to important facilities such as infrastructure within the transport network, restrictive immigration policies, involuntary sterilization to stop the disabled from having offspring, barriers to employment opportunities, a wage that is high enough to maintain a minimal standard of living, and institutionalization of the disabled in substandard conditions. Austerity decisions made by the government of Canada have also at time been referred to as ableist, such as funding cuts that put disabled people at risk of living in abusive arrangements.\n"}
{"id": "6308558", "url": "https://en.wikipedia.org/wiki?curid=6308558", "title": "Analysis of competing hypotheses", "text": "Analysis of competing hypotheses\n\nThe analysis of competing hypotheses (ACH) allegedly provides an unbiased methodology for evaluating multiple competing hypotheses for observed data. It was developed by Richards (Dick) J. Heuer, Jr., a 45-year veteran of the Central Intelligence Agency, in the 1970s for use by the Agency. ACH is used by analysts in various fields who make judgments that entail a high risk of error in reasoning. It helps an analyst overcome, or at least minimize, some of the cognitive limitations that make prescient intelligence analysis so difficult to achieve.\n\nACH was a step forward in intelligence analysis methodology, but it was first described in relatively informal terms. Producing the best available information from uncertain data remains the goal of researchers, tool-builders, and analysts in industry, academia and government. Their domains include data mining, cognitive psychology and visualization, probability and statistics, etc. Abductive reasoning is an earlier concept with similarities to ACH.\n\nHeuer outlines the ACH process in considerable depth in his book, \"Psychology of Intelligence Analysis\". It consists of the following steps:\n\nThere are many benefits of doing an ACH matrix. It is auditable. It is widely believed to help overcome cognitive biases, though there is a lack of strong empirical evidence to support this belief. Since the ACH requires the analyst to construct a matrix, the evidence and hypotheses can be backtracked. This allows the decisionmaker or other analysts to see the sequence of rules and data that led to the conclusion.\n\nThe process to create an ACH is time consuming. The ACH matrix can be problematic when analyzing a complex project. It can be cumbersome for an analyst to manage a large database with multiple pieces of evidence.\n\nEspecially in intelligence, both governmental and business, analysts must always be aware that the opponent(s) is intelligent and may be generating information intended to deceive. Since deception often is the result of a cognitive trap, Elsaesser and Stech use state-based hierarchical plan recognition (see abductive reasoning) to generate causal explanations of observations. The resulting hypotheses are converted to a dynamic Bayesian network and value of information analysis is employed to isolate assumptions implicit in the evaluation of paths in, or conclusions of, particular hypotheses. As evidence in the form of observations of states or assumptions is observed, they can become the subject of separate validation. Should an assumption or necessary state be negated, hypotheses depending on it are rejected. This is a form of root cause analysis.\n\nEvidence also presents a problem if it is unreliable. The evidence used in the matrix is static and therefore it can be a snapshot in time.\n\nAccording to social constructivist critics, ACH also fails to stress sufficiently (or to address as a method) the problematic nature of the initial formation of the hypotheses used to create its grid. There is considerable evidence, for example, that in addition to any bureaucratic, psychological, or political biases that may affect hypothesis generation, there are also factors of culture and identity at work. These socially constructed factors may restrict or pre-screen which hypotheses end up being considered, and then reinforce confirmation bias in those selected.\n\nPhilosopher and argumentation theorist Tim van Gelder has made the following criticisms:\n\nVan Gelder proposed \"hypothesis mapping\" as an alternative to ACH.\n\nThe structured analysis of competing hypotheses offers analysts an improvement over the limitations of the original ACH. The SACH maximizes the possible hypotheses by allowing the analyst to split one hypothesis into two complex ones.\n\nFor example, two tested hypotheses could be that Iraq has WMD or Iraq does not have WMD. If the evidence showed that it is more likely there are WMDs in Iraq then two new hypotheses could be formulated: WMD are in Baghdad or WMD are in Mosul. Or perhaps, the analyst may need to know what type of WMD Iraq has; the new hypotheses could be that Iraq has biological WMD, Iraq has chemical WMD and Iraq has nuclear WMD. By giving the ACH structure, the analyst is able to give a nuanced estimate.\n\nOne method, by Valtorta and colleagues uses probabilistic methods, adds Bayesian analysis to ACH. A generalization of this concept to a distributed community of analysts lead to the development of CACHE (the Collaborative ACH Environment), which introduced the concept of a Bayes (or Bayesian) community. The work by Akram and Wang applies paradigms from graph theory.\n\nOther work focuses less on probabilistic methods and more on cognitive and visualization extensions to ACH, as discussed by Madsen and Hicks. DECIDE, discussed under automation is visualization-oriented.\n\nWork by Pope and Jøsang uses subjective logic, a formal mathematical methodology that explicitly deals with uncertainty. This methodology forms the basis of the Sheba technology that is used in Veriluma's intelligence assessment software.\n\nA few online and downloadable tools help automate the ACH process. These programs leave a visual trail of evidence and allow the analyst to weigh evidence.\n\nPARC ACH 2.0 was developed by Palo Alto Research Center (PARC) in collaboration with Richards J. Heuer, Jr. It is a standard ACH program that allows analysts to enter evidence and rate its credibility and relevance. Another useful program is the Decision Command software created by Dr. Willard Zangwill.\n\nSSS Research, Inc. is an analytic research firm that created DECIDE. DECIDE not only allows analysts to manipulate ACH, but it provides multiple visualization products.\n\nThere is at least one open source ACH implementation.\n\n"}
{"id": "47556770", "url": "https://en.wikipedia.org/wiki?curid=47556770", "title": "Andrzej Niemojewski", "text": "Andrzej Niemojewski\n\nAndrzej Niemojewski (24 January 1864 – 3 November 1921) was a Polish social and political activist, poet, rationalist and writer of the Young Poland period.\n\nNiemojewski was best known as a proponent of the Christ myth theory. He was the author of \"Gott Jesus im Lichte fremder und eigener Forschungen samt Darstellung der evangelischen Astralstoffe, Astralszenen und Astralsysteme\" (1910). The book was translated in 1996 as \"God Jesus: The Sun, Moon and Stars as Background to the Gospel Stories\" (1910).\n\nAlbert Schweitzer added two new chapters in the 1913 second edition of his \"Quest of the Historical Jesus\". (\"Geschichte der Leben-Jesu-Forschung\", 2. Auflage, 1913) which features \"God Jesus\" in one chapter.\n\n\nArthur Drews featured \"God Jesus\" in his 1926 work, \"The Denial of the Historicity of Jesus in Past and Present\" ;A. Niemojewski's main book appeared in 1910. It also shows a divine Jesus preceding any rumours about a human one. Contradictions in the Gospel tales prove that they are impossibly about one and the same person -- especially the sayings aren't from a single person. The sayings are co-opted from Jewish common sources and stuffed into the mouth of the alleged master Jesus.\n\nThe most important part of the Gospels, besides the logia, are miracles. One can't neglect them as the liberal Jesus proponents do, without destroying Christian scripture. Miracles are proofs for the divinity of Jesus. Falsely so-called proofs about the human Jesus, like Flavius Josephus, are easily dismissed as chatty hearsays or interpolations.\n\nNiemojewski continues the line of Volney and Dupuis, by looking for parallels in astral mythology. The Tanakh and Talmud are already full of astral mythological images, like the 12 Jewish tribes and sons of Jacob. Even more, of course, the New Testament. This is seen strongest in the Apocalypse of John. The twin myth, applied to Jesus and John, is especially interesting for Niemojewski. The constellation of Gemini plays a central role for Niemojewski. The astral mythological elements are strongest in the Gospels of Matthew and Luke; Mark contains them only quite marginally.\n\nIn the end, Niemojewski's system was too confusing to get much consideration, when compared to simpler astral mythical interpretations.\n\nIn the Appendix of \"The Witnesses to the Historicity of Jesus\", Drews writes ;\nIf we substitute for the “crucified” Orion of the twenty-second psalm the two other important celestial crosses the vernal cross with the Earn (Lamb) and the autumnal cross with the Cup (skull) below it, the Virgin, Berenice's Hair (\"megaddela\"=Mary Magdalene), etc.—we have all the astral elements of what Niemojewski calls the “astral \"via dolorosa\"” (p. 413). May we suppose, in fine, that Orion itself plays the part of the crucified Saviour? In that case the (weeping) women at the cross are represented by the Pleiades (the “rainsisters”), one of which bears the name of Maja (Maria). The Pleiades also are hair-dressers (\"megaddela\"), as they are represented in medieval manuscripts on the basis of an old tradition, and they culminate when Berenice's Hair rises above the eastern horizon. Electra is supposed to be the centre of the Pleiades. She is the mother of Jasios (Jesus), and is represented as a mourner with a cloth over her head, just in the same way as the Christian Mary. But as Jasios was also regarded, according to another genealogy, as the son of Maja, the mourning Pleiad may also stand for her. As is known, the mother of Jesus also is a dove (\"peleids\", Pleiad) in the early Christian conception. \n\nAccording to Niemojewski, the cup (\"gulguleth\"= skull) represents the heavenly Golgotha. But we may refer it to the skull of the Bull and the head of Medusa, and regard “the place of skulls” as the region of the heavens where Orion is found. On this supposition the two evil-doers are recognised in the Twins, which we have already ascertained to be the astral criminals. Castor is regarded as evil on account of his relation to winter, and Pollux good on account of his relation to summer. Niemojewski sees the two evil-doers in the Dogs (Sirius and Procyon). The difference is not great, as the Dogs culminate at the same time as the Twins, and may therefore be substituted for them. \n\nHere we have firm ground on which to establish the originally astral and mythical character of the remainder of the story of Jesus, and we seem to have a very strong proof that there was a cult of “the crucified” before the time of Jesus, and that the nucleus of the figure of Jesus is in reality purely astral. \n\nAll the oriental religions, including Judaism, are essentially astral religions. We have previously (p. 223) shown that \"Revelation\" is a Jewish-Gnostic work, the Jesus of which is more primitive than the Jesus of the gospels. But \"Revelation\" is entirely and certainly of an astral character. It is a further proof that Christianity is no exception to the rule.\n\n\n\n"}
{"id": "567131", "url": "https://en.wikipedia.org/wiki?curid=567131", "title": "Biodiversity hotspot", "text": "Biodiversity hotspot\n\nA biodiversity hotspot is a biogeographic region with significant levels of biodiversity that is threatened with destruction. For example forests are considered as biodiversity hotspots.The Status is designated by Conservation International. \n\nNorman Myers wrote about the concept in two articles in “The Environmentalist” (1988), & 1990 revised after thorough analysis by Myers and others “Hotspots: Earth’s Biologically Richest and Most Endangered Terrestrial Ecoregions” and a paper published in the journal \"Nature\".\n\nTo qualify as a biodiversity hotspot on Myers 2000 edition of the hotspot-map, a region must meet two strict criteria: it must contain at least 0.5% or 1,500 species of vascular plants as endemics, and it has to have lost at least 70% of its primary vegetation. Around the world, 34 areas qualify under this definition. These sites support nearly 60% of the world's plant, bird, mammal, reptile, and amphibian species, with a very high share of those species as endemics.\n\nOnly a small percentage of the total land area within biodiversity hotspots is now protected. Several international organizations are working in many ways to conserve biodiversity hotspots.\n\nBy the influence of that the central government of india arrived a new authority named CAMPA(compensatorry afforestation fund management and planning authority) to control the destruction of forests and biological spots in india\n\nNorth and Central America\n\nThe Caribbean\n\nSouth America \n\nEurope\n\nAfrica\n\nCentral Asia\n\nSouth Asia\n\nSouth East Asia and Asia-Pacific\n\nEast Asia\n\nWest Asia\n\nCritiques of \"Hotspots\"\n\nThe high profile of the biodiversity hotspots approach has resulted in some criticism. Papers such as Kareiva & Marvier (2003) have argued that the biodiversity hotspots:\n\nA recent series of papers has pointed out that biodiversity hotspots (and many other priority region sets) do not address the concept of cost. The purpose of biodiversity hotspots is not simply to identify regions that are of high biodiversity value, but to prioritize conservation spending. The regions identified include some in the developed world (e.g. the California Floristic Province), alongside others in the developing world (e.g. Madagascar). The cost of land is likely to vary between these regions by an order of magnitude or more, but the biodiversity hotspot designations do not consider the conservation importance of this difference. However, the available resources for conservation also tend to vary in this way.\n\n\n\n"}
{"id": "19975009", "url": "https://en.wikipedia.org/wiki?curid=19975009", "title": "Black suffrage", "text": "Black suffrage\n\nBlack suffrage is Black people's right to vote. Black suffrage has been at issue in countries established under conditions of white supremacy. It may be limited through official or informal (\"de facto\") discrimination. In many places, black people have obtained suffrage through national independence. It should also be pointed out that \"Black suffrage\" in the United States in the aftermath of the American Civil War explicitly refers to \"Black Male Suffrage\". While women citizens, regardless of race, held rights to vote in some states, at the federal level, the U.S. Constitution was not interpreted to prohibit discrimination against women in voting, regardless of their race, until the passage of the 19th Amendment which was ratified by the United States Congress on August 18 and then certified by law on August 26, 1920.\n\n\n\n\n\n\n\n\n"}
{"id": "58607878", "url": "https://en.wikipedia.org/wiki?curid=58607878", "title": "Carbon Neutrality Coalition", "text": "Carbon Neutrality Coalition\n\nThe Carbon Neutrality Coalition (CNC) is a group of countries, cities and organisations who have committed to take concrete and ambitious action to achieve the aims of the Paris Agreement.. \n\nThe CNC was founded in 2017 by 16 countries and 32 cities. In December New Zealand Climate Change Minister James Shaw said:\n\nIn September 2018 the Coalition held its first meeting at the UN General Assembly and 4 new countries joined: the UK; Canada; Denmark and Spain.\n\nThe coalition aims to achieve benefits in 3 key areas: \n\nCoalition members agree to\n"}
{"id": "39547763", "url": "https://en.wikipedia.org/wiki?curid=39547763", "title": "Charlie Brandt", "text": "Charlie Brandt\n\nCarl \"Charlie\" Brandt (February 23, 1957 – September 13, 2004) was an American serial killer. A former resident of Fort Wayne, Indiana and longtime resident of the Florida Keys, Brandt committed suicide in September 2004 after he stabbed his wife, Teresa \"Teri\" Brandt, and decapitated and severely mutilated his niece, Michelle Jones, by removing her heart. An investigation by police concluded that Brandt hanged himself in Jones' garage after committing the murders.\n\nIt later came to light that Brandt had shot his parents – his pregnant mother fatally – in 1971, when he was 13; he spent one year at a psychiatric hospital before being released, and was never criminally charged. Because of this incident, and because of Brandt's efficiency in killing his wife and niece and his hidden obsession with human anatomy, investigators looked into the prospect that he had committed other murders since moving to Florida in 1973. Police have positively ascribed up to six homicides to Brandt.\n\nBrandt was the second child and only son of Herbert and Ilse Brandt, two German immigrants who originally lived in Connecticut. His father worked as a laborer for a division of International Harvester, eventually working his way up to draftsman and project engineer. However, the family frequently moved, and as a result, Brandt and his older sister Angela attended several different schools. He was regarded as a good student, but was shy and had difficulty adjusting to new surroundings. In September 1968, Herbert was transferred to International Harvester's plant in Fort Wayne, Indiana. Every summer and Christmas, the family would vacation in Florida, where Brandt went on hunting trips with his father.\n\nOn the night of January 3, 1971, Brandt, then 13, walked into his parents' bathroom while Herbert was shaving and Ilse, who was eight months pregnant, was taking a bath. Brandt shot both parents at point blank range; his father survived, but his mother died at the scene. Brandt then confronted his sister Angela, but his gun wouldn't fire. After a physical struggle, Angela managed to calm her brother down before she fled the house and sought help from neighbors. Brandt also left the house and knocked on the door of a girl next door named Sandi Radcliffe, telling her, \"Sandi, I just shot my mom and dad.\" Herbert Brandt later identified his son as his attacker.\n\nBecause he was too young to be charged with murder under Indiana law, Brandt appeared before a grand jury and was ordered to undergo three separate psychiatric evaluations, all of which couldn't determine what motivated him to shoot his parents. One of Brandt's psychiatrists, Ronald Pancner, later recounted, \"Basically, I was looking for mental illness. And he wasn't showing the signs and symptoms of serious mental illness, which I thought was what the court wanted to know.\" Interviews with Brandt's family and friends showed that he had no conflicts at home or at school, and had previously shown nothing but devotion to his mother. Brandt spent one year at a psychiatric hospital before being released back into the custody of his family. They never spoke of the incident again, and Brandt's younger sisters were never told about the shooting.\n\nShortly after Brandt's release, his family relocated to Florida. One year later, Brandt's father and sisters moved away after he remarried, while Brandt himself remained in Florida under the care of his grandparents. In 1984, Brandt got a degree on electronics and became a radar specialist. In 1986, he married his girlfriend Teri. No relatives were invited to their wedding. The couple settled in a beach house of Big Pine Key, the southernmost portion of the Florida Keys, in 1989. \n\nOn September 2, 2004, Brandt and Teri evacuated from their home ahead of Hurricane Ivan. Their niece, Michelle Lynn Jones, invited them to stay at her residence in near Orlando. Throughout the visit, Jones kept in regular contact with her mother, Mary Lou, as well as several friends. On the evening of September 13, one of Jones' friends, Lisa Emmons, was scheduled to visit her house. However, Jones discouraged her from coming after reporting that the Brandts had an argument after drinking. After that night, Jones stopped answering telephone calls, which alarmed her acquaintances.\n\nOn September 15, another one of Jones' friends, Debbie Knight, came to her house to check on her while on the phone with Jones' mother. After finding the front door locked, Knight tried to enter the house through the garage, where she found Brandt's decomposing body hanging from the rafters. Knight contacted the police, who entered the house and found the bodies of Brandt's wife and niece. Teri had been stabbed seven times in the chest while reclining on a couch. Jones had been decapitated and disemboweled, with her heart and organs removed. Jones' head was also placed next to her own body. The weapons used in the crimes had been knives from Jones' kitchen.\n\nA search of Brandt's residence on Big Pine Key revealed that he was a monthly subscriber to Victoria's Secret catalogs; had an extensive collection of surgery-themed books, posters, and clippings; and regularly searched online for autopsy photos and snuff film websites depicting violence against women. Because Brandt's murder of Jones indicated methodology and past experience, and because he traveled often due to his job, police checked cold cases in Florida that matched his apparent \"modus operandi\", and launched requests for similar inquiries in the United States and abroad. Ultimately, the search linked Brandt to twenty-six unsolved murders in Florida dating back to 1973.\n\n\n"}
{"id": "43123871", "url": "https://en.wikipedia.org/wiki?curid=43123871", "title": "Chord diagram", "text": "Chord diagram\n\nA chord diagram is a graphical method of displaying the inter-relationships between data in a matrix. The data are arranged radially around a circle with the relationships between the data points typically drawn as arcs connecting the data.\n\nThe format can be aesthetically pleasing, making it a popular choice in the world of data visualization.\n\nChord diagrams get their name from terminology used in geometry. A chord of a circle is a geometric line segment whose endpoints both lie on the circle. Chord diagrams are also known as radial network diagrams and may sometimes be referred to as a type of circular layout.\n\nWhile a small amount of data could be represented in a circular diagram using straight lines to show the interconnections, a diagram featuring numerous lines would quickly become illegible. \nTo reduce the visual complexity, chord diagrams employ a technique called hierarchical edge bundling.\n\nThis type of diagram was used in 2007 by the New York Times infographic Close-Ups of the Genome\n\n"}
{"id": "1518031", "url": "https://en.wikipedia.org/wiki?curid=1518031", "title": "Circular distribution", "text": "Circular distribution\n\nIn probability and statistics, a circular distribution or polar distribution is a probability distribution of a random variable whose values are angles, usually taken to be in the range A circular distribution is often a continuous probability distribution, and hence has a probability density, but such distributions can also be discrete, in which case they are called circular lattice distributions. Circular distributions can be used even when the variables concerned are not explicitly angles: the main consideration is that there is not usually any real distinction between events occurring at the lower or upper end of the range, and the division of the range could notionally be made at any point.\n\nIf a circular distribution has a density\n\nit can be graphically represented as a closed curve\n\nwhere the radius formula_3 is set equal to\n\nand where \"a\" and \"b\" are chosen on the basis of appearance.\n\nBy computing the probability distribution of angles along a handwritten ink trace,\na lobe-shaped polar distribution emerges. The main direction of the lobe in the\nfirst quadrant corresponds to the slant of handwriting (see: graphonomics).\n\nAn example of a circular lattice distribution would be the probability of being born in a given month of the year, with each calendar month being thought of as arranged round a circle, so that \"January\" is next to \"December\".\n\n\n"}
{"id": "2037473", "url": "https://en.wikipedia.org/wiki?curid=2037473", "title": "Clerical script", "text": "Clerical script\n\nThe clerical script (; Japanese: 隷書体, \"reishotai\"; Vietnamese: lệ thư), also formerly chancery script, is an archaic style of Chinese calligraphy which evolved from the Warring States period to the Qin dynasty, was dominant in the Han dynasty, and remained in use through the Wei-Jin periods. Due to its high legibility to modern readers, it is still used for artistic flavor in a variety of functional applications such as headlines, signboards, and advertisements. This legibility stems from the highly rectilinear structure, a feature shared with modern regular script (kaishu). In structure and rectilinearity, it is generally similar to the modern script; however, in contrast with the tall to square modern script, it tends to be square to wide, and often has a pronounced, wavelike flaring of isolated major strokes, especially a dominant rightward or downward diagonal stroke. Some structures are also archaic.\n\nClerical script is popularly but mistakenly thought to have developed or been invented in the early Han dynasty from the small seal script. The process of change between small seal script and clerical script is referred to as the \"Libian\" (lit: Clerical Evolution) (隸變). There are also historical traditions dating to the Hàn dynasty which mistakenly attributed the creation of clerical script to the Qín dynasty and in particular to Chéng Miǎo, who was said to have invented it at the behest of Qin Shi Huang. Another traditional account is that it was invented by government scribes, in particular those involved in the justice and penal systems. However, from written materials unearthed by archaeologists, it is now known that \"all\" stages of Chinese writing underwent periods of natural evolution, and none of them were inventions by one person; this is true of clerical script as well. Furthermore, rather than being established by government scribes, it has been argued that clerical script was already in popular use, and the Qín dynasty use by scribes merely reflects this trend. Archaeological discoveries now clearly show that an immature form of clerical script (\"proto-clerical\") was already developing in the state of Qín during the Warring States period, and into the early Western Hàn; this can be seen on a number of bamboo books unearthed recently. Furthermore, the writing immediately preceding clerical script was not merely seal script alone; rather, there was a coexistence of seal script (the at-first dominant and formal style) alongside an increasingly popular but secondary form of \"vulgar\", \"popular\", or \"common\" writing, which was very roughly executed and which was generally rectilinear. The popularity of this vulgar writing grew as the use of writing itself became more widespread. The structures and style of many of the characters executed in this vulgar writing were similar or even identical to their later clerical script counterparts, leading some to conclude that proto-clerical (and therefore clerical) script evolved not from seal script but from the vulgar writing of Qín, which coexisted with seal script in Warring States to Qín dynasty. The Qín bamboo script is a good example of this transition, having evolved from vulgar Qín writing and considered by some to constitute Qín clerical script.\n\nThe etymology of the Chinese name for the script lìshū () is uncertain. \"Lì\" meant a slave or prisoner serving the state, and thus, some infer that the script was used in recording the affairs related to such slaves, while others infer that it was used by prisoners conscripted as scribes.\n\nDuring Warring States, proto-clerical script emerged in casual, informal usage. During the Qin dynasty it appears to have also been used in some scribal capacity, but never in formal usage. Maturing into clerical script in the early Han, it soon became the dominant script for general purposes, while seal script remained in use for the most formal purposes such as some stelae, signet seals (name chops), and especially the titles of written works and stelae; some cursive was also in use at the time. At roughly the same time, the clerical script was used and inscribed onto many stelae which later influenced subsequent development of Chinese calligraphic styles. Out of clerical script, a new form then emerged in the middle of the Eastern Han dynasty, which Qiu (2000, p. 113) terms \"neo-clerical\" script; it was from this neo-clerical and from cursive that by late in the Eastern Han semi-cursive would then evolve, out of which then emerged the modern standard script. Thus, according to Qiu, the evolution from clerical script to standard script was not a direct step as commonly supposed.\n\n"}
{"id": "48797241", "url": "https://en.wikipedia.org/wiki?curid=48797241", "title": "Codex Totomixtlahuaca", "text": "Codex Totomixtlahuaca\n\nThe Codex Totomixtlahuaca or Codex Condumex is a colonial-era map produced on a large piece of cotton. The map represents Totomixlahuaca, Mexico, and includes a creation date of 1564. It documents a meeting over a land conflict in communities of the current Mexican state of Guerrero.\n\nThe \"Codex Totomixtlahuaca\" was found in the community of Totomixtlahuaca, located in the southeast of Guerrero, in Mexico. Using Aztec pictographs, the map represents the distribution of lands in 1564.\n\nThe map was drawn on a large piece of cotton, roughly 217 cm by 185 cm. It was painted with coal.\n\n\n"}
{"id": "12593785", "url": "https://en.wikipedia.org/wiki?curid=12593785", "title": "Culture change", "text": "Culture change\n\nCulture change is a term used in public policy making that emphasizes the influence of cultural capital on individual and community behavior. It has been sometimes called repositioning of culture, which means the reconstruction of the cultural concept of a society. It places stress on the social and cultural capital determinants of decision making and the manner in which these interact with other factors like the availability of information or the financial incentives facing individuals to drive behavior.\n\nThese cultural capital influences include the role of parenting, families and close associates; organizations such as schools and workplaces; communities and neighborhoods; and wider social influences such as the media. It is argued that this cultural capital manifests into specific values, attitudes or social norms which in turn guide the behavioral \"intentions\" that individuals adopt in regard to particular decisions or courses of action. These behavioral intentions interact with other factors driving behavior such as financial incentives, regulation and legislation, or levels of information, to drive actual behavior and ultimately feed back into underlying cultural capital.\n\nIn general, cultural stereotypes present great resistance to change and to their own redefinition. Culture, often appears fixed to the observer at any one point in time because cultural mutations occur incrementally. Cultural change is a long-term process. Policymakers need to make a great effort to improve some basics aspects of a society’s cultural traits.\n\nThe term is used by Knott et al. of the Prime Minister's Strategy Unit in the publication: \"Achieving Culture Change: A Policy Framework\" (Knott et al., 2008). The paper sets out how public policy can achieve social and cultural change through 'downstream' interventions including fiscal incentives, legislation, regulation and information provision and also 'upstream' interventions such as parenting, peer and mentoring programs, or development of social and community networks.\n\nThe key concepts the paper is based on include:\n\n\nKnott et al. use examples from a range of policy areas to demonstrate how the culture change framework can be applied to policymaking. For example:\n\n\n\n\n"}
{"id": "31265421", "url": "https://en.wikipedia.org/wiki?curid=31265421", "title": "Digital privacy", "text": "Digital privacy\n\nDigital Privacy is a collective definition that encompasses three sub-related categories; information privacy, communication privacy, and individual privacy. The term is often used in contexts that promote advocacy on behalf of individual and consumer privacy rights in digital spheres, and is typically used in opposition to the business practices of many e-marketers/businesses/companies to collect and use such information and data. \n\n\"Main article: Information Privacy\"\n\nIn the context of digital privacy, information privacy is the notion that individuals should have the freedom, or right, to determine how their digital information, mainly that pertaining to personally identifiable information, is collected and used. The EU has various laws that dictate how information may be collected and used by companies. Some of those laws are written to give agency to the preferences of individuals/consumers in how their data is used. In other places, like in the United States, privacy law is argued by some to be less developed in this regard. By example, some legislation, or lack of, allows companies to self-regulate their collection and dissemination practices of consumer information. \n\nIn the context of digital privacy, communication privacy is the notion that individuals should have the freedom, or right, to communicate information digitally with the expectation that their communications are secure; meaning that messages and communications will only be accessible to the sender's original intended recipient. However, communications can be intercepted or delivered to other recipients without the sender's knowledge, in a multitude of ways. Communications can be intercepted directly through various hacking methods. Communications can also be delivered to recipients unbeknownst to the sender because of false assumptions made regarding the platform or medium which was used to send information. An example of this is failure to read a company's privacy policy regarding communications on their platform could lead one to assume their communication is protected when it is in fact not. Additionally, companies frequently have been known to lack transparency in how they use information, this can be both intentional and unintentional. Discussion of communication privacy necessarily requires consideration of technological methods of protecting information/communication in digital mediums, the effectiveness and ineffectiveness of such methods/systems, and the development/advancement of new and current technologies.\n\nIn the context of digital privacy, individual privacy is the notion that individuals have a right to exist freely on the internet, in that they can choose what types of information they are exposed to, and more importantly that unwanted information should not interrupt them. An example of a digital breach of individual privacy would be an internet user receiving unwanted ads and emails/spam, or a computer virus that forces the user to take actions they otherwise wouldn't. In such cases the individual, during that moment, doesn't exist digitally without interruption from unwanted information; thus their individual privacy has been infringed upon. \n\nThe following examples are systems that allow a user to remain anonymous when accessing the web, and by extension the use of which better ensures the protection of their personally identifiable information. \n\nOnion Routing was originally developed by the U.S. Naval Research Lab and was intended to anonymize web traffic. The system created a path to any TCP/IP server by creating a pathway of onion routers. Once a pathway has been established, all information that is sent through it is anonymously delivered. When the user has finished utilizing the pathway it was essentially deleted which freed the resources to be used for a new pathway within Onion Routing. The Onion Routing project developed into what is today known as TOR, a completely open-sourced and free software. Unlike its predecessor, Onion Routing, Tor is able to protect both the anonymity of individuals as well as web providers.This allows people to set up anonymous web servers which in effect provides a censorship-resistant publishing service. \n\nThe previously mentioned information anonymity systems can also potentially protect the contents of communications between two people, but there are other systems that directly function to guarantee a communication remains between only two people; they function to accomplish that only the intended recipient of a communication will receive it.\n\nOne of these systems, PGP (which is an acronym for \"Pretty Good Privacy\"), has existed in various forms for many years. It functions to protect email messages by encrypting and decrypting them. It originally existed as a command-line-only program, but in recent years it has evolved to have its own full interface and a multitude of email providers offer built-in PGP support. Users can also install PGP-compatible software and manually configure it to encrypt emails on nearly any platform. \n\nSSL (acronym for Secure Sockets Layer) and TLS (acronym for Transport Layer Security) are measures to secure payments online. While these systems are not immune from breaches or failure, many users benefit greatly from their use as every major browser program has support for it built in. \n\n\"Main article: Phishing\"\n\nPhishing is a common method of obtaining someone's private information. This generally consists of an individual (often referred in this context as a hacker), developing a website that looks similar to other major websites that a target person commonly uses. The target person can be directed to the site through a link in a 'fake' email that is designed to look like it came from the website he/she commonly uses. The user then clicks on the URL, proceeds to sign in, or provide other personal information, and as opposed to the information being submitted to the website that the user thought they were on, it is actually sent directly to the hacker. \n\nDigital privacy is a trending social concern. For example, the TED talk by Eric Berlow and Sean Gourley subsequent to the 2013 mass surveillance disclosures cast a shadow over the privacy of cloud storage and social media. While digital privacy is concerned with the privacy of digital information in general, in many contexts it specifically refers to information concerning personal identity shared over public networks. \n\nBefore the Edward Snowden disclosures concerning the extent of the NSA PRISM program were revealed in 2013, the public debate on digital privacy mainly centered on privacy concerns with social networking services, as viewed from within these services.\n\nAs the secrecy of the American Foreign Intelligence Surveillance Act becomes widely disclosed, digital privacy is increasingly recognized as an issue in the context of mass surveillance.\n\nThe use of cryptographic software to evade prosecution and harassment while sending and receiving information over computer networks is associated with crypto-anarchism, a movement intending to protect individuals from mass surveillance by the government.\n\n"}
{"id": "28819023", "url": "https://en.wikipedia.org/wiki?curid=28819023", "title": "Disability in the media", "text": "Disability in the media\n\nThe depiction of disability in the media plays a major role in molding the public perception of disability. Perceptions portrayed in the media directly influence the way people with disabilities are treated in current society. \"[Media platforms] have been cited as a key site for the reinforcement of negative images and ideas in regard to people with disabilities.\"\n\nAs a direct response, there have been increasing examples worldwide of people with disabilities pursuing their own media projects, such as creating film series centered on disability issues, radio programs and podcasts designed around and marketed towards those with disabilities, and so on.\n\nThe media generally depicts people with disabilities according to common stereotypes such as pity and heroism.\nDisability advocates often call this type of societal situation the \"pity/heroism trap\" or \"pity/heroism dichotomy\" and call instead for its supporters to \"Piss On Pity\" and push forward with inclusion instead.\n\nWhen reports are about the \"plight of the disabled\" they rely on the pity or medical model of disability. Telethons are an example of this, such as the Jerry Lewis MDA Telethon which has been heavily criticised and sometimes even physically protested by disability rights advocates.\n\nNegative day-to-day reporting may occur chiefly by depicting a given person or people with a disability as a burden or drain on society.\n\nThe \"super-crip\" model, in which subjects are portrayed as heroically overcoming their afflictions, is also often used when reporting on people with disabilities.\n\nThe social model tends to be used for reporting on the activities of disability rights activists if the report is positive.\n\nThe term \"inspiration porn\" was coined in 2012 by disability rights activist Stella Young in an editorial in Australian Broadcasting Corporation's webzine \"Ramp Up\". The term describes when people with disabilities are called \"inspirational\" solely or in part on the basis of their disability.\n\nStereotypical depictions of disability that originate in the arts, film, literature, television, and other mass media fiction works, are frequently normalized through repetition to the general audience. Once such a stereotype is absorbed and accepted by the mainstream public, it continues to be repeated in the media, in many slightly varied forms, but staying close to the stereotype. Many media stereotypes about disability have been identified. They are sometimes referred to as \"tropes\", meaning a recurring image or representation in the mainstream culture that is widely recognizable. Tropes repeated in works of fiction have an influence on how society at large perceives people with disabilities. Other forms of media, in turn, then portray people with disabilities in ways that conform with tropes and repeat them. \n\nSome of these disability tropes (which may or may not be harmful) that have been identified in popular culture include:\n\nOther disability stereotypes that have been identified in popular culture include:\n\nThe existence of disability tropes in mass media is related to other stereotypes, or tropes, that have developed when other marginalized groups in society are depicted, such as the Magical Negro trope identified, and criticized, by film director Spike Lee. The mocking names often given to these tropes when they are identified indicates a rejection of the harmful stereotypes that they propagate.\n\nStereotypes may endure in a culture for several reasons: they are constantly reinforced in the culture, which mass media does easily and effectively; they reflect a common human need to organize people and categorize them; they reinforce discrimination that allows one group of society to exploit and marginalize another group. Several studies of mass media in Britain and the United States have identified common stereotypes, such as \"noble warrior\", \"charity cripple\", \"curio\", \"freak\", and \"Pollyanna\", where the researchers identified a position of \"disapproval\", on the part of the media, of some aspect of the disability. It has been shown that media portrayals of disability became more normalizing and accepting in the years immediately after World War II, when returning veterans with war-related disabilities were being reintegrated into society. A backlash of intolerance towards disability followed during the mid-20th century, with some researchers speculating that this may have been related to society's reaction against any identifiable \"difference\" as a result of Cold War tensions. Depictions of disability in media soon reverted to emphasizing the \"freakish\" nature of disability.\n\nBroadcast media has in recent years begun to recognize the large audience of people with disabilities that it reaches. Programming dedicated to disability issues is increasing.\n\nIn 1990, the signing of the Americans with Disabilities Act (ADA) became the first news story on disability issues to become a lead story on cable news broadcaster CNN. News Director Ed Turner contacted the Washington bureau of CNN to have the signing of the ADA by President Bush broadcast live. The next day, the signing of the ADA was covered as the top headline in the \"New York Times\", the \"Washington Post\", and every other major U.S. newspaper. Disability rights activist Lex Frieden has stated, \"That was the first time that millions of people were exposed to disability rights as the number one story\". These milestones were a major change in reducing exclusion and invisibility for people with disabilities.\n\n\"Ouch!\" by the British Broadcasting Corporation, \"The Largest Minority\" broadcast in New York City, and \"Dtv\" presented in sign language on SABC television in South Africa, are examples of programming produced for, and usually also by, people with disabilities.\n\nRadio reading services are radio stations that broadcast readings from newspapers, magazines and books to mainly blind or partially sighted audiences.\n\nIn recent years, some mainstream publications and broadcasters have added writing and programming about disability-related topics. The Creative Diversity Network in the United Kingdom is an organization that advocates increased cultural and disability-related programming. Clare Morrow, the organization's Network Manager, states that \"Disability is now at the heart of the diversity agenda for all of the UK's main television companies, thanks to their collective work\". The BBC Website includes \"Ouch!\", a disability news and discussion blog and internet talk show program.\n\nMany activist and charitable organisations have websites and publish their own magazines or newsletters.\n\nDisability has been shown to audiences since the early days of documentary film. Educational silent films showing hospital patients with various disabling conditions were shown to medical and nursing students. Films of schizophrenia patients with symptoms of catatonia, World War I veterans with extreme Post-Traumatic Stress Disorder (PTSD) (shell-shock) symptoms, and many other such films survive today. Michael J. Dowling (1866-1921) was a prominent Minnesota politician and newspaper publisher, who was also a quadruple amputee. World War I inspired him to further the cause of veterans with disabilities. Dowling had himself filmed performing routine tasks on his own, and had the films screened for groups such as the American Medical Association in 1918. His efforts promoted the rehabilitation of the physically disabled.\n\nDocumentary films sometimes had a tone that was a reflection of the public's morbid curiosity about visible disabilities that were considered shameful and ordinarily were hidden from public view. Nazi propagandists exploited this fear and prejudice to push the public to accept their euthanasia policies, including forcible sterilization, by screening films showing people with mental retardation and physical disabilities living in squalid conditions. At the same time, American President Franklin D. Roosevelt and his White House staff made a great effort to disguise his handicap (Roosevelt became paraplegic after contracting polio as an adult). Roosevelt was photographed and filmed only from positions that would hide his disability from the public, for fear that he would be perceived as weak.\n\nMore recently, documentary films about disability have been widely viewed on both public and cable television programming. The Channel 5 (UK) program \"Extraordinary Lives\", and Channel 4 program \"Body Shock\" in the United Kingdom, broadcast much documentary material about disability. Titles of some documentary programming includes: \"\"The Boy Who Sees Without Eyes\" — the 14-year-old American boy who navigates by sound; \"The Twin Within the Twin\" — the 34-year-old Bengali who carries his foetal twin within his abdomen; and \"The Twins Who Share a Body\" — Abby and Brittany Hensel, the world's only known dicephalus twins, ie. two heads with one body\". Some of the documentaries, perceived to be in the \"shock doc\" (shock documentary) genre, have been denounced by critics with disabilities. Although the documentary programming contains educational and scientific information, the sensationalized, overt emotional appeal of the \"tabloid tone\" of the programming has raised objections. Laurence Clark has written in the BBC Website's disability blog \"Ouch!\": \n\nThe first photographer to become widely known for depicting the visibly disabled was Diane Arbus, active in the 1950s and 1960s. Her photographs, which are in fact art photographs, have been, and remain, highly controversial.\n\nAmerican documentary photographers Tom Olin and Harvey Finkle, known for documenting the disability rights movement since the 1980s, have exhibited at many venues including the National Constitution Center Museum.\n\nMembers of the public with disabilities have criticized media depictions of disability on the grounds that stereotypes are commonly repeated. Media coverage that is \"negative\", \"unrealistic\", or displays a preference for the \"pitiful\" and \"sensationalistic\" over the \"everyday and human side of disability\" are identified at the root of the dissatisfaction. Journalist Leye Jeannette Chrzanowski, who uses a wheelchair, has written:\n\nVarious organisations and programmes have been established to try to positively influence the frequency and quality of reporting on disability issues. By 2000, it was estimated that in the United States, there were between 3000 and 3500 newsletters, 200 magazines, and 50 to 60 newspapers regularly published that focussed on disability issues.\n\nDisability lobbyists for the American Disability Association (ADA), are unable to rely on the media as a source to support and spread their ideas on disability because they believe that the media will continue to misrepresent the disabled community since they are more interested in the stereotypes of disabled people than in the facts.\n\n"}
{"id": "859292", "url": "https://en.wikipedia.org/wiki?curid=859292", "title": "Displacement (psychology)", "text": "Displacement (psychology)\n\nIn Freudian psychology, displacement (, \"shift, move\") is an unconscious defence mechanism whereby the mind substitutes either a new aim or a new object for goals felt in their original form to be dangerous or unacceptable.\n\nA term originating with Sigmund Freud, displacement operates in the mind unconsciously, its transference of emotions, ideas, or wishes being most often used to allay anxiety in the face of aggressive or sexual impulses.\n\nFreud initially saw displacement as a means of dream-distortion, involving a shift of emphasis from important to unimportant elements, or the replacement of something by a mere illusion. Freud called this “displacement of accent.” \n\nDisplacement of object: Feelings that are connected with one person are displaced onto another person. A man who has had a bad day at the office, comes home and yells at his wife and children, is displacing his anger from workplace onto his family. Freud thought that when children have animal phobias, they may be displacing fears of their parents onto an animal. \n\nDisplacement of attribution: A characteristic that one perceives in oneself but seems unacceptable is instead attributed to another person. This is essentially the mechanism of psychological projection; an aspect of the self is projected (displaced) onto someone else. Freud wrote that people commonly displace their own desires onto God’s will. \n\nBodily displacements: A genital sensation may be experienced in the mouth (displacement upward) or an oral sensation may be experienced in the genitals (displacement downward). Novelist John Cleland in ‘’Fanny Hill’’ referred to the vagina as “the nethermouth.” Sexual attraction toward a human body can be displaced in sexual fetishism, sometimes onto a particular body part like the foot or at other times onto an inanimate fetish object. \n\nFreud also saw displacement as occurring in jokes, as well as in neuroses – the obsessional neurotic being especially prone to the technique of displacement onto the minute. When two or more displacements occurs towards the same idea, the phenomenon is called condensation (from the German \"Verdichtung\").\n\nAmong Freud's mainstream followers, Otto Fenichel highlighted the displacement of affect, either through postponement or by redirection, or both. More broadly, he considered that \"in part the paths of displacement depend on the nature of the drives that are warded off\".\n\nEric Berne in his first, psychoanalytic work, maintained that \"some of the most interesting and socially useful displacements of libido occur when both the aim and the object are partial substitutions for the biological aim and object...sublimation\".\n\nIn 1957, Jacques Lacan, inspired by an article by linguist Roman Jakobson on metaphor and metonymy, argued that the unconscious has the structure of a language, linking displacement to the poetic function of metonymy, and condensation to that of metaphor.\n\nAs he himself put it, \"in the case of \"Verschiebung\", 'displacement', the German term is closer to the idea of that veering off of signification that we see in metonymy, and which from its first appearance in Freud is represented as the most appropriate means used by the unconscious to foil censorship\".\n\nThe aggressive drive – known as mortido – may be displaced quite as much as the libidinal - the sex drive. Business or athletic competition, or hunting, for instance, offer plentiful opportunities for the expression of displaced mortido.\n\nIn such scapegoating behavior, aggression may be displaced onto people with little or no connection with what is causing anger or frustration. Some people punch cushions when they are angry at friends; a college student may snap at his or her roommate when upset about an exam grade ... etc.\n\nDisplacement can also act in a what looks like a 'chain-reaction', with people unwittingly becoming both victims and perpetrators of displacement. For example, a man is angry with his boss, but he cannot express this properly, so he hits his wife. The wife, in turn, hits one of the children, possibly disguising this as a \"punishment\" (rationalization).\n\nEgo psychology sought to use displacement in child rearing, a dummy being used as a displaced target for toddler sibling rivalry.\n\nThe displacement of feelings and attitudes from past significant others onto the present-day analyst constitutes a central aspect of the transference, particularly in the case of the neurotic.\n\nA subsidiary form of displacement \"within\" the transference occurs when the patient disguises transference references by applying them to an apparent third party or to themself.\n\nLater writers have objected that whereas Freud only described the displacement of sex into culture, for example, the converse – social conflict being displaced into sexuality – is also true.\n\n\n"}
{"id": "27684483", "url": "https://en.wikipedia.org/wiki?curid=27684483", "title": "Eco-investing", "text": "Eco-investing\n\nEco-investing or green investing, is a form of socially responsible investing where investments are made in companies that support or provide environmentally friendly products and practices. These companies encourage (and often profit from) new technologies that support the transition from carbon dependence to more sustainable alternatives.\n\nAs industries' environmental impacts become more apparent, green topics have not only taken center stage in pop culture, but the financial world as well. In the 1990s many investors “began to look for those companies that were better than their competitors in terms of managing their environmental impact.” While some investors still focus their funds to avoid only “the most egregious polluters,” the emphasis for many investors has switched to changing “the way money is used,” and using “it in a positive, transformative way to get us from where we are now ultimately to a truly sustainable society.”\n\nThe Global Climate Prosperity Scoreboard – launched by Ethical Markets Media and The Climate Prosperity Alliance to monitor private investments in green companies – estimated that over $1.248 trillion has been invested in solar, wind, geothermal, ocean/hydro and other green sectors since 2007. This number represents investments from North America, China, India, and Brazil, as well at other developing countries.\n\nWhile many eco-investments may be considered socially responsible investments, and vice versa, the two are not mutually inclusive. Socially responsible investing is the practice of investing only in those companies which satisfy certain moral or ethical criteria. This may include companies with an interest in the environment, but also supports various other social and religious issues.\n\nEco-investing narrows in on the interests of sustainable environmental issues. Specifically, eco-investments focus on companies who work on renewable energy and clean technologies.\n\nThere are several sectors that fall under the eco-investing umbrella. Renewable energy refers to both solar, wind, tidal current,wave and conventional hydro technology. This includes companies that build solar panels or wind turbines, or the raw materials and services that contribute to these technologies It also refers to Energy Storage – companies that develop and use technologies to store large amounts of energy, particularly renewable energies. A good example of this is the fuel cells used in hybrid cars. Also under the renewable energy sector are Biofuels. This group includes companies that use or supply biological resources (like algae, corn or waster wood) to create energy or fuel. Other technologies that are included in the renewable energy group are: Geothermal (companies who use or convert heat to electric energy) and Hydroelectricity (companies who harness water energy to make electricity).\n\nThe Buildings and Efficiency sector refers to companies that manufacture green building materials or energy-efficient services in the world of engineering and architecture. Green building materials include energy-efficient glass, insulation, and lighting among others. Recycling companies and energy conservation companies also fall under this sector.\n\nThe Eco Living sector refers to companies that offer sustainable goods and services for healthy living. This includes organic farming, green pesticides, health care and pharmaceuticals.\n\nGreen investment has significantly grown in the UK and there are now 136 funds listed on the Worldwise Investor fund library under the themes: Agriculture, Carbon, Clean Energy, Forestry, Environmental, Multi-thematic and Water. All of these funds account for around £21.8bn in the UK.\n\n\n"}
{"id": "50460577", "url": "https://en.wikipedia.org/wiki?curid=50460577", "title": "Evolutionary theory of the self", "text": "Evolutionary theory of the self\n\nWhen trying to understand the self in terms of the brain, neuroscientists have found contradictory results and paradoxes. Nevertheless, Gonzalo Munevar has argued that neuroscience in an evolutionary context can give a proper explanation of the self. His Evolutionary Theory of the Self depends on, “the ability by the brain to coordinate new sensory information in light of the organism’s internal states and in the context of its personal history and genetic inheritance.\" His theory is an alternative conception for the explanation of the self, which takes into account the evolutionary biology of the brain.\n\nOrganisms that are highly complex execute the function of telling self from other with the brain and the immune system. To fulfill the function of recognizing self from other, the brain uses past experiences and genetic inheritance (e.g. survival, reproduction). The self is defined by these functions that distinguish an organisms from other organisms, which allow them to act as one whole entity in social and physical environments. Simply put, the theory revolves around the idea that the brain constitutes the self, which represents itself in a variety of internal states.\n\nThe brain/self evolves for action to be able to interact with social and physical environments. It is suggested that the brain performs a complex list of tasks to complete these interactions to distinguish self from other. This defines the brain to be characteristically distributive to complete complex tasks, and thus suggests that the self is also distributive. Therefore, the Evolutionary Theory of the Self detours from the traditional conceptions that include the self being a centralized, unitary mechanism that is both conscious (sense of self) and compiled of a collection of episodic memories. Alternatively, it suggests that the conception of self is mostly unconscious, and the brain evolves from past experiences and genetic inheritance to create the evolutionary self.\n\nIn Munevar’s study, “fMRI Study of Self vs. Others’ Attributions of Traits Consistent with Evolutionary Understanding of the Self,” he aimed to demonstrate the experimental feasibility of this conception of the self as a distributive system, and discovered results complimenting the Evolutionary Theory of the Self while resolving the contradictions and paradoxes of traditional conceptions of self.\n\nOne of the problems is that the results of brain-imaging studies of self-knowledge vary depending on the behavioral tasks chosen. Some studies use positron emission tomography (PET) to do self-attribution of personality traits, while others use functional magnetic resonance imaging (fMRI). Other studies prefer to do tasks involving self-recognition (recognizing a photograph or yourself and others). Studies that use self-attribution studies, “find neural activation of the medial prefrontal cortex (MPFC) as evidence for the self”, while studies consisting of self-recognition tasks find, “correlated activation of the right prefrontal lobe and regions of the medial and left hemisphere as evidence self-awareness and self-knowledge.\"\n\nThis variety of results of the studies have provided evidence for Gillihan and Farah to conclude that there is not a unitary and common neural system concerning the self, which leads into the next two problems of the traditional conception of self. These problems are that the traditional conceptions of the self argue that the self is a unified mechanism that exists in a centralized area in the brain, and that the self is something we can consciously sense. The problems with these accounts are that there is scientific research contradicting the claims of traditional conceptions. Llinas suggests that the self is a form of perception and thus that the self is an invention of the brain just as secondary sensory qualities are. He argues that there is no one brain area that could account for the self, and he concludes that the self doesn’t exist. Although Llinas is confusing the conscious and unconscious self, because when he suggests the self as a form of perception he should be referring to the conscious self being a form of perception. Munevar suggests that Llinas is incorrect in assuming the self does not exist, because just as an elephant and the perception of an elephant are different things, so are the self and our perception of the self.\n\nTraditional conceptions also assume that the self is a collection of episodic memories, since they are memories of the actions that sculpt our personalities, and therefore closely link personality and the self. According to Klein, in a study of patients who had lost their hippocampi (the part of the brain responsible for memories), patients were able to recollect their knowledge of personality even in the absence of episodic memories. This points to show that the trait summaries of their personality must have come around subconsciously, inferring that the self can exist even in the absence of episodic memories. Thus, the self cannot be a collection of episodic memories.\n\nThe Evolutionary Theory points out that distributive systems are characteristic of the brain. Organisms need to identify itself from others in many complex ways, thus many areas of the brain are used when distinguishes self from other. We need to understand that the brain is full of distributed mechanisms, thus creating an expectation that the self will be too. With this understanding, the Evolutionary Theory of the Self avoids all the difficulties about the unified and central mechanisms that traditional conceptions hold. In addition, the brain-based model of the evolutionary theory recognizes that the majority of the mental tasks highly complex organisms do are unconscious. Simply put, we may be aware of a decision we have made, but not the calculations that went into the decision as they are not capable to be understood by the conscious self. By attempting to take an evolutionary approach to understanding the self, the confrontation of self with the sense of self is avoided, in addition to, “all the problems that arise from an undue emphasis on consciousness.”\n\nAs suggested by Munevar, research on the evolutionary theory of self needs to firstly examine the existence of distributed activation in the brain when doing a self-attribution task. Secondly, they need to examine distinctions in brain activity when identifying self and close others in a manner of objective vs subjective to see if there are activation variances between the two. Thirdly, research needs to use non-personality traits as well as personality traits to take into account the importance of non-personality traits to the evolutionary needs of an organism. Fourthly, the research should aim to prove that humans, “should identify with those close to us, although not as strongly as with ourselves.\" These four aims were the four that structured the hypotheses in his study, and provide blueprint for further research to test.\n\nIn Munevar’s study, they compared self vs other conditions in which personality trait adjectives were rated in terms of Self vs Best Friend, Self vs Bill Gates, and Best Friend vs Bill Gates. He used a blocked-design fMRI paradigm, where non-personality and personality trait adjectives were rated as to whether they applied to themselves or other. The four hypotheses were as follows:(1) Self conditions would show a different pattern of brain activation from those shown by Best Friend (i.e., close other) and Bill Gates (i.e., far other) conditions; (2) our data should exhibit a fair degree of distributive performance by the brain in responding to these attribution tasks; and (3) the resulting patterns of activation should show some overlap with structures normally involved in preparedness to action (motion). In addition, (4) the Best Friend condition would also differ from the Bill Gates condition.The study found that several areas of the brain were active when doing the tasks related to ‘Self’ and Best Friend. In support of the first hypothesis (1), there was very significant differential activation in BA 31 (covering part of posterior cingulate gyrus and the medial parietal), and the substantia nigra. There was significant activity in the right and left portions of BA 23, and the caudate tail. In a lesser degree, there was some activity in the BA 10 and the thalamus. There was also greater activation in BA 24, particularly the anterior cingulate cortex, in Self-Bill Gates condition compared to the Best Friend-Bill Gates condition. With these results, there is different brain patterns shown by Best Friend and Bill Gate conditions. The contrasting results found also support the second hypothesis (2), and indicate that a large number of distributive structures throughout the brain are used when doing a self-attribution task.\n\nIn support of the third hypothesis (3), he found a large amount of activation of the substantia nigra (key stricture of basal ganglia for action), in addition to the activation of the caudate nucleus of the basal ganglia. These areas in the brain are known to be crucial to the process of movement, and thus an overlap with these structures supports their hypothesis.\n\nIn support of the fourth hypothesis (4), the greater activation of BA 24 in comparison of Best Friend vs Bill Gates condition and the Self-Bill Gates condition points to show that humans strongly identify with those you are close to us, and less strongly with ourselves.\n\nThe results displayed that the brain areas that we use for thinking about ourselves may also be used for thinking about those who are important to us. The fact that there is this relationship with multiple parts of the brain being active during both goes to show that there is an association between the two, furthering to suggest that one area in the brain nor one unified mechanism is not responsible for the self. The results also support the evolutionary expectation of a connection between self and the preparedness for action. The activation of key areas in the basal ganglia and the differential activation of BA 31 by the self-conditions in contrast with the Best Friend conditions in this study, elude to further support the idea of personality being a product of the unconscious self.\n\nFrom the results seen in Munevar’s study, there are several implications for the future progress of this theory. Since the study had a limitation of gender impacting the results of the conditions of Self vs Best Friend and Best Friend vs Bill Gates, further research should be done that classify results of fMRI by each gender. Another positive direction this study points to is by studying brains with deficits in activation of the anterior cingulate that bring problems when identifying self from non-self. This problem is prevalent in mental disorders such as schizophrenia, autism, and late-stage Alzheimer’s, where the ability to differentiate self vs other is compromised. Further research should use the results of Munevar's study and continue on to dissect the paradoxes and problems of traditional conceptions, while focusing on the evolutionary biology of the brain. Overall, the research done on the Evolutionary Theory of Self is promising and provides fruitful insight to the possibility of discovering new conclusions on what we know about the self.\n"}
{"id": "10919195", "url": "https://en.wikipedia.org/wiki?curid=10919195", "title": "Fermat Prize", "text": "Fermat Prize\n\nThe Fermat prize of mathematical research bi-annually rewards research works in fields where the contributions of Pierre de Fermat have been decisive:\n\n\nThe spirit of the prize is focused on rewarding the results of research accessible to the greatest number of professional mathematicians within these fields. The Fermat prize was created in 1989 and is awarded once every two years in Toulouse by the Institut de Mathématiques de Toulouse. The amount of the Fermat prize has been fixed at 20,000 Euros for the twelfth edition (2011).\n\n\nThere has also been a \"Pierre Fermat medal\", which has been awarded for example to Linus Pauling (1957) and Ernst Peschl (1965).\n\nThe Junior Fermat Prize is a mathematical prize, awarded every two years to a student in the first four years of university for a contribution to mathematics. The amount of the prize is 2000 Euros.\n\n"}
{"id": "2210759", "url": "https://en.wikipedia.org/wiki?curid=2210759", "title": "Finite strain theory", "text": "Finite strain theory\n\nIn continuum mechanics, the finite strain theory—also called large strain theory, or large deformation theory—deals with deformations in which strains and/or rotations are large enough to invalidate assumptions inherent in infinitesimal strain theory. In this case, the undeformed and deformed configurations of the continuum are significantly different, requiring a clear distinction between them. This is commonly the case with elastomers, plastically-deforming materials and other fluids and biological soft tissue.\n\nThe displacement of a body has two components: a rigid-body displacement and a deformation.\n\nA change in the configuration of a continuum body can be described by a displacement field. A \"displacement field\" is a vector field of all displacement vectors for all particles in the body, which relates the deformed configuration with the undeformed configuration. The distance between any two particles changes if and only if deformation has occurred. If displacement occurs without deformation, then it is a rigid-body displacement.\n\nThe displacement of particles indexed by variable may be expressed as follows. The vector joining the positions of a particle in the undeformed configuration formula_3 and deformed configuration formula_4 is called the displacement vector. Using formula_5 in place of formula_3 and formula_7 in place of formula_4, both of which are vectors from the origin of the coordinate system to each respective point, we have the Lagrangian description of the displacement vector:\n\nWhere formula_10 are the orthonormal unit vectors that define the basis of the spatial (lab-frame) coordinate system.\n\nExpressed in terms of the material coordinates, the displacement field is:\n\nWhere formula_12 is the displacement vector representing rigid-body translation.\n\nThe partial derivative of the displacement vector with respect to the material coordinates yields the \"material displacement gradient tensor\" formula_13. Thus we have,\nwhere formula_15 is the \"deformation gradient tensor\".\n\nIn the Eulerian description, the vector extending from a particle formula_16 in the undeformed configuration to its location in the deformed configuration is called the displacement vector:\n\nWhere formula_18 are the unit vectors that define the basis of the material (body-frame) coordinate system.\n\nExpressed in terms of spatial coordinates, the displacement field is:\n\nThe partial derivative of the displacement vector with respect to the spatial coordinates yields the \"spatial displacement gradient tensor\" formula_20. Thus we have,\n\nformula_22 are the direction cosines between the material and spatial coordinate systems with unit vectors formula_23 and formula_10, respectively. Thus\n\nThe relationship between formula_26 and formula_27 is then given by\n\nKnowing that\nthen\n\nIt is common to superimpose the coordinate systems for the deformed and undeformed configurations, which results in formula_31, and the direction cosines become Kronecker deltas, i.e.\n\nThus in material (undeformed) coordinates, the displacement may be expressed as:\n\nAnd in spatial (deformed) coordinates, the displacement may be expressed as:\n\nThe deformation gradient tensor formula_35 is related to both the reference and current configuration, as seen by the unit vectors formula_36 and formula_37, therefore it is a \"two-point tensor\".\n\nDue to the assumption of continuity of formula_38, formula_15 has the inverse formula_40, where formula_41 is the \"spatial deformation gradient tensor\". Then, by the implicit function theorem, the Jacobian determinant formula_42 must be nonsingular, i.e. formula_43\n\nThe \"material deformation gradient tensor\" formula_35 is a second-order tensor that represents the gradient of the mapping function or functional relation formula_38, which describes the motion of a continuum. The material deformation gradient tensor characterizes the local deformation at a material point with position vector formula_46, i.e. deformation at neighbouring points, by transforming (linear transformation) a material line element emanating from that point from the reference configuration to the current or deformed configuration, assuming continuity in the mapping function formula_38, i.e. differentiable function of formula_48 and time formula_49, which implies that cracks and voids do not open or close during the deformation. Thus we have,\n\nConsider a particle or material point formula_16 with position vector formula_52 in the undeformed configuration (Figure 2). After a displacement of the body, the new position of the particle indicated by formula_53 in the new configuration is given by the vector position formula_54. The coordinate systems for the undeformed and deformed configuration can be superimposed for convenience.\n\nConsider now a material point formula_55 neighboring formula_16, with position vector formula_57. In the deformed configuration this particle has a new position formula_58 given by the position vector formula_59. Assuming that the line segments formula_60 and formula_61 joining the particles formula_16 and formula_63 in both the undeformed and deformed configuration, respectively, to be very small, then we can express them as formula_64 and formula_65. Thus from Figure 2 we have\n\nwhere formula_67 is the relative displacement vector, which represents the relative displacement of formula_63 with respect to formula_16 in the deformed configuration.\n\nFor an infinitesimal element formula_64, and assuming continuity on the displacement field, it is possible to use a Taylor series expansion around point formula_16, neglecting higher-order terms, to approximate the components of the relative displacement vector for the neighboring particle formula_63 as\nThus, the previous equation formula_74 can be written as\n\nCalculations that involve the time-dependent deformation of a body often require a time derivative of the deformation gradient to be calculated. A geometrically consistent definition of such a derivative requires an excursion into differential geometry but we avoid those issues in this article.\n\nThe time derivative of formula_76 is\nwhere formula_78 is the velocity. The derivative on the right hand side represents a material velocity gradient. It is common to convert that into a spatial gradient, i.e.,\nwhere formula_80 is the spatial velocity gradient. If the spatial velocity gradient is constant, the above equation can be solved exactly to give\nassuming formula_82 at formula_83. There are several methods of computing the exponential above.\n\nRelated quantities often used in continuum mechanics are the rate of deformation tensor and the spin tensor defined, respectively, as:\nThe rate of deformation tensor gives the rate of stretching of line elements while the spin tensor indicates the rate of rotation or vorticity of the motion.\n\nTo transform quantities that are defined with respect to areas in a deformed configuration to those relative to areas in a reference configuration, and vice versa, we use Nanson's relation, expressed as\n\nwhere formula_86 is an area of a region in the deformed configuration, formula_87 is the same area in the reference configuration, and formula_88 is the outward normal to the area element in the current configuration while formula_89 is the outward normal in the reference configuration, formula_90 is the deformation gradient, and formula_91.\n\nThe corresponding formula for the transformation of the volume element is \n\nThe deformation gradient formula_90, like any invertible second-order tensor, can be decomposed, using the polar decomposition theorem, into a product of two second-order tensors (Truesdell and Noll, 1965): an orthogonal tensor and a positive definite symmetric tensor, i.e.\n\nwhere the tensor formula_95 is a proper orthogonal tensor, i.e. formula_96 and formula_97, representing a rotation; the tensor formula_98 is the \"right stretch tensor\"; and formula_99 the \"left stretch tensor\". The terms \"right\" and \"left\" means that they are to the right and left of the rotation tensor formula_95, respectively. formula_98 and formula_99 are both positive definite, i.e. formula_103 and formula_104 for all formula_105, and symmetric tensors, i.e. formula_106 and formula_107, of second order.\n\nThis decomposition implies that the deformation of a line element formula_64 in the undeformed configuration onto formula_65 in the deformed configuration, i.e. formula_110, may be obtained either by first stretching the element by formula_111, i.e. formula_112, followed by a rotation formula_113, i.e. formula_114; or equivalently, by applying a rigid rotation formula_113 first, i.e. formula_116, followed later by a stretching formula_117, i.e. formula_118 (See Figure 3).\n\nDue to the orthogonality of formula_119\n\nso that formula_111 and formula_117 have the same eigenvalues or \"principal stretches\", but different eigenvectors or \"principal directions\" formula_123 and formula_124, respectively. The principal directions are related by\n\nThis polar decomposition, which is unique as formula_15 is invertible with a positive determinant, is a corrolary of the singular-value decomposition.\n\nSeveral rotation-independent deformation tensors are used in mechanics. In solid mechanics, the most popular of these are the right and left Cauchy–Green deformation tensors.\n\nSince a pure rotation should not induce any strains in a deformable body, it is often convenient to use rotation-independent measures of deformation in continuum mechanics. As a rotation followed by its inverse rotation leads to no change (formula_127) we can exclude the rotation by multiplying formula_90 by its transpose.\n\nIn 1839, George Green introduced a deformation tensor known as the \"right Cauchy–Green deformation tensor\" or \"Green's deformation tensor\", defined as:\n\nPhysically, the Cauchy–Green tensor gives us the square of local change in distances due to deformation, i.e. formula_130\n\nInvariants of formula_131 are often used in the expressions for strain energy density functions. The most commonly used invariants are\nwhere formula_133 are stretch ratios for the unit fibers that are initially oriented along the eigenvector directions of the right (reference) stretch tensor (these are not generally aligned with the three axis of the coordinate systems).\n\nThe IUPAC recommends that the inverse of the right Cauchy–Green deformation tensor (called the Cauchy tensor in that document), i. e., formula_134, be called the Finger tensor. However, that nomenclature is not universally accepted in applied mechanics.\n\nReversing the order of multiplication in the formula for the right Green–Cauchy deformation tensor leads to the \"left Cauchy–Green deformation tensor\" which is defined as:\n\nThe left Cauchy–Green deformation tensor is often called the \"Finger deformation tensor\", named after Josef Finger (1894).\n\nInvariants of formula_137 are also used in the expressions for strain energy density functions. The conventional invariants are defined as\nwhere formula_139 is the determinant of the deformation gradient.\n\nFor incompressible materials, a slightly different set of invariants is used:\n\nEarlier in 1828, Augustin Louis Cauchy introduced a deformation tensor defined as the inverse of the left Cauchy–Green deformation tensor, formula_141. This tensor has also been called the Piola tensor and the Finger tensor in the rheology and fluid dynamics literature.\n\nIf there are three distinct principal stretches formula_143, the spectral decompositions of formula_131 and formula_137 is given by\n\nFurthermore,\n\nObserve that\nTherefore, the uniqueness of the spectral decomposition also implies that formula_150. The left stretch (formula_99) is also called the \"spatial stretch tensor\" while the right stretch (formula_98) is called the \"material stretch tensor\".\n\nThe effect of formula_90 acting on formula_123 is to stretch the vector by formula_133 and to rotate it to the new orientation formula_124, i.e.,\nIn a similar vein, \n\nDerivatives of the stretch with respect to the right Cauchy–Green deformation tensor are used to derive the stress-strain relations of many solids, particularly hyperelastic materials. These derivatives are\nand follow from the observations that\n\nLet formula_161 be a Cartesian coordinate system defined on the undeformed body and let formula_162 be another system defined on the deformed body. Let a curve formula_163 in the undeformed body be parametrized using formula_164. Its image in the deformed body is formula_165.\n\nThe undeformed length of the curve is given by\nAfter deformation, the length becomes\nNote that the right Cauchy–Green deformation tensor is defined as\nHence,\nwhich indicates that changes in length are characterized by formula_170.\n\nThe concept of \"strain\" is used to evaluate how much a given displacement differs locally from a rigid body displacement. One of such strains for large deformations is the \"Lagrangian finite strain tensor\", also called the \"Green-Lagrangian strain tensor\" or \"Green – St-Venant strain tensor\", defined as\n\nor as a function of the displacement gradient tensor\nor\n\nThe Green-Lagrangian strain tensor is a measure of how much formula_174 differs from formula_175.\n\nThe \"Eulerian-Almansi finite strain tensor\", referenced to the deformed configuration, i.e. Eulerian description, is defined as\n\nor as a function of the displacement gradients we have\n\nB. R. Seth from the Indian Institute of Technology, Kharagpur was the first to show that the Green and Almansi strain tensors are special cases of a more general strain measure. The idea was further expanded upon by Rodney Hill in 1968. The Seth–Hill family of strain measures (also called Doyle-Ericksen tensors) can be expressed as\n\nFor different values of formula_179 we have:\n\nThe second-order approximation of these tensors is\nwhere formula_182 is the infinitesimal strain tensor.\n\nMany other different definitions of tensors formula_183 are admissible, provided that they all satisfy the conditions that:\n\nAn example is the set of tensors \nwhich do not belong to the Seth–Hill class, but have the same 2nd-order approximation as the Seth–Hill measures at formula_191 for any value of formula_192.\n\nThe stretch ratio is a measure of the extensional or normal strain of a differential line element, which can be defined at either the undeformed configuration or the deformed configuration.\n\nThe stretch ratio for the differential element formula_193 (Figure) in the direction of the unit vector formula_194 at the material point formula_16, in the undeformed configuration, is defined as\n\nwhere formula_197 is the deformed magnitude of the differential element formula_64.\n\nSimilarly, the stretch ratio for the differential element formula_199 (Figure), in the direction of the unit vector formula_200 at the material point formula_53, in the deformed configuration, is defined as\n\nThe normal strain formula_203 in any direction formula_194 can be expressed as a function of the stretch ratio,\n\nThis equation implies that the normal strain is zero, i.e. no deformation, when the stretch is equal to unity. Some materials, such as elastometers can sustain stretch ratios of 3 or 4 before they fail, whereas traditional engineering materials, such as concrete or steel, fail at much lower stretch ratios, perhaps of the order of 1.1 (reference?)\n\nThe diagonal components formula_206 of the Lagrangian finite strain tensor are related to the normal strain, e.g.\n\nwhere formula_208 is the normal strain or engineering strain in the direction formula_209.\n\nThe off-diagonal components formula_206 of the Lagrangian finite strain tensor are related to shear strain, e.g.\n\nwhere formula_212 is the change in the angle between two line elements that were originally perpendicular with directions formula_209 and formula_214, respectively.\n\nUnder certain circumstances, i.e. small displacements and small displacement rates, the components of the Lagrangian finite strain tensor may be approximated by the components of the infinitesimal strain tensor\n\nA representation of deformation tensors in curvilinear coordinates is useful for many problems in continuum mechanics such as nonlinear shell theories and large plastic deformations. Let formula_215 denote the function by which a position vector in space is constructed from coordinates formula_216. The coordinates are said to be \"convected\" if they correspond to a one-to-one mapping to and from Lagrangian particles in a continuum body. If the coordinate grid is \"painted\" on the body in its initial configuration, then this grid will deform and flow with the motion of material to remain painted on the same material particles in the deformed configuration so that grid lines intersect at the same material particle in either configuration. The tangent vector to the deformed coordinate grid line curve formula_217 at formula_218 is given by\nThe three tangent vectors at formula_218 form a local basis. These vectors are related the reciprocal basis vectors by \n\nLet us define a second-order tensor field formula_222 (also called the metric tensor) with components\nThe Christoffel symbols of the first kind can be expressed as\n\nTo see how the Christoffel symbols are related to the Right Cauchy–Green deformation tensor let us similarly define two bases, the already mentioned one that is tangent to deformed grid lines and another that is tangent to the undeformed grid lines. Namely,\n\nUsing the definition of the gradient of a vector field in curvilinear coordinates, the deformation gradient can be written as\n\nThe right Cauchy–Green deformation tensor is given by\nIf we express formula_170 in terms of components with respect to the basis {formula_229} we have\nTherefore, \n\nand the corresponding Christoffel symbol of the first kind may be written in the following form.\n\nLet us consider a one-to-one mapping from formula_233 to formula_234 and let us assume that there exist two positive-definite, symmetric second-order tensor fields formula_235 and formula_222 that satisfy\nThen,\nNoting that\nand formula_240 we have\nDefine\nHence\nDefine\nThen\nDefine the Christoffel symbols of the second kind as\nThen\nTherefore,\nThe invertibility of the mapping implies that\nWe can also formulate a similar result in terms of derivatives with respect to formula_250. Therefore,\n\nThe problem of compatibility in continuum mechanics involves the determination of allowable single-valued continuous fields on bodies. These allowable conditions leave the body without unphysical gaps or overlaps after a deformation. Most such conditions apply to simply-connected bodies. Additional conditions are required for the internal boundaries of multiply connected bodies.\n\nThe necessary and sufficient conditions for the existence of a compatible formula_252 field over a simply connected body are\n\nThe necessary and sufficient conditions for the existence of a compatible formula_170 field over a simply connected body are\nWe can show these are the mixed components of the Riemann–Christoffel curvature tensor. Therefore, the necessary conditions for formula_170-compatibility are that the Riemann–Christoffel curvature of the deformation is zero.\n\nNo general sufficiency conditions are known for the left Cauchy–Green deformation tensor in three-dimensions. Compatibility conditions for two-dimensional formula_257 fields have been found by Janet Blume.\n\n\n"}
{"id": "8442822", "url": "https://en.wikipedia.org/wiki?curid=8442822", "title": "Genchi Genbutsu", "text": "Genchi Genbutsu\n\nTaiichi Ohno, creator of the Toyota Production System is credited, perhaps apocryphally, with taking new graduates to the shopfloor and drawing a chalk circle on the floor. The graduate would be told to stand in the circle, observe and note what he saw. When Ohno returned he would check; if the graduate had not seen enough he would be asked to keep observing. Ohno was trying to imprint upon his future engineers that the only way to truly understand what happens on the shop floor was to go there. It was where value was added and waste could be observed. \n\nGenchi Genbutsu is therefore a key approach in problem solving. If the problem exists on the shop floor then it needs to be understood and solved at the shop floor. \n\nGenchi Genbutsu is also called Gemba attitude. Gemba is the Japanese term for \"the place\" in this case \"the place where it actually happens\". Since real value is created at the shop floor in manufacturing, this is where managers need to spend their time.\n\nGenchi Genbutsu is sometimes referred to as \"Getcha boots on\" (and go out and see what is happening) due to its similar cadence and meaning. It has been compared to Peters and Waterman's idea of \"Management By Wandering Around\". This concept quickly became so universal that new managers instinctively knew that they had to \"walk around\" to achieve high effectiveness levels. Whilst these ideas, with their associated lists of how-tos, are probably good ideas they may miss the essential nature of Genchi Genbutsu which is less to \"visit\" and more to \"know\" by being there. Toyota has high levels of management presence on the production line whose role is to \"know\" and to constantly improve.\n\n\"Gemba attitude\" reflects the idea that whatever reports, measures and ideas are transmitted to management are only an abstraction of what is actually going on in the \"genba\" to create value. Metrics and reports will reflect the attitudes of the management questioner and the workplace responder as well as how the responder views the questioner. It also increases the chance that actual issues and unplanned events will be observed first hand and can be managed immediately; this includes issues that are not apparent to the \"genba\" workforce.\n\n"}
{"id": "40543209", "url": "https://en.wikipedia.org/wiki?curid=40543209", "title": "Hongaku", "text": "Hongaku\n\nHongaku () is an East Asian Buddhist doctrine often translated as \"inherent\", \"innate\", \"intrinsic\" or \"original\" enlightenment and is the view that all sentient beings already are enlightened or awakened in some way. It is closely tied with the concept of Buddha-nature.\n\nThe doctrine of innate enlightenment was developed in China out of the Buddha-nature doctrine. It is first mentioned in the Awakening of Faith in the Mahayana scripture. According to Jacqueline Stone, The awakening of faith in the Mahayana sees original enlightenment as \"true suchness considered under the aspect of conventional deluded consciousness and thus denotes the potential for enlightenment in unenlightened beings.\" In medieval China, the doctrine developed from the Huayan school and also influenced Chan Buddhism.\n\nThe doctrine is also a common theme of the \"Platform Sutra\" of Huineng and was taught by Chinese Chan masters as \"seeing original nature\". Inherent enlightenment was often associated with the teachings of sudden enlightenment and contrasted with the \"gradual\" approach and the idea of “acquired enlightenment” or \"shikaku\". The first Japanese to write of this doctrine was Kūkai (774–835), founder of Shingon Buddhism.\n\nThe doctrine of innate enlightenment was very influential in Tendai from the cloistered rule era (1086–1185) through the Edo period (1688–1735). The Tendai view of hongaku saw it as encompassing not only all sentient beings, but all living things and all nature, even inanimate objects - all were considered to be Buddha. This also includes all our actions and thoughts, even our deluded thoughts, as expressions of our innately enlightened nature.\n\nTamura Yoshirõ (1921–1989) saw original as being defined by two major philosophical elements. One was a radical non-dualism, in which everything was seen as empty and interconnected, so that the differences between ordinary person and Buddha and all other distinctions, were ontologically negated. The other feature of hongaku was the affirmation of the phenomenal world as an expression of the nondual realm of Buddha nature. This was expressed in phrases such as “the worldly passions are precisely enlightenment” and “birth and death are precisely nirvana.”\n\nThe Tendai doctrine of hongaku had deep impact on the development of New Kamakura Buddhism, for many of those who founded new Kamakura Buddhist schools (Eisai, Honen, Shinran, Dogen and Nichiren) studied Tendai at Mount Hiei.\n\nDuring the 1980s a Japanese movement known as Critical Buddhism has attacked original enlightenment as an ideology that supports the status quo and legitimates social injustice by accepting all things as they are as expressions of original Buddha nature.\n\n\n\n"}
{"id": "1018292", "url": "https://en.wikipedia.org/wiki?curid=1018292", "title": "Hylozoism", "text": "Hylozoism\n\nHylozoism is the philosophical point of view that matter is in some sense alive. The concept dates back at least as far as the Milesian school of pre-Socratic philosophers. The term was introduced to English by Ralph Cudworth in 1678.\n\nAlthough there is a distinction between possessing a mind (hylopsychism) and possessing life (hylozoism); in practice this division is difficult to maintain, because the ancient hylozoists not only regarded the spirits of the material universe and plant world as alive, but also as more or less conscious. Whereas animism tends to view life as taking the form of discrete spirits, and panpsychism tends to refer to strictly philosophical views like that of Gottfried Wilhelm Leibniz, hylozoism refers largely to views such as those of the earliest Greek philosophers (6th and 5th centuries BC), who treated the magnet as alive because of its attractive powers (Thales), or air as divine (Anaximenes), perhaps because of its apparently spontaneous power of movement, or because of its essentiality for life in animals. Later this primitive hylozoism reappeared in modified forms. Some scholars have since claimed that the term \"hylozoism\" should properly be used only where body and soul are explicitly distinguished, the distinction then being rejected as invalid. Nevertheless, hylozoism remains logically distinct both from early forms of animism, which personify nature, and from panpsychism, which attributes some form of consciousness or sensation to all matter.\n\nSome of the ancient Greek philosophers taught a version of hylozoism, as they, however vaguely, conceived the elemental matter as being in some sense animate if not actually conscious and (a directed effort, a striving or tendency; a \"nisus\"). Thales, Anaximenes, and Heraclitus all taught that there is a form of life in all material objects, and the Stoics believed that a \"world soul\" was the vital force of the universe. Note that these philosophies did not necessarily hold that material objects had \"separate life\" or \"identity\", only that they had life, either as part of an overriding entity or as living but insensible entities.\n\nIn the Renaissance, Bernardino Telesio, Paracelsus, Cardanus, and Giordano Bruno revived the doctrine of hylozoism. The latter, for example, held a form of Christian pantheism, wherein God is the source, cause, medium, and end of all things, and therefore all things are participatory in the ongoing Godhead. Bruno's ideas were so radical that he was entirely rejected by the Roman Catholic Church as well as excommunicated from a few Protestant groups, and he was eventually burned at the stake for various heresies. Telesio, on the other hand, began from an Aristotelian basis and, through radical empiricism, came to believe that a living force was what informed all matter. Instead of the intellectual universals of Aristotle, he believed that life generated form.\n\nIn England, some of the Cambridge Platonists approached hylozoism as well. Both Henry More and Ralph Cudworth (\"the Younger\", 1617–1688), through their reconciliation of Platonic idealism with Christian doctrines of deific generation, came to see the divine lifeforce as the informing principle in the world. Thus, like Bruno, but not nearly to the extreme, they saw God's generative impulse as giving life to all things that exist. Accordingly, Cudworth, the most systematic metaphysician of the Cambridge Platonist tradition, fought hylozoism. His work is primarily a critique of what he took to be the two principal forms of atheism—materialism and hylozoism.\n\nCudworth singled out Hobbes not only as a defender of the hylozoic atheism \"which attributes life to matter\", but also as one going beyond it and defending \"hylopathian atheism, which attributes all to matter.\" Cudworth attempted to show that Hobbes had revived the doctrines of Protagoras and was therefore subject to the criticisms which Plato had deployed against Protagoras in the \"Theaetetus\". On the side of hylozoism, Strato of Lampsacus was the official target. However, Cudworth's Dutch friends had reported to him the views which Spinoza was circulating in manuscript. Cudworth remarks in his \"Preface\" that he would have ignored hylozoism had he not been aware that a new version of it would shortly be published.\n\nSpinoza's idealism also tends toward hylozoism. In order to hold a balance even between matter and mind, Spinoza combined materialistic with pantheistic hylozoism, by demoting both to mere attributes of the one infinite substance. Although specifically rejecting identity in inorganic matter, he, like the Cambridge Platonists, sees a life force within, as well as beyond, all matter.\n\nImmanuel Kant presented arguments against hylozoism in the third chapter of his \"Metaphysische Anfangsgründe der Naturwissenschaften\" (\"First Metaphysical Principles of Natural Science,\" 1786) and also in his famous \"Kritik der reinen Vernunft\" (\"Critique of Pure Reason,\" 1783). Yet, in our times, scientific hylozoism – whether modified, or keeping the trend to make all beings conform to some uniform pattern, to which the concept was adhered in modernity by Herbert Spencer, Hermann Lotze, and Ernst Haeckel – was often called upon as a protest against a mechanistic worldview.\n\nIn the 19th century, Haeckel developed a materialist form of hylozoism, specially against Rudolf Virchow's and Hermann von Helmholtz's mechanical views of humans and nature. In his \"Die Welträtsel\" of 1899 (\"The Riddle of the Universe\" 1901), Haeckel upheld a unity of organic and inorganic nature and derived all actions of both types of matter from natural causes and laws. Thus, his form of hylozoism reverses the usual course by maintaining that living and nonliving things are essentially the same, and by erasing the distinction between the two and stipulating that they behave by a single set of laws.\n\nIn contrast, the Argentine-German neurobiological tradition terms \"hylozoic hiatus\" all of the parts of nature which can only behave lawfully or nomically and, upon such a feature, are described as lying outside of minds and amid them – i.e. extramentally. Thereby the hylozoic hiatus becomes contraposed to minds deemed able of behaving semoviently, i.e. able of inaugurating new causal series (semovience). Hylozoism in this contemporary neurobiological tradition is thus restricted to the portions of nature behaving nomically inside the minds, namely the minds' sensory reactions (Christfried Jakob's \"sensory intonations\") whereby minds react to the stimuli coming from the hylozoic hiatus or extramental realm.\n\nMartin Buber too takes an approach that is quasi-hylozoic. By maintaining that the essence of things is identifiable and separate, although not pre-existing, he can see a soul within each thing.\n\nThe French Pythagorean and Rosicrucian alchemist, Francois Jollivet-Castelot (1874-1937), established a hylozoic esoteric school which combined the insight of spagyrics, chemistry, physics, transmutations and metaphysics. He published many books, one of which was called \"L’Hylozoïsme, l’alchimie, les chimistes unitaires\" (1896). In his view there was no difference between spirit and matter except for the degree of frequency and other vibrational conditions.\n\nThe Mormon theologian Orson Pratt taught a form of hylozoism.\n\nAlice A. Bailey wrote a book called \"The Consciousness of the Atom\".\n\nInfluenced by Alice A. Bailey, Charles Webster Leadbeater, and their predecessor Madame Blavatsky, Henry T. Laurency produced voluminous writings describing a hylozoic philosophy.\n\nInfluenced by George Ivanovich Gurdjieff, the English philosopher and mathematician John Godolphin Bennett, in his four-volume work \"The Dramatic Universe\" and his book \"Energies\", developed a six-dimensional framework in which matter-energy takes on 12 levels of hylozoic quality.\n\nKen Wilber embraces hylozoism to explain subjective experience and provides terms describing the ladder of subjective experience experienced by entities from atoms up to Human beings in the \"upper left quadrant\" of his Integral philosophy chart.\n\nPhysicist Thomas Brophy, in The Mechanism Demands a Mysticism, embraces hylozoism as the basis of a framework for re-integrating modern physical science with perennial spiritual philosophy. Brophy coins two additional words to stand with hylozoism as the three possible ontological stances consistent with modern physics. Thus: hylostatism (universe is deterministic, thus “static” in a four-dimensional sense); hylostochastism (universe contains a fundamentally random or stochastic component); hylozoism (universe contains a fundamentally alive aspect).\n\nArchitect Christopher Alexander has put forth a theory of the living universe, where life is viewed as a pervasive patterning that extends to what is normally considered non-living things, notably buildings. He wrote a four-volume work called \"The Nature of Order\" which explicates this theory in detail.\n\nPhilosopher and ecologist David Abram articulates and elaborates a form of hylozoism grounded in the phenomenology of sensory experience. In his books \"Becoming Animal\" and \"The Spell of the Sensuous,\" Abram suggests that matter is never entirely passive in our direct experience, holding rather that material things actively \"solicit our attention\" or \"call our focus,\" coaxing the perceiving body into an ongoing participation with those things. In the absence of intervening technologies, sensory experience is inherently animistic, disclosing a material field that is animate and self-organizing from the get-go. Drawing upon contemporary cognitive and natural science as well as the perspectival worldviews of diverse indigenous, oral cultures, Abram proposes a richly pluralist and story-based cosmology, in which matter is alive through and through. Such an ontology is in close accord, he suggests, with our spontaneous perceptual experience; it calls us back to our senses and to the primacy of the sensuous terrain, enjoining a more respectful and ethical relation to the more-than-human community of animals, plants, soils, mountains, waters and weather-patterns that materially sustains us.\n\nBruno Latour's actor-network theory, in the sociology of science, treats non-living things as active agents and thus bears some metaphorical resemblance to hylozoism.\n\nArt\nLiterature\nMMORPGs \nMusic\n"}
{"id": "1962927", "url": "https://en.wikipedia.org/wiki?curid=1962927", "title": "Imperial ban", "text": "Imperial ban\n\nThe imperial ban () was a form of outlawry in the Holy Roman Empire. At different times, it could be declared by the Holy Roman Emperor, by the Imperial Diet, or by courts like the League of the Holy Court (\"Vehmgericht\") or the \"Reichskammergericht\".\n\nPeople under imperial ban, known as \"Geächtete\" (from about the 17th century, colloquially also as \"Vogelfreierei\", lit. \"free as a bird\"), lost all their rights and possessions. They were legally considered dead, and anyone was allowed to rob, injure or kill them without legal consequences. The imperial ban automatically followed the excommunication of a person, as well as extending to anyone offering help to a person under the imperial ban.\n\nThose banned could reverse the ban by submitting to the legal authority. The \"Aberacht\", a stronger version of the imperial ban, could not be reversed.\n\nThe imperial ban was sometimes imposed on whole Imperial Estates. In that case, other estates could attack and seek to conquer them. The effect of the ban on a city or other Estate was that it lost its Imperial immediacy and in the future would have a second overlord in addition to the emperor.\n\nFamous people placed under the imperial ban included:\n\n\nThe imperial ban imposed by the Emperor Rudolf II on the city of Donauwörth after an anti-Catholic riot was one of the incidents leading to the Thirty Years' War.\n\n"}
{"id": "21301772", "url": "https://en.wikipedia.org/wiki?curid=21301772", "title": "List of bottling incidents by year", "text": "List of bottling incidents by year\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "32034040", "url": "https://en.wikipedia.org/wiki?curid=32034040", "title": "List of rampage killers (school massacres)", "text": "List of rampage killers (school massacres)\n\nThis section of the list of rampage killers contains those cases that occurred at kindergartens, schools and universities, as well as their affiliated buildings. If the perpetrator was a member of the school staff and the victims primarily his colleagues, the case is not included here, but it is added to the .\n\nA rampage killer has been defined as follows:\n\nThis list should contain every case with at least one of the following features:\n\nAll abbreviations used in the table are explained below. \nThe W-column gives a basic description of the weapons used in the murders\n"}
{"id": "1659215", "url": "https://en.wikipedia.org/wiki?curid=1659215", "title": "Log–log plot", "text": "Log–log plot\n\nIn science and engineering, a log–log graph or log–log plot is a two-dimensional graph of numerical data that uses logarithmic scales on both the horizontal and vertical axes. Monomials – relationships of the form formula_1 – appear as straight lines in a log–log graph, with the power term corresponding to the slope, and the constant term corresponding to the intercept of the line. Thus these graphs are very useful for recognizing these relationships and estimating parameters. Any base can be used for the logarithm, though most common are 10, e, and 2.\n\nGiven a monomial equation formula_2 taking the logarithm of the equation (with any base) yields:\nSetting formula_4 and formula_5 which corresponds to using a log–log graph, yields the equation:\nwhere \"m\" = \"k\" is the slope of the line (gradient) and \"b\" = log \"a\" is the intercept on the (log \"y\")-axis, meaning where log \"x\" = 0, so, reversing the logs, \"a\" is the \"y\" value corresponding to \"x\" = 1.\n\nThe equation for a line on a log–log scale would be:\n\nwhere \"m\" is the slope and \"b\" is the intercept point on the log plot.\n\nTo find the slope of the plot, two points are selected on the \"x\"-axis, say \"x\" and \"x\". Using the above equation:\n\nand\n\nThe slope \"m\" is found taking the difference:\n\nwhere \"F\" is shorthand for \"F\" ( \"x\" ) and \"F\" is shorthand for \"F\" ( \"x\" ). The figure at right illustrates the formula. Notice that the slope in the example of the figure is \"negative\". The formula also provides a negative slope, as can be seen from the following property of the logarithm:\n\nThe above procedure now is reversed to find the form of the function \"F\"(\"x\") using its (assumed) known log–log plot. To find the function \"F\", pick some \"fixed point\" (\"x\", \"F\"), where \"F\" is shorthand for \"F\"(\"x\"), somewhere on the straight line in the above graph, and further some other \"arbitrary point\" (\"x\", \"F\") on the same graph. Then from the slope formula above:\n\nwhich leads to\n\nNotice that 10 = \"F\". Therefore, the logs can be inverted to find:\n\nor\n\nwhich means that\n\nIn other words, \"F\" is proportional to \"x\" to the power of the slope of the straight line of its log–log graph. Specifically, a straight line on a log–log plot containing points (\"F\", \"x\") and (\"F\", \"x\") will have the function:\n\nOf course, the inverse is true too: any function of the form\n\nwill have a straight line as its log–log graph representation, where the slope of the line is \"m\".\n\nTo calculate the area under a continuous, straight-line segment of a log–log plot (or estimating an area of an almost-straight line), take the function defined previously\n\nand integrate it. Since it is only operating on a definite integral (two defined endpoints), the area A under the plot takes the form\n\nRearranging the original equation and plugging in the fixed point values, it is found that\n\nSubstituting back into the integral, you find that for A over x to x\n\nTherefore: formula_26\n\nFor m=-1, the integral becomes formula_27\n\nformula_28\n\nThese graphs are useful when the parameters \"a\" and \"b\" need to be estimated from numerical data. Specifications such as this are used frequently in economics.\n\nOne example is the estimation of money demand functions based on inventory theory, in which it can be assumed that money demand at time \"t\" is given by\n\nwhere \"M\" is the real quantity of money held by the public, \"R\" is the rate of return on an alternative, higher yielding asset in excess of that on money, \"Y\" is the public's real income, \"U\" is an error term assumed to be lognormally distributed, \"A\" is a scale parameter to be estimated, and \"b\" and \"c\" are elasticity parameters to be estimated. Taking logs yields\n\nwhere \"m\" = log \"M\", \"a\" = log \"A\", \"r\" = log \"R\", \"y\" = log \"Y\", and \"u\" = log \"U\" with \"u\" being normally distributed. This equation can be estimated using ordinary least squares.\n\nAnother economic example is the estimation of a firm's Cobb–Douglas production function, which is the right side of the equation\n\nin which \"Q\" is the quantity of output that can be produced per month, \"N\" is the number of hours of labor employed in production per month, \"K\" is the number of hours of physical capital utilized per month, \"U\" is an error term assumed to be lognormally distributed, and \"A\", formula_32, and formula_33 are parameters to be estimated. Taking logs gives the linear regression equation\n\nwhere \"q\" = log \"Q\", \"a\"=log \"A\", \"n\"=log \"N\", \"k\"=log \"K\", and \"u\"=log \"U\".\n\nLog–log regression can also be used to estimate the fractal dimension of a naturally occurring fractal.\n\nHowever, going in the other direction – observing that data appears as an approximate line on a log–log scale and concluding that the data follows a power law – is invalid.\n\nIn fact, many other functional forms appear approximately linear on the log–log scale, and simply evaluating the goodness of fit of a linear regression on logged data using the coefficient of determination (\"R\") may be invalid, as the assumptions of the linear regression model, such as Gaussian error, may not be satisfied; in addition, tests of fit of the log–log form may exhibit low statistical power, as these tests may have low likelihood of rejecting power laws in the presence of other true functional forms. While simple log–log plots may be instructive in detecting possible power laws, and have been used dating back to Pareto in the 1890s, validation as a power laws requires more sophisticated statistics.\n\nThese graphs are also extremely useful when data are gathered by varying the control variable along an exponential function, in which case the control variable \"x\" is more naturally represented on a log scale, so that the data points are evenly spaced, rather than compressed at the low end. The output variable \"y\" can either be represented linearly, yielding a lin–log graph (log \"x\", \"y\"), or its logarithm can also be taken, yielding the log–log graph (log \"x\", log \"y\").\n\nBode plot (a graph of the frequency response of a system) is also log-log plot.\n\n"}
{"id": "40963614", "url": "https://en.wikipedia.org/wiki?curid=40963614", "title": "Marking your own homework", "text": "Marking your own homework\n\nMark(ing) your own homework (where \"your\" is often substituted with \"their\", \"one's\", \"its\", \"his\" or \"her\") is a British expression used in political discourse, the study of organisational behaviour and in everyday life. The expression implies that whenever a person or group self-assesses and/or self regulates their own work they will usually treat it more favorably than if it were assessed by an independent person or group. \n\n\n"}
{"id": "294218", "url": "https://en.wikipedia.org/wiki?curid=294218", "title": "Molecular assembler", "text": "Molecular assembler\n\nA molecular assembler, as defined by K. Eric Drexler, is a \"proposed device able to guide chemical reactions by positioning reactive molecules with atomic precision\". A molecular assembler is a kind of molecular machine. Some biological molecules such as ribosomes fit this definition. This is because they receive instructions from messenger RNA and then assemble specific sequences of amino acids to construct protein molecules. However, the term \"molecular assembler\" usually refers to theoretical human-made devices.\n\nBeginning in 2007, the British Engineering and Physical Sciences Research Council has funded development of ribosome-like molecular assemblers. Clearly, molecular assemblers are possible in this limited sense. A technology roadmap project, led by the Battelle Memorial Institute and hosted by several U.S. National Laboratories has explored a range of atomically precise fabrication technologies, including both early-generation and longer-term prospects for programmable molecular assembly; the report was released in December, 2007. In 2008 the Engineering and Physical Sciences Research Council provided funding of 1.5 million pounds over six years for research working towards mechanized mechanosynthesis, in partnership with the Institute for Molecular Manufacturing, amongst others.\n\nLikewise, the term \"molecular assembler\" has been used in science fiction and popular culture to refer to a wide range of fantastic atom-manipulating nanomachines, many of which may be physically impossible in reality. Much of the controversy regarding \"molecular assemblers\" results from the confusion in the use of the name for both technical concepts and popular fantasies. In 1992, Drexler introduced the related but better-understood term \"molecular manufacturing,\" which he defined as the programmed \"chemical synthesis of complex structures by mechanically positioning reactive molecules, not by manipulating individual atoms.\"\n\nThis article mostly discusses \"molecular assemblers\" in the popular sense. These include hypothetical machines that manipulate individual atoms and machines with organism-like self-replicating abilities, mobility, ability to consume food, and so forth. These are quite different from devices that merely (as defined above) \"guide chemical reactions by positioning reactive molecules with atomic precision\".\n\nBecause synthetic molecular assemblers have never been constructed and because of the confusion regarding the meaning of the term, there has been much controversy as to whether \"molecular assemblers\" are possible or simply science fiction. Confusion and controversy also stem from their classification as nanotechnology, which is an active area of laboratory research which has already been applied to the production of real products; however, there had been, until recently, no research efforts into the actual construction of \"molecular assemblers\".\n\nNonetheless, a 2013 paper by David Leigh's group, published in the journal \"Science\", details a new method of synthesizing a peptide in a sequence-specific manner by using an artificial molecular machine that is guided by a molecular strand. This functions in the same way as a ribosome building proteins by assembling amino acids according to a messenger RNA blueprint. The structure of the machine is based on a rotaxane, which is a molecular ring sliding along a molecular axle. The ring carries a thiolate group which removes amino acids in sequence from the axle, transferring them to a peptide assembly site. In 2018, the same group published a more advanced version of this concept in which the molecular ring shuttles along a polymeric track to assemble an oligopeptide that can fold into a α-helix that can perform the enantioselective epoxidation of a chalcone derivative (in a way reminiscent to the ribosome assembling an enzyme). In another paper published in \"Science\" in March 2015, chemists at the University of Illinois report a platform that automates the synthesis of 14 classes of small molecules, with thousands of compatible building blocks.\n\nIn 2017 David Leigh's group reported a molecular robot that could be programmed to construct any one of four different stereoisomers of a molecular product by using a nanomechanical robotic arm to move a molecular substrate between different reactive sites of an artificial molecular machine. An accompanying News and Views article, titled ‘A molecular assembler’, outlined the operation of the molecular robot as effectively a prototypical molecular assembler.\n\nA nanofactory is a proposed system in which nanomachines (resembling molecular assemblers, or industrial robot arms) would combine reactive molecules via mechanosynthesis to build larger atomically precise parts. These, in turn, would be assembled by positioning mechanisms of assorted sizes to build macroscopic (visible) but still atomically-precise products.\n\nA typical nanofactory would fit in a desktop box, in the vision of K. Eric Drexler published in \"Nanosystems: Molecular Machinery, Manufacturing and Computation\" (1992), a notable work of \"exploratory engineering\". During the 1990s, others have extended the nanofactory concept, including an analysis of nanofactory convergent assembly by Ralph Merkle, a systems design of a replicating nanofactory architecture by J. Storrs Hall, Forrest Bishop's \"Universal Assembler\", the patented exponential assembly process by Zyvex, and a top-level systems design for a 'primitive nanofactory' by Chris Phoenix (Director of Research at the Center for Responsible Nanotechnology). All of these nanofactory designs (and more) are summarized in Chapter 4 of \"Kinematic Self-Replicating Machines\" (2004) by Robert Freitas and Ralph Merkle. The Nanofactory Collaboration, founded by Freitas and Merkle in 2000, is a focused ongoing effort involving 23 researchers from 10 organizations and 4 countries that is developing a practical research agenda specifically aimed at positionally-controlled diamond mechanosynthesis and diamondoid nanofactory development.\n\nIn 2005, a computer-animated short film of the nanofactory concept was produced by John Burch, in collaboration with Drexler. Such visions have been the subject of much debate, on several intellectual levels. No one has discovered an insurmountable problem with the underlying theories and no one has proved that the theories can be translated into practice. However, the debate continues, with some of it being summarized in the molecular nanotechnology article.\n\nIf nanofactories could be built, severe disruption to the world economy would be one of many possible negative impacts, though it could be argued that this disruption would have little negative effect if everyone had such nanofactories. Great benefits also would be anticipated. Various works of science fiction have explored these and similar concepts. The potential for such devices was part of the mandate of a major UK study led by mechanical engineering professor Dame Ann Dowling.\n\n\"Molecular assemblers\" have been confused with self-replicating machines. To produce a practical quantity of a desired product, the nanoscale size of a typical science fiction universal molecular assembler requires an extremely large number of such devices. However, a single such theoretical molecular assembler might be programmed to self-replicate, constructing many copies of itself. This would allow an exponential rate of production. Then after sufficient quantities of the molecular assemblers were available, they would then be re-programmed for production of the desired product. However, if self-replication of molecular assemblers were not restrained then it might lead to competition with naturally occurring organisms. This has been called ecophagy or the grey goo problem.\n\nOne method to building molecular assemblers is to mimic evolutionary processes employed by biological systems. Biological evolution proceeds by random variation combined with culling of the less-successful variants and reproduction of the more-successful variants. Production of complex molecular assemblers might be evolved from simpler systems since \"A complex system that works is invariably found to have evolved from a simple system that worked. . . . A complex system designed from scratch never works and can not be patched up to make it work. You have to start over, beginning with a system that works.\" However, most published safety guidelines include\n\"recommendations against developing ... replicator designs which permit surviving mutation or undergoing evolution\".\n\nMost assembler designs keep the \"source code\" external to the physical assembler. At each step of a manufacturing process, that step is read from an ordinary computer file and \"broadcast\" to all the assemblers.\nIf any assembler gets out of range of that computer, or when the link between that computer and the assemblers is broken, or when that computer is unplugged, the assemblers stop replicating.\nSuch a \"broadcast architecture\" is one of the safety features recommended by the \"Foresight Guidelines on Molecular Nanotechnology\", and a map of the 137-dimensional replicator design space recently published by Freitas and Merkle provides numerous practical methods by which replicators can be safely controlled by good design.\n\nOne of the most outspoken critics of some concepts of \"molecular assemblers\" was Professor Richard Smalley (1943–2005) who won the Nobel prize for his contributions to the field of nanotechnology. Smalley believed that such assemblers were not physically possible and introduced scientific objections to them. His two principal technical objections were termed the \"fat fingers problem\" and the \"sticky fingers problem\". He believed these would exclude the possibility of \"molecular assemblers\" that worked by precision picking and placing of individual atoms. Drexler and coworkers responded to these two issues in a 2001 publication.\n\nSmalley also believed that Drexler's speculations about apocalyptic dangers of self-replicating machines that have been equated with \"molecular assemblers\" would threaten the public support for development of nanotechnology. To address the debate between Drexler and Smalley regarding molecular assemblers \"Chemical & Engineering News\" published a point-counterpoint consisting of an exchange of letters that addressed the issues.\n\nSpeculation on the power of systems that have been called \"molecular assemblers\" has sparked a wider political discussion on the implication of nanotechnology. This is in part due to the fact that nanotechnology is a very broad term and could include \"molecular assemblers.\" Discussion of the possible implications of fantastic molecular assemblers has prompted calls for regulation of current and future nanotechnology. There are very real concerns with the potential health and ecological impact of nanotechnology that is being integrated in manufactured products. Greenpeace for instance commissioned a report concerning nanotechnology in which they express concern into the toxicity of nanomaterials that have been introduced in the environment. However, it makes only passing references to \"assembler\" technology. The UK Royal Society and Royal Academy of Engineering also commissioned a report entitled \"Nanoscience and nanotechnologies: opportunities and uncertainties\" regarding the larger social and ecological implications on nanotechnology. This report does not discuss the threat posed by potential so-called \"molecular assemblers.\"\n\nIn 2006, U.S. National Academy of Sciences released the report of a study of molecular manufacturing as part of a longer report, \"A Matter of Size: Triennial Review of the National Nanotechnology Initiative\" The study committee reviewed the technical content of \"Nanosystems\", and in its conclusion states that no current theoretical analysis can be considered definitive regarding several questions of potential system performance, and that optimal paths for implementing high-performance systems cannot be predicted with confidence. It recommends experimental research to advance knowledge in this area:\n\nOne potential scenario that has been envisioned is out-of-control self-replicating molecular assemblers in the form of grey goo which consumes carbon to continue its replication. If unchecked such mechanical replication could potentially consume whole ecoregions or the whole Earth (ecophagy), or it could simply outcompete natural lifeforms for necessary resources such as carbon, ATP, or UV light (which some nanomotor examples run on). However, the ecophagy and 'grey goo' scenarios, like synthetic molecular assemblers, are based upon still-hypothetical technologies that have not yet been demonstrated experimentally.\n\nMolecular assemblers are a popular topic in science fiction, for example, the matter compiler in The Diamond Age and the cornucopia machine in Singularity Sky. The replicator in \"Star Trek\" might also be considered a molecular assembler. A molecular assembler is also a key element of the plot of the computer game \"Deus Ex\" (called a \"universal constructor\" in the game).\n\nIn the political sci-fi comic series Transmetropolitan, written by Warren Ellis, machines called \"Makers\" are used to replicate and reform matter. Each morning, Makers sweep the streets for garbage, gathering the matter to recycle it into more useful objects. The main character also uses a Maker in his apartment to instantly produce a pair of glasses which take photos, as well as other objects such as clothing.\n\nIn Dead Money, a DLC of the video game , the player can obtain useful items from vending machines that use an unknown form of molecular assembly technology to transform casino chips that the player can find into any of several items.\n\nIn the cyberpunk visual novel Baldr Sky, a nano machine simply given the name \"Assembler\" is present for the purpose of cleaning the polluted atmosphere.\n\n"}
{"id": "273569", "url": "https://en.wikipedia.org/wiki?curid=273569", "title": "Neurocognitive", "text": "Neurocognitive\n\nNeurocognitive functions are cognitive functions closely linked to the function of particular areas, neural pathways, or cortical networks in the brain substrate layers of neurological matrix at the cellular molecular level. Therefore, their understanding is closely linked to the practice of neuropsychology and cognitive neuroscience, two disciplines that broadly seek to understand how the structure and function of the brain relate to cognition and behaviour.\n\nA neurocognitive deficit is a reduction or impairment of cognitive function in one of these areas, but particularly when physical changes can be seen to have occurred in the brain, such as after neurological illness, mental illness, drug use, or brain injury.\n\nA clinical neuropsychologist may specialise in using neuropsychological tests to detect and understand such deficits, and may be involved in the rehabilitation of an affected person. The discipline that studies neurocognitive deficits to infer normal psychological function is called cognitive neuropsychology.\n\n"}
{"id": "6878878", "url": "https://en.wikipedia.org/wiki?curid=6878878", "title": "OSCE Representative on Freedom of the Media", "text": "OSCE Representative on Freedom of the Media\n\nThe OSCE Representative on Freedom of the Media functions as a watchdog on media developments in all 57 participating member states of the Organization for Security and Co-operation in Europe (OSCE). The representative provides early warning on violations of freedom of expression and promotes full compliance with OSCE principles and commitments regarding freedom of expression and press freedom.\n\nIn cases where serious violations have occurred, the Representative seeks direct contacts with the offending state and other parties involved, assesses the facts and assists in resolving problems. The Representative collects and receives information on the situation of the media from a variety of sources, including participating OSCE States, non-governmental organizations and media organizations. The Representative meets with member Governments.\n\nThe office of the Representative is based in Vienna, Austria, and has a staff of 15.\n\nEach year, they issue a joint declaration calling attention to worldwide free expression concerns.\n\nThe OSCE Representative is one of the four International Mechanisms for Promoting Freedom of Expression. The others are:\n\n\n"}
{"id": "56691018", "url": "https://en.wikipedia.org/wiki?curid=56691018", "title": "Online hate speech", "text": "Online hate speech\n\nOnline hate speech is a type of speech that takes place online (e.g. the Internet, social media platforms) with the purpose to attack a person or a group on the basis of attributes such as race, religion, ethnic origin, sexual orientation, disability, or gender.\n\nHate speech online is situated at the intersection of multiple tensions: it is the expression of conflicts between different groups within and across societies; it is a vivid example of how technologies with a transformative potential such as the Internet bring with them both opportunities and challenges; and it implies complex balancing between fundamental rights and principles, including freedom of expression and the defense of human dignity.\n\nHate speech is a broad and contested term. Multilateral treaties such as the International Covenant on Civil and Political Rights (ICCPR) have sought to define its contours. Multi-stakeholders processes (e.g. the Rabat Plan of Action) have been initiated to bring greater clarity and suggest mechanisms to identify hateful messages. And yet, hate speech continues largely to be used in everyday discourse as a generic term, mixing concrete threats to individuals' and groups' security with cases in which people may be simply venting their anger against authority. Internet intermediaries—organizations that mediate online communication such as Facebook, Twitter, and Google—have advanced their own definitions of hate speech that bind users to a set of rules and allow companies to limit certain forms of expression. National and regional bodies have sought to promote understandings of the term that are more rooted in local traditions.\n\nThe Internet's speed and reach makes it difficult for governments to enforce national legislation in the virtual world. Issues around hate speech online bring into clear relief the emergence of private spaces for expression that serve a public function (e.g. Facebook, Twitter), and the challenges that these spaces pose for regulators. Some of the companies owning these spaces have become more responsive towards tackling the problem of hate speech online.\n\nThe character of hate speech online and its relation to offline speech and action are widely talked about—by politicians, activists and academics—but the debates tend to be removed from systematic empirical evidence. The character of perceived hate speech and its possible consequences has led to placing much emphasis on the solutions to the problem and on how they should be grounded in international human rights norms. Yet this very focus has also limited deeper attempts to understand the causes underlying the phenomenon and the dynamics through which certain types of content emerge, diffuse and lead—or not—to actual discrimination, hostility or violence.\n\nHate speech lies in a complex nexus with freedom of expression, individual, group and minority rights, as well as concepts of dignity, liberty and equality. Its definition is often contested.\n\nIn national and international legislation, hate speech refers to expressions that advocate incitement to harm (particularly, discrimination, hostility or violence) based upon the target's being identified with a certain social or demographic group. It may include, but is not limited to, speech that advocates, threatens, or encourages violent acts. The concept may extend also to expressions that foster a climate of prejudice and intolerance on the assumption that this may fuel targeted discrimination, hostility and violent attacks. At critical times, such as during elections, the concept of hate speech may be prone to manipulation: accusations of fomenting hate speech may be traded among political opponents or used by those in power to curb dissent and criticism. Hate speech (be it conveyed through text, images or sound) can be identified by approximation through the degrading or dehumanizing functions that it serves. There may be two types of messages. The first is to the targeted group and functions to dehumanize and diminish members assigned to this group. It often sounds more or less like:\n\n\"Don't be fooled into thinking you are welcome here. [...] You are not wanted, and you and your families will be shunned, excluded, beaten, and driven out, whenever we can get away with it. We may have to keep a low profile right now. But don't get too comfortable. [...] Be afraid.\"\n\nAnother function of hate speech is to let others with similar views know they are not alone, to reinforce a sense of an in-group that is (purportedly) under threat. A typical message sent this time to like-minded individuals can read like:\n\n\"We know some of you agree that these people are not wanted here. We know that some of you feel that they are dirty (or dangerous or criminal or terrorist). Know now that you are not alone. [...] There are enough of us around to make sure these people are not welcome. There are enough of us around to draw attention to what these people are really like\".\n\nHate speech relies on tensions, which it seeks to re-produce and amplify. Such speech unites and divides at the same time. It creates \"us\" and \"them\".\n\nThe proliferation of hate speech online, observed by the UN Human Rights Council Special Rapporteur on Minority Issues (HRC, 2015), poses a new set of challenges. Both social networking platforms and organizations created to combat hate speech have recognized that hateful messages disseminated online are increasingly common and have elicited unprecedented attention to develop adequate responses. According to HateBase, a web-based application that collects instances of hate speech online worldwide, the majority of cases of hate speech target individuals based on ethnicity and nationality, but incitements to hatred focusing on religion and class have also been on the rise.\n\nWhile hate speech online is not intrinsically different from similar expressions found offline, there are peculiar challenges unique to online content and its regulation. Those challenges related to its permanence, itinerancy, anonymity and cross-jurisdictional character are among the most complex to address.\n\nHate speech can stay online for a long time in different formats across multiple platforms, which can be linked repeatedly. As Andre Oboler, the CEO of the Online Hate Prevention Institute, has noted, \"The longer the content stays available, the more damage it can inflict on the victims and empower the perpetrators. If you remove the content at an early stage you can limit the exposure. This is just like cleaning litter, it doesn't stop people from littering but if you do not take care of the problem it just piles up and further exacerbates.\" Twitter's conversations organized around trending topics may facilitate the quick and wide spreading of hateful messages, but they also offer the opportunity for influential speakers to shun messages and possibly end popular threads inciting violence. Facebook, on the contrary, may allow multiple threads to continue in parallel and go unnoticed; creating longer lasting spaces where certain individuals and groups are offended, ridiculed and discriminated.\n\nHate speech online can be itinerant. Even when content is removed, it may find expression elsewhere, possibly on the same platform under a different name or on different online spaces. If a website is shut down, it can quickly reopen using a web-hosting service with less stringent regulations or via the reallocation to a country with laws imposing higher threshold for hate speech. The itinerant nature of hate speech also means that poorly formulated thoughts that would have not found public expression and support in the past may now land on spaces where they can be visible to large audiences.\n\nAnonymity can also present a challenge to dealing with hate speech online. \"(T)he internet facilitates anonymous and pseudonymous discourse, which can just as easily accelerate destructive behavior as it can fuel public discourse\". As Drew Boyd, Director of Operations at The Sentinel Project, has stated, \"the Internet grants individuals the ability to say horrific things because they think they will not be discovered. This is what makes online hate speech so unique, because people feel much more comfortable speaking hate as opposed to real life when they have to deal with the consequences of what they say\". Some governments and social media platforms have sought to enforce real name policies. Such measures have been deeply contested as they hit at the right to privacy and its intersection with free expression. The majority of online trolling and hate speech attacks come from pseudonymous accounts, which are not necessarily anonymous to everyone. Genuinely anonymous online communications are rare, as they require the user to employ highly technical measures to ensure that he or she cannot be easily identifiable.\n\nA further complication is the transnational reach of the Internet, raising issues of cross jurisdictional co-operation in regard to legal mechanisms for combating hate speech. While there are Mutual Legal Assistance treaties in place amongst many countries, these are characteristically slow to work. The transnational reach of many private-sector Internet intermediaries may provide a more effective channel for resolving issues in some cases, although these bodies are also often impacted upon by cross-jurisdictional appeals for data (such as revealing the identity of the author of a particular content).\n\nUnlike the dissemination of hate speech through conventional channels, hate speech dissemination online often involves multiple actors, whether knowingly or not. When perpetrators makes use of an online social platform to disseminate their hateful message they do not only hurt their victims, but may also violate terms of service in that platform and at times even state law, depending on their location. The victims, on their part, may feel helpless in the face of online harassment, not knowing to whom they should turn to for help. In the types of responses mapped throughout the study, it appears that collective action, usually undertaken by nongovernmental organizations and lobby groups, has been an effective modus operandi to raise awareness and encourage different stakeholders to take action.\n\nIn the aftermath of 2014's dramatic incidents, calls for more restrictive or intrusive measures to contain the Internet's potential to spread hate and violence are common, as if the links between online and offline violence were well known. On the contrary, as the following example indicates, appearances may often be deceiving. Stormfront is considered the first \"hate website\". Launched in March 1995 by a former Ku Klux Klan leader, it quickly became a popular space for discussing ideas related to Neo-Nazism, White nationalism and White separatism, first in the United States of America and then globally. The forum hosts calls for a racial holy war and incitement to use violence to resist immigration and is considered a space for recruiting activists and possibly coordinating violent acts . The few studies that have explored who the users of Stormfront actually are depict a more complex picture. Rather than seeing it as a space for coordinating actions, well-known extreme right activists have accused the forum to be just a gathering for \"keyboard warriors\". One of them for example, as reported by De Koster and Houtman, stated, \"I have read quite a few pieces around the forum, and it strikes me that a great fuss is made, whereas little happens. The section activism/politics itself is plainly ridiculous. [...] Not to mention the assemblies where just four people turn up\". Even more revealing are some of the responses to these accusations provided by regular members of the website. As one of them argued, \"Surely, I am entitled to have an opinion without actively carrying it out. [...] I do not attend demonstrations and I neither join a political party. If this makes me a keyboard warrior, that is all right. I feel good this way. [...] I am not ashamed of it\". De Koster and Houtman surveyed only one national chapter of Stormfront and a non-representative sample of users, but answers like those above should at least invite to caution towards hypotheses connecting expressions and actions, even in spaces whose main function is to host extremist views.\n\nHate speech is not explicitly mentioned in many international human rights documents and treaties, but it is indirectly called upon by some of the principles related to human dignity and freedom of expression. For example, the 1948 Universal Declaration of Human Rights (UDHR), which was drafted as a response to the atrocities of the World War II, contains the right to equal protection under the law in Article 7, which proclaims that: \"All are entitled to equal protection against any discrimination in violation of this Declaration and against any incitement to such discrimination\". The UDHR also states that everyone has the right to freedom of expression, which includes \"freedom to hold opinions without interference and to seek, receive and impart information and ideas through any media and regardless of frontiers\".\n\nThe UDHR was decisive in setting a framework and agenda for human rights protection, but the Declaration is non-binding. A series of binding documents have been subsequently created to offer a more robust protection for freedom of expression and protection against discrimination. Out of those documents, the International Covenant on Civil and Political Rights (ICCPR) is the most important and comprehensive when addressing hate speech and contains the right to freedom of expression in Article 19 and the prohibition of advocacy to hatred that constitutes incitement to discrimination, hostility or violence in Article 20. Other more tailored international legal instruments contain provisions that have repercussions for the definition of hate speech and identification of responses to it, such as: the Convention on the Prevention and Punishment of the Crime of Genocide (1951), the International Convention on the Elimination of All Forms of Racial Discrimination, ICERD (1969), and, to a lesser extent, the Convention on the Elimination of All Forms of Discrimination against Women, CEDAW (1981).\n\nThe ICCPR is the legal instrument most commonly referred to in debates on hate speech and its regulation, although it does not explicitly use the term \"hate speech\". Article 19, which is often referred to as part of the \"core of the Covenant\", provides for the right to freedom of expression. This sets out the right, and it also includes general strictures to which any limitation of the right must conform in order to be legitimate. Article 19 is followed by Article 20 that expressly limits freedom of expression in cases of \"advocacy of national, racial or religious hatred that constitutes incitement to discrimination, hostility or violence\". The decision to include this provision, which can be characterised as embodying a particular conceptualisation of hate speech, has been deeply contested. The Human Rights Committee, the United Nations body created by the ICCPR to oversee its implementation, cognizant of the tension, has sought to stress that Article 20 is fully compatible with the right to freedom of expression. In the ICCPR, the right to freedom of expression is not an absolute right. It can legitimately be limited by states under restricted circumstances:\n\n\"3. The exercise of the rights provided for in paragraph 2 of this article carries with it special duties and responsibilities. It may therefore be subject to certain restrictions, but these shall only be such as are provided by law and are necessary: (a) For respect of the rights or reputations of others; (b) For the protection of national security or of public order (ordre public), or of public health or morals.\"\n\nBetween Article 19 (3) and Article 20, there is a distinction between optional and obligatory limitations to the right to freedom of expression. Article 19 (3) states that limitations on freedom of expression \"may therefore be subject to certain restrictions\", as long as they are provided by law and necessary to certain legitimate purposes. Article 20 states that any advocacy of (certain kinds of) hatred that constitutes incitement to discrimination, hostility or violence \"shall be prohibited by law\". Despite indications on the gravity of speech offenses that should be prohibited by law under Article 20, there remains complexity. In particular there is a grey area in conceptualising clear distinctions between (i) expressions of hatred, (ii) expression that advocate hatred, and (iii) hateful speech that specifically constitutes incitement to the practical harms of discrimination, hostility or violence. While states have an obligation to prohibit speech conceived as \"advocacy to hatred that constitutes incitement to discrimination, hostility or violence\", as consistent with Article 20 (2), how to interpret such is not clearly defined.\n\nThe International Convention on the Elimination of All Forms of Racial Discrimination (ICERD), which came into force in 1969, has also implications for conceptualising forms of hate speech. The ICERD differs from the ICCPR in three respects. Its conceptualisation of hate speech is specifically limited to speech that refers to race and ethnicity. It asserts in Article 4, paragraph (a), that state parties:\n\n\"Shall declare as an offence punishable by law all dissemination of ideas based on racial superiority or hatred, incitement to racial discrimination, as well as all acts of violence or incitement to such acts against any race or group of persons of another color or ethnic origin, and also the provision of any assistance to racist activities, including the financing thereof; This obligation imposed by the ICERD on state parties is also stricter than the case of Article 20 of the ICCPR covering the criminalisation of racist ideas that are not necessarily inciting discrimination, hostility or violence.\"\n\nAn important difference is in the issue of intent. The concept of \"advocacy of hatred\" introduced in the ICCPR is more specific than discriminatory speech described in the ICERD, since it is taken to require consideration of the intent of author and not the expression in isolation—this is because \"advocacy\" is interpreted in the ICCPR as requiring the intent to sow hatred. The Committee on the Elimination of Racial Discrimination has actively addressed hate speech in its General Recommendation 29, in which the Committee recommends state parties to:\n\n\"(r) Take measures against any dissemination of ideas of caste superiority and inferiority or which attempt to justify violence, hatred or discrimination against descent-based communities; (s) Take strict measures against any incitement to discrimination or violence against the communities, including through the Internet; (t) Take measures to raise awareness among media professionals of the nature and incidence of descent-based discrimination;\"\n\nThese points, which reflect the ICERD's reference to the dissemination of expression, have significance for the Internet. The expression of ideas in some online contexts may immediately amount to spreading them. This is especially relevant for private spaces that have begun to play a public role, as in the case of many social networking platforms.\n\nSimilarly to the ICERD, the Genocide Convention aims to protect groups defined by race, nationality or ethnicity, although it also extends its provisions to religious groups. When it comes to hate speech the Genocide Convention is limited only to acts that publicly incite to genocide, recognized as \"acts committed with intent to destroy, in whole or in part, a national, ethnical, racial or religious group\", regardless of whether such acts are undertaken in peacetime or in wartime. Specifically gender-based hate speech (as distinct from discriminatory actions) is not covered in depth in international law.\n\nThe Convention on the Elimination of All Forms of Discrimination against Women (CEDAW), which entered into force in 1981, imposes obligations on states to condemn discrimination against women and \"prevent, investigate, prosecute and punish\" acts of gender-based violence.\n\nMost regional instruments do not have specific articles prescribing prohibition of hate speech, but they more generally allow states to limit freedom of expression—which provisions can be applied to specific cases.\n\nThe American Convention on Human Rights describes limitations on freedom of expression in a manner similar to the ICCPR in Article 19 (3). The Organization of American States has also adopted another declaration on the principles of freedom of expression, which includes a specific clause stating that \"prior conditioning of expressions, such as truthfulness, timeliness or impartiality is incompatible with the right to freedom of expression recognized in international instruments\". The Inter-American Court has advised that \"(a)buse of freedom of information thus cannot be controlled by preventive measures but only through the subsequent imposition of sanctions on those who are guilty of the abuses\". The Court also imposes a test for States willing to enact restrictions on freedom of expression, as they need to observe the following requirements: \"a) the existence of previously established grounds for liability; b) the express and precise definition of these grounds by law; c) the legitimacy of the ends sought to be achieved; d) a showing that these grounds of liability are ‘necessary to ensure' the aforementioned ends.\" The Inter-American System has a Special Rapporteur on Freedom of Expression who conducted a comprehensive study on hate speech. His conclusion was that the Inter-American Human Rights System differs from the United Nations and the European approach on a key point: The Inter-American system covers only hate speech that actually leads to violence, and solely such speech can be restricted.\n\nThe African Charter on Human Rights and Peoples' Rights takes a different approach in Article 9 (2), allowing for restrictions on rights as long as they are \"within the law\". This concept has been criticized and there is a vast amount of legal scholarship on the so-called \"claw-back\" clauses and their interpretation. The criticism is mainly aimed at the fact that countries can manipulate their own legislation and weaken the essence of the right to freedom of expression. The Declaration of Principles on Freedom of Expression in Africa elaborates a higher standard for limitations on freedom of expression. It declares that the right \"should not be restricted on public order or national security grounds unless there is a real risk of harm to a legitimate interest and there is a close causal link between the risk of harm and the expression\".\n\nIn 1990, the Organization of the Islamic Conference (which was later renamed Organization of Islamic Cooperation, OIC) adopted the Cairo Declaration on Human Rights in Islam (CDHRI), which calls for criminalisation of speech that extends beyond cases of imminent violence to encompass \"acts or speech that denote manifest intolerance and hate\".\n\nThe Arab Charter on Human Rights, which was adopted by the Council of the League of Arab States in 2004, includes in Article 32 provisions that are relevant also for online communication as it guarantees the right to \"freedom of opinion and expression, and the right to seek, receive and impart information and ideas through any medium, regardless of geographical boundaries\". It allows a limitation on a broad basis in paragraph 2 \"Such rights and freedoms shall be exercised in conformity with the fundamental values of society\".\n\nThe ASEAN Human Rights Declaration includes the right to freedom of expression in Article 23. Article 7 of the Declaration provides for general limitations, affirming, \"the realisation of human rights must be considered in the regional and national context bearing in mind different political, economic, legal, social, cultural, historical and religious backgrounds.\"\n\nThe Charter of Fundamental Rights of the European Union which declares the right to freedom of expression in Article 11, has a clause which prohibits abuse of rights. It asserts that the Charter must not be interpreted as implying any \"limitation to a greater extent than is provided for therein\". An example of a limitation which implies a strict test of necessity and proportionality is the provision on freedom of expression in the European Convention on Human Rights, which underlines that the exercise of freedom of expression carries duties and responsibilities. It \"may be subject to such formalities, conditions, restrictions or penalties as are prescribed by law and are necessary in a democratic society, in the interests of national security, territorial integrity or public safety, for the prevention of disorder or crime, for the protection of health or morals, for the protection of the reputation or rights of others, for preventing the disclosure of information received in confidence, or for maintaining the authority and impartiality of the judiciary\".\n\nThe European Court of Human Rights is careful to distinguish between hate speech and the right of individuals to express their views freely, even if others take offence. There are regional instances relevant specifically to online hate speech. The Council of Europe (CoE) in 2000 issued a General Policy Recommendation on Combating the Dissemination of Racist, Xenophobic and Anti-Semitic Material via the Internet. The creation of the CoE Convention on Cybercrime in 2001, which regulates mutual assistance regarding investigative powers, provides signatory countries with a mechanism to deal with computer data, which would include transnational hate speech online. In 2003 the CoE launched an additional protocol to the Convention on Cybercrime which addresses online expression of racism and xenophobia. The convention and its protocol were opened for signature and ratification of countries outside Europe, and other countries, such as Canada and South Africa, are already part of this convention. The Protocol imposes an obligation on Member States to criminalise racist and xenophobic insults online of \"(i) persons for the reason that they belong to a group distinguished by race, color, descent or national or ethnic origin, as well as religion, if used as a pretext for any of these factors; or (ii) a group of persons which is distinguished by any of these characteristics\".\n\nInternet intermediaries such as social networking platforms, Internet Service Providers or Search Engines, stipulate in their terms of service how they may intervene in allowing, restricting, or channelling the creation and access to specific content. A vast amount of online interactions occur on social networking platforms that transcend national jurisdictions and which platforms have also developed their own definitions of hate speech and measures to respond to it. For a user who violates the terms of service, the content he or she has posted may be removed from the platform, or its access may be restricted to be viewed only by a certain category of users (e.g. users living outside a specific country).\n\nThe principles that inspire terms of service agreements and the mechanisms that each company develops to ensure their implementation have significant repercussions on the ability that people have to express themselves online as well as to be protected from hate speech. Most intermediaries have to enter in negotiations with national governments to an extent that varies according to the type of intermediary, areas where the company is registered, and the legal regime that applies. As Tsesis explains, \"(i)f transmissions on the Internet are sent and received in particular locations, then specific fora retain jurisdiction to prosecute illegal activities transacted on the Internet\". Internet Service Providers are the most directly affected by national legislation because they have to be located in a specific country to operate. Search Engines, while they can modify search results for self-regulatory or commercial reasons, have increasingly tended to adapt to the intermediary liability regime of both their registered home jurisdictions and other jurisdictions in which they provide their services, either removing links to content proactively or upon request by authorities.\n\nAll Internet intermediaries operated by private companies are also expected to respect human rights. This is set out in the Guiding Principles on Business and Human Rights elaborated by the United Nations Office of the High Commissioner for Human Rights. The document emphasizes corporate responsibility in upholding human rights. In principle 11, it declares that: \"Business enterprises should respect human rights. This means that they should avoid infringing on the human rights of others and should address adverse human rights impacts with which they are involved\". The United Nations Guiding Principles also indicate that in cases in which human rights are violated, companies should \"provide for or cooperate in their remediation through legitimate processes\". In the case of Internet intermediaries and conceptions of hate speech, this means that they should ensure that measures are in place to provide a commensurate response.\n\nMyanmar is transitioning towards greater openness and access to the Internet has grown at unprecedented rates. In this context, however, social media have often been used by some to spread calls to violence. In 2014, the UN Human Rights Council Special Rapporteur on Minority Issues expressed her concern over the spread of misinformation, hate speech and incitement to violence, discrimination and hostility in the media and Internet, particularly targeted against a minority community. The growing tension online has gone parallel with cases of actual violence leaving hundreds dead and thousands displaced. One challenge in this process has concerned ethnic and religious minorities. In 2013, 43 people were killed due to clashes that erupted after a dispute in the Rakhine state in the Western Part of the country. A year earlier, more than 200 people were killed and thousands displaced 37 because of ethnic violence, which erupted after an alleged rape case. Against this backdrop, the rapid emergence of new online spaces, albeit for a fraction of the population, has reflected some of these deeply rooted tensions in a new form. \n\nDealing with intolerance and hate speech online is an emerging issue. Facebook has rapidly become the platform of choice for those citizens making their first steps online. In this environment there have been individual and groups, which have championed a more aggressive use of the medium, especially when feeling protected by a sense of righteousness and by claims to be acting in defense of the national interest. Political figures have also used online media for particular causes. In social media, there has been the use of derogatory terms in reference to minorities. In this complex situation, a variety of actors has begun to mobilize, seeking to offer responses that can avoid further violence. Facebook has sought to take a more active role in monitoring the uses of the social network platform in Myanmar, developing partnerships with local organizations and making guidelines on reporting problems accessible in Burmese.\n\nThe local civil society has constituted a strong voice in openly condemning the spread of online hate speech, but at the same time calling for alternatives to censorship. Among the most innovative responses has been Panzagar, which in Burmese means \"flower speech\", a campaign launched by blogger and activist Nay Phone Latt to openly oppose hate speech. The goal of the initiative was offering a joyful example of how people can interact, both online and offline. Local activists have been focussed upon local solutions, rather than trying to mobilize global civil society on these issues. This is in contrast to some other online campaigns that have been able to attract the world's attention towards relatively neglected problems. Initiatives such as those promoted by the Save Darfur Coalition for the civil war in Sudan, or the organization Invisible Children with the Kony2012 campaign that denounced the atrocities committed by the Lord Resistance Army, are popular examples. As commentaries on these campaigns have pointed out, such global responses may have negative repercussions on the ability for local solutions to be found.\n\nInternet intermediaries have developed disparate definitions of hate speech and guidelines to regulate it. Some companies do not use the term hate speech, but have a descriptive list of terms related to it.\n\nYahoo!'s terms of service prohibit the posting of \"content that is unlawful, harmful, threatening, abusive, harassing, tortuous, defamatory, vulgar, obscene, libellous, invasive of another's privacy, hateful, or racially, ethnically or otherwise objectionable\".\n\nIn December of 2017, Twitter began enforcing new policies towards hate speech, banning multiple accounts as well as setting new guidelines for what will be allowed on their platform. There is an entire page in the Twitter Help Center devoted to describing their Hateful Conduct Policy, as well as their enforcement procedures. The top of this page states “Freedom of expression means little if voices are silenced because people are afraid to speak up. We do not tolerate behavior that harasses, intimidates, or uses fear to silence another person’s voice. If you see something on Twitter that violates these rules, please report it to us.” Twitter’s definition of hate speech ranges from “violent threats” and “wishes for the physical harm, death, or disease of individuals or groups” to “repeated and/or non-consensual slurs, epithets, racist and sexist tropes, or other content that degrades someone.”\n\nPunishments for violations range from suspending a user’s ability to tweet until they take down their offensive/ hateful post to the removal of an account entirely. In a statement following the implementation of their new policies, Twitter said “In our efforts to be more aggressive here, we may make some mistakes and are working on a robust appeals process” . . . “We’ll evaluate and iterate on these changes in the coming days and weeks, and will keep you posted on progress along the way”. These changes come amidst a time where action is being taken to prevent hate speech around the globe, including new laws in Europe which pose fines for sites unable to address hate speech reports within 24 hours.\n\nYouTube, a subsidiary of the tech company Google, has outlined a clear “Hate Speech Policy” amidst several other user policies on their website. The policy is worded as such: “We encourage free speech and try to defend your right to express unpopular points of view, but we don't permit hate speech. Hate speech refers to content that promotes violence against or has the primary purpose of inciting hatred against individuals or groups based on certain attributes, such as: race or ethnic origin, religion, disability, gender, age, veteran status, sexual orientation/gender identity”. YouTube has built in a user reporting system in order to counteract the growing trend of hate speech. Among the most popular deterrents against hate speech, users are able to anonymously report another user for content they deem inappropriate. The content is then reviewed against YouTube policy and age restrictions, and either taken down or left alone.\n\nFacebook's terms forbid content that is harmful, threatening or which has potential to stir hatred and incite violence. In its community standards, Facebook elaborates that \"Facebook removes hate speech, which includes content that directly attacks people based on their: race, ethnicity, national origin, religious affiliation, sexual orientation, sex, gender or gender identity, or serious disabilities or diseases\". It further states that \"We allow humour, satire or social commentary related to these topics, and we believe that when people use their authentic identity, they are more responsible when they share this kind of commentary. For that reason, we ask that Page owners associate their name and Facebook Profile with any content that is insensitive, even if that content does not violate our policies. As always, we urge people to be conscious of their audience when sharing this type of content.\"\nFacebook’s hate speech policies are enforced by 7,500 content reviewers. Because this requires difficult decision making, controversy arises among content reviewers over enforcement of policies. Some users seem to feel as though the enforcement is inconsistent. One apt past example is two separate but similarly graphic postings that wished death to members of a specific religion. Both post were flagged by users and reviewed by Facebook staff. However only one was removed even though they carried almost identical sentiments. In a quote regarding hate speech on the platform, Facebook Vice President of Global Operations, Justin Osofky stated, “We’re sorry for the mistakes we have made — they do not reflect the community we want to help build…We must do better.\" \nThere has been additional controversy due to the specificity of Facebook's hate speech policies. On many occasions there have been reports of status updates and comments that users feel are insensitive and convey hatred. However these posts do not technically breach any Facebook policies because their speech does not attack others based on the company's list of protected classes. For example, the statement “Female sports reporters need to be hit in the head with hockey pucks,\" would not be considered hate speech on Facebook’s platform and therefore would not be removed. While the company protects against gender based hatred, it does not protect against hatred based on occupation.\nFacebook also tries to accommodate users who promote other hate speech content with the intent of criticizing it. In these cases, users are required make it clear that their intention is to educate others. If this intention is unclear then Facebook reserves the right to censor the content. When Facebook initially flags content that may contain hate speech, they then designate it to a Tier 1, 2, and 3 scale, based on the content’s severity. Tier 1 is the most severe and Tier 3 is the least. Tier 1 includes anything that conveys “violent speech or support for death/disease/harm.” Tier 2 is classified as content that slanders another user's image mentally, physically, or morally. Tier 3 includes anything that can potentially exclude or discriminate against others, or that uses slurs about protected groups, but does not necessarily apply to arguments to restrict immigration or criticism of existing immigration policies.\n\nMicrosoft has specific rules concerning hate speech for a variety of its applications. Its policy for mobile phones prohibits applications that \"contain any content that advocates discrimination, hatred, or violence based on considerations of race, ethnicity, national origin, language, gender, age, disability, religion, sexual orientation, status as a veteran, or membership in any other social group.\" The company has also rules regarding online gaming, which prohibit any communication that is indicative of \"hate speech, controversial religious topics and sensitive current or historical events\".\n\nMedia and Information Literacy aims to help people to engage in a digital society by being able to use, understand, inquire, create, communicate and think critically; while being able to effectively access, organize, analyze, evaluate, and create messages in a variety of forms.\n\nCitizenship education focuses on preparing individuals to be informed and responsible citizens through the study of rights, freedoms, and responsibilities and has been variously employed in societies emerging from violent conflict. One of its main objectives is raising awareness on the political, social and cultural rights of individuals and groups, including freedom of speech and the responsibilities and social implications that emerge from it. The concern of citizenship education with hate speech is twofold: it encompasses the knowledge and skills to identify hate speech, and should enable individuals to counteract messages of hatred. One of its current challenges is adapting its goals and strategies to the digital world, providing not only argumentative but also technological knowledge and skills that a citizen may need to counteract online hate speech. \n\nInformation literacy cannot avoid issues such as rights to free expression and privacy, critical citizenship and fostering empowerment for political participation. Multiple and complementary literacies become critical. The emergence of new technologies and social media has played an important role in this shift. Individuals have evolved from being only consumers of media messages to producers, creators and curator of information, resulting in new models of participation that interact with traditional ones, like voting or joining a political party. Teaching strategies are changing accordingly, from fostering critical reception of media messages to include empowering the creation of media content.\n\nThe concept of media and information literacy itself continues to evolve, being augmented by the dynamics of the Internet. It is beginning to embrace issues of identity, ethics and rights in cyberspace. Some of these skills can be particularly important when identifying and responding to hate speech online.\n\nSeries of initiatives aimed both at providing information and practical tools for Internet users to be active digital citizens:\nEducation is also seen as being a tool against hate speech. Laura Geraghty from the ‘No Hate Speech Movement' affirmed: \"Education is key to prevent hate speech online. It is necessary to raise awareness and empower people to get online in a responsible way; however, you still need the legal background and instruments to prosecute hate crimes, including hate speech online, otherwise the preventive aspect won't help\".\n"}
{"id": "35198686", "url": "https://en.wikipedia.org/wiki?curid=35198686", "title": "Paolo Cirio", "text": "Paolo Cirio\n\nPaolo Cirio is a conceptual artist, hacktivist and cultural critic.\n\nCirio's work embodies hacker ethics, such as open access, privacy policies, and disrupting economic, legal, and political models. He received a number of legal threats for his Internet art performances with practices such as hacking, piracy, leaking sensitive information, identity theft, and cyber attacks.\n\nPaolo Cirio is known for having exposed over 200,000 Cayman Islands offshore firms with the work \"Loophole for All\" in 2013; the hacking of Facebook through publishing 1 million users on a dating website with \"Face to Facebook\" in 2011; the theft of 60000 financial news articles with \"Daily Paywall\" in 2014 and of e-books from Amazon.com with \"Amazon Noir\" in 2006; defrauding Google with \"GWEI\" in 2005; and the obfuscation of 15 million U.S. criminal records with \"Obscurity\" in 2016. His early works include his cyber attacks against NATO and reporting on its military operations since 2001. \n\nPaolo Cirio has won a number of awards, including Golden Nica first prize at Ars Electronica in 2013; the Eyebeam Fellowship in 2012; and Transmediale second prize 2008, among others.\n\nCirio has exhibited in international museums and institutions, he regularly gives public lectures and workshops at leading universities, and his artworks have been covered by hundreds of media outlets worldwide.\n\nIn 2002, Cirio's first international action was called Anti-NATO Day. As an act of Hacktivism he staged a virtual sit-in (DDoS attack) in the NATO website through a Flash Player script. The Canadian Department of National Defence investigated the action and the Eisenhower Institute used it as a case study to identify future vulnerabilities in space security. Cirio promoted the action through an anti-war web portal called StopTheNato.org, which he launched in 2001 and updated periodically until 2006.\n\nIn 2004, Cirio joined the Illegal Art Show network, which organized street art happenings in Italy in line with the Temporary Autonomous Zone-philosophy. They occupied public spaces and invited artists to create artwork. Cirio created several street art pieces and organized three such events independently: two in Turin in 2004 and a third in London in 2005.\n\nIn 2005, Cirio hacked Google's AdSense service by creating internet bots for a click fraud in order to buy Google's shares with its own money. In an attempt to stop the project, Google sent a cease and desist letter to the artists mentioning legal consequences for the project. Cirio worked on the project \"Google Will Eat Itself\" \"(GWEI)\" in conjunction with Alessandro Ludovico and Ubermorgen. The project questioned the information monopoly of Google and its revenue model. \n\nIn 2006 he eluded the protection from Amazon.com with internet bots using the front door of the \"search inside\" service. He scraped complete texts of books, reassembled them into PDF files, and redistributed them for free. The company refused to comment on the action. In collaboration with Alessandro Ludovico and Ubermorgen, Cirio created the project \"Amazon Noir\" to criticize the abuse of copyright laws for the protection of digital content.\n\n\"Face to Facebook\", \"Amazon Noir\" and \"Google Will Eat Itself\" together form the \"Hacking Monopolism Trilogy\".\n\nBetween 2008 and 2010, Cirio worked on experimental storytelling which involves actors and audiences for presenting real facts and issues through fictional stories across multiple media platforms. He called this technique of documentary fiction \"Recombinant Fiction.\" This socially engaged genre of transmedia storytelling has resulted in two projects: \"Drowning NYC\" (2010) and \"The Big Plot\" (2009).\n\nIn 2010, in reaction to the late-2000s financial crisis, Cirio created the piece \"P2P Gift Credit Card - Gift Finance\". Cirio issued thousands of illicit VISA credit cards in order to design a creative monetary policy, named \"Gift Finance\", which is a participatory and interest-free basic income guarantee system. In the following years, he presented the projects in relation to the economic recession and the related Occupy Wall Street protests.\n\nSince 2011, Cirio has been addressing the cultural shift and mainstream media attention toward popular perceptions of privacy and ownership of public and personal information, with the projects \"Street Ghosts\", \"Persecuting.US\", and \"Face to Facebook\". The methodology used to create these artworks was eventually formalized in a series called \"Anti-Social Sculptures\".\n\nIn 2011, Cirio created \"Face to Facebook\" with Alessandro Ludovico. For this piece, Cirio scraped one million Facebook profiles, filtered them with artificial intelligence for facial recognition software, and published 250,000 of them onto Lovely-Faces.com, a mock dating website designed by Cirio, with the profiles sorted according to facial expressions. This resulted in eleven lawsuit threats, five death threats, and four legal letters from Facebook. Within a few days, the project was covered by over a thousand media outlets from around the world including CNN, Fox News, Tagesschau, and Apple Daily.\n\nWith the \"Street Ghosts\" project in 2012, Cirio recontextualized photos of individuals found on Google Street View, by printing and posting life-sized pictures of people in the exact locations where they were photographed. The posters were wheatpasted on the walls of public buildings without authorization. These interventions took place in public spaces of several major cities, including London, Berlin, and New York. \n\nIn 2012, his web project \"Persecuting.US\" profiled the political affiliations of over one million Americans who used Twitter during the months leading up to the United States presidential election of 2012. Cirio appropriated the data and algorithmically determined users’ political affiliations to raise awareness on voter profiling and polarization in social bubbles that can be targeted for political manipulation.\n\nIn 2013, Cirio investigated offshore financial systems with the project \"Loophole for All\". The project made public the list of all the companies registered in the Cayman Islands for the first time, exposing tax evasion practices by counterfeiting Certificate of Incorporation documents signed with his name. This information was published on the website Loophole4All.com, engaging international participation through selling the real identities of anonymous Cayman companies for 99 cents. This provocation elicited reactions from Cayman authorities and global banks as well as legal threats by multinational companies, international law firms, and local Cayman businesses. After three weeks of selling conceptual and subversive artworks in the form of limited editions of firms’ identities, PayPal suspended the account, claiming the sales activity was in violation of PayPal's Acceptable Use Policy.. In 2014 the project won the Golden Nica, first prize of Prix Ars Electronica.\n\nIn 2014, Cirio created the \"Global Direct\" project, a creative political philosophy that the artist outlined for worldwide participatory democracy within the potentials offered by the Internet. To illustrate the conceptual work, the artist drew a series of fifteen Organizational chart to inspire values and functions for a global and participatory society. The fifteen diagrams of \"Global Direct\" were informed by the artist’s research into the social science of ancient, contemporary, and emergent democracy.\n\nIn 2014 Cirio created \"Daily Paywall\" by hacking the paywall of The Wall Street Journal, Financial Times, and The Economist. Through his paid subscriptions and Scripting language hack, he obtained over 60,000 news articles published during the course of 2014. The pay-per-view content was republished for free on the website DailyPaywall.com and the artist proposed to pay people to read featured financial news articles. Cirio conceived a provocative sharing economy model, where crowdsource practices were put in place to allow and incentivize people to access information on global economic matters. Using this system, readers were able to earn money for every quiz they correctly completed, and journalists were able to claim compensation for their work. Everyone could donate any amount to crowdfund the system. After a few days, the ISP hosting DailyPaywall.com disabled the site after receiving complaints of international Copyright infringement from Pearson PLC, the largest education and publisher company in the world and owner of the Financial Times and The Economist. Additionally The Wall Street Journal proceeded to terminate the artist’s subscription due to a violation of their Terms of Service. In 2016, Pearson sold both The Economist and the Financial Times and Cirio republished the whole content of DailyPaywall.com. He entire artistic act was pre-scripted as a performance for illustrating critical issues on the Information economy that Cirio outlined within the launch of the project.\n\nDuring the spring of 2015, Cirio conducted the street art campaign \"OVEREXPOSED\" concerning the aftermath of Edward Snowden’s global surveillance disclosures. He disseminated on public walls unauthorized photos of high-ranking U.S. intelligence officials of the NSA, CIA and FBI, who were accountable for political measures or advocacy for mass surveillance and espionage programs. The photos, mostly selfies from Facebook and Twitter accounts of civilians, were rendered with a particular technique called \"High Definition Stencils\" invented by the artist for the street art campaign that took place in NYC, London, Berlin and Paris between April and May 2015. The intervention generated media coverage and public interest internationally and particularly in Germany \n\nIn 2016, Cirio created the project \"Obscurity\" in which he obfuscated over 10 million online mugshots and the criminal records of victims of mass incarceration in United States. The project addressed the unregulated mug shot publishing industry that anonymous internet companies exploit in order to shame and blackmail people who have been arrested in the U.S. regardless of their charges and trial verdicts. Cirio targeted six mugshot websites and blurred millions of mugshots and shuffled names listed. In response he received support from mugshot extortion victims and was subject to a legal threat from Mugshots.com, an anonymous firm in Nevis, and US Data ltd., a Texan firm owning a few mugshot websites. With \"Obscurity\", Cirio questioned the Right to Be Forgotten law, which has been opposed by major search engine companies in the U.S. Ultimately, to point out the accountability of search engines in exposing personal sensitive information, Cirio designed the campaign \"Right2Remove.us\" to introduce a privacy policy adapting the Right to Be Forgotten law to the United States. With the \"Right to Remove\" policy, Cirio suggested types of sensitive personal data that should be removed from online search results to protect specific categories of vulnerable individuals.\n\nSelected awards include:\n\n\n"}
{"id": "47238501", "url": "https://en.wikipedia.org/wiki?curid=47238501", "title": "Pending Meal", "text": "Pending Meal\n\nA Pending Meal is one paid for at a restaurant or other food joint, to be made available to any future visitor who asks for it. The Pending Meal community initiative was started on social media in April 2015 by Sarah Rizvi, focusing on restaurants in Dubai. Once the news of the initiative started to spread, many local restaurants were willing to tie up and provide their support too. The idea spread to other areas including Sharjah, Abu Dhabi, Ras Al Khaimah, U.S.A, Canada, Sri Lanka, India and Pakistan.\n\nThe Pending Meal movement encourages people to pay for someone else’s meal at a food outlet so the less fortunate can walk in and eat. The initiative is not limited to United Arab Emirates only and the founder believes that wherever there is a hungry mouth, there is a Pending Meal to feed. The idea originated from the concept of suspended coffee practiced in some coffee shops in Naples. Customers would pay for their own coffee and when they wanted to, could pay for an extra ‘suspended coffee’ or a 'suspended meal' which was meant for the less fortunate. Thus, a person who was hungry or thirsty could walk into the cafe and ask if there was a suspended coffee or suspended meal that they could drink or eat.\n\nThere is no limit to how much one can donate to the cause. “There have been people who have paid in advance for the meal of just one person and some have even paid for 15,” said Rizvi.\n\nThe Pending Meal initiative is not a charity organization and does not collect funds (in any form). The initiative is purely encouraging people to give back to community by providing less fortunate with a meal so that no one has to stay hungry.\n"}
{"id": "6173781", "url": "https://en.wikipedia.org/wiki?curid=6173781", "title": "Polychotomy", "text": "Polychotomy\n\nA polychotomy (päl′i kät′ə mē; plural \"polychotomies\") is a division or separation into many parts or classes. Polychotomy is a generalization of dichotomy, which is a polychotomy of exactly two parts. In evolutionary biology, the term polychotomy can also be considered a historically based misspelling of polytomy.\n\n\n"}
{"id": "10008223", "url": "https://en.wikipedia.org/wiki?curid=10008223", "title": "Positive behavior support", "text": "Positive behavior support\n\nPositive behavior support (PBS) is a behavior management system used to understand what maintains an individual's challenging behavior. People's inappropriate behaviors are difficult to change because they are functional; they serve a purpose for them. These behaviors are supported by reinforcement in the environment. In the case of students and children, often adults in a child’s environment will reinforce his or her undesired behaviors because the child will receive objects and/or attention because of his behavior. Functional behavior assessments (FBAs) clearly describe behaviors, identify the contexts (events, times, and situation) that predict when behavior will and will not occur, and identify consequences that maintain the behavior. They also summarize and create a hypothesis about the behavior, directly observe the behavior and take data to get a baseline. The positive behavior support process involves goal identification, information gathering, hypothesis development, support plan design, implementation and monitoring.\n\nIn order for techniques to work in decreasing undesired behavior, they should include: feasibility, desirability, and effectiveness. Strategies are needed that teachers and parents are able and willing to use and that affect the child's ability to participate in community and school activities. Positive behavior support is increasingly being recognized as a strategy that meets these criteria. By changing stimulus and reinforcement in the environment and teaching the child to strengthen deficit skill areas the student's behavior changes in ways that allow him/her to be included in the general education setting. The three areas of deficit skills identified in the article were communication skills, social skills, and self-management skills. Re-directive therapy as positive behavior support is especially effective in the parent–child relationship. Where other treatment plans have failed re-directive therapy allows for a positive interaction between parents and children. Positive behavior support is successful in the school setting because it is primarily a teaching method (Swartz, 1999).\n\nSchools are required to conduct functional behavioral assessment (FBA) and use positive behavior support with students who are identified as disabled and are at risk for expulsion, alternative school placement, or more than 10 days of suspension. Even though FBA is required under limited circumstances it is good professional practice to use a problem-solving approach to managing problem behaviors in the school setting (Crone & Horner 2003).\n\nThe use of Positive Behavior Intervention Supports (PBIS) in schools is widespread (Sugai & Horner, 2002) in part because it is a professional skill in early special education programs (as opposed to Rogerian counseling). The program offers a primary, secondary, and tertiary level of intervention. A basic tenet of the PBIS approach includes identifying students in one of three categories based on risk for behavior problems. Once identified, students receive services in one of three categories: primary, secondary, or tertiary. To help practitioners with differences in interventions used at each of the levels the professional literature refers to a three-tiered (levels) model (Stewart, Martella, Marchand-Martella, & Benner, 2005; Sugai, Sprague, Horner & Walker, 2000; Tobin & Sugai, 2005; Walker et al., 1996.) Interventions are specifically developed for each of these levels with the goal of reducing the risk for academic or social failure. These interventions may be behavioral and or academic interventions incorporating scientifically proven forms of instruction such as direct instruction. The interventions become more focused and complex as one examines the strategies used at each level.\n\nPrimary prevention strategies focus on interventions used on a school-wide basis for all students (Sugai & Horner, 2002). PBS (positive behavioral supports) use for other than a designated population group has neither been approved by the professions or the public-at-large.This level of prevention is considered \"primary\" because all students are exposed in the same way, and at the same level, to the intervention. The primary prevention level is the largest by number. Approximately 80–85% of students who are not at risk for behavior problems respond in a positive manner to this prevention level. Primary prevention strategies include, but are not limited to, using effective teaching practices and curricula, explicitly teaching behavior that is acceptable within the school environment, focusing on ecological arrangement and systems within the school, consistent use of precorrection procedures, using active supervision of common areas, and creating reinforcement systems that are used on a school-wide basis (Lewis, Sugai, & Colvin, 1998; Martella & Nelson, 2003; Nelson, Crabtree, Marchand-Martella & Martella, 1998; Nelson, Martella, & Marchand-Martella, 2002).\n\nSecondary prevention strategies involve students (i.e., 10–15% of the school population) who do not respond to the primary prevention strategies and are at risk for academic failure or behavior problems but are not in need of individual support (Nelson, et al., 2002). Interventions at the secondary level often are delivered in small groups to maximize time and effort and should be developed with the unique needs of the students within the group. Examples of these interventions include social support such as social skills training (e.g., explicit instruction in skill-deficit areas, friendship clubs, check in/check out, role playing) or academic support (i.e., use of research-validated intervention programs and tutoring). Additionally, secondary programs could include behavioral support approaches (e.g., simple Functional Behavioral Assessments [FBA], precorrection, self-management training). Even with the heightened support within secondary level interventions, some students (1–7%) will need the additional assistance at the tertiary level (Walker et al., 1996).\n\nTertiary prevention programs focus on students who display persistent patterns of disciplinary problems (Nelson, Benner, Reid, Epstein, & Currin, 2002).\nTertiary-level programs are also called intensive or individualized interventions and are the most comprehensive and complex. The interventions within this level are strength-based in that the complexity and intensity of the intervention plans directly reflect the complexity and intensity of the behaviors. Students within the tertiary level continue involvement in primary and secondary intervention programs and receive additional support as well. These supports could include use of full FBA, de-escalation training for the student, heightened use of natural supports (e.g., family members, friends of the student), and development of a Behavior Intervention Plan (BIP).\n\nAlthough comprehensive services are important for all students, a critical aspect of the three-tiered model is the identification of students at one of the three levels. One method of identifying students in need of interventions is to analyze office disciplinary referrals (ODR) taken at the school (Irvin et al., 2006). ODRs may be a means of both identifying students' risk level for antisocial behavior and school failure (Walker et al., 1996). Researchers have advocated analyzing this naturally occurring data source as a relatively cheap, effective, and ongoing measurement device for PBS programs (Irvin et al., 2006; Putnam, Luiselli, Handler, & Jefferson, 2003; Sprague et al., 2001; Sugai et al., 2000; Tidwell, Flannery, & Lewis-Palmer, 2003; Walker, Cheney, Stage, & Blum, 2005.\n\nODRs have also been shown to be effective in determining where students fall within a three-leveled model (Sugai et al., 2000), developing professional development as well as helping coordinate school efforts with other community agencies (Tobin & Sugai, 1997; Tobin, Sugai, & Colvin, 2000), predicting school failure in older grades as well as delinquency (Sprague et al., 2001), indicating types of behavior resulting in referrals (Putnam et al., 2003), and determination of the effectiveness of precorrection techniques (Oswald, Safran, & Johanson, 2005). Analyzing discipline referral data can also help school personnel identify where to improve ecological arrangements within a school and to recognize how to increase active supervision in common areas (Nelson, Martella, & Galand, 1998; Nelson et al., 2002). A limitation of only using ODRs to measure behavior problems is that they have been found to be ineffective at measuring internalizing behavior problems such as anxiety, depression, and withdrawal.\n\nFunctional behavior assessment (FBA) emerged from applied behavior analysis. It is the first step in individual and cornerstone of a Positive Behavior Support plan. The assessment seeks to describe the behavior and environmental factors and setting events that predict the behavior in order to guide the development of effective support plans. Assessment lays the foundation of PBS. The assessment includes:\n\nIn some cases, the problem behavior identified in the functional behavior assessment is further analyzed by conducting a behavior chain analysis—in which the sequences of behavior that build up to the problem behavior become the focus.\n\nThe results of the assessment help in developing the individualized behavior support plan. This outlines procedures for teaching alternatives to the behavior problems, and redesign of the environment to make the problem behavior irrelevant, inefficient, and ineffective.\n\nAnother avenue of functional behavior assessment is growing in popularity—it is called behavior chain analysis. In behavior chain analysis, one looks at the progressive changes of behavior as they lead to problem behavior and then attempts to disrupt this sequence. Whereas FBA is concerned mostly with setting-antecedent-behavior-consequence relations, the behavior chain analysis looks at the progression of behavior, such as first the child may fidget, then he might begin to tease others, then he might start to throw things, and then finally hit another student.\n\nThere are many different behavioral strategies that PBS can use to encourage individuals to change their behavior. Some of these strategies are delivered through the consultation process to teachers. The strong part of functional behavior assessment is that it allows interventions to directly address the function (purpose) of a problem behavior. For example, a child who acts out for attention could receive attention for alternative behavior (contingency management) or the teacher could make an effort to increase the amount of attention throughout the day (satiation). Changes in setting events or antecedents are often preferred by PBS because contingency management often takes more effort. Another tactic especially when dealing with disruptive behavior is to use information from a behavior chain analysis to disrupt the behavioral problem early in the sequence to prevent disruption. Some of the most commonly used approaches are:\n\nThe main keys to developing a behavior management program include:\n\nThrough the use of effective behavior management at a school-wide level, PBS programs offer an effective method to reduce school crime and violence. To prevent the most severe forms of problem behaviors, normal social behavior in these programs should be actively taught.\n\nConsequential management is a positive response to challenging behavior. It serves to give the person informed choice and an opportunity to learn. Consequences must be clearly related to the challenging behavior. For example, if a glass of water was thrown and the glass smashed, the consequence (restitution) would be for the person to clean up the mess and replace the glass. These sorts of consequences are consistent with normal social reinforcement contingencies.\n\nProviding choices is very important and staff can set limits by giving alternatives that are related to a behavior they are seeking. It is important that the alternative is stated in a positive way and that words are used which convey that the person has a choice. For example:\n\nThe current trend of positive behavior support (PBS) is to use behavioral techniques to achieve cognitive goals. The use of cognitive ideas becomes more apparent when PBS is used on a school-wide setting. A measurable goal for a school may be to reduce the level of violence, but a main goal might be to create a healthy, respectful, and safe learning, and teaching, environment. PBS on a school-wide level is a system that can be used to create the \"perfect\" school, or at the very least a better school, particularly because before implementation it is necessary to develop a vision for what the school environment should look like in the future.\n\nAccording to Horner et al. (2004), a colleague of Julie Ann Racino supported living, family support, community integration, as cited in (Miller, Nickerson, & Jimerson, 2009), once a school decides to implement PBS, the following characteristics require addressing:\n\n\nIf adequate support and consistency using a positive behavior support program exists, then over time a school’s atmosphere will change for the better. PBS is capable of creating positive changes so pronounced that alumni would mention the differences upon a visit to the school. Such a program is able to create a positive atmosphere and culture in almost any school, but the support, resources, and consistency in using the program over time must be present.\n\nSchool-wide Positive behavior support (SW-PBS) consists of a broad range of systematic and individualized strategies for achieving important social and learning outcomes while preventing problem behavior with all students.\n\nSuch school-wide use of PBS has not been approved other than for special populations, and the work of Dr. Robert Horner, a leader of a Rehabilitation Research and Training Center on Positive Behavioral Supports, was approved on the basis of non-use of aversive technology (e.g., restraints, electric shock, transfers to criminal justice) with any students, including individuals in the most restrictive placements. His 1999 book, \"Positive Behavioral Support for People with Developmental Disabilities\", with our Beach Center on Families colleague Dr. Ann Turnbull of the University of Kansas, and traditional special education/mental retardation group (Dr. E.G. Carr) was reviewed by another behavioral specialist (recommended by special education) who uses a lifestyle approach, Wade Hitzing of Ohio.\n\n\nAlternatives to Special Education Approaches (Special Populations)\n\nTraining and consultancy services, \n"}
{"id": "285701", "url": "https://en.wikipedia.org/wiki?curid=285701", "title": "Principle of indifference", "text": "Principle of indifference\n\nThe principle of indifference (also called principle of insufficient reason) is a rule for assigning epistemic probabilities. Suppose that there are \"n\" > 1 mutually exclusive and collectively exhaustive possibilities. The principle of indifference states that if the \"n\" possibilities are indistinguishable except for their names, then each possibility should be assigned a probability equal to 1/\"n\".\n\nIn Bayesian probability, this is the simplest non-informative prior. The principle of indifference is meaningless under the frequency interpretation of probability, in which probabilities are relative frequencies rather than degrees of belief in uncertain propositions, conditional upon state information.\n\nThe textbook examples for the application of the principle of indifference are coins, dice, and cards.\n\nIn a macroscopic system, at least, it must be assumed that the physical laws which govern the system are not known well enough to predict the outcome. As observed some centuries ago by John Arbuthnot (in the preface of \"Of the Laws of Chance\", 1692),\n\nGiven enough time and resources,\nthere is no fundamental reason to suppose that suitably precise measurements could not be made, which would enable the prediction of the outcome of coins, dice, and cards with high accuracy: Persi Diaconis's work with coin-flipping machines is a practical example of this.\n\nA symmetric coin has two sides, arbitrarily labeled \"heads\" and \"tails\". Assuming that the coin must land on one side or the other, the outcomes of a coin toss are mutually exclusive, exhaustive, and interchangeable. According to the principle of indifference, we assign each of the possible outcomes a probability of 1/2.\n\nIt is implicit in this analysis that the forces acting on the coin are not known with any precision. If the momentum imparted to the coin as it is launched were known with sufficient accuracy, the flight of the coin could be predicted according to the laws of mechanics. Thus the uncertainty in the outcome of a coin toss is derived (for the most part) from the uncertainty with respect to initial conditions. This point is discussed at greater length in the article on coin flipping.\n\nA symmetric die has \"n\" faces, arbitrarily labeled from 1 to \"n\".\nAn ordinary cubical die has \"n\" = 6 faces,\nalthough a symmetric die with different numbers of faces can be constructed;\nsee dice.\nWe assume that the die will land on one face or another upwards,\nand there are no other possible outcomes.\nApplying the principle of indifference, we assign each of the possible outcomes a probability of 1/\"n\".\nAs with coins,\nit is assumed that the initial conditions of throwing the dice are not known\nwith enough precision to predict the outcome according to the laws of mechanics.\nDice are typically thrown so as to bounce on a table or other surface.\nThis interaction makes prediction of the outcome much more difficult.\n\nThe assumption of symmetry is crucial here. Suppose that we are asked to bet for or against the outcome \"6\". We might reason that there are two relevant outcomes here \"6\" or \"not 6\", and that these are mutually exclusive and exhaustive. This suggests assigning the probability 1/2 to each of the two outcomes.\n\nA standard deck contains 52 cards, each given a unique label in an arbitrary fashion, i.e. arbitrarily ordered. We draw a card from the deck; applying the principle of indifference, we assign each of the possible outcomes a probability of 1/52.\n\nThis example, more than the others, shows the difficulty of actually applying the principle of indifference in real situations. What we really mean by the phrase \"arbitrarily ordered\" is simply that we don't have any information that would lead us to favor a particular card. In actual practice, this is rarely the case: a new deck of cards is certainly not in arbitrary order, and neither is a deck immediately after a hand of cards. In practice, we therefore shuffle the cards; this does not destroy the information we have, but instead (hopefully) renders our information practically unusable, although it is still usable in principle. In fact, some expert blackjack players can track aces through the deck; for them, the condition for applying the principle of indifference is not satisfied.\n\nApplying the principle of indifference incorrectly can easily lead to nonsensical results, especially in the case of multivariate, continuous variables. A typical case of misuse is the following example.\n\n\nIn this example, mutually contradictory estimates of the length, surface area, and volume of the cube arise because we have assumed three mutually contradictory distributions for these parameters: a uniform distribution for any one of the variables implies a non-uniform distribution for the other two. (The same paradox arises if we make it discrete: the side is either exactly 3 cm, 4 cm, or 5 cm, mutatis mutandis.) In general, the principle of indifference does not indicate which variable (e.g. in this case, length, surface area, or volume) is to have a uniform epistemic probability distribution.\n\nAnother classic example of this kind of misuse is Bertrand's paradox. Edwin T. Jaynes introduced the principle of transformation groups, which can yield an epistemic probability distribution for this problem. This generalises the principle of indifference, by saying that one is indifferent between \"equivalent problems\" rather than indifference between propositions. This still reduces to the ordinary principle of indifference when one considers a permutation of the labels as generating equivalent problems (i.e. using the permutation transformation group). To apply this to the above box example, we have three random variables related by geometric equations. If we have no reason to favour one trio of values over another, then our prior probabilities must be related by the rule for changing variables in continuous distributions. Let \"L\" be the length, and \"V\" be the volume. Then we must have\n\nwhere formula_2 is the probability density function (pdf) of the stated variable. This equation has a general solution: formula_3, where \"K\" is a normalization constant, determined by the range of \"L\", in this case equal to:\n\nTo put this \"to the test\", we ask for the probability that the length is less than 4. This has probability of:\n\nFor the volume, this should be equal to the probability that the volume is less than 4 = 64. The pdf of the volume is\n\nAnd then probability of volume less than 64 is\n\nThus we have achieved invariance with respect to volume and length. One can also show the same invariance with respect to surface area being less than 6(4) = 96. However, note that this probability assignment is not necessarily a \"correct\" one. For the exact distribution of lengths, volume, or surface area will depend on how the \"experiment\" is conducted. This probability assignment is very similar to the maximum entropy one, in that the frequency distribution corresponding to the above probability distribution is the most likely to be seen. So, if one was to go to \"N\" people individually and simply say \"make me a box somewhere between 3 and 5 cm, or a volume between 27 and 125 cm³, or a surface area between 54 and 150 cm²\", then unless there is a systematic influence on how they make the boxes (e.g. they form a group, and choose one particular method of making boxes), about 56% of the boxes will be less than 4 cm - and it will get very close to this amount very quickly. So, for large N, any deviation from this basically indicates the makers of the boxes were \"systematic\" in how the boxes were made.\n\nThe fundamental hypothesis of statistical physics, that any two microstates of a system with the same total energy are equally probable at equilibrium, is in a sense an example of the principle of indifference. However, when the microstates are described by continuous variables (such as positions and momenta), an additional physical basis is needed in order to explain under \"which\" parameterization the probability density will be uniform. Liouville's theorem justifies the use of canonically conjugate variables, such as positions and their conjugate momenta.\n\nThe original writers on probability, primarily Jacob Bernoulli and Pierre Simon Laplace, considered the principle of indifference to be intuitively obvious and did not even bother to give it a name. Laplace wrote:\n\nThese earlier writers, Laplace in particular, naively generalized the principle of indifference to the case of continuous parameters, giving the so-called \"uniform prior probability distribution\", a function which is constant over all real numbers. He used this function to express a complete lack of knowledge as to the value of a parameter. According to Stigler (page 135), Laplace's assumption of uniform prior probabilities was not a meta-physical assumption. It was an implicit assumption made for the ease of analysis.\n\nThe principle of insufficient reason was its first name, given to it by later writers, possibly as a play on Leibniz's principle of sufficient reason. These later writers (George Boole, John Venn, and others) objected to the use of the uniform prior for two reasons. The first reason is that the constant function is not normalizable, and thus is not a proper probability distribution. The second reason is its inapplicability to continuous variables, as described above. (However, these paradoxical issues can be resolved. In the first case, a constant, or any more general finite polynomial, \"is\" normalizable within any finite range: the range [0,1] is all that matters here. Alternatively, the function may be modified to be zero outside that range, as with a continuous uniform distribution. In the second case, there is no ambiguity provided the problem is \"well-posed\", so that no unwarranted assumptions can be made, or have to be made, thereby fixing the appropriate prior probability density function or prior moment generating function (with variables fixed appropriately) to be used for the probability itself. See the Bertrand paradox (probability) for an analogous case.)\n\nThe \"principle of insufficient reason\" was renamed the \"principle of Indifference\" by the economist , who was careful to note that it applies only when there is no knowledge indicating unequal probabilities.\n\nAttempts to put the notion on firmer philosophical ground have generally begun with the concept of equipossibility and progressed from it to equiprobability.\n\nThe principle of indifference can be given a deeper logical justification by noting that equivalent states of knowledge should be assigned equivalent epistemic probabilities. This argument was propounded by E.T. Jaynes: it leads to two generalizations, namely the principle of transformation groups as in the Jeffreys prior, and the principle of maximum entropy.\n\nMore generally, one speaks of non-informative priors.\n\n\n"}
{"id": "2620511", "url": "https://en.wikipedia.org/wiki?curid=2620511", "title": "Romanian units of measurement", "text": "Romanian units of measurement\n\nThe measures of the old Romanian system varied greatly not only between the three Romanian states (Wallachia, Moldavia, Transylvania), but sometimes also inside the same country. The origin of some of the measures are the Latin (such as \"iugăr\" unit), Slavic (such as \"vadră\" unit), Greek (such as \"dram\" unit) and Turkish (such as \"palmac\" unit) systems.\n\nThis system is no longer in wide use since the adoption of the metric system in 1864, however some rural communities still use a small subset of these units.\n\n\n\n\n"}
{"id": "4302134", "url": "https://en.wikipedia.org/wiki?curid=4302134", "title": "Self-propaganda", "text": "Self-propaganda\n\nSelf-propaganda is a form of propaganda and indoctrination performed by an individual or a group on oneself.\n\nEssentially, it is the act of telling one's self (or a group telling themselves) something that they consider to be true, or to convince themselves, with the unfortunate repercussion of their having no doubts. Because of what they do to themselves, they will go over every aspect of their side of the \"argument\" to prove to themselves that they are right, and will refuse to look at any alternatives. Self-propaganda is a form of self-deception. It functions at individual and social levels: political, economic, and religious. It hides behind partial truths and ignores questions of critical thought.\n\nThe psychological process of utilizing self-propaganda can negatively influence values and beliefs, and subsequent perceptions and judgments, thus becoming a self-fulfilling prophecy.\n\n"}
{"id": "28791", "url": "https://en.wikipedia.org/wiki?curid=28791", "title": "Sovereignty", "text": "Sovereignty\n\nSovereignty is the full right and power of a governing body over itself, without any interference from outside sources or bodies. In political theory, sovereignty is a substantive term designating supreme authority over some polity. It is a basic principle underlying the dominant Westphalian model of state foundation.\n\nThe concepts of sovereignty have been discussed throughout history, and are still actively debated. Its definition, concept, and application has changed throughout, especially during the Age of Enlightenment. The current notion of state sovereignty contains four aspects consisting of territory, population, authority and recognition. According to Stephen D. Krasner, the term could also be understood in four different ways: \n\nOften, these four aspects all appear together, but this is not necessarily the case – they are not affected by one another, and there are historical examples of states that were non-sovereign in one aspect while at the same time being sovereign in another of these aspects. According to Immanuel Wallerstein, another fundamental feature of sovereignty is that it is a claim that must be recognised by others if it is to have any meaning: \n\nThe Roman jurist Ulpian observed that:\n\nUlpian was expressing the idea that the Emperor exercised a rather absolute form of sovereignty, that originated in the people, although he did not use the term expressly.\n\nUlpian's statements were known in medieval Europe, but sovereignty was an important concept in medieval times. Medieval monarchs were \"not\" sovereign, at least not strongly so, because they were constrained by, and shared power with, their feudal aristocracy. Furthermore, both were strongly constrained by custom.\n\nSovereignty existed during the Medieval period as the \"de jure\" rights of nobility and royalty, and in the \"de facto\" capability of individuals to make their own choices in life.\n\nAround c. 1380–1400, the issue of feminine sovereignty was addressed in Geoffrey Chaucer's Middle English collection of \"Canterbury Tales\", specifically in \"The Wife of Bath's Tale.\"\n\nA later English Arthurian romance, \"The Wedding of Sir Gawain and Dame Ragnell\" (c. 1450), uses many of the same elements of the Wife of Bath's tale, yet changes the setting to the court of King Arthur and the Knights of the Round Table. The story revolves around the knight Sir Gawain granting to Dame Ragnell, his new bride, what is purported to be wanted most by women: sovereignty.\n\nSovereignty reemerged as a concept in the late 16th century, a time when civil wars had created a craving for stronger central authority, when monarchs had begun to gather power onto their own hands at the expense of the nobility, and the modern nation state was emerging. Jean Bodin, partly in reaction to the chaos of the French wars of religion, presented theories of sovereignty calling for strong central authority in the form of absolute monarchy. In his 1576 treatise \"Les Six Livres de la République\" (\"Six Books of the Republic\") Bodin argued that it is inherent in the nature of the state that sovereignty must be:\n\n\nBodin rejected the notion of transference of sovereignty from people to the ruler (also known as \"the sovereign\"); natural law and divine law confer upon the sovereign the right to rule. And the sovereign is not above divine law or natural law. He is above (\"ie.\" not bound by) only positive law, that is, laws made by humans. He emphasized that a sovereign is bound to observe certain basic rules derived from the divine law, the law of nature or reason, and the law that is common to all nations (jus gentium), as well as the fundamental laws of the state that determine who is the sovereign, who succeeds to sovereignty, and what limits the sovereign power. Thus, Bodin’s sovereign was restricted by the constitutional law of the state and by the higher law that was considered as binding upon every human being. The fact that the sovereign must obey divine and natural law imposes ethical constraints on him. Bodin also held that the \"lois royales\", the fundamental laws of the French monarchy which regulated matters such as succession, are natural laws and are binding on the French sovereign.\n\nDespite his commitment to absolutism, Bodin held some moderate opinions on how government should in practice be carried out. He held that although the sovereign is not obliged to, it is advisable for him, as a practical expedient, to convene a senate from whom he can obtain advice, to delegate some power to magistrates for the practical administration of the law, and to use the Estates as a means of communicating with the people. Bodin believed that “the most divine, most excellent, and the state form most proper to royalty is governed partly aristocratically and partly democratically”.\n\nWith his doctrine that sovereignty is conferred by divine law, Bodin predefined the scope of the divine right of kings.\nDuring the Age of Enlightenment, the idea of sovereignty gained both legal and moral force as the main Western description of the meaning and power of a State. In particular, the \"Social contract\" as a mechanism for establishing sovereignty was suggested and, by 1800, widely accepted, especially in the new United States and France, though also in Great Britain to a lesser extent.\n\nThomas Hobbes, in \"Leviathan\" (1651) arrived a conception of sovereignty similar to Bodin's, which had just achieved legal status in the \"Peace of Westphalia\", but for different reasons. He created the first modern version of the social contract (or contractarian) theory, arguing that to overcome the \"nasty, brutish and short\" quality of life without the cooperation of other human beings, people must join in a \"commonwealth\" and submit to a \"Soveraigne Power\" that is able to compel them to act in the common good. This expediency argument attracted many of the early proponents of sovereignty. Hobbes strengthened the definition of sovereignty beyond either Westphalian or Bodin's, by saying that it must be:\n\n\nHobbes' hypothesis—that the ruler's sovereignty is contracted to him by the people in return for his maintaining their physical safety—led him to conclude that if and when the ruler fails, the people recover their ability to protect themselves by forming a new contract.\n\nHobbes's theories decisively shape the concept of sovereignty through the medium of social contract theories. Jean-Jacques Rousseau's (1712–1778) definition of popular sovereignty (with early antecedents in Francisco Suárez's theory of the origin of power), provides that the people are the legitimate sovereign. Rousseau considered sovereignty to be inalienable; he condemned the distinction between the origin and the exercise of sovereignty, a distinction upon which constitutional monarchy or representative democracy is founded. John Locke, and Montesquieu are also key figures in the unfolding of the concept of sovereignty; their views differ with Rousseau and with Hobbes on this issue of alienability.\n\nThe second book of Jean-Jacques Rousseau's \"Du Contrat Social, ou Principes du droit politique\" (1762) deals with sovereignty and its rights. Sovereignty, or the general will, is inalienable, for the will cannot be transmitted; it is indivisible, since it is essentially general; it is infallible and always right, determined and limited in its power by the common interest; it acts through laws. Law is the decision of the general will in regard to some object of common interest, but though the general will is always right and desires only good, its judgment is not always enlightened, and consequently does not always see wherein the common good lies; hence the necessity of the legislator. But the legislator has, of himself, no authority; he is only a guide who drafts and proposes laws, but the people alone (that is, the sovereign or general will) has authority to make and impose them.\n\nRousseau, in the Social Contract\"\nargued, \"the growth of the State giving the trustees of public authority more and means to abuse their power, the more the Government has to have force to contain the people, the more force the Sovereign should have in turn in order to contain the Government,\" with the understanding that the Sovereign is \"a collective being of wonder\" (Book II, Chapter I) resulting from \"the general will\" of the people, and that \"what any man, whoever he may be, orders on his own, is not a law\" (Book II, Chapter VI) – and furthermore predicated on the assumption that the people have an unbiased means by which to ascertain the general will. Thus the legal maxim, \"there is no law without a sovereign.\"\n\nAn important factor of sovereignty is its degree of absoluteness. A sovereign power has absolute sovereignty when it is not restricted by a constitution, by the laws of its predecessors, or by custom, and no areas of law or policy are reserved as being outside its control. International law; policies and actions of neighboring states; cooperation and respect of the populace; means of enforcement; and resources to enact policy are factors that might limit sovereignty. For example, parents are not guaranteed the right to decide some matters in the upbringing of their children independent of societal regulation, and municipalities do not have unlimited jurisdiction in local matters, thus neither parents nor municipalities have absolute sovereignty. Theorists have diverged over the desirability of increased absoluteness.\n\nA key element of sovereignty in a legalistic sense is that of exclusivity of jurisdiction. Specifically, the degree to which decisions made by a sovereign entity might be contradicted by another authority. Along these lines, the German sociologist Max Weber proposed that sovereignty is a community's monopoly on the legitimate use of force; and thus any group claiming the same right must either be brought under the yoke of the sovereign, proven illegitimate, or otherwise contested and defeated for sovereignty to be genuine. International law, competing branches of government, and authorities reserved for subordinate entities (such as federated states or republics) represent legal infringements on exclusivity. Social institutions such as religious bodies, corporations, and competing political parties might represent \"de facto\" infringements on exclusivity.\n\n\"De jure\", or legal, sovereignty concerns the expressed and institutionally recognised right to exercise control over a territory. \"De facto\", or actual, sovereignty is concerned with whether control in fact exists. Cooperation and respect of the populace; control of resources in, or moved into, an area; means of enforcement and security; and ability to carry out various functions of state all represent measures of \"de facto\" sovereignty. When control is practiced predominantly by military or police force it is considered \"coercive sovereignty\".\n\nState sovereignty is sometimes viewed synonymously with independence, however, sovereignty can be transferred as a legal right whereas independence cannot. A state can achieve \"de facto\" independence long after acquiring sovereignty, such as in the case of Cambodia, Laos and Vietnam. Additionally, independence can also be suspended when an entire region becomes subject to an occupation such as when Iraq had been overrun by the forces to take part in the Iraq War of 2003, Iraq had not been annexed by any country, so its sovereignty during this period was not contested by any state including those present on the territory. Alternatively, independence can be lost completely when sovereignty itself becomes the subject of dispute. The pre-World War II administrations of Latvia, Lithuania and Estonia maintained an exile existence (and considerable international recognition) whilst their territories were annexed by the Soviet Union and governed locally by their pro-Soviet functionaries. When in 1991 Latvia, Lithuania and Estonia re-enacted independence, it was done so on the basis of continuity directly from the pre-Soviet republics. Another complicated sovereignty scenario can arise when regime itself is the subject of dispute. In the case of Poland, the People's Republic of Poland which governed Poland from 1945 to 1989 is now seen to have been an illegal entity by the modern Polish administration. The post-1989 Polish state claims direct continuity from the Second Polish Republic which ended in 1939. For other reasons however, Poland maintains its communist-era outline as opposed to its pre-World War II shape which included areas now in Belarus, Czech Republic, Lithuania, Slovakia and Ukraine but did not include some of its western regions that were then in Germany.\n\nAt the opposite end of the scale, there is no dispute regarding the self-governance of certain self-proclaimed states such as Republic of Abkhazia, Republic of South Ossetia or the Republic of Kosovo (see List of states with limited recognition) since their governments neither answer to a bigger state, nor is their governance subjected to supervision. The sovereignty (i.e. legal right to govern) however, is disputed in all three cases as the first two entities are claimed by Georgia and the third by Serbia.\n\nInternal sovereignty is the relationship between a sovereign power and the political community. A central concern is legitimacy: by what right does a government exercise authority? Claims of legitimacy might refer to the divine right of kings or to a social contract (i.e. popular sovereignty).\n\nWith Sovereignty meaning holding supreme, independent authority over a region or state, Internal Sovereignty refers to the internal affairs of the state and the location of supreme power within it. A state that has internal sovereignty is one with a government that has been elected by the people and has the popular legitimacy. Internal sovereignty examines the internal affairs of a state and how it operates. It is important to have strong internal sovereignty in relation to keeping order and peace. When you have weak internal sovereignty, organisations such as rebel groups will undermine the authority and disrupt the peace. The presence of a strong authority allows you to keep agreement and enforce sanctions for the violation of laws. The ability for leadership to prevent these violations is a key variable in determining internal sovereignty. The lack of internal sovereignty can cause war in one of two ways: first, undermining the value of agreement by allowing costly violations; and second, requiring such large subsidies for implementation that they render war cheaper than peace. Leadership needs to be able to promise members, especially those like armies, police forces, or paramilitaries will abide by agreements. The presence of strong internal sovereignty allows a state to deter opposition groups in exchange for bargaining. It has been said that a more decentralized authority would be more efficient in keeping peace because the deal must please not only the leadership but also the opposition group. While the operations and affairs within a state are relative to the level of sovereignty within that state, there is still an argument between who should hold the authority in a sovereign state.\n\nThis argument between who should hold the authority within a sovereign state is called the traditional doctrine of public sovereignty. This discussion is between an internal sovereign or an authority of public sovereignty. An internal sovereign is a political body that possesses ultimate, final and independent authority; one whose decisions are binding upon all citizens, groups and institutions in society. Early thinkers believe sovereignty should be vested in the hands of a single person, a monarch. They believed the overriding merit of vesting sovereignty in a single individual was that sovereignty would therefore be indivisible; it would be expressed in a single voice that could claim final authority. An example of an internal sovereign or monarch is Louis XIV of France during the seventeenth century; Louis XIV claimed that he was the state. Jean-Jacques Rousseau rejected monarchical rule in favor of the other type of authority within a sovereign state, public sovereignty. Public Sovereignty is the belief that ultimate authority is vested in the people themselves, expressed in the idea of the general will. This means that the power is elected and supported by its members, the authority has a central goal of the good of the people in mind. The idea of public sovereignty has often been the basis for modern democratic theory.\n\nWithin the modern governmental system, internal sovereignty is usually found in states that have public sovereignty and rarely found within a state controlled by an internal sovereign. A form of government that is a little different from both is the UK parliament system. From 1790 to 1859 it was argued that sovereignty in the UK was vested neither in the Crown nor in the people but in the \"Monarch in Parliament\". This is the origin of the doctrine of parliamentary sovereignty and is usually seen as the fundamental principle of the British constitution. With these principles of parliamentary sovereignty majority control can gain access to unlimited constitutional authority, creating what has been called \"elective dictatorship\" or \"modern autocracy\". Public sovereignty in modern governments is a lot more common with examples like the USA, Canada, Australia and India where government is divided into different levels.\n\nExternal sovereignty concerns the relationship between a sovereign power and other states. For example, the United Kingdom uses the following criterion when deciding under what conditions other states recognise a political entity as having sovereignty over some territory;\nExternal sovereignty is connected with questions of international law – such as: when, if ever, is intervention by one country into another's territory permissible?\n\nFollowing the Thirty Years' War, a European religious conflict that embroiled much of the continent, the Peace of Westphalia in 1648 established the notion of territorial sovereignty as a norm of noninterference in the affairs of other nations, so-called Westphalian sovereignty, even though the actual treaty itself reaffirmed the multiple levels of sovereignty of the Holy Roman Empire. This resulted as a natural extension of the older principle of \"cuius regio, eius religio\" (Whose realm, his religion), leaving the Roman Catholic Church with little ability to interfere with the internal affairs of many European states. It is a myth, however, that the Treaties of Westphalia created a new European order of equal sovereign states.\n\nIn international law, sovereignty means that a government possesses full control over affairs within a territorial or geographical area or limit. Determining whether a specific entity is sovereign is not an exact science, but often a matter of diplomatic dispute. There is usually an expectation that both \"de jure\" and \"de facto\" sovereignty rest in the same organisation at the place and time of concern. Foreign governments use varied criteria and political considerations when deciding whether or not to recognise the sovereignty of a state over a territory. Membership in the United Nations requires that \"[t]he admission of any such state to membership in the United Nations will be effected by a decision of the General Assembly upon the recommendation of the Security Council.\"\n\nSovereignty may be recognized even when the sovereign body possesses no territory or its territory is under partial or total occupation by another power. The Holy See was in this position between the annexation in 1870 of the Papal States by Italy and the signing of the Lateran Treaties in 1929, a 59-year period during which it was recognised as sovereign by many (mostly Roman Catholic) states despite possessing no territory – a situation resolved when the Lateran Treaties granted the Holy See sovereignty over the Vatican City. Another case, \"sui generis\", is the Sovereign Military Order of Malta, the third sovereign entity inside Italian territory (after San Marino and the Vatican City State) and the second inside the Italian capital (since in 1869 the Palazzo di Malta and the Villa Malta receive extraterritorial rights, in this way becoming the only \"sovereign\" territorial possessions of the modern Order), which is the last existing heir to one of several once militarily significant, crusader states of sovereign military orders. In 1607 its Grand masters were also made Reichsfürst (princes of the Holy Roman Empire) by the Holy Roman Emperor, granting them seats in the Reichstag, at the time the closest permanent equivalent to a UN-type general assembly; confirmed 1620). These sovereign rights were never deposed, only the territories were lost. 100 modern states still maintain full diplomatic relations with the order (now \"de facto\" \"the most prestigious service club\"), and the UN awarded it observer status.\n\nThe governments-in-exile of many European states (for instance, Norway, Netherlands or Czechoslovakia) during the Second World War were regarded as sovereign despite their territories being under foreign occupation; their governance resumed as soon as the occupation had ended. The government of Kuwait was in a similar situation \"vis-à-vis\" the Iraqi occupation of its country during 1990–1991. The government of Republic of China was recognized as sovereign over China from 1911 to 1971 despite that its mainland China territory became occupied by Communist Chinese forces since 1949. In 1971 it lost UN recognition to Chinese Communist-led People's Republic of China and its sovereign and political status as a state became disputed and it lost its ability to use \"China\" as its name and therefore became commonly known as Taiwan.\n\nThe International Committee of the Red Cross is commonly mistaken to be sovereign. It has been granted various degrees of special privileges and legal immunities in many countries, that in cases like Switzerland are considerable, The Committee is a private organisation governed by Swiss law.\n\nJust as the office of head of state can be vested jointly in several persons within a state, the sovereign jurisdiction over a single political territory can be shared jointly by two or more consenting powers, notably in the form of a condominium.\n\nLikewise the member states of international organizations may voluntarily bind themselves by treaty to a supranational organization, such as a continental union. In the case of the European Union members states this is called \"pooled sovereignty\".\n\nAnother example of shared and pooled sovereignty is the Acts of Union 1707 which created the unitary state now known as the United Kingdom. It was a full economic union, meaning the Scottish and English systems of currency, taxation and laws regulating trade were aligned. Nonetheless, Scotland and England never fully surrendered or pooled all of their governance sovereignty; they retained many of their previous national institutional features and characteristics, particularly relating to their legal, religious and educational systems. In 2012, the Scottish Government, created in 1998 through devolution in the United Kingdom, negotiated terms with the Government of the United Kingdom for the Scottish independence referendum, 2014 which resulted in the people of Scotland deciding to continue the pooling of its sovereignty with the rest of the United Kingdom.\n\nA community of people who claim the right of self-determination based on a common ethnicity, history and culture might seek to establish sovereignty over a region, thus creating a nation-state. Such nations are sometimes recognised as autonomous areas rather than as fully sovereign, independent states.\n\nIn a federal system of government, \"sovereignty\" also refers to powers which a constituent state or republic possesses independently of the national government. In a confederation constituent entities retain the right to withdraw from the national body, but in a federation member states or republics do not hold that right.\n\nDifferent interpretations of state sovereignty in the United States of America, as it related to the expansion of slavery and fugitive slave laws, led to the outbreak of the American Civil War. Depending on the particular issue, sometimes both northern and southern states justified their political positions by appealing to state sovereignty. Fearing that slavery would be threatened by results of the 1860 presidential election, eleven slave states declared their independence from the federal Union and formed a new confederation. The United States government rejected the secessions as rebellion, declaring that secession from the Union by an individual state was unconstitutional, as the states were part of an indissolvable federation. \n\nA number of modes of acquisition of sovereignty are presently or have historically been recognised by international law as lawful methods by which a state may acquire sovereignty over territory. The classification of these modes originally derived from Roman property law and from the 15th and 16th century with the development of international law. The modes are:\n\nThere exist vastly differing views on the moral basis of sovereignty. A fundamental polarity is between theories that assert that sovereignty is vested directly in the sovereigns by divine or natural right and theories that assert it originates from the people. In the latter case there is a further division into those that assert that the people transfer their sovereignty to the sovereign (Hobbes), and those that assert that the people retain their sovereignty (Rousseau).\n\nDuring the brief period of absolute monarchies in Europe, the divine right of kings was an important competing justification for the exercise of sovereignty. The Mandate of Heaven had some similar implications in China.\n\nA republic is a form of government in which the people, or some significant portion of them, retain sovereignty over the government and where offices of state are not granted through heritage. A common modern definition of a republic is a government having a head of state who is not a monarch.\n\nDemocracy is based on the concept of \"popular sovereignty\". In a direct democracy the public plays an active role in shaping and deciding policy. Representative democracy permits a transfer of the exercise of sovereignty from the people to a legislative body or an executive (or to some combination of legislature, executive and Judiciary). Many representative democracies provide limited direct democracy through referendum, initiative, and recall.\n\nParliamentary sovereignty refers to a representative democracy where the parliament is ultimately sovereign and not the executive power nor the judiciary.\n\n\nAccording to Matteo Laruffa \"sovereignty resides in every public action and policy as the exercise of executive powers by institutions open to the participation of citizens to the decision-making processes\"\n\nAnother topic is whether the law is held to be sovereign, that is, whether it is above political or other interference. Sovereign law constitutes a true state of law, meaning the letter of the law (if constitutionally correct) is applicable and enforceable, even when against the political will of the nation, as long as not formally changed following the constitutional procedure. Strictly speaking, any deviation from this principle constitutes a revolution or a coup d'état, regardless of the intentions.\n\n\n"}
{"id": "37889300", "url": "https://en.wikipedia.org/wiki?curid=37889300", "title": "State formation", "text": "State formation\n\nState formation is the process of the development of a centralized government structure in a situation where one did not exist prior to its development. State formation has been a study of many disciplines of the social sciences for a number of years, so much so that Jonathan Haas writes that \"One of the favorite pastimes of social scientists over the course of the past century has been to theorize about the evolution of the world's great civilizations.\" The study of state formation is divided generally into either the study of early states (those that developed in stateless societies) or the study of modern states (particularly of the form that developed in Europe in the 17th century and spread around the world). Academic debate about various theories is a prominent feature in fields like Anthropology, Sociology, Economics and Political Science.\n\nA state is a political system with a centralized government, a military force, a civil service, an arranged society, and literacy. Though, there is no clear agreement on the defining characteristics of a state and the definition can vary significantly, based upon the focus of the particular definition. The state is considered to be territoriality bound and is distinct from tribes or units without centralized institutions.\n\nAccording to Painter & Jeffrey, there are 5 distinctive features of the modern state:\n\n1) They are ordered by precise boundaries with administrative control across the whole;\n\n2) They occupy large territories with control given to organized institutions;\n\n3) They have a capital city and are endowed with symbols that embody state power;\n\n4) The government within state creates organizations to monitor, govern and control its population through surveillance and record keeping;\n\n5) They increase monitoring over time.\n\nAdditionally, Herbst holds that there is another relevant characteristic of modern states: nationalism. This feeling of belonging to a certain territory plays a central role in state formation since it increases citizens' willingness to pay taxes.\n\nTheories of state formation have two distinct focuses, depending largely on the field of study:\n\n\nStates are minimally defined by anthropologist David S. Sandeford as socially stratified and bureaucratically governed societies with at least four levels of settlement hierarchy (e.g., a large capital, cities, villages, and hamlets). Primary states are those state societies that developed in regions where no states existed before. These states developed by strictly internal processes and interaction with other non-states societies. The exact number of cases which qualify as primary states is not clearly known because of limited information about political organization before the development of writing in many places, but Sandeford lists ten likely cases of primary state formation in Eurasia, the Americas, and the Pacific.\n\nStudies on the formation of early states tend to focus on processes that create and institutionalize a state in a situation where a state did not exist before. Examples of early states which developed in interaction with other states include the Aegean Bronze Age Greek civilizations and the Malagasy civilization in Madagascar. Unlike primary state formation, early state formation does not require the creation of the first state in that cultural context or development autonomously, independently from state development nearby. Early state formation causation can thus include borrowing, imposition, and other forms of interaction with already existing states.\n\nTheories on the formation of modern states focus on the processes that support the development of modern states, particularly those that formed in late-medieval Europe and then spread around the world with colonialism. Starting in the 1940s and 1950s, with decolonization processes underway, attention began to focus on the formation and construction of modern states with significant bureaucracies, ability to tax, and territorial sovereignty around the world. However, some scholars hold that the modern state model formed in other parts of the world prior to colonialism, but that colonial structures replaced it.\n\nThere are a number of different theories and hypotheses regarding early state formation that seek generalizations to explain why the state developed in some places but not others. Other scholars believe that generalizations are unhelpful and that each case of early state formation should be treated on its own.\n\nVoluntary theories contend that diverse groups of people came together to form states as a result of some shared rational interest. The theories largely focus on the development of agriculture, and the population and organizational pressure that followed and resulted in state formation. The argument is that such pressures result in integrative pressure for rational people to unify and create a state. Much of the social contract philosophical tradition proposed a voluntary theory for state formation.\n\nOne of the most prominent theories of early and primary state formation is the \"hydraulic hypothesis\", which contends that the state was a result of the need to build and maintain large-scale irrigation projects. The theory was most significantly detailed Karl August Wittfogel's argument that, in arid environments, farmers would be confronted by the production limits of small-scale irrigation. Eventually different agricultural producers would join together in response to population pressure and the arid environment, to create a state apparatus that could build and maintain large irrigation projects.\n\nIn addition to this, is what Carneiro calls the \"automatic hypothesis\", which contends that the development of agriculture easily produces conditions necessary for the development of a state. With surplus food stocks created by agricultural development, creation of distinct worker classes and a division of labor would automatically trigger creation of the state form.\n\nA third voluntary hypothesis, particularly common with some explanations of early state development, is that long distance trade networks created an impetus for states to develop at key locations: such as ports or oases. For example, the increased trade in the 16th century may have been a key to state formation in West African states such as Whydah, Dahomey, and the Benin Empire.\n\nConflict theories of state formation regard conflict and dominance of some population over another population as key to the formation of states. In contrast with voluntary theories, these arguments believe that people do not voluntarily agree to create a state to maximize benefits, but that states form due to some form of oppression by one group over others. A number of different theories rely on conflict, dominance, or oppression as a causal process or as a necessary mechanism within certain conditions and they may borrow from other approaches. In general the theories highlight: \"economic stratification\", \"conquest of other peoples\", conflict in \"circumscribed areas\", and the neoevolutionary growth of bureaucracy. \n\nOther aspects are highlighted in different theories as of contributing importance. It is sometimes claimed that technological development, religious development, or socialization of members are crucial to state development. However, most of these factors are found to be secondary in anthropological analysis. In addition to conquest, some theories contend that the need for defense from military conquest or the military organization to conquer other peoples is the key aspect leading to state formation.\n\nSome theories proposed in the 19th century and early 20th century have since been largely discredited by anthropologists. Carneiro writes that theories \"with a racial basis, for example, are now so thoroughly discredited that they need not be dealt with...We can also reject the belief that the state is an expression of the 'genius' of a people, or that it arose through a 'historical accident.' Such notions make the state appear to be something metaphysical or adventitious, and thus place it beyond scientific understanding.\" Similarly, social Darwinist perspectives like those of Walter Bagehot in \"Physics and Politics\" argued that the state form developed as a result of the best leaders and organized societies gradually gaining power until a state resulted. Such explanations are not considered sufficient to explain the formation of the state.\n\nIn the medieval period (500-1400) in Europe, there were a variety of authority forms throughout the region. These included feudal lords, empires, religious authorities, free cities, and other authorities. Often dated to the 1648 Peace of Westphalia, there began to be the development in Europe of modern states with large-scale capacity for taxation, coercive control of their populations, and advanced bureaucracies. The state became prominent in Europe over the next few centuries before the particular form of the state spread to the rest of the world via the colonial and international pressures of the 19th century and 20th century. Other modern states developed in Africa and Asia prior to colonialism, but were largely displaced by colonial rule.\n\nPolitical scientists, sociologists, and anthropologists began studying the state formation processes in Europe and elsewhere in the 17th century—beginning significantly with Max Weber. However, state formation became a primary interest in the 1970s. The question was often framed as a contest between state forces and society forces and the study of how the state became prominent over particular societies. A number of theories developed regarding state development in Europe. Other theories focused on the creation of states in late colonial and post-colonial societies. The lessons from these studies of the formation of states in the modern period are often used in theories about State-building. Other theories contend that the state in Europe was constructed in connection with peoples from outside Europe and that focusing on state formation in Europe as a foundation for study silences the diverse history of state formation.\n\nBased on the model of European states, it has been commonly assumed that development is the natural path that states will eventually walk through. However, Herbst holds that in the case African states, as well as in developing countries of other regions, development need not be the natural step. States that struggle their consolidation could remain permanently weak.\n\nTwo related theories are based on military development and warfare, and the role that these forces played in state formation. Charles Tilly developed an argument that the state developed largely as a result of \"state-makers\" who sought to increase the taxes they could gain from the people under their control so they could continue fighting wars. According to Tilly, the state makes war and war makes states. In the constant warfare of the centuries in Europe, coupled with expanded costs of war with mass armies and gunpowder, warlords had to find ways to finance war and control territory more effectively. The modern state presented the opportunity for them to develop taxation structures, the coercive structure to implement that taxation, and finally the guarantee of protection from other states that could get much of the population to agree. Taxes and revenue raising have been repeatedly pointed out as a key aspect of state formation and the development of state capacity. Economist Nicholas Kaldor emphasized on the importance of revenue raising and warned about the dangers of the dependence on foreign aid. Tilly argues, state making is similar to organized crime because it is a \"quintessential protection racket with the advantage of legitimacy.\"\n\nMichael Roberts and Geoffrey Parker, in contrast, finds that the primary causal factor was not the \"state-makers\" themselves, but simply the military revolutions that allowed development of larger armies. The argument is that with the expanded state of warfare, the state became the only administrative unit that could endure in the constant warfare in the Europe of this period, because only it could develop large enough armies. \nThis view—that the modern state replaced chaos and general violence with internal disciplinary structures—has been challenged as ethnocentric, and ignoring the violence of modern states.\n\nWar has played a key role not only in the consolidation of European states but also of some third world states. According to Herbst, external security threats have had a fundamental role in the development of the South Korean and Taiwanese states. A 2017 study which tests the predictions of warfare theories of Tilly and others found that the predictions do not match the empirical record. The study found that median state size decreased from 1100 to 1800, and that the number of states increases rapidly between the twelfth and thirteen centuries and remained constant until 1800.\n\nStein Rokkan and others have argued that the modern territorial state developed in places that were peripheral to the commercial \"city belt\" (\"a central regional band extending, roughly, in an arc from the Low Countries, through the Rhineland and into Northern Italy\") that ran through Central Europe. The existence of prosperous urban centers that relied on commerce in Central Europe prevented rulers from consolidaing their rule over others. The elites in those urban centers could rely on their wealth and on collective security institutions (like the Hanseatic or Swabian league) with other urban centers to sustain their independence. A lower density of urban centers in England and France made it easier for rulers to establish rule over expansive territories.\n\nAnother argument contends that the state developed out of economic and social crises that were prominent in late-medieval Europe. Religious wars between Catholics and Protestants, and the involvement of leaders in the domains of other leaders under religious reasons was the primary problem dealt with in the Peace of Westphalia. In addition, Marxist theory contends that the economic crisis of feudalism forced the aristocracy to adapt various centralized forms of organization so they could retain economic power, and this resulted in the formation of the modern state.\n\nSome scholarship, linked to wider debates in Anthropology, has increasingly emphasized the state as a primarily cultural artifact, and focuses on how symbolism plays a primary role in state formation. Most explicitly, some studies emphasize how the creation of national identification and citizenship were crucial to state formation. The state then is not simply a military or economic authority, but also includes cultural components creating consent by people by giving them rights and shared belonging.\n\nWhile modern states existed without European influence around the world before colonialism, post-colonial state formation has received the most significant attention. While warfare is primary in theories about state formation in Europe, the development of the international norm of non-interventionism means that other processes of state formation have become prominent outside Europe (including colonial imposition, assimilation, borrowing, and some internal political processes. John W. Meyer's \"World Society Theory\" contends that the state form was exported from Europe, institutionalized in the United Nations, and gradually the modern nation-state became the basis for both those in power and those challenging power. In addition, because many of the early modern states like the United Kingdom and France had significant empires, their institutional templates became standard for application globally.\n\n\n\n"}
{"id": "327941", "url": "https://en.wikipedia.org/wiki?curid=327941", "title": "Subjective idealism", "text": "Subjective idealism\n\nSubjective idealism, or empirical idealism, is the monistic metaphysical doctrine that only minds and mental contents exist. It entails and is generally identified or associated with immaterialism, the doctrine that material things do not exist. Subjective idealism rejects dualism, neutral monism, and materialism; indeed, it is the contrary of eliminative materialism, the doctrine that all or some classes of mental phenomena (such as emotions, beliefs, or desires) do not exist, but are sheer illusions.\n\nSubjective idealism is a fusion of phenomenalism or empiricism, which confers special status upon the immediately perceived, with idealism, which confers special status upon the mental. Idealism denies the knowability or existence of the non-mental, while phenomenalism serves to restrict the mental to the empirical. Subjective idealism thus identifies its mental reality with the world of ordinary experience, rather than appealing to the unitary world-spirit of pantheism or absolute idealism. This form of idealism is \"subjective\" not because it denies that there is an objective reality, but because it asserts that this reality is completely dependent upon the minds of the subjects that perceive it.\n\nThe earliest thinkers identifiable as subjective idealists were certain members of the Yogācāra school of Indian Buddhism, who reduced the world of experience to a stream of subjective perceptions. Subjective idealism made its mark in Europe in the 18th-century writings of George Berkeley, who argued that the idea of mind-independent reality is incoherent, concluding that the world consists of the minds of humans and of God. Subsequent writers have continuously grappled with Berkeley's skeptical arguments. Immanuel Kant responded by rejecting Berkeley's immaterialism and replacing it with transcendental idealism, which views the mind-independent world as existent but incognizable in itself. Since Kant, true immaterialism has remained a rarity, but is survived by partly overlapping movements such as phenomenalism, subjectivism, and perspectivism.\n\nThinkers such as Plato, Plotinus and Augustine of Hippo anticipated idealism's antimaterialism with their views of the inferior or derivative reality of matter. However, these Platonists did not make Berkeley's turn toward subjectivity. Indeed, Plato rationalistically condemned sense-experience, whereas subjective idealism presupposed empiricism and the irreducible reality of sense data. A more subjectivist methodology could be found in the Pyrrhonists' emphasis on the world of appearance, but their skepticism precluded the drawing of any ontological conclusions from the epistemic primacy of phenomena.\n\nThe first mature articulations of idealism arise in Yogacarin thinkers such as the 7th-century epistemologist Dharmakīrti, who identified ultimate reality with sense-perception. The most famous proponent of subjective idealism in the Western world was the 18th-century Irish philosopher George Berkeley, although Berkeley's term for his theory was \"immaterialism\". From the point of view of subjective idealism, the material world does not exist, and the phenomenal world is dependent on humans. Hence the fundamental idea of this philosophical system (as represented by Berkeley or Mach) is that things are complexes of ideas or sensations, and only subjects and objects of perceptions exist. Berkeley summarized his theory with the motto \"esse est percipi\" (\"To be is to be perceived\"), but went on to elaborate it with God as the source of consensus reality and other particulars.\n\nAccording to Berkeley, an object has real being as long as it is perceived by a mind. God, being omniscient perceives everything perceivable, thus all real beings exist in the mind of God. However, it is also evident that each of us has free will and understanding upon self-reflection, and our senses and ideas suggest that other people also possess these qualities as well. According to Berkeley there is no material universe, in fact he has absolutely no idea what that could possibly mean. To theorize about a universe that is composed of insensible matter is not a sensible thing to do. This matters because there is absolutely no positive account for a material universe, only speculation about things that are by fiat outside of our minds.\n\nBerkeley's assessment of immaterialism was criticized by Samuel Johnson, as recorded by James Boswell. Responding to the theory, Dr. Johnson exclaimed \"I refute it \"thus\"!\" while kicking a rock with \"mighty force\". This episode is alluded to by Stephen Dedalus in James Joyce's \"Ulysses\", chapter three. Reflecting on the \"ineluctable modality of the visible\", Dedalus conjures the image of Johnson's refutation and carries it forth in conjunction with Aristotle's expositions on the nature of the senses as described in \"Sense and Sensibilia\". Aristotle held that while visual perception suffered a compromised authenticity because it passed through the diaphanous liquid of the inner eye before being observed, sound and the experience of hearing were not thus similarly diluted. Dedalus experiments with the concept in the development of his aesthetic ideal.\n\nSubjective idealism is featured prominently in the Norwegian novel \"Sophie's World\", in which \"Sophie's world\" exists in fact only in the pages of a book.\n\nA parable of subjective idealism can be found in Jorge Luis Borges' short story \"Tlön, Uqbar, Orbis Tertius\", which specifically mentions Berkeley.\n\n\n"}
{"id": "374851", "url": "https://en.wikipedia.org/wiki?curid=374851", "title": "Type rule", "text": "Type rule\n\nIn type theory, a type rule is an inference rule that describes how a type system assigns a type to a syntactic construction. These rules may be applied by the type system to determine if a program is well typed and what type expressions have. A prototypical example of the use of type rules is in defining type inference in the simply typed lambda calculus, which is the internal language of Cartesian closed categories.\n\nAn expression formula_1 of type formula_2 is written as formula_3. The typing environment is written as formula_4. The notation for inference is the usual one for sequents and inference rules, and has the following general form\n\nThe sequents above the line are the premises that must be fulfilled for the rule to be applied, yielding the conclusion: the sequents below the line. This can be read as: \"if expression formula_6 has type formula_7 in environment formula_8, for all formula_9, then the expression formula_1 will have an environment formula_4 and type formula_2\".\n\nFor example, a simple language to perform arithmetic calculations on real numbers may have the following rules\n\nA type rule may have no premises, and usually the line is omitted in these cases. A type rule may also change an environment by adding new variables to a previous environment; for example, a declaration may have the following type rule, where a new variable formula_14,\nwith type formula_15, is added to formula_4:\n\nHere the syntax of the let expression is that of Standard ML. Thus type rules can be used to derive the types of composed expressions, much like in natural deduction.\n\n\n"}
{"id": "7599422", "url": "https://en.wikipedia.org/wiki?curid=7599422", "title": "Why–because analysis", "text": "Why–because analysis\n\nWhy–because analysis (WBA) is a method for accident analysis. It is independent of application domain and has been used to analyse, among others, aviation-, railway-, marine-, and computer-related accidents and incidents. It is mainly used as an after the fact (or a posteriori) analysis method. WBA strives to ensure objectivity, falsifiability and reproducibility of results.\n\nThe result of a WBA is a why–because graph (WBG). The WBG depicts causal relations between factors of an accident. It is a directed acyclic graph where the nodes of the graph are factors. Directed edges denote cause–effect relations between the factors.\n\nWBA starts with the question \"What is the accident or accidents in question?\". In most cases this is easy to define. Next comes an iterative process to determine causes. When causes for the accident have been identified, formal tests are applied to all potential cause-effect relations. This process can be iterated for the newfound causes, and so on, until a satisfactory result has been achieved.\n\nAt each node (factor), each contributing cause (related factor) must have been necessary to cause the accident, and the totality of causes must have been sufficient to do so.\n\nThe \"counterfactual test\" (CT) – The CT leads back to David Lewis' formal notion of causality and counterfactuals. The CT asks the following question: \"If the cause had not been, could the effect have happened?\". The CT proves or disproves that a cause is a necessary causal factor for an effect. Only if it is necessary for the cause in question then it is clearly contributing to the effect.\n\nThe \"causal sufficiency test\" – The CST asks the question: \"Will an effect always happen if all attributed causes happen?\". The CST aims at deciding whether a set of causes are sufficient for an effect to happen. The missing of causes can thus be identified.\n\nOnly if for all causal relations the CT is positive and for all sets of causes to their effects the CST is positive the WBG is correct: each cause must be necessary (CT), and the totality of causes must be sufficient (CST): nothing is omitted (CST: the listed causes are sufficient), and nothing is superfluous (CT: each cause is necessary).\n\n\n"}
{"id": "35937072", "url": "https://en.wikipedia.org/wiki?curid=35937072", "title": "Woese's dogma", "text": "Woese's dogma\n\nWoese's dogma is a principle of evolutionary biology first put forth by biophysicist Carl Woese in 1977. It states that the evolution of ribosomal RNA was a necessary precursor to the evolution of modern life forms. This led to the advancement of the phylogenetic tree of life consisting of three domains rather than the previously accepted two.While the existence of Eukarya and Prokarya were already accepted, Woese was responsible for the distinction between Bacteria and Archaea. Despite initial criticism and controversy surrounding his claims, Woese's three domain system, based on his work regarding the role of rRNA in the evolution of modern life, has become widely accepted.\n\nEvidence for Woese's dogma is well established through comparisons of RNA homology. Modern research allows more liberal use of RNA sequencing, allowing for a better comparative analysis between distant RNA. When analyzing multiple strains of \"E. coli\", Root-Bernstein et. al. have compared tRNA encodings found within rRNA with tRNA found in \"E. coli\" to see if the secondary structure was the same as more “modern” tRNA present in \"E. coli\". Comparisons between the tRNA encodings found in the rRNAs and mRNAs of the control sequences found that “sortings” for these sequences were extremely similar, and comparisons of translated protein structure indicated that homology was likely. Additionally, sequences homologous to all tRNAs necessary for translation were present in 16s and 23s rRNAs, and synthetases to load these tRNAs were also found, indicating that many of the functions of transcription and translation present in more modern life exist in rRNA, if vestigially.\nWhen comparing homologies of rRNA structures, it is necessary to analyze substructures. This is because models that study RNA structure on the whole do not currently exist. Generally, phylogenies of rRNA subunits are created to understand each component, and how they function and evolve. Through phylogenies created that depict rRNA structural elements that are present in all three domains of life, the oldest structural components can be determined through relative dating. These phylogenies were used in a study by Harish et. al., to show that a helical stem labeled h44 in small subunit rRNA can be described as the oldest structural component of rRNA, which holds particular significance, as this structure responsible for linking processes in the small subunit, which is responsible for decoding, with the large subunit, which is responsible for the formation of peptide bonds and the releasing of elongation factors. This essentially shows that the functional origin of the ribosome, responsible for protein synthesis, is common in all modern life throughout each of the three domains.\n\nEvidence has also been obtained in studying eukaryotic organelles, such as the chloroplast. Zablen et al.’s phylogenetic analysis conducted electrophoresis on chloroplast ribosomal RNA, specifically on the 16S rRNA of \"Euglena gracilis\"\".\" In conducting this experiment, researchers compared the electrophoretic fingerprint of this RNA to other chloroplasts and prokarya. In comparing these results, it was found that generally, these chloroplasts show a close genomic relationship, while a more distant one is seen for algae, and subsequently prokaryotic organisms. This experiment shows that the rRNA of distantly related organisms has a similar origin of that in eukaryotic organelles, supporting the idea that the evolution of rRNA was a necessary precursor of modern life.\n\nOne of the reasons that Woese’s Dogma holds significance is because of the potential that RNA was the first primordial self-replicating molecule (see: RNA World), meaning it would be key in the progression of modern life. In particular, it has been proposed that ribosomes exist as a missing link in prebiotic evolution, with rRNA being a vestige of an ancient genome. Some evidence exists for the proposal that rRNA functioned in the past to encode proteins that are key to ribosome function. One notable example is the fact that rRNA proteins are commonly known to bind with their own mRNA. In addition, some ribosomal proteins not only regulate their own expression, but the expression of other proteins as well. These are both indications of self-replication, and indicate the possibility that the mRNA that encodes ribosomal proteins evolved from rRNA.\n\nRNA existing as a primordial self replicating entity is an idea that faces criticism. The idea of rRNA in particular being sufficient on its own to explain the progression of modern life struggles due to the fact that it lacks certain key pieces of evidence. In particular RNA cannot be shown to be prebiotic, as there is no way for the nucleotides or nucleosides that compose it to be non-enzymatically replicated. Additionally, other criticisms exist, such as the fact that RNA is not stable enough to have arisen prebiotically, and that it is too complex to have arisen prebiotically. This has led to the development of other hypotheses, such as 'proteins first', which states that proteins arose prior to RNA, or coevolved with RNA. This has also led to the proposal of other primordial molecules that may have developed into RNA and DNA, such as peptide nucleic acids, which also show evidence of self replication. Despite the fact that criticisms might exist on the primordial or prebiotic nature of rRNA, these criticisms are not aimed at Woese's Dogma on the whole, as Woese's Dogma only claims that the evolution of rRNA was a necessary precursor to modern life, not that rRNA arose prebiotically.\n"}
{"id": "2727079", "url": "https://en.wikipedia.org/wiki?curid=2727079", "title": "Women's shelter", "text": "Women's shelter\n\nA women's shelter, also known as a women's refuge and battered women's shelter, is a place of temporary protection and support for women escaping domestic violence and intimate partner violence of all forms. The term is also frequently used to describe a location for the same purpose that is open to people of all genders at risk.\n\nRepresentative data samples done by the Centers for Disease Control and Prevention show that one in three women will experience physical violence during their lifetime. One in ten will experience sexual violence. Women's shelters help individuals escape these instances of domestic violence and intimate partner violence and act as a place for protection as they choose how to move forward. Additionally, many shelters offer a variety of other services to help women and their children including counseling and legal guidance.\n\nThe ability to escape is valuable for women subjected to domestic violence or intimate partner violence. Additionally, such situations frequently involve an imbalance of power that limits the victim's financial options when they want to leave. Shelters help women gain tangible resources to help them and their families create a new life. Lastly, shelters are valuable to battered women because they can help them find a sense of empowerment.\n\nWomen's shelters are available in more than forty-five countries. They are supported with government resources as well as non-profit funds. Additionally, many philanthropists also help and support these institutions.\n\nThe very first women's shelter in Canada was started in 1965 by the Harbour Rescue Mission (now Mission Services) in Hamilton, Ontario. It was named Inasmuch House, with the name referencing a Bible verse (Matthew 25:40) quoting Jesus Christ as saying \"Inasmuch as you have done it for the least of these, you have done it for me.\" It was designed to be a practical outworking of Christian values relating to justice and care. Although originally conceived as a shelter for women leaving prison, its clientele later became women escaping abuse by their partners. The concept of Inasmuch House was shared with other Christian inner-city missions across North America and led to the opening of other such shelters.\n\nThe first shelters in Canada developed from a feminist perspective were started by Interval House, Toronto in April 1973, and the Ishtar Transition Housing Society in Langley, B.C.in June 1973. These homes were grass roots organizations that lived on short term grants at first, with staff often working sacrificially in order to keep the houses running to ensure women's safety.\n\nFrom there,the movement in Canada grew, with women's shelters opening under a variety of names - often as a Transition House or Interval House - opening up across the country in order to help women flee from abusive situations.\n\nThe first women's shelter in the United States was likely established in St. Paul, Minnesota shortly after the first domestic violence hotline was established in the same location. However, other early locations include Rosie's Place in Boston, Massachusetts, which was opened in 1974 by Kip Tiernan, and the Atlanta Union Mission in Atlanta, opened by Elsie Huck.\n\nWomen's shelters evolved over time. Grassroots community advocates in the 1970s offered shelters as one of the first services for victims of intimate partner violence. At this time, most shelters were for emergencies and involved stays less than six months. Volunteers and shelter workers offered legal and welfare referrals to women when they exited but contact afterwards was limited. More recent programs, such as those funded by the Violence Against Women Act, offer longer term stays for women. These locations, as well as transitional housing, offer more services to women and their children. Another recent change is the increasing amount of shelters publicizing their locations to increase funding and visibility in the community.\n\nDue to a larger women's movement, the number of shelters quickly increased after their induction and by 1977 the United States had eighty-nine shelters available for victims of violence. By 2000, the United States had over 2,000 domestic violence programs in place, many with domestic violence shelters included.\n\nFor Asia, offering shelter to abused women is not a new concept. In feudal Japan, Buddhist temples known as Kakekomi Dera acted as locations where abused women could take shelter before filing for divorce. A formal system took more time, however, so it was not until 1993 that the grassroots women's movement of Japan built the first shelter. Today, there are thirty shelters throughout the country. A similar history did not lead to as much progress in China. Women's shelters did not exist until the nineties and since then the country only opened a small number. In Beijing there are no shelters for the twenty million residents.\n\nIn England, Erin Pizzey opened the first widely known shelter for battered women, Chiswick Women's Aid in 1971. Since this time almost every European country has opened shelters to help domestic violence victims. Two countries even offer shelters for particular ethnicities and cultures. Additionally, a new development in Europe is that countries like the Netherlands and Austria opened social housing for long term stays. One reason for this growth is the Istanbul Convention against Violence Against Women and Domestic Violence, a convention signed by forty-seven Council of Europe member states in 2011. An article in the Convention sets the creation of women's shelters as a minimum standard for compliance. Following austerity two thirds of local authorities in England have cut funding for women's refuges since 2010.\n\nIn Australia, the first women's refuge, known as Elsie Refuge, was opened in Glebe, New South Wales in 1974 by a group of women's liberation activists. Many others followed, with 11 established around the country by the middle of 1975 and many more to follow. Initially these services were entirely reliant on volunteer efforts and donations from the community, but they subsequently secured government funding under the Whitlam government. However, government policy has recently seen some moves to dismantle the women's refuge movement, so that in New South Wales since 2014 the management of many refuges has been handed over to large religious agencies so that they now often operate as generic homeless services rather than specifically catering to women and children escaping domestic violence.\n\nWomen's shelters offer temporary refuge for women escaping acts of domestic violence or intimate partner violence. Many women become homeless in this situation because they are financially dependent on their abuser and these resources help to incentivize and support escape. The average length of stay for women is between thirty and sixty days in the United States. However, this varies in different countries and in Europe, for example, four countries limit stays to a few weeks. Transitional housing, another form of women's shelter, offers stays of up to a year while certain communities offer public and private housing for even longer periods.\n\nThere is high demand for shelter services in the United States. A one-day national census done by the National Network to End Domestic Violence found that emergency shelters served over 66,581 people in one day and over 9,000 requests could not be met during the same period. In Europe there is a similar pattern of over-demand. Utilization by women is not consistent across the population of intimate partner violence victims, however. Women with children tend to use shelters more often as well as those that are injured physically. Additionally, rural women have more trouble accessing services due to isolation and a lack of resources.\n\nShelters are usually offered as part of a comprehensive domestic violence program that can also include a crisis hotline, services for non-sheltered children, an education program, a community speaker list, and an offender treatment program. Shelters themselves also offer a variety of services. They provide counseling, support groups and skills workshops to help women move on independently. These act as tools of empowerment for women in conjunction with goal setting programs. Lastly, they offer support for children as well as legal and medical advocacy.\n\nMost residents of women's shelters are the children of women who are victims of violence. This is one reason why more than half of shelters offered services to this portion of the population in a survey of 215 shelters in the United States. Services for children often include counseling and group therapy options that are meant to strengthen parent-child relationships and help with mental well-being. Recently, shelters also responded to increasing numbers of male victims by offering help mostly in the form of hotel vouchers.\n\nIn the United States, certain shelters do not permit access to men. This practice was challenged in \"Blumhorst v. Haven Hills,\" a court case in California (\"Los Angeles Superior Court Case No. BC291977\"). However, the court dismissed the case because the plaintiff lacked standing – he was not involved in an abusive relationship and did not need shelter. Certain groups are critical of the smaller amount of resources available to men in the United States and across the world. However, other sources dispute the view that male-only refuges are wanted or needed by most male victims, arguing that the issue has been misrepresented out of misogyny rather than genuine concern for male victims. The Istanbul Convention, for example, states that the creation of women's shelters is not discriminatory.\n\nSome shelters do permit access today, including the Domestic Abuse Project (DAP) of Delaware County which offers services to both sexes. According to their own reports, around three percent of DAP supported individuals have been men. In the United Kingdom, 100 places were opened to house male victims of domestic violence in Northamptonshire, or to house families barred from other shelters, such as women with older male children. In Canada, approximately 8 percent of women's shelters are also open to adult men.\n\nWomen's shelters in the United States are supported at a state and national level. Over 50% of the funding offered at the state level, however, comes from the federal government through grants. Services are generally administered through Domestic Violence Intervention Programs (DVIPs) funded by the Family Violence Services Act, the Victims of Crime Act of 1984, and the Violence Against Women Act. Various non-profits also contribute to the services offered and provide a national voice for the issue. Examples include the National Network to End Domestic Violence which represents fifty-six U.S. states and territories, the National Organization for Victim Assistance, and local United Ways.\n\nReports show that on any day over 5,000 women are unable to use services because of a lack of funding or space. Many states have also cut their funds for women's shelters. In 2009, Governor Schwarzenegger of California cut $16 million in state funding to domestic violence programs because of the state's budget deficit. In late 2011 Washington governor Christine Gregoire released a budget proposal stripping all state funding for domestic violence and women's shelters across Washington State. These types of budget cuts caused several shelters to close their doors, leaving women with no safe haven to escape Intimate partner violence. Local communities are now also taking it upon themselves to create a safe place for domestic violence refugees. In Grand Forks, British Columbia, a small community of less than 3,600, people organized the Boundary Women's Coalition, to support their local women's shelter.\n\nMany grants help fund women's shelters in the United States.\nWomen often suffer lasting mental conditions from their abuse including anxiety disorders, depression, and posttraumatic stress disorders (PTSD). Since women in shelters have more likely experienced severe physical and mental abuse than those who do not utilize these services, they are also more likely to experience PTSD. In fact, a national organizational survey compiled four separate studies of female support group or shelter users and reported PTSD rates between 45% and 84% (Astin, Lawrence, Pincus, & Foy, 1990; Houskamp & Foy,1991; Roberts, 1996a; Saunders, 1994). These emotional and mental consequences have an effect on women's career opportunities and ability to function in normal life. Women's shelters try to counteract these effects as well as prevent future instances of abuse. However, PTSD can prohibit women from utilizing shelter resources effectively.\n\nShelter utilization may lead to the better functioning of survivors and fewer reports of abuse in the short term. Research that studied 3,410 residents of 215 domestic violence across the United States linked longer shelter stays with increased well-being and better help-seeking behaviors. The latter is a result of increased knowledge about services and options available to women in vulnerable positions as well as increased empowerment. This may indicate that transition services and longer residential offerings are more valuable.\n\nMany women report re-abuse after leaving a shelter. A sample study done by Bybee and Sullivan, which analyzed data from 124 victims who utilized shelters, found no positive effect on re-abuse three years after shelter use. Additionally, with current resource restraints in the United States, standard shelters do not provide the PTSD or psychotherapeutic treatments necessary for full support. They also have issues with under-serving the community because of a shortage of funded staff, a lack of bilingual staff, and inadequate facilities.\n\nShelters in Europe are similarly limited and only eight countries fulfill the minimum standards for shelters set by the Istanbul Convention. Another criticism of the shelters in Europe is that they have strict age limits that keep male children out and certain shelters discriminate against women from other countries or who identify as lesbian or transgender.\n\n\n"}
{"id": "29584019", "url": "https://en.wikipedia.org/wiki?curid=29584019", "title": "Word family", "text": "Word family\n\nA word family is the base form of a word plus its inflected forms and derived forms made from affixes. In the English language, inflectional affixes include third person -\"s\", verbal \"-ed\" and \"-ing\", plural -\"s\", possessive -\"s\", comparative -\"er\" and superlative -\"est\". Derivational affixes include -\"able, -er, -ish, -less, -ly, -ness, -th, -y, non-, un-, -al, -ation, -ess, -ful, -ism, -ist, -ity, -ize/-ise, -ment, in-\". The idea is that a base word and its inflected forms support the same core meaning, and can be considered learned words if a learner knows both the base word and the affix.\nBauer and Nation proposed seven levels of affixes.\n\n"}
{"id": "4205520", "url": "https://en.wikipedia.org/wiki?curid=4205520", "title": "World Conference against Racism 2001", "text": "World Conference against Racism 2001\n\nThe 2001 World Conference against Racism (WCAR), also known as Durban I, was held at the Durban International Convention Centre in Durban, South Africa, under UN auspices, from 31 August to 8 September 2001.\n\nThe conference covered several controversial issues, including redress for transatlantic slavery and the Second-class citizenry issue in Palestine-Israel. The language of the final Declaration and Programme of Action produced by the conference was strongly disputed in these areas, both in the preparatory meetings in the months that preceded the conference and during the conference itself.\n\nTwo delegations, the United States and Israel, withdrew from the conference over objections to a draft document equating Zionism with racism. The final Declaration and Programme of Action did not contain the text that the U.S. and Israel had objected to, that text having been voted out by delegates in the days after the U.S. and Israel withdrew.\n\nIn parallel to the conference, a separately held NGO Forum also produced a Declaration and Programme of its own, that was not an official Conference document, which contained language relating to Israel that the WCAR had voted to exclude from its Declaration, and which was criticized by then United Nations High Commissioner for Human Rights Mary Robinson and many others.\n\nThe NGO Forum ended in discord. Mary Robinson lost the support of the United States in her office of High Commissioner, and many of the potential political aftereffects of the conference were annulled by the September 11, 2001 attacks. The attacks took place just three days after the conference ended, entirely eclipsing it in the news, and significantly affecting international relations and politics. The conference was followed by the 2009 Durban II conference in Geneva, which was boycotted by ten western countries. A commemorative Durban III conference in September 2011 in New York has also drawn significant criticism and was boycotted by 14 western countries.\n\nThe conference was authorized by United Nations General Assembly Resolution #52/111. Prior to the conference various preparatory meetings (PrepComs) were held in order to identify conference themes and to create initial drafts of the Declaration and Programme of Action. These PrepComs encountered difficulties from the start.\n\nThe first problem was the question of what the conference theme was to be. The Western European states, along with the United States, Canada, Australia, New Zealand, and Japan, all wanted the conference objectives to be those given in the authorizing resolution. The Africa Group, the Latin American states, and the Caribbean states wanted the conference objectives to go beyond what was in the resolution, and include items dealing with regional, national, and international measures for compensation for colonialism and slavery.\n\nPrior to the conference, there were also four Regional Conferences, in Strasbourg, Santiago, Dakar, and Tehran.\n\nThe Durban Declaration and Programme of Action was adopted by the governmental delegates attending the Conference at the International Convention Centre.\n\nThe issue of Compensation for Colonialism and Slavery is addressed in ¶ 13, ¶ 14, ¶ 15, and ¶ 29 of the Declaration. It was one of the most controversial issues debated at the conference, one that had the potential to derail the entire conference. It was dealt with cleverly in the Declaration, containing rhetoric that satisfied the African bloc, without applying retroactively against the descendants of colonizers the principle of crimes against humanity and without establishing a clear responsibility for reparations on the parts of former colonial states.\n\nThe wording of the Declaration struck a delicate balance. Whilst acknowledging historic and contemporary practices of slavery and the slave trade as morally outrageous, and something that would be a crime against humanity today, it did not apply that legal principle to an era before the principle actually existed.\n\nOne of the contentious points at the conference related to the issue was that of apartheid. During the preparatory processes of the conference, South Africa stressed that it did not want to link compensation to apartheid. At the Tehran Regional Conference, a paragraph making such a link was inserted by Asian governments. This was deleted at the request of the South African delegation. Linking compensation to apartheid had the potential to polarize South African society, and produce the same effects as had the controversial land reform programmes in Zimbabwe. Domestic political pressures, and the aim of the South African government to foster reconciliation within the country, made South Africa's position difficult.\n\nThe issue of compensation was thus a complex one, that was exacerbated by the President of Senegal, Abdoulaye Wade, calling campaigns to demand compensation for colonialism and slavery \"childish\".\n\nThe earliest point at which the issue of compensation caused problems was during preparations in May 2001, when delegations came to the decision of where to place it on the agenda. At the time, the fourth item on the agenda, out of five items, was \"Provision of effective remedies, recourses, redress, compensatory, and other measures, at the national, regional, and international levels\". The European Union, represented by Portugal, wanted to place the entire language in brackets. The United States just wanted to place the word \"compensatory\" in brackets. The African Group, Armenia, and Cuba strongly objected to both proposals, with the African Group stating that if the topic were placed in brackets, they would move for the entire text to be placed in brackets also. In the end, the U.S. proposal was adopted, with the addition of a statement in the report indicating the different perspectives on the exact meaning of those brackets. Western European states discussed informally amongst themselves, outside of the formal preparatory proceedings, what measures and levels of non-cöoperation they might adopt if the issue of compensation gained momentum at Durban itself.\n\nBefore the conference, the debate over compensation was seen as dealing with the transatlantic slave trade, and the colonization of Africa by Europeans, thus pitting Western European states (including the former colonial powers of Belgium, France, Germany, Italy, the Netherlands, Portugal, Spain, and the United Kingdom) and the United States against the African Group. The African Group was supported by Asia, Latin America, and the Caribbean.\n\nPrior to the conference, on 2001-08-03, the African Group circulated a Non-Paper on the \"Injustices of the Past\", containing strong language but a generally moderate position. To this paper the E.U. responded, on 2001-08-08, with a Non-Paper of its own that addressed most, but not all, of the issues in the African Group's paper. The United States circulated a Non-Paper as well, but this turned out to be less helpful than the E.U. one.\n\nThe African Group circulated a second Non-Paper on 2001-09-03 that was substantially stronger than its earlier one, with language shifts from \"debt cancellation\" to \"immediate and unconditional cancellation of debt\", emphasis upon crimes against humanity, and calls for reparation (something which the earlier paper had not included in part because of a U.S. demand, made at a preparatory meeting in Geneva, that such language be excluded from the text).\n\nSeveral members of the African Group openly opposed calling for reparations. President Wade stated \"We still suffer the effects of slavery and colonialism, and that cannot be evaluated in monetary terms. I find that not only absurd, but insulting.\". Similarly, South Africa was more interested in devoting time and effort to more pragmatic ends, such as Western aid for the Millennium Africa Recovery Programme, which would be more palatable to the U.S. and the E.U.\n\nA consensus on the reparations issue was reached by late August. On 2001-08-24 President of the United States George W. Bush announced in a press conference that \"the reparations issue has been solved — at least the last information I had was that the issue has … looks like it has been resolved\", albeit that news media at the time failed to realize the significance of the comment. The U.S. walked out of the conference a few weeks later.\n\nDuring preparatory meetings in Geneva, text that linked Zionism to racism was placed in brackets, with the expectation that it would be replaced by text that referred to violations of the rights of Palestinians. The U.S. had already threatened to boycott the conference should the conference draft documents include text that could be in any way interpreted as linking Zionism to racism. Mary Robinson had also said that regional political conflicts should not be imposed upon the agenda of the conference. The Australian, the Canadian, and some European delegations shared the U.S. view.\n\nThe Arab position was stated by the Secretary General of the Arab League, Amr Moussa: \"Israel's racist actions against the Palestinian people have to be dealt with in an international conference that aims to eradicate racism. Arab countries are not expecting the Durban conference to be a venue for dealing with the Arab- Israeli peace process, but they certainly expect that the Israeli racist practices against the Palestinian people will not be overlooked.\"\n\nThe Arab delegates were not insistent upon language that specifically equated Zionism with racism. It had been suggested that they were trying to revive United Nations General Assembly Resolution 3379 (issued 1975, annulled 1991) which stated that \"Zionism is a form of racism.\". Their position was that they were, rather, trying to underline that the actions being committed by Israel against Palestinians were racist.\n\nThis stance was in part influenced by the U.S. threat of boycott, which would have made it impractical to insist upon harsh language condemning Israel or equating the suffering of the Palestinians with that of holocaust victims. According to one Arab diplomat, no Arab state except for Syria had insisted upon any language linking Israel to racist practices.\n\nAt the start of the Geneva meeting, text had been presented that comprised six bracketed paragraphs dealing with \"Zionist racist practices\", including an appeal for Israel \"to revise its legislation based on racial or religious discrimination such as the law of return and all the policies of an occupying power which prevent the Palestinian refugees and displaced persons from returning to their homes and properties\", and a suggestion for the need \"to bring the foreign occupation of Jerusalem by Israel together with all its racist practices to an end\".\n\nBy the end of the meeting, all of this text had either been removed or toned down. One such phrase removed was a mention of \"holocausts\" suffered by other peoples, which had been seen as an affront to the memory of the Jewish victims of the Nazi holocaust. South African diplomats had already told Arab and Muslim countries that they would have to offer text that could describe the current situation without using such language as \"ethnic cleansing practices against Palestinians\".\n\nNonetheless, the United States, objecting to the remaining text, decided to send a low-level delegation, headed by Ambassador Michael Southwick, to the Conference, rather than have United States Secretary of State Colin Powell attend himself. German officials criticized this decision, and the United States Congressional Black Caucus urged him to attend. The Anti-Defamation League urged him to stay away.\n\nOn September 3, 2001, after four days of deadlocked negotiations that did not reach agreement on language, the United States and Israeli delegations withdrew from the conference. Both United States Secretary of State Colin Powell and Foreign Affairs Minister of Israel Shimon Peres stated that this was done with regret.\n\nThis decision was criticized by several people, including Jesse Jackson and President of South Africa Thabo Mbeki, both of whom stated their opinions that it had been a mistake by the United States to send a low-level delegation to the conference in the first place, and Amnesty International, which stated that the U.S. was \"letting down victims of racism\". Jackson had been involved in earlier attempts to create compromise language.\n\nThe low-level U.S. delegation had kept a low profile throughout conference proceedings until that point, with delegates working quietly in sub-committee meetings, without (unlike in earlier conferences) giving news briefings or off the record statements to journalists, to change the text of the draft declaration, to make it less forceful and less specific against Israel, and to bring it into line with U.S. foreign policy goals with respect to the International Criminal Court (see United States and the International Criminal Court) by removing language that strengthened the ICC.\n\nThe draft documents had stated \"deep concern\" at the \"increase of racist practices of Zionism and anti-Semitism\" and talked of the emergence of \"movements based on racism and discriminatory ideas, in particular the Zionist movement, which is based on racial superiority\". Alternative proposals, which the U.S. had supported, from Norway, acting as a mediator, and Canada were rejected by Israel.\n\nDespite Colin Powell's denunciation of the \"hateful language\" that \"singles out only one country in the world, Israel, for censure and abuse\" in the draft text and U.S. delegate Tom Lantos's statement that the conference had been \"wrecked by Arab and Islamic extremists\", some saw the U.S. delegation's withdrawal as not being entirely related to the language on Israel, but attributed it also, in part, to a reluctance on the part of the U.S. to address the issue of slavery.\n\nThe withdrawal of, the U.S. and Israel was taken as a warning by many delegates that there was a strong possibility of Canada and the E.U. states withdrawing as well if no compromise was reached. Several reports had the Europeans staying on solely in order to help South Africa salvage the Conference. After the withdrawal, senior conference officials became highly involved in the rewriting of the Declaration — something that critics maintained they should have also been doing before that point.\n\nIn the end, the Conference delegates voted to reject the language that implicitly accused Israel of racism, and the document actually published contained no such language.\n\nSeveral countries were unhappy with the final text's approach to the subject, but all for different reasons. Syria and Iran were unhappy because their demands for the language about racism and Israel had been rejected by the Conference, the latter continuing its insistence that Israel was a racist state. Australia was unhappy with the process, observing that \"far too much of the time at the conference [had been] consumed by bitter divisive exchanges on issues which have done nothing to advance the cause of combating racism\". Canada was also unhappy.\n\nThe language of the final text was carefully drafted for balance. The word \"diaspora\" is used four times, and solely to refer to the African Diaspora. The document is at pains to maintain a cohesive identity for everyone of African heritage as a victim of slavery, even including those who may have more European than African ancestors. The \"victim\" or \"victims\" of racism and slavery (the two words occurring 90 times in the document) are defined in only the most general geographic terms. The word \"Jewish\" is only used once, alongside \"Muslim\" and \"Arab\", and \"anti-Semitism\" is only used twice, once alongside its assumed counterpart of \"Islamophobia\" and once alongside \"anti-Arabism\". The difficulty that this generates is that it is politically impossible to act when the 219 calls for action in the Programme are couched in such generalities that only the \"countless human beings\" that the document explicitly talks of can be identified.\n\nSeparate from the actual Conference itself was an NGO Forum, held in the nearby Kingsmead Stadium in Durban, that ran from 2001-08-28 to 2001-09-01. This was a forum of 3,000 NGOs, attended by 8,000 representatives. It, too adopted a Declaration. However, this was not an official document of the WCAR and was not issued as such.\n\nThe Forum's proceedings were highly disorganized, with several NGO delegates walking out of the Forum, to the jeers of other delegates, and ending in discord; and the resultant declaration had 62 paragraphs of introduction, followed by a document that appeared to commentators as being the result of every lobby putting its pet aversions in. It described Israel as a \"racist, apartheid state\" that was guilty of \"racist crimes including war crimes, acts of genocide and ethnic cleansing\". The document was not intended to be presented to the Conference, although a copy of it was intended to be handed over, as a symbolic gesture, to the Conference secretary-general, Mary Robinson, at the conclusion of the Forum. Ms Robinson refused to accept the document, citing concerns over its language. In a later interview she said of the whole conference that \"there was horrible anti-Semitism present — particularly in some of the NGO discussions. A number people said they've never been so hurt or so harassed or been so blatantly faced with an anti-Semitism.\" The Palestinian Solidarity Committee of South Africa reportedly distributed copies of the antisemitic forgery \"The Protocols of the Elders of Zion\".\n\nCritics described the description of Israel as apartheid as the \"Durban Strategy\". They claim that this comparison was made with the intention of causing and encouraging divestment from and boycott of Israel.\n\nThe NGO Forum was attended by U.S. NGOs, with financial support from the Rockefeller Foundation, the MacArthur Foundation, and the Charles Stewart Mott Foundation. The Ford Foundation provided USD10 million in support to the WCAR and the NGO Forum. These NGOs provided research assistance at the Forum and helped to develop declarations and resolutions that dealt with the issue of compensation for slavery.\n\nThe resolutions adopted by the Forum dealing with reparations for slavery dealt only with the transatlantic slave trade, and did not mention the traffic in African slaves to Islamic lands in the Middle East. The Forum also called upon the United States to ratify all major human rights treaties that had already been ratified.\n\nOne such treaty was the UN Convention on the Elimination of Racial Discrimination (CERD), which the U.S. had ratified in 1994, but (per the Supremacy Clause of Article Six of the United States Constitution, which does not permit treaties to override the Constitution) had attached a reservation that its ratification did not accept treaty requirements that were incompatible with the Constitution of the United States. The NGOs, including Human Rights Watch and Amnesty International, demanded that U.S. drop its reservations and \"comply\" with the treaty. The U.S. Department of State had noted specifically that CERD's restrictions on freedom of speech and freedom of assembly were incompatible with the First Amendment to the Constitution of the United States. The United States was far from the only such country to do so, however. Incompatibility of the treaty with national constitutions, including the freedoms of assembly and speech guaranteed by those constitutions, is also noted by Antigua and Barbuda, the Bahamas, Barbados, France, Guyana, Jamaica, Japan, Nepal, Papua New Guinea, Switzerland, and Thailand. Several, including France, Ireland, Italy, Japan, Malta, Monaco, Nepal, the United Kingdom, note that they consider the provisions of the treaty to be restricted by and subject to the freedoms of speech and assembly set forth in the Universal Declaration of Human Rights.\n\nOne commentator noted that in order to comply with the interpretation of CERD created by the NGOs at the Forum, the United States would have to \"turn its political and economic system, together with their underlying principles, upside down — abandoning the free speech guarantees of the Constitution, bypassing federalism, and ignoring the very concept of majority rule, since practically nothing in the NGO agenda is supported by the [U.S.] electorate\", stating that these NGOs were \"a new challenge to liberal democracy\" that contested the principles of individual rights, democratic representation, and national citizenship, along with contesting the very idea of a liberal democratic nation-state.\n\nOther NGO demands included demands for:\n\nTom Lantos assigns the blame for the withdrawal of the U.S. in part to the radicalism of many of the NGOs at the NGO Forum, to an inadequate response thereto by U.S.-based NGOs, and to the reluctance of the U.S.'s European allies to take a strong stand.\n\nThe Conference was largely overshadowed in the news and in international affairs by the September 11, 2001 attacks, which occurred 3 days after the Conference ended.\n\nAs a consequence of the Conference, the United States did not support the continuation of Mary Robinson as United Nations High Commissioner for Human Rights, where once U.S. President Bill Clinton had called her a \"splendid choice\" for the post and the U.S. had considered her its favorite candidate for the job. She stepped down from the post in September 2002.\n\nMany faults were attributed to Ms Robinson, with cumulative effect on the U.S. position. Some people stated that she lacked mediation and bureaucratic experience, and thus was unable to resolve sensitive issues at the Conference. News reports attributed her differences with the U.S. to four things: First, her views on the Israel-Palestine conflict differed from U.S. policy. Second, the U.S. did not approve of the detached way in which she acted as secretary-general to the Conference. Third, she had openly criticized the U.S. on various matters including the treatment of prisoners at Camp X-Ray, the \"unsigning\" of the Rome Statute of the International Criminal Court by the U.S., and the administration of capital punishment in the United States. Fourth, she had opposed U.S. calls to reform the election process of the United Nations Commission on Human Rights.\n\nTom Lantos himself did not assign sole or even primary blame to Robinson for the breakdown of U.S. relations with the conference. That he assigned to the NGOs, as aforementioned, and to the member states of the Organisation of the Islamic Conference. Moreover, several people have defended Robinson's secretary-generalship of the conference.\n\nSeveral NGOs, including Human Rights Watch, Amnesty International, and the Lawyers Committee for Human Rights, disassociated themselves from the language of the NGO Forum's Declaration that dealt with Israel and with Jews.\n\nIt seems unlikely to analysts that the United States will support another WCAR. However, the Declaration and Programme of Action did make provision for follow-up mechanisms. Mary Robinson stated in her closing address that the Conference was intended to be a beginning, not an end. Dr. Manning Marable, of Columbia University in New York, pointed out that one of the objectives of the Conference was to increase coordination in human rights activities, and to strengthen networks amongst those combating racism; and as such the actions of governments in response to the Conference are not the sole intended outcomes — actions by civil society and non-governmental agencies are also required.\n\nOne such follow up provision is for national governments to provide the Office of the United Nations High Commissioner for Human Rights with reports on their actions towards implementing the recommendations in the Programme of Action. Another is for the Secretary General of the United Nations to appoint an expert body with the remit of following up on implementation. A third is a call for the establishment of a database of practical means for addressing racism, racial discrimination, and related intolerance.\n\nA Permanent Memorial Trust Fund has also been established for the creation of a memorial at the New York United Nations site. The sculpture, to be titled the Permanent Memorial to the Victims of Slavery and the Transatlantic Slave Trade, or the UN Slavery Memorial, is set to be completed in 2012.\n\nBy resolution #2002/68 of the United Nations Commission on Human Rights an Intergovernmental Working Group on the Effective Implementation of the Durban Declaration and Programme of Action was established, which held its first meeting in January 2003 and which meets on an annual basis.\n\nIn resolution #61/149 of the United Nations General Assembly, passed in 1996, a Durban Review Conference was called. The conference took place in 2009, however, a number of countries expressed concern as a result of the 2001 conference. Some countries, including Australia, Canada, Germany, Israel, Italy, the Netherlands, New Zealand, Poland, and the United States, boycotted the conference. The Czech Republic discontinued its attendance on the first day, and twenty-three other European Union countries sent low-level delegations. In an 18 April 2009 speech, President Barack Obama announced the United States' boycott of the 2009 Durban Review Conference, reaffirming the country's opposition to language perceived as anti-Israel and anti-Western.\n\nUnited Kingdom and other European countries remain undecided. On 17 February 2009, Foreign Office Minister Lord Malloch-Brown said: \"If we can’t go forward now, we will withdraw. I was at the first conference. I have never seen such a disgraceful event in quite a long international life.\"\n\nThe Institute for Global Jewish Affairs was founded, in part, as a response to the perceived Anti-Semitism of the Durban conference.\n\nBernard-Henri Lévy credits the conference with being one of the inspirations for his book, \"\".\n\n\n\n"}
{"id": "247414", "url": "https://en.wikipedia.org/wiki?curid=247414", "title": "Y-chromosomal Adam", "text": "Y-chromosomal Adam\n\nIn human genetics, the Y-chromosomal most recent common ancestor (Y-MRCA, informally known as Y-chromosomal Adam) is the most recent common ancestor (MRCA) from whom all currently living men are descended patrilineally. The term Y-MRCA reflects the fact that the Y chromosomes of all currently living males are directly derived from the Y chromosome of this remote ancestor. The analogous concept of the matrilineal most recent common ancestor is known as \"Mitochondrial Eve\" (mt-MRCA, named for the matrilineal transmission of mtDNA), the most recent woman from whom all living humans are descended matrilineally. As with \"Mitochondrial Eve\", the title of \"Y-chromosomal Adam\" is not permanently fixed to a single individual, but can advance over the course of human history as paternal lineages become extinct.\n\nEstimates of the time when Y-MRCA lived have also shifted as modern knowledge of human ancestry changes. In 2013, the discovery of a previously unknown Y-chromosomal haplogroup was announced,\nwhich resulted in a slight adjustment of the estimated age of the human Y-MRCA.\n\nBy definition, it is not necessary that the Y-MRCA and the mt-MRCA should have lived at the same time.\nWhile estimates as of 2014 suggested the possibility that the two individuals may well have been roughly contemporaneous (albeit with uncertainties ranging in the tens of thousands of years), the discovery of archaic Y-haplogroup has pushed back the estimated age of the Y-MRCA beyond the most likely age of the mt-MRCA. As of 2015, estimates of the age of the Y-MRCA range around 200,000 to 300,000 years ago, roughly consistent with the emergence of anatomically modern humans.\n\nY-chromosomal data taken from a Neanderthal from El Sidrón, Spain produced a Y-T-MRCA of 588,000 years ago for neanderthal and \"Homo sapiens\" patrilineages, dubbed \"ante\" Adam and 275,000 years ago for Y-MRCA.\n\nThe Y-chromosomal most recent common ancestor is the most recent common ancestor of the Y-chromosomes found in currently living human males.\n\nDue to the definition via the \"currently living\" population, the identity of a MRCA, and by extension of the human Y-MRCA, is time-dependent (it depends on the moment in time intended by the term \"currently\").\nThe MRCA of a population may move forward in time as archaic lineages within the population go extinct:\nonce a lineage has died out, it is irretrievably lost. This mechanism can thus only shift the title of Y-MRCA forward in time. Such an event could be due to the total extinction of several basal haplogroups.\nThe same holds for the concepts of matrilineal and patrilineal MRCAs: it follows from the definition of Y-MRCA that he had at least two sons who both have unbroken lineages that have survived to the present day. If the lineages of all but one of those sons die out, then the title of Y-MRCA shifts forward from the remaining son through his patrilineal descendants, until the first descendant is reached who had at least two sons who both have living, patrilineal descendants. The title of Y-MRCA is not permanently fixed to a single individual, and the Y-MRCA for any given population would himself have been part of a population which had its own, more remote, Y-MRCA.\n\nAlthough the informal name \"Y-chromosomal Adam\" is a reference to the biblical Adam, this should not be misconstrued as implying that the bearer of the chromosome was the only human male alive during his time. \nHis other male contemporaries may also have descendants alive today, but not, by definition, through solely patrilineal descent; in other words, none of them have an unbroken male line of descendants (\"son's son's son's … son\") connecting them to currently living people.\n\nBy the nature of the concept of most recent common ancestors, these estimates can only represent a \"terminus ante quem\" (\"limit before which\"), until the genome of the entire population has been examined (in this case, the genome of all living humans).\n\nEstimates on the age of the Y-MRCA crucially depend on the most archaic known haplogroup extant in contemporary populations. , this is haplogroup A00 (discovered in 2013). Age estimates based on this published during 2014–2015 range between 160,000 and 300,000 years, compatible with the time of emergence and early dispersal of \"Homo sapiens\".\n\nIn addition to the tendency of the title of Y-MRCA to shift forward in time, the estimate of the Y-MRCA's DNA sequence, his position in the family tree, the time when he lived, and his place of origin, are all subject to future revisions.\n\nThe following events would change the estimate of who the individual designated as Y-MRCA was:\n\nThe time when Y-MRCA lived is determined by applying a molecular clock to human Y-chromosomes. In contrast to mitochondrial DNA (mtDNA), which has a short sequence of 16,000 base pairs, and mutates frequently, the Y chromosome is significantly longer at 60 million base pairs, and has a lower mutation rate. These features of the Y chromosome have slowed down the identification of its polymorphisms; as a consequence, they have reduced the accuracy of Y-chromosome mutation rate estimates.\n\nMethods of estimating the age of the Y-MRCA for a population of human males whose Y-chromosomes have been sequenced are based on applying the theories of molecular evolution to the Y chromosome. Unlike the autosomes, the human Y-chromosome does not recombine often with the X chromosome during meiosis, but is usually transferred intact from father to son; however, it can recombine with the X chromosome in the pseudoautosomal regions at the ends of the Y chromosome. Mutations occur periodically within the Y chromosome, and these mutations are passed on to males in subsequent generations.\n\nThese mutations can be used as markers to identify shared patrilineal relationships. Y chromosomes that share a specific mutation are referred to as haplogroups. Y chromosomes within a specific haplogroup are assumed to share a common patrilineal ancestor who was the first to carry the defining mutation. (This assumption could be mistaken, as it is possible for the same mutation to occur more than once.) A family tree of Y chromosomes can be constructed, with the mutations serving as branching points along lineages. The Y-MRCA is positioned at the root of the family tree, as the Y chromosomes of all living males are descended from his Y chromosome.\n\nResearchers can reconstruct ancestral Y chromosome DNA sequences by reversing mutated DNA segments to their original condition. The most likely original or ancestral state of a DNA sequence is determined by comparing human DNA sequences with those of a closely related species, usually non-human primates such as chimpanzees and gorillas. By reversing known mutations in a Y-chromosome lineage, a hypothetical ancestral sequence for the MRCA, Y-chromosomal Adam, can be inferred.\n\nDetermining the Y-MRCA's DNA sequence, and the time when he lived, involves identifying the human Y-chromosome lineages that are most divergent from each other—the lineages that share the fewest mutations with each other when compared to a non-human primate sequence in a phylogenetic tree. The common ancestor of the most divergent lineages is therefore the common ancestor of all lineages.\n\nEarly estimates of the age for the Y-MRCA published during the 1990s ranged between roughly 200 and 300 kya, \nSuch estimates were later substantially revised downward, as in Thomson \"et al.\" 2000, which proposed an age of about 59,000.\nThis date suggested that the Y-MRCA lived about 84,000 years after his female counterpart mt-MRCA (the matrilineal most recent common ancestor), who lived 150,000–200,000 years ago.\nThis date also meant that Y-chromosomal Adam lived at a time very close to, and possibly after, the migration from Africa which is believed to have taken place 50,000–80,000 years ago.\nOne explanation given for this discrepancy in the time depths of patrilineal vs. matrilineal lineages was that females have a better chance of reproducing than males due to the practice of polygyny. When a male individual has several wives, he has effectively prevented other males in the community from reproducing and passing on their Y chromosomes to subsequent generations. On the other hand, polygyny does not prevent most females in a community from passing on their mitochondrial DNA to subsequent generations. This differential reproductive success of males and females can lead to fewer male lineages relative to female lineages persisting into the future. These fewer male lineages are more sensitive to drift and would most likely coalesce on a more recent common ancestor. This would potentially explain the more recent dates associated with the Y-MRCA.\n\nThe \"hyper-recent\" estimate of significantly below 100 kya was again corrected upward in studies of the early 2010s, which ranged at about 120 kya to 160 kya.\nThis revision was due to the discovery of additional mutations and the rearrangement of the backbone of the Y-chromosome phylogeny following the resequencing of Haplogroup A lineages.\nIn 2013, Francalacci \"et al.\" reported the sequencing of male-specific single-nucleotide Y-chromosome polymorphisms (MSY-SNPs) from 1204 Sardinian men, which indicated an estimate of 180,000 to 200,000 years for the common origin of all humans through paternal lineage. or again as high as 180 to 200 kya.\nAlso in 2013, Poznik \"et al.\" reported the Y-MRCA to have lived between 120,000 and 156,000 years ago, based on genome sequencing of 69 men from 9 different populations.\nIn addition, the same study estimated the age of Mitochondrial Eve to about 99,000 and 148,000 years. As these ranges overlap for a time-range of 28,000 years (148 to 120 kya), the results of this study have been cast in terms of the possibility that \"Genetic Adam and Eve may have walked on Earth at the same time\" in the popular press.\n\nThe announcement of yet another discovery of a previously unknown lineage, haplogroup A00, in 2013, resulted in another shift in the estimate for the age of Y-chromosomal. Elhaik et al. (2014) dated it to between 163,900 and 260,200 years ago (95% CI). Karmin et al. (2015) dated it to between 192,000 and 307,000 years ago (95% CI). The same study reports that non-African populations converge to a cluster of Y-MRCAs in a window close to 50kya (out-of-Africa migration), and an additional bottleneck for non-African populations at about 10kya, interpreted as reflecting cultural changes increasing the variance in male reproductive success (i.e. increased social stratification) in the Neolithic.\n\nInitial sequencing (Karafet et al., 2008) of the human Y chromosome suggested that two most basal Y-chromosome lineages were Haplogroup A and Haplogroup BT. Haplogroup A is found at low frequencies in parts of Africa, but is common among certain hunter-gatherer groups. Haplogroup BT lineages represent the majority of African Y-chromosome lineages and virtually all non-African lineages. Y-chromosomal Adam was represented as the root of these two lineages. Haplogroup A and Haplogroup BT represented the lineages of the two male descendants of Y-chromosomal Adam.\n\nCruciani et al. 2011, determined that the deepest split in the Y-chromosome tree was found between two previously reported subclades of Haplogroup A, rather than between Haplogroup A and Haplogroup BT.\nSubclades A1b and A1a-T are now believed to descend directly from the root of the tree and now represent the lineages of Y-chromosomal Adam's two sons. The rearrangement of the Y-chromosome family tree implies that lineages classified as Haplogroup A do not necessarily form a monophyletic clade. Haplogroup A therefore refers to a collection of lineages that do not possess the markers that define Haplogroup BT, though Haplogroup A includes the most distantly related Y chromosomes.\n\nThe M91 and P97 mutations distinguish Haplogroup A from Haplogroup BT. Within Haplogroup A chromosomes, the M91 marker consists of a stretch of 8 T nucleobase units. In Haplogroup BT and chimpanzee chromosomes, this marker consists of 9 T nucleobase units. This pattern suggested that the 9T stretch of Haplogroup BT was the ancestral version and that Haplogroup A was formed by the deletion of one nucleobase. Haplogroups A1b and A1a were considered subclades of Haplogroup A as they both possessed the M91 with 8Ts.\n\nBut according to Cruciani et al. 2011, the region surrounding the M91 marker is a mutational hotspot prone to recurrent mutations. It is therefore possible that the 8T stretch of Haplogroup A may be the ancestral state of M91 and the 9T of Haplogroup BT may be the derived state that arose by an insertion of 1T. This would explain why subclades A1b and A1a-T, the deepest branches of Haplogroup A, both possess the same version of M91 with 8Ts. Furthermore, Cruciani et al. 2011 determined that the P97 marker, which is also used to identify Haplogroup A, possessed the ancestral state in Haplogroup A but the derived state in Haplogroup BT.\n\nAs current estimates on TMRCA converge with estimates for the age of anatomically modern humans\nand well predate the Out of Africa migration, geographical origin hypotheses continue to be limited to the African continent.\n\nAccording to Cruciani \"et al.\" 2011, the most basal lineages have been detected in West, Northwest and Central Africa, suggesting plausibility for the Y-MRCA living in the general region of \"Central-Northwest Africa\".\n\nScozzari \"et al.\" (2012) agreed with a plausible placement in \"the north-western quadrant of the African continent\" for the emergence of the A1b haplogroup.\n\nThe revision of Y-chromosomal phylogeny since 2011 has affected estimates for the likely geographical origin of Y-MRCA as well as estimates on time depth. By the same reasoning, future discovery of presently-unknown archaic haplogroups in living people would again lead to such revisions. In particular, the possible presence of between 1% and 4% Neanderthal-derived DNA in Eurasian genomes implies that the (unlikely) event of a discovery of a single living Eurasian male exhibiting a Neanderthal patrilineal line would immediately push back T-MRCA (\"time to MRCA\") to at least twice its current estimate. However, the discovery of a neanderthal Y-chromosome by Mendez \"et al\". suggests the extinction of neanderthal patrilineages, as the lineage inferred from the neanderthal sequence is outside of the range of contemporary human genetic variation. Questions of geographical origin would become part of the debate on Neanderthal evolution from \"Homo erectus\".\n\n"}
