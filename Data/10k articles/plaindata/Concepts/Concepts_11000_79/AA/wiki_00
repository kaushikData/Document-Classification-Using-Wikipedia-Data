{"id": "25247171", "url": "https://en.wikipedia.org/wiki?curid=25247171", "title": "Andrey Efi", "text": "Andrey Efi\n\nAndrey Efi (born Andrey Ivanovich Efimoff (Андрей Иванович Ефимов) in 1960 in St. Petersburg) is a well-known Russian artist, painter and curator. He specializes in paintings, murals, and in graphic and syncretic art, and has also worked on art theory, cinema, literature, and as a teacher.\nIn 2004 Andrey Efi belong the Artconcept festival and then it's revived to ARTZOND festival of conceptual and tendentious arts.\nA member of the art group Foster Brothers, to which Andrei Kolkoutine and Eugeny Lindin are also affiliated, Efi is the author of the theoretical work \"PAINTING - CHARACTERS (sources of origination; interrelation)\", published in Moscow in 1993.\n\nHe has since the mid-1990s been producing concept art, completing over 40 projects, including scripts and animated movies such as the four-minute \"Carefree Princess\", whose soundtrack was composed by the American musician Demetrius Spaneas. Other works include \"Crucifixion dedicated to Bill Gates\" and the installation \"Red Cube\", later reworked as \"White Cube\" and ultimately \"Eternal Cube\", a project that involved 120 litres of vodka. The installations \"Red Cube\", \"White Cube\", \"Eternal Cube\" are all dedicated to Kazimir Malevich, as it’s started after the idea to revolve all flat squares to volumes. His performances have included \"Generation NEXT\", \"Answer to Chernyshevsky\",\"Talking Stove\", \"Ill - Each (dedicated to Lenin)\", \"Between Past and Future\", and others.\n\nEfi's paintings are held at the State Russian Museum (St. Petersburg), and in private galleries in Australia, the United States, Belgium, Canada, the UK, Germany, Russia, and Switzerland.\n\n\n"}
{"id": "42753154", "url": "https://en.wikipedia.org/wiki?curid=42753154", "title": "Art Ludique", "text": "Art Ludique\n\nArt Ludique – The Museum is a 1,200m² museum in the 13th arrondissement of Paris, France, housed in Les Docks, cité de la mode et du design and inaugurated on 16 November 2013.\n\nJean-Jacques Launier founded the Arludik gallery, now run by his wife Diane Launier, before organising an exhibition of works by Moebius and Myazaki at the Monnaie de Paris in 2004, followed by exhibitions on the art of Ice Age and of John Howe created for The Lord of the Rings. He has also written theoretical studies such as “Art Ludique”, published by Sonatine in 2011, and later decided to found a museum dedicated to contemporary art arising from the entertainment industry.\n\nThe museum holds several temporary exhibitions each year. A permanent collection is also being prepared, bringing together the most influential figurative narrative artists of the last few centuries, as well as contemporary artists from around the globe working in comic books, manga, cinema, live animation and videogames around a shared concept: ‘art ludique’. The museum opened with the exhibition “Pixar – 25 years of animation”, created in 2006 by Pixar and the MoMA, before hosting exhibitions dedicated to Marvel Superheroes and to Studio Ghibli's productions.\n\n• Pixar – 25 years of animation: Art Ludique – The Museum was inaugurated with the exhibition \"Pixar – 25 years of animation\". Co-created by Pixar and the New York Museum of Modern Art in 2006, the exhibition opened in Paris on 16 November 2013 after having visited London, Tokyo, Helsinki and Mexico (amongst others). The exhibition brings together over 500 artworks (drawings, sculptures, storyboards…) and two attractions: the Toy Story Zootrope created for the exhibition that gives an illusion of movement, and the Artscape, an immersive panoramic animated film. The exhibition saw over 180,000 visitors.\n\n• The Art of the Marvel Superheroes: this exhibition created for and by the museum opened its doors on 22 March 2014 with 300 pieces of original artwork, featuring the most famous superheroes from the Marvel universe, as well as preparatory drawings, storyboards and props from Marvel films such as Thor’s hammer Mjolnir, Iron Man’s helmet and Captain America’s shield, the original cover art from the first Silver Surfer comic, as well as digital paintings by Ryan Meinerding, artistic supervisor for the majority of Marvel films.\n\n• Studio Ghibli Layout Design : Understanding the secrets of Takahata and Miyazaki animation : this exhibition was open from 4 October 2014 to 1 March 2015. It revealed over 1,300 preparatory drawings from the most famous films of the Japanese studio, among which My Neighbor Totoro, Spirited Away, Grave of the Fireflies or The Wind rises.\n\n• Aardman, Art That Takes Shape: the exhibition of the art of Aardman Animations, creators of characters such as Morph, Wallace and Gromit, Shaun the Sheep, or the stop-motion animation films Chicken Run or The Pirates! In an Adventure with Scientists!, is open from 21 March to 30 August 2015, presenting original sets from movies and shows, 350 drawings, sculptures, storyboards, film extracts, clips and advertisements.\n\n• The Art in Video Games - French inspiration: the exhibition opened on September 25, 2015. It showcases the work of artists from French video game studios, such as Ubisoft, Spiders, Arkane, Osome, and Swing Swing Submarine, presenting more than 800 artworks: drawings and preparatory sketches, watercolors, sculptures and digital paintings. Emmanuel Ethis, in his contribution to the Nouvel Observateur, says that \"a video game is indeed a Total Art, because if it is ludique by nature, it also carries the sovereign ambition of being recorded in a connotated History, rich in correspondences and references to all art forms that preceded it, that we discover thanks to Jean-Jacques Launier, curator of the exhibition dedicated to French inspiration in the Art in the video games.\"\n\n"}
{"id": "3381997", "url": "https://en.wikipedia.org/wiki?curid=3381997", "title": "Australian Transaction Reports and Analysis Centre", "text": "Australian Transaction Reports and Analysis Centre\n\nAustralian Transaction Reports and Analysis Centre (AUSTRAC) is an Australian government financial intelligence agency set up to monitor financial transactions to identify money laundering, organised crime, tax evasion, welfare fraud and terrorism. AUSTRAC was established in 1989 under the \"Financial Transaction Reports Act 1988\" to implement in Australia the recommendations of the Financial Action Task Force on Money Laundering (FATF), which Australia joined in 1990. \n\nAUSTRAC's existence was continued under the \"Anti-Money Laundering and Counter-Terrorism Financing Act 2006\" (Cth) (AML/CTF Act). The AML/CTF Act came into effect on 12 December 2006, and extended the existing monitoring regime to cover financing of terrorism and listed terrorist organisations. Under Division 103 of the \"Criminal Code Act 1995\" (Cth), it is illegal to finance terrorism. The Attorney-General's Department maintains a list of outlawed terror organisations. In 2014 AUSTRAC released a report, \"Terrorism financing in Australia 2014\", which says, \"Terrorism financing poses a serious threat to Australians and Australian interests at home and abroad.\"\n\nAUSTRAC is a member of the Egmont Group of Financial Intelligence Units and an observer in the Camden Assets Recovery Interagency Network and is a member of FATF and the Global Forum on Transparency and Exchange of Information for Tax Purposes. \n\nCertain classes of financial services are required to be reported to AUSTRAC, in particular bank cash transactions (i.e., notes and coins) of A$10,000 or more, as well as suspicious transactions and all international transfers. Reports to AUSTRAC must be made within 10 business days. The information that AUSTRAC collects is available for use by law enforcement, revenue, regulatory, security and other agencies.\n\n\"Reporting entities\" are required to report transactions to AUSTRAC. Transactions which must be reported include:\n\n\nAustralia's cash controls require travellers to report to AUSTRAC when they carry $10,000 or more (or equivalent in a foreign currency) of cash (or equivalent) into or out of Australia, which can be done on forms available from the Border Force at airports and sea ports. The Border Force attempts to detect evasion of this requirement. Airlines are not liable for what their passengers carry. Cross-border movement of bearer negotiable instruments of any amount must also be reported if requested by a Border Force or police officer.\n\nIt's an offence under the Act for anyone to split a transaction into two or more parts with a dominant purpose of avoiding the reporting rules and thresholds.\n\nCertain classes of transactions are exempt, or may be exempted on application. For example, established customers transacting amounts typical of their lawful business, such as for payroll, or retail or vending machine takings, etc. Motor vehicle traders are specifically not eligible for exemption, as are boats, farm machinery and aircraft traders.\n\nUnder the \"Freedom of Information Act 1982\", any person can access records held by AUSTRAC, subject to certain exemptions.\n\nEntities which are required to report transactions to AUSTRAC are called \"reporting entities\", which are specified in the AML/CTF Act. These entities deal in cash, bullion and financial transactions, and include:\n\n\nReporting entities must identify their customers using the 100-point check system. Accounts may only be opened, but can only be operated (i.e., withdrawals made) by an identified customer; an unidentified customer is blocked from making withdrawals. Generally identification can be transferred from one account to another, so that for instance a person once identified does not need to produce documents again when opening a second account at the same institution.\n\nFor banks and similar reporting entities, identification requirements are determined by a risk-based approach, which may differ for each reporting entity.\n\nIt's an offence to open or operate an account with a reporting entity under an alias or false name, punishable by a fine or up to 2 years imprisonment.\n\nThe information that AUSTRAC collects is also available to a large number of government agencies, including:\n\n\nOne prominent attempted evasion of the AUSTRAC rules took place ahead of the Dutch takeover of TNT (see TNT N.V.) in 1999. Simon Hannes was an executive at Macquarie Bank, which was advising TNT, and he bought about $90,000 of TNT call options under the name \"Mark Booth\" to profit when the bid was announced. He was convicted of insider trading but also of two offences under the Financial Transactions Reports Act since he had made multiple cash withdrawals and deposits each just under the $10,000 threshold, apparently to avoid that reporting. His sentence for those transactions was 4 months jail.\n\nIn 2009, an investigation carried out by officers of AUSTRAC and other agencies determined that funds were being sent from Australia for use by the Somalia-based terrorist group, al-Shabaab. Money was remitted, with false names used to obscure the money trail. This investigation lead to the ultimate arrest of the suspects on charges of conspiring to commit a terrorist attack on an Australian army base. \n\nIn 2014, Australian authorities feared that money being transferred from Australia to Somalia could be used for terrorist purposes. In 2015, Australian banks ceased to provide money-transfer facilities to Somalia.\n\nOn 3 August 2017, AUSTRAC took action against the Commonwealth Bank alleging that it did not report cash transactions over $10,000 within the required 10 business day period, or at all. The alleged breach involves over 53,700 transactions over $10,000 through a type of ATM that allowed anonymous cash deposits up to $20,000.\n\n\n"}
{"id": "28357925", "url": "https://en.wikipedia.org/wiki?curid=28357925", "title": "Calculus of moving surfaces", "text": "Calculus of moving surfaces\n\nThe calculus of moving surfaces (CMS) is an extension of the classical tensor calculus to deforming manifolds. Central to the CMS is the Tensorial Time Derivative formula_1 whose original definition was put forth by Jacques Hadamard. It plays the role analogous to that of the covariant derivative formula_2 on differential manifolds. in that it produces a tensor when applied to a tensor.\n\nSuppose that formula_3 is the evolution of the surface formula_4 indexed by a time-like parameter formula_5. The definitions of the surface velocity formula_6 and the operator formula_1 are the geometric foundations of the CMS. The velocity C is the rate of deformation of the surface formula_4 in the instantaneous normal direction. The value of formula_6 at a point formula_10 is defined as the limit\n\nwhere formula_12 is the point on formula_13 that lies on the straight line perpendicular to formula_14 at point P. This definition is illustrated in the first geometric figure below. The velocity formula_6 is a signed quantity: it is positive when formula_16 points in the direction of the chosen normal, and negative otherwise. The relationship between formula_14 and formula_6 is analogous to the relationship between location and velocity in elementary calculus: knowing either quantity allows one to construct the other by differentiation or integration.\n\nThe Tensorial Time Derivative formula_1 for a scalar field F defined on formula_14 is the rate of change in formula_21 in the instantaneously normal direction:\n\nThis definition is also illustrated in second geometric figure.\n\nThe above definitions are \"geometric\". In analytical settings, direct application of these definitions may not be possible. The CMS gives \"analytical\" definitions of C and formula_1 in terms of elementary operations from calculus and differential geometry.\n\nFor analytical definitions of formula_6 and formula_1, consider the evolution of formula_26 given by\n\nwhere formula_28 are general curvilinear space coordinates and formula_29 are the surface coordinates. By convention, tensor indices of function arguments are dropped. Thus the above equations contains formula_26 rather than formula_31.The velocity object formula_32 is defined as the partial derivative\n\nThe velocity formula_6 can be computed most directly by the formula\n\nwhere formula_36 are the covariant components of the normal vector formula_37.\n\nAlso, defining the shift tensor representation of the Surface's Tangent Space formula_38 and the Tangent Velocity as formula_39 , then the definition of the formula_1 derivative for an invariant \"F\" reads\n\nwhere formula_42 is the covariant derivative on S.\n\nFor \"tensors\", an appropriate generalization is needed. The proper definition for a representative tensor formula_43 reads\n\nwhere formula_45 are Christoffel symbols and formula_46 is the surface's appropriate temporal symbols (formula_47 is a matrix representation of the surface's curvature shape operator)\n\nThe formula_1-derivative commutes with contraction, satisfies the product rule for any collection of indices\n\nand obeys a chain rule for surface restrictions of spatial tensors:\n\nChain rule shows that the formula_51-derivative of spatial \"metrics\"\nvanishes\n\nwhere formula_53 and formula_54 are covariant and contravariant metric tensors, formula_55 is the Kronecker delta symbol, and formula_56 and formula_57 are the Levi-Civita symbols. The main article on Levi-Civita symbols describes them for Cartesian coordinate systems. The preceding rule is valid in general coordinates, where the definition of the Levi-Civita symbols must include the square root of the determinant of the covariant metric tensor formula_53.\n\nThe formula_1 derivative of the key surface objects leads to highly concise and attractive formulas. When applied to the covariant surface metric tensor formula_60 and the contravariant metric tensor formula_61, the following identities result\n\nwhere formula_63 and formula_64 are the doubly covariant and doubly contravariant curvature tensors. These curvature tensors, as well as for the mixed curvature tensor formula_47, satisfy\n\nThe shift tensor formula_67 and the normalformula_68 satisfy\n\nFinally, the surface Levi-Civita symbols formula_70 and formula_71 satisfy\n\nThe CMS provides rules for time differentiation of volume and surface integrals.\n"}
{"id": "1692652", "url": "https://en.wikipedia.org/wiki?curid=1692652", "title": "Categories (Aristotle)", "text": "Categories (Aristotle)\n\nThe Categories (Greek Κατηγορίαι \"Katēgoriai\"; Latin \"Categoriae\") is a text from Aristotle's \"Organon\" that enumerates all the possible kinds of things that can be the subject or the predicate of a proposition. They are \"perhaps the single most heavily discussed of all Aristotelian notions\". The work is brief enough to be divided, not into books as is usual with Aristotle's works, but into fifteen chapters.\n\nThe \"Categories\" places every object of human apprehension under one of ten categories (known to medieval writers as the Latin term praedicamenta). Aristotle intended them to enumerate everything that can be expressed without composition or structure, thus anything that can be either the subject or the predicate of a proposition.\n\nThe text begins with an explication of what is meant by \"synonymous,\" or words, what is meant by \"homonymous,\" or words, and what is meant by \"paronymous,\" or (sometimes translated \"derivative\") words.\n\nIt then divides forms of speech as being:\nOnly composite forms of speech can be true or false.\n\nNext, he distinguishes between what is said \"of\" a subject and what is \"in\" a subject. What is said \"of\" a subject describes the kind of thing that it is as a whole, answering the question \"what is it?\" What is said to be \"in\" a subject is a predicate that does not describe it as a whole but cannot exist without the subject, such as the shape of something. The latter has come to be known as inherence.\n\nOf all the things that exist, \n\nThen we come to the categories themselves, whose definitions depend upon these four forms of predication. Aristotle's own text in Ackrill's standard English version is:\nOf things said without any combination, each signifies either substance or quantity or qualification or a relative or where or when or being-in-a-position or having or doing or being-affected. To give a rough idea, examples of substance are man, horse; of quantity: four-foot, five-foot; of qualification: white, grammatical; of a relative: double, half, larger; of where: in the Lyceum, in the market-place; of when: yesterday, last-year; of being-in-a-position: is-lying, is-sitting; of having: has-shoes-on, has-armour-on; of doing: cutting, burning; of being-affected: being-cut, being-burned. (1b25-2a4)\nA brief explanation (with some alternative translations) is as follows:\n\nThe first four are given a detailed treatment in four chapters, doing and being-affected are discussed briefly in a single small chapter, the remaining four are passed over lightly, as being clear in themselves. Later texts by scholastic philosophers also reflect this disparity of treatment.\n\nIn this part, Aristotle sets forth four ways things can be said to be opposed. Next, the work discusses five senses wherein a thing may be considered \"prior\" to another, followed by a short section on simultaneity. Six forms of movement are then defined: generation, destruction, increase, diminution, alteration, and change of place. The work ends with a brief consideration of the word 'have' and its usage.\n\n\n\n\n"}
{"id": "313565", "url": "https://en.wikipedia.org/wiki?curid=313565", "title": "Cognitivism (psychology)", "text": "Cognitivism (psychology)\n\nIn psychology, cognitivism is a theoretical framework for understanding the mind that gained credence in the 1950s. The movement was a response to behaviorism, which cognitivists said neglected to explain cognition. Cognitive psychology derived its name from the Latin \"cognoscere\", referring to knowing and information, thus cognitive psychology is an information-processing psychology derived in part from earlier traditions of the investigation of thought and problem solving.\n\nBehaviorists acknowledged the existence of thinking, but identified it as a behavior. Cognitivists argued that the way people think impacts their behavior and therefore cannot be a behavior in and of itself. Cognitivists later argued that thinking is so essential to psychology that the study of thinking should become its own field. However, cognitivists typically presuppose a specific form of mental activity, of the kind advanced by computationalism.\n\nThe process of assimilating and expanding our intellectual horizon is termed as cognitive development. We have a complex physiological structure that absorbs a variety of stimuli from the environment, stimuli being the interactions which are able to produce knowledge and skills. Parents process knowledge informally in the home while teachers process knowledge formally in school. Knowledge should be pursued with zest and zeal; if not, then learning becomes a burden.\n\nAttention is the first part of cognitive development. Learning takes place when the student gives attention towards the teacher. Interest and effort closely relate to attention. Attention is an active process which involves numerous outside stimuli. The attention of an organism at any point in time involves three concentric circles; beyond awareness, margin, and focus.\n\nCognitive theory mainly stresses the acquisition of knowledge and growth of the mental structure. Cognitive theory tends to focus on conceptualizing the student's learning process: how information is received; how information is processed and organized into existing schema; how information is retrieved upon recall. In other words, cognitive theory seeks to explain the process of knowledge acquisition and the subsequent effects on the mental structures within the mind. Learning is not about the mechanics of what a learner does, but rather a process depending on what the learner already knows (existing information) and their method of acquiring new knowledge (how they integrate new information into their existing schemas). Knowledge acquisition is an activity consisting of internal codification of mental structures within the student's mind. Inherent to the theory, the student must be an active participant in their own learning process. Cognitive approaches mainly focus on the mental activities of the learner like mental planning, goal setting, and organizational strategies (Shell, 1980).\nIn cognitive theories not only the environmental factors and instructional components play an important role in learning. There are additional key elements like learning to code, transform, rehearse, and store and retrieve the information. Learning process includes learner’s thoughts, beliefs, and attitude values(Winna, 1988).\n\nMemory plays a vital role in the learning process. Information is stored within memory in an organised, meaningful manner. Here, teacher and designers play different roles in the learning process. Teachers supposedly facilitate learning and the organization of information in an optimal way. Whereas designers supposedly use advanced techniques (such as analogies and hierarchical relationships) to help learners acquire new information to add to their prior knowledge. Forgetting is described as an inability to retrieve information from memory. Memory loss may be a mechanism used to discard situationally irrelevant information by assessing the relevance of newly acquired information.\n\nAccording to cognitive theory, if a learner knows how to implement knowledge in different contexts and conditions then we can say that transfer has occurred. (Schunk, 1991) Understanding is composed of knowledge - in the form of rules, concepts and discrimination (Duffy and Jonassen, 1991). Knowledge stored in memory is important, but the use of such knowledge is also important. Prior knowledge will be used for identifying similarities and differences between itself and novel information.\n\nCognitive theory mostly explains complex forms of learning in terms of reasoning, problem solving and information processing (Schunk, 1991). Emphasis must be placed on the fact that the goal of all aforementioned viewpoints is considered to be the same - the transfer of knowledge to the student in the most efficient and effective manner possible (Bednar et al., 1991). Simplification and standardization are two techniques used to enhance the effectiveness and efficiency of knowledge transfer. Knowledge can be analysed, decomposed and simplified into basic building blocks. There is a correlation with the behaviorist model of the knowledge transfer environment. Cognitivists stress the importance of efficient processing strategies.\n\nA behaviorist uses feedback (reinforcement) to change the behavior in the desired direction, while the cognitivist uses the feedback for guiding and supporting the accurate mental connections (Thomson, Simon son. & Hargrave, 1992).\nFor different reasons learners' task analyzers are critical to both cognitivists and behaviorists. Cognitivists look at the learner's predisposition to learning (How does the learner activate, maintain and direct his/her learning?) (Thompson et . al., 1992). Additionally, cognitivists examine the learner's 'how to design' instruction that it can be assimilated. (i. e ., what about the learners existing mental structures?) In contrast, the behaviorists look at learners how to determine where the lesson should begin (i. e., At what level the learners are performing successfully?) and what are the most effective reinforcements (i.e., What are the consequences that are most desired by the learner?).\n\nThere are some specific assumptions or principles that direct the instructional design: active involvement of the learner in the learning process, learner control, meta cognitive training (e.g., self-planning, monitoring and revising techniques), the use of hierarchical analyses to identify and illustrate prerequisite relationships (cognitive task analysis procedure), facilitating optimal processing of structuring, organizing and sequencing information (use of cognitive strategies such as outlining, summaries, synthesizers, advance organizers etc.), encouraging the students to make connections with previously learned material, and creating learning environments (recall of prerequisite skills; use of relevant examples, analogies).\n\nCognitive theories emphasize mainly on making knowledge meaningful and helping learners to organizing relate new information to existing knowledge in memory . Instruction should be based on students existing schema or mental structures, to be effective. The organisation of information is connected in such a manner that it should relate to the existing knowledge in some meaningful way. The examples of cognitive strategy are Analogies metaphors. The other cognitive strategies includes the use of framing, outlining the mnemonics, concept mapping, advance organizers and so forth ( West, Farmer, and Wolff,1991). The cognitive theory mainly emphasizes the major tasks of the teacher / designer and includes analyzing that various learning experiences to the learning situation which can impact learning outcomes of different individuals.\nOrganizing and structuring the new information to connect the learners previously acquired knowledge abilities and experiences.\nThe new information is effectively and efficiently assimilated/accommodated within the learners cognitive structure (Stepich and Newby, 1988 ).\n\nCognitivism has two major components, one methodological, the other theoretical. Methodologically, cognitivism adopts a positivist approach and the belief that psychology can be (in principle) fully explained by the use of the scientific method. This is also largely a reductionist goal, with the belief that individual components of mental function (the 'cognitive architecture') can be identified and meaningfully understood. The second is the belief that cognition consists of discrete, internal mental states (representations or symbols) whose manipulation can be described in terms of rules or algorithms..\n\nCognitivism became the dominant force in psychology in the late-20th century, replacing behaviorism as the most popular paradigm for understanding mental function. Cognitive psychology is not a wholesale refutation of behaviorism, but rather an expansion that accepts that mental states exist. This was due to the increasing criticism towards the end of the 1950s of simplistic learning models. One of the most notable criticisms was Chomsky's argument that language could not be acquired purely through conditioning, and must be at least partly explained by the existence of internal mental states.\n\nThe main issues that interest cognitive psychologists are the inner mechanisms of human thought and the processes of knowing. Cognitive psychologists have attempted to shed some light on the alleged mental structures that stand in a causal relationship to our physical actions.\n\nIn the 1990s, various new theories emerged and challenged cognitivism and the idea that thought was best described as computation. Some of these new approaches, often influenced by phenomenological and postmodern philosophy, include situated cognition, distributed cognition, dynamicism, embodied cognition. Some thinkers working in the field of artificial life (for example Rodney Brooks) have also produced non-cognitivist models of cognition. On the other hand, much of early cognitive psychology, and the work of many currently active cognitive psychologists does not treat cognitive processes as computational.\nThe idea that mental functions can be described as information processing models has been criticised by philosopher John Searle and mathematician Roger Penrose who both argue that computation has some inherent shortcomings which cannot capture the fundamentals of mental processes. \n\nAnother argument against cognitivism is the problems of Ryle's Regress or the homunculus fallacy. Cognitivists have offered a number of arguments attempting to refute these attacks.\n\n"}
{"id": "58447333", "url": "https://en.wikipedia.org/wiki?curid=58447333", "title": "Conjunct consonant", "text": "Conjunct consonant\n\nConjunct consonants are a type of letters, used for example in Brahmi or modern Devanagari, to write consonant clusters such as or . Although most of the time, letters are formed by using a simple consonant with the inherent value vowel \"a\" (as with \"k\" , pronounced \"ka\" in Brahmi), or by combining a consonant with an vowel in the form of an diacritic (as with \"ki\" in Brahmi), the usage of conjunct consonant permits the creation of more sophisticated sounds (as with \"kya\" , formed with the consonants k and y assembled verticaly). Conjuncts are often used with loan words. Native words typically use the basic consonant and native speakers know to suppress the vowel.\n\nIn modern Devanagari the components of a conjunct are written left to right when possible (when the first consonant has a vertical stem that can be removed at the right), whereas in Brahmi characters are joined vertically downwards. \n\nSome simple examples of conjunct consonants in Devanagari are: त + व = त्व tva, ण + ढ = ण्ढ ṇḍha, स + थ = स्थ stha, where the vertical stroke of the first letter is simply lost in the combination. Sometimes, conjunct consonants are not clearly derived from the letters making up their components: the conjunct for kṣ is क्ष (क् + ष) and for jñ it is ज्ञ (ज् + ञ). \n\nConjunct consonants are used in many other scripts as well, usually derived from the Brahmi script. In Balinese, conjunct consonants are called \"Haksara Wrehastra\".\n\nConjunct consonant are not limited to Brahmic languages, and can be seen in Navajo for example.\n"}
{"id": "254299", "url": "https://en.wikipedia.org/wiki?curid=254299", "title": "Curry–Howard correspondence", "text": "Curry–Howard correspondence\n\nIn programming language theory and proof theory, the Curry–Howard correspondence (also known as the Curry–Howard isomorphism or equivalence, or the proofs-as-programs and propositions- or formulae-as-types interpretation) is the direct relationship between computer programs and mathematical proofs. \n\nIt is a generalization of a syntactic analogy between systems of formal logic and computational calculi that was first discovered by the American mathematician Haskell Curry and logician William Alvin Howard. It is the link between logic and computation that is usually attributed to Curry and Howard, although the idea is related to the operational interpretation of intuitionistic logic given in various formulations by L. E. J. Brouwer, Arend Heyting and Andrey Kolmogorov (see Brouwer–Heyting–Kolmogorov interpretation) and Stephen Kleene (see Realizability). The relationship has been extended to include category theory as the three-way Curry–Howard–Lambek correspondence.\n\nThe beginnings of the Curry–Howard correspondence lie in several observations:\n\nIn other words, the Curry–Howard correspondence is the observation that two families of seemingly unrelated formalisms—namely, the proof systems on one hand, and the models of computation on the other—are in fact the same kind of mathematical objects.\n\nIf one abstracts on the peculiarities of either formalism, the following generalization arises: \"a proof is a program, and the formula it proves is the type for the program\". More informally, this can be seen as an analogy that states that the return type of a function (i.e., the type of values returned by a function) is analogous to a logical theorem, subject to hypotheses corresponding to the types of the argument values passed to the function; and that the program to compute that function is analogous to a proof of that theorem. This sets a form of logic programming on a rigorous foundation: \"proofs can be represented as programs, and especially as lambda terms\", or \"proofs can be run\".\n\nThe correspondence has been the starting point of a large spectrum of new research after its discovery, leading in particular to a new class of formal systems designed to act both as a proof system and as a typed functional programming language. This includes Martin-Löf's intuitionistic type theory and Coquand's Calculus of Constructions, two calculi in which proofs are regular objects of the discourse and in which one can state properties of proofs the same way as of any program. This field of research is usually referred to as modern type theory.\n\nSuch typed lambda calculi derived from the Curry–Howard paradigm led to software like Coq in which proofs seen as programs can be formalized, checked, and run.\n\nA converse direction is to \"use a program to extract a proof\", given its correctness—an area of research closely related to proof-carrying code. This is only feasible if the programming language the program is written for is very richly typed: the development of such type systems has been partly motivated by the wish to make the Curry–Howard correspondence practically relevant.\n\nThe Curry–Howard correspondence also raised new questions regarding the computational content of proof concepts that were not covered by the original works of Curry and Howard. In particular, classical logic has been shown to correspond to the ability to manipulate the continuation of programs and the symmetry of sequent calculus to express the duality between the two evaluation strategies known as call-by-name and call-by-value.\n\nSpeculatively, the Curry–Howard correspondence might be expected to lead to a substantial unification between mathematical logic and foundational computer science:\n\nHilbert-style logic and natural deduction are but two kinds of proof systems among a large family of formalisms. Alternative syntaxes include sequent calculus, proof nets, calculus of structures, etc. If one admits the Curry–Howard correspondence as the general principle that any proof system hides a model of computation, a theory of the underlying untyped computational structure of these kinds of proof system should be possible. Then, a natural question is whether something mathematically interesting can be said about these underlying computational calculi.\n\nConversely, combinatory logic and simply typed lambda calculus are not the only models of computation, either. Girard's linear logic was developed from the fine analysis of the use of resources in some models of lambda calculus; is there typed version of Turing's machine that would behave as a proof system? Typed assembly languages are such an instance of \"low-level\" models of computation that carry types.\n\nBecause of the possibility of writing non-terminating programs, Turing-complete models of computation (such as languages with arbitrary recursive functions) must be interpreted with care, as naive application of the correspondence leads to an inconsistent logic. The best way of dealing with arbitrary computation from a logical point of view is still an actively debated research question, but one popular approach is based on using monads to segregate provably terminating from potentially non-terminating code (an approach that also generalizes to much richer models of computation, and is itself related to modal logic by a natural extension of the Curry–Howard isomorphism). A more radical approach, advocated by total functional programming, is to eliminate unrestricted recursion (and forgo Turing completeness, although still retaining high computational complexity), using more controlled corecursion wherever non-terminating behavior is actually desired.\n\nIn its more general formulation, the Curry–Howard correspondence is a correspondence between formal proof calculi and type systems for models of computation. In particular, it splits into two correspondences. One at the level of formulas and types that is independent of which particular proof system or model of computation is considered, and one at the level of proofs and programs which, this time, is specific to the particular choice of proof system and model of computation considered.\n\nAt the level of formulas and types, the correspondence says that implication behaves the same as a function type, conjunction as a \"product\" type (this may be called a tuple, a struct, a list, or some other term depending on the language), disjunction as a sum type (this type may be called a union), the false formula as the empty type and the true formula as the singleton type (whose sole member is the null object). Quantifiers correspond to dependent function space or products (as appropriate). \nThis is summarized in the following table:\n\nAt the level of proof systems and models of computations, the correspondence mainly shows the identity of structure, first, between some particular formulations of systems known as Hilbert-style deduction system and combinatory logic, and, secondly, between some particular formulations of systems known as natural deduction and lambda calculus.\n\nBetween the natural deduction system and the lambda calculus there are the following correspondences:\n\nIt was at the beginning a simple remark in Curry and Feys's 1958 book on combinatory logic: the simplest types for the basic combinators K and S of combinatory logic surprisingly corresponded to the respective axiom schemes α → (β → α) and (α → (β → γ)) → ((α → β) → (α → γ)) used in Hilbert-style deduction systems. For this reason, these schemes are now often called axioms K and S. Examples of programs seen as proofs in a Hilbert-style logic are given below.\n\nIf one restricts to the implicational intuitionistic fragment, a simple way to formalize logic in Hilbert's style is as follows. Let Γ be a finite collection of formulas, considered as hypotheses. Then δ is \"derivable\" from Γ, denoted Γ ⊢ δ, in the following cases:\n\n\nThis can be formalized using inference rules, as in the left column of the following table.\n\nTyped combinatory logic can be formulated using a similar syntax: let Γ be a finite collection of variables, annotated with their types. A term T (also annotated with its type) will depend on these variables [Γ ⊢ T:δ] when:\n\n\nThe generation rules defined here are given in the right-column below. Curry's remark simply states that both columns are in one-to-one correspondence. The restriction of the correspondence to intuitionistic logic means that some classical tautologies, such as Peirce's law ((α → β) → α) → α, are excluded from the correspondence.\n\nSeen at a more abstract level, the correspondence can be restated as shown in the following table. Especially, the deduction theorem specific to Hilbert-style logic matches the process of abstraction elimination of combinatory logic.\n\nThanks to the correspondence, results from combinatory logic can be transferred to Hilbert-style logic and vice versa. For instance, the notion of reduction of terms in combinatory logic can be transferred to Hilbert-style logic and it provides a way to canonically transform proofs into other proofs of the same statement. One can also transfer the notion of normal terms to a notion of normal proofs, expressing that the hypotheses of the axioms never need to be all detached (since otherwise a simplification can happen).\n\nConversely, the non provability in intuitionistic logic of Peirce's law can be transferred back to combinatory logic: there is no typed term of combinatory logic that is typable with type ((α → β) → α) → α.\n\nResults on the completeness of some sets of combinators or axioms can also be transferred. For instance, the fact that the combinator X constitutes a one-point basis of (extensional) combinatory logic implies that the single axiom scheme\nwhich is the principal type of X, is an adequate replacement to the combination of the axiom schemes \n\nAfter Curry emphasized the syntactic correspondence between Hilbert-style deduction and combinatory logic, Howard made explicit in 1969 a syntactic analogy between the programs of simply typed lambda calculus and the proofs of natural deduction. Below, the left-hand side formalizes intuitionistic implicational natural deduction as a calculus of sequents (the use of sequents is standard in discussions of the Curry–Howard isomorphism as it allows the deduction rules to be stated more cleanly) with implicit weakening and the right-hand side shows the typing rules of lambda calculus. In the left-hand side, Γ, Γ and Γ denote ordered sequences of formulas while in the right-hand side, they denote sequences of named (i.e., typed) formulas with all names different.\n\nTo paraphrase the correspondence, proving Γ ⊢ α means having a program that, given values with the types listed in Γ, manufactures an object of type α. An axiom corresponds to the introduction of a new variable with a new, unconstrained type, the → I rule corresponds to function abstraction and the → E rule corresponds to function application. Observe that the correspondence is not exact if the context Γ is taken to be a set of formulas as, e.g., the λ-terms λx.λy.x and λx.λy.y of type α → α → α would not be distinguished in the correspondence. Examples are given below.\n\nHoward showed that the correspondence extends to other connectives of the logic and other constructions of simply typed lambda calculus. Seen at an abstract level, the correspondence can then be summarized as shown in the following table. Especially, it also shows that the notion of normal forms in lambda calculus matches Prawitz's notion of normal deduction in natural deduction, from which it follows that the algorithms for the type inhabitation problem can be turned into algorithms for deciding intuitionistic provability.\n\nHoward's correspondence naturally extends to other extensions of natural deduction and simply typed lambda calculus. Here is a non-exhaustive list:\n\n\nAt the time of Curry, and also at the time of Howard, the proofs-as-programs correspondence concerned only intuitionistic logic, i.e. a logic in which, in particular, Peirce's law was \"not\" deducible. The extension of the correspondence to Peirce's law and hence to classical logic became clear from the work of Griffin on typing operators that capture the evaluation context of a given program execution so that this evaluation context can be later on reinstalled. The basic Curry–Howard-style correspondence for classical logic is given below. Note the correspondence between the double-negation translation used to map classical proofs to intuitionistic logic and the continuation-passing-style translation used to map lambda terms involving control to pure lambda terms. More particularly, call-by-name continuation-passing-style translations relates to Kolmogorov's double negation translation and call-by-value continuation-passing-style translations relates to a kind of double-negation translation due to Kuroda.\n\nA finer Curry–Howard correspondence exists for classical logic if one defines classical logic not by adding an axiom such as Peirce's law, but by allowing several conclusions in sequents. In the case of classical natural deduction, there exists a proofs-as-programs correspondence with the typed programs of Parigot's λμ-calculus.\n\nA proofs-as-programs correspondence can be settled for the formalism known as Gentzen's sequent calculus but it is not a correspondence with a well-defined pre-existing model of computation as it was for Hilbert-style and natural deductions.\n\nSequent calculus is characterized by the presence of left introduction rules, right introduction rule and a cut rule that can be eliminated. The structure of sequent calculus relates to a calculus whose structure is close to the one of some abstract machines. The informal correspondence is as follows:\n\nN. G. de Bruijn used the lambda notation for representing proofs of the theorem checker Automath, and represented propositions as \"categories\" of their proofs. It was in the late 1960s at the same period of time Howard wrote his manuscript; de Bruijn was likely unaware of Howard's work, and stated the correspondence independently (Sørensen & Urzyczyn [1998] 2006, pp 98–99). Some researchers tend to use the term Curry–Howard–de Bruijn correspondence in place of Curry–Howard correspondence.\n\nThe BHK interpretation interprets intuitionistic proofs as functions but it does not specify the class of functions relevant for the interpretation. If one takes lambda calculus for this class of function, then the BHK interpretation tells the same as Howard's correspondence between natural deduction and lambda calculus.\n\nKleene's recursive realizability splits proofs of intuitionistic arithmetic into the pair of a recursive function and of\na proof of a formula expressing that the recursive function \"realizes\", i.e. correctly instantiates the disjunctions and existential quantifiers of the initial formula so that the formula gets true.\n\nKreisel's modified realizability applies to intuitionistic higher-order predicate logic and shows that the simply typed lambda term inductively extracted from the proof realizes the initial formula. In the case of propositional logic, it coincides with Howard's statement: the extracted lambda term is the proof itself (seen as an untyped lambda term) and the realizability statement is a paraphrase of the fact that the extracted lambda term has the type that the formula means (seen as a type).\n\nGödel's dialectica interpretation realizes (an extension of) intuitionistic arithmetic with computable functions. The connection with lambda calculus is unclear, even in the case of natural deduction.\n\nJoachim Lambek showed in the early 1970s that the proofs of intuitionistic propositional logic and the combinators of typed combinatory logic share a common equational theory which is the one of cartesian closed categories. The expression Curry–Howard–Lambek correspondence is now used by some people to refer to the three way isomorphism between intuitionistic logic, typed lambda calculus and cartesian closed categories, with objects being interpreted as types or propositions and morphisms as terms or proofs. The correspondence works at the equational level and is not the expression of a syntactic identity of structures as it is the case for each of Curry's and Howard's correspondences: i.e. the structure of a well-defined morphism in a cartesian-closed category is not comparable to the structure of a proof of the corresponding judgment in either Hilbert-style logic or natural deduction. To clarify this distinction, the underlying syntactic structure of cartesian closed categories is rephrased below.\n\nObjects (types) are defined by\n\nMorphisms (terms) are defined by\n\nWell-defined morphisms (typed terms) are defined by the following typing rules (in which the usual categorical morphism notation formula_19 is replaced with sequent calculus notation formula_20).\n\nIdentity:\n\nComposition:\n\nUnit type (terminal object):\n\nCartesian product:\n\nLeft and right projection:\n\nCurrying:\n\nApplication:\n\nFinally, the equations of the category are\n\nThese equations imply the following formula_40-laws:\n\nNow, there exists formula_13 such that formula_44 iff formula_45 is provable in implicational intuitionistic logic.\n\nThanks to the Curry–Howard correspondence, a typed expression whose type corresponds to a logical formula is analogous to a proof of that formula. Here are examples.\n\nAs an example, consider a proof of the theorem α → α. In lambda calculus, this is the type of the identity function I = λ\"x\".\"x\" and in combinatory logic, the identity function is obtained by applying S = λ\"fgx\".\"fx(gx)\" twice to K = λ\"xy\".\"x\". That is, I = ((S K) K). As a description of a proof, this says that the following steps can be used to prove α → α:\n\n\nIn general, the procedure is that whenever the program contains an application of the form (\"P\" \"Q\"), these steps should be followed:\n\nAs a more complicated example, let's look at the theorem that corresponds to the B function. The type of B is (β → α) → (γ → β) → γ → α. B is equivalent to (S (K S) K). This is our roadmap for the proof of the theorem (β → α) → (γ → β) → γ → α.\n\nThe first step is to construct (K S). To make the antecedent of the K axiom look like the S axiom, set α equal to (α → β → γ) → (α → β) → α → γ, and β equal to δ (to avoid variable collisions):\n\n<br>K[α = (α → β → γ) → (α → β) → α → γ, β=δ] : ((α → β → γ) → (α → β) → α → γ) → δ → (α → β → γ) → (α → β) → α → γ\n\nSince the antecedent here is just S, the consequent can be detached using Modus Ponens:\n\nThis is the theorem that corresponds to the type of (K S). Now apply S to this expression. Taking S as follows\n\nput α = δ, β = α → β → γ, and γ = (α → β) → α → γ, yielding\n\nand then detach the consequent:\n\nThis is the formula for the type of (S (K S)). A special case of this theorem has δ = (β → γ):\n\nThis last formula must be applied to K. Specialize K again, this time by replacing α with (β → γ) and β with α:\n\nThis is the same as the antecedent of the prior formula so, detaching the consequent:\n\nSwitching the names of the variables α and γ gives us\n\nwhich was what remained to prove.\n\nThe diagram below gives proof of (β → α) → (γ → β) → γ → α in natural deduction and shows how it can be interpreted as the λ-expression λ \"a\". λ\"b\". λ \"g\".(\"a\" (\"b\" \"g\")) of type (β → α) → (γ → β) → γ → α.\n\nRecently, the isomorphism has been proposed as a way to define search space partition in genetic programming. The method indexes sets of genotypes (the program trees evolved by the GP system) by their Curry–Howard isomorphic proof (referred to as a species).\n\nThe correspondences listed here go much farther and deeper. For example, cartesian closed categories are generalized by closed monoidal categories. The internal language of these categories is the linear type system (corresponding to linear logic), which generalizes simply-typed lambda calculus as the internal language of cartesian closed categories. Moreover, these can be shown to correspond to cobordisms, which play a vital role in string theory.\n\nAn extended set of equivalences is also explored in homotopy type theory, which became a very active area of research around 2013 and still is. Here, type theory is extended by the univalence axiom (\"equivalence is equivalent to equality\") which permits homotopy type theory to be used as a foundation for all of mathematics (including set theory and classical logic, providing new ways to discuss the axiom of choice and many other things). That is, the Curry–Howard correspondence that proofs are elements of inhabited types is generalized to the notion homotopic equivalence of proofs (as paths in space, the identity type or equality type of type theory being interpreted as a path).\n\n\n\n\n\n\n\n"}
{"id": "40941210", "url": "https://en.wikipedia.org/wiki?curid=40941210", "title": "Default effect", "text": "Default effect\n\nAmong the set of options that agents choose from, the default option is the option the chooser will obtain if he or she does nothing. Broader interpretations of default options include options that are normative or suggested. Experiments and observational studies show that making an option a default increases the likelihood that it is chosen; this is called the default effect. Different causes for this effect have been discussed. Setting or changing defaults therefore has been proposed as an effective way of influencing behavior—for example, with respect to deciding whether to become an organ donor, giving consent to receive e-mail marketing, or choosing the level of one's retirement contributions.\n\nIn a choice context, a default refers to that option which choosers end up with if they do not make an active choice. This notion is similar to the one in computer science where defaults are settings or values that are automatically assigned outside of user intervention. Setting the default affects how likely people end up with an option. This is called the default effect. More precisely, it refers to changes in the probability that an agent chooses a particular option when it is set as a default as opposed to the situation where this option has not been set as default. For example, different countries have different rules on how to become an organ donor. In countries with the opt-in policy, all citizens are automatically considered as non-donors unless they actively register as donors. In countries with the opt-out policy, all citizens are automatically considered as donors unless they actively seek to be struck from the register. It has been argued that this difference in policy is the main cause of the significant difference in donor rates across the respective countries.\n\nSome default effects are implied by the situation. In social settings, for example, the normative choice (what others are doing) may be adopted unconsciously as a social default effect. People are thus more likely to choose what they observe other choosing, even if they do not believe that other person is the more knowledgeable person. People are also more likely to treat choices that require less justification as defaults. The default option for parole hearings, for example, is to deny prisoners parole.\n\nA number of different explanations have been offered for how default setting causes a change in the choice distribution. These include Cognitive Effort, Switching Costs, Loss Aversion, Recommendation and Change of Meaning.\n\nIf an agent is indifferent or conflicted between options, it may involve too much cognitive effort to base a choice on explicit evaluations. In that case, he or she might disregard the evaluations and choose according to the default heuristic instead, which simply states “if there is a default, do nothing about it”. Evidence supporting this cognitive effort account is provided in the realm of social default effects. Participants distracted by a demanding concurrent task were more likely to choose the one of two snacks that they saw a previous participant choose. Increasing the number of snacks received as a function of the choice decreased this social default effect.\n\nIf an agent faces costs when diverging from a default that surmount the possible benefits from switching to another option, then it is rational to stick with the default option. Costs of diverging from the default might involve costs for the search of information (time, consultancy fees) and/or costs for registering the choice (time, postage, lawyer fees). This amounts to a standard transaction cost explanation from rational choice theory.\n\nIf an agent evaluates options on multiple dimensions, then the default functions as a reference point from which some dimensions are interpreted as losses and therefore becomes more important for the choice. This loss aversion explanation of the default effect can be illustrated with the following example. Let A, B and C be three different pension saving plans. The agent evaluates them (in terms of time-discounted utilities) on the two following dimensions, current consumption and future savings which obviously trade off: the more one saves for the future, the less one can consume in the present.\nSeen from plan A being the default, plans B and C constitute losses in the savings dimension. Seen from plan C being the default, however, plans A and B constitute losses in the consumption dimension. According to the theory of loss aversion, that dimension which is considered a loss influences the decision stronger than that which is considered a gain. Hence for either default A or C, the loss-averse agent would choose sticking with the default.\n\nIf an agent interprets the default as a signal from the policy maker, whom he or she sufficiently trusts, he or she might rationally decide to stick with this default. That the policy maker sets a default is interpreted as an implicit recommendation to choose that default option. The information taken from this recommendation might be sufficient to change some people's preferences.\n\nDefaults also might affect the meaning of choice options and thus agents' choices over them. For example, it has been shown that under an opt-in policy in organ donation choosing not to become an organ donor is perceived as a choice of little moral failing. Under an opt-out policy, in contrast, choosing not to be an organ donor is perceived as morally more deficient. These differences in evaluation might affect the rational choice over these options.\n\nSetting or changing defaults has been proposed as an effective way of influencing behaviour—for example, with respect to deciding whether to become an organ donor, giving consent to receive e-mail marketing, choosing car insurance plans, choosing which food to eat, selecting which car options to purchase, choosing between different energy providers, or choosing the level of one's retirement contributions. Setting defaults are an important example of nudges or soft paternalist policies.\n"}
{"id": "25782973", "url": "https://en.wikipedia.org/wiki?curid=25782973", "title": "Deferred sentence", "text": "Deferred sentence\n\nA deferred sentence is a sentence that is suspended until after a defendant has completed a period of probation. If the defendant fulfills the stipulations surrounding probation, a judge may then throw out the sentence and guilty plea, clearing the incident from their record. If the defendant violates probation, he or she must serve the full sentence immediately.\n\nIn the United States, a defendant must plead guilty to at least one of the crimes he or she is accused of in order to receive a deferred sentence. The promise of a deferred sentence is often traded in exchange for a guilty plea in plea bargains.\n\nDeferred sentences are often given to first time offenders, or to those who have committed relatively minor crimes, although ultimately, the choice to defer a sentence is left to a judge's discretion.\n\nIn the state of New York, a similar process is known as adjournment in contemplation of dismissal (ACOD). What typically happens in such a case is that the potential sentence is deferred for six months, and if the defendant stays out of trouble, the charge is dropped entirely with no public record of the offense.\n\nA deferred sentence is not exactly the same as an ACOD. One of the primary differences is that receiving an ACOD doesn't require an admission of guilt or plea of guilty unlike a deferred sentence which requires such. Upon completion of the ACOD, the charges are automatically sealed, fingerprints & mugshots are destroyed, and the arrest is annulled. There is no conviction and all rights you had prior to arrest are restored. In order to enter the deferred sentence program, a plea of guilt must be made. Even though successful completion of a deferred sentence results in a dismissal of charges and guilty plea withdrawal, most states still consider it to be a conviction since a plea of guilt was entered and you were considered \"convicted\" for the duration of the program. Furthermore, completion of a deferred sentence program usually requires a request to the court to have the underlying charges and arrest sealed.\n\n"}
{"id": "3731503", "url": "https://en.wikipedia.org/wiki?curid=3731503", "title": "Desiring-production", "text": "Desiring-production\n\nDesiring-production () is a term coined by the French thinkers Gilles Deleuze and Félix Guattari in their book \"Anti-Œdipus\" (1972).\n\nDeleuze and Guattari oppose the Freudian conception of the unconscious as a representational \"theater\", instead favoring a productive \"factory\" model: desire is not an imaginary force based on lack, but a real, productive force. They describe the machinic nature of desire as a kind of \"desiring-machine\" that functions as a circuit breaker in a larger \"circuit\" of various other machines to which it is connected. Meanwhile, the desiring-machine is also producing a flow of desire from itself. Deleuze and Guattari conceptualize a multi-functional universe composed of such machines all connected to each other: \"There are no desiring-machines that exist outside the social machines that they form on a large scale; and no social machines without the desiring machines that inhabit them on a small scale.\" Desiring-production is explosive: \"there is no desiring-machine capable of being assembled without demolishing entire social sectors\".\n\nThe concept of desiring-production is part of Deleuze and Guattari's more general appropriation of Friedrich Nietzsche's formulation of the Will to Power. In both concepts, a pleasurable force of appropriation of what is outside oneself, incorporating into oneself what is other than oneself, characterizes the essential process of all life. Similarly, a kind of reverse force of \"forgetting\" in Nietzsche and the body without organs in Deleuze and Guattari disavows the Will to Power and desiring-production, attempting to realize the ideal of an hermetic subject.\n\nThenceforth, while very interested by Wilhelm Reich's fundamental question — \"why did the masses desire fascism?\" — they criticized his dualist theory leading to a rational social reality on one side, and an irrational desire reality on the other side. \"Anti-Œdipus\" was thus an attempt to think beyond Freudo-Marxism; and Deleuze and Guattari tried to do for Freud what Marx had done for Adam Smith.\n\nPublished in the same year as \"Anti-Œdipus\", Guy Hocquenghem's \"Homosexual Desire\" re-articulated desiring-production within the emergent field of queer theory.\n\n\n"}
{"id": "31023441", "url": "https://en.wikipedia.org/wiki?curid=31023441", "title": "Docking and berthing of spacecraft", "text": "Docking and berthing of spacecraft\n\nDocking and berthing of spacecraft is the joining of two space vehicles. This connection can be temporary, or semipermanent such as for space station modules.\n\n\"Docking\" specifically refers to joining of two separate free-flying space vehicles. \"Berthing\" refers to mating operations where an inactive module/vehicle is placed into the mating interface of another space vehicle by using a robotic arm. Because the modern process of un-berthing is manually laborious, berthing operations are seen as unsuited for rapid crew evacuations in the event of an emergency.\n\nA docking/berthing connection is referred to as either \"soft\" or \"hard\". Typically, a spacecraft first initiates a \"soft dock\" by making contact and latching its docking connector with that of the target vehicle. Once the soft connection is secured, if both spacecraft are pressurized, they may proceed to a \"hard dock\" where the docking mechanisms form an airtight seal, enabling interior hatches to be safely opened so that crew and cargo can be transferred.\n\nSpacecraft docking capability depends on space rendezvous, the ability of two spacecraft to find each other and station-keep in the same orbit. This was first developed by the United States for Project Gemini. It was planned for the crew of Gemini 6 to rendezvous and manually dock under the command of Wally Schirra, with an unmanned Agena Target Vehicle in October 1965, but the Agena vehicle exploded during launch. On the revised mission Gemini 6A, Schirra successfully performed a rendezvous in December 1965 with the manned Gemini 7, approaching to within one foot, but there was no docking capability between two Gemini spacecraft. The first docking with an Agena was successfully performed under the command of Neil Armstrong on Gemini 8 on March 16, 1966. Manual dockings were performed on three subsequent Gemini missions in 1966.\n\nThe Apollo program depended on lunar orbit rendezvous to achieve its objective of landing men on the Moon. This required first a transposition, docking, and extraction maneuver between the Apollo Command/Service Module (CSM) mother spacecraft and the Lunar Module (LM) landing spacecraft, shortly after both craft were sent out of Earth orbit on a path to the Moon. Then after completing the lunar landing mission, two astronauts in the LM had to rendezvous and dock with the CSM in lunar orbit, in order to be able to return to Earth. The spacecraft were designed to permit intra-vehicular crew transfer through a tunnel between the nose of the Command Module and the roof of the Lunar Module. These maneuvers were first demonstrated in low Earth orbit on March 7, 1969, on Apollo 9, then in lunar orbit in May 1969 on Apollo 10, then in six lunar landing missions.\n\nUnlike the United States, which used manual piloted docking throughout the Apollo and Space Shuttle programs, the Soviet Union employed automated docking systems from the beginning of its docking attempts. The first such system, Igla, was successfully tested on October 30, 1967 when the two uncrewed Soyuz test vehicles Kosmos 186 and Kosmos 188 docked automatically in orbit. This was the first successful Soviet docking. Proceeding to manned docking attempts, the Soviet Union first achieved rendezvous of Soyuz 3 with the unmanned Soyuz 2 craft on October 25, 1968; docking was unsuccessfully attempted. The first manned Soviet docking was achieved on January 16, 1969, between Soyuz 4 and Soyuz 5. This early version of the Soyuz spacecraft had no internal transfer tunnel, but two cosmonauts performed an extravehicular transfer from Soyuz 5 to Soyuz 4, landing in a different spacecraft than they had launched in. \n\nIn the 1970s, the Soviet Union upgraded the Soyuz spacecraft to add an internal transfer tunnel and used it to transport cosmonauts during the Salyut space station program with the first successful space station visit beginning on 7 June 1971, when Soyuz 11 docked to Salyut 1. The United States followed suit, docking its Apollo spacecraft to the Skylab space station in May 1973. In July 1975, the two nations cooperated in the Apollo-Soyuz Test Project, docking an Apollo spacecraft with a Soyuz using a specially designed docking module to accommodate the different docking systems and spacecraft atmospheres.\n\nBeginning with Salyut 6 in 1978, the Soviet Union began using the uncrewed Progress cargo spacecraft to resupply its space stations in low earth orbit, greatly extending the length of crew stays. As an uncrewed spacecraft, Progress rendezvoused and docked with the space stations entirely automatically. In 1986, the Igla docking system was replaced with the updated Kurs system on Soyuz spacecraft. Progress spacecraft received the same upgrade several years later. The Kurs system is still used to dock to the Russian Orbital Segment of the ISS today.\n\nDocking/berthing systems may be either androgynous (ungendered) or non-androgynous (gendered), indicating which parts of the system may mate together.\n\nEarly systems for conjoining spacecraft were all non-androgynous docking system designs. Non-androgynous designs are a form of \"gender mating\" where each spacecraft to be joined has a unique design (\"male\" or \"female\") and a specific role to play in the docking process. The roles cannot be reversed. Furthermore, two spacecraft of the same gender cannot be joined at all.\n\nAndrogynous docking (and later androgynous berthing) by contrast has an identical interface on both spacecraft. In an androgynous interface, there is a single design which can connect to a duplicate of itself. This allows system-level redundancy (role reversing) as well as rescue and collaboration between any two spacecraft. It also provides more flexible mission design and reduces unique mission analysis and training.\n\nA docking or berthing adapter is a mechanical or electromechanical device that facilitates the connection of one type of docking or berthing interface to a different interface. While such interfaces may theoretically be docking/docking, docking/berthing, or berthing/berthing, only the first two types have been deployed in space to date. Previously launched and planned to be launched adapters are listed below:\n\n\nFor the first fifty years of spaceflight, the main objective of most \"docking\" and \"berthing\" missions was to transfer crew, construct or resupply a space station, or to test for such a mission (e.g. the docking between Kosmos 186 and Kosmos 188).\nTherefore, commonly at least one of the participating spacecraft was \"manned\", with a pressurized habitable volume (e.g. a space station or a lunar lander) being the target – the exceptions were a few fully unmanned Soviet docking missions (e.g. the dockings of Kosmos 1443 and Progress 23 to an unmanned Salyut 7 or Progress M1-5 to an unmanned Mir).\nAnother exception were a few missions of the manned US Space Shuttles, like berthings of the Hubble Space Telescope (HST) during the five HST servicing missions.\n\nChanges to the \"manned\" aspect began in 2015, as a number of economically driven commercial dockings of unmanned spacecraft were planned. In 2011, two commercial spacecraft providers announced plans to provide autonomous/teleoperated unmanned resupply spacecraft for servicing other unmanned spacecraft. Notably, both of these servicing spacecraft were intending to dock with satellites that weren't designed for docking, nor for in-space servicing.\n\nThe early business model for these services was primarily in near-geosynchronous orbit, although large delta-v orbital maneuvering services were also envisioned.\n\nBuilding off of the 2007 Orbital Express mission — a U.S. government-sponsored mission to test in-space satellite servicing with two vehicles designed from the ground up for on-orbit refueling and subsystem replacement — two companies announced plans for commercial satellite servicing missions that would require docking of two unmanned vehicles.\n\n\n\nThe SIS and MEV vehicles each planned to use a different docking technique.\nSIS planned to utilize a ring attachment around the kick motor\nwhile the Mission Extension Vehicle would use a somewhat more standard insert-a-probe-into-the-nozzle-of-the-kick-motor approach.\n\nA prominent spacecraft that received a mechanism for unmanned dockings is the Hubble Space Telescope (HST). In 2009 the STS-125 shuttle mission added the Soft-Capture Mechanism (SCM) at the aft bulkhead of the space telescope. The SCM is meant for unpressurized dockings and will be used at the end of Hubble's service lifetime to dock an unmanned spacecraft to de-orbit Hubble. The SCM used was designed to be compatible to the NASA Docking System (NDS) interface to reserve the possibility of a Multi-Purpose Crew Vehicle docked mission.\nThe SCM will, compared to the system used during the five HST Servicing Missions to capture and berth the HST to the Space Shuttle,\nsignificantly reduce the rendezvous and capture design complexities associated with such missions.\nThe NDS bears some resemblance to the APAS-95 mechanism, but is not compatible with it.\n\nDocking with a spacecraft (or other man made space object) that does not have an operable attitude control system might sometimes be desirable, either in order to salvage it, or to initiate a controlled de-orbit. Some theoretical techniques for docking with non-cooperative spacecraft have been proposed so far. Yet, with the sole exception of the Soyuz T-13 mission to salvage the crippled Salyut 7 space station, , all spacecraft dockings in the first fifty years of spaceflight had been accomplished with vehicles where both spacecraft involved were under either piloted, autonomous or telerobotic attitude control.\nIn 2007, however, a demonstration mission was flown that included an initial test of a non-cooperative spacecraft captured by a controlled spacecraft with the use of a robotic arm.\nResearch and modeling work continues to support additional autonomous noncooperative capture missions in the coming years.\n\nSalyut 7, the tenth space station of any kind launched, and Soyuz T-13 were docked in what author David S. F. Portree describes as \"one of the most impressive feats of in-space repairs in history\". Solar tracking failed and due to a telemetry fault the station did not report the failure to mission control while flying autonomously. Once the station ran out of electrical energy reserves it ceased communication abruptly in February 1985. Crew scheduling was interrupted to allow Russian military commander Vladimir Dzhanibekov and technical science flight engineer Viktor Savinykh to make emergency repairs.\n\nAll Soviet and Russian space stations were equipped with automatic rendezvous and docking systems, from the first space station Salyut 1 using the IGLA system, to the Russian Orbital Segment of the International Space Station using the Kurs system. The soyuz crew found the station was not broadcasting radar or telemetry for rendezvous, and after arrival and external inspection of the tumbling station, the crew judged proximity using handheld laser rangefinders.\n\nDzhanibekov piloted his ship to intercept the forward port of Salyut 7, matched the station's rotation and achieved soft dock with the station. After achieving hard dock they confirmed that the station's electrical system was dead. Prior to opening the hatch, Dzhanibekov and Savinykh sampled the condition of the station's atmosphere and found it satisfactory. Attired in winter fur-lined clothing, they entered the cold station to conduct repairs. Within a week sufficient systems were brought back online to allow robot cargo ships to dock with the station. Nearly two months went by before atmospheric conditions on the space station were normalized.\n\nNon-cooperative rendezvous and capture techniques have been theorized, and one mission has successfully been performed with uncrewed spacecraft in orbit.\n\nA typical approach for solving this problem involves two phases. First, attitude and orbital changes are made to the \"chaser\" spacecraft until it has zero relative motion with the \"target\" spacecraft. Second, docking maneuvers commence that are similar to traditional cooperative spacecraft docking. A standardized docking interface on each spacecraft is assumed.\n\nNASA has identified automated and autonomous rendezvous and docking — the ability of two spacecraft to rendezvous and dock \"operating independently from human controllers and without other back-up, [and which requires technology] advances in sensors, software, and realtime on-orbit positioning and flight control, among other challenges\" — as a critical technology to the \"ultimate success of capabilities such as in-orbit propellant storage and refueling,\" and also for complex operations in assembling mission components for interplanetary destinations.\n\nThe Automated/Autonomous Rendezvous & Docking Vehicle (ARDV) is a proposed NASA Flagship Technology Demonstration (FTD) mission, for flight as early as 2014/2015. An important NASA objective on the proposed mission is to advance the technology and demonstrate automated rendezvous and docking. One mission element defined in the 2010 analysis was the development of a laser proximity operations sensor that could be used for non-cooperative vehicles at distances between and . Non-cooperative docking mechanisms were identified as critical mission elements to the success of such autonomous missions.\n\nGrappling and connecting to non-cooperative space objects was identified as a top technical challenge in the 2010 NASA Robotics, tele-robotics and autonomous systems roadmap.\n\nDocking and undocking describe spacecraft using a docking port, without assistance and under their own power. Berthing takes place when a spacecraft or unpowered module cannot use a docking port or requires assistance to use one. This assistance may come from a spacecraft, such as when the Space Shuttle used its robotic arm to push ISS modules into their permanent berths. In a similar fashion the Poisk module was permanently berthed to a docking port after it was pushed into place by a modified Progress spacecraft which was then discarded. The Cygnus resupply spacecraft arriving at the ISS does not connect to a docking port, instead it is pulled into a berthing mechanism by the station's robotic arm and the station then closes the connection. The berthing mechanism is used only on the US segment of the ISS, the Russian segment of the ISS uses docking ports for permanent berths.\n\nDocking has been discussed by NASA in regards to a Crewed Mars rover, such as with Mars habitat or ascent stage.\n"}
{"id": "211889", "url": "https://en.wikipedia.org/wiki?curid=211889", "title": "Electrical injury", "text": "Electrical injury\n\nElectrical injury is a physiological reaction caused by electric current passing through the (human) body. Electric shock occurs upon contact of a (human) body part with any source of electricity that causes a sufficient magnitude of current to pass through the victim's flesh, viscera or hair. Physical contact with energized wiring or devices is the most common cause of an electric shock. In cases of exposure to high voltages, such as on a power transmission tower, physical contact with energized wiring or objects may not be necessary to cause electric shock, as the voltage may be sufficient to \"jump\" the air gap between the electrical device and the victim.\n\nThe injury related to electric shock depends on the magnitude of the current. Very small currents may be imperceptible or produce a light tingling sensation. A shock caused by low current that would normally be harmless could startle an individual and cause injury due to suddenly jerking away from the source of electricity, resulting in one striking a stationary object, dropping an object being held or falling. Stronger currents may cause some degree of discomfort or pain, while more intense currents may induce involuntary muscle contractions, preventing the victim from breaking free of the source of electricity. Still larger currents usually result in tissue damage and may trigger fibrillation of the heart or cardiac arrest, any of which may ultimately be fatal. If death results from an electric shock the cause of death is generally referred to as electrocution. \n\nHeating due to resistance can cause extensive and deep burns. Voltage levels of 500 to 1000 volts tend to cause internal burns due to the large energy (which is proportional to the duration multiplied by the square of the voltage divided by resistance) available from the source. Damage due to current is through tissue heating. For most cases of high-energy electrical trauma, the Joule heating in the deeper tissues along the extremity will reach damaging temperatures in a few seconds.\n\nA domestic power supply voltage (110 or 230 V), 50 or 60 Hz alternating current (AC) through the chest for a fraction of a second may induce ventricular fibrillation at currents as low as . With direct current (DC), 300 to 500 mA is required. If the current has a direct pathway to the heart (e.g., via a cardiac catheter or other kind of electrode), a much lower current of less than 1 mA (AC or DC) can cause fibrillation. If not immediately treated by defibrillation, fibrillation is usually lethal because all of the heart muscle fibres move independently instead of in the coordinated pulses needed to pump blood and maintain circulation. Above 200 mA, muscle contractions are so strong that the heart muscles cannot move at all, but these conditions prevent fibrillation.\n\nCurrent can cause interference with nervous control, especially over the heart and lungs. Repeated or severe electric shock which does not lead to death has been shown to cause neuropathy. Recent research has found that functional differences in neural activation during spatial working memory and implicit learning oculomotor tasks have been identified in electrical shock victims.\n\nWhen the current path is through the head, it appears that, with sufficient current applied, loss of consciousness almost always occurs swiftly. (This is borne out by some limited self-experimentation by early designers of the electric chair and by research from the field of animal husbandry, where electric stunning has been extensively studied).\n\nOSHA found that up to 80 percent of its electrical injuries involve thermal burns due to arcing faults. The arc flash in an electrical fault produces the same type of light radiation from which electric welders protect themselves using face shields with dark glass, heavy leather gloves, and full-coverage clothing. The heat produced may cause severe burns, especially on unprotected flesh. The arc blast produced by vaporizing metallic components can break bones and damage internal organs. The degree of hazard present at a particular location can be determined by a detailed analysis of the electrical system, and appropriate protection worn if the electrical work must be performed with the electricity on.\n\nThe minimum current a human can feel depends on the current type (AC or DC) as well as frequency for AC. A person can feel at least 1 mA (rms) of AC at 60 Hz, while at least 5 mA for DC. At around 10 milliamperes, AC current passing through the arm of a human can cause powerful muscle contractions; the victim is unable to voluntarily control muscles and cannot release an electrified object. This is known as the \"let go threshold\" and is a criterion for shock hazard in electrical regulations.\n\nThe current may, if it is high enough and is delivered at sufficient voltage, cause tissue damage or fibrillation which can cause cardiac arrest; of AC (rms, 60 Hz) or of DC at high voltage can cause fibrillation. A sustained electric shock from AC at 120 V, 60 Hz is an especially dangerous source of ventricular fibrillation because it usually exceeds the let-go threshold, while not delivering enough initial energy to propel the person away from the source. However, the potential seriousness of the shock depends on paths through the body that the currents take. If the voltage is less than 200 V, then the human skin, more precisely the stratum corneum, is the main contributor to the impedance of the body in the case of a macroshock—the passing of current between two contact points on the skin. The characteristics of the skin are non-linear however. If the voltage is above 450–600 V, then dielectric breakdown of the skin occurs. The protection offered by the skin is lowered by perspiration, and this is accelerated if electricity causes muscles to contract \nabove the let-go threshold for a sustained period of time.\n\nIf an electrical circuit is established by electrodes introduced in the body, bypassing the skin, then the potential for lethality is much higher if a circuit through the heart is established. This is known as a microshock. Currents of only 10 µA can be sufficient to cause fibrillation in this case with a probability of 0.2%.\n\nThe voltage necessary for electrocution depends on the current through the body and the duration of the current. Ohm's law states that the current drawn depends on the resistance of the body. The resistance of human skin varies from person to person and fluctuates between different times of day. The NIOSH states \"Under dry conditions, the resistance offered by the human body may be as high as 100,000 ohms. Wet or broken skin may drop the body's resistance to 1,000 ohms,\" adding that \"high-voltage electrical energy quickly breaks down human skin, reducing the human body's resistance to 500 ohms\".\n\nThe International Electrotechnical Commission gives the following values for the total body impedance of a hand to hand circuit for dry skin, large contact areas, 50 Hz AC currents (the columns contain the distribution of the impedance in the population percentile; for example at 100 V 50% of the population had an impedance of 1875Ω or less):\n\nThe voltage-current characteristic of human skin is non-linear and depends on many factors such as intensity, duration, history, and frequency of the electrical stimulus. Sweat gland activity, temperature, and individual variation also influence the voltage-current characteristic of skin. In addition to non-linearity, skin impedance exhibits asymmetric and time varying properties. These properties can be modeled with reasonable accuracy. Resistance measurements made at low voltage using a standard ohmmeter do not accurately represent the impedance of human skin over a significant range of conditions.\n\nFor sinusoidal electrical stimulation less than 10 volts, the skin voltage-current characteristic is quasilinear. Over time, electrical characteristics can become non-linear. The time required varies from seconds to minutes, depending on stimulus, electrode placement, and individual characteristics.\n\nBetween 10 volts and about 30 volts, skin exhibits non-linear but symmetric electrical characteristics. Above 20 volts, electrical characteristics are both non-linear and symmetric. Skin conductance can increase by several orders of magnitude in milliseconds. This should not be confused with dielectric breakdown, which occurs at hundreds of volts. For these reasons, current flow cannot be accurately calculated by simply applying Ohm's law using a fixed resistance model.\n\n\nThe term \"electrocution,\" coined about the time of the first use of the electric chair in 1890, originally referred only to \"electr\"ical exe\"cution\" and not to accidental or suicidal electrical deaths. However, since no English word was available for non-judicial deaths due to electric shock, the word \"electrocution\" eventually took over as a description of all circumstances of electrical death judicial punishment or not.\n\nThe lethality of an electric shock is dependent on several variables:\n\n\nOther issues affecting lethality are frequency, which is an issue in causing cardiac arrest or muscular spasms. Very high frequency electric current causes tissue burning, but does not penetrate the body far enough to cause cardiac arrest (see electrosurgery). Also important is the pathway: if the current passes through the chest or head, there is an increased chance of death. From a main circuit or power distribution panel the damage is more likely to be internal, leading to cardiac arrest. Another factor is that cardiac tissue has a chronaxie (response time) of about 3 milliseconds, so electricity at frequencies of higher than about 333 Hz requires more current to cause fibrillation than is required at lower frequencies.\n\nThe comparison between the dangers of alternating current at typical power transmission frequences (i.e., 50 or 60 Hz), and direct current has been a subject of debate ever since the War of Currents in the 1880s. Animal experiments conducted during this time suggested that alternating current was about twice as dangerous as direct current per unit of current flow (or per unit of applied voltage).\n\nIt is sometimes suggested that human lethality is most common with alternating current at 100–250 volts; however, death has occurred below this range, with supplies as low as 42 volts. Assuming a steady current flow (as opposed to a shock from a capacitor or from static electricity), shocks above 2,700 volts are often fatal, with those above 11,000 volts being usually fatal, though exceptional cases have been noted. According to a Guinness Book of World Records comic, seventeen-year-old Brian Latasa survived a 230,000 volt shock on the tower of an ultra-high voltage line in Griffith Park, Los Angeles on November 9, 1967. A news report of the event stated that he was \"jolted through the air, and landed across the line\", and though rescued by firemen, he suffered burns over 40% of his body and was completely paralyzed except for his eyelids. The shock with the highest voltage reported survived was that of Harry F. McGrew, who came in contact with a 340,000 volt transmission line in Huntington Canyon, Utah. \n\nThere were 550 reported electrocutions in the US in 1993, 2.1 deaths per million inhabitants. At that time, the incidence of electrocutions was decreasing. Electrocutions in the workplace make up the majority of these fatalities. From 1980–1992, an average of 411 workers were killed each year by electrocution. A recent study conducted by the National Coroners Information System (NCIS) in Australia has revealed three-hundred and twenty-one (321) closed case fatalities (and at least 39 case fatalities still under coronial investigation) that had been reported to Australian coroners where a person died from electrocution between July 2000 and October 2011.\n\nIn Sweden, Denmark, Finland and Norway the number of electric deaths per million inhabitants was 0.6, 0.3, 0.3 and 0.2, respectively, in years 2007-2011.\n\nPeople who survive electrical trauma may suffer a host of injuries including loss of consciousness, seizures, aphasia, visual disturbances, headaches, tinnitus, paresis, and memory disturbances. Even without visible burns, electric shock survivors may be faced with long-term muscular pain and discomfort, fatigue, headache, problems with peripheral nerve conduction and sensation, inadequate balance and coordination, among other symptoms. Electrical injury can lead to problems with neurocognitive function, affecting speed of mental processing, attention, concentration, and memory. The high frequency of psychological problems is well established and may be multifactorial. As with any traumatic and life-threatening experience, electrical injury may result in post traumatic psychiatric disorders. There exist several non-profit research institutes that coordinate rehabilitation strategies for electrical injury survivors by connecting them with clinicians that specialize in diagnosis and treatment of various traumas that arise as a result of electrical injury.\n\nElectric shock is also used as a medical therapy, under carefully controlled conditions:\n\nMild electric shocks are also used for entertainment, especially as a practical joke for example in such devices as a shocking pen or a shocking gum. However devices such as a joy buzzer and most other machines in amusement parks today only use vibration that feels somewhat like an electric shock to someone not expecting it.\n\nElectroshock weapons are incapacitant weapons used for subduing a person by administering electric shock to disrupt superficial muscle functions. One type is a conductive energy device (CED), an electroshock gun popularly known by the brand name \"Taser\", which fires projectiles that administer the shock through a thin, flexible wire. Although they are illegal for personal use in many jurisdictions, Tasers have been marketed to the general public. Other electroshock weapons such as stun guns, stun batons (\"cattle prods\"), and electroshock belts administer an electric shock by direct contact.\n\nElectric fences are barriers that uses electric shocks to deter animals or people from crossing a boundary. The voltage of the shock may have effects ranging from uncomfortable, to painful or even lethal. Most electric fencing is used today for agricultural fencing and other forms of animal control purposes, though it is frequently used to enhance security of restricted areas, and there exist places where lethal voltages are used.\n\nElectric shocks are used as a method of torture, since the received voltage and current can be controlled with precision and used to cause pain and fear without always visibly harming the victim's body.\n\nSuch torture uses electrodes attached to parts of the victim's body: most typically, while wires are wound around the fingers, toes, or tongue; attached to the genitals; or inserted in the vagina to provide a return circuit; the voltage source (typically some sort of prod) of precisely controllable pressure is applied to other sensitive parts of the body, such as the genitals, breasts, or head. The parrilla is an example of this technique. Other methods of electrical torture (such as the picana) do not use a fixed wire but the prod has two electrodes of different polarity a short distance apart so as to make a circuit through the flesh between them when it is placed on the body, thus making it easy for the operator to target the shocks accurately in the places that cause the victim most pain and distress. When the voltage and current is controlled (most typically, high voltage and low current) the victim feels the pain of electric shock but is not physically harmed. Repeated shocks to the genitals will result in the victim losing control of his or her bladder and unintentionally urinating, while extensive passage of the current through the buttocks will cause the victim to unintentionally defecate.\n\nElectrical torture has been used in war and by repressive regimes since the 1930s: The U.S. Army is known to have used electrical torture during World War II and during the Algerian War electrical torture was a favorite method of French military forces; Amnesty International published an official statement that Russian military forces in Chechnya tortured local women with electric shocks by attaching wires onto their breasts; Japanese serial killer Futoshi Matsunaga used electric shocks to control his victims.\n\nAdvocates for the mentally ill and some psychiatrists such as Thomas Szasz have asserted that electroconvulsive therapy (ECT) is torture when used without a \"bona fide\" medical benefit against recalcitrant or non-responsive patients. A similar argument and opposition apply to the use of painful shocks as punishment for behavior modification, a practice that is openly used only at the Judge Rotenberg Institute.\n\nElectric shock delivered by an electric chair is sometimes used as an official means of capital punishment in the United States, although its use has become rare in recent times. Although some original proponents of the electric chair considered it to be a more humane execution method than hanging, shooting, poison gassing, etc., it has now generally been replaced by lethal injections in states that practice capital punishment. Modern reporting has claimed that it sometimes takes several shocks to be lethal, and that the condemned person may actually catch fire before the process is complete.\n\nOther than in parts of the United States, only the Philippines reportedly has used this method, from 1926 to 1976. It was intermittently replaced by the firing squad, until the death penalty was abolished in that country. Electrocution remains legal in at least 5 states (Virginia, Florida, Alabama, North Carolina and Kentucky) of the United States.\n\nUse of insulated gloves , insulated boots, mats and tools.\nProtecting electrical circuit with RCD (Residual-current device).\n\n\n"}
{"id": "16305448", "url": "https://en.wikipedia.org/wiki?curid=16305448", "title": "Equality Act 2010", "text": "Equality Act 2010\n\nThe Equality Act 2010 is an Act of Parliament of the United Kingdom, and has the same goals as the four major EU Equal Treatment Directives, whose provisions it mirrors and implements.\n\nThe primary purpose of the Act is to codify the complicated and numerous array of Acts and Regulations, which formed the basis of anti-discrimination law in Great Britain. This was, primarily, the Equal Pay Act 1970, the Sex Discrimination Act 1975, the Race Relations Act 1976, the Disability Discrimination Act 1995 and three major statutory instruments protecting discrimination in employment on grounds of religion or belief, sexual orientation and age.\n\nThe Act requires equal treatment in access to employment as well as private and public services, regardless of the protected characteristics of age, disability, gender reassignment, marriage and civil partnership, race, religion or belief, sex, and sexual orientation. There are special protections for pregnant women. In the case of gender reassignment, the Act does not guarantee transgender people access to single-sex services where the restrictions are \"a proportionate means of achieving a legitimate aim\". In the case of disability, employers and service providers are under a duty to make reasonable adjustments to their workplaces to overcome barriers experienced by disabled people. In this regard, the Equality Act 2010 did not change the law. Under s.217, with limited exceptions the Act does not apply to Northern Ireland.\n\nThe Labour Party included a commitment to an Equality Bill in its 2005 election manifesto. The \"Discrimination Law Review\" was established in 2005 to develop the legislation and was led by the Government Equalities Office. The review considered the findings of the Equalities Review Panel, chaired by Trevor Phillips, which reported in February 2007. The Act is intended to simplify the law by bringing together existing anti-discrimination legislation. The Equality Act 2010 has replaced the Equal Pay Act 1970, Sex Discrimination Act 1975, Race Relations Act 1976, Disability Discrimination Act 1995, Employment Equality (Religion or Belief) Regulations 2003, Employment Equality (Sexual Orientation) Regulations 2003 and the Employment Equality (Age) Regulations 2006.\n\nPolly Toynbee wrote that the bill, which was drafted under the guidance of Harriet Harman, was \"Labour's biggest idea for 11 years. A public-sector duty to close the gap between rich and poor will tackle the class divide in a way that no other policy has... This new duty to narrow the gap would permeate every aspect of government policy. Its possible ramifications are mind-bogglingly immense.\" One cabinet member described it as \"socialism in one clause\".\n\nSections 104–105 of the Act extend until 2030 the exemption from sex discrimination law, which allows political parties to create all-women shortlists. The exemption was previously permitted by the Sex Discrimination (Election Candidates) Act 2002.\n\nThe Parliamentary process was completed, following a debate, shortly after 11 pm on 6 April 2010, when amendments by the House of Lords were accepted in full.\n\nIn April 2008, Solicitor General Vera Baird announced that as part of the Single Equality Bill, legislation would be introduced to repeal parts of the Act of Settlement 1701 that prevent Roman Catholics or those who marry Roman Catholics from ascending to the throne, and to change the inheritance of the monarchy from cognatic primogeniture to absolute primogeniture, so that the first-born heir would inherit the throne regardless of gender or religion.\n\nHowever, later in 2008 the Attorney General Baroness Scotland of Asthal decided not to sponsor a change in the law of succession, saying, \"To bring about changes to the law on succession would be a complex undertaking involving amendment or repeal of a number of items of related legislation, as well as requiring the consent of legislatures of member nations of the Commonwealth\". The published draft bill did not contain any provisions to change the succession laws. Cognatic primogeniture for the British monarchy was instead abolished separately three years after the Equality Act came into force, with the enactment of the Succession to the Crown Act 2013.\n\nAlthough the act was never going to change the law with regard to churches from its existing position, nor change the binding European Union law which covers many more Roman Catholics than those living in the United Kingdom, and although the position had been spelled out in the High Court in \"R (Amicus) v Secretary of State for Trade and Industry\", a small number of Roman Catholic bishops in England and Wales made claims that they might in future be prosecuted under the Equality Act 2010 for refusing to allow women, married men, transsexual people, and gay people into the priesthood. This claim was rejected by the government. A spokesman said an exemption in the law \"covers ministers of religion such as Catholic priests\" and a document released by the Government Equalities Office states that \"the Equality Bill will not change the existing legal position regarding churches and employment\". The legislation was also criticised by Anglican clergy.\n\nCertain employment is exempted from the act, including:\n\n\n\n\n"}
{"id": "5549818", "url": "https://en.wikipedia.org/wiki?curid=5549818", "title": "Figure of merit", "text": "Figure of merit\n\nA figure of merit is a quantity used to characterize the performance of a device, system or method, relative to its alternatives.\n\n\nBenchmarks are synthetic figures of merit that summarize the speed of computers in performing various typical tasks.\n\nIn modulation systems for communication, figure of merit of a device means the noise figure).\n\nFigure of merit for amplitude modulation (AM) is given by\n\nFigure of merit for double-sideband suppressed-carrier (DSB-SC) receiver or that of a single-sideband (SSB) modulation is always unity. Therefore, noise performance of AM receiver is inferior to that of a DSB-SC receiver or an SSB receiver.\n\nFigure of merit for frequency modulation is given by\n\nThe precision and verifiability of numbers sometimes make them a more effective sales tool than vague and non-numeric descriptions such as \"state of the art\" or \"leaves the others in the dust\". When used in deceptive advertising, the deception lies more in the question of relevance rather than truth since the number quoted, as a figure of merit may not be enough to determine performance when comparing products. For example, when purchasing a laptop a consumer could choose on the basis of the capacity of its hard drive. The RPM, buffer, and seek times may not be noted, but may significantly affect performance.\n\nSome figures such as peak music power are used in selling consumer merchandise and have the principal merit of yielding high numbers that can impress people who don't know what the numbers mean. Other figures such as specific fuel consumption are addressed to engineers and other studious buyers whom the sellers dare not mislead.\n\nAnother example is the megapixel count of a digital camera. A consumer unaware that the number of pixels on a sensor is only one factor in the quality of the image that is captured may, for example, buy a camera with more pixels squeezed onto a smaller image sensor, thus losing quality to the small pixel size.\nMakers of cheap, consumer-market telescopes often tout the magnification power of their products, sidestepping the fact that aperture, optical quality, and the type and quality of the telescope's mount are more important for obtaining a quality image.\n\n"}
{"id": "36787028", "url": "https://en.wikipedia.org/wiki?curid=36787028", "title": "Fluxus (programming environment)", "text": "Fluxus (programming environment)\n\nFluxus is a live coding environment for 3D graphics, music and games. It uses the programming language Racket (a dialect of Scheme/Lisp) to work with a games engine with built-in 3D graphics, physics simulation and sound synthesis. All programming is done on-the-fly, where the code editor appears on top of the graphics that the code is generating. It is an important reference for research and practice in exploratory programming, pedagogy, live performance and games programming.\n\nFluxus is known for hosting some of the most cutting-edge live coding research systems by its author Dave Griffiths, such as the BetaBlocker language inspired by Core War, the Al-Jazari music environment based on interacting robots, the Daisy Chain music environment based on the Petri net model of computation, and the SchemeBricks visual interface for Scheme.\n"}
{"id": "6760277", "url": "https://en.wikipedia.org/wiki?curid=6760277", "title": "Functionality doctrine", "text": "Functionality doctrine\n\nIn United States trademark law, the functionality doctrine prevents manufacturers from protecting specific features of a product by means of trademark law. There are two branches of the functionality doctrine: utilitarian functionality and aesthetic functionality. The rationale behind functionality doctrine is that product markets would not be truly competitive if newcomers could not make a product with a feature that consumers demand. Utilitarian functionality provides grounds to deny federal trademark protection to product features which do something useful. Patent law, not trademark, protects useful processes, machines, and material inventions. Patented designs are presumed to be functional until proven otherwise. Aesthetic functionality provides grounds to deny trademark protection to design features which are included to make the product more aesthetically appealing and commercially desirable. Aesthetic features are within the purview of copyright law, which provides protection to creative and original works of authorship.\n\nCourts will look to the following factors when determining utilitarian functionality:\n\nAs of 2014 the federal circuit courts are split on their utilitarian functionality analysis. Most circuits, such as the Fifth Circuit and the Sixth Circuit follow the Supreme Court's analysis in \"TrafFix Devices, Inc. v. Marketing Displays, Inc.\", which focuses on whether the feature is essential to the use or purpose of the product. The Federal Circuit in contrast focuses its analysis on whether permitting a product feature to be trademarked would impair competitors.\n\nIn the United States, the “functionality” doctrine exists to stop a party from obtaining exclusive trade dress or trademark rights in the functional features of a product or its packaging. The doctrine developed as a way to preserve the division between what trademark law protects and areas that are better protected by patent or copyright law. Thus, the functionality doctrine serves to prevent trademark owners from inhibiting legitimate competition \n\nWhen the aesthetic development of the good is intended to enhance the design and make the product more commercially desirable, trademark protection may be denied because the consumer is drawn to the design. The distinctiveness of the mark serves to identify the product rather than the source, and trademark protection becomes inappropriate. The underlying theory as aesthetics become integrated with functionality, the resulting product strongly resembles product design, which may receive no trademark protection absent secondary meaning.\n\nThis defense is generally seen in the fashion industry. Clothing brands can only be protected if they've acquired secondary meaning, and most of clothing design is held to be functional and is afforded no protection.\n\n"}
{"id": "54444880", "url": "https://en.wikipedia.org/wiki?curid=54444880", "title": "Genetic diagnosis of intersex", "text": "Genetic diagnosis of intersex\n\nIntersex people are born with variations in physical and sex characteristics including those of the chromosomes, gonads, sex hormones, or genitals that, according to the UN Office of the High Commissioner for Human Rights, \"do not fit the typical definitions for male or female bodies\". Such variations may involve genital ambiguity, and combinations of chromosomal genotype and sexual phenotype other than XY-male and XX-female. Preimplantation genetic diagnosis allows the elimination of embryos and fetuses with intersex traits and thus has an impact on discrimination against intersex people.\n\nPreimplantation genetic diagnosis (PGD or PIGD) refers to genetic evaluation of embryos and oocytes prior to implantation. When used to screen for a specific genetic condition, the method also makes it possible to select embryos with intersex conditions for termination. Some national authorities, such as the UK Human Fertilization and Embryology Authority, maintain lists of conditions for which PGD is permissible, including intersex conditions such as 5 alpha reductase deficiency, androgen insensitivity syndrome, congenital adrenal hyperplasia and others.\n\nSurgical interventions on children with intersex conditions are contentious and may lead to selection for other traits like same sex attraction. Though elimination of those identified prenatally to be intersex may be morally permissible, Robert Sparrow states that intersex conditions are comparable to sexual orientation in that harms may be associated with a \"hostile social environment\". He concluded that the acceptability of elimination of intersex conditions has \"uncomfortable\" implications for \"other nonpathological human variations\" that do not affect physical health.\n\nOrganisation Intersex International Australia has quoted research showing pregnancy termination rates of up to 88% in 47,XXY even while the World Health Organization describes the trait as \"compatible with normal life expectancy\", and \"often undiagnosed\". In 2014, it called for the Australian National Health and Medical Research Council to prohibit such interventions, noting a \"close entanglement of intersex status, gender identity and sexual orientation in social understandings of sex and gender norms, and in medical and medical sociology literature\". In 2016, the organization wrote about the sponsorship of lesbian, gay, bisexual, transgender and intersex (LGBTI) events by IVF clinics in Australia, stating that, in addition to ethical issues raised by the elimination of intersex traits, \"sponsorship of \"LGBTI\" events by such businesses raises more ethical issues still, including the nature of community and comprehension of issues relating to intersex bodily diversity\".\n\nIn response to Sparrow, Georgiann Davis argues that such discrimination fails to recognize that many people with intersex traits led full and happy lives, and that the \"intersex community is only \"invisible\" to those who choose to ignore it\", while \"the medical profession, not the intersex trait itself, is a major source of the social and psychological harm that perpetuates intersex stigmatization and the \"hostile social environment\" that individuals with intersex traits encounter\". Jeff Nisker links the elimination of intersex conditions to their pathologization, describing how \"[o]nce a difference becomes a medical disorder to which the medical profession is dedicating time and resources to prevent, procedures to this end become endowed with appropriateness\".\n\nJason Behrmann and Vardit Ravitsky state: \"Parental choice against intersex may ... conceal biases against same-sex attractedness and gender nonconformity.\" In 2014, Morgan Carpenter expressed concern about intersex variations appeared in a list by the Human Fertilisation and Embryology Authority of \"serious\" \"genetic conditions\" that may be de-selected in the United Kingdom. These include 5-alpha-reductase deficiency and androgen insensitivity syndrome, traits evident in elite Olympic-level women athletes and \"the world's first openly intersex mayor\".\n\nIn 2015, the Council of Europe published an Issue Paper on \"Human rights and intersex people\", remarking on a right to life:\n\nCurrently, prenatal testing and hormone treatment to prevent the physical and behavioral expression of intersex traits is available. In 1990, a paper by Heino Meyer-Bahlburg titled \"Will Prenatal Hormone Treatment Prevent Homosexuality?\" was published in the Journal of Child and Adolescent Psychopharmacology. It examined the use of \"prenatal hormone screening or treatment for the prevention of homosexuality\" using research conducted on foetuses with congenital adrenal hyperplasia and other traits.\n\nAlice Dreger, Ellen Feder, and Anne Tamar-Mattis describe how research published by Saroj Nimkarn and Maria New in 2010 constructs \"low interest in babies and men – and even interest in what they consider to be men's occupations and games – as \"abnormal\", and potentially preventable with prenatal dexamethasone\". The authors state that \"weak and unsupported conclusions\" of investigations into the attempted \"prevention of benign behavioral sex variations\" indicates gaps in the ethical management of clinical research.\n\nIn 2012, Hirvikoski and others described a lack of long-term follow-up studies of individuals exposed to prenatal treatment, and the results of a 10-year Swedish study of 43 mothers and children. The authors found evidence of unacceptable side-effects in their study, including neurological consequences. Treatment with dexamethasone was discontinued in Sweden.\n\n\n"}
{"id": "21244171", "url": "https://en.wikipedia.org/wiki?curid=21244171", "title": "Gentry", "text": "Gentry\n\nGentry (from Old French \"genterie\", from \"gentil\", \"high-born, noble\") are \"well-born, genteel and well-bred people\" of high social class, especially in the past. In the United Kingdom, the term \"gentry\" refers to the landed gentry, the majority of the land-owning social class who were typically armigerous (having a coat of arms), but did not have titles of nobility. \"Gentry\", in its widest connotation, refers to people of good social position connected to landed estates (see manorialism), upper levels of the clergy, and \"gentle\" families of long descent who never obtained the official right to bear a coat of arms. The historical term \"gentry\" by itself, so Peter Coss argues, is a construct that historians have applied loosely to rather different societies. Any particular model may not fit a specific society, yet a single definition nevertheless remains desirable. Linguistically, the word \"gentry\" arose to identify the social stratum created by the very small number, by the standards of Continental Europe, of the Peerage of England, and of the parts of Britain, where nobility and titles are inherited by a single person, rather than the family, as usual in Europe. \n\nBefore creation of the gentry, there were analogous traditional social elites. The adjective \"patrician\" (\"of or like a person of high social rank\") describes the governing elites in a medieval metropolis, such as those of the free cities of Italy (Venice and Genoa), and the free imperial cities of Germany and Switzerland, and the Hanseatic League, which were different from the gentry.\n\nThe Indo-Europeans who settled Europe, Western Asia and the Indian subcontinent conceived their societies to be ordered (not divided) in a tripartite fashion, the three parts being castes. Castes came to be further divided, perhaps as a result of greater specialisation.\n\nThe \"classic\" formulation of the caste system as largely described by Georges Dumézil was that of a priestly or religiously occupied caste, a warrior caste, and a worker caste. Dumézil divided the Proto-Indo-Europeans into three categories: sovereignty, military, and productivity (see Trifunctional hypothesis). He further subdivided sovereignty into two distinct and complementary sub-parts. One part was formal, juridical, and priestly, but rooted in this world. The other was powerful, unpredictable, and also priestly, but rooted in the \"other\", the supernatural and spiritual world. The second main division was connected with the use of force, the military, and war. Finally, there was a third group, ruled by the other two, whose role was productivity: herding, farming, and crafts.\n\nThis system of caste roles can be seen in the castes which flourished on the Indian subcontinent and amongst the Italic peoples.\n\nExamples of the Indo-European castes:\n\nKings were born out of the warrior or noble class.\n\nEmperor Constantine convoked the First Council of Nicaea in 325 whose Nicene Creed included belief in \"one holy catholic and apostolic Church\". Emperor Theodosius I made Nicene Christianity the state church of the Roman Empire with the Edict of Thessalonica of 380.\n\nAfter the fall of the Western Roman Empire in the 5th century, there emerged no single powerful secular government in the West, but there was a central ecclesiastical power in Rome, the Catholic Church. In this power vacuum, the Church rose to become the dominant power in the West.\n\nIn essence, the earliest vision of Christendom was a vision of a Christian theocracy, a government founded upon and upholding Christian values, whose institutions are spread through and over with Christian doctrine. The Catholic Church's peak of authority over all European Christians and their common endeavours of the Christian community—for example, the Crusades, the fight against the Moors in the Iberian Peninsula and against the Ottomans in the Balkans—helped to develop a sense of communal identity against the obstacle of Europe's deep political divisions.\n\nThe classical heritage flourished throughout the Middle Ages in both the Byzantine Greek East and Latin West. In Plato's ideal state there are three major classes (producers, auxiliaries and guardians), which was representative of the idea of the \"tripartite soul\", which is expressive of three functions or capacities of the human soul: \"appetites\" (or \"passions\"), \"the spirited element\" and \"reason\" the part that must guide the soul to truth. Will Durant made a convincing case that certain prominent features of Plato's ideal community were discernible in the organization, dogma and effectiveness of \"the\" Medieval Church in Europe:\n\nFor a thousand years Europe was ruled by an order of guardians considerably like that which was visioned by our philosopher. During the Middle Ages it was customary to classify the population of Christendom into laboratores (workers), bellatores (soldiers), and oratores (clergy). The last group, though small in number, monopolized the instruments and opportunities of culture, and ruled with almost unlimited sway half of the most powerful continent on the globe. The clergy, like Plato's guardians, were placed in authority ... by their talent as shown in ecclesiastical studies and administration, by their disposition to a life of meditation and simplicity, and ... by the influence of their relatives with the powers of state and church. In the latter half of the period in which they ruled [800 AD onwards], the clergy were as free from family cares as even Plato could desire [for such guardians] ... [Clerical] Celibacy was part of the psychological structure of the power of the clergy; for on the one hand they were unimpeded by the narrowing egoism of the family, and on the other their apparent superiority to the call of the flesh added to the awe in which lay sinners held them. ...\nGaetano Mosca wrote on the same subject matter in his book The Ruling Class concerning the Medieval Church and its structure that\n\nBeyond the fact that Clerical celibacy functioned as a spiritual discipline it also was guarantor of the independence of the Church.\nthe Catholic Church has always aspired to a preponderant share in political power, it has never been able to monopolize it entirely, because of two traits, chiefly, that are basic in its structure. Celibacy has generally been required of the clergy and of monks. Therefore no real dynasties of abbots and bishops have ever been able to establish themselves. ... Secondly, in spite of numerous examples to the contrary supplied by the warlike Middle Ages, the ecclesiastical calling has by its very nature never been strictly compatible with the bearing of arms. The precept that exhorts the Church to abhor bloodshed has never dropped completely out of sight, and in relatively tranquil and orderly times it has always been very much to the fore.\n\nThe fundamental social structure in Europe in the Middle Ages was between the ecclesiastical hierarchy, nobles i.e. the tenants in chivalry (counts, barons, knights, esquires, franklins) and the ignobles, the villeins, citizens, and burgesses. The division of society into classes of nobles and ignobles, in the smaller regions of medieval Europe was inexact. After the Protestant Reformation, social intermingling between the noble class and the hereditary clerical upper class became a feature in the monarchies of Nordic countries.\nThe gentility is primarily formed on the bases of the medieval societies' two higher estates of the realm, nobility and clergy, both exempted from taxation. Subsequent \"gentle\" families of long descent who never obtained official rights to bear a coat of arms were also admitted to the rural upper-class society: the gentry.\n\nThe three estates\n\nThe widespread three estates order was particularly characteristic of France:\n\nAt the top of the pyramid were the princes and estates of the king or emperor, or with the clergy, the bishops and the pope.\n\nThe feudal system was, for the people of the Middle Ages and early modern period, fitted into a God-given order. The nobility and the third estate were born into their class, and change in social position was slow. Wealth had little influence on what estate one belonged to. The exception was the Medieval Church, which was the only institution where competent men (and women) of merit could reach, in one lifetime, the highest positions in society.\n\nThe first estate comprised the entire clergy, traditionally divided into \"higher\" and \"lower\" clergy. Although there was no formal demarcation between the two categories, the upper clergy were, effectively, clerical nobility, from the families of the second estate or as in the case of Cardinal Wolsey, from more humble backgrounds.\n\nThe second estate was the nobility. Being wealthy or influential did not automatically make one a noble, and not all nobles were wealthy and influential (aristocratic families have lost their fortunes in various ways, and the concept of the \"poor nobleman\" is almost as old as nobility itself). Countries without a feudal tradition did not have a nobility as such.\n\nHistorically in some cultures, members of an upper class often did not have to work for a living, as they were supported by earned or inherited investments (often real estate), although members of the upper class may have had less actual money than merchants. Upper-class status commonly derived from the social position of one's family and not from one's own achievements or wealth. Much of the population that comprised the upper class consisted of aristocrats, ruling families, titled people, and religious hierarchs. These people were usually born into their status, and historically, there was not much movement across class boundaries. This is to say that it was much harder for an individual to move up in class simply because of the structure of society.\n\nIn many countries, the term \"upper class\" was intimately associated with hereditary land ownership and titles. Political power was often in the hands of the landowners in many pre-industrial societies (which was one of the causes of the French Revolution), despite there being no legal barriers to land ownership for other social classes. Power began to shift from upper-class landed families to the general population in the early modern age, leading to marital alliances between the two groups, providing the foundation for the modern upper classes in the West. Upper-class landowners in Europe were often also members of the titled nobility, though not necessarily: the prevalence of titles of nobility varied widely from country to country. Some upper classes were almost entirely untitled, for example, the Szlachta of the Polish-Lithuanian Commonwealth.\n\nBefore the Age of Absolutism, institutions, such as the church, legislatures, or social elites, restrained monarchical power. Absolutism was characterized by the ending of feudal partitioning, consolidation of power with the monarch, rise of state, rise of professional standing armies, professional bureaucracies, the codification of state laws, and the rise of ideologies that justify the absolutist monarchy. Hence, Absolutism was made possible by new innovations and characterized as a phenomenon of Early Modern Europe, rather than that of the Middle Ages, where the clergy and nobility counterbalanced as a result of mutual rivalry.\n\nFrom the middle of the 1860s the privileged position of Baltic Germans in the Russian Empire began to waver. Already during the reign of Nicholas I (1825–55), who was under pressure from Russian nationalists, some sporadic steps had been taken towards the russification of the provinces. Later, the Baltic Germans faced fierce attacks from the Russian nationalist press, which accused the Baltic aristocracy of separatism, and advocated closer linguistic and administrative integration with Russia.\n\nSocial division was based on the dominance of the Baltic Germans which formed the upper classes while the majority of indigenous population, called \"Undeutsch\", composed the peasantry. In the Imperial census of 1897, 98,573 Germans (7.58% of total population) lived in the Governorate of Livonia, 51,017 (7.57%) in the Governorate of Curonia, and 16,037 (3.89%) in the Governorate of Estonia. The social changes faced by the emancipation, both social and national, of the Estonians and Latvians where not taken seriously by the Baltic German gentry. The provisional government of Russia after 1917 revolution gave the Estonians and Latvians self-governance which meant the end of the Baltic German era in Baltics.\n\nThe Lithuanian gentry consisted mainly of Lithuanians, but due to strong ties to Poland, had been culturally Polonized. After the Union of Lublin in 1569, they became less distinguishable from Polish \"szlachta\", although preserved Lithuanian national awareness.\n\nIn the history of the Polish–Lithuanian Commonwealth, \"gentry\" is often used in English to describe the Polish landed gentry (, from \"ziemia\", \"land\"). They were the lesser members of the nobility (the \"szlachta\"), contrasting with the much smaller but more powerful group of \"magnate\" families (sing. \"magnat\", plural \"magnaci\" in Polish), the Magnates of Poland and Lithuania. Compared to the situation in England and some other parts of Europe, these two parts of the overall \"nobility\" to a large extent operated as different classes, and were often in conflict. After the Partitions of Poland, at least in the stereotypes of 19th-century nationalist lore, the magnates often made themselves at home in the capitals and courts of the partitioning powers, while the gentry remained on their estates, keeping the national culture alive.\n\nFrom the 15th century, only the \"szlachta\", and a few patrician bughers from some cities, were allowed to own rural estates of any size, as part of the very extensive szlachta privileges. These restrictions were reduced or removed after the Partitions of Poland, and commoner landowners began to emerge. By the 19th century, there were at least 60,000 \"szlachta\" families, most rather poor, and many no longer owning land. By then the \"gentry\" included many non-noble landowners.\n\nIn Spanish nobility and former Portuguese nobility, see hidalgos and infanzones.\n\nIn Sweden, there was not outright serfdom. Hence, the gentry was basically a class of well-off citizens that had grown from the wealthier or more powerful members of the peasantry. The two historically legally privileged classes in Sweden were the Swedish nobility \"(Adeln)\", a rather small group numerically, and the clergy, which were part of the so-called \"frälse\" (a classification defined by tax exemptions and representation in the diet).\n\nAt the head of the Swedish clergy stood the Archbishop of Uppsala since 1164. The clergy encompassed almost all the educated men of the day and furthermore was strengthened by considerable wealth, and thus it came naturally to play a significant political role. Until the Reformation, the clergy was the first estate but was relegated to the secular estate in the Protestant North Europe.\n\nIn the Middle Ages, celibacy in the Catholic Church had been a natural barrier to the formation of an hereditary priestly class. After compulsory celibacy was abolished in Sweden during the Reformation, the formation of a hereditary priestly class became possible, whereby wealth and clerical positions were frequently inheritable. Hence the bishops and the vicars, who formed the clerical upper class, would frequently have manors similar to those of other country gentry. Hence continued the medieval Church legacy of the intermingling between noble class and clerical upper class and the intermarriage as the distinctive element in several Nordic countries after the Reformation.\n\nSurnames in Sweden can be traced to the 15th century, when they were first used by the Gentry (Frälse), i.e., priests and nobles. The names of these were usually in Swedish, Latin, German or Greek.\n\nThe adoption of Latin names was first used by the Catholic clergy in the 15th century. The given name was preceded by Herr (Sir), such as Herr Lars, Herr Olof, Herr Hans, followed by a Latinized form of patronymic names, e.g., Lars Petersson Latinized as Laurentius Petri. Starting from the time of the Reformation, the Latinized form of their birthplace (Laurentius Petri Gothus, from Östergötland) became a common naming practice for the clergy.\n\nIn the 17th and 18th centuries, the surname was only rarely the original family name of the ennobled; usually, a more imposing new name was chosen. This was a period which produced a myriad of two-word Swedish-language family names for the nobility (very favored prefixes were Adler, \"eagle\"; Ehren – \"ära\", \"honor\"; Silfver, \"silver\"; and Gyllen, \"golden\"). The regular difference with Britain was that it became the new surname of the whole house, and the old surname was dropped altogether.\n\nThe Western Ukrainian Clergy of the Ukrainian Greek Catholic Church were a hereditary tight-knit social caste that dominated western Ukrainian society from the late eighteenth until the mid-20th centuries, following the reforms instituted by Joseph II, Emperor of Austria. Because, like their Orthodox brethren, Ukrainian Catholic priests could marry, they were able to establish \"priestly dynasties\", often associated with specific regions, for many generations. Numbering approximately 2,000–2,500 by the 19th century, priestly families tended to marry within their group, constituting a tight-knit hereditary caste. In the absence of a significant native nobility and enjoying a virtual monopoly on education and wealth within western Ukrainian society, the clergy came to form that group's native aristocracy. The clergy adopted Austria's role for them as bringers of culture and education to the Ukrainian countryside. Most Ukrainian social and political movements in Austrian-controlled territory emerged or were highly influenced by the clergy themselves or by their children. This influence was so great that western Ukrainians were accused of wanting to create a theocracy in western Ukraine by their Polish rivals. The central role played by the Ukrainian clergy or their children in western Ukrainian society would weaken somewhat at the end of the 19th century but would continue until the mid-20th century.\n\nThe British upper classes consist of two sometimes overlapping entities, the peerage and landed gentry; any male member of either may regard himself as a gentleman, in a special sense mutually understood between hereditary members of the class, which will often exclude life peers. In the British peerage, only the senior family member (typically the eldest son) inherits a substantive title (duke, marquess, earl, viscount, baron); these are referred to as peers or lords. The rest of the nobility are referred to as \"landed gentry\" (abbreviated \"gentry\"). Except for the eldest sons of peers, who bear their fathers' inferior titles as \"courtesy titles\" (but for Parliamentary purposes count as commoners), they usually bear no titles apart from the qualifications of esquire or gentleman (which are ranks recognised in law, although now without any legal consequence); exceptions include the baronet (a title corresponding to a hereditary knighthood), those that are knighted (for life, called Sir X Y), Scottish barons (who bear the designation Baron of X after their name), and Scottish lairds (whose names include a description of their lands in the form of a territorial designation).\n\nThe term \"landed gentry\", although originally used to mean nobility, came to be used for the lesser nobility in England around 1540. Once identical, these terms eventually became complementary. The term \"gentry\" by itself, as commonly used by historians, according to Peter Coss, is a construct applied loosely to rather different societies. Any particular model may not fit a specific society, yet a single definition nevertheless remains desirable. Titles, while often considered central to the upper class, are not strictly so. Both Captain Mark Phillips and Vice Admiral Sir Timothy Laurence, the respective first and second husbands of HRH Princess Anne, lacked any rank of peerage at the time of their marriage to Princess Anne. However, the backgrounds of both men were considered to be essentially patrician, and they were thus deemed suitable husbands for a princess.\n\nThe landed gentry is a traditional British social class consisting of gentlemen in the original sense; that is, those who owned land in the form of country estates to such an extent that they were not required to actively work, except in an administrative capacity on their own lands. The estates were often (but not always) made up of tenanted farms, in which case the gentleman could live entirely off rent income.\n\nEsquire (abbreviated Esq.) is a term derived from the French \"écuyer\" (which also gave equerry) the lowest designation for a nobleman, referring only to males, and used to denote a high but indeterminate social status. The most common occurrence of term \"Esquire\" today is the conferral as the suffix \"Esq.\" in order to pay an informal compliment to a male recipient by way of implying gentle birth. In the post-medieval world, the title of \"esquire\" came to apply to all men of the higher landed gentry; an esquire ranked socially above a gentleman but below a knight. In the modern world, where all men are assumed to be gentlemen, the term has often been extended (albeit only in very formal writing) to all men without any higher title. It is used post-nominally, usually in abbreviated form (for example, \"Thomas Smith, Esq.\").\n\nA knight can be either a medieval tenant giving military service as a mounted man-at-arms to a feudal landholder, or a medieval gentleman-soldier, usually high-born, raised by a sovereign to privileged military status after training as a page and squire (for a contemporary reference, see British honours system). In formal protocol, \"Sir\" is the correct styling for a knight or for a baronet, used with (one of) the knight's given name(s) or full name, but not with the surname alone. The equivalent for a woman who holds the title in her own right is Dame; for such women, the title \"Dame\" is used as \"Sir\" for a man, never before the surname on its own. This usage was devised in 1917, derived from the practice, up to the 17th century (and still also in legal proceedings), for the wife of a knight. The wife of a knight or baronet is now styled \"Lady [Surname]\".\n\nThe Colonial American use of \"gentry\" followed the British usage (i.e., landed gentry); before the independence of the United States, Southern planters were often the younger sons of British landowners, who perpetuated the British system in rural Virginia and Charleston, South Carolina, by employing tenant farmers, indentured servants, and chattel slaves. In the Northeastern United States, the gentry included (colonial and British) offshoot families who established the city of Boston, Massachusetts, and Harvard and Yale colleges.\n\nThe families of Virginia (see First Families of Virginia) formed the Virginia gentry class as the old guard of plantation owners in United States. As General Robert E. Lee's paternal ancestors were among the earliest settlers in Virginia, his family was considered among the oldest of the Virginia gentry class.\n\nThe concept of the gentleman farmer as a man who farms mainly for pleasure rather than for profit was not only a model for the Southern gentry, but very much an ideal befitting some of founding fathers of America, such as Thomas Jefferson and George Washington. Washington resumed the life of a gentleman farmer at his Mount Vernon estate in Virginia following his resignation as commander in chief of the army in December 1783.\n\nThe American gentry, even in cases where the family never had obtained official rights to bear a coat of arms in history, bore all the same hallmarks of traditional elite as in the old continent.\n\nFirst Families of Virginia originated with colonists from England who primarily settled at Jamestown and along the James River and other navigable waters in the Colony of Virginia during the 17th century. As there was a propensity to marry within their narrow social scope for many generations, many descendants bear surnames which became common in the growing colony.\n\nMany of the original English colonists considered members of the First Families of Virginia migrated to the Colony of Virginia during the English Civil War and English Interregnum period (1642–60). Royalists left England on the accession to power of Oliver Cromwell and his Parliament. Because most of Virginia's leading families recognised Charles II as King following the execution of Charles I in 1649, Charles II is reputed to have called Virginia his \"Old Dominion\", a nickname that endures today. Most of such early settlers in Virginia were so-called \"Second Sons\". Primogeniture favored the first sons' inheriting lands and titles in England. Virginia evolved in a society of second or third sons of English aristocracy who inherited land grants or land in Virginia. They formed part of the southern elite in America. Many of the great Virginia dynasties traced their roots to families like the Lees and the Fitzhughs, who traced lineage to England's county families and baronial legacies. But not all: even the most humble Virginia immigrants aspired to the English manorial trappings of their betters.\n\nThe Colonial families of Maryland were the leading families in the Province of Maryland. Several also had interests in the Colony of Virginia, and the two are sometimes referred to as the Chesapeake Colonies. Many of the early settlers came from the West Midlands in England, although the Maryland families were composed of a variety of European nationalities, e.g., French, Irish, Welsh, Scottish, and Swedish, in addition to English. Charles I of England granted the province palatinate status under Cecilius Calvert, 2nd Baron Baltimore. The foundational charter created an aristocracy of lords of the manor for Maryland. Maryland was uniquely created as a colony for Catholic aristocracy and landed gentry, but Anglicanism eventually came to dominate, partly through influence from neighbouring Virginia.\n\nThe 'four divisions of society' refers to the model of society in ancient China and was a meritocratic social class system in China and other subsequently influenced Confucian societies. The four castes—gentry, farmers, artisans and merchants—are combined to form the term Shìnónggōngshāng (士農工商).\n\nGentry (士) means different things in different countries. In China, Korea, and Vietnam, this meant that the Confucian scholar gentry that would – for the most part – make up most of the bureaucracy. This caste would comprise both the more-or-less hereditary aristocracy as well as the meritocratic scholars that rise through the rank by public service and, later, by imperial exams. Some sources, such as Xunzi, list farmers before the gentry, based on the Confucian view that they directly contributed to the welfare of the state. In China, the farmer lifestyle is also closely linked with the ideals of Confucian gentlemen. In Japan, this caste essentially equates to the samurai class. In the Edo period, with the creation of the Domains (\"han\") under the rule of Tokugawa Ieyasu, all land was confiscated and reissued as fiefdoms to the \"daimyōs\".\n\nThe small lords, the , were ordered either to give up their swords and rights and remain on their lands as peasants or to move to the castle cities to become paid retainers of the \"daimyōs\". Only a few samurai were allowed to remain in the countryside; the . Some 5 per cent of the population were samurai. Only the samurai could have proper surnames, which after the Meiji Restoration, became compulsory to all inhabitants (see Japanese name).\n\nThere were two leading classes, i.e. the gentry, in the time of feudal Japan: the \"daimyō\" and the \"samurai\". The Confucian ideals in the Japanese culture emphasised the importance of productive members of society, so farmers and fishermen were considered of a higher status than merchants.\n\nEmperor Meiji abolished the samurai's right to be the only armed force in favor of a more modern, Western-style, conscripted army in 1873. Samurai became \"Shizoku\" (), but the right to wear a katana in public was eventually abolished along with the right to execute commoners who paid them disrespect.\n\nIn defining how a modern Japan should be, members of the Meiji government decided to follow in the footsteps of the United Kingdom and Germany, basing the country on the concept of \"noblesse oblige\". Samurai were not to be a political force under the new order. The difference between the Japanese and European feudal systems was that European feudalism was grounded in Roman legal structure, while Japan feudalism had Chinese Confucian morality as its basis.\n\nKorean monarchy and the native ruling upper class existed in Korea until the end of the Japanese occupation. The system concerning the nobility is roughly the same as that of the Chinese nobility.\n\nAs the Jesuits and other preceding monastical orders did during Europe's Dark Ages, the Buddhist monks became the purveyors and guardians of Korea's literary traditions while documenting Korea's written history and legacies from the Silla period to the end of the Goryeo dynasty. Korean Buddhist monks also developed and used the first movable metal type printing presses in history—some 500 years before Gutenberg—to print ancient Buddhist texts. Buddhist monks also engaged in record keeping, food storage and distribution, as well as the ability to exercise power by influencing the Goryeo royal court.\n\nHistorically, the nobles in Europe became soldiers; the aristocracy in Europe can trace their origins to military leaders from the migration period and the Middle Ages. For many years, the British Army, together with the Church, was seen as the ideal career for the younger sons of the aristocracy. Although now much diminished, the practice has not totally disappeared. Such practices are not unique to the British either geographically or historically. As a very practical form of displaying patriotism, it has been at times \"fashionable\" for \"gentlemen\" to participate in the military.\n\nThe fundamental idea of gentry had come to be that of the essential superiority of the fighting man, usually maintained in the granting of arms. At the last, the wearing of a sword on all occasions was the outward and visible sign of a \"gentleman\"; the custom survives in the sword worn with \"court dress\". A suggestion that a gentleman must have a coat of arms was vigorously advanced by certain 19th- and 20th-century heraldists, notably Arthur Charles Fox-Davies in England and Thomas Innes of Learney in Scotland. The significance of a right to a coat of arms was that it was definitive proof of the status of gentleman, but it recognised rather than conferred such a status, and the status could be and frequently was accepted without a right to a coat of arms.\n\n\"Chivalry\" is a term related to the medieval institution of knighthood. It is usually associated with ideals of knightly virtues, honour and courtly love.\n\nChristianity had a modifying influence on the virtues of chivalry, with limits placed on knights to protect and honour the weaker members of society and maintain peace. The church became more tolerant of war in the defence of faith, espousing theories of the just war. In the 11th century, the concept of a \"knight of Christ\" (\"miles Christi\") gained currency in France, Spain and Italy. These concepts of \"religious chivalry\" were further elaborated in the era of the Crusades.\n\nIn the later Middle Ages, wealthy merchants strove to adopt chivalric attitudes. This was a democratisation of chivalry, leading to a new genre called the courtesy book, which were guides to the behaviour of \"gentlemen\".\n\nWhen examining medieval literature, chivalry can be classified into three basic but overlapping areas:\nThese three areas obviously overlap quite frequently in chivalry and are often indistinguishable. Another classification of chivalry divides it into warrior, religious and courtly love strands. One particular similarity between all three of these categories is honour. Honour is the foundational and guiding principle of chivalry. Thus, for the knight, honour would be one of the guides of action.\n\nThe term gentleman (from Latin \"gentilis\", belonging to a race or \"gens\", and \"man\", cognate with the French word \"gentilhomme\", the Spanish \"hombre gentil\" and the Italian \"gentil uomo\" or \"gentiluomo\"), in its original and strict signification, denoted a man of good family, analogous to the Latin \"generosus\" (its invariable translation in English-Latin documents). In this sense the word equates with the French \"gentilhomme\" (\"nobleman\"), which was in Great Britain long confined to the peerage. The term \"gentry\" (from the Old French \"genterise\" for \"gentelise\") has much of the social-class significance of the French \"noblesse\" or of the German \"Adel\", but without the strict technical requirements of those traditions (such as quarters of nobility). To a degree, \"gentleman\" signified a man with an income derived from landed property, a legacy or some other source and was thus independently wealthy and did not need to work.\n\nThe Far East also held similar ideas to the West of what a gentleman is, which are based on Confucian principles. The term \"Jūnzǐ\" (君子) is a term crucial to classical Confucianism. Literally meaning \"son of a ruler\", \"prince\" or \"noble\", the ideal of a \"gentleman\", \"proper man\", \"exemplary person\", or \"perfect man\" is that for which Confucianism exhorts all people to strive. A succinct description of the \"perfect man\" is one who \"combine[s] the qualities of saint, scholar, and gentleman\" (CE). A hereditary elitism was bound up with the concept, and gentlemen were expected to act as moral guides to the rest of society. They were to:\n\nThe opposite of the \"Jūnzǐ\" was the \"Xiǎorén\" (小人), literally \"small person\" or \"petty person\". Like English \"small\", the word in this context in Chinese can mean petty in mind and heart, narrowly self-interested, greedy, superficial, and materialistic.\n\nThe idea of \"noblesse oblige\", \"nobility obliges\", among gentry is, as the \"Oxford English Dictionary\" expresses, that the term \"suggests noble ancestry constrains to honorable behaviour; privilege entails to responsibility\". Being a noble meant that one had responsibilities to lead, manage and so on. One was not to simply spend one's time in idle pursuits.\n\nA coat of arms is a heraldic device dating to the 12th century in Europe. It was originally a cloth tunic worn over or in place of armour to establish identity in battle. The coat of arms is drawn with heraldic rules for a person, family or organisation. Family coats of arms were originally derived from personal ones, which then became extended in time to the whole family. In Scotland, family coats of arms are still personal ones and are mainly used by the head of the family.\n\nEcclesiastical heraldry is the tradition of heraldry developed by Christian clergy. Initially used to mark documents, ecclesiastical heraldry evolved as a system for identifying people and dioceses. It is most formalised within the Catholic Church, where most bishops, including the pope, have a personal coat of arms. Clergy in Anglican, Lutheran, Eastern Catholic, and Orthodox churches follow similar customs.\n\n"}
{"id": "19244324", "url": "https://en.wikipedia.org/wiki?curid=19244324", "title": "Gisela Bleibtreu-Ehrenberg", "text": "Gisela Bleibtreu-Ehrenberg\n\nGisela Bleibtreu-Ehrenberg (born August 2, 1929) is a German sociologist, ethnologist, sexologist, and writer further specializing into the fields of psychology, Indo-European studies, religious studies, and philosophy, since 1980 also increasingly anthropology. As Bleibtreu-Ehrenberg uses these approaches in research particularly in the fields of sexology, homophobia, and prejudice studies, the US \"Society of Lesbian and Gay Anthropologists\" (SOLGA; formerly \"Anthropology Research Group on Homosexuality\", ARGOH) of the American Anthropological Association ranked Bleibtreu-Ehrenberg's works on homophobia as internationally outstanding.\n\nBleibtreu-Ehrenberg studied sociology, psychology, ethnology, religious studies, philosophy and \"Indogermanistik\" (an interdisciplinarian German subject, not identical with purely linguistic Indo-European studies in Anglophone countries, consisting of historical, sociological, cultural, religious, ethnological, philological, and linguistic study relating to Proto-Indo-European and Indo-European peoples and Indo-European languages) in Bonn.\n\nIn 1969, Bleibtreu-Ehrenberg graduated at the University of Bonn, receiving her \"Magister artium\" (comparable to a Master's degree in the Anglo-American educational system) for her thesis \"Homosexualität und Transvestition im Schamanismus\" (\"Homosexuality and transvestition in shamanism\"). In 1970, she received her PhD for her doctoral dissertation on \"Sexuelle Abartigkeit im Urteil der abendländischen Religions-, Geistes-, und Rechtsgeschichte im Zusammenhang mit der Gesellschaftsentwicklung\" (\"The religious, philosophical, and legal construction of sexual deviance in interdependence with the development of Western society\").\n\nAfter university studies Bleibtreu-Ehrenberg became a scientific assistant at the Sociological Institute of Bonn University, and worked as a sociologist, writer, and independent journalist, holding memberships in a number of scientific and political organizations. She was a leading member of \"German Society for Social-Scientific Sexuality Research\". During the late 1980s, Bleibtreu-Ehrenberg was a member of the German-parliament commissioned \"Enquetekommission AIDS\", an inquiry commission researching into the disease's social, legal, and public health care consequences and challenges, a cooperation which spawned her 1989 book \"Angst und Vorurteil\" (see below).\n\nBleibtreu-Ehrenberg is married and lives in Germany.\n\nA label commonly applied to Bleibtreu-Ehrenberg is that of an \"ethno-sociologist\", even though that is not to limit her approaches exclusively to non-Western cultures. Her inclusion of ethnological, cross-cultural approaches serves as one device of many that she uses to globally study and analyze mankind's nature and nurture as well as the differences and interdependencies between the two, aiming for a global perspective also applicable in modern industrialized societies.\n\nFollowing the ideology-critical and identity-critical approach of Frankfurt School's Critical Theory, the emphasis of Bleibtreu-Ehrenberg's mostly post-structural and deconstructionist work (see social constructionism and social constructivism) on socio-psychological prejudice studies lies on the socio-cultural, socio-historical, and socio-psychological research into issues such as Western repression of sensuality (\"Leibfeindlichkeit\") in Indo-European cultures, and extends into research on topics such as deviant sexuality, homophobia, misogyny, gender roles, and patriarchy.\n\nMost of Bleibtreu-Ehrenberg's research is dedicated to cultural deconstruction of ethnocentric Western prejudices, analyzing their origins and later derivations in history, and emphasizes that this cultural nurture also determining social identities must not be misunderstood as man's essentialist human nature. According to Bleibtreu-Ehrenberg, positivist misinterpretation of ethnocentric prejudices as human nature is one of the key maladies of Western civilization, known to Critical Theory as society's totality also influencing much of Western scientific output. For instance, gays were throughout most of Western history said to be effeminate \"by their nature\", while this common stereotype also influenced social identities and behaviors of individual homosexuals, and these resulting identities and behaviors were in turn taken by society as justifying evidence for their prejudice.\n\nOnly two of her works, \"Mannbarkeitsriten\" (1980) and \"Der pädophile Impuls\" (1985/88) deal with anthropological research into the essential cross-cultural and cross-species nature of two particular sexual attractions beyond common Western stereotypes, paederasty (which Bleibtreu-Ehrenberg defines as \"male same-sex paedophilia\") in \"Mannbarkeitsriten\", and paedophilia (defined as \"sexual contact between fertile adults and infertile juveniles\" based upon preference for these activities on behalf of the adult side, rather than situational offences) in general in \"Der pädophile Impuls\".\n\nIn her publications on prejudice studies, both general and specifically sexual deviance-related, Bleibtreu-Ehrenberg incorporates influences of Critical Theory (especially the theory of \"Authoritarian personality\", also see Right-wing authoritarianism), \"Labeling theory\" by George Herbert Mead and Howard S. Becker, \"Social identity theory\" by Henri Tajfel and John Turner, \"Frustration-Aggression hypothesis\" by John Dollard and Neal E. Miller, \"Social learning theory\" by Albert Bandura, \"dispositif\" and \"discourse analysis\" by Michel Foucault, and the concept of \"derivation\" in the sociological sense of the term by Vilfredo Pareto, denoting an irrational, ideological after-the-fact rationalization. Other notable influences include Sigmund Freud, Mircea Eliade, Marija Gimbutas (though only acknowledged by Bleibtreu-Ehrenberg as a summary source for a century of scholarly Central European and Scandinavian \"Indogermanistik\" publications prior to Gimbutas), the concepts of \"magical thinking\" by James George Frazer and of \"the numinous\" by Rudolf Otto.\n\nBleibtreu-Ehrenberg's definition of prejudice is largely identical to Theodor W. Adorno's \"Verblendungszusammenhang\", by which Adorno denotes socially constructed \"delusions\" (\"Verblendungen\") based upon traditional socio-cultural and socio-psychological conditions or relations (\"Verhältnisse\") within Western society, but also takes influence from Foucault's \"dispositif\" or \"apparatus\", whereas according to Bleibtreu-Ehrenberg, traditional ethnocentric prejudices easily adapt to social paradigm shifts throughout history in order to update their rationalizations according to dominant epistemes, with new derivations of the same old prejudices as the result. For example, during the Medieval Age, according to Benedictus Levita, Thomas Aquinas, and the Malleus Maleficarum, same-sex activities were abhorred as the most deadliest sin of all, which was \"superbia\", i. e. the very pride to consider oneself above God and disobey His will as manifested in His most sacred commandments such as that \"sodomia\" (see sodomy), the common Medieval term for these activities, was considered identical to satanism and evil witchcraft, whereas when with the scientific revolution of Early Modernity the responsibility of priests towards sinners increasingly transformed into one of doctors and judges toward those now considered criminally insane, the rationale to keep the ban upon the very same activities along with the ostracization of those who committed such unspeakable \"abominations\" changed to emphasizing the alleged counter-naturality or perversion of their acts and pathologizing those who desired such.\n\nCombining her influences, Bleibtreu-Ehrenberg's synthetized definition of ethnocentric, inherently Authoritarian prejudice is one of socially learned manifest, recurring intrusive thoughts (\"Zwangsvorstellungen\") that may spill over into Allport's Scale discrimination, including violent hate crimes, as a form of obsessive-compulsive behavior (\"Zwangshandlung\"; also see Obsessive-compulsive personality disorder). Due to what Bleibtreu-Ehrenberg terms \"distorted perception\" (\"verzerrte Wahrnehmung\", resembling Bob Altemeyer's \"compartmentalized thinking\") of constructed social reality, the prejudiced aggressor regards themselves as a rectifying, maybe curing agent, as an upholder of \"natural order\" and \"society\", and/or as protector of people they perceive as \"victims\" of the discriminated.\n\nBleibtreu-Ehrenberg's main works consist of the following publications:\n\n\nOf interest to Anglophone readers might also be Bleibtreu-Ehrenberg's English-language publications not mentioned above:\n\n\n\"Tabu Homosexualität\" is considered a foundational standard work in Germanophone research into homophobia, misogyny, patriarchy, general repression of sensuality and particularly repression of sexual deviance (\"Leibfeindlichkeit\"). In spite of not having been translated into any other language as of 2008, since its first publication \"Tabu Homosexualität\" remains treated and quoted as a standard source internationally as well. As of 2008, it is found in a number of Western European libraries, and in the US is even available in libraries in 13 different states.\n\nAccording to Bleibtreu-Ehrenberg's socio-psychological, socio-historical interdisciplinary approach to the topic of homophobia, drawing from research fields such as cultural studies, religious studies, ethnology, philology, and linguistics, the ethnocentric prejudice towards particularly male same-sex attraction and activities in the history of Western, Indo-European cultures is intrinsically identical to misogyny, thus originally gave rise to, and until the modern age maintained, patriarchal structures of Indo-European society. Its roots and cultural elements can be traced back several millennia into Eurasian culture, and were originally based on the subsequent overlapping and conflict-ridden superimposition of the three basic ethnic and cultural strata (see stratification, social stratification, and archaeological horizon) underlying all modern Indo-European cultures.\n\nFrom there, Bleibtreu-Ehrenberg traces the genesis of homophobia via a number of historical derivations in Indo-European societies until the 20th century.\n\nThe book \"Angst und Vorurteil – AIDS-Ängste als Gegenstand der Vorurteilsforschung\" was based on the work of the parliamentary inquiry commission \"Enquetekommission AIDS\", of which Bleibtreu-Ehrenberg was a member, that was formed in order to research into the disease's social, cultural, legal, and public health care consequences and challenges, as well as Bleibtreu-Ehrenberg's own final report brought forth in parliamentary hearings and towards the Helmut Kohl administration.\n\nIn \"Angst und Vorurteil\", Bleibtreu-Ehrenberg on the one hand supplements the structural history of Western \"Leibfeindlichkeit\" (repression of sensuality) she related at a fuller scope in \"Tabu Homosexualität\" before, by pointing out in \"Angst und Vorurteil\" further aspects she had already brushed on in \"Der pädophile Impuls\" four years earlier.\n\nOn the other hand, in \"Angst und Vorurteil\" Bleibtreu-Ehrenberg gives a thorough, comprehensive description of post-WWII scientific prejudice studies, particularly regarding the re-inforcement of traditional Western \"Leibfeindlichkeit\" (prejudices directed against factual or putative sexual deviance) triggered by HIV, and chronicles the field's academic history from its roots. Bleibtreu-Ehrenberg's understanding of prejudice largely builds on Critical Theory and its concept of the Authoritarian personality, but also incorporates, among other schools of prejudice studies, Labeling theory, Social identity theory, Frustration-Aggression hypothesis, Social learning theory, and Foucault's \"dispositif\" and discourse analysis.\n\n\n\n\n"}
{"id": "439570", "url": "https://en.wikipedia.org/wiki?curid=439570", "title": "Hate mail", "text": "Hate mail\n\nHate mail (as electronic, posted, or otherwise) is a form of harassment, usually consisting of invective and potentially intimidating or threatening comments towards the recipient. Hate mail often contains exceptionally abusive, foul or otherwise hurtful language.\n\nThe recipient may receive disparaging remarks concerning their ethnicity, sexuality, gender, religion, intelligence, political ideology, sense of ethics, or sense of aesthetics. The text of hate mail often contains profanity, or it may simply contain a negative, disappropriating message. \n\nSenders of hate mail normally send anonymous letters or pose as someone else (either a different or fictitious individual) in order to avoid being identified and tracked down, as the nature of some hate mail would inevitably result in criminal charges if the sender was identified.\n\nFamous individuals including actors, members of the Royal Family, politicians and sports people are known to have frequently received hate mail. Players and managers have frequently received hate mail from supporters of rival football clubs, and hate mail has also frequently been sent by fans to players and managers of their own club when they feel that their performance or attitude has been inadequate. Racially motivated hate mail has also been a frequent occurrence, as well as hate mail relating to the Sectarian divide between Celtic and Rangers in Glasgow, Scotland, in particular Neil Lennon, who managed Celtic from 2010 to 2014.\n\nVictims of high-profile crimes have also received hate mail, including the parents of murdered and missing children. Sara Payne, whose daughter Sarah was murdered by a paedophile in July 2000, revealed in 2004 that she and her family received a letter while her daughter was still missing, alleging that Sarah had been killed by her father and grandfather. In the years after Sarah's body was found, Sara and her husband Michael received a number of letters criticising them for allowing Sarah and her siblings to play unsupervised on a beach before Sarah's disappearance. Kevin Wells, whose 10-year-old daughter Holly was one of the two girls murdered by school caretaker Ian Huntley at Soham, Cambridgeshire, in August 2002, later revealed that he received a catalogue of hate mail after his daughter's body was found. He was accused of murdering his daughter as well as her friend Jessica Chapman, but also of having murdered Sarah Payne (whose killer Roy Whiting had already been convicted by this stage), and Milly Dowler (the missing Surrey teenager whose body was found in September 2002 but whose killer Levi Bellfield was not convicted until nearly a decade later). Kevin Wells also received letters accusing him of being the leader of an international paedophile ring. He also received religion-influenced hate mail from several people who condemned him and his wife Nicola for allowing his daughter to \"play out on the Sabbath\", as they had been murdered on a Sunday. Milly Dowler's family later received letters from a hoaxer who claimed that the human remains found were not those of their daughter, and that she was in fact living in Poland. \n\nJane Tomlinson, the charity fundraiser who died in September 2007 after a long battle against cancer, received abusive letters, phone calls and e-mails from individuals accusing her of feigning her terminal illness during the final year of her life.\n\nGerry and Kate McCann, whose young daughter Madeleine McCann has been missing since May 2007, have frequently received hate mail since the unsolved disappearance of their daughter, often from individuals accusing them of killing their daughter, while others have berated them for leaving Madeleine and her younger twin siblings unsupervised in their Portuguese holiday home while they attended a party at a neighbouring house, and some have given what have since been identified as false accounts of what might have happened to their daughter.\n\nGraffiti aimed at an individual in the same manner as hate mail has also occurred on many occasions. A notable example during the 1990s was the desecration of the grave of Moors Murders victim Lesley Ann Downey, who had been murdered in December 1964. Some 30 years after her death, a string of messages appeared on her headstone, calling for the release from prison of Moors Murderer Myra Hindley, and making threats to Lesley Ann Downey's mother Ann West, who was at the centre of a campaign to keep Myra Hindley in prison.\n\n\n\n\n"}
{"id": "51695156", "url": "https://en.wikipedia.org/wiki?curid=51695156", "title": "Jeremiah Daniel Baltimore", "text": "Jeremiah Daniel Baltimore\n\nJeremiah Daniel Baltimore (April 15, 1852 – July 29, 1929) was an engineer and educator in Washington, DC. For many years, he was an engineer in the service of the United States Navy and served as chief engineer at the Freedmen's Hospital. He was also a teacher of mechanics, and was responsible for mechanical instruction in the African American schools in the city from 1890 to 1922. He was on the trial board of the Naval battleship USS Texas (1892) and was among the organizers and officers of the Potomac Hospital and Training School. In 1903 he was elected a member of the Franklin Institute of Philadelphia. In 1915 he was made member of the Royal Society for the Encouragement of Art, Manufactures, and Commerce of London.\n\nJeremiah Daniel Baltimore was born in Washington DC on April 15, 1852 to Thomas and Hannah Baltimore. While Thomas was a Catholic, Jeremiah followed his mother and became a Methodist, being baptized at the Wesley Zion Church in 1866. He attended Enoch Ambush's school and then Washington, D.C. public schools. As a boy he liked to experiment with steam and told his mother that he wished to be an engineer. His mother said that this was impossible as she and her father would not be able to get him through school. His experiments continued and he made a steam boiler and attached it to an engine, but this failed. In spite of this, the pastor at his church, Reverend William P. Ryder placed it in an exhibition at the Wesley Zion Sunday School and then it was put on exhibition in the United States Treasury Department where officers and employers were impressed. This encouraged Baltimore to keep trying. Baltimore then used bricks and flower pots to hold molten metal, using a kitchen stove to melt brass, and using a file, old shears and an old knife for cutting, created a steam engine of the horizontal high pressure style with a tubular boiler. The engine was brought to the patent office with the aid of Anthony Bowen, a Washington D.C. City Councilman. The device was widely discussed in the press, appearing the \"Iron Age\" of London and locally in the \"Sunday Chronicle\". The boy then went to the White House and sent a copy of the article to the desk of President Ulysses S. Grant, who then invited Baltimore to his office. Grant gave Baltimore a card to deliver to the Secretary of the Navy to ask that Baltimore be given work in the Navy Yard to learn and work on the machinery.\n\nBaltimore was then given a position as apprentice in the department of steam engineering at the Washington Navy Yard. Baltimore faced a lot of prejudice on account of his color, and after a few months complained. Professor John M. Langston brought the complaints to the Secretary of the Navy who allowed Baltimore to transfer to the Philadelphia Navy Yard. He again was discriminated against, but endeavored to study on his own early in the mornings and after work. His study paid off and he was admitted to the Franklin Institute in Philadelphia. He was the second black person to be admitted at the institute. His apprenticeship continued and every sign of growth and learning was countered with further examples of prejudice and intimidation. In spite of this, he finished his apprenticeship about September 6, 1873, working for three years and six months.\n\nHe then was assigned to the Philadelphia Naval Station on League Island to assist repairs on Monitors. He was released from the job when force sizes reduced. He then worked at a large mill, and next at a manufacturing firm, Sellers & Brother after being refused from Cramp & Sons and multiple previous applications at Sellers & Brother. He resigned for health reasons and moved back to Washington, DC.\n\nIn Washington he became engineer of the United States Coast Survey and opened a general repair shop. On August 2, 1880, he was appointed chief engineer and mechanician at the Freedmen's Hospital. He attended Howard University Medical College in 1880-1881 and received an A.M. (a master's degree) in 1883 from Livingstone College. He was frequently noted for inventing a pyrometer device, for which he received a patent. As an engineer, he was a member of the Mechanics Union and played a key role in merging black and white unions in 1887.\n\nBaltimore was active in Republican politics. In 1876 he was secretary of the 17th District Republicans and was a delegate to the 1876 Republican National Convention in Cincinnati. In 1880, he was made chairman of the group and was again delegate at the National convention in Chicago.\nHe was recommended as a trustee for public schools in 1883 and in 1889. In 1890 he was appointed to take charge of machine instruction in the colored public schools in Washington, a position he held until 1922. His position included the responsibility of teaching at the Armstrong Manual Training School. He was highly respected as a professor, adopting the methods of the United States Bureau of Steam Engineering. One of his techniques was that he constructed an engine made of glass to help the students see the workings of the engine and the effect of heat on water.\n\nIn 1895 he was selected by the Navy to serve as assistant engineer officer on the trial board of the battleship, USS Texas. In 1906, Baltimore was among the organizers of the Potomac Hospital and Training School and was its treasurer. The organizers included Reverend W. J. Howard, Dr. A. R. Collins, and Dr. Harry J. Williams.\n\nBaltimore was very active in DC Society. In 1875, he was elected recording secretary of the Colored Sabbath School Union of Washington, DC. However, Baltimore's belief in scientific reason occasionally put him at odds with the religious community, as in 1876 when he took part in a debate with Dr. J. L. N. Bowen wherein Baltimore argued that geological evidence showed that the first verses of Genesis did not definitely fix the age of the world at about 6,000 years.\n\nHe was a member of a number of literary and debating clubs, including in the late 1870s as an officer of \"The Institute\" led by Charles N. Otey, John Wesley Cromwell, and J. E. Blunheim, and an officer of the Young Men's Brilliant Star Association led by J. G. Mihales and J. Hicks; and in the 1890s as leader of the Literary Lyceum of Metropolitan Zion Wesley Church along with Dr. S. A. Sumby and the National Congressional Lyceum.\n\nIn 1882 he was made president of an association to promote industry among Washington blacks. He was an officer of the Association of Steam Engineers led by William H. Thomas and H. H. Anseer in 1905.\n\nHe was an officer of the Washington Branch of the Negro Development and Exposition Company of the Jamestown Exposition which developed presentations for the Jamestown Exposition in 1907 in Norfolk, Virginia.\n\nHe was a member and one of the first vice presidents of the Oldest Inhabitants Club. Also, he was a Mason and affiliated with the Eureka Lodge, F. A. A. M and the Old Ark Lodge, G. U. O. of O. F.\nBeyond his activities, he was highly honored while still alive. In 1903 he was elected a member of the Franklin Institute of Philadelphia. In 1915 he was made member of the Royal Society for the Encouragement of Art, Manufactures, and Commerce of London.\n\nBaltimore was for a long time a trustee of the Metropolitan A. M. E. Zion Church in Washington, DC. On May 29, 1872 he married Ella V. Waters. Ella died May 7, 1889. She was a member of the Daughters of Levi and the Young Ladies Brilliant Star and upon her death was a member of the Metropolitan A. M. E. Zion church.\n\nBaltimore remarried to Jeanett E. Anderson in November 1908. Jeanett was director of art in the public schools.\n\nHe had two sons, attorney Richard L. and Jeremiah A. He had one daughter, Ella A. Bryant.\n\nBaltimore died the evening of Monday, July 29, 1929. At his death, he was a member of the 19th Street Baptist Church, where his funeral was held.\n"}
{"id": "31255787", "url": "https://en.wikipedia.org/wiki?curid=31255787", "title": "Jewish skeleton collection", "text": "Jewish skeleton collection\n\nThe Jewish skeleton collection was an attempt by the Nazis to create an anthropological display to showcase the alleged racial inferiority of the \"Jewish race\" and to emphasize the Jews' status as \"Untermenschen\" (\"sub-humans\"), in contrast to the German race, which the Nazis considered to be Aryan \"Übermenschen\". The collection was to be housed at the Anatomy Institute at the Reich University of Strasbourg in the annexed region of Alsace, where the initial preparation of the corpses was performed.\n\nThe collection was sanctioned by Reichsführer of the SS Heinrich Himmler, and designed by and under the direction of August Hirt with Rudolf Brandt and Wolfram Sievers, general manager of the Ahnenerbe, being responsible for procuring and preparing the corpses.\n\nWork by Hans-Joachim Lang published in 2004 revealed the identities and family history of all the victims of this project, based on discovery of the prisoner numbers found at Natzweiler-Struthof in records of those vaccinated against typhus at Auschwitz. The list of names has been placed on a memorial at the cemetery where all were buried, at the facility used to murder them, and at the Anatomical Institute where the corpses were found in 1944.\n\nThe project was designed by August Hirt, who directed the phases that were performed before the end of the war ceased the project prior to its completion. Originally the \"specimens\" to be used in the collection were to be Jewish commisars in the Red Army captured on the Eastern front by the Wehrmacht. The 86 individuals ultimately chosen for the collection were obtained from among a pool of 115 Jewish inmates at Auschwitz concentration camp in Occupied Poland. They were chosen for their perceived stereotypical racial characteristics. The initial selections and preparations were carried out by SS-Hauptsturmführer Dr. Bruno Beger and Dr. Hans Fleischhacker, who arrived in Auschwitz in the first half of 1943 and finished the preliminary work by June 15, 1943. \n\nDue to a typhus epidemic at Auschwitz, the candidates chosen for the skeleton collection were quarantined in order to prevent them from becoming ill and ruining their value as anatomical specimens. In that time, the physical measurements were taken from the selected group of people. An excerpt from a letter written by Sievers in June 1943 reports on the preparation and the typhus epidemic: \"Altogether 115 persons were worked on, 79 were Jews, 30 were Jewesses, 2 were Poles, and 4 were Asiatics. At the present time these prisoners are segregated by sex and are under quarantine in the two hospital buildings of Auschwitz.\"\nIn February 1942, Sievers submitted to Himmler, through Rudolf Brandt, a report from which the following is an extract read at the Nuremberg Doctors Trial by General Telford Taylor, Chief Counsel for the prosecution at Nuremberg:\n\nUltimately, 87 of the inmates were shipped to Natzweiler-Struthof. These people were kept for about two weeks in Block 13 of the camp so that they might eat well to improve their appearance for the desired casts of their corpses. The deaths of 86 of these inmates were, in the words of Hirt, \"induced\" in an improvised gassing facility at Natzweiler-Struthof, and their corpses were sent to Strasbourg — 57 men and 29 women. The gassing occurred on August 11, 13, 17, and 19th, conducted by commandant Josef Kramer, who directed the victims to undress, placed the poison in the ventilation, and watched the people fall to their deaths. One victim was shot for fighting to avoid being gassed and thus was not part of the collection. Josef Kramer, acting commandant of Natzweiler-Struthof (who became the commandant at Auschwitz and the last commandant of Bergen Belsen), personally carried out the gassing of the victims, per his testimony at his post-war trial. It is believed that three men died in transport from Auschwitz to Natzweiler-Struthof.\n\nThe next part of the process for this \"collection\" was to make anatomical casts of the bodies prior to reducing them to skeletons. With the approach of the Allies in 1944, there was concern over the possibility that the corpses could be discovered, as they had still not been defleshed. In September 1944, Sievers telegrammed Brandt: \"The collection can be defleshed and rendered unrecognizable. This, however, would mean that the whole work had been done for nothing-at least in part-and that this singular collection would be lost to science, since it would be impossible to make plaster casts afterwards.\"\nSome work had been done at the Anatomical Institute, but the project was never completed. The body casts were not made, and the corpses were not defleshed as skeletons. When the Allies arrived, they found the corpses, some complete and some beheaded, preserved by formalin.\n\nBrandt and Sievers were indicted, tried, and convicted in the Doctors' Trial in Nuremberg, and both were hanged in Landsberg Prison on June 2, 1948. Josef Kramer was convicted of war crimes and hanged in by British executioner Albert Pierrepoint on December 13, 1945. August Hirt, who conceived the project, was sentenced to death in absentia at the Military War Crimes Trial at Metz on 23 December 1953. It was unknown at the time that Hirt had shot himself in the head on June 2, 1945 near the town of Schluchsee, while hiding in the Black Forest. In 1974, Bruno Beger was convicted by a West German court as an accessory to 86 murders for his role in procuring the victims of the Jewish skeleton collection. He was sentenced to three years imprisonment, the minimum sentence, but did not serve any time in prison. According to his family, Beger died in Königstein im Taunus on October 12, 2009. \nFor many years, only a single victim was positively identified through the efforts of Serge and Beate Klarsfeld: Menachem Taffel (prisoner no. 107969), a Polish born Jew who had been living in Berlin. In 2003, Dr. Hans-Joachim Lang, a German professor at the University of Tübingen, succeeded in identifying all the victims by comparing a list of inmate numbers of the 86 corpses at the Reichs University in Strasbourg, surreptitiously recorded by Hirt's French assistant Henri Henrypierre, with a list of numbers of inmates vaccinated at Auschwitz. The names and biographical information of the victims were published in the book \"Die Namen der Nummern\" (\"The Names of the Numbers\"). Rachel Gordon and Joachim Zepelin translated the Introduction to the book into English at the web site where the whole book is posted in German, including the biographies of the 86 people. Lang recounts in detail the story of how he determined the identities of the 86 victims gassed for Dr. August Hirt's project of the Jewish skeleton collection. Forty-six of these individuals were originally from Thessaloniki, Greece. The 86 were from eight countries in German-occupied Europe: Austria, Netherlands, France, Germany, Greece, Norway, Belgium, and Poland.\n\nIn 1951, the remains of the 86 victims were re-interred in one location in the Cronenbourg-Strasbourg Jewish Cemetery. On December 11, 2005, memorial stones engraved with the names of the 86 victims were placed at the cemetery. One is at the site of the mass grave, the other along the wall of the cemetery. Another plaque honoring the victims was placed outside the Anatomy Institute at Strasbourg's University Hospital. On July 9, 2015, French doctor Raphael Toledano discovered at the Forensic Institute's Museum of Strasbourg several tissue samples hidden away, presumed to be from Menachem Taffel. These last remains were buried in the Jewish cemetery of Cronenbourg on September 6, 2015. As journalist and researcher Lang stated, once his long research was published on the identities of the 86 people killed under Hirt's orders, \"The perpetrators should not be allowed to have the final word.\"\n\n\n"}
{"id": "31938666", "url": "https://en.wikipedia.org/wiki?curid=31938666", "title": "Knowledge space (philosophy)", "text": "Knowledge space (philosophy)\n\nIn philosophy and media studies, a knowledge space is described as an emerging anthropological space in which the knowledge of individuals becomes the primary focus for social structure, values, and beliefs. The concept is put forward and explored by philosopher and media critic Pierre Lévy in his 1997 book \"Collective Intelligence\".\n\nLevy's notion of the \"knowledge space\" relies on his conception of anthropological spaces, which he defines as \"a system of proximity (space) unique to the world of humanity (anthropological), and thus dependent on human technologies, significations, language, culture, conventions, representations, and emotions\" (5). Building on the language of the philosophers Gilles Deleuze and Félix Guattari, he states that \"anthropological spaces in themselves are neither infrastructures nor superstructures but planes of existence, frequencies, velocities, determined within the social spectrum\" (147). Each space contains \"worlds of signification\" (149) by which humans come to understand and make sense of the world. Furthermore, although one space may dominate, many spaces can and do exist simultaneously.\n\nLevy describes three existing anthropological spaces. They are:\n\nThe knowledge space is an emerging anthropological space which, while it has always existed (139), is only now coming into fruition as a guiding space of humanity. In this space, singularities (individuals) are recognized as singularities and knowledge becomes the guiding value for humanity. Since all human experience represents unique knowledge, within the knowledge space all individuals are valued for their unique knowledge regardless of race (earth space), nationality (territorial space), or economic status (commodity space). Within this space static identity gives way to the \"quantum identities\" as individuals become participates and the distinction between of \"us\" and \"them\" disappears (159). Instead, humanity forms \"collective intelligences\" in which knowledge is valued and freely traded. What is \"real\" becomes \"that which implies the practical activity, intellectual and imaginary, of living subjects\" (168). Life, experiences, and knowledge become the underlying and ever changing guiding path for human societies.\n\nLevy's theories rely heavily on the technological developments of the 1990s, particularly the rise of biotechnology, nanotechnology, the Internet, new media and information technologies. In chapter 3, he describes how technologies have made a shift from the molar to the molecular (a move which makes literal a distinction by Delueze and Guattari) in that technologies now handle units as individuals (his term is \"singularities\") rather than in mass. He suggests that this mirrors our rising recognition of the individuals as singularities rather than massive conglomerated groups.\n"}
{"id": "11716414", "url": "https://en.wikipedia.org/wiki?curid=11716414", "title": "Language preservation", "text": "Language preservation\n\nLanguage preservation is the effort to prevent languages from becoming unknown. A language is at risk of being lost when it no longer is taught to younger generations, while fluent speakers of the language (usually the elderly) die.\n\nLanguage is an important part of any society, because it enables people to communicate and express themselves. When a language dies out, future generations lose a vital part of the culture that is necessary to completely understand it. This makes language a vulnerable aspect of cultural heritage, and it becomes especially important to preserve it. According to the United Nations Educational, Scientific, and Cultural Organization (UNESCO), from facts published in their \"Atlas of Languages in Danger of Disappearing,\" there are an estimated 6,000 languages spoken worldwide today, and half of the world’s population speaks the eight most common. More than 3,000 languages are reportedly spoken by fewer than 10,000 people each. \"Ethnologue,\" a reference work published by SIL International, has cataloged the world’s known living languages, and it estimates that 417 languages are on the verge of extinction.\n\nThere are different factors that can put a language in danger of becoming extinct. One is when a language is no longer being taught to the children of the community, or at least to a large number of the children. In these cases, the remaining fluent speakers of the language are generally the older members of the community, and when they pass on, the language dies out with them.\n\nChild speakers are not enough to ensure the survival of a language however. If the children who do speak the language are relocated to another area where it is not spoken, it becomes endangered. Political and military turmoil can also endanger a language. When people are forced from their homes into new lands, they may have to learn the language of the new area to adapt, and they end up losing their language. Likewise, when a country or territory is successfully invaded, the population may be forced to learn the invader's language.\n\nA language can also become associated with a lower social class. In this instance, parents will encourage their children to use the language used more often in society to distance themselves from the perceived lower class. Within one or two generations of this occurrence, the language can easily be lost.\n\nWhen a language dies, the knowledge of and ability to understand the culture who spoke it is threatened because the teachings, customs, oral traditions and other inherited knowledge are no longer transmitted among native speakers. As each language dies, science in linguistics, anthropology, prehistory and psychology lose some diversity in data sources.\n\nThere are different ideas about the best ways to preserve a language. One way is to encourage younger generations to speak the language as they grow, so they will then teach their children the language as well. In many cases, this option is nearly impossible. There are often many factors that endanger a language, and it is impossible to control each of these factors to ensure its survival.\n\nThe internet can be used to raise awareness about the issues of language extinction and language preservation. It can be used to translate, catalog, store, and provide information and access to languages. New technologies such as podcasts can be used to preserve the spoken versions of languages, and written documents can preserve information about the native literature and linguistics of languages.\n\nThe international internet provider VeriSign estimates that 65-70% of all internet content is in English.\n\nUsing written documents to preserve information about the native literature and linguistics is also not without potential problems. Just because a language is written down, this does not mean it will survive. Written information in book or manuscript form is subject to acid issues, binding problems, environmental monitoring problems, and security concerns.\n\nTechnology can also be used to preserve the integrity of spoken versions of languages. Many of the same techniques used in recording oral history can be used to preserve spoken languages. Preservationists can use reel-to-reel audio tape recordings, along with video recordings, and new technologies like podcasts to record spoken accounts of languages. Technology is also vulnerable to new technology. Preservation efforts would fail if the technology to listen to or watch certain media such as audio tape recordings or video tapes is lost.\n\nThe Administration for Native Americans has published the \"Reference Guide for Establishing Archives and Repositories,\" which explains why language repositories are vital to long-term language preservation efforts. The guide offers practical advice on what to preserve and why; it explains what a language repository is, how to build one, and the costs involved; and lists other resources for creating an archive and repository.\n\n\n"}
{"id": "5211755", "url": "https://en.wikipedia.org/wiki?curid=5211755", "title": "Laysan millerbird", "text": "Laysan millerbird\n\nThe Laysan millerbird (\"Acrocephalus familiaris familiaris\") was a subspecies of the millerbird, similar in appearance to the remaining subspecies, the Nihoa millerbird. Its dorsal side was brown, and its belly was grayish. Its name derives from its favorite food, several species of moths of the genus \"Agrotis\" (such as the endemic and likewise extinct \"Agrotis laysanensis\") commonly referred to as \"millers\" (Butler & Usinger, 1963).\n\n \nVery tame, it was abundant on Laysan, where it was endemic, in the 1890s (Udvardy, 1996). After the fateful introduction of domestic rabbits in 1903, which nearly denuded the island of vegetation in the next few years, the birds probably declined rapidly. Supposedly, there were 1500 still alive in April 1915 as reported by the USCGC \"Thetis\" expedition (Clapp \"et al.\", 1996), but a thorough 1911 census by the State University of Iowa expedition had found only \"a few\" (Dill & Bryan, 1912), as did a brief visit in February 1916. As land bird populations on Laysan fluctuate heavily and because there was considerable poaching for the Japanese millinery trade in the 1910s, the supposed 1915 figure cannot be discounted, but it seems highly improbable. At any rate, the 1923 expedition by the reported only one unconfirmed sighting which seems to have been erroneous (Olson, 1996). Thus, it can be concluded that the bird disappeared at some time in the late 1910s.\n\nAs the vegetation disappeared, the bird suffered increased egg predation by Laysan finches (\"Telespiza cantans\"), ruddy turnstones (\"Arenaria interpres\") and bristle-thighed curlews (\"Numenius tahitiensis\"), as well as increased competition for food and nesting habitat; a small patch of tree tobacco (\"Nicotiana glauca\") was the only locality left where the millerbird, the Laysan rail (\"Porzana palmeri\") and the Laysan honeycreeper (\"Himatione fraithii\") could nest with a reasonable chance of success. Additionally, the moths which formed its main food source became likewise extinct or exceedingly rare as their food plants were eaten by the rabbits, and thus the only significant food left were brine flies, which, though abundant, would also be utilized by the other land birds and the Laysan duck (\"Anas laysanensis\"), all of which were more aggressive than the millerbird. It is most likely that the Laysan millerbird was the first of the three avian taxa to have gone extinct on Laysan, the last individuals of the apapane disappearing in a sandstorm around April 24, 1923 and the rail also disappearing around that time.\n\n"}
{"id": "50359719", "url": "https://en.wikipedia.org/wiki?curid=50359719", "title": "Lessons learned", "text": "Lessons learned\n\nLessons learned or lessons learnt are experiences distilled from a project that should be actively taken into account in future projects. \n\nThere are several definitions of the concept. The one used by the National Aeronautics and Space Administration, European Space Agency and Japan Aerospace Exploration Agency sounds as follows: \n“A lesson learned is knowledge or understanding gained by experience. The experience may be positive, as in a successful test or mission, or negative, as in a mishap or failure...A lesson must be significant in that it has a real or assumed impact on operations; valid in that is factually and technically correct; and applicable in that it identifies a specific design, process, or decision that reduces or eliminates the potential for failures and mishaps, or reinforces a positive result.” \n\nThe Development Assistance Committee of the Organisation for Economic Co-operation and Development defines lessons learned as “Generalizations based on evaluation experiences with projects, programs, or policies that abstract from the specific circumstances to broader situations. Frequently, lessons highlight strengths or weaknesses in preparation, design, and implementation that affect performance, outcome, and impact.” \n\nIn the practice of the United Nations the concept has been made explicit in the name of their Working Group on Lessons Learned of the Peacebuilding Commission.\nIn the military field, conducting a Lessons learned analysis requires a leader-led after-actions debriefing. These debriefings require the leader to extend the lessons-learned orientation of the standard after-action review. He uses the event reconstruction approach or has the individuals present their own roles and perceptions of the event, whichever best fits the situation and time available.\n\n"}
{"id": "77041", "url": "https://en.wikipedia.org/wiki?curid=77041", "title": "Maya (religion)", "text": "Maya (religion)\n\nMaya (; Devanagari: माया, IAST: \"māyā\"), literally \"illusion\" or \"magic\", has multiple meanings in Indian philosophies depending on the context. In ancient Vedic literature, Māyā literally implies extraordinary power and wisdom. In later Vedic texts and modern literature dedicated to Indian traditions, Māyā connotes a \"magic show, an illusion where things appear to be present but are not what they seem\". Māyā is also a spiritual concept connoting \"that which exists, but is constantly changing and thus is spiritually unreal\", and the \"power or the principle that conceals the true character of spiritual reality\".\n\nIn Buddhism, Maya is the name of Gautama Buddha's mother. In Hinduism, Maya is also an epithet for goddess, and the name of a manifestation of Lakshmi, the goddess of \"wealth, prosperity and love\". Maya is also a name for girls.\n\n\"Māyā\" (Sanskrit: माया) is a word with unclear etymology, probably comes from the root \"mā\" which means \"to measure\".\n\nAccording to Monier Williams, \"māyā\" meant \"wisdom and extraordinary power\" in an earlier older language, but from the Vedic period onwards, the word came to mean \"illusion, unreality, deception, fraud, trick, sorcery, witchcraft and magic\". However, P. D. Shastri states that the Monier Williams' list is a \"loose definition, misleading generalization\", and not accurate in interpreting ancient Vedic and medieval era Sanskrit texts; instead, he suggests a more accurate meaning of \"māyā\" is \"appearance, not mere illusion\".\n\nAccording to William Mahony, the root of the word may be \"man-\" or \"to think\", implying the role of imagination in the creation of the world. In early Vedic usage, the term implies, states Mahony, \"the wondrous and mysterious power to turn an idea into a physical reality\".\n\nFranklin Southworth states the word's origin is uncertain, and other possible roots of \"māyā\" include \"may-\" meaning mystify, confuse, intoxicate, delude, as well as \"māy-\" which means \"disappear, be lost\".\n\nJan Gonda considers the word related to \"mā\", which means \"mother\", as do Tracy Pintchman and Adrian Snodgrass, serving as an epithet for goddesses such as Lakshmi. Maya here implies art, is the maker’s power, writes Zimmer, \"a mother in all three worlds\", a creatrix, her magic is the activity in the Will-spirit.\n\nA similar word is also found in the Avestan \"māyā\" with the meaning of \"magic power\".\n\nWords related to and containing \"Māyā\", such as \"Mayava\", occur many times in the Vedas. These words have various meanings, with interpretations that are contested, and some are names of deities that do not appear in texts of 1st millennium BCE and later. The use of word Māyā in Rig veda, in the later era context of \"magic, illusion, power\", occurs in many hymns. One titled Māyā-bheda (मायाभेद:, Discerning Illusion) includes hymns 10.177.1 through 10.177.3, as the battle unfolds between the good and the evil, as follows,\n\nThe above Maya-bheda hymn discerns, using symbolic language, a contrast between mind influenced by light (sun) and magic (illusion of Asura). The hymn is a call to discern one's enemies, perceive artifice, and distinguish, using one's mind, between that which is perceived and that which is unperceived. Rig veda does not connote the word Māyā as always good or always bad, it is simply a form of technique, mental power and means. Rig veda uses the word in two contexts, implying that there are two kinds of Māyā: divine Māyā and undivine Māyā, the former being the foundation of truth, the latter of falsehood.\n\nElsewhere in Vedic mythology, Indra uses Maya to conquer Vritra. Varuna's supernatural power is called Maya. \"Māyā\", in such examples, connotes powerful magic, which both \"devas\" (gods) and \"asuras\" (demons) use against each other. In the Yajurveda, \"māyā\" is an unfathomable plan. In the Aitareya Brahmana Maya is also referred to as Dirghajihvi, hostile to gods and sacrifices. The hymns in Book 8, Chapter 10 of Atharvaveda describe the primordial woman \"Virāj\" (विराज्, chief queen) and how she willingly gave the knowledge of food, plants, agriculture, husbandry, water, prayer, knowledge, strength, inspiration, concealment, charm, virtue, vice to gods, demons, men and living creatures, despite all of them making her life miserable. In hymns of 8.10.22, \"Virāj\" is used by Asuras (demons) who call her as Māyā, as follows,\n\nThe contextual meaning of Maya in Atharvaveda is \"power of creation\", not illusion. Gonda suggests the central meaning of Maya in Vedic literature is, \"wisdom and power enabling its possessor, or being able itself, to create, devise, contrive, effect, or do something\". Maya stands for anything that has real, material form, human or non-human, but that does not reveal the hidden principles and implicit knowledge that creates it. An illustrative example of this in Rig veda VII.104.24 and Atharva veda VIII.4.24 where Indra is invoked against the Maya of sorcerers appearing in the illusory form – like a \"fata morgana\" – of animals to trick a person.\n\nThe Upanishads describe the universe, and the human experience, as an interplay of Purusha (the eternal, unchanging principles, consciousness) and Prakṛti (the temporary, changing material world, nature). The former manifests itself as Ātman (Soul, Self), and the latter as Māyā. The Upanishads refer to the knowledge of Atman as \"true knowledge\" (\"Vidya\"), and the knowledge of Maya as \"not true knowledge\" (\"Avidya\", Nescience, lack of awareness, lack of true knowledge). Brihadaranyaka Upanishad, states Ben-Ami Scharfstein, describes Maya as \"the tendency to imagine something where it does not exist, for example, atman with the body\". To the Upanishads, knowledge includes empirical knowledge and spiritual knowledge, complete knowing necessarily includes understanding the hidden principles that work, the realization of the soul of things.\n\nHendrick Vroom explains, \"The term \"Maya\" has been translated as 'illusion,' but then it does not concern normal illusion. Here 'illusion' does not mean that the world is not real and simply a figment of the human imagination. \"Maya\" means that the world is not as it seems; the world that one experiences is misleading as far as its true nature is concerned.\" Lynn Foulston states, \"The world is both real and unreal because it exists but is 'not what it appears to be'.\" According to Wendy Doniger, \"to say that the universe is an illusion (māyā) is not to say that it is unreal; it is to say, instead, that it is not what it seems to be, that it is something constantly being made. Māyā not only deceives people about the things they think they know; more basically, it limits their knowledge.\"\n\nMāyā pre-exists and co-exists with Brahman – the Ultimate Principle, Consciousness. Maya is perceived reality, one that does not reveal the hidden principles, the true reality. Maya is unconscious, Atman is conscious. Maya is the literal, Brahman is the figurative \"Upādāna\" – the principle, the cause. Maya is born, changes, evolves, dies with time, from circumstances, due to invisible principles of nature, state the Upanishads. Atman-Brahman is eternal, unchanging, invisible principle, unaffected absolute and resplendent consciousness. Maya concept in the Upanishads, states Archibald Gough, is \"the indifferent aggregate of all the possibilities of emanatory or derived existences, pre-existing with Brahman\", just like the possibility of a future tree pre-exists in the seed of the tree.\n\nThe concept of Maya appears in numerous Upanishads. The verses 4.9 to 4.10 of Svetasvatara Upanishad, is the oldest explicit occurrence of the idea that Brahman (Supreme Soul) is the hidden reality, nature is magic, Brahman is the magician, human beings are infatuated with the magic and thus they create bondage to illusions and delusions, and for freedom and liberation one must seek true insights and correct knowledge of the principles behind the hidden magic. Gaudapada in his Karika on Mandukya Upanishad explains the interplay of Atman and Maya as follows,\n\nSarvasara Upanishad refers to two concepts: \"Mithya\" and \"Maya\". It defines \"Mithya\" as illusion and calls it one of three kinds of substances, along with Sat (Be-ness, True) and Asat (not-Be-ness, False). \"Maya\", Sarvasara Upanishad defines as all what is not Atman. Maya has no beginning, but has an end. Maya, declares Sarvasara, is anything that can be studied and subjected to proof and disproof, anything with Guṇas. In the human search for Self-knowledge, Maya is that which obscures, confuses and distracts an individual.\n\nIn Puranas and Vaishnava theology, \"māyā\" is described as one of the nine shaktis of Vishnu. \"Māyā\" became associated with sleep; and Vishnu's \"māyā\" is sleep which envelopes the world when he awakes to destroy evil. Vishnu, like Indra, is the master of \"māyā\"; and \"māyā\" envelopes Vishnu's body. The \"Bhagavata Purana\" narrates that the sage Markandeya requests Vishnu to experience his \"māyā\". Vishnu appears as an infant floating on a fig leaf in a deluge and then swallows the sage, the sole survivor of the cosmic flood. The sage sees various worlds of the universe, gods etc. and his own hermitage in the infant's belly. Then the infant breathes out the sage, who tries to embrace the infant, but everything disappears and the sage realizes that he was in his hermitage the whole time and was given a flavor of Vishnu's \"māyā\". The magic creative power, \"Māyā\" was always a monopoly of the central Solar God; and was also associated with the early solar prototype of Vishnu in the early Aditya phase.\n\nIn Sangam period Tamil literature, Krishna is found as \"māyon\"; with other attributed names are such as Mal, Tirumal, Perumal and Mayavan. In the Tamil classics, Durga is referred to by the feminine form of the word, viz., \"māyol\"; wherein she is endowed with unlimited creative energy and the great powers of Vishnu, and is hence \"Vishnu-Maya\".\n\nMaya, to Shaiva Siddhanta sub-school of Hinduism, states Hilko Schomerus, is reality and truly existent, and one that exists to \"provide Souls with \"Bhuvana\" (a world), \"Bhoga\" (objects of enjoyment), \"Tanu\" (a body) and \"Karana\" (organs)\".\n\nThe various schools of Hinduism, particularly those based on naturalism (Vaiśeṣika), rationalism (Samkhya) or ritualism (Mimamsa), questioned and debated what is Maya, and the need to understand Maya. The Vedanta and Yoga schools explained that complete realization of knowledge requires both the understanding of ignorance, doubts and errors, as well as the understanding of invisible principles, incorporeal and the eternal truths. In matters of Self-knowledge, stated Shankara in his commentary on Taittiriya Upanishad, one is faced with the question, \"Who is it that is trying to know, and how does he attain Brahman?\" It is absurd, states Shankara, to speak of one becoming himself; because \"Thou Art That\" already. Realizing and removing ignorance is a necessary step, and this can only come from understanding Maya and then looking beyond it.\n\nThe need to understand Maya is like the metaphorical need for road. Only when the country to be reached is distant, states Shankara, that a road must be pointed out. It is a meaningless contradiction to assert, \"I am right now in my village, but I need a road to reach my village.\" It is the confusion, ignorance and illusions that need to be repealed. It is only when the knower sees nothing else but his Self that he can be fearless and permanent. Vivekananda explains the need to understand Maya as follows (abridged),\n\nThe text Yoga Vasistha explains the need to understand Maya as follows,\n\nThe early works of Samkhya, the rationalist school of Hinduism, do not identify or directly mention the Maya doctrine. The discussion of Maya theory, calling it into question, appears after the theory gains ground in Vedanta school of Hinduism. Vācaspati Miśra's commentary on the \"Samkhyakarika\", for example, questions the Maya doctrine saying \"It is not possible to say that the notion of the phenomenal world being real is false, for there is no evidence to contradict it\". Samkhya school steadfastly retained its duality concept of Prakrti and Purusha, both real and distinct, with some texts equating Prakrti to be Maya that is \"not illusion, but real\", with three Guṇas in different proportions whose changing state of equilibrium defines the perceived reality.\n\nJames Ballantyne, in 1885, commented on Kapila's Sánkhya aphorism 5.72 which he translated as, \"everything except nature and soul is uneternal\". According to Ballantyne, this aphorism states that the mind, ether, etc. in a state of cause (not developed into a product) are called Nature and not Intellect. He adds, that scriptural texts such as Shvetashvatara Upanishad to be stating \"He should know Illusion to be Nature and him in whom is Illusion to be the great Lord and the world to be pervaded by portions of him'; since Soul and Nature are also made up of parts, they must be uneternal\". However, acknowledges Ballantyne, Edward Gough translates the same verse in Shvetashvatara Upanishad differently, 'Let the sage know that Prakriti is Maya and that Mahesvara is the Mayin, or arch-illusionist. All this shifting world is filled with portions of him'. In continuation of the Samkhya and Upanishadic view, in the Bhagavata philosophy, Maya has been described as 'that which appears even when there is no object like silver in a shell and which does not appear in the atman'; with maya described as the power that creates, maintains and destroys the universe.\n\nThe realism-driven Nyaya school of Hinduism denied that either the world (Prakrti) or the soul (Purusa) are an illusion. Naiyayikas developed theories of illusion, typically using the term \"Mithya\", and stated that illusion is simply flawed cognition, incomplete cognition or the absence of cognition. There is no deception in the reality of Prakrti or \"Pradhana\" (creative principle of matter/nature) or Purusa, only confusion or lack of comprehension or lack of cognitive effort, according to Nyaya scholars. To them, illusion has a cause, that rules of reason and proper \"Pramanas\" (epistemology) can uncover.\n\nIllusion, stated Naiyayikas, involves the projection into current cognition of predicated content from memory (a form of rushing to interpret, judge, conclude). This \"projection illusion\" is misplaced, and stereotypes something to be what it is not. The insights on theory of illusion by Nyaya scholars were later adopted and applied by Advaita Vedanta scholars.\n\nMaya in Yoga school is the manifested world and implies divine force. Yoga and Maya are two sides of the same coin, states Zimmer, because what is referred to as Maya by living beings who are enveloped by it, is Yoga for the Brahman (Universal Principle, Supreme Soul) whose yogic perfection creates the Maya. Maya is neither illusion nor denial of perceived reality to the Yoga scholars, rather Yoga is a means to perfect the \"creative discipline of mind\" and \"body-mind force\" to transform Maya.\n\nThe concept of Yoga as power to create Maya has been adopted as a compound word \"Yogamaya\" (योगमाया) by the theistic sub-schools of Hinduism. It occurs in various mythologies of the Puranas; for example, Shiva uses his \"yogamāyā\" to transform Markendeya's heart in Bhagavata Purana's chapter 12.10, while Krishna counsels Arjuna about \"yogamāyā\" in hymn 7.25 of Bhagavad Gita.\n\nMaya is a prominent and commonly referred to concept in Vedanta philosophies. Maya is often translated as \"illusion\", in the sense of \"appearance\". Human mind constructs a subjective experience, states Vedanta school, which leads to the peril of misunderstanding Maya as well as interpreting Maya as the only and final reality. Vedantins assert the \"perceived world including people are not what they appear to be\". There are invisible principles and laws at work, true invisible nature in others and objects, and invisible soul that one never perceives directly, but this invisible reality of Self and Soul exists, assert Vedanta scholars. Māyā is that which manifests, perpetuates a sense of false duality (or divisional plurality). This manifestation is real, but it obfuscates and eludes the hidden principles and true nature of reality. Vedanta school holds that liberation is the unfettered realization and understanding of these invisible principles – the Self, that the Self (Soul) in oneself is same as the Self in another and the Self in everything (Brahman). The difference within various sub-schools of Vedanta is the relationship between individual soul and cosmic soul (Brahman). Non-theistic Advaita sub-school holds that both are One, everyone is thus deeply connected Oneness, there is God in everyone and everything; while theistic Dvaita and other sub-schools hold that individual souls and God's soul are distinct and each person can at best love God constantly to get one's soul infinitely close to His Soul.\n\nAdvaita Vedanta\n\nIn Advaita Vedanta philosophy, there are two realities: \"Vyavaharika\" (empirical reality) and \"Paramarthika\" (absolute, spiritual reality). Māyā is the empirical reality that entangles consciousness. Māyā has the power to create a bondage to the empirical world, preventing the unveiling of the true, unitary Self—the Cosmic Spirit also known as Brahman. The theory of māyā was developed by the ninth-century Advaita Hindu philosopher Adi Shankara. However, competing theistic Dvaita scholars contested Shankara's theory, and stated that Shankara did not offer a theory of the relationship between Brahman and Māyā. A later Advaita scholar Prakasatman addressed this, by explaining, \"Maya and Brahman together constitute the entire universe, just like two kinds of interwoven threads create a fabric. Maya is the manifestation of the world, whereas Brahman, which supports Maya, is the cause of the world.\"\n\nMāyā is a fact in that it is the appearance of phenomena. Since Brahman is the sole metaphysical truth, Māyā is true in epistemological and empirical sense; however, Māyā is not the metaphysical and spiritual truth. The spiritual truth is the truth forever, while what is empirical truth is only true for now. Since Māyā is the perceived material world, it is true in perception context, but is \"untrue\" in spiritual context of Brahman. Māyā is not false, it only clouds the inner Self and principles that are real. True Reality includes both \"Vyavaharika\" (empirical) and \"Paramarthika\" (spiritual), the Māyā and the Brahman. The goal of spiritual enlightenment, state Advaitins, is to realize Brahman, realize the fearless, resplendent Oneness.\n\nVivekananda said: \"When the Hindu says the world is Maya, at once people get the idea that the world is an illusion. This interpretation has some basis, as coming through the Buddhistic philosophers, because there was one section of philosophers who did not believe in the external world at all. But the Maya of the Vedanta, in its last developed form, is neither Idealism nor Realism, nor is it a theory. It is a simple statement of facts — what we are and what we see around us.\"\n\nThe Early Buddhist Texts contain some references to illusion, the most well known of which is the \"Pheṇapiṇḍūpama Sutta\" in Pali (and with a Chinese Agama parallel at SĀ 265) which states:\n\nSuppose, monks, that a magician (māyākāro) or a magician’s apprentice (māyākārantevāsī) would display a magical illusion (māyaṃ) at a crossroads. A man with good sight would inspect it, ponder, and carefully investigate it, and it would appear to him to be void (rittaka), hollow (tucchaka), coreless (asāraka). For what core (sāro) could there be in a magical illusion (māyāya)? So too, monks, whatever kind of cognition there is, whether past, future, or present, internal or external, gross or subtle, inferior or superior, far or near: a monk inspects it, ponders it, and carefully investigates it, and it would appear to him to be void (rittaka), hollow (tucchaka), coreless (asāraka). For what core (sāro) could there be in cognition?\n\nOne sutra in the Āgama collection known as \"Mahāsūtras\" of the (Mūla)Sarvāstivādin tradition entitled the \"Māyājāla\" (Net of Illusion) deals especially with the theme of Maya. This sutra only survives in Tibetan translation and compares the five aggregates with further metaphors for illusion, including: an echo, a reflection in a mirror, a mirage, sense pleasures in a dream and a madman wandering naked.\n\nThese texts give the impression that māyā refers to the insubstantial and essence-less nature of things as well as their deceptive, false and vain character. \n\nLater texts such as the Lalitavistara also contain references to illusion:\n\nThe Salistamba Sutra also puts much emphasis on illusion, describing all dharmas as being “characterized as illusory” and “vain, hollow, without core”. Likewise the Mahāvastu, a highly influential Mahāsāṃghikan text on the life of the Buddha, states that the Buddha “has shown that the aggregates are like a lightning flash, as a bubble, or as the white foam on a wave.”\n\nIn Theravada Buddhism 'Māyā' is the name of the mother of the Buddha as well as a metaphor for the consciousness aggregate (\"viññana\"). The Theravada monk Bhikkhu Bodhi considers the Pali \"Pheṇapiṇḍūpama Sutta\" “one of the most radical discourses on the empty nature of conditioned phenomena.” Bodhi also cites the Pali commentary on this sutra, the \"Sāratthappakāsinī\" (Spk), which states:\n\nCognition is like a magical illusion (māyā) in the sense that it is insubstantial and cannot be grasped. Cognition is even more transient and fleeting than a magical illusion. For it gives the impression that a person comes and goes, stands and sits, with the same mind, but the mind is different in each of these activities. Cognition deceives the multitude like a magical illusion (māyā).\n\nLikewise, Bhikkhu Katukurunde Nyanananda Thera has written an exposition of the \"Kàlakàràma Sutta\" which features the image of a magical illusion as its central metaphor.\n\nThe \"Nyānānusāra Śāstra\", a Vaibhāṣika response to Vasubandhu’s Abhidharmakosha cites the \"Māyājāla sutra\" and explains:\n\n“Seeing an illusory object (māyā)”: Although what one apprehends is unreal, nothing more than an illusory sign. If one does not admit this much, then an illusory sign should be non-existent. What is an illusory sign? It is the result of illusion magic. Just as one with higher gnosis can magically create forms, likewise this illusory sign does actually have manifestation and shape. Being produced by illusion magic, it acts as the object of vision. That object which is taken as really existent is in fact ultimately non-existent. Therefore, this [Māyājāla] Sūtra states that it is non-existent, due to the illusory object there is a sign but not substantiality. Being able to beguile and deceive one, it is known as a “deceiver of the eye.”\n\nIn Mahayana sutras, illusion is an important theme of the Prajñāpāramitā sutras. Here, the magician's illusion exemplifies how people misunderstand and misperceive reality, which is in fact empty of any essence and cannot be grasped. The Mahayana uses similar metaphors for illusion: magic, a dream, a bubble, a rainbow, lightning, the moon reflected in water, a mirage, and a city of celestial musicians.\" Understanding that what we experience is less substantial than we believe is intended to serve the purpose of liberation from ignorance, fear, and clinging and the attainment of enlightenment as a Buddha completely dedicated to the welfare of all beings. The Prajñaparamita texts also state that all dharmas (phenomena) are like an illusion, not just the five aggregates, but all beings, including Bodhisattvas and even Nirvana. The \"Prajñaparamita-ratnaguna-samcayagatha\" (Rgs) states:\n\nAnd also:\n\nAccording to Ven. Dr. Huifeng, what this means is that Bodhisattvas see through all conceptualizations and conceptions, for they are deceptive and illusory, and sever or cut off all these cognitive creations. \n\nDepending on the stage of the practitioner, the magical illusion is experienced differently. In the ordinary state, we get attached to our own mental phenomena, believing they are real, like the audience at a magic show gets attached to the illusion of a beautiful lady. At the next level, called actual relative truth, the beautiful lady appears, but the magician does not get attached. Lastly, at the ultimate level, the Buddha is not affected one way or the other by the illusion. Beyond conceptuality, the Buddha is neither attached nor non-attached. This is the middle way of Buddhism, which explicitly refutes the extremes of both eternalism and nihilism.\n\nNāgārjuna's Madhyamaka philosophy discusses \"nirmita\", or illusion closely related to māyā. In this example, the illusion is a self-awareness that is, like the magical illusion, mistaken. For Nagarjuna, the self is not the organizing command center of experience, as we might think. Actually, it is just one element combined with other factors and strung together in a sequence of causally connected moments in time. As such, the self is not substantially real, but neither can it be shown to be unreal. The continuum of moments, which we mistakenly understand to be a solid, unchanging self, still performs actions and undergoes their results. \"As a magician creates a magical illusion by the force of magic, and the illusion produces another illusion, in the same way the agent is a magical illusion and the action done is the illusion created by another illusion.\" What we experience may be an illusion, but we are living inside the illusion and bear the fruits of our actions there. We undergo the experiences of the illusion. What we do affects what we experience, so it matters. In this example, Nagarjuna uses the magician's illusion to show that the self is not as real as it thinks, yet, to the extent it is inside the illusion, real enough to warrant respecting the ways of the world.\n\nFor the Mahayana Buddhist, the self is māyā like a magic show and so are objects in the world. Vasubandhu's \"Trisvabhavanirdesa\", a Mahayana Yogacara \"Mind Only\" text, discusses the example of the magician who makes a piece of wood appear as an elephant. The audience is looking at a piece of wood but, under the spell of magic, perceives an elephant instead. Instead of believing in the reality of the illusory elephant, we are invited to recognize that multiple factors are involved in creating that perception, including our involvement in dualistic subjectivity, causes and conditions, and the ultimate beyond duality. Recognizing how these factors combine to create what we perceive ordinarily, ultimate reality appears. Perceiving that the elephant is illusory is akin to seeing through the magical illusion, which reveals the dharmadhatu, or ground of being.\n\nBuddhist Tantra, a further development of the Mahayana, also makes use of the magician's illusion example in yet another way. In the completion stage of Buddhist Tantra, the practitioner takes on the form of a deity in an illusory body (māyādeha), which is like the magician's illusion. It is made of wind, or prana, and is called illusory because it appears only to other yogis who have also attained the illusory body. The illusory body has the markings and signs of a Buddha. There is an impure and a pure illusory body, depending on the stage of the yogi's practice.\n\nThe concept that the world is an illusion is controversial in Buddhism. The Buddha does not state that the world is an illusion, but \"like\" an illusion. In the Dzogchen tradition the \"perceived reality\" is considered literally unreal, in that objects which make-up perceived reality are known as objects within one's mind, and that, \"as we conceive them\", there is no pre-determined object, or assembly of objects in isolation from experience that may be considered the \"true\" object, or objects. As a prominent contemporary teacher puts it: \"In a real sense, all the visions that we see in our lifetime are like a big dream [...]\". In this context, the term \"visions\" denotes not only visual perceptions, but appearances perceived through all senses, including sounds, smells, tastes and tactile sensations.\n\nDifferent schools and traditions in Tibetan Buddhism give different explanations of the mechanism producing the illusion usually called \"reality\".\n\nEven the illusory nature of apparent phenomena is itself an illusion. Ultimately, the yogi passes beyond a conception of things either existing or not existing, and beyond a conception of either samsara or nirvana. Only then is the yogi abiding in the ultimate reality.\n\n\"Maya\", in Jainism, means appearances or deceit that prevents one from \"Samyaktva\" (right belief). Maya is one of three causes of failure to reach right belief. The other two are \"Mithyatva\" (false belief) and \"Nidana\" (hankering after fame and worldly pleasures).\n\n\"Maya\" is a closely related concept to \"Mithyatva\", with Maya a source of wrong information while Mithyatva an individual's attitude to knowledge, with relational overlap.\n\nSvetambara Jains classify categories of false belief under \"Mithyatva\" into five: \"Abhigrahika\" (false belief that is limited to one's own scriptures that one can defend, but refusing to study and analyze other scriptures); \"Anabhigrahika\" (false belief that equal respect must be shown to all gods, teachers, scriptures); \"Abhiniviseka\" (false belief resulting from pre-conceptions with a lack of discernment and refusal to do so); \"Samsayika\" (state of hesitation or uncertainty between various conflicting, inconsistent beliefs); and \"Anabhogika\" (innate, default false beliefs that a person has not thought through on one's own).\n\nDigambara Jains classify categories of false belief under \"Mithyatva\" into seven: \"Ekantika\" (absolute, one sided false belief), \"Samsayika\" (uncertainty, doubt whether a course is right or wrong, unsettled belief, skepticism), \"Vainayika\" (false belief that all gods, gurus and scriptures are alike, without critical examination), \"Grhita\" (false belief derived purely from habits or default, no self-analysis), \"Viparita\" (false belief that true is false, false is true, everything is relative or acceptable), \"Naisargika\" (false belief that all living beings are devoid of consciousness and cannot discern right from wrong), \"Mudha-drsti\" (false belief that violence and anger can tarnish or damage thoughts, divine, guru or dharma).\n\n\"Māyā\" (deceit) is also considered as one of four \"Kaṣaya\" (faulty passion, a trigger for actions) in Jain philosophy. The other three are \"Krodha\" (anger), \"Māna\" (pride) and \"Lobha\" (greed). The ancient Jain texts recommend that one must subdue these four faults, as they are source of bondage, attachment and non-spiritual passions.\n\nIn Sikhism, the world is regarded as both transitory and relatively real. God is viewed as the only reality, but within God exist both conscious souls and nonconscious objects; these created objects are also real. Natural phenomena are real but the effects they generate are unreal. māyā is as the events are real yet māyā is not as the effects are unreal. Sikhism believes that people are trapped in the world because of five vices: lust, anger, greed, attachment, and ego. Maya enables these five vices and makes a person think the physical world is \"real,\" whereas, the goal of Sikhism is to rid the self of them. Consider the following example: In the moonless night, a rope lying on the ground may be mistaken for a snake. We know that the rope alone is real, not the snake. However, the failure to perceive the rope gives rise to the false perception of the snake. Once the darkness is removed, the rope alone remains; the snake disappears.\n\n\nIn some mythologies the symbol of the snake was associated with money, and māyā in modern Punjabi refers to money. However, in the Guru Granth Sahib māyā refers to the \"grand illusion\" of materialism. From this māyā all other evils are born, but by understanding the nature of māyā a person begins to approach spirituality.\n\n\nThe teachings of the Sikh Gurus push the idea of sewa (selfless service) and simran (prayer, meditation, or remembering one's true death). The depths of these two concepts and the core of Sikhism comes from sangat (congregation): by joining the congregation of true saints one is saved. By contrast, most people are believed to suffer from the false consciousness of materialism, as described in the following extracts from the Guru Granth Sahib:\n\n\n"}
{"id": "43864846", "url": "https://en.wikipedia.org/wiki?curid=43864846", "title": "Mechanical–electrical analogies", "text": "Mechanical–electrical analogies\n\nMechanical–electrical analogies are the representation of mechanical systems as electrical networks. At first, such analogies were used in reverse to help explain electrical phenomena in familiar mechanical terms. James Clerk Maxwell introduced analogies of this sort in the 19th century. However, as electrical network analysis matured it was found that certain mechanical problems could more easily be solved through an electrical analogy. Theoretical developments in the electrical domain that were particularly useful where the representation of an electrical network as an abstract topological diagram (the circuit diagram) using the lumped element model and the ability of network analysis to synthesise a network to meet a prescribed frequency function.\n\nThis approach is especially useful in the design of mechanical filters—these use mechanical devices to implement an electrical function. However, the technique can be used to solve purely mechanical problems, and can also be extended into other, unrelated, energy domains. Nowadays, analysis by analogy is a standard design tool wherever more than one energy domain is involved. It has the major advantage that the entire system can be represented in a unified, coherent way. Electrical analogies are particularly used by transducer designers, by their nature they cross energy domains, and in control systems, whose sensors and actuators will typically be domain-crossing transducers. A given system being represented by an electrical analogy may conceivably have no electrical parts at all. For this reason domain-neutral terminology is preferred when developing network diagrams for control systems.\n\nMechanical–electrical analogies are developed by finding relationships between variables in one domain that have a mathematical form identical to variables in the other domain. There is no one, unique way of doing this; numerous analogies are theoretically possible, but there are two analogies that are widely used: the impedance analogy and the mobility analogy. The impedance analogy makes force and voltage analogous while the mobility analogy makes force and current analogous. By itself, that is not enough to fully define the analogy, a second variable must be chosen. A common choice is to make pairs of power conjugate variables analogous. These are variables which when multiplied together have units of power. In the impedance analogy, for instance, this results in force and velocity being analogous to voltage and current respectively.\n\nVariations of these analogies are used for rotating mechanical systems, such as in electric motors. In the impedance analogy, instead of force, torque is made analogous to voltage. It is perfectly possible that both versions of the analogy are needed in, say, a system that includes rotating and reciprocating parts, in which case a force-torque analogy is required within the mechanical domain and a force-torque-voltage analogy to the electrical domain. Another variation is required for acoustical systems; here pressure and voltage are made analogous (impedance analogy). In the impedance analogy, the ratio of the power conjugate variables is always a quantity analogous to electrical impedance. For instance force/velocity is mechanical impedance. The mobility analogy does not preserve this analogy between impedances across domains, but it does have another advantage over the impedance analogy. In the mobility analogy the topology of networks is preserved, a mechanical network diagram has the same topology as its analogous electrical network diagram.\n\nMechanical–electrical analogies are used to represent the function of a mechanical system as an equivalent electrical system by drawing analogies between mechanical and electrical parameters. A mechanical system by itself can be so represented, but analogies are of greatest use in electromechanical systems where there is a connection between mechanical and electrical parts. Analogies are especially useful in analysing mechanical filters. These are filters constructed of mechanical parts but designed to work in an electrical circuit through transducers. Circuit theory is well developed in the electrical domain in general and in particular there is a wealth of filter theory available. Mechanical systems can make use of this electrical theory in mechanical designs through a mechanical–electrical analogy.\n\nMechanical–electrical analogies are useful in general where the system includes transducers between different energy domains. Another area of application is the mechanical parts of acoustic systems such as the pickup and tonearm of record players. This was of some importance in early phonographs where the audio is transmitted from the pickup needle to the horn through various mechanical components entirely without electrical amplification. Early phonographs suffered badly from unwanted resonances in the mechanical parts. It was found that these could be eliminated by treating the mechanical parts as components of a low-pass filter which has the effect of flattening out the passband.\n\nElectrical analogies of mechanical systems can be used just as a teaching aid, to help understand the behaviour of the mechanical system. In former times, up to about the early 20th century, it was more likely that the reverse analogy would be used; mechanical analogies were formed of the then little understood electrical phenomena.\n\nElectrical systems are commonly described by means of a circuit diagram. These are network diagrams that describe the topology of the electrical system using a specialised graph notation. The circuit diagram does not try and represent the true physical dimensions of the electrical components or their actual spatial relationship to each other. This is possible because the electrical components are represented as ideal lumped elements, that is, the element is treated as if it is occupying a single point (lumped at that point). Non-ideal components can be accommodated in this model by using more than one element to represent the component. For instance, a coil intended for use as an inductor has resistance as well as inductance. This can be represented on the circuit diagram as a resistor in series with an inductor. Thus, the first step in forming an analogy of a mechanical system is to describe it as a mechanical network in a similar way, that is, as a topological graph of ideal elements. Alternative, more abstract, representations to the circuit diagram are possible, for instance the bond graph.\nIn an electrical network diagram, limited to linear systems, there are three passive elements: resistance, inductance, and capacitance; and two active elements: the voltage generator, and the current generator. The mechanical analogs of these elements can be used to construct a mechanical network diagram. What the mechanical analogs of these elements are depends on what variables are chosen to be the fundamental variables. There is a wide choice of variables that can be used, but most commonly used are a power conjugate pair of variables (described below) and the pair of Hamiltonian variables derived from these.\n\nThere is a limit to the applicability of this lumped element model. The model works well if the components are small enough that the time taken for a wave to cross them is insignificant, or equivalently, if there is no significant phase difference in the wave either side of the component. What amounts to significant depends on how accurate the model is required to be, but a common rule of thumb is to require components to be smaller than one sixteenth of a wavelength. Since wavelength decreases with frequency, this puts an upper limit on the frequency that can be covered in this kind of design. This limit is much lower in the mechanical domain than the equivalent limit in the electrical domain. This is because the much higher propagation speeds in the electrical domain lead to longer wavelengths (mechanical vibrations in steel propagate at about 6,000 m/s, electromagnetic waves in common cable types propagate at about ). For instance, traditional mechanical filters are only made up to around 600 kHz (although MEMS devices can operate at much higher frequencies due to their very small size). In the electrical domain, on the other hand, the transition from the lumped element model to the distributed element model occurs in the hundreds of megahertz region.\n\nIn some cases it is possible to continue using a topological network diagram even when components needing a distributed element analysis are present. In the electrical domain, a transmission line, a basic distributed element component, can be included in the model with the introduction of the additional element of electrical length. The transmission line is a special case because it is invariant along its length and hence the full geometry need not be modelled. Another way of dealing with distributed elements is to use a finite element analysis whereaby the distributed element is approximated by a large number of small lumped elements. Just such an approach was used in one paper to model the cochlea of the human ear. Another condition required of electrical systems for the application of the lumped element model is that no significant fields exist outside the component since these can couple to other unrelated components. However, these effects can often be modelled by introducing some virtual lumped elements called strays or parasitics. An analog of this in mechanical systems is vibration in one component being coupled to an unrelated component.\n\nThe power conjugate variables are a pair of variables whose product is power. In the electrical domain the power conjugate variables chosen are invariably voltage (\"v\") and current (\"i\"). Thus, the power conjugate variables in the mechanical domain are analogs. However, this is not enough to make the choice of mechanical fundamental variables unique. The usual choice for a translational mechanical system is force (\"F\") and velocity (\"u\") but it is not the only choice. A different pair may be more appropriate for a system with a different geometry, such as a rotational system.\n\nEven after the mechanical fundamental variables have been chosen, there is still not a unique set of analogs. There are two ways that the two pairs of power conjugate variables can be associated with each other in the analogy. For instance the associations \"F\" with \"v\" and \"u\" with \"i\" can be made. However, the alternative associations \"u\" with \"v\" and \"F\" with \"i\" are also possible. This leads to two classes of analogies, the impedance analogies and the mobility analogies. These analogies are the dual of each other. The same mechanical network has analogs in two different electrical networks. These two electrical networks are the dual circuits of each other.\n\nThe Hamiltonian variables, also called the energy variables, are those variables which when time differentiated are equal to the power conjugate variables. The Hamiltonian variables are so called because they are the variables which usually appear in Hamiltonian mechanics. The Hamiltonian variables in the electrical domain are charge (\"q\") and flux linkage (λ) because,\n\nIn the translational mechanical domain the Hamiltonian variables are distance displacement (\"x\") and momentum (\"p\") because,\n\nThere is a corresponding relationship for other analogies and sets of variables. The Hamiltonian variables are also called the energy variables. The integrand of a power conjugate variable with respect to a Hamiltonian variable is a measure of energy. For instance,\n\nare both expressions of energy. They can also be called \"generalised momentum\" and \"generalised displacement\" after their analogs in the mechanical domain. Some authors discourage this terminology because it is not domain neutral. Likewise, the use of the terms \"I-type\" and \"V-type\" (after current and voltage) is also discouraged.\n\nThere are two principle classes of analogy in use. The impedance analogy (also called the Maxwell analogy) preserves the analogy between mechanical, acoustical and electrical impedance but does not preserve the topology of networks. The mechanical network is arranged differently to its analogous electrical network. The mobility analogy (also called the Firestone analogy) preserves network topologies at the expense of losing the analogy between impedances across energy domains. There is also the \"through and across\" analogy, also called the Trent analogy. The through and across analogy between the electrical and mechanical domain is the same as in the mobility analogy. However, the analogy between the electrical and acoustical domains is like the impedance analogy. Analogies between the mechanical and acoustical domain in the through and across analogy have a dual relationship with both the impedance analogy and mobility analogy.\n\nDifferent fundamental variables are chosen for mechanical translation and rotational systems leading to two variants for each of the analogies. For instance, linear distance is the displacement variable in a translational system, but this is not so appropriate for rotating systems where angle is used instead. Acoustical analogies have also been included in the descriptions as a third variant. While acoustical energy is ultimately mechanical in nature, it is treated in the literature as an instance of a different energy domain, the fluid domain, and has different fundamental variables. Analogies between all three domains − electrical, mechanical and acoustical − are required to fully represent electromechanical audio systems.\n\nImpedance analogies, also called the Maxwell analogy, classify the two variables making up the power conjugate pair as an \"effort\" variable and a \"flow\" variable. The effort variable in an energy domain is the variable analogous to force in the mechanical domain. The flow variable in an energy domain is the variable analogous to velocity in the mechanical domain. Power conjugate variables in the analog domain are chosen that bear some resemblance to force and velocity.\n\nIn the electrical domain, the effort variable is voltage and the flow variable is electrical current. The ratio of voltage to current is electrical resistance (Ohm's law). The ratio of the effort variable to the flow variable in other domains is also described as resistance. Oscillating voltages and currents give rise to the concept of electrical impedance when there is a phase difference between them. Impedance can be thought of as an extension to the concept of resistance. Resistance is associated with energy dissipation. Impedance encompasses energy storage as well as energy dissipation. The impedance analogy gives rise to the concept of impedance in other energy domains (but measured in different units).\n\nThe translational impedance analogy describes mechanical systems moving in a single linear dimension and gives rise to the idea of mechanical impedance. The unit of mechanical impedance is the mechanical ohm; in SI units this is N-s/m, or Kg/s.\n\nThe rotational impedance analogy describes rotating mechanical systems and gives rise to the idea of rotational impedance. The unit of rotational impedance in the SI system is N-m-s/rad.\n\nThe acoustical impedance analogy gives rise to the idea of acoustic impedance. The unit of acoustic impedance is the acoustic ohm; in SI units this is N-s/m.\n\nMobility analogies, also called the Firestone analogy, are the electrical duals of impedance analogies. That is, the effort variable in the mechanical domain is analogous to current (the flow variable) in the electrical domain, and the flow variable in the mechanical domain is analogous to voltage (the effort variable) in the electrical domain. The electrical network representing the mechanical system is the dual network of that in the impedance analogy.\n\nThe mobility analogy is characterised by admittance in the same way that the impedance analogy is characterised by impedance. Admittance is the algebraic inverse of impedance. In the mechanical domain, mechanical admittance is more usually called \"mobility\".\n\nThrough and across analogies, also called the Trent analogy, classify the two variables making up the power conjugate pair as an \"across\" variable and a \"through\" variable. The across variable is a variable that appears across the two terminals of an element. The across variable is measured relative to the element terminals. The through variable is a variable that passes through, or acts through an element, that is, it has the same value at both terminals of the element. The through variable is not a relative measure. Thus, in the electrical domain the across variable is voltage and the through variable is current. In the mechanical domain the analogous variables are velocity and force, as in the mobility analogy.\n\nPressure is an across variable because pressure is measured relative to the two terminals of an element, not as an absolute pressure. It is thus not analogous to force which is a through variable, even though pressure is in units of force per area. Forces act through an element; a rod with a force applied to the top will transmit the same force to an element connected to its bottom. Thus, in the through and across analogy the mechanical domain is analogous to the electrical domain like the mobility analogy, but the acoustical domain is analogous to the electrical domain like the impedance analogy.\n\nThe electrical analogy can be extended to many other energy domains. In the field of sensors and actuators, and for control systems using them, it is a common method of analysis to develop an electrical analogy of the entire system. Since sensors can be sensing a variable in any energy domain, and likewise outputs from the system can be in any energy domain, analogies for all energy domains are required. The following table gives a summary of the most common power conjugate variables used to form analogies.\n\nIt is perhaps more common in the thermal domain to choose temperature and thermal power as the fundamental variables because, unlike entropy, they can be measured directly. The concept of thermal resistance is based on this analogy. However, these are not power conjugate variables and are not fully compatible with the other variables in the table. An integrated electrical analogy across multiple domains that includes this thermal analogy will not correctly model energy flows.\n\nSimilarly, the commonly seen analogy using mmf and magnetic flux as the fundamental variables, which gives rise to the concept of magnetic reluctance, does not correctly model energy flow. The variable pair mmf and magnetic flux is not a power conjugate pair. This reluctance model is sometimes called the reluctance-resistance model since it makes these two quantities analogous. The analogy shown in the table, which does use a power conjugate pair, is sometimes called the gyrator-capacitor model.\n\nA transducer is a device that takes energy from one domain as input and converts it to another energy domain as output. They are often reversible, but are rarely used in that way. Transducers have many uses and there are many kinds, in electromechanical systems they can be used as actuators and sensors. In audio electronics they provide the conversion between the electrical and acoustical domains. The transducer provides the link between the mechanical and electrical domains and thus a network representation is required for it in order to develop a unified electrical analogy. To do this the concept of port from the electrical domain is extended into other domains.\n\nTransducers have (at least) two ports, one port in the mechanical domain and one in the electrical domain, and are analogous to electrical two-port networks. This is to be compared to the elements discussed so far which are all one-ports. Two-port networks can be represented as a 2×2 matrix, or equivalently, as a network of two dependent generators and two impedances or admittances. There are six canonical forms of these representations: impedance parameters, chain parameters, hybrid parameters and their inverses. Any of them can be used. However, the representation of a passive transducer converting between analogous variables (for instance an effort variable to another effort variable in the impedance analogy) can be simplified by replacing the dependent generators with a transformer.\n\nOn the other hand, a transducer converting non-analogous power conjugate variables cannot be represented by a transformer. The two-port element in the electrical domain that does this is called a gyrator. This device converts voltages to currents and currents to voltages. By analogy, a transducer that converts non-analogous variables between energy domains is also called a gyrator. For instance, electromagnetic transducers convert current to force and velocity to voltage. In the impedance analogy such a transducer is a gyrator. Whether a transducer is a gyrator or a transformer is analogy related; the same electromagnetic transducer in the mobility analogy is a transformer because it is converting between analogous variables.\n\nJames Clerk Maxwell developed very detailed mechanical analogies of electrical phenomena. He was the first to associate force with voltage (1873) and consequently is usually credited with founding the impedance analogy. This was the earliest mechanical–electrical analogy. However, the term \"impedance\" was not coined until 1886, long after Maxwell's death, by Oliver Heaviside. The idea of complex impedance was introduced by Arthur E. Kennelly in 1893, and the concept of impedance was not extended into the mechanical domain until 1920 by Kennelly and Arthur Gordon Webster.\n\nMaxwell's purpose in constructing this analogy was not to represent mechanical systems in terms of electrical networks. Rather, it was to explain electrical phenomena in more familiar mechanical terms. As electrical phenomena became better understood the reverse of this analogy, using electrical analogies to explain mechanical systems, started to become more common. Indeed, the lumped element abstract topology of electrical analysis has much to offer problems in the mechanical domain, and other energy domains for that matter. By 1900 the electrical analogy of the mechanical domain was becoming commonplace. From about 1920 the electrical analogy became a standard analysis tool. Vannevar Bush was a pioneer of this kind of modelling in his development of analogue computers, and a coherent presentation of this method was presented in a 1925 paper by Clifford A. Nickle.\n\nThe application of electrical network analysis, most especially the newly developed field of filter theory, to mechanical and acoustic systems led to huge improvements in performance. According to Warren P. Mason the efficiency of ship electric foghorns grew from less than one per cent to 50 per cent. The bandwidth of mechanical phonographs grew from three to five octaves when the mechanical parts of the sound transmission were designed as if they were the elements of an electric filter (\" see also \"). Remarkably, the conversion efficiency was improved at the same time (the usual situation with amplifying systems is that gain can be traded for bandwidth such that the gain-bandwidth product remains constant).\n\nIn 1933 Floyd A. Firestone proposed a new analogy, the mobility analogy, in which force is analogous to current instead of voltage. Firestone introduced the concept of across and through variables in this paper and presented a structure for extending the analogy into other energy domains. A variation of the force-current analogy was proposed by Horace M. Trent in 1955 and it is this version that is generally meant by the through and across analogy. Trent used a linear graph method of representing networks which has resulted in the force-current analogy historically being associated with linear graphs. The force-voltage analogy is historically used with bond graph representations, introduced in 1960 by Henry M. Paynter, however, it is possible to use either analogy with either representation if desired.\n\n\n"}
{"id": "1241997", "url": "https://en.wikipedia.org/wiki?curid=1241997", "title": "Medieval weights and measures", "text": "Medieval weights and measures\n\nThe following systems arose from earlier systems, and in many cases utilise parts of much older systems. For the most part they were used to varying degrees in the Middle Ages and surrounding time periods. Some of these systems found their way into later systems, such as the Imperial system and even SI. There were several types to measure that is given below.\n\nBefore Roman units were reintroduced in 1066 by Norman William the Conqueror, there was an Anglo-Saxon (Germanic) system of measure based on the units of the \"barleycorn\" and the \"gyrd\" (rod). The systems partly merged.\n\nLater development of the English system continued by issuing measurement standards from the then capital Winchester in about 1215. Standards were renewed in 1496, 1588 and 1758.\n\nThe last \"Imperial Standard Yard\" in bronze was made in 1845; it served as the standard in the United Kingdom until the yard was internationally redefined as 0.9144 metre in 1959 (statutory implementation: Weights and Measures Act of 1963).\n\nMuch of the units would go on to be used in later Imperial units and in the US system, which are based on the English system from the 1700s.\n\nFrom May 1, 1683, King Christian V of Denmark introduced an office to oversee weights and measures, a \"justervæsen\", to be led by Ole Rømer. The definition of the \"alen\" was set to 2 Rhine feet. Rømer later discovered that differing standards for the Rhine foot existed, and in 1698 an iron Copenhagen standard was made. A pendulum definition for the foot was first suggested by Rømer, introduced in 1820, and changed in 1835. The metric system was introduced in 1907.\n\n\n\n\n\nThe Dutch system was not standardised until Napoleon introduced the metric system. Different towns used measures with the same names but differing sizes.\n\nSome common measures:\n\n\n\n\nIn Finland, approximate measures derived from body parts and were used for a long time, some being later standardised for the purpose of commerce. Some Swedish, and later some Russian units have also been used.\n\nIn France, again, there were many local variants. For instance, the \"lieue\" could vary from 3.268 km in Beauce to 5.849 km in Provence. Between 1812 and 1839, many of the traditional units continued in \"metrified\" adaptations as the \"mesures usuelles\".\n\nIn Paris, the redefinition in terms of metric units made 1 m = 443.296 \"ligne\" = 3 \"pied\" 11.296 \"ligne\".\n\nIn Quebec, the surveys in French units were converted using the relationship 1 \"pied\" (of the French variety; the same word is used for English feet as well) = 12.789 inches (of English origin). Thus a square arpent was 5299296.0804 in² or about 36,801 ft² or 0.8448 acre.\n\nThere were many local variations; the metric conversions below apply to the Quebec and Paris definitions.\n\n\n\n\n\nUp to the introduction of the metric system, almost every town in Germany had their own definitions. It is said that by 1810, in Baden alone, there were 112 different \"Ellen\".\n\n\nBefore 1541, there were no common definition for length measures in Norway, and local variants flourished. In 1541, an \"alen\" in Denmark and Norway was defined by law to be the Sjælland \"alen\". Subsequently, the \"alen\" was defined by law as 2 Rhine feet from 1683. From 1824, the basic unit was defined as a \"fot\" being derived from astronomy as the length of a one-second pendulum times 12/38 at a latitude of 45°. The metric system was introduced in 1887.\n\n\n\n\n\n\n\n\nThe measures of the old Romanian system varied greatly not only between the three Romanian states (Wallachia, Moldavia, Transylvania), but sometimes also inside the same country. The origin of some of the measures are the Latin (such as \"iugăr\" unit), Slavic (such as \"vadră\" unit) and Greek (such as \"dram\" unit) and Turkish (such as \"palmac\" unit) systems.\n\nThis system is no longer in use since the adoption of the metric system in 1864.\n\n\n\n\n\nSee:\n\n\nThere were several variants. The Castilian is shown.\n\n\nIn Sweden, a common system for weights and measures was introduced by law in 1665. Before that, there were a number of local variants. The system was slightly revised in 1735. In 1855, a decimal reform was instituted that defined a new Swedish inch as 1/10 foot. It did not last long, because the metric system was subsequently introduced in 1889. Up to the middle of the 19th century there was a death penalty for falsifying weights or measures.\n\n\n\n\n\n\n\n\n\n"}
{"id": "25810773", "url": "https://en.wikipedia.org/wiki?curid=25810773", "title": "Mnemic neglect", "text": "Mnemic neglect\n\nMnemic neglect is a term used in social psychology to describe a pattern of selective forgetting in which certain autobiographical memories tend to be recalled more easily if they are consistent with positive self-concept. The mnemic neglect model stipulates that memory is self-protective if the information is negative, self-referent, and concerns central traits. \nExperiments have been conducted to test for the specific conditions under which mnemic neglect occurs. A standard procedure has been used across several experiments to test for these conditions. In this procedure, participants are presented with a list of 32 behavioral traits. They are asked to either encode each trait as if it applied to them (i.e. \"consider people describing you with these traits\") or as if it applied to a stranger named Chris (i.e. \"consider people describing Chris with these traits\"). Several of these experiments have found that the first condition under which mnemic neglect occurs is that the traits must be negative. Feedback from others is less easily recalled when it is negative in nature than when it is positive. Second, the information must be self-referent. Sedikides and Green (2004) conducted a study using the standard procedure in which half of the participants were asked to consider feedback as if it applied to them and half were asked to consider the same feedback as if it applied to someone else. They found that negative feedback was recalled as frequently as positive feedback when the feedback was directed at another. However, when the feedback was directed at oneself, negative feedback was recalled less frequently than positive feedback. Third, the information must pertain to a central, rather than peripheral, self-conception. A central self-conception is a generally positive trait that is considered fixed and highly descriptive (i.e. trustworthy vs. untrustworthy). In contrast, a peripheral self-conception is considered only moderately descriptive, less positive, and less important (i.e. modest vs. immodest). Feedback that counteracts a central self-conception is recalled much less frequently than feedback that counteracts a peripheral self-conception. In fact, the recall for feedback on modifiable traits was unaffected by whether the feedback was positive or negative.\n\nSedikides and Green (2004) offered a mechanism by which they believe mnemic neglect occurs. Self-referent information is processed in two stages. The first stage checks the information for compatibility with self-concept. Self-threatening information is confined to this stage because it is found to be incompatible with self-concept. During the second stage, self-flattering information is compared to similar episodic memories. The theory is that deeper processing occurs at the second stage, so information processed at this stage is recalled more easily.\n\nAlthough research has demonstrated a difference in recall between threatening and non-threatening information, there is no difference in recognition of threatening and non-threatening information. In other words, self-threatening information is recalled less easily than non-threatening information but it is recognized equally well, meaning that subjects can identify whether they received the information or not regardless of its connotations. However, in a free recall task, subjects show a memory bias for non-threatening information. To account for this difference, Pinter, Green, Sedikides, and Gregg (2011) modified the suggested mechanism behind mnemic neglect. They hypothesized that people compare incoming information to past experience. If the information is consistent with self-knowledge, then the information is integrated with this knowledge. If the information is inconsistent with self-knowledge, then it is processed separately. Therefore, self-threatening information is less easily recalled but just as easily recognized as non-threatening information because separate processing leads to fewer retrieval routes.\n\nThis mechanism was tested by Pinter and his colleagues experimentally. Half of the participants were prompted to incorporate the information they were given into their self-concept. They were instructed to find similarities between the information and their self-knowledge and determine reasons for why the information described them. This process was termed integration. The other half of the participants were prompted to separate the information from their self-concept. They were asked to identify differences between the information and their self-knowledge and determine reasons for why the information did not describe them. This process was termed separation. The first finding of the experiment was that recall for both positive and negative information was poorer for separation than for integration. However, recognition was the same between the two groups. The second finding was that self-protection is more important that self-enhancement. A greater effect was seen for attempting to integrate central negative information than for attempting to separate central positive information.\n\nThis mechanism has been challenged by those who believe that mnemic neglect is a product of expectancies rather than self-protection motives. In other words, negative information is not processed as deeply as positive information because it is incompatible with self-concept as opposed to simply unflattering. People do not recall a behavior describing them that indicates untrustworthiness not because they want to protect themselves but because this behavior is inconsistent with how they would actually behave. This alternative explanation accounts for the absence of memory bias in recalling information about a stranger (Chris). People have no expectancies about this stranger Chris, so positive and negative information are processed equally well.\n\nNewman, Nibert, and Winer (2009) emphasized ruling out the possibility that mnemic neglect occurs due to incompatibility with expectancies. It was suspected that mnemic neglect might be an issue of self-verification rather than self-protection or self-enhancement. Self-verification and self-protection are difficult concepts to separate because most people tend to think of themselves favorably. Green and Sedikides (2004) attempted to address this concern by conducting two experiments. In one experiment, the participants were pre-selected. Half viewed themselves positively on certain dimensions and half viewed themselves negatively. All participants had better recall for positive dimensions. In another experiment, a stranger Chris was described to participants before they read the behaviors. Chris was described as a superhuman, incredibly kind and trustworthy, in an attempt to establish expectancies about Chris. Other participants in the same study were asked to imagine the traits described a close friend whom the participants hold in high regard. In comparison to these two cases, recall for positive traits was still better when these traits were self-referent rather than directed at another.\n\nDespite these findings, the mechanism was still challenged because the results were open to interpretation. Perhaps participants had more favorable expectancies for themselves than for others. Newman, Nibert, and Winer (2009) sought to test the expectancies possibility more directly. They hypothesized that participants who were more focused on testing information against their expectancies would have different recall biases than participants who were more likely to be focused on self-protection. Defensive pessimists were identified as people who are more likely to focus on expectancies. Defensive pessimists think extensively about possible future outcomes, both positive and negative. Their ultimate goal is to be prepared. They should still exhibit a preference for positive over negative information, but they are more likely to approach a situation with “Is this a reasonable thing to expect?” as opposed to “Will this make me feel bad about myself?” Thus, it was hypothesized that defensive pessimists would recall more favorable than unfavorable behaviors in both themselves and others when subjected to the standard test for mnemic neglect. The findings supported the hypothesis. Defensive pessimists showed a bias for the recall of positive information over negative information for both themselves and others, thereby offering further support to the hypothesized mechanism of mnemic neglect (i.e. mnemic neglect is based on self-protection rather than self-verification).\n\nThe recollection of autobiographical memories can trigger emotions. Research shows that the intensity of the emotion experienced at the time of the event decreases over time, such that the intensity of the emotion associated with the recollection of the event is less than the intensity of the emotion experienced during the event. The fading affect bias is a phenomenon in which emotions associated with a negative event fade faster than emotions associated with a positive event.\n\nLike mnemic neglect, the fading affect bias is hypothesized to promote positive self-concept. Despite the similarities between mnemic neglect and fading affect bias, the hypothesized mechanisms of fading affect bias differ from that of mnemic neglect. One hypothesized mechanism of fading affect bias is that negative experiences are gradually reinterpreted as transformative events. In the process, these memories lose emotional intensity. For example, people often reinterpret hardships as opportunities to experience personal growth. A second hypothesized mechanism suggests that the conveying of autobiographical events to others minimizes negative aspects of the story and emphasizes positive aspects. In fact, a study by Skowronski, Gibbons, Vogl, and Walker (2004) demonstrated that fading affect bias is stronger for events frequently disclosed to others. Both of these mechanisms suggest that the superior retention of positive emotions over negative emotions is not a function of initial coding. Rather, poorer retention of negative emotions may be a function of recoding over time.\n\nOne limitation of the mnemic neglect model is its inability to explain why people respond to negative information differently. Why is it that sometimes people react to negative feedback with defensive anger and other times with thoughtful reflection? Green, Sedikides, Pinter, and Van Tongeren (2009) conducted two experiments to determine the boundaries of mnemic neglect. They hypothesized that the self-protection aspect of mnemic neglect is flexible and that the mnemic neglect model is only upheld under certain conditions. More specifically, these researchers hypothesized that mnemic neglect does not occur when there is an opportunity for self-improvement and when feedback concerning an individual’s personality is provided in the context of a close relationship.\n\nIn their first experiment, these researchers tested the effect motivation to improve the self has on mnemic neglect. They hypothesized that mnemic neglect would not be observed in individuals motivated to enhance themselves because this motivation leads to deeper processing which in turn creates more retrieval routes. In this experiment, half of the participants were exposed to a task aimed to prime them for self-motivation. The other half was not exposed to this task. After the priming task, all of the participants were exposed to the standard test for mnemic neglect in which they were asked to read through a list of personality traits. Some were asked to imagine that the traits applied to them while others were asked to imagine that the traits applied to another named Chris. In the condition where self-improvement was primed, self-threatening and self-affirming information were recalled equally. Researchers believe that the findings of this experiment indicate the existence of a balance between the desire to protect oneself and the desire to improve oneself.\n\nIn their second experiment, participants signed up as pairs. Half of the participants were assigned to work with their partner while the other half were assigned to work with strangers. One of the partners completed a personality test. The other partner received the first partner’s answers to the personality test and rated each of the responses as either positive or negative. The first participant then had an opportunity to review each of their partner’s positive/negative ratings. After a brief distractor test, the first participant was asked to recall as many of their partner’s feedback ratings as possible. Researchers found that when the two partners had a close relationship, mnemic neglect was not demonstrated. When the two partners were strangers, mnemic neglect was maintained.\n\nGreen, Sedikides, Pinter, and Van Tongeren admit that further research must be conducted to determine the mechanism behind the effect relationships have on mnemic neglect. Feedback from close friends may be more useful because individuals feel more comfortable to use the feedback constructively if it is provided within a supportive relationship. An alternative explanation is that recalling feedback from a friend may be important for maintaining a healthy relationship. However, a third explanation is that individuals commit this feedback to memory so that they may better prepare counterarguments for the future.\n\nThis research opens the door for future research to investigate individual differences in order to create a list of moderator variables that affect mnemic neglect.\n\nMood can affect cognitive performance. Research has demonstrated that individuals with dysphoria recall negative information more easily than positive information Saunders (2011) conducted three experiments to determine the relationship between dysphoria and mnemic neglect. In the first experiment, it was hypothesized that dysphoric patients are prone to malfunctioning mnemic neglect. They suffer from the inability to forget negative information. Thus, they should have better recall for self-threatening information than nondysphoric individuals. Participants were subjects to the standard mnemic neglect test procedure. It was determined that people with dysphoria have greater recall for central negative information. It was further hypothesized that a defect in mnemic neglect leads to more negative memories.\n\nExperiment 1 also indicated that dysphoria patients had better recall for central negative information than peripheral negative information. Thus, it was hypothesized a reverse mnemic neglect model in dysphoria patients. For the second experiment, the same traits were presented to two separate groups of dysphoria patients. The first group received information before receiving the list of traits that described the traits as all unmodifiable, meaning that if one exhibits the trait as a child, he or she will certainly exhibit the trait as an adult. The second group received information that described the traits as all modifiable, meaning that if one exhibits the trait as a child, he or she can change as an adult. It was concluded that dysphoria patients recall unmodifiable traits better than they do modifiable traits.\n\nIn Experiment 3, it was hypothesized that individuals with dysphoria would demonstrate better recall for highly diagnostic behaviors than low diagnostic behaviors. A highly diagnostic behavior is one that is very descriptive of a trait. For instance, “I can keep secrets” is highly diagnostic of the trait trustworthiness. A low diagnostic behavior for trustworthiness might be “I would take a pen from a bank after using it”. This experiment followed the same procedure as Experiment 1 except that half of the dysphoria patients were presented highly diagnostic behaviors and half were presented low diagnostic behaviors. As expected, the participants negative information was more easily recalled when the information was highly diagnostic as opposed to low diagnostic.\n\nSaunders explains her findings by suggesting that self-referent, negative, central information receives elaboration form dysphoric patients. They process this information more deeply than nondysphoric patients. In addition, individuals with dysphoria may have inhibitory deficits, where they are unable to keep negative information from flooding their conscious minds. These difficulties in suppressing negative memories could explain why the negative mood is sustained.\n\nDysphoric patients rated central negative traits just as negative as nondysphoric patients did, but they rated these traits as more important. It has been argued that the differences in mnemic neglect between individuals with and without dysphoria can be explained by mood-dependent memory (link). Simply, individuals with dysphoria believe that negative information is more self-referent than positive information. However, Saunders argues that there are effects beyond mood-dependent memory because the same effect would be seen for peripheral negative information, which in fact saw no difference in recall between the two groups. She also emphasizes that she would not have observed differences for modifiability and diagnosticity either if this were the case.\n\nThere is evidence that individuals with anxiety also experience reverse mnemic neglect. Many studies have found no difference in memory bias between anxious and non-anxious individuals, but these studies have been criticized for using traits low in self-reference. A meta-analysis conducted by Mitte (2008) found that anxious individuals have better recall than non-anxious individuals for negative information and that non-anxious individuals have better recall than anxious individuals for neutral and positive information.\n\nSaunders (2013) hypothesized that highly anxious individuals have better recall for central, negative traits than non-anxious individuals. A series of three experiments was conducted. Experiment 1 used the standard mnemic neglect procedure to compare the recall for various traits between high-anxious participants and low-anxious participants. As hypothesized, high-anxious participants showed greater recall than did low-anxious participants for central, negative traits.\n\nExperiment 2 tested for the recall of highly diagnostic traits. It was hypothesized that high-anxious participants would show mnemic neglect for modifiable traits because these traits are not perceived as very threatening. However, unmodifiable traits would be immune to mnemic neglect for these participants and thus they would be recalled more easily. In this experiment, traits were either presented following a description of the traits as being changeable over time or following a description of the traits as being unalterable throughout life. As hypothesized, anxious participants were able to recall more unmodifiable than modifiable central negative traits.\n\nIt was hypothesized that anxious participants have better recall for highly diagnostic central negative traits. Diagnosticity refers to how well a behavior indicates an underlying trait. In experiment 3, both anxious and non-anxious participants were asked to rate how diagnostic each behavior was of a trait (i.e. Based on this behavior, how likely is it that this person is trustworthy?). No difference in diagnostic ratings was found between the two groups. However, anxious participants had higher recall than non-anxious participants for highly diagnostic central negative traits when they directed at themselves.\n\nThus, reverse mnemic neglect is experienced for unmodifiable and highly diagnostic traits among high-anxious individuals.\n"}
{"id": "31589374", "url": "https://en.wikipedia.org/wiki?curid=31589374", "title": "Nukapu Expedition", "text": "Nukapu Expedition\n\nThe Nukapu Expedition occurred between October 1871 and February 1872 and was a British punitive operation in response to the murder of Bishop John Coleridge Patteson by the natives of Nukapu. A Royal Navy warship was sent to the island and it sank a group of hostile war-canoes and landed men to attack a fortified village.\n\nIn October 1871, the screw sloop-of-war HMS \"Rosario\" was operating against South Seas blackbirders when her captain, Acting-Commander Albert Hastings Markham received orders to sail for Nukapu in the Solomon Islands. A contemporary newspaper described the events thus:\nMarkham published an account of the cruise under the title \"The cruise of the \"Rosario\" amongst the New Hebrides and Santa Cruz Islands, exposing the recent atrocities connected with the kidnapping of natives in the South Seas.\" The measures taken by \"Rosario\" became the subject of questions in the House of Commons, and Markham's book on the subject may well have been prompted by them. The book itself makes clear that Markham clearly understood the cycle of violence and deplored both the murderous activities of the Blackbirders, and the apparent need for further violence in restoring order.\n\n\n"}
{"id": "1375378", "url": "https://en.wikipedia.org/wiki?curid=1375378", "title": "Obsolete Russian units of measurement", "text": "Obsolete Russian units of measurement\n\nA native system of weights and measures was used in Imperial Russia and after the Russian Revolution, but it was abandoned after July 21, 1925, when the Soviet Union adopted the metric system, per the order of the Council of People's Commissars.\n\nThe Tatar system is very similar to the Russian one, but some names are different.\n\nThe system existed since ancient Rus', but under Peter the Great, the Russian units were redefined relative to the English system. Until Peter the Great the system also used Cyrillic numerals, and only in the 18th century did Peter the Great replace it with the Hindu-Arabic numeral system.\n\nThe basic unit is the Russian cubit, called the \"arshin\", which has been in use since the 16th century. It was standardized by Peter the Great in the 18th century to measure exactly twenty-eight English inches (). Thus, 80 vershoks = 20 piads = 5 arshins = 140 English inches ().\n\nA \"piad\" (, “palm”, “five”) or \"chetvert\" (, “quarter”) is a hand span, the distance between ends of the spread thumb and index finger.\n\nAlternative units:\n\n\nAs in many ancient systems of measurement the Russian distinguishes between dry and liquid measurements of capacity. Note that the chetvert appears in both lists with vastly differing values.\n\nTwo systems of weight were in use, an ordinary one in common use, and an apothecaries' system.\n\nThe pood was first mentioned in a number of documents of the twelfth century. It may still be encountered in documents dealing with agricultural production (especially with reference to cereals), and has been revived in determining weights when casting bells in belfries following the rebirth of the Orthodox Churches in the former Soviet lands. It is also popular in the modern fitness industry of the 21st century, as the pood is used when referencing the weight of a kettlebell, a Russian invention, especially in CrossFit. \n\nThe Imperial Russian apothecaries' weight was defined by setting the grain () to be exactly seven-fifths of a \"dolia\". The only unit name shared between the two was the \"funt\" (pound), but the one in the apothecaries' system is exactly seven-eighths of the ordinary \"funt\".\n\nThe obsolete units of measurement survived in Russian culture in a number of idiomatic expressions and proverbs, for example:\n\n\n\n"}
{"id": "56389349", "url": "https://en.wikipedia.org/wiki?curid=56389349", "title": "Office of the Special Representative of the Secretary-General on Sexual Violence in Conflict", "text": "Office of the Special Representative of the Secretary-General on Sexual Violence in Conflict\n\nThe Office of the Special Representative of the Secretary-General on Sexual Violence in Conflict (OSRSG-SVC), is an office of the United Nations Secretariat tasked with serving the United Nations' spokesperson and political advocate on conflict-related sexual violence, the Special Representative of the Secretary-General on Sexual Violence in Conflict (SRSG-SVC). The Special Representative holds the rank of Under-Secretary-General of the United Nations and chairs the UN Action Against Sexual Violence in Conflict. The mandate of the SRSG-SVC was established by Security Council Resolution 1888, introduced by Hillary Clinton, and the first Special Representative, Margot Wallström, took office in 2010. The current Special Representative is Pramila Patten of Mauritius, who was appointed by United Nations Secretary General António Guterres in April 2017. The work of the SRSG-SVC is supported by the United Nations Team of Experts on the Rule of Law/Sexual Violence in Conflict, co-led by the Department of Peacekeeping Operations (DPKO), Office of the High Commissioner for Human Rights (OHCHR) and the United Nations Development Programme (UNDP), also established under Security Council Resolution 1888.\n\nThe mandate was established in 2009 by United Nations Security Council Resolution 1888, one in a series of resolutions which recognized the detrimental impact that sexual violence in conflict has on communities, and acknowledged that the crime undermines peace and security efforts. The resolution signaled a change in the way the international community views and responds to conflict-related sexual violence. It is no longer seen as an inevitable by-product of war, but rather a, a crime that is preventable and punishable under international human rights law.\n\nIn April 2010, the first Special Representative, Margot Wallström of Sweden took office and in September 2012 Zainab Hawa Bagura of Sierra Leone took over and served until early 2017. Progress achieved by the Office so far includes:\n\n• Greater visibility, political will and momentum than ever before;\n\n• Development of a robust legislative framework in the Security Council, which has given the Office new tools to drive the mandate to the ground and begin to effect some changes in behaviour;\n\n• More strategic and structured engagement with the security and justice sectors, as part of a prevention strategy;\n\n• Beginning to see some accountability at international and national levels, as a vital aspect of deterrence and prevention;\n\n• Beginning to see national ownership, leadership and responsibility, evident in formal commitments that many governments are making to deal with this problem in their countries as well as commitments by regional organizations.\n\nThe three priorities of current SRSG Pramila Patten are:\n\ni. Converting cultures of impunity into cultures of prevention and deterrence through justice and accountability\n\nii. Addressing structural gender-based inequality as the root cause and invisible driver of sexual violence in times of war and peace\n\niii. Fostering national ownership and leadership for a sustainable and holistic survivor-centered response.\n\nAccording to the 2017 Report of the Secretary-General on Conflict-Related Sexual Violence, the Office focuses on 19 country situations, including 13 conflict settings, four post-conflict countries and two additional situations of concern.\n\nThe Office's Team of Experts (TOE) on the Rule of Law and Sexual Violence in Conflict works to strengthen the capacity of national rule of law and justice actors to investigate and prosecute for acts of conflict-related sexual violence. The lack of adequate national capacity to deliver justice often leads to widespread impunity and threatens survivors' access to justice, security and safety. The Team of Experts has been operational since 2011 and is the sole Security Council-mandated body tasked with building national capacity to enhance accountability for conflict-related sexual violence. It includes experts from the Department of Peacekeeping Operations (DPKO), the Office of the High Commissioner for Human Rights (OHCHR) and the United Nations Development Programme (UNDP), which serve as co-lead entities. In addition, the Team is complemented by a law enforcement expert seconded by the Government of Sweden and a reparations expert. Pursuant to Security Council Resolution 1888 (2009), the Team focuses on: (i) working closely with national legal and judicial officials and other personnel in the relevant governments’ civilian and military justice systems to address impunity, including by strengthening national capacity and drawing attention to the full range of justice mechanisms to be considered; (ii) identifying gaps in national response and encouraging a holistic national approach in addressing conflict-related sexual violence, including by enhancing criminal accountability, judicial capacity and responsiveness to victims (such as reparations mechanisms); (iii) making recommendations to coordinate domestic and international efforts and resources to reinforce governments’ ability to address conflict-related sexual violence; and (iv) working with other UN mechanisms including UN Missions, Country Teams, and the SRSG-SVC towards the full implementation of resolutions 1820 (2008), 1888 (2009), 1960 (2010), 2106 (2013) and 2331 (2016). In line with its mandate, the Team of Experts provides assistance to governments, including in the areas of criminal investigation and prosecution; military justice; legislative reform; protection of victims and witnesses; reparations for survivors; and security sector oversight. In support of the Office of the SRSG-SVC, the TOE also serves a catalytic role in implementing joint communiqués and frameworks of cooperation agreed between the SRSG-SVC and national authorities, regional actors and other UN entities, complementing the work of UN country presences.\n\nUN Action is a cross-UN initiative that unites the work of 14 entities with the goal of ending conflict-related sexual violence. The Chair of UN Action is the Special Representative on Sexual Violence in Conflict, Ms. Pramila Patten. Endorsed by the Secretary-General's Policy Committee in June 2007, it represents a concerted effort by the UN to work as one by amplifying advocacy, improving coordination and accountability and supporting country efforts to prevent conflict-related sexual violence and respond effectively to the needs of survivors. The UN Action Secretariat is based in the OSRSG-SVC and focuses on three main pillars:\n\nCountry-level Action: Strategic support to integrated UN Missions to help them design comprehensive strategies to combat conflict-related sexual violence and targeted support to strengthen joint UN programming.\n\nAdvocacy for Action: Raising public awareness and generating political will to address conflict-related sexual violence as part of a broader campaigns to Stop Rape Now and Unite to End Violence Against Women.\n\nLearning by Doing: Creating a knowledge hub on the scale of conflict-related sexual violence and effective responses by the UN and partners.\n\nThe 14 entities in the network are: United Nations Department of Political Affairs (DPA), United Nations Entity for Gender Equality and the Empowerment of Women (UN Women), United Nations Population Fund (UNFPA), United Nations Development Programme (UNDP), Joint United Nations Program on HIV/AIDS (UNAIDS), Department of Peacekeeping Operations (DPKO), United Nations High Commissioner for Refugees (UNHCR), United Nations Peacebuilding Support Office (UN-PBSO), Office of the High Commissioner for Human Rights (OHCHR), World Health Organization (WHO), Office for the Coordination of Humanitarian Affairs (OCHA), United Nations Children's Fund (UNICEF), World Food Programme (WFP) and the International Organization for Migration (IOM).\n\nThe Office publishes the Report of the Secretary-General on Conflict-Related Sexual Violence annually to highlight a number of new and emerging concerns in relation to the use of sexual violence by parties to armed conflict as a tactic of war and terrorism. The report contains an annex of a list of parties credibly suspected of committing or being responsible for patterns of rape or other forms of sexual violence in situations of armed conflict, the majority of whom are non-state actors. It tracks developments relevant to the implementation of Resolutions 1820 (2008), 1888 (2009) and 1960 (2010) in 19 conflict-affected and post-conflict states and it is compiled through the analysis of data provided by United Nations offices, civil society and regional organizations, as well as Member States. The 2017 Report covers the following countries: Afghanistan, Bosnia and Herzegovina, Burundi, Central African Republic, Colombia, Cote d’Ivoire, the Democratic Republic of the Congo (DRC), Iraq, Libya, Mali, Myanmar, Nepal, Nigeria, Somalia, South Sudan, Sri Lanka, Sudan (Darfur), Syrian Arab Republic and Yemen.\n\n"}
{"id": "218271", "url": "https://en.wikipedia.org/wiki?curid=218271", "title": "Omnipresence", "text": "Omnipresence\n\nOmnipresence or ubiquity is the property of being present everywhere. The term omnipresence is most often used in a religious context as an attribute of a deity or supreme being, while the term ubiquity is generally used to describe something \"existing or being everywhere at the same time, constantly encountered, widespread, common.\" Ubiquitous can also be used as a synonym for words like worldwide, universal, global, pervasive, all over the place. \n\nThe omnipresence of a supreme being is conceived differently by different religious systems. In monotheistic beliefs like Christianity, Judaism, and Islam the divine and the universe are separate, but the divine is present everywhere. In pantheistic beliefs the divine and the universe are identical. In panentheistic beliefs the divine interpenetrates the universe, but extends beyond it in time and space.\n\nHinduism, and other religions that derive from it, incorporate the theory of \"transcendent and immanent omnipresence\" which is the traditional meaning of the word, Brahman. This theory defines a universal and fundamental substance, which is the source of all physical existence.\n\nDivine omnipresence is thus one of the divine attributes, although in Western Christianity it has attracted less philosophical attention than such attributes as omnipotence, omniscience, or being eternal.\n\nIn Western theism, omnipresence is roughly described as the ability to be \"present everywhere at the same time\", referring to an unbounded or universal presence. Omnipresence means minimally that there is no place to which God’s knowledge and power do not extend. It is related to the concept of ubiquity, the ability to be everywhere or in many places at once. This includes unlimited temporal presence. William Lane Craig states that we shouldn’t think of God as being in space in the sense of being spread out like an invisible ether throughout space. He is not like an invisible gas that is everywhere present in space. This would be incorrect for several reasons. For one, it would mean that if the universe is finite, which is perfectly possible, then God would be finite. We do not want to say that because God is infinite. More seriously, if God is spread out throughout space, like an invisible ether, that means that he is not fully present everywhere.\n\nSome argue that omnipresence is a derived characteristic: an omniscient and omnipotent deity knows everything and can be and act everywhere, simultaneously. Others propound a deity as having the \"Three O's\", including omnipresence as a unique characteristic of the deity. Most Christian denominations — following theology standardized by the Nicene Creed — explain the concept of omnipresence in the form of the \"Trinity\", by having a single deity (God) made up of three omnipresent persons, Father, Son and Holy Spirit.\n\nSeveral ancient cultures such as the Vedic and the Native American civilizations share similar views on omnipresent nature; the ancient Egyptians, Greeks and Romans did not worship an omnipresent being. While most Paleolithic cultures followed polytheistic practices, a form of omnipresent deity arises from a worldview that does not share ideas with mono-local deity cultures. Some omnipresent religions see the whole of existence as a manifestation of the deity. There are two predominant viewpoints here: pantheism, deity is the summation of Existence; and panentheism, deity is an emergent property of existence. The first is closest to the Native Americans' worldview; the latter resembles the Vedic outlook.\n\nIn traditional Jewish monotheism belief of panentheism, or an omnipresent God, is rejected. While the \"entire concept of God occupying physical space, or having any category of spatial reference apply to him was completely rejected by pure Judaic monotheism,\" Hasidic teachings, along with certain Kabbalistic systems, diverged to postulate belief in panentheism.\n\nIn Islamic beliefs, pantheism is also rejected and omnipresence is described to transcend the physical. According to Shia tradition in Nahj al-Balagha, a compilation of Ali's teachings and letters, with commentary by Morteza Motahhari, the only territory that God does not enter is that of nothingness and non-existence. God is with everything, but not in anything, and nothing is with him. God is not within things, though not out of them. He is over and above every kind of condition, state, similarity and likeness. Ali says about God's omnipresence: \n\nIn Christianity, as well as in Kabbalistic and Hasidic philosophy, God is omnipresent. However, the major difference between them and other religious systems is that God is still transcendent to His creation and yet immanent in relating to creation. God is not immersed in the substance of creation, even though he is able to interact with it as he chooses. He cannot be excluded from any location or object in creation. God's presence is continuous throughout all of creation, though it may not be revealed in the same way at the same time to people everywhere. At times, he may be actively present in a situation, while he may not reveal that he is present in another circumstance in some other area. God is omnipresent in a way that he is able to interact with his creation however he chooses, and is the very essence of his creation. While contrary to normal physical intuitions, such omnipresence is logically possible by way of the classic geometric point or its equivalent, in that such a point is, by definition, within all of space without taking up any space. The Bible states that God can be both present to a person in a manifest manner (Psalm 46:1, Isaiah 57:15) as well as being present in every situation in all of creation at any given time (Psalm 33:13-14). Specifically, Oden states that the Bible shows that God can be present in every aspect of human life:\n\n\nMarbaniang points out that omnipresence doesn't mean divine occupation of all space, nor divine distribution over all space, nor indwelling of every entity, nor that God cannot move in space, nor the divinification of the universe; but means that God is fully present every-where, and that God can do different things at different places at the same time.\n\n\n"}
{"id": "46256014", "url": "https://en.wikipedia.org/wiki?curid=46256014", "title": "Political subjectivity", "text": "Political subjectivity\n\nPolitical subjectivity is a term used to indicate the deeply embedded nature of subjectivity and subjective experience in a socially constructed system of power and meaning. The notion of political subjectivity is an emerging idea in social sciences and humanities. In some sense the term political subjectivity reflects the converging point of a number of traditionally distinct disciplinary lines of investigation, such as philosophy, anthropology, political theory, and psychoanalytic theory. Above all, the current conceptualization of political subjectivity has become possible due to a fundamental shift in humanities and social sciences during the 20th century, commonly known as the linguistic turn.\n\nMajor figures associated with the question of political subjectivity come from diverse disciplinary backgrounds, such as German philosopher GWF Hegel, French psychoanalyst Jacques Lacan, French historian Michel Foucault, American literary critic Fredric Jameson, American cultural anthropologist Clifford Geertz, American medical anthropologist Byron J. Good, American philosopher and gender theorist Judith Butler, Canadian medical anthropologist Sadeq Rahimi, Argentine political theorist Ernesto Laclau, Slovenian philosopher Slavoj Zizek, Greek political theorist Yannis Stavrakakis, and many others.\n\nThe term \"political subjectivity\" had been used in earlier literature, such as Steven Brown's book, \"Political subjectivity: Applications of Q methodology in political science\" to refer to individual political view points as affected by social and personal psychological processes. But the term was later re-appropriated to refer to the much more intricate idea that the very experience of subjectivity is fundamentally political. According to Sadeq Rahimi in \"Meaning, Madness and Political Subjectivity\", \"Politicality is not an added aspect of the subject, but indeed the mode of being of the subject, that is, precisely what the subject is.\"\n\nAn early (1981) book by Fredric Jameson, \"The Political Unconscious: Narrative as a Socially Symbolic Act\", can be considered one of the forerunners of the notion of political subjectivity. In his book Jameson attributed what he termed a \"political unconscious\" to text, asserting that all text has embedded in it, albeit in an implicit form, the encodings of the political history of the environment in which they have been produced. He then proposed “the doctrine of a political unconscious,” as an analytic method for unearthing the hermeneutically repressed political memories of text, and “restoring to the surface of the text the repressed and buried reality of this fundamental history” (p. 20). While Jameson's original theory of the political unconscious was primarily a neo-Marxist approach to literary criticism, later proliferation and interdisciplinary cross-fertilization of theories of subjectivity have greatly expanded Jameson's original ideas to include the range of political, cultural and psychological processes within the framework of political subjectivity.\n\n\n"}
{"id": "520196", "url": "https://en.wikipedia.org/wiki?curid=520196", "title": "Positivity effect", "text": "Positivity effect\n\nIn psychology and cognitive science, the positivity effect is the ability to constructively analyze a situation where the desired results are not achieved; but still obtain positive feedback that assists our future progression. When a person is considering people they like (including themselves), the person tends to make situational attributions about their negative behaviors and dispositional attributions about their positive behaviors. The reverse may be true for people that the person dislikes. This may well be because of the dissonance between liking a person and seeing them behave negatively.\nExample: If a friend hits someone, one would tell them the other guy deserved it or that he had to defend himself.\n\nThe positivity effect pertains to the tendency of people, when evaluating the causes of the behaviors of a person they like or prefer, to attribute the person's inherent disposition as the cause of their positive behaviors and the situations surrounding them as the cause of their negative behaviors. The positivity effect is the inverse of the \"negativity effect\", which is found when people evaluate the causes of the behaviors of a person they dislike. Both effects are attributional biases.\n\nOn online social networks like Twitter, users prefer to share positive news, and are emotionally affected by positive news more than twice than by negative news.\n\n\n\n"}
{"id": "37478104", "url": "https://en.wikipedia.org/wiki?curid=37478104", "title": "Positivity offset", "text": "Positivity offset\n\nIn psychology, the positivity offset is a phenomenon where people tend to interpret neutral situations as mildly positive, and rate their lives as good, most of the time. The positivity offset stands in notable asymmetry to the negativity bias.\n\nSocial neuroscience researcher John Cacioppo has assembled evidence that people typically see their surroundings as positive, whenever a clear threat is not present. Because of the positivity offset, people are motivated to explore and engage with their surroundings, instead of being balanced inactive between approach and avoidance.\n\nAcross most cultures, nations, and groups of people, the average and median ratings of life satisfaction are not neutral, as one might expect, but mildly positive.\n\nGroups of people who do not show a positivity offset include people with depression, people in severe poverty, and people who live in perpetually threatening situations. However, many groups of people that outsiders would not expect to show the positivity offset do, such as people with paraplegia and spinal injury, very elderly people, and people with many chronic illnesses. In some cases these individuals never become as satisfied or happy with their lives as before their illness or injury, but over time (generally approximately two years), they still stabilize at a level substantially above neutral. That is, they judge themselves overall as satisfied or happy and not dissatisfied or unhappy.\n\nMany of the major psychological publications on life satisfaction ratings have come from Ed Diener and colleagues. This empirical work gathered life-satisfaction judgments from many modern and traditional cultures worldwide.\n\n"}
{"id": "37231770", "url": "https://en.wikipedia.org/wiki?curid=37231770", "title": "Pregroup grammar", "text": "Pregroup grammar\n\nPregroup grammar (PG) is a grammar formalism intimately related to categorial grammars. Much like categorial grammar (CG), PG is a kind of type logical grammar. Unlike CG, however, PG does not have a distinguished function type. Rather, PG uses inverse types combined with its monoidal operation.\n\nA pregroup is a partially ordered algebra formula_1 such that formula_2 is a monoid, satisfying the following relations:\n\n\nThe contraction and expansion relations are sometimes called Ajdukiewicz laws.\n\nFrom this, it can be proven that the following equations hold:\n\n\nformula_8 and formula_9 are called the left and right adjoints of \"x\", respectively.\n\nThe symbol formula_10 and formula_11 are also written formula_12 and formula_13 respectively. In category theory, pregroups are also known as autonomous categories or (non-symmetric) compact closed categories. More typically, formula_14 will just be represented by adjacency, i.e. as formula_15.\n\nA pregroup grammar consists of a lexicon of words (and possibly morphemes) \"L\", a set of atomic types \"T\" which freely generates a pregroup, and an relation formula_16 that relates words to types. In simple pregroup grammars, typing is a function that maps words to only one type each.\n\nSome simple, intuitive examples using English as the language to model demonstrate the core principles behind pregroups and their use in linguistic domains.\n\nLet \"L\" = {John, Mary, the, dog, cat, met, barked, at}, let \"T\" = {\"N, S, N\"}, and let the following typing relation holds:\n\nA sentence \"S\" that has type \"T\" is said to be grammatical if formula_19. We can prove this by use of a chain of formula_11. For example, we can prove that formula_21 is grammatical by proving that formula_22:\n\nby first using contraction on formula_24 and then again on formula_25. A more convenient notation exists, however, that indicates contractions by connecting them with a drawn link between the contracting types (provided that the links are nested, i.e. don't cross). Words are also typically placed above their types to make the proof more intuitive. The same proof in this notation is simply\n\nA more complex example proves that \"the dog barked at the cat\" is grammatical:\n\nPregroup grammars have been introduced by Joachim Lambek in 1993 as a development of his syntactic calculus, replacing the quotients by adjoints. Such adjoints had already been used earlier by Harris but without iterated adjoints and expansion rules.\nAdding such adjoints was interesting to handle more complex linguistic cases, where the fact that formula_26 is needed. It was also motivated by a more algebraic viewpoint: the definition of a pregroup is a weakening of that of a group, introducing a distinction between the left and right inverses and replacing the equality by an order. This weakening was needed because using types from a free group would not work: an adjective would get the type formula_27, hence it could be inserted at any position in the sentence.\n\nPregroup grammars have then been defined and studied for various languages (or fragments of them) including English, Italian, French, Persian and Sanskrit. Languages with a relatively free word order such as Sanskrit required to introduce commutation relations to the pregroup, using precyclicity.\n\nBecause of the lack of function types in PG, the usual method of giving a semantics via the λ-calculus or via function denotations is not available in any obvious way. Instead, two different methods exist, one purely formal method that corresponds to the λ-calculus, and one denotational method analogous to (a fragment of) the tensor mathematics of quantum mechanics.\n\nThe purely formal semantics for PG consists of a logical language defined according to the following rules:\n\n\nSome examples of terms are \"f\"(\"x\"), \"g\"(\"a\",\"h\"(\"x\",\"y\")), formula_29. A variable \"x\" is free in a term \"t\" if [\"x\"] does not appear in \"t\", and a term with no free variables is a closed term. Terms can be typed with pregroup types in the obvious manner.\n\nThe usual conventions regarding α conversion apply.\n\nFor a given language, we give an assignment \"I\" that maps typed words to typed closed terms in a way that respects the pregroup structure of the types. For the English fragment given above we might therefore have the following assignment (with the obvious, implicit set of atomic terms and function symbols):\n\nwhere \"E\" is the type of entities in the domain, and \"T\" is the type of truth values.\n\nTogether with this core definition of the semantics of PG, we also have a reduction rules that are employed in parallel with the type reductions. Placing the syntactic types at the top and semantics below, we have\n\nFor example, applying this to the types and semantics for the sentence formula_31 (emphasizing the link being reduced)\n\nFor the sentence formula_32:\n\n\n"}
{"id": "1557562", "url": "https://en.wikipedia.org/wiki?curid=1557562", "title": "Propositional variable", "text": "Propositional variable\n\nIn mathematical logic, a propositional variable (also called a sentential variable or sentential letter) is a variable which can either be true or false. Propositional variables are the basic building-blocks of propositional formulas, used in propositional logic and higher logics.\nFormulas in logic are typically built up recursively from some propositional variables, some number of logical connectives, and some logical quantifiers. Propositional variables are the atomic formulas of propositional logic. \n\nIn a given propositional logic, we might define a formula as follows:\n\n\nIn this way, all of the formulas of propositional logic are built up from propositional variables as a basic unit. Propositional variables should not be confused with the metavariables which appear in the typical axioms of propositional calculus; the latter effectively range over well-formed formulae.\n\nPropositional variables are represented as nullary predicates in first order logic.\n\n\n"}
{"id": "7913725", "url": "https://en.wikipedia.org/wiki?curid=7913725", "title": "Rank abundance curve", "text": "Rank abundance curve\n\nA rank abundance curve or Whittaker plot is a chart used by ecologists to display relative species abundance, a component of biodiversity. It can also be used to visualize species richness and species evenness. It overcomes the shortcomings of biodiversity indices that cannot display the relative role different variables played in their calculation.\n\nThe curve is a 2D chart with relative abundance on the Y-axis and the abundance rank on the X-axis.\n\nThe rank abundance curve visually depicts both species richness and species evenness. Species richness can be viewed as the number of different species on the chart i.e., how many species were ranked. Species evenness is reflected in the slope of the line that fits the graph (assuming a linear, i.e. logarithmic series, relationship). A steep gradient indicates low evenness as the high-ranking species have much higher abundances than the low-ranking species. A shallow gradient indicates high evenness as the abundances of different species are similar.\n\nQuantitative comparison of rank abundance curves of different communities can be done using RADanalysis package in R. This package uses the max rank normalization method in which a rank abundance distribution is made by normalization of rank abundance curves of communities to the same number of ranks and then normalize the relative abundances to one.\n\n\n"}
{"id": "2935699", "url": "https://en.wikipedia.org/wiki?curid=2935699", "title": "Run-time algorithm specialisation", "text": "Run-time algorithm specialisation\n\nIn computer science, run-time algorithm specialization is a methodology for creating efficient algorithms for costly computation tasks of certain kinds. The methodology originates in the field of automated theorem proving and, more specifically, in the Vampire theorem prover project.\n\nThe idea is inspired by the use of partial evaluation in optimising program translation. \nMany core operations in theorem provers exhibit the following pattern.\nSuppose that we need to execute some algorithm formula_1 in a situation where a value of formula_2 \"is fixed for potentially many different values of\" formula_3. In order to do this efficiently, we can try to find a specialization of formula_4 for every fixed formula_2, i.e., such an algorithm formula_6, that executing formula_7 is equivalent to executing formula_1.\n\nThe specialized algorithm may be more efficient than the generic one, since it can \"exploit some particular properties\" of the fixed value formula_2. Typically, formula_7 can avoid some operations that formula_1 would have to perform, if they are known to be redundant for this particular parameter formula_2. \nIn particular, we can often identify some tests that are true or false for formula_2, unroll loops and recursion, etc.\n\nThe key difference between run-time specialization and partial evaluation is that the values of formula_2 on which formula_4 is specialised are not known statically, so the \"specialization takes place at run-time\".\n\nThere is also an important technical difference. Partial evaluation is applied to algorithms explicitly represented as codes in some programming language. At run-time, we do not need any concrete representation of formula_4. We only have to \"imagine\" formula_4 \"when we program\" the specialization procedure.\nAll we need is a concrete representation of the specialized version formula_6. This also means that we cannot use any universal methods for specializing algorithms, which is usually the case with partial evaluation. Instead, we have to program a specialization procedure for every particular algorithm formula_4. An important advantage of doing so is that we can use some powerful \"ad hoc\" tricks exploiting peculiarities of formula_4 and the representation of formula_2 and formula_3, which are beyond the reach of any universal specialization methods.\n\nThe specialized algorithm has to be represented in a form that can be interpreted.\nIn many situations, usually when formula_7 is to be computed on many values formula_3 in a row, we can write formula_6 as a code of a special abstract machine, and we often say that formula_2 is \"compiled\". \nThen the code itself can be additionally optimized by answer-preserving transformations that rely only on the semantics of instructions of the abstract machine.\n\nInstructions of the abstract machine can usually be represented as records. One field of such a record stores an integer tag that identifies the instruction type, other fields may be used for storing additional parameters of the instruction, for example a pointer to another\ninstruction representing a label, if the semantics of the instruction requires a jump. All instructions of a code can be stored in an array, or list, or tree.\n\nInterpretation is done by fetching instructions in some order, identifying their type\nand executing the actions associated with this type. \nIn C or C++ we can use a switch statement to associate \nsome actions with different instruction tags. \nModern compilers usually compile a switch statement with integer labels from a narrow range rather efficiently by storing the address of the statement corresponding to a value formula_27 in the formula_27-th cell of a special array. One can exploit this\nby taking values for instruction tags from a small interval of integers.\n\nThere are situations when many instances of formula_2 are intended for long-term storage and the calls of formula_1 occur with different formula_3 in an unpredictable order.\nFor example, we may have to check formula_32 first, then formula_33, then formula_34, and so on.\nIn such circumstances, full-scale specialization with compilation may not be suitable due to excessive memory usage. \nHowever, we can sometimes find a compact specialized representation formula_35\nfor every formula_2, that can be stored with, or instead of, formula_2. \nWe also define a variant formula_38 that works on this representation \nand any call to formula_1 is replaced by formula_40, intended to do the same job faster.\n\n\n\n"}
{"id": "3628399", "url": "https://en.wikipedia.org/wiki?curid=3628399", "title": "Scalar–tensor theory", "text": "Scalar–tensor theory\n\nIn theoretical physics, a scalar–tensor theory is a theory that includes both a scalar field and a tensor field to represent a certain interaction. For example, the Brans–Dicke theory of gravitation uses both a scalar field and a tensor field to mediate the gravitational interaction.\n\nModern physics tries to derive all physical theories from as few principles as possible. In this way, Newtonian mechanics as well as quantum mechanics are derived from Hamilton's \"principle of least action\". In this approach, the behavior of a system is not described via forces, but by functions which describe the energy of the system. Most important are the energetic quantities known as the Hamiltonian function and the Lagrangian function. Their derivatives in space are known as Hamiltonian density and the Lagrangian density. Going to these quantities leads to the field theories.\n\nModern physics uses field theories to explain reality. These fields can be scalar, vectorial or tensorial. An example of a scalar field is the temperature field. An example of a vector field is the wind velocity field. An example of a tensor field is the stress tensor field in a stressed body, used in continuum mechanics.\n\nIn physics, forces (as vectorial quantities) are given as the derivative (gradient) of scalar quantities named potentials. In classical physics before Einstein, gravitation was given in the same way, as consequence of a gravitational force (vectorial), given through a scalar potential field, dependent of the mass of the particles. Thus, Newtonian gravity is called a scalar theory. The gravitational force is dependent of the distance \"r\" of the massive objects to each other (more exactly, their centre of mass). Mass is a parameter and space and time are unchangeable.\n\nEinstein's theory of gravity, the General Relativity (GR) is of another nature. It unifies space and time in a 4-dimensional \"manifold\" called space-time. In GR there is no gravitational force, instead, the actions we ascribed to being a force are the consequence of the local curvature of space-time. That curvature is defined mathematically by the so-called metric, which is a function of the total energy, including mass, in the area. The derivative of the metric is a function that approximates the classical Newtonian force in most cases. The metric is a tensorial quantity of degree 2 (it can be given as a 4x4 matrix, an object carrying 2 indices).\n\nAnother possibility to explain gravitation in this context is by using both tensor (of degree n>1) and scalar fields, i.e. so that gravitation is given neither solely through a scalar field nor solely through a metric. These are scalar–tensor theories of gravitation.\n\nThe field theoretical start of General Relativity is given through the Lagrange density. It is a scalar and gauge invariant (look at gauge theories) quantity dependent on the curvature scalar R. This Lagrangian, following Hamilton's principle, leads to the field equations of Hilbert and Einstein. If in the Lagrangian the curvature (or a quantity related to it) is multiplied with a square scalar field, field theories of scalar–tensor theories of gravitation are obtained. In them, the gravitational constant of Newton is no longer a real constant but a quantity dependent of the scalar field.\n\nAn action of such a gravitational scalar–tensor theory can be written as follows:\n\nwhere formula_2 is the metric determinant, formula_3 is the Ricci scalar constructed from the metric formula_4, formula_5 is a coupling constant with the dimensions formula_6, formula_7 is the scalar-field potential, formula_8 is the material Lagrangian and formula_9 represents the non-gravitational fields. Here, the Brans–Dicke parameter formula_10 has been generalized to a function. Although formula_5 is often written as being formula_12, one has to keep in mind that the fundamental constant formula_13 there, is not the constant of gravitation that can be measured with, for instance, Cavendish type experiments. Indeed, the empirical gravitational constant is generally no longer a constant in scalar–tensor theories, but a function of the scalar field formula_14. The metric and scalar-field equations respectively write:\n\nand\nAlso, the theory satisfies the following conservation equation, implying that test-particles follow space-time geodesics such as in general relativity:\nwhere formula_18 is the stress-energy tensor defined as\n\nDeveloping perturbatively the theory defined by the previous action around a Minkowskian background, and assuming non-relativistic gravitational sources, the first order gives the Newtonian approximation of the theory. In this approximation, and for a theory without potential, the metric writes\nwith formula_21 satisfying the following usual Poisson equation at the lowest order of the approximation:\n\nwhere formula_23 is the density of the gravitational source and formula_24 (the subscript formula_25 indicates that the corresponding value is taken at present cosmological time and location). Therefore, the empirical gravitational constant is a function of the present value of the scalar-field background formula_26 and therefore theoretically depends on time and location. However, no deviation from the constancy of the Newtonian gravitational constant has been measured, implying that the scalar-field background formula_26 is pretty stable over time. Such a stability is not theoretically generally expected but can be theoretically explained by several mechanisms.\n\nDeveloping the theory at the next level leads to the so-called first post-Newtonian order. For a theory without potential and in a system of coordinates respecting the weak isotropy condition (i.e., formula_28), the metric takes the following form:\nwith\n\nwhere formula_34 is a function depending on the coordinate gauge\n\nIt corresponds to the remaining diffeomorphism degree of freedom that is not fixed by the weak isotropy condition. The sources are defined as\n\nthe so-called post-Newtonian parameters are\n\nand finally the empirical gravitational constant formula_38 is given by\n\nwhere formula_13 is the (true) constant that appears in the coupling constant formula_5 defined previously.\n\nCurrent observations indicate that formula_42, which means that formula_43. Although explaining such a value in the context of the original Brans–Dicke theory is impossible, Damour and Nordtvedt found that the field equations of the general theory often lead to an evolution of the function formula_10 toward infinity during the evolution of the universe. Hence, according to them, the current high value of the function formula_10 could be a simple consequence of the evolution of the universe.\n\nThe best current constraint on the post-Newtonian parameter formula_46 comes from Mercury's perihelion shift and is formula_47.\n\nBoth constraints show that while the theory is still a potential candidate to replace general relativity, the scalar field must be very weakly coupled in order to explain current observations.\n\nGeneralized scalar-tensor theories have also been proposed as explanation for the accelerated expansion of the universe but the measurement of the speed of gravity with the gravitational wave event GW170817 has ruled this out.\n\nAfter the postulation of the General Relativity of Einstein and Hilbert, Theodor Kaluza and Oskar Klein proposed in 1917 a generalization in a 5-dimensional manifold: Kaluza–Klein theory. This theory possesses a 5-dimensional metric (with a \"compactified and constant 5th metric component, dependent on the \"gauge potential\") and unifies gravitation and electromagnetism, i.e. there is a geometrization of electrodynamics.\n\nThis theory was modified in 1955 by P. Jordan in his \"Projective Relativity\" theory, in which, following group-theoretical reasonings, Jordan took a functional 5th metric component that lead to a variable gravitational constant \"G\". In his original work, he introduced coupling parameters of the scalar field, to change energy conservation as well, according to the ideas of Dirac.\n\nFollowing the \"Conform Equivalence theory\", multidimensional theories of gravity are \"conform equivalent\" to theories of usual General Relativity in 4 dimensions with an additional scalar field. One case of this is given by Jordan's theory, which, without breaking energy conservation (as it should be valid, following from microwave background radiation being of a black body), is equivalent to the theory of C. Brans and Robert H. Dicke of 1961, so that it is usually spoken about the \"Brans–Dicke theory\". The Brans–Dicke theory follows the idea of modifying Hilbert-Einstein theory to be compatible with Mach's principle. For this, Newton's gravitational constant had to be variable, dependent of the mass distribution in the universe, as a function of a scalar variable, coupled as a field in the Lagrangian. It uses a scalar field of infinite length scale (i.e. long-ranged), so, in the language of Yukawa's theory of nuclear physics, this scalar field is a \"massless field\". This theory becomes Einsteinian for high values for the parameter of the scalar field.\n\nIn 1979, R. Wagoner proposed a generalization of scalar–tensor theories using more than one scalar field coupled to the scalar curvature.\n\nJBD theories although not changing the geodesic equation for test particles, change the motion of composite bodies to a more complex one. The coupling of a universal scalar field directly to the gravitational field gives rise to potentially observable effects for the motion of matter configurations to which gravitational energy contributes significantly. This is known as the \"Dicke–Nordtvedt\" effect, which leads to possible violations of the Strong as well as the Weak Equivalence Principle for extended masses.\n\nJBD-type theories with short-ranged scalar fields use, according to Yukawa's theory, \"massive scalar fields\". The first of this theories was proposed by A. Zee in 1979. He proposed a Broken-Symmetric Theory of Gravitation, combining the idea of Brans and Dicke with the one of Symmetry Breakdown, which is essential within the Standard Model SM of elementary particles, where the so-called Symmetry Breakdown leads to mass generation (as a consequence of particles interacting with the Higgs field). Zee proposed the Higgs field of SM as scalar field and so the Higgs field to generate the gravitational constant.\n\nThe interaction of the Higgs field with the particles that achieve mass through it is short-ranged (i.e. of Yukawa-type) and gravitational-like (one can get a Poisson equation from it), even within SM, so that Zee's idea was taken 1992 for a scalar–tensor theory with Higgs field as scalar field with Higgs mechanism. There, the massive scalar field couples to the masses, which are at the same time the source of the scalar Higgs field, which generates the mass of the elementary particles through Symmetry Breakdown. For vanishing scalar field, this theories usually go through to standard General Relativity and because of the nature of the massive field, it is possible for such theories that the parameter of the scalar field (the coupling constant) does not have to be as high as in standard JBD theories. Though, it is not clear yet which of these models explains better the phenomenology found in nature nor if such scalar fields are really given or necessary in nature. Nevertheless, JBD theories are used to explain inflation (for massless scalar fields then it is spoken of the inflaton field) after the Big Bang as well as the quintessence. Further, they are an option to explain dynamics usually given through the standard cold dark matter models, as well as MOND, Axions (from Breaking of a Symmetry, too), MACHOS...\n\nA generic prediction of all string theory models is that the spin-2 graviton has a spin-0 partner called the dilaton. Hence, string theory predicts that the actual theory of gravity is a scalar–tensor theory rather than general relativity. However, the precise form of such a theory is not currently known because one does not have the mathematical tools in order to address the corresponding non-perturbative calculations. Besides, the precise effective 4-dimensional form of the theory is also confronted to the so-called landscape issue.\n\n\n"}
{"id": "1921562", "url": "https://en.wikipedia.org/wiki?curid=1921562", "title": "Sierra de la Plata", "text": "Sierra de la Plata\n\nThe Sierra de la Plata (\"Silver Mountains\") was a mythical source of silver in the interior of South America. The legend began in the early 16th century when castaways from the Juan Díaz de Solís expedition heard indigenous stories of a mountain of silver in an inland region ruled by the so-called White King. The first European to lead an expedition in search of it was the castaway Aleixo Garcia, who crossed nearly the entire continent to reach the Andean altiplano. On his way back to the coast, Garcia died in an ambush by indigenous tribespeople in Paraguay, but survivors brought precious metals back to corroborate their story. The legend inspired other expeditions, all of which ended in failure.\n\nThe Río de la Plata (literally \"Silver River\") and the modern country of Argentina (from the Latin \"argentum\", \"silver\") both take their names from the myth. The legend of the Sierra de la Plata may have been based on the Cerro Rico de Potosí in Bolivia, which was discovered by a Spanish expedition traveling from Peru in 1545.\n\nThe legend of the White King and the Sierra de la Plata began with the expeditions of Juan Díaz de Solís along the coast of South America. On his first voyage in 1512, Solís followed the coast of Brazil until he came across an enormous estuary, the Río de la Plata, which Amerigo Vespucci had named the River Jordan on his 1501-02 expedition and the local inhabitants called Paranaguazu (\"river like the sea\" or \"great water\"). Solís decided to call it the Mar Dulce (\"Freshwater Sea\") due to its great size. After exploring the area and guessing it could be a strait connecting the Atlantic to the Pacific, Solís returned to Spain to stake his claim as conqueror and governor of the region. In 1516, he returned with the title of Captain General, but when Solís and his party landed on the eastern bank of the Río de la Plata, they were attacked and killed by Guaranís. Seeing this, the crew remaining on the ships decided to weigh anchor and return to Spain.\n\nOn their way back to Europe, one of the Solís expedition's vessels shipwrecked off the coast of Santa Catarina Island in what is now Brazil, leaving eighteen men stranded. One of them, the Portuguese explorer Aleixo Garcia, became friendly with the local Tupí-Guaranís, and through them learned of a great mountain of shining metals far into the mainland.\n\nGarcia left Santa Catarina along with other castaways and a large indigenous party to search for the Sierra de la Plata, crossing most of South America before reaching the Andean altiplano. This was supposedly the home of the White King, whose throne was entirely decorated with silver. After taking a few valuable pieces, the explorers headed back to the Brazilian coast, but along the way, Aleixo Garcia and the other Europeans were killed in a Payaguá ambush. The few Tupí-Guaranís who managed to escape told their story, showing off the silver pieces they had gotten from the realms of the White King.\n\nIn 1526, the Venetian explorer Sebastian Cabot left Spain with the goal of reaching the Molucca Islands in Indonesia by way of the Straits of Magellan. During a stopover in Pernambuco in northern Brazil, he first heard the story about a land rich in precious metals far inland, which could be reached via an enormous estuary further south. The estuary ended up being called the Río de la Plata for its role as the supposed natural gateway to the treasure. The legend captivated Cabot, so he abandoned his mission and decided to find the Sierra de la Plata, assuming that the royal authorities would be indulgent if he found enough silver.\n\nOn Santa Catarina, the castaways Melchor Rodríguez and Enrique Morales confirmed the stories, telling Cabot about Aleixo Garcia's expedition and showing him the metals that had been brought back. Cabot headed toward Río de la Plata, where he disembarked to repair two ships that had been damaged in a storm. There, the expedition met former cabin boy Francisco del Puerto, the sole survivor of Solís's landing party. Del Puerto, who was living with the Guaranís, also verified the legend and offered his services as guide and interpreter.\n\nAfter entering the Río de la Plata, the expedition divided in two: Cabot would continue up the Paraná River and Antón de Grajeda would travel up the Uruguay River. In 1527, at the confluence of the Paraná and Carcarañá Rivers, Cabot established the fort of Sancti Spiritu, the first European settlement in the Río de la Plata basin, and a future base for expeditions to the land of the White King. The party was suffering from hunger and sickness, and since they could not travel by land, they continued north upriver until they landed at an island they named Año Nuevo (\"New Year\"). There, they traded colored glass with the Timbús for food, but Cabot, thinking he had been shortchanged, ordered his men to kill them, burn their homes, and take their food.\n\nIn February 1529, they reached an indigenous town they called Santa Ana, where they were treated hospitably, fed well, and told rumors of other \"white men\" who were coming up the river behind them. Cabot, however, stuck to his plan and continued up the Paraguay River until strong currents prevented him from going further. There, he had a brigantine sent ahead under the command of Miguel de Rifos. Near the confluence of the Pilcomayo River, Rifos decided to disembark with a few men after being welcomed by some indigenous people on the shore. The Europeans headed through the forest to the village, where they were unexpectedly ambushed. Supposedly, it was a trap arranged by the local chief and Del Puerto, who wanted a larger share of the plunder.\n\nThose who had stayed in the brigantine managed to escape, and when they returned to Cabot, he decided to head back to Sancti Spiritu. On the way, he came across Diego García, the \"other white man\" he had been told about. García, like Cabot, had been commissioned to travel to the Moluccas, but had deserted when he heard the tales of the White King. After a brief dispute, the two captains decided to join forces to find the Sierra de la Plata, with Cabot in charge of the unified fleet.\n\nAt Sancti Spiritu, Captain Francisco César was chosen to explore the local region together with another fifteen soldiers. Three months later César returned with half of his men and a rumor that nearby was a great city full of riches that from then on would be known as the Ciudad de los Césares (\"City of the Caesars\").\n\nThe Sebastian Cabot expedition ended in failure when Cabot and Diego García made their next attempt to find the Sierra de la Plata. The local indigenous people took advantage of their absence to attack and destroy Fort Sancti Spiritu, killing many of his men. Low on morale, food, and supplies, Cabot and his crew were finally forced to give up their goal and return to Europe.\n\nIn 1534, King Charles I authorized Pedro de Mendoza to \"conquer and populate the lands and provinces around the Solís River, which some call the Plate\". With fourteen ships and some 1,200 men, it was at the time the largest and most important expedition that had left Europe for America.\n\nIn 1536 Pedro de Mendoza founded the port of Santa María de los Buenos Ayres, which was probably located in what is now the Parque Lezama in Buenos Aires. Resources were in short supply to support such a population, and after the Spaniards mistreated the local Querandís, they stopped supplying food to the settlement. Mendoza decided to send out two fleets in search of food: the first, led by his nephew Gonzalo de Mendoza, headed toward Brazil, and the second, led by Juan de Ayolas, went up the River Paraná with three ships and 270 men. Meanwhile, Mendoza ordered his brother Diego to take 300 soldiers and 30 cavalrymen to fight the Querandís. The battle was a failure. Diego de Mendoza was killed, and the Querandís began a long-lasting siege of Buenos Aires. Cut off from supplies, the settlers began to eat their horses and their dead. Finally, the Querandís withdrew, and in a few days, the Juan de Ayolas expedition returned with the news that they had managed to build a fort called Corpus Christi upriver near the modern-day town of Gaboto and had encountered some indigenous people who talked about the Sierra de la Plata. Pedro de Mendoza decided to travel to the fort with Ayolas and some 400 men.\n\nDuring the voyage, some 200 men died of sickness and hunger, and once they reached Corpus Christi, Mendoza consulted Cabot's map to find that they were still far from their destination. These major setbacks, along with the syphilis he had contracted, convinced Mendoza to return to Europe after a short stop in Buenos Aires.\n\nPedro de Mendoza died on the open sea on June 23, 1537. Juan de Ayolas, who had left Corpus Christi on October 14, 1536 with a fleet of three brigantines and 170 soldiers, inherited his title of \"adelantado\". Meanwhile, Buenos Aires had overcome its famine thanks to provisions Gonzalo de Mendoza brought from Brazil, and was left under the provisional command of Captain Francisco Ruiz Galán, who ordered the first planting of corn with the goal of making the fort self-sustainable.\n\nBefore Pedro de Mendoza left Buenos Aires, having no news of the Ayolas expedition, he sent Juan de Salazar Espinosa upriver in search of him. On August 15, 1537, Espinosa established the fort of Nuestra Señora de la Asunción (today Asunción, Paraguay) at the junction of the rivers Paraguay and Pilcomayo.\n\nMeanwhile, Juan de Ayolas was further up the river Paraguay in Payagua territory, where he met one of Aleixo Garcia's former companions, who told him how difficult the journey had been, due to all the gold and silver that weighed them down. Hearing this story, Ayolas decided to found the port of Candelaria on the spot (close to present-day Corumbá) and commissioned Domingo Martínez de Irala as provisional Lieutenant Governor until he returned from an overland expedition with 130 soldiers. After a short time, with no news of Ayola, Irala decided to abandon his post and return downriver to the fort of Asunción.\n\nHowever, the Juan de Ayolas expedition had successfully reached a mountainous area where precious metals were mined. On his return trip, his party suffered losses from skirmishes with indigenous people, and before he reached the Paraguay River, he ordered his men to bury most of the treasure they carried. When he returned, he was disappointed to find the port of Candelaria abandoned, so he accepted the invitation of the Payaguas to rest in their village. On his way there, a fight broke out between the Spaniards and the local people, and almost the entire expedition was killed, including Juan de Ayolas.\n\nAfter this, Domingo Martínez de Irala became the expedition's new leader, and he took up the goal of conquering the Sierra de la Plata and the lands of the White King. Irala decided to convert Asunción into the headquarters of the conquest, and ordered the colonists at Buenos Aires to tear down their buildings and move to Asunción. However, their newly planted corn was yielding a good crop, and they refused to move. Six months later, Irala disembarked at Buenos Aires to carry out the order. The first settlement at Buenos Aires was finally destroyed and abandoned in 1541.\n\nWhile Irala was preparing his expedition to the Sierra de la Plata, Spain had chosen Álvar Núñez Cabeza de Vaca as the official successor to Pedro de Mendoza's title of \"adelantado\". Known for his long residence among the Indians of modern-day Texas and northern Mexico, Cabeza de Vaca decided to cross Paraguay on foot rather than travel by boat up the Río de la Plata. In October 1541, his expedition left Santa Catalina, crossing jungles, mountains, and rivers to reach Asunción. In January 1542, they passed Iguazú Falls and on March 11, 1542, they reached Asunción, where they met Domingo Martínez de Irala.\n\nCabeza de Vaca clashed with the colonists at Asunción, calling the village a \"Moorish paradise,\" as each colonist had taken multiple indigenous women as wives.\n\nSoon, Cabeza de Vaca began to prepare an expedition to the lands of the White King. First, he sent Irala up the Paraguay River to see if it led to the Sierra de la Plata. He reached La Gaiba Lake in the Pantanal region, where he founded the Puerto de los Reyes. In September 1543, Cabeza de Vaca led his own expedition through the forest, but sickness and clashes with his officers, mostly Irala's men, convinced him to abandon his search and return to Asunción.\n\nWith his authority undermined and disliked by the colonists, the Captain General was soon overthrown. On April 25, 1544, Irala's men entered Cabeza de Vaca's house and took him prisoner. Eleven months later, he was sent to Spain on a ship under the command of Gonzalo de Mendoza. During the voyage, a violent storm broke out, which the superstitious sailors interpreted as divine punishment, so they decided to free all of their prisoners. In Spain, Cabeza de Vaca denounced the colonists' actions to the court, but the case was never resolved, and he never returned to the Americas.\n\nWith his authority confirmed, Domingo Martínez de Irala organized an expedition of 300 Spanish men and 3,000 indigenous men in search of the Sierra de la Plata. After a few battles with indigenous peoples, Irala's men reached a tribe of Macasís, who immediately began speaking to them in Spanish. They told them their lord was Pedro Anzures, and therefore they were out of their jurisdiction. Irala sent a party to speak with the governor of Peru, Pedro de la Gasca, who only ordered the expedition to go no further under pain of death, so they had no choice but to return to Asunción.\n\nIrala organized several other expeditions to legendary locations like the \"Land of Riches\", the \"Lake of El Dorado\", and \"Paititi\". All of these expeditions ended in failure, with great cost in human lives and materials. Meanwhile, the king named Juan de Sanabria as the new \"adelantado\" in the region, but he died during preparations and was replaced by his son Diego, who ended up staying in Europe even though several of his ships had already sailed. Finally, the crown decided to formalize Irala's de facto power, so as Irala was preparing his next expedition, a royal emissary arrived in Asunción, informing Irala that he had been named governor of the Governorate of New Andalusia (also known as the Governorate of the Río de la Plata and Paraguay) with express orders not to lead any more expeditions. With Buenos Aires destroyed and the Sierra de la Plata under another jurisdiction, Paraguay experienced a long period of isolation under Irala, who finally died in October 1556 at the age of 70.\n\nThe Sierra de la Plata legend likely corresponds to the Cerro Rico de Potosí in Bolivia, and the White King to the Inca Huayna Cápac. When Aleixo Garcia explored the region and discovered precious metals in the early 16th century, Spain had barely begun colonizing the coasts of Panama and Colombia, and Portugal had barely begun colonizing the coast of Brazil. Neither of the two crowns knew about the existence of the Inca Empire until Francisco Pizarro encountered it in 1528, traveling from the Pacific coast. King Charles I tried to solve conflicts between conquistadors by dividing South America into several governorates: New Castile, under Francisco Pizarro, which reached from the Santiago River, Ecuador to Pisco, Peru; New Toledo, under Diego de Almagro, from Pisco to Taltal, Chile; and New Andalusia, under Pedro de Mendoza, two hundred leagues south. Of these three, it was Almagro's men who first found the Cerro Rico de Potosí, leaving New Andalusia without any claim to the Sierra de la Plata.\n\n\n"}
{"id": "509500", "url": "https://en.wikipedia.org/wiki?curid=509500", "title": "Social influence", "text": "Social influence\n\nSocial influence occurs when a person's emotions, opinions or behaviors are affected by others intentionally or unintentionally. Social influence takes many forms and can be seen in conformity, socialization, peer pressure, obedience, leadership, persuasion, sales, and marketing. In 1958, Harvard psychologist Herbert Kelman identified three broad varieties of social influence.\n\n\nMorton Deutsch and Harold Gerard described two psychological needs that lead humans to conform to the expectations of others. These include our need to be right (informational social influence) and our need to be liked (normative social influence). Informational influence (or \"social proof\") is an influence to accept information from another as evidence about reality. Informational influence comes into play when people are uncertain, either because stimuli are intrinsically ambiguous or because there is social disagreement. Normative influence is an influence to conform to the positive expectations of others. In terms of Kelman's typology, normative influence leads to public compliance, whereas informational influence leads to private acceptance.\n\nSocial influence is a broad term that relates to many different phenomena. Listed below are some major types of social influence that are being researched in the field of social psychology. For more information, follow the main article links provided.\n\nThere are three processes of attitude change as defined by Harvard psychologist Herbert Kelman in a 1958 paper published in the \"Journal of Conflict Resolution\". The purpose of defining these processes was to help determine the effects of social influence: for example, to separate public conformity (behavior) from private acceptance (personal belief).\n\nCompliance is the act of responding favorably to an explicit or implicit request offered by others. Technically, compliance is a change in behavior but not necessarily in attitude; one can comply due to mere obedience or by otherwise opting to withhold private thoughts due to social pressures. According to Kelman's 1958 paper, the satisfaction derived from compliance is due to the social effect of the accepting influence (i.e., people comply for an expected reward or punishment-aversion).\n\nIdentification is the changing of attitudes or behaviors due to the influence of someone who is admired. Advertisements that rely upon celebrity endorsements to market their products are taking advantage of this phenomenon. According to Kelman, the desired relationship that the identifier relates to the behavior or attitude change.\n\nInternalization is the process of acceptance of a set of norms established by people or groups that are influential to the individual. The individual accepts the influence because the content of the influence accepted is intrinsically rewarding. It is congruent with the individual's value system, and according to Kelman the \"reward\" of internalization is \"the content of the new behavior\".\n\nConformity is a type of social influence involving a change in behavior, belief, or thinking to align with those of others or with normative standards. It is the most common and pervasive form of social influence. Social psychology research in conformity tends to distinguish between two varieties: informational conformity (also called \"social proof\", or \"internalization\" in Kelman's terms ) and normative conformity (\"compliance\" in Kelman's terms).\n\nIn the case of peer pressure, a person is convinced to do something that they might not want to do (such as taking illegal drugs) but which they perceive as \"necessary\" to keep a positive relationship with other people (such as their friends). Conformity from peer pressure generally results from identification with the group members or from compliance of some members to appease others.\n\nConformity can be in appearance, or may be more complete in nature; impacting an individual both publicly and privately.\n\nCompliance (also referred to as acquiescence) demonstrates a public conformity to a group majority or norm, while the individual continues to privately disagree or dissent, holding on to their original beliefs or to an alternative set of beliefs differing from the majority. Compliance appears as conformity, but there is a division between the public and the private self.\n\nConversion includes the private acceptance that is absent in compliance. The individual's original behaviour, beliefs, or thinking changes to align with that of others (the influencers), both publicly and privately. The individual has accepted the behavior, belief, or thinking, and has internalized it, making it his own. Conversion may also refer to individual members of a group changing from their initial (and varied) opinions to adopt the opinions of others, which may differ from their original opinions. The resulting group position may be a hybrid of various aspects of individual initial opinions, or it may be an alternative independent of the initial positions reached through consensus.\n\nWhat appears to be conformity may in fact be congruence. Congruence occurs when an individual's behavior, belief, or thinking is already aligned with that of the others, and no change occurs.\n\nIn situations where conformity (including compliance, conversion, and congruence) is absent, there are non-conformity processes such as independence and anti-conformity. Independence, also referred to as dissent, involves an individual (either through their actions or lack of action, or through the public expression of their beliefs or thinking) being aligned with their personal standards but inconsistent with those of other members of the group (either all of the group or a majority). Anti-conformity, also referred to as counter-conformity, may appear as independence, but it lacks alignment with personal standards and is for the purpose of challenging the group. Actions as well as stated opinions and beliefs are often diametrically opposed to that of the group norm or majority. The underlying reasons for this type of behavior may be rebelliousness/obstinacy or it may be to ensure that all alternatives and view points are given due consideration.\n\nMinority influence takes place when a majority is influenced to accept the beliefs or behaviors of a minority. Minority influence can be affected by the sizes of majority and minority groups, the level of consistency of the minority group, and situational factors (such as the affluence or social importance of the minority). Minority influence most often operates through informational social influence (as opposed to normative social influence) because the majority may be indifferent to the liking of the minority.\n\nA self-fulfilling prophecy is a prediction that directly or indirectly causes itself to become true due to positive feedback between belief and behavior. A prophecy declared as truth (when it is actually false) may sufficiently influence people, either through fear or logical confusion, so that their reactions ultimately fulfill the once-false prophecy. This term is credited to sociologist Robert K. Merton from an article he published in 1948.\n\nReactance is the adoption of a view contrary to the view that a person is being pressured to accept, perhaps due to a perceived threat to behavioral freedoms. This phenomenon has also been called \"anticonformity\". While the results are the opposite of what the influencer intended, the reactive behavior is a result of social pressure. It is notable that anticonformity does not necessarily mean \"independence\". In many studies, reactance manifests itself in a deliberate rejection of an influence, even if the influence is clearly correct.\n\nObedience is a form of social influence that derives from an authority figure. The Milgram experiment, Zimbardo's Stanford prison experiment, and the Hofling hospital experiment are three particularly well-known experiments on obedience, and they all conclude that humans are surprisingly obedient in the presence of perceived legitimate authority figures.\n\nPersuasion is the process of guiding oneself or another toward the adoption of an attitude by rational or symbolic means. Robert Cialdini defined six \"weapons of influence\": reciprocity, commitment, social proof, authority, liking, and scarcity. These \"weapons of influence\" attempt to bring about conformity by directed means. Persuasion can occur through appeals to reason or appeals to emotion.\n\nPsychological manipulation is a type of social influence that aims to change the behavior or perception of others through abusive, deceptive, or underhanded tactics. By advancing the interests of the manipulator, often at another's expense, such methods could be considered exploitative, abusive, devious, and deceptive.\nSocial influence is not necessarily negative. For example, doctors can try to persuade patients to change unhealthy habits. Social influence is generally perceived to be harmless when it respects the right of the influenced to accept or reject it, and is not unduly coercive. Depending on the context and motivations, social influence may constitute underhanded manipulation.\n\nControlling abusers use tactics to exert power and control over their victims. The goal of the abuser is to control and intimidate the victim or to influence them to feel that they do not have an equal voice in the relationship.\n\nPropaganda is information that is not objective and is used primarily to influence an audience and further an agenda, often by presenting facts selectively to encourage a particular synthesis or perception, or using loaded language to produce an emotional rather than a rational response to the information that is presented.\n\nHard power is the use of military and economic means to influence the behavior or interests of other political bodies. This form of political power is often aggressive (coercion), and is most effective when imposed by one political body upon another of lesser military and/or economic power. Hard power contrasts with soft power, which comes from diplomacy, culture and history.\n\nMany factors can affect the impact of social influence.\n\nSocial impact theory was developed by Bibb Latané in 1981. This theory asserts that there are three factors which increase a person's likelihood to respond to social influence:\n\n\nRobert Cialdini defines six \"weapons of influence\" that can contribute to an individual's propensity to be influenced by a persuader:\n\nSocial Influence is strongest when the group perpetrating it is consistent and committed. Even a single instance of dissent can greatly wane the strength of an influence. For example, in Milgram's first set of obedience experiments, 65% of participants complied with fake authority figures to administer \"maximum shocks\" to a confederate. In iterations of the Milgram experiment where three people administered shocks (two of whom were confederates), once one confederate disobeyed, only ten percent of subjects administered the maximum shocks.\n\nThose perceived as experts may exert social influence as a result of their perceived expertise. This involves credibility, a tool of social influence from which one draws upon the notion of trust. People believe an individual to be credible for a variety of reasons, such as perceived experience, attractiveness, knowledge, etc. Additionally, pressure to maintain one's reputation and not be viewed as fringe may increase the tendency to agree with the group. This phenomenon is known as groupthink. Appeals to authority may especially affect norms of obedience. The compliance of normal humans to authority in the famous Milgram experiment demonstrate the power of perceived authority.\n\nThose with access to the media may use this access in an attempt to influence the public. For example, a politician may use speeches to persuade the public to support issues that he or she does not have the power to impose on the public. This is often referred to as using the \"bully pulpit.\" Likewise, celebrities don't usually possess any political power, but they are familiar to many of the world's citizens and, therefore, possess social status.\n\nPower is one of the biggest reasons an individual feels the need to follow through with the suggestions of another. A person who possesses more authority (or is perceived as being more powerful) than others in a group is an icon or is most \"popular\" within a group. This person has the most influence over others. For example, in a child's school life, people who seem to control the perceptions of the students at school are most powerful in having a social influence over other children.\n\nCulture appears to play a role in the willingness of an individual to conform to the standards of a group. Stanley Milgram found that conformity was higher in Norway than in France. This has been attributed to Norway's longstanding tradition of social responsibility, compared to France's cultural focus on individualism. Japan likewise has a collectivist culture and thus a higher propensity to conformity. However, a 1970 Asch-style study found that when alienated, Japanese students were more susceptible to \"anticonformity\" (giving answers that were \"incorrect\" even when the group had collaborated on \"correct\" answers) one third of the time, significantly higher than has been seen in Asch studies in the past.\n\nWhile gender does not significantly affect a person's likelihood to conform, under certain conditions gender roles do affect such a likelihood. Studies from the 1950s and 1960s concluded that women were more likely to conform than men. But a 1971 study found that experimenter bias was involved; all of the researchers were male, while all of the research participants were female. Studies thereafter found that the likelihood to conform almost equal between the genders. Furthermore, men conformed more often when faced with traditionally feminine topics, and women conformed more often when presented with masculine topics. In other words, ignorance about a subject can lead a person to defer to \"social proof\".\n\nEmotion and disposition may affect an individual's likelihood of conformity or anticonformity. In 2009, a study concluded that fear increases the chance of agreeing with a group, while romance or lust increases the chance of going against the group.\n\nA social network is a social structure made up of nodes (representing individuals or organizations) which are connected (through \"ties\", also called \"edges\", \"connections\", or \"links\") by one or more types of interdependency (such as friendship, common interests or beliefs, sexual relations, or kinship). Social network analysis uses the lens of network theory to examine social relationships. Social network analysis as a field has become more prominent since the mid-20th century in determining the channels and effects of social influence. For example, Christakis and Fowler found that social networks transmit states and behaviors such as obesity, smoking, drinking and happiness.\n\nIdentifying the extent of social influence, based on large-scale observational data with a latent social network structure, is pertinent to a variety of collective social phenomena including crime, civil unrest, and voting behavior in elections. For example, methodologies for disentangling social influence by peers from external influences—with latent social network structures and large-scale observational data—were applied to US presidential elections, stock markets, and civil unrest.\n"}
{"id": "398255", "url": "https://en.wikipedia.org/wiki?curid=398255", "title": "Sportsmanship", "text": "Sportsmanship\n\nSportsmanship is an aspiration or ethos that a sport or activity will be enjoyed for its own sake, with proper consideration for fairness, ethics, respect, and a sense of fellowship with one's competitors. A \"sore loser\" refers to one who does not take defeat well, whereas a \"good sport\" means being a \"good winner\" as well as being a \"good loser\" (someone who shows courtesy towards another in a sports game).\n\nSportsmanship can be conceptualized as an enduring and relatively stable characteristic or disposition such that individuals differ in the way they are generally expected to behave in sports situations. In general, sportsmanship refers to virtues such as fairness, self-control, courage, and persistence, and has been associated with interpersonal concepts of treating others and being treated fairly, maintaining self-control if dealing with others, and respect for both authority and opponents. Sportsmanship is also looked at as being the way one \nreacts to a sport/game/player.\n\nThe four elements of sportsmanship are often shown being good form, the will to win, equity and fairness. All four elements are critical and a balance must be found among all four for true sportsmanship to be illustrated. These elements may also cause conflict, as a person may desire to win more than play in equity and fairness and thus resulting in a clash within the aspects of sportsmanship. This will cause problems as the person believes they are being a good sportsman, but they are defeating the purpose of this idea as they are ignoring two key components of being sportsman like. When athletes become too self-centred, the idea of sportsmanship is dismissed.\n\nToday's sporting culture, in particular the base of elite sport, places great importance on the idea of competition and winning and thus sportsmanship takes a back seat as a result. In most, if not all sports, sportsmen at the elite level make the standards on sportsmanship and no matter whether they like it or not, they are seen as leaders and role models in society.\n\nSince every sport is rule driven, the most common offence of bad sportsmanship is the act of cheating or breaking the rules to gain an unfair advantage this is called unsportsmanlike conduct. A competitor who exhibits poor sportsmanship after losing a game or contest is often called a \"sore loser\", while a competitor who exhibits poor sportsmanship after winning is typically called a \"bad winner\". Sore loser behavior includes blaming others for the loss, not accepting responsibility for personal actions that contributed to the defeat, reacting to the loss in an immature or improper fashion, making excuses for the defeat, and citing unfavorable conditions or other petty issues as reasons for the defeat. A bad winner acts in a shallow fashion after his or her victory, such as by gloating about his or her win, rubbing the win in the face(s) of the opponent(s), and lowering the opponent(s)'s self-esteem by constantly reminding the opponent(s) of \"poor\" performance in comparison (even if the opponent(s) competed well). Not showing respect to the other team is considered to being a bad sportsman and could lead to demoralising effects; as Leslie Howe describes: \"If a pitcher in baseball decides to pitch not to his maximum ability suggest that the batter is not at an adequate level, [it] could lead to the batter to have low self-confidence or worth.\"\n\nThere are six different categories relating to sportsmanship: the elements of sports, the elements of sportsmanship, clarifications, conflicts, balance and irreducibility. All six of these characterize a person with good sportsmanship. Even though there is some affinity between some of the categories, they are distinct elements. \"In essence, play has for its directed and immediate end joy, pleasure, and delights and which is dominated by a spirit of moderation and generosity. Athletics, on the other hand, is essentially a competitive activity, which has for its end victory in the contest and which is characterized of dedication, sacrifice and intensity.\" (Feelezz, 1896, pp. 3) Hence, the virtues of a player are radically different from the virtues of an athlete. (Feelezz, 1896, pp. 3). When talking about misunderstanding sportsmanship, Rudd and Stoll (2013) provide an example from 1995, a U.S. high school athletic league banned the post-game handshake that was a part of sports such as football and basketball. The handshaking was banned because of fights that were ensuing after the handshake.(pp. 41) Most players are influenced by the leaders around them such as coaches and older players, \"if there are coaches and administrators who don't understand sportsmanship, then what about the players?\"\n\nThere are various ways that sportsmanship is practiced in different sports. Being a good sport often includes treating others as you would also like to be treated, cheer for good plays (even if it is made by the opposition), accept responsibility for your mistakes, and keep your perspective. An example of treating others how you would like to be treated would include being respectful and polite to other team members and the opposition because in return you would also like to be treated the same way. Cheer for good plays could include if in netball a player of the opposition made a good lead for the ball, which then resulted in a goal, everyone would either clap or make a supportive comment to acknowledge that what the player did was very well done. To accept responsibility for your mistakes will entail not placing the blame on other people.\n\nSome popular examples of good sportsmanship include shaking hands, help an opponent who may have fallen over, encourage everyone, cheer, clap or hi-five, and be respectful to everyone including teammates, the opposition, parents and officials. Most importantly it is often encouraged and said regarding sportsmanship that \"It's not whether you win or lose, it's how you play the game.\"\n\nSportsmanship can be manifested in different ways depending on the game itself or the culture of the group.\n\nSportsmanship can be affected by a few contributing factors such as the players' values and attitudes towards the sport and also the professional role models that are shown to the public. Role models in sport are expected to act in a moral and respectful way. When elite sporting role models do not encourage sportsmanship this can also encourage people in society to act in similar ways to the athletes that they look up to and idolize. For example, if an individual looked up to an athlete who was drinking excessively, they may see this as acceptable behavior. The direct correlation between sportsmanship and leadership is also considered to be another contributing factor. Having a positive environment in your sporting team will therefore create good sportsmanship from the individuals. Having a positive leadership by the captains, coaches and supporters would then encourage a positive sporting environment.\n\n\n"}
{"id": "48110", "url": "https://en.wikipedia.org/wiki?curid=48110", "title": "Tonya Harding", "text": "Tonya Harding\n\nTonya Maxene Price (née Harding; born November 12, 1970) is a retired American figure skater.\n\nA native of Portland, Oregon, Harding was raised primarily by her mother, who enrolled her in ice skating lessons beginning at age four. Harding would spend much of her early life training, eventually dropping out of high school to devote her time to the sport. After climbing the ranks in the U.S. Figure Skating Championships between 1986 and 1989, Harding won the 1989 Skate America competition. She was the 1991 and 1994 U.S. champion before being stripped of her 1994 title, and 1991 World silver medalist. In 1991, she earned distinction as being the first American woman to successfully land a triple Axel in competition, and the second woman to do so in history (behind Midori Ito). She is also a two-time Olympian and a two-time Skate America Champion.\n\nIn January 1994, Harding became embroiled in controversy when her ex-husband, Jeff Gillooly, orchestrated an attack on fellow US Olympian Nancy Kerrigan. After the 1994 Winter Olympics, Harding ultimately pleaded guilty to hindering the prosecution on March 16 and was banned for life on June 30, 1994 from the U.S. Figure Skating Association.\n\nIn the early 2000s, Harding competed as a professional boxer, and her life has been the subject of numerous films, documentaries, books, and academic studies. In 2014, two documentary television films about Harding's life and skating career (\"Nancy & Tonya\" and \"The Price of Gold\") were aired within two months of each other — inspiring Steven Rogers to write the darkly comedic biographical film \"I, Tonya\", released in 2017 and starring Margot Robbie as Harding. In 2018, she was a contestant on season 26 of \"Dancing with the Stars\", finishing in third place.\n\nTonya Maxene Harding was born on November 12, 1970, in Portland, Oregon, to LaVona Golden and Albert Gordon Harding (1933–2009). She was raised in East Portland, and began skating at age three, training with coach Diane Rawlinson. During her youth, Harding also hunted, drag raced, and learned automotive mechanics from her father. LaVona struggled to support the family while working as a waitress, and hand-sewed her daughter's skating costumes as they could not afford to purchase them. Harding's parents divorced after 19 years of marriage in 1987, when she was 16 years old. She later dropped out of Milwaukie High School during her sophomore year in order to focus on skating, and earned a General Equivalency Diploma.\n\nAccording to Harding, she was frequently abused by her mother. She stated that by the time she was seven years old, both physical and psychological abuse had become a regular part of her life. LaVona admitted to one instance of hitting Harding at an ice rink. In January 2018, Harding's childhood friend and filmmaker, Sandra Luckow, spoke in defence of Harding's mother because she felt that the 2017 film \"I, Tonya\" stretched some truths about LaVona's character. Luckow said that although Harding's mother could be \"egregious\" towards her daughter, LaVona actually funded and appreciated Harding's skating lessons – and had \"a huge amount of humanity.\"\n\nIn Harding's 2008 authorized biography, \"The Tonya Tapes\" (written by Lynda D. Prouse from recorded interviews she had with Harding), she revealed she was the victim of acquaintance rape in 1991 and that her half-brother, Chris Davison, molested her on several occasions when she was a child. In 1986, Harding called the police after Davison had been sexually harassing and terrorizing her. He was arrested and spent a short time in prison. Harding claimed that her parents were in denial about Davison's behavior and told her not to press criminal charges against him. Davison was killed in a 1988 unsolved vehicular hit-and-run accident. On May 3, 1994, during an interview with Rolonda Watts, Harding said that Chris Davison was the lone person in her life unworthy of forgiveness and \"the only person I've ever hated.\"\n\nHarding trained as a figure skater throughout her youth with coach Diane Rawlinson. In the mid-1980s, she began working her way up the competitive skating ladder. She placed sixth at the 1986 U.S. Figure Skating Championships, fifth in 1987 and 1988, and third in 1989. After competing in the February 1989 Nationals Championship, Harding began training with Dody Teachman as her coach. She then won the October 1989 Skate America competition, and was considered a strong contender at the February 1990 U.S. Figure Skating Championships. However, she was suffering from the flu and asthma and had a poor free skate. After the original program, she dropped from second place and finished seventh overall. Harding was a powerful free skater and typically had lower placements in the compulsory figures.\n\nHarding's breakthrough year came in 1991 when, at the U.S. Championships, she completed her first triple Axel in competition on February 16 — the first American woman to execute the jump. She won the 1991 U.S. Ladies' Singles title with the event's first 6.0 technical merit score since Janet Lynn's 1973 performance at the U.S. Championships. At the March 1991 World Championships, an international event, she again completed the triple Axel. Harding would finish second behind Kristi Yamaguchi, and in front of Nancy Kerrigan, marking the first time one country swept the ladies medal podium at the World Figure Skating Championships.\n\nAt the September 1991 Skate America competition, Harding recorded three more firsts:\n\nDespite these record-breaking performances, after 1991, Harding was never able to successfully complete the triple Axel in competition again; her competitive results began to decline. She and Dody Teachman had briefly parted ways in April 1991, but had reunited in June; Harding was still training under Teachman for the upcoming 1992 season. She placed third in the January 1992 U.S. Figure Skating Championships despite twisting her ankle during practice, and finished fourth in the February 1992 Winter Olympics. On March 1, 1992, Harding gave Teachman a summary dismissal and returned to Diane Rawlinson to be coached by her. On March 29 Harding placed sixth in the 1992 World Championships, although she had a better placement at the November 1992 Skate Canada International event finishing fourth. In the 1993 season, she skated poorly in the U.S. Championships and failed to qualify for the World Championship team.\n\nIn 1994, Harding won the U.S. Championships, but was later stripped of her title because of her involvement in the attack on Nancy Kerrigan. Kerrigan was unable to participate in the U.S. Championships due to her injury. Despite the legal controversy, Harding was permitted to remain a member of the U.S. ice skating team at the 1994 Winter Olympics in Lillehammer, Norway. After an issue with a broken skate lace, she was given a re-skate in the long program and finished in eighth place, far behind Oksana Baiul (gold) and Nancy Kerrigan (silver).\n\n In June 1994, Claire Ferguson, the President of the U.S. Figure Skating Association, voted to strip Harding of her 1994 title. However, the competition results were not changed and the title was left vacant rather than moving all the other competitors up one position.\n\nOn , one day before the U.S. Figure Skating Championship first Ladies' Singles competition, Nancy Kerrigan was attacked after a practice session at the Detroit Cobo Arena. The assailant was Shane Stant, contracted to break her right leg. Stant and his uncle, Derrick Smith, were hired for this assault by Harding's ex-husband, Jeff Gillooly, and her one-time bodyguard, Shawn Eckardt. After failing to find Kerrigan at her Massachusetts training rink, Stant had taken a 20-hour bus trip to Detroit. Nancy Kerrigan was walking behind a curtain to a corridor when Stant rushed behind her. Using both hands, he then swung a 21-inch ASP telescopic baton at her right leg, striking her above the knee. The intent was preventing her competing in both the National Championships (Kerrigan was the defending 1993 U.S. Ladies' Champion) and the Winter Olympics. Kerrigan's leg was not broken but severely bruised, forcing her to withdraw from the Championships and forgo competing to retain the U.S. Ladies' title. On January 8, Harding won the U.S. Ladies' Singles title; she and Kerrigan were then both selected for the 1994 Olympic team. On February 1, Gillooly's attorney negotiated a plea bargain in exchange for testimony regarding all involved parties in the attack. He was sentenced in July after publicly apologizing to Kerrigan – even though, he said, \"any apology coming from me rings hollow.\" Gillooly and Eckardt pleaded guilty to racketeering, Stant and Smith (who drove Stant in the getaway car and funneled money) pleaded guilty to conspiracy to commit second-degree assault — all served prison time. Judge Donald Londer noted the attack could have injured Kerrigan more seriously. On February 25, Harding finished eighth in the Olympics; Nancy Kerrigan, having recovered from her injury, won the Olympic silver medal behind Oksana Baiul from Ukraine. Eckardt was released from prison in September 1995 and changed his name to Brian Sean Griffith; he died at age 40 on December 12, 2007.\n\nOn January 11, 1994, Ann Schatz interviewed Harding at the KOIN-TV station in Portland, Oregon. Schatz asked if she had considered whether someone she knew had planned to attack Nancy. Harding answered \"I have definitely thought about it. No one controls my life but me...if there’s something in there that I don’t like, I’m going to change it.\" Harding also confirmed she had spoken with FBI agents while in Detroit and again in Portland. On January 13, Eckardt and Smith were arrested – Stant surrendered to an FBI office the next day. On January 14, the USFSA made a statement regarding whether Eckardt's arrest affected Harding's place on the U.S. Olympic team: \"we will deal only with the facts.\" Harding and Gillooly's lawyers publicly confirmed the couple were in daily contact and cooperation with law enforcement. On January 15, Harding and Gillooly spoke with reporters, but declined to comment about the investigation. On January 16, Harding's attorney read a news conference statement on her behalf, denying involvement in Kerrigan's attack. Harding left her home that evening to practice figure skating with her coaches, where she spoke with reporters and performed a triple Axel.\n\nOn January 18, 1994, Harding submitted to questioning by the district attorney and FBI, with her lawyers. She was interviewed for over 10 hours – 8 hours into the interview, her lawyer issued a statement announcing her separation from Jeff Gillooly: \"I continue to believe that Jeff is innocent of any wrongdoing. I wish him nothing but the best.\" Her full FBI transcript was press released on February 1. \"The Seattle Times\" reported the transcript stating that Harding had \"changed her story well into a long interview...After hours of denying any involvement in trying to cover up the plot, an FBI agent finally 'told [her] that he knew she had lied to him, that he would tell her exactly how she had lied to him'.\" In the transcript's final passage, Harding stated \"I hope everyone understands. I'm telling on someone I really care about. I know now [Jeff] is involved. I'm sorry.\" On January 19, Jeff Gillooly surrendered to the FBI. On January 20, Diane Sawyer asked Harding on \"Primetime Live\" about the ongoing criminal investigation. Harding said she had done nothing wrong. On January 27, it was reported that Gillooly had been testifying about the attack plot since January 26; possibly implicating Harding as having allegedly assisted. Harding's close friend, with whom she was living, spoke to reporters on her behalf: \"[Tonya] was shocked, very hurt…She was believing in [Jeff], what he was saying.\" Harding later held an 11am press-conference to read a prepared statement. She said she was sorry Nancy Kerrigan was attacked, that she respected Nancy, and claimed not to know in advance of the plot to disable her. Harding then publicly took responsibility \"for failing to report things [about the planned assault] when I returned home from Nationals [on January 10]...my failure to immediately report this information is not a crime.\" Many state laws including Oregon certify that the act of concealing criminal knowledge alone is not a crime.\n\nNancy Kerrigan's attack had received a lot of publicity; news media crews had camped outside her home. The story was on the covers of \"Sports Illustrated\", \"Newsweek\", and \"TIME\" in January 1994. There was now much speculation about Harding's own alleged involvement in the assault plot. As they would be competing together again in the February Hamar Olympic Games, speculation soon reached a media frenzy. Abby Haight and J.E. Vader, journalists for \"The Oregonian\", wrote a biography of Harding called \"Fire on Ice\", which included excerpts of her January 18 FBI interview. News media began regularly attending Harding's Portland practices, and unwelcomely recorded footage of her on February 7, running barefoot to stop a tow truck from hauling her illegally parked pickup. On February 10, Connie Chung interviewed Harding. When asked about Gillooly, Harding said \"I never did anything to hurt [Jeff]. If I ever did anything, it was to stick up for him and protect him.\" Chung also negotiated to fly on the same airplane with Harding to Oslo, leaving on February 15, and interviewed her again in Norway. Chung admitted she would not have travelled to Norway were it not for the scandal. The media frenzy continued on February 17, when Nancy Kerrigan and Harding shared the ice at a practice session in the Hamar Olympic Amphitheatre. Approximately 400 members of the press were there to document this practice. Scott Hamilton believed the sport was depicted as a \"tabloid event.\" It was noted that Nancy Kerrigan chose to wear the same skating costume at the practice session that she was wearing on January 6. Kerrigan later confirmed that her choice of dress that day was deliberate: \"Humour is good, it's empowering.\" The tape-delayed broadcast of the February 23 Ladies' Olympic technical program remains one of the most watched telecasts in American history.\nOn February 5, 1994, the disciplinary panel of the U.S. Figure Skating Association stated that reasonable grounds existed to believe Harding had violated the sport's code of ethics. Her admitted failure to report information about an assault on a fellow competitor, supported by her FBI interview transcripts, resulted in Harding being formally charged with \"[making] false statements about her knowledge.\" The panel also recommended that she face a disciplinary hearing. Claire Ferguson, president of the USFSA, decided not to suspend Harding's membership before any hearing took place. If Harding had been suspended, she likely still would have competed at the Winter Olympics after filing an injunction on the USFSA and asserting her rights under the Amateur Sports Act of 1978. Evidence examined by the panel included the testimonies of Stant and Smith, Harding and Gillooly's telephone records, and notes found in a Portland saloon trash receptacle on January 30. Harding was given 30 days to respond. On March 9, Judge Owen Panner granted her a requested halt on the 30-day deadline to delay her disciplinary hearing until June 27. Meanwhile, Portland authorities stated the criminal investigation would conclude by March 21 with any indictments and a grand jury report to be made at that time.\n\nOn March 16, 1994, Harding pleaded guilty to conspiracy to hinder prosecution as a Class C felony offense at a Multnomah County court hearing. She and her lawyer, Robert Weaver, negotiated a plea bargain ensuring no further prosecution. Judge Donald Londer conducted routine questioning to make certain Harding understood her agreement, that she was entering her plea \"knowingly and voluntarily.\" Harding told Londer she was. Her plea admissions were knowing of the assault plot after the fact, settling on a cover story with Gillooly and Eckardt on January 10, witnessing payphone calls to Smith affirming the story on January 10 and 11, and lying to FBI with the story on January 18. Law enforcement investigators had been following and videotaping the co-conspirators since January 10; they knew about the payphone calls. Her penalties included 3 years of probation, $100,000 fine, and 500 hours community service. She agreed to reimburse Multnomah County $10,000 in legal expenses, undergo a psychiatric examination, and volunteered to give $50,000 to the Special Olympics Oregon (SOOR) charity. Oregon sentencing guidelines carried a max penalty of 5-years-prison for the offense. Phil Knight, CEO of Nike, donated $25,000 toward Harding's legal fees. She had also made approximately $600,000 from an \"Inside Edition\" deal. Harding's plea conditions imposed her U.S. Figure Skating Assn resignation, necessitating her withdrawal from the World Championships (for which she was scheduled to leave tomorrow). District attorney Norman Frink stated that if Harding had not agreed to the plea, \"we would have proceeded with an indictment on all possible charges...punishment was taking away [skating] privilege.\" Weaver said the plea agreement was satisfactory to Harding, partly because she avoided prison. Regarding trial concerns, he stated \"we would have prevailed at trial.\" An executive of the USFSA commented \"[We] don't know if Tonya is innocent or guilty...if [she was involved before] the national championship.\" On March 18, Claire Ferguson decided Harding's disciplinary hearing would still proceed in June. The USFSA's executive committee convened to discuss their position should Harding seek reinstatement and whether they might strip her of the 1994 National Championship title. Neither issue was decided at that time.\n\nOn March 21, 1994, a Portland grand jury issued an indictment stating there was evidence Harding participated in the attack plot. The indictment concluded more than two months of investigation and witness testimonies from Diane Rawlinson, Erika Bakacs (Harding's choreographer), Eckardt's college instructor and classmates, and Vera Marano (a freelance figure skating writer in Philadelphia). It stated there was evidence Harding fraudulently used USFSA provided skating monies to finance the assault. It also read that Harding, Gillooly, Eckardt, Smith, and Stant agreed to \"knowingly cause physical injury...by means of a dangerous weapon.\" The grand jury foreman said the evidence inferred Harding as \"involved from the beginning or very close.\" She was not charged in the indictment due to the terms of her March 16 plea agreement. On June 29, the USFSA disciplinary panel met for nine hours over two days to consider Harding's alleged role in the attack. On June 30, chairman William Hybl stated \"By a preponderance of the evidence, the panel did conclude that she had prior knowledge and was involved prior to the incident. This is based on civil standards, not criminal standards...bank records, phone records – the way they came together to establish a case.\" The panel decided that pertinent FBI reports, court documents, and Harding's March 16 plea agreement presented \"a clear disregard for fairness, good sportsmanship, and ethical behaviour.\" Harding chose neither to attend nor participate in the two-day hearing. Robert Weaver said the decision disappointed her but was not a surprise, and that she had not decided on an appeal. Harding was stripped of the 1994 U.S. Championship title and banned for life from participating in USFSA events as either skater or coach. The USFSA has no dominion over professional skating events, yet Harding was also \"persona non grata\" on the pro circuit, few skaters and promoters would work with her. She did not benefit from the ensuing boom in professional skating after the scandal.\n\nShortly before the 1998 Winter Olympics, the CBS and Fox news divisions re-examined the scandal for two televised special reports. Harry Smith hosted the CBS special. He reported that Harding still held to her statement from her press-conference given on January 27, 1994: \"I had no prior knowledge of the planned assault on Nancy Kerrigan.\" Smith then interviewed Kerrigan, asking how she responded to that statement. Nancy Kerrigan referred to transcripts she had read from Harding's FBI interview on January 18, 1994. After reading through the interrogation of that day, she concluded that \"[Tonya] knew more than she admits.\" The Fox special report was called \"Breaking the Ice: The Women of '94 Revisited\", hosted by James Brown with interviews from Harding, Gillooly, and Kerrigan. Jeff Gillooly (granted a name change to Jeff Stone in 1995) said Harding's prison evasion did not anger him, and that he felt his own punishment was just. Stone reflected on Harding's position of \"limited involvement\" in Kerrigan's attack and speculated that a \"guilty conscience\" still troubled her. Brown then mediated a joint interview with both Kerrigan and Harding present. The two former competitors shared sincere desires for happy families and general well-wishes toward one other. Nancy Kerrigan said she hoped Harding could learn from past mistakes and \"find happiness.\" Harding said she was grateful to personally express remorse to Kerrigan again.\n\nIn Harding's 2008 biography, \"The Tonya Tapes\" (transcribed by Lynda D. Prouse from recorded interviews), she stated that she wanted to call the FBI in 1994 to reveal what she knew, but decided not to when Gillooly allegedly threatened her with death following a gunpoint gang rape by him and two other men she did not know. Jeff (Gillooly) Stone responded with surprise that groundless claims against him could be published and specifically contended her gang rape accusation to be \"utterly ridiculous.\" In 2013, \"Deadspin\" sought Jeff Stone for an interview and he again defended himself from the gunpoint gang rape allegation. Yet he expressed regret that Harding is often \"remembered for what I talked her into doing,\" meaning allegedly plotting to injure Nancy Kerrigan. Stone admitted that his past stupidity was part of Harding's 1994 ruin and maintained that he still considered her a great figure skater. He also said \"I've had it easy, compared to poor Tonya...she tends to be the butt of the joke. It's kind of sad to me.\"\n\nIn 2014, Nancy Kerrigan addressed the scandal during a brief interview with sportscaster Bob Costas: \"Whatever apology Tonya has given, I accept it. It's time for all us – I've always wished [Tonya] well – she has her own family, I have my family. It's time to make that our focus and move on with our lives.\"\n\nOn February 15, 1994, an explicit 1991 videotape clip of Harding topless was shown on \"A Current Affair\"; three still frames from this clip were also published in \"The Sun\" (a British tabloid newspaper). The \"New York Post\" reported that Jeff Gillooly had supplied the videotaped fragment for an undisclosed sum of money. On July 26, 1994, \"Penthouse\" magazine announced that its September issue would feature different stills of Harding and Gillooly having sex from the same extended videotape. This 35-minute sex tape would also be copied and marketed exclusively by \"Penthouse\". Both Gillooly and Harding used the same agent to negotiate equal payment on the \"Penthouse\" sale.\n\nOn June 22, 1994, in Portland, Oregon, Harding appeared on an AAA professional wrestling show as the manager for wrestling stable Los Gringos Locos. The night's performance included Art Barr and Eddie Guerrero. A promotional musical event was unsuccessful when Harding and her band, the Golden Blades, were booed off the stage at their only performance, in 1995 in Portland, Oregon.\n\nIn 1994, Harding was cast in a low-budget action film, \"Breakaway\". The film was released in 1996. On October 29, 1996, Harding received media attention after using mouth-to-mouth resuscitation to help revive an 81-year-old woman, Alice Olson, who collapsed at a bar in Portland while playing video poker.\n\nHarding has also appeared on television, on the game show \"The Weakest Link\": \"15 Minutes of Fame Edition\" in 2002 along with Kato Kaelin,\nand in March 2008 became a commentator for TruTV's \"\".\n\nIn 2002, Harding boxed against Paula Jones on the Fox TV network \"Celebrity Boxing\" event, winning the fight. On February 22, 2003, she made her official women's professional boxing debut, losing a four-round split decision against Samantha Browning on the undercard of Mike Tyson vs. Clifford Etienne. Harding's boxing career came about amid rumors that she was having financial difficulties and needed to fight in the ring to earn money. She did another celebrity boxing match, on \"The Man Show\", and won against co-host Doug Stanhope. Stanhope later claimed on his podcast that the fight was fixed because Tonya Harding refused to \"fight a man.\"\n\nOn March 23, 2004, it was reported that she canceled a planned boxing match against Tracy Carlton in Oakland, California, because of an alleged death threat against her.\n\nOn June 24, 2004, after reportedly not having boxed for over a year, she was beaten in a match in Edmonton, Alberta, by Amy Johnson. Fans reportedly booed her as she entered the ring and cheered wildly for Johnson when she won in the third round.\n\nHer boxing career was cut short by a physical condition she attributed to asthma. Her overall record was 3 wins and 3 losses.\n\nOn August 12, 2009, Harding set a new land speed record for a vintage gas coupe with a speed of driving a 1931 Ford Model A, named \"Lickity-Split,\" on the Bonneville Salt Flats. Her setting of that land speed record was featured on an episode of \"TruTV Presents: World's Dumbest...\" that focused on \"Record Breakers.\"\n\nIn April 2018, Harding was announced as one of the celebrities who would compete on season 26 of \"Dancing with the Stars.\" She was partnered with professional dancer Sasha Farber. The couple reached the finals of the competition, where Harding finished in third place overall, behind Adam Rippon and Josh Norman.\n\nIn September 1986, Harding began a relationship with 17-year-old Jeff Gillooly; they moved into a starter home together in 1988 when he worked in distribution at the Oregon Liquor Control Commission. They married on March 18, 1990, when she was 19 and he was 22. In January 1992, Harding told Terry Richard (a journalist for \"The Oregonian\") \"Jeff always put food on the table and a roof over my head. He paid for my skating for a couple of years. If it hadn't been for him during that time, I wouldn't have been skating.\" On August 28, 1993, they were divorced after a tumultuous marriage. During the autumn of 1993, it was reported that Gillooly was working part-time managing Harding's career and taking real estate classes. Harding and Gillooly had been continuing to see each other since early October 1993 and were sharing a rented chalet together in Beavercreek, Oregon until the evening of January 18, 1994.\n\nShe married her second husband, Michael Smith, in 1995; they divorced in 1996. She married and took the surname of 42-year-old Joseph Price, whom she met at a local restaurant called Timbers, on June 23, 2010 when she was 39 years old. She gave birth to her only child, a son named Gordon, on February 19, 2011.\n\nSince leaving skating and boxing, Harding has worked as a welder, a painter at a metal fabrication company, and a hardware sales clerk at Sears. As of 2017, she stated that she worked as a painter and deck builder. She resides in Washington state, north of her hometown of Portland, Oregon.\n\nOn an appearance on \"The Ellen DeGeneres Show\" on February 26, 2018, Harding stated that she is still active in skating and practices three times a week. In a segment during the show, she performed several jumps and spins. She trains with her former coach, Dody Teachman.\n\nHarding still receives criticism. However, her life, career, and role in the Kerrigan attack have been widely referenced in popular culture; appearing in television, film, music, as well as a primary campaign speech by former President Barack Obama. In 2014, Matt Harkins and Viviana Olen created the Nancy Kerrigan and Tonya Harding Museum in their Brooklyn, New York, apartment, dedicated to collecting and archiving memorabilia related to Harding and the incident. Harkins and Olen stated in a 2017 interview that they had been captivated by Harding's life for years, citing it as \"the most American story ever told.\" A contemporaneous article published in \"Vogue\" also noted that Harding had developed a \"cult following\" in the years since her notoriety.\n\n\nBecause Harding admitted to obstructing the investigation of Nancy Kerrigan's assault, her name is often first associated with the 1994 scandal. However, numerous academic essays have theorized additional reasons, particularly considering her national class culture and past figure skating membership, for Harding's perceived social stigmatization.\n\nIn 1995, the book \"Women on Ice: Feminist Essays on the Tonya Harding/Nancy Kerrigan Spectacle\" was published, containing numerous essays analyzing Harding's public image. For example, Abigail Feder-Kane wrote that there existed \"overdetermined femininity in Ladies' Figure Skating...femininity and athleticism are mutually exclusive concepts in American culture.\" Sam Stoloff believed that, during the scandal, the media placed a greater emphasis on Harding's class rather than her gender (femininity). He noted how she was subjected to a \"litany of vaguely pejorative or mocking expressions\" associated with \"low class\" cultural attributes, sometimes due to Harding's personal interests and hobbies. Stoloff theorized that Harding represented an American social class that required interpretation (\"the class Other\") as he referenced the anthropological tone of Susan Orlean's 1994 essay \"Figures in a Mall,\" written for \"The New Yorker\".\n\nIn academic Sarah Marshall's 2014 essay entitled \"Remote Control: Tonya Harding, Nancy Kerrigan, and the Spectacles of Female Power and Pain,\" she noted the pervasive role of the media in the 1994 scandal, particularly the manner in which Harding's life outside the realm of skating became publicly scrutinized: \"Somehow, in the scandal's aftermath, the form of the Tonya-bash was able to alchemize even the most chilling details of Tonya's life into tabloid gold.\" Marshall also examined the role of Harding's \"tomboy\" persona in the context of figure skating. She theorized that Harding was rejected by the figure skating ethos because she did not conform – as Marshall believed many figure skaters including Nancy Kerrigan did – to appearing as \"beautiful without being sexual, strong without being intimidating, and vulnerable without being weak.\"\n\nIn Tyler Chase Knowles' 2016 honors thesis entitled \"Taking Out the Trash: Middle Class Anxieties and The White Trash Menace,\" he extrapolated on a quotation made by Emma Gray in her 2014 \"HuffPost\" article. The quotation explored how Gray believed Harding was labelled \"the skating world's perfect villain\" due to her appearance, broken home, and impoverished background. Knowles theorized that there also existed the socio-cultural personified \"Defender\" who also chose to reject Harding. He defined the \"Defender\" as a person or persons who sympathize(s) with the working poor, a group including the white underclass. The \"Defender\" would blame the plight of the poor on societal and economic factors, yet the \"Defender\" would continue to reinforce conventional values such as distinguishing the class border. Knowles argued that Harding did not belong in the \"Defender's\" approved category of \"poor white\" because she did not – in the opinion of the \"Defender\" – make enough effort, given her figure skating membership, to assimilate towards elite culture. The \"Defender\" wanted Tonya Harding to use her USFSA membership for social mobility, caring little whether she could prove herself to be among the best Olympic competitors.\n\n\n"}
{"id": "44370", "url": "https://en.wikipedia.org/wiki?curid=44370", "title": "Trefoil", "text": "Trefoil\n\nTrefoil (from Latin \"\", \"three-leaved plant\") is a graphic form composed of the outline of three overlapping rings used in architecture and Christian symbolism. The term is also applied to other symbols of three-fold shape.\n\nTrefoil is a term in Gothic architecture given to the ornamental foliation or cusping introduced in the heads of window-lights, tracery, and panellings, in which the center takes the form of a three-lobed leaf (formed from three partially overlapping circles). One of the earliest examples is in the plate tracery at Winchester (1222–1235). The fourfold version of an architectural trefoil is a quatrefoil.\n\nA simple trefoil shape in itself can be symbolic of the Trinity, while a trefoil combined with an equilateral triangle was also a moderately common symbol of the Christian Trinity during the late Middle Ages in some parts of Europe. Two forms of this are shown below:\n\nA dove, which symbolizes the Holy Spirit, is sometimes depicted within the outlined form of the trefoil combined with a triangle.\n\nIn architecture and archaeology, trefoils describe a layout or floorplan consisting of three apses in clover-leaf shape, as for example in the Megalithic temples of Malta.\n\nParticularly in church architecture, such a layout may be called a \"triconchos\".\nThe heraldic \"trefoil\" is a stylized clover. It should not be confused with the figure named in French heraldry \"tiercefeuille\", which is a stylized flower with three petals. It differs from the heraldic trefoil in being not slipped. It could be translated as \"threefoil\".\n\nSymmetrical Trefoils are particularly popular as warning and informational symbols. If a box containing hazardous material is moved around and shifted into different positions, it is still easy to recognize the symbol, while the distinctive trefoil design of the recycling symbol makes it easy for a consumer to notice and identify the packaging the symbol has been printed on as recyclable. Easily stenciled symbols are also favored.\nWhile the green trefoil is considered by many to be the symbol of Ireland, the harp has much greater officially recognized status. Therefore, shamrocks generally do not appear on Irish coins or postage stamps.\n\nA trefoil is also part of the logo for Adidas Originals, which also includes three stripes.\n\n\n"}
{"id": "29920", "url": "https://en.wikipedia.org/wiki?curid=29920", "title": "Truth", "text": "Truth\n\nTruth is most often used to mean being in accord with fact or reality, or fidelity to an original or standard. Truth is also sometimes defined in modern contexts as an idea of \"truth to self\", or authenticity.\n\nTruth is usually held to be opposite to falsehood, which, correspondingly, can also suggest a logical, factual, or ethical meaning. The concept of truth is discussed and debated in several contexts, including philosophy, art, theology, and science. Most human activities depend upon the concept, where its nature as a concept is assumed rather than being a subject of discussion; these include most of the sciences, law, journalism, and everyday life. Some philosophers view the concept of truth as basic, and unable to be explained in any terms that are more easily understood than the concept of truth itself. To some, truth is viewed as the correspondence of language or thought to an independent reality, in what is sometimes called the correspondence theory of truth.\n\nVarious theories and views of truth continue to be debated among scholars, philosophers, and theologians. Language is a means by which humans convey information to one another. The method used to determine whether something is a truth is termed a criterion of truth. There are varying stances on such questions as what constitutes truth: what things are truthbearers capable of being true or false; how to define, identify, and distinguish truth; what roles do faith and empirical knowledge play; and whether truth can be subjective or is objective: relative truth versus absolute truth.\n\nThe English word \"truth\" is derived from Old English \"tríewþ, tréowþ, trýwþ\", Middle English \"trewþe\", cognate to Old High German \"triuwida\", Old Norse \"tryggð\". Like \"troth\", it is a \"-th\" nominalisation of the adjective \"true\" (Old English \"tréowe\").\n\nThe English word \"true\" is from Old English (West Saxon) \"(ge)tríewe, tréowe\", cognate to Old Saxon \"(gi)trûui\", Old High German \"(ga)triuwu\" (Modern German \"treu\" \"faithful\"), Old Norse \"tryggr\", Gothic \"triggws\", all from a Proto-Germanic \"*trewwj-\" \"having good faith\", perhaps ultimately from PIE *dru- \"tree\", on the notion of \"steadfast as an oak\" (e.g., Sanskrit \"taru\" tree).\nOld Norse ', \"faith, word of honour; religious faith, belief\" (archaic English \"troth\" \"loyalty, honesty, good faith\", compare ').\n\nThus, 'truth' involves both the quality of \"faithfulness, fidelity, loyalty, sincerity, veracity\", and that of \"agreement with fact or reality\", in Anglo-Saxon expressed by \"sōþ\" (Modern English \"sooth\").\n\nAll Germanic languages besides English have introduced a terminological distinction between truth \"fidelity\" and truth \"factuality\". To express \"factuality\", North Germanic opted for nouns derived from \"sanna\" \"to assert, affirm\", while continental West Germanic (German and Dutch) opted for continuations of \"wâra\" \"faith, trust, pact\" (cognate to Slavic \"věra\" \"(religious) faith\", but influenced by Latin \"verus\"). Romance languages use terms following the Latin \"veritas\", while the Greek \"aletheia\", Russian \"pravda\" and South Slavic \"istina\" have separate etymological origins.\n\nThe question of what is a proper basis for deciding how words, symbols, ideas and beliefs may properly be considered true, whether by a single person or an entire society, is dealt with by the five most prevalent substantive theories of truth listed below. Each presents perspectives that are widely shared by published scholars.\n\nTheories other than the most prevalent substantive theories are also discussed. More recently developed \"deflationary\" or \"minimalist\" theories of truth have emerged as possible alternatives to the most prevalent substantive theories. Minimalist reasoning centres around the notion that the application of a term like \"true\" to a statement does not assert anything significant about it, for instance, anything about its \"nature\". Minimalist reasoning realises \"truth\" as a label utilised in general discourse to express agreement, to stress claims, or to form general assumptions.\n\nCorrespondence theories emphasise that true beliefs and true statements correspond to the actual state of affairs. This type of theory stresses a relationship between thoughts or statements on one hand, and things or objects on the other. It is a traditional model tracing its origins to ancient Greek philosophers such as Socrates, Plato, and Aristotle. This class of theories holds that the truth or the falsity of a representation is determined in principle entirely by how it relates to \"things\", by whether it accurately describes those \"things.\" A classic example of correspondence theory is the statement by the thirteenth century philosopher and theologian Thomas Aquinas: \"Veritas est adaequatio rei et intellectus\" (\"Truth is the adequation of things and intellect\"), which Aquinas attributed to the ninth century Neoplatonist Isaac Israeli. Aquinas also restated the theory as: \"A judgment is said to be true when it conforms to the external reality\".\n\nCorrespondence theory centres heavily around the assumption that truth is a matter of accurately copying what is known as \"objective reality\" and then representing it in thoughts, words and other symbols. Many modern theorists have stated that this ideal cannot be achieved without analysing additional factors. For example, language plays a role in that all languages have words to represent concepts that are virtually undefined in other languages. The German word \"Zeitgeist\" is one such example: one who speaks or understands the language may \"know\" what it means, but any translation of the word apparently fails to accurately capture its full meaning (this is a problem with many abstract words, especially those derived in agglutinative languages). Thus, some words add an additional parameter to the construction of an accurate truth predicate. Among the philosophers who grappled with this problem is Alfred Tarski, whose semantic theory is summarized further below in this article.\n\nProponents of several of the theories below have gone further to assert that there are yet other issues necessary to the analysis, such as interpersonal power struggles, community interactions, personal biases and other factors involved in deciding what is seen as truth.\n\nFor coherence theories in general, truth requires a proper fit of elements within a whole system. Very often, though, coherence is taken to imply something more than simple logical consistency; often there is a demand that the propositions in a coherent system lend mutual inferential support to each other. So, for example, the completeness and comprehensiveness of the underlying set of concepts is a critical factor in judging the validity and usefulness of a coherent system. A pervasive tenet of coherence theories is the idea that truth is primarily a property of whole systems of propositions, and can be ascribed to individual propositions only according to their coherence with the whole. Among the assortment of perspectives commonly regarded as coherence theory, theorists differ on the question of whether coherence entails many possible true systems of thought or only a single absolute system.\n\nSome variants of coherence theory are claimed to describe the essential and intrinsic properties of formal systems in logic and mathematics. However, formal reasoners are content to contemplate axiomatically independent and sometimes mutually contradictory systems side by side, for example, the various alternative geometries. On the whole, coherence theories have been rejected for lacking justification in their application to other areas of truth, especially with respect to assertions about the natural world, empirical data in general, assertions about practical matters of psychology and society, especially when used without support from the other major theories of truth.\n\nCoherence theories distinguish the thought of rationalist philosophers, particularly of Spinoza, Leibniz, and G.W.F. Hegel, along with the British philosopher F.H. Bradley. They have found a resurgence also among several proponents of logical positivism, notably Otto Neurath and Carl Hempel.\n\nSocial constructivism holds that truth is constructed by social processes, is historically and culturally specific, and that it is in part shaped through the power struggles within a community. Constructivism views all of our knowledge as \"constructed,\" because it does not reflect any external \"transcendent\" realities (as a pure correspondence theory might hold). Rather, perceptions of truth are viewed as contingent on convention, human perception, and social experience. It is believed by constructivists that representations of physical and biological reality, including race, sexuality, and gender, are socially constructed.\n\nGiambattista Vico was among the first to claim that history and culture were man-made. Vico's epistemological orientation gathers the most diverse rays and unfolds in one axiom—\"verum ipsum factum\"—\"truth itself is constructed\". Hegel and Marx were among the other early proponents of the premise that truth is, or can be, socially constructed. Marx, like many critical theorists who followed, did not reject the existence of objective truth but rather distinguished between true knowledge and knowledge that has been distorted through power or ideology. For Marx, scientific and true knowledge is \"in accordance with the dialectical understanding of history\" and ideological knowledge is \"an epiphenomenal expression of the relation of material forces in a given economic arrangement\".\n\nConsensus theory holds that truth is whatever is agreed upon, or in some versions, might come to be agreed upon, by some specified group. Such a group might include all human beings, or a subset thereof consisting of more than one person.\n\nAmong the current advocates of consensus theory as a useful accounting of the concept of \"truth\" is the philosopher Jürgen Habermas. Habermas maintains that truth is what would be agreed upon in an ideal speech situation. Among the current strong critics of consensus theory is the philosopher Nicholas Rescher.\n\nIn the Islamic tradition, this principle is exemplified by the hadith in which Muhammad states, \"My community will never agree upon an error\"\n\nThe three most influential forms of the \"pragmatic theory of truth\" were introduced around the turn of the 20th century by Charles Sanders Peirce, William James, and John Dewey. Although there are wide differences in viewpoint among these and other proponents of pragmatic theory, they hold in common that truth is verified and confirmed by the results of putting one's concepts into practice.\n\nPeirce defines truth as follows: \"Truth is that concordance of an abstract statement with the ideal limit towards which endless investigation would tend to bring scientific belief, which concordance the abstract statement may possess by virtue of the confession of its inaccuracy and one-sidedness, and this confession is an essential ingredient of truth.\" This statement stresses Peirce's view that ideas of approximation, incompleteness, and partiality, what he describes elsewhere as \"fallibilism\" and \"reference to the future\", are essential to a proper conception of truth. Although Peirce uses words like \"concordance\" and \"correspondence\" to describe one aspect of the pragmatic sign relation, he is also quite explicit in saying that definitions of truth based on mere correspondence are no more than \"nominal\" definitions, which he accords a lower status than \"real\" definitions.\n\nWilliam James's version of pragmatic theory, while complex, is often summarized by his statement that \"the 'true' is only the expedient in our way of thinking, just as the 'right' is only the expedient in our way of behaving.\" By this, James meant that truth is a \"quality\", the value of which is confirmed by its effectiveness when applying concepts to practice (thus, \"pragmatic\").\n\nJohn Dewey, less broadly than James but more broadly than Peirce, held that inquiry, whether scientific, technical, sociological, philosophical or cultural, is self-corrective over time \"if\" openly submitted for testing by a community of inquirers in order to clarify, justify, refine and/or refute proposed truths.\n\nThough not widely known, a new variation of the pragmatic theory was defined and wielded successfully from the 20th century forward. Defined and named by William Ernest Hocking, this variation is known as \"negative pragmatism\". Essentially, what works may or may not be true, but what fails cannot be true because the truth always works. Richard Feynman also ascribed to it: \"We never are definitely right, we can only be sure we are wrong.\" This approach incorporates many of the ideas from Peirce, James, and Dewey. For Peirce, the idea of \"... endless investigation would tend to bring about scientific belief ...\" fits negative pragmatism in that a negative pragmatist would never stop testing. As Feynman noted, an idea or theory \"... could never be proved right, because tomorrow's experiment might succeed in proving wrong what you thought was right.\" Similarly, James and Dewey's ideas also ascribe truth to repeated testing which is \"self-corrective\" over time.\n\nPragmatism and negative pragmatism are also closely aligned with the coherence theory of truth in that any testing should not be isolated but rather incorporate knowledge from all human endeavors and experience. The universe is a whole and integrated system, and testing should acknowledge and account for its diversity. As Feynman said, \"... if it disagrees with experiment, it is wrong.\"\n\nModern developments in the field of philosophy, starting with the relatively modern notion that a theory being old does not necessarily imply that it is completely flawless, have resulted in the rise of a new thesis: that the term \"truth\" does not denote a real property of sentences or propositions. This thesis is in part a response to the common use of \"truth predicates\" (e.g., that some particular thing \"...is true\") which was particularly prevalent in philosophical discourse on truth in the first half of the 20th century. From this point of view, to assert that \"'2 + 2 = 4' is true\" is logically equivalent to asserting that \"2 + 2 = 4\", and the phrase \"is true\" is completely dispensable in this and every other context. In common parlance, truth predicates are not commonly heard, and it would be interpreted as an unusual occurrence were someone to utilise a truth predicate in an everyday conversation when asserting that something is true. Newer perspectives that take this discrepancy into account and work with sentence structures that are actually employed in common discourse can be broadly described:\n\nWhichever term is used, deflationary theories can be said to hold in common that \"[t]he predicate 'true' is an expressive convenience, not the name of a property requiring deep analysis.\" Once we have identified the truth predicate's formal features and utility, deflationists argue, we have said all there is to be said about truth. Among the theoretical concerns of these views is to explain away those special cases where it \"does\" appear that the concept of truth has peculiar and interesting properties. (See, e.g., Semantic paradoxes, and below.)\n\nIn addition to highlighting such formal aspects of the predicate \"is true\", some deflationists point out that the concept enables us to express things that might otherwise require infinitely long sentences. For example, one cannot express confidence in Michael's accuracy by asserting the endless sentence:\nThis assertion can also be succinctly expressed by saying: \"What Michael says is true\".\n\nAttributed to P. F. Strawson is the performative theory of truth which holds that to say \"'Snow is white' is true\" is to perform the speech act of signaling one's agreement with the claim that snow is white (much like nodding one's head in agreement). The idea that some statements are more actions than communicative statements is not as odd as it may seem. Consider, for example, that when the bride says \"I do\" at the appropriate time in a wedding, she is performing the act of taking this man to be her lawful wedded husband. She is not \"describing\" herself as taking this man, but actually doing so (perhaps the most thorough analysis of such \"illocutionary acts\" is J. L. Austin, \"How to Do Things With Words\").\n\nStrawson holds that a similar analysis is applicable to all speech acts, not just illocutionary ones: \"To say a statement is true is not to make a statement about a statement, but rather to perform the act of agreeing with, accepting, or endorsing a statement. When one says 'It's true that it's raining,' one asserts no more than 'It's raining.' The function of [the statement] 'It's true that...' is to agree with, accept, or endorse the statement that 'it's raining.'\"\n\nAccording to the redundancy theory of truth, asserting that a statement is true is completely equivalent to asserting the statement itself. For example, making the assertion that \" 'Snow is white' is true\" is equivalent to asserting \"Snow is white\". Redundancy theorists infer from this premise that truth is a redundant concept; that is, it is merely a word that is traditionally used in conversation or writing, generally for emphasis, but not a word that actually equates to anything in reality. This theory is commonly attributed to Frank P. Ramsey, who held that the use of words like \"fact\" and \"truth\" was nothing but a roundabout way of asserting a proposition, and that treating these words as separate problems in isolation from judgment was merely a \"linguistic muddle\".\n\nA variant of redundancy theory is the disquotational theory which uses a modified form of Tarski's schema: To say that '\"P\" is true' is to say that P. A version of this theory was defended by C. J. F. Williams in his book \"What is Truth?\". Yet another version of deflationism is the prosentential theory of truth, first developed by Dorothy Grover, Joseph Camp, and Nuel Belnap as an elaboration of Ramsey's claims. They argue that sentences like \"That's true\", when said in response to \"It's raining\", are prosentences, expressions that merely repeat the content of other expressions. In the same way that \"it\" means the same as \"my dog\" in the sentence \"My dog was hungry, so I fed it\", \"That's true\" is supposed to mean the same as \"It's raining\"—if you say the latter and I then say the former. These variations do not necessarily follow Ramsey in asserting that truth is \"not\" a property, but rather can be understood to say that, for instance, the assertion \"P\" may well involve a substantial truth, and the theorists in this case are minimizing only the redundancy or prosentence involved in the statement such as \"that's true.\"\n\nDeflationary principles do not apply to representations that are not analogous to sentences, and also do not apply to many other things that are commonly judged to be true or otherwise. Consider the analogy between the sentence \"Snow is white\" and the character named Snow White, both of which can be true in some sense. To a minimalist, saying \"Snow is white is true\" is the same as saying \"Snow is white,\" but to say \"Snow White is true\" is \"not\" the same as saying \"Snow White.\"\n\nPhilosophical skepticism is generally any questioning attitude or doubt towards one or more items of knowledge or belief which ascribe truth to their assertions and propositions. It is often directed at domains, such as the supernatural, morality (moral skepticism), religion (skepticism about the existence of God), or knowledge (skepticism about the possibility of knowledge, or of certainty). Formally, skepticism as a topic occurs in the context of philosophy, particularly epistemology, although it can be applied to any topic such as politics, religion, and pseudoscience.\n\nPhilosophical skepticism comes in various forms. Radical forms of skepticism deny that knowledge or rational belief is possible and urge us to suspend judgment regarding ascription of truth on many or all controversial matters. More moderate forms of skepticism claim only that nothing can be known with certainty, or that we can know little or nothing about the \"big questions\" in life, such as whether God exists or whether there is an afterlife. Religious skepticism is \"doubt concerning basic religious principles (such as immortality, providence, and revelation)\". Scientific skepticism concerns testing beliefs for reliability, by subjecting them to systematic investigation using the scientific method, to discover empirical evidence for them.\n\nSeveral of the major theories of truth hold that there is a particular property the having of which makes a belief or proposition true. Pluralist theories of truth assert that there may be more than one property that makes propositions true: ethical propositions might be true by virtue of coherence. Propositions about the physical world might be true by corresponding to the objects and properties they are about.\n\nSome of the pragmatic theories, such as those by Charles Peirce and William James, included aspects of correspondence, coherence and constructivist theories. Crispin Wright argued in his 1992 book \"Truth and Objectivity\" that any predicate which satisfied certain platitudes about truth qualified as a truth predicate. In some discourses, Wright argued, the role of the truth predicate might be played by the notion of superassertibility. Michael Lynch, in a 2009 book \"Truth as One and Many\", argued that we should see truth as a functional property capable of being multiply manifested in distinct properties like correspondence or coherence.\n\nAccording to a survey of professional philosophers and others on their philosophical views which was carried out in November 2009 (taken by 3226 respondents, including 1803 philosophy faculty members and/or PhDs and 829 philosophy graduate students) 45% of respondents accept or lean towards correspondence theories, 21% accept or lean towards deflationary theories and 14% epistemic theories.\n\nLogic is concerned with the patterns in reason that can help tell us if a proposition is true or not. However, logic does not deal with truth in the absolute sense, as for instance a metaphysician does. Logicians use formal languages to express the truths which they are concerned with, and as such there is only truth under some interpretation or truth within some logical system.\n\nA logical truth (also called an analytic truth or a necessary truth) is a statement which is true in all possible worlds or under all possible interpretations, as contrasted to a \"fact\" (also called a \"synthetic claim\" or a \"contingency\") which is only true in this world as it has historically unfolded. A proposition such as \"If p and q, then p\" is considered to be a logical truth because of the meaning of the symbols and words in it and not because of any fact of any particular world. They are such that they could not be untrue.\n\nDegrees of truth in logic may be represented using two or more discrete values, as with bivalent logic (or binary logic), three-valued logic, and other forms of finite-valued logic. Truth in logic can be represented using numbers comprising a continuous range, typically between 0 and 1, as with fuzzy logic and other forms of infinite-valued logic. In general, the concept of representing truth using more than two values is known as many-valued logic.\n\nThere are two main approaches to truth in mathematics. They are the \"model theory of truth\" and the \"proof theory of truth\".\n\nHistorically, with the nineteenth century development of Boolean algebra mathematical models of logic began to treat \"truth\", also represented as \"T\" or \"1\", as an arbitrary constant. \"Falsity\" is also an arbitrary constant, which can be represented as \"F\" or \"0\". In propositional logic, these symbols can be manipulated according to a set of axioms and rules of inference, often given in the form of truth tables.\n\nIn addition, from at least the time of Hilbert's program at the turn of the twentieth century to the proof of Gödel's incompleteness theorems and the development of the Church–Turing thesis in the early part of that century, true statements in mathematics were generally assumed to be those statements that are provable in a formal axiomatic system.\n\nThe works of Kurt Gödel, Alan Turing, and others shook this assumption, with the development of statements that are true but cannot be proven within the system. Two examples of the latter can be found in Hilbert's problems. Work on Hilbert's 10th problem led in the late twentieth century to the construction of specific Diophantine equations for which it is undecidable whether they have a solution, or even if they do, whether they have a finite or infinite number of solutions. More fundamentally, Hilbert's first problem was on the continuum hypothesis. Gödel and Paul Cohen showed that this hypothesis cannot be proved or disproved using the standard axioms of set theory. In the view of some, then, it is equally reasonable to take either the continuum hypothesis or its negation as a new axiom.\n\nReconsidered assumptions about the essence of truth arose in the wake of Gödel's incompleteness theorems. Martin Heidegger pointed out that truth may be essentially a matter of letting beings (entities of any kind, which can include logical propositions) be free to reveal themselves as they are, and stated:\nGödel agreed that the ability to perceive the truth of a mathematical or logical proposition is a matter of intuition, an ability he admitted could be ultimately beyond the scope of a formal theory of logic or mathematics and perhaps best considered in the realm of human comprehension and communication, but commented:\nThe semantic theory of truth has as its general case for a given language:\nwhere 'P' refers to the sentence (the sentence's name), and P is just the sentence itself.\n\nTarski's theory of truth (named after Alfred Tarski) was developed for formal languages, such as formal logic. Here he restricted it in this way: no language could contain its own truth predicate, that is, the expression \"is true\" could only apply to sentences in some other language. The latter he called an \"object language\", the language being talked about. (It may, in turn, have a truth predicate that can be applied to sentences in still another language.) The reason for his restriction was that languages that contain their own truth predicate will contain paradoxical sentences such as, \"This sentence is not true\". As a result, Tarski held that the semantic theory could not be applied to any natural language, such as English, because they contain their own truth predicates. Donald Davidson used it as the foundation of his truth-conditional semantics and linked it to radical interpretation in a form of coherentism.\n\nBertrand Russell is credited with noticing the existence of such paradoxes even in the best symbolic formations of mathematics in his day, in particular the paradox that came to be named after him, Russell's paradox. Russell and Whitehead attempted to solve these problems in \"Principia Mathematica\" by putting statements into a hierarchy of types, wherein a statement cannot refer to itself, but only to statements lower in the hierarchy. This in turn led to new orders of difficulty regarding the precise natures of types and the structures of conceptually possible type systems that have yet to be resolved to this day.\n\nKripke's theory of truth (named after Saul Kripke) contends that a natural language can in fact contain its own truth predicate without giving rise to contradiction. He showed how to construct one as follows:\n\nNotice that truth never gets defined for sentences like \"This sentence is false\", since it was not in the original subset and does not predicate truth of any sentence in the original or any subsequent set. In Kripke's terms, these are \"ungrounded.\" Since these sentences are never assigned either truth or falsehood even if the process is carried out infinitely, Kripke's theory implies that some sentences are neither true nor false. This contradicts the Principle of bivalence: every sentence must be either true or false. Since this principle is a key premise in deriving the Liar paradox, the paradox is dissolved.\n\nHowever, it has been shown by Gödel that self-reference cannot be avoided naively, since propositions about seemingly unrelated objects can have an informal self-referential meaning; in Gödel's work, these objects are integer numbers, and they have an informal meaning regarding propositions. In fact, this idea—manifested by the diagonal lemma—is the basis for Tarski's theorem that truth cannot be consistently defined.\n\nIt has thus been claimed that Kripke's system indeed leads to contradiction: while its truth predicate is only partial, it does give truth value (true/false) to propositions such as the one built in Tarski's proof, and is therefore inconsistent. While there is still a debate on whether Tarski's proof can be implemented to every similar partial truth system, none have been shown to be consistent by acceptable methods used in mathematical logic.\n\nThe revision theory of truth, as developed by Anil Gupta and Nuel Belnap, takes truth to be a circular concept whose definition is the set of biconditionals of the form\nUnlike Kripke's theory of truth, revision theory can be used with classical logic and can maintain the principle of bivalence.\n\nThe truth predicate \"\"P\" is true\" has great practical value in human language, allowing us to \"efficiently\" endorse or impeach claims made by others, to emphasize the truth or falsity of a statement, or to enable various indirect (Gricean) conversational implications. Individuals or societies will sometime punish \"false\" statements to deter falsehoods; the oldest surviving law text, the Code of Ur-Nammu, lists penalties for false accusations of sorcery or adultery, as well as for committing perjury in court. Even four-year-old children can pass simple \"false belief\" tests and successfully assess that another individual's belief diverges from reality in a specific way; by adulthood we have strong implicit intuitions about \"truth\" that form a \"folk theory\" of truth. These intuitions include:\n\n\nLike many folk theories, our folk theory of truth is useful in everyday life but, upon deep analysis, turns out to be technically self-contradictory; in particular, any formal system that fully obeys Capture and Release semantics for truth (also known as the \"T-schema\"), and that also respects classical logic, is provably inconsistent and succumbs to the liar paradox or to a similar contradiction.\n\nThe ancient Greek origins of the words \"true\" and \"truth\" have some consistent definitions throughout great spans of history that were often associated with topics of logic, geometry, mathematics, deduction, induction, and natural philosophy.\n\nSocrates', Plato's and Aristotle's ideas about truth are seen by some as consistent with correspondence theory. In his \"Metaphysics\", Aristotle stated: \"To say of what is that it is not, or of what is not that it is, is false, while to say of what is that it is, and of what is not that it is not, is true\". The Stanford Encyclopedia of Philosophy proceeds to say of Aristotle:\n\n[...] Aristotle sounds much more like a genuine correspondence theorist in the \"Categories\" (12b11, 14b14), where he talks of \"underlying things\" that make statements true and implies that these \"things\" (pragmata) are logically structured situations or facts (viz., his sitting, his not sitting). Most influential is his claim in \"De Interpretatione\" (16a3) that thoughts are \"likenesses\" (homoiosis) of things. Although he nowhere defines truth in terms of a thought's likeness to a thing or fact, it is clear that such a definition would fit well into his overall philosophy of mind. [...]\n\nVery similar statements can also be found in Plato (Cratylus 385b2, Sophist 263b).\n\nIn Hinduism, Truth is defined as \"unchangeable\", \"that which has no distortion\", \"that which is beyond distinctions of time, space, and person\", \"that which pervades the universe in all its constancy\". The human body, therefore is not completely true as it changes with time, for example. There are many references, properties and explanations of truth by Hindu sages that explain varied facets of truth, such as the national motto of India: \"Satyameva Jayate\" (Truth alone wins), as well as \"Satyam muktaye\" (Truth liberates), \"Satya' is 'Parahit'artham' va'unmanaso yatha'rthatvam' satyam\" (Satya is the benevolent use of words and the mind for the welfare of others or in other words responsibilities is truth too), \"When one is firmly established in speaking truth, the fruits of action become subservient to him ( patanjali yogasutras, sutra number 2.36 ), \"The face of truth is covered by a golden bowl. \"Unveil it, O Pusan (Sun), so that I who have truth as my duty (satyadharma) may see it!\"\" (Brhadaranyaka V 15 1–4 and the brief IIsa Upanisad 15–18), Truth is superior to silence (Manusmriti), etc. Combined with other words, satya acts as modifier, like \"ultra\" or \"highest,\" or more literally \"truest,\" connoting purity and excellence. For example, satyaloka is the \"highest heaven' and Satya Yuga is the \"golden age\" or best of the four cyclical cosmic ages in Hinduism, and so on.\n\nIn early Islamic philosophy, Avicenna (Ibn Sina) defined truth in his work Kitab Al-Shifa \"The Book of Healing\", Book I, Chapter 8, as:\n\nAvicenna elaborated on his definition of truth later in Book VIII, Chapter 6:\n\nHowever, this definition is merely a rendering of the medieval Latin translation of the work by Simone van Riet. A modern translation of the original Arabic text states:\n\nReevaluating Avicenna, and also Augustine and Aristotle, Thomas Aquinas stated in his \"Disputed Questions on Truth\":\n\nThus, for Aquinas, the truth of the human intellect (logical truth) is based on the truth in things (ontological truth). Following this, he wrote an elegant re-statement of Aristotle's view in his Summa I.16.1:\n\nRichard Firth Green examined the concept of truth in the later Middle Ages in his \"A Crisis of Truth\", and concludes that roughly during the reign of Richard II of England the very meaning of the concept changes. The idea of the oath, which was so much part and parcel of for instance Romance literature, changes from a subjective concept to a more objective one (in Derek Pearsall's summary). Whereas truth (the \"trouthe\" of \"Sir Gawain and the Green Knight\") was first \"an ethical truth in which truth is understood to reside in persons\", in Ricardian England it \"transforms...into a political truth in which truth is understood to reside in documents\".\n\nImmanuel Kant endorses a definition of truth along the lines of the correspondence theory of truth. Kant writes in the \"Critique of Pure Reason\": \"The nominal definition of truth, namely that it is the agreement of cognition with its object, is here granted and presupposed\". However, Kant denies that this correspondence definition of truth provides us with a test or criterion to establish which judgements are true. Kant states in his logic lectures:\n[...] Truth, it is said, consists in the agreement of cognition with its object. In consequence of this mere nominal definition, my cognition, to count as true, is supposed to agree with its object. Now I can compare the object with my cognition, however, only \"by cognizing it\". Hence my cognition is supposed to confirm itself, which is far short of being sufficient for truth. For since the object is outside me, the cognition in me, all I can ever pass judgement on is whether my cognition of the object agrees with my cognition of the object.\n\nThe ancients called such a circle in explanation a \"diallelon\". And actually the logicians were always reproached with this mistake by the sceptics, who observed that with this definition of truth it is just as when someone makes a statement before a court and in doing so appeals to a witness with whom no one is acquainted, but who wants to establish his credibility by maintaining that the one who called him as witness is an honest man. The accusation was grounded, too. Only the solution of the indicated problem is impossible without qualification and for every man. [...]\nThis passage makes use of his distinction between nominal and real definitions. A nominal definition explains the meaning of a linguistic expression. A real definition describes the essence of certain objects and enables us to determine whether any given item falls within the definition. Kant holds that the definition of truth is merely nominal and, therefore, we cannot employ it to establish which judgements are true. According to Kant, the ancient skeptics were critical of the logicians for holding that, by means of a merely nominal definition of truth, they can establish which judgements are true. They were trying to do something that is \"impossible without qualification and for every man\".\n\nGeorg Hegel distanced his philosophy from psychology by presenting truth as being an external self-moving object instead of being related to inner, subjective thoughts. Hegel's truth is analogous to the mechanics of a material body in motion under the influence of its own inner force. \"Truth is its own self-movement within itself.\" Teleological truth moves itself in the three-step form of dialectical triplicity toward the final goal of perfect, final, absolute truth. According to Hegel, the progression of philosophical truth is a resolution of past oppositions into increasingly more accurate approximations of absolute truth. Chalybäus used the terms \"thesis\", \"antithesis\", and \"synthesis\" to describe Hegel's dialectical triplicity. The \"thesis\" consists of an incomplete historical movement. To resolve the incompletion, an \"antithesis\" occurs which opposes the \"thesis.\" In turn, the \"synthesis\" appears when the \"thesis\" and \"antithesis\" become reconciled and a higher level of truth is obtained. This \"synthesis\" thereby becomes a \"thesis,\" which will again necessitate an \"antithesis,\" requiring a new \"synthesis\" until a final state is reached as the result of reason's historical movement. History is the Absolute Spirit moving toward a goal. This historical progression will finally conclude itself when the Absolute Spirit understands its own infinite self at the very end of history. Absolute Spirit will then be the complete expression of an infinite God.\n\nFor Arthur Schopenhauer, a judgment is a combination or separation of two or more concepts. If a judgment is to be an expression of knowledge, it must have a sufficient reason or ground by which the judgment could be called true. \"Truth is the reference of a judgment to something different from itself which is its sufficient reason (ground)\". Judgments can have material, formal, transcendental, or metalogical truth. A judgment has \"material\" truth if its concepts are based on intuitive perceptions that are generated from sensations. If a judgment has its reason (ground) in another judgment, its truth is called logical or \"formal\". If a judgment, of, for example, pure mathematics or pure science, is based on the forms (space, time, causality) of intuitive, empirical knowledge, then the judgment has \"transcendental\" truth.\n\nWhen Søren Kierkegaard, as his character \"Johannes Climacus\", ends his writings: \"My thesis was, subjectivity, heartfelt is the truth\", he does not advocate for subjectivism in its extreme form (the theory that something is true simply because one believes it to be so), but rather that the objective approach to matters of personal truth cannot shed any light upon that which is most essential to a person's life. Objective truths are concerned with the facts of a person's being, while subjective truths are concerned with a person's way of being. Kierkegaard agrees that objective truths for the study of subjects like mathematics, science, and history are relevant and necessary, but argues that objective truths do not shed any light on a person's inner relationship to existence. At best, these truths can only provide a severely narrowed perspective that has little to do with one's actual experience of life.\n\nWhile objective truths are final and static, subjective truths are continuing and dynamic. The truth of one's existence is a living, inward, and subjective experience that is always in the process of becoming. The values, morals, and spiritual approaches a person adopts, while not denying the existence of objective truths of those beliefs, can only become truly known when they have been inwardly appropriated through subjective experience. Thus, Kierkegaard criticizes all systematic philosophies which attempt to know life or the truth of existence via theories and objective knowledge about reality. As Kierkegaard claims, human truth is something that is continually occurring, and a human being cannot find truth separate from the subjective experience of one's own existing, defined by the values and fundamental essence that consist of one's way of life.\n\nFriedrich Nietzsche believed the search for truth, or 'the will to truth', was a consequence of the \"will to power\" of philosophers. He thought that truth should be used as long as it promoted life and the \"will to power\", and he thought untruth was better than truth if it had this life enhancement as a consequence. As he wrote in \"Beyond Good and Evil\", \"The falseness of a judgment is to us not necessarily an objection to a judgment... The question is to what extent it is life-advancing, life-preserving, species-preserving, perhaps even species-breeding...\" (aphorism 4). He proposed the \"will to power\" as a truth only because, according to him, it was the most life-affirming and sincere perspective one could have.\n\nRobert Wicks discusses Nietzsche's basic view of truth as follows:\n[...] Some scholars regard Nietzsche's 1873 unpublished essay, \"On Truth and Lies in a Nonmoral Sense\" (\"Über Wahrheit und Lüge im außermoralischen Sinn\") as a keystone in his thought. In this essay, Nietzsche rejects the idea of universal constants, and claims that what we call \"truth\" is only \"a mobile army of metaphors, metonyms, and anthropomorphisms.\" His view at this time is that arbitrariness completely prevails within human experience: concepts originate via the very artistic transference of nerve stimuli into images; \"truth\" is nothing more than the invention of fixed conventions for merely practical purposes, especially those of repose, security and consistence. [...]\n\nSeparately Nietzsche suggested that an ancient, metaphysical belief in the divinity of Truth lies at the heart of and has served as the foundation for the entire subsequent Western intellectual tradition: \"But you will have gathered what I am getting at, namely, that it is still a metaphysical faith on which our faith in science rests—that even we knowers of today, we godless anti-metaphysicians still take \"our\" fire too, from the flame lit by the thousand-year old faith, the Christian faith which was also Plato's faith, that God is Truth; that Truth is 'Divine'...\"\n\nOther philosophers take this common meaning to be secondary and derivative. According to Martin Heidegger, the original meaning and essence of truth in Ancient Greece was unconcealment, or the revealing or bringing of what was previously hidden into the open, as indicated by the original Greek term for truth, \"aletheia\". On this view, the conception of truth as correctness is a later derivation from the concept's original essence, a development Heidegger traces to the Latin term \"veritas\".\n\nAlfred North Whitehead, a British mathematician who became an American philosopher, said: \"There are no whole truths; all truths are half-truths. It is trying to treat them as whole truths that plays the devil\".\n\nThe logical progression or connection of this line of thought is to conclude that truth can lie, since half-truths are deceptive and may lead to a false conclusion.\n\nPragmatists like C. S. Peirce take truth to have some manner of essential relation to human practices for inquiring into and discovering truth, with Peirce himself holding that truth is what human inquiry would find out on a matter, if our practice of inquiry were taken as far as it could profitably go: \"The opinion which is fated to be ultimately agreed to by all who investigate, is what we mean by the truth...\"\n\nAccording to Kitaro Nishida, \"knowledge of things in the world begins with the differentiation of unitary consciousness into knower and known and ends with self and things becoming one again. Such unification takes form not only in knowing but in the valuing (of truth) that directs knowing, the willing that directs action, and the feeling or emotive reach that directs sensing.\"\n\nErich Fromm finds that trying to discuss truth as \"absolute truth\" is sterile and that emphasis ought to be placed on \"optimal truth\". He considers truth as stemming from the survival imperative of grasping one's environment physically and intellectually, whereby young children instinctively seek truth so as to orient themselves in \"a strange and powerful world\". The accuracy of their perceived approximation of the truth will therefore have direct consequences on their ability to deal with their environment. Fromm can be understood to define truth as a functional approximation of reality. His vision of optimal truth is described partly in \"Man from Himself: An Inquiry into the Psychology of Ethics\" (1947), from which excerpts are included below.\n\nTruth, says Michel Foucault, is problematic when any attempt is made to see truth as an \"objective\" quality. He prefers not to use the term truth itself but \"Regimes of Truth\". In his historical investigations he found truth to be something that was itself a part of, or embedded within, a given power structure. Thus Foucault's view shares much in common with the concepts of Nietzsche. Truth for Foucault is also something that shifts through various episteme throughout history.\n\nJean Baudrillard considered truth to be largely simulated, that is pretending to have something, as opposed to dissimulation, pretending to not have something. He took his cue from iconoclasts who he claims knew that images of God demonstrated that God did not exist. Baudrillard wrote in \"Precession of the Simulacra\":\n\nSome examples of simulacra that Baudrillard cited were: that prisons simulate the \"truth\" that society is free; scandals (e.g., Watergate) simulate that corruption is corrected; Disney simulates that the U.S. itself is an adult place. One must remember that though such examples seem extreme, such extremity is an important part of Baudrillard's theory. For a less extreme example, consider how movies usually end with the bad being punished, humiliated, or otherwise failing, thus affirming for viewers the concept that the good end happily and the bad unhappily, a narrative which implies that the status quo and established power structures are largely legitimate.\n\nThere is controversy as to the truth value of a proposition made in bad faith self-deception, such as when a hypochondriac has a complaint with no physical symptom.\n\nReference works\n\n"}
{"id": "4444344", "url": "https://en.wikipedia.org/wiki?curid=4444344", "title": "Vignette (literature)", "text": "Vignette (literature)\n\nIn a novel, theatrical script, screenplay, sketch stories, and poetry, a vignette (, ) is a short impressionistic scene that focuses on one moment or character and gives a trenchant impression about that character, an idea, setting, and/or object. It is a short, descriptive passage, more about evoking meaning through imagery than about plot.\n\nA blog or web series can also provide a form of vignette or be presented as a series of vignettes. An example of this is the critically acclaimed web series \"High Maintenance\", which presents a different set of characters in each episode, focusing intensely on their specific traits, ideas, and worlds.\n\nVignettes are more commonly used and have been particularly influential in the development of the contemporary notions of a scene as shown in postmodern theater, film and television, where less emphasis is placed on adhering to the conventions of traditional structure and story development. It is also a part of something bigger than itself: for example, a vignette about a house belonging to a collection of vignettes or a whole story, such as \"The House On Mango Street\", by Sandra Cisneros.\n\nThe word \"vignette\" means \"little vine\" in French, and the name of the literary form comes from the drawings of little vines that nineteenth-century printers used to decorate the title pages and beginnings of chapters.\n\n"}
