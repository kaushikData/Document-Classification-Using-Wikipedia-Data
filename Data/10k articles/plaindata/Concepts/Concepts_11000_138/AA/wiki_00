{"id": "1585868", "url": "https://en.wikipedia.org/wiki?curid=1585868", "title": "Abuse prevention program", "text": "Abuse prevention program\n\nAn abuse prevention program is a social program designed to help parents and teachers recognize the signs of violence in an abused child and teaches how to explain abuse protection to them. These programs also help children in establishing self-esteem.\n\nAn alternate definition of abuse prevention programme describes those projects which identify risk indicators such as poverty, poor housing, inadequate educational facilities etc. and aim to reduce the impact of such indicators, either through social reform or through developing parents and children's coping strategies.\n\n\n"}
{"id": "468230", "url": "https://en.wikipedia.org/wiki?curid=468230", "title": "Anava", "text": "Anava\n\nAnava (from \"anu\", meaning an atom or an exceedingly small entity) is a state - the consciousness of the ego, the sense of \"I\" and \"mine\". This represents a sense of individuality and a separation from a general existence of any \"divine plan\". One of the three Buddhist malas or bondages: anava, karma and maya. The three malas or pashas are also explicitly discussed in the theology of Shaivite Hinduism. In Shaivism, anava is the cause of the individual soul's mistaken sense of separate identity from Universal God Siva, and the last bond broken before union or Self-Realization (moksha).\n"}
{"id": "43853871", "url": "https://en.wikipedia.org/wiki?curid=43853871", "title": "Antarctica Weather Danger Classification", "text": "Antarctica Weather Danger Classification\n\nThe weather in Antarctica can be highly variable, and weather conditions will oftentimes change dramatically in a short period of time. There are various classifications for describing weather conditions in Antarctica; restrictions given to workers during the different conditions vary by station and nation.\n\nAn air temperature of or below would meet the criteria for Condition 2. And an air temperature of or colder would meet the criteria for Condition 1. However, the coldest temperature ever recorded at McMurdo Station was . So reaching would be unlikely to occur, and reaching would be virtually impossible. However, reaching Condition 2 based on windchill values of or below do happen. And reaching Condition 1 based on windchill values of or colder does occur.\n"}
{"id": "42283039", "url": "https://en.wikipedia.org/wiki?curid=42283039", "title": "Anupalabdhi", "text": "Anupalabdhi\n\nAnupalabdhi (Sanskrit: अनुपलब्धि) means 'non-recognition', 'non-perception'. This word refers to the Pramana of Non-perception which consists in the presentative knowledge of negative facts. \n\n\"Anupalabdhi\" or \"abhāvapramāṇa\" is the Pramana of Non-perception admitted by Kumārila for the perception of non-existence of a thing. He holds that the non-existence of a thing cannot be perceived by the senses for there is nothing with which the senses could come into contact in order to perceive the non-existence.\n\nAccording to the Bhāṭṭa school of Pūrva-Mīmāṃsā and Advaita-Vedānta system of philosophy, Anupalabdhi is a way to apprehend an absence; it is regarded as the source of knowledge, the other five being – \"pratyakṣa\" ('perception'), \"anumāna\" ('inference'), \"śabda\" ('testimony'), \"upamāna\" ('comparison') and \"arthāpatti\" ('presumption'). The perception of negation or non-existence in its various forms is also due to the relation of attributiveness.\n\nAll things exist in places either in a positive (\"sadrupa\") or in a negative (\"asadrūpa\") relation, and it is only in the former case that they come into contact with the senses, while in the latter case the perception of the negative existence can only be had by a separate mode of movement of the mind, a separate \"pramāṇa\" – \"anupalabdhi\". \n\nIndirect knowledge of non-existence can be attained by other means but direct knowledge of non-existence of perceptible objects and their attributes is available only through this kind of \"pramāṇa\" which is not inference.\n\nThere are four verities of \"Anupalabdhi\" which have been identified, they are – a) \"kāraṇa-anupalabdhi\" or 'non-perception of the causal condition', b) \"vyāpaka-anupalabdhi\" or 'non-perception of the pervader', c) \"svabhāva-anupalabdhi\" or 'non-perception of presence of itself', and d) \"viruddha-anupalabdhi\" or 'non-perception of the opposed'. The lack of perceptible (\"yogya\") adjuncts (\"upādhi\") is known through non-perception of what is perceptible (\"yogya-anupalabdhi\") and the lack of imperceptible adjuncts is known by showing that which is thought to be an adjunct.\n\nThe followers of Prabhākara and the Vishishtadvaita do not accept \"anupalabdhi\" as a separate \"parmāṇa\" because the same sense organs which apprehend an entity can also cognize its \"abhāva\" or the non-existence.\n\nAccording to Dharmakirti, \"anupalabdhi\" is the affirmative assertion of a negative prediction, and is same as \"anumāna\" of an \"abhāva\".\n"}
{"id": "58122233", "url": "https://en.wikipedia.org/wiki?curid=58122233", "title": "Arrest of Dov Haiyun", "text": "Arrest of Dov Haiyun\n\nDov Haiyun, a Conservative rabbi, was arrested by Haifa police in July 2018 for performing a non-Orthodox wedding. Though the rabbi was detained only for a few hours, the incident has sparked protests and has brought international attention to an Israeli law that prohibits any traditional Jewish marriage that is not sanctioned by the Chief Rabbinate. Hundreds of people gathered outside the Haifa Rabbinic Court to protest the arrest. While waiting to be interrogated Haiyun posted to his Facebook account: \"Iran is here already\". A few weeks after the arrest, in early August 2018, Haiyun announced that he has joined the Meretz Party.\n\nPolice reportedly woke Haiyun at his home at 5:30 AM and took him in for questioning. While waiting to be questioned, Haiyun posted: \"Iran is here\" to his Facebook account. According to Haiyun the Orthodox Rabbinical Court in Haifa had filed a complaint against him for performing unsanctioned weddings. Haiyun was scheduled to appear at a public event at the President's Residence, which led to some speculation that the arrest may have been intended to embarrass Israeli President Reuven Rivlin. A senior advisor to Attorney General Avihai Mendelblit said the police had arrested Haiyun at his home after Haiyun failed to respond to a police summons. \n\nThe marriage law's restrictions on marriage have long been a point of contention for secular Israelis, who have protested by getting married abroad in neighboring countries like Cyprus. (Weddings performed in Israel by Reform and Conservative rabbis are not recognized by the State of Israel, but Israel does recognize weddings that are performed abroad.) Under a 2013 amendment to the law, rabbis may be also prosecuted if they perform unsanctioned weddings that are not registered with the Chief Rabbinate.\n\nThe \"Associated Press\" reported that Haiyun's arrest was denounced by opposition lawmakers \"as a violation of religious freedom\". Yair Lapid who heads the opposition party Yesh Atid asked \"What does it say about us that Israel has become the only democracy in the world in which Jews don’t have freedom of religion?\"The director of \"Israel Hofsheet\" (Be Free Israel), a civil rights advocacy organization, said called for the \"immediate legislation of a civil marriage law\".\n\nThe \"Times of Israel\" reported that Haiyun's detention had been \"castigated by mainstream US Jewry\". The United Synagogue of Conservative Judaism issued a statement characterizing the incident as \"a new and dangerous step in the ongoing attack on religious freedom and civil liberties in Israel.\" It was similarly criticized by the Presidents' Conference, the UJA-Federation of New York and the Jewish Federations of North America. The Rabbinical Assembly said they were \"outraged\". In an opinion piece published by \"The Forward\", Brookings Institute fellow Dany Bahar wrote that full freedom of religion in Israel was a \"catchy Hasbara talking point\" that was \"outdated , after Rabbi Dov Haiyun was taken from his home at 5 a.m.\"\n"}
{"id": "76095", "url": "https://en.wikipedia.org/wiki?curid=76095", "title": "Avant-garde", "text": "Avant-garde\n\nThe avant-garde (; ; from French, \"advance guard\" or \"vanguard\", literally \"fore-guard\") are people or works that are experimental, , or unorthodox with respect to art, culture, or society. It may be characterized by nontraditional, aesthetic innovation and initial unacceptability, and it may offer a critique of the relationship between producer and consumer.\n\nThe avant-garde pushes the boundaries of what is accepted as the norm or the status quo, primarily in the cultural realm. The avant-garde is considered by some to be a hallmark of modernism, as distinct from postmodernism. Many artists have aligned themselves with the avant-garde movement and still continue to do so, tracing a history from Dada through the Situationists to postmodern artists such as the Language poets around 1981.\n\nThe avant-garde also promotes radical social reforms. It was this meaning that was evoked by the Saint Simonian Olinde Rodrigues in his essay \"L'artiste, le savant et l'industriel\" (\"The artist, the scientist and the industrialist\", 1825), which contains the first recorded use of \"avant-garde\" in its now customary sense: there, Rodrigues calls on artists to \"serve as [the people's] avant-garde\", insisting that \"the power of the arts is indeed the most immediate and fastest way\" to social, political and economic reform.\n\nSeveral writers have attempted to map the parameters of avant-garde activity. The Italian essayist Renato Poggioli provides one of the earliest analyses of vanguardism as a cultural phenomenon in his 1962 book \"Teoria dell'arte d'avanguardia\" (\"The Theory of the Avant-Garde\"). Surveying the historical, social, psychological and philosophical aspects of vanguardism, Poggioli reaches beyond individual instances of art, poetry, and music to show that vanguardists may share certain ideals or values which manifest themselves in the non-conformist lifestyles they adopt: He sees vanguard culture as a variety or subcategory of Bohemianism. Other authors have attempted both to clarify and to extend Poggioli's study. The German literary critic Peter Bürger's \"Theory of the Avant-Garde\" (1974) looks at the Establishment's embrace of socially critical works of art and suggests that in complicity with capitalism, \"art as an institution neutralizes the political content of the individual work\".\n\nBürger's essay also greatly influenced the work of contemporary American art-historians such as the German Benjamin H. D. Buchloh (born 1941). Buchloh, in the collection of essays \"Neo-avantgarde and Culture Industry\" (2000) critically argues for a dialectical approach to these positions. Subsequent criticism theorized the limitations of these approaches, noting their circumscribed areas of analysis, including Eurocentric, chauvinist, and genre-specific definitions.\n\nThe concept of avant-garde refers primarily to artists, writers, composers and thinkers whose work is opposed to mainstream cultural values and often has a trenchant social or political edge. Many writers, critics and theorists made assertions about vanguard culture during the formative years of modernism, although the initial definitive statement on the avant-garde was the essay \"Avant-Garde and Kitsch\" by New York art critic Clement Greenberg, published in \"Partisan Review\" in 1939. Greenberg argued that vanguard culture has historically been opposed to \"high\" or \"mainstream\" culture, and that it has also rejected the artificially synthesized mass culture that has been produced by industrialization. Each of these media is a direct product of Capitalism—they are all now substantial industries—and as such they are driven by the same profit-fixated motives of other sectors of manufacturing, not the ideals of true art. For Greenberg, these forms were therefore \"kitsch\": phony, faked or mechanical culture, which often pretended to be more than they were by using formal devices stolen from vanguard culture. For instance, during the 1930s the advertising industry was quick to take visual mannerisms from surrealism, but this does not mean that 1930s advertising photographs are truly surreal.\n\nVarious members of the Frankfurt School argued similar views: thus Theodor Adorno and Max Horkheimer in their essay \"The Culture Industry: Enlightenment as Mass-Deception\" (1944), and also Walter Benjamin in his highly influential \"The Work of Art in the Age of Mechanical Reproduction\" (1935, rev. 1939). Where Greenberg used the German word \"kitsch\" to describe the antithesis of avant-garde culture, members of the Frankfurt School coined the term \"mass culture\" to indicate that this bogus culture is constantly being manufactured by a newly emerged culture industry (comprising commercial publishing houses, the movie industry, the record industry, and the electronic media). They also pointed out that the rise of this industry meant that artistic excellence was displaced by sales figures as a measure of worth: a novel, for example, was judged meritorious solely on whether it became a best-seller, music succumbed to ratings charts and to the blunt commercial logic of the Gold disc. In this way the autonomous artistic merit so dear to the vanguardist was abandoned and sales increasingly became the measure, and justification, of everything. Consumer culture now ruled.\n\nThe avant-garde's co-option by the global capitalist market, by neoliberal economies, and by what Guy Debord called \"The Society of the Spectacle\", have made contemporary critics speculate on the possibility of a meaningful avant-garde today. Paul Mann's \"Theory-Death of the Avant-Garde\" demonstrates how completely the avant-garde is embedded within institutional structures today, a thought also pursued by Richard Schechner in his analyses of avant-garde performance.\n\nDespite the central arguments of Greenberg, Adorno and others, various sectors of the mainstream culture industry have co-opted and misapplied the term \"avant-garde\" since the 1960s, chiefly as a marketing tool to publicise popular music and commercial cinema. It has become common to describe successful rock musicians and celebrated film-makers as \"avant-garde\", the very word having been stripped of its proper meaning. Noting this important conceptual shift, major contemporary theorists such as Matei Calinescu in \"Five Faces of Modernity: Modernism, Avant-garde, Decadence, Kitsch, Postmodernism\" (1987), and Hans Bertens in \"The Idea of the Postmodern: A History\" (1995), have suggested that this is a sign our culture has entered a new post-modern age, when the former modernist ways of thinking and behaving have been rendered redundant.\n\nNevertheless, an incisive critique of vanguardism as against the views of mainstream society was offered by the New York critic Harold Rosenberg in the late 1960s. Trying to strike a balance between the insights of Renato Poggioli and the claims of Clement Greenberg, Rosenberg suggested that from the mid-1960s onward progressive culture ceased to fulfill its former adversarial role. Since then it has been flanked by what he called \"avant-garde ghosts to the one side, and a changing mass culture on the other\", both of which it interacts with to varying degrees. This has seen culture become, in his words, \"a profession one of whose aspects is the pretense of overthrowing it\".\n\nAvant-garde in music can refer to any form of music working within traditional structures while seeking to breach boundaries in some manner. The term is used loosely to describe the work of any musicians who radically depart from tradition altogether. By this definition, some avant-garde composers of the 20th century include Arnold Schoenberg, Charles Ives, Igor Stravinsky, Anton Webern, George Antheil (in his earliest works only), Alban Berg, Henry Cowell (in his earliest works), Philip Glass, Harry Partch, John Cage, Morton Feldman, Richard Strauss (in his earliest work), Karlheinz Stockhausen, Edgard Varèse, and Iannis Xenakis. Although most avant-garde composers have been men, this is not exclusively the case. Women avant-gardists include Pauline Oliveros, Diamanda Galás, Meredith Monk, and Laurie Anderson.\n\nThere is another definition of \"Avant-gardism\" that distinguishes it from \"modernism\": Peter Bürger, for example, says avant-gardism rejects the \"institution of art\" and challenges social and artistic values, and so necessarily involves political, social, and cultural factors. According to the composer and musicologist Larry Sitsky, modernist composers from the early 20th century who do not qualify as avant-gardists include Arnold Schoenberg, Anton Webern, and Igor Stravinsky; later modernist composers who do not fall into the category of avant-gardists include Elliott Carter, Milton Babbitt, György Ligeti, Witold Lutosławski, and Luciano Berio, since \"their modernism was not conceived for the purpose of goading an audience.\"\n\nWhereas the avant-garde has a significant history in 20th-century music, it is more pronounced in theatre and performance art, and often in conjunction with music and sound design innovations, as well as developments in visual media design. There are movements in theatre history that are characterized by their contributions to the avant-garde traditions in both the United States and Europe. Among these are Fluxus, Happenings, and Neo-Dada.\n\n\n\n"}
{"id": "20786524", "url": "https://en.wikipedia.org/wiki?curid=20786524", "title": "Bottle crate", "text": "Bottle crate\n\nA bottle crate or beverage crate is a container used for transport of beverage containers. In the present day they are usually made of plastic, but before the widespread use of plastic they tended to be made of wood or metal.\n\nBeverage crates began to be made of HDPE in the 1950s.\n\nSuch crates can be long-lasting. In the 1980s in Sweden, a take-back campaign was organized, when 25-bottle crates were replaced by the more ergonomic 20-bottle crates. Some of the crates returned for recycling had been in use since the 1960s.\n\nBecause manufacturers avoid lead-based and cadmium-based pigmentations, in response to legislation and public opinion, they have had to resort to other methods of colouring HDPE crates. In Japan, since the early 1970s, HDPE bottle crates have been pigmented with a variety of perylene, quinacridone, azo condensation, and Isoindoline pigments. Japanese manufacturers have modified these in order to control nucleating behaviour, and have improved weathering performance and impact-resistance properties by making the light-stabilisization systems more efficient.\n\n"}
{"id": "41144473", "url": "https://en.wikipedia.org/wiki?curid=41144473", "title": "Burnside category", "text": "Burnside category\n\nIn category theory and homotopy theory the Burnside category of a finite group \"G\" is a category whose objects are finite \"G\"-sets and whose morphisms are (equivalence classes of) spans of \"G\"-equivariant maps. It is a categorification of the Burnside ring of \"G\".\n\nLet \"G\" be a finite group (in fact everything will work verbatim for a profinite group). Then for any two finite \"G\"-sets \"X\" and \"Y\" we can define an equivalence relation among spans of \"G\"-sets of the form formula_1 where two spans formula_1 and formula_3are equivalent if and only if there is a \"G\"-equivariant bijection of \"U\" and \"W\" commuting with the projection maps to \"X\" and \"Y\". This set of equivalence classes form naturally a monoid under disjoint union; we indicate with formula_4 the group completion of that monoid. Taking pullbacks induces natural maps formula_5.\n\nFinally we can define the Burnside category \"A(G)\" of \"G\" as the category whose objects are finite \"G\"-sets and the morphisms spaces are the groups formula_4.\n\n\nIf \"C\" is an additive category, then a \"C\"-valued Mackey functor is an additive functor from \"A(G)\" to \"C\". Mackey functors are important in representation theory and stable equivariant homotopy theory.\n\n"}
{"id": "52872721", "url": "https://en.wikipedia.org/wiki?curid=52872721", "title": "Consequence argument", "text": "Consequence argument\n\nThe consequence argument is an argument against compatibilism popularised by Peter van Inwagen. The argument claims that if agents have no control over the facts of the past then the agent has no control of the consequences of those facts.\n\nThe Stanford Encyclopedia of Philosophy gives the following syllogism of the argument:\n\nOr in van Inwagan's own words, in \"An Essay on Free Will\":\n\nIf determinism is true, then our acts are the consequence of laws of nature and events in the remote past. But it's not up to us what went on before we were born, and neither is it up to us what the laws of nature are. Therefore, the consequences of these things (including our present acts) are not up to us. (p. 56)\n"}
{"id": "1338683", "url": "https://en.wikipedia.org/wiki?curid=1338683", "title": "Corecursion", "text": "Corecursion\n\nIn computer science, corecursion is a type of operation that is dual to recursion. Whereas recursion works analytically, starting on data further from a base case and breaking it down into smaller data and repeating until one reaches a base case, corecursion works synthetically, starting from a base case and building it up, iteratively producing data further removed from a base case. Put simply, corecursive algorithms use the data that they themselves produce, bit by bit, as they become available, and needed, to produce further bits of data. A similar but distinct concept is \"generative recursion\" which may lack a definite \"direction\" inherent in corecursion and recursion.\n\nWhere recursion allows programs to operate on arbitrarily complex data, so long as they can be reduced to simple data (base cases), corecursion allows programs to produce arbitrarily complex and potentially infinite data structures, such as streams, so long as it can be produced from simple data (base cases) in a sequence of \"finite\" steps. Where recursion may not terminate, never reaching a base state, corecursion starts from a base state, and thus produces subsequent steps deterministically, though it may proceed indefinitely (and thus not terminate under strict evaluation), or it may consume more than it produces and thus become non-\"productive\". Many functions that are traditionally analyzed as recursive can alternatively, and arguably more naturally, be interpreted as corecursive functions that are terminated at a given stage, for example recurrence relations such as the factorial.\n\nCorecursion can produce both finite and infinite data structures as results, and may employ self-referential data structures. Corecursion is often used in conjunction with lazy evaluation, to produce only a finite subset of a potentially infinite structure (rather than trying to produce an entire infinite structure at once). Corecursion is a particularly important concept in functional programming, where corecursion and codata allow total languages to work with infinite data structures.\n\nCorecursion can be understood by contrast with recursion, which is more familiar. While corecursion is primarily of interest in functional programming, it can be illustrated using imperative programming, which is done below using the generator facility in Python. In these examples local variables are used, and assigned values imperatively (destructively), though these are not necessary in corecursion in pure functional programming. In pure functional programming, rather than assigning to local variables, these computed values form an invariable sequence, and prior values are accessed by self-reference (later values in the sequence reference earlier values in the sequence to be computed). The assignments simply express this in the imperative paradigm and explicitly specify where the computations happen, which serves to clarify the exposition.\n\nA classic example of recursion is computing the factorial, which is defined recursively by \"0! := 1\" and \"n! := n × (n - 1)!\".\n\nTo \"recursively\" compute its result on a given input, a recursive function calls (a copy of) \"itself\" with a different (\"smaller\" in some way) input and uses the result of this call to construct its result. The recursive call does the same, unless the \"base case\" has been reached. Thus a call stack develops in the process. For example, to compute \"fac(3)\", this recursively calls in turn \"fac(2)\", \"fac(1)\", \"fac(0)\" (\"winding up\" the stack), at which point recursion terminates with \"fac(0) = 1\", and then the stack unwinds in reverse order and the results are calculated on the way back along the call stack to the initial call frame \"fac(3)\" that uses the result of \"fac(2) = 2\" to calculate the final result as \"3 × 2 = 3 × fac(2) =: fac(3)\" and finally return \"fac(3) = 6\". In this example a function returns a single value.\n\nThis stack unwinding can be explicated, defining the factorial \"corecursively\", as an iterator, where one \"starts\" with the case of formula_1, then from this starting value constructs factorial values for increasing numbers \"1, 2, 3...\" as in the above recursive definition with \"time arrow\" reversed, as it were, by reading it \"backwards\" as The corecursive algorithm thus defined produces a \"stream\" of \"all\" factorials. This may be concretely implemented as a generator. Symbolically, noting that computing next factorial value requires keeping track of both \"n\" and \"f\" (a previous factorial value), this can be represented as:\nor in Haskell, \n\nmeaning, \"starting from formula_3, on each step the next values are calculated as formula_4\". This is mathematically equivalent and almost identical to the recursive definition, but the formula_5 emphasizes that the factorial values are being built \"up\", going forwards from the starting case, rather than being computed after first going backwards, \"down\" to the base case, with a formula_6 decrement. Note also that the direct output of the corecursive function does not simply contain the factorial formula_7 values, but also includes for each value the auxiliary data of its index \"n\" in the sequence, so that any one specific result can be selected among them all, as and when needed.\n\nNote the connection with denotational semantics, where the denotations of recursive programs is built up corecursively in this way.\n\nIn Python, a recursive factorial function can be defined as:\n\nThis could then be called for example as codice_1 to compute \"5!\".\n\nA corresponding corecursive generator can be defined as:\n\nThis generates an infinite stream of factorials in order; a finite portion of it can be produced by:\n\nThis could then be called to produce the factorials up to \"5!\" via:\n\nIf we're only interested in a certain factorial, just the last value can be taken, or we can fuse the production and the access into one function,\n\nAs can be readily seen here, this is practically equivalent (just by substituting codice_2 for the only codice_3 there) to the accumulator argument technique for tail recursion, unwound into an explicit loop. Thus it can be said that the concept of corecursion is an explication of the embodiment of iterative computation processes by recursive definitions, where applicable.\n\nIn the same way, the Fibonacci sequence can be represented as:\nNote that because the Fibonacci sequence is a recurrence relation of order 2, the corecursive relation must track two successive terms, with the formula_9 corresponding to shift forward by one step, and the formula_10 corresponding to computing the next term. This can then be implemented as follows (using parallel assignment):\n\nIn Haskell, \n\nTree traversal via a depth-first approach is a classic example of recursion. Dually, breadth-first traversal can very naturally be implemented via corecursion.\n\nWithout using recursion or corecursion specifically, one may traverse a tree by starting at the root node, placing its child nodes in a data structure, then iterating by removing node after node from the data structure while placing each removed node's children back into that data structure. If the data structure is a stack (LIFO), this yields depth-first traversal, and if the data structure is a queue (FIFO), this yields breadth-first traversal.\n\nUsing recursion, a (post-order) depth-first traversal can be implemented by starting at the root node and recursively traversing each child subtree in turn (the subtree based at each child node) – the second child subtree does not start processing until the first child subtree is finished. Once a leaf node is reached or the children of a branch node have been exhausted, the node itself is visited (e.g., the value of the node itself is outputted). In this case, the call stack (of the recursive functions) acts as the stack that is iterated over.\n\nUsing corecursion, a breadth-first traversal can be implemented by starting at the root node, outputting its value, then breadth-first traversing the subtrees – i.e., passing on the \"whole list\" of subtrees to the next step (not a single subtree, as in the recursive approach) – at the next step outputting the value of all of their root nodes, then passing on their child subtrees, etc. In this case the generator function, indeed the output sequence itself, acts as the queue. As in the factorial example (above), where the auxiliary information of the index (which step one was at, \"n\") was pushed forward, in addition to the actual output of \"n\"!, in this case the auxiliary information of the remaining subtrees is pushed forward, in addition to the actual output. Symbolically:\nmeaning that at each step, one outputs the list of values of root nodes, then proceeds to the child subtrees. Generating just the node values from this sequence simply requires discarding the auxiliary child tree data, then flattening the list of lists (values are initially grouped by level (depth); flattening (ungrouping) yields a flat linear list). In Haskell, \nThese can be compared as follows. The recursive traversal handles a \"leaf node\" (at the \"bottom\") as the base case (when there are no children, just output the value), and \"analyzes\" a tree into subtrees, traversing each in turn, eventually resulting in just leaf nodes – actual leaf nodes, and branch nodes whose children have already been dealt with (cut off \"below\"). By contrast, the corecursive traversal handles a \"root node\" (at the \"top\") as the base case (given a node, first output the value), treats a tree as being \"synthesized\" of a root node and its children, then produces as auxiliary output a list of subtrees at each step, which are then the input for the next step – the child nodes of the original root are the root nodes at the next step, as their parents have already been dealt with (cut off \"above\"). Note also that in the recursive traversal there is a distinction between leaf nodes and branch nodes, while in the corecursive traversal there is no distinction, as each node is treated as the root node of the subtree it defines.\n\nNotably, given an infinite tree, the corecursive breadth-first traversal will traverse all nodes, just as for a finite tree, while the recursive depth-first traversal will go down one branch and not traverse all nodes, and indeed if traversing post-order, as in this example (or in-order), it will visit no nodes at all, because it never reaches a leaf. This shows the usefulness of corecursion rather than recursion for dealing with infinite data structures.\n\nIn Python, this can be implemented as follows.\nThe usual post-order depth-first traversal can be defined as:\n\nThis can then be called by codice_4 to print the values of the nodes of the tree in post-order depth-first order.\n\nThe breadth-first corecursive generator can be defined as:\n\nThis can then be called to print the values of the nodes of the tree in breadth-first order:\n\nInitial data types can be defined as being the least fixpoint (up to isomorphism) of some type equation; the isomorphism is then given by an initial algebra. Dually, final (or terminal) data types can be defined as being the greatest fixpoint of a type equation; the isomorphism is then given by a final coalgebra.\n\nIf the domain of discourse is the category of sets and total functions, then final data types may contain infinite, non-wellfounded values, whereas initial types do not. On the other hand, if the domain of discourse is the category of complete partial orders and continuous functions, which corresponds roughly to the Haskell programming language, then final types coincide with initial types, and the corresponding final coalgebra and initial algebra form an isomorphism.\n\nCorecursion is then a technique for recursively defining functions whose range (codomain) is a final data type, dual to the way that ordinary recursion recursively defines functions whose domain is an initial data type.\n\nThe discussion below provides several examples in Haskell that distinguish corecursion. Roughly speaking, if one were to port these definitions to the category of sets, they would still be corecursive. This informal usage is consistent with existing textbooks about Haskell. Also note that the examples used in this article predate the attempts to define corecursion and explain what it is.\n\nThe rule for \"primitive corecursion\" on codata is the dual to that for primitive recursion on data. Instead of descending on the argument by pattern-matching on its constructors (that \"were called up before\", somewhere, so we receive a ready-made datum and get at its constituent sub-parts, i.e. \"fields\"), we ascend on the result by filling-in its \"destructors\" (or \"observers\", that \"will be called afterwards\", somewhere - so we're actually calling a constructor, creating another bit of the result to be observed later on). Thus corecursion \"creates\" (potentially infinite) codata, whereas ordinary recursion \"analyses\" (necessarily finite) data. Ordinary recursion might not be applicable to the codata because it might not terminate. Conversely, corecursion is not strictly necessary if the result type is data, because data must be finite.\n\nIn \"Programming with streams in Coq: a case study: the Sieve of Eratosthenes\" we find\n\nwhere primes \"are obtained by applying the primes operation to the stream (Enu 2)\". Following the above notation, the sequence of primes (with a throwaway 0 prefixed to it) and numbers streams being progressively sieved, can be represented as \nor in Haskell, \n\nThe authors discuss how the definition of codice_5 is not guaranteed always to be \"productive\", and could become stuck e.g. if called with codice_6 as the initial stream.\n\nHere is another example in Haskell. The following definition produces the list of Fibonacci numbers in linear time:\nThis infinite list depends on lazy evaluation; elements are computed on an as-needed basis, and only finite prefixes are ever explicitly represented in memory. This feature allows algorithms on parts of codata to terminate; such techniques are an important part of Haskell programming.\n\nThis can be done in Python as well:\nThe definition of codice_7 can be inlined, leading to this:\n\nThis example employs a self-referential \"data structure\". Ordinary recursion makes use of self-referential \"functions\", but does not accommodate self-referential data. However, this is not essential to the Fibonacci example. It can be rewritten as follows:\n\nThis employs only self-referential \"function\" to construct the result. If it were used with strict list constructor it would be an example of runaway recursion, but with non-strict list constructor this guarded recursion gradually produces an indefinitely defined list.\n\nCorecursion need not produce an infinite object; a corecursive queue is a particularly good example of this phenomenon. The following definition produces a breadth-first traversal of a binary tree in linear time:\n\nThis definition takes an initial tree and produces a list of subtrees. This list serves dual purpose as both the queue and the result ( produces its output notches after its input back-pointer, , along the ). It is finite if and only if the initial tree is finite. The length of the queue must be explicitly tracked in order to ensure termination; this can safely be elided if this definition is applied only to infinite trees. \n\nAnother particularly good example gives a solution to the problem of breadth-first labeling. The function codice_8 visits every node in a binary tree in a breadth first fashion, and replaces each label with an integer, each subsequent integer is bigger than the last by one. This solution employs a self-referential data structure, and the binary tree can be finite or infinite.\n\nAn apomorphism (such as an anamorphism, such as unfold) is a form of corecursion in the same way that a paramorphism (such as a catamorphism, such as fold) is a form of recursion.\n\nThe Coq proof assistant supports corecursion and coinduction using the CoFixpoint command.\n\nCorecursion, referred to as \"circular programming,\" dates at least to , who credits John Hughes and Philip Wadler; more general forms were developed in . The original motivations included producing more efficient algorithms (allowing 1 pass over data in some cases, instead of requiring multiple passes) and implementing classical data structures, such as doubly linked lists and queues, in functional languages.\n\n\n"}
{"id": "52827956", "url": "https://en.wikipedia.org/wiki?curid=52827956", "title": "Critical Practice (art)", "text": "Critical Practice (art)\n\nCritical Practice is a discipline of art that places an equal emphasis on theory and practice, adopting an invigorated methodology that considers and interrogates the processes of art making, its changing contexts and the ways in which it engages an audience. In addition to the various forces that are implicated in the making of art, the research elements pursued under the auspices of Critical Practice engage the increasingly devolved experience of art, made available through art institutions to their audiences.\n\nCritical Practice work takes a range of forms from traditional wall-based work (collage, drawing, photography) through performance and video, from relational and socially engaged practices to site-specific installations.\n\nStudies in Critical Practice are offered at several universities, including the Royal College of Art, University of the Arts London, University of Brighton, and Yale University.\n"}
{"id": "26711499", "url": "https://en.wikipedia.org/wiki?curid=26711499", "title": "Declaration on Crimes of Communism", "text": "Declaration on Crimes of Communism\n\nThe Declaration on Crimes of Communism is a declaration signed on 25 February 2010 by several prominent European politicians, former political prisoners, human rights advocates and historians, which calls for the condemnation of communism. \nIt concluded the international conference Crimes of the Communist Regimes, that took place at the Czech Senate and the Office of the Government of the Czech Republic from 24 to 26 February 2010. The declaration reiterated many of the suggestions set forth by the Prague Declaration on European Conscience and Communism.\nThe conference was hosted by Jiří Liška, Vice President of the Czech Senate, and the Office of the Government of the Czech Republic, and organized by the Institute for the Study of Totalitarian Regimes under the patronage of Jan Fischer, Prime Minister of the Czech Republic, Heidi Hautala, Chairwoman of the Human Rights Subcommittee of the European Parliament, Göran Lindblad, Vice President of the Parliamentary Assembly of the Council of Europe, among others. The cooperation partners included the Konrad Adenauer Foundation, the Information Office of the European Parliament, the Representation of the European Commission in the Czech Republic, the Robert Schuman Foundation of the European People's Party, the Polish Institute in Prague, the Federal Commissioner for the Stasi Records and the National Endowment for Democracy.\n\nThe declaration both called for condemnation of communism, education about communist crimes, prosecution of communist criminals by establishing an international court within the EU for communist crimes, construction of a memorial to the victims of world communism, and reduction of pensions and social security benefits for communist perpetrators. The declaration stated that:\nThe signatories included:\n\n"}
{"id": "20539972", "url": "https://en.wikipedia.org/wiki?curid=20539972", "title": "Dyschronometria", "text": "Dyschronometria\n\nDyschronometria is a condition of cerebellar dysfunction in which an individual cannot accurately estimate the amount of time that has passed (i.e., distorted time perception). It is associated with cerebellar ataxia, when the cerebellum has been damaged and does not function to its fullest ability. Lesions to the cerebellum can cause dyssynergia, dysmetria, dysdiadochokinesia, dysarthria, and ataxia of stance and gait. Dyschronometria can result from autosomal dominant cerebellar ataxia (ADCA).\n\nCommon signs of dyschronometria are often generic to cerebellar ataxia, including a lack of spatial awareness, poor short term memory, and inability to keep track of time. The defining symptoms, while not completely understood, involve time perception. For example, when asked to wait for thirty seconds, or tap every second that has gone by, those affected will be able to perform the task for a short time and then become derailed. This can result from a loss of focus, however, more often than not the individual affected can no longer tell what they are doing and they become disoriented. This often takes form in forgetting basic time keeping unless a timer is set, such as when cooking for example. Dyschronometria does not affect the 24-hour circadian rhythm, which is sustained by a different biological process.\nThe most common cause of cerebellar ataxia, and by extension dyschronometria, is cerebellar damage. This can be by form of a trauma, or by disease and genetics. Examples of trauma include a car accident, stroke, epilepsy, and head trauma. These traumas are especially detrimental to elderly and children due to the decrease in brain matter, thus making these events if taken place have an increased risk of damaging the cerebellum. This also explains why dyschronometria is seen more commonly in the elderly due to the deterioration of physical brain matter. Other probable causes for the deterioration of brain matter in the elderly include increased supranational activation, decreased cerebellar activation (which is consistent with fronto-cerebellar dissociation).\n\nAn interesting case of dyschronometria has to do with dyslexia. When dyslexia was studied within children, it was found that dyslexic children were often stressed as well as mentally exhausted. These children would place little to no importance on their present state, a behavior that would continue into adulthood. It remains unclear as to whether dyslexia is a symptom of dyschronometria, a cause, or both.\n\nDementia has a huge effect on dyschronometria and was one of the main sources of how dyschronometria was discovered. Through studies, dementia is both a cause and an effect of dyschronometria. This has to do completely with the fact that with dementia the brain is constantly rewiring itself and thus information becomes lost causing the person who had dementia to become confused as well as disoriented, and in most cases completely unaware of the passage of time. As a cause, dyschronometria causes the person to become disoriented and completely unaware of time, thus making bits and pieces of their brain involving a memory to become lost, ultimately leading to dementia in the long term. This is how dyschronometria is a precursor to dementia.\n\nDespite dyschronometria's easily recognizable symptoms, the fact that they may also be present in other cerebellar ataxias can make diagnosis difficult. Other ataxias may also have symptoms that affect gait, speech, thought process, spatial awareness, and time orientation used in their diagnoses, covering up the fact that most of these patients also have dyschronometria. The most common ataxias dyschronometria has been seen to be evident in are dyssynergia, dysmetria, dysdiadochokinesia, dysarthria as well as ataxias effecting stance and gait. Dyslexia can be another problem in those individuals affected by dyschronometria, however it is uncertain whether dyslexia is developed or worsened by having it, or if it is the opposite in that having dyslexia increases the chance of developing dyschronometria. Another problem that dyschronometria faces in detection is that it is a relatively new term for this side effect and precursor to dementia, compared with other cerebellar ataxias such as those mentioned above. Even when dyschronometria is detected, it has usually progressed to the point where it cannot be reversed, and there is no benefit in taking the testing medication to either slow down the dyschronometria or the process of dementia setting in, which is what dyschronometria is a signal for.\n\nHowever the greatest error in diagnosing dyschronometria is that this cerebellar ataxia hides itself in its symptoms and signs. The signs seen in those diagnosed with dyschronometria are not obvious, and are often mistaken for other cerebellar ataxias or dementia by medical professionals. In addition, medical professionals usually expect to see circadian rhythm being disrupted by noting sleeping cycles and patterns that have no logical sense to them, which has nothing to do with dyschronometria. Other errors in diagnosing dyschronometria include the idea that those who have dyschronometria have a speech impediment, suffer from delusions bordering psychosis, impairment of long term memory, or the complete loss of conscious understanding of time. These misconceptions mostly stem from the fact that this cerebellar ataxia is rarely diagnosed without being seen in dementia or with another ataxia.\n\nTesting and diagnosing for dyschronometria also been shown to be ineffective. Dementia is caught so late despite the signals seeming obvious because psychological tests that try to catch signs of dementia, such as dyschronometria, are not very helpful. With these tests, the bell curves formed after statistical analysis and a wide range among normal test takers ensure that only patients that have extreme abnormal tests, test positive, and these cases are often already obvious to diagnose. Diagnosing of dyschronometria is also difficult due to the lack of research as well as professionals concentrating in this cerebellar ataxia. Neuroscientists are only just starting to do more research into this lack of awareness and keeping time. When the science and the tests are more specialized and this topic has been looked into at a greater depth, the sensitivity of the tests that have been and will be conducted will be able to give more answer to the questions that are continuing to arise.\n\nIt has not been determined what role drugs may play in the treatment of cerebellar ataxia. In the research done by Trouillas in Lyon, France, the pharmacology of cerebellar ataxia was examined by manipulating key components found at the nerve level within the cerebellum or the inferior Olive. This was done mostly through the modification of the GABA, dopamine, and serotonin receptors which did seem to show positive results in the primary stages of the experimentation. The clinical benefits presented in this study justifying the prescription of d-l-5-HTP or better with the l-5-HTP with benserazide to patients with certain cerebellar ataxias including that of Dyschronometria. At the present as stated, this is the best indication for treatment of the cerebellar cortical atrophies. Even still it is important to stress that the response to this treatment may be slow and irregular.\n\nPreviously, neuroplasticity used as a rehabilitation method were looked into as a potential treatment for dyschronometria. However these studies were not further developed since the 1980s. With current techniques and research from the neuroscience community, this is still a viable option not to eliminate the cerebellar ataxia, but to slow its progress of development.\n\n"}
{"id": "18815946", "url": "https://en.wikipedia.org/wiki?curid=18815946", "title": "EIOLCA", "text": "EIOLCA\n\nAn economic input-output life-cycle assessment, or EIO-LCA involves the use of aggregate sector-level data to quantify the amount of environmental impact that can be directly attributed to each sector of the economy and how much each sector purchases from other sectors in producing its output. Combining such data sets can enable accounting for long chains (for example, building an automobile requires energy, but producing energy requires vehicles, and building those vehicles requires energy, etc.), which somewhat alleviates the scoping problem of traditional Life-cycle assessments. EIO-LCA analysis traces out the various economic transactions, resource requirements and environmental emissions (including all the various manufacturing, transportation, mining and related requirements) required for producing a particular product or service.\n\nEIO-LCA relies on sector-level averages that may or may not be representative of the specific subset of the sector relevant to a particular product. To the extent that the good or service of interest is representative of a sector, EIOLCA can provide very fast estimates of full supply chain implications for that good or service.\n\nEconomic input-output analysis was developed by the Nobel Prize-winning economist Wassily Leontief. It quantifies the interrelationships among sectors of an economic system, enabling identification of direct and indirect economic inputs of purchases. This concept was extended by including data about environmental and energy analysis from each sector to account for supply chain environmental implications of economic activity.\n\nInput-output transactions tables, which track flows of purchases between sectors, are collected by the federal government in the United States. EIO works as follows: If formula_1 represents the amount that sector formula_2 purchased from sector formula_3 in a given year and formula_4 is the \"final demand\" for output from sector formula_3 (i.e., the amount of output purchased for consumption, as opposed to purchased by other businesses as supplies for more production), then the total output formula_6 from sector formula_3 includes output to consumers plus output sold to other sectors:\n\nformula_8\n\nIf we define formula_9 as the normalized production for each sector, so that formula_10, then\n\nformula_11\n\nIn vector notation\n\nformula_12\n\nformula_13\n\nformula_14\n\nThis result indicates that knowing only the final demand from each sector formula_15 and the normalized IO matrix formula_16, one can calculate the total implied production formula_17 from each sector of the economy. If data are available on a particular emissions release (or other attribute of interest) from each sector of the economy, then a matrix formula_18 can be compiled to represent various releases (columns) per $ output from each sector (rows). Total additional emissions formula_19 associated with additional final demand of formula_20 can then be calculated as:\n\nformula_21\n\nThis simple result enables very quick analysis, taking into account releases associated with the entire supply chain requirements needed to provide a specific final demand, on average. The equations are based on average data in the current economy, but they can be used to make predictions for marginal changes in output (such as one more unit of a particular product) if \n\nFinally, if the researcher has estimates for valuation of externality costs associated with each item in formula_23 (or, alternatively, if weighting coefficients are available that represent the relative importance of each item in formula_23, using ecological indicators, for example) then the externality costs (or weights) per unit of releases could be compiled into a vector formula_25 in order to calculate the scalar \"environmental impact metric\" formula_26:\n\nformula_27\n\nGenerally there is wide uncertainty associated with estimates of formula_25, so such aggregation should be done only with care, including sensitivity analysis. Typically, researchers examine specific elements of formula_23 rather than attempting to aggregate.\n\nThe big picture result is that by collecting data on average economic sector transactions formula_16 and average sector emissions formula_18, it is possible to make quick predictions about the full supply chain emissions associated with a product of interest by representing the product as marginal changes in production from relevant sectors formula_15.\n\nResearchers at the Green Design Institute of Carnegie Mellon University began developing a web-based tool for performing an EIO-LCA in the 1990s. The underlying software traces out the various economic transactions, resource requirements and environmental emissions associated with the production of a particular product or service. The model captures all the various manufacturing, transportation, mining and related requirements to produce a product or service. For example, one might wish to trace out the implications of purchasing $ 46,000 of reinforcing steel and $ 104,000 of concrete for a kilometer of roadway pavement. Environmental implications of these purchases can be estimated using EIO-LCA. The current (2002) model is based upon the Department of Commerce's 428 sector industry input-output model of the US economy.\n\nIn 2018, VitalMetrics Group, an environmental consultancy, developed a web-based Spend Analysis Tool for quantifying the environmental impacts associated with an organization’s entire upstream supply chain. It is compliant with the approach for quantifying spend-based impacts defined in the Greenhouse Gas Protocol Corporate Value Chain Accounting and Reporting Standard. The tool utilizes the Comprehensive Environmental Data Archive (CEDA), a peer-reviewed EIO-LCA database with a base year of 2014. CEDA represents 389 industrial sectors, the commodities and the linkages between them, and over 2,700 environmental exchanges arising from them, including extraction of various natural resources, water consumption, land use, and emissions to air, water and soil. \n\n\"This article uses text from Design Decisions Wiki under the GFDL.\"\n\n"}
{"id": "487949", "url": "https://en.wikipedia.org/wiki?curid=487949", "title": "Fear conditioning", "text": "Fear conditioning\n\nFear conditioning is a behavioral paradigm in which organisms learn to predict aversive events. It is a form of learning in which an aversive stimulus (e.g. an electrical shock) is associated with a particular neutral context (e.g., a room) or neutral stimulus (e.g., a tone), resulting in the expression of fear responses to the originally neutral stimulus or context. This can be done by pairing the neutral stimulus with an aversive stimulus (e.g., a shock, loud noise, or unpleasant odor). Eventually, the neutral stimulus alone can elicit the state of fear. In the vocabulary of classical conditioning, the neutral stimulus or context is the \"conditional stimulus\" (CS), the aversive stimulus is the \"unconditional stimulus\" (US), and the fear is the \"conditional response\" (CR).\n\nFear conditioning has been studied in numerous species, from snails to humans. In humans, conditioned fear is often measured with verbal report and galvanic skin response. In other animals, conditioned fear is often measured with freezing (a period of watchful immobility) or fear potentiated startle (the augmentation of the startle reflex by a fearful stimulus). Changes in heart rate, breathing, and muscle responses via electromyography can also be used to measure conditioned fear. A number of theorists have argued that conditioned fear coincides substantially with the mechanisms, both functional and neural, of clinical anxiety disorders. Research into the acquisition, consolidation and extinction of conditioned fear promises to inform new drug based and psychotherapeutic treatments for an array of pathological conditions such as dissociation, phobias and post-traumatic stress disorder.\n\nFear conditioning is thought to depend upon an area of the brain called the amygdala. Electrophysiological recordings from the amygdala have demonstrated that cells in that region undergo long-term potentiation (LTP), a form of synaptic plasticity believed to underlie learning. Additionally, inhibition of neurons in the amygdala disrupts fear acquisition, while stimulation of those neurons can drive fear-related behaviors, such as freezing behavior in rodents. This indicates that proper function of the amygdala is both necessary for fear conditioning and sufficient to drive fear behaviors.\n\nJoseph E. LeDoux has been instrumental in elucidating the amygdala's role in fear conditioning. He was one of the first to show that the amygdala undergoes long-term potentiation during fear conditioning, and that ablation of amygdala cells disrupts both learning and expression of fear, among many other achievements.\n\nSome types of fear conditioning (e.g. contextual and trace) also involve the hippocampus, an area of the brain believed to receive affective impulses from the amygdala and to integrate those impulses with previously existing information to make it meaningful. Some theoretical accounts of traumatic experiences suggest that amygdala-based fear bypasses the hippocampus during intense stress and can be stored somatically or as images that can return as physical symptoms or flashbacks without cognitive meaning.\n\nOne of the major neurotransmitters involved in conditioned fear learning is glutamate. It has been suggested that NMDA receptors (NMDARs) in the amygdala are necessary for fear memory acquisition, because disruption of NMDAR function disrupts development of fear responses in rodents.\n\nConditioned fear may be inherited transgenerationally. In one experiment, mice were conditioned to fear an acetophenone odor and then set up to breed subsequent generations of mice. Those subsequent generations of mice also showed a behavioral sensitivity to acetophenone, which was accompanied by neuroanatomical and epigenetic changes that are believed to have been inherited from the parents' gametes.\n\nThe learning involved in conditioned fear, as well as the underlying neurobiology, changes dramatically from infancy, across childhood and adolescence, into adulthood and aging. Specifically, infant animals show an inability to develop fear associations, whereas their adult counterparts develop fear memories much more readily.\n\n"}
{"id": "10438439", "url": "https://en.wikipedia.org/wiki?curid=10438439", "title": "Framing (social sciences)", "text": "Framing (social sciences)\n\nIn the social sciences, framing comprises a set of concepts and theoretical perspectives on how individuals, groups, and societies, organize, perceive, and communicate about reality. Framing involves social construction of a social phenomenon – by mass media sources, political or social movements, political leaders, or other actors and organizations. Participation in a language community necessarily influences an individual's \"perception\" of the meanings attributed to words or phrases. Politically, the language communities of advertising, religion, and mass media are highly contested, whereas framing in less-sharply defended language communities might evolve imperceptibly and organically over cultural time frames, with fewer overt modes of disputation.\n\nFraming can manifest in thought or interpersonal communication. \"Frames in thought\" consist of the mental representations, interpretations, and simplifications of reality. \"Frames in communication\" consist of the communication of frames between different actors.\n\nOne can view framing in communication as positive or negative – depending on the audience and what kind of information is being presented. The framing may be in the form of \"equivalence frames\", where two or more logically equivalent alternatives are portrayed in different ways (see framing effect) or \"emphasis frames\", which simplify reality by focusing on a subset of relevant aspects of a situation or issue. In the case of \"equivalence frames\", the information being presented is based on the same facts, but the \"frame\" in which it is presented changes, thus creating a reference-dependent perception.\n\nThe effects of framing can be seen in many journalism applications. With the same information being used as a base, the \"frame\" surrounding the issue can change the reader's perception without having to alter the actual facts. In the context of politics or mass-media communication, a frame defines the packaging of an element of rhetoric in such a way as to encourage certain interpretations and to discourage others. For political purposes, framing often presents facts in such a way that implicates a problem that is in need of a solution. Members of political parties attempt to frame issues in a way that makes a solution favoring their own political leaning appear as the most appropriate course of action for the situation at hand.\n\nIn social theory, framing is a schema of interpretation, a collection of anecdotes and stereotypes, that individuals rely on to understand and respond to events. In other words, people build a series of mental \"filters\" through biological and cultural influences. They then use these filters to make sense of the world. The choices they then make are influenced by their creation of a frame.\n\nFraming is also a key component of sociology, the study of social interaction among humans. Framing is an integral part of conveying and processing data on a daily basis. Successful framing techniques can be used to reduce the ambiguity of intangible topics by contextualizing the information in such a way that recipients can connect to what they already know.\n\nWhen one seeks to explain an event, the understanding often depends on the frame referred to. If a friend rapidly closes and opens an eye, we will respond very differently depending on whether we attribute this to a purely \"physical\" frame (they blinked) or to a social frame (they winked). Though the former might result from a speck of dust (resulting in an involuntary and not particularly meaningful reaction), the latter would imply a voluntary and meaningful action (to convey humor to an accomplice, for example). \n\nObservers will read events seen as purely physical or within a frame of \"nature\" differently from those seen as occurring with social frames. But we do not look at an event and then \"apply\" a frame to it. Rather, individuals constantly project into the world around them the interpretive frames that allow them to make sense of it; we only shift frames (or realize that we have habitually applied a frame) when incongruity calls for a frame-shift. In other words, we only become aware of the frames that we always already use when something forces us to replace one frame with another.\n\nFraming is so effective because it is an heuristic, or mental shortcut that may not always yield desired results; and is seen as a 'rule of thumb'. According to Susan T. Fiske and Shelley E. Taylor, human beings are by nature \"cognitive misers\", meaning they prefer to do as little thinking as possible. Frames provide people a quick and easy way to process information. Hence, people will use the previously mentioned mental filters (a series of which is called a schema) to make sense of incoming messages. This gives the sender and framer of the information enormous power to use these schemas to influence how the receivers will interpret the message.\n\nThough some consider framing to be synonymous with agenda setting, other scholars state that there is a distinction. According to an article written by Donald H. Weaver, framing selects certain aspects of an issue and makes them more prominent in order to elicit certain interpretations and evaluations of the issue, whereas agenda setting introduces the issue topic to increase its salience and accessibility.\n\nIn the field of communication, framing defines how news media coverage shapes mass opinion. Richard E. Vatz's discourse on creation of rhetorical meaning relates directly to framing, although he references it little. To be specific, framing effects refer to behavioral or attitudinal strategies and/or outcomes that are due to how a given piece of information is being framed in public discourse. Today, many volumes of the major communication journals contain papers on media frames and framing effects. Approaches used in such papers can be broadly classified into two groups: studies of framing as the dependent variable and studies of framing as the independent variable. The former usually deals with \"frame building\" (i.e. how frames create societal discourse about an issue and how different frames are adopted by journalists) and latter concerns \"frame setting\" (i.e. how media framing influences an audience).\n\nFrame building is related to at least three areas: journalist norms, political actors, and cultural situations. It assumes that several media frames compete to set one frame regarding an issue, and one frame finally gains influence because it resonates with popular culture, fits with media practices, or is heavily sponsored by elites.\nFirst, in terms of practices of news production, there are at least five aspects of news work that may influence how journalists frame a certain issue: larger societal norms and values, organizational pressures and constraints, external pressures from interest groups and other policy makers, professional routines, and ideological or political orientations of journalists. The second potential influence on frame building comes from elites, including interest groups, government bureaucracies, and other political or corporate actors. Empirical studies show that these influences of elites seem to be strongest for issues in which journalists and various players in the policy arena can find shared narratives. Finally, cultural contexts of a society are also able to establish frame. Erving Goffman assumes that the meaning of a frame has implicit cultural roots. This context dependency of media frame has been described as 'cultural resonance' or 'narrative fidelity'.\n\nWhen people are exposed to a novel news frame, they will accept the constructs made applicable to an issue, but they are significantly more likely to do so when they have existing schema for those constructs. This is called the applicability effect. That is, when new frames invite people to apply their existing schema to an issue, the implication of that application depends, in part, on what is in that schema. Therefore, generally, the more the audiences know about issues, the more effective are frames.\n\nThere are a number of levels and types of framing effects that have been examined. For example, scholars have focused on attitudinal and behavioral changes, the degrees of perceived importance of the issue, voting decisions, and opinion formations. Others are interested in psychological processes other than applicability. For instance, Iyengar suggested that news about social problems can influence attributions of causal and treatment responsibility, an effect observed in both cognitive responses and evaluations of political leaders, or other scholars looked at the framing effects on receivers' evaluative processing style and the complexity of audience members' thoughts about issues.\n\nNews media frame all news items by emphasizing specific values, facts, and other considerations, and endowing them with greater apparent applicability for making related judgments. News media promotes particular definitions, interpretations, evaluations and recommendations.\n\nAnthropologist Gregory Bateson first articulated the concept of framing in his 1972 book \"Steps to an Ecology of Mind\". A frame, Bateson wrote, is \"a spatial and temporal bounding of a set of interactive messages.\"\n\nMedia framing research has both sociological and psychological roots. Sociological framing focuses on \"the words, images, phrases, and presentation styles\" that communicators use when relaying information to recipients. Research on frames in sociologically driven media research generally examines the influence of \"social norms and values, organizational pressures and constraints, pressures of interest groups, journalistic routines, and ideological or political orientations of journalists\" on the existence of frames in media content.\n\nTodd Gitlin, in his analysis of how the news media trivialized the student New Left movement during the 1960s, was among the first to examine media frames from a sociological perspective. Frames, Gitlin wrote, are \"persistent patterns of cognition, interpretations, and presentation, of selection [and] emphasis ... [that are] largely unspoken and unacknowledged ... [and] organize the world for both journalists [and] for those of us who read their reports.\"\n\nResearch on frames in psychologically driven media research generally examines the effects of media frames on those who receive them. For example, Iyengar explored the impact of episodic and thematic news frames on viewers' attributions of responsibility for political issues including crime, terrorism, poverty, unemployment, and racial inequality. According to Iyengar, an episodic news frame \"takes the form of a case study or event-oriented report and depicts public issues in terms of concrete instances,\" while a thematic news frame \"places public issues in some more general abstract context ... directed at general outcomes or conditions.\" Iyengar found that the majority of television news coverage of poverty, for example, was episodic. In fact, in a content analysis of six years of television news, Iyengar found that the typical news viewer would have been twice as likely to encounter episodic rather than thematic television news about poverty. Further, experimental results indicate participants who watched episodic news coverage of poverty were more than twice as likely as those who watched thematic news coverage of poverty to attribute responsibility of poverty to the poor themselves rather than society. Given the predominance of episodic framing of poverty, Iyengar argues that television news shifts responsibility of poverty from government and society to the poor themselves. After examining content analysis and experimental data on poverty and other political issues, Iyengar concludes that episodic news frames divert citizens' attributions of political responsibility away from society and political elites, making them less likely to support government efforts to address those issue and obscuring the connections between those issues and their elected officials' actions or lack thereof.\n\nPerhaps because of their use across the social sciences, frames have been defined and used in many disparate ways. Entman called framing \"a scattered conceptualization\" and \"a fractured paradigm\" that \"is often defined casually, with much left to an assumed tacit understanding of the reader.\" In an effort to provide more conceptual clarity, Entman suggested that frames \"select some aspects of a perceived reality and make them more salient in a communicating text, in such a way as to promote a particular problem definition, causal interpretation, moral evaluation, and/or treatment recommendation for the item described.\"\n\nEntman's conceptualization of framing, which suggests frames work by elevating particular pieces of information in salience, is in line with much early research on the psychological underpinnings of framing effects (see also Iyengar, who argues that accessibility is the primary psychological explanation for the existence of framing effects). Wyer and Srull explain the construct of accessibility thus:\n\nThe argument supporting accessibility as the psychological process underlying framing can therefore be summarized thus: Because people rely heavily on news media for public affairs information, the most accessible information about public affairs often comes from the public affairs news they consume. The argument supporting accessibility as the psychological process underlying framing has also been cited as support in the debate over whether framing should be subsumed by agenda-setting theory as part of the second level of agenda setting. McCombs and other agenda-setting scholars generally agree that framing should be incorporated, along with priming, under the umbrella of agenda setting as a complex model of media effects linking media production, content, and audience effects. Indeed, McCombs, Llamas, Lopez-Escobar, and Rey justified their attempt to combine framing and agenda-setting research on the assumption of parsimony.\n\nScheufele, however, argues that, unlike agenda setting and priming, framing does not rely primarily on accessibility, making it inappropriate to combine framing with agenda setting and priming for the sake of parsimony. Empirical evidence seems to vindicate Scheufele's claim. For example, Nelson, Clawson, and Oxley empirically demonstrated that applicability, rather than their salience, is key. By operationalizing accessibility as the response latency of respondent answers where more accessible information results in faster response times, Nelson, Clawson, and Oxley demonstrated that accessibility accounted for only a minor proportion of the variance in framing effects while applicability accounted for the major proportion of variance. Therefore, according to Nelson and colleagues, \"frames influence opinions by stressing specific values, facts, and other considerations, endowing them with greater apparent relevance to the issue than they might appear to have under an alternative frame.\"\n\nIn other words, while early research suggested that by highlighting particular aspects of issues, frames make certain considerations more accessible and therefore more likely to be used in the judgment process, more recent research suggests that frames work by making particular considerations more applicable and therefore more relevant to the judgment process.\n\nChong and Druckman suggest framing research has mainly focused on two types of frames: equivalency and emphasis frames. Equivalency frames offer \"different, but logically equivalent phrases,\" which cause individuals to alter their preferences. Equivalency frames are often worded in terms of \"gains\" versus \"losses.\" For example, Kahneman and Tversky asked participants to choose between two \"gain-framed\" policy responses to a hypothetical disease outbreak expected to kill 600 people. Response A would save 200 people while Response B had a one-third probability of saving everyone, but a two-thirds probability of saving no one. Participants overwhelmingly chose Response A, which they perceived as the less risky option. Kahneman and Tversky asked other participants to choose between two equivalent \"loss-framed\" policy responses to the same disease outbreak. In this condition, Response A would kill 400 people while Response B had a one-third probability of killing no one but a two-thirds probability of killing everyone. Although these options are mathematically identical to those given in the \"gain-framed\" condition, participants overwhelmingly chose Response B, the risky option. Kahneman and Tversky, then, demonstrated that when phrased in terms of potential gains, people tend to choose what they perceive as the less risky option (i.e., the sure gain). Conversely, when faced with a potential loss, people tend to choose the riskier option.\n\nUnlike equivalency frames, emphasis frames offer \"qualitatively different yet potentially relevant considerations\" which individuals use to make judgments. For example, Nelson, Clawson, and Oxley exposed participants to a news story that presented the Ku Klux Klan's plan to hold a rally. Participants in one condition read a news story that framed the issue in terms of public safety concerns while participants in the other condition read a news story that framed the issue in terms of free speech considerations. Participants exposed to the public safety condition considered public safety applicable for deciding whether the Klan should be allowed to hold a rally and, as expected, expressed lower tolerance of the Klan's right to hold a rally. Participants exposed to the free speech condition, however, considered free speech applicable for deciding whether the Klan should be allowed to hold a rally and, as expected, expressed greater tolerance of the Klan's right to hold a rally.\n\nAmos Tversky and Daniel Kahneman have shown that framing can affect the outcome (i.e. the choices one makes) of choice problems, to the extent that several of the classic axioms of rational choice do not hold. This led to the development of prospect theory as an alternative to rational choice theory.\n\nThe context or framing of problems adopted by decision-makers results in part from extrinsic manipulation of the decision-options offered, as well as from forces intrinsic to decision-makers, e.g., their norms, habits, and unique temperament.\n\nTversky and Kahneman (1981) demonstrated systematic reversals of preference when the same problem is presented in different ways, for example in the Asian disease problem. Participants were asked to \"imagine that the U.S. is preparing for the outbreak of an unusual Asian disease, which is expected to kill 600 people. Two alternative programs to combat the disease have been proposed. Assume the exact scientific estimate of the consequences of the programs are as follows.\"\n\nThe first group of participants was presented with a choice between programs:\nIn a group of 600 people,\n\n72 percent of participants preferred program A (the remainder, 28%, opting for program B).\n\nThe second group of participants was presented with the choice between the following:\nIn a group of 600 people,\n\nIn this decision frame, 78% preferred program D, with the remaining 22% opting for program C.\n\nPrograms A and C are identical, as are programs B and D. The change in the decision frame between the two groups of participants produced a preference reversal: when the programs were presented in terms of lives saved, the participants preferred the secure program, A (= C). When the programs were presented in terms of expected deaths, participants chose the gamble D (= B).\n\nFraming effects arise because one can frequently frame a decision using multiple scenarios, wherein one may express benefits either as a relative risk reduction (RRR), or as absolute risk reduction (ARR). Extrinsic control over the cognitive distinctions (between risk tolerance and reward anticipation) adopted by decision makers can occur through altering the presentation of relative risks and absolute benefits.\n\nPeople generally prefer the absolute certainty inherent in a positive framing-effect, which offers an assurance of gains. When decision-options appear framed as a \"likely gain\", risk-averse choices predominate.\n\nA shift toward risk-seeking behavior occurs when a decision-maker frames decisions in negative terms, or adopts a negative framing effect.\n\nIn medical decision making, framing bias is best avoided by using absolute measures of efficacy.\n\nResearchers have found that framing decision-problems in a positive light generally results in less-risky choices; with negative framing of problems, riskier choices tend to result.\n\nIn a study by researchers at Dartmouth Medical School, 57% of the subjects chose a medication when presented with benefits in relative terms, whereas only 14.7% chose a medication whose benefit appeared in absolute terms. Further questioning of the patients suggested that, because the subjects ignored the underlying risk of disease, they perceived benefits as greater when expressed in relative terms.\n\nResearchers have proposed various models explaining the framing effect:\n\nCognitive neuroscientists have linked the framing-effect to neural activity in the amygdala, and have identified another brain-region, the orbital and medial prefrontal cortex (OMPFC), that appears to moderate the role of emotion on decisions. Using functional magnetic resonance imaging (fMRI) to monitor brain-activity during a financial decision-making task, they observed greater activity in the OMPFC of those research subjects less susceptible to the framing-effect.\n\nFraming theory and frame analysis provide a broad theoretical approach that analysts have used in communication studies, news (Johnson-Cartee, 1995), politics, and social movements (among other applications).\n\nAccording to some sociologists, the \"social construction of collective action frames\" involves \"public discourse, that is, the interface of media discourse and interpersonal interaction; persuasive communication during mobilization campaigns by movement organizations, their opponents and countermovement organizations; and consciousness raising during episodes of collective action.\"\n\nWord-selection or diction has been a component of rhetoric since time immemorial. But most commentators attribute the concept of framing to the work of Erving Goffman on frame analysis and point especially to his 1974 book, \"Frame analysis: An essay on the organization of experience\". Goffman used the idea of frames to label \"schemata of interpretation\" that allow individuals or groups \"to locate, perceive, identify, and label\" events and occurrences, thus rendering meaning, organizing experiences, and guiding actions.\nGoffman's framing concept evolved out of his 1959 work, \"The Presentation of Self in Everyday Life\", a commentary on the management of impressions. These works arguably depend on Kenneth Boulding's concept of image.\n\nSociologists have utilized framing to explain the process of social movements.\nMovements act as carriers of beliefs and ideologies (compare memes). In addition, they operate as part of the process of constructing meaning for participants and opposers (Snow & Benford, 1988). Sociologists deem the mobilization of mass-movements \"successful\" when the frames projected align with the frames of participants to produce resonance between the two parties. Researchers of framing speak of this process as \"frame re-alignment\".\n\nSnow and Benford (1988) regard frame-alignment as an important element in social mobilization or movement. They argue that when individual frames become linked in congruency and complementariness, \"frame alignment\" occurs,\nproducing \"frame resonance\", a catalyst in the process of a group making the transition from one frame to another (although not all framing efforts prove successful). The conditions that affect or constrain framing efforts include the following:\n\nSnow and Benford (1988) propose that once someone has constructed proper frames as described above, large-scale changes in society such as those necessary for social movement can be achieved through frame-alignment.\n\nFrame-alignment comes in four forms: frame bridging, frame amplification, frame extension and frame transformation.\n\n\nWhen this happens, the securing of participants and support requires new values, new meanings and understandings. Goffman (1974, pp. 43–44) calls this \"keying\", where \"activities, events, and biographies that are already meaningful from the standpoint of some primary framework, in terms of another framework\" (Snow et al., 1986, p. 474) such that they are seen differently. Two types of frame transformation exist:\n\n\nAlthough the idea of language-framing had been explored earlier by Kenneth Burke (terministic screens), political communication researcher Jim A. Kuypers first published work advancing frame analysis (framing analysis) as a rhetorical perspective in 1997. His approach begins inductively by looking for themes that persist across time in a text (for Kuypers, primarily news narratives on an issue or event) and then determining how those themes are framed. Kuypers's work begins with the assumption that frames are powerful rhetorical entities that \"induce us to filter our perceptions of the world in particular ways, essentially making some aspects of our multi-dimensional reality more noticeable than other aspects. They operate by making some information more salient than other information...\"\n\nIn his 2009 essay \"Framing Analysis\" in \"Rhetorical Criticism: Perspectives in Action\" and his 2010 essay \"Framing Analysis as a Rhetorical Process\", Kuypers offers a detailed conception for doing framing analysis from a rhetorical perspective. According to Kuypers, \"Framing is a process whereby communicators, consciously or unconsciously, act to construct a point of view that encourages the facts of a given situation to be interpreted by others in a particular manner. Frames operate in four key ways: they define problems, diagnose causes, make moral judgments, and suggest remedies. Frames are often found within a narrative account of an issue or event, and are generally the central organizing idea.\" Kuypers's work is based on the premise that framing is a rhetorical process and as such it is best examined from a rhetorical point of view. Curing the problem is not rhetorical and best left to the observer.\n\nFraming is used to construct, refine, and deliver messages. Framing in politics is essential to getting your message across to the masses. Frames are mental structures that shape the way we view the world (Lakoff, Don't Think of an Elephant! Know Your Values and Frame the Debate 2004). Reframing is used particularly well by both conservatives and liberals in the political arena, so well that they have news anchors and commentators discussing the ideas, supplied phrases and framing (Lakoff, Don't Think of an Elephant! Know Your Values and Frame the Debate 2004).\n\nPreference reversals and other associated phenomena are of wider relevance within behavioural economics, as they contradict the predictions of rational choice, the basis of traditional economics. Framing biases affecting investing, lending, borrowing decisions make one of the themes of behavioral finance.\n\nEdward Zelinsky has shown that framing effects can explain some observed behaviors of legislators.\n\nThe role framing plays in the effects of media presentation has been widely discussed, with the central notion that associated perceptions of factual information can vary based upon the presentation of the information.\n\nIn \"Bush's War: Media Bias and Justifications for War in a Terrorist Age,\"Jim A. Kuypers examined the differences in framing of the War on Terror between the Bush administration and the U.S. mainstream news media between 2001 and 2005. Kuypers looked for common themes between presidential speeches and press reporting of those speeches, and then determined how the president and the press had framed those themes. By using a rhetorical version of framing analysis, Kuypers determined that the U.S. news media advanced frames counter to those used by the Bush administration:\n\nTable One: Comparison of President and News Media Themes and Frames 8 Weeks after 9/11\n\nIn 1991 Robert M. Entman published findings surrounding the differences in media coverage between Korean Air Lines Flight 007 and Iran Air Flight 655. After evaluating various levels of media coverage, based on both amount of airtime and pages devoted to similar events, Entman concluded that the frames the events were presented in by the media were drastically different:\n\nDifferences in coverage amongst various media outlets:\n\nIn 1988 Irwin Levin and Gary Gaeth did a study on the effects of framing attribute information on consumers before and after consuming a product (1988). In this study they found that in a study on beef, people who ate beef labeled as 75% lean rated it more favorably than people whose beef was labelled 25% fat.\n\nLinguist and rhetoric scholar George Lakoff argues that, in order to persuade a political audience of one side of an argument or another, the facts must be presented through a rhetorical frame. It is argued that, without the frame, the facts of an argument become lost on an audience, making the argument less effective. The rhetoric of politics uses framing to present the facts surrounding an issue in a way that creates the appearance of a problem at hand that requires a solution. Politicians using framing to make their own solution to an exigence appear to be the most appropriate compared to that of the opposition. Counter-arguments become less effective in persuading an audience once one side has framed an argument, because it is argued that the opposition then has the additional burden of arguing the frame of the issue in addition to the issue itself.\n\nFraming a political issue, a political party or a political opponent is a strategic goal in politics, particularly in the United States of America. Both the Democratic and Republican political parties compete to successfully harness its power of persuasion. According to the \"New York Times\":\n\nBecause framing has the ability to alter the public's perception, politicians engage in battles to determine how issues are framed. Hence, the way the issues are framed in the media reflects who is winning the battle. For instance, according to Robert Entman, professor of Communication at George Washington University, in the build-up to the Gulf War the conservatives were successful in making the debate whether to attack sooner or later, with no mention of the possibility of not attacking. Since the media picked up on this and also framed the debate in this fashion, the conservatives won.\n\nOne particular example of Lakoff's work that attained some degree of fame was his advice to rename trial lawyers (unpopular in the United States) as \"public protection attorneys\". Though Americans have not generally adopted this suggestion, the Association of Trial Lawyers of America did rename themselves the \"American Association of Justice\", in what the Chamber of Commerce called an effort to hide their identity.\n\nThe \"New York Times\" depicted similar intensity among Republicans:\n\nFrom a political perspective, framing has widespread consequences. For example, the concept of framing links with that of agenda-setting: by consistently invoking a particular frame, the framing party may effectively control discussion and perception of the issue. Sheldon Rampton and John Stauber in \"Trust Us, We're Experts\" illustrate how public-relations (PR) firms often use language to help frame a given issue, structuring the questions that then subsequently emerge. For example, one firm advises clients to use \"bridging language\" that uses a strategy of answering questions with specific terms or ideas in order to shift the discourse from an uncomfortable topic to a more comfortable one.\nPractitioners of this strategy might attempt to draw attention away from one frame in order to focus on another. As Lakoff notes, \"On the day that George W. Bush took office, the words \"tax relief\" started coming out of the White House.\"\nBy refocusing the structure away from one frame (\"tax burden\" or \"tax responsibilities\"), individuals can set the agenda of the questions asked in the future.\n\nCognitive linguists point to an example of framing in the phrase \"tax relief\". In this frame, use of the concept \"relief\" entails a concept of (without mentioning the benefits resulting from) taxes putting strain on the citizen:\n\nAlternative frames may emphasize the concept of taxes as a source of infrastructural support to businesses:\n\nFrames can limit debate by setting the vocabulary and metaphors through which participants can comprehend and discuss an issue. They form a part not just of political discourse, but of cognition. In addition to generating new frames, politically oriented framing research aims to increase public awareness of the connection between framing and reasoning.\n\n\n\nLevin, Irwin P., and Gary J. Gaeth. \"How Consumers Are Affected By The Framing Of Attribute Information Before And After Consuming The Product.\" Journal of Consumer Research 15.3 (1988): 374. Print.\n\n\n"}
{"id": "11964", "url": "https://en.wikipedia.org/wiki?curid=11964", "title": "Genus–differentia definition", "text": "Genus–differentia definition\n\nA genus–differentia definition is a type of intensional definition, and it is composed of two parts:\n\nFor example, consider these two definitions:\nThose definitions can be expressed as one genus and two \"differentiae\":\n\nThe process of producing new definitions by \"extending\" existing definitions is commonly known as differentiation (and also as derivation). The reverse process, by which just part of an existing definition is used itself as a new definition, is called abstraction; the new definition is called \"an abstraction\" and it is said to have been \"abstracted away from\" the existing definition.\n\nFor instance, consider the following:\nA part of that definition may be singled out (using parentheses here):\nand with that part, an abstraction may be formed:\nThen, the definition of \"a square\" may be recast with that abstraction as its genus:\n\nSimilarly, the definition of \"a square\" may be rearranged and another portion singled out:\nleading to the following abstraction:\nThen, the definition of \"a square\" may be recast with that abstraction as its genus:\n\nIn fact, the definition of \"a square\" may be recast in terms of both of the abstractions, where one acts as the genus and the other acts as the differentia:\nHence, abstraction is crucial in simplifying definitions.\n\nWhen multiple definitions could serve equally well, then all such definitions apply simultaneously. Thus, \"a square\" is a member of both the genus \"[a] rectangle\" and the genus \"[a] rhombus\". In such a case, it is notationally convenient to consolidate the definitions into one definition that is expressed with multiple genera (and possibly no differentia, as in the following):\nor completely equivalently:\n\nMore generally, a collection of formula_1 equivalent definitions (each of which is expressed with one unique genus) can be recast as one definition that is expressed with formula_2 genera. Thus, the following:\ncould be recast as:\n\nA genus of a definition provides a means by which to specify an \"is-a relationship\":\nThe non-genus portion of the differentia of a definition provides a means by which to specify a \"has-a relationship\":\n\nWhen a system of definitions is constructed with genera and differentiae, the definitions can be thought of as nodes forming a hierarchy or—more generally—a directed acyclic graph; a node that has no predecessor is \"a most general definition\"; each node along a directed path is \"more differentiated (or \"more derived) than any one of its predecessors, and a node with no successor is \"a most differentiated\" (or \"a most derived\") definition.\n\nWhen a definition, \"S\", is the tail of each of its successors (that is, \"S\" has at least one successor and each direct successor of \"S\" is a most differentiated definition), then \"S\" is often called \"the species of each of its successors, and each direct successor of \"S\" is often called \"an individual (or \"an entity\") of the species \"S\"; that is, the genus of an individual is synonymously called \"the species\" of that individual. Furthermore, the differentia of an individual is synonymously called \"the identity\" of that individual. For instance, consider the following definition:\nIn this case:\n\nAs in that example, the identity itself (or some part of it) is often used to refer to the entire individual, a phenomenon that is known in linguistics as a \"pars pro toto synecdoche\".\n"}
{"id": "996699", "url": "https://en.wikipedia.org/wiki?curid=996699", "title": "Green economy", "text": "Green economy\n\nThe green economy is defined as an economy that aims at reducing environmental risks and ecological scarcities, and that aims for sustainable development without degrading the environment. It is closely related with ecological economics, but has a more politically applied focus. The 2011 UNEP Green Economy Report argues \"that to be green, an economy must not only be efficient, but also fair. Fairness implies recognising global and country level equity dimensions, particularly in assuring a just transition to an economy that is low-carbon, resource efficient, and socially inclusive.\"\n\nA feature distinguishing it from prior economic regimes is the direct valuation of natural capital and ecological services as having economic value (\"see The Economics of Ecosystems and Biodiversity and Bank of Natural Capital\") and a full cost accounting regime in which costs externalized onto society via ecosystems are reliably traced back to, and accounted for as liabilities of, the entity that does the harm or neglects an asset.\n\nGreen Sticker and ecolabel practices have emerged as consumer facing measurements of friendliness to the environment and sustainable development. Many industries are starting to adopt these standards as a viable way to promote their greening practices in a globalizing economy.\n\n\"Green economics\" is loosely defined as any theory of economics by which an economy is considered to be component of the ecosystem in which it resides (after Lynn Margulis). A holistic approach to the subject is typical, such that economic ideas are commingled with any number of other subjects, depending on the particular theorist. Proponents of feminism, postmodernism, the environmental movement, peace movement, Green politics, green anarchism and anti-globalization movement have used the term to describe very different ideas, all external to mainstream economics.\n\nThe use of the term is further ambiguated by the political distinction of Green parties which are formally organized and claim the capital-G \"Green\" term as a unique and distinguishing mark. It is thus preferable to refer to a loose school of \"'green economists\"' who generally advocate shifts towards a green economy, biomimicry and a fuller accounting for biodiversity. (\"see The Economics of Ecosystems and Biodiversity especially for current authoritative international work towards these goals and Bank of Natural Capital for a layperson's presentation of these.\")\n\nSome economists view green economics as a branch or subfield of more established schools. For instance, it is regarded as classical economics where the traditional land is generalized to natural capital and has some attributes in common with labor and physical capital (since natural capital assets like rivers directly substitute for man-made ones such as canals). Or, it is viewed as Marxist economics with nature represented as a form of Lumpenproletariat, an exploited base of non-human workers providing surplus value to the human economy, or as a branch of neoclassical economics in which the price of life for developing vs. developed nations is held steady at a ratio reflecting a balance of power and that of non-human life is very low.\n\nAn increasing commitment by the UNEP (and national governments such as the UK) to the ideas of natural capital and full cost accounting under the banner 'green economy' could blur distinctions between the schools and redefine them all as variations of \"green economics\". As of 2010 the Bretton Woods institutions (notably the World Bank and International Monetary Fund (via its \"Green Fund\" initiative) responsible for global monetary policy have stated a clear intention to move towards biodiversity valuation and a more official and universal biodiversity finance.\nTaking these into account targeting not less but radically zero emission and waste is what is promoted by the Zero Emissions Research and Initiatives. The UNEP 2011 Green Economy Report informs that \"based on existing studies, the annual financing demand to green the global economy was estimated to be in the range US$ 1.05 to US$ 2.59 trillion. To place this demand in perspective, it is about one-tenth of total global investment per year, as measured by global Gross Capital Formation.\" \n\nKarl Burkart defines a green economy as based on six main sectors:\n\nThe International Chamber of Commerce (ICC) representing global business defines green economy as “an economy in which economic growth and environmental responsibility work together in a mutually reinforcing fashion while supporting progress on social development”.\n\nIn 2012, the ICC published the Green Economy Roadmap, containing contributions from experts from around the globe brought together in a two-year consultation process. The Roadmap represents a comprehensive and multidisciplinary effort to clarify and frame the concept of “green economy”. It highlights the essential role of business in bringing solutions to common global challenges. It sets out the following 10 conditions which relate to business/intra-industry and collaborative action for a transition towards a green economy:\n\n\nMeasuring economic output and progress is done through the use of economic index indicators. Green indices emerged from the need to measure human ecological impact, efficiency sectors like transport, energy, buildings and tourism, as well as the investment flows targeted to areas like renewable energy and cleantech innovation. \n\nEcological footprint measurements are a way to gauge anthropogenic impact and are another standard used by municipal governments.\n\nGreen economies require green energy generation based on renewable energy to replace fossil fuels as well as energy conservation and efficient energy use.\n\nThere is justification for market failure to respond to environmental protection and climate protection needs with the excuse that high external costs and high initial costs for research, development, and marketing of green energy sources and green products prevents firms from voluntarily reducing their ecological footprints. The green economy may need government subsidies as market incentives to motivate firms to invest and produce green products and services. The German Renewable Energy Act, legislations of many other member states of the European Union and the American Recovery and Reinvestment Act of 2009, all provide such market incentives. However, other experts argue that green strategies can be highly profitable for corporations that understand the business case for sustainability and can market green products and services beyond the traditional green consumer.\n\nA number of organisations and individuals have criticised aspects of the 'Green Economy', particularly the mainstream conceptions of it based on using price mechanisms to protect nature, arguing that this will extend corporate control into new areas from forestry to water. The research organisation ETC Group argues that the corporate emphasis on bio-economy \"will spur even greater convergence of corporate power and unleash the most massive resource grab in more than 500 years.\" Venezuelan professor Edgardo Lander says that the UNEP's report, \"Towards a Green Economy\", while well-intentioned \"ignores the fact that the capacity of existing political systems to establish regulations and restrictions to the free operation of the markets – even when a large majority of the population call for them – is seriously limited by the political and financial power of the corporations.\" Ulrich Hoffmann, in a paper for UNCTAD also says that the focus on Green Economy and \"green growth\" in particular, \"based on an evolutionary (and often reductionist) approach will not be sufficient to cope with the complexities of climate\nchange\" and \"may rather give much false hope and excuses to do nothing really fundamental that can\nbring about a U-turn of global greenhouse gas emissions. Clive Spash, an ecological economist, has criticised the use of economic growth to address environmental losses, and argued that the Green Economy, as advocated by the UN, is not a new approach at all and is actually a diversion from the real drivers of environmental crisis. He has also criticised the UN's project on the economics of ecosystems and biodiversity (TEEB), and the basis for valuing ecosystems services in monetary terms.\n\n"}
{"id": "3563367", "url": "https://en.wikipedia.org/wiki?curid=3563367", "title": "Hypergamy", "text": "Hypergamy\n\nHypergamy (colloquially referred to as \"marrying up\" or \"gold-digging\") is a term used in social science for the act or practice of a person marrying a spouse of higher caste or social status than themselves.\n\nThe antonym \"hypogamy\" refers to the inverse: marrying a person of lower social class or status (colloquially \"marrying down\").\n\nBoth terms were coined in India in the 19th century while translating classical Hindu law books, which used the Sanskrit terms \"anuloma\" and \"pratiloma\", respectively, for the two concepts.\n\nForms of hypergamy have been practiced throughout history, including in India, imperial China, ancient Greece, the Ottoman Empire, and feudal Europe. \n\nToday most people marry their approximate social equals, and in much of the world hypergamy is in slow decline; for example, it is becoming less common for women to marry older men, although hypergamy does not require the man to be older, only of higher status. However, even in relatively gender-equal societies it is generally accepted that young women will often partner with powerful older men; while the general rule is that older men have had more time to gather wealth and status than younger men and they are on average wealthier and of higher status.\n\nStudies of heterosexual mate selection in dozens of countries around the world have found men and women report prioritizing different traits when it comes to choosing a mate, with men tending to prefer women who are young and attractive and women tending to prefer men who are rich, well-educated, ambitious, and attractive. Evolutionary psychologists contend this is an inherent sex difference arising out of sexual selection, with men driven to seek women who will give birth to healthy babies and women driven to seek men who will be able to provide the necessary resources for the family's survival. Social learning theorists, however, say women value men with high earning capacity because women's own ability to earn is constrained by their disadvantaged status in a male-dominated society. They argue that as societies shift towards becoming more gender-equal, women's mate selection preferences will shift as well. Some research supports that theory, including a 2012 analysis of a survey of 8,953 people in 37 countries, which found that the more gender-equal a country, the likelier male and female respondents were to report seeking the same qualities as each other rather than different ones. However, Townsend (1989) surveyed medical students regarding their perception of how the availability of marriage partners changed as their educational careers advanced. Eighty-five percent of the women indicated that \"As my status increases, my pool of acceptable partners decreases.\" In contrast, 90 percent of men felt that \"As my status increases, my pool of acceptable partners increases.\"\n\nGilles Saint-Paul (2008) argued that, based on mathematical models, human female hypergamy occurs because women have greater lost mating opportunity costs from monogamous mating (given their slower reproductive rate and limited window of fertility), and thus must be compensated for this cost of marriage. Marriage reduces the overall genetic quality of her offspring by precluding the possibility of impregnation by a genetically higher quality male, albeit without his parental investment. However, this reduction may be compensated by greater levels of parental investment by her genetically lower quality husband.\n\nAn empirical study examined the mate preferences of subscribers to a computer dating service in Israel that had a highly skewed sex ratio (646 men for 1,000 women). Despite this skewed sex ratio, they found that \"On education and socioeconomic status, women on average express greater hypergamic selectivity; they prefer mates who are superior to them in these traits... while men express a desire for an analogue of hypergamy based on physical attractiveness; they desire a mate who ranks higher on the physical attractiveness scale than they themselves do.\"\n\nOne study did not find a statistical difference in the number of women or men \"marrying-up\" in a sample of 1,109 first-time married couples in the United States.\n\nAnother study has shown that in the UK, hypergamy has decreased significantly since the 1950s.\n\nFor citizens of rural India, hypergamy is an opportunity to modernize. Marriages in rural India are increasingly examples of hypergamy. Farmers and other rural workers want their daughters to have access to city life, for with metropolitan connections comes internet access, better job opportunities, and upper-class social circles. A connection in an urban area creates a broader social horizon for the bride's family, and young children in the family can be sent to live with the couple in the city for better schooling. Hypergamy comes with a cost though; the dowry, which often costs as much or more than an entire house. The high price that has to be borne by parents to arrange a suitable marriage for a daughter has led to increasing rates of abortion of female fetuses.\n\nThe concept of marrying up in India is prevalent due to caste-based class stratification. The women from the higher castes were not allowed to marry men from lower castes. This concept, cited in the Vedas as the Anuloma, was justified as the mechanism to keep the Hindu ideological equivalent of the gene pool from degrading. The opposite of the Anuloma, called the Pratiloma, was not allowed in the ancient Indian society. However, the Vedas cite an example where one such exception was allowed: when the daughter of Sage Shukracharya, Devayani was allowed to marry a Kshatriya king (lower caste compared to Brahmanas in the Indian caste system) named Yayati.\n"}
{"id": "2892645", "url": "https://en.wikipedia.org/wiki?curid=2892645", "title": "Instantiation principle", "text": "Instantiation principle\n\nThe instantiation principle or principle of instantiation or principle of exemplification is the concept in metaphysics and logic (first put forward by David Malet Armstrong) that there can be no uninstantiated or unexemplified properties (or universals). In other words, it is impossible for a property to exist which is not had by some object.\n\nConsider a chair. Presumably chairs did not exist 150,000 years ago. Thus, according to the principle of instantiation, the property of being a chair did not exist 150,000 years ago either. Similarly, if all red objects were to suddenly go out of existence, then the property of being red would likewise go out of existence.\n\nTo make the principle more plausible in the light of these examples, the existence of properties or universals is not tied to their actual existence now, but to their existence in space-time considered as a whole. Thus, any property which \"is\", \"has been\", or \"will be\" instantiated exists. The property of being red would exist even if all red things were to be destroyed, because it has been instantiated. This broadens the range of properties which exist if the principle is true. \n\nThose who endorse the principle of instantiation are known as \"in re\" (in thing or in reality) realists or 'immanent realists'.\n\n"}
{"id": "41896414", "url": "https://en.wikipedia.org/wiki?curid=41896414", "title": "Interactive Futures", "text": "Interactive Futures\n\nInteractive Futures (IF) was a biennial conference and exhibition, hosted in Vancouver, British Columbia, Canada, that explored current tendencies, research and dialogue related to the intersection of technology and art. Interactive Futures included a variety of events such as lectures, workshops, exhibitions, and panels in an effort to provide opportunities for discourse by local, national and international researchers and practitioners.\n\nInteractive Futures (IF) has been active since 2002 holding a conference alongside its exhibitions and performances. From 2002 through 2007, Interactive Futures has grown from a single venue event, held at the University of Victoria that featured local researchers and artists, to a multi-venue event with international cultural partners, an active archival website and international publications. For IF'09, Julie Andreyev and Maria Lantin became co-Directors and relocated IF to Intersections Digital Studios, Emily Carr University of Art + Design, Vancouver, Canada.\n\nInteractive Futures (IF) is a forum for showing current tendencies in new media art as well as a conference for exploring ideas and research related to technology and art. IF supports a conference, keynote addresses, presentations, panel discussions, exhibitions, and performances. National and international presenters, performers and artists acknowledged for their research and production are invited to contribute to an expansion of knowledge within the IF thematic. IF provides a forum for artists and researchers of Canada to interact with international members of the media arts communities within an intensive setting.\n\nIF'11: \"Animal Influence\" conference was concerned with the themes of animal cognition, consciousness and agency in relation to emerging new media technologies of image, text, sound and video.\nThe conference was funded by Canada’s Social Sciences and Humanities Research Council (SSHRC), the British Columbia Arts Council (BCAC), and the Consulat Général de France à Vancouver.\n\nThe 2011 symposium, guest curated by Carol Gigliotti, focused on new media artwork influenced by the growing wealth of knowledge about other species' perceptions of the world\n\nThe sub-themes of Perception, Agency, and Consciousness/Compassion were explored through exhibitions, presentations, panel discussions, film screenings and workshops.\n\nLisa Jevbratt: Professor in the Media Art Technology program of University of California, Santa Barbara.\n<br>\nMarc Bekoff: Professor Emeritus of Ecology and Evolutionary Biology at the University of Colorado, Boulder, Fellow of the Animal Behaviour Society, and former Guggenheim Fellow.\n\nIF'09: \"Stereo\" focused on research, production, and experiments with perception in relation to the knowledge of two-channel sound, and stereographic 3D. \nThe sub-themes of Stereographics, Co-Locative, and Sensory Illusions were explored through exhibitions, presentations, panel discussions, film screenings and workshops.\n\nGeorge Legrady: Director of the Experimental Visualization Lab in the Media Arts & Technology doctoral program at the University of California in Santa Barbara.\n<br>\nPaula Levine: Associate Professor of Art at San Francisco State University.\n<br>\nMunro Ferguson: Award-winning Canadian Film Director and Animator.\n\nIF'07: \"The New Screen\" sought to explore emerging forms of screen-based media from the diverse repertoire of artists, theorists, writers, filmmakers, developers and educators. \nThe conference was sponsored by the Victoria Film Festival, Open Space Artist-Run Centre and the Canadian Council for the Arts.\n\nKate Pullinger: Canadian novelist and author of digital fiction, lecturing at De Montfort University, England.\n<br>\nDon Ritter: Canadian new media and installation artist, and writer.\n<br>\nMijram Struppek: Urbanist, researcher and consultant in Berlin.\n\nIF'06: \"Audio Visions\" focused on exploring new forms of audio-based multimedia art, music theatre, performance, and installation.\nThe event was held in joint collaboration with the Digital Art Weeks founded by Art Clay and Juerg Gutknecht on the Institute of Computer Systems of ETH Zurich, Switzerland\n\nIF'04: \"New Media Crossing Boundaries\", included exhibition work from Canadian artists Julie Andreyev, Jean Piché, Kenneth Newby and Aleksandra Dulic along with international artist, and experimental electronic musician DJ Spooky.\n\n"}
{"id": "31332", "url": "https://en.wikipedia.org/wiki?curid=31332", "title": "International Obfuscated C Code Contest", "text": "International Obfuscated C Code Contest\n\nThe International Obfuscated C Code Contest (abbreviated IOCCC) is a computer programming contest for the most creatively obfuscated C code. Held annually in the years 1984–1996, 1998, 2000, 2001, 2004–2006, 2011–2015 and then in 2018, it is described as \"celebrating [C's] syntactical opaqueness\". The winning code for the 25th contest, held in 2018, was released in May 2018.\n\nEntries are evaluated anonymously by a panel of judges. The judging process is documented in the competition guidelines and consists of elimination rounds. By tradition, no information is given about the total number of entries for each competition. Winning entries are awarded with a category, such as \"Worst Abuse of the C preprocessor\" or \"Most Erratic Behavior\", and then announced on the official IOCCC website. The contest states that being announced on the IOCCC website is the reward for winning.\n\nThe IOCCC was started by Landon Curt Noll and Larry Bassel in 1984 while employed at National Semiconductor's Genix porting group. The idea for the contest came after they compared notes with each other about some poorly written code that they had to fix, notably the Bourne shell, which used macros to emulate ALGOL 68 syntax, and buggy version of finger for BSD. The contest itself was the topic of a quiz question in the 1993 Computer Bowl. After a hiatus of five years starting in 2006, the contest returned in 2011.\n\nCompared with other programming contests, the IOCCC is described as \"not all that serious\" by Michael Swaine, editor of \"Dr. Dobbs\".\n\nEach year, the rules of the contest are published on the IOCCC website. Rules vary from year to year and are posted with a set of guidelines that attempt to convey the spirit of the rules.\n\nThe rules are often deliberately written with loopholes that contestants are encouraged to find and abuse. Entries that take advantage of loopholes can cause the rules for the following year's contest to be adjusted.\n\nEntries often employ strange or unusual tricks, such as using the C preprocessor to do things it was not designed to do, or avoiding commonly used constructs in the C programming language in favor of much more obscure ways of achieving the same thing. Two contest winners generated a list of prime numbers using the C preprocessor \"spectacularly\", according to \"Dr. Dobbs\". Some quotes from 2004 winners include:\n\nContributions have included source code formatted to resemble images, text, etc., after the manner of ASCII art, preprocessor redefinitions to make code harder to read, and self-modifying code. In several years, an entry was submitted that required a new definition of some of the rules for the next year. This is regarded as a high honor. An example is the world's shortest self-reproducing program. The entry was a program designed to output its own source code, and which had zero bytes of source code. When the program ran, it printed out zero bytes, equivalent to its source code.\n\nIn the effort to take obfuscation to its extremes, contestants have produced programs which skirt around the edges of C standards, or result in constructs which trigger rarely used code path combinations in compilers. As a result, several of the past entries may not compile directly in a modern compiler, and some may cause crashes.\n\nWithin the code size limit of only a few kilobytes, contestants have managed to do complicated things – a 2004 winner turned out an operating system.\n\n\"Toledo Nanochess\" is a chess engine developed by Mexican Oscar Toledo Gutiérrez, a five-time winner of the IOCCC. In accordance with IOCCC rules, it is 1255 characters long. The author claims that it is the world's smallest chess program written in C.\n\nThe source code for \"Toledo Nanochess\" and other engines is available.\nBecause \"Toledo Nanochess\" is based on Toledo's winning entry from the 18th IOCCC (Best Game), it is heavily obfuscated.\n\nOn February 2, 2014, the author published the book \"Toledo Nanochess: The commented source code\", which contains the fully commented source code.\n\nAs of February 7, 2010, it appears to be one of only two chess engines written in less than 2 kilobytes of C that are able to play full legal chess moves, along with \"Micro-Max\" by Dutch physicist H. G. Muller. In 2014 the 1 kilobyte barrier has been broken by \"Super Micro Chess\" – a derivative of Micro-Max – totaling 760 characters (spaces and newlines included). A smaller version, \"Toledo Picochess\", consisting of 944 non-blank characters, also exists.\n\nSource code excerpt\nBelow is a 1988 entry which calculates pi by looking at its own area:\n\nAnother example is the following flight simulator, the winner of the 1998 IOCCC, as listed and described in \"Calculated Bets: Computers, Gambling, and Mathematical Modeling to Win\" (2001) and shown below:\n\nThis program needs the following command line on a Linux system to be compiled:\n\n\n"}
{"id": "2759248", "url": "https://en.wikipedia.org/wiki?curid=2759248", "title": "Inventory control", "text": "Inventory control\n\nInventory control or stock control can be broadly defined as \"the activity of checking a shop’s stock.\" However, a more focused definition takes into account the more science-based, methodical practice of not only verifying a business' inventory but also focusing on the many related facets of inventory management (such as forecasting future demand) \"within an organisation to meet the demand placed upon that business economically.\" Other facets of inventory control include supply chain management, production control, financial flexibility, and customer satisfaction. At the root of inventory control, however, is the inventory control problem, which involves determining when to order, how much to order, and the logistics (where) of those decisions.\n\nAn extension of inventory control is the inventory control system. This may come in the form of a technological system and its programmed software used for managing various aspects of inventory problems , or it may refer to a methodology (which may include the use of technological barriers) for handling loss prevention in a business.\n\nAn inventory control system is used to keep inventories in a desired state while continuing to adequately supply customers, and its success depends on maintaining clear records on a periodic or perpetual basis. \n\nInventory management software often plays an important role in the modern inventory control system, providing timely and accurate analytical, optimization, and forecasting techniques for complex inventory management problems. Typical features of this type of software include:\n\n\nThrough this functionality, a business may better detail what has sold, how quickly, and at what price, for example. Reports could be used to predict when to stock up on extra products around a holiday or to make decisions about special offers, discontinuing products, and so on. \n\nInventory control techniques often rely upon barcodes and radio-frequency identification (RFID) tags to provide automatic identification of inventory objects—including but not limited to merchandise, consumables, fixed assets, circulating tools, library books, and capital equipment—which in turn can be processed with inventory management software. A new trend in inventory management is to label inventory and assets with a QR Code, which can then be read with smart-phones to keep track of inventory count and movement. These new systems are especially useful for field service operations, where an employee needs to record inventory transaction or look up inventory stock in the field, away from the computers and hand-held scanners.\n\nInventory control systems have advantages and disadvantages, based on what style of system is being run. A purely periodic (physical) inventory control system takes \"an actual physical count and valuation of all inventory on hand ... at the close of an accounting period,\" whereas a perpetual inventory control system takes an initial count of an entire inventory and then closely monitors any additions and deletions as they occur. Various advantages and disadvantages, in comparison, include:\n\n\n"}
{"id": "43193940", "url": "https://en.wikipedia.org/wiki?curid=43193940", "title": "Jónsson–Tarski algebra", "text": "Jónsson–Tarski algebra\n\nIn mathematics, a Jónsson–Tarski algebra or Cantor algebra is an algebraic structure encoding a bijection from an infinite set \"X\" onto the product \"X\"×\"X\". They were introduced by . , named them after Georg Cantor because of Cantor's pairing function and Cantor's theorem that an infinite set \"X\" has the same number of elements as \"X\"×\"X\"; the term \"Cantor algebra\" is also occasionally used to mean the Boolean algebra of all clopen subsets of the Cantor set, or the Boolean algebra of Borel subsets of the reals modulo meager sets (sometimes called the Cohen algebra).\n\nThe group of order preserving automorphisms of the free Jónsson–Tarski algebra on one generator is the Thompson group \"F\".\n\nA Jónsson–Tarski algebra of type 2 is a set \"A\" with a product \"w\" from \"A\"×\"A\" to \"A\" and two \"projection\" maps \"p\" and \"p\" from \"A\" to \"A\", satisfying \"p\"(\"w\"(\"a\",\"a\")) = \"a\", \"p\"(\"w\"(\"a\",\"a\")) = \"a\", and \"w\"(\"p\"(\"a\"),\"p\"(\"a\")) = \"a\". The definition for type > 2 is similar but with \"n\" projection operators.\n\nIf \"w\" is any bijection from \"A\"×\"A\" to \"A\" then it can be extended to a unique Jónsson–Tarski algebra by letting \"p\"(\"a\") be the projection of \"w\"(\"a\") onto the \"i\"th factor.\n\n"}
{"id": "287010", "url": "https://en.wikipedia.org/wiki?curid=287010", "title": "Kaizen", "text": "Kaizen\n\nBy improving standardized programmes and processes, kaizen aims to eliminate waste (see lean manufacturing). Kaizen was first practiced in Japanese businesses after World War II, influenced in part by American business and quality-management teachers, and most notably as part of The Toyota Way. It has since spread throughout the world and has been applied to environments outside business and productivity.\n\nThe Japanese word \"kaizen\" means \"change for better\", with inherent meaning of either \"continuous\" or \"philosophy\" in Japanese dictionaries and in everyday use. The word refers to any improvement, one-time or continuous, large or small, in the same sense as the English word \"improvement\". However, given the common practice in Japan of labeling industrial or business improvement techniques with the word \"kaizen\", particularly the practices spearheaded by Toyota, the word \"kaizen\" in English is typically applied to measures for implementing \"continuous\" improvement, especially those with a \"Japanese philosophy\". The discussion below focuses on such interpretations of the word, as frequently used in the context of modern management discussions. Two kaizen approaches have been distinguished:\n\n\nThe former is oriented towards the flow of materials and information, and is often identified with the reorganization of an entire production area, even a company. The latter means the improvement of individual workstands. Therefore, improving the way production workers do their job is a part of a process kaizen. The use of the kaizen model for continuous improvement demands that both flow and process kaizens are used, although process kaizens are used more often to focus workers on continuous small improvements. In this model, operators mostly look for small ideas which, if possible, can be implemented on the same day. This is in contrast to traditional models of work improvement, which generally have a long lag between concept development and project implementation.\n\nKaizen is a daily process, the purpose of which goes beyond simple productivity improvement. It is also a process that, when done correctly, humanizes the workplace, eliminates overly hard work (\"muri\"), and teaches people how to perform experiments on their work using the scientific method and how to learn to spot and eliminate waste in business processes. In all, the process suggests a humanized approach to workers and to increasing productivity: \"The idea is to nurture the company's people as much as it is to praise and encourage participation in kaizen activities.\" Successful implementation requires \"the participation of workers in the improvement.\"\nPeople at all levels of an organization participate in kaizen, from the CEO down to janitorial staff, as well as external stakeholders when applicable. Kaizen is most commonly associated with manufacturing operations, as at Toyota, but has also been used in non-manufacturing environments. The format for kaizen can be individual, suggestion system, small group, or large group. At Toyota, it is usually a local improvement within a workstation or local area and involves a small group in improving their own work environment and productivity. This group is often guided through the kaizen process by a line supervisor; sometimes this is the line supervisor's key role. Kaizen on a broad, cross-departmental scale in companies, generates total quality management, and frees human efforts through improving productivity using machines and computing power.\n\nWhile kaizen (at Toyota) usually delivers small improvements, the culture of continual aligned small improvements and standardization yields large results in terms of overall improvement in productivity. This philosophy differs from the \"command and control\" improvement programs (e.g., Business Process Improvement) of the mid-20th century. Kaizen methodology includes making changes and monitoring results, then adjusting. Large-scale pre-planning and extensive project scheduling are replaced by smaller experiments, which can be rapidly adapted as new improvements are suggested.\n\nIn modern usage, it is designed to address a particular issue over the course of a week and is referred to as a \"kaizen blitz\" or \"kaizen event\". These are limited in scope, and issues that arise from them are typically used in later blitzes. A person who makes a large contribution in the successful implementation of kaizen during kaizen events is awarded the title of \"Zenkai\". In the 21st century, business consultants in various countries have engaged in widespread adoption and sharing of the Kaizen framework as a way to help their clients restructure and refocus their business processes.\n\nThe small-step work improvement approach was developed in the USA under Training Within Industry program (TWI Job Methods). Instead of encouraging large, radical changes to achieve desired goals, these methods recommended that organizations introduce small improvements, preferably ones that could be implemented on the same day. The major reason was that during WWII there was neither time nor resources for large and innovative changes in the production of war equipment. The essence of the approach came down to improving the use of the existing workforce and technologies.\n\nAs part of the Marshall Plan after World War II, American occupation forces brought in experts to help with the rebuilding of Japanese industry while the Civil Communications Section (CCS) developed a management training program that taught statistical control methods as part of the overall material. Homer Sarasohn and Charles Protzman developed and taught this course in 1949-1950. Sarasohn recommended W. Edwards Deming for further training in statistical methods.\n\nThe Economic and Scientific Section (ESS) group was also tasked with improving Japanese management skills and Edgar McVoy was instrumental in bringing Lowell Mellen to Japan to properly install the Training Within Industry (TWI) programs in 1951. The ESS group had a training film to introduce TWI's three \"J\" programs: Job Instruction, Job Methods and Job Relations. Titled \"Improvement in Four Steps\" (\"Kaizen eno Yon Dankai\") it thus introduced kaizen to Japan.\n\nFor the pioneering, introduction, and implementation of kaizen in Japan, the Emperor of Japan awarded the Order of the Sacred Treasure to Dr. Deming in 1960. Subsequently, the Japanese Union of Scientists and Engineers (JUSE) instituted the annual Deming Prizes for achievement in quality and dependability of products. On October 18, 1989, JUSE awarded the Deming Prize to Florida Power & Light Co. (FPL), based in the US, for its exceptional accomplishments in process and quality-control management, making it the first company outside Japan to win the Deming Prize.\n\nThe Toyota Production System is known for kaizen, where all line personnel are expected to stop their moving production line in case of any abnormality and, along with their supervisor, suggest an improvement to resolve the abnormality which may initiate a kaizen.\nThe cycle of kaizen activity can be defined as: \"Plan → Do → Check → Act\". This is also known as the Shewhart cycle, Deming cycle, or PDCA.\n\nAnother technique used in conjunction with PDCA is the 5 Whys, which is a form of root cause analysis in which the user asks a series of five \"why\" questions about a failure that has occurred, basing each subsequent question on the answer to the previous. There are normally a series of causes stemming from one root cause, and they can be visualized using fishbone diagrams or tables. The Five Whys can be used as a foundational tool in personal improvement, or as a means to create wealth.\n\nMasaaki Imai made the term famous in his book \"Kaizen: The Key to Japan's Competitive Success\".\n\nIn the \"Toyota Way Fieldbook\", Liker and Meier discuss the kaizen blitz and kaizen burst (or kaizen event) approaches to continuous improvement. A kaizen blitz, or rapid improvement, is a focused activity on a particular process or activity. The basic concept is to identify and quickly remove waste. Another approach is that of the kaizen burst, a specific kaizen activity on a particular process in the value stream. Kaizen facilitators generally go through training and certification before attempting a Kaizen project.\n\nIn the 1990s, Professor Iwao Kobayashi published his book 20 Keys to Workplace Improvement and created a practical, step-by-step improvement framework called \"the 20 Keys\". He identified 20 operations focus areas which should be improved to attain holistic and sustainable change. He went further and identified the 5 levels of implementation for each of these 20 focus areas. 4 of the focus areas are called Foundation Keys. According to the 20 Keys, these foundation keys should be launched ahead of the others in order to form a strong constitution in the company. The four foundation keys are:\n\n\n\n"}
{"id": "4021739", "url": "https://en.wikipedia.org/wiki?curid=4021739", "title": "LaSalle's invariance principle", "text": "LaSalle's invariance principle\n\nLaSalle's invariance principle (also known as the invariance principle, Barbashin-Krasovskii-LaSalle principle, or Krasovskii-LaSalle principle ) is a criterion for the asymptotic stability of an autonomous (possibly nonlinear) dynamical system.\n\nSuppose a system is represented as\n\nwhere formula_2 is the vector of variables, with\n\nIf a formula_4 function formula_5 can be found such that\n\nthen the set of accumulation points of any trajectory is contained in formula_8 where formula_8 is the union of complete trajectories contained entirely in the set formula_10. \n\nIf we additionally have that the function formula_11 is positive definite, i.e.\n\nand if formula_8 contains no trajectory of the system except the trivial trajectory formula_16 for formula_17, then the origin is asymptotically stable.\n\nFurthermore, if formula_11 is radially unbounded, i.e.\n\nthen the origin is globally asymptotically stable.\n\nIf \n\nhold only for formula_24 in some neighborhood formula_25 of the origin, and the set\n\ndoes not contain any trajectories of the system besides the trajectory formula_27, then the local version of the invariance principle states that the origin is locally asymptotically stable.\n\nIf formula_28 is negative definite, the global asymptotic stability of the origin is a consequence of Lyapunov's second theorem. The invariance principle gives a criterion for asymptotic stability in the case when formula_29 is only negative semidefinite.\n\nThis section will apply the invariance principle to establish the local asymptotic stability of a simple system, the pendulum with friction. This system can be modeled with the differential equation \n\nwhere formula_31 is the angle the pendulum makes with the vertical normal, formula_32 is the mass of the pendulum, formula_33 is the length of the pendulum, formula_34 is the friction coefficient, and \"g\" is acceleration due to gravity.\n\nThis, in turn, can be written as the system of equations\n\nUsing the invariance principle, it can be shown that all trajectories which begin in a ball of certain size around the origin formula_37 asymptotically converge to the origin. We define formula_38 as\n\nThis formula_38 is simply the scaled energy of the system Clearly, formula_41 is positive definite in an open ball of radius formula_42 around the origin. Computing the derivative,\n\nObserve that formula_44. If it were true that formula_45, we could conclude that every trajectory approaches the origin by Lyapunov's second theorem. Unfortunately, formula_46 and formula_47 is only negative semidefinite. However, the set\n\nwhich is simply the set\n\ndoes not contain any trajectory of the system, except the trivial trajectory x = 0. Indeed, if at some time formula_50, formula_51, then because \nformula_52 must be less than formula_42 away from the origin, formula_54 and formula_55. As a result, the trajectory will not stay in the set formula_56.\n\nAll the conditions of the local version of the invariance principle are satisfied, and we can conclude that every trajectory that begins in some neighborhood of the origin will converge to the origin as formula_57 .\n\nThe general result was independently discovered by J.P. LaSalle (then at RIAS) and N.N. Krasovskii, who published in 1960 and 1959 respectively. While LaSalle was the first author in the West to publish the general theorem in 1960, a special case of the theorem was in communicated in 1952 by Barbashin and Krasovskii, followed by a publication of the general result in 1959 by Krasovskii .\n\n\n\n\n\n"}
{"id": "41677521", "url": "https://en.wikipedia.org/wiki?curid=41677521", "title": "Latanya Sweeney", "text": "Latanya Sweeney\n\nLatanya Arvette Sweeney is a Professor of Government and Technology in Residence at Harvard University, the Director of the Data Privacy Lab in the Institute of Quantitative Social Science (IQSS) at Harvard, and the Faculty Dean in Currier House at Harvard. She formerly served as the Chief Technologist of the Federal Trade Commission, a position she held from January 2014 until December 2014. She has made several contributions to privacy technology. Her best known academic work is on the theory of \"k\"-anonymity and she is credited with the observation that \"87% of the U.S. population is uniquely identified by date of birth, gender, postal code.\"\n\nSweeney develops technology to assess and solve societal problems and teaches others how to use the same technology. She has made several discoveries related to identifiability and privacy technologies. Her work has received awards from numerous organizations, including the American Psychiatric Association, the American Medical Informatics Association, and the Blue Cross Blue Shield Association. Her work was praised in the TAPAC Report that reviewed the Total Information Awareness Project of DARPA. She has testified before the Privacy and Integrity Advisory Committee of the Department of Homeland Security and the European Union Commission. Sweeney was a Distinguished Career Professor of Computer Science, Technology and Policy in the School of Computer Science at Carnegie Mellon University. In 2001, she received her PhD in computer science from the Massachusetts Institute of Technology where she became the first African American woman to earn a PhD in computer science from that school. Her undergraduate degree in computer science was completed at Harvard University.\n\nSweeney went to Dana Hall Schools in Wellesley, MA, where she received her high school diploma in 1977. She delivered the valedictory at the graduation ceremony.\n\nSweeney did undergraduate studies at Massachusetts Institute of Technology (MIT), where she focused on Electrical Engineering and Computer Science.\n\nShe went to Harvard Extension School to study Computer Science, where she received an ALB degree (Bachelor of Liberal Arts in Extension Studies) in Computer Science, Cum Laude. Her undergraduate research thesis was called “A Coin Toss: the Dialectical Odds Aren't Always 50/50”. She received honors grades in all courses and completed graduate courses in computer science, mathematics, physics, educational psychology, and philosophy. She also delivered the graduation speech.\n\nSweeney returned to MIT to study for her master degree, where she received an S.M. in Electrical Engineering and Computer Science in 1997. During her studying of master degree, she received a GPA of 4.9/5.0. Sweeney wrote a Master's thesis called “Sprees, a Finite-State Orthographic Learning System that Recognizes and Generates Phonologically Similar Spellings”, where she was the finalist in MasterWorks. She continued to study at MIT for her Ph.D. degree, where she advanced in Computer Science. She received the degree in 2001 and finished her Ph.D. thesis “Computational Disclosure Control: Theory and Practice”.\n\nIn 2001, Sweeney became director and founder of the Data Privacy Lab, at Carnegie Mellon University. She was a member of the Program Committee for Modeling Decisions for Artificial Intelligence (MDAI) in 2005. In 2004, she founded the Journal of Privacy Technology, later becoming the editor-in-chief in 2006.\n\nIn her PhD dissertation at MIT (Computational Disclosure Control: Theory and Practice), Sweeney examines various computational methodologies for the secure dissemination of anonymous data without revealing any identifying, or potentially identifying, information. She proposes novel approaches for secure data disclosure, defining and describing null-map, k-map and wrong-map models of protection. Sweeney then critiques and compares four electronic data-based computational programs on their capacity to protect private information. The systems evaluated are her Scrub System, her Datafly II System, Statistics Netherlands’ u-Argus System, and her k-Similar algorithm – which she concludes as the most effective system in minimizing privacy risks. Prior to her dissertation, Sweeney has already been published numerous times, in topics pertaining to healthcare data security, and she has also completed a Masters Thesis at MIT and a ALB Thesis at Harvard. Currently, Sweeney is a prominent data security researcher and continues ongoing work to advance this field.\n\nWho Knows What About Me? A Survey of behind the Scenes Personal Data Sharing to Third Parties by Mobile Apps.\n\nBy conducting research on 110 mobile apps over 9 categories (including business, games, fitness & health etc.), Sweeney found that many mobile apps transmits sensitive personal data to third-party domains, particularly name, location and email etc. According to her study, Android apps send potential sensitive personal data to 3.1 third-party domains. As for iOS apps, they connected with an average of 2.6 third party domains. While increasing bringing up awareness for current risks for privacy leakage, Sweeney also inspires us to think about possible future sharing permission system on mobile phones.\n\nIn 2016 L. Sweeny, M. Bar-Sinai, M. Crosas, introduced Data Tags, Data Handling Policy Spaces and the Tags Language in IEEE Security and Privacy Workshops 2016 in San Jose CA. The paper introduces the Tags programming language and toolset which through questionnaires suggests data handling policies appropriate to the level of security the dataset requires. The Tags Language and Tools simplifies the development of security policies by recommending policies that meet the legal requirements for that dataset, like HIPAA.\n\nIn 1997, Sweeney conducted her first re-identification experiment wherein she successfully identified then Massachusetts governor, William Weld to his medical records using publicly accessible records. Her results had a significant impact on privacy centered policymaking including the health privacy legislation HIPAA, however publication of the experiment was rejected twenty times. The several re-identification experiments she conducted after this were met with serious publication challenges as well. In fact, a court ruling in Southern Illinoisian v. Department of Public Health barred her from publication and sharing of her methods for a successful re-identification experiment. Fear of publicly exposing a serious issue with no known solution fueled majority of the backlash against publication of her works and similar re-identification experiments for over a decade. Unless experiments concluded that no risk existed or that the issue could be resolved through reasonable technological advancement, publication was largely denied.\n\nIn her article “Only You, Your Doctor, and Many Others May Know,” writer Latanya Sweeney discusses her research project in which she located and matched up identities and personal health records through a number of methods. Such methods, as she explains in depth later on, include looking at public health records from hospitals and newspaper stories. Towards the end of the article, Sweeney touches upon the different approaches of how she analyzed and matched the data, either through using computer programs or human effort. She then makes the conclusion that new and improved methods of data sharing are necessary.\n\nSince 2011 Sweeney’s Data Privacy Lab has been conducting research about data privacy at Harvard. It intends to provide a cross-disciplinary perspective about privacy in the process of disseminating data. The Data Privacy Lab is sponsored by government, corporate, and nonprofit organizations. It is also in partnership with Berkman Klein Center, the Institute for Quantitative Social Science, Center for Research on Computation and Society, and Program on Informational Sciences. One of her missions of the Data Privacy Lab includes creating a conversation about data in technology and policies on protecting personal data in technology. Sweeney’s Data Privacy Lab is working on 102 different projects regarding data privacy. Some of which include: The Genomic Privacy Project, Discrimination in Online Ad Delivery Project, Privacy-Enhanced Linking Project, and the Identifiability Project. The Genomic Privacy Project attempts to question the privacy of our genetic code and the use of genetic code to identify individuals. The Discrimination in Online Delivery Project examines the possibility of discrimination in the type of ads that show up in a search on a particular individual. There is a possibility that some searches will yield ads that are discriminatory to racial minorities. The Privacy-Enhanced Linking Project attempts to create algorithms in computer coding that will automatically protect privacy in the process of linking—which is the chain of searches that can be traced. The Identifiability Project examines how individuals can be identified through the use public census data. She argues that individuals can be identified using population census data through the combination of zip code, gender, and date of birth.\n\n\n\n"}
{"id": "4263176", "url": "https://en.wikipedia.org/wiki?curid=4263176", "title": "Launch and Early Orbit phase", "text": "Launch and Early Orbit phase\n\nIn spacecraft operations, The Launch and Early Orbit phase (LEOP) is one of the most critical phases of a mission. Spacecraft operations engineers take control of the satellite after it separates from the launch vehicle up to the time when the satellite is safely positioned in its final orbit.\n\nDuring this period, operations staff works 24 hours a day to activate, monitor and control the various subsystems of the satellite, including the deployment of any satellite appendages (antennas, solar array, reflector, etc.), and undertake critical orbit and attitude control manoeuvres.\n\nFor geostationary satellites, the launch vehicle typically carries the spacecraft to Geostationary Transfer Orbit, or GTO. From this elliptical orbit, the LEOP generally includes a sequence of apogee engine firings to reach the circular geostationary orbit.\n\n\n"}
{"id": "26357189", "url": "https://en.wikipedia.org/wiki?curid=26357189", "title": "Lean startup", "text": "Lean startup\n\nLean startup is a methodology for developing businesses and products, which aims to shorten product development cycles and rapidly discover if a proposed business model is viable; this is achieved by adopting a combination of business-hypothesis-driven experimentation, iterative product releases, and validated learning.\n\nCentral to the lean startup methodology is the assumption that when startup companies invest their time into iteratively building products or services to meet the needs of early customers, the company can reduce market risks and sidestep the need for large amounts of initial project funding and expensive product launches and failures. \n\nSimilar to the precepts of lean manufacturing and lean software development, the lean startup methodology seeks to eliminate wasteful practices and increase value-producing practices during the earliest phases of a company so that the company can have a better chance of success without requiring large amounts of outside funding, elaborate business plans, or a perfect product. Customer feedback during the development of products or services is integral to the lean startup process, and ensures that the company does not invest time designing features or services that consumers do not want. This is done primarily through two processes, using key performance indicators and a continuous deployment process.\n\nWhen a startup company cannot afford to have its entire investment depend upon the success of a single product or service, the lean startup methodology proposes that by releasing a minimum viable product that is not yet finalized, the company can then make use of customer feedback to help further tailor the product or service to the specific needs of its customers.\n\nThe lean startup methodology asserts that the \"lean has nothing to do with how much money a company raises\"; rather it has everything to do with assessing the specific demands of consumers and how to meet that demand using the least amount of resources possible.\n\nUse of the word \"lean\" to describe the streamlined production system of lean manufacturing was popularized by the 1990 book \"The Machine That Changed the World\". The Toyota Production System pioneered by Taiichi Ohno combined flow principles that had been used by Henry Ford since the early 1900s with innovations such as the TWI programs introduced to Japan in 1951.\n\nLean manufacturing systems consider as waste the expenditure of resources for any goal other than the creation of value for the end customer, and continually seek ways to eliminate such waste. In particular, such systems focus on:\n\nLean manufacturing was later applied to software as lean software development.\n\nThe lean startup methodology is based on the customer development methodology of Silicon Valley serial entrepreneur-turned-academic Steve Blank. In his book \"The Four Steps to the Epiphany: Successful Strategies for Products that Win\" (2005, 5th edition 2013), Blank pointed out the pitfalls of a narrow emphasis on product development; instead he argued that startups should focus on what he called \"customer development\", which emphasizes \"learning about customers and their problems as early in the development process as possible\". Blank's customer development methodology proposed four steps:\n\n\nIn an article published in the \"Harvard Business Review\" in 2013, Steve Blank described how the lean startup methodology also drew inspiration from the work of people like Ian C. MacMillan and Rita Gunther McGrath who developed a technique called discovery-driven planning, which was an attempt to bring an entrepreneurial mindset to planning.\n\nIn his blog and book \"The Lean Startup\", entrepreneur Eric Ries used specific terms to refer to the core lean startup principles.\n\nA minimum viable product (MVP) is the \"version of a new product which allows a team to collect the maximum amount of validated learning about customers with the least effort\" (similar to a pilot experiment). The goal of an MVP is to test fundamental business hypotheses (or leap-of-faith assumptions) and to help entrepreneurs begin the learning process as quickly as possible.\n\nAs an example, Ries noted that Zappos founder Nick Swinmurn wanted to test the hypothesis that customers were ready and willing to buy shoes online. Instead of building a website and a large database of footwear, Swinmurn approached local shoe stores, took pictures of their inventory, posted the pictures online, bought the shoes from the stores at full price after he'd made a sale, and then shipped them directly to customers. Swinmurn deduced that customer demand was present, and Zappos would eventually grow into a billion dollar business based on the model of selling shoes online.\n\nContinuous deployment, similar to continuous delivery, is a process \"whereby all code that is written for an application is immediately deployed into production,\" which results in a reduction of cycle times. Ries stated that some of the companies he's worked with deploy new code into production as often as 50 times a day. The phrase was coined by Timothy Fitz, one of Ries's colleagues and an early engineer at IMVU.\n\nA split or A/B test is an experiment in which \"different versions of a product are offered to customers at the same time.\" The goal of a split test is to observe differences in behavior between the two groups and to measure the impact of each version on an actionable metric.\n\nA/B testing is sometimes incorrectly performed in serial fashion, where a group of users one week may see one version of the product while the next week users see another. This undermines the statistical validity of the results, since external events may influence user behavior in one time period but not the other. For example, a split test of two ice cream flavors performed in serial during the summer and winter would see a marked decrease in demand during the winter where that decrease is mostly related to the weather and not to the flavor offer.\n\nAnother way to incorrectly A/B test is to assign users to one or another A/B version of the product using any non-random method.\n\nActionable metrics can lead to informed business decisions and subsequent action. These are in contrast to vanity metrics—measurements that give \"the rosiest picture possible\" but do not accurately reflect the key drivers of a business.\n\nVanity metrics for one company may be actionable metrics for another. For example, a company specializing in creating web based dashboards for financial markets might view the number of web page views per person as a vanity metric as their revenue is not based on number of page views. However, an online magazine with advertising would view web page views as a key metric as page views are directly correlated to revenue.\n\nA typical example of a vanity metric is \"the number of new users gained per day\". While a high number of users gained per day seems beneficial to any company, if the cost of acquiring each user through expensive advertising campaigns is significantly higher than the revenue gained per user, then gaining more users could quickly lead to bankruptcy.\n\nA pivot is a \"structured course correction designed to test a new fundamental hypothesis about the product, strategy, and engine of growth.\" A notable example of a company employing the pivot is Groupon; when the company first started, it was an online activism platform called The Point. After receiving almost no traction, the founders opened a WordPress blog and launched their first coupon promotion for a pizzeria located in their building lobby. Although they only received 20 redemptions, the founders realized that their idea was significant, and had successfully empowered people to coordinate group action. Three years later, Groupon would grow into a billion dollar business.\n\nSteve Blank defines a pivot as \"changing (or even firing) the plan instead of the executive (the sales exec, marketing or even the CEO).\"\n\nThis topic focuses on how entrepreneurs can maintain accountability and maximize outcomes by measuring progress, planning milestones, and prioritizing. The topic was later expanded upon to include three levels of innovation accounting related to the types of assumptions being validated.\n\nThe Build–Measure–Learn loop emphasizes speed as a critical ingredient to product development. A team or company's effectiveness is determined by its ability to ideate, quickly build a minimum viable product of that idea, measure its effectiveness in the market, and learn from that experiment. In other words, it's a learning cycle of turning ideas into products, measuring customers' reactions and behaviors against built products, and then deciding whether to persevere or pivot the idea; this process repeats as many times as necessary. The phases of the loop are: Ideas → Build → Product → Measure → Data → Learn.\n\nThis rapid iteration allows teams to discover a feasible path towards product/market fit, and to continue optimizing and refining the business model after reaching product/market fit.\n\nThe Business Model Canvas is a strategic management template invented by Alexander Osterwalder around 2008 for developing new business models or documenting existing ones. It is a visual chart with elements describing a firm's value proposition, infrastructure, customers, and finances. It assists firms in aligning their activities by illustrating potential trade-offs.\n\nThe template consists of nine blocks: activities, partners, resources, value proposition, customers, customer channels, customer relationships, costs and revenue. Startups use the template (and/or other templates described below) to formulate hypotheses and change their business model based on the success or failure of tested hypotheses.\n\nThe Lean Canvas is a version of the Business Model Canvas adapted by Ash Maurya in 2010 specifically for startups. The Lean Canvas focuses on addressing broad customer problems and solutions and delivering them to customer segments through a unique value proposition. \"Problem\" and \"solution\" blocks replace the \"key partners\" and \"key activities\" blocks in the Business Model Canvas, while \"key metrics\" and \"unfair advantage\" blocks replace the \"key resources\" and \"customer relationships\" blocks, respectively.\n\nThe Value Proposition Canvas is a supplement to the Business Model Canvas (\"customer segment\" and \"value proposition\" blocks) published in 2012 to address the customer–product relationship, the perceived value of the product or service, and potential product/market fit. The \"value proposition\" block is divided into three categories—products and services, gain creators, and pain relievers—that correspond to three categories in the \"customer segment\" block—customer jobs, gains, and pains.\n\nThe Mission Model Canvas is a version of the Business Model Canvas developed by Alexander Osterwalder and Steve Blank for entities such as government agencies that have a predetermined budget instead of a goal of raising revenue. It was published in 2016. Earlier publications by Osterwalder and colleagues had suggested how to adapt the Business Model Canvas for nonprofit enterprises that depend on raising revenue. \"Mission budget/cost\" and \"mission achievement/impact factors\" blocks replace the \"cost structure\" and \"revenue streams\" blocks in the Business Model Canvas, while \"beneficiaries\", \"buy-in/support\" and \"deployment\" blocks replace the \"customer segments\", \"customer relationships\" and \"channels\" blocks, respectively.\n\nRies and others created an annual technology conference called Startup Lessons Learned which has subsequently changed its name to the Lean Startup Conference. Lean startup meetups in cities around the world have garnered 20,000 regular participants. The first lean startup meetup named Lean Startup Circle was created by Rich Collins on June 26, 2009 hosting speaking events, workshops, and roundtable discussions. As of 2012, there are lean startup meetups in over 100 cities and 17 countries as well as an online discussion forum with over 5500 members. Third-party organizers have led lean startup meetups in San Francisco, Chicago, Boston, Austin, Beijing, Dublin, and Rio de Janeiro, among others—many of which are personally attended by Ries—with the Chicago and New York City Lean Startup Meetups attracting over 4,000 members each. The Lean Startup Machine created a new spin on the lean startup meetups by having attendees start a new company in three days. As of 2012, the Lean Startup Machine claimed to have created over 600 new startups this way.\n\nProminent high-tech companies have begun to publicly employ the lean startup methodology, including Intuit, Dropbox, Wealthfront, Votizen, Aardvark, and Grockit. The lean startup principles are also taught in classes at Harvard Business School and UC Berkeley and are implemented in municipal governments through Code for America.\n\nAcademic researchers in Finland have applied the lean startup methodology to accelerating research innovation.\n\nThe United States Government has employed lean startup ideas. The Federal Chief Information Officer of the United States, Steven VanRoekel noted in 2012 that he was taking a \"lean-startup approach to government\". Ries has worked with the former and current Chief Technology Officers of the United States—Aneesh Chopra and Todd Park respectively—to implement aspects of the lean startup model. In particular, Park noted that in order to understand customer demand, the Department of Health and Human Services recognized \"the need to rapidly prototype solutions, engage customers in those solutions as soon as possible, and then quickly and repeatedly iterate those solutions based on working with customers\". In May 2012, Ries and The White House announced the Presidential Innovation Fellows program, which brings together top citizen innovators and government officials to work on high-level projects and deliver measurable results in six months.\n\nSteve Blank, working with retired United States Army colonel Pete Newell and former United States Army Special Forces colonel Joe Felter, adapted lean startup principles for U.S. government innovation under the moniker \"Hacking for Defense\", a program in which university students solve problems that the Department of Defense, the United States Armed Forces, and the United States Intelligence Community submit to participating universities. Hacking for Defense and variants like Hacking for Diplomacy have expanded to the United States Department of State, Department of Energy, NASA, and nonprofits.\n\nLean startup principles have been applied to specific competencies within typical startups and larger organizations:\nThe lean startup methodology was first proposed in 2008 by Eric Ries, using his personal experiences adapting lean management and customer development principles to high-tech startup companies. The methodology has since been expanded to apply to any individual, team, or company looking to develop new products, services, or systems without unlimited resources. The lean startup's reputation is due in part to the success of Ries' bestselling book, \"The Lean Startup\", published in September 2011.\n\nRies' said that his first company, Catalyst Recruiting, failed because he and his colleagues did not understand the wants of their target customers, and because they focused too much time and energy on the initial product launch. Next, Ries was a senior software engineer with There, Inc., which Ries described as a classic example of a Silicon Valley startup with five years of stealth R&D, $40 million in financing, and nearly 200 employees at the time of product launch. In 2003, There, Inc. launched its product, There.com, but they were unable to garner popularity beyond the initial early adopters. Ries claimed that despite the many proximate causes for failure, the most important mistake was that the company's \"vision was almost too concrete\", making it impossible to see that their product did not accurately represent consumer demand.\n\nAlthough the lost money differed by orders of magnitude, Ries concluded that the failures of There, Inc. and Catalyst Recruiting shared similar origins: \"it was working forward from the technology instead of working backward from the business results you're trying to achieve.\"\n\nAfter Ries later co-founded IMVU Inc., IMVU investor Steve Blank insisted that IMVU executives audit Blank's class on entrepreneurship at UC Berkeley. Ries applied Blank's customer development methodology and integrated it with ideas from lean software development and elsewhere to form the lean startup methodology.\n\nBen Horowitz, the co-founder of venture capital firm Andreessen Horowitz, wrote an article in 2010 criticizing the lean startup method for over-emphasizing \"running lean\" (constantly cutting and reducing non-essential parts of the company to save time and money). He specifically disagreed with portraying \"running lean\" as an end rather than a means to winning the market without running out of cash. Horowitz gave as an example his startup Loudcloud, which by \"running fat\" was able to outperform 20 direct competitors and after 8 years reach a value of $1.6 billion. However, at least since 2008, numerous advocates of lean methods have pointed out that \"running lean\" does not mean cost cutting.\n\nTrey Griffith, the VP of technology at Teleborder, stated in 2012 that the majority of backing for the lean startup methodology was anecdotal and had not been rigorously validated when first presented. However, he goes on to note that better support of the method comes out of a 2011 analysis of the factors of success in growth companies as described in the 2011 book \"Great by Choice\".\n\nJohn Finneran, a business writer and former user of the lean startup method, described in 2013 a number of the method's assumptions that he did not recognize during his use of the method. In particular, he observed that his clients were often not motivated to invest time and effort into helping iterate a minimal viable product; instead they wanted a more polished product to begin with. Second, he found virtually no early adopters who were willing to try to give feedback on unpolished software simply to be the first to get a chance at it. Third, he argued that lean startup can distract from essential traditional management practices like development discipline and budget protection. In general, he stated that it is important to be critical and skeptical of lean startup methods rather than pre-supposing that they will be effective. Ries had already anticipated this criticism in his book: \"We cannot afford to have our success breed a new pseudoscience around pivots, MVPs, and the like. This was the fate of scientific management, and in the end, I believe, that set back its cause by decades.\" This implies that the concept of validated learning applies to the lean startup methods themselves, and not just to products.\n\n"}
{"id": "43545270", "url": "https://en.wikipedia.org/wiki?curid=43545270", "title": "Lebenslaute", "text": "Lebenslaute\n\nlebenslaute (motto: \"classical music – political action\") is an open direct action group that combines concerts of classical music with civil disobedience, mostly by open-air protest concerts in unexpected locations. \"lebenslaute\" organizes several nonviolence performances per year on both national and regional levels in Germany.\n\nIn 2014, \"lebenslaute\" was awarded the renowned Peace Prize of Aachen, Germany, together with the American organization Code Pink – Women for Peace. It is awarded in the Aula Carolina in the city of Aachen each year on 1 September, the date on which International Day of Peace is celebrated in Germany, warningly commemorating the day on which Nazi Germany started World War II by invading Poland on 1 September 1939.\n\nSince 1986, \"lebenslaute\" has been turning up in places where classical concerts are least expected, with group sizes between three and over one hundred and in professional black outfit. Locations are chosen in the eye of by-laws that would prohibit any activity of this kind in this spot, so political confrontation is consciously sought. Where possible, lebenslaute gives support to and cooperates with local civil disobedience groups. Direct action is deployed in places that are threatening to people or that design and manufacture objects used for killing, with the aim to protest against the official function of such organizations and locations in principle and in practise: any government agency where people are threatened, e.g. refugees, any business of the arms industry, any airport or prison used for deportation, any nuclear power plant, any military base and any other location that may have the aim of preparing for or conducting warfare.\n\nEvery lebenslaute activity is prepared in groups and decisions are taken accordance in Grassroots democracy. Nonviolence as a principle is followed inside the groups, too: None of the activists is asked to do more than they want to. In case of legal proceedings following suit, individuals can be sure to be shown solidarity by other group members and by the group itself in cooperation with allied organizations.\n\nThe first political action by \"lebenslaute\" took place in 1986 when a huge classical orchestra blocked the entrance to the US military base of Pershing II ballistic missiles near the town of Mutlangen in a rural area of Baden-Württemberg, South-Western Germany.\n\n"}
{"id": "56667439", "url": "https://en.wikipedia.org/wiki?curid=56667439", "title": "Mechanisms of the English common law", "text": "Mechanisms of the English common law\n\nIn the English system of common law, judges have devised a number of mechanisms to allow them to cope with precedent decisions.\n\nAccording to Montesquieu, it is Parliament that has the rightful power to legislate, not the judiciary. The legal fiction is that judges do not make law, they merely \"declare it\". Thus, common law is declaratory, and this is often retrospective in effect. For example, see \"Shaw v DPP\" and \"R v Knuller\" In the search for justice and fairness, there is a tension between the needs for, on one hand, predictability and stability, and \"up-to date law\", on the other.\n\nThere is a hierarchy of courts, and a hierarchy of decisions. All lower courts are bound by the judgments from higher courts; and higher courts are not bound by decisions from lower courts.\nWith one exception, courts of record are bound by their own precedent decisions. The House of Lords used to be bound by its own decisions, but in 1966 it issued a Practice Direction declaring that it would no longer feel so constrained; the Supreme Court is similarly free to depart from earlier decisions. By contrast, the Court of Appeal is bound by its own decisions, although for a period Lord Denning, MR, acted as though it were not. Inferior courts are not strictly courts of record, but some, such as employment tribunals methodically report their own cases, and have built up a specialist body of common law. Courts such as the magistrates court cannot establish precedent.\n\nEven if a court is bound to observe a precedent decision, it does not follow that the whole of the judgment is binding. One must distinguish between \"ratio decidendi\" and \"obiter dicta\". \"Ratio decidendi\" is the \"reason for the decision\", and forms the crux of the cases; whereas \"obiter dicta\" is \"other things that are said\", i.e. matters said in passing, judicial asides, hypothetical issues, and broad issues. \"Ratio decidendi\" is binding on other courts, whereas \"obiter dicta\" is persuasive only.\n\nAn effective test to see if a part of the judgment is \"ratio\" or \"obiter\" is \"Wambaugh's Inversion Test\", whereby one must invert the question, and ask, \"would the decision have been different without this part of the judgment?\". In other words, ask, \"Is it crucial?\". If not, it is \"obiter dicta\".\n\nIf a judgment establishes a broad principle of law, then strictly speaking that principle is too wide to be said to be \"ratio decidendi\". Nevertheless, if that broad principle is approved and applied by later courts, then the principle will eventually be treated as \"ratio\". A particular example is the broad \"neighbour principle\", enunciated by Lord Atkin in \"Donoghue v Stevenson\" 1932, which has become the basis of the modern law of negligence. When judges may face conflicting precedents, they may select the preferable case.\n\nDissenting judgments are not \"ratio\", and so must be \"obiter\". Sometimes, with the passage of time, more attention is given to the dissenting judgment that to the majority judgment. Scottish decisions (and decisions from the USA and common law jurisdictions in the Commonwealth) are, like \"obiter dicta\", merely persuasive in England.\n\nIf faced with a binding judicial precedent, a court has a number of ways to respond to it, and may use the following legal devices and mechanisms:\n\n"}
{"id": "862696", "url": "https://en.wikipedia.org/wiki?curid=862696", "title": "Mind over matter", "text": "Mind over matter\n\nMind over matter is a phrase that has been used in several contexts, such as mind-centric spiritual doctrines, parapsychology, and philosophy.\n\nThe phrase \"mind over matter\" first appeared in 1863 in \"The Geological Evidence of the Antiquity of Man\" by Sir Charles Lyell (1797–1875) and was first used to refer to the increasing status and evolutionary growth of the minds of animals and man throughout Earth history. \n\nAnother related saying, \"the mind drives the mass,\" was coined almost two millennia earlier in 19 B.C. by the poet Virgil in his work \"Aeneid\", book 6, line 727.\n\nIn the field of parapsychology, the phrase has been used to describe paranormal phenomena such as psychokinesis.\n\n\"Mind over matter\" was also Mao Zedong's idea that rural peasants could be \"proletarianized\" so they could lead the revolution and China could move from feudalism to socialism through New Democracy. According to some, it departs from Leninism in that the revolutionaries are peasants, instead of the urban proletariat. But others assert that this is a lazy analysis because Mao never disputed the fact that the proletariat should lead, he simply adapted Marxism to the conditions of China.\n\nThe phrase also relates to the ability to control the perception of pain that one may or may not be experiencing.\n"}
{"id": "13957518", "url": "https://en.wikipedia.org/wiki?curid=13957518", "title": "Nutshell", "text": "Nutshell\n\nA nutshell is the outer shell of a nut. Most nutshells are inedible and are removed before eating the nut meat inside.\n\nMost nutshells are useful to some extent, depending on the circumstances. Walnut shells can be used for cleaning and polishing, as a filler in dynamite, and as a paint thickening agent.\nShells from pecans, almonds, Brazil nuts, acorns, and most other nuts are useful in composting.\n\nTheir high porosity makes them also ideal in the production of activated carbon by pyrolysis. \n\nShells can also be used as loose-fill packing material, to protect fragile items in shipping.\n\nThe expression \"in a nutshell\" (of a story, proof, etc.) means \"in essence\", metaphorically alluding to the fact that the essence of the nut - its edible part - is contained inside its shell. The expression further gave rise to the journalistic term \"nut graph\", short for \"nutshell paragraph\".\n\nIn \"Hamlet\" (Act 2, Scene 2) the title character exclaims: \"O God, I could be bounded in a nutshell, and count myself a King of infinite space\".\n\nOlder uses of this have been reported, too. It is said to have been used by Pliny the Elder. He mentioned in the encyclopedic Naturalis historia a report by Cicero saying that a handwritten version of the \"Iliad\" by Homer would have fit in a nut[shell]: \"in nuce inclusam Iliadem Homeri carmen in membrana scriptum tradit Cicero\".\n\n\n"}
{"id": "169407", "url": "https://en.wikipedia.org/wiki?curid=169407", "title": "Pleasure", "text": "Pleasure\n\nPleasure is a broad class of mental states that humans and other animals experience as positive, enjoyable, or worth seeking. It includes more specific mental states such as happiness, entertainment, enjoyment, ecstasy, and euphoria. The early psychological concept of pleasure, the pleasure principle, describes it as a positive feedback mechanism that motivates the organism to recreate the situation it has just found pleasurable, and to avoid past situations that caused pain.\n\nThe experience of pleasure is subjective and different individuals experience different kinds and amounts of pleasure in the same situation. Many pleasurable experiences are associated with satisfying basic biological drives, such as eating, exercise, hygiene, sleep, and sex. The appreciation of cultural artifacts and activities such as art, music, dancing, and literature is often pleasurable.\n\nBased upon the incentive salience model of reward – the attractive and motivational property of a stimulus that induces approach behavior and consummatory behavior – an intrinsic reward has two components: a \"wanting\" or desire component that is reflected in approach behavior, and a \"liking\" or pleasure component that is reflected in consummatory behavior. While all pleasurable stimuli are rewards, some rewards do not evoke pleasure.\n\nPleasure is considered one of the core dimensions of emotion. It can be described as the positive evaluation that forms the basis for several more elaborate evaluations such as \"agreeable\" or \"nice\". As such, pleasure is an affect and not an emotion, as it forms one component of several different emotions. Pleasure is sometimes subdivided into fundamental pleasures that are closely related to survival (food, sex, and social belonging) and higher-order pleasures (e.g., viewing art and altruism). The clinical condition of being unable to experience pleasure from usually enjoyable activities is called anhedonia. An active aversion to obtaining pleasure is called hedonophobia.\n\nPleasure is often regarded as a bipolar construct, meaning that the two ends of the spectrum from pleasant to unpleasant are mutually exclusive. This view is e.g. inherent in the circumplex model of affect. Yet, some lines of research suggest that people do experience pleasant and unpleasant feelings at the same time, giving rise to so-called mixed feelings.\n\nThe degree to which something or someone is experienced as pleasurable not only depends on its objective attributes (appearance, sound, taste, texture, etc.), but on beliefs about its history, about the circumstances of its creation, about its rarity, fame, or price, and on other non-intrinsic attributes, such as the social status or identity it conveys. For example, a sweater that has been worn by a celebrity is more desired than an otherwise identical sweater that has not, though considerably less so if it has been washed. Another example was when Grammy-winning, internationally acclaimed violinist Joshua Bell played in the Washington D.C. subway for 43 minutes, attracting little attention from the 1,097 people who passed by, and earning about $59 in tips. Paul Bloom describes these phenomena as arising from a form of essentialism.\n\nEpicurus and his followers defined the highest pleasure as the absence of suffering and pleasure itself as \"freedom from pain in the body and freedom from turmoil in the soul\". According to Cicero (or rather his character Torquatus) Epicurus also believed that pleasure was the chief good and pain the chief evil.\n\nIn the 12th century Razi's \"Treatise of the Self and the Spirit\" (\"Kitab al Nafs Wa’l Ruh\") analyzed different types of pleasure, sensuous and intellectual, and explained their relations with one another. He concludes that human needs and desires are endless, and \"their satisfaction is by definition impossible.\"\n\nThe 19th-century German philosopher Arthur Schopenhauer understood pleasure as a negative sensation, one that negates the usual existential condition of suffering.\n\nUtilitarianism and hedonism are philosophies that advocate increasing to the maximum the amount of pleasure and minimizing the amount of suffering.\n\nIn the past, there has been debate as to whether pleasure is experienced by other animals rather than being an exclusive property of humankind; however, it is now known that animals do experience pleasure, as measured by objective behavioral and neural hedonic responses to pleasurable stimuli.\n\n\n"}
{"id": "25614", "url": "https://en.wikipedia.org/wiki?curid=25614", "title": "Race (human categorization)", "text": "Race (human categorization)\n\nA race is a grouping of humans based on shared physical or social qualities into categories generally viewed as distinct by society. First used to refer to speakers of a common language and then to denote national affiliations, by the 17th century the term \"race\" began to refer to physical (phenotypical) traits. Modern scholarship regards race as a social construct, that is, a symbolic identity created to establish some cultural meaning. While partially based on physical similarities within groups, race is not an inherent physical or biological quality.\n\nSocial conceptions and groupings of races vary over time, involving folk taxonomies that define of individuals based on perceived traits. Scientists consider biological essentialism obsolete, and generally discourage racial explanations for collective differentiation in both physical and behavioral traits.\n\nEven though there is a broad scientific agreement that essentialist and typological conceptualizations of race are untenable, scientists around the world continue to conceptualize race in widely differing ways, some of which have essentialist implications. While some researchers use the concept of race to make distinctions among fuzzy sets of traits or observable differences in behaviour, others in the scientific community suggest that the idea of race often is used in a naive or simplistic way, and argue that, among humans, race has no taxonomic significance by pointing out that all living humans belong to the same species, \"Homo sapiens\", and (as far as applicable) subspecies, \"Homo sapiens sapiens\".\n\nSince the second half of the 20th century, the association of race with the ideologies and theories of scientific racism has led to the use of the word \"race\" itself becoming problematic. Although still used in general contexts, \"race\" has often been replaced by less ambiguous and loaded terms: \"populations\", \"people(s)\", \"ethnic groups\", or \"communities\", depending on context.\n\nModern scholarship views racial categories as socially constructed, that is, race is not intrinsic to human beings but rather an identity created, often by socially dominant groups, to establish meaning in a social context. This often involves the subjugation of groups defined as racially inferior, as in the one-drop rule used in the 19th-century United States to exclude those with any amount of African ancestry from the dominant racial grouping, defined as \"white\". Such racial identities reflect the cultural attitudes of imperial powers dominant during the age of European colonial expansion. This view rejects the notion that race is biologically defined.\n\nAlthough commonalities in physical traits such as facial features, skin color, and hair texture comprise part of the race concept, the latter is a social distinction rather than an inherently biological one. Other dimensions of racial groupings include shared history, traditions and language. For instance, African-American English is a language spoken by many African Americans, especially in areas of the United States where racial segregation exists. Furthermore, people often self-identify as members of a race for political reasons.\n\nWhen people define and talk about a particular conception of race, they create a social reality through which social categorization is achieved. In this sense, races are said to be social constructs. These constructs develop within various legal, economic, and sociopolitical contexts, and may be the effect, rather than the cause, of major social situations. While race is understood to be a social construct by many, most scholars agree that race has real material effects in the lives of people through institutionalized practices of preference and discrimination.\n\nSocioeconomic factors, in combination with early but enduring views of race, have led to considerable suffering within disadvantaged racial groups. Racial discrimination often coincides with racist mindsets, whereby the individuals and ideologies of one group come to perceive the members of an outgroup as both racially defined and morally inferior. As a result, racial groups possessing relatively little power often find themselves excluded or oppressed, while hegemonic individuals and institutions are charged with holding racist attitudes. Racism has led to many instances of tragedy, including slavery and genocide.\n\nIn some countries, law enforcement uses race to profile suspects. This use of racial categories is frequently criticized for perpetuating an outmoded understanding of human biological variation, and promoting stereotypes. Because in some societies racial groupings correspond closely with patterns of social stratification, for social scientists studying social inequality, race can be a significant variable. As sociological factors, racial categories may in part reflect subjective attributions, self-identities, and social institutions.\n\nScholars continue to debate the degrees to which racial categories are biologically warranted and socially constructed. For example, in 2008, John Hartigan, Jr. argued for a view of race that focused primarily on culture, but which does not ignore the potential relevance of biology or genetics. Accordingly, the racial paradigms employed in different disciplines vary in their emphasis on biological reduction as contrasted with societal construction.\n\nIn the social sciences, theoretical frameworks such as racial formation theory and critical race theory investigate implications of race as social construction by exploring how the images, ideas and assumptions of race are expressed in everyday life. A large body of scholarship has traced the relationships between the historical, social production of race in legal and criminal language, and their effects on the policing and disproportionate incarceration of certain groups.\n\nGroups of humans have always identified themselves as distinct from neighboring groups, but such differences have not always been understood to be natural, immutable and global. These features are the distinguishing features of how the concept of race is used today. In this way the idea of race as we understand it today came about during the historical process of exploration and conquest which brought Europeans into contact with groups from different continents, and of the ideology of classification and typology found in the natural sciences. The term \"race\" was often used in a general biological taxonomic sense, starting from the 19th century, to denote genetically differentiated human populations defined by phenotype.\n\nAccording to Smedley and Marks the European concept of \"race\", along with many of the ideas now associated with the term, arose at the time of the scientific revolution, which introduced and privileged the study of natural kinds, and the age of European imperialism and colonization which established political relations between Europeans and peoples with distinct cultural and political traditions. As Europeans encountered people from different parts of the world, they speculated about the physical, social, and cultural differences among various human groups. The rise of the Atlantic slave trade, which gradually displaced an earlier trade in slaves from throughout the world, created a further incentive to categorize human groups in order to justify the subordination of African slaves.\n\nDrawing on sources from classical antiquity and upon their own internal interactions – for example, the hostility between the English and Irish powerfully influenced early European thinking about the differences between people – Europeans began to sort themselves and others into groups based on physical appearance, and to attribute to individuals belonging to these groups behaviors and capacities which were claimed to be deeply ingrained. A set of folk beliefs took hold that linked inherited physical differences between groups to inherited intellectual, behavioral, and moral qualities. Similar ideas can be found in other cultures, for example in China, where a concept often translated as \"race\" was associated with supposed common descent from the Yellow Emperor, and used to stress the unity of ethnic groups in China. Brutal conflicts between ethnic groups have existed throughout history and across the world.\n\nThe first post-Graeco-Roman published classification of humans into distinct races seems to be François Bernier's \"Nouvelle division de la terre par les différents espèces ou races qui l'habitent\" (\"New division of Earth by the different species or races which inhabit it\"), published in 1684. In the 18th century the differences among human groups became a focus of scientific investigation. But the scientific classification of phenotypic variation was frequently coupled with racist ideas about innate predispositions of different groups, always attributing the most desirable features to the White, European race and arranging the other races along a continuum of progressively undesirable attributes. The 1735 classification of Carl Linnaeus, inventor of zoological taxonomy, divided the human species \"Homo sapiens\" into continental varieties of \"europaeus\", \"asiaticus\", \"americanus\", and \"afer\", each associated with a different humour: sanguine, melancholic, choleric, and phlegmatic, respectively. \"Homo sapiens europaeus\" was described as active, acute, and adventurous, whereas \"Homo sapiens afer\" was said to be crafty, lazy, and careless.\n\nThe 1775 treatise \"The Natural Varieties of Mankind\", by Johann Friedrich Blumenbach proposed five major divisions: the Caucasoid race, the Mongoloid race, the Ethiopian race (later termed \"Negroid\"), the American Indian race, and the Malayan race, but he did not propose any hierarchy among the races. Blumenbach also noted the graded transition in appearances from one group to adjacent groups and suggested that \"one variety of mankind does so sensibly pass into the other, that you cannot mark out the limits between them\".\n\nFrom the 17th through 19th centuries, the merging of folk beliefs about group differences with scientific explanations of those differences produced what Smedley has called an \"ideology of race\". According to this ideology, races are primordial, natural, enduring and distinct. It was further argued that some groups may be the result of mixture between formerly distinct populations, but that careful study could distinguish the ancestral races that had combined to produce admixed groups. Subsequent influential classifications by Georges Buffon, Petrus Camper and Christoph Meiners all classified \"Negros\" as inferior to Europeans. In the United States the racial theories of Thomas Jefferson were influential. He saw Africans as inferior to Whites especially in regards to their intellect, and imbued with unnatural sexual appetites, but described Native Americans as equals to whites.\n\nIn the last two decades of the 18th century, the theory of polygenism, the belief that different races had evolved separately in each continent and shared no common ancestor, was advocated in England by historian Edward Long and anatomist Charles White, in Germany by ethnographers Christoph Meiners and Georg Forster, and in France by Julien-Joseph Virey. In the US, Samuel George Morton, Josiah Nott and Louis Agassiz promoted this theory in the mid-nineteenth century. Polygenism was popular and most widespread in the 19th century, culminating in the founding of the Anthropological Society of London (1863), which, during the period of the American Civil War, broke away from the Ethnological Society of London and its monogenic stance, their underlined difference lying, relevantly, in the so-called \"Negro question\": a substantial racist view by the former, and a more liberal view on race by the latter.\n\nToday, all humans are classified as belonging to the species \"Homo sapiens\". However, this is not the first species of homininae: the first species of genus \"Homo\", \"Homo habilis\", evolved in East Africa at least 2 million years ago, and members of this species populated different parts of Africa in a relatively short time. \"Homo erectus\" evolved more than 1.8 million years ago, and by 1.5 million years ago had spread throughout Europe and Asia. Virtually all physical anthropologists agree that \"Archaic Homo sapiens\" (A group including the possible species \"H. heidelbergensis\", \"H. rhodesiensis\" and \"H. neanderthalensis\") evolved out of African \"Homo erectus\" (\"sensu lato\") or \"Homo ergaster.\" Anthropologists support the idea that anatomically modern humans (\"Homo sapiens\") evolved in North or East Africa from an archaic human species such as \"H. heidelbergensis\" and then migrated out of Africa, mixing with and replacing \"H. heidelbergensis\" and \"H. neanderthalensis\" populations throughout Europe and Asia, and \"H. rhodesiensis\" populations in Sub-Saharan Africa (a combination of the Out of Africa and Multiregional models).\n\nIn the early 20th century, many anthropologists taught that race was an entirely biologically phenomenon and that this was core to a person's behavior and identity, a position commonly called racial essentialism. This, coupled with a belief that linguistic, cultural, and social groups fundamentally existed along racial lines, formed the basis of what is now called scientific racism. After the Nazi eugenics program, along with the rise of anti-colonial movements, racial essentialism lost widespread popularity. New studies of culture and the fledgling field of population genetics undermined the scientific standing of racial essentialism, leading race anthropologists to revise their conclusions about the sources of phenotypic variation. A significant number of modern anthropologists and biologists in the West came to view race as an invalid genetic or biological designation.\n\nThe first to challenge the concept of race on empirical grounds were the anthropologists Franz Boas, who provided evidence of phenotypic plasticity due to environmental factors, and Ashley Montagu, who relied on evidence from genetics. E. O. Wilson then challenged the concept from the perspective of general animal systematics, and further rejected the claim that \"races\" were equivalent to \"subspecies\".\n\nHuman genetic variation is predominantly within races, continuous, and complex in structure, which is inconsistent with the concept of genetic human races. According to Jonathan Marks,\n\nThe term \"race\" in biology is used with caution because it can be ambiguous. Generally, when it is used it is effectively a synonym of \"subspecies\". (For animals, the only taxonomic unit below the species level is usually the subspecies; there are narrower infraspecific ranks in botany, and \"race\" does not correspond directly with any of them.) Traditionally, subspecies are seen as geographically isolated and genetically differentiated populations. Studies of human genetic variation show that human populations are not geographically isolated, and their genetic differences are far smaller than those among comparable subspecies.\n\nIn 1978, Sewall Wright suggested that human populations that have long inhabited separated parts of the world should, in general, be considered different subspecies by the criterion that most individuals of such populations can be allocated correctly by inspection. Wright argued that, \"It does not require a trained anthropologist to classify an array of Englishmen, West Africans, and Chinese with 100% accuracy by features, skin color, and type of hair despite so much variability within each of these groups that every individual can easily be distinguished from every other.\" While in practice subspecies are often defined by easily observable physical appearance, there is not necessarily any evolutionary significance to these observed differences, so this form of classification has become less acceptable to evolutionary biologists. Likewise this typological approach to race is generally regarded as discredited by biologists and anthropologists.\n\nSome researchers have tried to clarify the idea of race by equating it to the biological idea of the clade. A clade is a taxonomic group of organisms consisting of a single common ancestor and all the descendants of that ancestor (a monophyletic group). Every creature produced by sexual reproduction has two immediate lineages, one maternal and one paternal. Whereas Carl Linnaeus established a taxonomy of living organisms based on anatomical similarities and differences, cladistics seeks to establish a taxonomy – the phylogenetic tree – based on genetic similarities and differences and tracing the process of acquisition of multiple characteristics by single organisms.\n\nPhilosopher Robin Andreasen (2000) proposes that cladistics can be used to categorize human races biologically, and that races can be both biologically real and socially constructed. Andreasen cites tree diagrams of relative genetic distances among populations published by Luigi Cavalli-Sforza as the basis for a phylogenetic tree of human races: \"Cavalli-Sforza's research illustrates that it is possible to reconstruct human evolutionary history, and this means that it is possible to provide a cladistic definition of race\" (p. S661). Evolutionary biologist Alan Templeton (2013) argues that while \"Much of the recent scientific literature on human evolution portrays human populations as separate branches on an evolutionary tree,\" multiple lines of evidence falsify a phylogenetic tree structure, and confirm the presence of gene flow among populations. Jonathan Marks (2008) argues that Andreasen has misinterpreted the genetic literature: \"These trees are phenetic (based on similarity), rather than cladistic (based on based on monophyletic descent, that is from a series of unique ancestors).\" Marks, Templeton, and Cavalli-Sforza all conclude that genetics does not provide evidence of human races.\n\nAnthropologists Lieberman and Jackson (1995) also critique the use of cladistics to support concepts of race. They claim that \"the molecular and biochemical proponents of this model explicitly use racial categories \"in their initial grouping of samples\"\". For example, the large and highly diverse macroethnic groups of East Indians, North Africans, and Europeans are presumptively grouped as Caucasians prior to the analysis of their DNA variation. This is claimed to limit and skew interpretations, obscure other lineage relationships, deemphasize the impact of more immediate clinal environmental factors on genomic diversity, and can cloud our understanding of the true patterns of affinity. They suggest that the authors of these studies find support for racial distinctions only because they began by assuming the validity of race. \"For empirical reasons we prefer to place emphasis on clinal variation, which recognizes the existence of adaptive human hereditary variation and simultaneously stresses that such variation is not found in packages that can be labeled \"races\".\"\n\nHuman population groups are not monophyletic, as there appears to always have been considerable gene flow between human populations. Keith Hunley, Graciela Cabana, and Jeffrey Long analyzed the Human Genome Diversity Project sample of 1,037 individuals in 52 populations. They found that non-African populations are a taxonomic subgroup of African populations, that \"some African populations are equally related to other African populations and to non-African populations,\" and that \"outside of Africa, regional groupings of populations are nested inside one another, and many of them are not monophyletic.\" Rachel Caspari (2003) has argued that since no groups currently regarded as races are monophyletic, none of those groups can be clades.\n\nPopulation geneticists have debated whether the concept of \"population\" can provide a basis for a new conception of race. To do this, a working definition of population must be found. Surprisingly, there is no generally accepted concept of population that biologists use. Although the concept of population is central to ecology, evolutionary biology and conservation biology, most definitions of population rely on qualitative descriptions such as \"a group of organisms of the same species occupying a particular space at a particular time\". Waples and Gaggiotti identify two broad types of definitions for populations; those that fall into an \"ecological paradigm\", and those that fall into an \"evolutionary paradigm\". Examples of such definitions are:\n\nSesardic argues that when several traits are analyzed at the same time, forensic anthropologists can classify a person's race with an accuracy of close to 100% based on only skeletal remains. Sesardic's claim has been disputed by Massimo Pigliucci, who accused Sesardic of \"cherry pick[ing] the scientific evidence and reach[ing] conclusions that are contradicted by it.\" Specifically, Pigliucci argues that Sesardic misrepresented a paper by Ousley et al. (2009), and neglected to mention that they identified differentiation not just between individuals from different races, but also between individuals from different tribes, local environments, and time periods. This is discussed in a later section.\n\nOne crucial innovation in reconceptualizing genotypic and phenotypic variation was the anthropologist C. Loring Brace's observation that such variations, insofar as it is affected by natural selection, slow migration, or genetic drift, are distributed along geographic gradations or clines. For example, with respect to skin color in Europe and Africa, Brace writes:To this day, skin color grades by imperceptible means from Europe southward around the eastern end of the Mediterranean and up the Nile into Africa. From one end of this range to the other, there is no hint of a skin color boundary, and yet the spectrum runs from the lightest in the world at the northern edge to as dark as it is possible for humans to be at the equator.In part this is due to isolation by distance. This point called attention to a problem common to phenotype-based descriptions of races (for example, those based on hair texture and skin color): they ignore a host of other similarities and differences (for example, blood type) that do not correlate highly with the markers for race. Thus, anthropologist Frank Livingstone's conclusion, that since clines cross racial boundaries, \"there are no races, only clines\".\n\nIn a response to Livingstone, Theodore Dobzhansky argued that when talking about race one must be attentive to how the term is being used: \"I agree with Dr. Livingstone that if races have to be 'discrete units', then there are no races, and if 'race' is used as an 'explanation' of the human variability, rather than vice versa, then the explanation is invalid.\" He further argued that one could use the term race if one distinguished between \"race differences\" and \"the race concept\". The former refers to any distinction in gene frequencies between populations; the latter is \"a matter of judgment\". He further observed that even when there is clinal variation, \"Race differences are objectively ascertainable biological phenomena ... but it does not follow that racially distinct populations must be given racial (or subspecific) labels.\" In short, Livingstone and Dobzhansky agree that there are genetic differences among human beings; they also agree that the use of the race concept to classify people, and how the race concept is used, is a matter of social convention. They differ on whether the race concept remains a meaningful and useful social convention.\n\nIn 1964, the biologists Paul Ehrlich and Holm pointed out cases where two or more clines are distributed discordantly – for example, melanin is distributed in a decreasing pattern from the equator north and south; frequencies for the haplotype for beta-S hemoglobin, on the other hand, radiate out of specific geographical points in Africa. As the anthropologists Leonard Lieberman and Fatimah Linda Jackson observed, \"Discordant patterns of heterogeneity falsify any description of a population as if it were genotypically or even phenotypically homogeneous\".\n\nPatterns such as those seen in human physical and genetic variation as described above, have led to the consequence that the number and geographic location of any described races is highly dependent on the importance attributed to, and quantity of, the traits considered. Scientists discovered a skin-lighting mutation that partially accounts for the appearance of Light skin in humans (people who migrated out of Africa northward into what is now Europe) which they estimate occurred 20,000 to 50,000 years ago. The East Asians owe their relatively light skin to different mutations. On the other hand, the greater the number of traits (or alleles) considered, the more subdivisions of humanity are detected, since traits and gene frequencies do not always correspond to the same geographical location. Or as put it:\n\nAnother way to look at differences between populations is to measure genetic differences rather than physical differences between groups. The mid-20th-century anthropologist William C. Boyd defined race as: \"A population which differs significantly from other populations in regard to the frequency of one or more of the genes it possesses. It is an arbitrary matter which, and how many, gene loci we choose to consider as a significant 'constellation'\". Leonard Lieberman and Rodney Kirk have pointed out that \"the paramount weakness of this statement is that if one gene can distinguish races then the number of races is as numerous as the number of human couples reproducing.\" Moreover, the anthropologist Stephen Molnar has suggested that the discordance of clines inevitably results in a multiplication of races that renders the concept itself useless. The Human Genome Project states \"People who have lived in the same geographic region for many generations may have some alleles in common, but no allele will be found in all members of one population and in no members of any other.\" Massimo Pigliucci and Jonathan Kaplan argue that human races do exist, and that they correspond to the genetic classification of ecotypes, but that real human races do not correspond very much, if at all, to folk racial categories. In contrast, Walsh & Yun reviewed the literature in 2011 and reported that \"Genetic studies using very few chromosomal loci find that genetic polymorphisms divide human populations into clusters with almost 100 percent accuracy and that they correspond to the traditional anthropological categories.\"\n\nSome biologists argue that racial categories correlate with biological traits (e.g. phenotype), and that certain genetic markers have varying frequencies among human populations, some of which correspond more or less to traditional racial groupings. For this reason, there is no current consensus about whether racial categories can be considered to have significance for understanding human genetic variation.\n\nThe distribution of genetic variants within and among human populations are impossible to describe succinctly because of the difficulty of defining a population, the clinal nature of variation, and heterogeneity across the genome (Long and Kittles 2003). In general, however, an average of 85% of statistical genetic variation exists within local populations, ~7% is between local populations within the same continent, and ~8% of variation occurs between large groups living on different continents. The recent African origin theory for humans would predict that in Africa there exists a great deal more diversity than elsewhere and that diversity should decrease the further from Africa a population is sampled. Hence, the 85% average figure is misleading: Long and Kittles find that rather than 85% of human genetic diversity existing in all human populations, about 100% of human diversity exists in a single African population, whereas only about 60% of human genetic diversity exists in the least diverse population they analyzed (the Surui, a population derived from New Guinea). Statistical analysis that takes this difference into account confirms previous findings that, \"Western-based racial classifications have no taxonomic significance.\"\n\nA 2002 study of random biallelic genetic loci found little to no evidence that humans were divided into distinct biological groups.\n\nIn his 2003 paper, \"\", A. W. F. Edwards argued that rather than using a locus-by-locus analysis of variation to derive taxonomy, it is possible to construct a human classification system based on characteristic genetic patterns, or \"clusters\" inferred from multilocus genetic data. Geographically based human studies since have shown that such genetic clusters can be derived from analyzing of a large number of loci which can assort individuals sampled into groups analogous to traditional continental racial groups. Joanna Mountain and Neil Risch cautioned that while genetic clusters may one day be shown to correspond to phenotypic variations between groups, such assumptions were premature as the relationship between genes and complex traits remains poorly understood. However, Risch denied such limitations render the analysis useless: \"Perhaps just using someone's actual birth year is not a very good way of measuring age. Does that mean we should throw it out? ... Any category you come up with is going to be imperfect, but that doesn't preclude you from using it or the fact that it has utility.\"\n\nEarly human genetic cluster analysis studies were conducted with samples taken from ancestral population groups living at extreme geographic distances from each other. It was thought that such large geographic distances would maximize the genetic variation between the groups sampled in the analysis, and thus maximize the probability of finding cluster patterns unique to each group. In light of the historically recent acceleration of human migration (and correspondingly, human gene flow) on a global scale, further studies were conducted to judge the degree to which genetic cluster analysis can pattern ancestrally identified groups as well as geographically separated groups. One such study looked at a large multiethnic population in the United States, and \"detected only modest genetic differentiation between different current geographic locales within each race/ethnicity group. Thus, ancient geographic ancestry, which is highly correlated with self-identified race/ethnicity – as opposed to current residence – is the major determinant of genetic structure in the U.S. population.\" ()\n\nAnthropologists such as C. Loring Brace, the philosophers Jonathan Kaplan and Rasmus Winther, and the geneticist Joseph Graves, have argued that while there it is certainly possible to find biological and genetic variation that corresponds roughly to the groupings normally defined as \"continental races\", this is true for almost all geographically distinct populations. The cluster structure of the genetic data is therefore dependent on the initial hypotheses of the researcher and the populations sampled. When one samples continental groups, the clusters become continental; if one had chosen other sampling patterns, the clustering would be different. Weiss and Fullerton have noted that if one sampled only Icelanders, Mayans and Maoris, three distinct clusters would form and all other populations could be described as being clinally composed of admixtures of Maori, Icelandic and Mayan genetic materials. Kaplan and Winther therefore argue that, seen in this way, both Lewontin and Edwards are right in their arguments. They conclude that while racial groups are characterized by different allele frequencies, this does not mean that racial classification is a natural taxonomy of the human species, because multiple other genetic patterns can be found in human populations that crosscut racial distinctions. Moreover, the genomic data underdetermines whether one wishes to see subdivisions (i.e., splitters) or a continuum (i.e., lumpers). Under Kaplan and Winther's view, racial groupings are objective social constructions (see Mills 1998) that have conventional biological reality only insofar as the categories are chosen and constructed for pragmatic scientific reasons. In earlier work, Winther had identified \"diversity partitioning\" and \"clustering analysis\" as two separate methodologies, with distinct questions, assumptions, and protocols. Each is also associated with opposing ontological consequences vis-a-vis the metaphysics of race. Philosopher Lisa Gannett has argued that biogeographical ancestry, a concept devised by Mark Shriver and Tony Frudakis, is not an objective measure of the biological aspects of race as Shriver and Frudakis claim it is. She argues that it is actually just a \"local category shaped by the U.S. context of its production, especially the forensic aim of being able to predict the race or ethnicity of an unknown suspect based on DNA found at the crime scene.\"\n\nRecent studies of human genetic clustering have included a debate over how genetic variation is organized, with clusters and clines as the main possible orderings. argued for smooth, clinal genetic variation in ancestral populations even in regions previously considered racially homogeneous, with the apparent gaps turning out to be artifacts of sampling techniques. disputed this and offered an analysis of the Human Genetic Diversity Panel showing that there were small discontinuities in the smooth genetic variation for ancestral populations at the location of geographic barriers such as the Sahara, the Oceans, and the Himalayas. Nonetheless, stated that their findings “should not be taken as evidence of our support of any particular concept of biological race... Genetic differences among human populations derive mainly from gradations in allele frequencies rather than from distinctive 'diagnostic' genotypes.\" Using a sample of 40 populations distributed roughly evenly across the Earth's land surface, found that \"genetic diversity is distributed in a more clinal pattern when more geographically intermediate populations are sampled.\"\n\nGuido Barbujani has written that human genetic variation is generally distributed continuously in gradients across much of Earth, and that there is no evidence that genetic boundaries between human populations exist as would be necessary for human races to exist.\n\nOver time, human genetic variation has formed a nested structure that is inconsistent with the concept of races that have evolved independently of one another.\n\nAs anthropologists and other evolutionary scientists have shifted away from the language of race to the term \"population\" to talk about genetic differences, historians, cultural anthropologists and other social scientists re-conceptualized the term \"race\" as a cultural category or social construct, i.e., a way among many possible ways in which a society chooses to divide its members into categories.\n\nMany social scientists have replaced the word race with the word \"ethnicity\" to refer to self-identifying groups based on beliefs concerning shared culture, ancestry and history. Alongside empirical and conceptual problems with \"race\", following the Second World War, evolutionary and social scientists were acutely aware of how beliefs about race had been used to justify discrimination, apartheid, slavery, and genocide. This questioning gained momentum in the 1960s during the civil rights movement in the United States and the emergence of numerous anti-colonial movements worldwide. They thus came to believe that race itself is a social construct, a concept that was believed to correspond to an objective reality but which was believed in because of its social functions.\n\nCraig Venter and Francis Collins of the National Institute of Health jointly made the announcement of the mapping of the human genome in 2000. Upon examining the data from the genome mapping, Venter realized that although the genetic variation within the human species is on the order of 1–3% (instead of the previously assumed 1%), the types of variations do not support notion of genetically defined races. Venter said, \"Race is a social concept. It's not a scientific one. There are no bright lines (that would stand out), if we could compare all the sequenced genomes of everyone on the planet.\" \"When we try to apply science to try to sort out these social differences, it all falls apart.\"\n\nStephan Palmié asserted that race \"is not a thing but a social relation\"; or, in the words of Katya Gibel Mevorach, \"a metonym\", \"a human invention whose criteria for differentiation are neither universal nor fixed but have always been used to manage difference.\" As such, the use of the term \"race\" itself must be analyzed. Moreover, they argue that biology will not explain why or how people use the idea of race: History and social relationships will.\n\nImani Perry has argued that race \"is produced by social arrangements and political decision making.\" Perry explains race more in stating, \"race is something that happens, rather than something that is. It is dynamic, but it holds no objective truth.\"\n\nSome scholars have challenged the notion that race is primarily a social construction by arguing that race has a biological basis. One of the researchers, Neil Risch, noted: \"we looked at the correlation between genetic structure [based on microsatellite markers] versus self-description, we found 99.9% concordance between the two. We actually had a higher discordance rate between self-reported sex and markers on the X chromosome! So you could argue that sex is also a problematic category. And there are differences between sex and gender; self-identification may not be correlated with biology perfectly. And there is sexism.\"\n\nCompared to 19th-century United States, 20th-century Brazil was characterized by a perceived relative absence of sharply defined racial groups. According to anthropologist Marvin Harris, this pattern reflects a different history and different social relations.\n\nBasically, race in Brazil was \"biologized\", but in a way that recognized the difference between ancestry (which determines genotype) and phenotypic differences. There, racial identity was not governed by rigid descent rule, such as the one-drop rule, as it was in the United States. A Brazilian child was never automatically identified with the racial type of one or both parents, nor were there only a very limited number of categories to choose from, to the extent that full siblings can pertain to different racial groups.\n\nOver a dozen racial categories would be recognized in conformity with all the possible combinations of hair color, hair texture, eye color, and skin color. These types grade into each other like the colors of the spectrum, and not one category stands significantly isolated from the rest. That is, race referred preferentially to appearance, not heredity, and appearance is a poor indication of ancestry, because only a few genes are responsible for someone's skin color and traits: a person who is considered white may have more African ancestry than a person who is considered black, and the reverse can be also true about European ancestry. The complexity of racial classifications in Brazil reflects the extent of miscegenation in Brazilian society, a society that remains highly, but not strictly, stratified along color lines. These socioeconomic factors are also significant to the limits of racial lines, because a minority of \"pardos\", or brown people, are likely to start declaring themselves white or black if socially upward, and being seen as relatively \"whiter\" as their perceived social status increases (much as in other regions of Latin America).\n\nFluidity of racial categories aside, the \"biologification\" of race in Brazil referred above would match contemporary concepts of race in the United States quite closely, though, if Brazilians are supposed to choose their race as one among, Asian and Indigenous apart, three IBGE's census categories. While assimilated Amerindians and people with very high quantities of Amerindian ancestry are usually grouped as \"caboclos\", a subgroup of \"pardos\" which roughly translates as both mestizo and hillbilly, for those of lower quantity of Amerindian descent a higher European genetic contribution is expected to be grouped as a \"pardo\". In several genetic tests, people with less than 60-65% of European descent and 5–10% of Amerindian descent usually cluster with Afro-Brazilians (as reported by the individuals), or 6.9% of the population, and those with about 45% or more of Subsaharan contribution most times do so (in average, Afro-Brazilian DNA was reported to be about 50% Subsaharan African, 37% European and 13% Amerindian).\n\nIf a more consistent report with the genetic groups in the gradation of miscegenation is to be considered (e.g. that would not cluster people with a balanced degree of African and non-African ancestry in the black group instead of the multiracial one, unlike elsewhere in Latin America where people of high quantity of African descent tend to classify themselves as mixed), more people would report themselves as white and \"pardo\" in Brazil (47.7% and 42.4% of the population as of 2010, respectively), because by research its population is believed to have between 65 and 80% of autosomal European ancestry, in average (also >35% of European mt-DNA and >95% of European Y-DNA).\n\nFrom the last decades of the Empire until the 1950s, the proportion of the white population increased significantly while Brazil welcomed 5.5 million immigrants between 1821 and 1932, not much behind its neighbor Argentina with 6.4 million, and it received more European immigrants in its colonial history than the United States. Between 1500 and 1760, 700.000 Europeans settled in Brazil, while 530.000 Europeans settled in the United States for the same given time. Thus, the historical construction of race in Brazilian society dealt primarily with gradations between persons of majority European ancestry and little minority groups with otherwise lower quantity therefrom in recent times.\n\nAccording to European Council:\n\nThe European Union uses the terms racial origin and ethnic origin synonymously in its documents and according to it \"the use of the term 'racial origin' in this directive does not imply an acceptance of such [racial] theories\". Haney López warns that using \"race\" as a category within the law tends to legitimize its existence in the popular imagination. In the diverse geographic context of Europe, ethnicity and ethnic origin are arguably more resonant and are less encumbered by the ideological baggage associated with \"race\". In European context, historical resonance of \"race\" underscores its problematic nature. In some states, it is strongly associated with laws promulgated by the Nazi and Fascist governments in Europe during the 1930s and 1940s. Indeed, in 1996, the European Parliament adopted a resolution stating that \"the term should therefore be avoided in all official texts\".\n\nThe concept of racial origin relies on the notion that human beings can be separated into biologically distinct \"races\", an idea generally rejected by the scientific community. Since all human beings belong to the same species, the ECRI (European Commission against Racism and Intolerance) rejects theories based on the existence of different \"races\". However, in its Recommendation ECRI uses this term in order to ensure that those persons who are generally and erroneously perceived as belonging to \"another race\" are not excluded from the protection provided for by the legislation. The law claims to reject the existence of \"race\", yet penalize situations where someone is treated less favourably on this ground.\n\nSince the end of the Second World War, France has become an ethnically diverse country. Today, approximately five percent of the French population is non-European and non-white. This does not approach the number of non-white citizens in the United States (roughly 28–37%, depending on how Latinos are classified (see Demographics of the United States). Nevertheless, it amounts to at least three million people, and has forced the issues of ethnic diversity onto the French policy agenda. France has developed an approach to dealing with ethnic problems that stands in contrast to that of many advanced, industrialized countries. Unlike the United States, Britain, or even the Netherlands, France maintains a \"color-blind\" model of public policy. This means that it targets virtually no policies directly at racial or ethnic groups. Instead, it uses geographic or class criteria to address issues of social inequalities. It has, however, developed an extensive anti-racist policy repertoire since the early 1970s. Until recently, French policies focused primarily on issues of hate speech – going much further than their American counterparts – and relatively less on issues of discrimination in jobs, housing, and in provision of goods and services.\n\nIn the United States, there is disagreement on the nature of race within the biological sciences, whereas the social constructionist view is dominant in the social sciences; over time, biological views on race have become more controversial across all disciplines, with clear divides along generational, cultural, and racial lines.\n\nThe immigrants to the Americas came from every region of Europe, Africa, and Asia. They mixed among themselves and with the indigenous inhabitants of the continent. In the United States most people who self-identify as African–American have some European ancestors, while many people who identify as European American have some African or Amerindian ancestors.\n\nSince the early history of the United States, Amerindians, African–Americans, and European Americans have been classified as belonging to different races. Efforts to track mixing between groups led to a proliferation of categories, such as mulatto and octoroon. The criteria for membership in these races diverged in the late 19th century. During Reconstruction, increasing numbers of Americans began to consider anyone with \"one drop\" of known \"Black blood\" to be Black, regardless of appearance. By the early 20th century, this notion was made statutory in many states. Amerindians continue to be defined by a certain percentage of \"Indian blood\" (called \"blood quantum\"). To be White one had to have perceived \"pure\" White ancestry. The one-drop rule or hypodescent rule refers to the convention of defining a person as racially black if he or she has any known African ancestry. This rule meant that those that were mixed race but with some discernible African ancestry were defined as black. The one-drop rule is specific to not only those with African ancestry but to the United States, making it a particularly African-American experience.\n\nThe decennial censuses conducted since 1790 in the United States created an incentive to establish racial categories and fit people into these categories.\n\nThe term \"Hispanic\" as an ethnonym emerged in the 20th century with the rise of migration of laborers from the Spanish-speaking countries of Latin America to the United States. Today, the word \"Latino\" is often used as a synonym for \"Hispanic\". The definitions of both terms are non-race specific, and include people who consider themselves to be of distinct races (Black, White, Amerindian, Asian, and mixed groups). However, there is a common misconception in the US that Hispanic/Latino is a race or sometimes even that national origins such as Mexican, Cuban, Colombian, Salvadoran, etc. are races. In contrast to \"Latino\" or \"Hispanic\", \"Anglo\" refers to non-Hispanic White Americans or non-Hispanic European Americans, most of whom speak the English language but are not necessarily of English descent.\n\nOne result of debates over the meaning and validity of the concept of race is that the current literature across different disciplines regarding human variation lacks consensus, though within some fields, such as some branches of anthropology, there is strong consensus. Some studies use the word race in its early essentialist taxonomic sense. Many others still use the term race, but use it to mean a population, clade, or haplogroup. Others eschew the concept of race altogether, and use the concept of population as a less problematic unit of analysis.\n\nEduardo Bonilla-Silva, Sociology professor at Duke University, remarks, \"I contend that racism is, more than anything else, a matter of group power; it is about a dominant racial group (whites) striving to maintain its systemic advantages and minorities fighting to subvert the racial status quo.\" The types of practices that take place under this new color-blind racism is subtle, institutionalized, and supposedly not racial. Color-blind racism thrives on the idea that race is no longer an issue in the United States. There are contradictions between the alleged color-blindness of most whites and the persistence of a color-coded system of inequality. In Poland, the race concept was rejected by 25 percent of anthropologists in 2001, although: \"Unlike the U.S. anthropologists, Polish anthropologists tend to regard race as a term without taxonomic value, often as a substitute for population.\"\n\nThe concept of typological race classification in physical anthropology lost credibility around the 1960s and is now considered untenable.\n\nWagner et al. (2017) surveyed 3,286 American anthropologists' views on race and genetics, including both cultural and biological andthropologists. They found a consensus among them that biological races do not exist in humans, but that race does exist insofar as the social experiences of members of different races can have significant effects on health.\n\nWang, Štrkalj et al. (2003) examined the use of race as a biological concept in research papers published in China's only biological anthropology journal, \"Acta Anthropologica Sinica\". The study showed that the race concept was widely used among Chinese anthropologists. In a 2007 review paper, Štrkalj suggested that the stark contrast of the racial approach between the United States and China was due to the fact that race is a factor for social cohesion among the ethnically diverse people of China, whereas \"race\" is a very sensitive issue in America and the racial approach is considered to undermine social cohesion – with the result that in the socio-political context of US academics scientists are encouraged not to use racial categories, whereas in China they are encouraged to use them.\n\nLieberman et al. in a 2004 study researched the acceptance of race as a concept among anthropologists in the United States, Canada, the Spanish speaking areas, Europe, Russia and China. Rejection of race ranged from high to low, with the highest rejection rate in the United States and Canada, a moderate rejection rate in Europe, and the lowest rejection rate in Russia and China. Methods used in the studies reported included questionnaires and content analysis.\n\nKaszycka et al. (2009) in 2002–2003 surveyed European anthropologists' opinions toward the biological race concept. Three factors, country of academic education, discipline, and age, were found to be significant in differentiating the replies. Those educated in Western Europe, physical anthropologists, and middle-aged persons rejected race more frequently than those educated in Eastern Europe, people in other branches of science, and those from both younger and older generations.\" The survey shows that the views on race are sociopolitically (ideologically) influenced and highly dependent on education.\"\n\nSince the second half of the 20th century, physical anthropology in the United States has moved away from a typological understanding of human biological diversity towards a genomic and population-based perspective. Anthropologists have tended to understand race as a social classification of humans based on phenotype and ancestry as well as cultural factors, as the concept is understood in the social sciences. Since 1932, an increasing number of college textbooks introducing physical anthropology have rejected race as a valid concept: from 1932 to 1976, only seven out of thirty-two rejected race; from 1975 to 1984, thirteen out of thirty-three rejected race; from 1985 to 1993, thirteen out of nineteen rejected race. According to one academic journal entry, where 78 percent of the articles in the 1931 \"Journal of Physical Anthropology\" employed these or nearly synonymous terms reflecting a bio-race paradigm, only 36 percent did so in 1965, and just 28 percent did in 1996.\n\nThe \"Statement on 'Race'\" (1998) composed by a select committee of anthropologists and issued by the executive board of the American Anthropological Association as a statement they \"believe [...] represents generally the contemporary thinking and scholarly positions of a majority of anthropologists\", declares:\n\nA survey, taken in 1985 , asked 1,200 American scientists how many disagree with the following proposition: \"There are biological races in the species \"Homo sapiens\".\" The responses were for anthropologists:\nThe figure for physical anthropologists at PhD granting departments was slightly higher, rising from 41% to 42%, with 50% agreeing. Lieberman's study also showed that more women reject the concept of race than men. This survey, however, did not specify any particular definition of race (although it did clearly specify \"biological race\" within the \"species\" \"Homo sapiens\"); it is difficult to say whether those who supported the statement thought of race in taxonomic or population terms.\n\nThe same survey, taken in 1999, showed the following changing results for anthropologists:\n\nHowever, a line of research conducted by Cartmill (1998) seemed to limit the scope of Lieberman's finding that there was \"a significant degree of change in the status of the race concept\". Goran Štrkalj has argued that this may be because Lieberman and collaborators had looked at all the members of the American Anthropological Association irrespective of their field of research interest, while Cartmill had looked specifically at biological anthropologists interested in human variation.\n\nAccording to the 2000 edition of a popular physical anthropology textbook, forensic anthropologists are overwhelmingly in support of the idea of the basic biological reality of human races. Forensic physical anthropologist and professor George W. Gill has said that the idea that race is only skin deep \"is simply not true, as any experienced forensic anthropologist will affirm\" and \"Many morphological features tend to follow geographic boundaries coinciding often with climatic zones. This is not surprising since the selective forces of climate are probably the primary forces of nature that have shaped human races with regard not only to skin color and hair form but also the underlying bony structures of the nose, cheekbones, etc. (For example, more prominent noses humidify air better.)\" While he can see good arguments for both sides, the complete denial of the opposing evidence \"seems to stem largely from socio-political motivation and not science at all\". He also states that many biological anthropologists see races as real yet \"not one introductory textbook of physical anthropology even presents that perspective as a possibility. In a case as flagrant as this, we are not dealing with science but rather with blatant, politically motivated censorship\".\n\nIn partial response to Gill's statement, Professor of Biological Anthropology C. Loring Brace argues that the reason laymen and biological anthropologists can determine the geographic ancestry of an individual can be explained by the fact that biological characteristics are clinally distributed across the planet, and that does not translate into the concept of race. He states: \n\"Race\" is still sometimes used within forensic anthropology (when analyzing skeletal remains), biomedical research, and race-based medicine. Brace has criticized this, the practice of forensic anthropologists for using the controversial concept \"race\" out of convention when they in fact should be talking about regional ancestry. He argues that while forensic anthropologists can determine that a skeletal remain comes from a person with ancestors in a specific region of Africa, categorizing that skeletal as being \"black\" is a socially constructed category that is only meaningful in the particular context of the United States, and which is not itself scientifically valid.\n\nIn 2007, Ann Morning interviewed over 40 American biologists and anthropologists and found significant disagreements over the nature of race, with no one viewpoint holding a majority among either group. Morning also argues that a third position, \"antiessentialism\", which holds that race is not a useful concept for biologists, should be introduced into this debate in addition to \"constructionism\" and \"essentialism\".\n\nIn the same 1985 survey , 16% of the surveyed biologists and 36% of the surveyed developmental psychologists disagreed with the proposition: \"There are biological races in the species \"Homo sapiens\".\"\n\nThe authors of the study also examined 77 college textbooks in biology and 69 in physical anthropology published between 1932 and 1989. Physical anthropology texts argued that biological races exist until the 1970s, when they began to argue that races do not exist. In contrast, biology textbooks did not undergo such a reversal but many instead dropped their discussion of race altogether. The authors attributed this to biologists trying to avoid discussing the political implications of racial classifications, instead of discussing them, and to the ongoing discussions in biology about the validity of the concept \"subspecies\". The authors also noted that some widely used textbooks in biology such as Douglas J. Futuyma's 1986 \"Evolutionary Biology\" had abandoned the race concept, \"The concept of race, masking the overwhelming genetic similarity of all peoples and the mosaic patterns of variation that do not correspond to racial divisions, is not only socially dysfunctional but is biologically indefensible as well (pp. 5 18–5 19).\"\n\nA 1994 examination of 32 English sport/exercise science textbooks found that 7 (21.9%) claimed that there are biophysical differences due to race that might explain differences in sports performance, 24 (75%) did not mention nor refute the concept, and 1 (3.12%) expressed caution with the idea.\n\nIn February 2001, the editors of \"Archives of Pediatrics and Adolescent Medicine\" asked \"authors to not use race and ethnicity when there is no biological, scientific, or sociological reason for doing so.\" The editors also stated that \"analysis by race and ethnicity has become an analytical knee-jerk reflex.\" \"Nature Genetics\" now ask authors to \"explain why they make use of particular ethnic groups or populations, and how classification was achieved.\"\n\nMorning (2008) looked at high school biology textbooks during the 1952–2002 period and initially found a similar pattern with only 35% directly discussing race in the 1983–92 period from initially 92% doing so. However, this has increased somewhat after this to 43%. More indirect and brief discussions of race in the context of medical disorders have increased from none to 93% of textbooks. In general, the material on race has moved from surface traits to genetics and evolutionary history. The study argues that the textbooks' fundamental message about the existence of races has changed little.\n\nSurveying views on race in the scientific community in 2008, Morning says that they often split along culture and demographic lines and that, since Lieberman's surveys, biologists have failed to come to a clear consensus, noting that \"At best, one can conclude that biologists and anthropologists now appear equally divided in their beliefs about the nature of race.\"\n\nGissis (2008) examined several important American and British journals in genetics, epidemiology and medicine for their content during the 1946–2003 period. He wrote that \"Based upon my findings I argue that the category of race only \"seemingly\" disappeared from scientific discourse after World War II and has had a \"fluctuating yet continuous use\" during the time span from 1946 to 2003, and has even \"become more pronounced from the early 1970s on\"\".\n\n33 health services researchers from differing geographic regions were interviewed in a 2008 study. The researchers recognized the problems with racial and ethnic variables but the majority still believed these variables were necessary and useful.\n\nA 2010 examination of 18 widely used English anatomy textbooks found that they all represented human biological variation in superficial and outdated ways, many of them making use of the race concept in ways that were current in 1950s anthropology. The authors recommended that anatomical education should describe human anatomical variation in more detail and rely on newer research that demonstrates the inadequacies of simple racial typologies.\n\n\"Black's Law Dictionary\" defines race as \"[a]n ethnical stock; a great division of mankind [sic] having in common certain distinguishing physical peculiarities constituting a comprehensive class appearance.\" \n\nLester Frank Ward (1841-1913), considered to be one of the founders of American sociology, rejected notions that there were fundamental differences that distinguished one race from another, although he acknowledged that social conditions differed dramatically by race. At the turn of the 20th century, sociologists viewed the concept of race in ways that were shaped by the scientific racism of the 19th and early 20th centuries. Many sociologists focused on African Americans, called Negroes at that time, and claimed that they were inferior to whites. White sociologist Charlotte Perkins Gilman (1860–1935), for example, used biological arguments to claim the inferiority of African Americans. American sociologist Charles H. Cooley (1864–1929) theorized that differences among races were \"natural,\" and that biological differences result in differences in intellectual abilities Edward Alsworth Ross (1866-1951), also an important figure in the founding of American sociology, and an eugenicist, believed that whites were the superior race, and that there were essential differences in \"temperament\" among races. In 1910, the \"Journal\" published an article by Ulysses G. Weatherly (1865-1940) that called for white supremacy and segregation of the races to protect racial purity.\n\nW. E. B. Du Bois (1868–1963), one of the first African-American sociologists, was the first sociologist to use sociological concepts and empirical research methods to analyze race as a social construct instead of a biological reality. Beginning in 1899 with his book \"The Philadelphia Negro\", Du Bois studied and wrote about race and racism throughout his career. In his work, he contended that social class, colonialism, and capitalism shaped ideas about race and racial categories. Social scientists largely abandoned scientific racism and biological reasons for racial categorization schemes by the 1930s. Sociologists associated with the Chicago School theorized that notions about race were socially constructed, and were not biological. In 1978, William Julius Wilson (1935–) argued that race and racial classification systems were declining in significance, and that instead, social class more accurately described what sociologists had earlier understood as race. By 1986, sociologists Michael Omi and Howard Winant successfully introduced the concept of racial formation to describe the process by which racial categories are created. Omi and Winant assert that \"there is no biological basis for distinguishing among human groups along the lines of race.\" Today, sociologists generally understand race and racial categories as socially constructed, and reject racial categorization schemes that depend on biological differences.\n\nIn the United States, federal government policy promotes the use of racially categorized data to identify and address health disparities between racial or ethnic groups. In clinical settings, race has sometimes been considered in the diagnosis and treatment of medical conditions. Doctors have noted that some medical conditions are more prevalent in certain racial or ethnic groups than in others, without being sure of the cause of those differences. Recent interest in race-based medicine, or race-targeted pharmacogenomics, has been fueled by the proliferation of human genetic data which followed the decoding of the human genome in the first decade of the twenty-first century. There is an active debate among biomedical researchers about the meaning and importance of race in their research. Proponents of the use of racial categories in biomedicine argue that continued use of racial categorizations in biomedical research and clinical practice makes possible the application of new genetic findings, and provides a clue to diagnosis. Biomedical researchers' positions on race fall into two main camps: those who consider the concept of race to have no biological basis and those who consider it to have the potential to be biologically meaningful. Members of the latter camp often base their arguments around the potential to create genome-based personalized medicine.\n\nOther researchers point out that finding a difference in disease prevalence between two socially defined groups does not necessarily imply genetic causation of the difference. They suggest that medical practices should maintain their focus on the individual rather than an individual's membership to any group. They argue that overemphasizing genetic contributions to health disparities carries various risks such as reinforcing stereotypes, promoting racism or ignoring the contribution of non-genetic factors to health disparities. International epidemiological data show that living conditions rather than race make the biggest difference in health outcomes even for diseases that have \"race-specific\" treatments. Some studies have found that patients are reluctant to accept racial categorization in medical practice.\n\nIn an attempt to provide general descriptions that may facilitate the job of law enforcement officers seeking to apprehend suspects, the United States FBI employs the term \"race\" to summarize the general appearance (skin color, hair texture, eye shape, and other such easily noticed characteristics) of individuals whom they are attempting to apprehend. From the perspective of law enforcement officers, it is generally more important to arrive at a description that will readily suggest the general appearance of an individual than to make a scientifically valid categorization by DNA or other such means. Thus, in addition to assigning a wanted individual to a racial category, such a description will include: height, weight, eye color, scars and other distinguishing characteristics.\n\nCriminal justice agencies in England and Wales use at least two separate racial/ethnic classification systems when reporting crime, as of 2010. One is the system used in the 2001 Census when individuals identify themselves as belonging to a particular ethnic group: W1 (White-British), W2 (White-Irish), W9 (Any other white background); M1 (White and black Caribbean), M2 (White and black African), M3 (White and Asian), M9 (Any other mixed background); A1 (Asian-Indian), A2 (Asian-Pakistani), A3 (Asian-Bangladeshi), A9 (Any other Asian background); B1 (Black Caribbean), B2 (Black African), B3 (Any other black background); O1 (Chinese), O9 (Any other). The other is categories used by the police when they visually identify\nsomeone as belonging to an ethnic group, e.g. at the time of a stop and search or an arrest: White – North European (IC1), White – South European (IC2), Black (IC3), Asian (IC4), Chinese, Japanese, or South East Asian (IC5), Middle Eastern (IC6), and Unknown (IC0). \"IC\" stands for \"Identification Code;\" these items are also referred to as Phoenix classifications. Officers are instructed to \"record the response that has been given\" even if the person gives an answer which may be incorrect; their own perception of the person's ethnic background is recorded separately. Comparability of the information being recorded by officers was brought into question by the Office for National Statistics (ONS) in September 2007, as part of its Equality Data Review; one problem cited was the number of reports that contained an ethnicity of \"Not Stated.\"\n\nIn many countries, such as France, the state is legally banned from maintaining data based on race, which often makes the police issue wanted notices to the public that include labels like \"dark skin complexion\", etc.\n\nIn the United States, the practice of racial profiling has been ruled to be both unconstitutional and a violation of civil rights. There is active debate regarding the cause of a marked correlation between the recorded crimes, punishments meted out, and the country's populations. Many consider \"de facto\" racial profiling an example of institutional racism in law enforcement. The history of misuse of racial categories to impact adversely one or more groups and/or to offer protection and advantage to another has a clear impact on debate of the legitimate use of known phenotypical or genotypical characteristics tied to the presumed race of both victims and perpetrators by the government.\n\nMass incarceration in the United States disproportionately impacts African American and Latino communities. Michelle Alexander, author of \"The New Jim Crow: Mass Incarceration in the Age of Colorblindness\" (2010), argues that mass incarceration is best understood as not only a system of overcrowded prisons. Mass incarceration is also, \"the larger web of laws, rules, policies, and customs that control those labeled criminals both in and out of prison.\" She defines it further as \"a system that locks people not only behind actual bars in actual prisons, but also behind virtual bars and virtual walls\", illustrating the second-class citizenship that is imposed on a disproportionate number of people of color, specifically African-Americans. She compares mass incarceration to Jim Crow laws, stating that both work as racial caste systems.\n\nMany research findings appear to agree that the impact of victim race in the IPV arrest decision might possibly include a racial bias in favor of white victims. A 2011 study in a national sample of IPV arrests found that female arrest was more likely if the male victim was white and the female offender was black, while male arrest was more likely if the female victim was white. For both female and male arrest in IPV cases, situations involving married couples were more likely to lead to arrest compared to dating or divorced couples. More research is needed to understand agency and community factors that influence police behavior and how discrepancies in IPV interventions/ tools of justice can be addressed.\n\nRecent work using DNA cluster analysis to determine race background has been used by some criminal investigators to narrow their search for the identity of both suspects and victims. Proponents of DNA profiling in criminal investigations cite cases where leads based on DNA analysis proved useful, but the practice remains controversial among medical ethicists, defense lawyers and some in law enforcement.\n\nSimilarly, forensic anthropologists draw on highly heritable morphological features of human remains (e.g. cranial measurements) to aid in the identification of the body, including in terms of race. In a 1992 article, anthropologist Norman Sauer noted that anthropologists had generally abandoned the concept of race as a valid representation of human biological diversity, except for forensic anthropologists. He asked, \"If races don't exist, why are forensic anthropologists so good at identifying them?\" He concluded:\n\nIdentification of the ancestry of an individual is dependent upon knowledge of the frequency and distribution of phenotypic traits in a population. This does not necessitate the use of a racial classification scheme based on unrelated traits, although the race concept is widely used in medical and legal contexts in the United States. Some studies have reported that races can be identified with a high degree of accuracy using certain methods, such as that developed by Giles and Elliot. However, this method sometimes fails to be replicated in other times and places; for instance, when the method was re-tested to identify Native Americans, the average rate of accuracy dropped from 85% to 33%. Prior information about the individual (e.g. Census data) is also important in allowing the accurate identification of the individual's \"race\".\n\nIn a different approach, anthropologist C. Loring Brace said:\n\nIn association with a NOVA program in 2000 about race, he wrote an essay opposing use of the term.\n\nA 2002 study found that about 13% of human craniometric variation existed between regions, while 81% existed within regions (the other 6% existed between local populations within the same region). In contrast, the opposite pattern of genetic variation was observed for skin color (which is often used to define race), with 88% of variation between regions. The study concluded that \"The apportionment of genetic diversity in skin color is atypical, and cannot be used for purposes of classification.\"\nSimilarly, a 2009 study found that craniometrics could be used accurately to determine what part of the world someone was from based on their cranium; however, this study also found that there were no abrupt boundaries that separated craniometric variation into distinct racial groups. Another 2009 study showed that American blacks and whites had different skeletal morphologies, and that significant patterning in variation in these traits exists within continents. This suggests that classifying humans into races based on skeletal characteristics would necessitate many different \"races\" being defined.\n\n\n\n\n\n\n\n\n"}
{"id": "81520", "url": "https://en.wikipedia.org/wiki?curid=81520", "title": "Reverse speech", "text": "Reverse speech\n\nReverse speech is a pseudoscientific topic first advocated by David John Oates which gained publicity when it was mentioned on Art Bell's nightly Coast to Coast AM radio talk show. It claims that during spoken language production, human speakers subconsciously produce hidden messages that give insights into their innermost thoughts. Oates claims that it has applications in psychotherapy, criminology and business negotiation. Its claims have been rejected, however, by mainstream science and academia.\n\nOates' claim is that, on average, once in every 15–20 seconds of casual conversation a person produces two related sentences—a \"forward-spoken\" message that is heard consciously, and a \"backwards\" message unconsciously embedded in the person's speech. These two modes of speech, forward and backward, are supposedly dependent upon each other and form an integral part of human communication. In the dynamics of interpersonal communication both modes of speech combine to communicate the total psyche of the person, conscious as well as unconscious. Oates claims that backward speech is always honest and reveals the truth about the speaker's intentions and motivations. The most famous recording that allegedly demonstrates this is the speech given by Neil Armstrong at the time of the first manned lunar landing on 20 July 1969. If played backwards, the words \"small step for man\" sound somewhat like \"Man will space walk.\"\n\nOne explanation for this phenomenon is pareidolia, the tendency of the human brain to perceive meaningful patterns in random noise. Pareidolia is even more likely to occur when a person consciously tries to detect a pattern, as is the case for someone listening for intelligible phrases in backwards speech. The power of suggestion is then used to nudge the listener to hear what the presenter wants him to hear. David John Oates, for example, almost always tells the listener in advance what he should expect to hear, thereby planting a suggestion that would make the listener more likely to actually \"hear\" that phrase. A study has shown that when listening to the same clips without being told in advance what to expect, the results have a higher variation.\n\nMost academics in the field of linguistics have not paid attention to Oates' work, and it has been called a pseudoscience. For the most part, universities and research institutes have refused to test Oates' theories because of a lack of theoretical basis to make his predictions even worth testing, and the fact that many of his claims are untestable, but one of the few scientific experiments to evaluate Oates' claims did not support his findings. Others have criticized \"reverse speech\" as lacking a rigorous methodology and not being informed by an understanding of issues in linguistics, and characterized Oates as \"more interested in making a profit than educating others,\" pointing out the large amount of merchandise and services his website sells. Reverse speech has been compared to the controversial field (labelled a pseudoscience by some) of neuro-linguistic programming. Because of the \"dogmatic\" tone of Oates' material, reverse speech has been compared to \"fringe literature.\"\n\nOates' own claims about the applications of reverse speech have also been challenged. One report has questioned whether reverse speech was ever really used in police work, as Oates claimed. Likewise, his claim that reverse speech has applications in psychology and psychotherapy is not supported by mainstream research in those fields. Oates' work has been described as \"dangerous\" because of its potential for misuse and the likelihood of leading to false accusations of people in criminal courts, similarly to the controversial practice of facilitated communication.\n\nThe CIA recently declassified documents on reverse speech.\n"}
{"id": "169650", "url": "https://en.wikipedia.org/wiki?curid=169650", "title": "Rod of Asclepius", "text": "Rod of Asclepius\n\nIn Greek mythology, the Rod of Asclepius (, ; Unicode symbol: ⚕), also known as the Staff of Asclepius (sometimes also spelled Asklepios or Aesculapius) and as the asklepian, is a serpent-entwined rod wielded by the Greek god Asclepius, a deity associated with healing and medicine. The symbol has continued to be used in modern times, where it is associated with medicine and health care, yet frequently confused with the staff of the god Hermes, the caduceus. Theories have been proposed about the Greek origin of the symbol and its implications.\n\nThe 'Rod of Asclepius' takes its name from the Greek god Asclepius, a deity associated with healing and medicinal arts in Greek mythology. Asclepius' attributes, the snake and the staff are sometimes depicted separately in antiquity, and are combined in this symbol.\n\nThe most famous temple of Asclepius was at Epidaurus in north-eastern Peloponnese. Another famous healing temple (or asclepeion) was located on the island of Kos, where Hippocrates, the legendary \"father of medicine\", may have begun his career. Other asclepieia were situated in Trikala, Gortys (in Arcadia), and Pergamum in Asia.\n\nIn honor of Asclepius, a particular type of non-venomous snake was often used in healing rituals, and these snakes – the Aesculapian snakes – crawled around freely on the floor in dormitories where the sick and injured slept. These snakes were introduced at the founding of each new temple of Asclepius throughout the classical world. From about 300 BCE onwards, the cult of Asclepius grew very popular and pilgrims flocked to his healing temples (Asclepieia) to be cured of their ills. Ritual purification would be followed by offerings or sacrifices to the god (according to means), and the supplicant would then spend the night in the holiest part of the sanctuary – the abaton (or adyton). Any dreams or visions would be reported to a priest who would prescribe the appropriate therapy by a process of interpretation. Some healing temples also used sacred dogs to lick the wounds of sick petitioners.\n\nThe original Hippocratic Oath began with the invocation \"I swear by Apollo the Physician and by Asclepius and by Hygieia and Panacea and by all the gods ...\"\n\nThe serpent and the staff appear to have been separate symbols that were combined at some point in the development of the Asclepian cult. The significance of the serpent has been interpreted in many ways; sometimes the shedding of skin and renewal is emphasized as symbolizing rejuvenation, while other assessments center on the serpent as a symbol that unites and expresses the dual nature of the work of the physician, who deals with life and death, sickness and health. The ambiguity of the serpent as a symbol, and the contradictions it is thought to represent, reflect the ambiguity of the use of drugs, which can help or harm, as reflected in the meaning of the term \"pharmakon\", which meant \"drug\", \"medicine\", and \"poison\" in ancient Greek. Products deriving from the bodies of snakes were known to have medicinal properties in ancient times, and in ancient Greece, at least some were aware that snake venom that might be fatal if it entered the bloodstream could often be imbibed. Snake venom appears to have been 'prescribed' in some cases as a form of therapy.\n\nThe staff has also been variously interpreted. One view is that it, like the serpent, \"conveyed notions of resurrection and healing\", while another (not necessarily incompatible) is that the staff was a walking stick associated with itinerant physicians. Cornutus, a Greek philosopher probably active in the first century CE, in the \"Theologiae Graecae Compendium\" (Ch. 33) offers a view of the significance of both snake and staff:\nIn any case the two symbols certainly merged in antiquity as representations of the snake coiled about the staff are common. It has been claimed that the snake wrapped around the staff was a species of rat snake, \"Elaphe longissima\", the Aesculapian snake.\n\nSome commentators have interpreted the symbol as a direct representation of traditional treatment of dracunculiasis, the Guinea worm disease. The worm emerges from painful ulcerous blisters. The blisters burn, causing the patient to immerse the affected area in water to cool and soothe it. The worm senses the temperature change and discharges its larvae into the water. The traditional treatment was to slowly pull the worm out of the wound over a period of hours to weeks and wind it around a stick. The modern treatment may replace the stick with a piece of sterile gauze but is otherwise largely identical.\n\nSee also the article on Nehustan which discusses the incident in the biblical Book of Numbers, where the Nehushtan (or Nohestan) (Hebrew: נחושתן or נחש הנחושת) was a bronze serpent on a pole which God told Moses to erect to protect the Israelites who saw it from dying from the bites of the \"fiery serpents\" which God had sent to punish them for speaking against God and Moses. King Hezekiah later instituted a religious iconoclastic reform and destroyed \"the brazen serpent that Moses had made; for unto those days the children of Israel did burn incense to it; and it was called Nehushtan\". (2 Kings 18:4) Many bible students have made the connection between the Rod of Asclepius and the Nehustan, where the stick and serpent were put together in the biblical incident of mass healing. Possibly this was the reason the ancients started to use the snake in this way.\n\nA number of organizations and services use the rod of Asclepius as their logo, or part of their logo. These include:\n\nBHP Emergency Services (West Australian Iron Ore)\n\n\n\nEmergency medical services in the United Kingdom use a variation, in which the snake is entwined with a wheel.\n\n\n\n\n\nThe rod of Asclepius has a representation on the \"Miscellaneous Symbols\" table of the Unicode Standard at U+2695 \"STAFF OF AESCULAPIUS\" (⚕).\n\n"}
{"id": "49888049", "url": "https://en.wikipedia.org/wiki?curid=49888049", "title": "SPEAK FREE Act of 2015", "text": "SPEAK FREE Act of 2015\n\nThe SPEAK FREE Act of 2015 (H.R. 2304) is a bipartisan Act of Congress that was introduced on May 13, 2015, designed to serve as federal anti-SLAPP legislation, to protect free speech in practice. Its title is an acronym (S.P.E.A.K. F.R.E.E.) that stands for \"Securing Participation, Engagement, and Knowledge Freedom by Reducing Egregious Efforts Act of 2015\".\n\nThe Bill was designed to prevent SLAPP lawsuits (strategic lawsuit against public participation), which are often brought to silence critics. SLAPP suits are used as legal retaliation, by burdening them with the costs of a legal defense, until they abandon their criticism.\n\nIn response to several notable SLAPP abuses in recent times, United States Representative Blake Farenthold (R-TX), along with 20 Democrat and 11 Republican co-sponsors, introduced the SPEAK FREE Act to dismiss lawsuits which are used to harass plaintiffs. The SPEAK FREE Act was formed by observing what has been shown to work, after being previously implemented in over twenty States where anti-SLAPP legislation had already been tested.\n\nWith the looming prospect that Donald Trump might become President, Congress decided to address the SPEAK FREE Act, with the Bill's sponsor, Rep. Farenthold (R), stating, \"Obama will sign this. I don’t think Trump will.\"\n\nOn June 22, 2016, the House Judiciary Committee held a hearing on the Bill. Witnesses who gave testimony included Aaron Schur (Senior Director of Litigation of Yelp), Bruce Brown (Reporters Committee for Freedom of the Press), Alexander A. Reinert (Professor of Law at Cardozo), and Laura Prather (partner of Haynes and Boone, LLP).\n\nThe Bill was praised for its bipartisan support in protecting free speech by the Public Participation Project, the Electronic Frontier Foundation, and Yelp, among others, as the Bill is also geared toward protecting online criticism.\n\n\n"}
{"id": "3276445", "url": "https://en.wikipedia.org/wiki?curid=3276445", "title": "Self-disclosure", "text": "Self-disclosure\n\nSelf-disclosure is a process of communication by which one person reveals information about himself or herself to another. The information can be descriptive or evaluative, and can include thoughts, feelings, aspirations, goals, failures, successes, fears, and dreams, as well as one's likes, dislikes, and favorites.\n\nSocial penetration theory posits that there are two dimensions to self-disclosure: breadth and depth. Both are crucial in developing a fully intimate relationship. The range of topics discussed by two individuals is the breadth of disclosure. The degree to which the information revealed is private or personal is the depth of that disclosure. It is easier for breadth to be expanded first in a relationship because of its more accessible features; it consists of outer layers of personality and everyday lives, such as occupations and preferences. Depth is more difficult to reach, and includes painful memories and more unusual traits that we might hesitate to share with others. We reveal ourselves most thoroughly and discuss the widest range of topics with our spouses and loved ones.\n\nSelf-disclosure is an important building block for intimacy and cannot be achieved without it. Reciprocal and appropriate self-disclosure is expected. Self-disclosure can be assessed by an analysis of cost and rewards which can be further explained by social exchange theory. Most self-disclosure occurs early in relational development, but more intimate self-disclosure occurs later.\n\nSocial penetration theory states that the development of a relationship is closely linked to systematic changes in communication. Relationships generally begin with the exchange of superficial information and gradually move on to more meaningful conversations. In order to develop a more intimate relationship, partners must increase the breadth and depth of their conversations. Breadth includes the variety of topics two people discuss and depth is the personal significance of these topics.\n\nAltman and Taylor use a wedge to explain this theory. In this example, the beginning of a relationship is represented by a narrow and shallow wedge because only a few topics are discussed. However, as the relationship goes on, the wedge should become broader and deeper, including more topics of personal significance. The wedge must drive through three \"layers\" in order for intimacy to develop. The first is superficial \"small talk\" with little personal information about the speakers. The next layer is intimate, with increasing breadth and depth and more personal details. The third is the very intimate level, where extremely private information is shared.\n\nIntimacy in these relationships can develop only if the persons involved reciprocate disclosures. Intimacy will not develop if only one partner discloses and the other continues to reveal only superficial information. Reciprocity must be gradual and match the intimacy of the other's disclosures. Too rapid, too personal disclosure creates an imbalance in a relationship that can be discomfiting. This gradual process varies from relationship to relationship and can depend on the specific partner with whom one is communicating.\n\nReciprocity is a positive response from the person with whom one is sharing information, whereby the person who received the disclosure self-discloses in turn. Self-disclosure usually influences whether two people will want to interact again. Research has shown that when one person self-discloses, another person is more likely to self-disclose. Initially, the process is started by one partner's reveal of personal information to the other partner. In return, the other will disclose something and behave in such a way so as to be responsive to the initial disclosure's content, while also conveying a degree of understanding and validation for what was revealed.\n\nResearch has found that people who consider themselves to be high in disclosure are likely to be good at eliciting more disclosure from those with whom they interact. Three theories describe reciprocity: The social attraction-trust hypothesis, social exchange theory and the norm of reciprocity. The social attraction-trust hypothesis says that people disclose to one another because they believe the person who disclosed to them likes and trusts them. Social exchange theory explains that people attempt to maintain equality in self-disclosure because an imbalance in this makes them uncomfortable. The third explanation, the norm of reciprocity, argues that reciprocating disclosure is a social norm and violating it makes a person uncomfortable.\n\nThere are two types of reciprocity: turn-taking reciprocity and extended reciprocity. Turn-taking is when partners immediately self-disclose with one another and extended is when disclosure happens over a period of time, in which one partner may be the only one disclosing while the other just listens. Those who engage in turn taking reciprocity are shown to like their interaction partners more than those who engage in extended reciprocity. Turn taking partners are also shown to feel closer and similar to each other and to enjoy the other's company more than extended pairs. This can be explained by the social attraction-trust hypothesis because the partners perceive the discloser as liking and trusting them because they disclosed personal information. Those who engage in extended reciprocity are affected by the social exchange theory and the norm of reciprocity which can account for the lower degree of liking. Since extended reciprocity limits reciprocating disclosure it creates an imbalance in disclosure which violates both of these theories. That said, people usually report that they themselves are disclosing more than is the other partner. This is called perceived partner reciprocity, and it is critical to the self-disclosure process in developing relationships.\n\nTwo key components for intimacy are disclosure and partner responsiveness. It is extremely important that when a speaker discloses personal information their partner also discloses something personally relevant. It is also essential that the listener understand, validate and care about what the speaker is disclosing. If the speaker does not feel accepted by the listener then they may not disclose something to them in the future, which stops the development of intimacy. Emotional disclosures are also shown to foster intimacy more than factual disclosures. Factual disclosures reveal facts and information about the self (e.g., \"I am divorced from my husband.\") while emotional disclosures reveal a person's feelings, thoughts and judgments (e.g., \"My divorce was so painful it has made it difficult for me to trust a romantic partner again\"). Emotional disclosures can increase intimacy because they allow the listener to confirm and support the discloser's self-view. The transition from sharing impersonal to personal facts is crucial to the building of an intimate relationship. One must feel accepted in order to feel comfortable enough to self-disclose. Without acceptance, one partner will withdraw and fail to reveal personal facts within the relationship. Sharing ourselves also brings us out of our imaginary worlds and allows us to see the realities of the world we live in. We are most comfortable sharing with those whom we like and feel like us. There is also evidence that someone who introduces himself with more intimacy is more likely to facilitate self-disclosure and intimacy with the recipient. Thus, self-disclosure breeds intimacy. This is why we reveal ourselves most and discuss the widest range of topics with our spouses and loved ones.\n\nWe often perceive our own self-disclosure as higher than our partner's, which can lead to ill feelings. It is hard for humans to accurately judge how fully another is disclosing to them.\n\nAccording to Snyder (1974) self-monitoring is the personality difference in individual's degree of preference to both self-expression and self-presentation.Self-monitoring is a form of impression management in which a person examines a situation and behaves accordingly. Although self-monitoring is measured on a continuous scale, researchers often group individuals into two types: high and low self-monitors. Someone who is a high self-monitor tends to examine a situation more closely and adjusts their behavior in order \"fit in\" with others in the scenario. High self-monitors tend to behave in a friendlier and extroverted manner in order to be well liked by peers. A low self-monitor does not do this and tends to follow their own emotions and thoughts when behaving in public. Since they are more attuned to social cues, high self-monitors are generally better at assessing the level of intimacy a partner is disclosing. By noticing these cues, high self-monitors tend to reciprocate equally in their self-disclosures.\n\nThis can be explained by the norm of reciprocity because the high self-monitors can pick up on these cues easily and know that they need to respond with their own disclosure. It can also be explained by social exchange theory. Research shows that high self-monitors are more uncomfortable when paired with a low self-monitor because low self-monitors do not tend to disclose intimate details so the balance in the conversation is uneven. High self-monitors are also shown to be the \"pace-setters\" of the conversation and generally initiate and maintain the flow of a conversation.\n\nThose in a positive mood have been found to disclose more intimately than those in a negative mood. This may be because of informational effects whereby happy people tend to access more positive information which leads them to behave in a more optimistic and confident manner. Unhappy people tend to access more negative information which increases the likelihood of cautious, pessimistic and restrained communications.\n\nThis may also be due to processing effects, in particular assimilation and accommodation effects. Assimilation effects rely on an individual's prior knowledge to guide their behavior in a situation and accommodation effects rely on careful monitoring of a situation and a greater attention to concrete information. Assimilative processing is ideal for safe, routine situations while accommodative processing is for problematic situations. Happy people tend to use assimilative processing, which leads to more daring and direct disclosures, while unhappy people use accommodative processing, which leads them to be more cautious in their disclosures. These accommodating effects for unhappy people tend to increase reciprocity because these individuals will match the level of disclosure from their partner but will not go beyond that.\n\nHowever, it can also be said that being distressed, anxious, or fearful (which would be classified as negative mood states) can accelerate disclosure as well. The exception to this is loneliness, for lonely individuals have shown decreased rates of self-disclosure.\n\nWhether or not one sex shares more readily is a heated debate in social psychology, but sex-role identities play a large part in the amount one chooses to reveal to another. Androgynous people disclose more intimately across contexts than do notably masculine and feminine people.\n\nResearch findings on gender differences in self-disclosure are mixed. Women self-disclose to enhance a relationship, while men self-disclose relative to their control and vulnerabilities. Men initially disclose more in heterosexual relationships. Women tend to put more emphasis on intimate communication with same-sex friends than men do.\n\nIn relationships, there are still other factors that contribute to the likelihood of disclosure. While people with high self-esteem tend to reveal themselves more, the reverse is also true, where self-esteem is enhanced by a partner's disclosures. In men, self-disclosure and the level of disclosure they perceive from their wives is positively correlated with their self-esteem. For both genders, the state of a relationship and the feelings associated with it are major contributors to how much each spouse reveals himself or herself. Husbands and wives in a relationship marked with satisfaction, love, and commitment rate their own levels of disclosure highly as well as their perceptions of their spouses' disclosures.\n\nBeing shy decreases self-disclosure. Among men, those who are or appear more \"tough\" are less likely to disclose and express themselves.\n\nMotivation for disclosure is also critical: does the individual need to present himself or herself in a certain way in order to gain certain benefits, and does the self-disclosure match the person's sense of ideal self? We like to present ourselves in ways that we feel are congruent with our own self-concepts, and what we tell others about ourselves often becomes how we actually are.\n\nSexual self-disclosure is the act of revealing one's sexual preferences to another, usually to one's sexual partner. This allows an even deeper level of understanding between two people and fosters even more intimacy as a result of the disclosures. Likewise, relationship satisfaction was found to correlate with sexual disclosures. For men, high levels of sexual self-disclosure predicted higher relationship satisfaction, though this was not found to be true for women. But, sexual satisfaction was linked to higher levels of sexual self-disclosure for both men and women. Further, those who disclose more sexually have been found to have less sexual dysfunction.\n\nSelf-disclosure is a method of relationship maintenance, aiming to keep partners satisfied with their relationship. Partners learn a shared communication system, and disclosures are a large part of building that system, which has been found to be very beneficial in highly satisfying relationships. Significant positive relationships have been found between multiple measures of relationship satisfaction and the levels of spouses' disclosure on the Social Penetration Scale. Further, affection and support are provided to most in the most important ways through marriage. Surveys done by a variety of researchers have found that people list marriage as the ultimate form of intimacy. Spouses feel responsible, in that they need to be responsive to their partners' self-disclosures, more so than they feel obligated to respond to the disclosures of people in their other relationships.\n\nIn a study by Laurenceau and colleagues, several differences were found in the satisfaction of spouses based on their daily-diary recordings of self-disclosures in their daily interactions. The results show that the actual disclosures in the process of self-disclosure may not be the only factors that facilitate intimacy in relationships. Husbands' intimacy was most strongly predicted by self-disclosure, while perceived responsiveness to disclosure was the stronger predictor for wives' feelings of intimacy with their husbands. A different study found evidence of wives' perceptions of their husbands' self-disclosures as being a strong predictor of how long a couple will stay together. Those who think their husbands are not sharing enough are likely to break up sooner. This finding links to the idea of positive illusions in relationship studies. For husbands, the actual act of self-disclosure is more indicative of their feelings of intimacy with their wives. On the other hand, wives are thought to value more the feelings of being understood and validated by their husbands' responsiveness to their disclosures, and this is the more important factor in their feelings of intimacy in their marriages.\n\nRelated to these findings, those husbands who reported the highest ratings of global marital satisfaction showed the highest ratings in daily intimacy. Similarly, the wives who rated their global satisfaction highest also had higher levels of daily intimacy. Greater marital satisfaction was found among those who had the higher ratings of intimacy. Further, couples with high levels of demand-withdraw communication rated their average daily intimacy as much lower. This suggests a relationship between one's overall marital satisfaction and the amount of intimacy in a relationship, though no causation can be proven with the present research. Self-esteem has also been found to be a predictor of satisfaction, with couples exhibiting both high self-esteem and high self-disclosure levels being the most satisfied in their relationships.\n\nMore disclosures of unpleasant feelings led to less marital satisfaction in recent studies, and disclosure is affected the minute a relationship is stressed, as feelings of less attachment to a spouse promote decreased self-disclosure. Likewise, less intimacy leads to more negative disclosures between partners. However, findings by Tolstedt and Stokes (1984) suggest that the depth of self-disclosure actually increases as the intimacy of a relationship decreases. The breadth of disclosure decreases with decreasing intimacy as originally predicted, but couples actually disclose more deeply. It is speculated that these results come about because a strained relationship causes spouses to restrict their topics of communication (breadth), but that they are also more willing to discuss deeply intimate subjects: the negative ones. Thus, while they are sharing more deeply, it is mostly in a negative light. The researchers then speculated that people might actually avoid disclosing very personal facts in the most satisfying relationships because they are fearful that their positive relationships will be negatively affected.\n\nAs time progresses, disclosure in marriage has been found to decrease, often around the time that spouses reach their 40's. It is suggested that at this stage partners know each other quite well and are very satisfied with what they communicate already.\n\nPeople first disclose facts then emotions and disclose mostly positive information in the early stages of a relationship. Some speculate that disclosures and their respective responses from a spouse lead to intimacy between the partners, and these exchanges accumulate into global and positive evaluations of the relationship by the couple. In support, studies show that couples who report greater levels of intimacy in self-reports of their daily interactions are also those who report increased global relationship functioning in their marriages. Further, the importance of disclosure in a relationship might change over time as it relates in different ways to various factors of a relationship, such as responsiveness and love, especially at the beginning of a relationship.\n\nDisclosure also changes as group size increases. As a group gets larger, people become less willing to disclose. Research has shown that individuals are more willing to disclose in groups of two than in larger groups and are more willing to disclose in a group of three rather than four. The actual disclosures mimic the willingness to disclose as individuals disclose more in pairs than they do in the larger groups. There are also gender differences in disclosure depending on group size. Men feel more inhibited in dyads, match the intimacy of the disclosure from their partner, and do not offer more information. Women, on the other hand, feel more inhibited in larger groups and disclose more personal information in dyads.\n\nNearly every school of thought is in agreement that self-disclosure is a necessary element of therapeutic technique. Self-disclosure by the therapist is often thought to facilitate increased disclosure by the client, which should result in increased understanding of the problem at hand. It helps to acknowledge the therapeutic relationship as a fundamental healing source, as an alliance between client and therapist is founded on self-disclosure from both parties. In some respects it is similar to modeling appropriate social behavior. Establishing common interests between therapists and clients is useful to maintain a degree of reality. Establishing such interests is especially beneficial in therapists' relationships with children, especially teens, who need to understand that the therapist is not an authority in order to fully benefit from therapy.\n\nIn studies of self-disclosure in therapy, two types have been identified: immediate and non-immediate. Immediate disclosure shows positive views of the therapeutic process in which the two are engaging and communicates self-involving feelings and information about the therapist's professional background. Many see the benefits of this type of disclosure. Non-immediate disclosure, however, is the revealing of more about the therapist than his or her professional background and includes personal insight. This type is rather controversial to psychologists in the present day; many feel it may be more detrimental than it is beneficial in the long-run, but there are significant findings that contradict this claim as well.\n\nFurther, there are two methods that therapists use to disclose: direct and indirect. Direct disclosures grant the client information about personal feelings, background, and professional issues. Indirect disclosures are those not explicitly granted, such as pictures on the therapist's desk and walls or wearing his or her wedding band.\n\nStudies have asked therapists to report their reasons to disclose to clients. The most common reasons are: to answer a direct question from the client, to help soothe the client's feelings of loneliness, to express understanding, to lower a client's anxiety levels and make his or her feelings seem more normal, and to build rapport.\n\nThe topics discussed by therapists who self-disclose in their sessions can vary. The preferred therapeutic approach and the effectiveness of treatments are two of the most common. Many also reveal their views of raising children, stress-coping methods, items that convey respect for the client, and emotions that will validate those the client has expressed. Anecdotes about sexual attraction, dreams, and personal problems seem to be disclosed to subjects with the least frequency by therapists.\n\nThe history of therapist disclosure has been a journey based largely on the therapists' perspectives. Early psychodynamic theorists strongly disagreed with the incorporation of therapist self-disclosure in the client-therapist relationship. Ferenczi notably maintained his belief that self-disclosure was of the utmost importance in children's therapy for traumas in that a neutral, flat therapist would only cause the child to relive the trauma. Object-relations theorists want the client to be able to see how he or she is seen by another and how what she shares is viewed by another, and the best way to operationalize these factors is through a trusting relationship with a therapist who also discloses. Self-theorists believe much the same as object-relations theorists. Intersubjective and relational schools of thought encourage disclosure due to its ability to bring subjectivity into therapy, which they deem a necessary element to real healing. They maintain that therapeutic relationships cannot be initiated and changed without intentional disclosures from both therapist and client.\n\nIn contemporary views, most agree with the inevitability of self-disclosure in therapy. Humanistic theorists want to trigger personal growth in clients and feel that a strong relationship with a therapist is a good facilitator of such, so long as the therapist's disclosures are genuine. Seeing that weakness and struggle are common among all people, even therapists, is useful to clients in the humanistic therapy setting. In order for existential psychologists to help clients, they try to disclose their own coping methods to serve as sources of inspiration to find one's own answers to questions of life. For therapists who value feminism, it is important to disclose personal feelings so that their clients have total freedom to choose the correct therapist and to eliminate power fights within the therapeutic setting. The ever-popular cognitive-behavioral approach also encourages disclosure in therapy so that clients can normalize their own thoughts with someone else's, have their thoughts challenged, and reinforce positive expectations and behaviors.\n\nHumanistic theorists have been the highest in rating self-disclosure as part of their normal therapeutic methods. Clearly, today's therapists are mostly supportive of disclosure in therapy, as the early psychoanalytic taboo of such is slowly being overridden through the recognition of many schools of thought. Most identify the benefit of self-disclosures in facilitating rewarding relationships and helping to reach therapeutic goals.\n\nIt is useful to discuss personal matters in therapy for a variety of reasons. Certain types of disclosures are almost universally recognized as necessary in the early stages of therapy, such as an explanation of the therapeutic approach to be used and particular characteristics of the therapist. Disclosure with another individual facilitates a closeness in that relationship and is strongly believed to lead to a deeper understanding of the self. One will often see his or her disclosure in a more positive perspective if it is shared with someone else. It is thought that disclosing the details of a traumatic experience can greatly help with the organization of related thoughts, and the process of retelling is itself a method of healing. An understanding between therapist and client is achieved when the client can share his or her perceptions without feeling threatened by judgments or unwanted advice. Further, expressing emotions lessens the toll of the autonomic nervous system and has been shown in several studies to improve overall physical health in this way. A disclosing therapist invites his or her client to compare cognitive perceptions and perhaps realize his or her own distortions.\n\nThe disclosure need not be verbal to be advantageous, as writing about traumas and positive experiences alike has been seen to produce less psychological and physiological distress. The Pennebaker Writing Disclosure Paradigm is a method commonly used in therapy settings to facilitate writing about one's experiences. Exposure theory also offers support in that reliving and talking about a negative event should help the negative affect to be more accepted by the individual overtime through extinction.\n\nA study by Watkins (1990) formulated four model hypotheses for the use of self-disclosure in therapy sessions. Supported heavily is the idea of mutuality: disclosure by one leads to disclosure by the other. The modeling hypothesis suggests that the client will model the disclosures of the therapist, thereby learning expression and gaining skills in communication. Some argue for the reinforcement model, saying that the use of self-disclosure by therapists is purely to reinforce self-disclosure in their clients. Lastly, the social exchange hypothesis sees the relationship between client and therapist as an interaction that requires a guide: self-disclosure. Clients' self-reported improvement when a therapist has used disclosure in therapy is high. Regardless, the benefits of validating the client's thoughts through self-disclosure has been shown to be largely beneficial in the scope of therapy.\n\nStudies have also shown the disadvantageous effects of keeping secrets, for they serve as stressors over time. Concealing one's thoughts, actions, or ailments does not allow a therapist to examine and work through the client's problem. Unwanted, recurrent thoughts, feelings of anxiousness and depression, sleeping problems, and many other physiological, psychological, and physical issues have been seen as the results of withholding important information from others.\n\nThe treatment of clients with adjustment disorders, anxiety disorders, mood disorders, and post-traumatic stress disorder have been thought to use self-disclosure techniques the most. Therapy sessions for personality disorders, behavior disorders, impulse control disorders, and psychotic disorders seem to use therapist self-disclosure far less often.\n\nTherapists who self-disclose, especially information that validates or reflects the information disclosed by the client, have been rated in studies consistently as demonstrating more warmth and being more personable. A study using participants who were to imagine themselves in hypothetical counseling situations found that therapists who responded to \"What would you do if you were me?\" when asked by the client, were viewed as more socially attractive, more expert, and more trustworthy. Their likability was increased by their willingness to disclose to their clients. The three dimensions mentioned have been said to be of utmost importance when determining one's likability. However, these therapists may also been seen as less professional for these disclosures. Additionally, a therapist who discloses too frequently risks losing focus in the session, talking too much about himself or herself and not allowing the client to actually harvest the benefits of the disclosures in the session through client-focused reflection. Much research has found that successful therapy treatments are enhanced when the client has a largely favorable view of the therapist.\n\nThe atmosphere in which the therapy takes place is crucial too. Research shows that \"soft\" architecture and decor in a room promotes disclosure from clients. This is achieved with rugs, framed photos, and mellow lighting. It is thought that this environment more closely imitates the setting in which friends would share feelings, and so the same might be facilitated between counselor and client. Further, a room should not be too crowded nor too small in order to foster good disclosures from the client\n\nThe efficacy of self-disclosure is widely debated by researchers, and findings have yielded a variety of results, both positive and negative. A typical method of researching such ideas involves self-reports of both therapists and clients. The evaluations of therapists on the positive effects of their own disclosures is far less positive than that of clients' self-reports. Clients are especially likely to assert that the disclosures of their therapists help in their recovery if the disclosures are perceived as more intimate in content. Clients report that disclosures are helpful when they encourage a positive relationship with the therapist, build trust in their therapists' abilities and general person, create a feeling of being better-understood, and make the therapist seem more human. Much of these results, however, are linked to how skilled the therapist is in disclosing.\n\nAny information revealed that could reverse the effects of therapy or switch the roles of therapist and client is thought to be the most detrimental. Therapists must choose wisely in what they disclose and when. A client who is suffering greatly or facing a horrific crisis is not likely to benefit much from therapist self-disclosures. If a client at any point feels he or she, should be acting as a source of support to the therapist, disclosure is only hindering the healing process. Further, clients might become overwhelmed if their initial ideas of therapy do not include any degree of self-disclosure from their counselor, and this will not lead to successful therapy sessions either. It is also a risk to reveal too much about a therapist because the client may begin to see the healer as flawed and untrustworthy. Clients should not feel like they are in competition for time to speak and express themselves during therapy sessions.\n\nDespite contradictory findings, self-disclosure is still used frequently in therapy and is often recommended. The American Psychological Association supports the technique, calling it \"promising and probably effective\". Therapists are advised, however, to use self-disclosure with a mild frequency, to disclose more immediate-disclosure information, to keep intimacy at a minimum, and to keep the focus on the client promptly after disclosure to ensure optimum effectiveness in therapy sessions. Therapist self-disclosure in a counseling setting is ethical so long as the client is not harmed or exploited.\n\nTherapists who use self-involving statements are likely to facilitate valuable self-disclosures from their clients. Using \"I\" statements, a therapist emits a certain level of care not otherwise felt by many clients, and they are likely to benefit from this feeling of being cared for. In cases of a therapist needing to provide feedback, self-involving statements are nearly inevitable, for he or she must state a true opinion of what the client has disclosed. These sorts of \"I\" statements, when used correctly and professionally, are usually seen as especially validating by clients. Largely, the use of self-involving statements by therapists is seen as a way of making the interaction more authentic for the client, and such exchanges can have a great impact on the success of the treatment at hand.\n\nCouples-therapy is often centered on creating more intimacy in a relationship. Spouses are encouraged, or even required, to disclose unexpressed emotions and feelings to their partners. The partners' responses are practiced to be nonjudgmental and accepting. Therapists utilize techniques like rehearsal and the teaching of listening skills. Some fear that this is of little long-term help to the couple because in their real lives, there is no mediator or guiding therapist's hand when one is disclosing to another.\n\nGiven that self-disclosure is related to husband's ratings of marital satisfaction, teaching proper ways for a couple to disclose to one another might be a very beneficial skill therapists can use both for prevention and treatment in therapy sessions.\n\nWhile striving to become more like adults, looking for greater independence, and learning to become more self-reliant, children are also trying to facilitate relationships of equality with their parents. Goals like these, as reported by young people fairly universally, can affect how they disclose to their parents to a large degree. Children's disclosures with their parents has been studied by many, especially recently, after the discoveries of disclosures' positive relationships with children's adjustment levels and psychological and physical health. Some go so far as to use the rate of self-disclosure between parents and children as a dominant measure of the strength of their relationship and its health.\n\nIn adolescents' relationships with their parents, self-disclosure is thought to serve three key functions: \nChildren still attempt to maintain a certain amount of control over their parents' knowledge of their lives by monitoring how and when to disclose to them. Thus, they moderate their parents' potential reactions. Because of this, it is important for parents to be aware of how they react to their children's disclosures, for these reactions will be used as judgment calls for the children's' future sharing.\n\nOften, the reason for disclosing given by children in studies is based on the parent's expectations: \"I've learned that [Mom or Dad] wants to have this information.\" This is adaptive, in that the child has learned what his or her parents want to know. Other times a reason is that the children do not want their parents to worry about them, and this is called parent-centered disclosures. Disclosing in order to make oneself feel better or to ensure protection from parents is considered to be another reason for youth to disclose, and it is called self-oriented disclosure. On a more manipulative level, some adolescents report telling their parents things based solely on gaining an advantage of some sort, whether this is the right to reveal less or the fact that being more open tends to result in more adolescent privileges. Sometimes children qualify their disclosures by merely stating that they only disclose what they feel they want to their parents. Thus, some information is kept secret. This is dubbed selective self-disclosure. In sum, adolescents feel different pulls that make them self-disclose to their parents that can be based on the parents' needs and the children's needs. There has not been a distinct pattern found to predict which reasons will be utilized to explain disclosures by different children. For this reason it is widely believed that the reason for disclosure is largely situation- and context- dependent.\n\nThe self-disclosure of children to their parents is the dominant source of information for parents to gain knowledge about their children and their daily lives. Parental knowledge of their children's whereabouts and daily lives has been linked to several positive outcomes. The more parents know about their kids, the lower the rate of behavior problems among children, and the higher the children's well-being. Adolescents who disclose have been found to have lower rates of substance abuse, lower rates of risky sexual behaviors, lower anxiety levels, and lower rates of depression. Additionally, those who are well-adjusted, meaning they exhibit the qualities discussed above, generally want and enjoy parental involvement and are likely to disclose more. In contrast, keeping secrets from one's parents has been linked to more physical illness, poor behavior, and depression in all cultural groups. Many theorize that in at least one significant relationship one should feel able to disclose nearly completely in order for a healthy personality to develop. While parental behavioral control was once thought to provide the greatest benefits to children in limiting their activities and serving as a source of forced protection, more recent research strongly suggests that disclosures to parents that provide the parents with information about daily activities actually shows the most promise in fostering positive development through childhood and adolescence.\n\nReciprocity in children's self-disclosures is often examined in children's friendships. It has been shown that children's understanding of friendship involves sharing secrets with another person. This mutual exchange of sharing secrets could be the norm of reciprocity, in which individuals disclose because it is a social norm. This norm of reciprocity is shown to begin occurring for children in sixth grade. Sixth graders are able to understand the norm of reciprocity because they realize that relationships require both partners to cooperate and to mutually exchange secrets. They realize this because they possess the cognitive ability to take another person's perspective into account and are able to understand a third person's views which allows them to view friendships as an ongoing systematic relationship.\n\nChildren in sixth grade are also shown to understand equivalent reciprocity. Equivalent reciprocity requires matching the level of intimacy a partner discloses, therefore, a high-intimacy disclosure would be matched with an equally revealing disclosure while a low-intimacy disclosure would be matched with little information revealed. Another type of reciprocity is covariant reciprocity, in which disclosures are more intimate if a partner communicates a high-intimacy disclosure instead of a low-intimacy disclosure. This differs from equivalent reciprocity, which matches the level of intimacy, while covariant reciprocity only focuses on whether someone disclosed something personal or not. Covariant reciprocity is shown to begin in fourth grade.\n\nIt has also been shown that girls across all ages disclose more intimate information than boys, and that the number of disclosures a child reveals increases with age.\n\nEarly studies note two distinct factors that contribute to how much children self-disclose to their parents. The first is intraindividual factors, which are those that are on the child's mind and cause him or her to need social input. Biological development, cultural and social pressures, and individual maturity determine these issues, and, thus, a child's age, personality, and background also contribute to his or her level and need of self-disclose in a relationship with a parent.\n\nThe second set of factors is called contextual factors, which include the opportunities and situations that the individual has to disclose as created by the sociocultural environment. These are most directly related, then, to the target of the disclosure; these targets are the parents.\n\nAlso, gender contributes: girls are noted for usually disclosing their problems, mostly to their mothers, while boys reveal more about bad grades, behavioral conflicts, and other issues to both parents.\n\nCertain people are more likely to get others to disclose. These are called high openers. Even people known to disclose very little are likely to disclose more to high openers. Thus, if parents are characterized as good listeners, trustworthy, accepting, relaxed, and sympathetic, as are high openers, then they will likely elicit more disclosure from their children. Adolescents who view their parents like this are also said to see them as less controlling and less likely to react negatively to their disclosures. Parental responsiveness has been said to be the dominant factor of influence on adolescents' rates of self-disclosure; warmth and affection facilitate more disclosures. Parental psychological control has also been linked to increased self-disclosure of personal issues and peer issues among youth. While this sort of control is not often thought of in a positive light, some hypothesize that these kids are likely just feeling coerced to disclose subtly and without being harmed. Much of what children choose to reveal to their parents is based on previous disclosures and their parents' reactions to them.\n\nFeelings about the parent-child relationship during one's upbringing have also be found to correlate with the child's disclosures to the parents. A child with a positive memory of his or her relationship with a parent during the past years is a predictor of a higher level of self-disclosure. In fact, the view of the parent-child relationship in the past is a stronger predictor than that of the child's view of the current parent-child relationship. The relationship with the mother, in particular, is extremely predictive of disclosures from adolescents. Such findings suggest to parents that fostering secure attachment early in their children will better set the stage for disclosures in the later years, and their children may then reap the benefits of such a relationship.\n\nAdolescents are able to determine their own amount of parental authority over certain issues by how much or how little they choose to disclose to their parents. Surveys revealed that they are least likely to share information that involves their personal feelings and activities. They actively resist disclosing this to their parents because they do not see the issues as being harmful, or they feel their parents will not listen to them, or because the matters are very private to them.\n\nThe way adolescents perceive their parents' authority as legitimate largely impacts how much they feel obligated to disclose to them. The more authority the children believe their parents rightly possess, the more obligation they perceive to share their lives accordingly. Parents who attempt a large degree of psychological control over their children are unlikely to be disclosed to as frequently, which only makes logical sense given the fact that most children are searching for a sense of autonomy. The adolescents have been found to feel the most obligation to tell their parents about such activities as drinking and smoking but less need to disclose information about personal issues. Not surprising either, less obligation is felt as age increases. Contrary to popular belief though, most adolescents in the US do not consider themselves to be adults between the ages of 18 and 27, and their parents feel the same way. The age at which children feel they no longer are obligated to disclose to their parents has increased over time, and the same trend is predicted over the next few decades.\n\nOften, the motivation to disclose negative behaviors is purely because the children fear that they will not get away with it or feel obligated to share. Adolescents also want to disclose more if they feel that the activities in question are out of their own jurisdiction. Jurisdiction is measured, in the adolescents' minds, as how short-term and close the activities are. Short-term, close activities are judged as ones to be handled without disclosure to parents, while activities that will take longer or require the adolescent to be farther from home are thought of as being issues to discuss with parents.\n\nCertain events and characteristics of the parent-child relationship make disclosures unlikely:\n\nCertain events and characteristics of the parent-child relationship make the child less willing to disclose to that parent in the future:\n\nCertain events and characteristics of the parent-child relationship make disclosures likely:\n\nCertain events and characteristics of the parent-child relationship make the child more likely to disclose to that parent in the future:\n\nThere are four major differences between online communication and face to face communication. The first is that Internet users can remain anonymous. The user can choose what personal information (if any) they share with other users. Even if the user decides to use their own name, if communicating with people in others cities or countries they are still relatively anonymous. The second is that physical distance does not limit interaction on the Internet the way it does in real life. The Internet gives the ability to interact with people all over the world and the chance to meet people who have similar interests that one may not have met in their offline life. Visual cues, including those pertaining to physical attractiveness, are also not always present on the Internet. These factors have been shown to influence initial attraction and relationship formation. Finally, Internet users have time to formulate conversations which is not allotted in face to face conversation. This gives a user more control in the conversation because they do not have to give an immediate response.\n\nAnonymity can allow individuals to take greater risks and discuss how they truly feel with others. A person might take these risks because they are more aware of their private self. Private self-awareness is when a person becomes more aware of personal features of the self. This is in contrast to public self-awareness in which a person realizes that they can be judged by others. This type of awareness can lead to evaluation apprehension, where a person fears receiving a negative evaluation from their peers. Public self-awareness is also associated with conforming to group norms even if they go against personal beliefs. With that said, the absence of visual cues from a partner in Internet discussion can activate a person's private self which encourages self-disclosure. This is because the discloser is not worried about being judged publicly and is able to express their own private thoughts. Anonymity also aids in identity construction. A person can change their gender and the way they relate to others due to anonymity. This can increase life satisfaction because those who can identify with multiple roles are shown to be more satisfied. Since the Internet can allow someone to adopt these roles, that close others may not accept in the real world, it can increase their self-worth and acceptance.\nThe anonymity that comes with Internet communication also makes it easier to reveal the \"true self\". The \"true self\", as described by McKenna and her colleagues includes the traits a person possesses but is unable to share freely with others. What they do share is the \"actual self\" which includes traits they do possess and are able to be shown in social settings. The actual self can be easier to present in face to face conversations because a person's true self may not fit societal norms. Disclosing one's \"true self\" has been shown to create empathetic bonds and aid in forming close relationships.\n\nAnonymity can also help stigmatized groups reveal their \"true selves\" and allow them to come together to discuss aspects of the self that cannot be discussed in one's social circle. This can help them in life because it allows them to form a group of similar others and the opportunity to receive emotional support. It has also been found that those who join these groups and disclose their identity were more likely to share this aspect of the self with their close family and friends. Sharing these long kept secrets has also shown to significantly reduce health symptoms over a length of time.\n\nThere are some negative consequences to being anonymous on the Internet. Deindividuation, where self-awareness is blocked by environmental conditions, can occur and be problematic. Some consequences of deindividuation include the reduced ability to control one's behavior and engage in rational, long-term planning, and the tendency to react immediately and emotionally. A person who is lacking this self-awareness is also less likely to care about other's opinions of his or her behavior. This all can lead to increased hostility towards others and the formation of anonymous hate groups.\n\nThere can also be some negative consequences to forming multiple selves. If these identities are not integrated it can lead to an incomplete sense of self. They could also be brought into the real world and lead to delusional and unrealistic behavior.\nOne downside to all of the connections that can be formed online regards the effect called the \"illusion of large numbers.\" This effect means that people overestimate how many people share the same opinion as them. This can be especially harmful if someone holds negative views of a particular group because they may not realize that their views are very different from the mainstream.\n\nPhysical attractiveness plays an important role in determining if two people will begin a relationship. In face to face conversation, if initial attraction is not present, the relationship is less likely to form. This, however, does not play a role in Internet communication. Relationships online must form based on things such as similarities, values, interests or an engaging conversation style. Since these relationships form at a deeper level they may be more durable and more important to the individual. Not being seen also assists in presenting ideal qualities (attributes an individual would ideally like to possess) to other users because there is no information to contradict what they say, the way there is in face to face conversation. This can help a person make these ideal qualities a social reality because when someone confirms these traits the individual can make them a part of their self-concept.\n\nAn individual is also liked more on the Internet than in face to face conversation. Even if partners think they are communicating with two different people they still like the person from the Internet more than the face to face interaction, even though they were the same person. This greater liking also continues after the initial interaction on the Internet when the pair meets face to face. This greater liking may occur because of the lack of physical information. Physical attractiveness plays an important role in impression formation and once these views are formed they are not likely to be changed even when presented with new information. Since the people communicating online cannot rely on attractiveness these factors may not play a role when they eventually meet face to face. An increase in disclosures can also foster this liking because intimate disclosure is associated with increased intimacy. Online disclosures are generally seen as more intimate than face to face disclosures. Since there is a lack of nonverbal cues in Internet communication, many people form a biased perception of their communication partner. The minimal cues that are available in computer based communication are often over interpreted and the person will attach greater value to them. For example, if there seems to be a similarity between the two communicating, an individual may intensify this perception and idealize their partner. This all then increases the perceived intimacy of the discloser.\n\nPeople are more likely to form relationships with those who are in close physical proximity of them. Individuals are also more likely to begin an interaction with someone who is seen on a regular basis, showing that familiarity also influences interactions. Communicating on the Internet can allow individuals to become familiar with those who frequent the pages they converse on by recognizing usernames and pages. Regardless of how far away these individuals may be from each other, they are all in one confined space on the Internet which can give the feeling of being in the same place. The Internet also brings people together that may not have met because of physical distance. They can also go to specific websites where people share the same interests so they enter conversations knowing they already have similarities. This can contribute to why Internet relationships form so quickly. These online users do not have to go through the traditional stages that face to face interactions require in order to find similar interests. These face to face interactions usually take longer to find common ground but online users are able to dive right into conversations.\n\nInternet communication differs significantly from face-to-face conversation in the timing and pacing of a conversation. For example, both users do not need to be online at the same time to have a conversation. E-mail, for example, allows individuals to send messages and wait for a reply that may not come for hours or even days. This can allow many people to stay in touch, even if they are in different time zones, which significantly broadens the range of communication.\n\nThis communication also allows an individual to take their time when speaking with someone. They do not have to have an immediate response that face-to-face conversation requires. This allows them to carefully select and edit their messages and gives them more control over their side of the conversation that they would not have outside of the Internet. There are also no interruptions in online communication that occur in face-to-face conversation. A person is able to \"hold the floor\" and say as little or as much as they would like in these communications, allowing them to fully form their point.\n\nThis control helps users to take greater risks with their self-disclosures online. These people also begin to incorporate their Internet lives with their non-Internet lives and engage in a presence–control exchange. In this exchange, Internet users start their relationships with relatively high control and gradually trade that for physical closeness as their comfort levels and knowledge of the other person increases. This seems to be the Internet version of social penetration theory, where individuals have a mutual exchange of self-disclosures. As the relationship develops in face-to-face communication the individuals' disclosures gradually become more revealing and cover a wide range of topics. This equivalent on the Internet includes the partners exchanging control of the conversation for physical closeness. The stages this occurs in could include moving from messaging online, to telephone conversations and eventually face-to-face communication.\n\nThe use of social media for self-disclosure has shown to be very helpful for those with low self-esteem. People with low self-esteem are more socially anxious and shy which can make it difficult to form close relationships with others. This can harm both their physical and mental health because feeling connected to others is considered a fundamental human motivation. Individuals with low self-esteem have difficulty disclosing to others because they are very focused on not revealing their flaws and fear criticism and disapproval from others. Disclosing less, therefore, protects them from the possibility of rejection or being ignored. \nIn light of these fears, social media can provide a safe environment for people with low self-esteem to disclose personal information because they cannot see their partner's reactions which can help them to more freely express themselves.\n\nWhile many with low self-esteem do view social media as a safe outlet for disclosure, many do not receive positive feedback for their disclosures. People with low self-esteem tend to post more negative thoughts on social media which has been shown to make them less liked by readers. Negative posts are also more likely to be ignored by readers in hopes that the discloser will stop and begin to post more positively. When someone who frequently shares negative thoughts posts something positive they do receive more positive feedback from readers. In contrast, someone with high self-esteem is more liked by readers and tends to post more positively. If they do post something negative they tend to get more responses than those with low self-esteem do.\n\nSocial media can also help those who are lonely. Many social networking sites give access to profiles, pictures and the ability to comment and message others which helps people to feel less lonely. It also aids them in gaining social capital like emotional satisfaction and access to information. These sites can facilitate disclosure because they make it easier to access others who can provide social support for someone to disclose personal information. Social support is extremely important in disclosure as it makes the discloser feel validated and cared for. Social support is also positively related to well-being. It has also been shown that having this social support and forming close relationships online decreases loneliness overtime.\n\nSome research does show that spending too much time on the Internet and forming these close relationships could take away time from existing non-Internet relationships. Neglecting these relationships could make a person lonelier in the long run because they could lose these face to face relationships.\n\nHowever, other research shows that there are certain personality traits that lead to increased loneliness which then leads to increased use of the Internet. In particular, extroversion and neuroticism have been linked to loneliness. An extrovert is someone who is outgoing, enjoys the company of others, requires stimulation, and is spontaneous, while an introvert prefers his or her own company, is quiet, and prefers quiet, small gatherings. Introverts can often be seen as distant and unfriendly because of this behavior which may explain some of their loneliness. A neurotic person is extremely anxious, emotional and reacts in a disproportional way to many situations. Someone high in neuroticism generally has a negative attitude which may push people away and prevent them from forming close relationships which may lead to their loneliness. Both of these groups (introverts and neurotics) have been shown to have increased Internet use and in particular increased use of social service sites (i.e. chatrooms, newsrooms, etc.). This may show that those who are already lonely are more attracted to the Internet as a means of social networking and not that the Internet increases loneliness. Introverts and neurotic individuals have also been shown to feel more comfortable revealing their \"true-self\" online than in face-to-face conversation and revealing the \"real you\" has been shown to help the discloser to form close relationships.\n\nIt can be very difficult for those with social anxiety to engage in face to face communication. These people can become anxious when meeting someone for the first time, speaking with someone attractive, or participating in group activities. This can limit their in-person interactions and deny them their basic needs of intimacy and belonging. With the absence of many of these worries in Internet communication, many with social anxieties use it to form social connections. It has been shown that individuals with social anxiety are more likely to use the Internet to form close relationships. These relationships are also shown to be stronger online relationships as opposed to weaker relationships (i.e. \"acquaintances\"). Forming these relationships can also help a socially anxious person express their true-self and form their social identity. This identity often involves the groups a person is a part of because belonging to a group frequently becomes a part of one's self-concept. Someone with social anxiety would be denied this because of their fear of face-to-face interaction. Therefore, disclosing with others online gives a socially anxious person access to a wide variety of people with which they can form relationships and belong to a group.\n\nSocially anxious people are also shown to become less anxious over time if they have formed close online relationships. They have also been shown to broaden their social circles in the \"real world\" when they have had this time to form online relationships. One possibility for this occurrence may be that these online relationships can give the anxious individuals confidence in forming relationships outside of the Internet. Being able to practice communications online can show them they are capable of communicating and can lessen their anxieties in face to face communication. They are also very likely to bring their online relationships into their offline lives in order to make them a \"social reality\" by sharing these relationships with family and friends in the real world.\n\nOnline support groups are another place where people from all over can come together to disclose common struggles. They provide an environment of mutual disclosure and support. People are more likely to use these forums to discuss personal struggles and disclose emotions and thoughts pertaining to these struggles than normal discussion forums. There is also a higher degree of reciprocity in online support groups than in normal discussion forums and reciprocity has been shown to help people feel valued after disclosing. Men and women are also equally likely to use these forums for disclosing personal information.\n\nWhile there are many benefits to engaging in self-disclosure online there are also some dangers. There is a relationship between Internet abuse, self-disclosure and problematic behavior. Internet abuse can be defined as, \"patterns of using the Internet that result in disturbances in a person's life but does not imply a specific disease process or addictive behavior.\" When a person is high for Internet abuse and high for self-disclosure it can lead to dangerous behaviors like sending personal information (addresses, home phone number etc.) and photos to online acquaintances. High ratings for Internet abuse and self-disclosure also positively influence online communication with all types of online relationships. These relationship types include long-distance relationships, in which people have met face to face and continue the relationship by communicating online; purely virtual relationships, where people meet online and stay in touch only by using the Internet; and migratory mixed-mode, where the relationship begins online and then proceeds to face to face interaction. The relationship between Internet abuse, self-disclosure and dangerous behaviors could pose an even bigger problem with the high number of communications this group has with others, particularly those they have only communicated with online.\n\nThe Internet, while providing most with a source of entertainment, knowledge, and social realms, is actually a large threat to children due to the ways in which they self-disclose. Their privacy is often more at risk than is an adults because of their openness to sites. Given that they are still developing, researchers say that they are in the \"Cued Processors\" group between the ages of eight and eleven. At this time, many children are using the Internet and are doing so alone, without the guidance and overseeing of an adult/guardian. Thus, they must use their own judgments to decide how much information to share on the various sites they visit.\n\nAs \"Cued Processors\", however, they are only able to think logically about concrete events; the notion of their disclosures online being used against them is far in the abstract world. They will likely not think of any sort of consequences that could result from their disclosures, and this is just what online marketers and predators alike expect and are looking for. Combined with behavioral profiling tracking programs, online advertisers and predators can build a pretty clear image of the child and what he or she likes to do, where he or she lives, his or her phone number, his or her school district, and other sources of identifying information that they use to prompt the child to disclose without his or her really knowing. A common strategy is the use of brand characters in online games who \"ask\" for the information; children are especially likely to give out very personal information in this sort of setting. The children's vulnerability online is a product of their cognitive limitations.\n\nUses and gratifications theory is often used to attempt to explain such things as motivation on the Internet. Studies have found that, if applied to the use of the Internet by children and their likelihood to disclose personal information, one can find significant correlations with various types of motivation. Children who use the Internet primarily as a source of information are less likely to give out personal information. Some theorize that these children are simply made to be more aware of the dangers of Internet disclosures and are more cautious because of this. But, children who mention social contact on the Internet as their first-order use are more often the ones who submit to the attempts of online marketers and predators who seek their personal contact information and behavioral preferences. These children have goals of social acceptance in mind, and it seems to them that acceptance can be easily gained from sharing and communicating with friends and strangers alike. Socializing motives reduce privacy concerns, and children will disclose nearly anything online in order to be seen and responded to socially. It was also discovered that a simple incentive is usually enough to elicit personal information from a child.\n\nParents' knowledge of their children's Internet use is rapidly decreasing. Children are withholding more and more from their parents, including how much information they are sharing over the Internet. Parent-child self-disclosure about this topic needs to be increased if interventions are to help keep children safer online. Notably, there are many parents who have even admitted to allowing their children to lie about their ages on social media sites in order to gain access to them. Parents, thus, are encouraged to remain open to discussing such things with their children, to use better judgment themselves when making decisions about their children's Internet usage, and to provide them with education about how privacy on the Internet is a risky notion.\n\nToday, many regulations are being proposed and implemented that will hopefully help protect children's personal information on the Internet. However, these will not be enough to guarantee safe exchanges of self-disclosure, so adults still must be open to discussion with their children.\n\nSelf-disclosure is an important matter to consider in the realm of education. The varying ways that it can impact social relations adds a new and important dynamic to the classroom. There are different results and experiences that students and teachers see from the implementation of self-disclosure in the classroom. The relationships that will be addressed through the lens of self-disclosure include the student to teacher relationship, the student to student relationship and how cultural relations impacts the situation as a whole.\n\nThe tone of the classroom is set by the attitudes and behaviors of those who participate in it. The teacher often has the most powerful role in leading a classroom and how that class will interact and connect through the subject matter. The practice of self-disclosure in the interactions between the teachers and the students has an impact on the classroom atmosphere and how the people perform in that atmosphere. The decision to practice self-disclosure as a teacher has many benefits and challenges.\n\nWhen the teacher engages in self-disclosure with the students, it opens up a new channel of communication in the classroom. As the teacher shares more information about who they are and their personal life, the students begin to see a new side of their teacher that is more than the person that stands in the front of their classroom everyday. The teacher is seen as a real person with their own difficulties and struggles in life. This would allow the teacher to appear more relatable to the students which would promote better student to teacher communication. Of course, the information shared with the class must be appropriate and relevant. A teacher may use an illustration of a concept using an example from their own life in order to connect with a particular audience in the class. These connections with the teacher promotes a more productive relationship.\n\nAs the teacher sets the tone of self-disclosure, students feel more willing to share in the practice of self-disclosure as well. The teacher demonstrates and helps to guide the students in understanding what is appropriate information to share in public discourses. As the students feel more comfortable with the teacher and begin sharing more about their own lives, the environment of the classroom is one of camaraderie and friendship. By understanding the people in the classroom on a deeper level can open up opportunities to provide support to those involved. The teacher can better understand who the students are, what they struggle with, what their strengths are and what they need to succeed. Self-disclosure from student to teacher allows the teacher to best support the students based on their individual needs, therefore providing an improved education.\n\nWith implementing self-disclosure into the classroom, comes a set of negative consequences and challenges. As the teacher shares more about their personal life, the students may become overly comfortable with the teacher. This could lead to a lack of respect for the teacher or an inability to maintain appropriate superior relationship. Self-disclosure may blur the lines of the roles between the student and the teacher, which could disrupt the authority the teacher needs to maintain their role in the classroom and have an effective teaching persona. There is the case that not all students will connect to this method of teaching. Some students may not choose to participate in this environment which could lead them to feel alienated. Self-disclosure from the teacher needs to be taken into deep consideration so that the sharing of information does not take away from the education being transferred.\n\nThere are some risks involved in bringing self-disclosure into the classroom when students begin sharing information with the teacher. As the student is more open with the teacher, there is the chance that the student could share information that would require the teacher to follow a reporting procedure. If a student reveals information about themselves in confidence to the teacher that implies that the students life is potentially at risk, or other matters of equal seriousness that would need to be reported to the school guidance counselor. Revealing this information although confidentiality was implied would inevitably break the trust the teacher has built with the student, ultimately harming their relationship. This hurt relationship could negatively impact that students ability to learn in the classroom. In another scenario, students may not fully understand the differences between public and private discourse. This would lead students to have conversations of self-disclosure in the classroom when the timing is not appropriate, therefore, taking away from the educational matters at hand.\n\nSelf-disclosure, just like anything varies and differs depending on the culture. Collectivistic culture and individualism are two types of ways to explain self disclosure is a culture. If a country is more on the collectivistic side then they will tend to disclose themselves more as an Avatar, like in China and Germany. However, in a more individualist culture setting people open up more about themselves, even personal details, like in America. There is also a difference in the boy vs. girl culture. Girls tend to open up more and easier than most boys.\n\nEach culture has a different idea about what and how much self-disclosure is acceptable. For example, American students tend to share more in class with their peers than Chinese students. They are usually more open about themselves and interests with most of their classmates than students in other countries. The difference is seen in the internet as well. Korean students usually talk more in blog form on social media pages keeping the posts short and to the point. However, American students share more often and share more personal information to their followers. Cultures like Korea and China, the collectivistic cultures, are more reserved whereas, the American culture is more about disclosing a lot of personal details.\n\n"}
{"id": "48304379", "url": "https://en.wikipedia.org/wiki?curid=48304379", "title": "Sensitivity auditing", "text": "Sensitivity auditing\n\nSensitivity auditing is an extension of sensitivity analysis for use in policy-relevant modelling studies. Its use is recommended - e.g. in the European Commission Impact assessment guidelines - when a sensitivity analysis (SA) of a model-based study is meant to demonstrate the robustness of the evidence provided by the model, but in a context where the inference feeds into a policy or decision-making process.\n\nIn these cases, the framing of the analysis itself, its institutional context, and the motivations of its author may become highly relevant, and a pure SA - with its focus on parametric (i.e. quantified) uncertainty - may be insufficient. The emphasis on the framing may, among other things, derive from the relevance of the policy study to different constituencies that are characterized by different norms and values, and hence by a different story about `what the problem is' and foremost about `who is telling the story'. Most often the framing includes implicit assumptions, which could be political (e.g. which group needs to be protected) all the way to technical (e.g. which variable can be treated as a constant).\n\nIn order to take these concerns into due consideration, sensitivity auditing extends the instruments of sensitivity analysis to provide an assessment of the entire knowledge- and model-generating process. It takes inspiration from NUSAP, a method used to qualify the worth (quality) of quantitative information with the generation of `Pedigrees' of numbers. Likewise, sensitivity auditing has been developed to provide pedigrees of models and model-based inferences. Sensitivity auditing is especially suitable in an adversarial context, where not only the nature of the evidence, but also the degree of certainty and uncertainty associated to the evidence, is the subject of partisan interests. These are the settings considered in Post-normal science or in Mode 2 science. Post-normal science (PNS) is a concept developed by Silvio Funtowicz and Jerome Ravetz, which proposes a methodology of inquiry that is appropriate when “facts are uncertain, values in dispute, stakes high and decisions urgent” (Funtowicz and Ravetz, 1992: 251–273). Mode 2 Science, coined in 1994 by Gibbons et al., refers to a mode of production of scientific knowledge that is context-driven, problem-focused and interdisciplinary. Carrozza (2015) offers a discussion of these concepts and approaches.\n\nSensitivity auditing is recommended by the European Commission for use in impact assessments in order to improve the quality of model-based evidence used to support policy decisions.\n\nSensitivity auditing is summarised by seven rules or guiding principles:\n\nThe first rule looks at the instrumental use of mathematical modeling to advance one's agenda. This use is called rhetorical, or strategic, like the use of Latin to confuse or obfuscate an interlocutor.\n\nThe second rule about `assumption hunting' is a reminder to look for what was assumed when the model was originally framed. Modes are full of ceteris paribus assumptions. For example, in economics, the model can predict the result of a shock to a given set of equations, assuming that all the rest - all other input variables and inputs - remain equal, but in real life \"ceteris\" are never \"paribus\", meaning that variables tend to be linked with one another, so they cannot realistically change independently of one another.\n\nRule three is about artificially exaggerating or playing down uncertainties wherever convenient. The tobacco lobbies exaggerated the uncertainties about the health effects of smoking according to Oreskes and Conway, while advocates of the death penalty played down the uncertainties in the negative relations between capital punishment and crime rate. Clearly the latter wanted the policy, in this case the death penalty, and were interested in showing that the supporting evidence was robust. In the former case the lobbies did not want regulation (e.g. bans on tobacco smoking in public places) and were hence interested in amplifying the uncertainty in the smoking-health effect causality relationship.\n\nRule four is about `confessing' uncertainties before going public with the analysis. This rule is also one of the commandments of applied econometrics according to\nKennedy: `Thou shall confess in the presence of sensitivity. Corollary: Thou shall anticipate criticism'. According to this rule, a sensitivity analysis should be performed before the results of a modeling study are published. There are many good reasons for doing this, one being that a carefully performed sensitivity analysis often uncovers plain coding mistakes or model inadequacies. The other is that, more often than not, the analysis reveals uncertainties that are larger than those anticipated by the model developers.\n\nRule five is about presenting the results of the modeling study in a transparent fashion. Both rules originate from the practice of impact assessment, where a modeling study presented without a proper SA, or as originating from a model which is in fact a black box, may end up being rejected by stakeholders. Both rules four and five suggest that reproducibility may be a condition for transparency and that this latter may be a condition for legitimacy.\n\nRule six, about doing the right sum, is not far from the `assumption-hunting' rule; it is just more general. It deals with the fact that often an analyst is set to work on an analysis arbitrarily framed to the advantage of a party. Sometime this comes via the choice of the discipline selected to do the analysis. Thus an environmental impact problem may be framed through the lenses of economics, and presented as a cost benefit or risk analysis, while the issue has little to do with costs or benefits or risks and a lot to do with profits, controls, and norms. An example is in Marris et al. on the issue of GMOs, mostly presented in the public discourse as a food safety issue while the spectrum of concerns of GMO opponents - including lay citizens - appears broader.\n\nRule seven is about avoiding a perfunctory sensitivity analysis. A SA where each uncertain input is moved at a time while leaving all other inputs fixed is perfunctory. A true SA should make an honest effort at exploring all uncertainties simultaneously, leaving the model free to display its full nonlinear and possibly non-additive behaviour. A similar point is made in Sam L. Savage's book `The flaw of averages'.\n\nIn conclusion, these rules are meant to help an analyst to anticipate criticism, in particular relating to model-based inference feeding into an impact assessment. What questions and objections may be received by the modeler? Here is a possible list:\n\nSensitivity auditing is described in the European Commission Guidelines for impact assessment . Relevants excerpts are (pp. 392): \n"}
{"id": "4818168", "url": "https://en.wikipedia.org/wiki?curid=4818168", "title": "Sexualization", "text": "Sexualization\n\nSexualization (or sexualisation) is to make something sexual in character or quality, or to become aware of sexuality, especially in relation to men and women. Sexualization is linked to sexual objectification. According to the American Psychological Association, sexualization occurs when \"individuals are regarded as sex objects and evaluated in terms of their physical characteristics and sexiness.\" \"In study after study, findings have indicated that women more often than men are portrayed in a sexual manner (e.g., dressed in revealing clothing, with bodily postures or facial expressions that imply sexual readiness) and are objectified (e.g., used as a decorative object, or as body parts rather than a whole person). In addition, a narrow (and unrealistic) standard of physical beauty is heavily emphasized. These are the models of femininity presented for young girls to study and emulate.\" Women who embrace their sexual desires are considered to be sexy and attractive to men who want nothing more than to have a woman as a sex toy. In the eyes of men, women that practice this behavior serve the pure purpose of providing satisfaction and showcasing their human nature. According to the Media Education Foundation's, \"Killing Us Softly 4: Advertising's Image of Women\", the sexualization of girls in media and the ways women are portrayed in the dominant culture are detrimental to the development of young girls as they are developing their identities and understanding themselves as sexual beings.\n\nReports have found that sexualization of younger children is becoming increasingly more common in advertisements. Research has linked sexualization of young girls to negative consequences for girls and society as a whole, finding that the viewing of sexually objectifying material can contribute to body dissatisfaction, eating disorders, low self-esteem, depression and depressive affect. Medical and social science researchers generally deployed \"sexualization\" to refer to a liminal zone between sexual abuse and normal family life, in which the child's relationship with their parents was characterized by an \"excessive\", improper sexuality, though without recognizable forms of abuse having occurred. American Psychological Association also argues that the sexualization of young girls contributes to sexist attitudes within society, and a societal tolerance of sexual violence.\n\nFrom 2003 to 2005, \"sexualization\" began to ascend to the status of an issue in the public eye. The cause of this rise was that it became positioned by a number of discursive actors as a feminist issue. This is not to say that a single \"feminist perspective on sexualization\" emerged in this period; among discursive actors mobilizing feminist discourses, or identifying themselves explicitly with feminism, there were a host of different views. Yet a particular, relatively cohesive position emerged after 2003 among a number of media discourses: these discourses tended to emphasize that, in the context of a commercialized and sexist culture, young women are unable to exercise meaningful choice even when they experience themselves as doing so. These media actors, in their problematization of sexualization, positioned themselves as the true heirs to the feminist tradition and its critical insights, in contrast to contemporary youth.\n\nConsumerism and globalization has led to sexualization of girls occurring across all advanced economies, in media and advertisements, to clothing and toys marketed for young girls.\n\nThe term \"sexualization\" itself only emerged in Anglophone discourse in recent decades. Beginning in the mid-nineteenth century, the term was infrequently drawn upon by English writers to refer the assignation of a gendered frame to a particular object, such as the gendering of nouns (e.g., de Quincey [1839]1909, 195). In contrast, the term \"asexualization\" saw greater use, as a synonym for sterilization in eugenics discourse from around the turn of the twentieth century (e.g., Lydston 1904). The opposite or antonym to sexualization is desexualization.\n\nIn 2006, an Australian report called \"Corporate paedophilia: sexualisation of children in Australia\" was published. The Australian report summarises its conclusion as follows:\n\nIn 2007, the American Psychological Association published a report titled \"Report of the APA Task Force on the Sexualization of Girls\", discussed below.\n\nIn 2010, the American Psychological Association published an additional report titled \" Report of the APA Task Force on the Sexualization of Girls\", which performed a study where college students were asked to try on and evaluate either a swimsuit or a sweater. While they waited for 10 minutes wearing the garment, they completed a math test. The results revealed that young women in swimsuits performed significantly worse on the math problems than did those wearing sweaters.The hypothesis is that individuals about to try on the sweaters had less pressure to look beautiful because they were not wearing revealing clothing therefore they performed better.\n\nIn 2012, an American study found that self-sexualization was common among 6–9-year old girls. Girls overwhelmingly chose the sexualized doll over the non-sexualized doll for their ideal self and as popular. However other factors, such as how often mothers talked to their children about what is going on in TV shows and maternal religiosity, reduced those odds. Surprisingly, the mere quantity of girls' media consumption (TV and movies) was unrelated to their self-sexualization for the most part; rather, maternal self-objectification and maternal religiosity moderated its effects.\n\nHowever, in 2010 the Scottish Executive released a report titled \"External research on sexualised goods aimed at children\". The report considers the drawbacks of the United States and Australian reviews, concluding:\n\nThe Scottish review also notes that:\n\nIt also notes that previous coverage \"rests on moral assumptions … that are not adequately explained or justified.\"\n\nLetting Children be Children: Report of an Independent Review of the Commercialisation and Sexualisation of Childhood (UK)\nThe report 'Letting Children Be Children', also known as the Bailey Report, is a report commissioned by the UK government on the subject of the commercialisation and sexualisation of childhood. The report was published in June 2011 and was commissioned as a result of concerns raised as to whether children's lives are negatively affected by the effects of commercialisation and sexualisation.\n\nThe Bailey Report is so-called as it was researched and compiled by Reg Bailey, the Chief Executive of the Mothers' Union, a \"charity supporting parents and children in 83 countries in the world\". The report asked for contributions from parents; children; organisations; businesses and the general public in order to consider their views and inform their recommendations and identified four themes that were of particular concern to parents and the wider public. These themes were:\n\nThe report returned recommendations based on the research from interested parties, on each of the key themes, in the form of \"what we would like to see\". On the theme of \"the wallpaper of children's lives\" it said that it would like to see that sexualised images used in public places should be more in line with what parents find acceptable, to ensure that images in public spaces becomes more child friendly. On theme two \"clothing, products and services for children\" the Bailey report said that it would like to see retailers no longer selling or marketing inappropriate clothing, products or services for children. What they would like to see on theme three \"children as consumers\" is comprehensive regulation protecting children from excessive commercial pressures across all media in-line with parental expectations; that marketers are ethical and do not attempt to exploit gaps in the market to influence children into becoming consumers and to ensure that parents and children have an awareness of marketing techniques and regulations. Finally in terms of \"making parents voices heard\" it would like to see parents finding it easier to voice their concerns to, and be listened to by, businesses and regulators.\n\nThere is a motion for a European Parliament resolution going through which gives the following definition of sexualization: \n\nThe sexualization of women of color is different than the sexualization of white women. The media plays a significant role in this sexualization. \"The media are likely to have powerful effects if the information is presented persistently, consistently, and corroborated among forms. As a media affect, stereotypes rely on the repetition to perpetuate and sustain them.\" According to Celine Parrenas Shimizu, \"To see race is to see sex, and vice versa.\"\n\nIn an NPR interview with Professor Herbert Samuels at LaGuardia Community College in New York and Mireille Miller-Young a professor at UC Santa Barbara they talk about sexual stereotypes of black bodies in America and how even in sex work, already a dangerous job, black women are treated much worse than their counterparts due to the effects of their oversexualization and objectification in society. Black women's bodies are either invisible or hypervisible. In the 1800s, a South African woman named Sarah Baartman was known as \"Hottentot Venus\" and her body was paraded around in London and Paris where they looked at her exotic features such as large breasts and behind. Her features were deemed lesser and oversexual. There is also the \"Jezebel\" stereotype that portrays black women as \"hypersexual, manipulative, animalistic and promiscuous females who cannot be controlled.\"\n\nIn the Dominican Republic, women are frequently stereotyped as sultry and sexual as the reputation of Dominican sex workers grows. Many poor women have resorted to sex work because the demand is high and the hours and pay are often dictated by the workers themselves. White European and American men \"exoticize dark-skinned 'native' bodies\" because \"they can buy sex for cut-rate prices\". This overgeneralizing of the sexuality of Dominican women can also carries back to the women's homes. Even \"women who...worked in Europe have become suspect...\" even if they had a legal job. They have become \"exports\" instead of people because of their sexualization.\n\nThe image of Asian women in Hollywood cinema is directly linked to sexuality as essential to any imagining about the roles they play as well as her actual appearance in popular culture. Asian female fatale's hypersexualized subjection is derived from her sexual behaviour that is considered as natural to her particular race and culture. Two types of Asian stereotypes that are commonly found in media are the Lotus Flower and the Dragon Lady. The Lotus Flower archetype is the \"self-sacrificing, servile, and suicidal Asian women.\" The dragon lady archetype is the opposite of the lotus flower, a \"self-abnegating Asian woman…[who] uses her 'Oriental' femininity, associated with seduction and danger to trap white men on behalf of conniving Asian males.\" According to film-maker and film scholar, Celine Shimizu, \"The figure of the Asian American femme fatale signifies a particular deathly seduction. She attracts with her soft, unthreatening, and servile femininity while concealing her hard, dangerous, and domineering nature.\"\n\nStarting from the time of white colonization of Native American land, some Native American women have been referred to as \"squaw,\" an Algonquin word for vagina. \"The 'squaw' [stereotype] is the dirty, subservient, and abused tribal female who is also haggard, violent, and eager to torture tribal captives.\" Another stereotype is the beautiful Indian princess who leaves her tribe and culture behind to marry a white man.\n\nLatina characters that embody the hot Latina stereotype in film and television is marked by easily identifiable behavioral characteristics such as \"'addictively romantic, sensual, sexual and even exotically dangerous', self-sacrificing, dependent, powerless, sexually naive, childlike, pampered, and irresponsible\". Stereotypical Latina physical characteristics include \"red lips, big bottoms, large hips, voluptuous bosoms, and small waists\" and \"high heels, huge hoop earrings, seductive clothing.\" Within the hot Latina stereotype lies three categories of representation: the Cantina Girl, the Faithful, self-sacrificing señorita, and the vamp. The Cantina Girl markers are \"'great sexual allure,' teasing, dancing, and 'behaving in an alluring fashion.'\" The faithful, self-sacrificing Señorita starts out as a good girl and turns bad by the end. The Señorita, in an attempt to save her Anglo love interest, utilizes her body to protect him from violence. The Vamp representation \"uses her intellectual and devious sexual wiles to get what she wants.\" The media represents Latinas \"as either [a] hot-blooded spitfire\" or \"[a] dutiful mother\". The sexual implications of the \"hot-blooded\" Latina has become an overgeneralized representation of Latin people. This has led many to see the Latin people as \"what is morally wrong\" with the United States. Some believe it to be wrong simply because the interpretation of this culture seems to go against white, Western culture. Culturally, the Latina is expected to dress \"as a proper señorita\" in order to be respected as a woman which conflicts with the Western ideals that a girl is sexual if she dresses \"too 'mature' for [her] age\". Even in the business world this stereotype continues; \"tight skirts and jingling bracelets [are misinterpreted] as a come-on\". This sexualization can also be linked to certain stereotypical jobs. The image of the Latina woman often is not in the business world but in the domestic. The sexualization of Latina women sexualizes the positions that they are expected to occupy. Domestic servants, maids, and waitresses are the typical \"media-engendered\" roles that make it difficult for Latinas to gain \"upward mobility\" despite the fact that many hold PhDs.\n\nThe American Psychological Association (APA) in its 2007 Report looked at the cognitive and emotional consequences of sexualization and the consequences for mental and physical health, and impact on development of a healthy sexual self-image. The report considers that a person is sexualized in the following situations:\n\nSome cultural critics have postulated that over recent decades children have evidenced a level of sexual knowledge or sexual behaviour inappropriate for their age group.\n\nThe causes of this premature sexualization that have been cited include portrayals in the media of sex and related issues, especially in media aimed at children; the marketing of products with sexual connotations to children, including clothing; the lack of parental oversight and discipline; access to adult culture via the internet; and the lack of comprehensive school sex education programs.\n\nFor girls and young women in particular, the APA reports that studies have found that sexualization has a negative impact on their \"self-image and healthy development\".\n\nThe APA cites the following as advertising techniques that contribute to the sexualization of girls:\nThe APA additionally further references the teen magazine market by citing a study by Roberts \"et al\" that found that \"47% of 8- to 18-year-old [girls] reported having read at least 5 minutes of a magazine the previous day.\" A majority of these magazines focused on a theme of presenting oneself as sexually desirable to men, a practice which is called \"costuming for seduction\" in a study by Duffy and Gotcher.\n\nStudies have found that thinking about the body and comparing it to sexualized cultural ideals may disrupt a girl's mental concentration, and a girl's sexualization or objectification may undermine her confidence in and comfort with her own body, leading to emotional and self-image problems, such as shame and anxiety.\n\nResearch has linked sexualization with three of the most common mental health problems diagnosed in girls and women: eating disorders, low self-esteem, and depression or depressed mood.\n\nResearch suggests that the sexualization of girls has negative consequences on girls' ability to develop a healthy sexual self-image.\n\nA result of the sexualization of girls in the media is that young girls are \"learning how to view themselves as sex objects\". When girls fail to meet the thin ideal and dominant culture's standard of beauty they can develop anxieties. Sexualization is problematic for young children who are developing their sexual identity as they may think that turning themselves into sex objects is empowering and related to having sexual agency.\n\nSome commercial products seen as promoting the sexualization of children have drawn considerable media attention:\n\nThe Scottish Executive report surveyed 32 High street UK retailers and found that many of the larger chains, including Tesco, Debenhams, JJ Sports, and Marks & Spencer did not offer sexualized goods aimed at children. The report noted that overall prevalence was limited but this was based on a very narrow research brief. Whilst this shows that not all High street retailers were aiming products deemed sexualized by the researchers, the research cannot be taken out of context and used to say that there is not an issue of sexualization.\n\nSexualization has also been a subject of debate for academics who work in media and cultural studies. Here, the term has not been used to simply to label what is seen as a social problem, but to indicate the much broader and varied set of ways in which sex has become more visible in media and culture. These include;\nthe widespread discussion of sexual values, practices and identities in the media;\nthe growth of sexual media of all kinds; for example, erotica, slash fiction, sexual self-help books and the many genres of pornography; \nthe emergence of new forms of sexual experience, for example instant message or avatar sex made possible by developments in technology; \na public concern with the breakdown of consensus about regulations for defining and dealing with obscenity; \nthe prevalence of scandals, controversies and panics around sex in the media.\n\nThe terms \"pornification\" and \"pornographication\" have also been used to describe the way that aesthetics that were previously associated with pornography have become part of popular culture, and that mainstream media texts and other cultural practices \"citing pornographic styles, gestures and aesthetics\" have become more prominent. This process, which Brian McNair has described as a \"pornographication of the mainstream\". has developed alongside an expansion of the cultural realm of pornography or \"pornosphere\" which itself has become more accessible to a much wider variety of audiences. According to McNair, both developments can be set in the context of a wider shift towards a \"striptease culture\" which has disrupted the boundaries between public and private discourse in late modern Western culture, and which is evident more generally in cultural trends which privilege lifestyle, reality, interactivity, self-revelation and public intimacy.\n\nChildren and adolescents spend more time engaging with media than any other age group. This is a time in their life that they are more susceptible to information that they receive. Children are getting sex education from the media, little kids are exposed to sexualized images and more information than ever before in human history but are not able to process the information, they are not developmentally ready to process it, and this impacts their development and behavior.\n\nSexualization of young girls in the media and infantilization of women creates an environment where it becomes more acceptable to view children as \"seductive and sexy\". It makes having healthy sexual relationships more difficult for people and creates sexist attitudes. Sexualization also contributes to sexual violence and childhood sexual abuse \"where 1 in 4 girls and 1 in 6 boys are sexually abused during childhood\".\n\nThe Australian writers, Catharine Lumby and Kath Albury (2010) have suggested that sexualization is \"a debate that has been simmering for almost a decade\" and concerns about sex and the media are far from new. Much of the recent writing on sexualization has been the subject of criticism that because of the way that it draws on \"one-sided, selective, overly simplifying, generalizing, and negatively toned\" evidence and is \"saturated in the languages of concern and regulation\". In these writings and the widespread press coverage that they have attracted, critics state that the term is often used as \"a non-sequitur causing everything from girls flirting with older men to child sex trafficking\" They believe that the arguments often ignore feminist work on media, gender and the body and present a very conservative and negative view of sex in which only monogamous heterosexual sexuality is regarded as normal. They say that the arguments tend to neglect any historical understanding of the way sex has been represented and regulated, and they often ignore both theoretical and empirical work on the relationship between sex and media, culture and technology.\n\nThe sexualization of women being influenced by society is a problem that should be avoided due to its impact on how women value and present themselves. The way society shapes ones personal interest is presented in a book review of \"Girls Gone Skank\" by Patrice Oppliger, Amanda Mills states that \"consequently, girls are socialized to participate in their own abuse by becoming avid consumers of and altering their behavior to reflect sexually exploitative images and goods.\" The belief that women are powerful and fully capable as men is stated in the text \"Uses Of The Erotic: The Erotic As Power\" by Audre Lorde stating that the suppression of the erotic of women has led them feeling superior to men \"the superficially, erotic had been encouraged as a sign of female inferiority on the other hand women have been made to suffer and to feel opposed contemptible and suspect by virtue of its existence\".\n\n\n\n\n\n"}
{"id": "51125950", "url": "https://en.wikipedia.org/wiki?curid=51125950", "title": "Soledad Cazorla", "text": "Soledad Cazorla\n\nSoledad Cazorla Prieto (19 February 1955 – 4 May 2015) was a jurist and the first Spanish prosecutor against gender violence. She directed a network of fiscal specialists who worked in this field in Spain after the creation of the 2004 Integral Law against Gender Violence. She held the position for a decade, from 2005 until her death. Remembered as a defender of equality, her professional career was closely linked to the development of this law.\n\nCazorla was born in 1955 in Larache, then a Spanish protectorate in Morocco, where her family had emigrated at the close of the 19th century. Her father was a military officer of high rank. Her older brother, Luis María Cazorla, is also a jurist. She started her career in 1981 at the Fiscalía of Girona. In 1984, she moved to Valladolid. In 1985, she joined the Audiencia Territorial de Madrid, followed by the Inspection Office of the Fiscalía General del Estado in 1993, and afterwards, the Secretaría Técnica.\n\nIn September 1996, she was appointed public prosecutor of the High Court, where she gained, between other responsibilities, a public indictment against Mario Conde in the Banesto case.\n\nIn 2005, she rose to Public Prosecutor in the Section against Violence towards Woman she had proposed to the fiscal general of the State, Cándido Conde-Pumpido, and in October 2010, she was re-appointed. The section formed part of the Observation Against Gender Violence.\n\nCazorla participated in international meetings in defence of women's rights (France, United Kingdom, Morocco, Dominican Republic, Bolivia, Ecuador, China or Niger) as a national jurist, collaborating in publications and articles concerning different matters related with Penal Laws.\n\nAs a defender of equality, and with a high personal commitment and professional interest in the fight against gender violence, her professional career was closely linked to the development of this law in Spain, especially, in the need to protect children who coexist with the hard reality of violence against mothers.\n\nCazorla was married to the journalist Joaquín Tagar and had 3 children. She died of stroke in Madrid on 4 May 2015 .\n\n"}
{"id": "14463872", "url": "https://en.wikipedia.org/wiki?curid=14463872", "title": "Sous rature", "text": "Sous rature\n\nSous rature is a strategic philosophical device originally developed by Martin Heidegger. Usually translated as 'under erasure', it involves the crossing out of a word within a text, but allowing it to remain legible and in place. Used extensively by Jacques Derrida, it signifies that a word is \"inadequate yet necessary\"; that a particular signifier is not wholly suitable for the concept it represents, but must be used as the constraints of our language offer nothing better.\n\nIn the philosophy of deconstruction, \"sous rature\" has been described as the typographical expression that seeks to identify sites within texts where key terms and concepts may be paradoxical or self-undermining, rendering their meaning undecidable. To extend this notion, deconstruction and the practice of \"sous rature\" also seek to demonstrate that meaning is derived from difference, not by reference to a pre-existing notion or freestanding idea.\n\n\"Sous rature\" as a literary practice originated in the works of German philosopher Martin Heidegger (1889–1976). The practice of placing words or terms under erasure first appeared in Heidegger's work \"The Fundamental Concepts of Metaphysics\", Heidegger's lecture course of 1929/30. And subsequently in a letter he penned to Ernst Jünger in 1956 titled \"Zur Seinsfrage\" (The Question of Being), in which Heidegger seeks to define nihilism. During the course of the letter, Heidegger also begins to speculate about the problematic nature of defining anything, let alone words. In particular, the meaning of the term ‘Being’ is contested and Heidegger crosses out the word, but lets both the deletion and the word remain. “Since the word is inaccurate, it is crossed out. Since it is necessary, it remains legible.” According to the Heideggerian model, erasure expressed the problem of presence and absence of meaning in language. Heidegger was concerned with trying to return the absent meaning to the present meaning and the placing of a word or term under erasure “simultaneously recognised and questioned the term’s meaning and accepted use”.\n\nFrench philosopher Jacques Derrida (1930–2004) adopted this technique and further explored the implications of Heidegger's erasure and its application in the wider setting of deconstructive literary theory. Derrida extended the problem of presence and absence to include the notion that erasure does not mark a lost presence, rather the potential impossibility of presence altogether - in other words, the potential impossibility of univocity of meaning ever having been attached to the word or term in the first place. Ultimately, Derrida argued, it was not just the particular signs that were placed under erasure, but the whole system of signification.\n\n\n"}
{"id": "730120", "url": "https://en.wikipedia.org/wiki?curid=730120", "title": "The Three Types of Legitimate Rule", "text": "The Three Types of Legitimate Rule\n\n\"The Three Types of Legitimate Rule\" (\"Die drei reinen Typen der legitimen Herrschaft\") is an essay written by Max Weber, a German economist and sociologist, explaining his tripartite classification of authority. Originally published in the journal \"Preussische Jahrbücher\" 187, 1-2, 1922, an English translation, translated by Hans Gerth, was published in the journal \"Berkeley Publications in Society and Institutions\" 4(1): 1-11, 1958. Weber also refers to the three types of legitimate rule in his famous essay \"Politics as a Vocation.\"\n\nAccording to Weber, beliefs in the legitimacy of a political system go beyond philosophy and they directly contribute to the state system stability and authority. All rulers have an explanation for their superiority, an explanation that is commonly accepted but during a crisis can be questioned. Weber sees only three categories of legitimation strategies (which he calls \"pure types\") used to justify the right of rulers to rule:\n\nThe types of authority change over time, when the ruled are no longer satisfied with the system. For example, after the death of a charismatic leader his followers, if they lack the charisma of their predecessor, will try to institute a system based on tradition or law. On the other hand, these systems can be challenged by the appearance of a new charismatic leader, especially during economic or military crises.\n\nThese 'pure types' are almost always found in combination with other 'pure types' — for example, familial charisma (important in kingship and the Indian caste system) is a combination of charismatic and traditional elements, while institutional charisma (existing in all church organizations, but absent from a priesthood that fails to develop such an organization) is a mixture of charismatic and legal elements.\n"}
{"id": "44494171", "url": "https://en.wikipedia.org/wiki?curid=44494171", "title": "United States v. Ike Brown", "text": "United States v. Ike Brown\n\nIn 2006, the United States Department of Justice alleged in United States v. Ike Brown that the Noxubee County Democratic Executive Committee, in Noxubee County, Mississippi, headed by Ike Brown, had violated the Voting Rights Act of 1965 (VRA) with regards to the county's white minority. The suit alleged that Brown, the chairman of the Committee, had conspired to orchestrate \"relentless racial discrimination\" against white voters. This was the first time the VRA had been used to allege discrimination against whites.\n\nOn June 29, 2007, United States District Judge Tom S. Lee ruled that Brown (a twice-convicted felon), in conjunction with the Noxubee Democratic Executive Committee, had \"manipulated the political process in ways specifically intended and designed to impair and impede participation of white voters and to dilute their votes.\" The 104-page opinion held that the Voting Rights Act is a colorblind statute and protects all voters from racial discrimination, regardless of the race of the voter. The Court ruled that the Noxubee County Democratic Party had an illegal intent to discriminate against white voters in violation of Section 2 of the VRA. \n\nThe United States entered in a consent decree with the Noxubee County superintendent of general elections, administrator of absentee ballots, registrar, and the county government. The consent decree prohibited a wide range of discriminatory and illegal voting practices, and required these officials to report such incidents if they received information that they were continuing. This consent decree was approved by the district court and filed simultaneously with the filing of the complaint. Brown told \"The New York Times\" that he had not signed any such papers.\n"}
{"id": "2052891", "url": "https://en.wikipedia.org/wiki?curid=2052891", "title": "Verisimilitude", "text": "Verisimilitude\n\nVerisimilitude (or truthlikeness) is a philosophical concept that distinguishes between the relative and apparent (or seemingly so) truth and falsity of assertions and hypotheses. The problem of verisimilitude is the problem of articulating what it takes for one false theory to be closer to the truth than another false theory.\n\nThis problem was central to the philosophy of Karl Popper, largely because Popper was among the first to affirm that truth is the aim of scientific inquiry while acknowledging that most of the greatest scientific theories in the history of science are, strictly speaking, false. If this long string of purportedly false theories is to constitute progress with respect to the goal of truth, then it must be at least possible for one false theory to be closer to the truth than others.\n\nPopper assumed that scientists are interested in highly informative theories, in part for methodological reasons—the more informative a theory, the easier it is to test, and the greater its predictive power. But clearly informative power by itself is rather easy to come by, and we do not want to gain content by sacrificing truths. So Popper proposed that closeness to the truth is a function of two factors—truth and content. The more truths that a theory entails (other things being equal) the closer it is to the truth. \n\nIntuitively at least, it seems that Newton's theory of motion entails a good many more truths than does, say, Aristotle's theory—despite the fact that both are known to have flaws. Even two true theories can have differing degrees of verisimilitude, depending on how much true information they deliver. For example, the claim \"it will be raining on Thursday next week,\" if true, is closer to the truth than the true yet logically weaker claim \"it will either be raining next Thursday or it will be sunny\".\n\nPopper's formal definition of verisimilitude was challenged since 1974 by Pavel Tichý, John Henry Harris, and David Miller, who argued that Popper's definition has an unintended consequence: that no false theory can be closer to the truth than another. Popper himself stated: \"I accepted the criticism of my definition within minutes of its presentation, wondering why I had not seen the mistake before.\" This result gave rise to a search for an account of verisimilitude that did not deem progress towards the truth an impossibility.\n\nSome of the new theories (e.g. those proposed by David Miller and by Theo Kuipers) build on Popper's approach, guided by the notion that truthlikeness is a function of a truth factor and a content factor. Others (e.g. those advanced by in collaboration with , by Mortensen, and by Ken Gemes) are also inspired by Popper's approach but locate what they believe to be the error of Popper's proposal in his overly generous notion of content, or consequence, proposing instead that the consequences that contribute to closeness to truth must be, in a technical sense, \"relevant\". A different approach (already proposed by Tichý and and developed especially by Ilkka Niiniluoto and Graham Oddie) takes the \"likeness\" in truthlikeness literally, holding that a proposition's likeness to the truth is a function of the overall likeness to the actual world of the possible worlds in which the proposition would be true. An attempt to use the notion of point-free metric space is proposed by Giangiacomo Gerla. There is currently a debate about whether or to what extent these different approaches to the concept are compatible.\n\nAnother problem in Popper's theory of verisimilitude is the connection between truthlikeness as the goal of scientific progress, on the one hand, and methodology, on the other hand, as the ways in which we can to some extent ensure that scientific research actually approaches this goal. Popper conceived of his definition as a justification of his own preferred methodology: falsificationism, in the following sense: suppose theory A is closer to the truth than theory B according to Popper's qualitative definition of verisimilitude; in this case, we will (or should, if that definition had been logically sound) have that all true consequences of B (ie: all predicted consequences of theory B's mathematical and physical predictions subject to a particular set of initial conditions) are consequences of [theory] A (['s] similarly predicted consequences – i.e., informally, B ≤ A), and that all false consequences of A are consequences of B (in that those set of events deemed impossible by theory A are a subset of those events deemed impossible B, subject to the same initial data conditions for both – i.e., informally, ¬B ≥ ¬A, so that ¬A ≤ ¬B); this means that, if A and B are so related, then it should be the case that all \"known\" false empirical consequences of A also follow from B, and all \"known\" true empirical consequences of B do follow from A. So, if A were closer to the truth than B, then A should be better corroborated than B by any possible amount of empirical evidence. Lastly, this easy theorem allows interpretation of the fact that A is actually better corroborated than B as a \"corroboration of the hypothesis\" (or 'meta-hypothesis') that A is more verisimilar than B.\n\n"}
{"id": "34948291", "url": "https://en.wikipedia.org/wiki?curid=34948291", "title": "Yan Xing (artist)", "text": "Yan Xing (artist)\n\nYan Xing (Chinese characters: 鄢醒, born 1986) is an artist known for performance, installation, video and photography. He grew up in Chongqing and currently lives and works in Beijing and Los Angeles.\n\nYan Xing was born in Chongqing in 1986. He studied at the Oil Painting Department of Sichuan Fine Arts Institute from 2005 to 2009. After receiving his B.A., he moved to Beijing.\n\nYan Xing is known for his interdisciplinary projects which have built a complex, compelling body of work that reflects critically on how history is manufactured today. He interrogates literature, history, and history of art. His work explores themes of negativity, resistance and order and the complexity of their connectivity. Yan Xing’s works involve an extremely broad range of media, including performance, video, photography, installation, and painting, among others.\n\nYan Xing’s career in art began with \"Daddy Project\" (2011), it was an hour-long performance he gave an account of his absent “father”. The work was first performed in a group exhibition curated by Carol Yinghua Lu, art critic Holland Cotter wrote in the New York Times: “First-person public exposure of a personal life, particularly related to family, is relatively rare in China, and Mr. Yan has become a controversial star.” The first institutional solo exhibition of his works was held on 2012 at Chinese Arts Centre, Manchester, UK; On 2016, his first solo exhibition in America opened at Eli and Edythe Broad Art Museum at Michigan State University in East Lansing, Michigan. From June 2 through August 27, 2017, the Kunsthalle Basel presented \"Yan Xing: Dangerous Afternoon\", curated by Elena Filipovic, this was the artist's first institutional solo exhibition in Switzerland.\n\nYan Xing has exhibited and performed extensively, at institutions such as the Stedelijk Museum Amsterdam; Contemporary Arts Museum Houston; Ullens Center for Contemporary Art (UCCA), Beijing; OCT Contemporary Art Terminal (OCAT), Shenzhen and the Power Station of Art, Shanghai. He has also been featured at 7th Shenzhen Sculpture Biennale (2012); 3rd Moscow International Biennale for Young Art (2012) and 3rd Ural Industrial Biennial of Contemporary Art (2015).\n\nYan Xing is both the initiator and a participant of artists’ collective COMPANY. His works have been public collections include: Rubell Family Collection, Miami; M+ Museum for Visual Culture, Hong Kong; Kadist Art Foundation, Paris; He Xiangning Art Museum, Shenzhen. Yan Xing has also curated exhibitions such as: \"Dream Plant\", Sichuan Fine Arts Institute, Chongqing; \"Mummery\", Art Channel, Beijing; and the \"Fact Study Institute\", Yangtze River Space, Wuhan.\n\n\n\nAs a young artist, Yan Xing made an impressive result during recent years. He has received several notable awards, in 2012 he won the Best Young Artist Award by Chinese Contemporary Art Award (CCAA). The same year, he was a finalist in the Future Generation Art Prize by Victor Pinchuk Foundation and Focus on Talents Project from Today Art Museum.\n\n"}
