{"id": "50226436", "url": "https://en.wikipedia.org/wiki?curid=50226436", "title": "2001 Sex Strike For Running Water", "text": "2001 Sex Strike For Running Water\n\nThe 2001 sex strike for running water in Turkey began when women in a Turkish village initiated a nonviolent direct action to persuade the men of the village to fix a broken water system. \nIn 2001, a southern Turkish village near Siirt suffered from a broken down water supply system. This was not the first time the system had stopped, leaving the 600-person village without running water for months at a time. When the water system breaks down, the women must walk several miles to a small public fountain in order to have water for drinking, cooking and bathing.\n\nIn mid-July 2001, women in the village began a sex strike to encourage their husbands to demand that the government repair the water system. The idea of the sex strike came from a Turkish movie from 1983 in which women held a sex strike for better equality between the genders.\n\nThe lack of running water prevented women from bathing after sex, which is traditionally required of Muslim women. This ritual is known as ritual purification, which has to do with preparation for prayer that occurs five times a day.\n\nWithin one month of the sex strike, the men began asking the Siirt government for assistance to fix the water system. On August 15, the government supplied the village with enough resources to fix the water system themselves.\n\nA film, \"The Source\", was created in 2011 capturing a similar story in a North Africa. The women of the village take part in a sex strike against having to walk to a water source to collect water.\n"}
{"id": "13001588", "url": "https://en.wikipedia.org/wiki?curid=13001588", "title": "Animal consciousness", "text": "Animal consciousness\n\nAnimal consciousness, or animal awareness, is the quality or state of self-awareness within an animal, or of being aware of an external object or something within itself. In humans, consciousness has been defined as: sentience, awareness, subjectivity, qualia, the ability to experience or to feel, wakefulness, having a sense of selfhood, and the executive control system of the mind. Despite the difficulty in definition, many philosophers believe there is a broadly shared underlying intuition about what consciousness is.\n\nThe topic of animal consciousness is beset with a number of difficulties. It poses the problem of other minds in an especially severe form because animals, lacking the ability to use human language, cannot tell us about their experiences. Also, it is difficult to reason objectively about the question, because a denial that an animal is conscious is often taken to imply that it does not feel, its life has no value, and that harming it is not morally wrong. The 17th-century French philosopher René Descartes, for example, has sometimes been blamed for mistreatment of animals because he argued that only humans are conscious.\n\nPhilosophers who consider subjective experience the essence of consciousness also generally believe, as a correlate, that the existence and nature of animal consciousness can never rigorously be known. The American philosopher Thomas Nagel spelled out this point of view in an influential essay titled \"What Is it Like to Be a Bat?\". He said that an organism is conscious \"if and only if there is something that it is like to be that organism—something it is like \"for\" the organism\"; and he argued that no matter how much we know about an animal's brain and behavior, we can never really put ourselves into the mind of the animal and experience its world in the way it does itself. Other thinkers, such as the cognitive scientist Douglas Hofstadter, dismiss this argument as incoherent. Several psychologists and ethologists have argued for the existence of animal consciousness by describing a range of behaviors that appear to show animals holding beliefs about things they cannot directly perceive—Donald Griffin's 2001 book \"Animal Minds\" reviews a substantial portion of the evidence.\n\nAnimal consciousness has been actively researched for over one hundred years. In 1927 the American functional psychologist Harvey Carr argued that any valid measure or understanding of awareness in animals depends on \"an accurate and complete knowledge of its essential conditions in man\". A more recent review concluded in 1985 that \"the best approach is to use experiment (especially psychophysics) and observation to trace the dawning and ontogeny of self-consciousness, perception, communication, intention, beliefs, and reflection in normal human fetuses, infants, and children\". In 2012, a group of neuroscientists signed the Cambridge Declaration on Consciousness, which \"unequivocally\" asserted that \"humans are not unique in possessing the neurological substrates that generate consciousness. Non-human animals, including all mammals and birds, and many other creatures, including octopuses, also possess these neural substrates.\"\n\nThe mind–body problem in philosophy examines the relationship between mind and matter, and in particular the relationship between consciousness and the brain. A variety of approaches have been proposed. Most are either dualist or monist. Dualism maintains a rigid distinction between the realms of mind and matter. Monism maintains that there is only one kind of stuff, and that mind and matter are both aspects of it. The problem was addressed by pre-Aristotelian philosophers, and was famously addressed by René Descartes in the 17th century, resulting in Cartesian dualism. Descartes believed that humans only, and not other animals have this non-physical mind.\n\nThe rejection of the mind–body dichotomy is found in French Structuralism, and is a position that generally characterized post-war French philosophy.\nThe absence of an empirically identifiable meeting point between the non-physical mind and its physical extension has proven problematic to dualism and many modern philosophers of mind maintain that the mind is not something separate from the body. These approaches have been particularly influential in the sciences, particularly in the fields of sociobiology, computer science, evolutionary psychology, and the neurosciences.\n\nEpiphenomenalism is the theory in philosophy of mind that mental phenomena are caused by physical processes in the brain or that both are effects of a common cause, as opposed to mental phenomena driving the physical mechanics of the brain. The impression that thoughts, feelings, or sensations cause physical effects, is therefore to be understood as illusory to some extent. For example, it is not the feeling of fear that produces an increase in heart beat, both are symptomatic of a common physiological origin, possibly in response to a legitimate external threat.\n\nThe history of epiphenomenalism goes back to the post-Cartesian attempt to solve the riddle of Cartesian dualism, i.e., of how mind and body could interact. La Mettrie, Leibniz and Spinoza all in their own way began this way of thinking. The idea that even if the animal were conscious nothing would be added to the production of behavior, even in animals of the human type, was first voiced by La Mettrie (1745), and then by Cabanis (1802), and was further explicated by Hodgson (1870) and Huxley (1874). Huxley (1874) likened mental phenomena to the whistle on a steam locomotive. However, epiphenomenalism flourished primarily as it found a niche among methodological or scientific behaviorism. In the early 1900s scientific behaviorists such as Ivan Pavlov, John B. Watson, and B. F. Skinner began the attempt to uncover laws describing the relationship between stimuli and responses, without reference to inner mental phenomena. Instead of adopting a form of eliminativism or mental fictionalism, positions that deny that inner mental phenomena exist, a behaviorist was able to adopt epiphenomenalism in order to allow for the existence of mind. However, by the 1960s, scientific behaviourism met substantial difficulties and eventually gave way to the cognitive revolution. Participants in that revolution, such as Jerry Fodor, reject epiphenomenalism and insist upon the efficacy of the mind. Fodor even speaks of \"epiphobia\"—fear that one is becoming an epiphenomenalist.\n\nThomas Henry Huxley defends in an essay titled \"On the Hypothesis that Animals are Automata, and its History\" an epiphenomenalist theory of consciousness according to which consciousness is a causally inert effect of neural activity—\"as the steam-whistle which accompanies the work of a locomotive engine is without influence upon its machinery\". To this William James objects in his essay \"Are We Automata?\" by stating an evolutionary argument for mind-brain interaction implying that if the preservation and development of consciousness in the biological evolution is a result of natural selection, it is plausible that consciousness has not only been influenced by neural processes, but has had a survival value itself; and it could only have had this if it had been efficacious. Karl Popper develops in the book \"The Self and Its Brain\" a similar evolutionary argument.\n\nBernard Rollin of Colorado State University, the principal author of two U.S. federal laws regulating pain relief for animals, writes that researchers remained unsure into the 1980s as to whether animals experience pain, and veterinarians trained in the U.S. before 1989 were simply taught to ignore animal pain. In his interactions with scientists and other veterinarians, Rollin was regularly asked to prove animals are conscious and provide scientifically acceptable grounds for claiming they feel pain. Academic reviews of the topic are equivocal, noting that the argument that animals have at least simple conscious thoughts and feelings has strong support, but some critics continue to question how reliably animal mental states can be determined. A refereed journal \"Animal Sentience\" launched in 2015 by the Institute of Science and Policy of The Humane Society of the United States is devoted to research on this and related topics.\n\nConsciousness is an elusive concept that presents many difficulties when attempts are made to define it. Its study has progressively become an interdisciplinary challenge for numerous researchers, including ethologists, neurologists, cognitive neuroscientists, philosophers, psychologists and psychiatrists.\n\nIn 1976 Richard Dawkins wrote, \"The evolution of the capacity to simulate seems to have culminated in subjective consciousness. Why this should have happened is, to me, the most profound mystery facing modern biology\". In 2004, eight neuroscientists felt it was still too soon for a definition. They wrote an apology in \"Human Brain Function\":\n\nConsciousness is sometimes defined as the quality or state of being aware of an external object or something within oneself. It has been defined somewhat vaguely as: subjectivity, awareness, sentience, the ability to experience or to feel, wakefulness, having a sense of selfhood, and the executive control system of the mind. Despite the difficulty in definition, many philosophers believe that there is a broadly shared underlying intuition about what consciousness is. Max Velmans and Susan Schneider wrote in \"The Blackwell Companion to Consciousness\": \"Anything that we are aware of at a given moment forms part of our consciousness, making conscious experience at once the most familiar and most mysterious aspect of our lives.\"\n\nRelated terms, also often used in vague or ambiguous ways, are:\n\nSentience (the ability to feel, perceive, or to experience subjectivity) is not the same as self-awareness (being aware of oneself as an individual). The mirror test is sometimes considered to be an operational test for self-awareness, and the handful of animals that have passed it are often considered to be self-aware. It remains debatable whether recognition of one's mirror image can be properly construed to imply full self-awareness, particularly given that robots are being constructed which appear to pass the test.\n\nMuch has been learned in neuroscience about correlations between brain activity and subjective, conscious experiences, and many suggest that neuroscience will ultimately explain consciousness; \"...consciousness is a biological process that will eventually be explained in terms of molecular signaling pathways used by interacting populations of nerve cells...\". However, this view has been criticized because consciousness has yet to be shown to be a process, and the so-called \"hard problem\" of relating consciousness directly to brain activity remains elusive.\n\nSince Descartes's proposal of dualism, it became a general consensus that the mind had become a matter of philosophy and that science was not able to penetrate the issue of consciousness - that consciousness was outside of space and time. However, over the last 20 years, many scholars have begun to move toward a science of consciousness. Antonio Damasio and Gerald Edelman are two neuroscientists who have led the move to neural correlates of the self and of consciousness. Damasio has demonstrated that emotions and their biological foundation play a critical role in high level cognition, and Edelman has created a framework for analyzing consciousness through a scientific outlook. The current problem consciousness researchers face involves explaining how and why consciousness arises from neural computation. In his research on this problem, Edelman has developed a theory of consciousness, in which he has coined the terms primary consciousness and secondary consciousness.\n\nEugene Linden, author of \"The Parrot's Lament\" suggests there are many examples of animal behavior and intelligence that surpass what people would suppose to be the boundary of animal consciousness. Linden contends that in many of these documented examples, a variety of animal species exhibit behavior that can only be attributed to emotion, and to a level of consciousness that we would normally ascribe only to our own species.\n\nPhilosopher Daniel Dennett counters that:\nConsciousness in mammals (including humans) is an aspect of the mind generally thought to comprise qualities such as subjectivity, sentience, and the ability to perceive the relationship between oneself and one's environment. It is a subject of much research in philosophy of mind, psychology, neuroscience, and cognitive science. Some philosophers divide consciousness into phenomenal consciousness, which is subjective experience itself, and access consciousness, which refers to the global availability of information to processing systems in the brain. Phenomenal consciousness has many different experienced qualities, often referred to as qualia. Phenomenal consciousness is usually consciousness \"of\" something or \"about\" something, a property known as intentionality in philosophy of mind.\n\nIn humans, there are three common methods of studying consciousness, i.e. verbal report, behavioural demonstrations, and neural correlation with conscious activity. Unfortunately these can only be generalized to non-human taxa with varying degrees of difficulty.\n\nThe sense in which animals (or human infants) can be said to have consciousness or a self-concept has been hotly debated; it is often referred to as the debate over animal minds. The best known research technique in this area is the mirror test devised by Gordon G. Gallup, in which the skin of an animal (or human infant) is marked, while it is asleep or sedated, with a mark that cannot be seen directly but is visible in a mirror. The animal is then allowed to see its reflection in a mirror; if the animal spontaneously directs grooming behaviour towards the mark, that is taken as an indication that it is aware of itself. Over the past 30 years, many studies have found evidence that animals recognise themselves in mirrors. Self-awareness by this criterion has been reported for:\n\nApes\n\nOther land mammals\n\nCetaceans\n\nBirds\n\nUntil recently it was thought that self-recognition was absent from animals without a neocortex, and was restricted to mammals with large brains and well developed social cognition. However, in 2008 a study of self-recognition in corvids reported significant results for magpies. Mammals and birds inherited the same brain components from their last common ancestor nearly 300 million years ago, and have since independently evolved and formed significantly different brain types. The results of the mirror and mark tests showed that neocortex-less magpies are capable of understanding that a mirror image belongs to their own body. The findings show that magpies respond in the mirror and mark test in a manner similar to apes, dolphins and elephants. This is a remarkable capability that, although not fully concrete in its determination of self-recognition, is at least a prerequisite of self-recognition. This is not only of interest regarding the convergent evolution of social intelligence; it is also valuable for an understanding of the general principles that govern cognitive evolution and their underlying neural mechanisms. The magpies were chosen to study based on their empathy/lifestyle, a possible precursor for their ability of self-awareness. However even in the chimpanzee, the species most studied and with the most convincing findings, clear-cut evidence of self-recognition is not obtained in all individuals tested. Occurrence is about 75% in young adults and considerably less in young and old individuals. For monkeys, non-primate mammals, and in a number of bird species, exploration of the mirror and social displays were observed. However, hints at mirror-induced self-directed behavior have been obtained.\n\nThe mirror test has attracted controversy among some researchers because it is entirely focused on vision, the primary sense in humans, while other species rely more heavily on other senses such as the olfactory sense in dogs. A study in 2015 showed that the “sniff test of self-recognition (STSR)” provides evidence of self-awareness in dogs.\n\nAnother approach to determine whether a non-human animal is conscious derives from passive speech research with a macaw (see Arielle). Some researchers propose that by passively listening to an animal's voluntary speech, it is possible to learn about the thoughts of another creature and to determine that the speaker is conscious. This type of research was originally used to investigate a child's crib speech by Weir (1962) and in investigations of early speech in children by Greenfield and others (1976).\n\nZipf's law might be able to be used to indicate if a given dataset of animal communication indicate an intelligent natural language. Some researchers have used this algorithm to study bottlenose dolphin language.\n\nFurther arguments revolve around the ability of animals to feel pain or suffering. Suffering implies consciousness. If animals can be shown to suffer in a way similar or identical to humans, many of the arguments against human suffering could then, presumably, be extended to animals. Others have argued that pain can be demonstrated by adverse reactions to negative stimuli that are non-purposeful or even maladaptive. One such reaction is transmarginal inhibition, a phenomenon observed in humans and some animals akin to mental breakdown.\n\nCarl Sagan, the American cosmologist, points to reasons why humans have had a tendency to deny animals can suffer:\nJohn Webster, a professor of animal husbandry at Bristol, argues:\nHowever, there is no agreement where the line between organisms that can feel pain and those that cannot should be drawn. Justin Leiber, a philosophy professor at Oxford University writes that:\nThere are also some who reject the argument entirely, arguing that although suffering animals feel anguish, a suffering plant also struggles to stay alive (albeit in a less visible way). In fact, no living organism 'wants' to die for another organism's sustenance. In an article written for \"The New York Times\", Carol Kaesuk Yoon argues that:\n\nCognitive bias in animals is a pattern of deviation in judgment, whereby inferences about other animals and situations may be drawn in an illogical fashion. Individuals create their own \"subjective social reality\" from their perception of the input. It refers to the question \"Is the glass half empty or half full?\", used as an indicator of optimism or pessimism. Cognitive biases have been shown in a wide range of species including rats, dogs, rhesus macaques, sheep, chicks, starlings and honeybees.\n\nThe neuroscientist Joseph LeDoux advocates avoiding terms derived from human subjective experience when discussing brain functions in animals. For example, the common practice of calling brain circuits that detect and respond to threats \"fear circuits\" implies that these circuits are responsible for feelings of fear. LeDoux argues that Pavlovian fear conditioning should be renamed Pavlovian threat conditioning to avoid the implication that \"fear\" is being acquired in rats or humans. Key to his theoretical change is the notion of survival functions mediated by survival circuits, the purpose of which is to keep organisms alive rather than to make emotions. For example, defensive survival circuits exist to detect and respond to threats. While all organisms can do this, only organisms that can be conscious of their own brain’s activities can feel fear. Fear is a conscious experience and occurs the same way as any other kind of conscious experience: via cortical circuits that allow attention to certain forms of brain activity. LeDoux argues the only differences between an emotional and non-emotion state of consciousness are the underlying neural ingredients that contribute to the state.\n\nNeuroscience is the scientific study of the nervous system. It is a highly active interdisciplinary science that collaborates with many other fields. The scope of neuroscience has broadened recently to include molecular, cellular, developmental, structural, functional, evolutionary, computational, and medical aspects of the nervous system. Theoretical studies of neural networks are being complemented with techniques for imaging sensory and motor tasks in the brain.\nAccording to a 2008 paper, neuroscience explanations of psychological phenomena currently have a \"seductive allure\", and \"seem to generate more public interest\" than explanations which do not contain neuroscientific information. They found that subjects who were not neuroscience experts \"judged that explanations with logically irrelevant neuroscience information were more satisfying than explanations without.\n\nThe neural correlates of consciousness constitute the minimal set of neuronal events and mechanisms sufficient for a specific conscious percept. Neuroscientists use empirical approaches to discover neural correlates of subjective phenomena. The set should be \"minimal\" because, if the brain is sufficient to give rise to any given conscious experience, the question is which of its components is necessary to produce it.\n\nVisual sense and representation was reviewed in 1998 by Francis Crick and Christof Koch. They concluded sensory neuroscience can be used as a bottom-up approach to studying consciousness, and suggested experiments to test various hypotheses in this research stream.\n\nA feature that distinguishes humans from most animals is that we are not born with an extensive repertoire of behavioral programs that would enable us to survive on our own (\"physiological prematurity\"). To compensate for this, we have an unmatched ability to learn, i.e., to consciously acquire such programs by imitation or exploration. Once consciously acquired and sufficiently exercised, these programs can become automated to the extent that their execution happens beyond the realms of our awareness. Take, as an example, the incredible fine motor skills exerted in playing a Beethoven piano sonata or the sensorimotor coordination required to ride a motorcycle along a curvy mountain road. Such complex behaviors are possible only because a sufficient number of the subprograms involved can be executed with minimal or even suspended conscious control. In fact, the conscious system may actually interfere somewhat with these automated programs.\n\nThe growing ability of neuroscientists to manipulate neurons using methods from molecular biology in combination with optical tools depends on the simultaneous development of appropriate behavioural assays and model organisms amenable to large-scale genomic analysis and manipulation. A combination of such fine-grained neuronal analysis in animals with ever more sensitive psychophysical and brain imaging techniques in humans, complemented by the development of a robust theoretical predictive framework, will hopefully lead to a rational understanding of consciousness.\n\nThe neocortex is a part of the brain of mammals. It consists of the grey matter, or neuronal cell bodies and unmyelinated fibers, surrounding the deeper white matter (myelinated axons) in the cerebrum. The neocortex is smooth in rodents and other small mammals, whereas in primates and other larger mammals it has deep grooves and wrinkles. These folds increase the surface area of the neocortex considerably without taking up too much more volume. Also, neurons within the same wrinkle have more opportunity for connectivity, while neurons in different wrinkles have less opportunity for connectivity, leading to compartmentalization of the cortex. The neocortex is divided into frontal, parietal, occipital, and temporal lobes, which perform different functions. For example, the occipital lobe contains the primary visual cortex, and the temporal lobe contains the primary auditory cortex. Further subdivisions or areas of neocortex are responsible for more specific cognitive processes. The neocortex is the newest part of the cerebral cortex to evolve (hence the prefix \"neo\"); the other parts of the cerebral cortex are the paleocortex and archicortex, collectively known as the allocortex. In humans, 90% of the cerebral cortex is neocortex.\n\nResearchers have argued that consciousness arises in the neocortex, and therefore cannot arise in animals which lack a neocortex. For example, Rose argued in 2002 that the \"fishes have nervous systems that mediate effective escape and avoidance responses to noxious stimuli, but, these responses must occur without a concurrent, human-like awareness of pain, suffering or distress, which depend on separately evolved neocortex.\" Recently that view has been challenged, and many researchers now believe that animal consciousness can arise from homologous subcortical brain networks.\n\nAttention is the cognitive process of selectively concentrating on one aspect of the environment while ignoring other things. Attention has also been referred to as the allocation of processing resources. Attention also has variations amongst cultures. Voluntary attention develops in specific cultural and institutional contexts through engagement in cultural activities with more competent community members.\n\nMost experiments show that one neural correlate of attention is enhanced firing. If a neuron has a certain response to a stimulus when the animal is not attending to the stimulus, then when the animal does attend to the stimulus, the neuron's response will be enhanced even if the physical characteristics of the stimulus remain the same. In many cases attention produces changes in the EEG. Many animals, including humans, produce gamma waves (40–60 Hz) when focusing attention on a particular object or activity.\n\nExtended consciousness is an animal's autobiographical self-perception. It is thought to arise in the brains of animals which have a substantial capacity for memory and reason. It does not necessarily require language. The perception of a historic and future self arises from a stream of information from the immediate environment and from neural structures related to memory. The concept was popularised by Antonio Damasio and is used in biological psychology. Extended consciousness is said to arise in structures in the human brain described as \"image spaces\" and \"dispositional spaces\". Image spaces imply areas where sensory impressions of all types are processed, including the focused awareness of the core consciousness. Dispositional spaces include convergence zones, which are networks in the brain where memories are processed and recalled, and where knowledge is merged with immediate experience.\n\nMetacognition is defined as \"cognition about cognition\", or \"knowing about knowing.\" It can take many forms; it includes knowledge about when and how to use particular strategies for learning or for problem solving. It has been suggested that metacognition in some animals provides evidence for cognitive self-awareness.\nThere are generally two components of metacognition: knowledge about cognition, and regulation of cognition. Writings on metacognition can be traced back at least as far as \"De Anima\" and the \"Parva Naturalia\" of the Greek philosopher Aristotle. Metacognologists believe that the ability to consciously think about thinking is unique to sapient species and indeed is one of the definitions of sapience. There is evidence that rhesus monkeys and apes can make accurate judgments about the strengths of their memories of fact and monitor their own uncertainty, while attempts to demonstrate metacognition in birds have been inconclusive. A 2007 study provided some evidence for metacognition in rats, but further analysis suggested that they may have been following simple operant conditioning principles, or a behavioral economic model.\n\nMirror neurons are neurons that fire both when an animal acts and when the animal observes the same action performed by another. Thus, the neuron \"mirrors\" the behavior of the other, as though the observer were itself acting. Such neurons have been directly observed in primate and other species including birds. The function of the mirror system is a subject of much speculation. Many researchers in cognitive neuroscience and cognitive psychology consider that this system provides the physiological mechanism for the perception action coupling (see the common coding theory). They argue that mirror neurons may be important for understanding the actions of other people, and for learning new skills by imitation. Some researchers also speculate that mirror systems may simulate observed actions, and thus contribute to theory of mind skills, while others relate mirror neurons to language abilities. Neuroscientists such as Marco Iacoboni (UCLA) have argued that mirror neuron systems in the human brain help us understand the actions and intentions of other people. In a study published in March 2005 Iacoboni and his colleagues reported that mirror neurons could discern if another person who was picking up a cup of tea planned to drink from it or clear it from the table. In addition, Iacoboni and a number of other researchers have argued that mirror neurons are the neural basis of the human capacity for emotions such as empathy. Vilayanur S. Ramachandran has speculated that mirror neurons may provide the neurological basis of self-awareness.\n\nConsciousness is likely an evolved adaptation since it meets George Williams' criteria of species universality, complexity, and functionality, and it is a trait that apparently increases fitness. Opinions are divided as to where in biological evolution consciousness emerged and about whether or not consciousness has survival value. It has been argued that consciousness emerged (i) exclusively with the first humans, (ii) exclusively with the first mammals, (iii) independently in mammals and birds, or (iv) with the first reptiles. Donald Griffin suggests in his book \"Animal Minds\" a gradual evolution of consciousness. Each of these scenarios raises the question of the possible survival value of consciousness.\n\nIn his paper \"Evolution of consciousness,\" John Eccles argues that special anatomical and physical adaptations of the mammalian cerebral cortex gave rise to consciousness. In contrast, others have argued that the recursive circuitry underwriting consciousness is much more primitive, having evolved initially in pre-mammalian species because it improves the capacity for interaction with both social \"and\" natural environments by providing an energy-saving \"neutral\" gear in an otherwise energy-expensive motor output machine. Once in place, this recursive circuitry may well have provided a basis for the subsequent development of many of the functions that consciousness facilitates in higher organisms, as outlined by Bernard J. Baars. Richard Dawkins suggested that humans evolved consciousness in order to make themselves the subjects of thought. Daniel Povinelli suggests that large, tree-climbing apes evolved consciousness to take into account one's own mass when moving safely among tree branches. Consistent with this hypothesis, Gordon Gallup found that chimps and orangutans, but not little monkeys or terrestrial gorillas, demonstrated self-awareness in mirror tests.\n\nThe concept of consciousness can refer to voluntary action, awareness, or wakefulness. However, even voluntary behaviour involves unconscious mechanisms. Many cognitive processes take place in the cognitive unconscious, unavailable to conscious awareness. Some behaviours are conscious when learned but then become unconscious, seemingly automatic. Learning, especially implicitly learning a skill, can take place outside of consciousness. For example, plenty of people know how to turn right when they ride a bike, but very few can accurately explain how they actually do so.\n\nNeural Darwinism is a large scale theory of brain function initially proposed in 1978 by the American biologist Gerald Edelman. Edelman distinguishes between what he calls primary and secondary consciousness:\n\n\nPrimary consciousness can be defined as simple awareness that includes perception and emotion. As such, it is ascribed to most animals. By contrast, secondary consciousness depends on and includes such features as self-reflective awareness, abstract thinking, volition and metacognition.\n\nEdelman's theory focuses on two nervous system organizations: the brainstem and limbic systems on one side and the thalamus and cerebral cortex on the other side. The brain stem and limbic system take care of essential body functioning and survival, while the thalamocortical system receives signals from sensory receptors and sends out signals to voluntary muscles such as those of the arms and legs. The theory asserts that the connection of these two systems during evolution helped animals learn adaptive behaviors.\n\nOther scientists have argued against Edelman's theory, instead suggesting that primary consciousness might have emerged with the basic vegetative systems of the brain. That is, the evolutionary origin might have come from sensations and primal emotions arising from sensors and receptors, both internal and surface, signaling that the well-being of the creature was immediately threatened—for example, hunger for air, thirst, hunger, pain, and extreme temperature change. This is based on neurological data showing the thalamic, hippocampal, orbitofrontal, insula, and midbrain sites are the key to consciousness of thirst. These scientists also point out that the cortex might not be as important to primary consciousness as some neuroscientists have believed. Evidence of this lies in the fact that studies show that systematically disabling parts of the cortex in animals does not remove consciousness. Another study found that children born without a cortex are conscious. Instead of cortical mechanisms, these scientists emphasize brainstem mechanisms as essential to consciousness. Still, these scientists concede that higher order consciousness does involve the cortex and complex communication between different areas of the brain.\n\nWhile animals with primary consciousness have long-term memory, they lack explicit narrative, and, at best, can only deal with the immediate scene in the remembered present. While they still have an advantage over animals lacking such ability, evolution has brought forth a growing complexity in consciousness, particularly in mammals. Animals with this complexity are said to have secondary consciousness.\nSecondary consciousness is seen in animals with semantic capabilities, such as the four great apes. It is present in its richest form in the human species, which is unique in possessing complex language made up of syntax and semantics. In considering how the neural mechanisms underlying primary consciousness arose and were maintained during evolution, it is proposed that at some time around the divergence of reptiles into mammals and then into birds, the embryological development of large numbers of new reciprocal connections allowed rich re-entrant activity to take place between the more posterior brain systems carrying out perceptual categorization and the more frontally located systems responsible for value-category memory. The ability of an animal to relate a present complex scene to its own previous history of learning conferred an adaptive evolutionary advantage. At much later evolutionary epochs, further re-entrant circuits appeared that linked semantic and linguistic performance to categorical and conceptual memory systems. This development enabled the emergence of secondary consciousness.\n\nUrsula Voss of the Universität Bonn believes that the theory of protoconsciousness may serve as adequate explanation for self-recognition found in birds, as they would develop secondary consciousness during REM sleep. She added that many types of birds have very sophisticated language systems. Don Kuiken of the University of Alberta finds such research interesting as well as if we continue to study consciousness with animal models (with differing types of consciousness), we would be able to separate the different forms of reflectiveness found in today's world.\n\nFor the advocates of the idea of a secondary consciousness, self-recognition serves as a critical component and a key defining measure. What is most interesting then, is the evolutionary appeal that arises with the concept of self-recognition. In non-human species and in children, the mirror test (see above) has been used as an indicator of self-awareness.\n\nIn 2012, a group of neuroscientists attending a conference on \"Consciousness in Human and non-Human Animals\" at the University of Cambridge in the UK, signed \"The Cambridge Declaration on Consciousness\" (see box on the right).\n\nIn the accompanying text they \"unequivocally\" asserted:\n\nA common image is the \"scala naturae\", the ladder of nature on which animals of different species occupy successively higher rungs, with humans typically at the top. A more useful approach has been to recognize that different animals may have different kinds of cognitive processes, which are better understood in terms of the ways in which they are cognitively adapted to their different ecological niches, than by positing any kind of hierarchy.\n\nDogs were previously listed as non-self-aware animals. Traditionally, self-consciousness was evaluated via the mirror test. But dogs and many other animals, are not (as) visually oriented. A 2015 study claims that the “sniff test of self-recognition” (STSR) provides significant evidence of self-awareness in dogs, and could play a crucial role in showing that this capacity is not a specific feature of only great apes, humans and a few other animals, but it depends on the way in which researchers try to verify it. According to the biologist Roberto Cazzolla Gatti (who published the study), \"the innovative approach to test the self-awareness with a smell test highlights the need to shift the paradigm of the anthropocentric idea of consciousness to a species-specific perspective\". This study has been confirmed by another study.\nResearch with captive grey parrots, especially Irene Pepperberg's work with an individual named Alex, has demonstrated they possess the ability to associate simple human words with meanings, and to intelligently apply the abstract concepts of shape, colour, number, zero-sense, etc. According to Pepperberg and other scientists, they perform many cognitive tasks at the level of dolphins, chimpanzees, and even human toddlers. Another notable African grey is N'kisi, which in 2004 was said to have a vocabulary of over 950 words which she used in creative ways. For example, when Jane Goodall visited N'kisi in his New York home, he greeted her with \"Got a chimp?\" because he had seen pictures of her with chimpanzees in Africa.\n\nIn 2011, research led by Dalila Bovet of Paris West University Nanterre La Défense, demonstrated grey parrots were able to coordinate and collaborate with each other to an extent. They were able to solve problems such as two birds having to pull strings at the same time to obtain food. In another example, one bird stood on a perch to release a food-laden tray, while the other pulled the tray out from the test apparatus. Both would then feed. The birds were observed waiting for their partners to perform the necessary actions so their behaviour could be synchronized. The parrots appeared to express individual preferences as to which of the other test birds they would work with.\n\nIt was recently thought that self-recognition was restricted to mammals with large brains and highly evolved social cognition, but absent from animals without a neocortex. However, in 2008, an investigation of self-recognition in corvids was conducted revealing the ability of self-recognition in the magpie. Mammals and birds inherited the same brain components from their last common ancestor nearly 300 million years ago, and have since independently evolved and formed significantly different brain types. The results of the mirror test showed that although magpies do not have a neocortex, they are capable of understanding that a mirror image belongs to their own body. The findings show that magpies respond in the mirror test in a manner similar to apes, dolphins, killer whales, pigs and elephants. This is a remarkable capability that, although not fully concrete in its determination of self-recognition, is at least a prerequisite of self-recognition. This is not only of interest regarding the convergent evolution of social intelligence, it is also valuable for an understanding of the general principles that govern cognitive evolution and their underlying neural mechanisms. The magpies were chosen to study based on their empathy/lifestyle, a possible precursor for their ability of self-awareness.\n\nOctopuses are highly intelligent, possibly more so than any other order of invertebrates. The level of their intelligence and learning capability are debated, but maze and problem-solving studies show they have both short- and long-term memory. Octopus have a highly complex nervous system, only part of which is localized in their brain. Two-thirds of an octopus' neurons are found in the nerve cords of their arms. Octopus arms show a variety of complex reflex actions that persist even when they have no input from the brain. Unlike vertebrates, the complex motor skills of octopuses are not organized in their brain using an internal somatotopic map of their body, instead using a non-somatotopic system unique to large-brained invertebrates. Some octopuses, such as the mimic octopus, move their arms in ways that emulate the shape and movements of other sea creatures.\n\nIn laboratory studies, octopuses can easily be trained to distinguish between different shapes and patterns. They reportedly use observational learning, although the validity of these findings is contested. Octopuses have also been observed to play: repeatedly releasing bottles or toys into a circular current in their aquariums and then catching them. Octopuses often escape from their aquarium and sometimes enter others. They have boarded fishing boats and opened holds to eat crabs. At least four specimens of the veined octopus (\"Amphioctopus marginatus\") have been witnessed retrieving discarded coconut shells, manipulating them, and then reassembling them to use as shelter.\n\nSome contributors to relevant research on animal consciousness include:\n\n\n\n"}
{"id": "14046990", "url": "https://en.wikipedia.org/wiki?curid=14046990", "title": "Assembly rules", "text": "Assembly rules\n\nCommunity assembly rules are a set of controversial rules in ecology, first proposed by Jared Diamond.\n\nThe rules were developed after more than a decade of research into the avian assemblages on islands near New Guinea. The rules assert that competition is responsible for determining the patterns of assemblage composition. Diamond's paper sparked nearly two decades worth of controversy in the literature, from the late seventies through the late nineties and is considered a turning point in community ecology. The disagreement continues to this day.\n\nThe first rule is \"forbidden species combinations\". Diamond's hypothesis was that competition, not random immigration, was the main force structuring the species composition of islands.\n\nSo for example, the Bismarck black myzomela (\"Myzomela pammelaena\") excludes the black sunbird (\"Nectarinia sericea\"). The Bismarck black myzomela lives on 23 of the 41 surveyed islands in the Bismarck Archipelago, but not on any of the 14 islands inhabited by the black sunbird. The two birds are about the same size, and both use their curved bills to sip nectar; Diamond argued that competition affects their distribution.\n\nCase tested the assembly rule that species occurring together on islands should have less niche overlap than random assemblages because they have undergone specialization. His study measured niche overlap of lizards on 37 islands near Baja California and compared niche overlap to the median niche overlap of computer generated random species assemblages. Case found that 30 of the 37 islands had lower niche overlap than the random assemblages and that some of the competition is due to interspecific competition.\n\nTesting the assembly rules is a complex process that often uses computer simulations to compare experimental data with characteristics of random assemblages of species. The rules are generally regarded as hypotheses that need to be tested on an individual basis, not as accepted conclusions.\n\nAs a reaction to the assembly rules controversy, ecologist Stephen Hubbell proposed that the abundance and diversity of species in a community is determined mainly by random dispersal, speciation, and extinction. This came to be known as the unified neutral theory of biodiversity.\n\n"}
{"id": "6811468", "url": "https://en.wikipedia.org/wiki?curid=6811468", "title": "Asteya", "text": "Asteya\n\nAsteya is the Sanskrit term for \"non-stealing\". It is a virtue in Jainism . The practice of \"asteya\" demands that one must not steal, nor have the intent to steal another's property through action, speech and thoughts.\n\nAsteya is considered as one of five major vows of Jainism. It is also considered one of ten forms of temperance (virtuous self-restraint) in Indian philosophy.\n\nThe word \"asteya\" is a compound derived from Sanskrit language, where \"a\" refers to \"non-\" and \"steya\" refers to \"practice of stealing\" or \"something that can be stolen\". Thus, \"asteya\" means \"non-stealing\".\n\nIn Jainism, it is one of the five vows that all Śrāvakas and Śrāvikās (householders) as well as monastics must observe. The five transgressions of this vow as mentioned in the Jain text, \"Tattvārthsūtra\" are: \"Prompting another to steal, receiving stolen goods, underbuying in a disordered state, using false weights and measures, and deceiving others with artificial or imitation goods\".\nThis is explained in the Jain text, Sarvārthasiddhi as (translated by S.A. Jain): \n\nAsteya is defined in Hindu scripts as \"the abstinence, in one's deeds or words or thoughts, from unauthorized appropriation of things of value from another human being\". It is a widely discussed virtue in ethical theories of Hinduism. For example, in the Yoga Sūtras (II.30), \"Asteya\" (non-stealing) is listed as the third Yamas or virtue of self-restraint, along with Ahimsa (nonviolence), Satya (non-falsehoods, truthfulness), Brahmacharya (sexual chastity in one's feelings and actions) and Aparigraha (non-possessiveness, non-craving).\n\nAsteya is thus one of the five essential restraints (\"yamas\", \"the don'ts\") in Hinduism, that with five essential practices (\"niyamas\", \"the dos\") are suggested for right, virtuous, enlightened living.\n\nAsteya in practice, states Patricia Corner, implies to \"not steal\", \"not cheat\" nor unethically manipulate other's property or others for one's own gain. Asteya as virtue demands that not only one \"not steal\" through one's action, one shouldn't want to encourage cheating through speech or writing, or want to cheat even in one's thinking. Smith states that the virtue of \"asteya\" arises out of the understanding that all misappropriation is an expression of craving and a feeling of lack of compassion for other beings. To steal or want to steal expresses lack of faith in oneself, one's ability to learn and create property. To steal another's property is also stealing from one's own potential ability to develop. The Sutras reason that misappropriation, conspiring to misappropriate or wanting to misappropriate, at its root reflects the sin of \"lobha\" (bad greed), \"moha\" (material delusion) or \"krodha\" (bad anger).\n\nGandhi held \"ahimsa\" as essential to the human right to life and liberty without fear, \"asteya\" as human right to property without fear. Asteya follows from Ahimsa, in Gandhi's views, because stealing is a form of violence and injury to another person. Asteya is not merely \"theft by action\", but it includes \"theft by intent\" and \"theft by manipulation\". Persistent exploitation of the weak or poor is a form of \"asteya in one's thought\".\n\nDāna, that is charity to a deserving person without any expectation in return, is a recommended \"niyama\" in Hinduism. The motive behind Dāna is reverse to that of \"stealing from others\". Dāna is a complementary practice to the yamas (restraint) of \"asteya\".\n\nAsteya and Aparigraha are two of several important virtues in Hinduism and Jainism. They both involve interaction between a person and material world, either as property, fame or ideas; yet Asteya and Aparigraha are different concepts. Asteya is the virtue of non-stealing and not wanting to appropriate, or take by force or deceit or exploitation, by deeds or words or thoughts, what is owned by and belongs to someone else. Aparigraha, in contrast, is the virtue of non-possessiveness and non-clinging to one's own property, non-accepting any gifts or particularly improper gifts offered by others, and of non-avarice, non-craving in the motivation of one's deeds, words and thoughts.\n"}
{"id": "16639868", "url": "https://en.wikipedia.org/wiki?curid=16639868", "title": "Attractiveness", "text": "Attractiveness\n\nAttractiveness or attraction is a quality that causes an interest, desire in, or gravitation to something or someone.\n\"Attraction\" may also refer to the object of the attraction itself, as in \"tourist attraction\".\n\nVisual attractiveness or visual appeal is attraction produced primarily by visual stimuli.\n\nPhysical attractiveness is the perception of the physical traits of an individual human person as pleasing or beautiful. It can include various implications, such as sexual attractiveness, cuteness, similarity and physique.\n\nPhysical attractiveness can also include the attraction of smell. Pheromones secreted by animals and humans alike can attract others, and this is viewed as being attracted to smell. Human sex pheromones may play a role in human attraction, although it is unclear how well humans can actually pick up on the pheromones of another. \n\nJudgment of attractiveness of physical traits is partly universal to all human cultures, partly dependent on culture or society or time period, partly biological, and partly subjective and individual.\n\nAccording to a study determining the golden ratio for facial beauty, the most attractive face is one with average distances between facial features, and an average length and width of the face itself.. Facial attractiveness, or beauty, can also be determined by symmetry. If a face is asymmetrical, this can indicate unhealthy genetic information. Therefore, if a face is symmetrical (see Facial symmetry), healthy genetic information is implied. People will judge potential mates based off the physical expression of the genetic health, which is their apparent attractiveness. This supports the good genes theory, which indicates that attractiveness is seen as a way to ensure that offspring will have the healthiest genes and therefore the best chance of survival. Certain trains that indicate good genes (such as clear skin or facial symmetry) are seen as desirable when choosing a partner.\n\nAmong animals, females often are \"choosy\" about who they pick for a mate. They will look for characteristics in their potential mates that ensure their offspring will have healthy genetic material. Therefore, they will be attracted to certain traits or behaviors of their potential mates. Attraction among animals is based mostly on what will give the females the best chance of having the most and healthiest offspring that will likely survive to adulthood because of the traits they will inherit from the male. Females will look for signs of health in the males, such as large and fancy adornments, bright colors, or other features that the male will only have if he is in good health. .\n\nEye candy is a slang term for superficial attractiveness. In a 2017 \"Boston Globe\" article about the potential for cheerleading at the Olympics, eye candy was used to describe cheerleaders as \"entertainment popularized by professional sports in the United States.\" The term has also been used in professional sports in the United States referring to female athletes.\n\n"}
{"id": "171323", "url": "https://en.wikipedia.org/wiki?curid=171323", "title": "Bandwagon effect", "text": "Bandwagon effect\n\nThe bandwagon effect is a phenomenon whereby the rate of uptake of beliefs, ideas, fads and trends increases the more that they have already been adopted by others. In other words, the bandwagon effect is characterized by the probability of individual adoption increasing with respect to the proportion who have already done so. As more people come to believe in something, others also \"hop on the bandwagon\" regardless of the underlying evidence.\n\nThe tendency to follow the actions or beliefs of others can occur because individuals directly prefer to conform, or because individuals derive information from others. Both explanations have been used for evidence of conformity in psychological experiments. For example, social pressure has been used to explain Asch's conformity experiments, and information has been used to explain Sherif's autokinetic experiment.\n\nAccording to this concept, the increasing popularity of a product or phenomenon encourages more people to \"get on the bandwagon\", too. The bandwagon effect explains why there are fashion trends.\n\nWhen individuals make rational choices based on the information they receive from others, economists have proposed that information cascades can quickly form in which people decide to ignore their personal information signals and follow the behavior of others. Cascades explain why behavior is fragile—people understand that they are based on very limited information. As a result, fads form easily but are also easily dislodged. Such informational effects have been used to explain political bandwagons.\n\nThe definition of a bandwagon is a wagon which carries a band during the course of a parade, circus or other entertainment event. The phrase \"jump on the bandwagon\" first appeared in American politics in 1848 when Dan Rice, a famous and popular circus clown of the time, used his bandwagon and its music to gain attention for his political campaign appearances. As his campaign became more successful, other politicians strove for a seat on the bandwagon, hoping to be associated with his success. Later, during the time of William Jennings Bryan's 1900 presidential campaign, bandwagons had become standard in campaigns, and the phrase \"jump on the bandwagon\" was used as a derogatory term, implying that people were associating themselves with success without considering that with which they associated themselves.\n\nThe bandwagon effect occurs in voting: some people vote for those candidates or parties who are likely to succeed (or are proclaimed as such by the media), hoping to be on the \"winner's side\" in the end. The bandwagon effect has been applied to situations involving majority opinion, such as political outcomes, where people alter their opinions to the majority view. Such a shift in opinion can occur because individuals draw inferences from the decisions of others, as in an informational cascade.\n\nBecause of time zones, election results are broadcast in the eastern parts of the United States while polls are still open in the west. This difference has led to research on how the behavior of voters in western United States is influenced by news about the decisions of voters in other time zones. In 1980, NBC News declared Ronald Reagan to be the winner of the presidential race on the basis of the exit polls several hours before the voting booths closed in the west.\n\nIt is also said to be important in the American presidential primary elections. States all vote at different times, spread over some months, rather than all on one day. Some states (Iowa, New Hampshire) have special precedence to go early while others choose to wait until a certain date. This is often said to give undue influence to these states, a win in these early states is said to give a candidate the \"Big Mo\" (momentum) and has propelled many candidates to win the nomination. Because of this, other states often try front loading (going as early as possible) to make their say as influential as they can. In the 2008 presidential primaries two states had all or some of their delegates banned from the convention by the central party organizations for voting too early.\n\nSeveral studies have tested this theory of the bandwagon effect in political decision making. In the 1994 study of Robert K. Goidel and Todd G. Shields in \"The Journal of Politics\", 180 students at the University of Kentucky were randomly assigned to nine groups and were asked questions about the same set of election scenarios. About 70% of subjects received information about the expected winner. Independents, which are those who do not vote based on the endorsement of any party and are ultimately neutral, were influenced strongly in favor of the person expected to win. Expectations played a significant role throughout the study. It was found that independents are twice as likely to vote for the Republican candidate when the Republican is expected to win. From the results, it was also found that when the Democrat was expected to win, independent Republicans and weak Republicans were more likely to vote for the Democratic candidate.\n\nA study by Albert Mehrabian, reported in the \"Journal of Applied Social Psychology\" (1998), tested the relative importance of the bandwagon (rally around the winner) effect versus the underdog (empathic support for those trailing) effect. Bogus poll results presented to voters prior to the 1996 Republican primary clearly showed the bandwagon effect to predominate on balance. Indeed, approximately 6% of the variance in the vote was explained in terms of the bogus polls, showing that poll results (whether accurate or inaccurate) can significantly influence election results in closely contested elections. In particular, assuming that one candidate \"is an initial favorite by a slim margin, reports of polls showing that candidate as the leader in the race will increase his or her favorable margin\". Thus, as poll results are repeatedly reported, the bandwagon effect will tend to snowball and become a powerful aid to leading candidates.\n\nDuring the 1992 U.S. presidential election, Vicki G. Morwitz and Carol Pluzinski conducted a study, which was published in \"The Journal of Consumer Research\" (1996). At a large northeastern university, some of 214 volunteer business students were given the results of student and national polls indicating that Bill Clinton was in the lead. Others were not exposed to the results of the polls. Several students who had intended to vote for Bush changed their minds after seeing the poll results.\n\nAdditionally, British polls have shown an increase to public exposure. Sixty-eight percent of voters had heard of the general election campaign results of the opinion poll in 1979. In 1987, this number of voters aware of the results increased to 74%. According to British studies, there is a consistent pattern of apparent bandwagon effects for the leading party.\n\nIn a study published in the \"European Economic Review\" (2015), Morton and co-authors, used the large number of time zones across French overseas territories, to study the bandwagon effect. Before 2002, all territories were voting on Sunday in the presidential elections. For that reason, voters in territories located to the West of mainland France (e.g. French Guyana) could observe the exit polls from mainland France before the close of their local polling booths. After 2002, a law was passed for these territories to vote on Saturday, in order to avoid this situation. The authors observed a bandwagon effect: when voters from Western territories could observe the winner in mainland France, this candidate was doing much better locally. After 2002, when voting in these territories took place before mainland France, this bandwagon voting disappeared.\n\nIn microeconomics, bandwagon effects may play out in interactions of demand and preference. The bandwagon effect arises when people's preference for a commodity increases as the number of people buying it increases. This interaction potentially disturbs the normal results of the theory of supply and demand, which assumes that consumers make buying decisions solely based on price and their own personal preference.\n\nGary Becker has argued that bandwagon effects could be so strong as to make the demand curve slope upward.\n\nMedical bandwagons have been identified as “the overwhelming acceptance of unproved but popular ideas”. They have led to inappropriate therapies for numerous numbers of patients, and have impeded the development of more appropriate treatment.\n\nIn Lawrence Cohen and Henry Rothschild's exposition \"The Bandwagons of Medicine\" (1979) several of these therapeutic misadventures, some of which persisted for centuries before they were abandoned, substituted by another bandwagon, or replaced by a scientifically valid alternative. The ancient serpent cult of Aesculapius, in which sacred snakes licked the afflicted as treatment of their diseases, is an example of a bandwagon gathering momentum based on a strong personality, in this case a Roman god.\n\nOne who supports a particular sports team, despite having shown no interest in that team until it started gaining success, can be considered a \"bandwagon fan\". One recent example in the United States is the Golden State Warriors, who rose to prominence by winning the 2015 NBA Finals, followed by a record-breaking 73-9 record the following year. Consequently, sales of point guard Stephen Curry's jersey skyrocketed. Curry merchandise sales in the first two weeks of the 2015–2016 season were 453% higher than in the first two weeks of the 2014–2015 season, including a 581% increase in sales of his jersey; his merchandise was a top-seller in 38 of the 50 U.S. states, and the Warriors' merchandise became the best-selling of any NBA team.\n\n\n"}
{"id": "20948", "url": "https://en.wikipedia.org/wiki?curid=20948", "title": "Brainwashing", "text": "Brainwashing\n\nBrainwashing (also known as mind control, menticide, coercive persuasion, thought control, thought reform, and re-education) is the concept that the human mind can be altered or controlled by certain psychological techniques. Brainwashing is said to reduce its subject’s ability to think critically or independently, to allow the introduction of new, unwanted thoughts and ideas into the subject’s mind, as well as to change his or her attitudes, values, and beliefs. \n\nThe concept of brainwashing was originally developed in the 1950s to explain how the Chinese government appeared to make people cooperate with them. Advocates of the concept also looked at Nazi Germany, at some criminal cases in the United States, and at the actions of human traffickers. It was later applied by Margaret Singer, Philip Zimbardo and some others in the anti-cult movement to explain conversions to some new religious movements and other groups. This resulted in scientific and legal debate with Eileen Barker, James Richardson, and other scholars, as well as legal experts, rejecting at least the popular understanding of brainwashing.\n\nOther views have been expressed by scholars including: Dick Anthony, Robert Cialdini, Stanley A. Deetz, Michael J. Freeman, Robert Jay Lifton, Joost Meerloo, Daniel Romanovsky, Kathleen Taylor, Louis Jolyon West, and Benjamin Zablocki. The concept of brainwashing is sometimes involved in legal cases, especially regarding child custody; and is also a major theme in science fiction and in criticism of modern political and corporate culture. Although the term appears in the fifth edition of the Diagnostic and Statistical Manual of Mental Disorders (DSM-5) of the American Psychiatric Association it is not accepted as scientific fact.\n\nThe Chinese term \"xǐnăo\" (洗脑，literally \"wash brain\") was originally used to describe the coercive persuasion used under the Maoist government in China, which aimed to transform \"reactionary\" people into \"right-thinking\" members of the new Chinese social system. The term punned on the Taoist custom of \"cleansing/washing the heart/mind\" (\"xǐxīn\"，洗心) before conducting ceremonies or entering holy places.\n\nThe \"Oxford English Dictionary\" records the earliest known English-language usage of the word \"brainwashing\" in an article by newspaperman Edward Hunter, in \"Miami News\", published on 24 September 1950. Hunter was an outspoken anticommunist and was said to be a CIA agent working undercover as a journalist. Hunter and others used the Chinese term to explain why, during the Korean War (1950-1953), some American prisoners of war (POWs) cooperated with their Chinese captors, even in a few cases defected to their side. British radio operator Robert W. Ford and British army Colonel James Carne also claimed that the Chinese subjected them to brainwashing techniques during their war-era imprisonment.\n\nThe U.S. military and government laid charges of brainwashing in an effort to undermine confessions made by POWs to war crimes, including biological warfare. After Chinese radio broadcasts claimed to quote Frank Schwable, Chief of Staff of the First Marine Air Wing admitting to participating in germ warfare, United Nations commander Gen. Mark W. Clark asserted:\n\nBeginning in 1953, Robert Jay Lifton interviewed American servicemen who had been POWs during the Korean War as well as priests, students, and teachers who had been held in prison in China after 1951. In addition to interviews with 25 Americans and Europeans, Lifton interviewed 15 Chinese citizens who had fled after having been subjected to indoctrination in Chinese universities. (Lifton's 1961 book \"\", was based on this research.) Lifton found that when the POWs returned to the United States their thinking soon returned to normal, contrary to the popular image of \"brainwashing.\"\n\nIn 1956, after reexamining the concept of brainwashing following the Korean War, the U.S. Army published a report entitled \"Communist Interrogation, Indoctrination, and Exploitation of Prisoners of War\", which called brainwashing a \"popular misconception\". The report states \"exhaustive research of several government agencies failed to reveal even one conclusively documented case of 'brainwashing' of an American prisoner of war in Korea.\"\n\nIn George Orwell's 1949 dystopian novel \"Nineteen Eighty-Four\" the main character is subjected to imprisonment, isolation, and torture in order to conform his thoughts and emotions to the wishes of the rulers of Orwell's fictional future totalitarian society. Orwell's vision influenced Hunter and is still reflected in the popular understanding of the concept of brainwashing. Written about the same time, J.R.R. Tolkien’s \"The Lord of the Rings\" also addressed brainwashing, although in a fantasy setting. The science fiction stories of Cordwainer Smith (written from the 1940s until his death in 1966) depict brainwashing to remove memories of traumatic events as a normal and benign part of future medical practice.\n\nIn the 1950s many American films were filmed that featured brainwashing of POWs, including \"The Rack\", \"The Bamboo Prison\", \"Toward the Unknown\", and \"The Fearmakers\". \"Forbidden Area\" told the story of Soviet secret agents who had been brainwashed through classical conditioning by their own government so they wouldn't reveal their identities. In 1962 \"The Manchurian Candidate\" (based on the 1959 novel by Richard Condon) \"put brainwashing front and center\" by featuring a plot by the Soviet government to take over the United States by use of a brainwashed presidential candidate. The concept of brainwashing became popularly associated with the research of Russian psychologist Ivan Pavlov, which mostly involved dogs, not humans, as subjects. In \"The Manchurian Candidate\" the head brainwasher is Dr. Yen Lo, of the Pavlov Institute.\n\nMind control remains an important theme in science fiction. Terry O'Brien comments: \"Mind control is such a powerful image that if hypnotism did not exist, then something similar would have to have been invented: the plot device is too useful for any writer to ignore. The fear of mind control is equally as powerful an image.\" A subgenre is \"corporate mind control\", in which a future society is run by one or more business corporations that dominate society using advertising and mass media to control the population's thoughts and feelings.\n\nFor twenty years starting in the early 1950s, the United States Central Intelligence Agency (CIA) and the United States Department of Defense conducted secret research, including Project MKUltra, in an attempt to develop practical brainwashing techniques; the results are unknown. (See also Sidney Gottlieb.) CIA experiments using various psychedelic drugs such as LSD and Mescaline drew from Nazi human experimentation.\n\nIn 1974, Patty Hearst, a member of the wealthy Hearst family, was kidnapped by a left-wing group calling itself the Symbionese Liberation Army. After several weeks of captivity she agreed to join the group and took part in their activities. In 1975, she was arrested and charged with bank robbery and use of a gun in committing a felony. Her attorney, F. Lee Bailey argued in her trial that she should not be held responsible for her actions since her treatment by her captors was the equivalent of the brainwashing of Korean War POWs. (See: diminished responsibility.) Hearst was found guilty, but her “brainwashing defense” brought the topic to renewed public attention in the United States, as did the 1969 to 1971 case of Charles Manson, who was said to have brainwashed his followers to commit murder and other crimes.\n\nBailey developed his case in conjunction with psychiatrist Louis Jolyon West and psychologist Margaret Singer. They had both studied the experiences of Korean War POWs. In 1996 Singer published her theories in her best-selling book \"Cults in Our Midst\". In 2003, the brainwashing defense was used unsuccessfully in the defense of Lee Boyd Malvo, who was charged with murder for his part in the D.C. sniper attacks. Some legal scholars have argued that the brainwashing defense undermines the law’s fundamental premise of free will.\n\nItaly has had controversy over the concept of \"plagio\", a crime consisting in an absolute psychological—and eventually physical—domination of a person. The effect is said to be the annihilation of the subject's freedom and self-determination and the consequent negation of his or her personality. The crime of plagio has rarely been prosecuted in Italy, and only one person was ever convicted. In 1981, an Italian court found that the concept is imprecise, lacks coherence, and is liable to arbitrary application. By the twenty-first century, the concept of brainwashing was being applied \"with some success\" in child custody and child sexual abuse cases. In some cases \"one parent is accused of brainwashing the child to reject the other parent, and in child sex abuse cases where one parent is accused of brainwashing the child to make sex abuse accusations against the other parent\" (possibly resulting in or causing parental alienation).\n\nIn 2003, forensic psychologist Dick Anthony said that \"no reasonable person would question that there are situations where people can be influenced against their best interests, but those arguments are evaluated on the basis of fact, not bogus expert testimony.\" In 2016, Israeli anthropologist of religion and fellow at the Van Leer Jerusalem Institute Adam Klin-Oron said about then-proposed \"anti-cult\" legislation: \n\nIn the 1970s, the anti-cult movement applied the concept of brainwashing to explain seemingly sudden and dramatic religious conversions to various new religious movements (NRMs) and other groups they considered cults. News media reports tended to support the brainwashing view and social scientists sympathetic to the anti-cult movement, who were usually psychologists, developed revised models of mind control. While some psychologists were receptive to the concept, sociologists were for the most part skeptical of its ability to explain conversion to NRMs.\n\nPhilip Zimbardo defined mind control as, \"the process by which individual or collective freedom of choice and action is compromised by agents or agencies that modify or distort perception, motivation, affect, cognition or behavioral outcomes,\" and he suggested that any human being is susceptible to such manipulation. Another adherent to this view, Jean-Marie Abgrall was heavily criticized by forensic psychologist Dick Anthony for employing a pseudo-scientific approach and lacking any evidence that anyone's worldview was substantially changed by these coercive methods. On the contrary, the concept and the fear surrounding it was used as a tool for the anti-cult movement to rationalize the persecution of minority religious groups.\n\nEileen Barker criticized the concept of mind control because it functioned to justify costly interventions such as deprogramming or exit counseling. She has also criticized some mental health professionals, including Singer, for accepting expert witness jobs in court cases involving NRMs. Her 1984 book, \"\" describes the religious conversion process to the Unification Church (whose members are sometimes informally referred to as \"Moonies\"), which had been one of the best known groups said to practice brainwashing. Barker spent close to seven years studying Unification Church members. She interviewed in depth or gave probing questionnaires to church members, ex-members, \"non-joiners,\" and control groups of uninvolved people from similar backgrounds, as well as parents, spouses, and friends of members. She also attended numerous church workshops and communal facilities. Barker writes that she rejects the \"brainwashing\" theory, because it explains neither the many people who attended a recruitment meeting and did not become members, nor the voluntary disaffiliation of members.\n\nJames Richardson observed that if the new religious movements had access to powerful brainwashing techniques, one would expect that they would have high growth rates, yet in fact most have not had notable success in recruitment. Most adherents participate for only a short time, and the success in retaining members is limited. For this and other reasons, sociologists of religion including David Bromley and Anson Shupe consider the idea that \"cults\" are brainwashing American youth to be \"implausible.\" In addition, Thomas Robbins, Massimo Introvigne, Lorne Dawson, Gordon Melton, Marc Galanter, and Saul Levine, amongst other scholars researching NRMs, have argued and established to the satisfaction of courts, relevant professional associations and scientific communities that there exists no generally accepted scientific theory, based upon methodologically sound research, that supports the concept of brainwashing as advanced by the anti-cult movement.\n\nBenjamin Zablocki responded that brainwashing is not \"a process that is directly observable,\" and that the \"real sociological issue\" is whether \"brainwashing occurs frequently enough to be considered an important social problem\", and that Richardson misunderstands brainwashing, conceiving of it as a recruiting process, instead of a retaining process, and that the number of people who attest to brainwashing in interviews (performed in accordance with guidelines of the National Institute of Mental Health and National Science Foundation) is too large result from anything other than a genuine phenomenon. Zablocki also pointed out that in the two most prestigious journals dedicated to the sociology of religion there have been no articles \"supporting the brainwashing perspective,\" while over one hundred such articles have been published in other journals \"marginal to the field.\" He concludes that the concept of brainwashing has been unfairly blacklisted.\n\nIn 1983, the American Psychological Association (APA) asked Singer to chair a taskforce called the APA Task Force on Deceptive and Indirect Techniques of Persuasion and Control (DIMPAC) to investigate whether brainwashing or coercive persuasion did indeed play a role in recruitment by NRMs.\n\"Cults and large group awareness trainings have generated considerable controversy because of their widespread use of deceptive and indirect techniques of persuasion and control. These techniques can compromise individual freedom, and their use has resulted in serious harm to thousands of individuals and families. This report reviews the literature on this subject, proposes a new way of conceptualizing influence techniques, explores the ethical ramifications of deceptive and indirect techniques of persuasion and control, and makes recommendations addressing the problems described in the report.\"\nOn 11 May 1987, the APA's Board of Social and Ethical Responsibility for Psychology (BSERP) rejected the DIMPAC report because the report \"lacks the scientific rigor and evenhanded critical approach necessary for APA imprimatur\", and concluded that \"after much consideration, BSERP does not believe that we have sufficient information available to guide us in taking a position on this issue.\"\n\nKathleen Barry, co-founder of the United Nations NGO, the Coalition Against Trafficking in Women (CATW), in her 1979 book \"Female Sexual Slavery\" prompted international awareness of human sex trafficking. In his 1986 book \"Woman Abuse: Facts Replacing Myths\" Lewis Okun reported that: “Kathleen Barry shows in Female Sexual Slavery that forced female prostitution involves coercive control practices very similar to thought reform.” In their 1996 book, \"Casting Stones: Prostitution and Liberation in Asia and the United States\", Rita Nakashima Brock and Susan Brooks Thistlethwaite report that the methods commonly used by pimps to control their victims \"closely resemble the brainwashing techniques of terrorists and paranoid cults.\"\n\nSome of the techniques used by traffickers include feigning love and concern for the victims' well-being to gain trust before beginning to track, manipulate and control the entire life of the victim, including environment, relationships, access to information and daily activities, promises of lucrative employment or corrupt marriage proposals, debt bondage, kidnapping, induced drug dependency and fear tactics such as threats about law enforcement, deportation, and harm to friends or family members. Physical captivity, shame, Stockholm Syndrome, traumatic bonding and fear of arrest can contribute to victims’ inability to seek assistance.\n\nRussian historian Daniel Romanovsky, who interviewed survivors and eyewitnesses in the 1970s, reported on what he called \"Nazi brainwashing\" of the people of Belarus by the occupying Germans during the Second World War, which took place through both mass propaganda and intense re-education, especially in schools. Romanovsky noted that very soon most people had adopted the Nazi view that the Jews were an inferior race and were closely tied to the Soviet government, views that had not been at all common before the German occupation.\n\nJoost Meerloo, a Dutch psychiatrist, was an early proponent of the concept of brainwashing. (\"Menticide\" is a neologism coined by him meaning: \"killing of the mind.\") Meerloo's view was influenced by his experiences during the German occupation of his country and his work with the Dutch government and the American military in the interrogation of accused Nazi war criminals. He later emigrated to the United States and taught at Columbia University. His best-selling 1956 book, \"The Rape of the Mind\", concludes by saying: \n\nScholars have said that modern business corporations practice mind control to create a work force that shares common values and culture. Critics have linked \"corporate brainwashing\" with globalization, saying that corporations are attempting to create a worldwide monocultural network of producers, consumers, and managers. Modern educational systems have also been criticized, by both the left and the right, for contributing to corporate brainwashing. In his 1992 book, \"Democracy in an Age of Corporate Colonization\", Stanley A. Deetz says that modern \"self awareness\" and \"self improvement\" programs provide corporations with even more effective tools to control the minds of employees than traditional brainwashing.\n\nIn his 2000 book, \"Destroying the World to Save It: Aum Shinrikyo, Apocalyptic Violence, and the New Global Terrorism\", Robert Lifton applied his original ideas about thought reform to Aum Shinrikyo and the War on Terrorism, concluding that in this context thought reform was possible without violence or physical coercion. He also pointed out that in their efforts against terrorism Western governments were also using some mind control techniques, including thought-terminating clichés.\n\nIn her 2004 popular science book, \"\", neuroscientist and physiologist Kathleen Taylor reviewed the history of mind control theories, as well as notable incidents. She suggests that persons under its influence have more rigid neurological pathways, and that can make it more difficult to rethink situations or be able to later reorganize these pathways. Reviewers praised her book for its clear presentation, while some criticized it for oversimplification.\n\n"}
{"id": "39293555", "url": "https://en.wikipedia.org/wiki?curid=39293555", "title": "Constant spectrum melody", "text": "Constant spectrum melody\n\nA constant timbre at a constant pitch is characterized by a spectrum.\nAlong a piece of music, the spectrum measured within a narrow time window varies with the melody and the possible effects of instruments.\nTherefore, it may seem paradoxical that a constant spectrum can be perceived as a melody rather than a stamp.\nThe paradox is that the ear is not an abstract spectrograph: it \"calculates\" the Fourier transform of the sound signal in a narrow time window, but the slower variations are seen as temporal evolution and not as pitch.\n\nHowever, the example of paradoxical melody above contains no infrasound (i.e. pure tone of period slower than the time window).\nThe second paradox is that when two pitches are very close, they create a beat. If the period of this beat is longer than the integration window, it is seen as a sinusoidal variation in the average rating: sin(2π(f+ε)t) + sin(2π(f-ε)t) = sin(2πft)cos(2πεt), where 1/ε is the slow period.\nThe present spectrum is made of multiple frequencies beating together, resulting in a superimposition of various pitches fading in and out at different moments and pace, thus forming the melody.\n\nHere is the program used to generate the paradoxical melody:\n\n"}
{"id": "52821475", "url": "https://en.wikipedia.org/wiki?curid=52821475", "title": "D-space", "text": "D-space\n\nIn mathematics, a topological space formula_1 is a D-space if for any family formula_2 of open sets such that formula_3 for all points formula_4, there is a closed discrete subset formula_5 of the space formula_1 such that formula_7.\n\nThe notion of D-spaces was introduced by Eric Karel van Douwen and E.A. Michael. It first appeared in a 1979 paper by van Douwen and Washek Frantisek Pfeffer in the Pacific Journal of Mathematics. Whether every Lindelöf and regular topological space is a D-space is known as the D-space problem. This problem is among twenty of the most important problems of set theoretic topology.\n\n"}
{"id": "17684928", "url": "https://en.wikipedia.org/wiki?curid=17684928", "title": "Diamond–Dybvig model", "text": "Diamond–Dybvig model\n\nThe Diamond–Dybvig model is an influential model of bank runs and related financial crises. The model shows how banks' mix of illiquid assets (such as business or mortgage loans) and liquid liabilities (deposits which may be withdrawn at any time) may give rise to self-fulfilling panics among depositors.\n\nThe model, published in 1983 by Douglas W. Diamond of the University of Chicago and Philip H. Dybvig, then of Yale University and now of Washington University in St. Louis, shows how an institution with long-maturity assets and short-maturity liabilities can be unstable.\n\nDiamond and Dybvig's paper points out that business investment often requires expenditures in the present to obtain returns in the future. Therefore, they prefer loans with a long maturity (that is, low liquidity). The same principle applies to individuals seeking financing to purchase large-ticket items such as housing or automobiles. On the other hand, individual savers (both households and firms) may have sudden, unpredictable needs for cash, due to unforeseen expenditures. So they demand liquid accounts which permit them immediate access to their deposits (that is, they value short maturity deposit accounts).\n\nThe banks in the model act as intermediaries between savers who prefer to deposit in liquid accounts and borrowers who prefer to take out long-maturity loans. Under ordinary circumstances, banks can provide a valuable service by channeling funds from many individual deposits into loans for borrowers. Individual depositors might not be able to make these loans themselves, since they know they may suddenly need immediate access to their funds, whereas the businesses' investments will only pay off in the future (moreover, by aggregating funds from many different depositors, banks help depositors save on the transaction costs they would have to pay in order to lend directly to businesses). Since banks provide a valuable service to both sides (providing the long-maturity loans businesses want and the liquid accounts depositors want), they can charge a higher interest rate on loans than they pay on deposits and thus profit from the difference.\n\nDiamond and Dybvig point out that under ordinary circumstances, savers' unpredictable needs for cash are likely to be random, as depositors' needs reflect their individual circumstances. Since depositors' demand for cash are unlikely to occur at the same time, by accepting deposits from many different sources the bank expects only a small fraction of withdrawals in the short term, even though all depositors have the right to withdraw their full deposit at any time. Thus, a bank can make loans over a long horizon, while keeping only relatively small amounts of cash on hand to pay any depositors that wish to make withdrawals. Mathematically, individual withdrawals are largely uncorrelated, and by the law of large numbers banks expect a relatively stable number of withdrawals on any given day.\n\nHowever a different outcome is also possible. Since banks lend out at long maturity, they cannot quickly call in their loans. And even if they tried to call in their loans, borrowers would be unable to pay back quickly, since their loans were, by assumption, used to finance long-term investments. Therefore, if all depositors attempt to withdraw their funds simultaneously, a bank will run out of money long before it is able to pay all the depositors. The bank will be able to pay the first depositors who demand their money back, but if all others attempt to withdraw too, the bank will go bankrupt and the last depositors will be left with nothing.\n\nThis means that even healthy banks are potentially vulnerable to panics, usually called bank runs. If a depositor expects all other depositors to withdraw their funds, then it is irrelevant whether the banks' long term loans are likely to be profitable; the only rational response for the depositor is to rush to take his or her deposits out before the other depositors remove theirs. In other words, the Diamond–Dybvig model views bank runs as a type of self-fulfilling prophecy: each depositor's incentive to withdraw funds depends on what they expect other depositors to do. If enough depositors expect other depositors to withdraw their funds, then they all have an incentive to rush to be the first in line to withdraw their funds.\n\nIn theoretical terms, the Diamond–Dybvig model provides an example of an economic game with more than one Nash equilibrium. If depositors expect most other depositors to withdraw only when they have real expenditure needs, then it is rational for all depositors to withdraw only when they have real expenditure needs. But if depositors expect most other depositors to rush quickly to close their accounts, then it is rational for all depositors to rush quickly to close their accounts. Of course, the first equilibrium is better than the second (in the sense of Pareto efficiency). If depositors withdraw only when they have real expenditure needs, they all benefit from holding their savings in a liquid, interest-bearing account. If instead everyone rushes to close their accounts, then they all lose the interest they could have earned, and some of them lose all their savings. Nonetheless, it is not obvious what any one depositor could do to prevent this mutual loss.\n\nIn practice, due to fractional reserve banking, banks faced with a bank run usually shut down and refuse to permit more withdrawals. This is called a \"suspension of convertibility\", and engenders further panic in the financial system. While this may prevent some depositors who have a real need for cash from obtaining access to their money, it also prevents immediate bankruptcy, thus allowing the bank to wait for its loans to be repaid, so that it has enough resources to pay back some or all of its deposits.\n\nHowever, Diamond and Dybvig argue that unless the total amount of real expenditure needs per period is known with certainty, suspension of convertibility cannot be the optimal mechanism for preventing bank runs. Instead, they argue that a better way of preventing bank runs is deposit insurance backed by the government or central bank. Such insurance pays depositors all or part of their losses in the case of a bank run. If depositors know that they will get their money back even in case of a bank run, they have no reason to participate in a bank run.\n\nThus, sufficient deposit insurance can eliminate the possibility of bank runs. In principle, maintaining a deposit insurance program is unlikely to be very costly for the government: as long as bank runs are prevented, deposit insurance will never actually need to be paid out. Bank runs became much rarer in the U.S. after the Federal Deposit Insurance Corporation was founded in the aftermath of the bank panics of the Great Depression. On the other hand, a deposit insurance scheme is likely to lead to moral hazard: by protecting depositors against bank failure, it makes depositors less careful in choosing where to deposit their money, and thus gives banks less incentive to lend carefully.\n\n\n"}
{"id": "2574377", "url": "https://en.wikipedia.org/wiki?curid=2574377", "title": "Diffusion MRI", "text": "Diffusion MRI\n\nDiffusion-weighted magnetic resonance imaging (DWI or DW-MRI) is the use of specific MRI sequences as well as software that generates images from the resulting data, that uses the diffusion of water molecules to generate contrast in MR images. It allows the mapping of the diffusion process of molecules, mainly water, in biological tissues, in vivo and non-invasively. Molecular diffusion in tissues is not free, but reflects interactions with many obstacles, such as macromolecules, fibers, and membranes. Water molecule diffusion patterns can therefore reveal microscopic details about tissue architecture, either normal or in a diseased state. A special kind of DWI, diffusion tensor imaging (DTI), has been used extensively to map white matter tractography in the brain.\n\nIn diffusion weighted imaging (DWI), the intensity of each image element (voxel) reflects the best estimate of the rate of water diffusion at that location. Because the mobility of water is driven by thermal agitation and highly dependent on its cellular environment, the hypothesis behind DWI is that findings may indicate (early) pathologic change. For instance, DWI is more sensitive to early changes after a stroke than more traditional MRI measurements such as T1 or T2 relaxation rates. A variant of diffusion weighted imaging, diffusion spectrum imaging (DSI), was used in deriving the Connectome data sets; DSI is a variant of diffusion-weighted imaging that is sensitive to intra-voxel heterogeneities in diffusion directions caused by crossing fiber tracts and thus allows more accurate mapping of axonal trajectories than other diffusion imaging approaches.\n\nDiffusion-weighted images are very useful to diagnose vascular strokes in the brain. It is also used more and more in the staging of non-small-cell lung cancer, where it is a serious candidate to replace positron emission tomography as the 'gold standard' for this type of disease. Diffusion tensor imaging is being developed for studying the diseases of the white matter of the brain as well as for studies of other body tissues (see below). DWI is most applicable when the tissue of interest is dominated by isotropic water movement e.g. grey matter in the cerebral cortex and major brain nuclei, or in the body—where the diffusion rate appears to be the same when measured along any axis. However, DWI also remains sensitive to T1 and T2 relaxation. To entangle diffusion and relaxation effects on image contrast, one may obtain quantitative images of the diffusion coefficient, or more exactly the apparent diffusion coefficient (ADC). The ADC concept was introduced to take into account the fact that the diffusion process is complex in biological tissues and reflects several different mechanisms.\n\nDiffusion tensor imaging (DTI) is important when a tissue—such as the neural axons of white matter in the brain or muscle fibers in the heart—has an internal fibrous structure analogous to the anisotropy of some crystals. Water will then diffuse more rapidly in the direction aligned with the internal structure, and more slowly as it moves perpendicular to the preferred direction. This also means that the measured rate of diffusion will differ depending on the direction from which an observer is looking.\n\nTraditionally, in diffusion-weighted imaging (DWI), three gradient-directions are applied, sufficient to estimate the trace of the diffusion tensor or 'average diffusivity', a putative measure of edema. Clinically, trace-weighted images have proven to be very useful to diagnose vascular strokes in the brain, by early detection (within a couple of minutes) of the hypoxic edema.\n\nMore extended DTI scans derive neural tract directional information from the data using 3D or multidimensional vector algorithms based on six or more gradient directions, sufficient to compute the diffusion tensor. The diffusion model is a rather simple model of the diffusion process, assuming homogeneity and linearity of the diffusion within each image voxel. From the diffusion tensor, diffusion anisotropy measures such as the fractional anisotropy (FA), can be computed. Moreover, the principal direction of the diffusion tensor can be used to infer the white-matter connectivity of the brain (i.e. tractography; trying to see which part of the brain is connected to which other part).\n\nRecently, more advanced models of the diffusion process have been proposed that aim to overcome the weaknesses of the diffusion tensor model. Amongst others, these include q-space imaging and generalized diffusion tensor imaging.\n\nDiffusion imaging is an MRI method that produces in vivo magnetic resonance images of biological tissues sensitized with the local characteristics of molecular diffusion, generally water (but other moieties can also be investigated using MR spectroscopic approaches).\nMRI can be made sensitive to the motion of molecules. Regular MRI acquisition utilizes the behaviour of protons in water to generate contrast between clinically relevant features of a particular subject. The versatile nature of MRI is due to this capability of producing contrast related to the structure of tissues at microscopic level. In a typical formula_1-weighted image, water molecules in a sample are excited with the imposition of a strong magnetic field. This causes many of the protons in water molecules to precess simultaneously, producing signals in MRI. In formula_2-weighted images, contrast is produced by measuring the loss of coherence or synchrony between the water protons. When water is in an environment where it can freely tumble, relaxation tends to take longer. In certain clinical situations, this can generate contrast between an area of pathology and the surrounding healthy tissue.\n\nTo sensitize MRI images to diffusion, instead of a homogeneous magnetic field, the homogeneity is varied linearly by a pulsed field gradient. Since precession is proportional to the magnet strength, the protons begin to precess at different rates, resulting in dispersion of the phase and signal loss. Another gradient pulse is applied in the same magnitude but with opposite direction to refocus or rephase the spins. The refocusing will not be perfect for protons that have moved during the time interval between the pulses, and the signal measured by the MRI machine is reduced. This \"field gradient pulse\" method was initially devised for NMR by Stejskal and Tanner who derived the reduction in signal due to the application of the pulse gradient related to the amount of diffusion that is occurring through the following equation:\n\nwhere formula_4 is the signal intensity without the diffusion weighting, formula_5 is the signal with the gradient, formula_6 is the gyromagnetic ratio, formula_7 is the strength of the gradient pulse, formula_8 is the duration of the pulse, formula_9 is the time between the two pulses, and finally, formula_10 is the diffusion-coefficient.\n\nIn order to localize this signal attenuation to get images of diffusion one has to combine the pulsed magnetic field gradient pulses used for MRI (aimed at localization of the signal, but those gradient pulses are too weak to produce a diffusion related attenuation) with additional \"motion-probing\" gradient pulses, according to the Stejskal and Tanner method. This combination is not trivial, as cross-terms arise between all gradient pulses. The equation set by Stejskal and Tanner then becomes inaccurate and the signal attenuation must be calculated, either analytically or numerically, integrating all gradient pulses present in the MRI sequence and their interactions. The result quickly becomes very complex given the many pulses present in the MRI sequence and, as a simplication, Le Bihan suggested to gather all the gradient terms in a \"b factor\" (which depends only on the acquisition parameters), so that the signal attenuation simply becomes:\n\nAlso, the diffusion coefficient, formula_10, is replaced by an apparent diffusion coefficient, formula_13, to indicate that the diffusion process is not free in tissues, but hindered and modulated by many mechanisms (restriction in closed spaces, tortuosity around obstacles, etc.) and that other sources of IntraVoxel Incoherent Motion (IVIM) such as blood flow in small vessels or cerebrospinal fluid in ventricles also contribute to the signal attenuation.\nAt the end, images are \"weighted\" by the diffusion process: In those diffusion-weighted images (DWI) the signal is more attenuated the faster the diffusion and the larger the b factor is. However, those diffusion-weighted images are still also sensitive to T1 and T2 relaxivity contrast, which can sometimes be confusing. It is possible to calculate \"pure\" diffusion maps (or more exactly ADC maps where the ADC is the sole source of contrast) by collecting images with at least 2 different values, formula_14 and formula_15, of the b factor according to:\n\nAlthough this ADC concept has been extremely successful, especially for clinical applications, it has been challenged recently, as new, more comprehensive models of diffusion in biological tissues have been introduced. Those models have been made necessary, as diffusion in tissues is not free. In this condition, the ADC seems to depend on the choice of b values (the ADC seems to decrease when using larger b values), as the plot of ln(S/So) is not linear with the b factor, as expected from the above equations. This deviation from a free diffusion behavior is what makes diffusion MRI so successful, as the ADC is very sensitive to changes in tissue microstructure. On the other hand, modeling diffusion in tissues is becoming very complex. Among most popular models are the biexponential model, which assumes the presence of 2 water pools in slow or intermediate exchange and the cumulant-expansion (also called Kurtosis) model \nwhich does not necessarily require the presence of 2 pools.\n\nGiven the concentration formula_17 and flux formula_18, Fick's first law gives a relationship between the flux and the concentration gradient:\n\nwhere D is the diffusion coefficient. Then, given conservation of mass, the continuity equation relates the time derivative of the concentration with the divergence of the flux:\n\nPutting the two together, we get the diffusion equation:\n\nWith no diffusion present, the change in nuclear magnetization over time is given by the classical Bloch equation\n\nwhich has terms for precession, T2 relaxation, and T1 relaxation.\n\nIn 1956, H.C. Torrey mathematically showed how the Bloch equations for magnetization would change with the addition of diffusion. Torrey modified Bloch's original description of transverse magnetization to include diffusion terms and the application of a spatially varying gradient. Since the magnetization formula_23 is a vector, there are 3 diffusion equations, one for each dimension. The Bloch–Torrey equation is:\n\nwhere formula_25 is now the diffusion tensor.\n\nFor the simplest case where the diffusion is isotropic the diffusion tensor is a multiple of the identity:\n\nthen the Bloch–Torrey equation will have the solution\n\nThe exponential term will be referred to as the \"attenuation\" formula_28. Anisotropic diffusion will have a similar solution for the diffusion tensor, except that what will be measured is the \"apparent diffusion coefficient\" (ADC). In general, the attenuation is:\n\nwhere the formula_30 terms incorporate the gradient fields formula_31, formula_32, and formula_33.\n\nThe standard grayscale of DWI images is to represent increased diffusion restriction as brighter.\n\nConventional DWI (without DTI) directly visualizes the ischemic necrosis in cerebral infarction in the form of a cytotoxic edema, appearing as a high DWI signal within minutes of arterial occlusion. With perfusion MRI detecting both the infarcted core and the salvageable penumbra, the latter can be quantified by DWI + perfusion MRI.\n\nAn \"apparent diffusion coefficient\" (ADC) image or an \"ADC map\" is an MRI image that more specifically shows diffusion than conventional DWI, by eliminating the T2 weighing that is otherwise inherent in conventional DWI. ADC imaging does so by acquiring multiple conventional DWI images with different amounts of DWI weighing, and the change in signal is proportional to the rate of diffusion. Contrary to DWI images, the standard grayscale of ADC images is to represent a smaller magnitude of diffusion as darker.\n\nCerebral infarction leads to diffusion restriction, and the difference between images with various DWI weighing will therefore be minor, leading to an ADC image with low signal in the infarcted area. A decreased ADC may be detected minutes after a cerebral infarction. The high signal of infarcted tissue on conventional DWI is a result of its partial T2 weighing.\n\nDiffusion tensor imaging (DTI) is a magnetic resonance imaging technique that enables the measurement of the restricted diffusion of water in tissue in order to produce neural tract images instead of using this data solely for the purpose of assigning contrast or colors to pixels in a cross sectional image. It also provides useful structural information about muscle—including heart muscle—as well as other tissues such as the prostate.\n\nIn DTI, each voxel has one or more pairs of parameters: a rate of diffusion and a preferred direction of diffusion—described in terms of three-dimensional space—for which that parameter is valid. The properties of each voxel of a single DTI image is usually calculated by vector or tensor math from six or more different diffusion weighted acquisitions, each obtained with a different orientation of the diffusion sensitizing gradients. In some methods, hundreds of measurements—each making up a complete image—are made to generate a single resulting calculated image data set. The higher information content of a DTI voxel makes it extremely sensitive to subtle pathology in the brain. In addition the directional information can be exploited at a higher level of structure to select and follow neural tracts through the brain—a process called tractography.\n\nA more precise statement of the image acquisition process is that the image-intensities at each position are attenuated, depending on the strength (\"b\"-value) and direction of the so-called magnetic diffusion gradient, as well as on the local microstructure in which the water molecules diffuse. The more attenuated the image is at a given position, the greater diffusion there is in the direction of the diffusion gradient. In order to measure the tissue's complete diffusion profile, one needs to repeat the MR scans, applying different directions (and possibly strengths) of the diffusion gradient for each scan.\n\nIn present-day clinical neurology, various brain pathologies may be best detected by looking at particular measures of anisotropy and diffusivity. The underlying physical process of diffusion causes a group of water molecules to move out from a central point, and gradually reach the surface of an ellipsoid if the medium is anisotropic (it would be the surface of a sphere for an isotropic medium). The ellipsoid formalism functions also as a mathematical method of organizing tensor data. Measurement of an ellipsoid tensor further permits a retrospective analysis, to gather information about the process of diffusion in each voxel of the tissue.\n\nIn an isotropic medium such as cerebro-spinal fluid, water molecules are moving due to diffusion and they move at equal rates in all directions. By knowing the detailed effects of diffusion gradients we can generate a formula that allows us to convert the signal attenuation of an MRI voxel into a numerical measure of diffusion—the diffusion coefficient \"D\". When various barriers and restricting factors such as cell membranes and microtubules interfere with the free diffusion, we are measuring an \"apparent diffusion coefficient\" or ADC because the measurement misses all the local effects and treats it as if all the movement rates were solely due to Brownian motion. The ADC in anisotropic tissue varies depending on the direction in which it is measured. Diffusion is fast along the length of (parallel to) an axon, and slower perpendicularly across it.\n\nOnce we have measured the voxel from six or more directions and corrected for attenuations due to T2 and T1 effects, we can use information from our calculated ellipsoid tensor to describe what is happening in the voxel. If you consider an ellipsoid sitting at an angle in a Cartesian grid then you can consider the projection of that ellipse onto the three axes. The three projections can give you the ADC along each of the three axes ADC, ADC, ADC. This leads to the idea of describing the average diffusivity in the voxel which will simply be\n\nWe use the \"i\" subscript to signify that this is what the isotropic diffusion coefficient would be with the effects of anisotropy averaged out.\n\nThe ellipsoid itself has a principal long axis and then two more small axes that describe its width and depth. All three of these are perpendicular to each other and cross at the center point of the ellipsoid. We call the axes in this setting eigenvectors and the measures of their lengths eigenvalues. The lengths are symbolized by the Greek letter \"λ\". The long one pointing along the axon direction will be \"λ\" and the two small axes will have lengths \"λ\" and \"λ\". In the setting of the DTI tensor ellipsoid, we can consider each of these as a measure of the diffusivity along each of the three primary axes of the ellipsoid. This is a little different from the ADC since that was a projection on the axis, while \"λ\" is an actual measurement of the ellipsoid we have calculated.\n\nThe diffusivity along the principal axis, \"λ\" is also called the longitudinal diffusivity or the axial diffusivity or even the parallel diffusivity \"λ\". Historically, this is closest to what Richards originally measured with the vector length in 1991. The diffusivities in the two minor axes are often averaged to produce a measure of radial diffusivity\n\nThis quantity is an assessment of the degree of restriction due to membranes and other effects and proves to be a sensitive measure of degenerative pathology in some neurological conditions. It can also be called the perpendicular diffusivity (formula_36).\n\nAnother commonly used measure that summarizes the total diffusivity is the Trace—which is the sum of the three eigenvalues,\n\nwhere formula_38 is a diagonal matrix with eigenvalues formula_39, formula_40 and formula_41 on its diagonal.\n\nIf we divide this sum by three we have the mean diffusivity,\n\nwhich equals \"ADC\" since\nwhere formula_44 is the matrix of eigenvectors and formula_45 is the diffusion tensor.\nAside from describing the amount of diffusion, it is often important to describe the relative degree of anisotropy in a voxel. At one extreme would be the sphere of isotropic diffusion and at the other extreme would be a cigar or pencil shaped very thin prolate spheroid. The simplest measure is obtained by dividing the longest axis of the ellipsoid by the shortest = (\"λ\"/\"λ\"). However, this proves to be very susceptible to measurement noise, so increasingly complex measures were developed to capture the measure while minimizing the noise. An important element of these calculations is the sum of squares of the diffusivity differences = (\"λ\" − \"λ\") + (\"λ\" − \"λ\") + (\"λ\" − \"λ\"). We use the square root of the sum of squares to obtain a sort of weighted average—dominated by the largest component. One objective is to keep the number near 0 if the voxel is spherical but near 1 if it is elongate. This leads to the fractional anisotropy or FA which is the square root of the sum of squares (SRSS) of the diffusivity differences, divided by the SRSS of the diffusivities. When the second and third axes are small relative to the principal axis, the number in the numerator is almost equal the number in the denominator. We also multiply by formula_46 so that FA has a maximum value of 1. The whole formula for FA looks like this:\n\nThe fractional anisotropy can also be separated into linear, planar, and spherical measures depending on the \"shape\" of the diffusion ellipsoid. For example, a \"cigar\" shaped prolate ellipsoid indicates a strongly linear anisotropy, a \"flying saucer\" or oblate spheroid represents diffusion in a plane, and a sphere is indicative of isotropic diffusion, equal in all directions. If the eigenvalues of the diffusion vector are sorted such that formula_48, then the measures can be calculated as follows:\n\nFor the linear case, where formula_49,\n\nFor the planar case, where formula_51,\n\nFor the spherical case, where formula_53,\n\nEach measure lies between 0 and 1 and they sum to unity. An additional anisotropy measure can used to describe the deviation from the spherical case:\n\nThere are other metrics of anisotropy used, including the \"relative anisotropy\" (RA):\n\nand the \"volume ratio\" (VR):\n\nThe principal application is in the imaging of white matter where the location, orientation, and anisotropy of the tracts can be measured. The architecture of the axons in parallel bundles, and their myelin sheaths, facilitate the diffusion of the water molecules preferentially along their main direction. Such preferentially oriented diffusion is called anisotropic diffusion.\nThe imaging of this property is an extension of diffusion MRI. If a series of diffusion gradients (i.e. magnetic field variations in the MRI magnet) are applied that can determine at least 3 directional vectors (use of 6 different gradients is the minimum and additional gradients improve the accuracy for \"off-diagonal\" information), it is possible to calculate, for each voxel, a tensor (i.e. a symmetric positive definite 3×3 matrix) that describes the 3-dimensional shape of diffusion. The fiber direction is indicated by the tensor's main eigenvector. This vector can be color-coded, yielding a cartography of the tracts' position and direction (red for left-right, blue for superior-inferior, and green for anterior-posterior). The brightness is weighted by the fractional anisotropy which is a scalar measure of the degree of anisotropy in a given voxel. Mean diffusivity (MD) or trace is a scalar measure of the total diffusion within a voxel. These measures are commonly used clinically to localize white matter lesions that do not show up on other forms of clinical MRI.\n\nDiffusion tensor imaging data can be used to perform tractography within white matter. Fiber tracking algorithms can be used to track a fiber along its whole length (e.g. the corticospinal tract, through which the motor information transit from the motor cortex to the spinal cord and the peripheral nerves). Tractography is a useful tool for measuring deficits in white matter, such as in aging. Its estimation of fiber orientation and strength is increasingly accurate, and it has widespread potential implications in the fields of cognitive neuroscience and neurobiology.\n\nSome clinical applications of DTI are in the tract-specific localization of white matter lesions such as trauma and in defining the severity of diffuse traumatic brain injury. The localization of tumors in relation to the white matter tracts (infiltration, deflection), has been one of the most important initial applications. In surgical planning for some types of brain tumors, surgery is aided by knowing the proximity and relative position of the corticospinal tract and a tumor.\n\nThe use of DTI for the assessment of white matter in development, pathology and degeneration has been the focus of over 2,500 research publications since 2005. It promises to be very helpful in distinguishing Alzheimer's disease from other types of dementia. Applications in brain research cover e.g. connectionistic investigation of neural networks in vivo.\n\nDTI also has applications in the characterization of skeletal and cardiac muscle. The sensitivity to fiber orientation also appears to be helpful in the area of sports medicine where it greatly aids imaging of structure and injury in muscles and tendons.\n\nDiffusion MRI relies on the mathematics and physical interpretations of the geometric quantities known as tensors. Only a special case of the general mathematical notion is relevant to imaging, which is based on the concept of a symmetric matrix. Diffusion itself is tensorial, but in many cases the objective is not really about trying to study brain diffusion per se, but rather just trying to take advantage of diffusion anisotropy in white matter for the purpose of finding the orientation of the axons and the magnitude or degree of anisotropy. Tensors have a real, physical existence in a material or tissue so that they don't move when the coordinate system used to describe them is rotated. There are numerous different possible representations of a tensor (of rank 2), but among these, this discussion focuses on the ellipsoid because of its physical relevance to diffusion and because of its historical significance in the development of diffusion anisotropy imaging in MRI.\n\nThe following matrix displays the components of the diffusion tensor:\n\nThe same matrix of numbers can have a simultaneous second use to describe the shape and orientation of an ellipse and the same matrix of numbers can be used simultaneously in a third way for matrix mathematics to sort out eigenvectors and eigenvalues as explained below.\n\nThe idea of a tensor in physical science evolved from attempts to describe the quantity of physical properties. The first properties they were applied to were those that can be described by a single number, such as temperature. Properties that can be described this way are called scalars; these can be considered tensors of rank 0, or 0th-order tensors. Tensors can also be used to describe quantities that have directionality, such as mechanical force. These quantities require specification of both magnitude and direction, and are often represented with a vector. A three-dimensional vector can be described with three components: its projection on the \"x,\" \"y\", and \"z\" axes. Vectors of this sort can be considered tensors of rank 1, or 1st-order tensors.\n\nA tensor is often a physical or biophysical property that determines the relationship between two vectors. When a force is applied to an object, movement can result. If the movement is in a single direction, the transformation can be described using a vector—a tensor of rank 1. However, in a tissue, diffusion leads to movement of water molecules along trajectories that proceed along multiple directions over time, leading to a complex projection onto the Cartesian axes. This pattern is reproducible if the same conditions and forces are applied to the same tissue in the same way. If there is an internal anisotropic organization of the tissue that constrains diffusion, then this fact will be reflected in the pattern of diffusion. The relationship between the properties of driving force that generate diffusion of the water molecules and the resulting pattern of their movement in the tissue can be described by a tensor. The collection of molecular displacements of this physical property can be described with nine components—each one associated with a pair of axes \"xx\", \"yy\", \"zz\", \"xy\", \"yx\", \"xz\", \"zx\", \"yz\", \"zy\". These can be written as a matrix similar to the one at the start of this section.\n\nDiffusion from a point source in the anisotropic medium of white matter behaves in a similar fashion. The first pulse of the Stejskal Tanner diffusion gradient effectively labels some water molecules and the second pulse effectively shows their displacement due to diffusion. Each gradient direction applied measures the movement along the direction of that gradient. Six or more gradients are summed to get all the measurements needed to fill in the matrix, assuming it is symmetric above and below the diagonal (red subscripts).\n\nIn 1848, Henri Hureau de Sénarmont applied a heated point to a polished crystal surface that had been coated with wax. In some materials that had \"isotropic\" structure, a ring of melt would spread across the surface in a circle. In anisotropic crystals the spread took the form of an ellipse. In three dimensions this spread is an ellipsoid. As Adolf Fick showed in the 1850s, diffusion exhibits many of the same patterns as those seen in the transfer of heat.\n\nAt this point, it is helpful to consider the mathematics of ellipsoids. An ellipsoid can be described by the formula: \"ax\" + \"by\" + \"cz\" = 1. This equation describes a quadric surface. The relative values of \"a\", \"b\", and \"c\" determine if the quadric describes an ellipsoid or a hyperboloid.\n\nAs it turns out, three more components can be added as follows: \"ax\" + \"by\" + \"cz\" + \"dyz\" + \"ezx\" + \"fxy\" = 1. Many combinations of \"a\", \"b\", \"c\", \"d\", \"e\", and \"f\" still describe ellipsoids, but the additional components (\"d\", \"e\", \"f\") describe the rotation of the ellipsoid relative to the orthogonal axes of the Cartesian coordinate system. These six variables can be represented by a matrix similar to the tensor matrix defined at the start of this section (since diffusion is symmetric, then we only need six instead of nine components—the components below the diagonal elements of the matrix are the same as the components above the diagonal). This is what is meant when it is stated that the components of a matrix of a second order tensor can be represented by an ellipsoid—if the diffusion values of the six terms of the quadric ellipsoid are placed into the matrix, this generates an ellipsoid angled off the orthogonal grid. Its shape will be more elongated if the relative anisotropy is high.\n\nWhen the ellipsoid/tensor is represented by a matrix, we can apply a useful technique from standard matrix mathematics and linear algebra—that is to \"diagonalize\" the matrix. This has two important meanings in imaging. The idea is that there are two equivalent ellipsoids—of identical shape but with different size and orientation. The first one is the measured diffusion ellipsoid sitting at an angle determined by the axons, and the second one is perfectly aligned with the three Cartesian axes. The term \"diagonalize\" refers to the three components of the matrix along a diagonal from upper left to lower right (the components with red subscripts in the matrix at the start of this section). The variables \"ax\", \"by\", and \"cz\" are along the diagonal (red subscripts), but the variables \"d\", \"e\" and \"f\" are \"off diagonal\". It then becomes possible to do a vector processing step in which we rewrite our matrix and replace it with a new matrix multiplied by three different vectors of unit length (length=1.0). The matrix is diagonalized because the off-diagonal components are all now zero. The rotation angles required to get to this equivalent position now appear in the three vectors and can be read out as the \"x\", \"y\", and \"z\" components of each of them. Those three vectors are called \"eigenvectors\" or characteristic vectors. They contain the orientation information of the original ellipsoid. The three axes of the ellipsoid are now directly along the main orthogonal axes of the coordinate system so we can easily infer their lengths. These lengths are the eigenvalues or characteristic values.\n\nDiagonalization of a matrix is done by finding a second matrix that it can be multiplied with followed by multiplication by the inverse of the second matrix—wherein the result is a new matrix in which three diagonal (\"xx\", \"yy\", \"zz\") components have numbers in them but the off-diagonal components (\"xy\", \"yz\", \"zx\") are 0. The second matrix provides eigenvector information.\n\nEarly in the development of DTI based tractography, a number of researchers pointed out a flaw in the diffusion tensor model. The tensor analysis assumes that there is a single ellipsoid in each imaging voxel— as if all of the axons traveling through a voxel traveled in exactly the same direction. This is often true, but it can be estimated that in more than 30% of the voxels in a standard resolution brain image, there are at least two different neural tracts traveling in different directions that pass through each other. In the classic diffusion ellipsoid tensor model, the information from the crossing tract just appears as noise or unexplained decreased anisotropy in a given voxel. David Tuch was among the first to describe a solution to this problem. The idea is best understood by conceptually placing a kind of geodesic dome around each image voxel. This icosahedron provides a mathematical basis for passing a large number of evenly spaced gradient trajectories through the voxel—each coinciding with one of the apices of the icosahedron. Basically, we are now going to look into the voxel from a large number of different directions (typically 40 or more). We use \"\"n\"-tuple\" tessellations to add more evenly spaced apices to the original icosahedron (20 faces)—an idea that also had its precedents in paleomagnetism research several decades earlier. We just want to know which direction lines turn up the maximum anisotropic diffusion measures. If there is a single tract, there will be just two maxima pointing in opposite directions. If two tracts cross in the voxel, there will be two pairs of maxima, and so on. We can still use tensor math to use the maxima to select groups of gradients to package into several different tensor ellipsoids in the same voxel, or use more complex higher rank tensors analyses, or we can do a true \"model free\" analysis that just picks the maxima and goes on about doing the tractography.\n\nThe Q-Ball method of tractography is an implementation in which David Tuch provides a mathematical alternative to the tensor model. Instead of forcing the diffusion anisotropy data into a group of tensors, the mathematics used deploys both probability distributions and a classic bit of geometric tomography and vector math developed nearly 100 years ago—the Funk Radon Transform.\n\nFor DTI, it is generally possible to use linear algebra, matrix mathematics and vector mathematics to process the analysis of the tensor data.\n\nIn some cases, the full set of tensor properties is of interest, but for tractography it is usually necessary to know only the magnitude and orientation of the primary axis or vector. This primary axis—the one with the greatest length—is the largest eigenvalue and its orientation is encoded in its matched eigenvector. Only one axis is needed because the interest is in the vectorial property of axon direction to accomplish tractography.\n\n\n"}
{"id": "48719972", "url": "https://en.wikipedia.org/wiki?curid=48719972", "title": "Diversification rates", "text": "Diversification rates\n\nDiversification rates are the rates at which new species form (the Speciation rate, λ) and living species go extinct (the extinction rate, μ). Diversification rates can be estimated from fossils, data on the species diversity of clades and their ages, or phylogenetic trees. Diversification rates are typically reported on a per-lineage basis (e.g. speciation rate per lineage per unit of time), and refer to the diversification dynamics expected under a birth-death process.\n\nA broad range of studies have demonstrated that diversification rates can vary tremendously both through time and across the tree of life. Current research efforts are focused on predicting diversification rates based on aspects of species or their environment. Diversification rates are also subject to various survivorship biases such as the \"Push of the past\"\n\nDiversification rates can be estimated time-series data on fossil occurrences. With perfect data, this would be an easy task; one could just count the number of speciation and extinction events in a given time interval, and then use these data to calculate per-lineage rates of speciation and extinction per unit time. However, the incomplete nature of the fossil record means that our calculations need to include the possibility that some fossil lineages were not sampled, and that we do not have precise estimates for the times of speciation and extinction of the taxa that are sampled. More sophisticated methods account for the probability of sampling any lineage, which might also depend on some properties of the lineage itself (e.g. whether it has any hard body parts that tend to fossilize) as well as the environment in which it lives.\n\nMany estimates of diversification rates for fossil lineages are for higher-level taxonomic groups like genera or families. Such rates are informative about general patterns and trends of diversification through time and across clades but can be difficult to compare directly to rates of speciation and extinction of individual species.\n\nDiversification rates can be estimated from data on the ages and diversities of monophyletic clades in the tree of life. For example, if a clade is 100 million years old and includes 1000 species, we can estimate the net diversification rate of that clade by using a formula derived from a birth-death model of diversification:\n\nEquations are also available for estimating speciation and extinction rates separately when one has ages and diversities for multiple clades.\n\nDiversification rates can be estimated using the information available in phylogenetic trees. To calculate diversification rates, such phylogenetic trees have to include branch lengths. Various methods are available to estimate speciation and extinction rates from phylogenetic trees using both maximum likelihood and Bayesian statistical approaches. One can also use phylogenetic trees to test for changing rates of speciation and/or extinction, both through time and across clades, and to associate rates of evolution with potential explanatory factors.\n\n"}
{"id": "7705752", "url": "https://en.wikipedia.org/wiki?curid=7705752", "title": "Egalitarianism as a Revolt Against Nature and Other Essays", "text": "Egalitarianism as a Revolt Against Nature and Other Essays\n\nEgalitarianism as a Revolt Against Nature and Other Essays is a 1974 book by economist Murray Rothbard. The book represents the author's theorizing on topics impacting human liberty. Rothbard looks beyond conventional left-right thinking and hence contributes to the groundwork for the current intellectual challenge against centralized social and economic management.\n\nThe book's title comes from the lead essay, which argues that egalitarian theory always results in a politics of statist control because it is founded on revolt against the ontological structure of reality itself. According to Rothbard in this lead essay, statist intellectuals attempt to replace what exists with a Romantic image of an idealized primitive state of nature, an ideal which cannot and should not be achieved, according to Rothbard. The implications of this point are worked out on topics such as market economics, child rights, environmentalism, feminism, foreign policy, redistribution and others.\n\nRoy Childs writes in the Foreword:\nFor until Rothbard's work is carefully studied by every advocate of liberty, the value of his contributions to the libertarian system cannot be fully appreciated and, moreover, the unity and true historical context of libertarianism will not even be fully grasped.\n\n\n"}
{"id": "51053611", "url": "https://en.wikipedia.org/wiki?curid=51053611", "title": "El Greco fallacy", "text": "El Greco fallacy\n\nThe El Greco fallacy is typically a perceptive fallacy, where it is assumed that particular perceptual abnormalities will influence interactions with the world of a similar nature. It is named after an erroneous explanation for the vertically distorted painting style of El Greco, which held that the artist must have seen the world as distorted by a peculiar astigmatism, and thus painted this distorted world. This theoretical astigmatism cannot explain El Greco's style though, as he would have seen his canvases distorted in the same way, and painting onto them would have cancelled out any distortion.\n\nIt is believed the term originated with Irvin Rock, in his 1966 book, \"The Nature of Perceptual Adaptation\".\n\nWhen explored in experiment — by having subjects wear distorting lenses — it seems likely that El Greco would have completely adapted to seeing a distorted world, and this could not have been an explanation.\n\nChaz Firestone and Brian Scholl have alleged that this fallacy has been the cause for mistaken thinking in perception research, including in studies that presumed to show that wearing a heavy backpack makes hills literally appear steeper, or holding rods outstretched horizontally would make doorways look narrower.\n\nIn the example of holding rods outstretched, the original experiment had subjects evaluate the width of apertures (intended to simulate doorways), then show this width on a ruler held by experimenters. The width was evaluated as narrower by those holding rods than those who were not. When Firestone and Scholl repeated this experiment, substituting rulers for a separate aperture that was adjusted by experimenters to the subjects instruction, the aperture was still evaluated as narrower. This narrowing effect was later implied to be a demand characteristic after another group was told that their holding a rod was an evaluation of balance abilities.\n"}
{"id": "368328", "url": "https://en.wikipedia.org/wiki?curid=368328", "title": "Electrostatics", "text": "Electrostatics\n\nElectrostatics is a branch of physics that studies electric charges at rest.\n\nSince classical physics, it has been known that some materials such as amber attract lightweight particles after rubbing. The Greek word for amber, , or \"\", was the source of the word 'electricity'. Electrostatic phenomena arise from the forces that electric charges exert on each other. Such forces are described by Coulomb's law.\nEven though electrostatically induced forces seem to be rather weak, some electrostatic forces such as the one between an electron and a proton, that together make up a hydrogen atom, is about 36 orders of magnitude stronger than the gravitational force acting between them.\n\nThere are many examples of electrostatic phenomena, from those as simple as the attraction of the plastic wrap to one's hand after it is removed from a package to the apparently spontaneous explosion of grain silos, the damage of electronic components during manufacturing, and photocopier & laser printer operation. Electrostatics involves the buildup of charge on the surface of objects due to contact with other surfaces. Although charge exchange happens whenever any two surfaces contact and separate, the effects of charge exchange are usually only noticed when at least one of the surfaces has a high resistance to electrical flow. This is because the charges that transfer are trapped there for a time long enough for their effects to be observed. These charges then remain on the object until they either bleed off to ground or are quickly neutralized by a discharge: e.g., the familiar phenomenon of a static 'shock' is caused by the neutralization of charge built up in the body from contact with insulated surfaces.\n\nCoulomb's law states that:\n\n'The magnitude of the electrostatic force of attraction or repulsion between two point charges is directly proportional to the product of the magnitudes of charges and inversely proportional to the square of the distance between them.'\n\nThe force is along the straight line joining them. If the two charges have the same sign, the electrostatic force between them is repulsive; if they have different signs, the force between them is attractive.\n\nIf formula_1 is the distance (in meters) between two charges, then the force (in newtons) between two point charges formula_2 and formula_3 (in coulombs) is:\n\nwhere ε is the vacuum permittivity, or permittivity of free space:\n\nThe SI units of ε are equivalently  As kgm or CNm or F m. Coulomb's constant is:\n\nThe use of ε instead of k in expressing Coulomb's Law is related to the fact that the force is inversely proportional to the surface area of a sphere with radius equal to the separation between the two charges.\n\nA single proton has a charge of \"e\", and the electron has a charge of −\"e\", where,\n\nThese physical constants (ε, k, e) are currently defined so that ε and k are exactly defined, and \"e\" is a measured quantity.\n\nThe electric field, formula_8, in units of newtons per coulomb or volts per meter, is a vector field that can be defined everywhere, except at the location of point charges (where it diverges to infinity). It is defined as the electrostatic force formula_9 in newtons on a hypothetical small test charge at the point due to Coulomb's Law, divided by the magnitude of the charge formula_10 in coulombs\n\nElectric field lines are useful for visualizing the electric field. Field lines begin on positive charge and terminate on negative charge. They are parallel to the direction of the electric field at each point, and the density of these field lines is a measure of the magnitude of the electric field at any given point.\n\nConsider a collection of formula_12 particles of charge formula_13, located at points formula_14 (called \"source points\"), the electric field at formula_15 (called the \"field point\") is:\n\nwhere formula_17 is the displacement vector from a \"source point\" \nformula_14 to the \"field point\" \nformula_15, and \nformula_20 \nis a unit vector that indicates the direction of the field. For a single point charge at the origin, the magnitude of this electric field is formula_21 and points away from that charge is positive. The fact that the force (and hence the field) can be calculated by summing over all the contributions due to individual source particles is an example of the superposition principle. The electric field produced by a distribution of charges is given by the volume charge density formula_22 and can be obtained by converting this sum into a triple integral:\n\nGauss' law states that \"the total electric flux through any closed surface in free space of any shape drawn in an electric field is proportional to the total electric charge enclosed by the surface.\" Mathematically, Gauss's law takes the form of an integral equation:\n\nwhere formula_25 is a volume element. If the charge is distributed over a surface or along a line, replace formula_26 by formula_27 or formula_28. The Divergence Theorem allows Gauss's Law to be written in differential form:\n\nwhere formula_30 is the divergence operator.\n\nThe definition of electrostatic potential, combined with the differential form of Gauss's law (above), provides a relationship between the potential Φ and the charge density ρ:\n\nThis relationship is a form of Poisson's equation. In the absence of unpaired electric charge, the equation becomes Laplace's equation:\n\nThe validity of the electrostatic approximation rests on the assumption that the electric field is irrotational:\n\nFrom Faraday's law, this assumption implies the absence or near-absence of time-varying magnetic fields:\n\nIn other words, electrostatics does not require the absence of magnetic fields or electric currents. Rather, if magnetic fields or electric currents \"do\" exist, they must not change with time, or in the worst-case, they must change with time only \"very slowly\". In some problems, both electrostatics and magnetostatics may be required for accurate predictions, but the coupling between the two can still be ignored. Electrostatics and magnetostatics can both be seen as Galilean limits for electromagnetism.\n\nBecause the electric field is irrotational, it is possible to express the electric field as the gradient of a scalar function,formula_35, called the electrostatic potential (also known as the voltage). An electric field, formula_36, points from regions of high electric potential to regions of low electric potential, expressed mathematically as\n\nThe gradient theorem can be used to establish that the electrostatic potential is the amount of work per unit charge required to move a charge from point formula_38 to point formula_39 with the following line integral:\n\nFrom these equations, we see that the electric potential is constant in any region for which the electric field vanishes (such as occurs inside a conducting object).\n\nA single test particle's potential energy, formula_41, can be calculated from a line integral of the work, formula_42. We integrate from a point at infinity, and assume a collection of formula_12 particles of charge formula_44, are already situated at the points formula_14. This potential energy (in Joules) is:\n\nwhere formula_47 is the distance of each charge formula_13 from the test charge formula_2, which situated at the point formula_15, and formula_51 is the electric potential that would be at formula_15 if the test charge were not present. If only two charges are present, the potential energy is formula_53. The total electric potential energy due a collection of \"N\" charges is calculating by assembling these particles one at a time:\n\nwhere the following sum from, \"j = 1\" to \"N\", excludes \"i = j\":\n\nThis electric potential, formula_56 is what would be measured at formula_14 if the charge formula_13 were missing. This formula obviously excludes the (infinite) energy that would be required to assemble each point charge from a disperse cloud of charge. The sum over charges can be converted into an integral over charge density using the prescription formula_59:\n\nThis second expression for electrostatic energy uses the fact that the electric field is the negative gradient of the electric potential, as well as vector calculus identities in a way that resembles integration by parts. These two integrals for electric field energy seem to indicate two mutually exclusive formulas for electrostatic energy density, namely formula_61 and formula_62; they yield equal values for the total electrostatic energy only if both are integrated over all space.\n\nOn a conductor, a surface charge will experience a force in the presence of an electric field. This force is the average of the discontinuous electric field at the surface charge. This average in terms of the field just outside the surface amounts to:\n\nThis pressure tends to draw the conductor into the field, regardless of the sign of the surface charge.\n\nThe triboelectric effect is a type of contact electrification in which certain materials become electrically charged when they are brought into contact with a different material and then separated. One of the materials acquires a positive charge, and the other acquires an equal negative charge. The polarity and strength of the charges produced differ according to the materials, surface roughness, temperature, strain, and other properties. Amber, for example, can acquire an electric charge by friction with a material like wool. This property, first recorded by Thales of Miletus, was the first electrical phenomenon investigated by humans. Other examples of materials that can acquire a significant charge when rubbed together include glass rubbed with silk, and hard rubber rubbed with fur.\n\nThe presence of surface charge imbalance means that the objects will exhibit attractive or repulsive forces. This surface charge imbalance, which yields static electricity, can be generated by touching two differing surfaces together and then separating them due to the phenomena of contact electrification and the triboelectric effect. Rubbing two nonconductive objects generates a great amount of static electricity. This is not just the result of friction; two nonconductive surfaces can become charged by just being placed one on top of the other. Since most surfaces have a rough texture, it takes longer to achieve charging through contact than through rubbing. Rubbing objects together increases amount of adhesive contact between the two surfaces. Usually insulators, e.g., substances that do not conduct electricity, are good at both generating, and holding, a surface charge. Some examples of these substances are rubber, plastic, glass, and pith. Conductive objects only rarely generate charge imbalance except, for example, when a metal surface is impacted by solid or liquid nonconductors. The charge that is transferred during contact electrification is stored on the surface of each object. Static electric generators, devices which produce very high voltage at very low current and used for classroom physics demonstrations, rely on this effect.\n\nNote that the presence of electric current does not detract from the electrostatic forces nor from the sparking, from the corona discharge, or other phenomena. Both phenomena can exist simultaneously in the same system.\n\nNatural electrostatic phenomena are most familiar as an occasional annoyance in seasons of low humidity, but can be destructive and harmful in some situations (e.g. electronics manufacturing). When working in direct contact with integrated circuit electronics (especially delicate MOSFETs), or in the presence of flammable gas, care must be taken to avoid accumulating and suddenly discharging a static charge (see electrostatic discharge).\n\nElectrostatic induction, discovered by British scientist John Canton in 1753 and Swedish professor Johan Carl Wilcke in 1762 is a redistribution of charges in an object caused by the electric field of a nearby charge. For example, if a positively charged object is brought near an uncharged metal object, the mobile negatively-charged electrons in the metal will be attracted the external charge, and move to the side of the metal facing it, creating a negative charge on the surface. When the electrons move out of an area they leave a positive charge due to the metal atoms' nuclei, so the side of the metal object facing away from the charge acquires a positive charge. These \"induced charges\" disappear when the external charge is removed. Induction is also responsible for the attraction of light objects, such as balloons, paper scraps and styrofoam packing peanuts to static charges. The surface charges induced in conductive objects exactly cancel external electric fields inside the conductor, so there is no electric field inside a metal object. This is the basis for the electric field shielding action of a Faraday cage. Since the electric field is the gradient of the voltage, electrostatic induction is also responsible for making the electric potential (voltage) constant throughout a conductive object.\n\nBefore the year 1832, when Michael Faraday published the results of his experiment on the identity of electricities, physicists thought \"static electricity\" was somehow different from other electrical charges. Michael Faraday proved that the electricity induced from the magnet, voltaic electricity produced by a battery, and static electricity are all the same.\n\nStatic electricity is usually caused when certain materials are rubbed against each other, like wool on plastic or the soles of shoes on carpet. The process causes electrons to be pulled from the surface of one material and relocated on the surface of the other material.\n\nA static shock occurs when the surface of the second material, negatively charged with electrons, touches a positively charged conductor, or vice versa.\n\nStatic electricity is commonly used in xerography, air filters, and some automotive paints.\nStatic electricity is a buildup of electric charges on two objects that have become separated from each other.\nSmall electrical components can easily be damaged by static electricity. Component manufacturers use a number of antistatic devices to avoid this.\n\nWhen different materials are brought together and then separated, an accumulation of electric charge can occur which leaves one material positively charged while the other becomes negatively charged. The mild shock that you receive when touching a grounded object after walking on carpet is an example of excess electrical charge accumulating in your body from frictional charging between your shoes and the carpet. The resulting charge build-up upon your body can generate a strong electrical discharge. Although experimenting with static electricity may be fun, similar sparks create severe hazards in those industries dealing with flammable substances, where a small electrical spark may ignite explosive mixtures with devastating consequences.\n\nA similar charging mechanism can occur within low conductivity fluids flowing through pipelines—a process called flow electrification. Fluids which have low electrical conductivity (below 50 picosiemens per meter), are called accumulators. Fluids having conductivities above 50 pS/m are called non-accumulators. In non-accumulators, charges recombine as fast as they are separated and hence electrostatic charge generation is not significant. In the petrochemical industry, 50 pS/m is the recommended minimum value of electrical conductivity for adequate removal of charge from a fluid.\n\nAn important concept for insulating fluids is the static relaxation time. This is similar to the time constant (tau) within an RC circuit. For insulating materials, it is the ratio of the static dielectric constant divided by the electrical conductivity of the material. For hydrocarbon fluids, this is sometimes approximated by dividing the number 18 by the electrical conductivity of the fluid. Thus a fluid that has an electrical conductivity of 1 pS/cm (100 pS/m) will have an estimated relaxation time of about 18 seconds. The excess charge within a fluid will be almost completely dissipated after 4 to 5 times the relaxation time, or 90 seconds for the fluid in the above example.\n\nCharge generation increases at higher fluid velocities and larger pipe diameters, becoming quite significant in pipes or larger. Static charge generation in these systems is best controlled by limiting fluid velocity. The British standard BS PD CLC/TR 50404:2003 (formerly BS-5958-Part 2) Code of Practice for Control of Undesirable Static Electricity prescribes velocity limits. Because of its large impact on dielectric constant, the recommended velocity for hydrocarbon fluids containing water should be limited to 1 m/s.\n\nBonding and earthing are the usual ways by which charge buildup can be prevented. For fluids with electrical conductivity below 10 pS/m, bonding and earthing are not adequate for charge dissipation, and anti-static additives may be required.\n\n1.BS PD CLC/TR 50404:2003 Code of Practice for Control of Undesirable Static Electricity\n\n2.NFPA 77 (2007) Recommended Practice on Static Electricity\n\n3.API RP 2003 (1998) Protection Against Ignitions Arising Out of Static, Lightning, and Stray Currents\n\nElectrostatic induction was used in the past to build high-voltage generators known as Influence machines.\nThe main component that emerged in these times is the capacitor. \nElectrostatic induction is also used for electro-mechanic precipitation or projection.In such technologies, charged particles of small sizes are collected or deposited intentionally on surfaces. Applications range from Electrostatic precipitator to Spray painting or Inkjet printing.\nRecently a new Wireless power Transfer Technology has been based on electrostatic induction between oscillating distant dipoles.\n\n\n\n\n"}
{"id": "24106558", "url": "https://en.wikipedia.org/wiki?curid=24106558", "title": "Endurance running hypothesis", "text": "Endurance running hypothesis\n\nThe endurance running hypothesis is the hypothesis that the evolution of certain human characteristics can be explained as adaptations to long distance running. The hypothesis suggests that endurance running played an important role for early hominins in obtaining food. Researchers have proposed that endurance running began as an adaptation for scavenging and later for persistence hunting.\n\nMuch research has been geared towards the mechanics of how bipedal walking has evolved in the genus \"Homo\". However, little research has been conducted to examine how the specific adaptations for running emerged, and how they influenced human evolution.\n\nThe bit of research that has focused on human running provides much evidence for bodily function and structures that improve running only, and are not used in walking. This suggests that running was an adaptation, not that it came about as a byproduct of walking.\n\nRunning and walking incorporated different biomechanisms. Walking requires an \"inverted pendulum\" where the body's center of mass is shifted over the extended leg, to exchange potential and kinetic energy with each step. Running involves a \"mass spring\" mechanism to exchange potential and kinetic energy, with the use of tendons and ligaments. Tendons and ligaments are elastic tissues that store energy. They are stretched and then release energy as they recoil. This mass spring mechanism becomes less energetically costly at faster speeds and is therefore more efficient than the inverted pendulum of walking mechanics when traveling at greater speeds. Tendons and ligaments, however, do not provide these benefits in walking.\n\nAlthough the mass spring mechanism can be more energetically favorable at higher speeds, it also results in an increase in ground reaction forces and is less stable because there is more movement and pitching of the limbs and core of the body. Ground forces and body pitching movement is less of an issue in the walking gait, where the position of the body's center of mass varies less, making walking an inherently more stable gait. In response to the destabilization of the running gait, the human body appears to have evolved adaptations to increase stabilization, as well as for the mass-spring mechanism in general. These adaptations, described below, are all evidence for selection for endurance running.\n\nMany researchers compare the skeletal structures of early hominins such as \"Australopithecus\" to those of \"Homo\" in order to identify structural differences that may be significant to endurance running.\n\nNuchal ligament: Because the head is decoupled from the shoulders, early \"Homo\" needed a way to stabilize the head. The nuchal ligament is an important evolved feature in head stabilization. It starts at the midline of the occiput and connects to the upper trapezius. This ligament is also important in terms of archaeological findings, because it leaves a small indentation and ridge in the skull, allowing researchers to see if various species had a nuchal ligament. The ability to see traces of ligaments in archaeological findings is rare because they degrade quickly and often leave no trace. In the case of the nuchal ligament, a trace of its existence is left with the presence of the skull ridge. Because neither \"Australopithecus\" nor \"Pan\" had the skull ridge, it has been concluded that this feature is unique to \"Homo\". Because the nuchal ligament is only activated while running, the amount of running can be inferred from the muscle markings. In the case of Homo Erectus and Neanderthals, very strong nuchal ligament markings are present, but are less marked in modern humans, indicating a decrease in running behavior \n\nShoulder and head stabilization: The human skeleton is different from early hominins as there is less of a connection between the pectoral girdle parts of the shoulders and upper back and head, which would be advantageous for climbing but would hinder the movements of the upper body needed to counter leg movement and therefore stabilize the body and head when running. This stabilization is unnecessary in walking.\n\nLimb length and mass: \"Homo\" has longer legs relative to body mass, which helps to decrease the energetic costs of running, as time in contact with the ground increases. There is also a decrease in mass of distal parts of limbs of humans, which is known to decrease metabolic costs in endurance running, but has little effect on walking. Additionally, the mass of the upper body limbs in \"Homo\" has decreased considerably, relative to total body mass, which is important to reduce the effort of stabilizing the arms in running.\n\nJoint surface: Humans have evolved to absorb great shock and force on the skeletal structure while running. The impact force on the body can reach up to 3-4 times body weight in endurance running, putting the skeletal structure under great stress. To reduce this stress humans have increased joint surfaces relative to body mass to spread force over larger surface areas, particularly in the lower body. This adaptation, which allows humans to absorb great shock and force applied to the skeleton, is not seen in australopithecine skeletal structures.\n\nPlantar arch: The plantar arch in the human foot has an elastic spring function that generates energy for running but not walking. Fossils of the australopithecine foot show only partial arch, suggesting less of a spring capacity. For the plantar arch spring mechanism to function fully, there must also be restricted rotation in the hind and front parts of the foot. This restriction comes from projected toe bone and compacted mid-foot joint structures in humans, which does not become present until \"Homo habilis\".\n\nCalcaneal tuber and Achilles tendon: Studies have explored the calcaneal tuber, the posterior half of the calcaneus bone, as a correlate for Achilles tendon length and have found correlation between calcaneal tuber length and Achilles tendon length. Because shorter calcaneal tuber length leads to greater Achilles stretch, more kinetic energy is converted to elastic energy, translating into better overall running economy. Comparisons between Neanderthals and modern humans reveal that this adaptation was absent in Neanderthals, leading researchers to conclude that endurance running capabilities may have been enhanced in anatomically modern humans.\nShorter toes: Human toes are straight and extremely short in relation to body size compared to other animals. In running, the toes support 50 to 75% of body mass in humans. Impulse and mechanical work increase in humans as toe length increases, showing that it is energetically favorable to have shorter toes. The costs of shorter toes are decreased gripping capabilities and power output. However, the efficiency benefits seem to outweigh these costs, as the toes of \"A. afarensis\" remains were shorter than great apes, but 40% longer than modern humans, meaning that there is a trend toward shorter toes as the primate species moves away from tree-dwelling. This 40% increase in toe length would theoretically induce a flexor impulse 2.5 times that of modern humans, which would require twice as much mechanical work to stabilize.\n\nSemicircular canal: The semicircular canal, a series of three interconnected tubes within each ear, is important for sensing angular rotations of the head and thus plays a crucial role in maintaining balance and sensing and coordinating movement. Comparative studies have shown that animals with larger semicircular canals are able to sense a greater range of head movements and therefore have greater speed and agility. Evolutionarily, greatly reduced semicircular canal diameters are evident in Neanderthals but expanded in modern humans, suggesting that this adaptation was selected for in response to increased endurance running.\n\nVestibulo-ocular reflexes (VORs): VORs are enabled by muscles in the eye, which sense angular accelerations of the head and adjust eye movements to stabilize these images. This was an important adaptation for running because it allowed \"Homo\" to see more clearly during the rough pitching motion that occurs during running.\n\nGluteals: The gluteus maximus in \"Homo erectus\" is significantly larger than that of \"Australopithecus\". It is suited to stabilize the trunk while running, but gluteals of that size and strength are not necessary for walking.\n\nIliac spine: \"Homo\" has expanded areas on the sacrum and posterior iliac spine for greater muscle attachment. These areas are used to stabilize the trunk and reduce the body's forward pitch caused by running strides.\n\nIn addition to advances in skeletal structure and stabilization, adaptations that led to increased efficiency in dissipation of heat were instrumental in the evolution of endurance running in \"Homo\". The duration for which an animal can run is determined by its capacity to release more heat than is produced to avoid lethal temperatures.\n\nThe majority of mammals, including humans, rely on evaporative cooling to maintain body temperature. Most medium-to-large mammals rely on panting, while humans rely on sweating, to dissipate heat. Advantages to panting include cooler skin surface, little salt loss, and heat loss by forced convection instead of reliance on wind or other means of convection. On the other hand, sweating is advantageous in that evaporation occurs over a much larger surface area (the skin), and it is independent of respiration, thus is a much more flexible mode of cooling during intense activity such as running. Because human sweat glands are under a higher level of neuronal control than those of other species, they allow for the excretion of more sweat per unit surface area than any other species. Heat dissipation of later hominins was also enhanced by the reduction in body hair. By ridding themselves of an insulating fur coat, running humans are better able to dissipate the heat generated by exercise.\n\nIn addition to improved thermoregulation, hominins have evolved an enhanced method of respiration consistent with the demands of running. Due to their orientation, respiration in quadrupedal mammals is affected by skeletal and muscular stresses generated through the motion of running. The bones and muscles of the chest cavity are not only responsible for shock absorption, but are also subjected to continuous compression and expansion during the running cycle. Because of this movement, quadrupeds are restricted to one breath per locomotor cycle, and thus must coordinate their running gait and respiration rate. This tight coordination then translates into another restriction: a specific running speed that is most energetically favorable. The upright orientation of bipedal hominins, however, frees them from this respiration-gait restriction. Because their chest cavities are not directly compressed or involved in the motion of running, hominins are able to vary their breathing patterns with gait. This flexibility in respiration rate and running gait contributes to hominins having a broader range of energetically favorable running speeds.\n\nDuring periods of prolonged exercise, animals are dependent on a combination of two sources of fuel: glycogen stored in the muscles and liver, and fat. Because glycogen is more easily oxidized than fat, it is depleted first. However, over longer periods of time, energy demands require that fat stores be utilized as fuel. This is true for all mammals, but hominins, and later modern humans, have an advantage of being able to alter their diet to meet these prolonged energy demands.\n\nIn addition to flexibility in the utilization of energy, hominins have evolved larger thyroid and adrenal glands which enable them to utilize the energy in carbohydrates and fatty acids more readily and efficiently. These organs are responsible for releasing hormones including epinephrine, norepinephrine, adrenocorticotropic hormone (ACTH), glucagon, and thyroxine. Larger glands allows for greater production of these key hormones and ultimately, maximized utilization of stored fuel.\n\nTaken together, the flexibility in diet and the enhanced usage of fuel heightens the previously mentioned finding that, unlike quadrupeds, hominins do not have a single energetically optimal running speed. For quadrupeds, increasing running speed means increasing the demand for oxygen and fuel. Due to skeletal structure and bipedalism, hominins are free to run energetically over a broader range of speeds and gaits, while maintaining a constant energy consumption rate of approximately 4.1 MJ per 15 km. Thus their utilization of energy is greatly enhanced.\n\nAll of the aforementioned adaptations enabled Homo to scavenge for food more effectively. Endurance running could have been used as a means of gaining access to distant carcasses or food stores faster than other scavengers and/or carnivores. Scavenging may have taken one or both of two forms: Opportunistic scavenging and strategic scavenging.\n\nEarly Homo almost certainly scavenged opportunistically. Scavenging is considered opportunistic when one \"come[s] across carcasses in the course of [their] daily foraging activities\".\n\nStrategic scavenging involves a planned search for carcasses. This style of scavenging would have benefitted from endurance running much more than opportunistic scavenging. Strategic scavenging would have involved the use of long range cues, such as birds circling overhead. Endurance running would have been advantageous in this setting because it allowed hominins to reach the carcass more quickly. Selection pressures would have been very high for strategic scavenging, because hominins were diurnal, while their major competitors (hyenas, lions, etc.) were not. Thus, they would have had to make sure to capitalize on daytime carcasses. Selection pressure also came from the weakness of Homo. Because they were very weak, they were unlikely to drive off any large competition at the carcass. This fact led to an even higher need for a way to reach the carcass before these competitors.\n\nPersistence hunting is \"a form of pursuit hunting in which [the hunter uses] endurance running during the midday heat to drive [prey] into hyperthermia and exhaustion so they can easily be killed\". Many question persistence hunting's plausibility when bow and arrow and other technologies were so much more efficient. However, in the Early Stone Age (ESA), spears were only sharpened wood, and hominins had not begun using tools. The lack of spearheads or bows meant they could only hunt from very close range—between 6 and 10 meters. Hominins thus must have developed a way to stab prey from close range without causing serious bodily harm to themselves. Persistence hunting makes killing an animal easier by first bringing it to exhaustion, so that it can no longer retaliate violently.\n\nPersistence hunters work by hunting in the middle of the day, when it is hottest. Hunters choose a single target prey and chase it at a speed between its trot and gallop, which is extremely inefficient for the animal. The hunter then continues pursuing over a period of hours, during which he may lose sight of the animal. In this case, the hunter must use tracks and an understanding of the animal to continue the chase. The prey eventually overheats and becomes unable to continue fleeing. Homo, which does not overheat as quickly because of its superior thermoregulation capabilities, is then able to stab the prey while it is incapacitated and cannot attack.\n\nDue to the complexity of following a fleeing animal, tracking methods must have been a prerequisite for the use of endurance running in persistence hunting. Scientists posit that early tracking methods were developed in open, sparsely vegetated terrain such as the Kalahari Desert in southern Africa. This \"systemic tracking\" involves simply following the footprints of animals and was most likely used for tracking grassland species on soft terrain. Skeletal remains suggest that during the Middle Stone Age, hominins used systemic tracking to scavenge for medium-sized animals in vegetation cover, but for hunting antelope in more open grasslands. From the Middle Stone Age into the Later Stone Age, tracking methods developed into what is termed \"speculative tracking\". When tracks could not easily be found and followed, \"Homo\" predicted where tracks were most likely to be found and interpreted other signs to locate prey. This advanced method of tracking allowed for the exploitation of prey in a variety of terrains, making endurance running for persistence hunting more plausible.\n\nAlthough exact dates and methods of persistence hunting are difficult to study, several recent accounts of persistence hunting have been recorded. Tribes in the Kalahari Desert in Botswana have been known to employ endurance running to scavenge and hunt prey. In the open country, the !Xo and /Gwi tribes run down slow-moving animals such as aardvark and porcupines, while during the hotter part of the day, they target animals such as eland, kudu, gemsbok, hartebeest, duiker, steenbok, cheetah, caracal, and African wildcats. In addition to these existing African tribes, it has been suggested that the Tarahumara people in Mexico and the Paiute people and Navajo in the American Southwest, used persistence hunting to capture prey including deer and pronghorn. The Aborigines in Australia are known to have hunted kangaroo in similar ways. Due to the increased availability of weapons, nutrition, tracking devices, and motor vehicles, one may argue that persistence hunting is no longer an effective method of hunting animals for food. However, there are examples of the practice occurring in modern times: the !Xo and /Gwi in the central Kalahari, still practice persistence hunting and have developed advanced methods of doing so. Similarly, the Russian Lykov family that lived in isolation for 40 years also used persistence hunting due to a lack of weapons.\n\nIn particular, these two tribes maximize the efficiency of persistence hunting by targeting specific species during different seasons. In the rainy season, prime targets include steenbok, duiker, and gemsbok, as wet sand opens their hooves and stiffens their joints. Hunting in the early rainy season is particularly advantageous because dry leaves form \"rocks\" in the animals' stomachs, resulting in diarrhea. Stiff joints and suboptimal digestion make the prey weaker and more available targets. In contrast, in the dry season, hunters run down kudu, eland, and red hartebeest because these species tire more easily in the loose sand. Hunters say that the best time to practice persistence hunting is near the end of the dry season when animals are poorly nourished and therefore more easily run to exhaustion. By targeting the most vulnerable prey during each season, the hunters maximize the advantages of endurance running.\n\n\nIn the oral traditions of the Hadza, an isolated aboriginal people of hunter-gatherers living in Tanzania, the \"Tlaatlanebe\" in their folk history's second epoch practiced this.\n\nIn the first epoch, the world was inhabited by large hairy humanoids called \"Akakaanebe\" (\"ancestors\"), who did not yet possess tools or fire. They simply \"stared\" at game until it fell dead, referring to either scavenging or early persistence hunting without weapons, or a combination of the two. They did not build houses but slept under trees.\n\nThe Tlaatlanebe of the second epoch, however, were large but without hair and lived in caves. As animals had grown more wary of humans due to earlier hunting, they now had to be chased and hunted with dogs.\n\nWhile there is evidence supporting selection on human morphology to improve endurance running ability, there is some dispute over whether the ecological benefits of scavenging and persistence hunting foraging behaviors were the driving force behind this development.\n\nThe majority of the arguments opposing persistence hunting and scavenging behaviors are linked to the fact that the paleohabitat and paleoecology of early \"Homo\" were not conducive to these behaviors. It is thought that the earliest members of \"Homo\" lived in African savanna-woodlands. This environment consisted of open grassland, as well as parts with dense vegetation—an intermediate between forest and open savannas. The presence of such tree covering would reduce visibility and so require tracking skills. This causes problems for the hypothesis of persistence hunting and running to aid scavenging.\n\nUngulates are known from archaeological evidence to have been the main prey of the early \"Homo\", and given their great speed, they would have easily been able to outrun early hominins. Ungulate speed, coupled with the variable visibility of the savanna-woodland, meant that hunting by endurance running required the ability to track prey. Pickering and Bunn argue that tracking is part of a sophisticated cognitive skill set that early hominins would not have had, and that even if they were following a trail of blood left by an injured ungulate—which may have been in their cognitive capacity—the ability to craft penetrating projectile technology was absent in early hominins.\n\nIt has been suggested that modern hunters in Africa do not use persistence hunting as a foraging method, and most often give up a chase where the trail they were following ends in vegetation. The rare groups of hunters who do occasionally participate in persistence hunting are able to do so because of the extremely hot and open environments. In these groups, a full day of rest and recovery is required after a hunt, indicating the great toll persistence hunts take on the body, making them rare undertakings.\n\nFinally, in critique of Liebenberg's research on modern day persistence hunting, it was revealed that the majority of the hunts initiated were prompted for filming rather than spontaneous, and that few of these hunts were successful. The hunts that were successful involved external factors such as the hunters being able to stop and refill water bottles.\n\nA response to these criticisms has been formulated by Lieberman et al., noting that it is unclear how humans could have grown to occupy a new niche as a diurnal social carnivore without persistence hunting, as the weapons preferred in modern hunter-gatherer tribes would not have been available at the time.\n\nThe proposed benefit of endurance running in scavenging is the ability of early hominins to outcompete other scavengers in reaching food sources. However paleoanthropological studies suggest that the savanna-woodland habitat caused a very low competition environment. Due to low visibility, carcasses were not easily located by mammalian carnivores, resulting in less competition.\n\n"}
{"id": "43415449", "url": "https://en.wikipedia.org/wiki?curid=43415449", "title": "Equivalent definitions of mathematical structures", "text": "Equivalent definitions of mathematical structures\n\nIn mathematics, equivalent definitions are used in two somewhat different ways. First, within a particular mathematical theory (for example, Euclidean geometry), a notion (for example, ellipse or minimal surface) may have more than one definition. These definitions are equivalent in the context of a given mathematical structure (Euclidean space, in this case). Second, a mathematical structure may have more than one definition (for example, topological space has at least seven definitions; ordered field has at least two definitions).\n\nIn the former case, equivalence of two definitions means that a mathematical object (for example, geometric body) satisfies one definition if and only if it satisfies the other definition.\n\nIn the latter case, the meaning of equivalence (between two definitions of a structure) is more complicated, since a structure is more abstract than an object. Many different objects may implement the same structure.\n\nNatural numbers may be implemented as 0 = , 1 = = , 2 = = , 3 = = and so on; or alternatively as 0 = , 1 = =, 2 = = and so on. These are two different but isomorphic implementations of natural numbers in set theory.\nThey are isomorphic as models of Peano axioms, that is, triples (\"N\",0,\"S\") where \"N\" is a set, 0 an element of \"N\", and \"S\" (called the successor function) a map of \"N\" to itself (satisfying appropriate conditions). In the first implementation \"S\"(\"n\") = \"n\" ∪ ; in the second implementation \"S\"(\"n\") = . As emphasized in Benacerraf's identification problem, the two implementations differ in their answer to the question whether 0 ∈ 2; however, this is not a legitimate question about natural numbers (since the relation ∈ is not stipulated by the relevant signature(s), see the next section). Similarly, different but isomorphic implementations are used for complex numbers.\n\nThe successor function \"S\" on natural numbers leads to arithmetic operations, addition and multiplication, and the total order, thus endowing \"N\" with an ordered semiring structure. This is an example of a deduced structure. The ordered semiring structure (\"N\", +, ·, ≤) is deduced from the Peano structure (\"N\", 0, \"S\") by the following procedure:\n\"n\" + 0 = \"n\",   \"m\" + S (\"n\") = S (\"m\" + \"n\"),   \"m\" · 0 = 0,   \"m\" · S (\"n\") = \"m\" + (\"m\" · \"n\"), and \"m\" ≤ \"n\" if and only if there exists \"k\" ∈ \"N\" such that \"m\" + \"k\" = \"n\". And conversely, the Peano structure is deduced from the ordered semiring structure as follows: \"S\" (\"n\") = \"n\" + 1, and 0 is defined by 0 + 0 = 0. It means that the two structures on \"N\" are equivalent by means of the two procedures.\n\nThe two isomorphic implementations of natural numbers, mentioned in the previous section, are isomorphic as triples (\"N\",0,\"S\"), that is, structures of the same signature (0,\"S\") consisting of a constant symbol 0 and a unary function \"S\". An ordered semiring structure (\"N\", +, ·, ≤) has another signature (+, ·, ≤) consisting of two binary functions and one binary relation. The notion of isomorphism does not apply to structures of different signatures. In particular, a Peano structure cannot be isomorphic to an ordered semiring. However, an ordered semiring deduced from a Peano structure may be isomorphic to another ordered semiring. Such relation between structures of different signatures is sometimes called a cryptomorphism.\n\nA structure may be implemented within a set theory ZFC, or another set theory such as NBG, NFU, ETCS. Alternatively, a structure may be treated in the framework of first-order logic, second-order logic, higher-order logic, a type theory, homotopy type theory etc.\n\nAccording to Bourbaki, the scale of sets on a given set \"X\" consists of all sets arising from \"X\" by taking Cartesian products and power sets, in any combination, a finite number of times. Examples: \"X\"; \"X\" × \"X\"; \"P\"(\"X\"); \"P\"(\"P\"(\"X\" × \"X\") × \"X\" × \"P\"(\"P\"(\"X\"))) × \"X\". (Here \"A\" × \"B\" is the product of \"A\" and \"B\", and \"P\"(\"A\") is the powerset of \"A\".) In particular, a pair (0,\"S\") consisting of an element 0 ∈ \"N\" and a unary function \"S\" : \"N\" → \"N\" belongs to \"N\" × \"P\"(\"N\" × \"N\") (since a function is a subset of the Cartesian product). A triple (+, ·, ≤) consisting of two binary functions \"N\" × \"N\" → \"N\" and one binary relation on \"N\" belongs to \"P\"(\"N\" × \"N\" × \"N\") × \"P\"(\"N\" × \"N\" × \"N\") × \"P\"(\"N\" × \"N\"). Similarly, every algebraic structure on a set belongs to the corresponding set in the scale of sets on \"X\".\n\nNon-algebraic structures on a set \"X\" often involve sets of subsets of \"X\" (that is, subsets of \"P\"(\"X\"), in other words, elements of \"P\"(\"P\"(\"X\"))). For example, the structure of a topological space, called a topology on \"X\", treated as the set of \"open\" sets; or the structure of a measurable space, treated as the σ-algebra of \"measurable\" sets; both are elements of \"P\"(\"P\"(\"X\")). These are second-order structures.\n\nMore complicated non-algebraic structures combine an algebraic component and a non-algebraic component. For example, the structure of a topological group consists of a topology and the structure of a group. Thus it belongs to the product of \"P\"(\"P\"(\"X\")) and another (\"algebraic\") set in the scale; this product is again a set in the scale.\n\nGiven two sets \"X\", \"Y\" and a bijection \"f\" : \"X\" → \"Y\", one constructs the corresponding bijections between scale sets. Namely, the bijection \"X\" × \"X\" → \"Y\" × \"Y\" sends (\"x\",\"x\") to (\"f\"(\"x\"),\"f\"(\"x\")); the bijection \"P\"(\"X\") → \"P\"(\"Y\") sends a subset \"A\" of \"X\" into its image \"f\"(\"A\") in \"Y\"; and so on, recursively: a scale set being either product of scale sets or power set of a scale set, one of the two constructions applies.\n\nLet (\"X\",\"U\") and (\"Y\",\"V\") be two structures of the same signature. Then \"U\" belongs to a scale set \"S\", and \"V\" belongs to the corresponding scale set \"S\". Using the bijection \"F\" : \"S\" → \"S\" constructed from a bijection \"f\" : \"X\" → \"Y\", one defines:\nThis general notion of isomorphism generalizes many less general notions listed below.\nIn fact, Bourbaki stipulates two additional features. First, several sets \"X\", ..., \"X\" (so-called principal base sets) may be used, rather than a single set \"X\". However, this feature is of little use. All the items listed above use a single principal base set. Second, so-called auxiliary base sets \"E\", ..., \"E\" may be used. This feature is widely used. Indeed, the structure of a vector space stipulates not only addition \"X\" × \"X\" → \"X\" but also scalar multiplication R × \"X\" → \"X\" (if R is the field of scalars). Thus, R is an auxiliary base set (called also \"external\"). The scale of sets consists of all sets arising from all base sets (both principal and auxiliary) by taking Cartesian products and power sets. Still, the map \"f\" (possibly an isomorphism) acts on \"X\" only; auxiliary sets are endowed by identity maps. (However, the case of \"n\" principal sets leads to \"n\" maps.)\n\nSeveral statements formulated by Bourbaki without mentioning categories can be reformulated readily in the language of category theory. First, some terminology. \n\n\"Proposition.\" Each echelon construction scheme leads to a functor from Set* to itself.\n\nIn particular, the permutation group of a set \"X\" acts on every scale set \"S\".\n\nIn order to formulate one more proposition, the notion \"species of structures\" is needed, since echelon construction scheme gives only preliminary information on a structure. For example, commutative groups and (arbitrary) groups are two different species of the same echelon construction scheme. Another example: topological spaces and measurable spaces. They differ in the so-called axiom of the species. This axiom is the conjunction of all required properties, such as \"multiplication is associative\" for groups, or \"the union of open sets is an open set\" for topological spaces.\n\n\"Proposition.\" Each species of structures leads to a functor from Set* to itself.\n\nExample. For the species of groups, the functor \"F\" maps a set \"X\" to the set \"F\"(\"X\") of all group structures on \"X\". For the species of topological spaces, the functor \"F\" maps a set \"X\" to the set \"F\"(\"X\") of all topologies on \"X\". The morphism \"F\"(\"f\") : \"F\"(\"X\") → \"F\"(\"Y\") corresponding to a bijection \"f\" : \"X\" → \"Y\" is the transport of structures. Topologies on \"Y\" correspond bijectively to topologies on \"X\". The same holds for group structures, etc.\n\nIn particular, the set of all structures of a given species on a given set is invariant under the action of the permutation group on the corresponding scale set \"S\", and is a fixed point of the action of the group on another scale set \"P\"(\"S\"). However, not all fixed points of this action correspond to species of structures.\n\nGiven two species, Bourbaki defines the notion \"procedure of deduction\" (of a structure of the second species from a structure of the first species). A pair of mutually inverse procedures of deduction leads to the notion \"equivalent species\".\n\nExample. The structure of a topological space may be defined as an open set topology or alternatively, a closed set topology. The two corresponding procedures of deduction coincide; each one replaces all given subsets of \"X\" with their complements. In this sense, these are two equivalent species.\n\nIn the general definition of Bourbaki, deduction procedure may include a change of the principal base set(s), but this case is not treated here. In the language of category theory one have the following result.\n\n\"Proposition.\" Equivalence between two species of structures leads to a natural isomorphism between the corresponding functors.\n\nHowever, in general, not all natural isomorphisms between these functors correspond to equivalences between the species.\n\nIn practice, one makes no distinction between equivalent species of structures.\n\nUsually, a text based on natural numbers (for example, the article \"prime number\") does not specify the used definition of natural numbers. Likewise, a text based on topological spaces (for example, the article \"homotopy\", or \"inductive dimension\") does not specify the used definition of a topological space. Thus, it is possible (and rather probable) that the reader and the author interpret the text differently, according to different definitions. Nevertheless, the communication is successful, which means that such different definitions may be thought of as equivalent.\n\nA person acquainted with topological spaces knows basic relations between neighborhoods, convergence, continuity, boundary, closure, interior, open sets, closed sets, and does not need to know that some of these notions are \"primary\", stipulated in the definition of a topological space, while others are \"secondary\", characterized in terms of \"primary\" notions. Moreover, knowing that subsets of a topological space are themselves topological spaces, as well as products of topological spaces, the person is able to construct some new topological spaces irrespective of the definition.\n\nThus, in practice a topology on a set is treated like an abstract data type that provides all needed notions (and constructors) but hides the distinction between \"primary\" and \"secondary\" notions. The same applies to other kinds of mathematical structures. \"Interestingly, the formalization of structures in set theory is a similar task as the formalization of structures for computers.\"\n\nAs was mentioned, equivalence between two species of structures leads to a natural isomorphism between the corresponding functors. However, \"natural\" does not mean \"canonical\". A natural transformation is generally non-unique.\n\nExample. Consider again the two equivalent structures for natural numbers. One is the \"Peano structure\" (0,\"S\"), the other is the structure (+, ·, ≤) of ordered semiring. If a set \"X\" is endowed by both structures then, on one hand, \"X\" = where \"S\"(\"a\") = \"a\" for all \"n\" and 0 = \"a\"; and on the other hand, \"X\" = where \"b\" = \"b\" + \"b\", \"b\" = \"b\" · \"b\", and \"b\" ≤\"b\" if and only if \"m\" ≤ \"n\". Requiring that \"a\" = \"b\" for all \"n\" one gets the canonical equivalence between the two structures. However, one may also require \"a\" = \"b\", \"a\" = \"b\", and \"a\" = \"b\" for all \"n\" > 1, thus getting another, non-canonical, natural isomorphism. Moreover, every permutation of the index set leads to a natural isomorphism; they are uncountably many!\n\nAnother example. A structure of a (simple) graph on a set \"V\" = of vertices may be described by means of its adjacency matrix, a (0,1)-matrix of size \"n\"×\"n\" (with zeros on the diagonal). More generally, for arbitrary \"V\" an adjacency function on \"V\" × \"V\" may be used. The canonical equivalence is given by the rule: \"1\" means \"connected\" (with an edge), \"0\" means \"not connected\". However, another rule, \"0\" means \"connected\", \"1\" means \"not\", may be used, and leads to another, natural but not canonical, equivalence. In this example, canonicity is rather a matter of convention. But here is a worse case. Instead of \"0\" and \"1\" one may use, say, the two possible orientations of the plane R (\"clockwise\" and \"counterclockwise\"). It is difficult to choose a canonical rule in this case!\n\n\"Natural\" is a well-defined mathematical notion, but it does not ensure uniqueness. \"Canonical\" does, but generally is more or less conventional. A consistent choice of canonical equivalences is an inevitable component of equivalent definitions of mathematical structures.\n\n\n\n"}
{"id": "1540841", "url": "https://en.wikipedia.org/wiki?curid=1540841", "title": "Felony disenfranchisement", "text": "Felony disenfranchisement\n\nFelony disenfranchisement is the exclusion from voting of people otherwise eligible to vote (known as disfranchisement) due to conviction of a criminal offense, usually restricted to the more serious class of crimes: felonies (generally crimes of incarceration for a duration of more than a year and/or a fine exceeding $1000). Jurisdictions vary as to whether they make such disfranchisement permanent, or restore suffrage after a person has served a sentence, or completed parole or probation. Felony disenfranchisement is one among the collateral consequences of criminal conviction and the loss of rights due to conviction for criminal offense.\n\nProponents have argued that persons who commit felonies have 'broken' the social contract, and have thereby given up their right to participate in a civil society. Some argue that felons have shown poor judgment, and that they should therefore not have a voice in the political decision-making process. Opponents have argued that such disfranchisement restricts and conflicts with principles of universal suffrage. It can affect civic and communal participation in general. Opponents argue that felony disenfranchisement can create dangerous political incentives to skew criminal law in favor of disproportionately targeting groups who are political opponents of those who hold power.\n\nIn Western countries, felony disenfranchisement can be traced back to ancient Greek and Roman traditions: disenfranchisement was commonly imposed as part of the punishment on those convicted of \"infamous\" crimes, as part of their \"civil death\", whereby these persons would lose all rights and claim to property. Most medieval common law jurisdictions developed punishments that provided for some form of exclusion from the community for felons, ranging from execution on sight to exclusion from community processes.\n\nThe United States is among the most punitive nations in the world when it comes to denying the vote to those who have been convicted of a felony offense.\n\nIn the U.S., the Constitution implicitly permits the states to adopt rules about disenfranchisement \"for participation in rebellion, or other crime\", by the Fourteenth Amendment, section 2. It is up to the states to decide which crimes could be grounds for disenfranchisement, and they are not formally bound to restrict this to felonies; however, in most cases, they do. Felons who have completed their sentences are allowed to vote in most U.S. states. Between 1996 and 2008 twenty-eight states changed their laws on felon voting rights, mostly to restore rights or to simplify the process of restoration. Since 2008 state laws have continued to shift, both curtailing and restoring voter rights, sometimes over short periods of time within the same state.\n\nIn several Southern states, felony disenfranchisement was implemented as part of a strategy to bar blacks from voting. Conjoint with felony disenfranchisement, these Southern states implemented Black Codes which established severe penalties for petty crimes and were used to target black Americans.\n\nAs of 2008 over 5.3 million people in the United States were denied the right to vote due to felony disenfranchisement. In the national elections in 2012, the various state felony disenfranchisement laws together blocked an estimated 5.85 million felons from voting, up from 1.2 million in 1976. This comprised 2.5% of the potential voters in general. The state with the highest number of disenfranchised voters was Florida, with 1.5 million disenfranchised.\n\nFelony disenfranchisement was a topic of debate during the 2012 Republican presidential primary. Primary candidate Rick Santorum from Pennsylvania argued for the restoration of voting rights for convicted felons who had completed sentences and parole/probation. Santorum's position was attacked and distorted by Mitt Romney, who alleged that Santorum supported voting rights for felons while incarcerated. Former President Barack Obama supports voting rights for ex-offenders.\n\nIn the years 1997 to 2008, there was a trend to lift the disenfranchisement restrictions, or simplify the procedures for applying for the restoration of civil rights for persons who had fulfilled their punishments for felonies. As a result, in 2008 more than a half million people had the right to vote who would have been disenfranchised under the older rules. Since then, more severe disenfranchisement rules have been passed in several states.\n\nIn 2007, Florida's Republican Governor Charlie Crist pushed to make it easier for most convicted felons to regain their voting rights reasonably quickly after serving their sentences and probation terms. In March 2011, however, Republican Governor Rick Scott reversed the 2007 reforms. Felons were not able to apply to the court for restoration of voting rights until seven years after completion of sentence, probation and parole. On November 6, 2018, Florida voters approved an amendment to the state constitution to automatically restore voting rights to convicted felons who have served their sentences. Lifetime bans still apply for those convicted of either murder or sexual offenses.\n\nIn Iowa in July 2005, Democratic Governor Tom Vilsack issued an executive order restoring the right to vote for all persons who had completed supervision. On October 31, 2005, Iowa's Supreme Court upheld mass re-enfranchisement of convicted felons. But, on his inauguration day, January 14, 2011, Republican Governor Terry Branstad reversed Vilsack's executive order, disenfranchising thousands of people.\n\nThe Virginia legislature in 2017 debated relaxation of the state's policy that restoration of voting rights requires an individual act by the governor.\n\nNine other states disenfranchise felons for various lengths of time following their conviction. Except for Maine and Vermont every state prohibits felons from voting while in prison.\n\n, Iowa and Kentucky are the only two states with lifetime voting bans for felons, regardless of the crime committed.\n\nUnlike most laws that burden the right of citizens to vote based on some form of social status, felony disenfranchisement laws have been held to be constitutional. In \"Richardson v. Ramirez\" (1974), the United States Supreme Court upheld the constitutionality of felon disenfranchisement statutes, finding that the practice did not deny equal protection to disenfranchised voters. The Court looked to Section 2 of the Fourteenth Amendment to the United States Constitution, which proclaims that States in which adult male citizens are denied the right to vote for any reason other than \"participation in rebellion, or other crime,\" will suffer a reduction in the basis of their representation in Congress. Based on this language, the Court found that this amounted to an \"affirmative sanction\" of the practice of felon disenfranchisement, and the 14th Amendment could not prohibit in one section that which is expressly authorized in another.\n\nBut, critics of the practice argue that Section 2 of the 14th Amendment allows, but does not represent an endorsement of, felony disenfranchisement statutes as constitutional in light of the equal protection clause and is limited only to the issue of reduced representation. The Court ruled in \"Hunter v. Underwood\" 471 U.S. 222, 232 (1985) that a state's crime disenfranchisement provision will violate Equal Protection if it can be demonstrated that the provision, as enacted, had \"both [an] impermissible racial motivation and racially discriminatory impact.\" (The law in question also disenfranchised people convicted of vagrancy, adultery, and any misdemeanor \"involving moral turpitude\"; the test case involved two individuals who faced disenfranchisement for presenting invalid checks, which the state authorities had found to be morally turpid behavior.) A felony disenfranchisement law, which on its face is indiscriminate in nature, cannot be invalidated by the Supreme Court unless its enforcement is proven to racially discriminate and to have been enacted with racially discriminatory animus.\n\nRestoration of voting rights for people who are ex-offenders varies across the United States. Primary classification of voting rights include:\n\nMaine and Vermont are the only states with unrestricted voting rights for people who are felons. Both states allow the person to vote during incarceration, via absentee ballot and after terms of conviction end.\n\nIn fourteen states and the District of Columbia, disenfranchisement ends after incarceration is complete:\nDistrict of Columbia,\nHawaii,\nIllinois, Indiana, Maryland, Massachusetts, \nMichigan, Montana,\nNew Hampshire,\nNorth Dakota, Ohio, Oregon, Pennsylvania, Rhode Island,\nand Utah.\n\nIn February 2016 the Maryland General Assembly restored the right to vote for more than 40,000 released felons, overriding a veto by Governor Larry Hogan. Maryland's Senate approved the bill on a narrow 29-18 vote, while the state House of Delegates voted 85-56 in favor of it on January 20. Convicted felons under parole or probation had their right to vote restored. The law went into effect in late March, one month before the state's April 26 primaries.\n\nIn four states, disenfranchisement ends after incarceration and parole (if any) is complete:\nCalifornia, \nColorado, \nConnecticut, and New York.\n\nTwenty states require not only that incarceration/parole if any be complete but also that any probation sentence (which is often an alternative to incarceration) be complete:\nAlaska,\nArkansas, Florida, Georgia, Idaho, Kansas,\nLouisiana, Minnesota,\nMissouri,\nNebraska (Completion of probation + 2 years; treason convicts permanently lose the right to vote),\nNew Jersey,\nNew Mexico, \nNorth Carolina, \nOklahoma,\nSouth Carolina,\nSouth Dakota,\nTexas, \nWashington,\nWest Virginia (the prosecutor can request the court to revoke voting rights if financial obligations are unmet),\nand Wisconsin.\n\nSix states have laws that relate disenfranchisement to the detail of the crime. These laws restore voting rights to some offenders on the completion of incarceration, parole, and probation. Other offenders must make an individual petition that could be denied.\n\n\nFour states require individual petition to the court for restoration of voting after all offenses.\n\n\nIn general, during the recent centuries, the European countries have increasingly made suffrage more accessible. This has included retaining disenfranchisement in fewer and fewer cases, including for criminal offenses. Moreover, most European states, including most of those outside the European Union, have ratified the European Convention on Human Rights, and thereby agreed to respect the decisions of the European Court of Human Rights. In the case \"Hirst v United Kingdom (No 2)\" the Court in 2005 found that general rules for automatic disenfranchisements resulting from convictions to be against human rights. This ruling applied equally for prisoners and for ex-convicts. The ruling did not exclude the possibility of disenfranchisement as a consequence of deliberation in individual cases (such as that of Mohammed Bouyeri). The United Kingdom has not respected this Court opinion, although it is a signatory to the Convention (see below).\n\nIn the United Kingdom, prohibitions from voting are codified in section 3 and 3A of the Representation of the People Act 1983. Excluded are incarcerated criminals (including those sentenced by courts-martial, those unlawfully at large from such sentences, and those committed to psychiatric institutions as a result of a criminal court sentencing process). Civil prisoners sentenced (for non-payment of fines, or contempt of court, for example), and those on remand unsentenced retain the right to vote.\n\nThe UK is subject to Europe-wide rules due to various treaties and agreements associated with its membership of the European Community. The Act does not apply to elections to the European Parliament. Following \"Hirst v United Kingdom (No 2)\" (2005), in which the European Court of Human Rights (ECHR) ruled such a ban to be disproportionate, the policy was reviewed by the UK government. In 2005 the Secretary of State for Constitutional Affairs, Lord Falconer of Thoroton, stated that the review may result in the UK allowing some prisoners to vote. In 2010 the UK was still reviewing the policy, following an \"unprecedented warning\" from the Council of Europe. The UK government position was then that\n\nParliament voted in favor of maintaining disenfranchisement of prisoners in 2011 in response to Government plans to introduce legislation. Since then the Government has repeatedly stated that prisoners will not be given the right to vote in spite of the ECHR ruling.\n\nIn response to the ECHR ruling, Lord Chancellor and Secretary of State for Justice Chris Grayling produced a draft Voting Eligibility (Prisoners) Bill for discussion by a Joint Committee, incorporating two clear options for reform and one which would retain the blanket ban.\n\nFor elections in the Republic of Ireland, there is no disenfranchisement based on criminal conviction, and prisoners remain on the electoral register at their pre-imprisonment address. Prior to 2006, the grounds for postal voting did not include imprisonment, and hence those in prison on election day were in practice unable to vote, although those on temporary release could do so. In 2000 the High Court ruled that this breached the Constitution, and the government drafted a bill extending postal voting to prisoners on remand or serving sentences of less than six months. However, in 2001, the Supreme Court overturned the High Court ruling and the bill was withdrawn. After the 2005 ECHR ruling in the Hirst case, the Electoral (Amendment) Act 2006 was passed to allow postal voting by all prisoners.\n\nIn Italy, the most serious offenses involve the loss of voting rights, while for less serious offenses disqualification the judge can choose if there will be some disenfranchisement. Recently, however, the 'decree Severino' added a loss of only the right to stand for an election, against some offenders above a certain threshold of imprisonment: it operates administratively, with fixed duration and without intervention of the court. Many court actions have been presented, but the electoral disputes follows antiquated rules and the danger of causes seamless in terms of eligibility and incompatibility is very high, also at local level.\n\nSeveral European countries permit disenfranchisement by special court order, including France, Germany (reinstated after 2–5 years) and the Netherlands. In several others, no disenfranchisements due to criminal convictions exist. Moreover, many European countries encourage people to vote, such as by making pre-voting in other places than the respective election locales easily accessible. This often includes possibilities for prisoners to pre-vote from the prison itself. This is the case for example in Finland.\n\nIn Germany the law calls on prisons to encourage prisoners to vote. Only those convicted of electoral fraud and crimes undermining the \"democratic order\", such as treason, are barred from voting while in prison.\n\nAt Federation in Australia the \"Commonwealth Franchise Act 1902\" denied the franchise to vote to anyone 'attainted of treason, or who had been convicted and is under sentence or subject to be sentenced for any offence ... punishable by imprisonment for one year or longer'.\n\nIn 1983 this disqualification was relaxed and prisoners serving a sentence for a crime punishable under the law for less than a maximum five years were allowed to vote. A further softening occurred in 1995 when the disenfranchisement was limited to those serving a sentence of five years or longer, although earlier that year the Keating Government had been planning legislation to extend voting rights to all prisoners. Disenfranchisement does not continue after release from jail/prison.\n\nThe Howard Government legislated in 2006 to ban all prisoners from voting. In 2007, the High Court of Australia in \"Roach v Electoral Commissioner\" found that the Australian constitution enshrined a limited right to vote, which meant that citizens serving relatively short prison sentences (generally less than three years) cannot be barred from voting. The threshold of three years or more sentence will only result in removal of a prisoner's right to vote in federal elections. Depending on the threshold of exclusion which is distinct in each state, a prisoner may be able to vote in either state elections or federal elections. For example, prisoners in New South Wales serving a sentence of longer than one year are not entitled to vote in state elections.\n\nMost democracies give convicted criminals the same voting rights as other citizens.\n\nIn Taiwan the abrogation of political rights is a form of punishment used in sentencing, available only for some crimes or along with a sentence of death or imprisonment for life. Rights that are suspended in such a sentence include the right to vote and to take public office, as well as the rights to political expression, assembly, association, and protest. In China, there is a similar punishment of Deprivation of Political Rights.\n\nIn New Zealand, people who are in prison are not entitled to enroll while they are in prison. Persons who are convicted of electoral offenses in the past 3 years cannot vote or stand for office. In November 2018, the New Zealand Supreme Court ruled that such restrictions are inconsistent with the nation's Bill of Rights.\n\nMany countries allow inmates to vote, including Canada, Croatia, Czech Republic, Denmark, Finland, France, Germany, Israel, Kenya, Netherlands, Norway, Peru, Poland, Romania, Serbia, Sweden, and Zimbabwe. \n\nOn 8 December 2008, Leung Kwok Hung (Long Hair), member of Hong Kong's popularly elected Legislative Council (LegCo), and two prison inmates, successfully challenged disenfranchisement provisions in the LegCo electoral laws. The court found blanket disenfranchisement of prisoners to be in violation of Article 26 of the Basic Law and Article 21 of the Bill of Rights and the denial to persons in custody of access to polling stations as against the law. The government introduced a bill to repeal the provisions of the law disenfranchising persons convicted of crimes (even those against the electoral system) as well as similar ones found in other electoral laws, and it made arrangements for polling stations to be set up at detention centers and prisons. LegCo passed the bill, and it took effect from 31 October 2009, even though no major elections were held until the middle of 2011.\n\n\n\n"}
{"id": "43397800", "url": "https://en.wikipedia.org/wiki?curid=43397800", "title": "Fields of Science and Technology", "text": "Fields of Science and Technology\n\nFields of Science and Technology (FOS) is a compulsory classification for statistics of branches of scholarly and technical fields, published by the OECD in 2002. It was created out of the need to interchange data of research facilities, research results etc. It was revised in 2007 under the name \"Revised Fields of Science and Technology\". \n\n\n"}
{"id": "52980074", "url": "https://en.wikipedia.org/wiki?curid=52980074", "title": "Five safes", "text": "Five safes\n\nThe Five Safes is a framework for helping make decisions about making effective use of data which is confidential or sensitive. It is mainly used to describe or design research access to statistical data held by government agencies, and by data archives such as the UK Data Service.\n\nTwo of the Five Safes refer to statistical disclosure control (SDC), and so the Five Safes is usually used to contrast statistical and non-statistical controls when comparing data management options.\n\nThe Five Safes proposes that data management decisions be considered as solving problems in five 'dimensions': projects, people, settings, data and outputs. These are most commonly expressed as questions, for example:\n\nThese dimensions are scales, not limits. That is, solutions can have a mix of more or fewer controls in each dimension, but the overall solution is 'safe' independent of the particular mix. For example, a public use file available for open download cannot control who uses it, where or for what purpose, and so all the control (protection) must be in the data itself. In contrast, a file which is only accessed through a secure environment with certified users can contain very sensitive information: the non-statistical controls allow the data to be 'unsafe'. One academic likened the process to a graphic equalizer, where bass and treble can be combined independently to produce a sound the listener likes.\n\nThe Five Safes concept is associated with other topics which developed from the same programme at ONS, although these are not necessarily implemented. Safe people is associated with 'active researcher management', while safe outputs is linked with principles-based output statistical disclosure control. The Five Safes is associated with the 'Data Access Spectrum': as the non-data controls tend to work together, these are contrasted with data detail reduction to present a linear representation of data access options.\n\nThe Five Safes was devised in the winter of 2002/2003 by Felix Ritchie at the UK Office for National Statistics (ONS) to describe its secure remote-access Virtual Microdata Laboratory (VML). It was described at this time as the 'VML Security Model'. This was adopted by the NORC data enclave, and more widely in the US, as the 'portfolio model' (although this is now also used to refer to a slightly different legal/statistical/educational breakdown). In 2012 the framework as was still being referred to as the 'VML security model', but its increasing use among non-UK organisations led to the adoption of the more general and informative phrase 'Five Safes'.\n\nThe original framework only had four safes (projects, people, settings and outputs): the framework was used to describe highly detailed data access through a secure environment, and so the 'data' dimension was irrelevant. From 2007 onwards, 'safe data' was included as the framework was used to a describe a wider range of ONS activities. As the US version was based upon the 2005 specification, some US iterations uses have the original four dimensions (eg).\n\nSome discussions, such as the OECD, use the term 'secure' instead 'safe'.\n\nThe framework has had three uses: pedagogical, descriptive, and design. The latter is a relatively recent development.\n\nThe first significant use of the framework, other than internal administrative use, was to structure researcher training courses at the UK Office for National Statistics from 2003. UK Data Archive, Administrative Data Research Network, Eurostat, Statistics New Zealand, the Mexican National Institute of Statistics and Geography, NORC and the Australian Bureau of Statistics, amongst others, have also used this framework. Most of these courses are for researchers using restricted-access facilities; the Eurostat courses are unusual in that they are designed for all users of sensitive data.\n\nThe framework is often used to describe existing data access solutions (e.g. UK HMRC Data Lab, UK Data Service, Statistics New Zealand) or planned/conceptualised ones (e.g. Eurostat in 2011). An early use was to help identify areas where ONS' still had 'irrreducible risks' in its provision of secure remote access.\n\nThe framework is mostly used for confidential social science data. To date it appears to have made little impact on medical research planning, although it is now included in the revised guidelines on implementing HIPAA regulations in the US, and by Cancer Research UK and the Health Foundation in the UK. It has also been used to describe a security model for the Scottish Health Informatics Programme.\n\nIn general the Five Safes has been used to describe solutions post-factum, and to explain/justify choices made, but an increasing number of organisations have used the framework to design data access solutions. For example, the Hellenic Statistical Agency developed a data strategy built around the Five Safes in 2016; the UK Health Foundation used the Five Safes to design its data management and training programmes.\n\nThe major design use is in Australia: both the Australian Bureau of Statistics and the Australian Department of Social Service used the Five Safes as an ex ante design tool. In 2017 the Australian Productivity Commission recommended adopting a version of the framework to support cross-government data sharing and re-use.\n\nIn 2015 the UK Data Service organized a workshop to encourage data users from the academic and private sectors to think about how to manage confidential research data, using the Five Safes to demonstrate alternative options and best practice.\n\nTwo laws have incorporated the Fives Safes: explicitly in the South Australian Public Sector (Data Sharing) Act 2016, implicitly in the research provisions of the Digital Economy Act.\n\nThe UK Data Service has produced a blog and video for the general public about the use of Five Safes in re-using administrative data. Statistics New Zealand produced a non-technical description, as did ONS for Data Privacy Day 2017.\n\nNotes\n\nCitations\n"}
{"id": "12686181", "url": "https://en.wikipedia.org/wiki?curid=12686181", "title": "Fossil fuel phase-out", "text": "Fossil fuel phase-out\n\nFossil fuel phase out refers to the discontinuation of the use of fossil fuels, through the decommissioning of operating fossil fuel-fired power plants, the prevention of the construction of new ones, and the use of alternative energy to replace the role of fossil fuels.\n\nThe purpose of fossil fuel phase-out is to reduce the negative externalities that use of fossil fuels cause. Negative externalities refer to the costs a certain activity has over people who did not choose to incur in them. A direct negative externality from fossil fuels' use is air pollution, and an indirect negative externality are mining accidents, that happen as a consequence of the extraction of fossil fuels. Fossil fuel burning contributes to climate change, as it releases greenhouse gas emissions.\n\nCoal is one of the largest sources of energy, supplying 28.6% percent of the world's primary energy in 2014 (equivalent to 3,917 Mtoe) according to the International Energy Agency. Coal combustion accounted for 14,863 Mt of CO emissions in 2014, which is equivalent to a 45.9% of fossil fuel emissions from combustion (excluding non-energy emissions).\n\nTo decrease carbon emissions and thus possibly stop extreme climate change, some have called for coal to be phased out. Climatologist James E. Hansen said \"We need a moratorium on coal now...with phase-out of existing plants over the next two decades.\" According to a study published in Science in 2017, coal has to be phased-out globally by about 2030, if the agreed 2 °C target is taken seriously.\n\nSome nations have decreased their coal consumption thus far in the 21st century, the greatest reductions being in the United States (coal consumption reduced by 176 million metric tons per year over the period 2000–2012), Canada (reduced by 21 million tons per year) and Spain (20 million tons per year). Other nations have increased their coal consumption in the same period, led by China (increased 2,263 million metric tons per year in the period 2000–2012), India (increased 367 million tons per year), and South Korea (59 million tons per year). Worldwide, coal consumption increased 60% during the period 2000–2012. As of 2012, 1200 new coal power plants were reportedly being planned worldwide, most of them in China and India. In the 2011–2013 period, the OECD group of Western European countries has increased the use of coal, attributed largely to the low cost of coal and the high price of imported natural gas in Western Europe.\nHowever, coal consumption has peaked in China in 2013 or 2014, depending on the data used and fell in 2015 by 3.6%, even though there was a growth of GDP of 6.9%. After smaller increases, consumption remained below the prior peak by 2017. Worldwide coal consumption peaked in 2014 and declined in 2015 and 2016.\n\nAccording to Scientific American, the average coal plant emits more than 100 times as much radiation per year than does a comparatively sized nuclear power plant, in the form of fly ash.\n\nSome like the \"coal advisory board\" of the IEA believe that coal should not be phased out, considering that longer-term global economic growth cannot be achieved without adequate and affordable energy supplies, which will require continuing significant contributions from fossil fuels including coal. In this viewpoint, clean coal technology could reduce greenhouse gas emissions compatible with a low-emissions future. Some environmentalists and climatologists support a phase-out and criticise clean coal as not a solution to climate change. Entrepreneurs promote improved regulations and modernised technology. Sometimes coal is replaced by natural gas, which has lower carbon emissions and produces less pollutants. However natural gas is also a fossil fuel, so a switch from coal to natural gas does not contribute to a fossil fuel phase-out.\n\n, 28 national governments, 19 sub-national governments and 28 organisations had become members of the Powering Past Coal Alliance, each making a declaration to advance the transition away from unabated coal power generation.\n\nOil is refined into fuel oil, diesel and gasoline. The refined products are primarily for transportation by conventional cars, trucks, trains, planes and ships. Popular alternatives are human-powered transport, public transport, electric vehicles, and biofuels.\n\nAlthough natural gas has about half the carbon intensity of coal it is also the single largest source of atmospheric methane in the United States. It is seen by many as a temporary \"bridge fuel\" to replace coal, but in turn to be replaced by renewable sources. However this \"bridge fuel\" is likely to significantly extend the use of fossil fuel as the average plant life is 35 years. Gas consumption has tripled since 1971, and by 2015 it was generating half as much electricity as coal. Since the consumption of gas is expected to grow an additional 10% by 2040, the phase out is likely to be many years in the future.\n\nThe basis of phasing-out fossil fuels consists mainly of the projected lower cost of renewable sources of energy, but the avoidance of risks in health and mitigation of global warming are also important considerations.\n\nUsing computer modeling he developed over 20 years, Mark Z. Jacobson has found that carbonaceous fuel soot emissions (which lead to respiratory illness, heart disease, and asthma) have resulted in 1.5 million premature deaths each year, mostly in the developing world where the non-fossil fuels wood and animal dung are used for cooking. Jacobson has also said that soot from diesel engines, coal-fired power plants, and burning wood is a \"bigger cause of global warming than previously thought, and is the major cause of the rapid melting of the Arctic's sea ice\".\n\nIn 2011, new evidence has emerged that there are considerable risks associated with traditional energy sources, and that major changes to the mix of energy technologies are needed:\n\nIn 2008, James Hansen and nine other scientists published a journal article titled \"Target atmospheric : Where should humanity aim?\" which calls for a complete phase-out of coal power by 2030.\n\nMore recently, Hansen has stated that continued opposition to nuclear power threatens humanity's ability to avoid dangerous climate change. The letter, co-authored with other climate change experts declared \"If we stay on the current path,\" he said, \"those are the consequences we'll be leaving to our children. The best candidate to avoid that is nuclear power. It's ready now. We need to take advantage of it.\" and \"Continued opposition to nuclear power threatens humanity's ability to avoid dangerous climate change.\"\n\nAlso in 2008, Pushker Kharecha and James Hansen published a peer-reviewed scientific study analyzing the effect of a coal phase-out on atmospheric carbon dioxide (CO) levels. Their baseline mitigation scenario was a phaseout of global coal emissions by 2050. The authors describe the scenario as follows:\n\nKharecha and Hansen also consider three other mitigation scenarios, all with the same coal phase-out schedule but each making different assumptions about the size of oil and gas reserves and the speed at which they are depleted. Under the Business as Usual scenario, atmospheric CO peaks at 563 parts per million (ppm) in the year 2100. Under the four coal phase-out scenarios, atmospheric CO peaks at 422-446 ppm between 2045 and 2060 and declines thereafter. The key implications of the study are as follows: a phase-out of coal emissions is the most important remedy for mitigating human-induced global warming; actions should be taken toward limiting or stretching out the use of conventional oil and gas; and strict emissions-based constraints are needed for future use of unconventional fossil fuels such as methane hydrates and tar sands.\n\nThe impulse of renewable energy can create jobs through the construction of new power plants and the manufacturing of the equipment that they need, as could be seen in the case of Germany and the wind power industry.\n\nIn the Greenpeace and EREC's Energy (R)evolution scenario, the world would eliminate all fossil fuel use by 2090.\n\nIn December 2015, Greenpeace and Climate Action Network Europe released a report highlighting the need for an active phase-out of coal-fired generation across Europe. Their analysis derived from a database of 280 coal plants and included emissions data from official EU registries.\n\nA September 2016 report by Oil Change International, concludes that the carbon emissions embedded in the coal, oil, and gas in currently working mines and fields, assuming that these run to the end of their working lifetimes, will take the world to just beyond the 2°C limit contained in the 2015 Paris Agreement and even further from the 1.5°C goal. The report observes that \"one of the most powerful climate policy levers is also the simplest: stop digging for more fossil fuels\".\n\nIn October 2016, the Overseas Development Institute (ODI) and 11 other NGOs released a report on the impact of building new coal-fired power plants in countries where a significant proportion of the population lacks access to electricity. The report concludes that, on the whole, building coal-fired power plants does little to help the poor and may make them poorer. Moreover, wind and solar generation are beginning to challenge coal on cost.\n\nA 2018 study in Nature Energy, suggests that 10 countries in Europe could completely phase out coal-fired electricity generation with their current infrastructure, whilst the United States and Russia could phase out at least 30%.\n\nThe phase-out of fossil fuels involves many challenges, and one of them is the reliance that currently the world has on them. In 2014, fossil fuels provided 81.1% of the primary energy consumption of the world, with approximately 11,109 Mtoe. This number is composed by 4,287 Mtoe of oil consumption; 3,918 Mtoe of coal consumption, and 2,904 Mtoe of natural gas consumption.\n\nFossil fuel phase-out can lead to an increment in electricity prices, because of the new investments needed to replace their share in the electricity mix with alternative energy sources. Another cause to increasing electricity price comes from the need to import the electricity that can't be generated nationally.\n\nAnother impact of a phase-out of fossil fuels is in the employment. In the case of employments in the fossil fuel industry, a phase-out is logically undesired, therefore, people in the industry will usually oppose any measures that put their industries under scrutiny. Endre Tvinnereim and Elisabeth Ivarsflaten studied the relationship between employment in the fossil fuel industry with the support to climate change policies. They proposed that one opportunity for displaced drilling employments in the fossil fuel industry could be in the geothermal energy industry. This was suggested as a result of their conclusion: people and companies in the fossil fuel industry will likely oppose measures that endanger their employments, unless they have other stronger alternatives. This can be extrapolated to political interests, that can push against the phase-out of fossil fuels initiative. One example is how the vote of US Congress members is related to the preeminence of fossil fuel industries in their respective states.\n\nIn 8 June 2015, several newspapers ran an article wrote that the leaders of the Group of Seven (or G7, consisting of Canada, France, Germany, Italy, Japan, the United Kingdom, and the United States) agreed to phase-out fossil fuel use by 2100, as part of the efforts to keep global temperature increase under 2°C. This was done as a prelude for the United Nations Climate Change Conference (a.k.a. COP 21) hosted in Paris, on December of the same year.\n\nThe Australian Greens party have proposed to phase out coal power stations. The NSW Greens proposed an immediate moratorium on coal-fired power stations and want to end all coal mining and coal industry subsidies. The Australian Greens and the Australian Labor Party also oppose nuclear power. The Federal Government and Victorian State Government want to modify existing coal-fired power stations into clean coal power stations. The Federal Labor government extended the mandatory renewable energy targets, an initiative to ensure that new sources of electricity are more likely to be from wind power, solar power and other sources of renewable energy in Australia. Australia is one of the largest consumers of coal per capita, and also the largest exporter. The proposals are strongly opposed by industry, unions and the main Opposition Party in Parliament (now forming the party in government after the September 2013 election).\n\nIn 2005, Canada annually burned 60 million tons of coal, mainly for electrical power, increasing by 15 percent annually.\n\nIn November 2016, Canada announced plans to phase-out coal-fired electricity generation by 2030.\n\nBeginning in 2005, Ontario, Canada planned coal phase-out legislation. Ontario annually consumed 15 million tons of coal in large power plants to supplement nuclear power. Nanticoke Generating Station was a major source of air pollution, and Ontario suffered \"smog days\" during the summer.\nIn 2007, Ontario's Liberal government committed to phasing out all coal generation in the province by 2014. Premier Dalton McGuinty said, \"By 2030 there will be about 1,000 more new coal-fired generating stations built on this planet. There is only one place in the world that is phasing out coal-fired generation and we're doing that right here in Ontario.\" The Ontario Power Authority projects that in 2014, with no coal generation, the largest sources of electrical power in the province will be nuclear (57 percent), hydroelectricity (25 percent), and natural gas (11 percent). In April 2014 Ontario Canada was the first jurisdiction in North America to eliminate coal in electricity generation. The final coal plant in Ontario, Thunder Bay Generating Station, stopped burning coal in April 2014.\n\nThere are currently no plans to phase out coal burning power stations in the People's Republic of China on the national level.\n\nChina's exceedingly high energy demand has pushed the demand for relatively cheap coal-fired power. Each week, another 2 GW of coal-fired power is put online in China. Coal supplies about 80% of China's energy needs today, and that ratio is expected to continue, even as overall power usage grows rapidly. Serious air quality deterioration has resulted from the massive use of coal and many Chinese cities suffer severe smog events.\n\nAs a consequence the region of Beijing has decided to phase out all its coal-fired power generation by the end of 2015. \n\nIn 2009, China had 172 GW of installed hydro capacity the largest in the world, producing 16% of China's electricity, the Eleventh Five-Year Plan has set a 300 GW target for 2020. China built the world's largest power plant of any kind, the Three Gorges Dam.\n\nIn addition to the huge investments in coal power, China has 32 reactors under construction, the highest number in the world.\n\nAnalysis in 2016, showed that China's coal consumption appears to have peaked in 2014.\n\nIn July 2014, CAN Europe, WWF European Policy Office, HEAL, EEB and Climate-Alliance Germany published a report calling for the decommissioning of the thirty most polluting coal-fired power plants in Europe.\n\nAs part of their Climate policy Plan, Denmark stated that it will phase out oil for heating purposes and coal by 2030. Additionally, their goal is to supply a 100% of their electricity and heating needs with renewable energy five years later (i.e. 2035).\n\nIn December 2017, to fight against global warming, France adopted a law banning new fossil fuel exploitation projects and closing current ones by 2040 in all of its territories. France thus became the first country to programme the end of fossil fuel exploitation.\n\nHard coal mining has long been subsidized in Germany, reaching a peak of €6.7billion in 1996 and dropping to €2.7billion in 2005 due to falling output. These subsidies represent a burden on public finances and imply a substantial opportunity cost, diverting funds away from other, more beneficial public investments.\n\nIn 2007, Germany announced plans to phase out hard coal-industry subsidies by 2018, a move which is expected to end hard coal mining in Germany. This exit is later than the EU-mandated end by 2014. Solar and wind are major sources of energy and renewable energy generation, around 15% as of December 2013, and growing. Coal is still the largest source of power in Germany.\n\nIn 2007, German Chancellor Angela Merkel and her party agreed to legislation to phase out Germany's hard coal mining sector. That does not mean that they support phasing out coal in general. There were plans to build about 25 new plants in the coming years. Most German coal power plants were built in the 1960s, and have a low energy efficiency. Public sentiment against coal power plants is growing and the construction or planning of some plants has been stopped. A number are under construction and still being built. No concrete plan is in place to reduce coal-fired electricity generation. As of October 2015, the remaining coal plants still under planning include: Niederaussem, Profen, and Stade. The coal plants currently under construction include: Mannheim, Hamm D, Datteln, and Willhelmshaven. Between 2012 and 2015, six new plants went online. All of these plants are 600–1800  MW.\n\nIn 2014, Germany's coal consumption dropped for the first time, having risen each year since the low during the 2009 recession.\n\nA 2014 study, found that coal is not making a comeback in Germany, as is sometimes claimed. Rather renewables have more than offset the nuclear facilities that have been shutdown as a result of Germany's nuclear phase-out (\"Atomausstieg\"). Hard coal plants now face financial stringency as their operating hours are cut back by the market. But in contrast, lignite-fired generation is in a safe position until the unless government policies change. To phase-out coal, Germany should seek to strength the emissions trading system (EU-ETS), consider a carbon tax, promote energy efficiency, and strengthen the use of natural gas as a bridge fuel.\n\nIn 2016, the German government and affected lignite power plant operators , RWE, and Vattenfall reached an understanding (\"Verständigung\") on the transfer of lignite power plant units into security standby (\"Überführung von Braunkohlekraftwerksblöcken in die Sicherheitsbereitschaft\"). As a result, eight lignite-fired power plants are to be mothballed and later closed, with the first plant scheduled to cease operation in October 2016 and the last in October 2019. The affected operators will receive state compensation for foregone profits. The European Commission has declared government plans to use €1.6billion of public financing for this purpose to be in line with EU state aid rules.\n\nA 2016 study, found that the phase-out of lignite in Lusatia (\"Lausitz\") by 2030 can be financed by future owner EPH in a manner that avoids taxpayer involvement. Instead, liabilities covering decommissioning and land rehabilitation could be paid by EPH directly into a foundation, perhaps run by the public company . The study calculates the necessary provisions at €2.6billion.\n\nIn November 2016, the German utility STEAG announced it will be decommissioning five coal-fired generating units in North Rhine-Westphalia and Saarland due to low wholesale electricity prices.\n\nA coal phase-out for Germany is implied in Germany's Climate Action Plan 2050, environment minister Barbara Hendricks said in an interview on 21November 2016. \"If you read the Climate Action Plan carefully, you will find that the exit from coal-fired power generation is the immanent consequence of the energy sector target... By 2030... half of the coal-fired power production must have ended, compared to 2014\", she said.\n\nPlans to cut down the ancient Hambach Forest to extend the Hambach open pit mine in 2018 have resulted in massive protests. On 5 Oct, 2018 a German court ruled against the further destruction of the forest for mining purposes. The ruling states, the court needs more time to reconsider the complaint. Angela Merkel, the chancellor of Germany, welcomed the court's ruling. The forest is located approximately 29 km km west of the city center of Cologne (specifically Cologne Cathedral).\n\nOn 22 September 2016, the Dutch parliament voted for a 55% cut in emissions by 2030, a move which would require the closure of the country's five coal-fired power plants. The vote is not binding on the government however.\n\nIn October 2018, the Sánchez government and Spanish Labour unions settled an agreement to close ten Spanish coal mines at the end of 2018. The government pre-engaged to spend 250 million Euro to pay for early retirements, occupational retraining and structural change. In 2018, about 2.3 per cent of the electric energy produced in Spain was produced in coal-burning power plants.\n\nSweden is constructing hydrogen-based pilot steel plant to replace coke and coal usage in steel production. Once this technology is commercialized with the hydrogen generated from renewable energy sources (biogas or electricity), the carbon foot print of steel production would reduce drastically.\n\nIndia is the third largest consumer of coal in the world. India's federal energy minister is planning to stop importing thermal coal by 2018.\nThe annual report of India's Power Ministry has a plan to grow power by about 80 GW as part of their 11th 5-year plan, and 79% of that growth will be in fossil fuel–fired power plants, primarily coal. India plans four new \"ultra mega\" coal-fired power plants as part of that growth, each 4000 MW in capacity. , there are six nuclear reactors under construction. In the first half of 2016, the amount of coal-fired generating capacity in pre-construction planning in India fell by 40,000 MW, according to results released by the Global Coal Plant Tracker. In June 2016, India's Ministry of Power stated that no further power plants would be required in the next three years, and \"any thermal power plant that has yet to begin construction should back off.\"\n\nIn cement production, carbon neutral biomass is being used to replace coal for reducing carbon foot print drastically.\n\nIn October 2007, the Clark Labour government introduced a 10year moratorium on new fossil fuel thermal power generation. The ban was limited to state-owned utilities, although an extension to the private sector was considered. The new government under MP John Key (NZNP) elected in November 2008 repealed this legislation.\n\nIn 2014, almost 80 per cent of the electricity produced in New Zealand was from sustainable energy. \nOn 6 August 2015, Genesis Energy Limited announced that it would close its two last coal-fired power stations.\n\nAs of 2007, South Africa's power sector is the 8th highest global emitter of CO. In 2005/2006, 77% of South Africa's energy demand was directly met by coal, and when current projects come online, this ratio will increase in the near term.\n\nThere are no plans to phase out coal-fired power plants in South Africa, and indeed, the country is investing in building massive amounts of new coal-fired capacity to meet power demands, as well as modernizing the existing coal-fired plants to meet environmental requirements.\n\nOn April 6, 2010, the World Bank approved a $3.75B loan to South Africa to support the construction of the world's 4th largest coal-fired plant, at Medupi.\nThe proposed World Bank loan includes a relatively small amount $260 million for wind and solar power.\n\nRated at 4800 GW, Medupi Power Station would join other mammoth coal-fired power plants already in operation in the country, namely Kendal Power Station (4100 GW), Majuba Power Station (4100 GW), and Matimba Power Station (4000 GW), as well as a similar-capacity Kusile Power Station, at 4800 GW, currently under construction. Kusile is expected to come online in stages, starting in 2012, while Medupi is expected to first come online in 2013, with full capacity available by 2017. These schedules are provisional, and may change.\n\nSince 2008, South Africa's government started funding solar water heating installations. As of January 2016, there have been 400 000 domestic installations in total, with free-of-charge installation of low-pressure solar water heaters for low-cost homes or low-income households which have access to the electricity grid, while other installations are subsidised.\n\nEd Miliband (energy secretary from 3 October 2008 11 May 2010) announced that no new coal-fired power stations will be built in Britain from 2009 onwards unless they capture and bury at least 25% of greenhouse gases immediately and 100% by 2025 although at the time this was a statement of intent rather than something he was able to enforce.\n\nChris Huhne (energy secretary from 12 May 2010 5 February 2012) has confirmed that the legislation required to allow his office to enforce emissions standards are proceeding.\n\nThe UK is also subject to the EU's Large Combustion Plant Directive covering non-CO2 emissions which is expected to bring many older plants to a close over the next few years as they are too expensive to upgrade.\n\nAmber Rudd (energy secretary from 11 May 2015) announced on 18 November 2015 that all coal-fired power stations would close by 2025. This will not be a complete phase out of fossil fuels because gas-fired power stations will continue to provide some firm power.\n\nThe closure of the last coal power station in March 2016 ended coal-fired power production in Scotland.\n\nIn 2017, fossil fuels provided 81 percent of the energy consumed in the United States, down from 86 percent in 2000.\n\nIn 2007, 154 new coal-fired plants were on the drawing board in 42 states. By 2012, that had dropped to 15, mostly due to new rules limiting mercury emissions, and limiting carbon emissions to 1,000 pounds of CO per megawatt-hour of electricity produced.\n\nIn July 2013, US Secretary of Energy Ernest Moniz outlined Obama administration policy on fossil fuels:\n\nThen-US Energy Secretary Steven Chu and researchers for the US National Renewable Energy Laboratory have noted that greater electrical generation by non-dispatchable renewables, such as wind and solar, will also increase the need for flexible natural gas-powered generators, to supply electricity during those times when solar and wind power are unavailable. Gas-powered generators have the ability to ramp up and down quickly to meet changing loads.\n\nIn the US, many of the fossil fuel phase-out initiatives have taken place at the state or local levels.\n\nCalifornia's SB 1368 created the first governmental moratorium on new coal plants in the United States. The law was signed in September 2006 by Republican Governor Arnold Schwarzenegger, took effect for investor-owned utilities in January 2007, and took effect for publicly owned utilities in August 2007. SB 1368 applied to long-term investments (five years or more) by California utilities, whether in-state or out-of-state. It set the standard for greenhouse gas emissions at 1,100 pounds of carbon dioxide per megawatt-hour, equal to the emissions of a combined-cycle natural gas plant. This standard created a de facto moratorium on new coal, since it could not be met without carbon capture and sequestration.\n\nOn April 15, 2008, Maine Governor John E. Baldacci signed LD 2126, \"An Act To Minimize Carbon Dioxide Emissions from New Coal-Powered Industrial and Electrical Generating Facilities in the State.\" The law, which was sponsored by Rep. W. Bruce MacDonald (D-Boothbay), requires the Board of Environmental Protection to develop greenhouse gas emission standards for coal gasification facilities. It also puts a moratorium in place on building any new coal gasification facilities until the standards are developed.\n\nIn early March 2016, Oregon lawmakers approved a plan to stop paying for out-of-state coal plants by 2030 and require a 50 percent renewable energy standard by 2040. Environmental groups such as the American Wind Energy Association and leading Democrats praised the bill.\n\nIn 2006 a coalition of Texas groups organized a campaign in favor of a statewide moratorium on new coal-fired power plants. The campaign culminated in a \"Stop the Coal Rush\" mobilization, including rallying and lobbying, at the state capital in Austin on February 11 and 12th, 2007. Over 40 citizen groups supported the mobilization.\n\nIn January, 2007, A resolution calling for a 180-day moratorium on new pulverized coal plants was filed in the Texas Legislature by State Rep. Charles \"Doc\" Anderson (R-Waco) as House Concurrent Resolution 43. The resolution was left pending in committee. On December 4, 2007, Rep. Anderson announced his support for two proposed integrated gasification combined cycle (IGCC) coal plants proposed by Luminant (formerly TXU).\n\nWashington has followed the same approach as California, prohibiting coal plants whose emissions would exceed those of natural gas plants. Substitute Senate Bill 6001 (SSB 6001), signed on May 3, 2007, by Governor Christine Gregoire, enacted the standard. As a result of SSB 6001, the Pacific Mountain Energy Center in Kalama was rejected by the state. However, a new plant proposal, the Wallula Energy Resource Center, shows the limits of the \"natural gas equivalency\" approach as a means of prohibiting new coal plants. The proposed plant would meet the standard set by SSB 6001 by capturing and sequestering a portion (65 percent, according to a plant spokesman) of its carbon.\n\n\nJapan, the world's third-largest economy, made a major move to use more fossil fuels in 2012, when the nation shut down nuclear reactors following the Fukishima accident. Nuclear, which had supplied 30 percent of Japanese electricity from 1987 to 2011, supplied only 2 percent in 2012 (hydropower supplied 8 percent). Nuclear electricity was replaced with electricity from petroleum, coal, and liquified natural gas. As a result, electricity generation from fossil fuels rose to 90 percent in 2012.\n\nIn January 2017, the Japanese government announced plans to build 45 new coal-fired power plants in the next ten years, largely to replace expensive electricity from petroleum power plants.\n\nIn October 2007, Civil Society Institute released the results of a poll of 1,003 US citizens conducted by Opinion Research Corporation.\n\nThe authors of the poll reported:\n\"75 percent of Americans—including 65 percent of Republicans, 83 percent of Democrats and 76 percent of Independents—would 'support a five-year moratorium on new coal-fired power plants in the United States if there was stepped-up investment in clean, safe renewable energy—such as wind and solar—and improved home energy-efficiency standards.' Women (80 percent) were more likely than men (70 percent) to support this idea. Support also was higher among college graduates (78 percent) than among those who did not graduate from high school (68 percent).\"\n\nThe exact question posed by the survey was as follows:\n\"More than half of power plant-generated electricity comes from coal. Experts say that power plants are responsible for about 40 percent of U.S. carbon dioxide pollution linked to global warming. There are plans to build more than 150 new coal-fired power plants over the next several years. Would you support a five-year moratorium on new coal-fired power plants in the United States if there was stepped-up investment in clean, safe and renewable energy—such as wind and solar—and improved home energy-efficiency standards? Would you say definitely yes, probably yes, probably no, definitely no, or don't know.\"\n\nThe results were as follows:\n\n\nIn 2013, the Gallup organization determined that 41% of Americans wanted less emphasis placed on coal energy, versus 31% who wanted more. Large majorities wanted more emphasis placed on solar (76%), wind (71%), and natural gas (65%).\n\nA 2009 ABC/Washington Post poll found 52% of Americans favored more coal mining (33% strongly favored), while 45% opposed (27% strongly opposed). The most support was for wind and solar, which were favored by 91% (79% strongly favored).\n\nIn October 2007, fifteen groups led by Citizens Lead for Energy Action Now (CLEAN) called for a five-year moratorium on new coal-fired power plants, with no exception for plants sequestering carbon. The groups included Save Our Cumberland Mountains (Tennessee); Ohio Valley Environmental Council (West Virginia); Cook Inlet Keeper (Alaska); Christians for the Mountains (West Virginia); Coal River Mountain Watch (West Virginia); Kentuckians for the Commonwealth (Kentucky); Civil Society Institute (Massachusetts); Clean Power Now (Massachusetts); Indigenous Environmental Network (Minnesota); Castle Mountain Coalition (Alaska); Citizens Action Coalition (Indiana); Appalachian Center for the Economy & the Environment (West Virginia); Appalachian Voices (NC); and Rhode Island Wind Alliance (Rhode Island).\n\nThe US-based Environmental Defense Fund (EDF) has taken a stand in favor of natural gas production and hydraulic fracturing, while pressing for stricter environmental controls on gas drilling, as a feasible way to replace coal. The organization has funded studies jointly with the petroleum industry on the environmental effects of natural gas production. The organization sees natural gas as a way to quickly replace coal, and that natural gas in time will be replaced by renewable energy. The policy has been criticized by some environmentalists. EDF counsel and blogger Mark Brownstein answered:\n\n\nRESOLVED: Shareholders request that BOA's board of directors amend its GHG emissions policies to observe a moratorium on all financing, investment and further involvement in activities that support MTR coal mining or the construction of new coal-burning power plants that emit carbon dioxide.\n\n\n\nOn 13 October 2007, Pocatello, Idaho, mayor Roger Chase told other mayors from across the state attending an Association of Idaho Cities legislative committee that he favored a moratorium no new coal plants in the state.\n\nOn 1 June 2007, Park City, Utah, mayor Dana Wilson wrote a letter to Warren Buffett expressing the city's opposition to three coal plants proposed by Rocky Mountain Power.\n\nIn November 2007, Salt Lake City mayor Rocky Anderson expressed his support for a coal moratorium at a rally organized by the Step It Up! campaign.\n\nIn December 2007, Charlottesville, VA, mayor Dave Norris blogged in favor of a moratorium on new coal-fired power plants. On 19 December 2007, Charlottesville passed the Charlottesville Clean Energy Resolution putting the city on record as supporting a moratorium.\n\nIn January 2008, Black Hawk County (Iowa) Health Board recommended that the state adopt a moratorium on new coal-fired power plants until it enacts tougher air pollution standards.\n\nAlternative energy refers to any source of energy that can substitute the role of fossil fuels. Renewable energy, or energy that is harnessed from renewable sources, is an alternative energy. However, alternative energy can refer to non renewable sources as well, like nuclear energy. Between the alternative sources of energy are: solar energy, hydroelectricity, marine energy, wind energy, geothermal energy, biofuels, ethanol and Hydrogen.\n\nEnergy efficiency is complementary to the use of alternative energy sources, when phasing-out fossil fuels.\n\nRenewable energy is energy that comes from resources which are naturally replenished such as sunlight, wind, rain, tides, waves, and geothermal heat. , 19% of global final energy consumption comes from renewable resources, with 9% of all energy from traditional biomass, mainly used for heating, 1% from biofuels, 4% from hydroelectricity and 4% from biomass, geothermal or solar heat. Popular renewables (wind, solar, geothermal and biomass for power) accounted for another 1.4% and are growing rapidly. While renewable energy supplies are growing and have displaced coal in some regions, the amount of coal burned in 2021, is expected to be the same as it was in 2014.\n\nIn 2015, hydroelectric energy generated 16.6% of the worlds total electricity and 70% of all renewable electricity. In Europe and North America environmental concerns around land flooded by large reservoirs ended 30 years of dam construction in the 1990s. Since then large dams and reservoirs continue to be built in countries like China, Brazil and India. Run-of-the-river hydroelectricity and small hydro have become popular alternatives to conventional dams that may create reservoirs in environmentally sensitive areas.\n\nA wind farm is a group of wind turbines in the same location used to produce electric power. A large wind farm may consist of several hundred individual wind turbines, and cover an extended area of hundreds of square miles, but the land between the turbines may be used for agricultural or other purposes. A wind farm may also be located offshore.\n\nWind power has grown dramatically since 2005 and by 2015 supplied almost 1% of global energy consumption.\n\nMany of the largest operational onshore wind farms are located in the United States and China. The Gansu Wind Farm in China has over 5,000 MW installed with a goal of 20,000 MW by 2020. China has several other \"wind power bases\" of similar size. The Alta Wind Energy Center in California, United States is the largest onshore wind farm outside of China, with a capacity of 1020 MW of power. As of February 2012, the Walney Wind Farm in the United Kingdom is the largest offshore wind farm in the world at 367 MW, followed by Thanet Offshore Wind Project (300 MW), also in the United Kingdom. As of February 2012, the Fântânele-Cogealac Wind Farm in Romania is the largest onshore wind farm in Europe at 600 MW.\n\nThere are many large wind farms under construction and these include Sinus Holding Wind Farm (700 MW), Anholt Offshore Wind Farm (400 MW), BARD Offshore 1 (400 MW), Clyde Wind Farm (350 MW), Greater Gabbard wind farm (500 MW), Lincs Wind Farm (270 MW), London Array (1000 MW), Lower Snake River Wind Project (343 MW), Macarthur Wind Farm (420 MW), Shepherds Flat Wind Farm (845 MW), and Sheringham Shoal (317 MW).\n\nWind power in Denmark produced the equivalent of 42.1% of total electricity consumption in 2015, however, use of wind for heating is minor.\n\nIn 2017, solar power provided 1.7% of total worldwide electricity production, growing at 35% per annum.\nBy 2020 the solar contribution to global final energy consumption is expected to exceed 1%.\n\nSolar photovoltaic cells convert sunlight into electricity and many solar photovoltaic power stations have been built. The size of these stations has increased progressively over the last decade with frequent new capacity records.\n\nAs of January 2013, the largest individual photovoltaic (PV) power plants in the world are Agua Caliente Solar Project, (Arizona, over 247 MW connected to increase to 397 MW), Golmud Solar Park (China, 200 MW), Mesquite Solar project (Arizona, 150 MW), Neuhardenberg Solar Park (Germany, 145 MW), Templin Solar Park (Germany, 128 MW), Toul-Rosières Solar Park (France, 115 MW), and Perovo Solar Park (Ukraine, 100 MW). The Charanka Solar Park is a collection of solar power stations of which 214 MW were reported complete in April 2012, on a 2000 ha site. It is part of Gujarat Solar Park, a group of solar farms at various locations in the Gujarat state of India, with overall capacity of 702 MW. There are a total of 570 MW of solar parks in Golmud, with 500 MW more expected in 2012.\n\nMany large plants are under construction. The Desert Sunlight Solar Farm is a 550 MW solar power plant under construction in Riverside County, California, that will use thin-film solar photovoltaic modules made by First Solar. The Topaz Solar Farm is a 550 MW photovoltaic power plant, being built in San Luis Obispo County, California. The Blythe Solar Power Project is a 500 MW photovoltaic station under construction in Riverside County, California. The Agua Caliente Solar Project is a 290 megawatt photovoltaic solar generating facility being built in Yuma County, Arizona. The California Valley Solar Ranch (CVSR) is a 250 megawatt (MW) solar photovoltaic power plant, which is being built by SunPower in the Carrizo Plain, northeast of California Valley. The 230 MW Antelope Valley Solar Ranch is a First Solar photovoltaic project which is under construction in the Antelope Valley area of the Western Mojave Desert, and due to be completed in 2013.\n\nMany of these plants are integrated with agriculture and some use innovative tracking systems that follow the sun's daily path across the sky to generate more electricity than conventional fixed-mounted systems. Solar power plants have no fuel costs or emissions during operation.\n\nConcentrating Solar Power (CSP) systems use lenses or mirrors and tracking systems to focus a large area of sunlight into a small beam. The concentrated heat is then used as a heat source for a conventional power plant. A wide range of concentrating technologies exists; the most developed are the parabolic trough, the concentrating linear fresnel reflector, the Stirling dish and the solar power tower. Various techniques are used to track the Sun and focus light. In all of these systems a working fluid is heated by the concentrated sunlight, and is then used for power generation or energy storage.\n\nBiofuels, in the form of liquid fuels derived from plant materials, are entering the market. However, many of the biofuels that are currently being supplied have been criticised for their adverse impacts on the natural environment, food security, and land use.\n\nBiomass is biological material from living, or recently living organisms, most often referring to plants or plant-derived materials. As a renewable energy source, biomass can either be used directly, or indirectly once or converted into another type of energy product such as biofuel. Biomass can be converted to energy in three ways: \"thermal conversion\", \"chemical conversion\", and \"biochemical conversion\".\n\nUsing biomass as a fuel produces air pollution in the form of carbon monoxide, carbon dioxide, NOx (nitrogen oxides), VOCs (volatile organic compounds), particulates and other pollutants at levels above those from traditional fuel sources such as coal or natural gas in some cases (such as with indoor heating and cooking). Utilization of wood biomass as a fuel can also produce fewer particulate and other pollutants than open burning as seen in wildfires or direct heat applications. Black carbon a pollutant created by combustion of fossil fuels, biofuels, and biomass is possibly the second largest contributor to global warming. In 2009 a Swedish study of the giant brown haze that periodically covers large areas in South Asia determined that it had been principally produced by biomass burning, and to a lesser extent by fossil fuel burning. Denmark has increased the use of biomass and garbage, and decreased the use of coal.\n\nThe 2014 Intergovernmental Panel on Climate Change report identifies nuclear energy as one of the technologies that can provide electricity with less than 5% of the lifecycle greenhouse gas emissions of coal power. There are more than 60 nuclear reactors shown as under construction in the list of Nuclear power by country with China leading at 23. Globally, more nuclear power reactors have closed than opened in recent years but overall capacity has increased. China has stated its plans to double nuclear generation by 2030. India also plans to greatly increase its nuclear power.\n\nSeveral countries have enacted laws to cease construction on new nuclear power stations. Several European countries have debated nuclear phase-outs and others have completely shut down some reactors. Three nuclear accidents have influenced the slowdown of nuclear power: the 1979 Three Mile Island accident in the United States, the 1986 Chernobyl disaster in the USSR, and the 2011 Fukushima nuclear disaster in Japan. Following the March 2011 Fukushima nuclear disaster, Germany has permanently shut down eight of its 17 reactors and pledged to close the rest by the end of 2022. Italy voted overwhelmingly to keep their country non-nuclear. Switzerland and Spain have banned the construction of new reactors. Japan's prime minister has called for a dramatic reduction in Japan's reliance on nuclear power. Taiwan's president did the same. Shinzō Abe, the new prime minister of Japan since December 2012, announced a plan to restart some of the 54 Japanese nuclear power plants and to continue some nuclear reactors under construction.\n\nAs of 2016, countries such as Australia, Austria, Denmark, Greece, Malaysia, New Zealand, and Norway have no nuclear power stations and remain opposed to nuclear power. Germany, Italy, Spain and Switzerland are phasing-out their nuclear power.\n\nMoving away from fossil fuels will require changes not only in the way energy is supplied, but in the way it is used, and reducing the amount of energy required to deliver various goods or services is essential. Opportunities for improvement on the demand side of the energy equation are as rich and diverse as those on the supply side, and often offer significant economic benefits.\n\nA sustainable energy economy requires commitments to both renewables and efficiency. Renewable energy and energy efficiency are said to be the \"twin pillars\" of sustainable energy policy. The American Council for an Energy-Efficient Economy has explained that both resources must be developed in order to stabilize and reduce carbon dioxide emissions:\n\nEfficiency is essential to slowing the energy demand growth so that rising clean energy supplies can make deep cuts in fossil fuel use. If energy use grows too fast, renewable energy development will chase a receding target. Likewise, unless clean energy supplies come online rapidly, slowing demand growth will only begin to reduce total emissions; reducing the carbon content of energy sources is also needed.\nThe IEA has stated that renewable energy and energy efficiency policies are complementary tools for the development of a sustainable energy future, and should be developed together instead of being developed in isolation.\n\n\n"}
{"id": "1005936", "url": "https://en.wikipedia.org/wiki?curid=1005936", "title": "Fundamental theorem", "text": "Fundamental theorem\n\nThe fundamental theorem of a field of mathematics is the theorem considered central to that field. The naming of such a theorem is not necessarily based on how often it is used or the difficulty of its proofs.\n\nFor example, the fundamental theorem of calculus gives the relationship between differential calculus and integral calculus, which are two distinct branches that are not obviously related. Being \"fundamental\" does not necessarily mean that it is the most basic result. For example, the proof of the fundamental theorem of arithmetic requires Euclid's lemma, which in turn requires Bézout's identity.\n\nThe names are mostly traditional, so that for example the \"fundamental theorem of arithmetic\" is basic to what would now be called number theory.\n\nThe mathematical literature sometimes refers to the fundamental lemma of a field. The term lemma is conventionally used to denote a proven proposition which is used as a stepping stone to a larger result rather than as a useful statement in-and-of itself. The fundamental lemma of a field is often, but not always, the same as the fundamental theorem of that field.\n\n\n\nThere are also a number of \"fundamental theorems\" not directly related to mathematics:\n\n"}
{"id": "34518735", "url": "https://en.wikipedia.org/wiki?curid=34518735", "title": "Geochemical modeling", "text": "Geochemical modeling\n\nGeochemical modeling is the practice of using chemical thermodynamics, chemical kinetics, or both, to analyze the chemical reactions that affect geologic systems, commonly with the aid of a computer. It is used in high-temperature geochemistry to simulate reactions occurring deep in the Earth's interior, in magma, for instance, or to model low-temperature reactions in aqueous solutions near the Earth's surface, the subject of this article.\n\nGeochemical modeling is used in a variety of fields, including environmental protection and remediation, the petroleum industry, and economic geology. Models can be constructed, for example, to understand the composition of natural waters; the mobility and breakdown of contaminants in flowing groundwater or surface water; the formation and dissolution of rocks and minerals in geologic formations in response to injection of industrial wastes, steam, or carbon dioxide; and the generation of acidic waters and leaching of metals from mine wastes.\n\nGarrels and Thompson (1962) first applied chemical modeling to geochemistry in 25 °C and one atmosphere total pressure. Their calculation, computed by hand, is now known as an \"equilibrium model\", which predicts species distributions, mineral saturation states, and gas fugacities from measurements of bulk solution composition. By removing small aliquots of solvent water from an equilibrated spring water and repeatedly recalculating the species distribution, Garrels and Mackenzie (1967) simulated the reactions that occur as spring water evaporated. This coupling of mass transfer with an equilibrium model, known as a \"reaction path model\", enabled geochemists to simulate reaction processes.\n\nHelgeson (1968) introduced the first computer program to solve equilibrium and reaction path models, which he and coworkers used to model geological processes like weathering, sediment diagenesis, evaporation, hydrothermal alteration, and ore deposition. Later developments in geochemical modeling included reformulating the governing equations, first as ordinary differential equations, then later as algebraic equations. Additionally, chemical components came to be represented in models by aqueous species, minerals, and gases, rather than by the elements and electrons which make up the species, simplifying the governing equations and their numerical solution.\n\nRecent improvements in the power of personal computers and modeling software have made geochemical models more accessible and more flexible in their implementation. Geochemists are now able to construct on their laptops complex reaction path or reactive transport models which previously would have required a supercomputer.\n\nAn aqueous system is uniquely defined by its chemical composition, temperature, and pressure. Creating geochemical models of such systems begins by choosing the basis, the set of aqueous species, minerals, and gases which are used to write chemical reactions and express composition. The number of basis entries required equals the number of components in the system, which is fixed by the phase rule of thermodynamics. Typically, the basis is composed of water, each mineral in equilibrium with the system, each gas at known fugacity, and important aqueous species. Once the basis is defined, a modeler can solve for the equilibrium state, which is described by mass action and mass balance equations for each component.\n\nIn finding the equilibrium state, a geochemical modeler solves for the distribution of mass of all species, minerals, and gases which can be formed from the basis. This includes the activity, activity coefficient, and concentration of aqueous species, the saturation state of minerals, and the fugacity of gases. Minerals with a saturation index (log Q/K) equal to zero are said to be in equilibrium with the fluid. Those with positive saturation indices are termed supersaturated, indicating they are favored to precipitate from solution. A mineral is undersaturated if its saturation index is negative, indicating that it is favored to dissolve.\n\nGeochemical modelers commonly create reaction path models to understand how systems respond to changes in composition, temperature, or pressure. By configuring the manner in which mass and heat transfer are specified (i.e., open or closed systems), models can be used to represent a variety of geochemical processes. Reaction paths can assume chemical equilibrium, or they can incorporate kinetic rate laws to calculate the timing of reactions. In order to predict the distribution in space and time of the chemical reactions that occur along a flowpath, geochemical models are increasingly being coupled with hydrologic models of mass and heat transport to form reactive transport models. Specialized geochemical modeling programs that are designed as cross-linkable re-entrant software objects enable construction of reactive transport models of any flow configuration.\n\nGeochemical models are capable of simulating many different types of reactions. Included among them are: \n\nSimple phase diagrams or plots are commonly used to illustrate such geochemical reactions. Eh-pH (Pourbaix) diagrams, for example, are a special type of activity diagram which represent acid-base and redox chemistry graphically.\n\nVarious sources can contribute to a range of simulation results. The range of the simulation results is defined as model uncertainty. One of the most important sources not possible to quantify is the conceptual model, which is developed and defined by the modeller. Further sources are the parameterization of the model regarding the hydraulic (only when simulating transport) and mineralogical properties. The parameters used for the geochemical simulations can also contribute to model uncertainty. These are the applied thermodynamic database and the parameters for the kinetic minerals dissolution. Differences in the thermodynamic data (i.e. equilibrium constants, parameters for temperature correction, activity equations and coefficients) can result in large uncertainties. Furthermore, the large spans of experimentally derived rate constants for minerals dissolution rate laws can cause large variations in simulation results. Despite this is well-known, uncertainties are not frequently considered when conducting geochemical modelling.\n\nReducing uncertainties can be achieved by comparison of simulation results with experimental data, although experimental data does not exist at every temperature-pressure condition and for every chemical system. Although such a comparison or calibration can not be conducted consequently the geochemical codes and thermodynamic databases are state-of-the-art and the most useful tools for predicting geochemical processes.\n\n\n\n"}
{"id": "5930560", "url": "https://en.wikipedia.org/wiki?curid=5930560", "title": "History of pawnbroking", "text": "History of pawnbroking\n\nThe history of pawnbroking began in the earliest ages of the world. Lending money on portable security is one of the oldest professions.\n\nThe Mosaic Law struck at the root of pawnbroking as a profitable business, since it forbade the taking of interest from a poor borrower, while no Jew was to pay another for timely accommodation. Jews were not, however, forbidden to lend money at interest to gentiles. Until the Protestant Reformation, Christians were likewise forbidden to lend to each other at interest. They too have done so and continue to do so against biblical principle.\n\nThe earliest pawnbrokers were in the 5th Century and were established, owned, and operated by Buddhist monasteries; only later did they become more widely seen in society. They were described in many different terms. The word \"changshengku\" (长生库, long-life treasuries) originally referred to Buddhist monasteries in general. Other terms included \"jifupu\" (seen in Tang texts), \"didangku\" (Song period), and \"jiedianku\" (Yuan period). During the Southern Song dynasty, wealthy laypeople would sometimes form partnerships with Buddhist monasteries and open pawnshops (in doing so they had also managed to avoid certain property taxes from which monasteries were sometimes exempt from). For example, a 1202 memorial records the practice of ten people coming together to form an association known as a \"ju (局)\", which would then back the establishment of a pawnshop in a monastery.\"\n\nPawning was common in Ancient Greece and Ancient Rome; indeed, most of the contemporary law on the subject is derived from the Roman jurisprudence. The chief difference between Roman and English law is that under Roman law certain things, such as wearing apparel, furniture, and instruments of tillage, could not be pledged, whereas there is no such restriction in English legislation. The emperor Augustus converted the surplus arising to the state from the confiscated property of criminals into a fund from which the state lent money without interest to those who pledged valuables equal to double the amount borrowed.\n\nIn Italy, but in more modern times, the pledge system that became almost universal on the continent of Europe arose. In its origin that system was purely benevolent, the early \"monts de piete\" established by the authority of the popes lending money to the poor only, without interest, on the sole condition of the advances being covered by the value of the pledges. This was virtually the Augustan system, but it is obvious that an institution that costs money to manage and derives no income from its operations must either limit its usefulness to the extent of the voluntary support it can command, or must come to a speedy end.\n\nIn 1198 something of the kind was started at Freising in Bavaria.\n\nIn 1350 a similar endeavour was made at Salins in Franche-Comté, where interest at the rate of 7½% was charged.\n\nNor was England backward, for in 1361 Michael Northbury, or de Northborough, bishop of London, bequeathed 1000 silver marks for the establishment of a free pawnshop.\n\nThese primitive efforts, like the later Italian ones, all failed. The Vatican was therefore constrained to allow the \"Sacri monti di pietà\" (satisfactory derivation of the phrase has yet been suggested) to charge sufficient interest to their customers to enable them to pay for expenses. Thereupon a learned and tedious controversy arose upon the lawfulness of charging interest, which was only finally set at rest by Pope Leo X, who in the tenth sitting of the First Council of the Lateran, declared that the pawnshop was a lawful and valuable institution, and threatened with excommunication those who should presume to express doubts on the subject. The Council of Trent inferentially confirmed this decision, and at a somewhat later date we find St Charles Borromeo counselling the establishment of state or municipal pawnshops.\n\nLong before this, however, \"monti di pietà\" commonly charged interest for loans in Italy. The date of their establishment was not later than 1464, when the earliest of which there appears to be any record in that country—it was at Orvieto—was confirmed by Pius II. Three years later another was opened at Perugia by the efforts of two Franciscans, Barnabus Interamnensis and Fortunatus de Copolis. They collected the necessary capital by preaching, and the Perugian pawnshop was opened with such success that there was a substantial balance of profit at the end of the first year.\n\nThe Dominicans endeavoured to preach down the lending-house, but without avail. Viterbo obtained one in 1469, and Sixtus IV confirmed another to his native town in Savona in 1479.\n\nAfter the death of Brother Barnabus in 1474, a strong impulse was given to the creation of these establishments by the preaching of another Franciscan, Father Bernandino di Feltre, who was in due course canonized. By his efforts monti di pietà were opened at Assisi, Mantua, Parma, Lucca, Piacenza, Padua, Vicenza, Pavia and a number of places of less importance.\n\nAt Florence the veiled opposition of the municipality and the open hostility of the Jews prevailed against him, and it was reserved to Savonarola, a Dominican, to create the first Florentine pawnshop, after the local theologians had declared that there was no sin, even venial, in charging interest. The readiness of the popes to give permission for pawnshops all over Italy, makes it the more remarkable that the papal capital possessed nothing of the kind until 1539, and even then owed the convenience to a Franciscan.\n\nFrom Italy the pawnshop spread gradually all over Europe. Augsburg adopted the system in 1591, Nuremberg copied the Augsburg regulations in 1618, and by 1622 it was established at Amsterdam, Brussels, Antwerp and Ghent. Madrid followed suit in 1705, when a priest opened a charitable pawnshop with a capital of fivepence taken from an alms-box.\n\nIn England the pawnbroker, like so many other distinguished personages, came in with William the Conqueror. Yet, despite the valuable services the class rendered, not infrequently to the Crown itself, the usurer was treated with studied cruelty. Sir Walter Scott's Isaac of York was no mere creation of fiction. These barbarities began, by diminishing the number of Jews in the country, long before Edward's decree of banishment, to make it worth the while of the Lombard merchants to settle in England. In 1338 Edward III pawned his jewels to the Lombards to raise money for his war with France. An equally great king Henry V did much the same in 1415.\n\nThe Lombards were not a popular class, and Henry VII harried them a good deal. In the very first year of James I's reign an Act against Brokers was passed and remained on the statute-book until 1872. It was aimed at counterfeit brokers, of whom there were then many in London. This type of broker was evidently regarded as a mere receiver of stolen goods, for the act provided that no sale or pawn of any stolen jewels, plate or other goods to any pawnbroker in London or Westminster or Southwark shall alter the property therein, and that pawnbrokers refusing to produce goods to their owner from whom stolen shall forfeit double the value.\n\nIn the time of Charles I another act made it clear that the pawnbroker was not deemed a respectable or trustworthy person. Nevertheless, a plan was mooted for setting that king up in the business. The Civil War was approaching and supplies were badly needed, when a too ingenious Royalist proposed the establishment of a state pawnhouse. The preamble of the scheme recited how \"the intolerable injuries done to the poore subjects by brokers and usurers that take 30, 40, 50, 60, and more in the hundredth, may be remedied and redressed, the poor thereby greatly relieved and eased, and His Majestie much benefited\". That the king would have been much benefited is obvious, since he was to enjoy two-thirds of the profits, while the working capital of £100,000 was to be found by the city of London. The reform of what Shakespeare calls \"broking pawn\" was in the air at that time, although nothing ever came of it, and in the early days of the commonwealth it was proposed to establish a kind of \"mont de pieté\". The idea was emphasized in a pamphlet of 1651 entitled \"Observatsons manifesting the Conveniency and Commodity of Mount Pieteyes, or Public Bancks for Relief of the Poor or Others in Distress, upon Pawns\". No doubt many a ruined cavalier would have been glad enough of some such means of raising money, but this radical change in the principles of English pawnbroking was never brought about. It is said that the Bank of England, under its charter, has power to establish pawnshops; and we learn from \"A Short History of the Bank of England\", published in its very early days, that it was the intention of the directors, for the ease of the poor, to institute a Lombard for small pawns at a penny a pound interest per month (= 5% per year).\n\nThroughout both the 17th and 18th centuries the general suspicion of the pawnbroker appears to have been only too well founded. During George II's reign Fielding wrote a book \"Amelia\"; references in that book seem to show that, taken in the mass, Fielding was not a very scrupulous tradesman. Before about that time it was customary for publicans to lend money on pledges so their customers could drink, but the practice was at last stopped by Act of Parliament.\n\nAlso, respect for the honesty of pawnbroking was harmed by the Charitable Corporation's actions, during which affair it was pointed out that pawnbroking, by affording an easy method of raising money upon valuables, encourages dishonesty, by:-\n\nThe pawnbroker's licence dates from 1785, the duty being fixed at £10 in London and £5 in the country; and at the same time the interest chargeable was settled at ½% per Modern month, the duration of loans being confined to one Regulations year. Five years later the interest on advances in England between 2 and 10 was raised to 15%.\n\nThe modern history of legislation affecting pawnbroking begins, however, in 1800, when the act of ~9 & 40 Geo. III. c. 99 (1800) was passed, in great measure by the influence of Lord Eldon, who never made any secret of the fact that, when he was a young barrister without briefs, he had often been indebted to the timely aid of the pawnshop. The pawnbrokers were grateful, and for many years after Lord Eldon's death they continued to drink his health at their trade dinners. The measure increased the rate of interest to % per month, which is 20% a year unless unpaid interest adds to the debt. Loans were to be granted for a year, but pledges might be redeemed up to fifteen months, and the first week of the second month was not to count for interest.\n\nThe act worked well, on the whole, for 75 years, but it was amended 3 times:-\n\nAs time went on, however, the main provisions of the act of 1800 were found to be very irksome, and the Pawnbrokers National Association and the Pawnbrokers Defence Association worked hard to obtain a liberal revision of the law. It was argued that the usury laws had been abolished for the whole of the community except only for the pawnbroker who advanced less than £10. The limitations of the act of 1800 interfered so considerably with the pawnbrokers' profits that, it was argued, they could not afford to lend money on bulky articles needing extensive storage room. In 1870 the House of Commons appointed a Select Committee on Pawnbrokers, and it was stated in evidence before that body that in the previous year 207,780,000 pledges were lodged, of which between thirty and forty millions were lodged in London. The average value of pledges appeared to be about 4s., and it was claimed that the proportion of articles pawned dishonestly was found to be only 1 in 14,000. Later official statistics show that of the forfeited pledges sold in London less than 20 per million are claimed by the police.\n\nThe result of the Select Committee was the Pawnbrokers Act of 1872, which repealed, altered and consolidated all previous legislation on the subject, and is still the measure which regulates the relations between the public and pawnbrokers. It was based mainly on the Irish law passed by the Union Parliament. It put an end to the old irritating restrictions, and reduced the annual tax in London from £15 to the £7.50 paid in the provinces. By the provisions of the Act (which does not affect loans above £10):-\n\nElaborate provisions are made to safeguard the interests of borrowers whose unredeemed pledges are sold under the act. Thus the sales by auction may take place only on the first Monday of January, April, July and October, and on the following days should one day not be sufficient. This legislation was, no doubt, favorable to the pawnbroker rather than to the borrower. The annual interest on loans of 2s. had been increased by successive acts of parliament from 6% in 1784 to 25% in 1800, and to 27% in 1860, and re-set at 27% in 1872. The annual interest on a loan of half-a-crown (now 12½p) is now 260%, as compared with 173 in 1860 and 86 in 1784; the extreme point is reached in the case of a loan of 1s. for three days, in which case the interest is at the rate of 1014% per annum.\n\nAn English \"mont de piété\" was once projected by the Salvation Army, and in 1894 the London County Council considered the practicability of municipal effort on similar lines; but in neither case was anything done.\n\nThe growth of pawnbroking in Scotland, where the law as to pledge agrees generally with that of England, is remarkable.\n\nEarly in the 19th century there was only one pawnbroker in Ireland, and in 1833 there were only 52. Even in 1865 there were no more than 312. It is probable that Glasgow and Edinburgh together contain nearly as many as that total. In Ireland the rates for loans are practically identical with those charged in England, but a penny instead of a halfpenny is paid for the ticket. Articles pledged for less than Li must be redeemed within six months, but nine months are allowed when the amount is between 3os. and 2. For sums over 2 the period is a year, as in England. In Ireland, too, a fraction of a month is calculated as a full month for purposes of interest, whereas in England, after the first month, fortnights are recognized. In 1838 there was an endeavour to establish monts de piété in Ireland, but the scheme was so unsuccessful that in 1841 the eight charitable pawnshops that had been opened had a total adverse balance of 5340. By 1847 only three were left, and eventually they collapsed likewise.\n\nThe pawnbroker in the United States is, generally speaking, subject to considerable legal restriction, but violations of the laws and ordinances are frequent. Each state has its own regulations, but those of New York and Massachusetts may be taken as fairly representative.\n\nBrokers of pawn are usually licensed by the mayors, or by the mayors and aldermen, but in Boston the police commissioners are the licensing authority. In the state of New York permits are renewable annually on payment of $500, and the pawnbroker must file a surety bond with the Department of Consumer Affairs, in the sum of $10,000. The business is conducted on much the same lines as in England. The rate of interest is 4% per month and the loan is written for a period of 4 months, with an additional 30-day grace period. To enact higher rates is a misdemeanor. Unredeemed pledges may be sold at any time after the legal time for the loan. New York contains one pawnshop to every 12,000 inhabitants. In the state of Massachusetts unredeemed pledges may be sold four months after the date of deposit. The licensing authority may fix the rate of interest, which may vary for different amounts, and in Boston every pawnbroker is bound to furnish to the police daily a list of the pledges taken in during the preceding twenty-four hours, specifying the hour of each transaction and the amount lent.\n\nMajor pawnshop chains include Cash America International and First Cash Financial Services, both headquartered in Texas.\n\nThe fact that on the continent of Europe monts de piété are almost invariably either a state or a municipal monopoly necessarily places them upon an entirely different footing from the British pawnshop, but, compared with the English system, the foreign is very elaborate and rather cumbersome. Moreover, in addition to being slow in its operation, it is, generally speaking, based upon the supposition that the borrower carries in his pockets papers testifying to his identity. On the other hand, it is argued that the English borrower of more than 2 is at the mercy of the pawnbroker in the matter of interest, that sum being the highest for which a legal limit of interest is fixed. The rate of interest upon a special contract may be, and often is, high. For the matter of that, indeed, this system of obtaining loans is always expensive, either in actual interest or in collateral disadvantages, whether the lender be a pawnbroker intent upon profit, or the official of a mont de pit. In Paris the rate charged is 7%, and even then the business is conducted at a loss except in regard to long and valuable pledges. Some of the French provincial rates are as high as 12%, but in almost every case they are less than they were prior to the legislation of 1851 and 1852. The French establishments can only be created by decree of the president of the Republic, with the consent of the local conseil communal. In Paris the prefect of the Seine presides over the business; in the provinces the mayor is the president. The administrative council is drawn one-third each from the conseil communal, the governors of charitable societies, and the townspeople. A large proportion of the capital required for conducting the institutions has to be raised by loan, while some part of the property they possess is the product of gifts and legacies. The profits of the Paris mont de pit are paid over to the Assistance Publique, the comprehensive term used by France to indicate the body of charitable foundations. Originally this was the rule throughout France, but now many of them are entirely independent of the charitable institutions. Counting the head office, the branches and the auxiliary shops, the Paris establishment has its doors open in some fifty or sixty districts; but the volume of its annual business is infinitely smaller than that transacted by the London pawnbrokers. The amount to be advanced by a municipal pawnshop is fixed by an official called the commissaire-priseur, who is compelled to load the scales against the borrower, since, should the pledge remain unredeemed and be sold for less than was lent upon it, he has to make good the difference. This official is paid at the rate of 3/4% upon loans and renewals, and 3% on the amount obtained by the sales of forfeited pledges. This is obviously the weakest part of the French system. The Paris mont de pit undertakes to lend four-fifths of the intrinsic value of articles made from the precious metals, and two-thirds of that of other articles. The maximum and minimum that may be advanced are also fixed. The latter varies in different parts of the country from one to three francs, and the former from a very small sum to the 10,000 francs that is the rule in Paris. Loans are granted for twelve months with right of renewal, and unredeemed pledges may then be sold by auction, but the proceeds may be claimed by the borrower at any time within three years. Pledges may be redeemed by instalments.\n\nSomewhere between forty and fifty French towns possess municipal pawnshops, a few of which, like those of Grenoble and Montpellier, having been endowed, charge no interest. Elsewhere the rate varies from nil in some towns, for very small pledges, to 10%. The constant tendency throughout France has been to reduce the rate. The great establishment in Paris obtains part of its working capital reserves and surplus forming the balance by borrowing money at a rate varying from 2 to 3% according to the loan term (duration). Under a law passed in 18gf the Paris mont de pit makes advances upon securities at 6%, plus a duty of 5 centimes upon every hundred francs. The maximum that can be lent in this way is 20. Up to 80% is lent on the face value of government stock and on its own bonds, and 75% upon other securities; but 60% only may be advanced on railway shares. These advances are made for six months. Persons wishing to borrow a larger sum than sixteen francs from the Paris mont de pit have to produce their papers of identity. In every case a numbered metal check is given to the customer, and a duplicate is attached to the article itself. The appraising clerks decide upon the sum that can be lent, and the amount is called out with the number. If the borrower is dissatisfied he can take away his property, but if he accepts the offer he has to give I till particulars of his name, address and occupation. Experts calculate that every transaction that involves less than twenty-two francs results in a loss to the Paris mont de pit—only those that exceed eighty-five francs are profitable. The average loan is under thirty francs.\n\nBorrowing money on the security of deposited goods has been the subject of minute regulations in the Low Countries from as far back as the year 1600.\n\nThe archdukes Albert and Isabella, governors of the Spanish Netherlands under Philip III, reduced the lawful rate of interest from 32 3/4 to 21 3/4%; but since extortion continued they introduced the mont de pit in 1618, and, as we have already seen, in the course of a dozen years the institution was established in all the populous Belgian towns, with one or two exceptions. The interest chargeable to borrowers was fixed originally at 15%, but was shortly afterwards reduced, to be again increased to nearly the old level. Meanwhile, various towns possessed charitable funds for gratuitous loans, apart from the official institutions. Shortly after the mont de pit was introduced in the Spanish provinces, the prince-bishop of Liège, Ferdinand of Bavaria, followed the example set by the archdukes. He ordained that the net profits were to accumulate, and the interest upon the fund to be used in reduction of the charges. The original rate was 15%, when the Lombard money-lenders had been charging 43; but the prince-bishops monts de pit were so successful that for many years their rate of interest did not exceed 5% - it was, indeed, not until 1788 that it was increased by one-half. These flourishing institutions, along with those in Belgium proper, were ruined by the French Revolution. They were, however, re-established under French dominion, and for many years the laws governing them were constantly altered by the French, Dutch and Belgian governments in turn. The whole subject is now regulated by a law of 1848, supplemented by a new constitution for the Brussels mont de pit dating from 1891.\n\nThe working capital of these official pawnshops is furnished by charitable institutions or the municipalities, but the Brussels one possesses a certain capital of its own in addition. The rate of interest charged in various parts of the country varies from 4 to 16%, but in Brussels it is usually less than half the maximum. The management is very similar to that of the French monts de pit, but the arrangements are much more favorable to the borrower. The ordinary limit of loans is I2o. In Antwerp there is an anonymous pawnshop, where the customer need not give his name or any other particulars. In the Netherlands private pawnbrokers flourish side by side with the municipal Banken van Leening, nor are there any limitations upon the interest that may be charged. The rules of the official institutions are very similar to those of the monts de pit in the Latin countries, and unredeemed pledges are sold publicly 15 months after being pawned. A large proportion of the advances are made upon gold and diamonds; workmen's tools are not taken in pledge, and the amount lent varies from 8d. upwards. On condition of finding such sum of money as may be required for working capital over and above loans from public institutions, and the caution money deposited by the city officials, the municipality receives the profits.\n\nPawnbroking in Germany is conducted at once by the state, by the municipalities, and by private enterprise; but of all these institutions the state loan office in Berlin is the most interesting. It dates from 1834, and the working capital was found, and still continues to be in part provided, by the Prussian State Bank. The profits are invested, and the interest devoted to charitable purposes. The maximum and minimum rates of interest are fixed, but the rate varies, and often stands at about 12%. Two-thirds of the estimated value is the usual extent of a loan; four-fifths is advanced on silver, and five-sixths on fine gold. State and municipal bonds may be pledged up to a maximum of L150, the advance being 80% of the value, and a fixed interest of 6% is charged upon these securities. The values are fixed by professional valuers, who are liable to make good any loss that may result from over-estimation. The bulk of the loans are under 5, and the state office is used less by the poor than by the middle classes. Loans run for six months, but a further six months' grace is allowed for redemption before the article pledged can be sold by auction. The net annual profit usually amounts to little more than I % upon the capital employed. Pawnbroking laws of Austria-Hungary are similar to those of England. Free trade exists, and the private trader, who does most of the business, must obtain a government concession and deposit caution-money from 80 to 800, according to the size of the town. He must, however, compete with the monts de pit or Versatzaemter, which are sometimes municipal and sometimes state institutions. The chief of these is the imperial pawn office of Vienna, which was founded with charitable objects by the emperor Joseph I in 1707, and one-half of the annual surplus has still to be paid over to the Vienna poor fund. Here, as in Berlin, the profits are relatively small. Interest is charged at the uniform rate of 10%, which is calculated in two-week periods, however speedily redemption may follow upon pawning. For small loans varying from two to three kronen, 5% only is charged. The Hungarian state and municipal institutions appear, on the whole, to compete somewhat more successfully with the private firms than is the case in Vienna.\n\nIn Italy, the country of origin of the mont de piété, the institution still flourishes. It is, as a rule, managed by a committee or commission, and the regulations follow Italy pretty closely the lines of the one in Rome, which never lends less than fod. or more than 40. Four-fifths of the value is lent upon gold, silver and jewels, and two-thirds upon other articles. The interest, which is reckoned monthly, varies with the amount of the loan from 5 to 7%, but no interest is chargeable upon loans up to 5 lire. A loan runs for six months, and may be renewed for similar periods up to a maximum of five years. If the renewal does not take place within a fortnight of the expiration of the ticket, the pledge is sold, any surplus there may be being paid to the pawner. When more than 10 lire are lent there is a charge of 1% for the ticket. Agencies of the mont de pit are scattered about Rome, and carry on their business under the same rules as the central office, with the disadvantage to the borrower that he has to pay an agents fee. The amount to be advanced by a municipal pawnshop is fixed by an official called the commissaire-priseur, who is compelled to load the scales against the borrower, since, should the pledge remain unredeemed and be sold for less than was lent upon it, he has to make good the difference. This official is paid at the rate of 3/4% upon loans and renewals, and 3% on the amount obtained by the sales of forfeited pledges. The borrower has to pay an agents fee of 2%, which is deducted from the loan. Private pawnshops also exist in Italy, under police authority; but they charge very high interest.\n\nThe institution was very slow in obtaining a footing in France. It was adopted at Avignon in 1577, and at Arras in 1624. The doctors of the once powerful Sorbonne could not reconcile themselves to the lawfulness of interest, and when a pawnshop was opened in Paris in 1626, it had to be closed within a year. Then it was that Jean Boucher published his \"Défense des monts de piété\" in favor of pawnbroking. Marseilles obtained one in 1695; but it was not until 1777 that the first mont de pit was founded in Paris by royal patent. Statistics for the first few years of its existence show that in the twelve years between 1777 and the Revolution, the average value of the pledges was 42 francs 50 centimes, which is double the present average. The interest charged was 10% per annum, and large profits were made upon the sixteen million livres that were lent every year. The National Assembly, in an evil moment, destroyed the monopoly of the monti de pietà, but it struggled on until 1795, when the competition of the money-lenders compelled it to close its doors. So great, however, were the extortions of the usurers that the people began to clamour for its reopening, and in July 1797 it recommenced business with a fund of ~20,000 found by five private capitalists. At first it charged interest at the rate of 36% per annum, which was gradually reduced, the gradations being 30, 24, 18, 15, and finally 12% in 1804. In i806 it fell to 9%, and in 1887 to 7%. In 1806 Napoleon I re-established its monopoly, while Napoleon III, as prince-president, regulated it by new laws that are still in force. In Paris the pledge-shop is, in effect, a department of the administration; in the French provinces it is a municipal monopoly; and this remark holds good, with modifications, for most parts of the continent of Europe.\nThe Paris mont de piété undertakes to lend four-fifths of the intrinsic value of articles made from the precious metals, and two-thirds of that of other articles. The maximum and minimum that may be advanced are also fixed. The latter varies in different parts of the country from one to three francs, and the former from a very small sum to 10,000 francs that is the rule in Paris. Loans are granted for twelve months with right of renewal, and unredeemed pledges may then be sold by auction, but the proceeds may be claimed by the borrower at any time within three years. Pledges may be redeemed by instalments.\n\nThe monts de piété in Spain have for a generation past been inseparably connected with the savings banks. We have already seen that the institution owes its origin in that country to the charitable exertions of a priest who charged no interest, and the system grew until in 1840, a century after his death, the \"mont de pit\" began to receive the sums deposited in the savings bank, which had just been established, for which it paid 5% interest. In 1869 the two institutions were united. This official pawnshop charges 6% upon advances for periods varying from four to twelve months, according to the nature of the article pledged, and a further months grace is allowed before the pledges are sold by auction. Private pawnbrokers are also very numerous, especially in Madrid; but their usual charges amount to about 60% per annum. They appear, however, to derive advantage from making larger advances than their official rivals, and from doing business during more convenient hours. In Portugal the monte pio is an amalgamation of bank, benefit society and pawnshop. Its business consists chiefly in lending money upon marketable securities, but it also makes advances upon plate, jewelry and precious stones, and it employs officially licensed valuers. The rate of interest varies with the bank rate, which it slightly exceeds, and the amount advanced upon each article is about three-fourths of its certified value. There is in Portugal a second class of loan establishment answering exactly to the English pawnshop. The pawnbroker is compelled to deposit a sum, in acceptable securities, equal to the capital he proposes to embark, and the register of his transactions must be submitted quarterly to the chief of the police for examination. As regards small transactions, there appears to be no legal limit to the rate of interest. The sale of unredeemed pledges is governed by the law affecting the monte pio geral.\n\nIn imperial Russia the state maintained two pawnbroking establishments, one at St Petersburg and the other at Moscow, but only articles of gold and silver, precious stones and ingots of the precious metals are accepted by them. Advances are made upon such securities at 6% per annum, and the amounts of the loans are officially limited. Loans run for twelve months, with a months grace before unredeemed pledges are put up to auction. The bulk of this class of business in Russia was, however, conducted by private companies, which advance money upon all descriptions of movable property except stocks and shares. The interest charged was not allowed to exceed I % per month, but there is an additional charge of 4% per month for insurance and safe keeping. The loan runs for a year, with two months grace for redemption before sale. There were also pawnshops conducted by individuals, who find it very difficult to compete with the companies. These shops can only be opened by a police permit, which runs for five years, and security, varying from 100 to 700, has to be deposited; 2% per month is the limit of interest fixed, and two months grace is allowed for redemption after the period for which an article is pledged.\n\nPawnbroking in Denmark dates from 1753, when the Royal Naval Hospital was granted the monopoly of advancing money on pledges and of charging higher interest than the law permitted. The duration of a loan is three months, renewals being allowed. The old law was extended in 1867, and now all pawnbrokers have to be licensed by the municipalities and to pay a small annual licence fee. The rate of interest varies from 6 to 12% according to the amount of the loan, which must not be less than 7d., and unredeemed pledges must be sold by auction.\n\nSweden has no statutes specifically aimed at pawnbroking, with the exception of a proclamation by the governor of Stockholm that prohibits lending money on articles suspected being stolen. Individuals still carry on the business on a small scale, but the bulk of it is now conducted by companies, which give general satisfaction. For many years Stockholm had a municipal establishment that charged 10% for loans paid out of the city funds. The cost of administration was, however, so great that the establishment suffered an annual loss, and was abolished when, in 1880, a private company called the \"Pant Aktie Bank\" 'pawn bond bank' formed to lend money on furniture and wearing apparel at the rate of 3 öre per krona a month, and 2 öre per krona a month on gold, silver and other valuables. A krona, which equals 1s. 14d., contains 100 öre. Some years later an opposition started that charged only half these rates, with the result that the original enterprise reduced its interest to the same level, charging, however, 2 öre per krona per mensem for bulky articles—a figure now usual for pledges of that description. The money is lent for three months, and at the end of five months the pledge, if unredeemed, is sold by auction under very carefully prescribed conditions. In Norway a police licence is required for lending money on pawn where the amount advanced does not exceed 4, 10s. Beyond that sum no licence is necessary, but the interest charged must not exceed such a rate as the king may decide.\n\nThe fate of pawnbroking in Switzerland appears to be not very dissimilar from that of the Jew who is fabled to have once started in business at Aberdeen. Nevertheless, the cantons of Bern and Zürich enacted elaborate laws for the regulation of the business. In Zürich the broker must be licensed by the cantonal government, and the permit can be refused only when the applicant is known to be a person undeserving of confidence. Regular books have to be kept, which must be at all times open to the inspection of the police, and not more than 1% interest per month may be charged. A loan runs for six months, and unredeemed pledges may be sold by auction a month after the expiration of the fixed period, and then the sale must take place in the parish in which the article was pledged. No more than two persons at a time have ever been licensed under this law, the business being unprofitable owing to the low rate of interest. In the canton of Bern there were once two pawnbrokers. One died and the other put up his shutters. The Zürich cantonal bank, however, conducts a pawnbroking department, which lends nothing under 4s. or over £40 without the special sanction of the bank commission. Loans must not exceed two-thirds of the trade value of the pledge, but 80% may be lent upon the intrinsic value of gold and silver articles. The establishment makes practically no profit. The Swiss disinclination to go to the pawnshop is, perhaps, accounted for in some measure by the growing number of dealers in second-hand articles, to whom persons in want of ready money sell outright such things as are usually pledged, in the hope of subsequently buying them back. Since, however, the dealer is at liberty to ask his own price for repurchase, the expectation is often illusory, and can usually be fulfilled only upon ruinous terms.\n\nPawnbroking declined after the Second World War due to an increase in social welfare provision by governments and consumer credit by banks. Many pawnbrokers attempted to modernise their image and diversify their businesses.\n\nSince the 1980s the decline of pawnbroking has stabilised, and even increased in some areas, with many people using pawnbrokers as an alternative to bank or payday loans.\n\nThe pawnbroker's symbol shows three balls suspended from a bar. The origins of the symbol are unclear, but it is often attributed to the Medici Family of Florence, Italy. The family insignia featured rounded objects, which may have represented gold coins or rocks. According to one legend, a Medici employed by Charles the Great slew a giant using three bags of rocks. Since the Medicis were so successful in the financial, banking, and money-lending industries, other families also adopted the symbol. Throughout the Middle Ages, coats of arms bore three balls, orbs, plates, discs, coins and more as symbols of monetary success. \n\nAnother theory attributes the symbol to the Lombards. Pawn shop banking originated under the name of Lombard banking, and many European towns called the pawn shop the \"Lombard\". The three golden balls were originally the symbol medieval Lombard merchants hung up in front of their houses.\n\nA third theory links the symbol to Saint Nicholas of Myra, the patron saint of pawnbrokers. According to legend he gave a bag of gold to three poor girls to save them from destitution, and these three bags of gold became the three orbs of the symbol.\n\nIt has been conjectured that the golden balls were originally three flat yellow effigies of byzants, or gold coins, laid heraldically upon a sable field, but that they were presently converted into balls the better to attract attention.\n\nPawnbrokers (and their detractors) joke that the three balls mean \"Two to one, you won't get your stuff back\".\n\nIn the late twentieth century some pawnbrokers responded to a decline in the industry by removing the traditional symbol.\n\n\n"}
{"id": "27899682", "url": "https://en.wikipedia.org/wiki?curid=27899682", "title": "Hot hand", "text": "Hot hand\n\nThe \"hot hand\" (also known as the \"hot hand phenomenon\" or \"hot hand fallacy\") is the purported phenomenon that a person who experiences a successful outcome with a random event has a greater probability of success in further attempts. The concept is often applied to sports and skill-based tasks in general and originates from basketball, whereas a shooter is allegedly more likely to score if their previous attempts were successful, i.e. while having \"hot hands\". While previous success at a task can indeed change the psychological attitude and subsequent success rate of a player, researchers for many years did not find evidence for a \"hot hand\" in practice, dismissing it as fallacious. However, later research questioned whether the belief is indeed a fallacy. Recent studies using modern statistical analysis show there is evidence for the \"hot hand\" in some sporting activities.\n\nThe fallacy was first described in a 1985 paper by Amos Tversky, Thomas Gilovich, and Robert Vallone. The \"Hot Hand in Basketball\" study questioned the theory that basketball players have \"hot hands\", which the paper defined as the claim that players are more likely to make a successful shot if their previous shot was successful. The study looked at the inability of respondents to properly understand randomness and random events; much like innumeracy can impair a person's judgement of statistical information, the hot hand fallacy can lead people to form incorrect assumptions regarding random events. The three researchers provide an example in the study regarding the \"coin toss\"; respondents expected even short sequences of heads and tails to be approximately 50% heads and 50% tails. The study proposed two biases that are created by the kind of thought pattern applied to the coin toss: it could lead an individual to believe that the probability of heads or tails increases after a long sequence of either has occurred (known as the gambler's fallacy); or it could cause an individual to reject randomness due to a belief that a streak of either outcome is not representative of a random sample.\n\nThe first study was conducted via a questionnaire of 100 basketball fans from the colleges of Cornell and Stanford. The other looked at the individual records of players from the 1980–81 Philadelphia 76ers. The third study analyzed free-throw data and the fourth study was of a controlled shooting experiment. The reason for the different studies was to gradually eliminate external factors around the shot. For example, in the first study there is the factor of how the opposing team's defensive strategy and shot selection would interfere with the shooter. The second and third take out the element of shot selection, and the fourth eliminates the game setting and the distractions and other external factors mentioned before. The studies primarily found that the outcomes of both field goal and free throw attempts are independent of each other. In the later studies involving the controlled shooting experiment the results were the same; evidently, the researchers concluded that the sense of being \"hot\" does not predict hits or misses.\n\nGilovich offers two different explanations for why people believe hot hands exist. The first is that a person may be biased towards looking for streaks before watching a basketball game. This bias would then affect their perceptions and recollection of the game (confirmation bias). The second explanation deals with people's inability to recognize chance sequences. People expect chance sequences to alternate between the options more than they actually do. Chance sequences can seem too lumpy, and are thus dismissed as non-chance (clustering illusion).\n\nThere are many proposed explanations for why people are susceptible to the hot-hand fallacy. Alan D. Castel, and others investigated the idea that age would alter an individual's belief in the fallacy. To test this idea researchers conducted a cross-sectional study where they sampled 455 participants ranging in age from 22 to 90 years old. These participants were given a questionnaire preceded by a prompt that said in college and professional basketball games no players make 100% of their attempted shots. Then the questionnaire asked two important questions: (1) Does a basketball player have a better chance of making a shot after having just made the last two or three shots than after having missed the last two or three shots? (2) Is it important to pass the ball to someone who has just made several shots in a row?\n\nThe main interest of the questionnaire was to see if a participant answered yes to the first question, implying that they believed in the hot-hand fallacy. The results showed that participants over 70 years of age were twice as likely to believe the fallacy than adults 40–49, confirming that the older individuals relied more on heuristic-based processes. Older adults are more likely to remember positive information, making them more sensitive to gains and less to losses than younger adults.\n\nOne study looked at the root of the hot-hand fallacy as being from an inability to appropriately judge sequences. The study compiled research from dozens of behavioral and cognitive studies that examined the hot-hand and gambler's fallacies with random mechanisms and skill-generated streaks. In terms of judging random sequences the general conclusion was that people do not have a statistically correct concept of random. It concluded that human beings are built to see patterns in sensory and conceptual data of all types.\n\nA 2003 study by Koehler, J. J. & Conley C. A. was conducted to examine the hot hand in professional basketball. In this study the researchers examined film from the NBA shooting contests from 1994–1997. Through studying the film of the contests the researchers hoped to find evidence of sequential dependency within each shooter across all shots. They also searched for sequential dependencies within each shooter per set of 25 continuous shots, and employed a variety of novel techniques for isolating hot performance. According to the hot hand a player should have very few runs and instead their hits and misses should be in clusters.\n\nIn their research there were only two players who had a significantly lower number of runs than expected by chance. No shooter had significantly more runs than would be expected by chance. About half of the shooters (12 of 23 = 52%) had fewer runs than expected, and about half (11 of 23 = 48%) had more runs than expected. The researchers also compared the shooters hits and misses. The data were more in accordance with chance than the hot hand. Through their analysis of the data the conclusion was drawn that there was nothing that supported the hot hand hypothesis.\n\nA study reported that a belief in the hot-hand fallacy affects a player's perceptions of success.\n\nMore recent research has questioned the earlier findings, instead finding support for the belief of a hot hand phenomenon.\n\nA 2003 paper from researchers at Monash University noted that Gilovich et al. did not examine the statistical power of their own experiments. By performing power analysis on the 1985 data, the researchers concluded that even if the Philadelphia 76ers did shoot in streaks, it is highly unlikely that Gilovich, Vallone and Tversky would have discovered that fact.\n\nA paper from October 2011 by Yaari and Eisenmann, a large dataset of more than 300,000 NBA free throws were found to show \"strong evidence\" for the \"hot hand\" phenomenon at the individual level. They analyzed all free throws taken during five regular NBA seasons from 2005 to 2010. They found that there was a significant increase in players' probabilities of hitting the second shot in a two-shot series compared to the first one. They also found that in a set of two consecutive shots, the probability of hitting the second shot is greater following a hit than following a miss on the previous one.\n\nIn November 2013, researchers at Stanford University used data from Major League Baseball and found that there was \"strong evidence\" that the hot hand existed in ten different statistical categories.\n\nIn 2014, a paper from three Harvard graduates presented at the Sloan Sports Analytics Conference, which used advanced statistics that for the first time could control for variables in basketball games such as the player's shot location and a defender's position, showed a \"small yet significant hot-hand effect.\"\n\nIn 2015, an examination of the 1985 study by Joshua Miller and Adam Sanjurjo found flaws in the methodology of the 1985 study and showed that, in fact, the hot hands may exist. The researchers said that instead it may be attributable to a misapplication of statistical techniques. The authors concluded that people were right to believe that the hot hand exists in basketball.\n\nThere are places other than sport that can be affected by the hot-hand fallacy. A study conducted by Joseph Johnson et al. examined the characteristics of an individual's buying and selling behavior as it pertained to the hot hand and gambler's heuristic. Both of these occur when a consumer misunderstands random events in the market and is influenced by a belief that a small sample is able to represent the underlying process. To examine the effect of the hot hand and gambler's heuristic on the buying and selling behaviors of consumers, three hypotheses were made. Hypothesis one stated that consumers that were given stocks with positive and negative trends in earning would be more likely to buy a stock that was positive when it was first getting started but would become less likely to do so as the trend lengthened. Hypothesis two was that consumers would be more likely to sell a stock with negative earnings as the trend length initially increased but would decrease as the trend length increased more. Finally, the third hypothesis was that consumers in the buy condition would be more likely to choose a winning stock over those in the selling condition.\n\nThe results of the experiment did not support the first hypothesis but did support hypotheses two and three, suggesting that the use of these heuristics is dependent on buying or selling and the length of the sequence. This means that those who had the shorter length and the buying condition would fall under the influence of the hot-hand fallacy. The opposite would be in accordance with the gambler's fallacy which has more of an influence on longer sequences of numerical information.\n\nA study was conducted to examine the difference between the hot-hand and gambler's fallacy. The gambler's fallacy is the expectation of a reversal following a run of one outcome. Gambler's fallacy occurs mostly in cases in which people feel that an event is random, such as rolling a pair of dice on a craps table or spinning the roulette wheel. It is caused by the false belief that the random numbers of a small sample will balance out the way they do in large samples; this is known as the law of small numbers heuristic. The difference between this and the hot-hand fallacy is that with the hot-hand fallacy an individual expects a run to continue. There is a much larger aspect of the hot hand that relies on the individual. This relates to a person's perceived ability to predict random events, which is not possible for truly random events. The fact that people believe that they have this ability is in line with the illusion of control.\n\nIn this study, the researchers wanted to test if they could manipulate a coin toss, and counter the gambler's fallacy by having the participant focus on the person tossing the coin. In contrast, they attempted to initiate the hot-hand fallacy by centering the participant's focus on the person tossing the coin as a reason for the streak of either heads or tails. In either case the data should fall in line with sympathetic magic, whereby they feel that they can control the outcomes of random events in ways that defy the laws of physics, such as being \"hot\" at tossing a specific randomly determined outcome.\n\nThey tested this concept under three different conditions. The first was person focused, where the person who tossed the coin mentioned that \"she\" was tossing a lot of heads or tails. Second was a coin focus, where the person who tossed the coin mentioned that \"the coin\" was coming up with a lot of heads or tails. Finally there was a control condition in which there was nothing said by the person tossing the coin. The participants were also assigned to different groups, one in which the person flipping the coin changed and the other where the person remained the same.\n\nThe researchers found the results of this study to match their initial hypothesis that the gambler's fallacy could in fact be countered by the use of the hot hand and people's attention to the person who was actively flipping the coin. It is important to note that this counteraction of the gambler's fallacy only happened if the person tossing the coin remained the same. This study shed light on the idea that the gambler's and hot hand fallacies at times fight for dominance when people try to make predictions about the \"same\" event.\n\n\n"}
{"id": "197639", "url": "https://en.wikipedia.org/wiki?curid=197639", "title": "Inquisitorial system", "text": "Inquisitorial system\n\nAn inquisitorial system is a legal system where the court or a part of the court is actively involved in investigating the facts of the case, as opposed to an adversarial system where the role of the court is primarily that of an impartial referee between the prosecution and the defense. Inquisitorial systems are used primarily in countries with civil legal systems as opposed to common law systems. Countries using common law, including the United States, may use an inquisitorial system for summary hearings in the case of misdemeanors such as minor traffic violations. The distinction between an adversarial and inquisitorial system is theoretically unrelated to the distinction between a civil legal and common law system. Some legal scholars consider \"inquisitorial\" misleading, and prefer the word nonadversarial. The function is often vested in the office of the public procurator, as in China, Japan, Germany, and Scotland.\n\nIn an inquisitorial system, the trial judges (mostly plural in serious crimes) are inquisitors who actively participate in fact-finding public inquiry by questioning defense, prosecutors and witnesses. They could even order certain pieces of evidence to be examined if they find presentation by the defense or prosecution to be inadequate.\n\nThe inquisitorial system applies to questions of criminal procedure, not substantive law; that is, it determines how criminal inquiries and trials are conducted, not the kind of crimes for which one can be prosecuted or the sentences that they carry. It is most readily used in some civil legal systems. However, some jurists do not recognize this dichotomy and see procedure and substantive legal relationships as being interconnected and part of a theory of justice as applied differently in various legal cultures.\n\nIn an adversarial system, judges focus on the issues of law and procedure and act as a referee in the contest between the defense and the prosecutor. Juries decide matters of fact, and sometimes matters of the law. Neither judge nor jury can initiate an inquiry, and judges rarely ask witnesses questions directly during trial. In some American jurisdictions, it is common practice for jurors to submit questions to the court that they feel were not resolved in direct or cross-examination. After testimony and other evidence are presented and summarized in arguments, the jury will declare a verdict (literally: \"the spoken truth\") and in some jurisdictions the reasoning behind the verdict. However, the discussions among jurors cannot be made public except in extraordinary circumstances. Appeals on the basis of factual issues, such as sufficiency of the sum total of evidence that was properly admitted, are subject to a standard of review that is in most jurisdictions heavily deferential to the judgment of the fact-finder at trial, be that a judge or a jury. The failure of a prosecutor to disclose evidence to the defense, for example, or a violation of the defendant's constitutional rights (legal representation, right to remain silent, an open and public trial) can trigger a dismissal or re-trial. In some adversarial jurisdictions (e.g., the United States), a prosecutor cannot appeal a \"not guilty\" verdict (absent corruption or gross malfeasance by the court).\n\nIn adversarial systems, the defendant may plead \"guilty\" or \"no contest\" in exchange for reduced sentences, a practice known as plea bargaining, which is an extremely common practice in the United States. In theory, the defendant must allocute or \"voice\" his or her crimes in open court, and the judge must believe the defendant is telling the truth about his or her guilt. In an inquisitorial system, a mere confession of guilt would not be regarded as ground for a guilty verdict, and the prosecutor is required to provide evidence supporting a guilty verdict; however, this requirement is not unique to inquisitorial systems, as many or most adversarial systems impose a similar requirement under the name \"corpus delicti\".\n\nUntil the development of the Medieval Inquisition in the 12th century, the legal systems used in medieval Europe generally relied on the adversarial system to determine whether someone should be tried and whether a person was guilty or innocent. Under this system, unless people were caught in the act of committing crimes, they could not be tried until they had been formally accused by their victim, the voluntary accusations of a sufficient number of witnesses, or by an inquest (an early form of grand jury) convened specifically for that purpose. A weakness of this system was that because it relied on the voluntary accusations of witnesses, and because the penalties for making a false accusation were severe, victims and would-be witnesses could be hesitant to actually make their accusations to the court, for fear of implicating themselves. Because of the difficulties in deciding cases, procedures such as trial by ordeal or combat were accepted.\n\nBeginning in 1198, Pope Innocent III issued a series of decretals that reformed the ecclesiastical court system. Under the new (inquisitional procedure) an ecclesiastical magistrate no longer required a formal accusation to summon and try a defendant. Instead, an ecclesiastical court could summon and interrogate witnesses of its own initiative, and if the (possibly secret) testimony of those witnesses accused a person of a crime, that person could then be summoned and tried. In 1215, the Fourth Council of the Lateran affirmed the use of the inquisitional system. The council also forbade clergy from conducting trials by ordeal or combat. As a result, in parts of continental Europe, the ecclesiastical courts operating under the inquisitional procedure became the dominant method by which disputes were adjudicated. In France, the — lay courts — also employed inquisitorial proceedings.\n\nIn England, however, King Henry II had established separate secular courts during the 1160s. While the ecclesiastical courts of England, like those on the continent, adopted the inquisitional system, the secular common law courts continued to operate under the adversarial system. The adversarial principle that a person could not be tried until formally accused continued to apply for most criminal cases. In 1215 this principle became enshrined as article 38 of the Magna Carta: \"No bailiff for the future shall, upon his own unsupported complaint, put anyone to his law, without credible witnesses brought for this purposes.\"\n\nThe first territory to wholly adapt the inquisitional system was the Holy Roman Empire. The new German legal process was introduced as part of the of 1498 and then the of 1507. The adoption of the ( of Charles V) in 1532 made inquisitional procedures empirical law. It was not until Napoleon introduced the of the French code of criminal procedure on November 16, 1808, that the classical procedures of inquisition were ended in all German territories.\n\nIn the development of modern legal institutions which occurred in the 19th century, for the most part jurisdictions did not only codify their private law and criminal law, but the rules of civil procedure were reviewed and codified as well. It was through this movement that the role of an inquisitorial system became enshrined in most European civilian legal systems. However, there exist significant differences of operating methods and procedures between 18th century courts and 19th-century courts. In particular, limits on the powers of investigators were typically added, as well as increased rights of the defense.\n\nIt would be too much of a generalization to state that the civil law is purely inquisitorial and the common law adversarial. Indeed, the ancient Roman custom of arbitration has now been adapted in many common law jurisdictions to a more inquisitorial form. In some mixed civil law systems such as those in Scotland, Quebec, and Louisiana, while the substantive law is civil in nature and evolution, the procedural codes that have developed over the last several hundred years are based upon the English adversarial system.\n\nThe main feature of the inquisitorial system in criminal justice in France and other countries functioning along the same lines is the function of the examining or investigating judge (\"juge d'instruction\"). The examining judge conducts investigations into serious crimes or complex inquiries. As a member of the judiciary, he or she is independent and outside the province of the executive branch, and therefore separate from the Office of Public Prosecutions which is supervised by the Minister of Justice.\n\nDespite high media attention and frequent TV portrayals, examining judges are actually active in only a small minority of cases. In 2005, there were 1.1 million criminal rulings in France, while only 33,000 new cases were investigated by judges. The vast majority of cases are therefore investigated directly by law enforcement agencies (police, gendarmerie) under the supervision of the Office of Public Prosecutions (\"procureurs\").\n\nExamining judges are used for serious crimes, e.g., murder and rape, and for crimes involving complexity, such as embezzlement, misuse of public funds, and corruption. The case may be brought before the examining judge either by the public prosecutor (\"procureur\") or, more rarely, by the victim (who may compel an \"instruction\" even if the public prosecutor rules the charges to be insufficient).\n\nThe judge questions witnesses, interrogates suspects, and orders searches for other investigations. Their role is not to prosecute the accused, but to gather facts, and as such their duty is to look for any and all evidence (\"à charge et à décharge\"), incriminating or exculpatory. Both the prosecution and the defense may request the judge to act and may appeal the judge's decisions before an appellate court. The scope of the inquiry is limited by the mandate given by the prosecutor's office: the examining judge cannot open a criminal investigation sua sponte.\n\nIn the past the examining judge could order committal of the accused, this power being subject to appeal. However, this is no longer the case, and other judges have to approve a committal order.\n\nIf the examining judge decides there is a valid case against a suspect, the accused is sent for adversarial trial by jury. The examining judge does not sit on the trial court which tries the case and is in fact prohibited from sitting for future cases involving the same defendant. The case is tried before the court in a manner similar to that of adversarial courts: the prosecution (and on occasion a plaintiff) seeks the conviction of accused criminals, the defense attempts to rebut the prosecution claims, and the judge and jury draw their conclusions from the evidence presented at trial.\n\nAs a result of judicial investigation and defendants being able to have judicial proceedings dismissed on procedural grounds during the examining phase, cases where the evidence is weak tend not to reach the trial stage. Conversely, the guilty plea and plea bargaining were until recently unknown to French law, and now it only applies to crimes for which the prosecution seeks a sentence not exceeding one year imprisonment. Therefore, most cases go to trial, including cases where the prosecution is almost sure to gain a conviction, whereas, in countries such as the United States, these would be settled by plea bargain.\n\nIn administrative courts such as the Council of State, litigation proceedings are markedly more inquisitorial. Most of the procedure is conducted in writing; the plaintiff writes to the court, which asks explanations from the administration or public service concerned; when answered, the court may then ask further detail from the plaintiff, etc. When the case is sufficiently complete, the lawsuit opens in court; however, the parties are not even required to attend the court appearance. This method reflects the fact that administrative lawsuits are for the most part about matters of formal procedure and technicalities.\n\nCertain administrative proceedings within some common law jurisdictions in the United States may be similar to their civil law counterparts but are conducted on a more inquisitorial model. For instance tribunals dealing with minor traffic violations at the New York City Traffic Violations Bureau are held before an adjudicator who also functions as a prosecutor. They question witnesses before rendering judgements and setting fines.\n\nThese types of tribunals or boards function as an expedited form of justice where the state agents conduct an initial investigation and the adjudicator's job is to confirm these preliminary findings through a simplified form of procedure that grants some basic amount of due process or fundamental justice in which the accused party has an opportunity to place his or her objections on the record.\n\n\n"}
{"id": "6606599", "url": "https://en.wikipedia.org/wiki?curid=6606599", "title": "Jagra", "text": "Jagra\n\nIn Hindu philosophy, Jagra is one of the four states of consciousness a man (or a being) can have. It can be roughly translated as \"wakefulness\". It is that part of consciousness when a person or being can sense this physical universe. Other states of consciousness are swapna, susupti and turiya. Later commentators increased the states of consciousness from four to seven.\n"}
{"id": "12799942", "url": "https://en.wikipedia.org/wiki?curid=12799942", "title": "Learned intermediary", "text": "Learned intermediary\n\nLearned intermediary is a defense doctrine used in the legal system of the United States. This doctrine states that a manufacturer of a product has fulfilled his duty of care when he provides all of the necessary information to a \"learned intermediary\" who then interacts with the consumer of a product. This doctrine is primarily used by pharmaceutical and medical device manufacturers in defense of tort suits. \n\nIn a clear majority of states, the courts have accepted this as a liability shield for pharmaceutical companies. \n\nThis doctrine was adopted by the Supreme Court of Canada in \"Hollis v Dow Corning Corp.\", 129 DLR 609 (1995).\n\nThe use of the term \"learned intermediary\" was first used in the Eighth Circuit decision of \"Sterling Drug v. Cornish\" (370 F.2d 82, 85), in 1966, and has now become the prevailing doctrine in the majority of jurisdictions in the United States.\n\nRecently, this doctrine has been called into question due to the increased use of direct to consumer advertising, whereby drug manufacturers market pharmaceutical products to individuals rather than to doctors. For example, in \"Rimbert v. Eli Lilly & Co.\", 577 F. Supp. 2d 1174, 1218-19 (D. N.M. 2008), the District Court of New Mexico reasoned that the \"dramatically increased marketing directed to consumers . . . would persuade the Supreme Court of New Mexico that the justification for the learned-intermediary doctrine is quickly becoming, if not already the case, outdated.\" However, other recent cases have declined to adopt this so-called \"direct-to-consumer advertising\" exception to the learned intermediary doctrine. \"See DiBartolo v. Abbott Labs.\", 2012 WL 6681704 (S.D.N.Y. Dec.21, 2012), \"Centocor Inc. v. Hamilton\", 372 S.W.3d 140, 161 (Tex. 2012), \"Calisi v. Abbott Labs.\", 2013 WL 5462274 (D. Mass. Feb. 25, 2013).\n\n"}
{"id": "39104173", "url": "https://en.wikipedia.org/wiki?curid=39104173", "title": "LennonOno Grant for Peace", "text": "LennonOno Grant for Peace\n\nThe LennonOno Grant for Peace is an award presented by artist and peace activist Yoko Ono. The grant, a sum of $50,000, has been awarded biennially to people and organisations chosen by Ono herself since 2002, in honour of Ono's late husband John Lennon.\n\n2002\n2004\n2006\n2008\n2010\n2012\n2014\n2016 \n"}
{"id": "39993157", "url": "https://en.wikipedia.org/wiki?curid=39993157", "title": "Leuckart's law", "text": "Leuckart's law\n\nLeuckart's law is an empirical law in zoology that states that the size of the eye of an animal is related to its maximum speed of movement; fast-moving animals have larger eyes, after allowing for the effects of body mass. The hypothesis dates from 1876, and in older literature is usually referred to as Leuckart's ratio.\n\nThe principle was initially applied to birds; it has also been applied to mammals.\n\nA study of 88 bird species, published in 2011, found no useful correlation between flight speed and eye size.\n"}
{"id": "18631", "url": "https://en.wikipedia.org/wiki?curid=18631", "title": "Lorentz force", "text": "Lorentz force\n\nIn physics (particularly in electromagnetism) the Lorentz force (or electromagnetic force) is the combination of electric and magnetic force on a point charge due to electromagnetic fields. A particle of charge \"q\" moving with a velocity \"v\" in an electric field E and a magnetic field B experiences a force\n(in SI units). Variations on this basic formula describe the magnetic force on a current-carrying wire (sometimes called Laplace force), the electromotive force in a wire loop moving through a magnetic field (an aspect of Faraday's law of induction), and the force on a charged particle which might be traveling near the speed of light (relativistic form of the Lorentz force).\n\nThe first derivation of the Lorentz force is commonly attributed to Oliver Heaviside in 1889, although other historians suggest an earlier origin in an 1865 paper by James Clerk Maxwell. Hendrik Lorentz derived it in 1895, a few years after Heaviside.\n\nThe force F acting on a particle of electric charge \"q\" with instantaneous velocity v, due to an external electric field E and magnetic field B, is given by (in SI units):\n\nwhere × is the vector cross product (all boldface quantities are vectors). In terms of cartesian components, we have:\n\nformula_2\n\nformula_3\n\nformula_4\n\nIn general, the electric and magnetic fields are functions of the position and time. Therefore, explicitly, the Lorentz force can be written as:\n\nin which r is the position vector of the charged particle, \"t\" is time, and the overdot is a time derivative.\n\nA positively charged particle will be accelerated in the \"same\" linear orientation as the E field, but will curve perpendicularly to both the instantaneous velocity vector v and the B field according to the right-hand rule (in detail, if the fingers of the right hand are extended to point in the direction of v and are then curled to point in the direction of B, then the extended thumb will point in the direction of F).\n\nThe term \"qE is called the electric force, while the term \"qv × B is called the magnetic force. According to some definitions, the term \"Lorentz force\" refers specifically to the formula for the magnetic force, with the \"total\" electromagnetic force (including the electric force) given some other (nonstandard) name. This article will \"not\" follow this nomenclature: In what follows, the term \"Lorentz force\" will refer to the expression for the total force.\n\nThe magnetic force component of the Lorentz force manifests itself as the force that acts on a current-carrying wire in a magnetic field. In that context, it is also called the Laplace force.\n\nThe Lorentz force is a force exerted by the electromagnetic field on the charged particle, that is, it is the rate at which linear momentum is transferred from the electromagnetic field to the particle. Associated with it is the power which is the rate at which energy is transferred from the electromagnetic field to the particle. That power is\nNotice that the magnetic field does not contribute to the power because the magnetic force is always perpendicular to the velocity of the particle.\n\nFor a continuous charge distribution in motion, the Lorentz force equation becomes:\n\nwhere \"d\"F is the force on a small piece of the charge distribution with charge \"dq\". If both sides of this equation are divided by the volume of this small piece of the charge distribution \"dV\", the result is:\n\nwhere f is the \"force density\" (force per unit volume) and \"ρ\" is the charge density (charge per unit volume). Next, the current density corresponding to the motion of the charge continuum is\n\nso the continuous analogue to the equation is\n\nThe total force is the volume integral over the charge distribution:\n\nBy eliminating ρ and J, using Maxwell's equations, and manipulating using the theorems of vector calculus, this form of the equation can be used to derive the Maxwell stress tensor σ, in turn this can be combined with the Poynting vector S to obtain the electromagnetic stress–energy tensor T used in general relativity.\n\nIn terms of σ and S, another way to write the Lorentz force (per unit volume) is\n\nwhere \"c\" is the speed of light and ∇· denotes the divergence of a tensor field. Rather than the amount of charge and its velocity in electric and magnetic fields, this equation relates the energy flux (flow of \"energy\" per unit time per unit distance) in the fields to the force exerted on a charge distribution. See Covariant formulation of classical electromagnetism for more details.\n\nThe density of power associated with the Lorentz force in a material medium is\n\nIf we separate the total charge and total current into their free and bound parts, we get that the density of the Lorentz force is\n\nwhere: ρ is the density of free charge; \"P\" is the polarization density; \"J\" is the density of free current; and \"M\" is the magnetization density. In this way, the Lorentz force can explain the torque applied to a permanent magnet by the magnetic field. The density of the associated power is\n\nThe above-mentioned formulae use SI units which are the most common among experimentalists, technicians, and engineers. In cgs-Gaussian units, which are somewhat more common among theoretical physicists as well as condensed matter experimentalists, one has instead\nwhere \"c\" is the speed of light. \nAlthough this equation looks slightly different, it is completely equivalent, since\none has the following relations:\n\nwhere ε is the vacuum permittivity and μ the vacuum permeability. In practice, the subscripts \"cgs\" and \"SI\" are always omitted, and the unit system has to be assessed from context.\n\nEarly attempts to quantitatively describe the electromagnetic force were made in the mid-18th century. It was proposed that the force on magnetic poles, by Johann Tobias Mayer and others in 1760, and electrically charged objects, by Henry Cavendish in 1762, obeyed an inverse-square law. However, in both cases the experimental proof was neither complete nor conclusive. It was not until 1784 when Charles-Augustin de Coulomb, using a torsion balance, was able to definitively show through experiment that this was true. Soon after the discovery in 1820 by H. C. Ørsted that a magnetic needle is acted on by a voltaic current, André-Marie Ampère that same year was able to devise through experimentation the formula for the angular dependence of the force between two current elements. In all these descriptions, the force was always given in terms of the properties of the objects involved and the distances between them rather than in terms of electric and magnetic fields.\n\nThe modern concept of electric and magnetic fields first arose in the theories of Michael Faraday, particularly his idea of lines of force, later to be given full mathematical description by Lord Kelvin and James Clerk Maxwell. From a modern perspective it is possible to identify in Maxwell's 1865 formulation of his field equations a form of the Lorentz force equation in relation to electric currents, however, in the time of Maxwell it was not evident how his equations related to the forces on moving charged objects. J. J. Thomson was the first to attempt to derive from Maxwell's field equations the electromagnetic forces on a moving charged object in terms of the object's properties and external fields. Interested in determining the electromagnetic behavior of the charged particles in cathode rays, Thomson published a paper in 1881 wherein he gave the force on the particles due to an external magnetic field as\nThomson derived the correct basic form of the formula, but, because of some miscalculations and an incomplete description of the displacement current, included an incorrect scale-factor of a half in front of the formula. Oliver Heaviside invented the modern vector notation and applied it to Maxwell's field equations; he also (in 1885 and 1889) had fixed the mistakes of Thomson's derivation and arrived at the correct form of the magnetic force on a moving charged object. Finally, in 1895, Hendrik Lorentz derived the modern form of the formula for the electromagnetic force which includes the contributions to the total force from both the electric and the magnetic fields. Lorentz began by abandoning the Maxwellian descriptions of the ether and conduction. Instead, Lorentz made a distinction between matter and the luminiferous aether and sought to apply the Maxwell equations at a microscopic scale. Using Heaviside's version of the Maxwell equations for a stationary ether and applying Lagrangian mechanics (see below), Lorentz arrived at the correct and complete form of the force law that now bears his name.\n\nIn many cases of practical interest, the motion in a magnetic field of an electrically charged particle (such as an electron or ion in a plasma) can be treated as the superposition of a relatively fast circular motion around a point called the guiding center and a relatively slow drift of this point. The drift speeds may differ for various species depending on their charge states, masses, or temperatures, possibly resulting in electric currents or chemical separation.\n\nWhile the modern Maxwell's equations describe how electrically charged particles and currents or moving charged particles give rise to electric and magnetic fields, the Lorentz force law completes that picture by describing the force acting on a moving point charge \"q\" in the presence of electromagnetic fields. The Lorentz force law describes the effect of E and B upon a point charge, but such electromagnetic forces are not the entire picture. Charged particles are possibly coupled to other forces, notably gravity and nuclear forces. Thus, Maxwell's equations do not stand separate from other physical laws, but are coupled to them via the charge and current densities. The response of a point charge to the Lorentz law is one aspect; the generation of E and B by currents and charges is another.\n\nIn real materials the Lorentz force is inadequate to describe the collective behavior of charged particles, both in principle and as a matter of computation. The charged particles in a material medium not only respond to the E and B fields but also generate these fields. Complex transport equations must be solved to determine the time and spatial response of charges, for example, the Boltzmann equation or the Fokker–Planck equation or the Navier–Stokes equations. For example, see magnetohydrodynamics, fluid dynamics, electrohydrodynamics, superconductivity, stellar evolution. An entire physical apparatus for dealing with these matters has developed. See for example, Green–Kubo relations and Green's function (many-body theory).\n\nIn many textbook treatments of classical electromagnetism, the Lorentz force Law is used as the \"definition\" of the electric and magnetic fields E and B. To be specific, the Lorentz force is understood to be the following empirical statement:\n\nThis is valid, even for particles approaching the speed of light (that is, magnitude of v = |v| ≈ \"c\"). So the two vector fields E and B are thereby defined throughout space and time, and these are called the \"electric field\" and \"magnetic field\". The fields are defined everywhere in space and time with respect to what force a test charge would receive regardless of whether a charge is present to experience the force.\n\nAs a definition of E and B, the Lorentz force is only a definition in principle because a real particle (as opposed to the hypothetical \"test charge\" of infinitesimally-small mass and charge) would generate its own finite E and B fields, which would alter the electromagnetic force that it experiences. In addition, if the charge experiences acceleration, as if forced into a curved trajectory by some external agency, it emits radiation that causes braking of its motion. See for example Bremsstrahlung and synchrotron light. These effects occur through both a direct effect (called the radiation reaction force) and indirectly (by affecting the motion of nearby charges and currents). Moreover, net force must include gravity, electroweak, and any other forces aside from electromagnetic force.\n\nWhen a wire carrying an electric current is placed in a magnetic field, each of the moving charges, which comprise the current, experiences the Lorentz force, and together they can create a macroscopic force on the wire (sometimes called the Laplace force). By combining the Lorentz force law above with the definition of electric current, the following equation results, in the case of a straight, stationary wire:\n\nwhere ℓ is a vector whose magnitude is the length of wire, and whose direction is along the wire, aligned with the direction of conventional current flow \"I\".\n\nIf the wire is not straight but curved, the force on it can be computed by applying this formula to each infinitesimal segment of wire \"d\"ℓ, then adding up all these forces by integration. Formally, the net force on a stationary, rigid wire carrying a steady current \"I\" is\n\nThis is the net force. In addition, there will usually be torque, plus other effects if the wire is not perfectly rigid.\n\nOne application of this is Ampère's force law, which describes how two current-carrying wires can attract or repel each other, since each experiences a Lorentz force from the other's magnetic field. For more information, see the article: Ampère's force law.\n\nThe magnetic force () component of the Lorentz force is responsible for \"motional\" electromotive force (or \"motional EMF\"), the phenomenon underlying many electrical generators. When a conductor is moved through a magnetic field, the magnetic field exerts opposite forces on electrons and nuclei in the wire, and this creates the EMF. The term \"motional EMF\" is applied to this phenomenon, since the EMF is due to the \"motion\" of the wire.\n\nIn other electrical generators, the magnets move, while the conductors do not. In this case, the EMF is due to the electric force (\"q\"E) term in the Lorentz Force equation. The electric field in question is created by the changing magnetic field, resulting in an \"induced\" EMF, as described by the Maxwell–Faraday equation (one of the four modern Maxwell's equations).\n\nBoth of these EMFs, despite their apparently distinct origins, are described by the same equation, namely, the EMF is the rate of change of magnetic flux through the wire. (This is Faraday's law of induction, see below.) Einstein's special theory of relativity was partially motivated by the desire to better understand this link between the two effects. In fact, the electric and magnetic fields are different facets of the same electromagnetic field, and in moving from one inertial frame to another, the solenoidal vector field portion of the \"E\"-field can change in whole or in part to a \"B\"-field or \"vice versa\".\n\nGiven a loop of wire in a magnetic field, Faraday's law of induction states the induced electromotive force (EMF) in the wire is:\n\nwhere\n\nis the magnetic flux through the loop, B is the magnetic field, Σ(\"t\") is a surface bounded by the closed contour ∂Σ(\"t\"), at all at time \"t\", dA is an infinitesimal vector area element of Σ(\"t\") (magnitude is the area of an infinitesimal patch of surface, direction is orthogonal to that surface patch).\n\nThe \"sign\" of the EMF is determined by Lenz's law. Note that this is valid for not only a \"stationary\" wirebut also for a \"moving\" wire.\n\nFrom Faraday's law of induction (that is valid for a moving wire, for instance in a motor) and the Maxwell Equations, the Lorentz Force can be deduced. The reverse is also true, the Lorentz force and the Maxwell Equations can be used to derive the Faraday Law.\n\nLet Σ(\"t\") be the moving wire, moving together without rotation and with constant velocity v and Σ(\"t\") be the internal surface of the wire. The EMF around the closed path ∂Σ(\"t\") is given by:\n\nwhere\n\nis the electric field and dℓ is an infinitesimal vector element of the contour ∂Σ(\"t\").\n\nNB: Both dℓ and dA have a sign ambiguity; to get the correct sign, the right-hand rule is used, as explained in the article Kelvin–Stokes theorem.\n\nThe above result can be compared with the version of Faraday's law of induction that appears in the modern Maxwell's equations, called here the \"Maxwell–Faraday equation\":\n\nThe Maxwell–Faraday equation also can be written in an \"integral form\" using the Kelvin–Stokes theorem.\n\nSo we have, the Maxwell Faraday equation:\n\nThe two are equivalent if the wire is not moving. Using the Leibniz integral rule and that \"div\" B = 0, results in,\n\nand using the Maxwell Faraday equation,\n\nsince this is valid for any wire position it implies that,\n\nFaraday's law of induction holds whether the loop of wire is rigid and stationary, or in motion or in process of deformation, and it holds whether the magnetic field is constant in time or changing. However, there are cases where Faraday's law is either inadequate or difficult to use, and application of the underlying Lorentz force law is necessary. See inapplicability of Faraday's law.\n\nIf the magnetic field is fixed in time and the conducting loop moves through the field, the magnetic flux Φ linking the loop can change in several ways. For example, if the B-field varies with position, and the loop moves to a location with different B-field, Φ will change. Alternatively, if the loop changes orientation with respect to the B-field, the differential element will change because of the different angle between B and dA, also changing Φ. As a third example, if a portion of the circuit is swept through a uniform, time-independent B-field, and another portion of the circuit is held stationary, the flux linking the entire closed circuit can change due to the shift in relative position of the circuit's component parts with time (surface ∂Σ(\"t\") time-dependent). In all three cases, Faraday's law of induction then predicts the EMF generated by the change in Φ.\n\nNote that the Maxwell Faraday's equation implies that the Electric Field E is non conservative when the Magnetic Field B varies in time, and is not expressible as the gradient of a scalar field, and not subject to the gradient theorem since its rotational is not zero.\n\nThe E and B fields can be replaced by the magnetic vector potential A and (scalar) electrostatic potential \"ϕ\" by\n\nwhere ∇ is the gradient, ∇⋅ is the divergence, ∇× is the curl.\n\nThe force becomes\n\nand using an identity for the triple product simplifies to\n\n(v has no dependence on position, so there's no need to use Feynman's subscript notation). Using the chain rule, the total derivative of A is:\n\nso the above expression can be rewritten as:\n\nWith v = ẋ, we can put the equation into the convenient Euler–Lagrange form\n\n(\\phi-\\dot{\\mathbf{x}}\\cdot\\mathbf{A})+ \\frac{\\mathrm{d}}{\\mathrm{d}t}\\nabla_{\\dot{\\mathbf{x}}}(\\phi-\\dot{\\mathbf{x}}\\cdot\\mathbf{A})\\right]</math>\n\nwhere\n\nformula_35\n\nand\n\nformula_36.\n\nThe Lagrangian for a charged particle of mass \"m\" and charge \"q\" in an electromagnetic field equivalently describes the dynamics of the particle in terms of its \"energy\", rather than the force exerted on it. The classical expression is given by:\n\nwhere A and \"ϕ\" are the potential fields as above. Using Lagrange's equations, the equation for the Lorentz force can be obtained.\n\nThe potential energy depends on the velocity of the particle, so the force is velocity dependent, so it is not conservative.\n\nThe relativistic Lagrangian is\n\nThe action is the relativistic arclength of the path of the particle in space time, minus the potential energy contribution, plus an extra contribution which quantum mechanically is an extra phase a charged particle gets when it is moving along a vector potential.\n\nUsing the metric signature , the Lorentz force for a charge \"q\" can be written in covariant form:\n\nwhere \"p\" is the four-momentum, defined as\n\n\"τ\" the proper time of the particle, \"F\" the contravariant electromagnetic tensor\n\nand \"U\" is the covariant 4-velocity of the particle, defined as:\n\nin which\n\nis the Lorentz factor.\n\nThe fields are transformed to a frame moving with constant relative velocity by:\n\nwhere Λ\"\" is the Lorentz transformation tensor.\n\nThe component (\"x\"-component) of the force is\n\nSubstituting the components of the covariant electromagnetic tensor \"F\" yields\n\nUsing the components of covariant four-velocity yields\n\nThe calculation for , 3 (force components in the \"y\" and \"z\" directions) yields similar results, so collecting the 3 equations into one:\n\nand since differentials in coordinate time \"dt\" and proper time \"dτ\" are related by the Lorentz factor,\n\nso we arrive at\n\nThis is precisely the Lorentz force law, however, it is important to note that p is the relativistic expression,\n\nThe electric and magnetic fields are dependent on the velocity of an observer, so the relativistic form of the Lorentz force law can best be exhibited starting from a coordinate-independent expression for the electromagnetic and magnetic fields formula_51, and an arbitrary time-direction, formula_52. This can be settled through Space-Time Algebra (or the geometric algebra of space-time), a type of Clifford's Algebra defined on a pseudo-Euclidean space, as\n\nand\n\nformula_55 is a space-time bivector (an oriented plane segment, just like a vector is an oriented line segment), which has six degrees of freedom corresponding to boosts (rotations in space-time planes) and rotations (rotations in space-space planes). The dot product with the vector formula_52 pulls a vector (in the space algebra) from the translational part, while the wedge-product creates a trivector (in the space algebra) who is dual to a vector which is the usual magnetic field vector.\nThe relativistic velocity is given by the (time-like) changes in a time-position vector formula_57, where\n\n(which shows our choice for the metric) and the velocity is\n\nThe proper (invariant is an inadequate term because no transformation has been defined) form of the Lorentz force law is simply\n\nNote that the order is important because between a bivector and a vector the dot product is anti-symmetric. Upon a space time split like one can obtain the velocity, and fields as above yielding the usual expression.\n\nIn the general theory of relativity the equation of motion for a particle with mass formula_60 and charge formula_61, moving in a space with metric tensor formula_62 and electromagnetic field formula_63, is given as\nwhere formula_65 (formula_66 is taken along the trajectory), formula_67, and formula_68.\n\nThe equation can also be written as\nwhere formula_70 is the Christoffel symbol (of the torsion-free metric connection in general relativity), or as\nwhere formula_72 is the covariant differential in general relativity (metric, torsion-free).\n\nThe Lorentz force occurs in many devices, including:\n\nIn its manifestation as the Laplace force on an electric current in a conductor, this force occurs in many devices including:\n\nThe numbered references refer in part to the list immediately below.\n\n\n"}
{"id": "1914046", "url": "https://en.wikipedia.org/wiki?curid=1914046", "title": "Panthera leo leo", "text": "Panthera leo leo\n\nPanthera leo leo is the nominate subspecies of the lion, which today is present in West Africa, northern Central Africa and India.\nIt is regionally extinct in southern Europe, West Asia and North Africa. In India, the sole lion population lives in and around Gir Forest National Park. In West and Central Africa, it is restricted to fragmented and isolated populations, most of them declining.\nThe West African population is listed as Critically Endangered on the IUCN Red List; this population is isolated and comprises fewer than 250 mature individuals. \nIn 2005, a Lion Conservation Strategy was developed for West and Central Africa.\n\nResults of a phylogeographic study indicate that lion populations in West and Central African range countries are genetically close to populations in India, forming a clade distinct from lion populations in Southern and East Africa. In 2017, the Cat Classification Task Force of the IUCN Cat Specialist Group subsumed lion populations to two subspecies, namely \"P. l. leo\" and \"P. l. melanochaita\".\n\nA lion from Constantine, Algeria was the type specimen for the specific name \"Felis leo\" used by Linnaeus in 1758. In the 19th and 20th centuries, several lion zoological specimens from Africa and Asia were described and proposed as subspecies:\nIn 1930, Reginald Innes Pocock subordinated the lion to the genus \"Panthera\" when he wrote about Asiatic lion specimens in the zoological collection of the British Museum of Natural History.\n\nIn the following decades, there has been much debate among zoologists on the validity of proposed subspecies:\nIn 2017, the Cat Classification Task Force of the IUCN Cat Specialist Group subsumed lion populations in North, West and Central Africa and Asia to \"P. l. leo\", based on results of genetic research on lion samples.\n\nSince the beginning of the 21st century, several phylogenetic studies were conducted to aid clarifying the taxonomic status of lion samples kept in museums and collected in the wild. Scientists analysed between 32 and 480 lion samples from up to 22 countries. They all agree that the lion species comprises two evolutionary groups, one in the northern and eastern parts of its historical range, and the other in Southern and East Africa that diverged between 245,000 and 50,000 years ago. They assume that tropical rainforest and the East African Rift constituted major barriers between the two groups.\n\nIn a comprehensive study about the evolution of lions, 357 samples of 11 lion populations were examined, including some hybrid lions. The hybrids had descended from lions captured in Angola and Zimbabwe, and apparently West or Central Africa. Results indicated that four lions from Morocco did not exhibit any unique genetic characteristics and shared mitochondrial haplotypes H5 and H6 with lions from West Africa, and together with them were part of a major mtDNA grouping (lineage III) that also included Asiatic samples. This scenario was well in line with theories on lion evolution: lineage III developed in East Africa and traveled north and west in the first wave of lion expansions about 118,000 years ago. It apparently broke up into haplotypes H5 and H6 within Africa, and then into H7 and H8 in West Asia\n\nResults of genetic analyses indicate that lions in West Africa and northern parts of Central Africa form distinct lion clades, which are more closely related to North African and Asiatic lions than to lions in Southern Africa and southern parts of East Africa. Lions from North Africa and India however, do form one single clade. Analysis of phylogenetic data of 194 lion samples from 22 different countries revealed that Central and West African lions form a phylogeographic group that probably diverged about 186,000–128,000 years ago from the \"melanochaita\" group in East and Southern Africa.\n\nSeveral lions kept in Ethiopia's Addis Ababa Zoo were found to be genetically similar to wild lions from Cameroon and Chad.\n\nThe lion's fur varies in colour from light buff to dark brown. It has rounded ears and a black tail tuft. Average head-to-body length of male lions is with a weight of . Females are smaller and less heavy.\n\nA few lion specimens from West Africa obtained by museums were described as having shorter manes than lions from other African regions. In general, the West African lion is similar in general appearance and size as lions in other parts of Africa and Asia.\n\nZoological specimens range in colour from light to dark tawny. Male skins have short manes, light manes, dark manes or long manes. Taxonomists recognised that neither skin nor mane colour and length of lions can be adduced as distinct subspecific characteristics. Then they turned to measuring and comparing lion skulls and found that skull length of Barbary and Indian lion samples does not differ significantly, ranging from in females and in males.\n\nHistorically, lion range encompassed North Africa, southeastern Europe, the Arabian Peninsula and Middle East. In these regions, lions occurred in:\n\nToday, \"P. l. leo\" occurs in West and Central Africa and India. It is regionally extinct in Gambia, Mauritania, Sierra Leone, the Western Sahara, Morocco, Algeria, Tunisia, Libya, Egypt, Saudi Arabia, Kuwait, Jordan, Lebanon, Syria, Turkey, Palestine, Israel, Iraq, Iran, Afghanistan and Pakistan. Its range has declined to the:\n\nContemporary lion distribution and habitat quality in savannahs of West and Central Africa was assessed in 2005, and Lion Conservation Units (LCU) mapped. Educated guesses for size of populations in these LCUs ranged from 3,274 to 3,909 individuals between 2002 and 2012.\n\nMale Asiatic lions are solitary or associate with up to three males forming a loose pride. Pairs of males rest, hunt and feed together, and display marking behaviour at the same sites. Females associate with up to 12 females forming a stronger pride together with their cubs. They share large carcasses among each other, but seldom with males. Female and male lions usually associate only for a few days when mating, but rarely travel and feed together.\n\nIn Pendjari National Park, groups of lions range from 1–8 individuals. Outside the National Park, groups are smaller and with a single male.\nIn Waza National Park, three female and two male lions were radio-collared in 1999 and tracked until 2001. The females moved in home ranges of between and stayed inside the park during most of the survey period. The males used home ranges of between , both inside and outside the park, where they repeatedly killed livestock. One was killed and the other shot at by local people. After the pellets were removed, he recovered and shifted his home range to inside the park, and was not observed killing livestock any more.\n\nIn general, lions prefer large prey species within a weight range of . They hunt large ungulates in the range of including zebra, warthog, blue wildebeest, impala, gemsbok, Thomson's gazelle, kob, giraffe and Cape Buffalo. In India's Gir Forest National Park, lions predominantly kill chital, sambar, nilgai, cattle, buffalo and less frequently also wild boar. Outside the protected area where wild prey species do not occur, lions prey on buffalo and cattle, rarely also on camel. They kill most prey less than away from water bodies, charge prey from close range and drag carcasses into dense cover.\n\nLions probably prey on livestock when wild prey species occur at lower densities, especially during the wet season. An interview survey among livestock owners in six villages in the park's vicinity revealed that lions attack cattle mostly during the rainy season when wild prey disperses away from artificial waterholes.\n\nIn Africa, lions are killed pre-emptively or in retaliation for preying on livestock. Populations are also threatened by depletion of prey base, loss and conversion of habitat.\n\nThe lion population in West Africa is fragmented and isolated, comprising fewer than 250 mature individuals. It is threatened by poaching and illegal trade of body parts. Lion body parts from Benin are smuggled to Niger, Nigeria, Gabon, Ivory Coast, Senegal and Guinea, and from Burkina Faso to Benin, Ivory Coast, Senegal and Guinea.\nIn Nigeria, the isolated lion population in Gashaka Gumti National Park is hunted and poisoned by local people.\n\nThe Central African lion is threatened by loss of habitat and prey base and trophy hunting. Between seven and 12 lion trophies were exported from Cameroon every year in the years from 1985 to 2010. \nIn northern parts of Cameroon, the lion population is threatened due to increased migration of people from Nigeria following the political insecurity in the region.\nIn Bénoué National Park, local people were observed at a lion kill cutting off chunks of meat. Local people living in the vicinity of the protected area accounted in interviews that lions frequently attack livestock during the dry season. They use poison on carcasses to kill carnivores. In Waza National Park, two of four radio-collared lions were killed between 2007 and 2008, and probably also an adult female, two other adult males and three cubs. Nomadic herders use bow and arrows poisoned with cobra venom to kill lions in retaliation for attacks on livestock.\n\nIn India, the lion is protected, and included in CITES Appendix I. African lions are included in CITES Appendix II. In 2004, it was proposed in 2004 to list all lion populations in CITES Appendix I to reduce exports of lion trophies and implement a stricter permission process, due to the negative impact of trophy hunting.\n\nIn 2006, a Lion Conservation Strategy for West and Central Africa was developed in cooperation between IUCN regional offices and several wildlife conservation organisations. The strategy envisages to maintain sufficient habitat, ensure a sufficient wild prey base, make lion-human coexistence sustainable and reduce factors that lead to further fragmentation of populations.\nSurveys and interviews with herders around protected areas revealed that improved enclosures for livestock significantly decreased depredation by lions, and hence contributed to mitigating human-lion conflict.\n\nIn 2006, 1258 captive lions were registered in the International Species Information System, including 13 individuals originating from Senegal to Cameroon, 115 from India and 970 with uncertain origin.\n\n"}
{"id": "17673378", "url": "https://en.wikipedia.org/wiki?curid=17673378", "title": "Peace congress", "text": "Peace congress\n\nA peace congress, in international relations, has at times been defined in a way that would distinguish it from a peace conference (usually defined as a diplomatic meeting to decide on a peace treaty), as an ambitious forum to carry out dispute resolution in international affairs, and prevent wars. This idea was widely promoted during the nineteenth century, anticipating the international bodies that would be set up in the twentieth century with comparable aims.\n\nThe genesis of the idea of a meeting of representatives of different nations to obtain by peaceful arbitrament a settlement of differences has been traced to the year 1623 in modern history, to a French monk, Émeric Crucé, who wrote a work entitled \"The New Cyneas\", a discourse showing the opportunities and the means for establishing a general peace and liberty of conscience to all the world and addressed to the monarch and the sovereign princes of the time. He proposed that a city, preferably Venice, should be selected where all the powers had ambassadors and that there should be a universal union, including all peoples. He suggested careful arrangement as to priority, giving the first place to the pope.\n\nTwo years after this publication, in 1625, appeared in Latin the work of Hugo Grotius \"On the Right of War and Peace\", pleading for a mitigation of some of the barbarous usages of war.\n\nWilliam Penn had a plan for the establishment of a \"European Dyet, Parliament or Estates\". He was followed by other writers of different nationalities.\n\nThe concept of a peaceful community of nations had also been outlined in 1795, when Immanuel Kant’s \"Perpetual Peace: A Philosophical Sketch\" outlined the idea of a league of nations that would control conflict and promote peace between states.\n\nInternational co-operation to promote collective security originated in the Concert of Europe that developed after the Napoleonic War in the nineteenth century in an attempt to maintain the status quo between European states and so avoid war. This period also saw the development of international law with the first Geneva Conventions establishing laws about humanitarian relief during war and the international Hague Conventions of 1899 and 1907 governing rules of war and the peaceful settlement of international disputes.\n\nThe forerunner of the League of Nations, the Inter-Parliamentary Union (IPU), was formed by peace activists William Randal Cremer and Frédéric Passy in 1889. The organization was international in scope with a third of the members of parliament, in the 24 countries with parliaments, serving as members of the IPU by 1914. Its aims were to encourage governments to solve international disputes by peaceful means and arbitration and annual conferences were held to help governments refine the process of international arbitration. The IPU's structure consisted of a Council headed by a President which would later be reflected in the structure of the League.\n\nAfter the defeat of Napoleon I of France an international peace congress took place in Vienna called the Congress of Vienna in 1815.\n\nIn 1826, a congress composed of representatives of Spanish-American countries was planned by Bolívar for military as well as political purposes. One of its declared objects was \"to promote the peace and union of American nations and establish amicable methods for the settlement of disputes between them\". This congress failed, as only four Spanish-American countries were represented and only one ratified the agreement.\n\nIn 1831, however, Mexico took up the subject and proposed a conference of American Republics \"for the purpose of bringing about not only a union and close alliance for defence, but also the acceptance of friendly mediation for the settlement of disputes between them, and the framing and promulgation of a code of penal laws to regulate their mutual relations\". It does not appear that anything came of this congress, and in 1847 another was held at Lima, attended by representatives of Bolivia, Chile, Ecuador, New Granada, and Peru, for the purpose of forming an alliance of American republics. The United States was invited but as it was then at war with Mexico it sent no representative.\n\nAnother congress was held by representatives from the Argentine Republic, Bolivia, Chile, Colombia, Ecuador, Guatemala, Peru, and Venezuela, in 1864.\n\nAn effort to hold a congress was made by the governments of Chile and Colombia in 1880, \"to the end that the settlement by arbitration of each and every international controversy should become a principle of American public law\". This congress did not meet, however, owing to a war between Chile and Peru.\n\nIn 1881, the President of the United States invited the independent countries of North and South America to meet in a general congress at Washington, D.C. on 24 November 1882, \"for the purpose of considering and discussing methods of preventing war between the nations of America\". This meeting did not take place owing to a variety of reasons, but subsequently, by virtue of an Act of Congress of the United States an invitation was issued by the president to Mexico, the Central and South American Republics, Haiti, Dominican Republic, and Brazil to join in a conference to be held in the city of Washington, the project being to consider:\n\n\nThe First International Conference of American States assembled at Washington on 2 October 1889. Eighteen American nations, including the United States, had their representatives. The conference adopted a plan of arbitration of international differences, together with various recommendations relating to trade, law, extradition, patents, customs, and sanitary regulations. It further declared arbitration to be a principle of American International Law and obligatory \"in all controversies concerning diplomatic and consular privileges, boundaries, territories, indemnities, the right of navigation, and the validity, construction and enforcement of treaties; and that it should be equally obligatory in all other cases, whatever might be their origin, nature or object, with the sole exception of those which in the judgment of one of the nations involved in the controversy, might imperil its independence; but that even in this case, while arbitration for that nation should be optional, it should be obligatory on the adversary power\" (7 Moore Int. Law Dig. p. 7). One notable result of the conference was the establishment of the Bureau of the American Republics. All the republics of South America are represented in this bureau, which continues for periods of ten years subject to renewal.\n\nThe Peace Conference of 1861 was a last-ditch effort to avert the coming Civil War.\n\nFollowing an initial congress at London in 1843, an annual series of congresses called International Congress of the Friends of Peace or more informally \"International Peace Congress\" were organised from 1848 until 1853.\n\nElihu Burritt organized the Congress of 1848. The participants met at Brussels in September of that year. Among the distinguished delegates were Cobden, Thierry, Girardin, and Bastiat. The congress adopted resolutions urging limitation of armaments and the placing of a ban upon foreign loans for war purposes. Through the next decade, more congresses were convened in various cities without the development of anything new in principle or method.\n\n\nOn 12 August 1898, in a circular letter addressed to the representatives of different nations, the Emperor of Russia proposed to all governments, which had duly accredited representatives at the imperial court, the holding of a conference to consider the problem of the preservation of peace among nations. During the summer of 1900 the conference assembled at The Hague and on 4 September the formal notification of the ratification of the convention for the pacific settlement of international disputes was given by the United States, Austria, Belgium, Denmark, United Kingdom, France, Germany, Italy, Persia, Portugal, Romania, Russia, Siam, Spain, Sweden, Norway, and the Netherlands, and subsequently by Japan. A Permanent Court of Arbitration was established at The Hague, composed representatives of each of the signatory powers appointed for a term of six years. Arbitrators called upon to form a competent tribunal may be chosen from a general list of the members of the court when any of the signatory powers desire to have recourse to the court for a settlement of any difference between them.\n\nThe South and Central American republics were not represented at the conference, but at the second International Conference of American States which was initiated by President McKinley and held in the City of Mexico, 22 October 1901, to 31 January 1902, a plan was adopted looking to adhesion to The Hague convention, the protocol being signed by all of the delegations except Chile and Ecuador, who subsequently gave their adhesion. The conference authorized the Governments of the United States and Mexico to negotiate with the other signatory powers for the adherence of other American nations. At this conference the project of a treaty for the arbitration of pecuniary claims was adopted, and the signatories agreed for a term of five years to submit to arbitration (preferably to the permanent court at The Hague) all claims for pecuniary loss or damage presented by their respective citizens and not capable of settlement through diplomatic channels, where they were of sufficient importance to warrant the expense of a court of arbitration.\n\nA second international peace conference was held at The Hague from 15 June to 18 October 1907. Forty-four States were represented, including the principal nations of Europe, North and South America, and Asia. The conference drew up thirteen conventions and one declaration. They are as follows: for the pacific settlement of international disputes; respecting the limitation of the employment of force for the recovery of contract debts relative to the opening of hostilities; respecting the laws and customs of war on land; respecting the rights and duties of neutral powers and persons in case of war on land; relative to the status of enemy merchant-ships at the outbreak of hostilities; relative to the conversion of merchant-ships into war-ships; relative to the laying of automatic submarine contact mines; respecting bombardment by naval forces in time of war; for the adaptation to naval war of the principles of the Geneva Convention; relative to certain restrictions with regard to the exercise of the right of capture in naval war; relative to the creation of an International Prize Court; concerning the rights and duties of neutral powers in naval war; and a declaration prohibiting the discharge of projectiles and explosives from balloons.\n\nThe International League of Peace and Liberty organised a series of international peace congresses.\n\n\nA series of international peace congresses called Universal Peace Congress (French: Congrès universel de la paix) took place between 1889 and 1939.\n\n\nThe Paris Peace Conference, that sought a lasting peace after World War I, approved the proposal to create the League of Nations (French: \"Société des Nations\", German: \"Völkerbund\") on 25 January 1919. The Covenant of the League of Nations was drafted by a special commission, and the League was established by Part I of the Treaty of Versailles. On 28 June 1919, the Covenant was signed by 44 states, including 31 states which had taken part in the war on the side of the Triple Entente or joined it during the conflict. Despite American President Woodrow Wilson's efforts to establish and promote the League, for which he was awarded the Nobel Peace Prize in 1919, the United States did not join the League.\n\nThe League held its first council meeting in Paris on 16 January 1920, six days after the Versailles Treaty came into force. In November, the headquarters of the League moved to Geneva, where the first General Assembly was held on 15 November 1920 with representatives from 41 nations in attendance.\n\n\n\n"}
{"id": "1140981", "url": "https://en.wikipedia.org/wiki?curid=1140981", "title": "Racks and quandles", "text": "Racks and quandles\n\nIn mathematics, racks and quandles are sets with binary operations satisfying axioms analogous to the Reidemeister moves used to manipulate knot diagrams.\n\nWhile mainly used to obtain invariants of knots, they can be viewed as algebraic constructions in their own right. In particular, the definition of a quandle axiomatizes the properties of conjugation in a group.\n\nIn 1943, Mituhisa Takasaki (高崎光久) introduced an algebraic structure which he called a \"Kei\" (圭), which would later come to be known as an involutive quandle. His motivation was to find a nonassociative algebraic structure to capture the notion of a reflection in the context of finite geometry. The idea was rediscovered and generalized in (unpublished) 1959 correspondence between John Conway and Gavin Wraith, who at the time were undergraduate students at the University of Cambridge. It is here that the modern definitions of quandles and of racks first appear. Wraith had become interested in these structures (which he initially dubbed sequentials) while at school. Conway renamed them wracks, partly as a pun on his colleague's name, and partly because they arise as the remnants (or 'wrack and ruin') of a group when one discards the multiplicative structure and considers only the conjugation structure. The spelling 'rack' has now become prevalent.\n\nThese constructs surfaced again in the 1980s: in a 1982 paper by David Joyce (where the term quandle was coined), in a 1982 paper by (under the name distributive groupoids) and in a 1986 conference paper by Egbert Brieskorn (where they were called automorphic sets). A detailed overview of racks and their applications in knot theory may be found in the paper by Colin Rourke and Roger Fenn.\n\nA rack may be defined as a set formula_1 with a binary operation formula_2 such that for every formula_3 the self-distributive law holds:\n\nand for every formula_5 there exists a unique formula_6 such that \n\nThis definition, while terse and commonly used, is suboptimal for certain purposes because it contains an existential quantifier which is not really necessary. To avoid this, we may write the unique formula_6 such that formula_9 as formula_10. We then have\n\nand thus \n\nand\n\nUsing this idea, a rack may be equivalently defined as a set formula_1 with two binary operations formula_15 and formula_16 such that for all formula_3:\n\nIt is convenient to say that the element formula_22 is acting from the left in the expression formula_23, and acting from the right in the expression formula_24. The third and fourth rack axioms then say that these left and right actions are inverses of each other. Using this, we can eliminate either one of these actions from the definition of rack. If we eliminate the right action and keep the left one, we obtain the terse definition given initially.\n\nMany different conventions are used in the literature on racks and quandles. For example, many authors prefer to work with just the \"right\" action. Furthermore, the use of the symbols formula_15 and formula_16 is by no means universal: many authors use exponential notation\n\nand\n\nwhile many others write\n\nYet another equivalent definition of a rack is that it is a set where each element acts on the left and right as automorphisms of the rack, with the left action being the inverse of the right one. In this definition, the fact that each element acts as automorphisms encodes the left and right self-distributivity laws, and also these laws:\n\nwhich are consequences of the definition(s) given earlier.\n\nA quandle is defined as a rack, formula_31, such that formula_32\n\nor equivalently\n\nEvery group gives a quandle where the operations come from conjugation:\n\nIn fact, every equational law satisfied by conjugation in a group follows from the quandle axioms. So, one can think of a quandle as what is left of a group when we forget multiplication, the identity, and inverses, and only remember the operation of conjugation.\n\nEvery tame knot in three-dimensional Euclidean space has a 'fundamental quandle'. To define this, one can note that the fundamental group of the knot complement, or knot group, has a presentation (the Wirtinger presentation) in which the relations only involve conjugation. So, this presentation can also be used as a presentation of a quandle. The fundamental quandle is a very powerful invariant of knots. In particular, if two knots have isomorphic fundamental quandles then there is a homeomorphism of three-dimensional Euclidean space, which may be orientation reversing, taking one knot to the other.\n\nLess powerful but more easily computable invariants of knots may be obtained by counting the homomorphisms from the knot quandle to a fixed quandle formula_31. Since the Wirtinger presentation has one generator for each strand in a knot diagram, these invariants can be computed by counting ways of labelling each strand by an element of formula_31, subject to certain constraints. More sophisticated invariants of this sort can be constructed with the help of quandle cohomology.\n\nThe Alexander quandles are also important, since they can be used to compute the Alexander polynomial of a knot. Let formula_38 be a module over the ring formula_39 of Laurent polynomials in one variable. Then the Alexander quandle is formula_38 made into a quandle with the left action given by\n\nRacks are a useful generalization of quandles in topology, since while quandles can represent knots on a round linear object (such as rope or a thread), racks can represent ribbons, which may be twisted as well as knotted.\n\nA quandle formula_42 is said to be involutory if for all formula_43\n\nor equivalently\n\nAny symmetric space gives an involutory quandle, where formula_23 is the result of 'reflecting formula_47 through formula_48'.\n\n\n"}
{"id": "34779626", "url": "https://en.wikipedia.org/wiki?curid=34779626", "title": "Reflexive self-consciousness", "text": "Reflexive self-consciousness\n\nReflexive self-consciousness is a concept, related to that of enlightenment, formulated by Eugene Halliday during the 1940s-1950s in England. He set out his concept in his book, \"Reflexive Self-Consciousness\", which, having been circulated in softback format for many years, was published in hardback in 1989 by Melchisedec Press, .\n\nIn his book, Eugene Halliday sets out a way by which we can develop the ability to respond adequately to the demands life makes of us, the ability to assimilate the shocks and blows of experience, so we can live a whole and balanced life. The way to this balance is through an understanding of the centre of our own being, our consciousness, and through this, finding our place in relation to the universe.\n\nEugene Halliday made a lifelong study of art, religion, philosophy, psychology and science. From his understanding, he formulated a coherent set of ideas. In his seminal work \"Reflexive Self-Consciousness\", he sets out the nature of consciousness and its relation to the world of phenomena, being, and mankind. From this, he explains how consciousness itself can become \"reflexive\". By this he means that consciousness becomes completely self-transparent and continuously aware of its own presence and nature.\n\nHe says that when observing a thing or situation one can promote this reflexive state by turning our own consciousness back onto itself. \"It is the self, which is consciousness itself which is observing this thing, this self I am, I return to the self.\" By placing our nature as observer at the heart of his work, Halliday sets out a method by which to liberate ourselves from object-identification, which locks us into a cycle of conditioned reflexes, pleasure pursuit and pain avoidance.\n\nHe sees a complex structure of cells, such as the brain, as \"a vehicle for the expression of the complex processes of consciousness\" and not as the origin of that consciousness. No matter how complex the arrangement, consciousness cannot arise from the biochemical interactions of a large number of non-sentient particles.\n\nHalliday posits that the ultimate source and origin of our being is sentience or consciousness. He sees this origin as an infinite field of sentient power. Halliday compares the activity of this infinite field of sentient power, the source of all beings, to that of the sea. Its internal movements, its waves, create vortices within it, which give rise to all the observable phenomena of the world. Atoms, molecules, cells, plants, animals, mankind, human beings, all are formed within this infinite sentient field, and all are sentient. There is no non-sentient level of being. Thus agreeing with the philosopher Alfred North Whitehead when he said \"there are no dead gaps in Nature\". This infinite field of sentient power, which is the ultimate source of the universe and all within it, is the Godhead of the theologians, the Absolute of the philosophers.\n\nBefore evolution, Eugene Halliday posits an \"involution\", whereby the motions of this absolute sentient power creates the universe and all the beings in it. Consciousness tends to fall into identification with beings, down to the grossest physical level of the mineral world. Through the process of evolution, sentience evolves through mineral, plant, animal and human to rediscover its true nature as Consciousness itself, at one with the infinite field of consciousness. This return of consciousness to its source, is the \"Reflexive Self-Consciousness\" of the title of the book.\n\n\n"}
{"id": "1042006", "url": "https://en.wikipedia.org/wiki?curid=1042006", "title": "Resettlement of the Jews in England", "text": "Resettlement of the Jews in England\n\nThe resettlement of the Jews in England was an informal arrangement during the Commonwealth of England in the mid-1650s, which allowed Jews to practise their faith openly. It forms a prominent part of the history of the Jews in England. It happened directly after two events. Firstly a prominent rabbi Menasseh ben Israel came to the country from the Netherlands to make the case for Jewish resettlement, and secondly a Spanish New Christian (a supposedly converted Jew, who secretly practised his religion) merchant Antonio Robles requested that he be classified as a Jew rather than Spaniard during the war between England and Spain.\n\nHistorians have disagreed about the reasons behind the resettlement, particularly regarding Oliver Cromwell’s motives, but the move is generally seen as a part of a current of religious and intellectual thought moving towards liberty of conscience, encompassing philosemitic millenarianism and Hebraicism, as well as political and trade interests favouring Jewish presence in England. The schools of thought that led to the resettlement of the Jews in England is the most heavily studied subject of Anglo-Jewish history in the period before the Eighteenth Century.\n\nIn 1290, King Edward I of England had issued an edict expelling all Jews from England. However the English Reformation, which started in the 1530s, brought a number of changes that benefited Jews in the long term. Doctrines and rituals of the Roman Catholic church that hurt Jews were eliminated, especially those that emphasised their role in the death of Jesus. Further anti-Catholicism, with the Pope as antichrist, came to replace anti-Semitism. The period of the English Civil Wars and Interregnum, were marked by both widespread millennial beliefs and a beginning of religious toleration. Significantly, millenarianism in England often had a strong Hebraist character, that emphasised the study of Hebrew and Judaism. This often extended to seeing the English as the new Israelites, for instance Oliver Cromwell saw the British as the descendants of the ten lost tribes of Israel.\nAfter both the Alhambra Decree of 1492, which expelled Jews from Spain in 1492, and similar measures in Portugal in 1496, some converso traders (Jewish converts to Christianity, who often practised Judaism in secret, sometimes also known as New Christians or derogatively as Marranos) settled in London and Bristol. The small community was largely linked by trade to Antwerp, and was expelled altogether in 1609. It was with London’s growing importance as a trading city that Jews from the Netherlands began to settle in the country once more from the 1630s. It is from this first that the current Jewish population of the UK has grown.\n\nThe 1640s and 1650s in England were marked by intense debates about religious toleration, marked by speeches and tracts by radical puritans and dissenters who called for liberty of conscience. This extreme diversity of opinion about religious toleration was sorted into 12 schools of thought in the seminal study of the period by W.K. Jordan. John Coffey uses a simpler three point schema: anti-tolerationists, conservative tolerationists, and radical tolerationists, pointing out that although the latter were in a minority, they formed an important part of the debate. Nonetheless it is important to remember that although figures such as William Walwyn, Henry Vane, John Milton, and others made powerful apologia for religious toleration, their frame of reference was theological, rather than secular in nature and they were not calling for religious pluralism as is understood today. The early and mid Seventeenth century was also marked by a rise in Hebraism, the study of Jewish scriptures, which were often used to discuss political issues such as the existence of a monarchy or republic, and religious toleration. This debate used Jewish sources to justify its conclusions. The most prominent scholar in the field was the MP and jurist, John Selden, whose thought was influenced by Erastus and Grotius. Selden proposed minimal government intervention on matters of religion, a view he modelled on the Hebrew Commonwealth. He in turn influenced similar approaches in John Milton (whose plea for freedom of the press, the Areopagitica (1644), directly named him), Thomas Hobbes and James Harrington (the latter of whom, proposed settling Jews in Ireland in his book \"The Commonwealth of Oceana\").\n\nOverall the strongest political group of the 1640s and 50s, the English Puritans, had a negative view of toleration, seeing it as a concession to evil and heresy. It was often associated with tolerating the heresies of Arminianism, the philosophy of free will and free thought, and Socinianism, a doctrine of Anti-trinitarianism. But despite this Puritan hostility to toleration, England did see a certain religious laissez-faire emerge (for instance, the Rump Parliament repealed the recusancy laws in 1650). This was partly due to the impossibility of stopping religious free expression, but it also became a part of the cause of the new model army. The doctrinal policies of the protectorate were largely conservative, however this Puritan train of thought could also point towards liberty of conscience. For Congregationalists, truth lay in the spirit rather than institutions; like the Platonists they searched for internal unity amidst external diversity. Further, Puritans valued conscience, which could be neither forced nor tested, over ritual and ceremony. So rather than toleration, the key debate among key figures in the Protectorate revolved around liberty of conscience. For Blair Worden, Cromwell’s religious policy was rooted in a search for union of believers, rather than toleration of differing beliefs, and religious persecution was the largest obstacle to this union. However, liberty of conscience extended only to \"God’s peculiar\" and not heretics (such as Quakers, Socinians, and Ranters).\n\nThere was a great increase of religious freedom and the ecclesiastical diversity in Cromwellian England, this marked a revolutionary change and led to increasing toleration in the years after the interregnum ended. On the one hand, while the loosely Calvinist Cromwell allowed the punishment of men such as the Unitarian John Biddle and the Quaker James Nayler, and accepted the restrictions on religious tolerance found in the Humble Petition and Advice of 1657. But on the other hand, his entourage included men who wanted more liberty of belief than he allowed. These non-sectarian ‘merciful men’ or politiques, who wanted to understand and tolerate beliefs different to their own, included Bulstrode Whitelocke, Matthew Hale, Sir Charles Worsley.\n\nThe toleration of Jews was largely borne by the hope of converting them to Christianity. Leonard Busher was one of the first to call for the readmission of the Jews to England and the toleration of their faith in 1616. Lawyer and MP, Henry Finch and the scholar Joseph Mede both wrote of the benefits of the conversion of the Jews in the 1620s. The Scottish minister John Wemyss advocated readmitting Jews to Christian lands with a view to converting them in the 1630s. So, by the 1640s the imminent conversion of the Jews was a widespread belief among Puritans. Indeed during that decade the Christians who were most liberal towards Jews are also those who were most committed to their conversion. A number of these ‘admissionists’ were close to Cromwell, including John Sadler, John Dury, and Hugh Peter. Other notable readmissionists include exiled Royalist cleric Thomas Barlow and the Dissenter Henry Jessey. The Fifth Monarchy Men were another example of Puritan millenarians who saw the readmission of the Jews as hastening the kingdom of Christ. The exiled Royalist Sir Edward Nicholas is one of the few admissionists who did not seem interested in conversion. By contrast, the anti-admissionists were often animated by the belief that it would be difficult or impossible to convert the Jews. William Prynne’s anti-Semitic pamphlet \"A Short Demurrer\", which was printed on the eve of the Whitehall Conference, and the pamphlet \"Anglo-Judaeus or The History of the Jews Whilst Here in England\" by W.H. both doubt that the Jews would be converted once in England. Many millenarians at the time emphasised the chosen role of England in God’s plan, and this was often accompanied by the identification the Jews as the true Israel of the Bible. Indeed, they saw the Jews as a superior group, sharing some characteristics with the chosen nation of England. This belief was rooted in the literal interpretation of the Biblical primacy of the Jews found in the writings of Thomas Brightman. This meant that if the Jews were specially favoured by God, the English must listen to their appeals for help. These philo-semitic figures, who also believed in the restoration of the Jews to the Holy Land, included Jeremiah Burroughs, Peter Bulkeley (whose father had given Brightman’s funeral sermon), John Fenwicke, and John Cotton.\n\nThe original petition for readmission was submitted by Johanna and Ebenezer Cartwright, two English baptists living in Amsterdam, to Thomas Fairfax’s Council of War in January 1649. As well as asking that Jews be allowed to live in England, their petition also expressed the wish that the Jews \"shall come to know the Emanuell\" and that they be transported to the \"Land promised to their fore-fathers\". It can be seen as a distillation of the Judeo-centric trends of Puritan thought that had developed over the previous century since John Bale (1495 – 1563). However, the petition was sent the day before the high court was established to try Charles I, so in the ensuing turmoil the Cartwrights never received an answer. The following year Amsterdam-based Rabbi and diplomat Menasseh Ben Israel wrote in his book \"Hope of Israel\" of the necessity of the Jews being \"spread out to the ends of the earth\" (Daniel 12:7) before they could be redeemed. The book was originally published in Dutch and Latin in 1650, and then in English (dedicated to Parliament and the Council of State) in 1652. In 1651 Ben Israel met Oliver St John and his envoys on their mission to secure an Anglo-Dutch coalition. The English were impressed by learning and manner, and advised him to formally apply for Jewish readmission to England.\n\nIn 1653, at Oliver St John’s suggestion, Cromwell issued an official directive to authorise, \"Menasseh ben Israel, a rabbi of the Jewish nation, well respected for his learning and good affection to the State, to come from Amsterdam to these parts.\" Fearing local anti-English opinion so soon after war, ben Israel turned down the invitation. But by the middle of the decade, Cromwell was taking advice from Marrano trader Simon de Caceres. At de Caceres suggestion, Cromwell dispatched Marrano physician Abraham de Mercado and his son Raphael to Barbados (which a few years previously had already started admitting Jews escaping from the Portuguese reconquest of Dutch Brazil), where he explored the possibility of Jews setting in Jamaica. There they would be offered full civil rights and even land grants.\n\nThere is some difference of opinion as to Oliver Cromwell’s opinions regarding the readmission of the Jews. It has been pointed out that he held many of the same hopes regarding the readmission and conversion of the Jews as the millenarians. Paul Rycaut, later ambassador to the port of Smyrna recalled the Whitehall Conference, \"When they all met, he (Cromwell) ordered the Jews to speak for themselves. After that he turned to the clergy, who inveigled much against the Jews as a cruel and cursed people. Cromwell in his answer to the Clergy called them ‘Men of God’ and desired to be informed by the whether it was not their opinion that the Jews were one day to be called into the Church? He then desired to know whether it was not every Christian man’s duty to forward that good end all he could?… was it not then our duty… to encourage them to settle ere where they could be taught the thuth…[sic]\" It has also been pointed out that Cromwell held more practical beliefs. Cromwell believed that Jews could be used as skilled purveyors of foreign intelligence (which would assist his imperial ambitions). Further, toleration of Protestant sects made political sense for Cromwell as it prevented disorder and promoted harmony. He justified the readmission of the Jews using this same tolerant approach, as well as believing that it would improve trade (he saw the Jews as an important part of Amsterdam’s financial success).\n\nCompetition with the Dutch for trade and the increasingly protectionist commercial policy that led to the Navigation Act in October 1651 made Oliver Cromwell want to attract the rich Jews of Amsterdam to London so that they might transfer their important trade interests with the Spanish Main from the Netherlands to England. The mission of Oliver St John to Amsterdam, though failing to establish a coalition between English and Dutch commercial interests as an alternative to the Navigation Act, had negotiated with Menasseh Ben Israel and the Amsterdam community. A pass was granted to Menasseh to enter England, but he was unable to use it because of the First Anglo-Dutch War, which lasted from 1652 to 1654.\n\nThe years 1655 and 1656 were to prove decisive in the history of the resettlement of the Jews in England. The first of these was the visit of Menasseh ben Israel and the second was the case of the Murrano trader Antonio Rodrigues Robles.\n\nMenasseh ben Israel’s son Samuel had arrived in England accompanied by trader David Dormido in 1653 to investigate the possibility of the resettlement of the Jews. In May 1655, he was sent back to Amsterdam in order to try to convince his father to visit England. The rabbi came to England in September 1655 with three other local rabbis, where they were lodged as guests of Cromwell. There he printed his \"humble address\" to Cromwell. (When ben Israel began his stay in London it is reckoned that there were about 20 New Christian families living in the city.) As a consequence, a national conference was summoned at Whitehall in the early part of December, which included some of the most eminent lawyers, clergymen, and merchants in the country. The lawyers declared no opposition to the Jews' residing in England, but both the clergymen and merchants were opposed to readmission, leading Cromwell to stop the discussion to prevent an adverse decision. Nonetheless, some change to official policy must have occurred, because the diarist John Evelyn wrote in his diary on 14 December, \"Now were the Jews admitted\".\n\nEarly in the following year (1656), the question came to a practical issue through the declaration of war against Spain, which resulted in the arrest of Antonio Rodrigues Robles, one of the community of Iberian New Christians who traded between London and the Canary Islands. Robles petitioned for the return of his seized property on account of his being ‘of the Hebrew nation’ rather than Spanish. At the same time six leading members of the New Christian community petitioned Cromwell for permission to gather to worship and acquire a burial ground. Although no formal permission was granted, some assurances must have been given because in the summer Menasseh asked for the Torah scroll to be sent over from Amsterdam, and in the autumn Moses Athias moved from Hamburg to act as religious preceptor. By December 1656 they had rented a house for use as a synagogue, and services began in January of 1657. In February of 1657 the new community, represented by Antonio Fernandez Carvajal and Simon de Caceres, acquired land near Mile End for use as a Synagogue. Historian Todd Endelman makes the point that it is unlikely this activity could have happened without Cromwell’s permission that they could live as professing Jews. The informal nature of the resettlement also meant the forces ranged against it had no target and never united to form any significant opposition. Further, at a later date it meant there were no restrictive laws to repeal when Jews wanted fuller citizenship rights. By the end of the decade the number of Jewish families had risen to thirty five. In 1657 Solomon Dormido, a nephew of Menasseh Ben Israel, was admitted to the Royal Exchange as a duly licensed broker of the City of London, without taking the usual oath involving a statement of faith in Christianity, (when he was finally sworn in in 1668, the oath was changed for him). Carvajal had previously been granted letters of denization for himself and his son, which guaranteed certain rights of citizenship.\n\nDuring the years 1655-56 the question of the return of Jews to England was fought in a pamphlet war. Conservative opponents including William Prynne opposed the return while the Quaker Margaret Fell was in favour. Christian supporters believed the conversion of Jews was a sign of the end times and the readmission to England was a step towards that goal.\n\nThis method of debate had the advantage of not raising anti-Semitic feelings too strongly; and it likewise enabled Charles II, on his Restoration in 1660, to avoid taking any action on the petition of the merchants of London asking him to revoke Cromwell's concession. He had been assisted during his exile by several Jews of royalist sympathies, such as Andrea Mendes da Costa (Chamberlain of Catherine of Braganza, wife of Charles II), Antonio Mendes (the physician brother of Andrea, who had cured Catherine of erysipelas while in Portugal) and Augustine Coronel-Chacon. In 1664 a further attempt was made by the Earl of Berkshire and Paul Ricaut to bring about the expulsion of the Jews, but the King-in-Council assured the latter of the continuance of former favour. Similar appeals to prejudice were made in 1673, when Jews, for meeting in Duke's Place for a religious service, were indicted on a charge of rioting, and in 1685, when thirty-seven were arrested on the Royal Exchange; but the proceedings in both cases were put a stop to by direction of the Privy Council. The status of the Jews was still very indeterminate. In 1684, it was contended by the East India Company that they were alien infidels, and perpetual enemies to the English crown. Even the Attorney-General declared that they resided in England only under an implied licence. As a matter of fact, the majority of them were still aliens and liable to all the disabilities that condition carried with it.\n\nWilliam III is reported to have been assisted in his ascent to the English throne by a loan of 2,000,000 guilders from Antonio Lopez Suasso (1614–1685) (of the well-known Lopes Suasso family), later made first Baron d'Avernas le Gras by Charles II of Spain. William did not interfere when in 1689 some of the chief Jewish merchants of London were forced to pay the duty levied on the goods of aliens, but he refused a petition from Jamaica to expel the Jews. William's reign brought about a closer connection between the predominantly Sephardic communities of London and Amsterdam; this aided in the transfer of the European finance centre from the Dutch capital to the English capital. Over this time a small German Ashkenazi community had arrived and established their own synagogue in 1692, but they were of little mercantile consequence, and did not figure in the relations between the established Jewish community and the government.\n\nEarly in the eighteenth century the Jewish community of London comprised representatives of the chief Jewish financiers in northern Europe; these included the Mendez da Costa, Abudiente, Salvador, Lopez, Fonseca, and Seixas families. The utility of these prominent Jewish merchants and financiers was widely recognised. Marlborough in particular made great use of the services of Sir Solomon de Medina, and indeed was publicly charged with taking an annual subvention from him. The early merchants of the resettlement are estimated to have brought with them a capital of £1,500,000 into the country; this amount is estimated to have increased to £5,000,000 by the middle of the 18th century.\n\nAs early as 1723 an act of Parliament allowed Jews holding land to omit the words \"on the true faith of a Christian\", when registering their title. Only once more would this allowance be made in the passage of the Plantation Act 1740, but more significantly the act allowed Jews who had or would have resided in British America for seven years to become naturalised British subjects.\n\nShortly afterwards a similar bill was introduced into the Irish Parliament, where it passed the Commons in 1745 and 1746, but failed to pass the Lords in 1747; it was ultimately dropped. Meanwhile, during the Jacobite rising of 1745 the Jews had shown particular loyalty to the government. Their chief financier, Samson Gideon, had strengthened the stock market, and several of the younger members had volunteered in the corps raised to defend London.\n\n\n\n"}
{"id": "31595608", "url": "https://en.wikipedia.org/wiki?curid=31595608", "title": "Social tuning", "text": "Social tuning\n\nSocial tuning, the process whereby people adopt other people's attitudes, is cited by social psychologists to demonstrate an important lack of people's conscious control over their actions.\n\nThe process of social tuning is particularly powerful in situations where one person wants to be liked or accepted by another person or group. However, social tuning occurs both when people meet for the first time, as well as among people who know each other well. Social tuning occurs both consciously and subconsciously. As research continues, the application of the theory of social tuning broadens.\n\nSocial psychology bases many of its concepts on the belief that a person's self concept is shaped by the people with whom he or she interacts. Social tuning allows people to learn about themselves and the social world through their interactions with others. People mold their own views to match those of the people surrounding them through social tuning in order to develop meaningful relationships. These relationships then play an integral role in developing one's self-esteem and self-concept.\n\nSocial tuning theory describes the process whereby people adopt another person's attitudes or opinions regarding a particular subject. This phenomenon is also termed \"shared reality theory.\" The study of this occurrence began in 1902 when Charles Cooley coined the term \"looking glass self\", stating that people see themselves and their own social world through the eyes of others. Research further discovered that people create their self-images through their beliefs of how others perceive them. Many people adopt the views of those surrounding them in an effort to feel like they belong and feel liked. In 1934, Mead determined that not only do individuals shape their self-concepts according to the perspectives of others, but also that people's views of themselves are continually maintained according to these adopted ideas.\n\nLater research showed that social tuning tends to be a particularly strong phenomenon when two people want to get along with each other. This is shown through social bonds which can be strengthened and reinforced through a perceived sense of shared beliefs. In addition, these shared ideas create a person's comprehension of their environment and world as a whole. Individuals believe that they have the same attitudes regarding certain ideas and experiences as the other.\n\nOne particular aspect of social tuning, stereotyping, has been a popular theme in this field's research over time. One specific method explores the idea that individuals of a certain group are influenced by the ideas of others from the out-group (Crocker, Major & Steele, 1998). These interactions yield the particular stereotyped group to internalize and believe the way others view them. Therefore, self-stereotyping manifests in certain individuals. However, in this case, the presence of self-stereotyping is immensely determined by the relationship with whom the stereotyped individual interacts with.\n\nIn 2006, Sinclair and Huntsinger explored the idea of why other people will change their beliefs and attitudes in order to get along with others and feel accepted. Their research focuses on why individuals from targeted groups will act and behave according to \"cultural stereotypes\". They used two hypotheses originally coined by Hardin & Conley in 2001, \"Affiliative Social-Tuning\" and \"Domain Relevance Hypothesis\". The first of these, \"Affiliative Social-Tuning Hypothesis\", pertains to the idea that certain concepts will be shared between individuals especially when affiliative motivation is high. For example, in a situation with a member from a targeted group and a member from a neutral group, the former will act accordingly to how the latter stereotypes his group. However, this is contingent on the fact that affiliative motivation is high, in other words, if there is a desire for the former to create a bond with the latter. The second, \"Domain Relevance Hypothesis\", explains that \"when confronted with multiple applicable views on which to construct a shared understanding with another person, an individual will choose to social tune toward only those views that will lead to the development of the most precise shared understanding with the person\". In other words, when many views are available to be socially tuned between individuals, only certain concepts will be shared. The concepts chosen are the ideas that yield the best common understanding between the two individuals.\n\nOne of the most famous experiments demonstrating the social tuning phenomenon was done by Stacy Sinclair. Her research reveals the effect of likability on people's drive to social tune. Participants engaged with researchers who were either likable or non-likable and who were either wearing a shirt stating anti-racist thoughts, specifically a shirt with the word \"Eracism\", or a blank shirt. The participants were then asked to complete a subconscious prejudice test, and when the researcher was likable, participants demonstrated significantly less racist attitudes on the test, than if that researcher was unlikable. Since the participants seemed to shape their views to that of the experimenter only when he or she was \"likable\", this study can be shown to reveal that people are more prone to adopt the views of others through social tuning when they like that person. This aspect of social tuning could be explained by the psychological assumption that people want to be liked by those that they themselves like, and therefore people will shape their views to match those of a person from whom they seek social acceptance.\n\nA similar study by Janetta Lun demonstrates another aspect of social tuning with respect to racism, and suggests that people who do not already hold strong beliefs about social prejudice are more likely to socially tune their beliefs with others around them than those who do already hold strong opinions. In this study, participants were given an implicit attitude test to determine their existing levels of implicit prejudice. Next they were taken into another room with either an experimenter who had the word \"ERACISM\" on their shirt, or with an experimenter in a plain T-shirt for the control condition. In the first condition, participants were asked to read the word \"ERACISM\" off the experimenter's T-shirt, and then in the control they were asked to read a series of nonsense letters. They were then given another implicit attitudes test to determine their implicit prejudice.\n\nLun found that people who had less accessible attitudes (determined by the first implicit attitudes test) had lower implicit prejudice after interacting with the experimenter who held clear egalitarian views. Alternatively, those who already held strong beliefs about prejudice did not change their implicit prejudice after interacting with the egalitarian experimenter. \nThis study demonstrates that when individuals do not hold already strong beliefs, they are more likely to seek knowledge from those around them, and therefore more likely to engage in social tuning. Lun's experiment suggests the likelihood of social tuning when people seek knowledge on a particular subject. In this case, the participants who did not hold strong opinions on the subject of prejudice, and thus presumably had less knowledge on the subject, molded their opinions to match the information they were given by the experimenter in the form of the word \"ERACISM\" on her shirt, and therefore they demonstrated stronger egalitarian views than they had when initially arriving at the experiment. People who are uncomfortable in situations where they feel they do not have enough information will attempt to get information through their interactions with others. Lun's experiment reveals how social tuning is a part of such a process, in which people with less knowledge are more likely to mold their beliefs to that of others.\n\nFurther research by Aaron Root has been completed, examining opinions on homosexuality. During this experiment, the researcher, who always wore a pro-homosexuality shirt, followed a script that was designed to control the participant's desire to get along with the researcher. In the high-level condition, the experimenter was friendly and amiable; he offered candy at the beginning of the study and spoke enthusiastically about the experiment. In the low-level condition, the experimenter's dialogue was concise (to the point of rudeness) and he even made a point of putting away the candy basket without offering any. To emphasize this action, the experimenter made a comment about not knowing why the other experimenters insisted on giving candy out. In the trials with an amicable experimenter, the participants were more gay-friendly on the implicit attitudes tests. The opposite resulted from the interaction with a less-likable experimenter. The subject would adopt the views of the message printed on the experimenter's shirt if he was nicer.\n\nCurtis Hardin, co-author of \"Shared Reality, System Justification, and the Relational Basis of Ideological Beliefs\", has performed numerous experiments in social tuning across a wide variety of ideals. His experiments explore how individual experience reflects a kind of tension among relationships. In one experiment, automatic homophobic attitudes manifest in the participant after an interaction with an evidently gay experimenter, but only for subjects who do not have gay friends. In another study by Hardin, unconscious threats to religious experience reduce commitment for participants who do not share the experience with their father or minister. For those who do perceive the religious experience to be shared, the unconscious threat causes increased religious commitment. In a third study, people become more anti-black when they are included (as opposed to excluded) in a game played with ostensible racists. The effect is reversed when the participants have are extra motivated to engage with the racists. Similar studies have been performed with gender.\n\nIn an experiment by Hardin and Higgins (1996), participants were given information about a \"target\" which they would inform a perceived audience about. The \"communicators\" changed the different summaries of information (sometimes to incorrect information) to best correspond with the attitudes of the audience which they were informed of by the researchers. The motivation of the communicator determined the extent to which they would alter their message to suit the audience. However, the communicator soon began believing their edited information about the target as a direct source of information. The memory and beliefs towards the target were then influenced. As time passed, communicators' belief that their message as a source of information about the target increased and their memory was altered. Thus the communicator adopted the believed beliefs of the audience which he was trying to inform.\n\nSocial tuning has resounding influences on both the memory and cognition of those affected by this process. Though social tuning could potentially aid memory and cognition should the views of the other person be correct, this phenomenon could also impede memory and cause incorrect cognition. For example, if an individual seeks a relationship with another individual who holds negative opinions about homosexuality, the first individual could be at risk for mirroring those negative opinions in order to be liked by the second individual. Such phenomenon can be harmful, in that they can cause people to hold opinions on subjects that are not based on concrete information, but on the opinions of others.\n\nSocial tuning can be particularly strong when in relation to controversial topics. Judgments based on sexual orientation, race, religion and even a politics can be substantially altered based on the opinions of those around a subject. The result being that it is not only highly unlikely that a subject would openly disagree with these feelings, but actually adopt them and proclaim them as genuinely their own. In addition, social tuning is a large contributor to prejudice and racism. For example, many individuals adopt their views about race or about their out-groups according to the ideas of those surrounding them. Social tuning with respect to race often occurs through parental influence. A child without sufficient information about race, and who seeks the approval of his or her parent will likely shape their own beliefs about race based on the beliefs of their parents.\n\nAs noted earlier (see \"Major Theoretical Approaches\"), self tuning has been linked to many issues regarding stereotyping. For example, individuals of commonly stereotyped groups are at risk of social tuning in certain situations. For example, Michael Inzlicht coined the term \"threatening environments\", which pertain to occasions when individuals perceive that they are being \"devalued, stigmatized, or discriminated against\" by a non-stereotyped group. In this environment, it is common that the individuals of a perceived \"lower-status\" will social tune to the ideas of the \"higher-status\". This results in internalized racism of the former group. However, it has been noted that under certain conditions interactions between heterogeneous groups can result more in a more positive manner. For example, situations that are perceived as \"safe and nonthreatening\", by both the stigmatized and non-stigmatized group \"are likely to lead to positive self-expansion and social tuning, reduced prejudice and discrimination, and positive group attitudes\".\n\nOn the other hand, research has showed that self tuning to ideas of one's ingroup, and not one's outgroup can often lead to more damaging results (7). Due to the fact that members of the same group are closer and trust one another, they are more likely to tune to the ideas of each other. Therefore, a member that holds a negative self-stereotype of himself and his own group is more dangerous to his comrades than an individual on the outside who shares the same views. Research has been conducted on how an individual from a stereotyped group can best avoid the dangers of self-tuning from an out-group. As Sinclair suggests, \"members of stigmatized groups need to be careful with whom they develop relationships\", and thus they \"can reduce the likelihood of negative social tuning by remaining interpersonally distant from those with stereotypical views\".\n\nSocial tuning is an intriguing social phenomenon that affects our personal beliefs and views both on a long-term and short-term basis. It impacts many important aspects of an individual's life, and can even play a role in determining a person's beliefs on a variety of important subjects. For example, it plays a large role in our self-concept and our views of others. There are certain situations which heighten the likelihood that a person will engage in social tuning, for example when an individual wants to be liked by another or when an individual does not already hold strong opinions on a subject. Overall, social tuning is an important social psychological theory as it explains the many beliefs we hold about ourselves, others, and the world around us.\n\n\n"}
{"id": "12515898", "url": "https://en.wikipedia.org/wiki?curid=12515898", "title": "Spamusement!", "text": "Spamusement!\n\nSpamusement! is a webcomic originally created by software developer Steven Frank, in which Frank took subject lines from real spam emails and turned them into single-panel comics. Some of these were literal interpretations of subject lines, while others put a twist on what illustration the reader may have expected.\n\nFrank describes his work as \"poorly drawn cartoons inspired by actual spam subject lines.\" To draw the comics, Frank uses a Wacom graphics tablet to sketch his ideas freehand into his computer, using the application Adobe Photoshop Elements.\n\nFrank's cartoons have occasionally featured repeating characters such Cabinet Sanchez, Eggplant Mike and Troy Powell. From the email spam line, \"you were wrong cabinet sanchez\", Frank drew a file cabinet wearing a bandolier, a Mexican sombrero, and sporting a large black mustache. He published it on July 23, 2004. The cartoon proved popular and was made into merchandise such as T-shirts. Later in 2004, Worm Quartet began performing an original song called \"You Were Wrong Cabinet Sanchez\" and in 2008 the artist released it as part of the album \"Mental Notes\". The song has been featured on the Dr. Demento radio show during the final \"Funny Five\" countdown, and was released on \"Laughter is a Powerful Weapon Volume 2\", a compilation of songs from the radio show. \n\nThe \"Spamusement!\" site also features forums, one section of which allows readers to create their own spam-inspired comics. Frank did not contribute for three months in early 2006, then posted irregularly from June 2006. Since October 2006, Frank completely turned over the responsibility of comic-making to the forums, although another original comic by Frank appeared on the site in late June 2007.\n\n\n"}
{"id": "5894781", "url": "https://en.wikipedia.org/wiki?curid=5894781", "title": "Standard solar model", "text": "Standard solar model\n\nThe standard solar model (SSM) is a mathematical treatment of the Sun as a spherical ball of gas (in varying states of ionisation, with the hydrogen in the deep interior being a completely ionised plasma). This model, technically the spherically symmetric quasi-static model of a star, has stellar structure described by several differential equations derived from basic physical principles. The model is constrained by boundary conditions, namely the luminosity, radius, age and composition of the Sun, which are well determined. The age of the Sun cannot be measured directly; one way to estimate it is from the age of the oldest meteorites, and models of the evolution of the Solar System. The composition in the photosphere of the modern-day Sun, by mass, is 74.9% hydrogen and 23.8% helium. All heavier elements, called \"metals\" in astronomy, account for less than 2 percent of the mass. The SSM is used to test the validity of stellar evolution theory. In fact, the only way to determine the two free parameters of the stellar evolution model, the helium abundance and the mixing length parameter (used to model convection in the Sun), are to adjust the SSM to \"fit\" the observed Sun.\n\nA star is considered to be at zero age (protostellar) when it is assumed to have a homogeneous composition and to be just beginning to derive most of its luminosity from nuclear reactions (so neglecting the period of contraction from a cloud of gas and dust). To obtain the SSM, a one solar mass () stellar model at zero age is evolved numerically to the age of the Sun. The abundance of elements in the zero age solar model is estimated from primordial meteorites. Along with this abundance information, a reasonable guess at the zero-age luminosity (such as the present-day Sun's luminosity) is then converted by an iterative procedure into the correct value for the model, and the temperature, pressure and density throughout the model calculated by solving the equations of stellar structure numerically assuming the star to be in a steady state. The model is then evolved numerically up to the age of the Sun. Any discrepancy from the measured values of the Sun's luminosity, surface abundances, etc. can then be used to refine the model. For example, since the Sun formed, some of the helium and heavy elements have settled out of the photosphere by diffusion. As a result, the Solar photosphere now contains about 87% as much helium and heavy elements as the protostellar photosphere had; the protostellar Solar photosphere was 71.1% hydrogen, 27.4% helium, and 1.5% metals. A measure of heavy-element settling by diffusion is required for a more accurate model.\n\nThe differential equations of stellar structure, such as the equation of hydrostatic equilibrium, are integrated numerically. The differential equations are approximated by difference equations. The star is imagined to be made up of spherically symmetric shells and the numerical integration carried out in finite steps making use of the equations of state, giving relationships for the pressure, the opacity and the energy generation rate in terms of the density, temperature and composition.\n\nNuclear reactions in the core of the Sun change its composition, by converting hydrogen nuclei into helium nuclei by the proton-proton chain and (to a lesser extent in the Sun than in more massive stars) the CNO cycle. This increases the mean molecular weight in the core of the Sun, which should lead to a decrease in pressure. This does not happen as instead the core contracts. By the virial theorem half of the gravitational potential energy released by this contraction goes towards raising the temperature of the core, and the other half is radiated away. This increase in temperature also increases the pressure and restores the balance of hydrostatic equilibrium. The luminosity of the Sun is increased by the temperature rise, increasing the rate of nuclear reactions. The outer layers expand to compensate for the increased temperature and pressure gradients, so the radius also increases.\n\nNo star is completely static, but stars stay on the main sequence (burning hydrogen in the core) for long periods. In the case of the Sun, it has been on the main sequence for roughly 4.6 billion years, and will become a red giant in roughly 6.5 billion years for a total main sequence lifetime of roughly 11 billion (10) years. Thus the assumption of steady state is a very good approximation. For simplicity, the stellar structure equations are written without explicit time dependence, with the exception of the luminosity gradient equation:\n\nHere L is the luminosity, ε is the nuclear energy generation rate per unit mass and ε is the luminosity due to neutrino emission (see below for the other quantities). The slow evolution of the Sun on the main sequence is then determined by the change in the nuclear species (principally hydrogen being consumed and helium being produced). The rates of the various nuclear reactions are estimated from particle physics experiments at high energies, which are extrapolated back to the lower energies of stellar interiors (the Sun burns hydrogen rather slowly). Historically, errors in the nuclear reaction rates have been one of the biggest sources of error in stellar modelling. Computers are employed to calculate the varying abundances (usually by mass fraction) of the nuclear species. A particular species will have a rate of production and a rate of destruction, so both are needed to calculate its abundance over time, at varying conditions of temperature and density. Since there are many nuclear species, a computerised reaction network is needed to keep track of how all the abundances vary together.\n\nAccording to the Vogt-Russell theorem, the mass and the composition structure throughout a star uniquely determine its radius, luminosity, and internal structure, as well as its subsequent evolution (though this \"theorem\" was only intended to apply to the slow, stable phases of stellar evolution and certainly does not apply to the transitions between stages and rapid evolutionary stages).\nThe information about the varying abundances of nuclear species over time, along with the equations of state, is sufficient for a numerical solution by taking sufficiently small time increments and using iteration to find the unique internal structure of the star at each stage.\n\nThe SSM serves two purposes:\n\nLike the Standard Model of particle physics and the standard cosmology model the SSM changes over time in response to relevant new theoretical or experimental physics discoveries.\n\nAs described in the Sun article, the Sun has a radiative core and a convective outer envelope. In the core, the luminosity due to nuclear reactions is transmitted to outer layers principally by radiation. However, in the outer layers the temperature gradient is so great that radiation cannot transport enough energy. As a result, thermal convection occurs as thermal columns carry hot material to the surface (photosphere) of the Sun. Once the material cools off at the surface, it plunges back downward to the base of the convection zone, to receive more heat from the top of the radiative zone.\n\nIn a solar model, as described in stellar structure, one considers the density formula_2, temperature T(r), total pressure (matter plus radiation) P(r), luminosity l(r) and energy generation rate per unit mass ε(r) in a spherical shell of a thickness dr at a distance r from the center of the star.\n\nRadiative transport of energy is described by the radiative temperature gradient equation:\nwhere κ is the opacity of the matter, σ is the Stefan-Boltzmann constant, and the Boltzmann constant is set to one.\n\nConvection is described using mixing length theory and the corresponding temperature gradient equation (for adiabatic convection) is:\nwhere γ = c / c is the adiabatic index, the ratio of specific heats in the gas. (For a fully ionized ideal gas, γ = 5/3.)\n\nNear the base of the Sun's convection zone, the convection is adiabatic, but near the surface of the Sun, convection is not adiabatic.\n\nA more realistic description of the uppermost part of the convection zone is possible through detailed three-dimensional and time-dependent hydrodynamical simulations, taking into account radiative transfer in the atmosphere. Such simulations successfully reproduce the observed surface structure of solar granulation, as well as detailed profiles of lines in the solar radiative spectrum, without the use of parametrized models of turbulence. The simulations only cover a very small fraction of the solar radius, and are evidently far too time-consuming to be included in general solar modeling. Extrapolation of an averaged simulation through the adiabatic part of the convection zone by means of a model based on the mixing-length description, demonstrated that the adiabat predicted by the simulation was essentially consistent with the depth of the solar convection zone as determined from helioseismology. An extension of mixing-length theory, including effects of turbulent pressure and kinetic energy, based on numerical simulations of near-surface convection, has been developed.\n\nThis section is adapted from the Christensen-Dalsgaard review of helioseismology, Chapter IV.\n\nThe numerical solution of the differential equations of stellar structure requires equations of state for the pressure, opacity and energy generation rate, as described in stellar structure, which relate these variables to the density, temperature and composition.\n\nHelioseismology is the study of the wave oscillations in the Sun. Changes in the propagation of these waves through the Sun reveal inner structures and allow astrophysicists to develop extremely detailed profiles of the interior conditions of the Sun. In particular, the location of the convection zone in the outer layers of the Sun can be measured, and information about the core of the Sun provides a method, using the SSM, to calculate the age of the Sun, independently of the method of inferring the age of the Sun from that of the oldest meteorites. This is another example of how the SSM can be refined.\n\nHydrogen is fused into helium through several different interactions in the Sun. The vast majority of neutrinos are produced through the pp chain, a process in which four protons are combined to produce two protons, two neutrons, two positrons, and two electron neutrinos. Neutrinos are also produced by the CNO cycle, but that process is considerably less important in the Sun than in other stars.\n\nMost of the neutrinos produced in the Sun come from the first step of the pp chain but their energy is so low (<0.425 MeV) they are very difficult to detect. A rare side branch of the pp chain produces the \"boron-8\" neutrinos with a maximum energy of roughly 15 MeV, and these are the easiest neutrinos to detect. A very rare interaction in the pp chain produces the \"hep\" neutrinos, the highest energy neutrinos predicted to be produced by the Sun. They are predicted to have a maximum energy of about 18 MeV.\n\nAll of the interactions described above produce neutrinos with a spectrum of energies. The electron capture of Be produces neutrinos at either roughly 0.862 MeV (~90%) or 0.384 MeV (~10%).\n\nThe weakness of the neutrino's interactions with other particles means that most neutrinos produced in the core of the Sun can pass all the way through the Sun without being absorbed. It is possible, therefore, to observe the core of the Sun directly by detecting these neutrinos.\n\nThe first experiment to successfully detect cosmic neutrinos was Ray Davis's chlorine experiment, in which neutrinos were detected by observing the conversion of chlorine nuclei to radioactive argon in a large tank of perchloroethylene. This was a reaction channel expected for neutrinos, but since only the number of argon decays was counted, it did not give any directional information, such as where the neutrinos came from. The experiment found about 1/3 as many neutrinos as were predicted by the standard solar model of the time, and this problem became known as the solar neutrino problem.\n\nWhile it is now known that the chlorine experiment detected neutrinos, some physicists at the time were suspicious of the experiment, mainly because they did not trust such radiochemical techniques. Unambiguous detection of solar neutrinos was provided by the Kamiokande-II experiment, a water Cerenkov detector with a low enough energy threshold to detect neutrinos through neutrino-electron elastic scattering. In the elastic scattering interaction the electrons coming out of the point of reaction strongly point in the direction that the neutrino was travelling, away from the Sun. This ability to \"point back\" at the Sun was the first conclusive evidence that the Sun is powered by nuclear interactions in the core. While the neutrinos observed in Kamiokande-II were clearly from the Sun, the rate of neutrino interactions was again suppressed compared to theory at the time. Even worse, the Kamiokande-II experiment measured about 1/2 the predicted flux, rather than the chlorine experiment's 1/3.\n\nThe solution to the solar neutrino problem was finally experimentally determined by the Sudbury Neutrino Observatory (SNO). The radiochemical experiments were only sensitive to electron neutrinos, and the signal in the water Cerenkov experiments was dominated by the electron neutrino signal. The SNO experiment, by contrast, had sensitivity to all three neutrino flavours. By simultaneously measuring the electron neutrino and total neutrino fluxes the experiment demonstrated that the suppression was due to the MSW effect, the conversion of electron neutrinos from their pure flavour state into the second neutrino mass eigenstate as they passed through a resonance due to the changing density of the Sun. The resonance is energy dependent, and \"turns on\" near 2MeV. The water Cerenkov detectors only detect neutrinos above about 5MeV, while the radiochemical experiments were sensitive to lower energy (0.8MeV for chlorine, 0.2MeV for gallium), and this turned out to be the source of the difference in the observed neutrino rates at the two types of experiments.\n\nAll neutrinos from the proton–proton chain reaction (PP neutrinos) have been detected except hep neutrinos (next point). Three techniques have been adopted: The radiochemical technique, used by Homestake, Gallex, GNO and SAGE allowed to measure the neutrino flux above a minimum energy. The detector SNO used scattering on deuterium that allowed to measure the energy of the events, thereby identifying the single components of the predicted SSM neutrino emission. Finally, Kamiokande, Super-Kamiokande, SNO, Borexino and KamLAND used elastic scattering on electrons, which allows the measurement of the neutrino energy. Boron8 neutrinos have been seen by Kamiokande, Super-Kamiokande, SNO, Borexino, KamLAND. Beryllium7, pep, and PP neutrinos have been seen only by Borexino to date.\n\nThe highest energy neutrinos have not yet been observed due to their small flux compared to the boron-8 neutrinos, so thus far only limits have been placed on the flux. No experiment yet has had enough sensitivity to observe the flux predicted by the SSM.\n\nNeutrinos from the CNO cycle of solar energy generation – i.e., the CNO-neutrinos – are also expected to provide observable events below 1 MeV. They have not yet been observed due to experimental noise (background). Ultra-pure scintillator detectors have the potential to probe the flux predicted by the SSM. This detection could be possible already in Borexino; the next scientific occasions will be in SNO+ and, on the longer term, in LENA and JUNO, three detectors that will be larger but will use the same principles of Borexino.\n\nWhile radiochemical experiments have in some sense observed the pp and Be7 neutrinos they have measured only integral fluxes. The \"holy grail\" of solar neutrino experiments would detect the Be7 neutrinos with a detector that is sensitive to the individual neutrino energies. This experiment would test the MSW hypothesis by searching for the turn-on of the MSW effect. Some exotic models are still capable of explaining the solar neutrino deficit, so the observation of the MSW turn on would, in effect, finally solve the solar neutrino problem.\n\nThe flux of boron-8 neutrinos is highly sensitive to the temperature of the core of the Sun, formula_5. For this reason, a precise measurement of the boron-8 neutrino flux can be used in the framework of the standard solar model as a measurement of the temperature of the core of the Sun. This estimate was performed by Fiorentini and Ricci after the first SNO results were published, and they obtained a temperature of formula_6 from a determined neutrino flux of 5.2·10/cm·s.\n\nStellar models of the Sun's evolution predict the solar surface chemical abundance pretty well except for lithium (Li).\nThe surface abundance of Li on the Sun is 140 times less than the protosolar value (i.e. the primordial abundance at the Sun's birth), yet the temperature at the base of the surface convective zone is not hot enough to burn – and hence deplete – Li. This is known as the solar lithium problem. A large range of Li abundances is observed in solar-type stars of the same age, mass, and metallicity as the Sun. Observations of an unbiased sample of stars of this type with or without observed planets (exoplanets) showed that the known planet-bearing stars have less than one per cent of the primordial Li abundance, and of the remainder half had ten times as much Li. It is hypothesised that the presence of planets may increase the amount of mixing and deepen the convective zone to such an extent that the Li can be burned. A possible mechanism for this is the idea that the planets affect the angular momentum evolution of the star, thus changing the rotation of the star relative to similar stars without planets; in the case of the Sun slowing its rotation. More research is needed to discover where and when the fault in the modelling lies. Given the precision of helioseismic probes of the interior of the modern-day Sun, it is likely that the modelling of the protostellar Sun needs to be adjusted.\n\n\n"}
{"id": "70671", "url": "https://en.wikipedia.org/wiki?curid=70671", "title": "Stress–energy tensor", "text": "Stress–energy tensor\n\nThe stress–energy tensor, sometimes stress–energy–momentum tensor or energy–momentum tensor, is a tensor quantity in physics that describes the density and flux of energy and momentum in spacetime, generalizing the stress tensor of Newtonian physics. It is an attribute of matter, radiation, and non-gravitational force fields. The stress–energy tensor is the source of the gravitational field in the Einstein field equations of general relativity, just as mass density is the source of such a field in Newtonian gravity.\n\nThe stress–energy tensor involves the use of superscripted variables (\"not\" exponents; see tensor index notation and Einstein summation notation). If Cartesian coordinates in SI units are used, then the components of the position four-vector are given by: , , , and , where \"t\" is time in seconds, and \"x\", \"y\", and \"z\" are distances in meters.\n\nThe stress–energy tensor is defined as the tensor \"T\" of order two that gives the flux of the \"α\"th component of the momentum vector across a surface with constant \"x\" coordinate. In the theory of relativity, this momentum vector is taken as the four-momentum. In general relativity, the stress–energy tensor is symmetric,\n\nIn some alternative theories like Einstein–Cartan theory, the stress–energy tensor may not be perfectly symmetric because of a nonzero spin tensor, which geometrically corresponds to a nonzero torsion tensor.\n\nBecause the stress–energy tensor is of order two, its components can be displayed in 4 × 4 matrix form:\n\nIn the following, \"i\" and \"k\" range from 1 through 3.\n\nThe time–time component is the density of relativistic mass, i.e. the energy density divided by the speed of light squared. Its components have a direct physical interpretation. In the case of a perfect fluid this component is\n\nwhere formula_4 is the relativistic mass per unit volume, and for an electromagnetic field in otherwise empty space this component is\n\nwhere \"E\" and \"B\" are the electric and magnetic fields, respectively.\n\nThe flux of relativistic mass across the \"x\" surface is equivalent to the density of the \"i\"th component of linear momentum,\n\nThe components\nrepresent flux of \"i\"th component of linear momentum across the \"x\" surface. In particular,\n(not summed) represents normal stress, which is called pressure when it is independent of direction. The remaining components\nrepresent shear stress (compare with the stress tensor).\n\nIn solid state physics and fluid mechanics, the stress tensor is defined to be the spatial components of the stress–energy tensor in the proper frame of reference. In other words, the stress energy tensor in engineering \"differs\" from the stress–energy tensor here by a momentum convective term.\n\nIn most of this article we work with the contravariant form, \"T\" of the stress–energy tensor. However, it is often necessary to work with the covariant form,\n\nor the mixed form,\n\nor as a mixed tensor density\n\nIn this article we use the spacelike sign convention (−+++) for the metric signature.\n\nThe stress–energy tensor is the conserved Noether current associated with spacetime translations.\n\nThe divergence of the non-gravitational stress–energy is zero. In other words, non-gravitational energy and momentum are conserved,\nWhen gravity is negligible and using a Cartesian coordinate system for spacetime, this may be expressed in terms of partial derivatives as \n\nThe integral form of this is\n\nwhere \"N\" is any compact four-dimensional region of spacetime; formula_16 is its boundary, a three-dimensional hypersurface; and formula_17 is an element of the boundary regarded as the outward pointing normal.\n\nIn flat spacetime and using Cartesian coordinates, if one combines this with the symmetry of the stress–energy tensor, one can show that angular momentum is also conserved:\n\nWhen gravity is non-negligible or when using arbitrary coordinate systems, the divergence of the stress–energy still vanishes. But in this case, a coordinate free definition of the divergence is used which incorporates the covariant derivative\n\nwhere formula_20 is the Christoffel symbol which is the gravitational force field.\n\nConsequently, if formula_21 is any Killing vector field, then the conservation law associated with the symmetry generated by the Killing vector field may be expressed as\n\nThe integral form of this is\n\nIn general relativity, the symmetric stress–energy tensor acts as the source of spacetime curvature, and is the current density associated with gauge transformations of gravity which are general curvilinear coordinate transformations. (If there is torsion, then the tensor is no longer symmetric. This corresponds to the case with a nonzero spin tensor in Einstein–Cartan gravity theory.)\n\nIn general relativity, the partial derivatives used in special relativity are replaced by covariant derivatives. What this means is that the continuity equation no longer implies that the non-gravitational energy and momentum expressed by the tensor are absolutely conserved, i.e. the gravitational field can do work on matter and vice versa. In the classical limit of Newtonian gravity, this has a simple interpretation: energy is being exchanged with gravitational potential energy, which is not included in the tensor, and momentum is being transferred through the field to other bodies. In general relativity the Landau–Lifshitz pseudotensor is a unique way to define the \"gravitational\" field energy and momentum densities. Any such stress–energy pseudotensor can be made to vanish locally by a coordinate transformation.\n\nIn curved spacetime, the spacelike integral now depends on the spacelike slice, in general. There is in fact no way to define a global energy–momentum vector in a general curved spacetime.\n\nIn general relativity, the stress tensor is studied in the context of the Einstein field equations which are often written as\n\nwhere formula_25 is the Ricci tensor, formula_26 is the Ricci scalar (the tensor contraction of the Ricci tensor), formula_27 is the metric tensor, is the cosmological constant (negligible at the scale of a galaxy or smaller), and formula_28 is the universal gravitational constant.\n\nIn special relativity, the stress–energy of a non-interacting particle with mass \"m\" and trajectory formula_29 is:\n\nwhere formula_31 is the velocity vector (which should not be confused with four-velocity, since it is missing a formula_32)\nδ is the Dirac delta function and formula_34 is the energy of the particle.\n\nFor a perfect fluid in thermodynamic equilibrium, the stress–energy tensor takes on a particularly simple form\n\nwhere formula_4 is the mass–energy density (kilograms per cubic meter), formula_37 is the hydrostatic pressure (pascals), formula_38 is the fluid's four velocity, and formula_39 is the reciprocal of the metric tensor. Therefore, the trace is given by\n\nThe four velocity satisfies\n\nIn an inertial frame of reference comoving with the fluid, better known as the fluid's proper frame of reference, the four velocity is \n\nthe reciprocal of the metric tensor is simply\n\nand the stress–energy tensor is a diagonal matrix\n\nThe Hilbert stress–energy tensor of a source-free electromagnetic field is\n\nwhere formula_46 is the electromagnetic field tensor.\n\nThe stress–energy tensor for a complex scalar field formula_47 which satisfies the Klein–Gordon equation is\nand when the metric is flat (Minkowski) its components work out to be:\n\nThere are a number of inequivalent definitions of non-gravitational stress–energy:\n\nThe Hilbert stress–energy tensor is defined as the functional derivative\n\nwhere formula_51 is the nongravitational part of the Lagrangian density of the action. This is symmetric and gauge-invariant. See Einstein–Hilbert action for more information.\n\nNoether's theorem implies that there is a conserved current associated with translations through space and time. This is called the canonical stress–energy tensor. Generally, this is not symmetric and if we have some gauge theory, it may not be gauge invariant because space-dependent gauge transformations do not commute with spatial translations.\n\nIn general relativity, the translations are with respect to the coordinate system and as such, do not transform covariantly. See the section below on the gravitational stress–energy pseudo-tensor.\n\nIn the presence of spin or other intrinsic angular momentum, the canonical Noether stress energy tensor fails to be symmetric. The Belinfante–Rosenfeld stress energy tensor is constructed from the canonical stress–energy tensor and the spin current in such a way as to be symmetric and still conserved. In general relativity, this modified tensor agrees with the Hilbert stress–energy tensor.\n\nBy the equivalence principle gravitational stress–energy will always vanish locally at any chosen point in some chosen frame, therefore gravitational stress–energy cannot be expressed as a non-zero tensor; instead we have to use a pseudotensor.\n\nIn general relativity, there are many possible distinct definitions of the gravitational stress–energy–momentum pseudotensor. These include the Einstein pseudotensor and the Landau–Lifshitz pseudotensor. The Landau–Lifshitz pseudotensor can be reduced to zero at any event in spacetime by choosing an appropriate coordinate system.\n\n\n"}
{"id": "27873", "url": "https://en.wikipedia.org/wiki?curid=27873", "title": "Surjective function", "text": "Surjective function\n\nIn mathematics, a function \"f\" from a set \"X\" to a set \"Y\" is surjective (or onto), or a surjection, if for every element \"y\" in the codomain \"Y\" of \"f\" there is at least one element \"x\" in the domain \"X\" of \"f\" such that \"f\"(\"x\") = \"y\". It is not required that \"x\" be unique; the function \"f\" may map one or more elements of \"X\" to the same element of \"Y\".\n\nThe term \"surjective\" and the related terms \"injective\" and \"bijective\" were introduced by Nicolas Bourbaki, a group of mainly French 20th-century mathematicians who under this pseudonym wrote a series of books presenting an exposition of modern advanced mathematics, beginning in 1935. The French word \"sur\" means \"over\" or \"above\" and relates to the fact that the image of the domain of a surjective function completely covers the function's codomain.\n\nAny function induces a surjection by restricting its codomain to its range. Every surjective function has a right inverse, and every function with a right inverse is necessarily a surjection. The composite of surjective functions is always surjective. Any function can be decomposed into a surjection and an injection.\n\nA surjective function is a function whose image is equal to its codomain. Equivalently, a function \"f\" with domain \"X\" and codomain \"Y\" is surjective if for every \"y\" in \"Y\" there exists at least one \"x\" in \"X\" with formula_1. Surjections are sometimes denoted by a two-headed rightwards arrow (), as in \"f\" : \"X\" ↠ \"Y\".\n\nSymbolically,\n\nFor any set \"X\", the identity function id on \"X\" is surjective.\n\nThe function defined by \"f\"(\"n\") = \"n\" mod 2 (that is, even integers are mapped to 0 and odd integers to 1) is surjective.\n\nThe function defined by \"f\"(\"x\") = 2\"x\" + 1 is surjective (and even bijective), because for every real number \"y\" we have an \"x\" such that \"f\"(\"x\") = \"y\": an appropriate \"x\" is (\"y\" − 1)/2.\n\nThe function defined by \"f\"(\"x\") = \"x\" − 3\"x\" is surjective, because the pre-image of any real number \"y\" is the solution set of the cubic polynomial equation \"x\" − 3\"x\" − \"y\" = 0 and every cubic polynomial with real coefficients has at least one real root. However, this function is not injective (and hence not bijective) since e.g. the pre-image of \"y\" = 2 is {\"x\" = −1, \"x\" = 2}. (In fact, the pre-image of this function for every \"y\", −2 ≤ \"y\" ≤ 2 has more than one element.)\n\nThe function defined by \"g\"(\"x\") = \"x\" is \"not\" surjective, because there is no real number \"x\" such that \"x\" = −1. However, the function defined by \"g\"(\"x\") = \"x\" (with restricted codomain) \"is\" surjective because for every \"y\" in the nonnegative real codomain \"Y\" there is at least one \"x\" in the real domain \"X\" such that \"x\" = \"y\".\n\nThe natural logarithm function is a surjective and even bijective mapping from the set of positive real numbers to the set of all real numbers. Its inverse, the exponential function, is not surjective as its range is the set of positive real numbers, and its domain is usually defined to be the set of all real numbers. The matrix exponential is not surjective when seen as a map from the space of all \"n\"×\"n\" matrices to itself. It is, however, usually defined as a map from the space of all \"n\"×\"n\" matrices to the general linear group of degree \"n\", i.e. the group of all \"n\"×\"n\" invertible matrices. Under this definition the matrix exponential is surjective for complex matrices, although still not surjective for real matrices.\n\nThe projection from a cartesian product to one of its factors is surjective unless the other factor is empty.\n\nIn a 3D video game, vectors are projected onto a 2D flat screen by means of a surjective function.\n\nA function is bijective if and only if it is both surjective and injective.\n\nIf (as is often done) a function is identified with its graph, then surjectivity is not a property of the function itself, but rather a property of the mapping. This is, the function together with its codomain. Unlike injectivity, surjectivity cannot be read off of the graph of the function alone.\n\nThe function is said to be a right inverse of the function if \"f\"(\"g\"(\"y\")) = \"y\" for every \"y\" in \"Y\" (\"g\" can be undone by \"f\"). In other words, \"g\" is a right inverse of \"f\" if the composition of \"g\" and \"f\" in that order is the identity function on the domain \"Y\" of \"g\". The function \"g\" need not be a complete inverse of \"f\" because the composition in the other order, , may not be the identity function on the domain \"X\" of \"f\". In other words, \"f\" can undo or \"reverse\" \"g\", but cannot necessarily be reversed by it.\n\nEvery function with a right inverse is necessarily a surjection. The proposition that every surjective function has a right inverse is equivalent to the axiom of choice.\n\nIf is surjective and \"B\" is a subset of \"Y\", then \"f\"(\"f\"(\"B\")) = \"B\". Thus, \"B\" can be recovered from its preimage .\n\nFor example, in the first illustration, above, there is some function \"g\" such that \"g\"(\"C\") = 4. There is also some function \"f\" such that \"f\"(4) = \"C\". It doesn't matter that \"g\"(\"C\") can also equal 3; it only matters that \"f\" \"reverses\" \"g\".\n\nA function is surjective if and only if it is right-cancellative: given any functions , whenever \"g\" \"f\" = \"h\" \"f\", then \"g\" = \"h\". This property is formulated in terms of functions and their composition and can be generalized to the more general notion of the morphisms of a category and their composition. Right-cancellative morphisms are called epimorphisms. Specifically, surjective functions are precisely the epimorphisms in the category of sets. The prefix \"epi\" is derived from the Greek preposition \"ἐπί\" meaning \"over\", \"above\", \"on\".\n\nAny morphism with a right inverse is an epimorphism, but the converse is not true in general. A right inverse \"g\" of a morphism \"f\" is called a section of \"f\". A morphism with a right inverse is called a split epimorphism.\n\nAny function with domain \"X\" and codomain \"Y\" can be seen as a left-total and right-unique binary relation between \"X\" and \"Y\" by identifying it with its function graph. A surjective function with domain \"X\" and codomain \"Y\" is then a binary relation between \"X\" and \"Y\" that is right-unique and both left-total and right-total.\n\nThe cardinality of the domain of a surjective function is greater than or equal to the cardinality of its codomain: If is a surjective function, then \"X\" has at least as many elements as \"Y\", in the sense of cardinal numbers. (The proof appeals to the axiom of choice to show that a function\n\nSpecifically, if both \"X\" and \"Y\" are finite with the same number of elements, then is surjective if and only if \"f\" is injective.\n\nGiven two sets \"X\" and \"Y\", the notation is used to say that either \"X\" is empty or that there is a surjection from \"Y\" onto \"X\". Using the axiom of choice one can show that and together imply that |\"Y\"| = |\"X\"|, a variant of the Schröder–Bernstein theorem.\n\nThe composite of surjective functions is always surjective: If \"f\" and \"g\" are both surjective, and the codomain of \"g\" is equal to the domain of \"f\", then is surjective. Conversely, if is surjective, then \"f\" is surjective (but \"g\", the function applied first, need not be). These properties generalize from surjections in the category of sets to any epimorphisms in any category.\n\nAny function can be decomposed into a surjection and an injection: For any function there exist a surjection and an injection such that \"h\" = \"g\" \"f\". To see this, define \"Y\" to be the set of preimages where \"z\" is in . These preimages are disjoint and partition \"X\". Then \"f\" carries each \"x\" to the element of \"Y\" which contains it, and \"g\" carries each element of \"Y\" to the point in \"Z\" to which \"h\" sends its points. Then \"f\" is surjective since it is a projection map, and \"g\" is injective by definition.\n\nAny function induces a surjection by restricting its codomain to its range. Any surjective function induces a bijection defined on a quotient of its domain by collapsing all arguments mapping to a given fixed image. More precisely, every surjection can be factored as a projection followed by a bijection as follows. Let \"A\"/~ be the equivalence classes of \"A\" under the following equivalence relation: \"x\" ~ \"y\" if and only if \"f\"(\"x\") = \"f\"(\"y\"). Equivalently, \"A\"/~ is the set of all preimages under \"f\". Let \"P\"(~) : \"A\" → \"A\"/~ be the projection map which sends each \"x\" in \"A\" to its equivalence class [\"x\"], and let \"f\" : \"A\"/~ → \"B\" be the well-defined function given by \"f\"([\"x\"]) = \"f\"(\"x\"). Then \"f\" = \"f\" o \"P\"(~).\n\n"}
{"id": "37673", "url": "https://en.wikipedia.org/wiki?curid=37673", "title": "Symbol", "text": "Symbol\n\nA symbol is a mark, sign or word that indicates, signifies, or is understood as representing an idea, object, or relationship. Symbols allow people to go beyond what is known or seen by creating linkages between otherwise very different concepts and experiences. All communication (and data processing) is achieved through the use of symbols. Symbols take the form of words, sounds, gestures, ideas or visual images and are used to convey other ideas and beliefs. For example, a red octagon may be a symbol for \"STOP\". On a map, a blue line might represent a river. Numerals are symbols for numbers. Alphabetic letters may be symbols for sounds. Personal names are symbols representing individuals. A red rose may symbolize love and compassion. The variable 'x', in a mathematical equation, may symbolize the position of a particle in space.\n\nIn cartography, an organized collection of symbols forms a legend for a map.\n\nThe word \"symbol\" derives from the Greek σύμβολον \"symbolon\", meaning \"token, watchword\" from σύν \"syn\" \"together\" and βάλλω \"bállō\" \" \"I throw, put.\" The sense evolution in Greek is from \"throwing things together\" to \"contrasting\" to \"comparing\" to \"token used in comparisons to determine if something is genuine.\" Hence, \"outward sign\" of something. The meaning \"something which stands for something else\" was first recorded in 1590, in Edmund Spenser's \"Faerie Queene\".\n\nIn considering the effect of a symbol on the psyche, in his seminal essay \"The Symbol without Meaning\" Joseph Campbell proposes the following definition:\n\"A symbol is an energy evoking, and directing, agent\".\n\nLater, expanding on what he means by this definition Campbell says:\n\nJared Elisha defined symbolism that is something that stands for another, it can be place, object, or a person\n\nHeinrich Zimmer gives a concise overview of the nature, and perennial relevance, of symbols.\n\nIn the book \"Signs and Symbols, \"it is stated that \"A symbol ... is a visual image or sign representing an idea -- a deeper indicator of a universal truth.\"\n\nSymbols are a means of complex communication that often can have multiple levels of meaning. This separates symbols from signs, as signs have only one meaning.\n\nHuman cultures use symbols to express specific ideologies and social structures and to represent aspects of their specific culture. Thus, symbols carry meanings that depend upon one’s cultural background; in other words, the meaning of a symbol is not inherent in the symbol itself but is culturally learned.\n\nSymbols are the basis of all human understanding and serve as vehicles of conception for all human knowledge. Symbols facilitate understanding of the world in which we live, thus serving as the grounds upon which we make judgments. In this way, people use symbols not only to make sense of the world around them, but also to identify and cooperate in society through constitutive rhetoric.\n\nSemiotics is the study of signs, symbols, and signification as communicative behavior. Semiotics studies focus on the relationship of the signifier and the signified, also taking into account interpretation of visual cues, body language, sound, and other contextual clues. Semiotics is linked with both linguistics and psychology. Semioticians thus not only study what a symbol implies, but also how it got its meaning and how it functions to make meaning in society. Symbols allow the human brain continuously to create meaning using sensory input and decode symbols through both denotation and connotation.\n\nSwiss psychoanalyst Carl Jung, who studied archetypes, proposed an alternative definition of symbol, distinguishing it from the term \"sign\". In Jung's view, a sign stands for something known, as a word stands for its referent. He contrasted this with \"symbol\", which he used to stand for something that is unknown and that cannot be made clear or precise. An example of a symbol in this sense is Christ as a symbol of the archetype called \"self\". For example, written languages are composed of a variety of different symbols that create words, p. . Through these written words humans communicate with each other. Kenneth Burke described \"Homo sapiens\" as a \"symbol-using, symbol making, and symbol misusing animal\" to suggest that a person creates symbols as well as misuses them. One example he uses to indicate what he means by the misuse of symbol is the story of a man who, when told that a particular food item was whale blubber, could barely keep from throwing it up. Later, his friend discovered it was actually just a dumpling. But the man's reaction was a direct consequence of the symbol of \"blubber\" representing something inedible in his mind. In addition, the symbol of \"blubber\" was created by the man through various kinds of learning.\n\nBurke goes on to describe symbols as also being derived from Sigmund Freud's work on condensation and displacement, further stating that symbols are not just relevant to the theory of dreams but also to \"normal symbol systems\". He says they are related through \"substitution\", where one word, phrase, or symbol is substituted for another in order to change the meaning. In other words, if one person does not understand a certain word or phrase, another person may substitute a synonym or symbol in order to get the meaning across. However, upon learning the new way of interpreting a specific symbol, the person may change his or her already-formed ideas to incorporate the new information.\n\nJean Dalby Clift says that people not only add their own interpretations to symbols, they also create personal symbols that represent their own understanding of their lives: what she calls \"core images\" of the person. She argues that symbolic work with these personal symbols or core images can be as useful as working with dream symbols in psychoanalysis or counseling.\n\nWilliam Indick suggests that the symbols that are commonly found in myth, legend, and fantasy fulfil psychological functions and hence are why archetypes such as \"the hero,\" \"the princess\" and \"the witch\" have remained popular for centuries.\n\nPaul Tillich argued that, while signs are invented and forgotten, symbols are born and die. There are, therefore, dead and living symbols. A living symbol can reveal to an individual hidden levels of meaning and transcendent or religious realities. For Tillich a symbol always \"points beyond itself\" to something that is unquantifiable and mysterious; symbols open up the \"depth dimension of reality itself\". Symbols are complex, and their meanings can evolve as the individual or culture evolves. When a symbol loses its meaning and power for an individual or culture, it becomes a dead symbol.\nWhen a symbol becomes identified with the deeper reality to which it refers, it becomes idolatrous as the \"symbol is taken for reality.\" The symbol itself is substituted for the deeper meaning it intends to convey. The unique nature of a symbol is that it gives access to deeper layers of reality which are otherwise inaccessible.\n\nA symbol's meaning may be modified by various factors including popular usage, history, and contextual intent.\n\nThe history of a symbol is one of many factors in determining a particular symbol's apparent meaning. Consequently, symbols with emotive power carry problems analogous to false etymologies.\n\nThe context of a symbol may change its meaning. Similar five-pointed stars might signify a law enforcement officer or a member of the armed services, depending upon the uniform.\n\nSymbols are used in cartography to communicate geographical information (generally as point, line, or area features). As with other symbols, visual variables such as size, shape, orientation, texture, and pattern provide meaning to the symbol.\n\nThe form, or shape, of a cartographic symbol is classified into one of three main groups:\n\nPictorial/Representational - a shape or image that clearly resembles the geographic feature being symbolized and can be interpreted without a legend.\n\nAssociative - a mixture of pictorial and geometric elements that produce an easily recognizable shape.\n\nAbstract/Geometric - completely arbitrary shapes chosen to represent a certain feature.\n\nA symbolic action is an action that has no, or little, practical effect but symbolizes, or signals, what the actor wants or believes. The action conveys meaning to the viewers.\n\nSymbolic action may overlap with symbolic speech, such as the use of flag burning to express hostility or saluting the flag to express patriotism.\n\nIn response to intense public criticism, businesses, organizations, and governments may take symbolic actions rather than, or in addition to, directly addressing the identified problems.\n\nSymbolic actions are sometimes derided as slacktivism.\n\n\n"}
{"id": "1798923", "url": "https://en.wikipedia.org/wiki?curid=1798923", "title": "T-structure", "text": "T-structure\n\nIn the branch of mathematics called homological algebra, a \"t\"-structure is a way to axiomatize the properties of an abelian subcategory of a derived category. A \"t\"-structure on formula_1 consists of two subcategories formula_2 of a triangulated category or stable infinity category which abstract the idea of complexes whose cohomology vanishes in positive, respectively negative, degrees. There can be many distinct \"t\"-structures on the same category, and the interplay between these structures has implications for algebra and geometry. The notion of a \"t\"-structure arose in the work of Beilinson, Bernstein, Deligne, and Gabber on perverse sheaves.\n\nFix a triangulated category formula_1 with translation functor formula_4. A \"t\"-structure on formula_1 is a pair formula_2 of full subcategories, each of which is stable under isomorphism, which satisfy the following three axioms.\nIt can be shown that the subcategories formula_7 and formula_8 are closed under extensions in formula_1. In particular, they are stable under finite direct sums.\n\nIf formula_1 is a stable ∞-category, then a \"t\"-structure on formula_1 is defined to be a \"t\"-structure on its homotopy category formula_23 (which is a triangulated category).\n\nSuppose that formula_2 is a \"t\"-structure on formula_1 (either a triangulated category or a stable ∞-category). In this case, for any integer \"n\", we define formula_26 to be the full subcategory of formula_1 whose objects have the form formula_28, where formula_29 is an object of formula_7. Similarly, formula_31 is the full subcategory of objects formula_32, where formula_33 is an object of formula_8. More briefly, we define\nWith this notation, the axioms above may be rewritten as:\n\nThe heart or core of the \"t\"-structure is the full subcategory formula_45 consisting of objects in both formula_7 and formula_8, that is,\nThe heart of a \"t\"-structure is an abelian category (whereas a triangulated category is additive but almost never abelian), and it is stable under extensions.\n\nA triangulated or stable ∞-category with a choice of \"t\"-structure is sometimes called a \"t\"-category.\n\nIt is clear that, to define a \"t\"-structure, it suffices to fix integers \"m\" and \"n\" and specify formula_49 and formula_31. Some authors define a \"t\"-structure to be the pair formula_51.\n\nThe two subcategories formula_7 and formula_37 determine each other. An object \"X\" is in formula_7 if and only if formula_55 for all objects \"Y\" in formula_37, and vice versa. That is, formula_51 are left and right orthogonal complements of each other. Consequently it is enough to specify only one of formula_7 and formula_37.\n\nThe above notation is adapted to the study of cohomology. When the goal is to study homology, slightly different notation is used. A homological \"t\"-structure on formula_1 is a pair formula_61 such that, if we define\nthen formula_2 is a (cohomological) \"t\"-structure on formula_1. That is, the definition is the same except that upper indices are converted to lower indices and the roles of formula_65 and formula_66 are swapped. If we define\nthen the axioms for a homological \"t\"-structure may be written explicitly as\n\nThe most fundamental example of a \"t\"-structure is the natural \"t\"-structure on a derived category. Let formula_77 be an abelian category, and let formula_78 be its derived category. Then the natural \"t\"-structure is defined by the pair of subcategories\nIt follows immediately that\nIn this case, the third axiom for a \"t\"-structure, the existence of a certain distinguished triangle, can be made explicit as follows. Suppose that formula_81 is a cochain complex with values in formula_77. Define\nIt is clear that formula_84 and that there is a short exact sequence of complexes\nThis exact sequence furnishes the required distinguished triangle.\n\nThis example can be generalized to exact categories (in the sense of Quillen). There are also similar \"t\"-structures for the bounded, bounded above, and bounded below derived categories. If formula_86 is an abelian subcategory of formula_77, then the full subcategory formula_88 of formula_78 consisting of those complexes whose cohomology is in formula_86 has a similar \"t\"-structure whose heart is formula_86.\n\nThe category of \"perverse sheaves\" is, by definition, the core of the so-called \"perverse t-structure\" on the derived category of the category of sheaves on a complex analytic space \"X\" or (working with l-adic sheaves) an algebraic variety over a finite field. As was explained above, the heart of the standard t-structure simply contains ordinary sheaves, regarded as complexes concentrated in degree 0. For example, the category of perverse sheaves on a (possibly singular) algebraic curve \"X\" (or analogously a possibly singular surface) is designed so that it contains, in particular, objects of the form\nwhere formula_93 is the inclusion of a point, formula_94 is an ordinary sheaf, formula_95 is a smooth open subscheme and formula_96 is a locally constant sheaf on \"U\". Note the presence of the shift according to the dimension of \"Z\" and \"U\" respectively. This shift causes the category of perverse sheaves to be well-behaved on singular spaces. The simple objects in this category are the intersection cohomology sheaves of subvarieties with coefficients in an irreducible local system.\nThis t-structure was introduced by Beilinson, Bernstein and Deligne. It was shown by Beilinson that the derived category of the heart formula_97 is in fact equivalent to the original derived category of sheaves. This is an example of the general fact that a triangulated category may be endowed with several distinct t-structures.\n\nA non-standard example of a t-structure on the derived category of (graded) modules over a graded ring has the property that its heart consists of complexes\nwhere formula_99 is a module generated by its (graded) degree \"n\". This t-structure called geometric t-structure plays a prominent role in Koszul duality.\n\nThe category of spectra is endowed with a t-structure generated, in the sense above, by a single object, namely the sphere spectrum. The category formula_100 is the category of connective spectra, i.e., those whose negative homotopy groups vanish. (In areas related to homotopy theory, it is common to use homological conventions, as opposed to cohomological ones, so in this case it is common to replace \"formula_66\" by \"formula_65\". Using this convention, the category of connective spectra the notation is denoted formula_103.)\n\nA conjectural example in the theory of motives is the so-called \"motivic t-structure\". Its (conjectural) existence is closely related to certain standard conjectures on algebraic cycles and vanishing conjectures, such as the Beilinson-Soulé conjecture.\n\nIn the above example of the natural \"t\"-structure on an abelian category, the distinguished triangle guaranteed by the third axiom was constructed by truncation. As operations on the category of complexes, the truncations formula_104 and formula_105 are functorial, and the resulting short exact sequence of complexes is natural in formula_81. Using this, it can be shown that there are truncation functors on the derived category and that they induce a natural distinguished triangle.\n\nIn fact, this is an example of a general phenomenon. While the axioms for a \"t\"-structure do not assume the existence of truncation functors, such functors can always be constructed and are essentially unique. Suppose that formula_1 is a triangulated category and that formula_2 is a \"t\"-structure. The precise statement is that the inclusion functors\nadmit adjoints. These are functors\nsuch that\nMoreover, for any object formula_112 of formula_1, there exists a unique\nsuch that \"d\" and the counit and unit of the adjunctions together define a distinguished triangle\nUp to unique isomorphism, this is the unique distinguished triangle of the form formula_42 with formula_29 and formula_33 objects of formula_7 and formula_37, respectively. It follows from the existence of this triangle that an object formula_112 lies in formula_26 (resp. formula_31) if and only if formula_124 (resp. formula_125).\n\nThe existence of formula_126 implies the existence of the other truncation functors by shifting and taking opposite categories. If formula_112 is an object of formula_1, the third axiom for a \"t\"-structure asserts the existence of an formula_29 in formula_7 and a morphism formula_131 fitting into a certain distinguished triangle. For each formula_112, fix one such triangle and define formula_133. The axioms for a \"t\"-structure imply that, for any object formula_134 of formula_7, we have\nwith the isomorphism being induced by the morphism formula_131. This exhibits formula_29 as a solution to a certain universal mapping problem. Standard results on adjoint functors now imply that formula_29 is unique up to unique isomorphism and that there is a unique way to define formula_126 on morphisms that makes it a right adjoint. This proves the existence of formula_126 and hence the existence of all the truncation functors.\n\nRepeated truncation for a \"t\"-structure behaves similarly to repeated truncation for complexes. If formula_142, then there are natural transformations\nwhich yield natural equivalences\n\nThe \"n\"th \"cohomology functor\" formula_145 is defined as\nAs the name suggests, this is a cohomological functor in the usual sense for a triangulated category. That is, for any distinguished triangle formula_147, we obtain a long exact sequence\nIn applications to algebraic topology, the cohomology functors may be denoted formula_149 instead of formula_145. The cohomology functors take values in the heart formula_45.\n\nFor the natural \"t\"-structure on a derived category formula_78, the cohomology functor formula_145 is, up to quasi-isomorphism, the usual \"n\"th cohomology group of a complex. However, considered as functors on complexes, this is \"not\" true. Consider, for example, formula_154 as defined in terms of the natural \"t\"-structure. By definition, this is\nThis complex is non-zero in degrees formula_156 and formula_157, so it is clearly not the same as the zeroth cohomology group of the complex formula_81. However, the non-trivial differential is an injection, so the only non-trivial cohomology is in degree formula_157, where it is formula_160, the zeroth cohomology group of the complex formula_81. It follows that the two possible definitions of formula_162 are quasi-isomorphic.\n\nA \"t\"-structure is non-degenerate if the intersection of all formula_26, as well as the intersection of all formula_31, consists only of zero objects. For a non-degenerate \"t\"-structure, the collection of functors formula_165 is conservative. Moreover, in this case, formula_26 (resp. formula_31) may be identified with the full subcategory of those objects formula_112 for which formula_169 for formula_170 (resp. formula_171).\n\nFor formula_172, let formula_173 be a triangulated category with a fixed \"t\"-structure formula_174 Suppose that formula_175 is an exact functor (in the usual sense for triangulated categories, that is, up to a natural equivalence it commutes with translation and preserves distinguished triangles). Then formula_176 is:\n\nIt is elementary to see that if formula_176 is fully faithful and \"t\"-exact, then an object formula_112 of formula_181 is in formula_182 (resp. formula_183) if and only if formula_184 is in formula_185 (resp. formula_186). It is also elementary to see that if formula_187 is another left (resp. right) \"t\"-exact functor, then the composite formula_188 is also left (resp. right) \"t\"-exact.\n\nThe motivation for the study of one-sided \"t\"-exactness properties is that they lead to one-sided exactness properties on hearts. Let formula_189 be the inclusion. Then there is a composite functor\nIt can be shown that if formula_176 is left (resp. right) exact, then formula_192 is also left (resp. right) exact, and that if formula_193 is also left (resp. right) exact, then formula_194.\n\nIf formula_176 is left (resp. right) \"t\"-exact, and if formula_112 is in formula_7 (resp. formula_8), then there is a natural isomorphism formula_199 (resp. formula_200).\n\nIf formula_201 are exact functors with formula_202 left adjoint to formula_203, then formula_202 is right \"t\"-exact if and only if formula_203 is left \"t\"-exact, and in this case, formula_206 are a pair of adjoint functors formula_207.\n\nLet formula_2 be a \"t\"-structure on formula_1. If \"n\" is an integer, then the translation by \"n\" \"t\"-structure is formula_210. The dual \"t\"-structure is the \"t\"-structure on the opposite category formula_211 defined by formula_212.\n\nLet formula_213 be a triangulated subcategory of a triangulated category formula_1. If formula_2 is a \"t\"-structure on formula_1, then\nis a \"t\"-structure on formula_213 if and only if formula_213 is stable under the truncation functor formula_126. When this condition holds, the \"t\"-structure formula_221 is called the induced \"t\"-structure. The truncation and cohomology functors for the induced \"t\"-structure are the restriction to formula_213 of those on formula_1. Consequently the inclusion of formula_213 in formula_1 is \"t\"-exact, and formula_226.\n\nTo construct the category of perverse sheaves, it is important to be able to define a \"t\"-structure on a category of sheaves over a space by working locally in that space. The precise conditions necessary for this to be possible can be abstracted somewhat to the following setup. Suppose that there are three triangulated categories and two morphisms\nsatisfying the following properties.\nIn this case, given \"t\"-structures formula_236 and formula_237 on formula_238 and formula_239, respectively, there is a \"t\"-structure on formula_1 defined by\nThis \"t\"-structure is said to be the gluing of the \"t\"-structures on \"U\" and \"F\". The intended use cases are when formula_1, formula_238, and formula_239 are bounded below derived categories of sheaves on a space \"X\", an open subset \"U\", and the closed complement \"F\" of \"U\". The functors formula_232 and formula_230 are the usual pullback and pushforward functors. This works, in particular, when the sheaves in question are left modules over a sheaf of rings formula_247 on \"X\" and when the sheaves are ℓ-adic sheaves.\n\nMany t-structures arise by means of the following fact: in a triangulated category with arbitrary direct sums, and a set formula_248 of compact objects in formula_1, the subcategories\ncan be shown to be a t-structure. The resulting \"t\"-structure is said to be generated by formula_248.\n\nGiven an abelian subcategory formula_86 of a triangulated category formula_1, it is possible to construct a subcategory of formula_1 and a \"t\"-structure on that subcategory whose heart is formula_86.\n\nIf the requirement formula_256, formula_257 is replaced by the opposite inclusion\nand the other two axioms kept the same, the resulting notion is called a \"co-t-structure\" or \"weight structure\".\n"}
{"id": "1399633", "url": "https://en.wikipedia.org/wiki?curid=1399633", "title": "Tar-Baby", "text": "Tar-Baby\n\nThe Tar-Baby is the second of the Uncle Remus stories published in 1880; it is about a doll made of tar and turpentine used by the villainous Br'er Fox to entrap Br'er Rabbit. The more that Br'er Rabbit fights the Tar-Baby, the more entangled he becomes. \n\nIn modern usage, \"tar baby\" refers to a problem situation that is only aggravated by additional involvement with it.\n\nIn one tale, Br'er Fox constructs a doll out of a lump of tar and dresses it with some clothes. When Br'er Rabbit comes along, he addresses the tar \"baby\" amiably, but receives no response. Br'er Rabbit becomes offended by what he perceives as the tar baby's lack of manners, punches it and, in doing so, becomes stuck. The more Br'er Rabbit punches and kicks the tar baby out of rage, the worse he gets stuck. \n\nNow that Br'er Rabbit is stuck, Br'er Fox ponders how to dispose of him. The helpless but cunning Br'er Rabbit pleads, \"Do anything you want with me --- roas' me, hang me, skin me, drown me --- but please, Br'er Fox, don't fling me in dat brier-patch,\" prompting the sadistic Br'er Fox to do exactly that because he gullibly believes it will inflict the maximum pain on Br'er Rabbit. As rabbits are at home in thickets, however, the resourceful Br'er Rabbit escapes. \n\nThe story was originally published in \"Harper's Weekly\" by Robert Roosevelt; years later Joel Chandler Harris wrote of the Tar-Baby in his Uncle Remus stories.\n\nVariations on the tar-baby legend are found in the folklore of more than one culture. In the \"Journal of American Folklore\", Aurelio M. Espinosa examined 267 versions of the tar-baby story. The next year, Archer Taylor added a list of tarbaby stories from more sources around the world, citing scholarly claims of its earliest origins in India and Iran. Espinosa later published documentation on tarbaby stories from a variety of language communities around the world.\n\nA very similar West African tale is told of the mythical hero Anansi the Spider. In this version, Anansi creates a wooden doll and covers it over with gum, then puts a plate of yams in its lap, in order to capture the she-fairy Mmoatia (sometimes described as an \"elf\" or \"dwarf\"). Mmoatia takes the bait and eats the yams, but grows angry when the doll does not respond and strikes it, becoming stuck in the process. \n\nIn The Bahamas, the Tar-Baby story was published by \"The Journal of American Folklore\" in the year 1891 in \"Some Tales from Bahama Folk-Lore\" by Charles Lincoln Edwards. Edwards had collected the stories from Green Turtle Cay, Abaco in the summer of 1888.\n\nIn the tale, B' Rabby refused to dig for water, and didn't help grow the field. He tricks B' Lizard and B' Bouki while they were standing watch by the water and the field. The other animals got tired of his tricks, got together and created a Tar Baby. B' Rabby was caught by Tar Baby and the other animals who wanted to throw him into the sea but he talked them into throwing him into a bush. They threw B' Rabby into the bush and he got away.\n\nIn a variant recorded in Jamaica, Anansi himself was once similarly trapped with a tar-baby made by the eldest son of Mrs. Anansi, after Anansi pretended to be dead in order to steal her peas. In a Spanish language version told in the mountainous parts of Colombia, an unnamed rabbit is trapped by the \"Muñeco de Brea\" (tar doll). A Buddhist myth tells of Prince Five-weapons (the Future Buddha) who encounters the ogre Sticky-Hair in a forest.\n\nThe tar-baby theme is present in the folklore of various tribes of Meso-America and of South America: it is found in such stories as the Nahuatl (of Mexico) \"Lazy Boy and Little Rabbit\" (González Casanova 1946, pp. 55–67), Pipil (of El Salvador) \"Rabbit and Little Fox\" (Schultes 1977, pp. 113–116), and Palenquero (of Colombia) \"Rabbit, Toad, and Tiger\" (Patiño Rosselli 1983, pp. 224–229). In Mexico, the tar baby story is also found among Mixtec, Zapotec, and Popoluca. In North America, the tale appears in White Mountain Apache lore as \"Coyote Fights a Lump of Pitch\". In this story, white men are said to have erected the pitch-man that ensnares Coyote.\n\nAccording to James Mooney in \"Myths of the Cherokee\", the tar-baby story may have been influenced in America by the Cherokee \"Tar Wolf\" story, considered unlikely to have been derived from similar African stories: \"Some of these animal stories are common to widely separated [Native American] tribes among whom there can be no suspicion of [African] influences. Thus the famous \"tar baby\" story has variants, not only among the Cherokee, but also in New Mexico, Washington [State], and southern Alaska—wherever, in fact, the pine supplies enough gum to be molded into a ball for [Native American] uses...\". \n\nIn the Tar Wolf story, the animals were thirsty during a dry spell, and agreed to dig a well. The lazy rabbit refused to help dig, and so had no right to drink from the well. But she was thirsty, and stole from the well at night. The other animals fashioned a wolf out of tar and placed it near the well to scare the thief. The rabbit was scared at first, but when the tar wolf did not respond to her questions, she struck it and was held fast. Then she struggled with it and became so ensnared that she could not move. The next morning, the animals discovered the rabbit and proposed various ways of killing her, such as cutting her head off, and the rabbit responded to each idea saying that it would not harm her. Then an animal suggested throwing the rabbit into the thicket to die. At this, the rabbit protested vigorously and pleaded for her life. The animals threw the rabbit into the thicket. The rabbit then gave a whoop and bounded away, calling out to the other animals \"This is where I live!\".\n\nWalt Disney Studios released \"Song of the South\", which contains the Tar-Baby story, in 1946. The film was never released on VHS or DVD in North America due to concerns about racially insensitive content. The ride Splash Mountain, which is in three of the Walt Disney theme parks, is based on the stories by Uncle Remus. However, instead of the Tar-Baby, Br'er Rabbit is captured in a beehive. \n\nThe Tar-Baby appears in the Toontown countryside in \"Who Framed Roger Rabbit\" and was featured as one of the guests in \"House of Mouse\".\n\n\"Lollipop and the Tar Baby\" is a 1977 science fiction short story by John Varley, which takes place in the lonely space at the edge of the Solar System, and is part of Varley's far-future \"Eight Worlds\" universe. It is not a simple re-telling of the original tale, but undertones of it appear in the way in which the story's protagonist finally resolves her predicament.\n\nIn an episode of \"The New Adventures of Winnie the Pooh\", Rabbit and his friends build a tar baby-like \"watchamadingle\" out of glue and molasses in order to trap the Masked Offender, who is actually a disguised Tigger.\n\nIn Hollywood's \"Roots\" 2016 miniseries, which is given a contemporary perspective, a group of white men order Kunta Kinte to hand over his infant daughter Kizzy, referring to her by the historically inaccurate (thus historical revisionist) term \"tar baby\". (See Racist interpretation below)\n\nThe story has given rise to two American English idioms. References to Br'er Rabbit's feigned protestations such as \"please don't fling me in dat brier-patch\" refer to guilefully seeking something by pretending to protest, with a \"briar patch\" often meaning a more advantageous situation or environment for one of the parties. The term \"tar baby\" has come to refer to a problem that is exacerbated by attempts to struggle with it, or by extension to a situation in which mere contact can lead to becoming inextricably involved.\n\nAlthough the term's provenance rests in African folklore (i.e., the gum doll Anansi created to trap Mmoatia), some Americans consider \"tar baby\" to be a pejorative term for African Americans. The Oxford English Dictionary defines \"tar baby\" as \"a difficult problem which is only aggravated by attempts to solve it\", but the online subscription-only version adds a second definition: \"a derogatory term for a Black (U.S.) or a Maori (N.Z.)\".\n\nSeveral United States politicians—including presidential candidates John Kerry, John McCain, Michele Bachmann, and Mitt Romney—have been criticized by civil rights leaders, the media, and fellow politicians for using the \"tar baby\" metaphor. An article in \"The New Republic\" argued that people are \"unaware that some consider it to have a second meaning as a slur\" and it \"is an obscure slur, not even known to be so by a substantial proportion of the population\". It continued that, \"those who feel that \"tar baby\" status as a slur is patently obvious are judging from the fact that it \"sounds like\" a racial slur\".\n\n\n\n"}
{"id": "1861608", "url": "https://en.wikipedia.org/wiki?curid=1861608", "title": "Ternary plot", "text": "Ternary plot\n\nA ternary plot, ternary graph, triangle plot, simplex plot, Gibbs triangle or de Finetti diagram is a barycentric plot on three variables which sum to a constant. It graphically depicts the ratios of the three variables as positions in an equilateral triangle. It is used in physical chemistry, petrology, mineralogy, metallurgy, and other physical sciences to show the compositions of systems composed of three species. In population genetics, it is often called a de Finetti diagram. In game theory, it is often called a \"simplex plot\".\n\nIn a ternary plot, the proportions of the three variables , , and must sum to some constant, . Usually, this constant is represented as 1.0 or 100%. Because for all substances being graphed, any one variable is not independent of the others, so only two variables must be known to find a sample's point on the graph: for instance, must be equal to . Because the three proportions cannot vary independently—there are only two degrees of freedom—it is possible to graph the combinations of all three variables in only two dimensions.\n\nThe advantage of using a ternary plot for depicting chemical compositions is that three variables can be conveniently plotted in a two-dimensional graph. Ternary plots can also be used to create phase diagrams by outlining the composition regions on the plot where different phases exist.\n\nEvery point on a ternary plot represents a different composition of the three components.\n\nA parallel to a side of the triangle is the locus of points representing systems with constant chemical composition in the component situated in the vertex opposed to the side.\n\nThere are three common methods used to determine the ratios of the three species in the composition. \n\nThe first method is an estimation based upon the phase diagram grid. The concentration of each species is 100% (pure phase) in each corner of the triangle and 0% at the line opposite it. The percentage of a specific species decreases linearly with increasing distance from this corner, as seen in figures 3–8. By drawing parallel lines at regular intervals between the zero line and the corner (as seen in the images), fine divisions can be established for easy estimation of the content of a species. For a given point, the fraction of each of the three materials in the composition can be determined by the first.\n\nFor phase diagrams that do not possess grid lines, the easiest way to determine the composition is to set the altitude of the triangle to 100% and determine the shortest distances from the point of interest to each of the three sides. By Viviani's theorem, the distances (the ratios of the distances to the total height of 100%) give the content of each of the species, as shown in figure 1.\n\nThe third method is based upon a larger number of measurements, but does not require the drawing of perpendicular lines. Straight lines are drawn from each corner, through the point of interest, to the opposite side of the triangle. The lengths of these lines, as well as the lengths of the segments between the point and the corresponding sides, are measured individually. Ratios can then be determined by dividing these segments by the entire corresponding line as shown in the figure 2. (The sum of the ratios should add to 1).\n\nFigure (1) shows an oblique projection of point in a 3-dimensional Cartesian space with axes , and , respectively.\n\nIf (a positive constant), is restricted to a plane containing , and . If , and each cannot be negative, is restricted to the triangle bounded by , and , as in (2).\n\nIn (3), the axes are rotated to give an isometric view. The triangle, viewed face-on, appears equilateral.\n\nIn (4), the distances of from lines , and are denoted by , and , respectively.\n\nFor any line in vector form ( is a unit vector) and a point , the perpendicular distance from to is\n\nIn this case, point is at\n\nLine has\n\nUsing the perpendicular distance formula,\n\nSimilar calculation on lines and gives\n\nThis shows that the distance of the point from the respective lines is linearly proportional to the original values , and .\n\nCartesian coordinates are useful for plotting points in the triangle. Consider an equilateral ternary plot where is placed at and at . Then is , and the triple is\n\nThis example shows how this works for a hypothetical set of three soil samples:\n\nHere is a list of software that help enable the creation of ternary plots\n\n\n"}
{"id": "4746964", "url": "https://en.wikipedia.org/wiki?curid=4746964", "title": "Thematic map", "text": "Thematic map\n\nA thematic map is a type of map specifically designed to show a particular theme connected with a specific geographic area. \n\nA 'Thematic map' is a map that focuses on a specific theme or subject area. This is in contrast to \"general reference maps\", which regularly show the variety of phenomena—geological, geographical, political—together. The contrast between them lies in the fact that thematic maps use the base data, such as coastlines, boundaries and places, only as points of reference for the phenomenon being mapped. General maps portray the base data, such as landforms, lines of transportation, settlements, and political boundaries, for their own sake. \n\nThematic maps emphasize spatial variation of one or a small number of geographic distributions. These distributions may be physical phenomena such as climate or human characteristics such as population density and health issues. Barbara Petchenik described the difference as \"in place, about space.\" While general reference maps show where something is in space, thematic maps tell a story about that place (e.g., city map).\n\nThematic maps are sometimes referred to as graphic essays that portray spatial variations and interrelationships of geographical distributions. Location, of course, is important to provide a reference base of where selected phenomena are occurring.\n\nAn important cartographic element preceding thematic mapping was the development of accurate base maps. Improvements in accuracy proceeded at a gradual pace, and even until the mid-17th century, general maps were usually of poor quality. Still, base maps around this time were good enough to display appropriate information, allowing for the first thematic maps to come into being.\n\nOne of the earliest thematic maps was a map entitled \"Designatio orbis christiani\" (1607) by Jodocus Hondius showing the dispersion of major religions, using map symbols in the French edition of his \"Atlas Minor\" (1607). This was soon followed by a thematic globe (in the form of a six-gore map) showing the same subject, using Hondius' symbols, by Franciscus Haraeus, entitled: \"Novus typus orbis ipsus globus, ex Analemmate Ptolomaei diductus\" (1614) \n\nAn early contributor to thematic mapping in England was the English astronomer Edmond Halley (1656–1742). His first significant cartographic contribution was a star chart of the constellation of the Southern Hemisphere, made during his stay on St. Helena and published on 1686. In that same year he also published his first terrestrial map in an article about trade winds, and this map is called the first meteorological chart. In 1701 he published the \"New and Correct Chart Shewing the Variations of the Compass\", see first image, the first chart to show lines of equal magnetic variation. \n\nAnother example of early thematic mapping comes from London physician John Snow. Though disease had been mapped thematically, Snow’s cholera map in 1854 is the best known example of using thematic maps for analysis. Essentially, his technique and methodology anticipate principles of a geographic information system (GIS). Starting with an accurate base map of a London neighborhood which included streets and water pump locations, Snow mapped out the incidents of cholera death. The emerging pattern centered around one particular pump on Broad Street. At Snow’s request, the handle of the pump was removed, and new cholera cases ceased almost at once. Further investigation of the area revealed the Broad Street pump was near a cesspit under the home of the outbreak's first cholera victim.\n\nAnother 19th-century example of thematic maps, according to Friendly (2008), was the earliest known choropleth map in 1826 created by Charles Dupin. Based on this work Louis-Léger Vauthier (1815–1901) developed the population contour map, a map that shows the population density of Paris in 1874 by contours or isolines.\n\nThematic maps serve three primary purposes. \nCommon examples are maps of demographic data such as population density. When designing a thematic map, cartographers must balance a number of factors in order to effectively represent the data. Besides spatial accuracy, and aesthetics, quirks of human visual perception and the presentation format must be taken into account.\n\nIn addition, the audience is of equal importance. Who will “read” the thematic map and for what purpose helps define how it should be designed. A political scientist might prefer having information mapped within clearly delineated county boundaries (choropleth maps). A state biologist could certainly benefit from county boundaries being on a map, but nature seldom falls into such smooth, man-made delineations. In which case, a dasymetric map charts the desired information underneath a transparent county boundary map for easy location referencing.\n\nA thematic map is univariate if the non-location data is all of the same kind. Population density, cancer rates, and annual rainfall are three examples of univariate data.\n\nBivariate mapping shows the geographical distribution of two distinct sets of data. For example, a map showing both rainfall and cancer rates may be used to explore a possible correlation between the two phenomena.\n\nMore than two sets of data leads to multivariate mapping. For example, a single map might show population density in addition to annual rainfall and cancer rates.\n\nCartographers use many methods to create thematic maps, but five techniques are especially noted.\n\nChoropleth mapping shows statistical data aggregated over predefined regions, such as counties or states, by coloring or shading these regions. For example, countries with higher rates of infant mortality might appear darker on a choropleth map. This technique assumes a relatively even distribution of the measured phenomenon within each region. Generally speaking, differences in hue are used to indicate qualitative differences, such as land use, while differences in saturation or lightness are used to indicate quantitative differences, such as population.\n\nThe proportional symbol technique uses symbols of different sizes to represent data associated with different areas or locations within the map. For example, a disc may be shown at the location of each city in a map, with the area of the disc being proportional to the population of the city.\n\nIsarithmic maps, also known as contour maps or isoline maps depict smooth continuous phenomena such as precipitation or elevation. Each line-bounded area on this type of map represents a region with the same value. For example, on an elevation map, each elevation line indicates an area at the listed elevation. An Isarithmic map is a planimetric graphic representation of a 3-D surface. Isarithmic mapping requires 3-D thinking for surfaces that vary spatially.\n\nA dot distribution map might be used to locate each occurrence of a phenomenon, as in the map made by Dr. Snow during the 1854 Broad Street cholera outbreak, where each dot represented one death due to cholera. Where appropriate, the dot distribution technique may also be used in combination the proportional symbol technique\n\nA dasymetric map is an alternative to a choropleth map. As with a choropleth map, data are collected by enumeration units. But instead of mapping the data so that the region appears uniform, \"ancillary information\" is used to model internal distribution of the phenomenon. For example, population density will be much lower in forested area than urbanized area, so in a common operation, land cover data (forest, water, grassland, urbanization) may be used to model the distribution of population reported by census enumeration unit such as a tract or county.\n\n\n"}
{"id": "29595962", "url": "https://en.wikipedia.org/wiki?curid=29595962", "title": "Tithing settlement", "text": "Tithing settlement\n\nTithing settlement is the name of a formalized series of meetings held at local congregations of The Church of Jesus Christ of Latter-day Saints (LDS Church). During tithing settlement, each member of the church is individually interviewed by the bishop or branch president of the congregation and asked to declare whether he or she has paid a full tithe to the church, which is defined as ten per cent of the member's income. Tithing settlement meetings are generally held in November or December.\n\nDuring the meeting, each member is given a copy of his or her donation record for the year, and asked to confirm it is correct.\n\nEach church member is required to choose one of the following tithing statuses as defined by the church:\n\n\nChurch members are invited to make a payment to correct any deficiency before the records are closed for year-end.\n\nIf a church member declines to participate or attend a tithing settlement meeting, the bishop or branch president is required to choose a tithing status on their behalf.\n\nPayment of tithing is a core doctrine of the church for all members, including children. Church members must declare themselves to be \"full-tithe payers\" in order to receive a temple recommend and attend one of the church's temples. However, that is done in a separate meeting from the tithing settlement.\n\nThe LDS Church uses the honor system and personal accountability of the individual tithe payer. It is not the church's general position to question the honesty of tithe payers. The church predominantly accepts the status declared by the tithe payer through counsel and consideration of the leaders. It is up to the individual tithe payer to determine and in turn declare to their leaders whether he or she feels they are a full tithe payer or not.\n"}
{"id": "8597086", "url": "https://en.wikipedia.org/wiki?curid=8597086", "title": "Von Neumann universal constructor", "text": "Von Neumann universal constructor\n\nJohn von Neumann's universal constructor is a self-replicating machine in a cellular automata (CA) environment. It was designed in the 1940s, without the use of a computer. The fundamental details of the machine were published in von Neumann's book \"Theory of Self-Reproducing Automata\", completed in 1966 by Arthur W. Burks after von Neumann's death.\n\nVon Neumann's goal was to specify an abstract machine which, when run, would replicate itself. In his design, the machine consists of three parts: a 'blueprint' for itself, a mechanism that can read any blueprint and construct the machine (sans blueprint) specified by that blueprint, and a 'copy machine' that can make copies of any blueprint. After the mechanism has been used to construct the machine specified by the blueprint, the copy machine is used to create a copy of that blueprint, and this copy is placed into the new machine, resulting in a working replication of the original machine. Some machines will do this backwards, copying the blueprint and then building a machine.\n\nTo define his machine in more detail, von Neumann invented the concept of a cellular automaton. The one he used consists of a two-dimensional grid of cells, each of which can be in one of 29 states at any point in time. At each timestep, each cell updates its state depending on the states of the surrounding cells at the prior timestep. The rules governing these updates are identical for all cells. \n\nThe universal constructor is a certain pattern of cell states in this cellular automaton. It contains one line of cells that serve as a 'tape', encoding a sequence of instructions that serve as a 'blueprint' for the machine. The machine reads these instructions one by one and performs the corresponding actions. The instructions direct the machine to use its 'construction arm' to build a copy of the machine, without tape, at some other location in the cell grid. The tape can't contain instructions to build an equally long tape, just as a container can't contain a container of the same size. Therefore, the machine contains a separate 'copy machine' which reads the tape and places a copy into the newly constructed machine. The resulting new machine and tape is identical to the old one, and it proceeds to replicate again.\n\nVon Neumann's design has traditionally been understood to be a demonstration of the logical requirements for machine self-replication. However, it is clear that far simpler machines can achieve self-replication. Examples include trivial crystal-like growth, template replication, and Langton's loops. But von Neumann was interested in something more profound: construction, universality, and evolution.\n\nThis universal constructor can be seen as an abstract simulation of a physical universal assembler.\n\nNote that the simpler self-replicating CA structures (especially, Byl's loop and the Chou–Reggia loop) cannot exist in a wide variety of forms and thus have very limited evolvability. Other CA structures such as the Evoloop are somewhat evolvable but still don't support open-ended evolution. Commonly, simple replicators do not fully contain the machinery of construction, there being a degree to which the replicator is information copied by its surrounding environment. Although the Von Neumann design is a logical construction, it is in principle a design that could be instantiated as a physical machine. The issue of the environmental contribution to replication is somewhat open, since there are different conceptions of raw material and its availability.\n\nThe concept of a \"universal constructor\" is non-trivial because of the existence of Garden of Eden patterns. But a simple definition is that a universal constructor is able to construct any finite pattern of non-excited (quiescent) cells.\n\nVon Neumann's crucial insight is that part of the replicator has a double use; being both an active component of the construction mechanism, and being the target of a passive copying process. This part is played by the tape of instructions in Von Neumann's combination of universal constructor plus instruction tape.\n\nThe combination of a universal constructor and a tape of instructions would i) allow self-replication, and also ii) guarantee that the open-ended complexity growth observed in biological organisms was possible. The image below illustrates this possibility.\n\nThis insight is all the more remarkable because it preceded the discovery of the structure of the DNA molecule by Watson and Crick, though it followed the Avery–MacLeod–McCarty experiment which identified DNA as the molecular carrier of genetic information in living organisms. The DNA molecule is processed by separate mechanisms that carry out its instructions and copy the DNA for insertion for the newly constructed cell. The ability to achieve open-ended evolution lies in the fact that, just as in nature, errors (mutations) in the copying of the genetic tape can lead to viable variants of the automaton, which can then evolve via natural selection.\n\nArthur Burks and others extended the work of von Neumann, giving a much clearer and complete set of details regarding the design and operation of von Neumann's self-replicator. The work of J. W. Thatcher is particularly noteworthy, for he greatly simplified the design. Still, their work did not yield a complete design, cell by cell, of a configuration capable of demonstrating self-replication.\n\nRenato Nobili and Umberto Pesavento published the first fully implemented self-reproducing cellular automaton in 1995, nearly fifty years after von Neumann's work. They used a 32-state cellular automaton instead of von Neumann's original 29-state specification, extending it to allow for easier signal-crossing, explicit memory function and a more compact design. They also published an implementation of a general constructor within the original 29-state CA but not one capable of complete replication - the configuration cannot duplicate its tape, nor can it trigger its offspring; the configuration can only construct.\n\nIn 2004, D. Mange et al. reported an implementation of a self-replicator that is consistent with the designs of von Neumann.\n\nIn 2007, Nobili published a 32-state implementation that uses run-length encoding to greatly reduce the size of the tape.\n\nIn 2008, William R. Buckley published two configurations which are self-replicators within the original 29-state CA of von Neumann. Buckley claims that the crossing of signal within von Neumann 29-state cellular automata is not necessary to the construction of self-replicators. Buckley also points out that for the purposes of evolution, each replicator should return to its original configuration after replicating, in order to be capable (in theory) of making more than one copy. As published, the 1995 design of Nobili-Pesavento does not fulfill this requirement but the 2007 design of Nobili does; the same is true of Buckley's configurations.\n\nIn 2009, Buckley published with Golly a third configuration for von Neumann 29-state cellular automata, which can perform either holistic self-replication, or self-replication by partial construction. This configuration also demonstrates that signal crossing is not necessary to the construction of self-replicators within von Neumann 29-state cellular automata.\n\nC. L. Nehaniv in 2002, and also Y. Takada et al. in 2004, proposed a universal constructor directly implemented upon an asynchronous cellular automaton, rather than upon a synchronous cellular automaton.\n\nAs defined by von Neumann, universal construction entails the construction of passive configurations, only. As such, the concept of universal construction constituted nothing more than a literary (or, in this case, mathematical) device. It facilitated other proof, such as that a machine well constructed may engage in self-replication, while universal construction itself was simply assumed over a most minimal case. Universal construction under this standard is trivial. Hence, while all the configurations given here can construct any passive configuration, none can construct the real-time crossing organ devised by Gorman.\n\nAll the implementations of von Neumann's self-reproducing machine require considerable resources to run on computer. For example, in the Nobili-Pesavento 32-state implementation shown above, while the body of the machine is just 6,329 non-empty cells (within a rectangle of size 97x170), it requires a tape that is 145,315 cells long, and takes 63 billion timesteps to replicate. A simulator running at 1,000 timesteps per second would take over 2 years to make the first copy. In 1995, when the first implementation was published, the authors had not seen their own machine replicate. However, in 2008, the hashlife algorithm was extended to support the 29-state and 32-state rulesets in Golly. On a modern desktop PC, replication now takes only a few minutes, although a significant amount of memory is required.\n\nIt has been argued that the problem Von Neumann was trying to address was not self-reproduction \"per se\", but the evolutionary growth of complexity. His “proof-of-principle” designs showed how it is logically possible, by using a general purpose programmable (“universal”) constructor, to exhibit an indefinitely large class of self-reproducing machines, spanning a wide range of “complexity” (in von Neumann's sense of “the ability to do complicated things”), interconnected by a network of potential mutational pathways, including pathways from the most simple to the most complex. This is an important result, as prior to that it might have been conjectured that there is a fundamental logical barrier to the existence of such pathways; in which case, biological organisms, which do support such pathways, could not be “machines”, as conventionally understood. But the result does not show what \"other\" conditions are necessary, in practice, for evolution along such pathways from simple to complex to be spontaneously realised or followed. In his unfinished work he briefly considers conflict and interactions between his self-reproducing machines; but in practice, his particular model cannot yield evolutionary dynamics because the machines are too fragile - the vast majority of perturbations cause them effectively to disintegrate.\n\n\n"}
