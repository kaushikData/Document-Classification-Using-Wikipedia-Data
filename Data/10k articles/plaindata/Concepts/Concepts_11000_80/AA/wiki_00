{"id": "2349", "url": "https://en.wikipedia.org/wiki?curid=2349", "title": "Abstract data type", "text": "Abstract data type\n\nIn computer science, an abstract data type (ADT) is a mathematical model for data types, where a data type is defined by its behavior (semantics) from the point of view of a \"user\" of the data, specifically in terms of possible values, possible operations on data of this type, and the behavior of these operations. This contrasts with data structures, which are concrete representations of data, and are the point of view of an implementer, not a user.\n\nFormally, an ADT may be defined as a \"class of objects whose logical behavior is defined by a set of values and a set of operations\"; this is analogous to an algebraic structure in mathematics. What is meant by \"behavior\" varies by author, with the two main types of formal specifications for behavior being \"axiomatic (algebraic) specification\" and an \"abstract model;\" these correspond to axiomatic semantics and operational semantics of an abstract machine, respectively. Some authors also include the computational complexity (\"cost\"), both in terms of time (for computing operations) and space (for representing values). In practice many common data types are not ADTs, as the abstraction is not perfect, and users must be aware of issues like arithmetic overflow that are due to the representation. For example, integers are often stored as fixed width values (32-bit or 64-bit binary numbers), and thus experience integer overflow if the maximum value is exceeded.\n\nADTs are a theoretical concept in computer science, used in the design and analysis of algorithms, data structures, and software systems, and do not correspond to specific features of computer languages—mainstream computer languages do not directly support formally specified ADTs. However, various language features correspond to certain aspects of ADTs, and are easily confused with ADTs proper; these include abstract types, opaque data types, protocols, and design by contract. ADTs were first proposed by Barbara Liskov and Stephen N. Zilles in 1974, as part of the development of the CLU language.\n\nFor example, integers are an ADT, defined as the values …, −2, −1, 0, 1, 2, …, and by the operations of addition, subtraction, multiplication, and division, together with greater than, less than, etc., which behave according to familiar mathematics (with care for integer division), independently of how the integers are represented by the computer. Explicitly, \"behavior\" includes obeying various axioms (associativity and commutativity of addition etc.), and preconditions on operations (cannot divide by zero). Typically integers are represented in a data structure as binary numbers, most often as two's complement, but might be binary-coded decimal or in ones' complement, but the user is abstracted from the concrete choice of representation, and can simply use the data as data types.\n\nAn ADT consists not only of operations, but also of values of the underlying data and of constraints on the operations. An \"interface\" typically refers only to the operations, and perhaps some of the constraints on the operations, notably pre-conditions and post-conditions, but not other constraints, such as relations between the operations.\n\nFor example, an abstract stack, which is a last-in-first-out structure, could be defined by three operations: push, that inserts a data item onto the stack; pop, that removes a data item from it; and peek or top, that accesses a data item on top of the stack without removal. An abstract queue, which is a first-in-first-out structure, would also have three operations: enqueue, that inserts a data item into the queue; dequeue, that removes the first data item from it; and front, that accesses and serves the first data item in the queue. There would be no way of differentiating these two data types, unless a mathematical constraint is introduced that for a stack specifies that each pop always returns the most recently pushed item that has not been popped yet. When analyzing the efficiency of algorithms that use stacks, one may also specify that all operations take the same time no matter how many data items have been pushed into the stack, and that the stack uses a constant amount of storage for each element.\n\nAbstract data types are purely theoretical entities, used (among other things) to simplify the description of abstract algorithms, to classify and evaluate data structures, and to formally describe the type systems of programming languages. However, an ADT may be implemented by specific data types or data structures, in many ways and in many programming languages; or described in a formal specification language. ADTs are often implemented as modules: the module's interface declares procedures that correspond to the ADT operations, sometimes with comments that describe the constraints. This information hiding strategy allows the implementation of the module to be changed without disturbing the client programs.\n\nThe term abstract data type can also be regarded as a generalized approach of a number of algebraic structures, such as lattices, groups, and rings. The notion of abstract data types is related to the concept of data abstraction, important in object-oriented programming and design by contract methodologies for software development.\n\nAn abstract data type is defined as a mathematical model of the data objects that make up a data type as well as the functions that operate on these objects.\nThere are no standard conventions for defining them. A broad division may be drawn between \"imperative\" and \"functional\" definition styles.\n\nIn the philosophy of imperative programming languages, an abstract data structure is conceived as an entity that is \"mutable\"—meaning that it may be in different \"states\" at different times. Some operations may change the state of the ADT; therefore, the order in which operations are evaluated is important, and the same operation on the same entities may have different effects if executed at different times—just like the instructions of a computer, or the commands and procedures of an imperative language. To underscore this view, it is customary to say that the operations are \"executed\" or \"applied\", rather than \"evaluated\". The imperative style is often used when describing abstract algorithms. (See The Art of Computer Programming by Donald Knuth for more details)\n\nImperative-style definitions of ADT often depend on the concept of an \"abstract variable\", which may be regarded as the simplest non-trivial ADT. An abstract variable \"V\" is a mutable entity that admits two operations:\nwith the constraint that\n\nAs in so many programming languages, the operation store(\"V\", \"x\") is often written \"V\" ← \"x\" (or some similar notation), and fetch(\"V\") is implied whenever a variable \"V\" is used in a context where a value is required. Thus, for example, \"V\" ← \"V\" + 1 is commonly understood to be a shorthand for store(\"V\",fetch(\"V\") + 1).\n\nIn this definition, it is implicitly assumed that storing a value into a variable \"U\" has no effect on the state of a distinct variable \"V\". To make this assumption explicit, one could add the constraint that\n\nMore generally, ADT definitions often assume that any operation that changes the state of one ADT instance has no effect on the state of any other instance (including other instances of the same ADT) — unless the ADT axioms imply that the two instances are connected (aliased) in that sense. For example, when extending the definition of abstract variable to include abstract records, the operation that selects a field from a record variable \"R\" must yield a variable \"V\" that is aliased to that part of \"R\".\n\nThe definition of an abstract variable \"V\" may also restrict the stored values \"x\" to members of a specific set \"X\", called the \"range\" or \"type\" of \"V\". As in programming languages, such restrictions may simplify the description and analysis of algorithms, and improve their readability.\n\nNote that this definition does not imply anything about the result of evaluating fetch(\"V\") when \"V\" is \"un-initialized\", that is, before performing any store operation on \"V\". An algorithm that does so is usually considered invalid, because its effect is not defined. (However, there are some important algorithms whose efficiency strongly depends on the assumption that such a fetch is legal, and returns some arbitrary value in the variable's range.)\n\nSome algorithms need to create new instances of some ADT (such as new variables, or new stacks). To describe such algorithms, one usually includes in the ADT definition a create() operation that yields an instance of the ADT, usually with axioms equivalent to\nThis axiom may be strengthened to exclude also partial aliasing with other instances. On the other hand, this axiom still allows implementations of create() to yield a previously created instance that has become inaccessible to the program.\n\nAs another example, an imperative-style definition of an abstract stack could specify that the state of a stack \"S\" can be modified only by the operations\nwith the constraint that\n\nSince the assignment \"V\" ← \"x\", by definition, cannot change the state of \"S\", this condition implies that \"V\" ← pop(\"S\") restores \"S\" to the state it had before the push(\"S\", \"x\"). From this condition and from the properties of abstract variables, it follows, for example, that the sequence\nwhere \"x\", \"y\", and \"z\" are any values, and \"U\", \"V\", \"W\" are pairwise distinct variables, is equivalent to\n\nHere it is implicitly assumed that operations on a stack instance do not modify the state of any other ADT instance, including other stacks; that is,\n\nAn abstract stack definition usually includes also a Boolean-valued function empty(\"S\") and a create() operation that returns a stack instance, with axioms equivalent to\n\nSometimes an ADT is defined as if only one instance of it existed during the execution of the algorithm, and all operations were applied to that instance, which is not explicitly notated. For example, the abstract stack above could have been defined with operations push(\"x\") and pop(), that operate on \"the\" only existing stack. ADT definitions in this style can be easily rewritten to admit multiple coexisting instances of the ADT, by adding an explicit instance parameter (like \"S\" in the previous example) to every operation that uses or modifies the implicit instance.\n\nOn the other hand, some ADTs cannot be meaningfully defined without assuming multiple instances. This is the case when a single operation takes two distinct instances of the ADT as parameters. For an example, consider augmenting the definition of the abstract stack with an operation compare(\"S\", \"T\") that checks whether the stacks \"S\" and \"T\" contain the same items in the same order.\n\nAnother way to define an ADT, closer to the spirit of functional programming, is to consider each state of the structure as a separate entity. In this view, any operation that modifies the ADT is modeled as a mathematical function that takes the old state as an argument, and returns the new state as part of the result. Unlike the imperative operations, these functions have no side effects. Therefore, the order in which they are evaluated is immaterial, and the same operation applied to the same arguments (including the same input states) will always return the same results (and output states).\n\nIn the functional view, in particular, there is no way (or need) to define an \"abstract variable\" with the semantics of imperative variables (namely, with fetch and store operations). Instead of storing values into variables, one passes them as arguments to functions.\n\nFor example, a complete functional-style definition of an abstract stack could use the three operations:\n\nIn a functional-style definition there is no need for a create operation. Indeed, there is no notion of \"stack instance\". The stack states can be thought of as being potential states of a single stack structure, and two stack states that contain the same values in the same order are considered to be identical states. This view actually mirrors the behavior of some concrete implementations, such as linked lists with hash cons.\n\nInstead of create(), a functional-style definition of an abstract stack may assume the existence of a special stack state, the \"empty stack\", designated by a special symbol like Λ or \"()\"; or define a bottom() operation that takes no arguments and returns this special stack state. Note that the axioms imply that\nIn a functional-style definition of a stack one does not need an empty predicate: instead, one can test whether a stack is empty by testing whether it is equal to Λ.\n\nNote that these axioms do not define the effect of top(\"s\") or pop(\"s\"), unless \"s\" is a stack state returned by a push. Since push leaves the stack non-empty, those two operations are undefined (hence invalid) when \"s\" = Λ. On the other hand, the axioms (and the lack of side effects) imply that push(\"s\", \"x\") = push(\"t\", \"y\") if and only if \"x\" = \"y\" and \"s\" = \"t\".\n\nAs in some other branches of mathematics, it is customary to assume also that the stack states are only those whose existence can be proved from the axioms in a finite number of steps. In the abstract stack example above, this rule means that every stack is a \"finite\" sequence of values, that becomes the empty stack (Λ) after a finite number of pops. By themselves, the axioms above do not exclude the existence of infinite stacks (that can be poped forever, each time yielding a different state) or circular stacks (that return to the same state after a finite number of pops). In particular, they do not exclude states \"s\" such that pop(\"s\") = \"s\" or push(\"s\", \"x\") = \"s\" for some \"x\". However, since one cannot obtain such stack states with the given operations, they are assumed \"not to exist\".\n\nAside from the behavior in terms of axioms, it is also possible to include, in the definition of an ADT operation, their algorithmic complexity. Alexander Stepanov, designer of the C++ Standard Template Library, included complexity guarantees in the STL specification, arguing:\n\nAbstraction provides a promise that any implementation of the ADT has certain properties and abilities; knowing these is all that is required to make use of an ADT object. The user does not need any technical knowledge of how the implementation works to use the ADT. In this way, the implementation may be complex but will be encapsulated in a simple interface when it is actually used.\n\nCode that uses an ADT object will not need to be edited if the implementation of the ADT is changed. Since any changes to the implementation must still comply with the interface, and since code using an ADT object may only refer to properties and abilities specified in the interface, changes may be made to the implementation without requiring any changes in code where the ADT is used.\n\nDifferent implementations of the ADT, having all the same properties and abilities, are equivalent and may be used somewhat interchangeably in code that uses the ADT. This gives a great deal of flexibility when using ADT objects in different situations. For example, different implementations of the ADT may be more efficient in different situations; it is possible to use each in the situation where they are preferable, thus increasing overall efficiency.\n\nSome operations that are often specified for ADTs (possibly under other names) are\n\nIn imperative-style ADT definitions, one often finds also\n\nThe free operation is not normally relevant or meaningful, since ADTs are theoretical entities that do not \"use memory\". However, it may be necessary when one needs to analyze the storage used by an algorithm that uses the ADT. In that case one needs additional axioms that specify how much memory each ADT instance uses, as a function of its state, and how much of it is returned to the pool by free.\n\nSome common ADTs, which have proved useful in a great variety of applications, are\n\nEach of these ADTs may be defined in many ways and variants, not necessarily equivalent. For example, an abstract stack may or may not have a count operation that tells how many items have been pushed and not yet popped. This choice makes a difference not only for its clients but also for the implementation.\n\nAn extension of ADT for computer graphics was proposed in 1979: an abstract graphical data type (AGDT). It was introduced by Nadia Magnenat Thalmann, and Daniel Thalmann. AGDTs provide the advantages of ADTs with facilities to build graphical objects in a structured way.\n\nImplementing an ADT means providing one procedure or function for each abstract operation. The ADT instances are represented by some concrete data structure that is manipulated by those procedures, according to the ADT's specifications.\n\nUsually there are many ways to implement the same ADT, using several different concrete data structures. Thus, for example, an abstract stack can be implemented by a linked list or by an array.\n\nIn order to prevent clients from depending on the implementation, an ADT is often packaged as an \"opaque data type\" in one or more modules, whose interface contains only the signature (number and types of the parameters and results) of the operations. The implementation of the module—namely, the bodies of the procedures and the concrete data structure used—can then be hidden from most clients of the module. This makes it possible to change the implementation without affecting the clients. If the implementation is exposed, it is known instead as a \"transparent data type.\"\n\nWhen implementing an ADT, each instance (in imperative-style definitions) or each state (in functional-style definitions) is usually represented by a handle of some sort.\n\nModern object-oriented languages, such as C++ and Java, support a form of abstract data types. When a class is used as a type, it is an abstract type that refers to a hidden representation. In this model an ADT is typically implemented as a class, and each instance of the ADT is usually an object of that class. The module's interface typically declares the constructors as ordinary procedures, and most of the other ADT operations as methods of that class. However, such an approach does not easily encapsulate multiple representational variants found in an ADT. It also can undermine the extensibility of object-oriented programs.\nIn a pure object-oriented program that uses interfaces as types, types refer to behaviors not representations.\n\nAs an example, here is an implementation of the abstract stack above in the C programming language.\nAn imperative-style interface might be:\nThis interface could be used in the following manner:\nThis interface can be implemented in many ways. The implementation may be arbitrarily inefficient, since the formal definition of the ADT, above, does not specify how much space the stack may use, nor how long each operation should take. It also does not specify whether the stack state \"s\" continues to exist after a call \"x\" ← pop(\"s\").\n\nIn practice the formal definition should specify that the space is proportional to the number of items pushed and not yet popped; and that every one of the operations above must finish in a constant amount of time, independently of that number. To comply with these additional specifications, the implementation could use a linked list, or an array (with dynamic resizing) together with two integers (an item count and the array size).\n\nFunctional-style ADT definitions are more appropriate for functional programming languages, and vice versa. However, one can provide a functional-style interface even in an imperative language like C. For example:\nMany modern programming languages, such as C++ and Java, come with standard libraries that implement several common ADTs, such as those listed above.\n\nThe specification of some programming languages is intentionally vague about the representation of certain built-in data types, defining only the operations that can be done on them. Therefore, those types can be viewed as \"built-in ADTs\". Examples are the arrays in many scripting languages, such as Awk, Lua, and Perl, which can be regarded as an implementation of the abstract list.\n\n\n"}
{"id": "309947", "url": "https://en.wikipedia.org/wiki?curid=309947", "title": "Agalega day gecko", "text": "Agalega day gecko\n\nAgalega day gecko (Phelsuma borbonica agalegae ) is a subspecies of geckos.\n\nThis diurnal gecko only lives on the Agaléga Islands. It typically inhabits coconut trees or cheval trees. The Agalega day gecko feeds on insects and nectar.\n\nThis lizard belongs to the medium-sized day geckos. Males are slightly larger than females and can reach a total length of about 16 cm whereas females measure only 13.5 cm. The basic body colour is grayish green. Both the head and the neck are yellow-brown. The tail and back can be bright turquoise. The flanks are grey or beige. The dorso-lateral bands are turquoise. On the back and tail there are brownish or red-brick coloured dots or bars.\n\nThis species is endemic to the two Agalega islets.\n\nThe two islands where \"P. borbonica agalega\" occurs, are small low coral sand islands. On these islands are mainly coconut tree plantations. The Agalega day gecko is often found on these trees as well as on \"Terminalia spec.\" and mango trees.\n\nThese day geckos feed on various insects and other invertebrates. They also like to lick soft, sweet fruit, pollen and nectar.\n\nThis \"Phelsuma\" species is rather shy.\n\nThe pairing season is between April and the first weeks of September. During this period, the females lay up to 4 pairs of eggs, often under loose bark of coconut trees. Females often share the same location for their eggs. The young will hatch after approximately 70–100 days, depending on the temperature. The juveniles measure 45–50 mm.\n\nThese animals should be housed in pairs and need a large, well planted terrarium. The temperature should be between 24 and 30 °C. During the night the temperature can drop to 18–22 °C. The relative humidity should be maintained between 40 and 60% during the day and 75% at night. These animals can be fed with crickets, wax moths, fruit flies, mealworms and houseflies.\n\n"}
{"id": "2266434", "url": "https://en.wikipedia.org/wiki?curid=2266434", "title": "Best of all possible worlds", "text": "Best of all possible worlds\n\nThe phrase \"the best of all possible worlds\" (; ) was coined by the German polymath Gottfried Leibniz in his 1710 work \"Essais de Théodicée sur la bonté de Dieu, la liberté de l'homme et l'origine du mal\" (\"Essays of Theodicy on the Goodness of God, the Freedom of Man and the Origin of Evil\"). The claim that the actual world is the best of all possible worlds is the central argument in Leibniz's theodicy, or his attempt to solve the problem of evil.\n\nAmong his many philosophical interests and concerns, Leibniz took on this question of theodicy: If God is omnibenevolent, omnipotent and omniscient, how do we account for the suffering and injustice that exists in the world? Historically, attempts to answer the question have been made using various arguments, for example, by explaining away evil or reconciling evil with good.\n\nLeibniz outlined his perfect world theory in his work \"The Monadology\", stating the argument in five statements: \nTo further understand his argument, these five statements can be grouped in three main premises. The first premise (corresponding to the first and second statements) state that God can only choose one universe from the infinite amount of possible universes. (The term \"one universe\" does not necessarily mean a single three-dimensional physical reality, but refers to the sum total of God's creation, and thus might include multiple worlds.) The second premise (the third and fourth statements) state that God is a perfect existence, and he makes decisions based on reason. The third premise (the fifth statement) concludes that the existing world, chosen by God, is the best.\n\nLeibniz used Christianity to back up the validity of all the premises. For the first premise, God's existence and role as the creator of the world was proven by the Bible. The second premise is proven since \"God acts always in the most perfect and most desirable manner possible\". Therefore, His choice will always be the best, and only perfect existence can make perfect decision throughout time. Since all the premises are right, then Leibniz concluded, \"The universe that God chose to exist is the best of all possible worlds\".\n\nTo set his argument, Leibniz wrestled with the problem of sin and evil in the world that obviously exists and is considered as the imperfection of the world. Leibniz, in this letter said, \"I do not believe that a world without evil, preferable in order to ours, is possible; otherwise it would have been preferred. It is necessary to believe that the mixture of evil has produced the greatest possible good: otherwise the evil would not have been permitted\". In other words, if a world without evil is more perfect in any way, then evil would have not happened, and the world without evil would be our world instead. God put evilness in the world for us to understand goodness which is achieved through contrasting it with evil. Once we understood evil and good, it gives us the ability to produce the \"greatest possible good\" out of all the goodness. Evil fuels goodness, which leads to a perfect system.\n\nFor Leibniz, an additional central concern is the matter of reconciling human freedom (indeed, God's own freedom) with the determinism inherent in his own theory of the universe. Leibniz' solution casts God as a kind of \"optimizer\" of the collection of all original possibilities: Since he is good and omnipotent, and since he chose this world out of all possibilities, this world must be good—in fact, this world is the best of \"all\" possible worlds.\n\nIn his writing, \"Discourse on Metaphysics\", Leibniz first establishes that God is an absolutely perfect being. He says people can logically conclude this through reason since \"the works must bear the imprint of the workman, because we can learn who he was just by inspecting them\". He calls this the Principle of Perfection, which says that God's knowledge and power is to the highest degree, more so than any human can comprehend. Due to God's omnipotence, Leibniz makes the premise that God has thought of every single possibility there is to be thought of before creating this world. His perfection gives him the ability to think \"beyond the power of a finite mind\", so he has sufficient reason to choose one world over the other.\n\nOut of all the possibilities, God chose the very best world because not only is God powerful, but he is also morally good. He writes \"the happiness of minds is God's principal aim, which He carries out as far as the general harmony will permit\", meaning a benevolent God will only do actions with the intention of good will towards his creation. If one supposed that this world is not the best, then it assumes that the creator of the universe is not knowledgeable enough, powerful enough, or inherently good, for an inherently good God would have created the best world to the best of his ability. It would overall be a contradiction to his good and perfect nature, and so the universe that God has chosen to create can only be the best of all possible worlds.\n\nOn the one hand, this view might help us rationalize some of what we experience: Imagine that all the world is made of good and evil. The best possible world would have the most good and the least evil. Courage is better than no courage. It might be observed, then, that without evil to challenge us, there can be no courage. Since evil brings out the best aspects of humanity, evil is regarded as necessary. So in creating this world God made some evil to make the best of all possible worlds. On the other hand, the theory explains evil not by denying it or even rationalizing it—but simply by declaring it to be part of the optimum combination of elements that comprise the best possible godly choice. Leibniz thus does not claim that the world is overall very good, but that because of the necessary interconnections of goods and evils, God, though omnipotent, could not improve it in one way without making it worse in some other way.\n\nGiovanni Gentile, in his work \"The General Theory of Mind as Pure Act\", claimed that if God had created everything to fall into line with the most favorable possible condition, it would suppose that all of reality is pre-realized and determined in the mind of God. Therefore, the apparent free will displayed by both God, by his necessity of being bound by what is the most good, and humanity in their limitations derived from God to be in line with the most good, are not free wills at all but entirely determinate. Thus ultimately relegated to blind naturalistic processes entrapping both God and humanity to necessity, robbing both of any true freely creative will.\n\nCritics of Leibniz, such as Voltaire, argue that the world contains an amount of suffering too great to justify optimism. While Leibniz argued that suffering is good because it incites human will, critics argue that the degree of suffering is too severe to justify belief that God has created the \"best of all possible worlds\". Leibniz also addresses this concern by considering what God desires to occur (his antecedent will) and what God allows to occur (his consequent will). Others, such as the Christian philosopher Alvin Plantinga, criticized Leibniz's theodicy by arguing that there probably is not such a thing as \"the best\" of all possible worlds, since one can always conceive a better world, such as a world with one more morally righteous person.\n\nThe \"Theodicy\" was deemed illogical by the philosopher Bertrand Russell. Russell argues that moral and physical evil must result from metaphysical evil (imperfection). But imperfection is merely finitude or limitation; if existence is good, as Leibniz maintains, then the mere existence of evil requires that evil also be good. In addition, libertarian Christian theology defines sin as not necessary but contingent, the result of free will. Russell maintains that Leibniz failed to logically show that metaphysical necessity (divine will) and human free will are not incompatible or contradictory.\n\nThe mathematician Paul du Bois-Reymond, in his \"Leibnizian Thoughts in Modern Science\", wrote that Leibniz thought of God as a mathematician:\n\nAs is well known, the theory of the maxima and minima of functions was indebted to him for the greatest progress through the discovery of the method of tangents. Well, he conceives God in the creation of the world like a mathematician who is solving a minimum problem, or rather, in our modern phraseology, a problem in the calculus of variationsthe question being to determine among an infinite number of possible worlds, that for which the sum of necessary evil is a minimum.\n\nThe statement that \"we live in the best of all possible worlds\" drew scorn, most notably from Voltaire, who lampooned it in his comic novella \"Candide\" by having the character Dr. Pangloss (a parody of Leibniz and Maupertuis) repeat it like a mantra. From this, the adjective \"Panglossian\" describes a person who believes that the world about us is the best possible one.\n\nWhile Leibniz does state this universe is the best possible version of itself, the standard for goodness does not seem clear to many of his critics. To Leibniz, the best universe means a world that is “the simplest in hypotheses and the richest in phenomena”, in addition to the “happiness of minds” being God's main goal. Voltaire, Bertrand Russell, and other critics seem to equate goodness of the universe to no evil or evil acts whatsoever, presuming that a universe that did not contain evil would be \"better\" and that God could have created such a universe, but chose not to. According to Leibniz, that is not the case. He believes that if a better alternative existed “God would have brought it into actuality”. Essentially, Leibniz affirms that no human can truly think up of a better universe because they lack a holistic understanding of the universe, and God, who has that holistic understanding, has already chosen the best option. All of this shifts the meaning of goodness from morality and actions to the quality and phenomena of this universe's existence. Despite that, the concept of the goodness of the universe is still a point of major contention in Leibniz's argument, as someone could always argue about the lack of goodness in the universe based on those parameters\n\nWhile not directly criticizing Leibniz, Spinoza holds a drastically different view on creation and the universe. Spinoza believes “that everything that God thinks or conceives must also exist”, and he combines God's will and God's understanding where Leibniz separates them. In other words, God cannot imagine an infinite number of worlds and “as a separate act of will” choose one of those to create. How does Spinoza explain creation then? To put it simply, everything in the universe “is a direct result of God’s nature”. The moment God thinks of something, it exists. As there are not an infinite amount of universes (according to Spinoza and Leibniz) God must have only conceived of one universe. This, however, still runs into the problem of the existence of evil. How can God, in his perfection, create a world capable of evil if the world is an extension of his mind? In any case, Spinoza still tries to justify a non-infinite basis for the universe, where reality is everything God has ever thought of.\n\nAquinas, using Scholasticism, treats the \"Best of all possible worlds\" problem in the \"Summa Theologica\" (1273):\n\nObjection 1: It seems that God does not exist; because if one of two contraries be infinite, the other would be altogether destroyed. But the word \"God\" means that He is infinite goodness. If, therefore, God existed, there would be no evil discoverable; but there is evil in the world. Therefore God does not exist.\n\nHe counters this in general by the \"quinque viae\", and in particular with this refutation:\n\nReply to Objection 1: As Augustine says (Enchiridion xi): \"Since God is the highest good, He would not allow any evil to exist in His works, unless His omnipotence and goodness were such as to bring good even out of evil.\" This is part of the infinite goodness of God, that He should allow evil to exist, and out of it produce good.\n\nThe theological theory of pandeism has been classed as a logical derivation of this proposition with the contention that:\n\n\n"}
{"id": "5548325", "url": "https://en.wikipedia.org/wiki?curid=5548325", "title": "Compensation (engineering)", "text": "Compensation (engineering)\n\nIn engineering, compensation is planning for side effects or other unintended issues in a design. In a more simpler term, it's a \"counter-procedure\" plan on expected side effect performed to produce more efficient and useful results. The design of an invention can itself also be to compensate for some other existing issue or exception.\n\nOne example is in a voltage-controlled crystal oscillator (VCXO), which is normally affected not only by voltage, but to a lesser extent by temperature. A temperature-compensated version (a TCVCXO) is designed so that heat buildup within the enclosure of a transmitter or other such device will not alter the piezoelectric effect, thereby causing frequency drift.\n\nAnother example is motion compensation on digital cameras and video cameras, which keep a picture steady and not blurry.\n\nOther examples in electrical engineering include:\n\nThere are also examples in civil engineering:\n\n"}
{"id": "17389946", "url": "https://en.wikipedia.org/wiki?curid=17389946", "title": "Crying", "text": "Crying\n\nCrying is the shedding of tears (or welling of tears in the eyes) in response to an emotional state, pain or a physical irritation of the eye. Emotions that can lead to crying include anger, happiness, or sadness. The act of crying has been defined as \"a complex secretomotor phenomenon characterized by the shedding of tears from the lacrimal apparatus, without any irritation of the ocular structures\", instead, giving a relief which protects from conjunctivitis. A related medical term is lacrimation, which also refers to non-emotional shedding of tears. Various forms of crying are known as \"sobbing\", \"weeping\", \"wailing\", \"whimpering\", \"bawling\", and \"blubbering\".\n\nFor crying to be described as sobbing, it usually has to be accompanied by a set of other symptoms, such as slow but erratic inhalation, occasional instances of breath holding and muscular tremor.\n\nA neuronal connection between the lacrimal gland (tear duct) and the areas of the human brain involved with emotion has been established. Scientists debate over whether humans are the only animals that produce tears in response to emotional states. Charles Darwin wrote in \"The Expression of the Emotions in Man and Animals\" that the keepers of Indian elephants in the London Zoo told him that their charges shed tears in sorrow. \n\nTears produced during emotional crying have a chemical composition which differs from other types of tears. They contain significantly greater quantities of the hormones prolactin, adrenocorticotropic hormone, and Leu-enkephalin, and the elements potassium and manganese.\n\nThe question of the function or origin of emotional tears remains open. Theories range from the simple, such as response to inflicted pain, to the more complex, including nonverbal communication in order to elicit altruistic behavior from others. Some have also claimed that crying can serve several biochemical purposes, such as relieving stress. Crying is believed to be an outlet or a result of a burst of intense emotional sensations, such as agony, surprise or joy. This theory could explain why people cry during cheerful events, as well as very painful events.\n\nIndividuals tend to remember the positive aspects of crying, and may create a link between other simultaneous positive events, such as resolving feelings of grief. Together, these features of memory reinforce the idea that crying helped the individual.\n\nIn Hippocratic and medieval medicine, tears were associated with the bodily humors, and crying was seen as purgation of excess humors from the brain. William James thought of emotions as reflexes prior to rational thought, believing that the physiological response, as if to stress or irritation, is a precondition to cognitively becoming aware of emotions such as fear or anger.\n\nWilliam H. Frey II, a biochemist at the University of Minnesota, proposed that people feel \"better\" after crying due to the elimination of hormones associated with stress, specifically adrenocorticotropic hormone. This, paired with increased mucosal secretion during crying, could lead to a theory that crying is a mechanism developed in humans to dispose of this stress hormone when levels grow too high. However, tears have a limited ability to eliminate chemicals, reducing the likelihood of this theory.\n\nRecent psychological theories of crying emphasize the relationship of crying to the experience of perceived helplessness. From this perspective, an underlying experience of helplessness can usually explain why people cry. For example, a person may cry after receiving surprisingly happy news, ostensibly because the person feels powerless or unable to influence what is happening.\n\nEmotional tears have also been put into an evolutionary context. One study proposes that crying, by blurring vision, can handicap aggressive or defensive actions, and may function as a reliable signal of appeasement, need, or attachment. Oren Hasson, an evolutionary psychologist in the zoology department at Tel Aviv University believes that crying shows vulnerability and submission to an attacker, solicits sympathy and aid from bystanders, and signals shared emotional attachments.\n\nAnother theory that follows evolutionary psychology is given by Paul D. MacLean, who suggests that the vocal part of crying was used first as a \"separation cry\" to help reunite parents and offspring. The tears, he speculates, are a result of a link between the development of the cerebrum and the discovery of fire. MacLean figures that since early humans must have relied heavily on fire, their eyes were frequently producing reflexive tears in response to the smoke. As humans evolved the smoke possibly gained a strong association with the loss of life and, therefore, sorrow.\n\nMore recently, CVBellieni analysed the weeping behavior, and concluded that most animals can cry but only humans have psychoemotional shedding of tears, also known as “weeping”. Weeping is a behavior that induces empathy perhaps with the mediation of the mirror neurons network, and influences the mood through the release of hormones elicited by the massage effect made by the tears on the cheeks, or through the relief of the sobbing rhythm.\n\nIt can be very difficult to observe biological effects of crying, especially considering many psychologists believe the environment in which a person cries can alter the experience of the crier. However, crying studies in laboratories have shown several physical effects of crying, such as increased heart rate, sweating, and slowed breathing. Although it appears that the type of effects an individual experiences depends largely on the individual, for many it seems that the calming effects of crying, such as slowed breathing, outlast the negative effects, which could explain why people remember crying as being helpful and beneficial.\n\nThe most common side effect of crying is feeling a lump in the throat of the crier, otherwise known as a globus sensation. Although many things can cause a globus sensation, the one experienced in crying is a response to the stress experienced by the sympathetic nervous system. When an animal is threatened by some form of danger, the sympathetic nervous system triggers several processes to allow the animal to fight or flee. This includes shutting down unnecessary body functions, such as digestion, and increasing blood flow and oxygen to necessary muscles. When an individual experiences emotions such as sorrow, the sympathetic nervous system still responds in this way. Another function increased by the sympathetic nervous system is breathing, which includes opening the throat in order to increase air flow. This is done by expanding the glottis, which allows more air to pass through. As an individual is undergoing this sympathetic response, eventually the parasympathetic nervous system attempts to undo the response by decreasing high stress activities and increasing recuperative processes, which includes running digestion. This involves swallowing, a process which requires closing the fully expanded glottis to prevent food from entering the larynx. The glottis, however, attempts to remain open as an individual cries. This fight to close the glottis creates a sensation that feels like a lump in the individual's throat.\n\nOther common side effects of crying are quivering lips, a runny nose, and an unsteady, cracking voice.\n\nAccording to the German Society of Ophthalmology, which has collated different scientific studies on crying, the average woman cries between 30 and 64 times a year, and the average man cries between 6 and 17 times a year.\n\nMen tend to cry for between two and four minutes, and women cry for about six minutes. Crying turns into sobbing for women in 65% of cases, compared to just 6% for men. Until adolescence, however, no difference between the sexes was found.\n\nAlthough crying is an infant's mode of communication, it is not limited to a monotonous sound. There are three different types of cries apparent in infants. The first of these three is a basic cry, which is a systematic cry with a pattern of crying and silence. The basic cry starts with a cry coupled with a briefer silence, which is followed by a short high-pitched inspiratory whistle. Then, there is a brief silence followed by another cry. Hunger is a main stimulant of the basic cry. An anger cry is much like the basic cry; however, in this cry, more excess air is forced through the vocal cords, making it a louder, more abrupt cry. This type of cry is characterized by the same temporal sequence as the basic pattern but distinguished by differences in the length of the various phase components. The third cry is the pain cry, which, unlike the other two, has no preliminary moaning. The pain cry is one loud cry, followed by a period of breath holding. Most adults can determine whether an infant's cries signify anger or pain. Most parents also have a better ability to distinguish their own infant's cries than those of a different child. A 2009 study found that babies mimic their parents' pitch contour. French infants wail on a rising note while German infants favor a falling melody.\nCarlo Bellieni found a correlation between the features of babies' crying and the level of pain, though he found no direct correlation between the cause of crying and its characteristics.\n\nT. Berry Brazelton has suggested that overstimulation may be a contributing factor to infant crying and that periods of active crying might serve the purpose of discharging overstimulation and helping the baby’s nervous system regain homeostasis.\n\nSheila Kitzinger found a correlation between the mother's prenatal stress level and later amount of crying by the infant. She also found a correlation between birth trauma and crying. Mothers who had experienced obstetrical interventions or who were made to feel powerless during birth had babies who cried more than other babies. Rather than try one remedy after another to stop this crying, she suggested that mothers hold their babies and allow the crying to run its course. Other studies have supported Kitzinger's findings. Babies who had experienced birth complications had longer crying spells at three months of age and awakened more frequently at night crying.\n\nBased on these various findings, Aletha Solter has proposed a general emotional release theory of infant crying. When infants cry for no obvious reason after all other causes (such as hunger or pain) are ruled out, she suggests that the crying may signify a beneficial stress-release mechanism. She recommends the \"crying-in-arms\" approach as a way to comfort these infants. Another way of comforting and calming the baby is to mimics the familiarity and coziness of mother’s womb. Dr. Robert Hamilton developed a technique to parents where a baby can be calmed and stop crying in 5 seconds.\n\nThere have been many attempts to differentiate between the two distinct types of crying: positive and negative. Different perspectives have been broken down into three dimensions to examine the emotions being felt and also to grasp the contrast between the two types.\n\nSpatial perspective explains sad crying as reaching out to be \"there\", such as at home or with a person who may have just died. In contrast, joyful crying is acknowledging being \"here.\" It emphasized the intense awareness of one's location, such as at a relative's wedding.\n\nTemporal perspective explains crying slightly differently. In temporal perspective, sorrowful crying is due to looking to the past with regret or to the future with dread. This illustrated crying as a result of losing someone and regretting not spending more time with them or being nervous about an upcoming event. Crying as a result of happiness would then be a response to a moment as if it is eternal; the person is frozen in a blissful, immortalized present.\n\nThe last dimension is known as the public-private perspective. This describes the two types of crying as ways to imply details about the self as known privately or one's public identity. For example, crying due to a loss is a message to the outside world that pleads for help with coping with internal sufferings. Or, as Arthur Schopenhauer suggested, sorrowful crying is a method of self-pity or self-regard, a way one comforts oneself. Joyful crying, in contrast, is in recognition of beauty, glory, or wonderfulness.\n\nThe Shia Ithna Ashari (Muslims who believe in twelve Imams after Muhammad) consider crying to be an important responsibility towards their leaders who were martyred.\nThey believe a true lover of Imam Hussain can feel the afflictions and oppressions Imam Hussain suffered; his feelings are so immense that they break out into tears and wail. The pain of the beloved is the pain of the lover. Crying on Imam Husain is the sign or expression of true love.\nThe Imams of Shias have encouraged crying especially on Imam Husaain and have informed about rewards for this act. They support their view through a tradition (saying) from Muhammad who said: (On the Day of Judgment, a group would be seen in the most excellent and honourable of states. They would be asked if they were of the Angels or of the Prophets. In reply they would state): \"We are neither Angels nor Prophets but of the indigent ones from the ummah of Muhammad\". They would then be asked: \"How then did you achieve this lofty and honourable status?\" They would reply: \"We did not perform very many good deeds nor did we pass all the days in a state of fasting or all the nights in a state of worship but yes, we used to offer our (daily) prayers (regularly) and whenever we used to hear the mention of Muhammad, tears would roll down our cheeks\".(Mustadrak al‑Wasail, vol 10, pg. 318)\n\nIn Orthodox and Catholic Christianity, tears are considered to be a sign of genuine repentance, and a desirable thing in many cases. Tears of true contrition are thought to be sacramental, helpful in forgiving sins, in that they recall the Baptism of the penitent.\n\nThere are three types of tears: basal tears, reflexive tears, and psychic tears. Basal tears are produced at a rate of about 1 to 2 microliters a minute, and are made in order to keep the eye lubricated and smooth out irregularities in the cornea. Reflexive tears are tears that are made in response to irritants to the eye, such as when chopping onions or getting poked in the eye. Psychic tears are produced by the lacrimal system and are the tears expelled during emotional states.\n\nThe lacrimal system is made up of a secretory system, which produces tears, and an excretory system, which drains the tears. The lacrimal gland is primarily responsible for producing emotional or reflexive tears. As tears are produced, some fluid evaporates between blinks, and some is drained through the lacrimal punctum. The tears that are drained through the punctum will eventually be drained through the nose. Any excess fluid that did not go into the punctum will fall over the eyelid, which produces tears that are cried.\n\n\n\n"}
{"id": "1690245", "url": "https://en.wikipedia.org/wiki?curid=1690245", "title": "Cyclopean image", "text": "Cyclopean image\n\nCyclopean image is a single mental image of a scene created by the brain by combining two images received from the two eyes. The mental process behind construction of the Cyclopean image is crucial to stereo vision. Autostereograms take advantage of this process to trick the brain into forming an apparent Cyclopean image from seemingly random patterns.\n\nCyclopean image is named after the mythical Cyclops with a single eye. Literally it refers to the way stereo sighted viewers perceive the centre of their fused visual field as lying between the two physical eyes, as if seen by a cyclopean eye. Alternative terms for cyclopean eye include third central imaginary eye and binoculus.\n\nThe term \"cyclopean stimuli\" refers to a form of visual stimuli that is defined by binocular disparity alone. It was named after the one-eyed Cyclops of Homer’s Odyssey by Bela Julesz. Julesz was a Hungarian radar engineer. He thought that stereopsis might help to discover hidden objects, this might be useful to find camouflaged objects. The important aspect of this research that Julesz showed using random dot stereograms was that disparity is sufficient for stereopsis, where Charles Wheatstone had only shown that binocular disparity was necessary for stereopsis. The Cyclops would not have been able to see a cyclopean stimulus, because having only one eye, he would not have been able to perceive binocular depth cues such as binocular disparity.\n\n"}
{"id": "525081", "url": "https://en.wikipedia.org/wiki?curid=525081", "title": "Desert bighorn sheep", "text": "Desert bighorn sheep\n\nDesert bighorn sheep (\"Ovis canadensis nelsoni\") is a subspecies of bighorn sheep (\"Ovis canadensis\") that is native to the deserts of the USA's intermountain west and southwestern regions, as well as northwestern Mexico.\n\nThe trinomial of this species commemorates the American naturalist Edward William Nelson (1855–1934). The characteristics and behavior of desert bighorn sheep generally follow those of other bighorn sheep, except for adaptation to the lack of water in the desert. They can go for extended periods of time without drinking water.\n\nThe desert bighorn sheep is also the mascot of the Universidad Autónoma de Baja California. \n\nThe range of Desert bighorn sheep includes habitats in the Mojave Desert, Colorado Desert, and Sonoran Desert. Anza-Borrego Desert State Park, Joshua Tree National Park, Death Valley National Park, Kofa National Wildlife Refuge, Cabeza Prieta National Wildlife Refuge, and Mojave National Preserve all offer protected habitat for this animal.\n\nPopulations of the desert bighorn sheep declined drastically with European colonization of the American Southwest beginning in the 16th century. These declines were followed by a period of population stabilization ascribed to conservation measures. As of 2004, desert bighorn sheep numbers remained extremely low, although the overall population trend had increased since 1960.\n\nDesert bighorn sheep are stocky, heavy-bodied sheep, similar in size to mule deer. Weights of mature rams range from 115 to 280 pounds (52 to 127 kg), while ewes are somewhat smaller. Due to their unique concave elastic hooves, bighorn are able to climb the steep, rocky terrain of the desert mountains with speed and agility. They rely on their keen eyesight to detect potential predators, such as mountain lions, coyotes, and bobcats, and they use their climbing ability to escape.\n\nBoth genders develop horns soon after birth, with horn growth continuing more or less throughout life. Older rams have impressive sets of curling horns measuring over three feet long with more than one foot of circumference at the base. The ewes' horns are much smaller and lighter and do not tend to curl. After eight years of growth, the horns of an adult ram may weigh more than 30 pounds. Annual growth rings indicate the animal's age. The rams may rub their own horns to improve their field of view. Both rams and ewes use their horns as tools to break open cactus, which they consume, and for fighting.\n\nDesert bighorn sheep typically live for 10–20 years. The typical diet of a desert bighorn sheep is mainly grasses. When grasses are unavailable, they turn to other food sources, such as sedges, forbs, or cacti.\n\nThe desert bighorn has become well adapted to living in the desert heat and cold and, unlike most mammals, their body temperature can safely fluctuate several degrees. During the heat of the day, they often rest in the shade of trees and caves.\n\nSouthern desert bighorn sheep are adapted to a desert mountain environment with little or no permanent water. Some may go without visiting water for weeks or months, sustaining their body moisture from food and from rainwater collected in temporary rock pools. They may have the ability to lose up to 30% of their body weight and still survive. After drinking water, they quickly recover from their dehydrated condition. Wildlife ecologists are just beginning to study the importance of this adaptive strategy, which has allowed small bands of desert bighorns to survive in areas too dry for many of their predators.\n\nDesert bighorn sheep are social, forming herds of eight to 10 individuals; sometimes herds of 100 are observed.\n\nRams battle to determine the dominant animal, which then gains possession of the ewes. Facing each other, rams charge head-on from distances of or more, crashing their massive horns together with tremendous impact, until one or the other ceases.\n\nBighorn sheep live in separate ram and ewe bands most of the year. They gather during the breeding season (usually July–October), but breeding may occur anytime in the desert due to suitable climatic conditions. Gestation lasts 150–180 days, and the lambs are usually born in late winter.\n\nThe number of desert bighorn sheep in North America in pristine times is unknown, but most likely was in the tens of thousands. In 1929, E.T. Seton estimated the pre-Columbian numbers of all subspecies of bighorn sheep in North America at 1.5-2.0 million. By 1960, however, the overall bighorn population in the United States, including desert bighorns, had dwindled to 15,000-18,200. Buechner documented major declines from the 1850s to the early 20th century. These declines were attributed to excessive hunting; competition and diseases from domestic livestock, particularly domestic sheep; usurpation of watering areas and critical range by human activities; and human-induced habitat changes.\n\nIn 1939, after intense lobbying by Frederick Russell Burnham and the Arizona Boy Scouts, President Franklin D. Roosevelt signed a proclamation to establish two desert areas in southwestern Arizona to help preserve the desert bighorn sheep: Cabeza Prieta National Wildlife Refuge and the Kofa National Wildlife Refuge. In 1941, the San Andres National Wildlife Refuge in New Mexico was added.\n\nDesert bighorn sheep populations have trended upward since the 1960s when their population was estimated at 6,700-8,100. The upward trend was caused by conservation measures, including habitat preservation. In 1980, desert bighorn sheep populations were estimated at 8,415-9,040. A state-by-state survey was conducted a few years later and estimated the overall US desert bighorn sheep population at 15,980. The 1993 estimate of the population is 18,965-19,040. The results of the state-by-state survey are shown to the right.\n\nIn southern California, by 1998, only 280 individuals of the peninsular bighorn sheep population remained, and that population was added to the list of the United States' most imperiled species. Populations in three southern counties had suffered greatly from disease, development, and predation. As of 2008, about 800 peninsular bighorns are believed to populate the desert backcountry from the US-Mexico border to the San Jacinto Mountains, with known populations in Anza-Borrego Desert State Park. These gains, combined with Bush Administration policies, prompted the US Fish and Wildlife Service to propose a reduction in protected sheep habitat by more than 50%, from .\n\n"}
{"id": "36585623", "url": "https://en.wikipedia.org/wiki?curid=36585623", "title": "Edraw Max", "text": "Edraw Max\n\nEdraw Max is a 2D business technical diagramming software which help create flowcharts, organizational charts, mind map, network diagrams, floor plans, workflow diagrams, business charts, and engineering diagrams. The current version, Edraw Max 8 was released on April 9, 2016 for Microsoft Windows, macOS, and Linux.\n\nThe current version, Edraw Max, is available in two editions: Free Viewer Version, Professional Editable Version. The latter has additional templates and examples for creating diagrams.\n\nA free trial is available for 30 days. It is not possible to save Edraw files in the trial version.\n\nEdraw Max gives users a Visio-like, professional quality diagramming tool.\n\nThe interface uses a ribbon layout.\n\nEdraw Max can generate organizational charts, Gantt charts and mind maps from data.\n\nEdraw Max saves content in an xml file format.\nThe .edx suffix is the default file format.\nThe .edxz suffix is a compressed xml file format used for sharing.\n\n"}
{"id": "195407", "url": "https://en.wikipedia.org/wiki?curid=195407", "title": "Einstein notation", "text": "Einstein notation\n\nIn mathematics, especially in applications of linear algebra to physics, the Einstein notation or Einstein summation convention is a notational convention that implies summation over a set of indexed terms in a formula, thus achieving notational brevity. As part of mathematics it is a notational subset of Ricci calculus; however, it is often used in applications in physics that do not distinguish between tangent and cotangent spaces. It was introduced to physics by Albert Einstein in 1916.\n\nAccording to this convention, when an index variable appears twice in a single term and is not otherwise defined (see free and bound variables), it implies summation of that term over all the values of the index. So where the indices can range over the set ,\n\nis simplified by the convention to:\nformula_2.\n\nThe upper indices are not exponents but are indices of coordinates, coefficients or basis vectors. That is, in this context should be understood as the second component of rather than the square of (this can occasionally lead to ambiguity). The upper index position in is because, typically, an index occurs once in an upper (superscript) and once in a lower (subscript) position in a term (see 'Application' below). And typically would be equivalent to the traditional .\n\nIn general relativity, a common convention is that \n\nIn general, indices can range over any indexing set, including an infinite set. This should not be confused with a typographically similar convention used to distinguish between tensor index notation and the closely related but distinct basis-independent abstract index notation.\n\nAn index that is summed over is a \"summation index\", in this case \". It is also called a dummy index since any symbol can replace \" without changing the meaning of the expression provided that it does not collide with index symbols in the same term.\n\nAn index that is not summed over is a \"free index\" and should appear only once per term. If such an index does appear, it usually also appears in terms belonging to the same sum, with the exception of special values such as zero.\n\nEinstein notation can be applied in slightly different ways. Typically, each index occurs once in an upper (superscript) and once in a lower (subscript) position in a term; however, the convention can be applied more generally to any repeated indices within a term. When dealing with covariant and contravariant vectors, where the position of an index also indicates the type of vector, the first case usually applies; a covariant vector can only be contracted with a contravariant vector, corresponding to summation of the products of coefficients. On the other hand, when there is a fixed coordinate basis (or when not considering coordinate vectors), one may choose to use only subscripts; see \"\" below.\n\nIn terms of covariance and contravariance of vectors, \n\nThey transform contravariantly or covariantly, respectively, with respect to change of basis.\n\nIn recognition of this fact, the following notation uses the same symbol both for a (co)vector and its \"components\", as in:\n\nwhere is the vector and are its components (not the th covector ), is the covector and are its components.\n\nIn the presence of a non-degenerate form (an isomorphism , for instance a Riemannian metric or Minkowski metric), one can raise and lower indices.\n\nA basis gives such a form (via the dual basis), hence when working on with a Euclidean metric and a fixed orthonormal basis, one has the option to work with only subscripts.\n\nHowever, if one changes coordinates, the way that coefficients change depends on the variance of the object, and one cannot ignore the distinction; see covariance and contravariance of vectors.\n\nIn the above example, vectors are represented as matrices (column vectors), while covectors are represented as matrices (row covectors).\n\nWhen using the column vector convention\n\n\n\nThe virtue of Einstein notation is that it represents the invariant quantities with a simple notation.\n\nIn physics, a scalar is invariant under transformations of basis. In particular, a Lorentz scalar is invariant under a Lorentz transformation. The individual terms in the sum are not. When the basis is changed, the \"components\" of a vector change by a linear transformation described by a matrix. This led Einstein to propose the convention that repeated indices imply the summation is to be done.\n\nAs for covectors, they change by the inverse matrix. This is designed to guarantee that the linear function associated with the covector, the sum above, is the same no matter what the basis is.\n\nThe value of the Einstein convention is that it applies to other vector spaces built from using the tensor product and duality. For example, , the tensor product of with itself, has a basis consisting of tensors of the form . Any tensor in can be written as:\n\n, the dual of , has a basis , , ..., </sup> which obeys the rule\nwhere is the Kronecker delta. As\n\nthe row/column coordinates on a matrix correspond to the upper/lower indices on the tensor product.\n\nIn Einstein notation, the usual element reference for the th row and th column of matrix becomes . We can then write the following operations in Einstein notation as follows.\n\n\nUsing an orthogonal basis, the inner product is the sum of corresponding components multiplied together:\n\nThis can also be calculated by multiplying the covector on the vector.\n\n\nAgain using an orthogonal basis (in 3 dimensions) the cross product intrinsically involves summations over permutations of components:\n\nwhere\n\n\nThe product of a matrix with a column vector is :\n\nequivalent to\n\nThis is a special case of matrix multiplication.\n\n\nThe matrix product of two matrices and is:\n\nequivalent to\n\n\nFor a square matrix , the trace is the sum of the diagonal elements, hence the sum over a common index .\n\n\nThe outer product of the column vector by the row vector yields an matrix :\n\nSince and represent two \"different\" indices, there is no summation and the indices are not eliminated by the multiplication.\n\n\nGiven a tensor, one can raise an index or lower an index by contracting the tensor with the metric tensor, . For example, take the tensor , one can raise an index:\n\nformula_17\n\nOr one can lower an index:\n\nformula_18\n\n\n"}
{"id": "37606777", "url": "https://en.wikipedia.org/wiki?curid=37606777", "title": "Emotions in virtual communication", "text": "Emotions in virtual communication\n\nEmotions in virtual communication differ in a variety of ways from those in face-to-face interactions due to the characteristics of computer-mediated communication (CMC). CMC may lack many of the auditory and visual cues normally associated with the emotional aspects of interactions. Research in this area has investigated how and when individuals display and interpret various emotions in virtual settings.\n\nWhile text-based communication eliminates audio and visual cues, there are other methods for adding emotion. Emoticons, or emotional icons, can be used to display various types of emotions. Similar to emotional displays in face-to-face communication, it was found that females tend to use more emoticons than their male counterparts. Beyond simply using emoticons, in virtual communication platforms, people tend to capitalize letters or words to add emphasis to speaking.\n\nThere are a variety of characteristics of virtual communication that result in an increase in the amount of emotion displayed. The lack of social cues in CMC has been found to have a depersonalizing effect. Additionally, there can be greater anonymity or perceptions of anonymity in virtual communication. This combination of anonymous and social detached communication has been shown to increase the likelihood of flaming, or angry and hostile language as a result of uninhibited behavior.\n\nFurthermore, it has been shown that virtual communication can reduce normative social pressures. As a result of decreased social pressures, individuals may feel more comfortable disclosing either positive or negative affect, which may not be considered appropriate in normal face-to-face interactions. For example, in a large part due to decreased social hierarchies, Gilmore and Warren (2007) found many instances of feelings of intimacy, playfulness, and pride in a virtual teaching environment.\n\nThe lack of social and emotional cues over virtual communication platforms can result in increased instances of misinterpreting emotion and intentions. Kruger, Epley, Parker, and Ng (2005) found that individuals overestimate both their ability to clearly relay and interpret emotions via email. They attribute this inability to relay emotions effectively to others over CMC to a combination of egocentrism and a lack of paralinguistic cues including gestures, emphasis, and intonations.\n\nOne of the reasons that emails that are intended to be positive may come across as more neutral is that the process of email itself tends to be less stimulating than face-to-face communication. Since many people tend to associate emails with work-related matters, they come to expect less positive affect to be displayed in emails. Furthermore, the emotional ambiguity of email messages may actually lead to them to be interpreted as more negatively than they were intended. Byron (2008) notes that emails from senders higher in status will be more likely be perceived as negative than emails received from people who are lower in status.\n\nGiven the permanent and potentially public nature of virtual communication, it is much more likely that unintended parties will view and interpret messages as opposed face-to-face communication, which is fleeting. It has been found that when third parties view virtual communications, the third parties may interpret interactions as contentious disputes, when in fact there may not have actually been any conflict.\n\nDue to the excess disclosure of and the potential to misinterpret emotions, conflicts can arise in virtual communication. Communication mediums that have the greatest amount of emotional cues and immediacy of feedback will be best to reduce conflict. Increased emotional cues allow for better detection of negative affect, and greater displays of positive affect to counter any negative emotions. Immediacy of feedback relates to how quickly messages are transmitted via a particular communication medium, and the expectation for which they will be responded. For example, instant messaging has a higher degree of immediacy of feedback than email because instant messaging tends to result in much more synchronous communication than email. Immediacy of feedback allows individuals to detect and address frustration and other negative emotions more quickly. Furthermore, communication mediums that are more synchronous better allow for spontaneous comments, such as jokes, which are necessary for positive affect. Increased positive affect helps to create positive interactions which reduce the likelihood of conflict.\n\n"}
{"id": "2132356", "url": "https://en.wikipedia.org/wiki?curid=2132356", "title": "Fixed-point index", "text": "Fixed-point index\n\nIn mathematics, the fixed-point index is a concept in topological fixed-point theory, and in particular Nielsen theory. The fixed-point index can be thought of as a multiplicity measurement for fixed points.\n\nThe index can be easily defined in the setting of complex analysis: Let \"f\"(\"z\") be a holomorphic mapping on the complex plane, and let \"z\" be a fixed point of \"f\". Then the function \"f\"(\"z\") − \"z\" is holomorphic, and has an isolated zero at \"z\". We define the fixed point index of \"f\" at \"z\", denoted \"i\"(\"f\", \"z\"), to be the multiplicity of the zero of the function \"f\"(\"z\") − \"z\" at the point \"z\".\n\nIn real Euclidean space, the fixed-point index is defined as follows: If \"x\" is an isolated fixed point of \"f\", then let \"g\" be the function defined by\n\nThen \"g\" has an isolated singularity at \"x\", and maps the boundary of some deleted neighborhood of \"x\" to the unit sphere. We define \"i\"(\"f\", \"x\") to be the Brouwer degree of the mapping induced by \"g\" on some suitably chosen small sphere around \"x\".\n\nThe importance of the fixed-point index is largely due to its role in the Lefschetz–Hopf theorem, which states:\n\nwhere Fix(\"f\") is the set of fixed points of \"f\", and \"Λ\" is the Lefschetz number of \"f\".\n\nSince the quantity on the left-hand side of the above is clearly zero when \"f\" has no fixed points, the Lefschetz–Hopf theorem trivially implies the Lefschetz fixed point theorem.\n\n"}
{"id": "44975179", "url": "https://en.wikipedia.org/wiki?curid=44975179", "title": "Giraud subcategory", "text": "Giraud subcategory\n\nIn mathematics, Giraud subcategories form an important class of subcategories of Grothendieck categories. They are named after Jean Giraud.\n\nLet formula_1 be a Grothendieck category. A full subcategory formula_2 is called \"reflective\", if the inclusion functor formula_3 has a left adjoint. If this left adjoint of formula_4 also preserves \nkernels, then formula_2 is called a \"Giraud subcategory\".\n\nLet formula_2 be Giraud in the Grothendieck category formula_1 and formula_3 the inclusion functor.\n\n\n"}
{"id": "1945794", "url": "https://en.wikipedia.org/wiki?curid=1945794", "title": "Gnosology", "text": "Gnosology\n\nIn philosophy, gnosology literally means the study of \"gnosis\", meaning knowledge or esoteric knowledge. Gnosology has also been used to render Johann Gottlieb Fichte's term for his own version of transcendental idealism, \"Wissenschaftslehre\", meaning \"Doctrine of Knowledge\".\n\n"}
{"id": "1123587", "url": "https://en.wikipedia.org/wiki?curid=1123587", "title": "Habeas Corpus Act 1679", "text": "Habeas Corpus Act 1679\n\nThe Habeas Corpus Act 1679 is an Act of Parliament in England (31 Cha. 2 c. 2) during the reign of King Charles II. It was passed by what became known as the Habeas Corpus Parliament to define and strengthen the ancient prerogative writ of \"habeas corpus\", which required a court to examine the lawfulness of a prisoner's detention and thus prevent unlawful or arbitrary imprisonment.\n\nThe Act is often wrongly described as the origin of the writ of \"habeas corpus\". But the writ of \"habeas corpus\" had existed in various forms in England for at least five centuries before and is thought to have originated in the 12th Century Assize of Clarendon. It was guaranteed, but not created, by Magna Carta in 1215, whose article 39 reads: \"No freeman shall be taken or imprisoned or disseised or exiled or in any way destroyed, nor will we go upon him nor will we send upon him except upon the lawful judgement of his peers or the law of the land.\" The Act of 1679 followed an earlier Habeas Corpus Act of 1640, which established that the command of the King or the Privy Council was no answer to a petition of \"habeas corpus\". Further Habeas Corpus Acts were passed by the British Parliament in 1803, 1804, 1816, and 1862, but it is the Act of 1679 which is remembered as one of the most important statutes in English constitutional history. Though amended, it remains on the statute book to this day.\n\nIn criminal matters other than treason and felonies, the act gave prisoners or third parties acting on their behalf the right to challenge their detention by demanding from the Lord Chancellor, Justices of the King's Bench, and the Barons of the Exchequer of the jurisdiction a judicial review of their imprisonment. The act laid out certain temporal and geographical conditions under which prisoners had to be brought before the courts. Jailors were forbidden to move prisoners from one prison to another or out of the country to evade the writ. In case of disobedience jailers would be punished with severe fines which had to be paid to the prisoner.\n\nThe Act came about because the Earl of Shaftesbury encouraged his friends in the Commons to introduce the Bill where it passed and was then sent up to the House of Lords. Shaftesbury was the leading Exclusionist—those who wanted to exclude Charles II's brother James, Duke of York from the succession—and the Bill was a part of that struggle as they believed James would rule arbitrarily. The Lords decided to add many wrecking amendments to the Bill in an attempt to kill it; the Commons had no choice but to pass the Bill with the Lords' amendments because they learned that the King would soon end the current parliamentary session.\n\nThe Bill went back and forth between the two Houses, and then the Lords voted on whether to set up a conference on the Bill. If this motion was defeated the Bill would stay in the Commons and therefore have no chance of being passed. Each side—those voting for and against—appointed a teller who stood on each side of the door through which those Lords who had voted \"aye\" re-entered the House (the \"nays\" remained seated). One teller would count them aloud whilst the other teller listened and kept watch to know if the other teller was telling the truth. Shaftesbury's faction had voted for the motion, so they went out and re-entered the House. Gilbert Burnet, one of Shaftesbury's friends, recorded what then happened:\n\nLord Grey and Lord Norris were named to be the tellers: Lord Norris, being a man subject to vapours, was not at all times attentive to what he was doing: so, a very fat lord coming in, Lord Grey counted him as \"ten\", as a jest at first: but seeing Lord Norris had not observed it, he went on with this misreckoning of ten: so it was reported that they that were for the Bill were in the majority, though indeed it went for the other side: and by this means the Bill passed.\n\nThe clerk recorded in the minutes of the Lords that the \"ayes\" had fifty-seven and the \"nays\" had fifty-five, a total of 112, but the same minutes also state that only 107 Lords had attended that sitting.\n\nThe King arrived shortly thereafter and gave Royal Assent before proroguing Parliament. The Act is now stored in the Parliamentary Archives.\n\nThe \"Habeas Corpus Act 1679\" and the later acts of 1803, 1804, 1816 and 1862 were reprinted in New Zealand as Imperial Acts in force in New Zealand in 1881. \n\n\n"}
{"id": "246545", "url": "https://en.wikipedia.org/wiki?curid=246545", "title": "Hedone", "text": "Hedone\n\nHedone was the personification and goddess of pleasure, enjoyment, and delight. Hedone, also known as Voluptas in Roman mythology, is the daughter of the Greek gods Eros (Cupid) and Psyche. She was associated more specifically with sensual pleasure. Her opposites were the Algos, personifications of pain.\n\n\"Hēdonē\" () is a Greek word meaning pleasure, and is the root of the English word \"hedonism\". In the philosophy of Epicurus, hēdonē is described as a pleasure that may or may not derive from actions that are virtuous, whereas another form of pleasure, \"terpsis\", is always virtuous. According to the Stanford Encyclopedia of Philosophy, Epicurus uses hēdonē in reference to only physical pleasures\n\n"}
{"id": "973730", "url": "https://en.wikipedia.org/wiki?curid=973730", "title": "Helsinki Watch", "text": "Helsinki Watch\n\nHelsinki Watch was a private American NGO established by Robert L. Bernstein in 1978, designed to monitor the former Soviet Union’s compliance with the 1975 Helsinki Accords. Expanding in size and scope, Helsinki Watch began using media coverage to document human rights violations committed by abusive governments Since its inception, it has produced several other watch committees dedicated to monitoring human rights in other parts of the world. In 1988, Helsinki Watch and all its companion watch committees were combined to form Human Rights Watch \n\nFollowing the multinational agreement establishing the Helsinki Accords in 1975, the Helsinki Watch was established to ensure Eastern Bloc countries undergoing severe civil conflict comply with the provisions originally established in the Helsinki Accords. This was the result of an emergence of pressing requests on behalf of organizations located in Moscow, Prague, and Warsaw who were tasked with monitoring the Soviet Union and regions of Eastern Europe to ensure their compliance in facilitating various human rights pledges made throughout the Accords, many of which were arrested by Soviet authorities in early 1977. One of the primary objectives of Helsinki Watch was to serve as an instrument of advocacy for freeing the imprisoned monitors arrested by Soviet officials, but its most noteworthy accomplishments was predicated on promoting civil and political freedoms in the Soviet Union and regions of Eastern Europe. Helsinki Watch developed a means of identifying the corrupt actions of governments by publicly acknowledging unethical behaviour carried out by different governmental bodies through media coverage and directly through policymakers on an international scale.\n\nAs tensions built between the United States and the Soviet Union, The Americas Watch was created in 1981 to avoid criticism of hypocrisy. The Americas watch set out to observe and acknowledge abuses carried out by governmental bodies situated in Central America, and most notably criticized governments such as the United States for their involvement in providing arms and support to dangerous regimes situated in the Americas. The establishment of other similar organizations rapidly increased through their classification as “The Watch Committees” with the creation of the Asia Watch (1985), Africa Watch (1986), and Middle East Watch (1989). In 1988, these committees formally adopted the overarching title of The Human Rights Watch.\n\nThe establishment of the Helsinki Watch was made possible by a $400,000 grant donated by the Ford Foundation.\n\n\nUpon its establishment, the Helsinki Watch immediately became a major organization with significant leverage internationally. Initially, the Helsinki Watch would directly appeal to communist leaders by creating petitions and publicly \"naming and shaming\" abusive governments. When this method proved to be ineffective, they quickly graduated to using political influence from important Western and European politicians to further their mission of influencing government policy both directly and indirectly. As the Helsinki Watch grew it continued to build its reputation of providing accurate and reliable information on human rights violations in Eastern Europe and the Soviet Union. The Helsinki Watch is said to have played an important role in shaping human rights in the 1980s.\n\nThe Helsinki Watch was initially met with allegations of bias during its early days. It was criticized for narrowing its scope to human rights violations committed by the Soviet bloc while ignoring human rights violations that were occurring in other parts of the world. Many suggested that its strategy of enlisting the help of Europeans to denounce the Soviet Union reflected this. It was specifically criticized for being hypocritical in its reporting, as in its early days it neglected to recognize abuses taking place within America. In response to such criticism, the founders of Helsinki Watch created a new division called Americas Watch. From there the organization expanded rapidly, establishing Watches to cover other parts of the world. In 1988, all of the Helsinki Watch’s separate divisions were amalgamated into one unit called the Human Rights Watch.\n\nPublished in 1991, major publications of The Helsinki Watch include:\n\n"}
{"id": "1959536", "url": "https://en.wikipedia.org/wiki?curid=1959536", "title": "ISO 31-1", "text": "ISO 31-1\n\nISO 31-1 is the part of international standard ISO 31 that defines names and symbols for quantities and units related to \"space and time\". It was superseded in 2006 by ISO 80000-3.\n\nIts definitions include:\n\nAnnex A of ISO 31-1 lists units of space and time based on the foot, pound, and second.\n\nAnnex B lists some other non-SI units of space and time, namely the gon, light year, astronomical unit, parsec, tropical year, and gal.\n"}
{"id": "1241928", "url": "https://en.wikipedia.org/wiki?curid=1241928", "title": "James Hutchison Stirling", "text": "James Hutchison Stirling\n\nJames Hutchison Stirling (22 June 1820 – 19 March 1909) was a Scottish idealist philosopher and physician. His work \"The Secret of Hegel\" (1st edition, 1865, in 2 vols.; revised edition, 1898, in 1 vol.) gave great impetus to the study of Hegelian philosophy both in Britain and in the United States, and it was also accepted as an authoritative work on Hegel's philosophy in Germany and Italy.\n\nJames Hutchison Stirling was born in Glasgow, Scotland, the 5th son (and the youngest of 6 children) of William Stirling (died 14 March 1851) and Elizabeth Christie (d. 1828). William was a wealthy textile manufacturer who was a partner in the Glasgow firm of James Hutchison & Co., which manufactured muslin (lightweight cotton cloth in a plain weave, used for making sheets and for a variety of other purposes). William was known for his deeply-held religious views, many of which strongly influenced his son James.\n\nStirling studied at Young's Academy in Glasgow, followed by 9 years of education (1833-1842) at the University of Glasgow, where he studied medicine, history, and classics. He became a Licentiate (1842, medical diploma) and Fellow (1860) of the Royal College of Surgeons of Edinburgh.\n\nStirling married Jane Hunter Mair (died 5 July 1903), an old family friend, on 28 April 1847 in Irvine, North Ayrshire, Scotland. The couple had seven children (5 daughters and 2 sons), as follows: Jessie Jane Stirling (born 26 June 1850) (who married Rev. Robert Armstrong of Glasgow), Elizabeth Margaret Stirling (11 February 1852 - 1871), Amelia Hutchison Stirling, Florence Hutchison Stirling (1858 - 6 May 1948), Lucy Stirling, William Stirling, and David Stirling. Stirling's daughter Amelia wrote many books on historical subjects, and she was the joint-translator - with William Hale White (1831-1913) - of Spinoza's \"Ethics\" (1883). She also wrote a biography of her father James titled \"James Hutchison Stirling: His Life and Work\" (London and Leipzig: T. Fisher Unwin, 1912). Stirling's daughter Florence won the Scottish Ladies' Championship (a chess championship) 5 times (in 1905, 1906, 1907, 1912, and 1913).\n\nAfter receiving a large inheritance from his father's estate in 1851, Stirling left his medical practice. He then set out to learn French and German, for the purpose of being able to better understand continental philosophical trends. In pursuit of this goal, he moved his family briefly to Boulogne-sur-Mer, France (located in the current Pas-de-Calais department of the Hauts-de-France region), then to Paris for 18 months, then to St. Servan (located 2 miles from the ferry port of St. Malo in the Ille-et-Vilaine department of the Brittany region of France) for four and a half years, and then finally to Heidelberg, Germany. In November 1857, Stirling and his family took up residence in London (at 3 Wilton Terrace, Kensington), where they lived for about 3 years. After this, in 1860 Stirling returned to Edinburgh - his address there was 4 Laverock Bank Road, Trinity, Edinburgh - which then became his permanent residence until the end of his life, and where he wrote on the philosophy of Georg Wilhelm Friedrich Hegel (1770-1831) and many other subjects. The primary result of his comprehensive Hegel studies was his influential work \"The Secret of Hegel\" (2 vols., 1865).\n\nOne of Stirling's other major philosophical works - \"Philosophy and Theology\" (1890) (consisting of his 20 Gifford Lectures, delivered at the University of Edinburgh in 1889-1890) - focuses not on Hegelian philosophical topics, but on Charles Darwin's evolutionary theories.\n\nFrederick Copleston (\"A History of Philosophy\" vol. VII, p. 12) wrote \"...we may be inclined to smile at J. H. Stirling's picture of Hegel as the great champion of Christianity.\"\n\nIn his final years he lived at 4 Laverockbank Road in Trinity, Edinburgh.\n\nStirling died in Edinburgh. He is buried in Warriston Cemetery on the north side of the city. His grave lies in the centre of the long, upper section north of the vaults, facing south onto an east-west path.\n\nOther works: \n\nMore concerned with literature:\n\n\n"}
{"id": "584553", "url": "https://en.wikipedia.org/wiki?curid=584553", "title": "Joel Feinberg", "text": "Joel Feinberg\n\nJoel Feinberg (October 19, 1926 in Detroit, Michigan – March 29, 2004 in Tucson, Arizona) was an American political and legal philosopher. He is known for his work in the fields of ethics, action theory, philosophy of law, and political philosophy as well as individual rights and the authority of the state. Feinberg was one of the most influential figures in American jurisprudence of the last fifty years.\n\nFeinberg studied at the University of Michigan, writing his dissertation on the philosophy of the Harvard professor Ralph Barton Perry under the supervision of Charles Stevenson. He taught at Brown University, Princeton University, UCLA and Rockefeller University, and, from 1977, at the University of Arizona, where he retired in 1994 as Regents Professor of Philosophy and Law.\n\nFeinberg was internationally distinguished for his research in moral, social and legal philosophy. His major four-volume work, \"The Moral Limits of the Criminal Law\", was published between 1984 and 1988. Feinberg held many major fellowships during his career and lectured by invitation at universities around the world. He was an esteemed and highly successful teacher, and many of his students are now prominent scholars and professors at universities across the US. His former students include Jules Coleman, Russ Shafer-Landau, and Clark Wolf.\n\nFeinberg's most important contribution to legal philosophy is his four-volume book, \"The Moral Limits of the Criminal Law\" (1984-1988), a work that is frequently characterized as \"magisterial.\" Feinberg's goal in the book is to answer the question: What sorts of conduct may the state rightly make criminal? John Stuart Mill, in his classic On Liberty (1859), had given a staunchly \"liberal\" answer to this question. According to Mill, the only kind of conduct that the state may rightly criminalize is conduct that causes harm to others. Though Feinberg, who had read and re-read Mill's classic text many times, shares Mill's liberal leanings, he thinks that liberals can and should admit that certain kinds of non-harmful but profoundly offensive conduct can also properly be prohibited by law. In \"The Moral Limits of the Criminal Law\", Feinberg seeks to develop and defend a broadly Millian view of the limits of state power over the individual. In the process, he defends what many would view as characteristically \"liberal\" positions on topics such as suicide, obscenity, pornography, hate speech, and euthanasia. He also analyzes with great subtlety and skill concepts such as \"harm,\" \"offense,\" \"wrong,\" \"autonomy,\" \"responsibility,\" \"paternalism,\" \"coercion,\" and \"exploitation.\" In a surprising twist, Feinberg concedes in the conclusion to the final volume that liberalism may not be fully defensible. Liberals, he claims, may have to concede that in rare cases it may be legitimate for the government to criminalize certain kinds of moral harms and harmless immoralities.\n\nIn \"Offense to Others\", the second volume of \"The Moral Limits of the Criminal Law\", Feinberg offers one of the most famous thought-experiments in recent philosophy: a series of imaginary scenarios he calls \"a ride on the bus.\" Feinberg invites us to imagine a bus ride in which you, a passenger rushing to an important appointment, are confronted by a series of deeply offensive but harmless acts. Some of the acts involve affronts to the senses (e.g., a man scratching his fingernails across a slate). Others involve acts that are deeply disgusting or revolting (e.g., eating various kinds of nauseatingly repulsive things). Still others involve affronts to our religious, moral, or patriotic sensibilities (e.g., overt acts of flag desecration); shocks to our sense of shame or embarrassment (such as acts of public sex); and a wide range of offensive conduct based on fear, anger, humiliation, boredom or frustration. The thought experiment is designed to test the limits of our tolerance for harmless but deeply offensive forms of behavior. More precisely, it raises the question \"whether there are any human experiences that are harmless in themselves yet so unpleasant that we can rightly demand legal protection from them even at the cost of other persons' liberties.\" Feinberg argues that even left-leaning, highly tolerant liberals must recognize that some forms of harmless but profoundly offensive conduct can properly be criminalized.\n\nIn a paper prepared in 1958 for the benefit of students at Brown, Feinberg seeks to refute the philosophical theory of psychological egoism, which in his opinion is fallacious. So far as he can tell, there are four primary arguments for it:\n\n\nFeinberg observes that such arguments for psychological egoism are rarely mounted on the basis of empirical proof when, being psychological, they very well ought to. The opening argument he dubs a tautology from which \"nothing whatever concerning the nature of my motives or the objective of my desires can possibly follow [...]. It is not the genesis of an action or the \"origin\" of its motives which makes it a 'selfish' one, but rather the 'purpose' of the act or the \"objective\" of its motives; \"not where the motive comes from\" (in voluntary actions it always comes from the agent) but \"what it aims at\" determines whether or not it is selfish.\"\n\nSimilarly flawed in Feinberg's opinion is the second argument. Just because all successful endeavour engenders pleasure does not necessarily entail that pleasure is the sole objective of all endeavour. He uses William James's analogy to illustrate this fallacy: although an ocean liner always consumes coal on its trans-Atlantic voyages, it is unlikely that the sole purpose of these voyages is coal consumption.\n\nThe third argument, unlike the first two, contains no \"non sequitur\" that Feinberg can see. He nevertheless adjudges that such a sweeping generalisation is unlikely to be true.\n\nIn the final argument, Feinberg sees a paradox. The only way to achieve happiness, he believes, is to forget about it, but psychological egoists hold that all human endeavour, even that which achieves happiness, is geared towards happiness. Feinberg poses a thought experiment in which a character named Jones is apathetic about all but the pursuit of his own happiness. Because he has no means to achieve that end, however, \"[i]t takes little imagination [...] to see that Jones's one desire is bound to be frustrated.\" To pursue only happiness, then, is to fail utterly to achieve it.\n\n\n\n\n"}
{"id": "13830115", "url": "https://en.wikipedia.org/wiki?curid=13830115", "title": "Jump-and-Walk algorithm", "text": "Jump-and-Walk algorithm\n\nJump-and-Walk is an algorithm for point location in triangulations (though most of the theoretical analysis were performed in 2D and 3D random Delaunay triangulations). Surprisingly, the algorithm does not need any preprocessing or complex data structures except some simple representation of the triangulation itself. The predecessor of Jump-and-Walk was due to Lawson (1977) and Green and Sibson (1978), which picks a random starting point S and then walks from S toward the query point Q one triangle at a time. But no theoretical analysis was known for these predecessors until after mid-1990s.\n\nJump-and-Walk picks a small group of sample points and starts the walk from the sample point which is the closest to Q until the simplex containing Q is found. The algorithm was a folklore in practice for some time, and the formal presentation of the algorithm and the analysis of its performance on 2D random Delaunay triangulation was done by Devroye, Mucke and Zhu in mid-1990s (the paper appeared in Algorithmica, 1998). The analysis on 3D random Delaunay triangulation was done by Mucke, Saias and Zhu (ACM Symposium of Computational Geometry, 1996). In both cases, a boundary condition was assumed, namely, Q must be slightly away from the boundary of the convex domain where the vertices of the random Delaunay triangulation are drawn. In 2004, Devroye, Lemaire and Moreau showed that in 2D the boundary condition can be withdrawn (the paper appeared in Computational Geometry: Theory and Applications, 2004).\n\nJump-and-Walk has been used in many famous software packages, e.g., QHULL, Triangle and CGAL.\n\n \n"}
{"id": "356041", "url": "https://en.wikipedia.org/wiki?curid=356041", "title": "Left–right political spectrum", "text": "Left–right political spectrum\n\nThe left–right political spectrum is a system of classifying political positions, ideologies and parties, from equality on the left to social hierarchy on the right. Left-wing politics and right-wing politics are often presented as opposed, although a particular individual or group may take a left-wing stance on one matter and a right-wing stance on another; and some stances may overlap and be considered either left- or right-wing depending on the ideology. In France, where the terms originated, the Left has been called 'the party of movement' and the Right 'the party of order'. The intermediate stance is called centrism and a person with such a position is a moderate or centrist.\n\nThe terms \"left\" and \"right\" appeared during the French Revolution of 1789 when members of the National Assembly divided into supporters of the king to the president's right and supporters of the revolution to his left. One deputy, the Baron de Gauville, explained: \"We began to recognize each other: those who were loyal to religion and the king took up positions to the right of the chair so as to avoid the shouts, oaths, and indecencies that enjoyed free rein in the opposing camp\". However, the Right opposed the seating arrangement because they believed that deputies should support private or general interests but should not form factions or political parties. The contemporary press occasionally used the terms \"left\" and \"right\" to refer to the opposing sides.\n\nWhen the National Assembly was replaced in 1791 by a Legislative Assembly comprising entirely new members, the divisions continued. \"Innovators\" sat on the left, \"moderates\" gathered in the centre, while the \"conscientious defenders of the constitution\" found themselves sitting on the right, where the defenders of the Ancien Régime had previously gathered. When the succeeding National Convention met in 1792, the seating arrangement continued, but following the coup d'état of 2 June 1793 and the arrest of the Girondins the right side of the assembly was deserted and any remaining members who had sat there moved to the centre. However, following the Thermidorian Reaction of 1794 the members of the far-left were excluded and the method of seating was abolished. The new constitution included rules for the assembly that would \"break up the party groups\". However, following the Restoration in 1814–1815 political clubs were again formed. The majority ultraroyalists chose to sit on the right. The \"constitutionals\" sat in the centre while independents sat on the left. The terms extreme right and extreme left as well as centre-right and centre-left came to be used to describe the nuances of ideology of different sections of the assembly.\n\nThe terms \"left\" and \"right\" were not used to refer to political ideology per se, but only to seating in the legislature. After 1848, the main opposing camps were the \"democratic socialists\" and the \"reactionaries\" who used red and white flags to identify their party affiliation. With the establishment of the Third Republic in 1871, the terms were adopted by political parties: the Republican Left, the Centre Right and the Centre Left (1871) and the Extreme Left (1876) and Radical Left (1881). The beliefs of the group called the Radical Left were actually closer to the Centre Left than the beliefs of those called the Extreme Left. Beginning in the early twentieth century, the terms \"left\" and \"right\" came to be associated with specific political ideologies and were used to describe citizens' political beliefs, gradually replacing the terms \"reds\" and \"the reaction\". Those on the Left often called themselves \"republicans\", while those on the Right often called themselves \"conservatives\". The words Left and Right were at first used by their opponents as slurs.\n\nBy 1914, the Left half of the legislature in France was composed of Unified Socialists, Republican Socialists and Socialist Radicals, while the parties that were called \"Left\" now sat on the right side. The use of the words Left and Right spread from France to other countries and came to be applied to a large number of political parties worldwide, which often differed in their political beliefs. There was asymmetry in the use of the terms Left and Right by the opposing sides. The Right mostly denied that the left–right spectrum was meaningful because they saw it as artificial and damaging to unity. However, the Left, seeking to change society, promoted the distinction. As Alain observed in 1931: \"When people ask me if the division between parties of the Right and parties of the Left, men of the Right and men of the Left, still makes sense, the first thing that comes to mind is that the person asking the question is certainly not a man of the Left\". In British politics, the terms \"right\" and \"left\" came into common use for the first time in the late 1930s in debates over the Spanish Civil War. The Scottish sociologist Robert M. MacIver noted in \"The Web of Government\" (1947): The right is always the party sector associated with the interests of the upper or dominant classes, the left the sector expressive of the lower economic or social classes, and the centre that of the middle classes. Historically this criterion seems acceptable. The conservative right has defended entrenched prerogatives, privileges and powers; the left has attacked them. The right has been more favorable to the aristocratic position, to the hierarchy of birth or of wealth; the left has fought for the equalization of advantage or of opportunity, for the claims of the less advantaged. Defense and attack have met, under democratic conditions, not in the name of class but in the name of principle; but the opposing principles have broadly corresponded to the interests of the different classes.\n\nGenerally, the left-wing is characterized by an emphasis on 'ideas such as freedom, equality, fraternity, rights, progress, reform and internationalism', while the right-wing is characterized by an emphasis on 'notions such as authority, hierarchy, order, duty, tradition, reaction and nationalism'.\n\nPolitical scientists and other analysts regard the left as including anarchists, communists, socialists, democratic socialists, social democrats, left-libertarians, progressives and social liberals. Movements for racial equality and trade unionism have also been associated with the left.\n\nPolitical scientists and other analysts regard the Right as including Christian democrats, conservatives, right-libertarians, neoconservatives, imperialists, monarchists, fascists, reactionaries and traditionalists.\n\nA number of significant political movements—including feminism and regionalism—do not fit precisely into the left-right spectrum. Though nationalism is often regarded as a right-wing doctrine, many nationalists favor egalitarian distributions of resources. There are also \"liberal nationalists\". Populism is regarded as having both left-wing and right-wing manifestations (see left-wing populism and right-wing populism). Green politics is often regarded as a movement of the left, but in some ways the green movement is difficult to definitively categorize as left or right.\n\nPolitical scientists have observed that the ideologies of political parties can be mapped along a single left–right axis. Klaus von Beyme categorized European parties into nine families, which described most parties. Beyme was able to arrange seven of them from left to right: communist, socialist, green, liberal, Christian democratic, conservative and right-wing extremist. The position of agrarian and regional/ethnic parties varied. A study conducted in the late 1980s on two bases, positions on ownership of the means of production and positions on social issues, confirmed this arrangement.\n\nThere has been a tendency for party ideologies to persist and values and views that were present at a party's founding have survived. However, they have also adapted for pragmatic reasons, making them appear more similar. Seymour Martin Lipset and Stein Rokkan observed that modern party systems are the product of social conflicts played out in the last few centuries. They said that lines of cleavage had become \"frozen\".\n\nThe first modern political parties were liberals, organized by the middle class in the 19th century to protect them against the aristocracy. They were major political parties in that century, but declined in the twentieth century as first the working class came to support socialist parties and economic and social change eroded their middle class base. Conservative parties arose in opposition to liberals in order to defend aristocratic privilege, but in order to attract voters they became less doctrinaire than liberals. However, they were unsuccessful in most countries and generally have been able to achieve power only through cooperation with other parties.\n\nSocialist parties were organized in order to achieve political rights for workers and were originally allied with liberals. However, they broke with the liberals when they sought worker control of the means of production. Christian democratic parties were organized by Catholics who saw liberalism as a threat to traditional values. Although established in the 19th century, they became a major political force following the Second World War. Communist parties emerged following a division within socialism first on support of the First World War and then support of the Bolshevik Revolution.\n\nRight-wing extremist parties are harder to define other than being more right-wing than other parties, but include fascists and some extreme conservative and nationalist parties.\n\nGreen parties were the most recent of the major party groups to develop. They have mostly rejected socialism and are very liberal on social issues.\n\nThese categories can be applied to many parties outside Europe. Ware (1996) asserted that even though there are left–right policy differences between them.\n\nIn the 2001 book \"The Government and Politics of France\", Andrew Knapp and Vincent Wright say that the main factor dividing the left and right wings in Western Europe is class. The left seeks social justice through redistributive social and economic policies, while the Right defends private property and capitalism. The nature of the conflict depends on existing social and political cleavages and on the level of economic development. Left-wing values include the belief in the power of human reason to achieve progress for the benefit of the human race, secularism, sovereignty exercised through the legislature, social justice and mistrust of strong personal political leadership. To the right, this is regularly seen as anti-clericalism, unrealistic social reform, doctrinaire socialism and class hatred. The Right are skeptical about the capacity for radical reforms to achieve human well-being while maintaining workplace competition. They believe in the established church both in itself and as an instrument of social cohesion; and believe in the need for strong political leadership to minimize social and political divisions. To the Left, this is seen as a selfish and reactionary opposition to social justice, a wish to impose doctrinaire religion on the population and a tendency to authoritarianism and repression.\n\nThe differences between left and right have altered over time. The initial cleavage at the time of the French Revolution was between supporters of absolute monarchy (the Right) and those who wished to limit the king's authority (the Left). During the 19th century, the cleavage was between monarchists and republicans. Following the establishment of the Third Republic in 1871, the cleavage was between supporters of a strong executive on the Right and supporters of the primacy of the legislature on the Left.\n\nThe terms left-wing and right-wing are widely used in the United States, but as on the global level there is no firm consensus about their meaning. The only aspect that is generally agreed upon is that they are the defining opposites of the United States political spectrum. Left and right in the U.S. are generally associated with liberal and conservative respectively, although the meanings of the two sets of terms do not entirely coincide. Depending on the political affiliation of the individual using them, these terms can be spoken with varying implications. A 2005 poll of 2,209 American adults showed that \"respondents generally viewed the paired concepts liberals and left-wingers and conservatives and right-wingers as possessing, respectively, generally similar political beliefs\", but also showed that around ten percent fewer respondents understood the terms left and right than understood the terms liberal and conservative.\n\nThe contemporary left in the United States is usually understood as a category that includes New Deal social-liberals (in contrast to traditions of social democracy more common to Western Europe), Rawlsian liberals and civil libertarians, who are often identified with the Democratic Party. There are also leftists who reject many of the platforms of the Democratic Party in favor of more socialist policies. In general, the term left-wing is understood to imply a commitment to egalitarianism, support for social policies that appeal to the working class and multiculturalism. The contemporary center-left usually defines itself as promoting government regulation of business, commerce and industry; protection of fundamental rights such as freedom of speech and freedom of religion (separation of church and state); and government intervention on behalf of racial, ethnic and sexual minorities and the working class.\n\nSome political scientists have suggested that the classifications of \"left\" and \"right\" are no longer meaningful in the modern complex world. Although these terms continue to be used, they advocate a more complex spectrum that attempts to combine political, economic and social dimensions.\n\nA survey conducted between 1983 and 1994 by Bob Altemeyer of Canadian legislative caucuses showed an 82% correlation between party affiliation and score on a scale for right-wing authoritarianism when comparing right-wing and social democratic caucuses. There was a wide gap between the scores of the two groups, which was filled by liberal caucuses. His survey of American legislative caucuses showed scores by American Republicans and Democrats were similar to the Canadian Right and liberals, with a 44% correlation between party affiliation and score.\n\nNorberto Bobbio saw the polarization of the Italian Chamber of Deputies in the 1990s as evidence that the linear left–right axis remained valid. Bobbio thought that the argument that the spectrum had disappeared occurred when either the Left or Right were weak. The dominant side would claim that its ideology was the only possible one, while the weaker side would minimize its differences. He saw the Left and Right not in absolute terms, but as relative concepts that would vary over time. In his view, the left–right axis could be applied to any time period.\n\nLibertarian writer David Boaz argued that terms left and right are used to spin a particular point of view rather than as simple descriptors, with those on the \"left\" typically emphasizing their support for working people and accusing the right of supporting the interests of the upper class; and those on the \"right\" usually emphasizing their support for individualism and accusing the Left of supporting collectivism. Boaz asserts that arguments about the way the words should be used often displaces arguments about policy by raising emotional prejudice against a preconceived notion of what the terms mean.\n\nIn 2006, British Prime Minister Tony Blair described the main cleavage in politics as not left versus right, but open versus closed. In this model, attitudes towards social issues and globalism are more important than the conventional economic left–right issues: \"open\" voters tend to be socially liberal, multicultural and in favour of globalism, while \"closed\" voters are culturally conservative, opposed to immigration and in favour of protectionism. This model has seen increased support following the rise of populist and centrist parties in the 2010s.\n\n\n"}
{"id": "3946908", "url": "https://en.wikipedia.org/wiki?curid=3946908", "title": "Legal maxim", "text": "Legal maxim\n\nA legal maxim is an established principle or proposition of law in Western civilization, and a species of aphorism and general maxim. The word is apparently a variant of the Latin \"maxima\", but this latter word is not found in extant texts of Roman law with any denotation exactly analogous to that of a legal maxim in the Medieval or modern definition, but the treatises of many of the Roman jurists on \"regular definitiones\" and \"sententiae iuris\" are to some degree collections of maxims. Most of the Latin maxims originate from the Medieval era in European states that used Latin as their legal language.\n\nThe attitude of early English commentators towards the maximal of the law was one of unmingled adulation. In Thomas Hobbes, \"Doctor and Student\" (p. 26), they are described as of the same strength and effect in the law as statutes. Not only, observes Francis Bacon in the Preface to his Collection of Maxims, will the use of maxims be in deciding doubt and helping soundness of judgment, but, further, in gracing argument, in correcting unprofitable subtlety, and reducing the same to a more sound and substantial sense of law, in reclaiming vulgar errors, and, generally, in the amendment in some measure of the very nature and complexion of the whole law. \n\nA similar note was sounded in Scotland; and it has been well observed that a glance at the pages of Morrisons Dictionary or at other early reports will show how frequently in the older Scots law questions respecting the rights, remedies and liabilities of individuals were determined by an immediate reference to legal maxims.\n\nIn later times, less value has been attached to the maxims of the law, as the development of civilization and the increasing complexity of business relations have shown the necessity of qualifying the propositions which they enunciate. But both historically and practically, they must always possess interest and value.\n\n\n\n\n"}
{"id": "50377400", "url": "https://en.wikipedia.org/wiki?curid=50377400", "title": "Meschac Gaba", "text": "Meschac Gaba\n\nMeschac Gaba (born 1961) is a Beninese conceptual artist based in Rotterdam and Cotonou. His installations of everyday objects whimsically juxtapose African and Western cultural identities and commerce. He is best known for \"The Museum of Contemporary African Art 1997–2002\", an autobiographical 12-room installation acquired and displayed by the Tate Modern in 2013. He has also exhibited at the Studio Museum in Harlem and at the 2003 Venice Biennale.\n\nMeschac Gaba was born in Cotonou, Benin, in 1961. He had drifted from his training as a painter until a bag of decommissioned money cut into confetti led him to make paintings with the material. Gaba became known for his installations of everyday objects that whimsically juxtapose African and Western cultural identities and commerce.\n\nHe held a residency at the Amsterdam Rijksakademie in 1996 for two years. In the absence of opportunities to display his work in the city, he set out over the next five years to make his own museum. This piece became his seminal \"The Museum of Contemporary African Art 1997–2002\", which consists of 12 rooms (some based on museum function and others personal) filled with objects made by Gaba. Throughout the exhibition ran a vein of confessional narrative about the artist's art travails between Africa and Europe. The wedding room, which he made while in love, holds mementos as museum artifacts from Gaba's wedding to the Dutch curator Alexandra van Dongen in 2000 at the Amsterdam Stedelijk Museum. The Library room holds art books and tells of Gaba's childhood. The games room showed sliding puzzle tables that form African national flags. It had its own gift shop and café. The exhibited \"Museum\" had couches for reading, a piano for playing, and featured objects reflecting Africa's polycultural character, including Ghanaian money featuring the face of Picasso, a Swiss bank mimicking an African street market, and gilded ceramic chicken legs.\n\nThe \"Museum\" exhibited widely. The work was first displayed in part in 2002 at Documenta 11. Gaba received a Rotterdam space in which he could live and store the work. When his son requested a more normal house, Gaba sold and gifted most of the work to the Tate Modern, save for his Library, which Gaba returned to his hometown. Around 2013, Gaba lived half the year in his hometown of Cotonou and the other half in Rotterdam with his wife and son. The Tate Modern displayed the work as a whole in 2013 as part of the Tate's two-year program of African-focused exhibitions. The wedding room enchanted the British art critic Jonathan Jones, who described the \"Museum\" as autobiographical, novelistic, protest showing \"the strength of modern African art\". For instance, the Art and Religion room showed \"classic\" African ceremonial sculpture alongside kitschy Buddhist and Christian objects, as if to group the types together as poor representations of their respective cultures. Gaba saw the work as correcting lacks of art education in Africa and African art representation outside the continent.\n\nIn-between finishing the \"Museum\" and its Tate exhibition, Gaba presented at the 2003 Venice Biennale and held his first solo show in the United States at the Studio Museum in Harlem, \"Tresses\", a series of architectural models of New York City and Benin landmarks made from artificial braided hair extensions. The accessory, popularized by African-American pop stars based on West African culture, was repatriated to Africa. Gaba worked with a Beninese hair braider to make the sculptures from his photographs. \"The New York Times\" wrote that the works were \"delightful\" and recognizable without becoming caricatures.\n\nGaba held his first solo gallery show, \"Exchange Market\", in New York in 2014. On the ground floor, 10 sculptures of unvarnished wood tables each with a wire umbrella stand, from which African banknotes hung. Each table was associated with a type of commodity: cotton, cocoa, diamonds. Along the walls hung bank-shaped works made of wood, plexiglass, and decommissioned money. Upstairs, reminiscent of the games room of Gaba's museum, were four foosball tables and small souvenir sculptures such as hand-painted cricket bats and a miniature billiards table.\n\n\"Artsy\" selected Gaba's work as a highlight of the 2014 London art fair.\n\n"}
{"id": "4165078", "url": "https://en.wikipedia.org/wiki?curid=4165078", "title": "National Library of Medicine classification", "text": "National Library of Medicine classification\n\nThe National Library of Medicine (NLM) classification system is a library indexing system covering the fields of medicine and preclinical basic sciences. The NLM classification is patterned after the Library of Congress (LC) Classification system: alphabetical letters denote broad subject categories which are subdivided by numbers. For example, \"QW 279\" would indicate a book on an aspect of microbiology or immunology.\n\nThe one- or two-letter alphabetical codes in the NLM classification use a limited range of letters: only QS–QZ and W–WZ. This allows the NLM system to co-exist with the larger LC coding scheme as neither of these ranges are used in the LC system. There are, however, three pre-existing codes in the LC system which overlap with the NLM: \"Human Anatomy\" (QM), \"Microbiology\" (QR), and \"Medicine\" (R). To avoid further confusion, these three codes are not used in the NLM.\n\nThe headings for the individual \"schedules\" (letters or letter pairs) are given in brief form (e.g., QW - \"Microbiology and Immunology\"; WG - \"Cardiovascular System\") and together they provide an outline of the subjects covered by the NLM classification. Headings are interpreted broadly and include the physiological system, the specialties connected with them, the regions of the body chiefly concerned and subordinate related fields. The NLM system is hierarchical, and within each schedule, division by organ usually has priority. Each main schedule, as well as some sub-sections, begins with a group of form numbers ranging generally from 1–49 which classify materials by publication type, e.g., dictionaries, atlases, laboratory manuals, etc.\n\nThe main schedules QS-QZ, W-WY, and WZ (excluding the range WZ 220–270) classify works published after 1913; the 19th century schedule is used for works published 1801-1913; and WZ 220-270 is used to provide century groupings for works published before 1801.\n\nPreclinical Sciences\n\nMedicine and Related Subjects\n\n\n"}
{"id": "54709700", "url": "https://en.wikipedia.org/wiki?curid=54709700", "title": "Oil purification", "text": "Oil purification\n\nOil purification (transformer, turbine, industrial, etc.) removes oil contaminants in order to prolong oil service life.\nContaminants and various impurities get into industrial oils during storage and operation. The most common contaminants are :\nIndustrial oils are purified through sedimentation, filtration, centrifugation, vacuum treatment and adsorption purification .\n\nSedimentation is precipitation of solid particles and water to the bottom of oil tanks under gravity. The main drawback of this process is its longevity .\n\nFiltration is a partial removal of solid particles through filter medium. Oil filtration systems generally use a multistage filtration with coarse and fine filters .\n\nCentrifugation is separation of oil and water, or oil and solid particles by centrifugal forces.\n\nVacuum treatment degasses and dehydrates industrial oil. This method is well suited for removing dispersed and dissolved water, as well as dissolved gases .\n\nAdsorption purification, in contrast to the methods mentioned above, does not remove solid particles and gases, but it shows good results at removing water, oil sludge and aging products. This process uses adsorbents of natural or artificial origin: bleaching clays, synthetic aluminosilicates, silica gels, zeolites, etc .\n\nOften the terms \"oil purification\" and \"oil regeneration\" are used synonymously. Although in fact they are not the same. Oil purification cleans oil from contaminants. it can be used independently or as a part of oil regeneration. Oil regeneration also removes aging products (with the help of adsorbents) and stabilizes oil with additives. Regenerated oil is clean from carcinogenic products of oil aging and stabilized with the help of additives.\n"}
{"id": "26952333", "url": "https://en.wikipedia.org/wiki?curid=26952333", "title": "Paritarian Institutions", "text": "Paritarian Institutions\n\nParitarian (from the French “paritaire”; ”paritair” in Dutch, “paritätische“ in German, “Paritetico” in Italian) means jointly managed on an equal basis (parity basis).\n\nIn the field of social protection, paritarian institutions are non-profit institutions which are jointly managed by the social partners (representatives of the employers and employees). In other words, the governance of these institutions is based on the equal representation of employees (normally the trade unions) and employers in their governing bodies.\n\nThe social protection funds managed by the paritarian institutions are set up through collective agreements at the company, the industry-wide (such as construction sector, metal sector, etc.) or the inter-sectoral level, and they can provide several social benefits such as pension (in particular occupational pension funds), health care, unemployment, disability, paid holidays and other such benefits.\n\nWithin the paritarian model there are two phases: in the negotiation phase, when a collective agreement between the trade unions and the employers’ representatives set up the social fund; and in the management phase, the signatory parties decide to manage their negotiated social funds themselves by establishing a Paritarian Institution in which they are equally represented.\n\nParitarian Institutions of Social Protection are widespread in Europe, especially in Western Europe and Scandinavia. The combined funds currently managed by the Paritarian Institutions of Social Protection total to a rough estimate of 1.3 trillion euro in assets and cover about 80 million European citizens.\n\nIn 1996, a European Organization the European Association of Paritarian Institutions of Social Protection (AEIP) was created in order to represent the Paritarian Institutions to the European Union. AEIP underlines the specific peculiarities of the Paritarian Institutions compared to other similar actors like private insurance companies or mutual organizations.\n\nEven though of European origins, paritarian institutions also exist in other parts of the world like in North America, South America, India and Japan.\n\n"}
{"id": "1914845", "url": "https://en.wikipedia.org/wiki?curid=1914845", "title": "Parosmia", "text": "Parosmia\n\nParosmia (from the Greek παρά \"pará\" and ὀσμή \"osmḗ\"), also known as troposmia (Gk.), is an olfactory dysfunction that is characterized by the inability of the brain to properly identify an odor's \"natural\" smell.\nWhat happens instead, is that the natural odor is transcribed into what is most often described as an unpleasant aroma, typically a \"'burned,' 'rotting,' 'fecal,' or 'chemical' smell\".\nThere are rare instances, however, of pleasant odors; this is more specifically called euosmia (Greek).\n\nOne method used to establish parosmia is the University of Pennsylvania Smell Identification Test, or UPSIT. \"Sniffin' Sticks\" are another method that can be used to properly diagnose parosmia. These different techniques can also help deduce whether a specific case of parosmia can be attributed to just one stimulating odor or if there is a group of stimulating odors that will generate the displaced smell. One case study performed by Frasnelli \"et al.\" offers a situation where certain smells, specifically coffees, cigarettes, onions, and perfumes, induced a \"nauseating\" odor for the patient, one which was artificial but unable to be aptly related to another known smell. In another case study cited in the same paper, one woman had parosmia in one nostril but not the other. Medical examinations and MRIs did not reveal any abnormalities; however the parosmia in this case was degenerative and only got worse with time. The authors do comment, however, that cases of parosmia can predict regeneration of olfactory senses.\n\nThere are numerous diseases that parosmia is associated with. In the case study cited above, Frasnelli \"et al.\" examined five patients that endured parosmia or phantosmia, most as a result of upper respiratory tract infections (URTIs). It is hypothesized that URTIs can result in parosmia because of damage to olfactory receptor neurons (ORNs).\nExposure to harmful solvents has also been linked to parosmia and more specifically damaging ORNs.\nDamage to these neurons could end in the inability to correctly encode a signal representing a particular odor, which would send an erroneous signal to the odor processing center, the olfactory bulb. This, in turn, leads to the signal activating a different trigger, i.e. a different smell, than the stimulating odor, and thus the patient cannot sync the input and output odors. Damage to ORNs describes a peripheral defect in the pathway, but there are also instances where damage to the processing center in the brain can lead to distorted odors as well.\n\nDifferent types of head traumas could obviously lead to dysfunctions that relate to what the afflicted brain area controls. In humans, the olfactory bulb is located on the inferior side of the brain. Physical damage to this area would alter how the area processes information in a variety of ways, but there are also other types of diseases that can alter how this area works. If the part of the brain that interprets these input signals is damaged, then a distorted output is possible. This would also lead to parosmia. Temporal lobe epilepsy has also led to cases of parosmia, but these were only temporary; the onset of parosmia was a seizure and it typically lasted a week or two after.\nParosmia is also a known symptom for Parkinson's disease, though not ubiquitous for patients with it, and although the specific pathway is undetermined, the lack of dopamine has resulted in documented cases of parosmia and phantosmia.\n\nFor most patients afflicted with parosmia, symptoms usually decrease with time. Although there are instances of parosmia affecting patients for years at a time, this is certainly not the majority of cases. There have been experiments done to treat parosmia with L-Dopa, but besides that there are no current treatments other than inducing anosmia or hyposmia to the point where the odors are negligible.\n\n\n"}
{"id": "1329397", "url": "https://en.wikipedia.org/wiki?curid=1329397", "title": "Passion (emotion)", "text": "Passion (emotion)\n\nPassion (Greek \"πασχω\" \"to suffer, to be acted on\" and Late Latin (chiefly Christian) \"passio\" \"passion; suffering\" (from Latin \"pati\" \"to suffer\")) is a feeling of intense enthusiasm towards or compelling desire for someone or something. Passion can range from eager interest in or admiration for an idea, proposal, or cause; to enthusiastic enjoyment of an interest or activity; to strong attraction, excitement, or emotion towards a person. It is particularly used in the context of romance or sexual desire, though it generally implies a deeper or more encompassing emotion than that implied by the term \"lust\".\n\nDenis Diderot describes passions as \"penchants, inclinations, desires and aversions carried to a certain degree of intensity, combined with an indistinct sensation of pleasure or pain, occasioned or accompanied by some irregular movement of the blood and animal spirits, are what we call passions. They can be so strong as to inhibit all practice of personal freedom, a state in which the soul is in some sense rendered passive; whence the name passions. This inclination or so-called disposition of the soul, is born of the opinion we hold that a great good or a great evil is contained in an object which in and of itself arouses passion\".\n\nHe further breaks down pleasure and pain, which are the guiding principles of passion into four major categories:\n\n\nStrong Desire for something: \nIn whatever context, if someone desires for something and that desire has some strong feeling or emotion is defined in terms of passion. Passion has no boundary, being passionate about something which is boundless can be sometimes dangerous, In which person forget about everything and is fully determined towards the particular thing-(Sanyukta)\n\nIn his wake, Stoics like Epictetus emphasized that \"the most important and especially pressing field of study is that which has to do with the stronger emotions...sorrows, lamentations, envies...passions which make it impossible for us even to listen to reason\". The Stoic tradition still lay behind Hamlet's plea to \"Give me that man That is not passion's slave, and I will wear him In my heart's core\", or Erasmus's lament that \"Jupiter has bestowed far more passion than reason – you could calculate the ratio as 24 to one\". It was only with the Romantic movement that a valorisation of passion \"over\" reason took hold in the Western tradition: \"the more Passion there is, the better the Poetry\".\n\nThe recent concerns of emotional intelligence have been to find a synthesis of the two forces—something that \"turns the old understanding of the tension between reason and feeling on its head: it is not that we want to do away with emotion and put reason in its place, as Erasmus had it, but instead find the intelligent balance of the two\".\n\nAntonio Damasio studied what ensued when something \"severed ties between the lower centres of the emotional brain...and the thinking abilities of the neocortex\". He found that while \"emotions and feelings can cause havoc in the processes of reasoning...the \"absence\" of emotion and feeling is no less damaging\"; and was led to \"the counter-intuitive position that feelings are typically \"indispensable\" for rational decisions\". The passions, he concluded, \"have a say on how the rest of the brain and cognition go about their business. Their influence is immense...[providing] a frame of reference – as opposed to Descartes' error...the Cartesian idea of a disembodied mind\".\n\nA tension or dialectic between marriage and passion can be traced back in Western society at least as far as the Middle Ages, and the emergence of the cult of courtly love. Denis de Rougemont has argued that 'since its origins in the twelfth century, passionate love was constituted in opposition to marriage'. Stacey Oliker writes that while \"Puritanism prepared the ground for a marital love ideology by prescribing love in marriage\", only from the eighteenth century has \"romantic love ideology resolved the Puritan antagonism between passion and reason\" in a marital context. (Note though that Saint Paul spoke of loving one's wife in Ephesians 5.)\n\nGeorge Bernard Shaw \"insists that there are passions far more exciting than the physical ones...'intellectual passion, mathematical passion, passion for discovery and exploration: the mightiest of all passions'\". His contemporary, Sigmund Freud, argued for a continuity (not a contrast) between the two, physical and intellectual, and commended the way \"Leonardo had energetically sublimated his sexual passions into the passion for independent scientific research\".\n\nThere are different reasons individuals are motivated for an occupation. One of these includes passion for the occupation. When an individual is passionate about their occupation they tend to be less obsessive about their behavior while on their job, resulting in more work being done and more work satisfaction. These same individuals have higher levels of psychological well-being. When people genuinely enjoy their profession and are motivated by their passion, they tend to be more satisfied with their work and more psychologically healthy. When individuals are unsatisfied with their profession they tend to also be dissatisfied with their family relationships and experience psychological distress. Other reasons people are more satisfied when they are motivated by their passion for their occupation include the effects of intrinsic and external motivations. When an individual is doing the job to satisfy others, they tend to have lower levels of satisfaction and psychological health. Also, these same individuals have shown they are motivated by several beliefs and fears concerning other people. Thirdly, though some individuals believe one should not work extreme hours, many prefer it because of how passionate they are about the occupation. On the other hand, this may also put a strain on family relationships and friendships. The balance of the two is something that is hard to achieve and it is always hard to satisfy both parties.\n\nThere are different components that qualify as reasons for considering an individual as a workaholic. Burke & Fiksenbaum refer to Spence and Robbins (1992) by stating two of the three workaholism components that are used to measure workaholism. These include feeling driven to work because of inner pressure and work enjoyment. Both of these affect an individual differently and each has different outcomes. To begin, work enjoyment brings about more positive work outcomes and is unrelated to health indicators. Inner pressure, on the other hand, is negatively related with work outcomes and has been related negatively to measures of psychological health. Burke & Fiksenbaum make a reference to Graves et al. (2006) when examining work enjoyment and inner pressures. Work enjoyment and inner pressure were tested with performance ratings. The former was positively related to performance ratings while the latter interfered with the performance-enhancing aspects of work enjoyment. Burke & Fiksenbaum refer to Virick and Baruch (2007) when explaining how these two workaholism components affect life satisfaction. Not surprisingly, inner pressure lowered the balance between work-life and life satisfaction but enhanced people's performance at their occupation, whereas work enjoyment led to a positive balance between the two. Again, when individuals are passionate about their occupation and put in many hours, they then become concerned that their occupation will satisfy personal relationships and the balance must then be found according to the importance levels of the individual.\n\nThe researchers indicate different patterns of correlations between these two components. These patterns include antecedents and consequences. The two components offer unique motivations or orientations to work which result in its effects on work and well-being. Inner pressures will hinder performance while work enjoyment will smooth performance. Inner pressures of workaholism have characteristics such as persistence, rigidity, perfectionism, and heightened levels of job stress. This component is also associated with working harder, not smarter. On a more positive note, individuals who enjoy their work will have higher levels of performance for several reasons. These include creativity, trust in their colleagues, and reducing levels of stress.\n\nBurke and Fiksenbaum refer to Schaufeli, Taris, and Bakker (2007) when they made a distinction between an individual being a good workaholic and him being a bad workaholic. A good workaholic will score higher on measures of work engagement and a bad workaholic will score higher on measures of burnout. They also suggest why this is – some individuals work because they are satisfied, engaged, and challenged and to prove a point. On the other hand, the opposite kind work hard because they are addicted to work; they see that the occupation makes a contribution to finding an identity and purpose.\n\nPassion and desire go hand in hand, especially as a motivation. Linstead & Brewis refer to Merriam-Webster to say that passion is an \"intense, driving, or overmastering feeling or conviction\". This suggests that passion is a very intense emotion, but can be positive or negative. Negatively, it may be unpleasant at times. It could involve pain and has obsessive forms that can destroy the self and even others. In an occupation, when an individual is very passionate about their job, they may be so wrapped up in work that they cause pain to their loved ones by focusing more on their job than on their friendships and relationships. This is a constant battle of balance that is difficult to achieve and only an individual can decide where that line lies. Passion is connected to the concept of desire. In fact, they are inseparable, according to a mostly western way of thinking related to Plato, Aristotle, and Augustine. These two concepts cause individuals to reach out for something, or even someone. They both can either be creative or destructive and this dark side can very well be dangerous to the self or others.\n\nHobbies require a certain level of passion in order to continue engaging in the hobby. Singers, athletes, dancers, artists, and many others describe their emotion for their hobby as a passion. Although this might be the emotion they're feeling, passion is serving as a motivation for them to continue their hobby. Recently there has been a model to explain different types of passion that contribute to engaging in an activity.\n\nAccording to researchers who have tested this model, \"A dualistic model in which passion is defined as a strong inclination or desire toward a self-defining activity that one likes (or even loves), that one finds important (high valuation), and in which one invests time and energy.\" It is proposed that there exist two types of passion. The first type of passion is harmonious passion.\n\n\"A harmonious passion refers to a strong desire to engage in the activity that remains under the person's control.\" This is mostly obtained when the person views their activity as part of their identity. Furthermore, once an activity is part of the person's identity then the motivation to continue the specific hobby is even stronger. The harmony obtained with this passion is conceived when the person is able both to freely engage in or to stop the hobby. It's not so much that the person is forced to continue this hobby, but on his or her own free will is able to engage in it. For example, if a girl loves to play volleyball, but she has a project due the next day and her friends invite her to play, she should be able to say \"no\" on the basis of her own free will.\n\nThe second kind of passion in the dualistic model is \"obsessive passion\". Being the opposite of \"harmonious passion\". This type has a strong desire to engage in the activity, but it's not under the person's own control and he or she is forced to engage in the hobby. This type of passion has a negative effect on a person where they could feel they need to engage in their hobby to continue, for example, interpersonal relationships, or \"fit in\" with the crowd. To change the above example, if the girl has an obsessive passion towards volleyball and she is asked to play with her friends, she will likely say \"yes\" even though she needs to finish her project for the next day.\n\nSince passion can be a type of motivation in hobbies then assessing intrinsic motivation is appropriate. Intrinsic motivation helps define these types of passion. Passion naturally helps the needs or desires that motivate a person to some particular action or behavior. Certain abilities and hobbies can be developed early and the innate motivation is also something that comes early in life. Although someone might know how to engage in a hobby, this doesn't necessarily mean they are motivated to do it. Christine Robinson makes the point in her article that, \" ...knowledge of your innate motivation can help guide action toward what will be fulfilling.\" Feeling satisfied and fulfilled builds the passion for the hobby to continue a person's happiness.\n\nIn Margaret Drabble's \"The Realms of Gold\", the hero flies hundreds of miles to reunite with the heroine, only to miss her by 24 hours – leaving the onlookers \"wondering what grand passion could have brought him so far...a quixotic look about him, a look of harassed desperation\". When the couple \"do\" finally reunite, however, the heroine is less than impressed. \"'If you ask me, it was a very \"childish\" gesture. You're not twenty-one now, you know'. 'No, I know. It was my last fling'\".\n\nIn Alberto Moravia's \"1934\", the revolutionary double-agent, faced with the girl he is betraying, \"was seized by violent desire...he never took his eyes off my bosom...I believe those two dark spots at the end of my breasts were enough to make him forget tsarism, revolution, political faith, ideology, and betrayal\".\n\n\n\n"}
{"id": "155710", "url": "https://en.wikipedia.org/wiki?curid=155710", "title": "Peer pressure", "text": "Peer pressure\n\nPeer pressure (or social pressure) is the direct influence on people by peers, or the effect on an individual who gets encouraged to follow their peers by changing their attitudes, values or behaviors to conform to those of the influencing group or individual. This can result in either a positive or negative effect. Social groups affected include both \"membership groups\", in which individuals are \"formally\" members (such as political parties and trade unions), and cliques, in which membership is not clearly defined. However, a person does not need to be a member or be seeking membership of a group to be affected by peer pressure.peer pressure can decrease one's confidence .It can affect the lives of the students drastically.\nThere has been considerable study regarding peer pressure's effects on children and adolescents, and in popular discourse the term is mostly used in the contexts of those age groups. For children, the common themes for study regard their abilities for independent decision making; for adolescents, peer pressure's relationship with sexual intercourse and substance abuse have been significantly researched. Peer pressure can affect individuals of all ethnicities, genders and ages, however. Peer pressure has moved from strictly face-to-face interaction to digital interaction as well. Social media offers opportunities for adolescents and adults alike to instill and/or experience pressure everyday. Research suggests that not just individuals but also organizations, such as large corporations, are susceptible to peer pressures, such as pressures from other firms in their industry or headquarters city.\n\nImitation plays a large role in children's lives; in order to pick up skills and techniques that they use in their own life, children are always searching for behaviours and attitudes around them that they can co-opt. Children are aware of their position in the social hierarchy from a young age: their instinct is to defer to adults' judgements and majority opinions. Similar to the Asch conformity experiments, a study done on groups of preschool children showed that they were influenced by groups of their peers to change their opinion to a demonstrably wrong one. Each child was handed a book with two sets of images on each page, with a groups of differently sized animals on the left hand page and one animal on the right hand, and each child was asked to indicate the size of the lone animal. All the books appeared the same, but the last child would sometimes get a book that was different. The children reported their size judgements in turn, and the child being tested was asked last. Before him or her, however, were a group of children working in conjunction with the researchers. Sometimes, the children who answered before the test subject all gave an answer that was incorrect. When asked in the presence of the other children, the last child's response was often the same as his or her peers. However, when allowed to privately share their responses with a researcher the children proved much more resistant to their peers' pressure, illustrating the importance of the physical presence of their peers in shaping their opinions.\n\nAn insight is that children can monitor and intervene in their peers' behavior through pressure. A study conducted in a remedial kindergarten class in the Edna A. Hill Child Development Laboratory in the University of Kansas designed a program to measure how children could ease disruptive behavior in their peers through a two-part system. After describing a series of tasks to their classroom that included bathroom usage, cleaning up, and general classroom behavior, teachers and researchers would observe children's performance on the tasks. The study focused on three children who were clearly identified as being more disruptive than their peers, and looked at their responses to potential techniques. The system utilized was a two-part one: first, each student would be given points by their teachers for correctly completing tasks with little disruption (e.g. sitting down on a mat for reading time), and if a student reached three points by the end of the day they would receive a prize. The second part brought in peer interaction, where students who reached three points were appointed \"peer monitors\" whose role was to lead their small groups and assign points at the end of the day. The results were clear-cut, showing that the monitored students' disruption dropped when teachers started the points system and monitored them, but when peer monitors were introduced the target students' disruption dropped to average rates of 1% for student C1, 8% for student C2, and 11% for student C3 (down from 36%, 62%, and 59%, respectively). Even small children, then, are susceptible to pressure from their peers, and that pressure can be used to effect positive change in academic and social environments.\n\nAdolescence is the/a time when a person is most susceptible to peer pressure because peers become an important influence on behavior during adolescence, and peer pressure has been called a hallmark of adolescent experience. Children entering this period in life become aware for the first time of the other people around them and realize the importance of perception in their interactions. Peer conformity in young people is most pronounced with respect to style, taste, appearance, ideology, and values. Peer pressure is commonly associated with episodes of adolescent risk taking because these activities commonly occur in the company of peers. Affiliation with friends who engage in risk behaviors has been shown to be a strong predictor of an adolescent's own behavior. Peer pressure can also have positive effects when youth are pressured by their peers toward positive behavior, such as volunteering for charity or excelling in academics. The importance of peers declines upon entering adulthood.\n\nEven though socially accepted children often have the most opportunities and the most positive experiences, research shows that social acceptance (being in the popular crowd) may increase the likelihood of engaging in risky behavior, depending on the norms in the group. Groups of popular children showed a propensity to increase risky, drug-related and delinquent behavior when this behavior was likely to receive approval in their groups. Peer pressure was greatest among more popular children because they were the children most attuned to the judgments of their peers, making them more susceptible to group pressures. Gender also has a clear effect on the amount of peer pressure an adolescent experiences: girls report significantly higher pressures to conform to their groups in the form of clothing choices or speech patterns. Additionally, girls and boys reported facing differing amounts of pressures in different areas of their lives, perhaps reflecting a different set of values and priorities for each gender.\nPeer pressure is widely recognized as a major contributor to the initiation of drug use, particularly in adolescence. This has been shown for a variety of substances, including nicotine and alcohol. While this link is well established, moderating factors do exist. For example, parental monitoring is negatively associated with substance use; yet when there is little monitoring, adolescents are more likely to succumb to peer coercion during initiation to substance use, but not during the transition from experimental to regular use. Caldwell and colleagues extended this work by finding that peer pressure was a factor leading to heightened risk in the context of social gatherings with little parental monitoring, and if the individual reported themselves as vulnerable to peer pressure. Conversely, some research has observed that peer pressure can be a protective factor against substance use.\n\nPeer pressure produces a wide array of negative outcomes. Allen and colleagues showed that susceptibility to peer pressure in 13- and 14-year-olds was predictive of not only future response to peer pressure, but also a wider array of functioning. For example, greater depression symptomatology, decreasing popularity, more sexual behavior, and externalizing behavior were greater for more susceptible teens. Of note, substance use was also predicted by peer pressure susceptibility such that greater susceptibility was predictive of greater alcohol and drug use.\n\nSubstance use is likely not attributed to peer pressure alone. Evidence of genetic predispositions for substance use exists and some have begun to examine gene x environment interactions for peer influence. In a nationally representative sample, adolescents who had genetic predisposition were more likely to have good friends who were heavy substance users and were furthermore, more likely to be vulnerable to the adverse influence of these friends. Results from specific candidate gene studies have been mixed. For instance, in a study of nicotine use Johnson and colleagues found that peer smoking had a lower effect on nicotine dependence for those with the high risk allele (CHRNA5). This suggests that social contexts do not play the significant role in substance use initiation and maintenance as it may for others and that interventions for these individuals should be developed with genetics in mind as well.\n\nThough the impact of peer influence in adolescence has been well established, it was unclear at what age this effect begins to diminish. It is accepted that such peer pressure to use alcohol or illicit substances is less likely to exist in elementary school and very young adolescents given the limited access and exposure. Using the Resistance to Peer Influence Scale, Sumter and colleagues found that resistance to peer pressure grew as age increased in a large study of 10- to 18-year-olds. This study also found that girls were generally more resistant to peer influence than boys, particularly at mid-adolescence (i.e. ages 13–15). The higher vulnerability to peer pressure for teenage boys makes sense given the higher rates of substance use in male teens. For girls, increased and positive parental behaviors (e.g. parental social support, consistent discipline) have been shown to be an important contributor to the ability to resist peer pressure to use substances.\n\nit is believed that peer pressure of excessive drinking in college comes down to three factors; being offered alcohol, modeling and social norms. Offering alcohol can be both as a kind gesture or the other extreme which is forceful. Then you have the modeling which is being a “copycat” and following your friends then finally you have the social norms which are drinking. There are two reasons why people do it; because everyone does it, or as a means to fit into social groups. on entering college most people begin to increase their amount of alcohol intake, this is more so true to those who do not live at home. This would be because you have shifted from being influenced by your parents to being influenced by your college peers. (Borsari and Carey, 2001)\n\nSubstance use prevention and intervention programs have utilized multiple techniques in order to combat the impact of peer pressure. One major technique is, naturally, peer influence resistance skills. The known correlational relationship between substance use and relationships with others that use makes resistance skills a natural treatment target. This type of training is meant to help individuals refuse participation with substance use while maintaining their membership in the peer group. Other interventions include normative education approaches (interventions designed to teach students about the true prevalence rates and acceptability of substance use), education interventions that raise awareness of potential dangers of substance use, alcohol awareness training and classroom behavior management. The literature regarding the efficacy of these approaches, however, is mixed. A study in Los Angeles and Orange Counties that established conservative norms and attempted to correct children's beliefs about substance abuse among their peers showed a statistically significant decrease in alcohol, tobacco, and marijuana use but other studies that systematically reviewed school-based attempts to prevent alcohol misuse in children found \"no easily discernible pattern\" in both successful and failed programs. A systematic review of intervention programs in schools conducted by Onrust et al. found that programs in elementary school were successful in slightly reducing a student's likelihood to abuse drugs or alcohol. However, this effect started to wear off with programs that targeted older students. Programs that targeted students in grades 8–9 reduced smoking, but not alcohol and other drug abuse, and programs that targeted older children reported no effect at all.\n\nIn a non-substance context, however, research has showed that decision-making training can produce concrete gains in risk perception and decision-making ability among autistic children. When administered the training in several short sessions that taught the children how to recognize risk from peers and react accordingly, the children showed through post-training assessments that they were able to identify potential threats and sources of pressure from peers and deflect them far better than normal adolescents in a control group.\n\nThere is evidence supporting the conclusion that parental attitudes disapproving sex tends to lead toward lower levels of adolescent unplanned pregnancy. These disparities are not due solely to parental disposition but also to communication.\n\nA study completed in Cape Town, South Africa, looked at students at four secondary schools in the region. They found a number of unhealthy practices derived from peer pressure: condoms are derided, threats of ridicule for abstinence, and engaging in sexual activity with multiple partners as part of a status symbol (especially for males). The students colloquially call others who choose abstinence as \"umqwayito\", which means dried fruit/meat. An important solution for these problems is communication with adults, which the study found to be extremely lacking within adolescent social groups.\n\nLiterature reviews in this field have attempted to analyze the norms present in the interactions and decision making behind these behaviors. A review conducted by Bongardt et al. defined three types of peer norms that led to a person's participation in sexual intercourse: descriptive norms, injunctive norms, and outright peer pressure. Descriptive norms and injunctive norms are both observed behaviors and are thus more indirect forms of pressure, but differ in one key aspect: descriptive norms describe peers' sexual behaviors, but injunctive norms describe peers' attitudes toward those behaviors (e.g. approval or disapproval). The last norm defined by the study is called \"peer pressure\" by the authors, and is used to describe direct encouragement or pressure by a person's peers to engage in sexual behavior.\n\nThe review found that indirect norms (descriptive and injunctive) had a stronger effect on a person's decision to engage in sexual behavior than direct peer pressure. Between the two indirect norms, descriptive norms had a stronger effect: people were likely to try what they thought their peers were engaging in rather than what they thought had approval in their peer group.\n\nAdditionally, studies have found a link between self-regulation and likeliness to engage in sexual behavior. The more trouble a subject had with self-regulation and self-control growing up, the more they were likely to fall prey to peer pressure that would lead them to engage in risky sexual acts. Based on these findings, it may be a good idea to prevent these through either a decision-making program or by targeting adolescents' ability to self-regulate against possible risks.\n\nFrom a purely neurological perspective, the medial prefrontal cortex (mPFC) and the striatum play an important role in figuring out the value of specific actions. The mPFC is active when determining \"socially tagged\" objects, which are objects that peers have expressed an opinion about; the striatum is significant for determining the value of these \"socially tagged\" objects and rewards in general. An experiment performed by Mason et al. utilizing fMRI scans analyzed individuals who were assigned to indicate if a chosen symbol appeared consecutively. The researchers did not tell the subjects the real purpose of the experiment, which was to collect data regarding mPFC and striatum stimulation. Before the actual experiment began, the subjects were subject to a phase of \"social\" influence, where they learned which symbols were preferred by other subjects of completed the experiment (while in actuality these other subjects did not exist). Mason et al. found that determining an object's social value/significance is dependent on combined information from the mPFC and the striatum [along the lines denoted in the beginning of the paragraph]. Without both present and functional, it would be difficult to determine the value of action based upon social circumstances.\n\nA similar experiment was conducted by Stallen, Smidts, and Sanfrey. Twenty-four subjects were manipulated using a minimal group paradigm approach. Unbeknownst to them, they were all selected as part of the \"in-group\", although there was an established \"out-group\". Following this socialization, the subjects estimated the number of dots seen on the screen while given information about what an in-group or out-group member chose. Participants were more likely to conform to in-group decisions as compared to out-group ones. The experiment confirmed the importance of the striatum in social influence, suggesting that conformity with the in-group is mediated with a fundamental value signal—rewards. In other words, the brain associates social inclusion with positive reward. The posterior superior temporal sulcus (pSTS), which is associated with perspective taking, appeared to be active as well, which correlated with patients' self-reports of in-group trustworthiness.\n\nIn adolescence, risk-taking appears to increase dramatically. Researchers conducted an experiment with adolescent males who were of driving age and measured their risk-taking depending on whether a passenger (a peer of the same age) was in the car. A driving simulation was created, and certain risky scenarios, such as a decaying yellow light as the car was approaching, were modeled and presented to the subjects. Those who were most likely to take risks in the presence of peers (but took fewer risks when there were no passengers) had greater brain activity in the social-cognitive and social-affective brain systems during solo activity (no passengers.) The social-cognitive aspect refers to the ability to gauge what others are thinking and is primarily controlled by the mPFC, right temporal parietal junction, and the posterior cingulate cortex. The social-affective aspect relates to the reward system for committing actions that are accepted or rejected by other people. One side of the reward system is \"social pain\", which refers to the emotional pain felt by individual due to group repudiation and is associated with heightened activity in the anterior insula and the subgenual anterior cingulate cortex.\n\nSocial media provides a massive new digital arena for peer pressure and influence. Research suggests there are a variety of benefits from social media use, such as increased socialization, exposure to ideas, and greater self-confidence. There is also evidence of negative influences such as advertising pressure, exposure to inappropriate behavior and/or dialogue, and fake news. These versions of digital peer pressure exist between youth, adults and businesses. In some cases, people can feel pressure to make themselves available 24/7 or to be perfect. Within this digital conversation there can be pressure to conform, especially as people are impacted by the frequency of times others hit the like button. The way others portray themselves on social media might lead to young people trying to mimic those qualities or actions in an attempt at conformity. It may also lead to a fear of missing out, which can pressure youth into irresponsible actions or decisions. Actions and influence on social media may lead to changes in identity, confidence, or habits in real life for children, adolescents, and adults.\n\nOver 3 billion social media users across the world are using a variety of platforms, in turn, the type, frequency, and scope of the resulting peer pressure fluctuates. Some research suggests social media has a greater influence on purchasing decisions for consumers in China than in other countries in the world. In addition, Chinese consumers say that they are more likely to consider buying a product if they see it discussed positively by friends on a social media site. Some countries have a very low usage rate of social media platforms, or have cultures that do not value it as highly. As a result, the power and impact of digital peer pressure may vary throughout the world. Overall, there is limited research on this topic and its global scope.\n\nThe Holocaust is probably the most well-known of genocides. In the 1940s, Nazi Germany, led by Adolf Hitler, began a systematic purge against the Jewish people living in Europe, killing around six million Jews by the end of World War II. It is clear that some Germans are culpable for the Holocaust; SS officers and soldiers clearly bought into the Jewish genocide and participated as executioners, jailers, and hunters (for hiding Jews). However, a broader statement is harder to make—as seen below, not all Germans wanted to kill the Jews. When bringing the concept of peer pressure into the Holocaust, German culpability is even harder to decide.\n\nThe primary issue revolves around collective responsibility and beliefs. As such, there are two positions, most notably held by Christopher Browning and David Goldhagen.\n\nChristopher Browning, most known for his book \"Ordinary Men: Reserve Police Battalion 101\", relies on an analysis of the men in Reserve Police Battalion 101. The men of the 101st were not ardent Nazis but ordinary middle-aged men of working-class background from Hamburg. They were drafted but found ineligible for regular military duty. Their test as an Order Police battalion first came in the form of Jozefow, a Jewish ghetto in Poland. The Battalion was ordered to round up the men in the ghetto and kill all women, children, and elderly on sight. During the executions, a few dozen men were granted release of their execution tasks and were reassigned to guard or truck duty. Others tried to stall as long as possible, trying not to be assigned to a firing squad. After the executions were completed, the men drank heavily, shaken by their ordeal.\n\nAt the end of his book, Browning supplies his theory on 101's actions: a combination of authoritative and peer pressure was a powerful coercive tool. First, the Nazi leadership wanted to keep the country's soldiers psychologically healthy, so soldiers were not forced to commit these murders. Throughout the German ranks, nothing negative happened to the soldiers and policemen who refused to join in on a firing squad or Jewish search party. They would simply be assigned other or additional duties, and perhaps subject to a little verbal abuse deriding their \"cowardice\". For the officers, no official sanction was given, but it was well known that being unable to carry out executions was the sign of a \"weak\" leader, and the officer would be passed for promotions. Second, Major Trapp, the head of Battalion 101, consistently offered protection from committing these actions, even so far as supporting one man who was blatantly and vocally against these practices. He established \"ground\" rules in which only volunteers were taking on 'Jewish Hunts\" and raids.\n\nBrowning relies on Milgram's experiments on authority to expand his point. Admitting that Trapp was not a particularly strong authority figure, Browning instead points to the Nazi leadership and the orders of the \"highest order\" that were handed down. Furthermore, according to Browning's analysis, one reason so few men separated themselves from their task was peer pressure—individual policemen did not want to \"lose face\" in front of their comrades. Some argued that it was better to shoot one and quit than to be a coward immediately. Some superior officers treated those who did not want to execute Jews with disdain; on the other hand, those selected for the executions or Jewish hunts were regarded as real \"men\" and were verbally praised accordingly. For some, refusing their tasks meant that their compatriots would need to carry the burden and the guilt of abandoning their comrades (as well as fear of ostracization) compelled them to kill.\n\nDavid Goldhagen, disagreeing with Browning's conclusion, decided to write his own book, \"Hitler's Willing Executioners\". Its release was highly controversial. He argues that the Germans were always anti-Semitic, engaging in a form of \"eliminationism\". Taking photos of the deceased, going on \"Jew-Hunts\", death marches near the end of the war, and a general focus on hate (rather than ignorance) are points Goldhagen utilizes in his book.\n\nHe does not believe that peer pressure or authoritative pressure can explain why ordinary Germans engaged in these actions. He believes that in order for the policemen in Battalion 101 (and those in similar situations) to kill, they must all be fully committed to the action—no half-heartedness. As he notes,\"For that matter, for someone to be pressured into doing something, by peer pressure, everyone else has to want to do it. Peer pressure can, of course, operate on isolated individuals, or small groups, but it depends upon the majority wanting to do it. So the peer pressure argument contradicts itself. If the majority of the people hadn't wanted to kill Jews, then there would have been peer pressure not to do it\" (37).Instead, he places a significant emphasis on the German people's anti-Semitism, to the extent of drawing ire from other historians. Browning notes Goldhagen's \"uniform portrayal\" of Germans, dehumanizing all of the perpetrators without looking at the whole picture. For example, in the town of Niezdow, the Police Battalion executed over a dozen elderly Poles in retaliation for the murder of a German policeman. It is less clear, then, if the Germans in the Police Battalion are antagonistic only towards Jews. The German-Canadian historian Ruth Bettina Birn has—in collaboration with Volker Rieß— checked Goldhagen's archival sources from Ludwigsburg. Their findings confirm the arbitrary nature of his selection and evaluation of existing records as opposed to a more holistic combination of primary sources. Furthermore, Konrad Kwiet, a Holocaust historian, argues that Goldhagen's narrow focus on German anti-Semitism has blinded him to other considerations. He points to the massacres of non-Jews as an example:\"[Goldhagen does not shine light] on the motives of “Hitler’s willing executioners” in murdering handicapped people within the so-called “Euthanasia Program”, in liquidating 2.7 million Soviet prisoners of war, in exterminating Romas or in killing hundreds of thousands of other people classified as enemies of the “German People and Nation”. The emphasis on German responsibility allows Goldhagen to push aside the willingness of genocidal killers of other nationalities [such as Latvians] who, recruited from the vast army of indigenous collaborators, were often commissioned with the task of carrying out the ‘dirty work’, such as the murder of women and children, and who, in many cases, surpassed their German masters in their cruelty and spontaneous brutality\" (27).\n\nThe Rwandan genocide occurred in 1994, with ethnic violence between the Hutu and Tutsi ethnicities. The primary belligerents were the Hutu; however, as with most ethnic conflict conflicts, not all Hutu wanted to kill Tutsi. A survivor named Mectilde described the Hutu breakdown as follows: 10% helped, 30% forced, 20% reluctant, and 40% willing. For the willing, a rewards structure was put in place. For the unwilling, a punishment system was in effect. The combination, Professor Bhavnani argues, is a behavioral norm enforced by in-group policing. Instead of the typical peer pressure associated with western high school students, the peer pressure within the Rwandan genocide, where Tutsi and Hutu have inter-married, worked under coercion. Property destruction, rape, incarceration, and death faced the Hutu who were unwilling to commit to the genocide or protected the Tutsi from violence.\n\nWhen looking at a sample community of 3426 in the village of Tare during the genocide, McDoom found that neighborhoods and familial structures as important micro-spaces that helped determine if an individual would participate in violence. Proximity increases the likelihood of social interaction and influence. For example, starting at a set point such as the home of a \"mobilizing\" agent for the Hutu (any individual who planned or led an attack in the village), the proportion of convicts living in a 100m radius of a resident is almost twice as many for convicts (individuals convicted of genocide by the \"gacaca\", a local institution of transitional justice that allows villagers to adjudicate on many of the perpetrators’ crimes by themselves) as for non-convicts. As the radius increases, so does the proportion decrease. This data hints that \"social influence\" was at play. Looking at neighborhoods, an individual is 4% more likely to join the genocide for every single percentage point increase in the proportion of convicted perpetrators living within a 100m radius of them. Looking at familial structures, for any individual, each percentage point increase in the proportion of genocide participants in the individual's household increased his chances of joining the violence by 21 to 25%.\n\nOf course, the complete situation is a little more nuanced. The extreme control of citizens' daily lives by the government in social affairs facilitated the rapidity of the genocide's spread and broke down the resolve of some who initially wanted to have no part in the genocide. First, prior to the genocide, Rwandans' sense of discipline was introduced and reinforced through weekly \"umuganda\" (collective work) sessions, involving praise for the regime and its leaders and a host of collective activities for the community. Respect for authority and the fear of stepping out of line were strong cultural values of pre-genocide Rwanda and so were included in these activities. Second, their value of social conformity only increased in the decades leading up to the genocide in both social and political manners. Peasants were told exactly when and what to farm and could be fined given any lack of compliance. These factors helped to drive the killing's fast pace.\n\nMost importantly, there were already ethnic tensions among the groups for a variety of reasons: conflicts over land allocation (farming versus pasture) and declining prices of Rwanda's main export: coffee. These problems combined with a history of previously existing conflict. With the introduction of the Second Republic under Habyarimana, former Tutsis in power were immediately purged, and racism served as an explanation as keeping the majority Hutu in legitimate government power. As a result, when the war came, the Hutu were already introduced to the concept of racism against their very own peers.\n\nThe division in Rwanda was reinforced for hundreds of years. King Kigeli IV, a Tutsi, centralized Rwandan power in the 1800s, just in time for German colonization. The Germans furthered the message of distinct races, allowing Tutsi men to remain the leaders in the society.\n\n Principals who served as strong \"instructional\" leaders and introduced new curricula and academic programs were able to create a system of peer pressure at the teaching level, where the teachers placed accountability pressure on themselves.\n\nPeer pressure can be especially effective (more so than door-to-door visits and telephone calls) in getting people to vote. Gerber, Green, and Larimer conducted a large-scale field experiment involving over 180,000 Michigan households in 2006 and four treatments: one was a reminder to vote, one was a reminder to vote and a note informing them that they were being studied, one that listed the voting records for all potential household individuals, and finally one that listed the voting records for the household individuals and their neighbors. The final treatment emphasized peer pressure within a neighborhood; neighbors could view each other's voting habits with the lists, and so the social norm of \"voting is best for the community\" is combined with the fear that individuals' peers would judge their lack of voting. Compared to a baseline rate of 29.7% (only the voting reminder), the treatment that utilized peer pressure increased the percentage of household voters by 8.1% (to 37.8%), which exceeds the value of in-person canvassing and personalized phone calls.\n\nA similar large-scale field experiment conducted by Todd Rogers, Donald P. Green, Carolina Ferrerosa Young, and John Ternovski (2017) studied the impact of a social pressure mailing in the context of a high-salience election, the 2012 Wisconsin gubernatorial election. Social pressure mailers included the line, “We’re sending this mailing to you and your neighbors to publicize who does and does not vote.” This study found a treatment effect of 1.0 percentage point, a statistically significant but far weaker effect than the 8.1 percentage point effect reported by Gerber, Green, and Larimer. The 2017 study’s effects were particularly sizable for low-propensity voters.\n\nAn experiment conducted by Diane Reyniers and Richa Bhalla measured the amount donated by a group of London School of Economics students. The group was split into individual donators and pair donators. The donation amounts were revealed within each pair; then, the pair was given time to discuss their amounts and then revise them as necessary. In general, pair subjects donated an average of 3.64 pounds (Sterling) while individuals donated an average of 2.55 pounds. Furthermore, in pairs where one subject donated significantly more than the other, the latter would on average increase the donation amount by 0.55 pounds. This suggests that peer pressure \"shames\" individuals for making smaller donations. But when controlling for donation amount, paired subjects were significantly less happy with their donation amount than individual subjects—suggesting that paired subjects felt coerced to donate more than they would have otherwise. This leads to a dilemma: charities will do better by approaching groups of people (such as friends); however, this could result in increased donor discomfort, which would impact their future donations.\n\nOrganizational researchers have found a generally similar phenomenon among large corporations: executives and managers of large companies look to similar organizations in their industry or headquarters city to figure out the appropriate level of corporate charitable donations, and those that make smaller donations might be seen as stingy and suffer damage to their reputations. \n\n\n"}
{"id": "390003", "url": "https://en.wikipedia.org/wiki?curid=390003", "title": "Peter L. Berger", "text": "Peter L. Berger\n\nPeter Ludwig Berger (; March 17, 1929 – June 27, 2017) was an Austrian-born American sociologist and Protestant theologian. Berger became known for his work in the sociology of knowledge, the sociology of religion, study of modernization, and theoretical contributions to sociological theory. \n\nBerger is arguably best known for his book, co-authored with Thomas Luckmann, \"The Social Construction of Reality: A Treatise in the Sociology of Knowledge\" (New York, 1966), which is considered one of the most influential texts in the sociology of knowledge and played a central role in the development of social constructionism. In 1998 the International Sociological Association named this book as the fifth most-influential book written in the field of sociology during the 20th century. In addition to this book, some of the other books that Berger has written include: \"\" (1963); \"\" (1969); and \"The Sacred Canopy: Elements of a Social Theory of Religion\" (1967). \n\nBerger spent most of his career teaching at The New School for Social Research, at Rutgers University, and at Boston University. Before retiring, Berger had been at Boston University since 1981 and was the director of the Institute for the Study of Economic Culture.\n\nPeter Ludwig Berger was born on March 17, 1929, in Vienna, Austria, to George William and Jelka (Loew) Berger, who were Jewish converts to Christianity. He died on June 27, 2017, in his Brookline, Massachusetts, home after a prolonged illness. He emigrated to the United States shortly after World War II in 1946 at the age of 17 and in 1952 he became a naturalized citizen.\n\nOn September 28, 1959, he married Brigitte Kellner, herself an eminent sociologist who was on the faculty at Wellesley College and Boston University where she was the chair of the sociology department at both schools. Brigitte was born in Eastern Germany in 1928. She moved to the United States in the mid-1950s. She was a sociologist who focused on the sociology of the family, arguing that the nuclear family was one of the main causes of modernization. Although she studied traditional families, she supported same-sex relationships. She was on the faculties of Hunter College of the City University of New York, Long Island University, Wellesley College, and Boston University. Additionally, she was author of \"Societies in Change\" (1971), \"The Homeless Mind\" (1974), \"The War over the Family\" (1984), and \"The Family in the Modern Age\" (2002).Brigitte Kellner Berger died May 28, 2015. \n\nThey had two sons, Thomas Ulrich Berger and Michael George Berger. Thomas is himself a scholar of international relations, now a Professor at the Pardee School of Global Studies at Boston University and author of \"War, Guilt and World Politics After World War II\" (2012) and \"Cultures of Antimilitarism: National Security in Germany and Japan\" (2003).\n\nAfter the Nazi takeover of Austria in 1938, Berger and his family emigrated to Palestine, then under British rule. He attended a British High school, St. Luke's. Following the German bombings of Haifa, he was evacuated to Mt. Carmel, where he developed his life-long interest in religion. In 1947 Berger and his family emigrated again, this time to the United States, where they settled in New York City. Berger attended Wagner College for his Bachelor of Arts and received his M.A. and Ph.D. from the New School for Social Research in New York in 1954. Berger, in his memoir, described himself as an \"accidental sociologist\", enrolling here in an effort to learn about American society and help become a Lutheran minister, and learning under Alfred Schütz. In 1955 and 1956 he worked at the Evangelische Akademie in Bad Boll, West Germany. From 1956 to 1958 Berger was an assistant professor at the University of North Carolina at Greensboro; from 1958 to 1963 he was an associate professor at Hartford Theological Seminary. The next stations in his career were professorships at the New School for Social Research, Rutgers University, and Boston College. Since 1981 Berger was the University Professor of Sociology and Theology at Boston University. He retired from BU in 2009. In 1985 he founded the Institute for the Study of Economic Culture, which later transformed into the Institute on Culture, Religion and World Affairs (CURA), and is now part of the Boston University Pardee School of Global Studies. He remained the Director of CURA from 1985 to 2010.\n\nCURA\n\nBerger founded The Institute on Culture, Religion, and World Affairs at Boston University in 1985. It is a world-center for research, education, and public scholarship on religion and world affairs. Some of the questions it attempts to answer are: How do religion and values affect political, economic, and public ethical developments around the world? Defying earlier forecasts, why have religious actors and ideas become more rather than less globally powerful in recent years? and In a world of increasing religious and ethical diversity, what are the implications of the revival of public religion for citizenship, democracy, and civil coexistence? CURA has over 140 projects in 40 countries.\n\nReligious views\n\nBerger was a moderate Christian Lutheran conservative whose work in theology, secularization, and modernity at times has challenged the views of contemporary mainstream sociology which tends to lean away from any right-wing political thinking. Ultimately, however, Berger's approach to sociology was humanist with special emphasis on \"value-free\" analysis.\n\nHuman beings construct a shared social reality. This is explained in Berger and Thomas Luckmann's book \"The Social Construction of Reality\" (1966). This reality includes things from ordinary language to large-scale institutions. Our lives are governed by the knowledge about the world that we have and use the information that is relevant to our lives. We take into account typificatory schemes, which are general assumptions about society. As one encounters a new scheme, one must compare it to the ones that are already established in one's mind and determine whether to keep those schemes or replace the old ones with new ones. Social structure is the total of all these typificatory schemes.\n\nBerger and Luckmann present this as the sphere of reality that presents itself upon human existence most intensely and immediately. Everyday life is contrasted with other spheres of reality – dreamworlds, theatre – and is considered by a person to be the objective, intersubjective (shared with others) and self-evident. Life is ordered spatially and temporally. Spatial ordering allows interaction with other people and objects; the human ability to manipulate zones of space can intersect with another's ability.\n\nSocial interactions in everyday life favour personal, face-to-face encounters as the best scenarios where human beings can actually connect with each other through interactions. Humans perceive the other in these interactions as more real than they would themselves; we can place a person in everyday life by seeing them, yet we need to contemplate our own placement in the world as it is not so concrete. Berger believes that although you know yourself on a much deeper scale than you would the other person, they are more real to you because they are constantly making \"What he is\" available to you. It is difficult to recognize \"What I am\" without separating oneself from the conversation and reflecting on it. Even then, that self-reflection is caused by the other person's interactions leading to that self contemplation.\n\nLanguage is imperative to the understanding of everyday life. People understand knowledge through language. The knowledge relevant to us is the only necessary knowledge to our survival, but humans interact through sharing and connecting the relevant structures of our lives with each other. Language helps create shared symbols and stocks of knowledge and participation in these things inherently makes us participate in society.\n\nSocial reality exists at both the subjective and objective levels. At the subjective level, people find reality personally meaningful and created by human beings in aspects such as personal friendships. At the objective level, people find reality is aspects such as government bureaucracies and large corporations where reality is seen as more out of one's control.\n\nObjectively, social order is a product of our social enterprise: it is an ongoing process that results from human activity. Institutions are a product of the historicity and need to control human habitualization (the repeated behaviours or patterns). The shared nature of these experiences and their commonality results in sedimentation, meaning they lose their memorability. Many behaviours lose sedimented institutional meanings. Institutional order involves specified roles for people to play. These roles are seen as performing as this objective figure – an employee is not judged as a human but by that role they have taken.\n\nThe process of building a socially-constructed reality takes place in three phases. Initially, externalization is the first step in which humans pour out meaning (both mental and physical) into their reality, thus creating things through language. In externalization, social actors create their social worlds and it is seen through action. Following that, reality becomes established by the products of externalization through the course of objectivation (things and ideas \"harden\" in a sense). People see either a social practice or institution as an objective reality that cannot be changed, such as something like language. Lastly, this newly-made, and man-made reality (or society) has an effect on humans themselves. In this third phase, internalization, the external, objective world to a person becomes part of their internal, subjective world. Social actors internalize norms and values, accepting them as givens, and make them our reality.\n\nSubjectively, we experience first and second socialization into society. Firstly, we are socialized into the world during one's childhood by family members and friends. Secondly, we internalize institutional \"sub worlds\" during one's adulthood, put in various positions in the economy. We maintain our subjective world through reaffirmation with social interactions with others. Our identity and the society are seen as dialectically related: our identity is formed by social processes, which are in turn ordered by our society. Berger and Luckmann see socialization as very powerful and able to influence things such as sexual and nutritional choices. People have the ability to do whatever they want in these spheres, but socialization causes people to only choose certain sexual partners or certain foods to eat to satisfy biological needs.\n\nThe humanistic perspective is generally outside of mainstream, contemporary sociology. It is considered as a view that relates more to the humanities – literature, philosophy – than to social science. Its ultimate purpose lies in freeing society of illusions to help make it more humane. In this sense, we are the \"puppets of society,\" but sociology allows us to see the strings that we are attached to, which helps to free ourselves. Berger's \"Invitation to Sociology\" outlines his approach to the field of sociology in these humanistic terms. Methodologically, sociologists should attempt to understand and observe human behaviour outside the context of its social setting and free from whatever influence a sociologist's personal biases or feelings might be. The study of sociology, Berger posits, should be value-free. Research should be accrued in the same manner as the scientific method, using observation, hypothesis, testing, data, analysis and generalization. The meaning derived from the results of research should be contextualized with historical, cultural, environmental, or other important data.\n\nBerger saw the field of sociology as not only just a way to help people and the community, but sociological insights are also important to all people interested in instilling action in society. Sociologists are a part of a multitude of fields, not just social work. Berger stated that sociology is not a practice, but an attempt to understand the social world. These understandings could be used by people in any field for whatever purpose and with whatever moral implications. He believed that sociologists, even if their values varied greatly, should at the very least have scientific integrity. Sociologists are only humans and will still have to deal with things such as convictions, emotions, and prejudices, but being trained in sociology should learn to understand and control these things and try to eliminate them from their work. A sociologist's job is to accurately report on a certain social terrain. Sociology is a science, and its findings are found through observation of certain rules of evidence that allow people to repeat and continue to develop the findings.\n\nBerger believed that society is made aware of what he referred to as the nomos, or the patterns a particular society wants its members to see as objectively right and to internalize. The nomos is all the society's knowledge about how things are, and all of its values and ways of living. This is upheld through legitimacy, either giving special meaning to these behaviors or by creating a structure of knowledge that enhances the plausibility of the nomos. The existence of an eternal cosmic entity that legitimizes a nomos makes the nomos itself eternal; an individual's actions within its set society are all based on a universal and orderly pattern based on their beliefs.\n\nModern pluralization, which has stemmed from the Protestant Reformation in the 16th century, set forth a new set of values, including: separation of the religious and secular spheres of life, a person's wealth as a determinant of value, maximizing freedom to enhance wealth, increasing prediction and control to increase wealth, and identifying oneself as a member of a nation-state. This, in turn, spread capitalism and its ideals and beliefs of individualism and rationalization and separated Christians from their Gods. With globalization, even more beliefs and cultures were confronted with this.\n\nBerger believed that modernity – technological production paradigms of thinking and bureaucracy, namely – alienated the individual from primary institutions and forced individuals to create separate spheres of public and private life. There is no plausibility structure for any system of beliefs in the modern world; people are made to choose their own with no anchors to our own perceptions of reality. This lowers feelings of belonging and forces our own subjectivities onto themselves. Berger called this a \"homelessness of the mind.\" It is the product of the modern world, he believed, as it has transformed the technology of production into our consciousness, making our cognition componential, always searching for a \"means to an end.\" Ideas and beliefs are varied in the modern world, and an individual, not sharing their system of beliefs with the public whole, relegates any behaviors that are contingent on it to their private life. Certain beliefs that an individual has that may not be widely accepted by society as a whole, are then kept to one's self and may only be seen within one's private life and are not seen by society.\n\nThe socialist myth, a non-pejorative term of Berger's, actually arises from intellectual leftism masking a need to resolve the lacking sense of community in the modern world through the promise to destroy the oppression of capitalism. Berger believed resolving community in modern society needs to emphasize the role of \"mediating structures\" in their lives to counter the alienation of modernity. Human existence in the age of modernity requires there to be structures like church, neighbourhood, and family to help establish a sense of belonging rooted in a commitment to values or beliefs. This builds a sense of community and belonging in an individual. In addition, these structures can serve a role in addressing larger social problems without the alienation that larger society creates. The role of mediating structures in civil society is both private and public, in this sense.\n\nThe general meaning of pluralism is the coexistence, generally peaceful, of different religions, worldviews, and value systems within the same society. Berger believes pluralism exists in two ways. The first being that many religions and worldviews coexist in the same society. The second is the coexistence of the secular discourse with all these religious discourses. Some people avoid pluralism by only operating within their own secular or religious discourse, meaning they do not interact with others outside of their beliefs. Pluralism generally today is that it is globalized. Berger sees benefits in pluralism. One is that pluralism makes complete consensus in beliefs very rare, which allows people to form and hold their own beliefs without trying to conform to a society that holds all the same beliefs. This ties into the second benefit which is that pluralism gives freedom and allows people free decisions. Another benefit is that if pluralism is connected to religious freedom, religious institutions now become voluntary associations. Lastly, pluralism influences individual believers and religious communities to define the core of their faith separately from less central elements, which allows people to pick and choose certain aspects of their chosen form of belief that they may or may not agree with, while still remaining true to the central parts of it.\n\nIn daily life, people experience symbols and glimpses of existence beyond empirical order and of transcendent existence. Berger calls these \"rumours of angels\". People feel in times of great joy, in never-ending pursuit of order against chaos, in the existence of objective evil, and in the sense of hope that there exists some supernatural reality beyond that of human existence. People who choose to believe in the existence of a supernatural other require faith – a wager of belief against doubt – in the modern rationalised world. Knowledge can no longer sufficiently ground human belief in the pluralized world, forcing people to wager their own beliefs against the current of doubt in our society.\n\nLike most other sociologists of religion of his day, Berger once predicted the all-encompassing secularization of the world. He has admitted to his own miscalculations about secularization, concluding that the existence of resurgent religiosity in the modernised world has proven otherwise. In \"The Desecularization of the World\", he cites both Western academia and Western Europe itself as exceptions to the triumphant desecularization hypothesis: that these cultures have remained highly secularized despite the resurgence of religion in the rest of the world. Berger finds that his and most sociologists' misconsensus about secularisation may have been the result of their own bias as members of academia, which is a largely atheist concentration of people.\n\nIn \"Making Sense of Modern Times: Peter L. Berger and the Vision of Interpretive Sociology\", James Davison Hunter and Stephen C. Ainlay build upon the social theories of Berger's. Hunter and Ainlay use Berger's ideologies as a foundation and framework for this particular book. Nicholas Abercrombie begins by examining his reformation of the sociology of knowledge. Shifting his focus on the subjective reality of everyday life, Berger enters a dialogue with traditional sociologies of knowledge – more specific, those of Karl Marx and Karl Mannheim. Abercrombie digs deeper into this dialogue Berger brings up, and he considers ways in which Berger goes beyond these figures. Stephen Ainlay then pursues the notable influence on Berger's work.\n\nIn the field of sociology, Berger has been somewhat excluded from the mainstream; his humanistic perspective was denounced by much of the intellectual elite in the field, though it sold well over a million copies. Berger's leftist criticisms do not help him much in that regard either. Berger's theories on religion have held considerable weight in contemporary neoconservative and theological fields of thinking, however.\n\nIn 1987 Berger argued about the emergence of a new social class he called the \"knowledge class\". He views it as a result of what was known as the middle class into two groups: the \"old middle class\" of those who produce material goods and services and the \"knowledge class\" whose occupations relate to the production and distribution of \"symbolic knowledge.\" He followed Helmut Schelsky's definition of \"Sinn- und Hellsvermittler\", \"agents (intemediaries) of meanings and purposes\".\n\nBerger's work was notably influenced by Max Weber. Weber focused on the empirical realities of rationality as a characteristic of action and rationalization. In comparison, Berger proposed the usage of the word 'options' rather than freedom as an empirical concept. Therefore, much of the empirical work of Berger and Weber have revolved around the relationship between modern rationalization and options for social action. Weber argued that rationalism can mean a variety of things at the subjective level of consciousness and at the objective level of social institutions. The connection between Berger's analysis of the sociology of religion in modern society and Max Weber's \"The Protestant Ethic and the Spirit of Capitalism\" aligns. Weber saw capitalism as a result of the Protestant secularisation of work ethic and morality in amassing wealth, which Berger integrates into his analysis about the effects of losing the non-secular foundations for belief about life's ultimate meaning.\n\nBerger's own experiences teaching in North Carolina in the 1950s showed the shocking American prejudice of that era's Southern culture and influenced his humanistic perspective as a way to reveal the ideological forces from which it stemmed.\n\nBerger was elected a Fellow of the American Academy of Arts and Sciences in 1982.\nHe was doctor \"honoris causa\" of Loyola University, Wagner College, the College of the Holy Cross, the University of Notre Dame, the University of Geneva, and the University of Munich, and an honorary member of many scientific associations.\n\nIn 2010, he was awarded the Dr. Leopold Lucas Prize by the University of Tübingen.\n\n\n"}
{"id": "593693", "url": "https://en.wikipedia.org/wiki?curid=593693", "title": "Point (geometry)", "text": "Point (geometry)\n\nIn modern mathematics, a point refers usually to an element of some set called a space.\n\nMore specifically, in Euclidean geometry, a point is a primitive notion upon which the geometry is built, meaning that a point cannot be defined in terms of previously defined objects. That is, a point is defined only by some properties, called axioms, that it must satisfy. In particular, the geometric points do not have any length, area, volume or any other dimensional attribute. A common interpretation is that the concept of a point is meant to capture the notion of a unique location in Euclidean space.\n\nPoints, considered within the framework of Euclidean geometry, are one of the most fundamental objects. Euclid originally defined the point as \"that which has no part\". In two-dimensional Euclidean space, a point is represented by an ordered pair (, ) of numbers, where the first number conventionally represents the horizontal and is often denoted by , and the second number conventionally represents the vertical and is often denoted by . This idea is easily generalized to three-dimensional Euclidean space, where a point is represented by an ordered triplet (, , ) with the additional third number representing depth and often denoted by . Further generalizations are represented by an ordered tuplet of terms, where is the dimension of the space in which the point is located.\n\nMany constructs within Euclidean geometry consist of an infinite collection of points that conform to certain axioms. This is usually represented by a set of points; As an example, a line is an infinite set of points of the form formula_1, where through and are constants and is the dimension of the space. Similar constructions exist that define the plane, line segment and other related concepts. A line segment consisting of only a single point is called a degenerate line segment.\n\nIn addition to defining points and constructs related to points, Euclid also postulated a key idea about points, that any two points can be connected by a straight line. This is easily confirmed under modern extensions of Euclidean geometry, and had lasting consequences at its introduction, allowing the construction of almost all the geometric concepts known at the time. However, Euclid's postulation of points was neither complete nor definitive, and he occasionally assumed facts about points that did not follow directly from his axioms, such as the ordering of points on the line or the existence of specific points. In spite of this, modern expansions of the system serve to remove these assumptions.\n\nThere are several inequivalent definitions of dimension in mathematics. In all of the common definitions, a point is 0-dimensional.\n\nThe dimension of a vector space is the maximum size of a linearly independent subset. In a vector space consisting of a single point (which must be the zero vector 0), there is no linearly independent subset. The zero vector is not itself linearly independent, because there is a non trivial linear combination making it zero: formula_2.\n\nThe topological dimension of a topological space \"X\" is defined to be the minimum value of \"n\", such that every finite open cover formula_3 of \"X\" admits a finite open cover formula_4 of \"X\" which refines formula_3 in which no point is included in more than \"n\"+1 elements. If no such minimal \"n\" exists, the space is said to be of infinite covering dimension.\n\nA point is zero-dimensional with respect to the covering dimension because every open cover of the space has a refinement consisting of a single open set.\n\nLet \"X\" be a metric space. If \"S\" ⊂ \"X\" and \"d\" ∈ [0, ∞), the \"d\"-dimensional Hausdorff content of \"S\" is the infimum of the set of numbers δ ≥ 0 such that there is some (indexed) collection of balls formula_6 covering \"S\" with \"r\" > 0 for each \"i\" ∈ \"I\" that satisfies formula_7.\n\nThe Hausdorff dimension of \"X\" is defined by\n\nA point has Hausdorff dimension 0 because it can be covered by a single ball of arbitrarily small radius.\n\nAlthough the notion of a point is generally considered fundamental in mainstream geometry and topology, there are some systems that forgo it, e.g. noncommutative geometry and pointless topology. A \"pointless\" or \"pointfree\" space is defined not as a set, but via some structure (algebraic or logical respectively) which looks like a well-known function space on the set: an algebra of continuous functions or an algebra of sets respectively. More precisely, such structures generalize well-known spaces of functions in a way that the operation \"take a value at this point\" may not be defined.\nA further tradition starts from some books of A. N. Whitehead in which the notion of region is assumed as a primitive together with the one of \"inclusion\" or \"connection\".\n\nOften in physics and mathematics, it is useful to think of a point as having non-zero mass or charge (this is especially common in classical electromagnetism, where electrons are idealized as points with non-zero charge). The Dirac delta function, or function, is (informally) a generalized function on the real number line that is zero everywhere except at zero, with an integral of one over the entire real line. The delta function is sometimes thought of as an infinitely high, infinitely thin spike at the origin, with total area one under the spike, and physically represents an idealized point mass or point charge. It was introduced by theoretical physicist Paul Dirac. In the context of signal processing it is often referred to as the unit impulse symbol (or function). Its discrete analog is the Kronecker delta function which is usually defined on a finite domain and takes values 0 and 1.\n\n\n\n"}
{"id": "32747511", "url": "https://en.wikipedia.org/wiki?curid=32747511", "title": "Political violence in Germany (1918–33)", "text": "Political violence in Germany (1918–33)\n\nSubstantial political violence existed in Germany from the fall of the House of Hohenzollern and the rise of the Weimar Republic through the German Revolution of 1918–19 until the rise of the Nazi Party to power in 1933 when a Nazi totalitarian state was formed and opposition figures were arrested.\n\nDue to unrest left from the change of government from a monarchy, based on social standing, to a democratic republic, the people of Germany turned to riots and violence. The drastic change allowed for mobility amongst the classes and new voices to be heard. Many large cities, especially Berlin, experienced political rallies which resulted in violence from opposition. The quick overturn of leaders also influenced crises in the interwar period. Ultimately the National socialists took advantage of the radical setting of Germany but leading to this there was great amounts of political violence.\n\n"}
{"id": "11894762", "url": "https://en.wikipedia.org/wiki?curid=11894762", "title": "Polyad", "text": "Polyad\n\nIn mathematics, polyad is a concept of category theory introduced by Jean Bénabou in generalising monads. A polyad in a bicategory \"D\" is a bicategory morphism \"Φ\" from a locally punctual bicategory \"C\" to \"D\", . (A bicategory \"C\" is called locally punctual if all hom-categories \"C\"(\"X\",\"Y\") consist of one object and one morphism only.) Monads are polyads where \"C\" has only one object.\n"}
{"id": "8246989", "url": "https://en.wikipedia.org/wiki?curid=8246989", "title": "Psychological resistance", "text": "Psychological resistance\n\nPsychological resistance is the phenomenon often encountered in clinical practice in which patients either directly or indirectly exhibits paradoxical opposing behaviors in presumably a clinically initiated push and pull of a change process. It impedes the development of authentic, reciprocally nurturing experiences in a clinical setting. It is established that the common source of resistances and defenses is shame, further its pervasive nature in trans diagnostic roles are identified.\n\nExamples of psychological resistance are perfectionism, criticizing, contemptuous attitude, being self-critical, preoccupation with appearance, social withdrawal, need to be seen as independent and invulnerable, or an inability to accept compliments or constructive criticism. \n\nThe discovery of resistance () was central to Sigmund Freud's theory of psychoanalysis: for Freud, the theory of repression is the corner-stone on which the whole structure of psychoanalysis rests, and all his accounts of its discovery \"are alike in emphasizing the fact that the concept of repression was inevitably suggested by the clinical phenomenon of resistance\".\n\nIn an early exposition of his new technique, Freud wrote that \"There is, however, another point of view which you may take up in order to understand the psychoanalytic method. The discovery of the unconscious and the introduction of it into consciousness is performed in the face of a continuous resistance on the part of the patient. The process of bringing this unconscious material to light is associated with pain, and because of this pain the patient again and again rejects it\". He went on to add that \"It is for you then to interpose in this conflict in the patient's mental life. If you succeed in persuading him to accept, by virtue of a better understanding, something that up to now, in consequence of this automatic regulation by pain, he has rejected (repressed), you will then have accomplished something towards his education...Psychoanalytic treatment may in general be conceived of as such a re-education in overcoming internal resistances\".\n\nAlthough the term resistance as it is known today in psychotherapy is largely associated with Sigmund Freud, the idea that some patients \"cling to their disease\" was a popular one in medicine in the nineteenth century, and referred to patients whose maladies were presumed to persist due to the secondary gains of social, physical, and financial benefits associated with illness. While Freud was trained in what is known as the (secondary) gain from illness that follows a neurosis, he was more interested in the unconscious processes through which he could explain the primary gains that patients derive from their psychiatric symptoms.\n\nThe model he devised to do so suggests that the symptoms represent an unconscious tradeoff in exchange for the sufferer being spared other, experientially worse, psychological displeasures, by way of what Freud labeled a compromise formation; \"settling the conflict by constructing a symptom is the most convenient way out and the one most agreeable to the pleasure principle\".\n\nThus, contrasting the primary gain (internal benefits) and secondary gain (external benefits) from illness, Freud wrote: \"In civil life illness can be used as a screen to gloss over incompetence in one's profession or in competition with other people; while in the family it can serve as a means for the other members and extorting proofs of their love or for imposing one's will upon them… we sum it up in the term 'gain from illness'… But there are other motives, that lie still deeper, for holding on to being ill… [b]ut these cannot be understood without a fresh journey into psychological theory\".\n\nTo Freud, the primary gains that stood behind the patient's resistance were the result of an intrapsychic compromise, reached between two or more conflicting agencies: \"psychoanalysis...maintains that the isolation and unconsciousness of this [one] group of ideas have been caused by an active opposition on the part of other groups\". Freud called the one psychic agency the \"repressing\" consciousness, and the other agency, the unconscious, he eventually referred to as the \"id\". \n\nThe compromise the two competing parties strive for is to achieve maximum drive satisfaction with minimum resultant pain (negative reactions from within and without). Freud theorized that psychopathology was due to unsuccessful compromises – \"We have long observed that every neurosis has the result, and therefore probably the purpose, of forcing the patient out of real life, of alienating him from actuality\" – as opposed to \"successful defense\" which resulted in \"apparent health\".\n\nKey players in the Kompromisslösung theory of symptom production, at the core of Freud's theory of resistance, were: Repression (often used interchangeably with the term anticathexis), defense, displeasure, anxiety, danger, compromise, and symptom. As Freud wrote, \"The action undertaken to protect repression is observable in analytic treatment as resistance. Resistance presupposes the existence of what I have called anticathexis.\"\n\nIn 1926, Freud was to alter his view of anxiety, with implications for his view of resistance. \"Whereas the old view made it natural to suppose that anxiety arose from the libido belonging to the repressed instinctual impulses, the new one, on the contrary, made the ego the source of anxiety\". \n\nFreud still understood resistance to be intimately bound up with the fact of transference: \"It may thus be said that the theory of psycho-analysis is an attempt to account for two observed facts that strike one conspicuously and unexpectedly whenever an attempt is made to trace the symptoms of a neurotic back to their source in his past life: the facts of transference and resistance. Any line of investigation, no matter what its direction, which recognizes these two facts and takes them as the starting-point of its work may call itself psychoanalysis, though it arrives at results other than my own\". Indeed, to this day most major schools of psychotherapeutic thought continue to at least recognize, if not \"take as the starting-point\", the two phenomena of transference and resistance.\n\nNevertheless his new conceptualisation of the role of anxiety caused him to reframe the phenomena of resistance, to embrace how \"The analyst has to combat no less than five kinds of resistance, emanating from three directions – the ego, the id and the superego\". He considered the ego to be the source of three types of resistance: repression, transference and gain from illness, i.e., secondary gain. Freud defined a fourth variety, arising from the \"id\", as resistance that requires \"working-through\" the product of the repetition compulsion. A fifth, coming from the \"superego\" and the last to be discovered, \"...seems to originate from the sense of guilt or the need for punishment\" – i.e., self-sabotage.\n\nAll these serve the explicit purpose of defending the ego against feelings of discomfort, for, as Freud wrote: \"It is hard for the ego to direct its attention to perceptions and ideas which it has up till now made a rule of avoiding, or to acknowledge as belonging to itself impulses that are the complete opposite of those which it knows as its own.\"\n\nFreud viewed all five categories of resistance as requiring more than just intellectual insight or understanding to overcome. Instead he favored a slow process of working through.\n\nWorking through allows patients \"...to get to know this resistance\" and \"...discover the repressed instinctual trends which are feeding the resistance\" and it is this experientially convincing process that \"distinguishes analytic treatment from every kind of suggestive treatment\". For this reason Freud insisted that therapists remain neutral, saying only as much as \"is absolutely necessary to keep him [the patient] talking\", so that resistance could be seen as clearly as possible in patients' transference, and become obvious to the patients themselves. The inextricable link suggested by Freud between transference and resistance perhaps encapsulates his legacy to psychotherapy.\n\nResistance is based on our instinctively autonomous ways of reacting in which clients both reveal and keep hidden aspects of themselves from the therapist or another person. These behaviors occur mostly during therapy, in interaction with the therapist. It is a way of avoiding and yet expressing unacceptable drives, feelings, fantasies, and behavior patterns.\n\nExamples of causes of resistance include: resistance to the recognition of feelings, fantasies, and motives; resistance to revealing feelings toward the therapist; resistance as a way of demonstrating self-sufficiency; resistance as clients' reluctance to change their behavior outside the therapy room; resistance as a consequence of failure of empathy on the part of the therapist.\n\nExamples of the expression of resistance are canceling or rescheduling appointments, avoiding consideration of identified themes, forgetting to complete homework assignments and the like. This will make it more difficult for the therapist to work with the client, but it will also provide him with information about the client.\n\nResistance is an automatic and unconscious process. According to Van Denburg and Kiesler, it can be either for a certain period of time (state resistance) but it can also be a manifestation of more longstanding traits or character (trait resistance).\n\nIn psychotherapy, state resistance can occur at a certain moment, when an anxiety provoking experience is triggered. Trait resistance, on the other hand, occurs repeatedly during sessions and interferes with the task of therapy. The client shows a pattern of off-task behaviors that makes the therapist experience some level of negative emotion and cognition against the client. Therefore the maladaptive pattern of interpersonal behavior and the therapist's response interfere with the task or process of therapy. This ‘state resistance' is cumulative during sessions and its development can best be prevented by empathic interventions on the therapist's part.\n\nOutside therapy, trait resistance in a client is demonstrated by distinctive patterns of interpersonal behavior which are often caused by typical patterns of communication with significant others, like family, friends and partners.\n\nNowadays many therapists work with resistance as a way to understand the client better. They emphasize the importance to work with the resistance and not against it. This is because working against the resistance of a client can result in a counterproductive relationship with the therapist; the more attention is drawn to the resistance, the less productive the therapy. Working with the resistance provides a positive working relationship and gives the therapist information about the unconscious of the client.\n\nA therapist can use countertransference as a tool to understand the client's resistance. The feelings the client evokes in the therapist with his/her resistance will give you a hint what the resistance is about. For example, a very directive client can make the therapist feel very passive. When the therapist pays attention to their passive feelings, it can make him/her understand this behavior of the client as resistance coming from fear of losing control.\n\nIt can also be useful to identify resistance with the client. This can not only work towards addressing the issue, but can also allow the client to think about and discuss their resistance and the cognitive processes that underlie it. In this way, the client takes an active involvement in their therapy, which may reduce resistance in future. It also helps the client's ability to identify their resistance in the future and respond to it.\n\nImportant to the question of treatment planning are research studies that have looked at resistance traits as indicators and contra-indicators for different types of interventions. Beutler, Moleiro and Talebi reviewed 20 studies that inspected the differential effects of therapist directiveness as moderated by client resistance and found that 80% (n=16) of the studies demonstrated that directive interventions were most productive among clients who had relatively low levels of state or trait-like resistance, while nondirective interventions worked best among clients who had relatively high levels of resistance. These findings provide strong support for the value of resistance level as a predictor of treatment outcome, as well as treatment-planning. In these studies cognitive behavioral therapy has been used as a prototype for directive therapy and psychodynamic, self-directed, or other relation oriented therapy have been used as a prototype for non-directive therapy.\n\nBehavior analytic and social learning models of resistance focus on the setting events, antecedents, and consequences for resistant behavior with the goal to understand the function of the behavior. At least five behavioral models of resistance exist. These models share many common features. The most explored research model, with more than 10 years of support, is the model created by Gerald Patterson for resistance in parent training. With supporting research, this model has even been extended to consultation. \n\nPatterson's suggested intervention of struggle with and work through is often contrasted as an intervention with motivational interviewing. In motivational interviewing the therapist makes no attempt to prompt the client back to the problem area but reinforces the occurrence when it comes up as opposed to struggle with and working through where the therapist directly guides the client back to the problem. Behavior analytic models can accommodate both interventions as pointed out by Cautilli and colleagues depending on function and what needs to be accomplished in the treatment.\n\n"}
{"id": "39423830", "url": "https://en.wikipedia.org/wiki?curid=39423830", "title": "Rain Room", "text": "Rain Room\n\nRain Room is a 2012 experiential artwork by Hannes Koch and Florian Ortkrass for Random International, which found its first permanent installation in Sharjah, United Arab Emirates in 2018. The piece had previously shown in a number of international art venues, including New York's Museum of Modern Art (MoMA) and London's Barbican.\n\n\"Rain Room\" allows visitors to the installation to walk through a downpour without getting wet. Motion sensors detect visitors' movements as they navigate through the darkened space, becoming \"performers in this intersection of art, technology and nature\".\n\nThis site-specific sound and light installation uses 2,500 litres of self-cleaning recycled water, controlled through a system of 3D tracking cameras placed around the ceiling. The cameras detect a visitor's movement and signal groups of the water nozzles in the ceiling, stopping the flow of water in a roughly six-foot radius around the person.\n\nFounded in 2005, Random International is a London-based collaborative studio for experimental and digital practice within contemporary art. Their work, which includes sculpture, performance and large-scale architectural installations, reflects the relationship between man and machine and centres on audience interaction. \n\nSharjah’s \"Rain Room\" is the work's Middle Eastern debut and the first installation of the project in a purpose-built, permanent structure. The work was previously shown at the Barbican, London (2012); MoMA, New York (2013); Yuz Museum, Shanghai (2015) and LACMA, Los Angeles (2015–2017).\n\nSharjah Art Foundation constructed a purpose-built visitor centre located in the city's residential area of Al Majarrah to house the permanent installation of \"Rain Room\", with up to six visitors at a time taking fifteen minutes to explore the experience. Tickets are sold online for scheduled visits and cost Dhs25 for adults. \n\nOpened in May 2018 by Ruler of Sharjah Dr Sheikh Sultan Bin Mohammad Al Qasimi together with Shaikha Hoor Bint Sultan Al Qasimi, president of Sharjah Art Foundation, \"Rain Room\" is part of the Sharjah Art Foundation Collection and the first of a series of artist-designed permanent spaces planned for Sharjah.\n\nAt the Sharjah inauguration, Koch and Ortkrass commented, \"That Rain Room has found a permanent home at Sharjah Art Foundation is a humbling thought. The organisation [Sharjah Art Foundation] is unparalleled in its approach to art, exhibition-making and relationships with a wider public audience.\"\n\nKate Bush (curator), Head of Art Galleries, Barbican Centre, said: \"The Curve has previously played host to guitar-playing finches, a World War II bunker and a digital bowling alley. Random International have created a new work every bit as audacious and compelling – Rain Room surpasses all our expectations.\"\n\n\"At the cutting edge of digital technology, Rain Room is a carefully choreographed downpour – a monumental installation that encourages people to become performers on an unexpected stage, whilst creating an intimate atmosphere of contemplation. The work also invites us to explore what role science, technology and human ingenuity might play in stabilising our environment by rehearsing the possibilities of human adaptation.\"\n\n\"Timeout\" described \"Rain Room\" at the Barbican as \"one of the most popular art installations of the past few years. It was incredible, featuring endlessly dripping water that magically avoided you as you walked through it.\"\n\nDespite being dubbed \"Wildly successful\" at the Barbican, \"Business Insider\" reviewed the \"Rain Room\" at MoMA as \"Not worth the wait\", their reviewer having spent over three hours queuing for a ten minute 'experience'. However, Gizmodo called it a \"blockbuster\" and \"the kind of installation that museums dream of\".\n\n"}
{"id": "3042574", "url": "https://en.wikipedia.org/wiki?curid=3042574", "title": "Reactance (psychology)", "text": "Reactance (psychology)\n\nReactance is an unpleasant motivational arousal (reaction) to offers, persons, rules, or regulations that threaten or eliminate specific behavioral freedoms. Reactance occurs when a person feels that someone or something is taking away their choices or limiting the range of alternatives.\n\nReactances can occur when someone is heavily pressured to accept a certain view or attitude. Reactance can cause the person to adopt or strengthen a view or attitude that is contrary to what was intended, and also increases resistance to persuasion. People using reverse psychology are playing on reactance, attempting to influence someone to choose the opposite of what they request.\n\nPsychological reactance is \"an unpleasant motivational arousal that emerges when people experience a threat to or loss of their free behaviors.\" An example of such behavior can be observed when an individual engages in a prohibited activity in order to deliberately taunt the authority who prohibits it, regardless of the utility or disutility that the activity confers. An individual's freedom to select when and how to conduct their behavior, and the level to which they are aware of the relevant freedom—and are able to determine behaviors necessary to satisfy that freedom—affect the generation of psychological reactance. It is assumed that if a person's behavioral freedom is threatened or reduced, they become motivationally aroused. The fear of loss of further freedoms can spark this arousal and motivate them to re-establish the threatened freedom. Because this motivational state is a result of the perceived reduction of one's freedom of action, it is considered a counterforce, and thus is called \"psychological reactance\".\n\nThere are four important elements to reactance theory: perceived freedom, threat to freedom, reactance, and restoration of freedom. Freedom is not an abstract consideration, but rather a feeling associated with real behaviors, including actions, emotions, and attitudes.\n\nReactance also explains denial as it is encountered in addiction counselling. According to William R. Miller, \"Research demonstrates that a counselor can drive resistance (denial) levels up and down dramatically according to his or her personal counseling style\". Use of a \"respectful, reflective approach\" described in motivational interviewing and applied as motivation enhancement therapy, rather than by argumentation, the accusation of \"being in denial\", and direct confrontations, lead to the motivation to change and avoid the resistance and denial, or reactance, elicited by strong direct confrontation.\n\nReactance theory assumes there are \"free behaviors\" individuals perceive and can take part in at any given moment. For a behavior to be free, the individual must have the relevant physical and psychological abilities to partake in it, and must know they can engage in it at the moment, or in the near future.\n\n\"Behavior\" includes any imaginable act. More specifically, behaviors may be explained as \"what one does (or doesn't do)\", \"how one does something\", or \"when one does something\". It is not always clear, to an observer, or the individuals themselves, if they hold a particular freedom to engage in a given behavior. When a person has such a free behavior they are likely to experience reactance whenever that behavior is restricted, eliminated, or threatened with elimination.\n\nThere are several rules associated with free behaviors and reactance:\n\nOther core concepts of the theory are justification and legitimacy. A possible effect of justification is a limitation of the threat to a specific behavior or set of behaviors. For example, if Mr Doe states that he is interfering with Mrs. Smith's expectations because of an emergency, this keeps Mrs Smith from imagining that Mr Doe will interfere on future occasions as well. Likewise, legitimacy may point to a set of behaviors threatened since there will be a general assumption that an illegitimate interference with a person's freedom is less likely to occur. With legitimacy there is an additional implication that a person's freedom is equivocal.\n\nIn the phenomenology of reactance, there is no assumption that a person will be aware of reactance. When a person becomes aware of reactance, they will feel a higher level of self-direction in relationship to their own behavior. In other words, they will feel that if they are able to do what they want, then they do not have to do what they do not want. In this case, when the freedom is in question, that person alone is the director of their own behavior.\n\nWhen considering the direct re-establishment of freedom, the greater the magnitude of reactance, the more the individual will try to re-establish the freedom that has been lost or threatened. When a freedom is threatened by a social pressure, then reactance will lead a person to resist that pressure. Also, when there are restraints against a direct re-establishment of freedom, there can be attempts at re-establishment by implication whenever possible.\n\nFreedom can and may be reestablished by a social implication. When an individual has lost a free behavior because of a social threat, then the participation in a free-like behavior by a similar person will allow one to re-establish one's own freedom.\n\nIn summary, the definition of psychological reactance is a motivational state that is aimed at re-establishment of a threatened or eliminated freedom. A short explanation of the concept is that the level of reactance has a direct relationship between the importance of a freedom which is eliminated or threatened, and a proportion of free behaviors eliminated or threatened.\n\nA number of studies have looked at psychological reactance, providing empirical evidence for the behaviour; some key studies are discussed below.\n\nBrehm's 1981 study \"Psychological reactance and the attractiveness of unobtainable objects: sex differences in children's responses to an elimination of freedom\" examined the differences in sex and age in a child's view of the attractiveness of obtained and unobtainable objects. The study reviewed how well children respond in these situations and determined if the children being observed thought the \"grass was greener on the other side\". It also determined how well the child made peace with the world if they devalued what they could not have. This work concluded that when a child cannot have what they want, they experience emotional consequences of not getting it.<ref name=\"Brehm1981/2\">Brehm, Sharon S. (1981). Psychological reactance and the attractiveness of unobtainable objects: Sex differences in children's responses to an elimination of freedom. \"Sex Roles, Volume 7, Number 9\", 937–949</ref> \nIn this study the results were duplicated from a previous study by Hammock and J. Brehm (1966). The male subjects wanted what they could not obtain, however the female subjects did not conform to the theory of reactance. Although their freedom to choose was taken away, it had no overall effect on them.\n\nSilvia's 2005 study \"Deflecting reactance: The role of similarity in increasing compliance and reducing resistance\" concluded that one way to increase the activity of a threatened freedom is to censor it, or provide a threatening message toward the activity. In turn a \"boomerang effect\" occurs, in which people choose forbidden alternatives. This study also shows that social influence has better results when it does not threaten one's core freedoms. Two concepts revealed in this study are that a communicator may be able to increase the positive force towards compliance by increasing their credibility, and that increasing the positive communication force and decreasing the negative communication force simultaneously should increase compliance.\n\nMiller and colleagues concluded in their 2006 study, \"Identifying principal risk factors for the initiation of adolescent smoking behaviors: The significance of psychological reactance\", that psychological reactance is an important indicator in adolescent smoking initiation. Peer intimacy, peer individuation, and intergenerational individuation are strong predictors of psychological reactance. The overall results of the study indicate that children think that they are capable of making their own decisions, although they are not aware of their own limitations. This is an indicator that adolescents will experience reactance to authoritative control, especially the proscriptions and prescriptions of adult behaviors that they view as hedonically relevant.\n\nDillard & Shen have provided evidence that psychological reactance can be measured, in contrast to the contrary opinion of Jack Brehm, who developed the theory. In their work they measured the impact of psychological reactance with two parallel studies: one advocating flossing and the other urging students to limit their alcohol intake.\n\nThey formed several conclusions about reactance. Firstly reactance is mostly cognitive; this allows reactance to be measurable by self-report techniques. Also, in support of previous research, they conclude reactance is in part related to an anger response. This verifies Brehm's description that during the reactance experience one tends to have hostile or aggressive feelings, often aimed more at the source of a threatening message than at the message itself. Finally, within reactance, both cognition and affect are intertwined; Dillard and Shen suggest they are so intertwined that their effects on persuasion cannot be distinguished from each other.\n\nDillard and Shen's research indicates reactance can effectively be studied using established self-report methods. Furthermore, it provided a better understanding of reactance theory and its relationship to persuasive health communication.\n\nMiller and colleagues conducted their 2007 study \"Psychological reactance and promotional health messages: the effects of controlling language, lexical concreteness, and the restoration of freedom\" at the University of Oklahoma, with the primary goal being to measure the effects of controlling language in promotional health messages. Their research revisited the notion of restoring freedom by examining the use of a short postscripted message tagged on the end of a promotional health appeal. Results of the study indicated that more concrete messages generate greater attention than less concrete (more abstract) messages. Also, the source of concrete messages can be seen as more credible than the source of abstract messages. They concluded that the use of more concrete, low-controlling language, and the restoration of freedom through inclusion of a choice-emphasizing postscript, may offer the best solution to reducing ambiguity and reactance created by overtly persuasive health appeals.\n\n\n"}
{"id": "19079549", "url": "https://en.wikipedia.org/wiki?curid=19079549", "title": "Republic (Zeno)", "text": "Republic (Zeno)\n\nThe Republic () was a work written by Zeno of Citium, the founder of Stoic philosophy at the beginning of the 3rd century BC. Although it has not survived, it was his most famous work, and various quotes and paraphrases were preserved by later writers. The purpose of the work was to outline the ideal society based on Stoic principles, where virtuous men and women would live a life of simple asceticism in an equal society.\n\nWritten, it would seem, in conscious opposition to Plato's \"Republic\", Zeno's \"Republic\" (\"politeia\") outlined the principles of an ideal state written from the point of view of early Stoic philosophy. The work has not survived; but it was widely known in antiquity and more is known about it than any of his other works. Plutarch provides a summary of its intent:\nIt is not obvious from Plutarch's remarks whether he had read the work himself. One person who had read it was an otherwise unknown figure known as \"Cassius the Skeptic\", whose polemic written against Zeno's \"Republic\" is paraphrased by Diogenes Laërtius:\nFurther on, Laërtius makes some further remarks which also seem to be from the same work by Cassius:\nThese paraphrases by Cassius are not a neutral summary of the \"Republic\", his purpose seems to be to describe all the doctrines in the work which he found shocking. These include Zeno's denouncement of general education; his exhortation that only the virtuous can be regarded as true citizens; his view that men and women should wear the same clothes; and the idea that \"there should be a community of wives\", which in practice seems to have meant \"recognizing no other form of marriage than the union of the man who lives freely with a consenting woman\".\n\nA few other statements from the \"Republic\" are preserved by other writers. We learn from Laërtius that Zeno stated that the wise man will marry and produce children, and several writers mention Zeno's view that there is no need to build temples to the gods, \"for a temple not worth much is also not sacred, and nothing made by builders or workmen is worth much\". Athenaeus also preserves a quote on the need for a city to be built on the principle of love:\nZeno's \"Republic\" seems to have been viewed with some embarrassment by some of the later Stoics. This was not helped when Chrysippus, Zeno's most illustrious successor as the head of the Stoic school, wrote his own treatise \"On the Republic\" (probably a commentary on Zeno's work), in which (among many other things) he defended both incest and cannibalism. It is unlikely that Chrysippus urged the adoption of such behaviors; Chrysippus was probably responding to criticisms that in a society practicing free love, in which people often did not know who their relatives were, rare instances of incest would unintentionally occur; his discussion of cannibalism is probably connected with the Stoic contempt for dead bodies as an empty shell. Nevertheless, these points provided extra ammunition for those people who wished to attack both Zeno and Stoicism in general. Some blamed the influence which Crates of Thebes, the famous Cynic philosopher and teacher of Zeno, may have had when he wrote the \"Republic\": it was joked that Zeno \"had written it at the tail of the dog.\" By the 1st century BC, there was an attempt among the Stoics to downplay the involvement which Cynic philosophy had played in the development of early Stoicism; it was said that Zeno had been \"young and thoughtless\" when he wrote his \"Republic\". It was also said that \"by Zeno things were written which they [the Stoics] do not readily allow disciples to read, without their first giving proof whether or not they are genuine philosophers.\" Regardless of these views, it is clear that Zeno was one of the first philosophers in a long tradition begun by Plato of depicting an ideal society in order to understand ethical principles.\n\n\n"}
{"id": "42025094", "url": "https://en.wikipedia.org/wiki?curid=42025094", "title": "Res Ingold", "text": "Res Ingold\n\nRes Ingold (born 1954 in Burgdorf, Switzerland) is a Swiss contemporary artist.\nHe is known for his superfiction airline company Ingold Airlines.\nRes Ingold a professor at the Academy of Fine Arts Munich.\n\n\n\n\n\n"}
{"id": "57211036", "url": "https://en.wikipedia.org/wiki?curid=57211036", "title": "Safety of journalists", "text": "Safety of journalists\n\nSafety of journalists is the ability for journalists and media professionals to receive, produce and share information without facing physical or moral threats.\n\nJournalists can face violence and intimidation for exercising their fundamental right to freedom of expression. The range of threats they are confronted to include murder, kidnapping, hostage-taking, offline and online harassment, intimidation, enforced disappearances, arbitrary detention and torture. Women journalists also face specific dangers and are especially vulnerable to sexual assault, \"whether in the form of a targeted sexual violation, often in reprisal for their work; mob-related sexual violence aimed against journalists covering public events; or the sexual abuse of journalists in detention or captivity. Many of these crimes are not reported as a result of powerful cultural and professional stigmas.\"\n\nIncreasingly, journalists, and particularly women journalists, are facing abuse and harassment online, such as hate speech, cyber-bullying, cyber-stalking, docking, trolling, public shaming and intimidation and threats.\n\nFrom 2012 to 2016, UNESCO’s Director-General condemned the killing of 530 journalists, an average of two deaths per week. During the previous five-year period, 2007 to 2011, UNESCO recorded 316 killings. The year 2012 proved to be the deadliest year on record, with 124 journalists killed. The majority of journalists killed between 2012 and 2016 occurred in countries experiencing armed conflict, representing 56 per cent of overall killings.\n\nAccording to the Committee to Protect Journalists (CPJ) nearly 50 per cent of those whose death was confirmed to be related to their work as journalist were murdered, while 36 per cent were caught in the crossfire and 14 per cent killed while on dangerous assignment. According to the NGO, political groups were the most likely source of violence (36 per cent) in these killings, followed by military officials (22 per cent) and unknown sources (20 per cent).\n\nAs the reliance on freelance journalists by news organizations is increasing, a rising proportion of journalists killed have been freelance. UNESCO's study, \"World Trends in Freedom of Expression and Media Development Global Report 2017/2018\", has found that over the past five years, 113 freelance journalists were killed, representing 21 per cent of the total. Freelance journalists are particularly vulnerable, often working alone on stories, in dangerous environments, and without the same level of assistance and protection as staff-journalists.\n\nIn the same period, according to UNESCO data, the number of journalists targeted who work primarily online fluctuated significantly, but accounts for 14 per cent of journalists killed overall. Journalists and crew who work primarily in television experienced the most casualties (166), followed by those mainly working for print (142), radio (118), online (75) and those working across multiple platforms (29).\n\nTerrorism represents a direct and growing threat for journalists, which has taken the form of kidnappings, executions threats or hacking. At the end of the 1970s, the general policy of welcoming journalists into areas of guerrillas control changed. Organizations such as the Khmer Rouge in Cambodia, the Red Brigades in Italy, the Shining Path in Peru and the Armed Islamic Group (GIA) in Algeria targeted journalists, considering them as the auxiliaries of the powers they were combating, and thus as enemies. Between 1993 and 1997, more than 100 journalists and media workers were killed in Algeria. During the Lebanese Civil War (1975-1990), kidnapping international journalists became a common tactic.\n\nAccording to the Committee to Protect Journalists (CPJ), 40% of the journalists murdered in 2015 were killed by groups claiming adherence to radical Islam. International press correspondents, in particular, are considered potential hostages, or sacrificial lambs, whose execution is dramatized to serve terrorist propaganda. This happened to James Foley, Steven Sotloff (United States) and Kenji Goto (Japan), who were beheaded by Daesh.\n\nTrauma and the emotional impact of witnessing terrorism is also an issue for journalists, as they may experience anxiety, insomnia, irritation and physical problems such as fatigue or headaches. It can also lead to post-traumatic stress disorder, which can cause incapacitating feelings of horror, fear and despair. According to the study Eyewitness Media Hub of 2015, 40% of the journalists who were interviewed admitted that viewing video testimonies had had negative effects on their personal life.\n\nThe protection of sources and surveillance is one of the major issues in the coverage of terrorism in order to protect witnesses and interviewees against reprisals.\n\nThere is a continuing trend of impunity for crimes against journalists, with over 90% of cases of killings of journalists unresolved.The Special Rapporteur for Freedom of Expression of the Inter-American Commission on Human Rights, Edison Lanza considers impunity as a key obstacle to ensuring journalists' safety. Frank La Rue, UNESCO's former Assistant Director-General for Communication and Information considers that its \"root cause has to be attributed to lack of political will to pursue investigations, including for fear of reprisals from criminal networks in addition to inadequate legal frameworks, a weak judicial system, lack of resources allocated to law enforcement, negligence, and corruption\". \n\nUNESCO has set up a mechanism to monitor the status of judicial enquiries into the killings of journalists. Each year, UNESCO’s Director-General sends a request to Member States in which killings of journalists have occurred asking them to inform UNESCO of the status of ongoing investigations on each killing condemned by the Organization. UNESCO records the responses to these requests in a public report submitted every two years to the International Programme for the Development of Communication (IPDC) Council by the Director-General. The IPDC's 2016 Report is summarized in the \"Time to break the cycle of violence against journalists\" publication which highlights key findings, provides analysis of the killings, and of Member States responses. In 2017, UNESCO sent letters to 62 Member States requesting information on the status of unresolved cases that occurred between 2006 and 2016. Based on Member State responses, the percentage of resolved cases has remained low, at less than 10%. Cumulatively, since UNESCO began requesting information on the judicial follow-up to journalists’ killings condemned by the Director-General, the Organization has received information from 63 out of 75 Member States. The information covers 622 cases out of a total of 930 recorded by UNESCO between 2006 and 2016 (67 per cent).\n\nAt its 68th session in 2013, the United Nations General Assembly adopted resolution A/RES/68/163 proclaiming 2 November as the International Day to End Impunity for Crimes Against Journalists. The day acts to promote understanding of the broader issues that accompany impunity and to strengthen international commitment to ensuring a safe and enabling environment for journalists.\n\nThe United Nations Plan of Action on the Safety of Journalists and the Issue of Impunity supports Member States in the implementation of proactive initiatives to address the prevailing culture of impunity, such as judicial capacity building and the strengthening of monitor and prosecution mechanisms.\n\nAccording to data compiled by the Committee to Protect Journalists (CPJ), the imprisonment of journalists on charges relating to anti-state activities, criminal defamation, blasphemy, retaliation or on no charge at all, has reportedly continued to rise. In 2016, the CPJ reported that 259 journalists were imprisoned worldwide on a range of charges, the highest number since the non-governmental organization began keeping records in 1990.\n\nReporters Without Borders (RSF)—which tracks the imprisonment of citizen journalists, Netizens and media contributors, along with professional journalists—reported that 348 journalists were detained in 2016 on a range of charges, an increase of six per cent on 2015 figures. 2016 reportedly saw the proportion of women journalists detained more than double, with nearly half of those detained located in the Asia-Pacific region. The Western Europe and North America region has the highest number of journalists imprisoned, holding 34 per cent of imprisoned journalists worldwide with 73 journalists imprisoned in Turkey in 2017.\n\nRSF Secretary-General Christophe Deloire has stated that \"a full-blown hostage industry has developed in certain conflict zones\", with a 35 per cent increase in 2015 compared to the previous year of the number of media hostages held worldwide.\n\nThe Media Institute of Southern Africa has documented incidents of intimidation such as the torching of vehicles, physical assault and death threats. In parts of the Arab region, journalists and prominent writers have reportedly suffered death threats, been severely beaten and had travel restrictions imposed upon them. In the Asia Pacific region, the Southeast Asian Press Alliance has noted that in some insecure contexts, physical insecurity is reportedly so tenuous that some journalists have chosen to arm themselves.\n\nThreats and actual cases of violence and imprisonment, as well as harassment, are reported to have forced a large number of local journalists into exile each year. Between 1 June 2012 and 31 May 2015, at least 272 journalists reportedly went into exile for work-related persecution worldwide.\n\nOne survey conducted by PEN America of over 520 writers found that the majority reported concerns about government surveillance, which led to a reluctance to write, research or speak about certain topics. Almost a quarter of the writers had deliberately avoided certain topics in phone and email conversations, while 16 per cent had avoided writing or speaking about a certain topic and another 11 per cent had seriously considered it. A 2017 survey conducted by the Council of Europe of 940 journalists throughout 47 Member States found that in the face of physical violence or coercion, 15 per cent of journalists abandon covering sensitive, critical stories, while 31 per cent tone down their coverage and 23 per cent opt to withhold information in Europe.\n\nSurveillance, data storage capabilities and digital attack technologies are becoming more sophisticated, less expensive and more pervasive, making journalists increasingly vulnerable to digital attacks from both state and Non-state actors.\n\nIn a number of states across multiple regions, broadly defined legislative acts have been seen by some as working to silence digital dissent, prosecute whistle blowers and expand arbitrary surveillance across multiple digital platforms.\n\nIn late 2016, the International Press Institute launched the OnTheLine database, a project that aims to systematically monitor online harassment of journalists as a response to their reporting. As of July 2017, the project had collected 1,065 instances of online harassment in the two countries (Turkey and Austria) in which the project collected data. In Pakistan, the Digital Rights Foundation has launched the country's first cyber harassment helpline for journalists, which aims to provide legal advice, digital security support, psychological counselling and a referral system to victims. As of May 2017, the helpline handled a total of 563 cases since its launch six months earlier, with 63 per cent of calls received from women and 37 per cent from men. Research undertaken by Pew Research Center indicated that 73 per cent of adult internet users in the United States had seen someone be harassed in some way online and 40 per cent had personally experienced it, with young women being particularly vulnerable to sexual harassment and stalking.\n\nSimilar to their male counterparts, women journalists, whether they are working in an insecure context, or in a newsroom, face risks of attacks while exercising their profession but are also particularly vulnerable to attacks of a sexual nature such as, sexual harassment, sexual assault, or even rape. Such attacks can come from those attempting to silence their coverage but also from sources, colleagues and others. A 2014 global survey of nearly 1,000 women journalists, initiated by the International News Safety Institute (INSI) in partnership with the International Women's Media Foundation (IWMF) and with the support of UNESCO, found that nearly two-thirds of women who took part in the survey had experienced intimidation, threats or abuse in the workplace.\n\nIn the period from 2012 through 2016, UNESCO's Director-General denounced the killing of 38 women journalists, representing 7 per cent of all journalists killed. The percentage of journalists killed who are women is significantly lower than their overall representation in the media workforce. This large gender gap is likely partly the result of the persistent under-representation of women reporting from war-zones or insurgencies or on topics such as politics and crime.\n\nThe September 2017 report of the United Nations Secretary-General on the safety of journalists outlines a way forward for a gender-sensitive approach to strengthening the safety of women journalists. In 2016, the Council of Europe’s Committee of Ministers adopted Recommendation CM/Rec(2016)4 on the protection of journalism and safety of journalists and other media actors, in particular noting the gender-specific threats that many journalists face and calling for urgent, resolute and systematic responses. The same year, the IPDC Council requested the UNESCO Director-General's report to include gender-related information.\n\nAn analysis of more than two million tweets performed by the think tank Demos found that women journalists experienced approximately three times as many abusive comments as their male counterparts on Twitter.\n\nThe Guardian surveyed the 70 million comments recorded on its website between 1999 and 2016 (only 22,000 of which were recorded before 2006). Of these comments, approximately 1.4 million (approximately two per cent) were blocked for abusive or disruptive behavior. Of the 10 staff journalists who received the highest levels of abuse and ‘dismissive trolling’, eight were women.\n\nThe INSI and IWMF survey found that more than 25 per cent of ‘verbal, written and/or physical intimidation including threats to family and friends’ took place online.\n\nCountering online abuse is a significant challenge, and few legislative and policy frameworks exist on the international or national level to protect journalists from digital harassment.\n\nThe International Federation of Journalists and the South Asia Media Solidarity Network launched the Byte Back campaign to raise awareness and combat online harassment of women journalists in the Asia-Pacific region.\n\nThe Organization for Security and Cooperation in Europe (OSCE) organized an expert meeting titled ‘New Challenges to Freedom of Expression: Countering Online Abuse of Female Journalists’ which produced a publication of the same title that includes the voices of journalists and academics on the realities of online abuse of women journalists and how it can be combated.\n\nInternational legal instruments represent one of the key tools that can support the creation of an enabling environment for the safety of journalists. These include the Universal Declaration of Human Rights; the Geneva Conventions; the International Covenant on Civil and Political Rights; the United Nations Commission on Human Rights Resolution 2005/81; the United Nations Security Council Resolution 1738 (2006).\n\nThe safety of journalists and their role in promoting inclusive and sustainable societies has been recognized by the United Nations in the 2030 Agenda for Sustainable Development. Goal 16 outlines the promotion of peaceful and inclusive societies for sustainable development, provide access to justice for all and build effective and inclusive institutions at all levels.\n\nSince 2012 there has been a total of 12 UN resolutions on safety of journalists adopted by several UN organs.\n\nIn December 2015, Resolution A/70/125 of the General Assembly recognized serious threats to freedom of expression in the context of reviewing progress since the 2005 World Summit on the Information Society (WSIS). The Resolution called for the protection of journalists and media workers.\n\nThe United Nations Secretary General also produces reports on the safety of journalists and the issue of impunity (in 2014 A/69/268, 2015 A/HRC/30/68 respectively, 2017 A/72/290).\n\nSDG 16.10.1 relates to the number of verified cases of killings, kidnapping, enforced disappearance, arbitrary detention and torture of journalists, associated media personnel, trade unionists and human rights defenders. The indicator was decided upon by the United Nations Statistical Commission for tracking progress in the achievement of target 10.\n\nThe United Nations (UN) Plan of Action, coordinated by UNESCO, was elaborated to provide a comprehensive, coherent, and action-oriented UN-wide approach to the safety of journalists and the issue of impunity. Since its launch, it has become a guiding framework for activities in this area. Following its endorsement by the UN System Chief Executives Board for Coordination in 2012, the UN Plan of Action has been welcomed by the UN General Assembly, UNESCO and the Human Rights Council. Outside of the UN, it has been referred to by various regional bodies, and it has given impetus to and fostered a spirit of cooperation between the UN and various stakeholders in many countries.\n\nUNESCO celebrates World Press Freedom Day every year on 3 May during which the Guillermo Cano Prize is attributed to honor the work of an individual or an organization defending or promoting freedom of expression.\n\n\nUNESCO's IPDC developed the Journalists Safety Indicators (JSI) to \"pinpoint significant matters that show, or impact upon, the safety of journalists and the issue of impunity\". These indicators map the key features that can help assess safety of journalists, and determine whether adequate follow-up is given to crimes committed against them. Analysis based on the Journalists Safety indicators have been conducted in Guatemala (2013), Kenya (2016), Nepal (2016), Pakistan (2013–2014).\n\nThe OSCE Representative on Freedom of the Media performs an early warning function and provides rapid response to serious noncompliance with regard to free media and free expression. The Representative maintains direct contacts with authorities, media and civil society representatives and other parties and shares his/ her observations and recommendations with the OSCE participating States twice a year.\n\nIn May 2014, the Council of the European Union adopted the European Union Human Rights Guidelines on Freedom of Expression Online and Offline, which stated that the European Union would ‘take all appropriate steps to ensure the protection of journalists, both in terms of preventive measures and by urging effective investigations when violations occur’. In April 2014, the Council of Europe’s Committee of Ministers adopted a resolution on the protection of journalism and safety of journalists and other media actors, which called for concerted international efforts and led to the creation of an online platform for monitoring infringements of freedom of expression.\n\nThe Organization of American States (OAS) has played a proactive role in promoting the safety of journalists. In June 2017, the General Assembly of the OAS passed Resolution R86/17, which urged States ‘to implement comprehensive measures for prevention, protection, investigation and punishment of those responsible, as well as to put into action strategies to end impunity for crimes against journalists and share good practices'.\n\nThe Secretary-General of the Commonwealth has pledged support for the United Nations Plan of Action, working to promote journalist safety and institutional mechanisms that foster freedom of expression within member states across different regions.\n\nThe Centre for Freedom of the Media at the University of Sheffield launched the Journalists’ Safety Research Network (JSRN). The JSRN contributes to advancing academic cooperation on the safety of journalists by increasing research capacity, collaboration and knowledge sharing within the academic community.\n\nIn 2014, Columbia University, United States, established Columbia Global Freedom of Expression, which brings together international experts and activists with the university's faculty and students, in order to ‘survey, document and strengthen free expression’.\n\nIn 2015, the International Press Institute, Al Jazeera Media Network, Geneva Global Media and the Geneva Press Club presented the International Declaration and Best Practices on the Promotion of Journalists Safety. The declaration aims to reinforce and promote existing international obligations and mechanisms associated with the safety of journalists and contribute to the protection of their rights.\n\nThe same year, news media organizations joined forces with press freedom NGOs and journalists to launch the A Culture of Safety (ACOS) Alliance. The ACOS Alliance’s Freelance Journalist Safety Principles, a set of practices for newsrooms and journalists on dangerous assignments, have been endorsed by 90 organizations around the world. In addition, a network of safety officers in media companies expanded following the meeting ‘Media Organizations Standing Up for the Safety of Journalists’ held at UNESCO in February 2016.\n\nReporters without Borders (RSF) is an international non-governmental organization recognized as being of public utility in France. It aims to defend press freedom and the protection of journalism sources. It is mainly known for its annual ranking on the status of press freedom worldwide and monitoring of the crimes committed against journalists.\n\nThe Committee to Protect Journalists (CPJ) is a non-governmental organization that aims to \"defend the right of journalists to report the news without fear of reprisal\" and promote freedom of expression. The NGO monitors the imprisonment, disappearances and killings of journalists worldwide. The Organization informs on the status of freedom of the press and journalists safety in conflict countries and others, and helps journalists through services such as the emergencies response teams, the safety advisories and the security guide. CPJ's annual International Press Freedom Awards recognizes bravery for journalists from all around the world.\n"}
{"id": "13263024", "url": "https://en.wikipedia.org/wiki?curid=13263024", "title": "Self-envy", "text": "Self-envy\n\nSelf-envy is a psychoanalytic concept developed by Rafael López-Corvo who argues that it is crucial for the understanding of disorders of the self that are manifested in addictions, acting out, and inhibition of creativity. This concept is based on the use of object relations theory, that many psychoanalysts view as a fundamental instrument for examining the architecture of the internal world that describes behavior as influenced by the multiple interactions of early representations of self and other that operate in our inner selves. \n\nSelf-envy is produced by 'child part self-objects', self representations from early development that remain split off from the self and harbor destructive and envious feelings toward the creative aspects of the self and results from direct aggressive attacks by these childhood self-objects against the part of the self identified with a harmonious mother-father or parent-sibling relationship. \n\nThe internal dynamics of self-envy cause intense unconscious conflict, dissociation, and disturbances of the self, all of which underlie severe psychopathologies, such as repetitive destructive behavior, and even the living of seemingly \"normal\" but constricted lives. López-Corvo suggests that the psychoanalytic interpretation of such patients, for whom one part of the self is ruthlessly pitted against another, should address directly the phenomenon of a war within the internal self.\n\nW.C.M. Scott first used this term in 1975 in his article \"Remembering sleep and dreaming: self-envy and envy of dreams and dreaming\" in the International Review of Psychoanalysis, Vol. 2/3 (1975): 333-354. López-Corvo credits Scott in his paper \"About Interpretation of Self-Envy\" in the same journal (1992): 73, 719, and also in his book \"Self-Envy: Therapy and the Divided Inner World\", which also draws from Melanie Klein, Wilfred Bion, and other object-relations theorists in order to develop its more extended argument.\n\n\n"}
{"id": "8531860", "url": "https://en.wikipedia.org/wiki?curid=8531860", "title": "Social class in the United Kingdom", "text": "Social class in the United Kingdom\n\nThe social structure of the United Kingdom has historically been highly influenced by the concept of social class, which continues to affect British society today.\n\nBritish society, like its European neighbours and most societies in world history, was traditionally (before the Industrial Revolution) divided hierarchically within a system that involved the hereditary transmission of occupation, social status and political influence. Since the advent of industrialisation, this system has been in a constant state of revision, and new factors other than birth (for example, education) are now a greater part of creating identity in Britain.\n\nAlthough definitions of social class in the United Kingdom vary and are highly controversial, most are influenced by factors of wealth, occupation and education. Until recently the Parliament of the United Kingdom was organised on a class basis, with the House of Lords representing the hereditary upper-class and the House of Commons representing everybody else. The British monarch is usually viewed as being at the top of the social class structure.\n\nBritish society has experienced significant change since the Second World War, including an expansion of higher education and home ownership, a shift towards a service-dominated economy, mass immigration, a changing role for women and a more individualistic culture, and these changes have had a considerable impact on the social landscape. However, claims that the UK has become a classless society have frequently been met with scepticism. Research has shown that social status in the United Kingdom is influenced by, although separate from, social class.\nThe biggest current study of social class in the United Kingdom is the Great British Class Survey.\nPrior to the eighteenth century, one did not speak of class or classes. Older terms like estates, rank, and orders were predominant. This change in terminology corresponded to a general decrease in significance ascribed to hereditary characteristics, and increase in the significance of wealth and income as indicators of position in the social hierarchy.\n\nThe \"class system\" in the United Kingdom is widely studied in academia but no definition of the word \"class\" is universally agreed to. Some scholars may adopt the Marxist view of class where persons are classified by their relationship to means of production, as owners or as workers, which is the most important factor in that person's social rank. Alternatively, Max Weber developed a three-component theory of stratification under which \"a person’s power can be shown in the social order through their status, in the economic order through their class, and in the political order through their party. Besides these academic models, there are myriad popular explanations of class in Britain. In the work \"Class\", Jilly Cooper quotes a shopkeeper on the subject of bacon: \"When a woman asks for back I call her 'madam'; when she asks for streaky I call her 'dear'.\"\n\nThe United Kingdom never experienced the sudden dispossession of the estates of the nobility, which occurred in much of Europe after the French Revolution or in the early 20th century, and the British nobility, in so far as it existed as a distinct social class, integrated itself with those with new wealth derived from commercial and industrial sources more comfortably than in most of Europe. Opportunities resulting from consistent economic growth and the expanding British Empire also enabled some from much poorer backgrounds (generally men who had managed to acquire some education) to rise through the class system.\n\nThe historian David Cannadine sees the period around 1880 as a peak after which the position of the old powerful families declined rapidly, from a number of causes, reaching a nadir in the years after World War II, symbolised by the widespread destruction of country houses. However their wealth, if not their political power, has rebounded strongly since the 1980s, benefiting from greatly increased values of the land and fine art which many owned in quantity.\n\nMeanwhile, the complex British middle-classes had also been enjoying a long period of growth and increasing prosperity, and achieving political power at the national level to a degree unusual in Europe. They avoided the strict stratification of many Continental middle-classes, and formed a large and amorphous group closely connected at their edges with both the gentry and aristocracy and the labouring classes. In particular the great financial centre of the City of London was open to outsiders to an unusual degree, and continually expanding and creating new employment.\n\nThe British working class, on the other hand, was not notable in Europe for prosperity, and Early Modern British travellers often remarked on the high standard of living of the farm-workers and artisans of the Netherlands, though the peasantry in other countries such as France were remarked on as poorer than their English equivalents. Living standards certainly improved greatly over the period, more so in England than other parts of the United Kingdom, but the Industrial Revolution was marked by extremely harsh working conditions and poor housing until about the middle of the 19th century.\n\nAt the time of the formation of Great Britain in 1707, England and Scotland had similar class-based social structures. Some basic categories covering most of the British population around 1500 to 1700 are as follows.\n\nThe social grade classification created by the National Readership Survey over 50 years ago achieved widespread usage during the 20th century in marketing and government reports and statistics.\n\nThe UK Office for National Statistics (ONS) produced a new socio-economic classification in 2001. The reason was to provide a more comprehensive and detailed classification to take newer employment patterns into account.\n\nOn 2 April 2013 analysis of the results of a survey, which was conducted by the BBC in 2011 and developed in collaboration with academic experts, was published online in the journal \"Sociology\". The results released were based on a survey of 160,000 residents of the United Kingdom most of whom lived in England and described themselves as \"white.\" Class was defined and measured according to the amount and kind of economic, cultural, and social resources, \"capitals\", reported. Economic capital was defined as income and assets; cultural capital as amount and type of cultural interests and activities, and social capital as the quantity and social status of their friends, family and personal and business contacts. This theoretical framework was inspired by that of Pierre Bourdieu, who published his theory of social distinction in 1979.\n\nAnalysis of the survey revealed seven classes: a wealthy \"elite;\" a prosperous salaried \"middle class\" consisting of professionals and managers; a class of technical experts; a class of ‘new affluent’ workers, and at the lower levels of the class structure, in addition to an ageing traditional working class, a ‘precariat’ characterised by very low levels of capital, and a group of emergent service workers. The fracturing of the middle sectors of the social structure into distinguishable factions separated by generational, economic, cultural, and social characteristics was considered notable by the authors of the research.\n\nMembers of the elite class are the top 6% of British society with very high economic capital (particularly savings), high social capital, and very 'highbrow' cultural capital. Occupations such as chief executive officers, IT and telecommunications directors, marketing and sales directors; functional managers and directors, solicitors, barristers and judges, financial managers, higher education teachers, dentists, doctors and advertising and public relations directors were strongly represented. However, those in the established and 'acceptable' professions, such as academia, law and medicine are more traditional upper middle class identifiers, with IT and sales being the preserve of the economic if not social middle class.\n\nMembers of the established middle class, about 25% of British society, reported high economic capital, high status of mean social contacts, and both high highbrow and high emerging cultural capital. Well-represented occupations included electrical engineers, occupational therapists, midwives, environmental professionals, quality assurance and regulatory professionals, town planning officials, and special needs teaching professionals.\n\nThe technical middle class, about 6% of British society, shows high economic capital, very high status of social contacts, but relatively few contacts reported, and moderate cultural capital. Occupations represented include medical radiographers, aircraft pilots, pharmacists, natural and social science professionals and physical scientists, and business, research, and administrative positions.\n\nNew affluent workers, about 15% of British society, show moderately good economic capital, relatively poor status of social contacts, though highly varied, and moderate highbrow but good emerging cultural capital. Occupations include electricians and electrical fitters; postal workers; retail cashiers and checkout operatives; plumbers and heating and ventilation engineers; sales and retail assistants; housing officers; kitchen and catering assistants; quality assurance technicians.\n\nThe traditional working class, about 14% of British society, shows relatively poor economic capital, but some housing assets, few social contacts, and low highbrow and emerging cultural capital. Typical occupations include electrical and electronics technicians; care workers; cleaners; van drivers; electricians; residential, day, and domiciliary care\nThe emergent service sector, about 19% of British society, shows relatively poor economic capital, but reasonable household income, moderate social contacts, high emerging (but low highbrow) cultural capital. Typical occupations include bar staff, chefs, nursing auxiliaries and assistants, assemblers and routine operatives, care workers, elementary storage occupations, customer service occupations, and musicians.\n\nThe precariat, about 15% of British society, shows poor economic capital, and the lowest scores on every other criterion. Typical occupations include cleaners, van drivers, care workers, carpenters and joiners, caretakers, leisure and travel service occupations, shopkeepers and proprietors, and retail cashiers.\n\nThe term \"underclass\" is used to refer to those people who are \"chronically unemployed\", and in many instances have been for generations.\n\nTypical characteristics are:\n\nThere is a contention that there are homologies between the meaning context and tenor of the abusive popular word \"chav\" and the term \"underclass\" in media discourses: the obvious difference being the former relates to supposed dispositions of a social class in consumption and the later to difficulties of a social class in productive labour relations. The \"underclass\" has also been blamed for the 2011 England riots.\n\nTraditionally, these people would have worked as manual labourers. They would typically have left school as soon as legally permissible and not have been able to take part in higher education. Many would go on to work in semi-skilled and unskilled jobs on the assembly lines and machine shops of Britain's major car factories, steel mills, coal mines, foundries and textile mills in the highly industrialised cities in the West Midlands, North of England, South Wales and the Scottish Lowlands.\n\nHowever, since the mid-1970s and early-1980s, de-industrialisation has shattered many of these communities, resulting in a complete deterioration in quality of life and a reversal in rising living standards for the industrial working class. Many either dropped in status to the working poor or fell into permanent reliance on welfare dependence. Some dropped out altogether and joined the black market economy, while a limited few did manage to ascend to the lower middle-class.\n\nThe Mosaic 2010 groups where the proportion of residents in NRS social grade D was rated \"high\" in the 2010 Mosaic Index are \"Residents with sufficient incomes in right-to-buy social housing\" and \"Families in low-rise social housing with high levels of benefit need\".\n\nFictional stereotypes include: Andy Capp and Albert Steptoe, who is not only unaspirational himself; but crushes the aspirations of his son Harold.\n\nIt has been argued that with the decline in manufacturing and increase in the service sector, lower-paid office workers are effectively working-class. Call centres in particular, have sprung up in former centres of industry. However, since the early-2000s; there has been a trend for many call centres to close down in the UK and outsource their jobs to India, as part of cost-cutting measures.\n\nDuring the post-war era, White working-class Britons witnessed a big rise in their standard of living. As noted by Denys Blakeway:\n\n\"The White working-class have prospered hugely since the war. They have experienced unparalleled growth in disposable income and today they are now richer than their parents and grandparents could ever have imagined. There are shared values in White working-class culture but I think it is incredibly difficult to put your finger on exactly what it is that defines \"White working-class\" because a lot of them are shared by the middle-class, such as football and the pub.\"\n\nThis class of people would be in skilled industrial jobs or tradesmen, traditionally in the construction and manufacturing industry, but in recent decades showing entrepreneurial development as the stereotypical white van man, or self-employed contractors. These people would speak in regional accents and have completed craft apprenticeships rather than a university education. The only Mosaic 2010 group where the proportion of residents in NRS social grade C2 was rated \"high\" in the 2010 Mosaic Index is \"Residents with sufficient incomes in right-to-buy social housing\".\n\nThe British lower middle-class primarily consists of office workers. In the nineteenth century, the middle and lower middle classes were able to live in suburbs due to the development of horse-drawn omnibuses and railways. One radical Liberal politician (Charles Masterman), writing in 1909 used \"the Middle Classes\" and \"the suburbans\" synonymously. In the early twenty-first century, there were no Mosaic 2010 geodemographic groups where the proportion of residents in NRS social grade C1 was rated as \"high\" or \"low\" in the 2010 Index; it was rated as \"average\" in all Mosaic groups, whether these were of a suburban, rural, city or small-town nature.\n\nThey are typically employed in relatively unskilled service sector jobs (such as in retail sales or travel agents), or work in local government or are factory and other industrial building owners. Prior to the expansion in higher education from the 1960s onwards, members of this class generally did not have a university education.\n\nMembers of the lower middle-class typically speak in local accents, although relatively mild. Votes in this area are split and minority parties will have a stronger proportion. The comedy character Hyacinth Bucket is a satirical stereotype for this social group.\n\nThe middle-class in Britain often consists of people with tertiary education and may have been educated at either state or private schools.\n\nTypical jobs include: accountants, architects, solicitors, surveyors, social workers, teachers, managers, specialist IT workers, engineers, doctors, university-educated nurses and civil servants. Displays of conspicuous consumption are considered vulgar by them; instead they prefer to channel excess income into investments, especially property.\n\nMembers of the middle-class are often politically and socially engaged (a Mori poll in 2005 found 70% of grades AB voted at the 2005 general election compared to 54% of grades DE) and might be regular churchgoers (a YouGov poll in 2014 found 62% of those attending church at least once a month were NRS grades ABC1), might sit on local committees and governing boards or stand for political office. Education is greatly valued by the middle-classes: they will make every effort to ensure their children get offered a place at university; they may send their children to a private school, hire a home tutor for out of school hours so their child learns at a faster rate, or go to great lengths to get their children enrolled into good state or selective grammar schools; such as moving house into the catchment area.\n\nThey also value culture and make up a significant proportion of the book-buying and theatre-going public. They typically read broadsheet newspapers rather than tabloids. Politically, they are disproportionately supporters of the Liberal Democrats. The only Mosaic 2010 geodemographic type where the proportion of residents in NRS social grade B was rated as \"high\" in the 2010 index was \"People living in brand new residential developments\". The middle classes particularly of England are often popularly referred to as \"Middle England\".\n\nThe comedy character Margo Leadbetter is a satirical stereotype for this group, as is Jilly Cooper's Howard Weybridge.\n\nThe upper middle-class in Britain broadly consists of people who were born into families which have traditionally possessed high incomes, although this group is defined more by family background than by job or income. This stratum, in England, traditionally uses the Received Pronunciation dialect natively.\n\nThe upper middle-class are traditionally educated at independent schools, preferably one of the \"major\" or \"minor\" \"public schools\" which themselves often have pedigrees going back for hundreds of years and charge fees of as much as £33,000 per year per pupil (as of 2014).\n\nMany upper-middle-class families may have previous ancestry that often directly relates to the upper classes. Although not necessarily of the landowning classes – as a result, perhaps, of lack of a male heir – many families' titles/styles have not been inherited and therefore many families' past status became dissolved.\n\nAlthough such categorisations are not precise, popular contemporary examples of upper-middle-class people may include Boris Johnson, Catherine, Duchess of Cambridge, David Cameron, Helena Bonham Carter, (actress), Matthew Pinsent (athlete) and Jacob Rees-Mogg.\n\nThe British \"upper-class\" is statistically very small and consists of the peerage, gentry and hereditary landowners, among others. Those in possession of a hereditary peerage (but not a life peerage; for example, a dukedom, a marquessate, an earldom, a viscounty, or a barony/Scottish lord of parliament) are typically members of the upper class.\n\nTraditionally, upper-class children were brought up at home by a nanny for the first few years of their lives, and then home schooled by private tutors. From the late-nineteenth century, it became increasingly popular for upper-class families to mimic the middle-classes in sending their children to public schools, which had been predominantly founded to serve the educational needs of the middle-class. Nowadays, when children are old enough, they may attend a prep school or pre-preparatory school. Moving into secondary education, it is still commonplace for upper-class children to attend a public school, although it is not unheard of for certain families to send their children to state schools. Continuing education goals can vary from family to family; it may, in part, be based on the educational history of the family. In the past, both the British Army and Royal Navy have been the institutions of choice. Equally, the clergy, as well as academia, particularly within the arts and humanities divisions of Britain's oldest and most prestigious universities (Oxbridge), have been traditional career paths amongst the upper class - indeed until 1840 the majority Oxbridge graduates were destined for ordination.\n\nReceived Pronunciation, also known as RP or BBC English, was a term introduced as way of defining \"standard\" English, but the accent has acquired a certain prestige from being associated with the middle (and above) classes in the South East; the wealthiest part of England. Use of RP by people from the \"regions\" outside the South East can be indicative of a certain educational background, such as public school or elocution lessons.\n\n\"The Queen's English\" was once a synonym for RP. However, the Queen and some other older members of the aristocracy are now perceived as speaking in a way that is both more old-fashioned and higher class than \"general\" RP. Phoneticians call this accent \"Conservative Received Pronunciation\". The Queen's pronunciation has, however, also changed over the years. The results of the Harrington & al. study can be interpreted either as a change, in a range not normally perceptible, in the direction of the mainstream RP of a reference corpus of 1980s newsreaders, or showing showing subtle changes that might well have been influenced by the vowels of Estuary English.\n\nBBC English was also a synonym for RP; people seeking a career in acting or broadcasting once learnt RP as a matter of course if they did not speak it already. However, the BBC and other broadcasters are now much more willing to use – indeed desire to use – regional accents.\n\nLanguage and writing style have consistently been one of the most reliable indicators of class, although pronunciation did not become such an indicator until the late-nineteenth century. The variations between the language employed by the upper classes and non-upper classes has, perhaps, been best documented by linguistic Professor Alan Ross's 1954 article on U and non-U English usage, with \"U\" representing upper and upper middle class vocabulary of the time, and \"Non-U\" representing lower middle class vocabulary. The discussion was furthered in \"Noblesse Oblige\" and featured contributions from, among others, Nancy Mitford. The debate was revisited in the mid-1970s, in a publication by Debrett's called \"U and Non-U Revisited\". Ross also contributed to this volume, and it is remarkable to notice how little the language (amongst other factors) changed in the passing of a quarter of a century.\n\nIn England, the upper class or prestige dialect is almost always a form of RP; however, some areas have their\n\"own\" prestige dialect, distinct from both RP and the working-class dialect of the region.\n\nEngland has a wide variety of regional dialects for a small country, most of which have working-class or lower middle-class connotations:\n\nAn English citizen with arms registered in the College of Arms, or a Scottish citizen in the Lyon Court, can be referred to as armigerous. Any British citizen can apply for arms from their respective authority but only those of sufficient social standing would be granted arms. Arms in and of themselves are imperfectly aligned with social status, in that many of high status will have no right to arms whilst, on the other hand, those entitled to arms by descent can include branches of families from anywhere on the social scale.\n\nNevertheless, a right to bear arms under the Law of Arms is, by definition, linked either to the personal acquisition of social status, inspiring application for a personal grant of arms, or to descent from a person who did so in the past. Rightly or wrongly, therefore, the use of a coat of arms is linked to social prestige.\n\nIn the early twentieth century, it was argued by heraldic writers such as Arthur Charles Fox-Davies that only those with a right to a coat of arms could correctly be described (if men) as gentlemen and of noble status; however, even at the time this argument was controversial, and it was rejected by other writers such as Oswald Barron and Horace Round. In the Order of Malta, where proof of technical nobility is a requirement of certain grades of membership, British members must still base their proof upon an ancestral right to a coat of arms.\n\nIn 1941, George Orwell wrote that Britain was “the most class-ridden society under the sun.”\nIn an interview in 1975 Helmut Schmidt, the then Chancellor of West Germany stated that:\n\nLater in the same interview, Schmidt noted that \n\n\n\n\n\n"}
{"id": "962509", "url": "https://en.wikipedia.org/wiki?curid=962509", "title": "Solitude", "text": "Solitude\n\nSolitude is a state of seclusion or isolation, i.e., lack of contact with people. It may stem from bad relationships, loss of loved ones, deliberate choice, infectious disease, mental disorders, neurological disorders or circumstances of employment or situation (see castaway).\n\nShort-term solitude is often valued as a time when one may work, think or rest without being disturbed. It may be desired for the sake of privacy.\n\nA distinction has been made between solitude and loneliness. In this sense, these two words refer, respectively, to the joy and the pain of being alone.\n\nSymptoms from complete isolation, called sensory deprivation, often include anxiety, sensory illusions, or even distortions of time and perception. However, this is the case when there is no stimulation of the sensory systems at all, and not only lack of contact with people. Thus, by having other things to keep one's mind busy, this is avoided.\n\nStill, long-term solitude is often seen as undesirable, causing loneliness or reclusion resulting from inability to establish relationships. Furthermore, it might even lead to clinical depression. However, for some people, solitude is not depressing. Still others (e.g. monks) regard long-term solitude as a means of spiritual enlightenment. Indeed, marooned people have been left in solitude for years without any report of psychological symptoms afterwards.\n\nEnforced loneliness (solitary confinement) has been a punishment method throughout history. It is often considered a form of torture.\nIn contrast, some psychological conditions (such as schizophrenia and schizoid personality disorder) are strongly linked to a tendency to seek solitude. In animal experiments, solitude has been shown to cause psychosis.\n\nEmotional isolation is a state of isolation where one has a well-functioning social network but still feels emotionally separated from others.\n\nIn the last few years, however, researchers like Robert J. Coplan and Julie C. Bowker have bucked the trend that solitary practices and solitude are inherently dysfunctional and undesirable. In their groundbreaking work \"The Handbook of Solitude\", the authors note how solitude can allow for enhancements in self-esteem, generates clarity, and can be highly therapeutic. In the edited work, Coplan and Bowker invite not only fellow psychology colleagues to chime in on this issue, but they also invite a variety of other faculty from different disciplines to address the issue. Arguably the most interesting of these alternative views comes from Fong's chapter on how solitude is more than just a personal trajectory for one to take inventory on life; it also yields a variety of important sociological cues that allow the protagonist to navigate through society, even highly politicized societies. In the process, political prisoners in solitary confinement were examined to see how they concluded their views on society. Thus Fong, Coplan, and Bowker conclude that a person's experienced solitude generates immanent and personal content as well as collective and sociological content, depending on context.\n\nThere are both positive and negative psychological effects of solitude. Much of the time, these effects and the longevity is determined by the amount of time a person spends in isolation. The positive effects can range anywhere from more freedom to increased spirituality, while the negative effects are socially depriving and may trigger the onset of mental illness. While positive solitude is often desired, negative solitude is often involuntary or undesired at the time it occurs.\n\nThere are many benefits to spending time alone. Freedom is considered to be one of the benefits of solitude; the constraints of others will not have any effect on a person who is spending time in solitude, therefore giving the person more latitude in their actions. With increased freedom, a person’s choices are less likely to be affected by exchanges with others.\n\nA person's creativity can be sparked when given freedom. Solitude can increase freedom and moreover, freedom from distractions has the potential to spark creativity. In 1994, psychologist Mihaly Csikszentmihalyi found that adolescents who cannot bear to be alone often stop enhancing creative talents.\n\nAnother proven benefit to time given in solitude is the development of self. When a person spends time in solitude from others, they may experience changes to their self-concept. This can also help a person to form or discover their identity without any outside distractions. Solitude also provides time for contemplation, growth in personal spirituality, and self-examination. In these situations, loneliness can be avoided as long as the person in solitude knows that they have meaningful relations with others.\n\nToo much solitude is not always considered beneficial. Many of the negative effects have been observed in prisoners. Often, prisoners spend extensive time in solitude, where their behavior may worsen. Solitude can trigger physiological responses that increase health risks.\n\nNegative effects of solitude may also depend on age. Elementary age school children who experience frequent solitude may react negatively. This is largely because, often, solitude at this age is not something chosen by the child. Solitude in elementary-age children may occur when they are unsure of how to interact socially with others so they prefer to be alone, causing shyness or social rejection.\n\nWhile teenagers are more likely to feel lonely or unhappy when not around others, they are also more likely to have a more enjoyable experience with others if they have had time alone first. However, teenagers who frequently spend time alone do not have as good a global adjustment as those who balance their time of solitude with their social time.\n\nSolitude does not necessarily entail feelings of loneliness, and in fact may, for those who choose it with deliberate intent, be one's sole source of genuine pleasure. For example, in religious contexts, some saints preferred silence and found immense pleasure in their perceived uniformity with God. Solitude is a state that can be positively modified utilizing it for prayer allowing to \"be alone with ourselves and with God, to put ourselves in listening to his will, but also of what moves in our hearts, let purify our relationships; solitude and silence thus become spaces inhabited by God, and ability to recover ourselves and grow in humanity. \"\nThe Buddha attained enlightenment through uses of meditation, deprived of sensory input, bodily necessities, and external desires, including social interaction. The context of solitude is attainment of pleasure from within, but this does not necessitate complete detachment from the external world.\n\nThis is well demonstrated in the writings of Edward Abbey with particular regard to \"Desert Solitaire\" where solitude focused only on isolation from other people allows for a more complete connection to the external world, as in the absence of human interaction the natural world itself takes on the role of the companion. In this context, the individual seeking solitude does so not strictly for personal gain or introspection, though this is often an unavoidable outcome, but instead in an attempt to gain an understanding of the natural world as entirely removed from the human perspective as possible, a state of mind much more readily attained in the complete absence of outside human presence. In psychology, introverted individuals may require spending time away from people to recharge. Those who are simply socially apathetic might find it a pleasurable environment in which to occupy oneself with solitary tasks.\n\nIsolation in the form of solitary confinement is a punishment or precaution used in many countries throughout the world for prisoners accused of serious crimes, those who may be at risk in the prison population, those who may commit suicide, or those unable to participate in the prison population due to sickness or injury. Research has found that solitary confinement does not deter inmates from committing further violence in prison.\n\nPsychiatric institutions may institute full or partial isolation for certain patients, particularly the violent or subversive, in order to address their particular needs and to protect the rest of the recovering population from their influence.\n\n"}
{"id": "11098174", "url": "https://en.wikipedia.org/wiki?curid=11098174", "title": "Structure of observed learning outcome", "text": "Structure of observed learning outcome\n\nThe structure of observed learning outcomes (SOLO) taxonomy is a model that describes levels of increasing complexity in student's understanding of subjects. It was proposed by John B. Biggs and K. Collis and has since gained popularity.\n\nThe model consists of five levels of understanding \n\n"}
{"id": "39201489", "url": "https://en.wikipedia.org/wiki?curid=39201489", "title": "The Noun Project", "text": "The Noun Project\n\nThe Noun Project is a website that aggregates and catalogs symbols that are created and uploaded by graphic designers around the world. Based in Los Angeles, the project functions both as a resource for people in search of typographic symbols and a design history of the genre. \n\nThe Noun Project was co-founded by Sofya Polyakov, Edward Boatman, and Scott Thomas and is headed by Polyakov. Boatman recalled his frustration while working at an architectural firm at the lack of a central repository for common icons, \"things such as airplanes, bicycles and people.\" That idea morphed into a broader platform for visual communication. The site was launched on Kickstarter in December 2010, which raised more than $14,000 in donations, with symbols from the National Park Service and other sources whose content was in the public domain. Site design was by the firm Simple.Honest.Work, with mentoring from the Designer Fund.\n\nThe Noun Project has generated interest and new symbols by hosting a series of \"Iconathons\", the first of which was held in the summer of 2011. The sessions typically run five hours and include graphic designers, content experts, and interested volunteers, all working in small groups that focus on a specific issue, such as democracy, transportation or nutrition. The idea for the event came from Chacha Sikes, who was at the time a fellow at Code for America.\n\nContributors come from around the world. A 2012 New York Times story profiled one of them: Luis Prado, a graphic designer at the Washington State Department of Natural Resources, who uploaded 83 icons he had created for his agency, including a pruning saw, a logging truck and a candidate symbol for global warming, which he created when he couldn't find one online.\n\nThe site has four stylistic guidelines: include only the essential characteristics of the idea conveyed, maintain a consistent design style, favor an industrial look over a hand-drawn one, and avoid conveying personal opinions, feelings and beliefs. Contributors select a public domain mark or a Creative Commons attribution license, which enables others to use the symbol with attribution, free of charge. The attribution requirement can be waived upon payment of a nominal fee, which is split between the artist and The Noun Project. The founders envisioned the site as being primarily useful for designers and architects, but the range of users includes people with autism and amyotrophic lateral sclerosis, who sometimes favor a visual language, as well as business professionals incorporating the symbols into presentations.\n\n"}
{"id": "1010924", "url": "https://en.wikipedia.org/wiki?curid=1010924", "title": "Tittle", "text": "Tittle\n\nA tittle or superscript dot is a small distinguishing mark, such as a diacritic or the dot on a lowercase \"i\" or \"j\". The tittle is an integral part of the glyph of \"i\" and \"j\", but diacritic dots can appear over other letters in various languages. In most languages, the tittle of \"i\" or \"j\" is omitted when a diacritic is placed in the tittle's usual position (as í or ĵ), but not when the diacritic appears elsewhere (as į, ɉ).\n\nThe word \"tittle\" is rarely used. One notable occurrence is in the King James Bible at : \"For verily I say unto you, Till heaven and earth pass, one jot or one tittle shall in no wise pass from the law, till all be fulfilled\" (KJV). The quotation uses them as an example of extremely minor details. The phrase \"jot and tittle\" indicates that every small detail has received attention.\n\nIn the Greek original translated as English \"jot and tittle\" are found the words \"iota\" and \"keraia\" (). Iota is the smallest letter of the Greek alphabet (ι); the even smaller iota subscript was a medieval introduction. Alternatively, it may represent yodh (י), the smallest letter of the Hebrew and Aramaic alphabets. \"Keraia\" is a hook or serif, possibly referring to other Greek diacritics, or possibly to the hooks on Hebrew letters (ב) versus (כ) or cursive scripts for languages derived from Aramaic, such as Syriac, written in Serṭā (, 'short line'), or for adding explicit vowel marks such as crowns (e.g. the Vulgate \"apex\") known as Niqqud that developed with later scribal practices in the Torah. A keraia is also used in printing modern Greek numerals. In many abjads only consonants such as yodh in Hebrew have character forms; a word's phonetic pronunciation depends on unwritten or indistinct vowel markings such that many meanings can be rendered ambiguous or corrupted via oral transmission over time.\n\nA number of alphabets use dotted and dotless I, both upper and lower case.\n\nIn the modern Turkish alphabet, the absence or presence of a tittle distinguishes two different letters representing two different phonemes: the letter \"I\" / \"ı\", with the absence of a tittle also on the lower case letter, represents the close back unrounded vowel , while \"İ\" / \"i\", with the inclusion of a tittle even on the capital letter, represents the close front unrounded vowel . This practice has carried over to several other Turkic languages, like the Azerbaijani alphabet, Crimean Tatar alphabet, and Tatar alphabet.\n\nIn some of the Dene languages of the Northwest Territories in Canada, specifically North Slavey, South Slavey, Tłı̨chǫ and Dëne Sųłıné, all instances of \"i\" are undotted to avoid confusion with tone-marked vowels \"í\" or \"ì\". The other Dene language of the Northwest Territories, Gwich’in, always includes the tittle on lowercase \"i\".\n\nThere is only one letter I in Irish, but \"i\" is undotted in the traditional uncial Gaelic script to avoid confusion of the tittle with the \"buailte\" overdot found over consonants. Modern texts replace the \"buailte\" with an \"h\", and use the same antiqua-descendant fonts, which have a tittle, as other Latin-alphabet languages. However, bilingual road signs use dotless i in lowercase Irish text to better distinguish \"i\" from \"í\". The letter \"j\" is not used in Irish other than in foreign words.\n\nIn most Latin-based orthographies, the lowercase letter \"i\" loses its dot when a diacritical mark, such as an acute or grave accent, is placed atop the letter. However, the tittle is sometimes retained in some languages. In the Baltic languages, the lowercase letter \"i\" sometimes retains a tittle when accented. In Vietnamese in the 17th century, the tittle is preserved atop \"ỉ\" and \"ị\" but not \"ì\" and \"í\", as seen in the seminal \"quốc ngữ\" reference \"Dictionarium Annamiticum Lusitanum et Latinum\". In modern Vietnamese, a tittle can be seen in \"ì\", \"ỉ\", \"ĩ\", and \"í\" in cursive handwriting and some signage. This detail rarely occurs in computers and on the Internet, due to the obscurity of language-specific fonts. In any case, the tittle is always retained in \"ị\".\n\n\n\n"}
{"id": "850663", "url": "https://en.wikipedia.org/wiki?curid=850663", "title": "Trust (emotion)", "text": "Trust (emotion)\n\nIn a social context, trust has several connotations. Definitions of trust typically refer to a situation characterized by the following aspects: One party (trustor) is willing to rely on the actions of another party (trustee); the situation is directed to the future. In addition, the trustor (voluntarily or forcedly) abandons control over the actions performed by the trustee. As a consequence, the trustor is uncertain about the outcome of the other's actions; they can only develop and evaluate expectations. The uncertainty involves the risk of failure or harm to the trustor if the trustee will not behave as desired.\n\nTrust can be attributed to relationships between people. It can be demonstrated that humans have a natural disposition to trust and to judge trustworthiness that can be traced to the neurobiological structure and activity of a human brain. Some studies indicate that trust can be altered e.g. by the application of oxytocin.\n\nWhen it comes to the relationship between people and technology, the attribution of trust is a matter of dispute. The intentional stance demonstrates that trust can be validly attributed to human relationships with complex technologies. However, rational reflection leads to the rejection of an ability to trust technological artefacts.\n\nOne of the key current challenges in the social sciences is to re-think how the rapid progress of technology has impacted constructs such as trust. This is specifically true for information technology that dramatically alters causation in social systems.\n\nIn the social sciences, the subtleties of trust are a subject of ongoing research. In sociology and psychology the degree to which one party trusts another is a measure of belief in the honesty, fairness, or benevolence of another party. The term \"confidence\" is more appropriate for a belief in the competence of the other party. A failure in trust may be forgiven more easily if it is interpreted as a failure of competence rather than a lack of benevolence or honesty. In economics, trust is often conceptualized as reliability in transactions. In all cases trust is a heuristic decision rule, allowing the human to deal with complexities that would require unrealistic effort in rational reasoning.\n\nWhen it comes to trust, sociology is concerned with the position and role of trust in social systems. Interest in trust has grown significantly since the early eighties, from the early works of Luhmann, Barber and Giddens (see for a more detailed overview). This growth of interest in trust has been stimulated by on-going changes in society, characterised as late modernity and post-modernity.\n\nTrust is one of several social constructs, an element of the social reality. It does not exist outside of our vision of the other. This image can be real or imaginary, but it is this one which permits the creation of the Trust. Other constructs, frequently discussed together with trust, are: control, confidence, risk, meaning and power. Trust is naturally attributable to relationships between social actors, both individuals and groups (social systems). Because trust is a social construct, it is valid to discuss whether trust can be trusted (e.g.), i.e. whether social trust operates as expected.\n\nSviatoslav contends that society needs trust because it increasingly finds itself operating at the edge between confidence in what is known from everyday experience, and contingency of new possibilities. Without trust, all contingent possibilities should be always considered, leading to a paralysis of inaction. Trust can be seen as a bet on one of contingent futures, the one that may deliver benefits. Once the bet is decided (i.e. trust is granted), the trustor suspends his or her disbelief, and the possibility of a negative course of action is not considered at all. Because of it, trust acts as a reductor of social complexity, allowing for actions that are otherwise too complex to be considered (or even impossible to consider at all); specifically for cooperation. Sociology tends to focus on two distinct views: the macro view of social systems, and a micro view of individual social actors (where it borders with social psychology). Similarly, views on trust follow this dichotomy. Therefore, on one side the systemic role of trust can be discussed, with a certain disregard to the psychological complexity underpinning individual trust. The behavioural approach to trust is usually assumed while actions of social actors are measurable, leading to statistical modelling of trust. This systemic approach can be contrasted with studies on social actors and their decision-making process, in anticipation that understanding of such a process will explain (and allow to model) the emergence of trust.\n\nSociology acknowledges that the contingency of the future creates dependency between social actors, and specifically that the trustor becomes dependent on the trustee. Trust is seen as one of the possible methods to resolve such a dependency, being an attractive alternative to control. Trust is specifically valuable if the trustee is much more powerful than the trustor, yet the trustor is under social obligation to support the trustee.\n\nModern information technologies not only facilitated the transition towards post-modern society, but they also challenged traditional views on trust. Empirical studies confirms the new approach to the traditional question regarding whether technology artefacts can be attributed with trust. Trust is not attributable to artefacts, but it is a representation of trust in social actors such as designers, creators and operators of technology. Properties of technological artefacts form a message to determine trustworthiness of those agents.\n\nThe discussion about the impact of information technologies is still in progress. However, a conceptual re-thinking of technology-mediated social groups, or the proposition of a unifying socio-technical view on trust, from the perspective of social actors.\n\nIn psychology, trust is believing that the person who is trusted will do what is expected. It starts at the family and grows to others. According to the psychoanalyst Erik Erikson development of basic trust is the first state of psychosocial development occurring, or failing, during the first two years of life. Success results in feelings of security, trust, and optimism, while failure leads towards an orientation of insecurity and mistrust possibly resulting in attachment disorders.\n\nA person's dispositional tendency to trust others can be considered a personality trait and as such is one of the strongest predictors of subjective well-being. It has been argued that trust increases subjective well-being because it enhances the quality of one's interpersonal relationships, and happy people are skilled at fostering good relationships.\n\nTrust is integral to the idea of social influence: it is easier to influence or persuade someone who is trusting. The notion of trust is increasingly adopted to predict acceptance of behaviors by others, institutions (e.g. government agencies) and objects such as machines. However, once again perception of honesty, competence and value similarity (slightly similar to benevolence) are essential. There are three different forms of trust. Trust is being vulnerable to someone even when they are trustworthy; trustworthiness are the characteristics or behaviors of one person that inspire positive expectations in another person, and trust propensity being able to rely on people. Once trust is lost, by obvious violation of one of these three determinants, it is very hard to regain. Thus there is clear asymmetry in the building versus destruction of trust. Hence being and acting trustworthy should be considered the only sure way to maintain a trust level.\n\nIncreasingly much research has been done on the notion of trust and its social implications:\n\n\nIn addition to the social influence, in organizational settings, trust may have a positive influence on the behaviors, perceptions, and performances of a person. Trust has a circular relationship with organizational justice perceptions such that perceived justice leads to trust which, in turn, promotes future perceptions of justice. One factor that enhances trust in a human being is facial resemblance. Through digital manipulation of facial resemblance in a two-person sequential trust game, supporting evidence was found that having similar facial features (facial resemblance) enhanced trust in a subject’s respective partner. Though facial resemblance was shown to increase trust, facial resemblance had the effect of decreased sexual desire in a particular partner. In a series of tests, digitally manipulated faces were presented to subjects to be evaluated for attractiveness within the context of a long term or short term relationship. The results showed that within the context of a short term relationship, which is dependent on sexual desire, similar facial features caused a decrease in said desire. Within the context of a long term relationship, which is dependent on trust, similar facial features increased the attractiveness of an individual, leading one to believe that facial resemblance and trust have great effects on relationships. Structure often creates trust in a person that encourages them to feel comfortable and excel in the workplace. Working anywhere may be stressful and takes effort. By having a conveniently organized area to work on, concentration will increase as well as effort. Structure is not just a method of order. It increases trust and therefore makes a workplace manageable. A structured, ordered environment produces trust as one may contain increased cooperation and perform on a higher level.\n\nPeople may work together and achieve success through trust while working on projects that rely on each individual’s contribution.\n\nConversely, where trust is absent, projects can fail, especially if this lack of trust has not been identified and addressed. This is one facet of VPEC-T analysis: This thinking framework is used when studying information systems. Identifying and dealing with cases where information providers, information users, and those responsible for processing information do not trust one another can result in the removal of a risk factor for a project.\n\nOne's social relationship characterized by low trust and norms that discourage academic engagement are expected to be associated with low academic achievement. Individuals that are in relationships characterized by high levels of social trust are more apt to openly exchange information and to act with caring benevolence toward one another than those in relationships lacking trust.\n\nAn important key to treating sexual victimization of a child is the rebuilding of trust between parent and child. Failure for the adults to validate the sexual abuse contributes to the child's difficulty towards trusting self and others. Trust is often affected by the erosion of a marriage. Children of divorce do not exhibit less trust in mothers, partners, spouses, friends, and associates than their peers of intact families. The impact of parental divorce is limited to trust in the father.\n\nThe social identity approach explains trust in strangers as a function of group-based stereotypes or in-group favouring behaviours based on salient group memberships. With regard to ingroup favoritism, people generally think well of strangers but expect better treatment from in-group members in comparison to out-group members. This greater expectation then translates into a higher propensity to trust an in-group rather than out-group member. It has been pointed out that it is only advantageous to form such expectations of an in-group stranger if they too know the group membership of the recipient.\n\nThere is considerable empirical activity related to the social identity approach. Allocator studies have frequently been employed to understand group-based trust in strangers. They may be operationalised as unilateral or bilateral relationships of exchange. General social categories such as university affiliation, course majors, and even ad-hoc groups have been used to distinguish between in-group and out-group members. In unilateral studies of trust, the participant would be asked to choose between envelopes containing money that was previously allocated by an in-group or out-group member. They would have had no prior or future opportunities for interaction, simulating Brewer’s notion that group membership was sufficient in bringing about group-based trust and hence cooperation. Participants could expect an amount ranging from nothing to the maximum value an allocator could give out. In bilateral studies of trust have employed an investment game devised by Berg and colleagues where individuals could choose to give a portion or none of their money to another. Any amount given would be tripled and the receiver would then decide on whether they would return the favour by giving money back to the sender. Trusting behaviour on the part of the sender and the eventual trustworthiness of the receiver was exemplified through the giving of money.\n\nThe above empirical research has demonstrated that when group membership is made salient and known to both parties, trust is granted more readily to in-group members than out-group members. This occurred even when the in-group stereotype was comparatively less positive than an out-group’s (e.g. psychology versus nursing majors), in the absence of personal identity cues, and when participants had the option of a sure sum of money (i.e. in essence opting out of the need to trust a stranger). In contrast, when only the recipient was made aware of group membership trust becomes reliant upon group stereotypes. The group with the more positive stereotype was trusted (e.g. one’s university affiliation over another), even over that of the in-group (e.g. nursing over psychology majors). Another reason for in-group favouring behaviours in trust could be attributed to the need to maintain in-group positive distinctiveness, particularly in the presence of social identity threat. It should also be noted that trust in out-group strangers increased when personal cues to identity were revealed.\n\nSome philosophers argue that trust is more than a relationship of reliance. Philosophers such as Annette Baier have made a difference between trust and reliance by saying that trust can be betrayed, whilst reliance can only be disappointed (Baier 1986, 235). Carolyn McLeod explains Baier's argument by giving the following examples: we can rely on our clock to give the time, but we do not feel betrayed when it breaks, thus, we cannot say that we trusted it; we are not trusting when we are suspicious of the other person, because this is in fact an expression of distrust (McLeod 2006). Thus, trust is different from reliance in the sense that a truster accepts the risk of being betrayed.\n\nThe definition of trust as a belief in something or a confident expectation about something leads to eliminate the notion of risk from the definition, because it does not include whether the expectation or belief is favorable or unfavorable. For example, to have an expectation of a friend arriving to dinner late because she has habitually arrived late for the last fifteen years, is a confident expectation (whether or not we agree with her annoying late arrivals.) The trust is not about what we wish for, rather it is in the consistency of the data of our habits. As a result, there is no risk or betrayal because the data now exists as collective knowledge.\n\nTrust in economics is treated as an explanation for a difference between actual human behaviour and the one that can be explained by the individual desire to maximize one's utility. In economic terms, trust can provide an explanation of a difference between Nash equilibrium and the observed equilibrium. Such an approach can be applied to individuals as well as societies.\n\nTrust is also seen as an economic lubricant, reducing the cost of transactions between parties, enabling new forms of cooperation and generally furthering business activities; employment and prosperity. This observation created a significant interest in considering trust as a form of social capital and has led research into closer understanding of the process of creation and distribution of such capital. It has been claimed that higher level of social trust is positively correlated with economic development. Even though the original concept of 'high trust' and 'low trust' societies may not necessarily hold, it has been widely accepted and demonstrated that social trust benefits the economy and that a low level of trust inhibits economic growth.\n\nTheoretical economical modelling demonstrated that the optimum level of trust that a rational economic agent should exhibit in transactions is equal to trustworthiness of the other party. Such a level of trust leads to efficient market. Trusting less lead to the loss of economic opportunities, trusting more leads to unnecessary vulnerabilities and potential exploitation.\n\nEconomics is also interested in quantifying trust, usually in monetary terms. The level of correlation between increase in profit margin or decrease in transactional cost can be used as indicators of economic value of trust.\n\nEconomic 'trust games' are popularly used to empirically quantify trust in relationships under laboratory conditions. There are several games and game-like scenarios related to trust that have been tried, with certain preferences to those that allow to estimate confidence in monetary terms. Games of trust are designed in a way that their Nash equilibrium differ from Pareto optimum so that no player alone can maximise his own utility by altering his selfish strategy without cooperation while cooperating partners can benefit.\n\nThe classical version of the game of trust has been described in as an abstracted investment game, using the scenario of an investor and a broker. Investor can invest a fraction of his money, and broker can return only part of his gains. If both players follow their economical best interest, the investor should never invest and the broker will never be able to re-pay anything. Thus the flow of money flow, its volume and character is attributable entirely to the existence of trust.\n\nThe game can be played as one-off, or as a repetitive one, between the same or different sets of players, to distinguish between a general propensity to trust and trust within particular relationships. Several other variants of this game exist. Reversing rules lead to the game of distrust, pre-declarations can be used to establish intentions of players, while alterations to the distribution of gains can be used to manipulate perception of both players. The game can be also played by several players on the closed market, with or without information about reputation.\n\nOther interesting games are e.g. binary-choice trust games, the gift-exchange game and various other forms of social games. Specifically games based on the Prisoner's Dilemma are popularly used to link trust with economic utility and demonstrate the rationality behind reciprocity.\n\nThe work of Rachel Botsman is also very important about collaboration economy.\n\nThe popularisation of e-commerce opened the discussion of trust in economy to new challenges while at the same time elevating the importance of trust, and desire to understand customer decision to trust. For example, inter-personal relationship between the buyer and the seller has been dis-intermediated by the technology, and had to be improved upon. Alternatively, web sites could be made to convince the buyer to trust the seller, regardless of seller's actual trustworthiness (e.g.) . Reputation-based systems improved on trust assessment by allowing to capture the collective perception of trustworthiness, generating significant interest in various models of reputation.\n\nIn systems, a trusted component has a set of properties which another component can rely on. If A trusts B, this means that a violation in those properties of B might compromise the correct operation of A. Observe that those properties of B trusted by A might not correspond quantitatively or qualitatively to B’s actual properties. This happens when the designer of the overall system does not take the relation into account. In consequence, trust should be placed to the extent of the component’s trustworthiness. The trustworthiness of a component is thus, not surprisingly, defined by how well it secures a set of functional and non-functional properties, deriving from its architecture, construction, and environment, and evaluated as appropriate.\n\n\n"}
{"id": "38560990", "url": "https://en.wikipedia.org/wiki?curid=38560990", "title": "Two wrongs make a right", "text": "Two wrongs make a right\n\nIn rhetoric and ethics, \"two wrongs make a right\" and \"two wrongs don't make a right\" are phrases that denote philosophical norms. \"Two wrongs make a right\" has been considered as a fallacy of relevance, in which an allegation of wrongdoing is countered with a similar allegation. Its antithesis, \"two wrongs don't make a right\", is a proverb used to rebuke or renounce wrongful conduct as a response to another's transgression.\n\nThe phrase \"two wrongs infer one right\" appears in a poem dated to 1734, published in \"The London Magazine\".\n\nThis is an informal fallacy that occurs when assuming that, if one wrong is committed, then another wrong will cancel it out.\n\n\nIf Speaker B believes in the maxim \"the law should be followed,\" then his unstated premise is that breaking the law (or the wrong) is justified, as long as the other party also does so. Yet if Speaker B believes the maxim \"it is acceptable to break the law to wrong those who also break the law\" he is committing no logical fallacy. From the conversation above, it is impossible to know which Speaker B believes.\n\nThis fallacy is often used as a red herring, or an attempt to change or distract from the issue. For example:\n\nEven if President Roberts lied in his Congressional testimony, this does not establish a precedent that makes it acceptable for President Williams to do so as well. (At best, it means Williams is \"no worse than\" Roberts.) By invoking the fallacy, the contested issue of lying is ignored.\n\nThe \"tu quoque\" fallacy is a specific type of \"two wrongs make a right\". Accusing another of not practicing what they preach, while appropriate in some situations, does not in itself invalidate an action or statement that is perceived as contradictory.\n\nCommon use of the term, in the realm of business ethics, has been criticized by scholar Gregory S. Kavka writing in the \"Journal of Business Ethics\". Kavka refers back to philosophical concepts of retribution by Thomas Hobbes. He states that if something supposedly held up as a moral standard or common social rule is violated enough in society, then an individual or group within society can break that standard or rule as well since this keeps them from being unfairly disadvantaged. As well, in specific circumstances violations of social rules can be defensible if done as direct responses to other violations. For example, Kavka states that it is wrong to deprive someone of their property, but it is right to take property back from a criminal who takes another's property in the first place. He also states that one should be careful not to use this ambiguity as an excuse recklessly to violate ethical rules.\n\nConservative journalist Victor Lasky wrote in his book \"It Didn't Start With Watergate\" that while two wrongs don't make a right, if a set of immoral things are done and left unprosecuted, this creates a legal precedent. Thus, people who do the same wrongs in the future should rationally expect to get away as well. Lasky uses as an analogy the situation between John F. Kennedy's wiretapping of Martin Luther King, Jr. (which led to nothing) and Richard Nixon's actions in Watergate (which Nixon thought would also lead to nothing).\n\nTwo wrongs don't make a right is a proverb that contradicts this fallacy – a wrongful action is not a practical or morally appropriate way to correct or cancel a previous wrongful action.\n\n\n"}
{"id": "23400470", "url": "https://en.wikipedia.org/wiki?curid=23400470", "title": "Weighted Voronoi diagram", "text": "Weighted Voronoi diagram\n\nIn mathematics, a weighted Voronoi diagram in \"n\" dimensions is a special case of a Voronoi diagram. The Voronoi cells in a weighted Voronoi diagram are defined in terms of a distance function. The distance function may specify the usual Euclidean distance, or may be some other, special distance function. Usually, the distance function is a function of the generator points' weights.\n\nThe multiplicatively weighted Voronoi diagram is defined when the distance between points is multiplied by positive weights. In the plane under the ordinary Euclidean distance, the multiplicatively weighted Voronoi diagram is also called circular Dirichlet tessellation and its edges are circular arc and straight line segments. A Voronoi cell may be non-convex, disconnected and may have holes. This diagram arises, e.g., as a model of crystal growth, where crystals from different points may grow with different speed. Since crystals may grow in empty space only and are continuous objects, a natural variation is the crystal Voronoi diagram, in which the cells are defined somewhat differently.\n\nThe additively weighted Voronoi diagram is defined when positive weights are subtracted from the distances between points. In the plane under the ordinary Euclidean distance this diagram is also known as the hyperbolic Dirichlet tessellation and its edges are hyperbolic arc and straight line segments.\n\nThe power diagram is defined when weights are added to the squared Euclidean distance. It can also be defined using the power distance defined from a set of circles.\n\n"}
