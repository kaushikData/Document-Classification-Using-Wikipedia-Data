{"id": "27658583", "url": "https://en.wikipedia.org/wiki?curid=27658583", "title": "AeroVironment Global Observer", "text": "AeroVironment Global Observer\n\nThe AeroVironment Global Observer is a concept for a high-altitude, long endurance unmanned aerial vehicle, designed by AeroVironment (AV) to operate as a stratospheric geosynchronous satellite system with regional coverage.\n\nTwo Global Observer aircraft, each flying for up to a week at an altitude of , could alternate coverage over any area on the earth, providing a platform for communications relays, remote sensing, or long-term surveillance. In addition to flying above weather and above other conventional aircraft, operation at this altitude permits communications and sensor payloads on the aircraft to service an area on the surface of the earth up to in diameter, equivalent to more than of coverage. Global Observer may offer greater flexibility than a satellite and longer duration than conventional manned and unmanned aircraft.\n\nThe Global Observer Joint Capabilities Technology Demonstration (JCTD) program had the goal of helping solve the capability gap in persistent ISR and communications relay for the US military and homeland security. The Global Observer JCTD demonstrated a new stratospheric, extreme endurance UAS that could be transitioned for post-JCTD development, extended user evaluation, and fielding. The program was a joint effort with the U.S. Department of Defense, Department of Homeland Security, and AeroVironment that started in September 2007, to culminate in a Joint Operational Utility Assessment (JOUA) in 2011.\n\nThe program provided for the system development, production of two aircraft, development flight testing, and JOUA with ISR and communications relay payload. The flight testing and JOUA was conducted at the Air Force Flight Test Center at Edwards Air Force Base, California. The primary objectives of the Global Observer JCTD Program were:\n\n\n\n\nHigh-altitude, long endurance unmanned aerial vehicles, such as Global Observer, may enable several capabilities that enable rapid and effective actions or countermeasures:\n\nA Global Observer prototype, called \"Odyssey,\" flew in May 2005. It had a , one-third the size of the planned full-sized version, and ran solely on hydrogen fuel-cells powering electric motors that drove eight propellers, flying the aircraft for several hours. The JCTD started in September 2007. In August 2010, Aerovironment announced that the full-sized Global Observer wing had passed wing load testing. The 53 m (175 ft) all-composite wing, which comes in five sections and is designed to maximize wing strength while minimizing weight, had loads applied to it that approximated the maximum loads it is designed to withstand during normal flight, turbulence and maneuvers. In its third year of testing, the demonstrator had also undergone ground and taxi tests as well as taken a \"short hop\" lifting off the ground briefly during taxiing.\n\nThe Global Observer performed its first flight on 5 August 2010, taking off from Edwards AFB and reaching an altitude of for one hour. The flight was performed using battery power.The aircraft completed initial flight testing, consisting of multiple low-altitude flights, at Edwards AFB in August and September 2010. This phase used batteries to power the hybrid-electric aircraft and approximate full aircraft weight and center of gravity for flight control, performance, and responsiveness evaluation. Following this, the program team installed and ground tested the aircraft's hydrogen-fueled generator and liquid hydrogen fuel tanks which will power it for up to a week in the stratosphere.\n\nThe first flight of the Global Observer using hydrogen fuel occurred on 11 January 2011, reaching an altitude of for four hours. On 1 April 2011, Global Observer-1 (GO-1), the first aircraft to be completed, crashed 18 hours into its 9th test flight. AeroVironment said it was undergoing flight test envelope expansion and had been operating for nearly twice the endurance and at a higher altitude than previous flights when the crash occurred. At the time, the second aircraft developed as part of the JCTD program was nearing completion at a company facility; the $140 million program was originally scheduled for completion in late 2011, but the crash delayed this by a year. AeroVironment was looking for sources of incremental funding to provide a bridge between the demonstration and a future procurement program.\n\nIn December 2012, the Pentagon closed the development contract for the Global Observer, the reason being the crash in April 2011. The Global Observer was used as a technology demonstration, not a program for a functioning aircraft. In April 2013, the Pentagon stated that no service or defense agency had advocated for it to be a program. AeroVironment is currently in possession of the second prototype Global Observer. On 6 February 2014, AeroVironment announced that it had teamed with Lockheed Martin to sell the Global Observer to international customers. The partnership is focused around building \"atmospheric satellite systems\" around the UAV. The Global Observer may compete for orders with the Boeing Phantom Eye liquid hydrogen-powered long endurance UAV.\n\n\n\n"}
{"id": "52526594", "url": "https://en.wikipedia.org/wiki?curid=52526594", "title": "Arched-hill symbol", "text": "Arched-hill symbol\n\nThe Arched-hill symbol is a fairly current symbol on ancient Coinage of India. There some variations at to the number of the hills depicted, or the symbol surmounting the hill, such as a crescent or a star.\n\nIt is thought that the three-arched hill symbol was initiated during the Maurya Empire (3rd–2nd century BCE). Later, in coins from Taxila dated from 220 BCE, the three-arched symbol appears regularly, and from 185 BCE is regularly associated with the animal figures of the elephant and the lion. In contrast, the Nandipada is generally associated with the zebu bull. On coins of the Shunga period, the three-arched hill can appear among a multitude of other symbols, such as the Nandipada, the tree-in-railing, the elephant, or the empty cross.\n\nThe symbol is generally considered a representation of a Buddhist Chaitya or a Meru. It has also been argued that it was the imperial symbol of the Mauryas. The symbol however, appears in many post-Mauryan contexts as seen with the coins of Taxila and the Shungas.\n\n"}
{"id": "25572219", "url": "https://en.wikipedia.org/wiki?curid=25572219", "title": "Ashura protests", "text": "Ashura protests\n\nThe 2009 Ashura protests were a series of protests which occurred on 27 December 2009 in Iran against the outcome of the June 2009 Iranian presidential election, which demonstrators claim was rigged. The demonstrations were part of the 2009 Iranian election protests and were the largest since June. In December 2009, the protests saw an escalation in violence.\n\nIn response to this protest, pro-government protesters hold a rally in a \"show of force\" three days later on December 30 (9 Dey) to condemn Green Movement protester.\n\nIrregularities during the 2009 Iranian presidential election caused resentment among many Iranians. While post-election protests were mostly peaceful, some violence erupted, leading to clashes between security forces and protesters, while some outspoken political dissenters were detained.\n\nHowever, dissenters continued to speak out against the Government, leading to further protests in December 2009. On 19 December 2009, the Grand Ayatollah Hossein Ali Montazeri, who had become a \"spiritual leader\" of the opposition, died. Montazeri's funeral, held on 21 December in the city of Qom, was attended by a large gathering of people and clashes ensued between security forces and mourners, leading on to further demonstrations in Qom and Isfahan. On 26 December, a paramilitary Basij force subordinate to the Iranian Revolutionary Guard stormed a mosque in Tehran where scholar and former President Mohammad Khatami was speaking. This was followed by continued clashes in Tehran in which \"Jaras\", a news media of the critics, estimated eight to ten people had died.\n\nPrior to Ashura, Mohsen Kadivar said he could not \"rule out the possibility\" of state intervention in the planned protests.\n\nOn 27 December, demonstrations in several cities continued into the holy day of Ashura the climax of Muharram, the month of mourning. Protesters in Tehran gathered \"From Imam Hussain Square to Freedom Square\", \"from east to west along Revolution Street\", and it was on this day that \"the political and religious symbology of Iran's Islamic regime was turned on its head\". The protesters made another symbolic move- a \"symbolic journey from a square named after its most revered hero toward a monument dedicated to freedom, along a street called Revolution.\"\n\nSeyed Ali Mousavi, the 35-year-old nephew of Mir-Hossein Mousavi, was among those killed in the violence. Later, it was reported that his body had disappeared, precluding the possibility of a quick burial, while state sources indicated that an autopsy was being performed. Mousavi was buried on 30 December.\n\nSimilar protests took place in other Iranian cities including Isfahan, Najafabad, Shiraz, Mashhad, Arak, Tabriz, Babol, Ardabil and Orumieh. Four people were reportedly killed in Tabriz, in north western Iran on 27 December, and one in Shiraz in the south of Iran. Access for international news media has been severely restricted by the Iranian government.\n\nState controlled media initially denied any deaths, though it was indicated on 28 December that 15 had died, including ten \"well-known anti-revolutionary terrorists\". According to the official news agency of the Islamic Republic of Iran, Tehran's Safety Services said that \"Nine residential buildings, 9 vehicles, 7 shops, 2 banks and 3 power stations were set on fire [by anti-government protesters].\" On 30 December, counter-rallies staged and organized by the government at various cities, including Tehran, Qom, Arak, Shiraz and Isfahan called for the death of the protesters, with government workers receiving the day off work in order to attend the demonstrations.\n\nLolagar mosque in Tehran was set into fire by the \"rioters\", according to the State TV of Iran leading to death of \"few\" people in mosque. Security forces allegedly opened fire on the day of Ashura, the Shiite holy day \"symbolically about justice\", a day on which any kind of violence is forbidden. Security forces initially denied reports of deaths and the Police Chief, Azizollah Rajabzadeh, stated that the police had not been armed, however, state television later acknowledged fatalities. Although official sources in Iran denied involvement of security forces in killing of protesters, at least one amateur video shows, the security truck which was deliberately running over the protesters. Other evidence says that security forces were armed with guns and shot at protesters, including one amateur video showing a plainclothes security force directly shooting at protesters.\n\nVandalism was reported by the Iranian government, with Tehran's Safety Services saying that \"Nine residential buildings, 9 vehicles, 7 shops, 2 banks and 3 power stations were set on fire.\" by the anti-government forces\n\nAmong the hundreds of people arrested in the aftermath of the Ashura demonstrations area are prominent lawyers, journalists, clerics and politicians, as well as family members of prominent human rights activists and reformist politicians. Some notable people arrested in the aftermath of the protests include:\n\nAccording to Ibrahim Moussawi, associate professor of Lebanese University and head of Hizbullah's media relations, the incident damaged \"public relations\" of the Green Movement with Iranian citizenry more than all events as the acts of the protesters on that day including \"applauding, whistling, and engaging in other cheerful displays,\" was \"widely\" seen as violation of a \"red line\" and targeting Husayn ibn Ali and Ashura commemoration itself. Various society groups including \"marej-'e taqlid, the society of Iranian doctors, university student groups, the Iranian Parliament, Oil Industry Workers, the Iranian Women's Culture and Education Society, the Society of Iranian Teachers, the Iranian Professors Society, provincial governors and municipalities and bazaars\" expressed their condemnation and many of them publicly asked for the \"prosecution of the opposition leaders\".\n\nMany people are set to stand trial for taking part in the protests. At least one person arrested in connection with the protest, a university lecturer Abdolreza Ghanbari living in Pakdasht, has been accused of \"moharebeh,\" (an Islamic term meaning \"warring against God\") and sentenced to death.\n\nThe governments of Canada, France, Germany, the United Kingdom and the United States are among those who have condemned the violence. US President Barack Obama openly criticized the Iranian government's violent crackdown on the protests in a speech and declared \"The decision of Iran's leaders to govern through fear and tyranny will not continue.\" Russia's Foreign Ministry expressed concern at the violence. It encouraged \"a compromise on the basis of the law, and also to take political efforts to prevent a further escalation of the confrontation.\"\n\nVenezuela condemned what it called Western governments' interference in Iran's internal affairs.\n\nSince the protest coincided with Ashura, the commemoration observed by Shi'as for the death of Imam Hussein, the third Imam of Shia's who were killed by the order of Umayyad Caliph Yazid I, protesters deliberately blended their political message with the Ashura's religious one in this protest. They alternated anti-government slogans with ancient cries of mourning for Imam Hussein.\n\n\n\n"}
{"id": "2797431", "url": "https://en.wikipedia.org/wiki?curid=2797431", "title": "Asian psychology", "text": "Asian psychology\n\nAsian psychology is a branch of cultural psychology that studies psychological concepts as they relate to Asian culture. Psychologists studying these issue are often aligned with cross-cultural psychology.\n\nAs pointed out by Shinobu Kitayama, professor of psychology and Director of the Culture & Cognition Program at the University of Michigan, culture can have a profound impact on the way people think about and perceive the world around them. East Asians may think differently from Westerners (See also, Cultural differences in self-concept and Self-construal.) Kitayama proposed that unlike the traditional American point-of-view which accentuates the importance of one's self and makes oneself independent, an Asian will instead feel more interdependent.\n\nAsian psychologists wanted to have an expanding role in the science of psychology, but felt limited due to the heavy western influence. Predominant figures in Asian psychology are Quicheng Jing in China, Hiroshi Azuma in Japan, Ku-Shu Yang in Taiwan, and Durganand Sinha in India.\n\nThe \"Asian American Journal of Psychology\"® is the official publication of the Asian American Psychological Association and is dedicated to research, practice, advocacy, education, and policy within Asian American psychology. The Journal publishes empirical, theoretical, methodological, and practice oriented articles and book reviews covering topics relevant to Asian American individuals and communities, including prevention, intervention, training, and social justice. Particular consideration is given to empirical articles using quantitative, qualitative, and mixed methodology.\n\nThe \"Asian Journal of Social Psychology\" stimulates research and encourages academic exchanges for the advancement of social psychology in Asia. It publishes theoretical and empirical papers by Asian scholars and those interested in Asian cultures and societies.\nThe \"Asian Journal of Social Psychology\" is partly funded by a Grant-in-Aid for Publication of Scientific Research Results from the Japan Society for the Promotion of Science.\n\nAAPA was founded in 1972 and is the largest organization of faculty, students, researchers, and practitioners interested in Asian American psychology. Our members and initiatives have positively impacted psychological treatment, education, training, research, policy and social justice advocacy, through research dissemination, organizational policy statements and collaboration with other psychological organizations for publications, training initiatives, and disseminating resources for serving Asian American communities.\n\nOver the years, the contribution to the study of psychology was done mostly by US European psychologists, however, in recent years this has been changing. More Asian countries than ever before are contributing to psychology at an ever increasing rate.\n\n\n"}
{"id": "8208639", "url": "https://en.wikipedia.org/wiki?curid=8208639", "title": "Brenda Dervin", "text": "Brenda Dervin\n\nBrenda Dervin, currently a professor of communication at Ohio State University, is a researcher in the communication and library and information science fields. Her research about information seeking and information use led to the development of the sense-making methodology (Ross, Nilsen, & Dewdney, 2003, p. 93). Dervin received a bachelor's degree in journalism and home economics from Cornell University, with a minor in philosophy of religion, and her M.S. and PhD degrees in communication research from Michigan State University. In 1986 she acted as the first president of the International Communication Association. Dervin reviews articles and also is on editorial boards for communication and library and information science journals.\n\n\n\n"}
{"id": "51426", "url": "https://en.wikipedia.org/wiki?curid=51426", "title": "Cantor's diagonal argument", "text": "Cantor's diagonal argument\n\nIn set theory, Cantor's diagonal argument, also called the diagonalisation argument, the diagonal slash argument or the diagonal method, was published in 1891 by Georg Cantor as a mathematical proof that there are infinite sets which cannot be put into one-to-one correspondence with the infinite set of natural numbers.\nSuch sets are now known as uncountable sets, and the size of infinite sets is now treated by the theory of cardinal numbers which Cantor began.\n\nThe diagonal argument was not Cantor's first proof of the uncountability of the real numbers, which appeared in 1874.\nHowever, it demonstrates a powerful and general technique that has since been used in a wide range of proofs, including the first of Gödel's incompleteness theorems and Turing's answer to the \"Entscheidungsproblem\". Diagonalization arguments are often also the source of contradictions like Russell's paradox and Richard's paradox.\n\nIn his 1891 article, Cantor considered the set \"T\" of all infinite sequences of binary digits (i.e. each digit is zero or one).\nHe begins with a constructive proof of the following theorem:\nThe proof starts with an enumeration of elements from \"T\", for example:\nNext, a sequence \"s\" is constructed by choosing the 1st digit as complementary to the 1st digit of \"s\" (swapping 0s for 1s and vice versa), the 2nd digit as complementary to the 2nd digit of \"s\", the 3rd digit as complementary to the 3rd digit of \"s\", and generally for every \"n\", the \"n\" digit as complementary to the \"n\" digit of \"s\". For the example above, this yields:\nBy construction, \"s\" differs from each \"s\", since their \"n\" digits differ (highlighted in the example).\nHence, \"s\" cannot occur in the enumeration.\n\nBased on this theorem, Cantor then uses a proof by contradiction to show that:\nThe proof starts by assuming that \"T\" is countable.\nThen all its elements can be written as an enumeration \"s\", \"s\", … , \"s\", … .\nApplying the previous theorem to this enumeration produces a sequence \"s\" not belonging to the enumeration. However, this contradicts \"s\" being an element of \"T\" and therefore belonging to the enumeration. This contradiction implies that the original assumption is false. Therefore, \"T\" is uncountable.\n\nThe interpretation of Cantor's result will depend upon one's view of mathematics. To constructivists, the argument shows no more than that there is no bijection between the natural numbers and \"T\". It does not rule out the possibility that the latter are subcountable. In the context of classical mathematics, this is impossible, and the diagonal argument establishes that, although both sets are infinite, there are actually \"more\" infinite sequences of ones and zeros than there are natural numbers.\n\nThe uncountability of the real numbers was already established by Cantor's first uncountability proof, but it also follows from the above result. To prove this, an injection will be constructed from the set \"T\" of infinite binary strings to the set R of real numbers. Since \"T\" is uncountable, the image of this function, which is a subset of R, is uncountable. Therefore, R is uncountable. Also, by using a method of construction devised by Cantor, a bijection will be constructed between \"T\" and R. Therefore, \"T\" and R have the same cardinality, which is called the \"cardinality of the continuum\" and is usually denoted by formula_1 or formula_2.\n\nAn injection from \"T\" to R is given by mapping strings in \"T\" to decimals, such as mapping \"t\" = 0111… to the decimal 0.0111…. This function, defined by , is an injection because it maps different strings to different numbers. \n\nInstead of mapping 0111… to the decimal 0.0111…, it can be mapped to the base \"b\" number: 0.0111…. This leads to the family of functions: . The functions are injections, except for . This function will be modified to produce a bijection between \"T\" and R.\n\nA generalized form of the diagonal argument was used by Cantor to prove Cantor's theorem: for every set \"S\", the power set of \"S\"—that is, the set of all subsets of \"S\" (here written as P(\"S\"))—has a larger cardinality than \"S\" itself. This proof proceeds as follows:\n\nLet \"f\" be any function from \"S\" to P(\"S\"). It suffices to prove \"f\" cannot be surjective. That means that some member \"T\" of P(\"S\"), i.e. some subset of \"S\", is not in the image of \"f\". As a candidate consider the set:\n\nFor every \"s\" in \"S\", either \"s\" is in \"T\" or not. If \"s\" is in \"T\", then by definition of \"T\", \"s\" is not in \"f\"(\"s\"), so \"T\" is not equal to \"f\"(\"s\"). On the other hand, if \"s\" is not in \"T\", then by definition of \"T\", \"s\" is in \"f\"(\"s\"), so again \"T\" is not equal to \"f\"(\"s\"); cf. picture.\nFor a more complete account of this proof, see Cantor's theorem.\n\nThis result implies that the notion of the set of all sets is an inconsistent notion. If \"S\" were the set of all sets then P(\"S\") would at the same time be bigger than \"S\" and a subset of \"S\".\n\nRussell's Paradox has shown us that naive set theory, based on an unrestricted comprehension scheme, is contradictory. Note that there is a similarity between the construction of \"T\" and the set in Russell's paradox. Therefore, depending on how we modify the axiom scheme of comprehension in order to avoid Russell's paradox, arguments such as the non-existence of a set of all sets may or may not remain valid.\n\nThe diagonal argument shows that the set of real numbers is \"bigger\" than the set of natural numbers (and therefore, the integers and rationals as well). Therefore, we can ask if there is a set whose cardinality is \"between\" that of the integers and that of the reals. This question leads to the famous continuum hypothesis. Similarly, the question of whether there exists a set whose cardinality is between |\"S\"| and |P(\"S\")| for some infinite \"S\" leads to the generalized continuum hypothesis.\n\nAnalogues of the diagonal argument are widely used in mathematics to prove the existence or nonexistence of certain objects. For example, the conventional proof of the unsolvability of the halting problem is essentially a diagonal argument. Also, diagonalization was originally used to show the existence of arbitrarily hard complexity classes and played a key role in early attempts to prove P does not equal NP.\n\nThe above proof fails for W. V. Quine's \"New Foundations\" set theory (NF). In NF, the naive axiom scheme of comprehension is modified to avoid the paradoxes by introducing a kind of \"local\" type theory. In this axiom scheme,\n\nis \"not\" a set — i.e., does not satisfy the axiom scheme. On the other hand, we might try to create a modified diagonal argument by noticing that\n\n\"is\" a set in NF. In which case, if P(\"S\") is the set of one-element subsets of \"S\" and \"f\" is a proposed bijection from P(\"S\") to P(\"S\"), one is able to use proof by contradiction to prove that |P(\"S\")| < |P(\"S\")|.\n\nThe proof follows by the fact that if \"f\" were indeed a map \"onto\" P(\"S\"), then we could find \"r\" in \"S\", such that \"f\"({\"r\"}) coincides with the modified diagonal set, above. We would conclude that if \"r\" is not in \"f\"({\"r\"}), then \"r\" is in \"f\"({\"r\"}) and vice versa.\n\nIt is \"not\" possible to put P(\"S\") in a one-to-one relation with \"S\", as the two have different types, and so any function so defined would violate the typing rules for the comprehension scheme.\n\n\n"}
{"id": "507305", "url": "https://en.wikipedia.org/wiki?curid=507305", "title": "Cement chemist notation", "text": "Cement chemist notation\n\nCement chemist notation (CCN) was developed to simplify the formulas cement chemists use on a daily basis. It is a shorthand way of writing the chemical formula of oxides of calcium, silicon, and various metals.\n\nThe main oxides present in cement (or in glass and ceramics) are abbreviated in the following way:\n\nFor the sake of mass balance calculations, hydroxides present in hydrated phases found in hardened cement paste, such as in portlandite, Ca(OH), must first be converted into oxide and water.\n\nTo better understand the conversion process of hydroxide anions in oxide and water, it is necessary to consider the autoprotolysis of the hydroxyl anions; it implies a proton exchange between two OH, like in a classical acid-base reaction:\nor also,\n\nFor portlandite this gives thus the following mass balance:\n\nThus portlandite can be written as CaO · HO or CH.\n\nThese oxides are used to build more complex compounds. The main crystalline phases described hereafter are related respectively to the composition of: \n\nFour main phases are present in the clinker and in the non-hydrated Portland cement. They are formed at high temperature (1,450 °C) in the cement kiln and are the following:\n\nThe four compounds referred as CS, CS, CA and CAF are known as the main crystalline phases of Portland cement. The phase composition of a particular cement can be quantified through a complex set of calculation known as the Bogue formula.\n\nHydration products formed in hardened cement pastes (also known as HCPs) are more complicated, because many of these products have nearly the same formula and some are solid solutions with overlapping formulas. Some examples are given below:\n\nThe hyphens in C-S-H indicate a calcium silicate hydrate phase of variable composition, while 'CSH' would indicate a calcium silicate phase, CaHSiO.\n\nThe cement chemist notation is not restricted to cement applications but is in fact a more general notation of oxide chemistry applicable to other domains than cement chemistry \"sensu stricto\".\n\nFor instance, in ceramics applications, the kaolinite formula can also be written in terms of oxides, thus the corresponding formula for kaolinite, \n\nis\n\nor in CCN\n\nAlthough not a very developed practice in mineralogy, some chemical reactions involving silicate and oxide in the melt or in hydrothermal systems, and silicate weathering processes could also be successfully described by applying the cement chemist notation to silicate mineralogy.\n\nAn example could be the formal comparison of belite hydration and forsterite serpentinisation dealing both with the hydration of two structurally similar earth -alkaline silicates, CaSiO and MgSiO, respectively.\n\n\n\nThe ratio Ca/Si (C/S) and Mg/Si (M/S) decrease from 2 for the dicalcium and dimagnesium silicate reagents to 1.5 for the hydrated silicate products of the hydration reaction. In other term, the C-S-H or the serpentine are less rich in Ca and Mg respectively. This is why the reaction leads to the elimination of the excess of portlandite (Ca(OH)) and brucite (Mg(OH)), respectively, out of the silicate system, giving rise to the crystallization of both hydroxides as separate phases.\n\nThe rapid reaction of belite hydration in the setting of cement is formally \"chemically analogue\" to the slow natural hydration of forsterite (the magnesium end-member of olivine) leading to the formation of serpentine and brucite in nature. However, the kinetic of hydration of poorly crystallized artificial belite is much swifter than the slow conversion/weathering of well crystallized Mg-olivine under natural conditions.\n\nThis comparison suggests that mineralogists could probably also benefit from the concise formalism of the cement chemist notation in their works.\n\n\n"}
{"id": "10070819", "url": "https://en.wikipedia.org/wiki?curid=10070819", "title": "Claims to a crown", "text": "Claims to a crown\n\nThere are five ways in which a person may lay claim to a crown. The below ordering is based on strength of case and possession.\n\n\n"}
{"id": "2339023", "url": "https://en.wikipedia.org/wiki?curid=2339023", "title": "Constitutional right", "text": "Constitutional right\n\nA constitutional right can be a prerogative or a duty, a power or a restraint of power, recognized and established by a sovereign state or union of states. All constitutional rights are expressly stipulated and written in a consolidated national constitution, which is the supreme law of the land, meaning that any other laws which are in contradiction with it are considered unconstitutional and thus regarded as invalid. Usually any constitution defines the structure, functions, powers, and limits of the national government and the individual freedoms, rights, and obligations which will be protected and enforced when needed by the national authorities.\n\nNowadays, most countries have a written constitution comprising similar or distinct constitutional rights. Since 1789, along with the Constitution of the United States of America (hereinafter U.S. Constitution), which is the oldest and shortest written constitution still in force, around 220 other similar constitutions were adopted around the world by independent states.\n\nIn the late 18th century, Thomas Jefferson predicted that a period of 20 years will be the optimal time for any Constitution to still be in force since \"the earth belongs to the living, and not to the dead.\" Coincidence or not, according to recent studies the average life expectancy of any new written constitution is around 19 years. However, a great number of constitutions do not exceed more than 10 years and around 10% do not last more than 1 year, as it was the case of the French Constitution from 1971 and not only.\n\nThe most common reasons for these continuous changes are the political desire of an immediate outcome and the scarcity of time devoted to the constitutional drafting process. A study from 2009 showed that the average time allocated for the drafting part of the process is around 16 months however there were also some extreme cases registered. For example, the Myanmar 2008 Constitution was secretly drafted for more than 17 years, whereas on the other extreme, like the case of the Japan's 1946 Constitution, the bureaucrats drafted everything in no more than a week.Nevertheless, the record for the shortest overall process of drafting, adoption and ratification of a national Constitution belongs to the Romania's 1938 Constitution which installed a royal dictatorship in less than a month. Studies on the matter showed as a general conclusion that usually non-democracies where the registered extreme cases where the constitution-making process either takes too long or is incredibly short. Important not to forget or make any confusions about it is that constitutional rights are not a specific characteristic of democratic countries, but also non-democratic countries have Constitutions, such as North Korea for example, which officially grants every citizen, among other rights, the freedom of expression.\n\nOther coded set of laws have existed before the first Constitutions were developed having some similar purpose and functions, like the United Kingdom's 1215 Magna Carta or the Virginia Bill of Rights of 1776.\n\nOn September 17, 1787 the United States Constitution was signed during the Constitutional Convention (United States) which took place at the Pennsylvania State House in Philadelphia, now the Independence Hall.\n\nThe oldest person signing the Constitution was Benjamin Franklin, one of the founding fathers, being 81 years old at the time and requesting assistance during the process, whereas the youngest one was Jonathan Dayton from New Jersey, being only 26 years old. James Madison and George Washington were the only two signers that later became Presidents of the United States.\n\nPerhaps the first fascinating fact about the U.S. Constitution is its length, containing only 4.400 words and thus being the shortest and oldest written Constitution in the world. Only on December 15, 1791 the Bill of Rights comprising the first 10 Amendments became part of the U.S. Constitution. Later on, other 17 Amendments were added. Thus, the U.S Constitution is summing a total of 27 Amendments and 7 Articles. During all this time, only one amendment overturned a previous one, more precisely the twenty-first Amendment ratified on December 5, 1933 repealed the prohibition of alcohol established by the eighteenth Amendment on January 16, 1919.\n\nThe provisions providing for rights under the Bill of Rights were originally binding upon only the federal government. In time, most of these provisions became binding upon the states through selective incorporation into the due process clause of the 14th Amendment. When a provision is made binding on a state, a state can no longer restrict the rights guaranteed in that provision.\n\nExamples of provisions made binding upon the states are the Second Amendment to the United States Constitution which was made \"fully applicable\" by being Incorporated with the 14th Amendment in 2010, see, McDonald vs. City of Chicago; the 6th Amendment's guarantee of a right to confrontation of witnesses, known as the Confrontation Clause, and the various provisions of the 1st Amendment, guaranteeing the freedoms of speech, the press, government and assembly.\n\nFor example, the Fifth Amendment protects the right to grand jury proceedings in federal criminal cases. However, because this right was not selectively incorporated into the due process clause of the 14th amendment, it is not binding upon the states. Therefore, persons involved in state criminal proceedings as a defendant have no federal constitutional right to grand jury proceedings. Whether an individual has a right to a grand jury becomes a question of state law.\n\nThe content of each Article and Amendment of the U.S. Constitution is easy to predict since they start with a suggestive title. For example, the First Amendment guarantees the freedom of religion, speech, and the press along with the rights of assembly and petition, the Second Amendment the right to bear arms and so on. However, in order to be easier to distinguish, the legal professionals have divided the constitutional rights into two categories: process rights and substantive rights. Whereas, the process rights refer to the powers and obligations of the government with respect to individuals, the substantive rights, more diverse than the process ones, incorporate the individual freedoms granted and protected by the national government.\n\nEach of the United States has its own governing Constitution. The States Constitutions are usually longer and written in much more detail than the U.S. Constitution. For example, the Alabama Constitution has more than 600 pages and the New Jersey Constitution of 1947 is three times longer than the U.S. Constitution. The reason for this difference between the federal Constitution and the states Constitutions is what Justice Brennan called 'the new judicial federalism'. meaning that rights granted by the States Constitutions can be broader than those comprised by the federal Constitution but not narrowed.\n\nState constitutions cannot reduce legal protections afforded by the federal charter, but they can provide additional protections. \"California v. Ramos\", 463 U.S. 992, 1014, 103 S.Ct. 3446, 77 l.Ed.2d 1171 (1983). Even where the text of a state constitution matches verbatim that of the federal constitution, the state document may be held to provide more to the citizen. State constitutional rights can also include those entirely unaddressed in the federal constitution, such as the right to adequate education or the right to affordable housing.\n\nMany other democratic nations have followed the US model in enshrining certain rights in their constitutions. Countries whose written constitutions include a bill of rights include Germany, India and Japan.\n\nThe United Kingdom, as it has an uncodified constitution, does not have a constitutional bill of rights, although the Human Rights Act 1998 fulfills a similar role.\n\nThe European Convention of Human Rights applies in those nations which are members of the Council of Europe. Persons who have experienced Convention-infringing human rights violations on the territory of ECHR-signatory nations can appeal to the European Court of Human Rights.\n\nIn authoritarian regimes there are generally few or no guaranteed inalienable rights; alternatively, such rights may exist but be unobserved in practice (as was generally the case in the former Francoist Spain).\n"}
{"id": "57954001", "url": "https://en.wikipedia.org/wiki?curid=57954001", "title": "Dawson casting", "text": "Dawson casting\n\nDawson casting is an observed cultural phenomenon and movie trope in film and television where many of the actors appear, and in reality are, much older than the characters they are portraying. The concept is observable in teen dramas such as \"Glee\", and \"Gossip Girl\", and \"Pretty Little Liars\" where grown adults are cast to play teenage characters. The term was originally circulated on the Internet in response to the casting choices in \"Dawson's Creek\".\n\nIt has been suggested on several occasions by critics that Dawson casting has several negative implications, specifically for adolescents. These commonly include accusations of unrealistic beauty standards, negative body image, low self-esteem, and general mental health problems, especially in regards to one's self-perception.\n\nA clinical psychologist, Barbara Greenberg, told \"Teen Vogue\" that casting twenty-year-old actors for the roles of high-school students can worsen the struggles of adolescents, stating \"It can give the message that they’re supposed to look good all the time\" adding \"That leads to all kinds of body-image and social-comparison issues\".\n\n"}
{"id": "4606682", "url": "https://en.wikipedia.org/wiki?curid=4606682", "title": "Diagonal morphism", "text": "Diagonal morphism\n\nIn category theory, a branch of mathematics, for any object formula_1 in any category formula_2 where the product formula_3 exists, there exists the diagonal morphism \n\nsatisfying \n\nwhere formula_7 is the canonical projection morphism to the formula_8-th component. The existence of this morphism is a consequence of the universal property which characterizes the product (up to isomorphism). The restriction to binary products here is for ease of notation; diagonal morphisms exist similarly for arbitrary products. The image of a diagonal morphism in the category of sets, as a subset of the Cartesian product, is a relation on the domain, namely equality.\n\nFor concrete categories, the diagonal morphism can be simply described by its action on elements formula_9 of the object formula_1. Namely, formula_11, the ordered pair formed from formula_9. The reason for the name is that the image of such a diagonal morphism is diagonal (whenever it makes sense), for example the image of the diagonal morphism formula_13 on the real line is given by the line which is a graph of the equation formula_14. The diagonal morphism into the infinite product formula_15 may provide an injection into the space of sequences valued in formula_16; each element maps to the constant sequence at that element. However, most notions of sequence spaces have convergence restrictions which the image of the diagonal map will fail to satisfy.\n\n"}
{"id": "5896051", "url": "https://en.wikipedia.org/wiki?curid=5896051", "title": "Digital Life", "text": "Digital Life\n\nDigital Life is a research and educational program about radically rethinking of the human-computer interactive experience. It integrates digital world (information & services) and physical world (physical objects/environment). It makes interfaces more responsive and proactive (objects & environments monitor user and (proactively) present information & services relevant to user’s current needs/interests)\n\nThe program is to use information technology to augment physical environments and objects around the people that can draw attention. When one is walking around town, for example, the system points out buildings/places of particular interest to a user. The program is also to augment reality in order to provide a composite view for the participants: a mix of a real scene with the virtual scene that augments the digital environment with interactive information.\n\nThe Program was originally initiated by MIT Media Lab as: Digital Life is a multi-sponsor, Lab-wide research consortium that conducts basic research on technologies and techniques that spur expression as well as social and economic activity. They first explore the design and scalability of agile, grassroots communications systems that incorporate a growing understanding of emergent social behaviors in a digital world; the second considers a cognitive architecture that can support many features of “human intelligent thinking” and its expressive and economic use; and the third extends the idea of inclusive design to immersive, affective, and biological interfaces and actions.\n\n"}
{"id": "11073025", "url": "https://en.wikipedia.org/wiki?curid=11073025", "title": "Epistemic closure", "text": "Epistemic closure\n\nEpistemic closure is a property of some belief systems. It is the principle that if a subject formula_1 knows formula_2, and formula_1 knows that formula_2 entails formula_5, then formula_1 can thereby come to know formula_5. Most epistemological theories involve a closure principle and many skeptical arguments assume a closure principle.\n\nOn the other hand, some epistemologists, including Robert Nozick, have denied closure principles on the basis of reliabilist accounts of knowledge. Nozick, in \"Philosophical Explanations\", advocated that, when considering the Gettier problem, the least counter-intuitive assumption we give up should be epistemic closure. Nozick suggested a \"truth tracking\" theory of knowledge, in which the x was said to know P if x's belief in P tracked the truth of P through the relevant modal scenarios.\n\nA subject may not actually believe q, for example, regardless of whether he or she is justified or warranted. Thus, one might instead say that knowledge is closed under \"known\" deduction: if, while knowing p, S believes q because S knows that p entails q, then S knows q. An even stronger formulation would be as such: If, while knowing various propositions, S believes p because S knows that these propositions entail p, then S knows p. While the principle of epistemic closure is generally regarded as intuitive, philosophers such as Robert Nozick and Fred Dretske have argued against it.\n\nThe epistemic closure principle typically takes the form of a modus ponens argument:\n\n\nThis epistemic closure principle is central to many versions of skeptical arguments. A skeptical argument of this type will involve knowledge of some piece of widely accepted information to be knowledge, which will then be pointed out to entail knowledge of some skeptical scenario, such as the brain in a vat scenario or the cartesian evil demon scenario. A skeptic might say, for example, that if you know that you have hands, then you know that you are not a handless brain in a vat (because knowledge that you have hands implies that you know you are not handless, and if you know that you are not handless, then you know that you are not a handless brain in a vat). The skeptic will then utilize this conditional to form a modus tollens argument. For example, the skeptic might make an argument like the following:\nMuch of the epistemological discussion surrounding this type of skeptical argument involves whether to accept or deny the conclusion, and how to do each. Ernest Sosa says that there are three possibilities in responding to the skeptic:\n\nIn the seminal 1963 paper, “Is Justified True Belief Knowledge?”, Edmund Gettier gave an assumption (later called the “principle of deducibility for justification” by Irving Thalberg, Jr.) that would serve as a basis for the rest of his piece: “for any proposition P, if S is justified in believing P and P entails Q, and S deduces Q from P and accepts Q as a result of this deduction, then S is justified in believing Q.” This was seized upon by Thalberg, who rejected the principle in order to demonstrate that one of Gettier's examples fails to support Gettier's main thesis that justified true belief is not knowledge (in the following quotation, (1) refers to “Jones will get the job”, (2) refers to “Jones has ten coins”, and (3) is the logical conjunction of (1) and (2)):\n\nWhy doesn't Gettier's principle (PDJ) hold in the evidential situation he has described? You multiply your risks of being wrong when you believe a conjunction. [… T]he most elementary theory of probability indicates that Smith's prospects of being right on both (1) and (2), namely, of being right on (3), are bound to be less favorable than his prospects of being right on either (1) or (2). In fact, Smith's chances of being right on (3) might not come up to the minimum standard of justification which (1) and (2) barely satisfy, and Smith would be unjustified in accepting (3). \n\nThe term \"epistemic closure\" has been used in U.S. political debate to refer to the claim that political belief systems can be closed systems of deduction, unaffected by empirical evidence. This use of the term was popularized by libertarian blogger and commentator Julian Sanchez in 2010 as an extreme form of confirmation bias.\n\n\n"}
{"id": "581358", "url": "https://en.wikipedia.org/wiki?curid=581358", "title": "Family values", "text": "Family values\n\nFamily values, sometimes referred to as familial values, are traditional or cultural values that pertain to the family's structure, function, roles, beliefs, attitudes, and ideals.\n\nIn the social sciences and U.S. political discourse, the term \"traditional family\" refers to a nuclear family − a child-rearing environment composed of a breadwinning father, a homemaking mother, and their biological children; sociologists formerly referred to this model as the norm. A family deviating from this model is considered a nontraditional family. However, in most cultures at most times, the extended family model has been most common, not the nuclear family, and the nuclear family became the most common form in the U.S. in the 1960s and 1970s.\n\nSeveral well-known online dictionaries define \"family values\" as the following:\n\nFamilialism or \"familism\" is the ideology that puts priority on family and family values. Familialism prioritizes the needs of the family over the needs of individuals, and advocates for a welfare system where families, rather than the government, take responsibility for the care of their members.\n\nIn the United States, the banner of \"family values\" has been used by conservatives to fight abortion, gay rights, and major feminist objectives in politics.\n\nInterpretations of Islamic learnings and Arab culture are common for the majority of Saudis. Islam is a driving cultural force that dictates a submission to the will of God. The academic literature suggests that the family is regarded as the main foundation of Muslim society and culture; the family structure and nature of the relationship between family members are influenced by the Islamic religion. Marriage in Saudi culture means the union of two families, not just two individuals. In Muslim society, marriage involves a social contract that occurs with the consent of parents or guardians. Furthermore, marriage is considered the only legitimate outlet for sexual desires, and sex outside marriage (fornication) is a crime that is punished under Islamic law. This view of marriage is similar to the Western Christian view of marriage, created in 12th century France, which promised salvation, sex without sin, and much more.\n\nThe Saudi family includes extended families, as the extended family provides the individual with a sense of identity. The father is often the breadwinner and protector of the family, whereas the mother is often the homemaker and the primary caretaker of the children. Parents are regarded with high respect, and children are strongly encouraged to respect and obey their parents. Often, families provide care for elders. Until recently, because families and friends are expected to provide elderly care, nursing homes were considered culturally unacceptable.\n\nIn sociological terms, nontraditional families make up the majority of American households. As of 2014, only 46% of children in the U.S. live in a traditional family, down from 61% in 1980. This number includes only families with parents who are in their first marriage, whereas the percentage of children simply living with two married parents is 65% as of 2016. However, there are many who hold that the nuclear family is the fabric that holds society together and work to promote stronger family values. The Chemin Neuf community, the Bruderhof, and the Evangelical Alliance all work to promote the nuclear family.\n\n\n\n"}
{"id": "677213", "url": "https://en.wikipedia.org/wiki?curid=677213", "title": "Foot roasting", "text": "Foot roasting\n\nFoot roasting is a method of torture used since ancient times. The torture takes ingenious advantage of the extreme sensitivity of the sole of the foot to heat. The Romans immobilized the prisoner and pressed red-hot iron plates to the soles of his feet. The Spanish Inquisition bound the prisoner face-upward to the rack with his bare feet secured in a stocks. The soles of the feet were basted with lard or oil and slowly barbecued over a brazier of burning coals. A screen could be interposed between the feet and the coals to modulate the exposure, while a bellows controlled the intensity of the flame. Variants included placing slivers of hot coals between the toes, or suspending the prisoner head-downward and placing hot coals directly on the soles. The destruction of the Order of the Knights Templars is credited to these tortures, which were of sufficient cruelty literally to drive their sufferers insane.\n\nFoot roasting remains a popular technique of torture to this day. During the Cold War, KGB torturers made use of metal clothes irons heated red-hot and applied directly to the naked soles or explored the delicate webbing between the prisoner's toes using either a soldering iron or an electric wood-burning pencil.\n\nFoot roasting was one of the principal tortures used to extract supposed confessions of heresy and other accusations made against the Knights Templar after their arrest in October 1307. It is recorded that one Templar's feet were so savagely tortured that—as he was being carried back to his cell—various pieces of charred bone fell from his feet to the floor. Prisoners could also be suspended head-downwards from stocks, with hot coals placed directly on the soles of the feet—held in place by gravity—while thin slivers of burning embers were slid between pairs of adjacent toes.\n\nIn Brittany, an enhanced interrogation chair was used that immobilized the feet and provided a movable tray of coals that could be cranked up and down, eventually making physical contact with the soles of the feet.\n\nA form of torture called \"star kicking\" supposedly began with Countess Elizabeth Bathory, who would place oiled bits of paper or string between the prisoner's toes and light the material on fire.\n"}
{"id": "11592", "url": "https://en.wikipedia.org/wiki?curid=11592", "title": "Freeware", "text": "Freeware\n\nFreeware is software that is available for use at no monetary cost. In other words, while freeware may be used without payment it is most often proprietary software, and usually modification, re-distribution or reverse-engineering without the author's permission is prohibited. Two historic examples of freeware include Skype and Adobe Acrobat Reader. There is no agreed set of rights or a license or an EULA which would define \"freeware\" unambiguously; every freeware publisher defines their own rules for their freeware. For instance, redistribution of freeware by third-parties is often permitted but there is a significant portion of freeware which prohibits redistribution. \n\nFreeware, although itself free of charge, may be intended to benefit its producer, e.g. by encouraging sales of a more capable version (\"Freemium\" or Shareware business model). The source code of freeware is typically not available, unlike free and open-source software which are also often distributed free of charge.\n\nThe term \"freeware\" was coined in 1982 by Andrew Fluegelman when he wanted to sell a communications program named PC-Talk that he had created but for which he did not wish to use commercial distribution channels. Fluegelman actually distributed PC-Talk via a process now referred to as shareware, no longer called freeware.\n\nThe term \"freeware\" was used often in the 1980s and 1990s for programs released without source code.\n\nSoftware classified as freeware may be used without payment and is typically either fully functional for an unlimited time, or has limited functionality, with a more capable version available commercially or as shareware. In contrast to what the FSF calls free software, the author usually restricts the rights of the user to use, copy, distribute, modify, make derivative works, or reverse-engineer the software. The software license may impose various additional restrictions on the type of use, e.g. only for personal use, private use, individual use, non-profit use, non-commercial use, academic use, educational use, use in charity or humanitarian organizations, non-military use, use by public authorities or various other combinations of these type of restrictions. For instance, the license may be \"free for private, non-commercial use\". The software license may also impose various other restrictions, such as restricted use over a network, restricted use on a server, restricted use in a combination with some types of other software or with some hardware devices, prohibited distribution over the Internet other than linking to author's website, restricted distribution without author's consent, restricted number of copies, etc. Restrictions may be required by the licence, or enforced by the software (e.g., not usable over a network).\n\nThe U.S. Department of Defense (DoD) defines \"open source software\" (i.e., free software or free and open-source software), as distinct from \"freeware\" or \"shareware\"; it is software where \"the Government does not have access to the original source code\". The \"free\" in \"freeware\" refers to the price of the software, which is typically proprietary and distributed without source code. By contrast, the \"free\" in \"free software\" refers to freedoms granted users under the software license (for example, to run the program for any purpose, modify and redistribute the program to others), and such software may be sold at a price.\n\nAccording to the Free Software Foundation (FSF), \"freeware\" is a loosely defined category and it has no clear accepted definition, although FSF asks that free software (libre; unrestricted and with source code available) should not be called freeware.\nIn contrast the Oxford English Dictionary simply characterizes freeware as being \"available free of charge (sometimes with the suggestion that users should make a donation to the provider)\".\n\nSome freeware products are released alongside paid versions that either have more features or less restrictive licensing terms. This approach is known as freemium (\"free\" + \"premium\"), since the free version is intended as a promotion for the premium version. The two often share a code base, using a compiler flag to determine which is produced. For example, BBEdit has a BBEdit Lite edition which has fewer features. XnView is available free of charge for personal use but must be licensed for commercial use. The free version may be advertising supported, as was the case with the DivX.\n\nAd-supported software and free registerware also bear resemblances to freeware. Ad-supported software does not ask for payment for a license, but displays advertising to either compensate for development costs or as a means of income. Registerware forces the user to subscribe with the publisher before being able to use the product. While commercial products may require registration to ensure licensed use, free registerware do not.\n\nThe Creative Commons offer licenses, applicable to all by copyright governed works including software, which allow a developer to define \"freeware\" in a legal safe and internationally law domains respecting way. The typical freeware use case \"share\" can be further refined with Creative Commons restriction clauses like non-commerciality (CC BY-NC) or no-derivatives (CC BY-ND), see description of licenses. There are , for instance The White Chamber, Mari0 or Assault Cube, all freeware by being CC BY-NC-SA licensed: free sharing allowed, selling not.\n\nFreeware cannot economically rely on commercial promotion. In May 2015 advertising freeware on Google AdWords was restricted to \"authoritative source\"[s]. Thus web sites and blogs are the primary resource for information on which freeware is available, useful, and is not malware. However, there are also many computer magazines or newspapers that provide ratings for freeware and include compact discs or other storage media containing freeware. Freeware is also often bundled with other products such as digital cameras or scanners.\n\nFreeware has been criticized as \"unsustainable\" because it requires a single entity to be responsible for updating and enhancing the product, which is then given away without charge. Other freeware projects are simply released as one-off programs with no promise or expectation of further development. These may include source code, as does free software, so that users can make any required or desired changes themselves, but this code remains subject to the license of the compiled executable and does not constitute free software.\n\n\n"}
{"id": "57531234", "url": "https://en.wikipedia.org/wiki?curid=57531234", "title": "Goal Structuring Notation", "text": "Goal Structuring Notation\n\nGoal Structuring Notation is a graphical argument used to document and present proof that safety goals have been achieved in a clearer format than plain text. The notation is a diagram that builds its safety case through logic-based maps. Originally developed at the University of York during the 1990s, it gained popularity in 2012 and has been used to track safety assurances in industries such as traffic management and nuclear power. By 2014, it had become the standard format for graphic documentation of safety cases and was being used in other contexts such as patent claims, debate strategy, and legal arguments.\n"}
{"id": "23550115", "url": "https://en.wikipedia.org/wiki?curid=23550115", "title": "HDL System", "text": "HDL System\n\nThe HDL Universal Tactical Role-Playing System is a role-playing game system produced by Tremorworks, LLC.\n\nHDL is a flexible, simple system of rules that are adaptable to a great number or settings, allowing them to be used as alternate rules for existing games or as a mechanic for running campaigns of your own design. Unlike other “universal” gaming systems, HDL’s focus is on simplicity and realism. To play, all you need is the main book (a small and inexpensive volume as compared to many others), a set of HDL cards, and a few dice. There is no need to purchase a slew of “required” supplemental information; everything you need is in one book.\n\nCharacter creation is point-based; allocate 65 points to stats, ranging from 1 to 10, and 5 points to Backgrounds. Additional Background Points can be gained by taking Weaknesses. Skill Points are determined by the top three mental stats: Reason, Knowledge, and Resolve. The final step is to draw a Background Card, to help fill in some aspect of the character’s background or past.\n\nGame play is almost entirely skill-based, with a very detailed skill list across a number of categories, such as Combat, Computer, and Technical: Design.\n\nExperience Points (EXP) are awarded to players each game session, through role-playing, overcoming challenges, and playing Game Cards. EXP is then spent to improve the character’s skills or stats, and can also be used to re-roll poor dice rolls.\n\nThe HDL namesake originates from the dice mechanic for the game, called the Half Die Level. For simplicity, all variables boil down to a simple number, which determines the die rolled. The number equals half the sides on the die. Thus, HDL 2 = 1d4, HDL 6 = 1d12, and HDL 10 = 2d10. In general, all rolls are based on an HDL equal to a certain stat, plus the character’s rating in the associated skill. For instance, attacking with a sword is based on gross motor coordination, or the Coordination stat, and the Sword: Slashing skill, so a character would roll the HDL of his Coordination, then add his Sword: Slashing skill.\n\nWith the emphasis on realism, and a greater focus on role-playing than dice-rolling and fighting, combat is intentionally quick and deadly; even the strongest enemy (or character) can still be felled by a well-placed\nbullet or sword strike. Racking up a body count, or accumulating wealth, are secondary to the story and\ncharacters. In this way, power-gamers quickly learn the benefits of negotiation and skill diversity, rather than the ability to smash down any obstacle or opponent. At the same time, the system is designed to help the players succeed; the Narrator is encouraged to challenge players, but in the end they are expected to survive and, ultimately, win.\n\nESPers are another focus of the main rules: people with incredible psychic powers. Built using a Background and special skills, ESPers have a wide range of paranormal abilities, including mind reading, telekinesis, and even teleportation. The ESPer rules can also be adapted for use as magical abilities in fantasy settings.\n\nFor war-game players, strategic hex-map based tactical combat is possible with the optional SCom rules. This optional addition to standard combat rules uses a point-based action system that is simple yet deeply strategic. With this system, direct competitive play is even possible.\n\n"}
{"id": "528679", "url": "https://en.wikipedia.org/wiki?curid=528679", "title": "Hazard symbol", "text": "Hazard symbol\n\nHazard symbols or warning symbols are recognisable symbols designed to warn about hazardous or dangerous materials, locations, or objects, including electric currents, poisons, and radioactivity. The use of hazard symbols is often regulated by law and directed by standards organisations. Hazard symbols may appear with different colors, backgrounds, borders and supplemental information in order to specify the type of hazard and the level of threat (for example, toxicity classes). Warning symbols are used in many places in lieu of or addition to written warnings as they are quickly recognized (faster than reading a written warning) and more commonly understood (the same symbol can be recognized as having the same meaning to speakers of different languages).\n\nOn roadside warning signs, an exclamation mark is often used to draw attention to a generic warning of danger, hazards, and the unexpected. In Europe, this type of sign is used if there are no more-specific signs to denote a particular hazard. When used for traffic signs, it is accompanied by a supplementary sign describing the hazard, usually mounted under the exclamation mark.\n\nThis symbol has also been more widely adopted for generic use in many other contexts not associated with road traffic. It often appears on hazardous equipment or in instruction manuals to draw attention to a precaution, when a more-specific warning symbol is not available.\n\nThe skull-and-crossbones symbol (☠), consisting of a human skull and two bones crossed together behind the skull, is today generally used as a warning of danger, particularly in regard to poisonous substances.\n\nThe symbol, or some variation thereof, specifically with the bones (or swords) below the skull, was also featured on the Jolly Roger, the traditional flag of European and American seagoing pirates. It is also part of the Canadian WHMIS home symbols placed on containers to warn that the contents are poisonous.\n\nIn the United States, due to concerns that the skull-and-crossbones symbol's association with pirates might encourage children to play with toxic materials, the Mr. Yuk symbol is also used to denote poison.\n\nThe international radiation symbol (also known as the trefoil) first appeared in 1946, at the University of California, Berkeley Radiation Laboratory. At the time, it was rendered as magenta, and was set on a blue background. The original version used in America is magenta against a yellow background, and it is drawn with a central circle of radius \"R\", an internal radius of 1.5\"R\" and an external radius of 5\"R\" for the blades, which are separated from each other by 60°. The trefoil is black in the international version, which is also used in America.\n\nThe sign is commonly referred to as a radioactivity warning sign, but it is actually a warning sign of ionizing radiation. Ionizing radiation is a much broader category than radioactivity alone, as many non-radioactive sources also emit potentially dangerous levels of ionizing radiation. This includes x-ray apparatus, radiotherapy linear accelerators, and particle accelerators. Non-ionizing radiation can also reach potentially dangerous levels, but this warning sign is different from the trefoil ionizing radiation warning symbol.\n\nOn February 15, 2007, two groups—the International Atomic Energy Agency (IAEA) and the International Organization for Standardization (ISO)—jointly announced the adoption of a new ionizing radiation warning symbol to supplement the traditional trefoil symbol. The new symbol, to be used on sealed radiation sources, is aimed at alerting anyone, anywhere to the danger of being close to a strong source of ionizing radiation. It depicts, on a red background, a black trefoil with waves of radiation streaming from it, along with a black skull and crossbones, and a running figure with an arrow pointing away from the scene. The radiating trefoil suggests the presence of radiation, while the red background and the skull and crossbones warn of the danger. The figure running away from the scene is meant to suggest taking action to avoid the labeled material. The new symbol is not intended to be generally visible, but rather to appear on internal components of devices that house radiation sources so that if anybody attempts to disassemble such devices they will see an explicit warning not to proceed any further.\n\nThe biohazard symbol is used in the labeling of biological materials that carry a significant health risk (biohazards), including viral samples and used hypodermic needles (see sharps waste).\n\nThe biohazard symbol was developed by the Dow Chemical Company in 1966 for their containment products.\n\nAccording to Charles Baldwin, an environmental-health engineer who contributed to its development: \"We wanted something that was memorable but meaningless, so we could educate people as to what it means.\" In an article in \"Science\" in 1967, the symbol was presented as the new standard for all biological hazards (\"biohazards\"). The article explained that over 40 symbols were drawn up by Dow artists, and all of the symbols investigated had to meet a number of criteria: \"(i) striking in form in order to draw immediate attention; (ii) unique and unambiguous, in order not to be confused with symbols used for other purposes; (iii) quickly recognizable and easily recalled; (iv) easily stenciled; (v) symmetrical, in order to appear identical from all angles of approach; and (vi) acceptable to groups of varying ethnic backgrounds.\" The chosen scored the best on nationwide testing for memorability.\n\nAll parts of the biohazard sign can be drawn with a compass and straightedge. The basic outline of the symbol is a plain trefoil, which is three circles overlapping each other equally like in a triple Venn diagram with the overlapping parts erased. The diameter of the overlapping part is equal to half the radius of the three circles. Then three inner circles are drawn in with radius of the original circles so that it is tangent to the outside three overlapping circles. A tiny circle in center has a diameter of the radius of the three inner circles, and arcs are erased at 90°, 210°, and 330°. The arcs of the inner circles and the tiny circle are connected by a line. Finally, the ring under is drawn from the distance to the perimeter of the equilateral triangle that forms between the centers of the three intersecting circles. An outer circle of the ring under is drawn and finally enclosed with the arcs from the center of the inner circles with a shorter radius from the inner circles.\n\nA chemical hazard symbol is a pictogram applied to containers of dangerous chemical compounds to indicate the specific hazard, and thus the required precautions. There are several systems of labels, depending on the purpose, such as on the container for end use, or on a vehicle during transportation. \n\nThe United Nations has designed GHS hazard pictograms and GHS hazard statements to internationally harmonize chemical hazard warnings. Several European countries have started to implement these new global standards, but older warning symbols are still used in many parts of the world.\n\nEuropean standards are set by:\n\nThe Workplace Hazardous Materials Information System, or WHMIS, is Canada's national workplace hazard communication standard.\n\nThe US-based National Fire Protection Association (NFPA) has a standard NFPA 704 using a diamond with four colored sections each with a number indicating severity 0—4 (0 for no hazard, 4 indicates a severe hazard). The red section denotes flammability. The blue section denotes health risks. Yellow represents reactivity (tendency to explode). The white section denotes special hazard information. One example of a special hazard would be the capital letter W crossed out (pictured left), indicating it is water reactant. \n\nA large number of warning symbols with non-standard designs are in use around the world.\n\nSome warning symbols have been redesigned to be more comprehensible to children, such as the Mr. Ouch (depicting an electricity danger as a snarling, spiky creature) and Mr. Yuk (a green frowny face sticking its tongue out, to represent poison) designs in the United States.\n\n\n"}
{"id": "37822732", "url": "https://en.wikipedia.org/wiki?curid=37822732", "title": "History of network traffic models", "text": "History of network traffic models\n\nDesign of robust and reliable networks and network services relies on an understanding of the traffic characteristics of the network. Throughout history, different models of network traffic have been developed and used for evaluating existing and proposed networks and services.\n\nDemands on computer networks are not entirely predictable. Performance modeling is necessary for deciding the quality of service (QoS) level. Performance models in turn, require accurate traffic models that have the ability to capture the statistical characteristics of the actual traffic on the network. Many traffic models have been developed based on traffic measurement data. If the underlying traffic models do not efficiently capture the characteristics of the actual traffic, the result may be the under-estimation or over-estimation of the performance of the network. This impairs the design of the network. Traffic models are hence, a core component of any performance evaluation of networks and they need to be very accurate.\n\n“Teletraffic theory is the application of mathematics to the measurement, modeling, and control of traffic in telecommunications networks. The aim of traffic modeling is to find stochastic processes to represent the behavior of traffic. Working at the Copenhagen Telephone Company in the 1910s, A. K. Erlang famously characterized telephone traffic at the call level by certain probability distributions for arrivals of new calls and their holding times. Erlang applied the traffic models to estimate the telephone switch capacity needed to achieve a given call blocking probability. The Erlang blocking formulas had tremendous practical interest for public carriers because telephone facilities (switching and transmission) involved considerable investments. Over several decades, Erlang’s work stimulated the use of queuing theory, and applied probability in general, to engineer the public switched telephone network. Teletraffic theory for packet networks has seen considerable progress in recent decades. Significant advances have been made in long-range dependence, wavelet, and multifractal approaches. At the same time, traffic modeling continues to be challenged by evolving network technologies and new multimedia applications. For example, wireless technologies allow greater mobility of users. Mobility must be an additional consideration for modeling traffic in wireless networks. Traffic modeling is an ongoing process without a real end. Traffic models represent our best current understanding of traffic behavior, but our understanding will change and grow over time.”\n\nMeasurements are useful and necessary for verifying the actual network performance. However, measurements do not have the level of abstraction that makes traffic models useful. Traffic models can be used for hypothetical problem solving whereas traffic measurements only reflect current reality. In probabilistic terms, a traffic trace is a realization of a random process, whereas a traffic model is a random process. Thus, traffic models have universality. A traffic trace gives insight about a particular traffic source, but a traffic model gives insight about all traffic sources of that type. Traffic models have three major uses. One important use of traffic models is to properly dimension network resources for a target level of QoS. It was mentioned earlier that Erlang developed models of voice calls to estimate telephone switch capacity to achieve a target call blocking probability. Similarly, models of packet traffic are needed to estimate the bandwidth and buffer resources to provide acceptable packet delays and packet loss probability. Knowledge of the average traffic rate is not sufficient. It is known from queuing theory that queue lengths increase with the variability of traffic. Hence, an understanding of traffic burstiness or variability is needed to determine sufficient buffer sizes at nodes and link capacities. A second important use of traffic models is to verify network performance under specific traffic controls. For example, given a packet scheduling algorithm, it would be possible to evaluate the network performance resulting from different traffic scenarios. For another example, a popular area of research is new improvements to the TCP congestion avoidance algorithm. It is critical that any algorithm is stable and allows multiple hosts to share bandwidth fairly, while sustaining a high throughput. Effective evaluation of the stability, fairness, and throughput of new algorithms would not be possible without realistic source models. A third important use of traffic models is admission control. In particular, connection oriented networks such as ATM depends on admission control to block new connections to maintain QOS guarantees. A simple admission strategy could be based on the peak rate of a new connection; a new connection is admitted if the available bandwidth is greater than the peak rate. However, that strategy would be overly conservative because a variable bit-rate connection may need significantly less bandwidth than its peak rate. A more sophisticated admission strategy is based on effective bandwidths. The source traffic behavior is translated into an effective bandwidth between the peak rate and average rate, which is the specific amount of bandwidth required to meet a given QoS constraint. The effective bandwidth depends on the variability of the source.\n\nTraffic modeling consists of three steps:\nParameter estimation is based on a set of statistics (e.g. mean, variance, density function or auto covariance function, multifractal characteristics) that are measured or calculated from observed data. The set of statistics used in the inference process depends on the impact they may have in the main performance metrics of interest.\n\nIn recent years several types of traffic behavior, that can have significant impact on network performance, were discovered: long-range dependence, self-similarity and, more recently, multifractality.\nThere are two major parameters generated by network traffic models: packet length distributions and packet inter-arrival distributions. Other parameters, such as routes, distribution of destinations, etc., are of less importance. Simulations that use traces generated by network traffic models usually examine a single node in the network, such as a router or switch; factors that depend on specific network topologies or routing information are specific to those topologies and simulations. The problem of packet size distribution is fairly well-understood today. Existing models of packet sizes have proven to be valid and simple. Most packet size models do not consider the problem of order in packet sizes. For example, a TCP datagram in one direction is likely to be followed by a tiny ACK in the other direction about half of one Round-Trip Time (RTT) later. The problem of packet inter-arrival distribution is much more difficult. Understanding of network traffic has evolved significantly over the years, leading to a series of evolutions in network traffic models.\n\nOne of the earliest objections to self-similar traffic models was the difficulty in mathematical analysis. Existing self-similar models could not be used in conventional queuing models. This limitation was rapidly overturned and workable models were constructed. Once basic self-similar models became feasible, the traffic modeling community settled into the “detail” concerns. TCP’s congestion control algorithm complicated the matter of modeling traffic, so solutions needed to be created. Parameter estimation of self-similar models was always difficult, and recent research addresses ways to model network traffic without fully understanding it.\n\nWhen self-similar traffic models were first introduced, there were no efficient, analytically tractable processes to generate the models. Ilkka Norros devised a stochastic process for a storage model with self-similar input and constant bit-rate output. While this initial model was continuous rather than discrete, the model was effective, simple, and attractive.\nAll self-similar traffic models suffer from one significant drawback: estimating the self-similarity parameters from real network traffic requires huge amounts of data and takes extended computation. The most modern method, wavelet multi-resolution analysis, is more efficient, but still very costly. This is undesirable in a traffic model. SWING uses a surprisingly simple model for the network traffic analysis and generation. The model examines characteristics of users, Request-Response Exchanges (RREs), connections, individual packets, and the overall network. No attempt is made to analyze self-similarity characteristics; any self-similarity in the generated traffic comes naturally from the aggregation of many ON/OFF sources.\nThe Pareto distribution process produces independent and identically distributed (IID) inter-arrival times. In general if X is a random variable with a Pareto distribution, then the probability that X is greater than some number x is given by P(X > x) = (x/x_m)-k for all x ≥ x_m where k is a positive parameter and x_m is the minimum possible value of Xi The probability distribution and the density functions are represented as:\nF(t) = 1 – (α/t)β where α,β ≥ 0 & t ≥ α\nf(t) = βαβ t-β-1\nThe parameters β and α are the shape and location parameters, respectively. The Pareto distribution is applied to model self-similar arrival in packet traffic. It is also referred to as double exponential, power law distribution. Other important characteristics of the model are that the Pareto distribution has infinite variance, when β ≥ 2 and achieves infinite mean, when β ≤ 1.\nThe Weibull distributed process is heavy-tailed and can model the fixed rate in ON period and ON/OFF period lengths, when producing self-similar traffic by multiplexing ON/OFF sources. The distribution function in this case is given by:\nF(t) = 1 – e-(t/β)α t > 0\nand the density function of the weibull distribution is given as:\nf(t) = αβ-α tα-1 e -(t/β)α t > 0\nwhere parameters β ≥ 0 and α > 0 are the scale and location parameters respectively.\nThe Weibull distribution is close to a normal distribution. For β ≤ 1 the density function of the distribution is L shaped and for values of β > 1, it is bell shaped. This distribution gives a failure rate increasing with time. For β > 1, the failure rate decreases with time. At, β = 1, the failure rate is constant and the lifetimes are exponentially distributed.\nThe Autoregressive model is one of a group of linear prediction formulas that attempt to predict an output y_n of a system based on previous set of outputs {y_k} where k < n and inputs x_n and {x_k} where k < n. There exist minor changes in the way the predictions are computed based on which, several variations of the model are developed. Basically, when the model depends only on the previous outputs of the system, it is referred to as an auto-regressive model. It is referred to as a Moving Average Model (MAM), if it depends on only the inputs to the system. Finally, Autoregressive-Moving Average models are those that depend both on the inputs and the outputs, for prediction of current output. Autoregressive model of order p, denoted as AR(p), has the following form:\nXt = R1 Xt-1 + R2 Xt-2 + ... + Rp Xt-p + Wt\nwhere Wt is the white noise, Ri are real numbers and Xt are prescribed correlated random numbers. The auto-correlation function of the AR(p) process consists of damped sine waves depending on whether the roots (solutions) of the model are real or imaginary. Discrete Autoregressive Model of order p, denoted as DAR(p), generates a stationary sequence of discrete random variables with a probability distribution and with an auto-correlation structure similar to that of the Autoregressive model of order p.[3]\nRegression models define explicitly the next random variable in the sequence by previous ones within a specified time window and a moving average of a white noise.[5]\nTransform-expand-sample (TES) models are non-linear regression models with modulo-1 arithmetic. They aim to capture both auto-correlation and marginal distribution of empirical data. TES models consist of two major TES processes: TES+ and TES–. TES+ produces a sequence which has positive correlation at lag 1, while TES– produces a negative correlation at lag 1.\n\nEarly traffic models were derived from telecommunications models and focused on simplicity of analysis. They generally operated under the assumption that aggregating traffic from a large number of sources tended to smooth out bursts; that burstiness decreased as the number of traffic sources increased.\nOne of the most widely used and oldest traffic models is the Poisson Model. The memoryless Poisson distribution is the predominant model used for analyzing traffic in traditional telephony networks. The Poisson process is characterized as a renewal process. In a Poisson process the inter-arrival times are exponentially distributed with a rate parameter λ: P{An ≤ t} = 1 – exp(-λt). The Poisson distribution is appropriate if the arrivals are from a large number of independent sources, referred to as Poisson sources. The distribution has a mean and variance equal to the parameter λ.\nThe Poisson distribution can be visualized as a limiting form of the binomial distribution, and is also used widely in queuing models. There are a number of interesting mathematical properties exhibited by Poisson processes. Primarily, superposition of independent Poisson processes results in a new Poisson process whose rate is the sum of the rates of the independent Poisson processes. Further, the independent increment property renders a Poisson process memoryless. Poisson processes are common in traffic applications scenarios that consist of a large number of independent traffic streams. The reason behind the usage stems from Palm's Theorem which states that under suitable conditions, such large number of independent multiplexed streams approach a Poisson process as the number of processes grows, but the individual rates decrease in order to keep the aggregate rate constant. Nevertheless, it is to be noted that traffic aggregation need not always result in a Poisson process. The two primary assumptions that the Poisson model makes are:\n1. The number of sources is infinite\n2. The traffic arrival pattern is random.\nIn the compound Poisson model, the base Poisson model is extended to deliver batches of packets at once. The inter-batch arrival times are exponentially distributed, while the batch size is geometric Mathematically, this model has two parameters, λ, the arrival rate, and ρ in (0,1), the batch parameter. Thus, the mean number of packets in a batch is 1/ ρ, while the mean inter-batch arrival time is 1/ λ. Mean packet arrivals over time period t are tλ/ ρ.\nThe compound Poisson model shares some of the analytical benefits of the pure Poisson model: the model is still memoryless, aggregation of streams is still (compound) Poisson, and the steady-state equation is still reasonably simple to calculate, although varying batch parameters for differing flows would complicate the derivation.\nMarkov models attempt to model the activities of a traffic source on a network, by a finite number of states. The accuracy of the model increases linearly with the number of states used in the model. However, the complexity of the model also increases proportionally with increasing number of states. An important aspect of the Markov model - the Markov Property, states that the next (future) state depends only on the current state. In other words, the probability of the next state, denoted by some random variable Xn+1, depends only on the current state, indicated by Xn, and not on any other state Xi, where i<n. The set of random variables referring to different states {Xn} is referred to as a Discrete Markov Chain.\nAnother attempt at providing a bursty traffic model is found in Jain and Routhier’s Packet Trains model. This model was principally designed to recognize that address locality applies to routing decisions; that is, packets that arrive near each other in time are frequently going to the same destination. In generating a traffic model that allows for easier analysis of locality, the authors created the notion of packet trains, a sequence of packets from the same source, traveling to the same destination (with replies in the opposite direction). Packet trains are optionally sub-divided into tandem trailers. Traffic between a source and a destination usually consists of a series of messages back and forth. Thus, a series of packets go one direction, followed by one or more reply packets, followed by a new series in the initial direction. Traffic quantity is then a superposition of packet trains, which generates substantial bursty behavior. This refines the general conception of the compound Poisson model, which recognized that packets arrived in groups, by analyzing why they arrive in groups, and better characterizing the attributes of the group. Finally, the authors demonstrate that packet arrival times are not Poisson distributed, which led to a model that departs from variations on the Poisson theme. The packet train model is characterized by the following parameters and their associated probability distributions:\nThe train model is designed for analyzing and categorizing real traffic, not for generating synthetic loads for simulation. Thus, little claim has been made about the feasibility of packet trains for generating synthetic traffic. Given accurate parameters and distributions, generation should be straightforward, but derivation of these parameters is not addressed.\n\nNS-2 is a popular network simulator; PackMimeHTTP is a web traffic generator for NS-2, published in 2004. It does take long-range dependencies into account, and uses the Weibull distribution. Thus, it relies on heavy tails to emulate true self-similarity. Over most time scales, the effort is a success; only a long-running simulation would allow a distinction to be drawn. This follows suggestions from where it is suggested that self-similar processes can be represented as a superposition of many sources each individually modeled with a heavy-tailed distribution. It is clear that self-similar traffic models are in the mainstream.\n\n\n"}
{"id": "5333892", "url": "https://en.wikipedia.org/wiki?curid=5333892", "title": "In situ resource utilization", "text": "In situ resource utilization\n\nIn space exploration, in situ resource utilization (ISRU) is defined as \"the collection, processing, storing and use of materials encountered in the course of human or robotic space exploration that replace materials that would otherwise be brought from Earth.\"\nISRU is the practice of leveraging resources found or manufactured on other astronomical objects (the Moon, Mars, asteroids, etc.) to fulfill or enhance the requirements and capabilities of a space mission.\n\nISRU can provide materials for life support, propellants, construction materials, and energy to a spacecraft payloads or space exploration crews.\nIt is now very common for spacecraft and robotic planetary surface mission to harness the solar radiation found \"in situ\" in the form of solar panels. The use of ISRU for material production has not yet been implemented in a space mission, though several field tests in the late 2000s demonstrated various lunar ISRU techniques in a relevant environment.\n\nISRU has long been considered as a possible avenue for reducing the mass and cost of space exploration architectures, in that it may be a way to drastically reduce the amount of payload that must be launched from Earth in order to explore a given planetary body.\nAccording to NASA, \"in-situ resource utilisation will enable the affordable establishment of extraterrestrial exploration and operations by minimizing the materials carried from Earth.\"\n\nIn the context of ISRU water is most often sought directly as fuel or as feedstock for fuel production. Applications include its use in life support either directly by drinking, for growing food, producing oxygen, or numerous other industrial processes. All of which require a ready supply of water in the environment and the equipment to extract it. Such extraterrestrial water has been discovered in a variety of forms throughout the solar system, and a number of potential water extraction technologies have been investigated. For water that is chemically bound to regolith, solid ice, or some manner of permafrost, sufficient heating can recover the water. However this is not as easy as it appears because ice and permafrost can often be harder than plain rock, necessitating laborious mining operations. Where there is some level of atmosphere, such as on Mars, water can be extracted directly from the air using a simple process such as WAVAR. Another possible source of water is deep aquifers kept warm by Mars's latent geological heat, which can be tapped to provide both water and geothermal power.\n\nRocket propellant from water ice has also been proposed for the Moon, mainly from ice that has been found at the poles. The likely difficulties include working at extremely low temperatures and extraction from the regolith. Most schemes electrolyse the water and form hydrogen and oxygen and liquify and cryogenically store them. This requires large amounts of equipment and power to achieve. Alternatively it is possible to simply heat the water in a nuclear or solar thermal rocket, which seems to give very much more mass delivered to low Earth orbit (LEO) in spite of the much lower specific impulse, for a given amount of equipment.\n\nThe monopropellant hydrogen peroxide (HO) can be made from water on Mars and the Moon.\n\nAluminum as well as other metals have been proposed for use as rocket propellant made using lunar resources, and proposals include reacting the aluminum with water. For Mars, methane propellant can be manufactured via the Sabatier process.\n\nIt has long been suggested that solar cells could be produced from the materials present in lunar soil. Silicon, aluminium, and glass, three of the primary materials required for solar cell production, are found in high concentrations in lunar soil and can be utilised to produce solar cells. In fact, the native vacuum on the lunar surface provides an excellent environment for direct vacuum deposition of thin-film materials for solar cells.\n\nSolar arrays produced on the lunar surface can be used to support lunar surface operations as well as satellites off the lunar surface. Solar arrays produced on the lunar surface may prove more cost effective than solar arrays produced and shipped from Earth, but this trade depends heavily on the location of the particular application in question.\n\nAnother potential application of lunar-derived solar arrays is providing power to Earth. In its original form, known as the solar power satellite, the proposal was intended as an alternate power source for Earth. Solar cells would be shipped to Earth orbit and assembled, the power being transmitted to Earth via microwave beams. Despite much work on the cost of such a venture, the uncertainty lay in the cost and complexity of fabrication procedures on the lunar surface.\n\nAsteroid mining could also involve extraction of metals for construction material in space, which may be more cost-effective than bringing such material up out of Earth's deep gravity well, or that of any other large body like the Moon or Mars. Metallic asteroids contain huge amounts of siderophilic metals, including precious metals.\n\nThe colonisation of planets or moons will require obtaining local building materials, such as regolith. For example, studies employing artificial Mars soil mixed with epoxy resin and tetraethoxysilane, produce high enough values of strength, resistance, and flexibility parameters.\n\nISRU research for Mars is focused primarily on providing rocket propellant for a return trip to Earth — either for a manned or a sample return mission — or for use as fuel on Mars. Many of the proposed techniques utilise the well-characterised atmosphere of Mars as feedstock. Since this can be easily simulated on Earth, these proposals are relatively simple to implement, though it is by no means certain that NASA or the ESA will favour this approach over a more conventional direct mission.\n\nA typical proposal for ISRU is the use of a Sabatier reaction, , in order to produce methane on the Martian surface, to be used as a propellant. Oxygen is liberated from the water by electrolysis, and the hydrogen recycled back into the Sabatier reaction. The usefulness of this reaction is that—, when the availability of water on Mars was less scientifically demonstrated—only the hydrogen (which is light) was thought to need to be brought from Earth.\n\n, SpaceX is currently developing the technology for a Mars propellant plant that will use a variation on what is described in the previous paragraph. Rather than transporting hydrogen from Earth to use in making the methane and oxygen, they intend to mine the requisite water from subsurface water ice that is now known to be abundant across much of the Martian surface, produce and then store the post-Sabatier reactants, and then use it as propellant for return flights of their \"Interplanetary Spaceship\" no earlier than 2023.\n\nA similar reaction proposed for Mars is the reverse water gas shift reaction, . This reaction takes place rapidly in the presence of an iron-chrome catalyst at 400 Celsius, and has been implemented in an Earth-based testbed by NASA. Again, oxygen is recycled from the water by electrolysis, and the reaction only needs a small amount of hydrogen from Earth. The net result of this reaction is the production of oxygen, to be used as the oxidizer component of rocket fuel.\n\nAnother reaction proposed for the production of oxygen and fuel is the electrolysis of the atmospheric carbon dioxide, \n\nMore recently, it has been proposed the in situ production of oxygen, hydrogen and CO from the martian hematite deposits via a two-step thermochemical /HO splitting process, and specifically in the magnetite/wustite redox cycle. Although thermolysis is the most direct, one-step process for splitting molecules, it is neither practical nor efficient in the case of either HO or CO. This is because the process requires a very high temperature (> 2500 C) to achieve a meaningful dissociation fraction. This poses problems in finding suitable reactor materials, losses due to vigorous product recombination, and excessive aperture radiation losses when concentrated solar heat is used. The magnetite/wustite redox cycle was first proposed for solar application on earth by Nakamura, and was one of the first used for solar-driven two-step water splitting. In this cycle, water reacts with wustite (FeO) to form magnetite (FeO) and hydrogen. The summarised reactions in this two-step splitting process are as follows:\n\nand the obtained FeO is used for the thermal splitting of water or CO :\n\nThis process is repeated cyclically. The above process results in a substantial reduction in the thermal input of energy if compared with the most direct, one-step process for splitting molecules.\n\nHowever, the process needs wustite (FeO) to start the cycle, but on Mars there is no wustite or at least not in significant amounts. Nevertheless, wustite can be easily obtained by reduction of hematite (FeO) which is an abundant material on Mars, being specially conspicuous the strong hematite deposits located at Terra Meridiani.\nThe intention of wustite from the hematite -abundantly available on Mars, is an industrial process well-known on earth, and us performed by the following two main reduction reactions, namely: \n\nMars Surveyor 2001 Lander MIP (Mars ISPP Precursor) was to demonstrate manufacture of oxygen from the atmosphere of Mars, and test solar cell technologies and methods of mitigating the effect of Martian dust on the power systems. The proposed Mars 2020 rover mission might include ISRU technology demonstrator that would extract CO from the atmosphere and produce O for rocket fuel.\n\nIt has been suggested that buildings on Mars could be made from basalt as it has good insulating properties. An underground structure of this type would be able to protect life forms against radiation exposure.\n\nAll of the resources required to make plastics exist on Mars. Many of these complex reactions are able to be completed from the gases harvested from the martian atmosphere. Traces of free oxygen, carbon monoxide, water and methane are all known to exist. Hydrogen and oxygen can be made by the electrolysis of water, carbon monoxide and oxygen by the electrolysis of carbon dioxide and methane by the Sabatier reaction of carbon dioxide and hydrogen. These basic reactions provide the building blocks for more complex reaction series which are able to make plastics. Ethylene is used to make plastics such as polyethylene and polypropylene and can be made from carbon monoxide and hydrogen, \n\nThe Moon possesses abundant raw materials that are potentially relevant to a hierarchy of future applications, beginning with the use of lunar materials to facilitate human activities on the Moon itself and progressing to the use of lunar resources to underpin a future industrial capability within the Earth-Moon system.\n\nThe lunar highland material anorthite can be used as aluminium ore. Smelters can produce pure aluminium, calcium metal, oxygen and silica glass from anorthite. Raw anorthite is also good for making fiberglass and other glass and ceramic products. One particular processing technique is to use fluorine brought from Earth as potassium fluoride to separate the raw materials from the lunar rocks.\n\nOver twenty different methods have been proposed for oxygen extraction on the Moon. Oxygen is often found in iron rich lunar minerals and glasses as iron oxide. The oxygen can be extracted by heating the material to temperatures above 900 °C and exposing it to hydrogen gas. The basic equation is: FeO + H → Fe + HO. This process has recently been made much more practical by the discovery of significant amounts of hydrogen-containing regolith near the Moon's poles by the Clementine spacecraft.\n\nLunar materials may also be valuable for other uses. It has also been proposed to use lunar regolith as a general construction material, through processing techniques such as sintering, hot-pressing, liquification, and the cast basalt method. Cast basalt is used on Earth for construction of, for example, pipes where a high resistance to abrasion is required. Cast basalt has a very high hardness of 8 Mohs (diamond is 10 Mohs) but is also susceptible to mechanical impact and thermal shock which could be a problem on the Moon.\n\nGlass and glass fiber are straightforward to process on the Moon and Mars, and it has been argued that the glass is optically superior to that made on the Earth because it can be made anhydrous. Successful tests have been performed on Earth using two lunar regolith simulants MLS-1 and MLS-2. Basalt fibre has also been made from lunar regolith simulators.\n\nIn August 2005, NASA contracted for the production of 16 tonnes of simulated lunar soil,\nor \"Lunar Regolith Simulant Material.\"\nThis material is now commercially available for research on how lunar soil could be utilized \"in situ\".\n\nOther proposals are based on Phobos and Deimos. These moons are in reasonably high orbits above Mars, have very low escape velocities, and unlike Mars have return delta-v's from their surfaces to LEO which are less than the return from the Moon.\n\nCeres is further out than Mars, with a higher delta-v, but launch windows and travel times are better, and the surface gravity is just 0.028 g, with a very low escape velocity of 510 m/s. Researchers have speculated that the interior configuration of Ceres includes a water-ice-rich mantle over a rocky core.\n\nNear Earth Asteroids and bodies in the asteroid belt could also be sources of raw materials for ISRU.\n\nProposals have been made for \"mining\" for rocket propulsion, using what is called a Propulsive Fluid Accumulator. Atmospheric gases like oxygen and argon could be extracted from the atmosphere of planets like the Earth, Mars, and the outer Gas Giants by Propulsive Fluid Accumulator satellites in low orbit.\n\nIn October 2004, NASA’s Advanced Planning and Integration Office commissioned an ISRU capability roadmap team.\nThe team's report, along with those of 14 other capability roadmap teams, were published May 22, 2005.\nThe report identifies seven ISRU capabilities:\n(i) resource extraction, (ii) material handling and transport, (iii) resource processing, (iv) surface manufacturing with \"in situ\" resources, (v) surface construction, (vi) surface ISRU product and consumable storage and distribution, and (vii) ISRU unique development and certification capabilities. \n\nThe report focuses on lunar and martian environments. It offers a detailed timeline and capability roadmap to 2040 but it assumes lunar landers in 2010 and 2012.\n\nThe Mars Surveyor 2001 Lander was intended to carry to Mars a test payload, MIP (Mars ISPP Precursor), that was to demonstrate manufacture of oxygen from the atmosphere of Mars, but the mission was cancelled.\n\nThe Mars Oxygen ISRU Experiment (MOXIE) is a 1% scale prototype model aboard the planned Mars 2020 rover that will produce oxygen from Martian atmospheric carbon dioxide (CO) in a process called solid oxide electrolysis.\n\n\n"}
{"id": "1677048", "url": "https://en.wikipedia.org/wiki?curid=1677048", "title": "Inattentional blindness", "text": "Inattentional blindness\n\nInattentional blindness, also known as perceptual blindness, is a psychological lack of attention that is not associated with any vision defects or deficits. It may be further defined as the event in which an individual fails to perceive an unexpected stimulus that is in plain sight. When it simply becomes impossible for one to attend to all the stimuli in a given situation, a temporary blindness effect can take place as a result; that is, individuals fail to see objects or stimuli that are unexpected and quite often salient. The term was coined by Arien Mack and Irvin Rock in 1992 and was used as the title of their book of the same name, published by MIT press in 1998, in which they describe the discovery of the phenomenon and include a collection of procedures used in describing it. A famous study that demonstrated inattentional blindness asked participants whether or not they noticed a gorilla walking through the scene of a visual task they had been given.\n\nResearch on inattentional blindness suggests that the phenomenon can occur in any individual, independent of cognitive deficits. However, recent evidence shows that patients with ADHD performed better attentionally when engaging in inattentional blindness tasks than control patients did, suggesting that some mental deficits may decrease the effects of this phenomenon. Recent studies have also looked at age differences and inattentional blindness scores, and results show that the effect increases as humans age. There is mixed evidence that consequential unexpected objects are noticed more: Some studies suggest that we can detect threatening unexpected stimuli more easily than nonthreatening ones, but other studies suggest that this is not the case. There is some evidence that objects associated with reward are noticed more.\n\nNumerous experiments and art work has demonstrated that inattentional blindness also has an effect on people's perception.\n\nThe following criteria are required to classify an event as an inattentional blindness episode: 1) the observer must fail to notice a visual object or event, 2) the object or event must be fully visible, 3) observers must be able to readily identify the object if they are consciously perceiving it, and 4) the event must be unexpected and the failure to see the object or event must be due to the engagement of attention on other aspects of the visual scene and not due to aspects of the visual stimulus itself. Individuals who experience inattentional blindness are usually unaware of this effect, which can play a subsequent role on behavior.\n\nInattentional blindness is related to but distinct from other failures of visual awareness such as change blindness, repetition blindness, visual masking, and attentional blink. The key aspect of inattentional blindess which makes it distinct from other failures in awareness rests on the fact that the undetected stimulus is unexpected. It is the unexpected nature of said stimulus that differentiates inattentional blindness from failures of awareness such as attentional failures like the aforementioned attentional blink. It is critical to acknowledge that occurrences of inattentional blindness are attributed to the failure to consciously attend to an item in the visual field as opposed the absence of cognitive processing.\n\nFindings such as inattentional blindness – the failure to notice a fully visible but unexpected object because attention was engaged on another task, event, or object – has changed views on how the brain stores and integrates visual information, and has led to further questioning and investigation of the brain and importantly of cognitive processes.\n\n\"Cognitive capture\" or, \"cognitive tunneling\", is an inattentional blindness phenomenon in which the observer is too focused on instrumentation, task at hand, internal thought, etc. and not on the present environment. For example, while driving, a driver focused on the speedometer and not on the road is suffering from cognitive capture.\n\nOne of the most foremost conflicts among researchers of inattentional blindness surrounds the processing of unattended stimuli. More specifically, there is disagreement in the literature about exactly how much processing of a visual scene is completed before selection dictates which stimuli will be consciously perceived, and which will not be (i.e. inattentional blindness). There exists two basic schools of thought on the issue – those who believe selection occurs early in the perceptual process, and those who believe it occurs only after significant processing. Early selection theorists propose that perception of stimuli is a limited process requiring selection to proceed. This suggests that the decision to attend to specific stimuli occurs early in processing, soon after the rudimentary study of physical features; only those selected stimuli are then fully processed. On the other hand, proponents of late selection theories argue that perception is an unlimited operation, and all stimuli in a visual scene are processed simultaneously. In this case, selection of relevant information is done after full processing of all stimuli.\n\nWhile early research on the topic was heavily focused on early selection, research since the late 1970s has been shifted mainly to the late selection theories. This change resulted primarily from a shift in paradigms used to study inattentional blindness which revealed new aspects of the phenomenon. Today, late selection theories are generally accepted, and continue to be the focus of the majority of research concerning inattentional blindness.\n\nA significant body of research has been gathered in support of late selection in the perception of visual stimuli.\n\nOne of the popular ways of investigating late selection is to assess the priming properties (i.e. influencing subsequent acts) of unattended stimuli. Often used to demonstrate such effects is the stem completion task. While there exist a few variations, these studies generally consist of showing participants the first few letters of words, and asking them to complete the string of letters to form an English word. It has been demonstrated that observers are significantly more likely to complete word fragments with the unattended stimuli presented in a trial than with another similar word. This effect holds when stimuli are not words, but instead objects. When photos of objects are shown too quickly for participants to identify, subsequent presentation of those items lead to significantly faster identification in comparison to novel objects.\n\nA notable study by Mack and Rock has also revealed that showing a word stimulus differing from the participant's name by one letter did not generally call conscious attention. By simply changing a character, transforming the presented word into the observer's first name, the now highly meaningful stimulus is significantly more likely to be attended to. This suggests that the stimuli are being extensively processed, at least enough to analyze their meaning. These results point to the fact that attentional selection may be determined late in processing.\n\nThe evidence outlined above suggests that even when stimuli are not processed to the level of conscious attention, they are nonetheless perceptually and cognitively processed, and can indeed exert effects on subsequent behavior.\n\nWhile the evidence supporting late selection hypotheses is significant and has been consistently reproduced, there also exists a body of research suggesting that unattended stimuli in fact may not receive significant processing.\n\nFor example, in an functional magnetic resonance imaging (fMRI) study by Rees and colleagues, brain activity was recorded while participants completed a perceptual task. Here they examined the neural processing of meaningful (words) and meaningless (consonant string) stimuli both when attended to, and when these same items were unattended. While no difference in activation patterns were found between the groups when the stimuli were unattended, differences in neural processing were observed for meaningful versus meaningless stimuli to which participants overtly attended. This pattern of results suggests that ignored stimuli are not processed to the level of meaning, i.e. less extensively than attended stimuli. Participants do not seem to be detecting meaning in stimuli to which they are not consciously attending.\n\nThis particular hypothesis bridges the gap between the early and late selection theories. Authors integrate the viewpoint of early selection stating that perception is a limited process (i.e. cognitive resources are limited), and that of the late selection theories assuming perception as an automatic process. This view proposes that the level of processing which occurs for any one stimulus is dependent on the current perceptual load. That is, if the current task is attentionally demanding and its processing exhausts all the available resources, little remains available to process other non-target stimuli in the visual field. Alternatively, if processing requires a small amount of attentional resources, perceptual load is low and attention is inescapably directed to the non-target stimuli.\n\nThe effects of perceptual load on the occurrence of inattentional blindness is demonstrated in a study by Fougnie and Marois. Here, participants were asked to complete a memory task involving either the simple maintenance of verbal stimuli, or the rearrangement of this material, a more cognitively demanding exercise. While subjects were completing the assigned task, an unexpected visual stimulus was presented. Results revealed that unexpected stimuli were more likely to be missed during manipulation of information than in the more simple rehearsal task.\n\nIn a similar type of study, fMRI recordings were done while subjects took part in either low-demand or high-demand subtraction tasks. While performing these exercises, novel visual distractors were presented. When task demands were low and used a smaller portion of the finite resources, distractors captured attention and sparked visual analysis as shown by brain activation in the primary visual cortex. These results, however, did not hold when perceptual load was high; in this condition, distractors were significantly less often attended to and processed.\n\nThus, higher perceptual load, and therefore more significant use of attentional resources, appears to increase the likelihood of inattentional blindness episodes.\n\nThe theory of inattentional amnesia provides an alternative in the explanation of inattentional blindness in suggesting that the phenomenon does not stem from failures in capture of attention or in actual perception of stimuli, but instead from a failure in memory. The unnoticed stimuli in a visual scene are attended to and consciously perceived, but are rapidly forgotten rendering them impossible to report. In essence, inattentional amnesia refers to the failure in creating a lasting explicit memory: by the time a subject is asked to recall seeing an item, their memory for the stimulus has vanished.\n\nWhile it is difficult to tease apart a failure in perception from one in memory, some research has attempted to shed light on the issue. In a now-classic study of inattentional blindness, a woman carrying an umbrella through a scene goes unnoticed. Despite stopping the video while she is walking through and immediately asking participants to identify which of two people they have seen – leaving as little delay as possible between presentation and report – observers very often fail to correctly identify the woman with the umbrella. No differences in performance were identified whether the video was stopped immediately after the unexpected event or moments later. These findings would seem to oppose the idea of inattentional amnesia, however advocates of the theory could always contend that the memory test simply came too late and that the memory had already been lost.\n\nThe very phenomenon of inattentional blindness is defined by a lack of expectation for the unattended stimulus. Some researchers believe that it is not inattention that produces blindness, but in fact the aforementioned lack of expectation for the stimuli. Proponents of this theory often state that classic methods for testing inattentional blindness are not manipulating attention per se, but instead the expectation for the presentation of a visual item.\n\nStudies investigating the effect of expectation on episodes of inattentional blindness have shown that once observers are made aware of the importance of the stimuli to be presented, for example stating that one will later be tested on it, the phenomenon essentially disappears. While admitting to possible ambiguities in methodology, Mack, one of the foremost researchers in the field, holds strongly that inattentional blindness stems predominantly from a failure of attentional capture. She points out that if expectation does not mediate instances of very closely linked phenomena such as attentional blink and change blindness (whereby participants have difficulty identifying the changing object even when they are explicitly told to look for it), it is unlikely that inattentional blindness can be explained solely by a lack of expectation for stimulus presentation.\n\nThe perceptual cycle framework has been used as another theoretical basis for inattentional blindness. The perceptual cycle framework describes attention capture and awareness capture as occurring at two different stages of processing. Attention capture occurs when there is a shift in attention due to the salience of a stimuli, and awareness capture refers to the conscious acknowledgement of stimuli. Attentional sets are important because it is composed of characteristics of stimuli an individual is processing. Inattentional blindness occurs when there is an interaction between an individual's attentional set and the salience of the unexpected stimulus. Recognizing the unexpected stimulus can occur when the characteristics of the unexpected stimulus resembles the characteristics of the perceived stimuli. The attentional set theory of inattentional blindness has implications for false memories and eyewitness testimony. The perceptual cycle framework offers four major implications about inattentional blindness 1) environmental cues aid in the detection of stimuli by providing orienting cues but is not enough to produce awareness, 2) perception requires effortful sustained attention, interpretation, and reinterpretation, 3) implicit memory may precede conscious perception, and 4) visual stimuli that is not expected, explored, or interpreted may not be perceived.\n\nOther bases for attentional blindness include top down and bottom up processing.\n\nTo test for inattentional blindness, researchers ask participants to complete a primary task while an unexpected stimulus is presented. Afterwards, researchers ask participants if they saw anything unusual during the primary task. Arien Mack and Irvin Rock describe a series of experiments that demonstrated inattentional blindness in their 1998 book, \"Inattentional Blindness.\"\n\nThe best-known study demonstrating inattentional blindness is the Invisible Gorilla Test, conducted by Daniel Simons of the University of Illinois at Urbana-Champaign and Christopher Chabris of Harvard University. This study, a revised version of earlier studies conducted by Ulric Neisser, Neisser and Becklen in 1975, asked subjects to watch a short video of two groups of people (wearing black and white T-shirts) passing a basketball around. The subjects are told either to count the passes made by one of the teams or to keep count of bounce passes vs. aerial passes. In different versions of the video a woman walks through the scene carrying an umbrella (as discussed above) or wearing a full gorilla suit. After watching the video, the subjects are asked whether they noticed anything out of the ordinary taking place. In most groups, 50% of the subjects did not report seeing the gorilla (or the woman with the umbrella). Failure to perceive the anomalies is attributed to failure to attend to it while engaged in the difficult task of counting passes of the ball. These results indicate that the relationship between what is in one's visual field and perception is based much more on attention than was previously thought.\n\nOut 228 participants of the tests, only 194 – those who did count the passes correctly – were used for statistical purposes further. The percentage was even as low as 8% in one of the 16 tests performed.\n\nThe basic Simons and Chabris study was reused on British television as a public safety advert designed to point out the potential dangers to cyclists caused by inattentional blindness in motorists. In the advert the gorilla is replaced by a moon-walking bear.\n\nIn 1995, Officer Kenny Conley was chasing a shooting suspect. An undercover officer was in the same vicinity and was mistakenly taken down by other officers while Conley ran by and failed to notice. A jury later convicted Officer Conley of perjury and obstruction of justice, believing he had seen the fight and lied about it to protect fellow officers, yet he stood by his word that he had, in fact, not seen it.\n\nChristopher Chabris, Adam Weinberger, Matthew Fontaine and Daniel J. Simons took it upon themselves to see if this scenario was possible. They designed an experiment in which participants were asked to run about 30 feet behind an experimenter, and count how many times he touched his head. A fight was staged to appear about 8 meters off the path, and was visible for approximately 15 seconds. The procedure in its entirety lasted about 2 minutes and 45 seconds, and participants were then asked to report the number of times they had seen the experimenter touch his head with either hand (medium load), both hands (high load), or were not instructed to count at all (low load). After the run, participants were asked 3 questions: 1) If they had noticed the fight; 2) if they had noticed a juggler, and 3) if they had noticed someone dribbling a basketball. Questions 2) and 3) were control questions, and no one falsely reported these as true.\n\nParticipants were significantly more likely to notice the fight when the experiment was done during the day as opposed to in the dark. Additionally, sightings of the fight were most likely to be reported in the low load condition (72%) than in either the medium load (56%), or high load conditions (42%). These results exemplify a real world occurrence of inattentional blindness, and provide evidence that officer Conley could indeed have missed the fight because his attention was focused elsewhere. Moreover, these results add to the body of knowledge suggesting that as perceptual load increases, less resources remain to process items not explicitly focused on, and in turn episodes of inattentional blindness become more frequent.\n\nAnother experiment was conducted by Steven Most, along with Daniel Simons, Christopher Chabris and Brian Scholl. Instead of a basketball game, they used stimuli presented by computer displays. In this experiment objects moved randomly on a computer screen. Participants were instructed to attend to the black objects and ignore the white, or vice versa. After several trials, a red cross unexpectedly appeared and traveled across the display, remaining on the computer screen for five seconds. The results of the experiment showed that even though the cross was distinctive from the black and white objects both in color and shape, about a third of participants missed it. They had found that people may be attentionally tuned to certain perceptual dimensions, such as brightness or shape. Inattentional blindness is most likely to occur if the unexpected stimuli presented resembles the environment.\n\nOne interesting experiment displayed how cell phones contributed to inattentional blindness in basic tasks such as walking. The stimulus for this experiment was a brightly colored clown on a unicycle. The individuals participating in this experiment were divided into four sections. They were either talking on the phone, listening to an mp3 player, walking by themselves or walking in pairs. The study showed that individuals engaged in cell phone conversations were least likely to notice the clown. This experiment was designed by Ira E. Hyman, S. Matthew Boss, Breanne M. Wise, Kira E. Mckenzie and Jenna M. Caggiano at Western Washington University.\n\nDaniel Memmert conducted an experiment which suggests that an individual can look directly at an object and still not perceive it. This experiment was based on the invisible gorilla experiment. The participants were children with an average age of 7.7 years. Participants watched a short video of a six-player basketball game (three with white shirts, three with black shirts). The participants were instructed to watch only the players wearing black shirts and to count the times the team passed the ball. During the video a person in a gorilla suit walks through the scene. The film was projected onto a large screen (3.2 m X 2.4 m) and the participants sat in a chair 6 meters from the screen. Participants' eye movement and fixations were recorded during the video, and afterward the participants answered a series of questions.\n\nOnly 40% of the participants reported seeing the gorilla. There was no significant difference in accuracy of the counting between the two groups. Analyzing the eye movement and fixation data showed no significant difference in time spent looking at the players (black or white) between the two groups. However, the 60% of participants who did not report seeing the gorilla spent an average of 25 frames (about one second) fixated on the gorilla, despite not perceiving it.\n\nA more common example of blindness despite fixation is illustrated in the game of Three-card Monte.\n\nAnother experiment conducted by Daniel Memmert tested the effects of different levels of expertise can have on inattentional blindness. The participants in this experiment included six different groups: Adult basketball experts with an average of twelve years of experience, junior basketball experts with an average of five years, children who had practiced the game for an average of two years, and novice counterparts for each age group. In this experiment the participants watched the invisible gorilla experiment video. The participants were instructed to watch only the players wearing white and to count the times the team passed the ball.\n\nThe results showed that experts did not count the passes more accurately than novices but did show that adult subjects were more accurate than the junior and child subjects. A much higher percentage of experts noticed the gorilla compared with novices and even the practiced children. 62% of the adult experts and 60% of the junior experts noticed the gorilla, suggesting that the difference between five and twelve years of experience has minimal effect on inattentional blindness. However, only 38% of the adult, 35% of the junior, and none of the child novices noticed the gorilla. Only 18% of the children with two years of practice noticed. This suggests that both age and experience can have a significant effect on inattentional blindness.\n\nArien Mack and Irvin Rock's concluded in 1998 that no conscious perception can occur without attention. Evidence through research on inattentional blindness contemplates that it may be possible that inattentional blindness reflects a problem with memory rather than with perception. It is argued that at least some instances of inattentional blindness are better characterized as memory failures than perceptual failures. The extent to which unattended stimuli fail to engage perceptual processing is an empirical question that the combination of inattentional blindness and other various measures of processing can be used to address.\n\nThe theory behind inattentional blindness research suggests that we consciously experience only those objects and events to which we directly attend. That means that the vast majority of information in our field of vision goes unnoticed. Thus if we miss the target stimulus in an experiment, but are later told about the existence of the stimulus, this sufficient awareness allows participants to report and recall the stimulus now that attention has been allocated to it. Mack and Rock, and their colleagues discovered a striking array of visual events to which people are inattentionally blind. However the debate arises whether this inattentional blindness was due to memory or perceptual processing limitations.\n\nMack and Rock note that explanations for inattentional blindness can reflect a basic failure of perceptual processes to be engaged by unattended stimuli. Or that it may reflect a failure of memorial processes to encode information about unattended stimuli. It is important to note that the memory failure does not have to do with forgetting something that has been encoded by losing access to the memory of the stimulus from time of presentation to time of retrieval, rather that the failure is attributed to information not being encoded when the stimulus was present. It seems that inattentional blindness can be explained by both memory and perceptual failures because in experimental research participants may fail to report what was on display due to failures in encoded information (memory) or a failure in perceptually processed information (perception).\n\nThere are similarities in the types of unconscious processing apparent in inattentional blindness and in neuropsychological syndromes such as visual neglect and extinction. The analogy between these phenomenon's seems to generate more questions as well as answers. These answers are fundamental for our understanding of the relationship between attention, stimulus coding and behavior.\n\nResearch has shown that some aspects of the syndrome of unilateral visual neglect appear to be similar to normal subjects in a state of inattentional blindness. In neglect, patients with lesions to the parietal cortex fail to respond to and report stimuli presented on the side of space contralateral to damage. That is, they appear to be functionally blind to a range of stimuli. Since such lesions do not result in any sensory deficits, shortcomings have been explained in terms of a lack of attentional processing, for which the parietal cortex plays a large role. These phenomena draw strong parallels to one another, as in both cases stimuli are perceptible but unreported when unattended.\n\nIn the phenomenon of extinction, patients can report the presence of a single stimulus presented on the affected side, but then fail to detect it when a second stimulus is presented simultaneously on the \"good\" (ipsilateral) side. Here the stimulus on the affected side seems to lose under conditions of attentional competition from stimuli in the ipsilesional field. The consequence of this competition is that the extinguished items may not be detected.\n\nSimilar to studies of inattentional blindness, there is evidence of processing taking place in the neglected field. For example, there can be semantic priming from a stimulus presented in the neglected field, which affects responses to stimuli subsequently presented on the unimpaired side. Apparently in both neglect and inattentional blindness, there is some level processing of stimuli even when they are unattended. However one major difference between neuropsychological symptoms such as neglect and extinction, and inattentional blindness concerns the role of expectation. In inattentional blindness, subjects do not expect the unreported stimulus. In contrast, in neglect and extinction, patients may expect a stimulus to be presented on the affected side but still fail to report it when another it may be that expectation affects reportability but not the implicit processing of stimuli.\n\nFurther explanations of the phenomenon of inattentional blindness include inattentional amnesia, inattentional agnosia and change blindness.\n\nAn explanation for this phenomenon is that observers see the critical object in their visual field but fail to process it extensively enough to retain it. Individuals experience inattentional agnosia after having seen the target stimuli but not consciously being able to identify what the stimuli is. It is possible that observers are not even able to identify that the stimuli they are seeing are coherent objects. Thus observers perceive some representation of the stimuli but are actually unaware of what that stimulus is. It is because the stimulus is not encoded as a specific thing, that it later is not remembered. Individuals fail to report what the stimuli is after it has been removed. However, despite a lack in ability to fully process the stimuli, experiments have shown a priming effect of the critical stimuli. This priming effect indicates that the stimuli must have been processed to some degree, this occurs even if observers are unable to report what the stimuli is.\n\nInattentional blindness is the failure to see a stimulus, such as an object that is present in a visual field. However, change blindness is the failure to notice something different about a visual display. Change blindness is a directly related to memory, individuals who experience the effects of change blindness fail to notice something different about a visual display from one moment to the next. In experiments that test for this phenomenon participants are shown an image that is then followed by another duplicate image that has had a single change made to it. Participants are asked to compare and contrast the two images and identify what the change is. In inattentional blindness experiments, participants fail to identify some stimulus in a single display, a phenomenon that doesn't rely on memory the way change blindness does. Inattentional blindness refers to an inability to identify an object all together whereas change blindness is a failure to compare a new image or display to one that was previously stored in memory.\n\nIn 2006, Daniel Memmert conducted a series of studies in which he tested the how age and expertise of participants affect inattentional blindness. Using the gorilla video, he tested 6 different groups of participants. There were 2 groups of children (average age=7) half with no experience in basketball, and the other half with 2 years experience; 2 groups of juniors (average age=13) half with no experience in basketball, and the other half with 5 years of experience; and 2 groups of adults (average age = 24) half with no experience in basketball, the other half with over 12 years of experience. He then instructed all the groups to keep track of how many passes the people on the black team made.\n\nOverall, the children with or without any basketball experience failed to perceive the gorilla more than the juniors or the adults. There were no significant difference between the inexperienced junior and adult groups, or between the experienced junior and adult groups. This pattern of results suggests that until the approximate age of 13, presumably because certain aspects of cognition are still under development, inattentional blindness occurrences are more frequent, but become consistent throughout the remainder of the life span.\n\nAdditionally, the juniors with basketball experience noticed the gorilla significantly more than the juniors with no basketball experience; and the group of experienced adults noticed the gorilla significantly more than the non-experienced adults. This suggests that if one has had much experience with the stimuli in a visual field, they are more likely to consciously perceive the unexpected object.\n\nIn 2011, Elizabeth Graham and Deborah Burke conducted a study that assessed whether or not older adults are more susceptible to inattentional blindness than younger adults by having 51 younger-aged participants (17 to 22 years) and 61 older-aged participants (61 to 81 years) watch the classic gorilla video. Overall, they found that younger-aged participants were more likely to notice the unexpected gorilla than older-aged participants.\n\nIn a 2015 study, Cary Stothart, Walter Boot, and Daniel Simons attempted to replicate and extend the findings from both Graham and Burke's 2011 study and Steven Most and colleague's 2000 study on Amazon Mechanical Turk using a sample of 515 participants that varied in age. In this study, participants were tasked with counting the number of times a number of white moving objects crossed the vertical midpoint of a display while ignoring a number of black moving objects. The unexpected object in this case was a gray cross that moved horizontally across the display at various distances from the vertical midpoint (this was manipulated between participants). Overall, they found that inattentional blindness susceptibility increases with age, which replicates the finding from Graham and Burke. In fact, they found that every 10 years of age was associated with a 1.3 fold increase in the probability of displaying inattentional blindness. They also found that the probability of inattentional blindness increases as the distance between the observer's focus of attention and the unexpected object increases, which replicates the finding from Most and colleagues. However, they also found that the relationship that age has with inattentional blindness does not change as a function of the unexpected object's distance from the focus of attention, suggesting that useful field of view does not mediate the relationship between age and inattentional blindness.\n\nA series of studies conducted to test how similarity can influence the perception of a present stimulus. In the study, they asked participants to fixate on a central point on a computer screen and count how many times either white or black letters bounced off the edges of the screen. The first 2 trials did not contain an unexpected event, but the third trial was the critical trial in which a cross that had the same dimensions as the letters and varied in colour (white/light gray/dark gray/black) moved from the right side of the screen to the left side and passed through the central point. The results revealed the following: during the critical event, the more similar the colour of the cross was to the colour of the attended letters, the more likely the participants were to perceive it, and the less similar the colour of the cross was to the attended colour decreased the likelihood of the cross being noticed. For the participants attending to the black letters, 94% perceived the black cross; 44% perceived the dark gray cross; 12% perceived the light gray cross, and only 6% perceived the white cross. Similarly, if the participant was attending to the white letters, they were more likely to notice the cross it was white (94%) than if it was light gray (75%), dark gray (56%), or black (0%). This study demonstrates that the more similar an unexpected object is to the attended object, the more likely it is to be perceived, thus reducing the chance of inattentional blindness.\n\nA large experiment conducted on 794 participants by Schofield, Creswell and Denson found evidence that completing a brief mindfulness exercise reduced rates on inattentional blindness, but did not improve the depth of encoding of the unexpected distractor. Participants in this experiment engaged in a guided-audio task of mindfully eating a raisin, a well-known task introduced by Kabat-Zinn in his mindfulness-based stress reduction program, or listened to factual descriptions about raisins. The audio recordings used to manipulate mindful states in this experiment are freely available online. Participants who completed the raisin-eating task had 41% greater odds of noticing an unexpected red cross that floated across the screen. Participants were then asked to select the shape that had unexpectedly appeared (i.e., the red cross) out of a line-up of 3 red and 3 green shapes. Those in the mindfulness condition were no better than those in the control condition at selecting the red cross out of the line-up. This was true regardless of whether or not detection of the unexpected distractor was statistically controlled. This experiment demonstrated that not only does mindfulness affect inattentional blindness, but that detailed encoding of the unexpected distractor can be dissociated from the detection of the unexpected distractor.\n\nThe research that has been done on inattentional blindness suggests that there are four possible causes for this phenomenon. These include: conspicuity, mental workload, expectations, and capacity.\n\nConspicuity refers to an object's ability to catch a person's attention. When something is conspicuous it is easily visible. There are two factors which determine conspicuity: sensory conspicuity and cognitive conspicuity. Sensory conspicuity factors are the physical properties an object has. If an item has bright colors, flashing lights, high contrast with environment, or other attention-grabbing physical properties it can attract a person's attention much easier. For example, people tend to notice objects that are bright colors or crazy patterns before they notice other objects. Cognitive conspicuity factors pertain to objects that are familiar to someone. People tend to notice objects faster if they have some meaning to their lives. For example, when a person hears his/her name, their attention is drawn to the person who said it. The cocktail party effect describes the cognitive conspicuity factor as well. When an object isn't conspicuous, it is easier to be inattentionally blind to it. People tend to notice items if they capture their attention in some way. If the object isn't visually prominent or relevant, there is a higher chance that a person will miss it.\n\nMental workload is a person's cognitive resources. The amount of a person's workload can interfere with processing of other stimuli. When a person focuses a lot of attention on one stimulus, he/she focuses less attention on other stimuli. For example, talking on the phone while driving – the attention is mostly focused on the phone conversation, so there is less attention focused on driving. The mental workload could be anything from thinking about tasks that need to be done to tending to a baby in the backseat. When people have most of their attention focused on one thing, they are more vulnerable to inattentional blindness. However, the opposite is true as well. When a person has a very small mental workload – he/she is doing an everyday task – the task becomes automatic. Automatic processing can lessen one's mental workload, which can lead to a person to missing the unexpected stimuli.\n\nWorking memory also contributes to inattentional blindness. Cognitive psychologists have examined the relationship between working memory and inattention, but evidence is inconclusive. The rate of this phenomenon can be impacted by a number of factors. Researchers have found evidence for a number of components that may play a role. These include features of the object and the current task, where an individual's attention lies relative to the object, and mental workload as mentioned above. Researchers Kreitz, Furley, and Memmery in 2015, asserted that working memory capacity is not an indicator of susceptibility to inattentional blindness. Instead, it is a combination of what stimulus the attention is directed to as well as the individual's personal expectations. There are individual differences that can play a role, but some argue those disparities are separate from capacity for working memory. On the other hand, there are researchers who consider differences between individuals and their working memory capacity to be a stronger determinant of inattentional blindness. Seegmiller, Watson, and Strayer in 2011 for example, studied individual differences in working memory capacity and how that overall impacted their attention on a given task. They utilized the same Invisible Gorilla video Simons and Chabris did (as mentioned above), but they additionally had participants complete a mathematics test to measure their capacity. From their results, they were able to find a high correlation between an individual's working memory capacity and their susceptibility to inattentional blindness. Those who were calculated to have a lower capacity, more often experienced the blindness.\n\nIn a follow up study the same year, Kreitz and her team looked specifically at the cognitive abilities between individuals. Her team employed a variety of tasks, both static and dynamic, to compare the participants who had their cognitive capacity measured beforehand. Even though they included different tasks to test individuals, there was not a measurable relationship between the cognitive abilities of a participant and their attention performance. They did, however, find evidence to support the idea that noticing a certain stimuli was better in those demonstrating expertise in the task subject (referenced above). Overall, Kreitz concluded that cognitive/working memory capacity might not be an accurate measure for inattentional blindness. Instead, they determined that the rate of noticing might be both circumstantial and dependent on the requirements of the task.\n\nThere are also researchers who subscribe to the idea that working memory does not play a measurable role in attentional blindness. This is different from the study by Kreitz and her team finding that individual differences in cognitive abilities might not be relative to noticing rates. Bredemeier and Simons conducted two studies in 2012. The first involved identifying the location of letters as well as counting how many times a group of shapes touched one another. These served as spatial and attention tasks respectively. The second study utilized the same tasks as the previous, but included a verbal one. Participants had to solve math problems and then remember a particular letter that followed each equation. From their results, the two researchers questioned if there was a relationship between noticing a particular stimuli and cognitive abilities. Instead of other factors contributing to the working memory of an individual's noticing, Bredemeier and Simons postulated that external variables establish the appearance of this relationship. Finally, the two researchers attempted to explain why studies were yielding conflicting results. The reason for why this research seems particularly inconclusive might be a result of disparities between the design of the actual research. Essentially, a variety of confounded variables might be prevalent across the studies when considering methodology and sampling processes. A more regulated, large-scale experiment could lead to more conclusive findings.\n\nWhen a person expects certain things to happen, he/she tends to block out other possibilities. This can lead to inattentional blindness. For example, person X is looking for their friend at a concert, and that person knows their friend (person Y) was wearing a yellow jacket. In order to find person Y, person X looks around for people wearing yellow. It is easier to pick a color out of the crowd than a person. However, if person Y took off the jacket, there is a chance person X could walk right past person Y and not notice because he/she was looking for the yellow jacket. Because of expectations, experts are more prone to inattentional blindness than beginners. An expert knows what to expect when certain situations arise. Therefore, that expert will know what to look for. This could cause that person to miss out on other important details that he/she may not have been looking for.\n\nAttentional capacity, or neurological salience, is a measure of how much attention must be focused to complete a task. For example, an expert pianist can play a piano without thinking much, but a beginner would have to consciously think of every note they hit. This capacity can be lessened by drugs, alcohol, fatigue, and age. With a small capacity, it is more possible to miss things. Therefore, if a person is drunk, he/she will probably miss more than a sober person would. If your attentional capacity is large, you are less likely to experience inattentional blindness.\n\nWilliam James addressed the benefits of attention by saying, \"Only those items which I notice shape my mind – without selective interest, experience is utter chaos\". Humans have a limited mental capacity that is incapable of attending to all the sights, sounds and other inputs that rush the senses every moment. Inattentional blindness is beneficial in the sense that it is a mechanism that has evolved with attention to help filter out irrelevant input, allowing only important information to reach consciousness. Several researchers, notably James J. Gibson, have argued that, even before the retina, perception begins in the ecology, which has turned perceptual processes into informational relationships in the environment through evolution. This allows humans to focus our limited mental resources more efficiently in our environment. For example, New et al. maintain that survival required monitoring animals, both human and non-human, to become part of the evolutionary adaptiveness of the human species. They found that when participants were shown an image with a rapidly altering scene where the scene change included an animate or inanimate object that the participants were significantly better at identifying humans and animals. New et al. argue that better performance in detecting animals and humans is not a factor of acquired expertise, rather it is an evolved survival mechanism in human perception.\n\nInattentional blindness is also beneficial as a response to advertising overload. Irrelevant marketing makes it more likely for consumers to ignore initiatives that aim at capturing their attention. This phenomenon called 'purposeful blindness' has a compelling illustration regarding banner ads. Banner blindness shows that consumers can adopt fast and become good at ignoring marketing messages that are not relevant.\n\nAlthough the bulk of inattentional blindness research has been conducted in laboratory studies, the phenomenon occurs in a variety of everyday contexts. Depending upon the context, the occurrence of inattentional blindness could range from embarrassing and/or humorous to potentially devastating.\n\nSeveral recent studies of explicit attention capture have found that when observers are focused on some other object or event, they often experience inattentional blindness. This finding has potentially tragic implications for distracted driving. If a person's attention is focused elsewhere while driving, carrying on a conversation or text messaging, for example, they could fail to notice salient and distinctive objects, such as a stop sign, which could lead to serious injury and possibly even death. There have also been heinous incidents attributed to inattentional blindness behind the wheel. For example, a Pennsylvania highway crew accidentally paved over a dead deer that was lying on the road. When questioned regarding their actions, the workers claimed to have never seen it.\n\nMany policies are being implemented around the world to decrease the competition for explicit attention capture while operating a vehicle. For example, there are legislative efforts in many countries aimed at banning or restricting the use of cell phones while driving. Research has shown that the use of both hands-free and hand-held cellular devices while driving results in the failure of attention to explicitly capture other salient and distinctive objects, leading to significantly delayed reaction times, as well as inattentional blindness. A study published in 1997, based on accident data in Toronto, found the risk involved in driving while using a cell phone to be similar to that of driving drunk. In both cases, the risk of a collision was three to six times higher compared to a sober driver not using a cell phone. Moreover, Strayer et al. (2006) found that when controlling for driving difficulty and time on task, cell-phone drivers exhibited greater impairment than intoxicated drivers, using a high-fidelity driving simulator.\n\nInattentional blindness is also prevalent in aviation. The development of heads-up display (HUD) for pilots, which projects information onto the windshield or onto a helmet-mounted display, has enabled pilots to keep their eyes on the windshield, but simulator studies have found that HUD may cause runway incursion accidents, where one plane collides with another on the runway. This finding is particularly concerning because HUDs are being employed in automobiles, which could lead to potential roadway incursions. When a particular object or event captures attention to the extent to which the beholders' attentional capacity is completely absorbed, the resulting inattentional blindness has been known to cause dramatic accidents. For example, an airliner crew, engrossed with a blinking console light, failed to notice the approaching ground and register hearing the danger alarm sounding before the airliner crashed.\n\nCollaborative efforts to establish links between science and illusion have examined the relationship of the processes underlying inattentional blindness and the concept of misdirection—a magician's ability to manipulate attention in order to prevent his/her audience from seeing how a trick was performed. In several misdirection studies, including Kuhn and Tatler (2005), participants watch a \"vanishing item\" magic trick. After the initial trial, participants are shown the trick until they detect the item dropping from the magician's hand. Most participants see the item drop on the second trial. The critical analyses involved differences in eye movements between the detected and undetected trials. These repetition trials are similar to the full-attention trial in the inattentional blindness paradigm, as both involve the detection of the unexpected event and, by detecting the unexpected event on the second trial, demonstrate that the event is readily perceivable.\n\nThe main difference between inattentional blindness and misdirection involves how attention is manipulated. While inattentional blindness tasks require an explicit distractor, the attentional distraction in misdirection occurs through the implicit yet systematic orchestration of attention. Moreover, there are several varieties of misdirection and different types are likely to induce different cognitive and perceptual processes, which vary the misdirection paradigm's resemblance to inattentional blindness.\n\nAlthough the aims of magic and illusion differ from those of neuroscience, magicians wish to exploit cognitive weaknesses, whereas neuroscientists seek to understand the brain and the neuronal significance of cognitive functions. Several researchers have argued that neuroscientists and psychologists can learn from incorporating the real world experience and knowledge of magicians into their fields of research. The techniques developed over centuries of stage magic by magicians may also be utilized by neuroscience as powerful probes of human cognition.\n\nWhen a police officer's version of events differs from video or forensic evidence, inattentional blindness has been used by defense lawyers as a possibility. The criticism of this defense is that this view could be used to defend nearly any police shooting.\n\n\n"}
{"id": "2330609", "url": "https://en.wikipedia.org/wiki?curid=2330609", "title": "Jonathan Ned Katz", "text": "Jonathan Ned Katz\n\nJonathan Ned Katz (born 1938) is an American historian of human sexuality who has focused on same-sex attraction and changes in the social organization of sexuality over time. His works focus on the idea, rooted in social constructionism, that the categories with which we describe and define human sexuality are historically and culturally specific, along with the social organization of sexual activity, desire, relationships, and sexual identities.\n\nKatz graduated from The High School of Music & Art in New York City with a major in art in 1956. Since 2004, he has begun to emerge publicly as a visual artist. He went on to study at Antioch College, the City College of New York, The New School, and Hunter College. As a teenager, Katz was featured in \"Life\" magazine for his efforts to create a film version of \"Tom Sawyer\".\n\nKatz taught as an adjunct at Yale University, Eugene Lang College, and New York University, was the convener of a faculty seminar at Princeton University, and was a keynote speaker at Harvard University. He is a founding member of the Gay Academic Union in 1973 and the National Writers Union in 1980. He was the initiator and is the director of OutHistory.org, a site devoted to lesbian, gay, bisexual, transgender, queer, (LGBTQ) and heterosexual history, that went online in September 2008, and was produced in its first four years by the Center for Lesbian and Gay Studies, an institute at the City University of New York Graduate Center, under a grant from the Arcus Foundation. Since 2012, the site has been co-directed by Katz and John D'Emilio.\n\nKatz received the Magnus Hirschfeld Medal for Outstanding Contributions to Sex Research from the German Society for Social-Scientific Sexuality Research in 1997. In 2003, he was given Yale University's Brudner Prize, an annual honor recognizing scholarly contributions in the field of lesbian and gay studies. His papers are collected by the manuscript division of The Research Libraries of The New York Public Library.\n\nHe received the Bill Whitehead Award for Lifetime Achievement from Publishing Triangle in 1995.\n\nKatz artist practice focused on same-sex and different-sex relationships, and changes in the social construction of sexuality over time. His works stress that the social organization of human sexual activity, desire, relationships, and sexual identities are historically and culturally specific, along with the categories with which we name, describe, define and understand human sexuality.\n\n\"The Invention of Heterosexuality\" was first published as an essay in 1990 and then expanded into a larger book. In it, Katz traces the development of \"heterosexual\" and \"homosexual\" and all the ideology, social and economic relations, gender expectations that were packed into it. He notes the radical change, in the late nineteenth century, from a sexual ethic of procreation to one based on erotic pleasure and sexual object choice. Noting the distinction that a procreation-based ethic condemns all non-procreative sex, categorizing sexual relations based primarily on this point. A gender-based sexual ethic is concerned with procreative sex on a secondary level, if at all.\n\nKatz follows the development of \"heterosexual\" as going through several stages. Coined in 1868 (in German, \"Heterosexualität\") by Karl Heinrich Ulrichs, the term, used to pathologize certain behaviors, initially referred to a person with an overwhelming drive toward the opposite sex and was associated with a number of pathologized behaviors. In 1889, Richard Freiherr von Krafft-Ebing used the term in something like its modern-day sense. The first known use in America was in 1892, by James G. Kiernan. Here, it referred to some combination of bisexuality and a tendency to thwart the then-existing procreation ethic.\n\nKrafft-Ebing's \"Psychopathia Sexualis\", published in 1889, and then in English in 1892, marked the clear turning point from a procreation-based sexuality to a pleasure-based ethic which focused on gender to define the normal and the abnormal. Krafft-Ebing did not, however, make a clean break from the old procreative standards. In much of the discourse of the time, the heterosexual was still a deviant figure, since it signified a person unconcerned with the old sexual norms.\n\nFor a variety of economic and social reasons, Katz argues, during the end of the nineteenth and the beginning of the twentieth centuries, this new norm became more firmly established and naturalized, marking out new gender and sexual norms, new social and family arrangements, and new deviants and perverts. One of the important consequences of this line of thought which Katz notes in \"\"Homosexual\" and \"Heterosexual\": Questioning the Terms\", is that we can only generalize sexual identities onto the past with a limited degree of accuracy: \"So profound is the historically specific character of sexual behavior that only with the loosest accuracy can we speak of sodomy in the early colonies and 'sodomy' in present-day New York as 'the same thing.' In another example, to speak of 'heterosexual behavior' as occurring universally is to apply one term to a great variety of activities produced within a great variety of sexual and gender systems.\"\n\n\n\n\n"}
{"id": "261987", "url": "https://en.wikipedia.org/wiki?curid=261987", "title": "Kanban", "text": "Kanban\n\nKanban became an effective tool to support running a production system as a whole, and an excellent way to promote improvement. Problem areas are highlighted by measuring lead time and cycle time of the full process and process steps. One of the main benefits of kanban is to establish an upper limit to work in process inventory to avoid overcapacity. Other systems with similar effect are for example CONWIP. A systematic study of various configurations of kanban systems, of which CONWIP is an important special case, can be found in Tayur (1993), among other papers.\n\nA goal of the kanban system is to limit the buildup of excess inventory at any point in production. Limits on the number of items waiting at supply points are established and then reduced as inefficiencies are identified and removed. Whenever a limit is exceeded, this points to an inefficiency that should be addressed.\n\nThe system originates from the simplest visual stock replenishment signaling system, an empty box. This was first developed in the UK Spitfire factories during the war and was known as the “two bin system.” In the late 1940s, Toyota started studying supermarkets with the idea of applying shelf-stocking techniques to the factory floor. In a supermarket, customers generally retrieve what they need at the required time—no more, no less. Furthermore, the supermarket stocks only what it expects to sell in a given time, and customers take only what they need, because future supply is assured. This observation led Toyota to view a process as being a customer of one or more preceding processes and to view the preceding processes as a kind of store.\n\nKanban aligns inventory levels with actual consumption. A signal tells a supplier to produce and deliver a new shipment when a material is consumed. This signal is tracked through the replenishment cycle, bringing visibility to the supplier, consumer, and buyer.\n\nKanban uses the rate of demand to control the rate of production, passing demand from the end customer up through the chain of customer-store processes. In 1953, Toyota applied this logic in their main plant machine shop.\n\nA key indicator of the success of production scheduling based on demand, \"pushing,\" is the ability of the demand-forecast to create such a \"push\". Kanban, by contrast, is part of an approach where the \"pull\" comes from demand and products are made to order. Re-supply or production is determined according to customer orders.\n\nIn contexts where supply time is lengthy and demand is difficult to forecast, often, the best one can do is to respond quickly to observed demand. This situation is exactly what a kanban system accomplishes, in that it is used as a demand signal that immediately travels through the supply chain. This ensures that intermediate stock held in the supply chain are better managed, and are usually smaller. Where the supply response is not quick enough to meet actual demand fluctuations, thereby causing potential lost sales, a stock building may be deemed more appropriate and is achieved by placing more kanban in the system.\n\nTaiichi Ohno stated that to be effective, kanban must follow strict rules of use. Toyota, for example, has six simple rules, and close monitoring of these rules is a never-ending task, thereby ensuring that the kanban does what is required.\n\nToyota has formulated six rules for the application of kanban:\n\nKanban cards are a key component of kanban and they signal the need to move materials within a production facility or to move materials from an outside supplier into the production facility. The kanban card is, in effect, a message that signals a depletion of product, parts, or inventory. When received, the kanban triggers replenishment of that product, part, or inventory. Consumption, therefore, drives demand for more production, and the kanban card signals demand for more product—so kanban cards help create a demand-driven system.\n\nIt is widely held by proponents of lean production and manufacturing that demand-driven systems lead to faster turnarounds in production and lower inventory levels, helping companies implementing such systems be more competitive.\n\nIn the last few years, systems sending kanban signals electronically have become more widespread. While this trend is leading to a reduction in the use of kanban cards in aggregate, it is still common in modern lean production facilities to find the use of kanban cards. In various software systems, kanban is used for signalling demand to suppliers through email notifications. When stock of a particular component is depleted by the quantity assigned on kanban card, a \"kanban trigger\" is created (which may be manual or automatic), a purchase order is released with predefined quantity for the supplier defined on the card, and the supplier is expected to dispatch material within a specified lead-time.\n\nKanban cards, in keeping with the principles of kanban, simply convey the need for more materials. A red card lying in an empty parts cart conveys that more parts are needed.\n\nAn example of a simple kanban system implementation is a \"three-bin system\" for the supplied parts, where there is no in-house manufacturing. One bin is on the factory floor (the initial demand point), one bin is in the factory store (the inventory control point), and one bin is at the supplier. The bins usually have a removable card containing the product details and other relevant information—the classic kanban card.\n\nWhen the bin on the factory floor is empty (because the parts in it were used up in a manufacturing process), the empty bin and its kanban card are returned to the factory store (the inventory control point). The factory store replaces the empty bin on the factory floor with the full bin from the factory store, which also contains a kanban card. The factory store sends the empty bin with its kanban card to the supplier. The supplier's full product bin, with its kanban card, is delivered to the factory store; the supplier keeps the empty bin. This is the final step in the process. Thus, the process never runs out of product—and could be described as a closed loop, in that it provides the exact amount required, with only one spare bin so there is never oversupply. This 'spare' bin allows for uncertainties in supply, use, and transport in the inventory system. A good kanban system calculates just enough kanban cards for each product. Most factories that use kanban use the coloured board system (heijunka box).\n\nMany manufacturers have implemented electronic kanban (sometimes referred to as E-kanban) systems. These help to eliminate common problems such as manual entry errors and lost cards. E-kanban systems can be integrated into enterprise resource planning (ERP) systems, enabling real-time demand signaling across the supply chain and improved visibility. Data pulled from E-kanban systems can be used to optimize inventory levels by better tracking supplier lead and replenishment times.\n\nE-kanban is a signaling system that uses a mix of technology to trigger the movement of materials within a manufacturing or production facility. Electronic Kanban differs from traditional kanban in that it uses technology to replace traditional elements such as kanban cards with barcodes and electronic messages such as email or Electronic data interchange.\n\nA typical electronic kanban system marks inventory with barcodes, which workers scan at various stages of the manufacturing process to signal usage. The scans relay messages to internal/external stores to ensure the restocking of products. Electronic kanban often uses the internet as a method of routing messages to external suppliers and as a means to allow a real-time view of inventory, via a portal, throughout the supply chain.\n\nOrganizations such as the Ford Motor Company and Bombardier Aerospace have used electronic kanban systems to improve processes. Systems are now widespread from single solutions or bolt on modules to ERP systems.\n\nIn a kanban system, adjacent upstream and downstream workstations communicate with each other through their cards, where each container has a kanban associated with it. Economic Order Quantity is important. The\ntwo most important types of kanbans are:\n\n\n"}
{"id": "57675143", "url": "https://en.wikipedia.org/wiki?curid=57675143", "title": "Kia (magic)", "text": "Kia (magic)\n\nWithin the magical system of Austin Osman Spare, \"Kia\" is a mystical concept – a sort of universal consciousness or unity, similar to the Tao. The concept has been adopted by numerous other occultists, such as Kenneth Grant, and has been particularly influential on the chaos magic movement.\n\nPhil Baker has speculated that the word \"Kia\" may have been influenced by \"esoteric Eastern and cabalistic words\" such as \"ki\", \"chi\", \"khya\" and \"chiah\". Or, alternatively, it may have been taken from \"The Secret Doctrine\" by Madame Blavatsky, which refers to a universal power called \"Kia-yu\".\n\nThe first reference to \"Kia\" appears on a painting exhibited by Spare in 1904, when he was just 17 years old. Spare conceived of Kia as a sort of universal mind, of which individual human consciousnesses are aspects. Spare further elaborated on the concept in \"The Book of Pleasure\" (1913), introducing it with the words:\nOf name it has no name, to designate. I call it Kia I dare not claim it as myself. The Kia which can be expressed by conceivable ideas, is not the eternal Kia, which burns up all belief but is the archetype of \"self,\" the slavery of mortality.\n\nSpare's words here bear marked resemblance to the \"Tao Te Ching\", which states \"The Tao that can be told is not the eternal Tao; The name that can be named is not the eternal name.\" – and like the Tao, or the Hindu Brahman, or the Sunyata of Buddhism, Spare's conception of Kia combined pure transcendent consciousness with a voidness inherent in all things.\n\nThe writer and occultist Kenneth Grant, who had known Spare in the 1950s, promoted Spare's ideas after his death, referring to his system as \"Zos Kia Cultus\". Grant emphasises the similarities between Kia and certain concepts from Eastern philosophy, comparing it to the Yab-Yum of Tantra, and describing reality as the dream of Kia. However, Grant also states that Kia differs from the Buddhist concept of voidness, in that it is realised through the body, rather than the mind, stating: \"The Kia is present everywhere, but the immediacy of its realisation is sought through the flesh, as in Zen it is apprehended through the mind. The object is the same in both methods, but the means appear to vary.\"\n\nGrant also departs from Spare in conceptualising Kia as inherently \"feminine\", equating it with both the Wiccan Goddess and Aleister Crowley's Babalon – the basic idea being that Kia is a sort of all-encompassing void that is impregnated by the will of the magician, and that gives birth to magical results.\n\nPeter J. Carroll, one of the founders of chaos magic (along with Ray Sherwin), elaborated a system heavily influenced by Spare in his early writings, particularly \"Liber Null\" (1978). However, somewhat confusingly, Carroll uses the term \"Kia\" to refer to the consciousness of the individual: \"the elusive 'I' which confers self-awareness\". The more general universal force, of which Kia is an aspect, Carroll termed \"Chaos\".\nThe unity which appears to the mind to exert the twin functions of will and perception is called \"Kia\" by magicians. Sometimes it is called the spirit, or soul, or life force, instead... Kia is capable of occult power because it is a fragment of the great life force of the universe... The \"thing\" responsible for the origin and continued action of events is called \"Chaos\" by magicians... Chaos... is the force which has caused life to evolve itself out of dust, and is currently most concentratedly manifest in the human life force, or Kia, where it is the source of consciousness... To the extent that the Kia can become one with Chaos it can extend its will and perception into the universe to accomplish magic.\n\n"}
{"id": "24006726", "url": "https://en.wikipedia.org/wiki?curid=24006726", "title": "Marc Bogaerts", "text": "Marc Bogaerts\n\nMarc Bogaerts is a Belgian choreographer and artistic director living in Berlin. He has worked internationally for over more than 50 dance, opera and circus companies.\nBogaerts stipulates that our current generational dynamic is complex and routed in the mundane and monotonous rigors of our daily need to survive or surpass. Therefore, he believes in the need to counterbalance routine by symmetrically presenting traditionally segregated styles such as opera, ballet, nouveau circus and stationary art.\nFollowing postmodernism, Marc Bogaerts builds a bridge between all forms of physical expression. Making unconventional combinations (modern dancers with athletes, circus artists with ballroom dancers, ice skaters with snake women, breakdancers with classical dancers) and bringing in his productions sport, art, ecology/innovations together he creates unusual symbiosis.\n\nHe studied 7 years Latin, Greek and philosophy with the Jesuits, which influenced his later work by integrated understanding of all sports, the unity of body and soul, as it was formulated by the ancient Greeks. In 1975 he started his career as a dancer and soon also as a choreographer with the Royal Ballet of Flanders. From 1982 to 1993 he resided in New York City, where he danced as an honorary guest performer with Merce Cunningham, Trisha Brown and Laura Dean, taught at the Actors Studio in New York and initiated numerous artistic social projects raising awareness about abortion, drugs and genocide. From New York he moved to Berlin. As the only choreographer working for all three opera houses in Berlin: Deutsche Oper Berlin, Staatsoper Unter den Linden, Komische Oper Berlin he created performances such as \"Carmina Burana\", \"L’Histoire du Soldat\" or \"Four Seasons\". He created a lot of social-cultural works worldwide with major involvement of youth. E.g. USA \"Missing Persons\" on human rights, or issues on abortion, drugabuse, terrorism each time touching the nerf of times without insulting or scandalizing, which for him is the easy way out. Living all around the world he finds inspiration for his interdisciplinary productions. At the moment creating a synthesis of sport and stage art is in the centre of his artistic work.\n\nHe choreographed so far for more than 48 operas, dance companies and circus groups including:\n\n\nHe represented Belgium at the Contemporary Dance Festival in the Guggenheim Museum (Bilbao, Spain)\n\n\"The Emperor’s Dream\" with performances in Brussels, Frankfurt (Jahrhunderhalle) and Madrid was a production made for the celebrations of the change of millennium.\n\nFor The National Ballet of the Opera Bucharest he made a contemporary dance production \"Midsummernight’s dream\".\n\nFor the International Theatre Festival of Sibiu he choreographed the production \"Moi Rodin\" (director- Mihai Maniutiu, text by Patrick Roegiers). Performances were shown also at the International Theatre Festival in Russia, Poland, Israel, France, Korea, Canada.\n\nHe created a multi dance project \"De koninginnen van de nacht\" with participation of: and Geike Arnaert, European and World Champion in Karate Tina Bellemans, boxer and ex-Mr. Universe Bill Richardson accompanied by live drum music with the Japanese Wadokyo / taiko Tenbe.\n\nHis production \"Not strictly Rubens\" for the Royal Ballet of Flanders (music- Praga Kahn, costumes- Walter Van Beirendonck) involved creating new MTV video clip, a CD and a fashion exhibition in Antwerp.\n\nHis production \"DuXtrou\" (Antwerp Accent) was described by the press as a poetic, unprecedented event.\n\nHe made \"Dorian Gray\" (Oscar Wilde) for the ballet and orchestra of the .\n\nHis \"Sleeping Beauty\" with 40 actors and dancers was shown on a contemporary dance theater presentation for the Cluj-Napoca National Theatre, Romania.\n\n\n\n\n\n\n\nSince 10 years Bogaerts works worldwide in close cooperation with numerous Olympic training centers in order to bring more value in the world of sport. He led many artistic skate dancers to the Olympic Games, did the opening of the Pan American Games and was also the director of the Berlin International in Deutschlandhalle where he proposed 18,000 spectators (live RBB broadcast) his vision of bringing sport and art together. With the idea of Sport vs Art he made in Belgium productions for Rotterdam Sport Congress, FIG-Gala (Ghent 2006), Flemish Sport Prize Award (Brussels, KVS 2006), \"Apotheosis\" (Antwerp), Ghent Sports Arena, West-Flemish Sport Prize, \"S.P.O.R.T.\" Hour Culture at the Catholic University of Leuven.\n\nConvinced about stagnation of modern dance after the death of Limón and Graham, foreseeing new development in contemporary and classical dance, Bogaerts developed a methodical-pedagogic system starting at age of twelve, that combines the Vaganova method with Limón technique and emphasises on the knowledge he acquired working for many Olympic Training centres worldwide.\nHe taught this system at the , Ballettschule der Wiener Staatsoper, Heinz-Bosl-Stiftung Munich, the Royal Ballet School Antwerp, the Australian Conservatory of Ballet, the Victoria College of the Arts, University of North Carolina School of the Arts, Saint Petersburg and the Royal Danish Ballet School.\n\n\n\n"}
{"id": "14240521", "url": "https://en.wikipedia.org/wiki?curid=14240521", "title": "McGuire's Motivations", "text": "McGuire's Motivations\n\nMcGuire’s Psychological Motivations is a classification system that organizes theories of motives into 16 categories. The system helps marketers to isolate motives likely to be involved in various consumption situations.\n\nMcGuire first divided the motivation into two main categories using two criteria:\n\nThen for each division in each category he stated there is two more basic elements.\n\n\n\n"}
{"id": "716975", "url": "https://en.wikipedia.org/wiki?curid=716975", "title": "Metallum Martis", "text": "Metallum Martis\n\nMetallum Martis, a 1665 book by Dud Dudley, is the earliest known reference to the use of coal in metallurgical smelting. The book is also referred to as \"Iron made with Pit-Coale, Sea-Coale, &c. And with the same Fuell to Melt and Fine Imperfect Mettals, And Refine perfect Mettals.\"\n\nMany attendant difficulties had to be overcome before this fuel could be applied to the purpose of smelting iron. Dudley does not describe in his book how he was using coal, only that he was. In so doing, he described his use successively of an ironworks on Pensnett Chase and at Cradley, of a furnace at Himley, and of a furnace at Hasco Bridge near Gornal.\n\nDudley does mention several things that indicate what he was doing. The coal he used was the small pieces and slack which were \"little or of no use in that inland country\" and so brought in no money. This coal debris was left in heaps and \"crowded moist slack heat naturally, and kindle in the middle of these great heaps, often sets the coal works on fire\" and that \"Also from these sulphurous heaps, mixed with ironstone (for out of many of the same pits is gotten much ironstone or mine), the fires heating vast quantities of water, passing through these soughs or adits becometh as hot as the bath at Bath\". Dudley describes two rival attempts to smelt iron with coal instigated by supporters of Parliament during the Civil War and the Interregnum. Dudley visited both sites and having examined their furnaces and production methods, when asked his opinion, informed the proprietors that they would fail. The first attempt was by Captain Buck, with the backing of many parliamentary officers including Oliver Cromwell, with technical help from Edward Dagney, an Italian. In the second attempt in the late 1656–67 by Captain John Copley also failed despite Dudley, at no charge, improving the efficiency of Copley's bellows. Dudley reapplied for a patent from Charles II, in 1660 stating \"and seeing no man able to perform the mastery of making of iron with pit-coal or sea-coal, ... [without my] laudable inventions the author was, and is, unwilling [that they] should fall to the ground and die with him\".\nA significant feature of his great work \"Metallum Martis\" is a map showing Dudley Castle where he correctly identifies the order and geographic layout of strata of coal and ironstone under survey.\n\nConsidered to be the earliest of recorded geologic maps, \"Metallum Martis\" marks a turning point in the evolution of scientific rationale concerning the recording and interpretation of geological information. It is considered to have been made at Castle Hill in Dudley by Dud Dudley in 1665.\n\n"}
{"id": "779427", "url": "https://en.wikipedia.org/wiki?curid=779427", "title": "Miranda Devine", "text": "Miranda Devine\n\nMiranda Devine (born 1960s) is an Australian columnist and writer noted for her conservative stance on a range of social and political issues. Her column, formerly printed twice weekly in Fairfax Media newspapers \"The Sydney Morning Herald\" and \"The Sun-Herald\", now appears in the News Limited newspapers \"Daily Telegraph\", \"Sunday Telegraph\", Melbourne's \"Sunday Herald Sun\" and Perth's \"Sunday Times\". She hosted \"The Miranda Devine Show\", a weekly syndicated radio show on Sydney station 2GB. The show ended in 2015. \n\nDevine is the daughter of journalist Frank Devine. She and her two younger sisters spent considerable parts of their youth overseas while their father was working as a foreign correspondent for Australian newspapers and, later, edited various American newspapers. While living in Tokyo, Devine attended an American International School and learned to speak Japanese fluently. A devout Roman Catholic, she completed her high school education at Loreto Kirribilli, a Catholic girls' private school in Sydney. After school, she completed a BSc in mathematics at Macquarie University. She joined the CSIRO in their textile physics division, where she worked for four years. She then completed a one-year graduate program at the Medill School of Journalism, Northwestern University in Chicago. \n\nOn 6 August 2010, \"The Daily Telegraph\" announced that Devine would be returning as a columnist for both \"The Daily Telegraph\" and \"Sunday Telegraph\".\n\nShe describes herself as \"a Catholic and a mother\".\n\nDevine has been accused by media of promoting the white genocide conspiracy theory and described as pivotal in popularising the concept within Australian politics. Referring to white South African refugees as \"oppressed white, Christian, industrious, rugby and cricket-playing Commonwealth cousins\", she has claimed they would \"integrate seamlessly\" with European Australians.\n\nDevine is a friend of fellow conservative columnist Tim Blair, who said of her, \"She's got good antennae. She can read people which is why she accurately predicts election results\". When interviewed for an April 2007 article in \"The Australian\" about hate mail received by female columnists, Devine commented, \"You are contesting ideas and you have to do it in a polarising way. When you write a column, you can't sit on the fence\".\n\nIn their book \"Silencing Dissent\" (Allen & Unwin), Clive Hamilton and Sarah Maddison accuse Devine of belonging to a \"syndicate of right-wing commentators who receive favour from the Howard Government.\"\n\nIn 2011, Devine used the news of Australian federal government minister Penny Wong's decision to parent a child with her female partner as the basis of a column in which she argued that the 2011 riots in England were the result of a \"fatherless society\".\n\nIn 2015, Devine sparked considerable controversy after claiming that \"women abusing welfare\" were the main cause of domestic violence. According to Devine, \"If you want to break the cycle of violence, end the welfare incentive for unsuitable women to keep having children to a string of feckless men\".\n\nIn 2017, she wrote that share bicycle schemes were a terror threat.\n\nIn 2018, Devine advocated for the continuation of coal-fired electricity options; she has repeatedly suggested that climate change is a political conspiracy.\n\n"}
{"id": "1215598", "url": "https://en.wikipedia.org/wiki?curid=1215598", "title": "Negligent infliction of emotional distress", "text": "Negligent infliction of emotional distress\n\nThe tort of negligent infliction of emotional distress (NIED) is a controversial cause of action, which is available in nearly all U.S. states but is severely constrained and limited in the majority of them. The underlying concept is that one has a legal duty to use reasonable care to avoid causing emotional distress to another individual. If one fails in this duty and unreasonably causes emotional distress to another person, that actor will be liable for monetary damages to the injured individual. The tort is to be contrasted with intentional infliction of emotional distress in that there is no need to prove intent to inflict distress. That is, an accidental infliction, if negligent, is sufficient to support a cause of action.\n\nNIED began to develop in the late nineteenth century, but only in a very limited form, in the sense that plaintiffs could recover for consequential emotional distress as a component of damages when a defendant negligently inflicted physical harm upon them. By 1908, most industrial U.S. states had adopted the \"physical impact\" form of NIED. However, NIED started developing into its more mature and more controversial form in the mid-20th century, as the new machines of the Second Industrial Revolution flooded the legal system with all kinds of previously unimaginable complex factual scenarios. Courts began to allow plaintiffs to recover for emotional distress resulting from negligent physical injuries to not only themselves, but other persons with whom they had a special relationship, like a relative. The first step, then, was to remove the requirement of physical injury to the actual plaintiff while keeping the requirement of physical injury to \"someone.\" In the 1968 landmark decision of \"Dillon v. Legg\", the Supreme Court of California was the first court to allow recovery for emotional distress aloneeven in the absence of any physical injury to the plaintiffin the particular situation where the plaintiff simply witnessed the death of a close relative at a distance, and was not within the \"zone of danger\" where the relative was killed. A 2007 statistical study commissioned by the Court found that \"Dillon\" was the most persuasive decision published by the Court between 1940 and 2005; \"Dillon\" has been favorably cited and followed by at least twenty reported out-of-state appellate decisions, more than any other California appellate decision.\n\nThe next step after \"Dillon\" was to make optional the element of another person (so that the injury could be to \"anything\" where it would be reasonably foreseeable that such injury would cause some person emotional distress). The first such case was \"Rodrigues v. State\", in which the Supreme Court of Hawaii held that plaintiffs could recover for negligent infliction of emotional distress as a result of negligently caused flood damage to their home. This is generally considered to be the true birth of NIED as a separate tort.\n\nTwelve years after \"Dillon\", California expanded NIED again, by holding that a relative could recover even where the underlying physical injury was \"de minimis\" (unnecessary medications and medical tests) if the outcome was foreseeable (the breakup of the plaintiffs' marriage as a result of the defendants' negligent and incorrect diagnosis of a sexually transmitted disease).\n\nIn 1994, the U.S. Supreme Court for the first time recognized NIED as part of federal common law, by holding that railroad workers could pursue NIED claims against their employers under the Federal Employers Liability Act. The Court recognized only the pre-\"Dillon\" form of NIED, though, in that the plaintiff had to be within a zone of danger to recover in the absence of physical injury. \n\nIn 1999, Hawaii took NIED even further by expressly holding that \"damages may be based solely upon serious emotional distress, even absent proof of a predicate physical injury.\"\n\nIt is generally disfavored by most states because it appears to have no definable parameters and because so many potential claims can be made under it. The situations that would give rise to such a claim are difficult to define. Because of this substantial uncertainty, most legal theorists find the theory to be unworkable in practice. \n\nA corollary of this critique is that the tort runs the risk (in the bystander NIED context) of overcompensating plaintiffs for distress which would have occurred anyway \"regardless\" of the cause of death of the decedent. In a landmark decision of the Supreme Court of California, which severely limited the availability of bystander NIED, Associate Justice David Eagleson wrote in \"Thing v. La Chusa\", 48 Cal. 3d 644 (1989): \n\nAn additional criticism of the tort is that it leads to abuse of liability insurance coverage. Most liability insurance policies provide for coverage of negligently inflicted injuries but exclude coverage of intentionally inflicted injuries. If a victim is intentionally injured by a person, many theorists perceive that the victim will tend to recast the claim as being one for negligence in order to fall within the coverage of the insurance policy.\n\nThe Texas case of \"Boyles v. Kerr\", 855 S.W.2d 593 (Tex. 1993) is illustrative. In this case, the defendant secretly videotaped himself engaging in sexual activities with the plaintiff. The defendant then showed this videotape to numerous individuals and caused severe distress to the plaintiff. The plaintiff brought suit against the defendant, asserting a claim for negligent infliction of emotional distress.\n\nOn appeal, the Supreme Court of Texas observed that the facts did not support a claim of negligence. Rather, the Court noted, the facts clearly supported a claim of an intentional injury by the defendant and it was evident that the claim had been cast as \"negligence\" solely to obtain insurance coverage. The Court then went on to hold that Texas did not recognize a claim for negligent infliction of emotional distress and remanded the case to the trial court for consideration of a claim for intentional infliction of emotional distress. \n\nJurisdictions that have rejected the claim of negligent infliction of emotional distress do not forbid the recovery of damages for mental injuries. Instead, these jurisdictions usually allow recovery for emotional distress where such distress:\n\n"}
{"id": "4082177", "url": "https://en.wikipedia.org/wiki?curid=4082177", "title": "PAH world hypothesis", "text": "PAH world hypothesis\n\nThe PAH world hypothesis is a speculative hypothesis that proposes that polycyclic aromatic hydrocarbons (PAH), known to be abundant in the universe, including in comets, and, as well, assumed to be abundant in the primordial soup of the early Earth, played a major role in the origin of life by mediating the synthesis of RNA molecules, leading into the RNA world. However, as yet, the hypothesis is untested.\n\nThe Miller–Urey experiment in 1952, and others since, demonstrated the synthesis of organic compounds, such as amino acids, formaldehyde and sugars, from the original inorganic precursors the researchers presumed to have been present in the primordial soup (but is no longer considered likely). This experiment inspired many others. In 1961, Joan Oró found that the nucleotide base adenine could be made from hydrogen cyanide (HCN) and ammonia in a water solution. Experiments conducted later showed that the other RNA and DNA nucleobases could be obtained through simulated prebiotic chemistry with a reducing atmosphere.\n\nThe RNA world hypothesis shows how RNA can become its own catalyst (a ribozyme). In between there are some missing steps such as how the first RNA molecules could be formed. The PAH world hypothesis was proposed by Simon Nicholas Platts in May 2004 to try to fill in this missing step. A more thoroughly elaborated idea has been published by Ehrenfreund \"et al.\".\n\nPolycyclic aromatic hydrocarbons are the most common and abundant of the known polyatomic molecules in the visible universe, and are considered a likely constituent of the primordial sea. PAHs, along with fullerenes (or \"buckyballs\"), have been recently detected in nebulae. (Fullerenes are also implicated in the origin of life; according to astronomer Letizia Stanghellini, \"It’s possible that buckyballs from outer space provided seeds for life on Earth.”) In September 2012, NASA scientists reported that PAHs, subjected to interstellar medium (ISM) conditions, are transformed, through hydrogenation, oxygenation and hydroxylation, to more complex organics — \"a step along the path toward amino acids and nucleotides, the raw materials of proteins and DNA, respectively\". Further, as a result of these transformations, the PAHs lose their spectroscopic signature which could be one of the reasons \"for the lack of PAH detection in interstellar ice grains, particularly the outer regions of cold, dense clouds or the upper molecular layers of protoplanetary disks.\"\n\nOn June 6, 2013, scientists at the IAA-CSIC reported the detection of polycyclic aromatic hydrocarbons in the upper atmosphere of Titan, the largest moon of the planet Saturn.\n\nIn October 2018, researchers reported low-temperature chemical pathways from simple organic compounds to complex PAHs. Such chemical pathways may help explain the presence of PAHs in the low-temperature atmosphere of Saturn moon Titan, and may be significant pathways, in terms of the PAH world hypothesis, in producing presursors to biochemcals related to life as we know it.\n\nIn addition, PAHs are not normally very soluble in sea water, but when subject to ionizing radiation such as solar UV light, the outer hydrogen atoms can be stripped off and replaced with a hydroxyl group, rendering the PAHs far more soluble in water.\n\nThese modified PAHs are amphiphilic, which means that they have parts that are both hydrophilic and hydrophobic. When in solution, they assemble in discotic mesogenic (liquid crystal) stacks which, like lipids, tend to organize with their hydrophobic parts protected.\n\nOn February 21, 2014, NASA announced a greatly upgraded database for tracking polycyclic aromatic hydrocarbons (PAHs) in the universe. More than 20% of the carbon in the universe may be associated with PAHs, possible starting materials for the formation of life. PAHs seem to have been formed as early as a couple of billion years after the Big Bang, are abundant in the universe, and are associated with new stars and exoplanets.\n\nIn the self-ordering PAH stack, the separation between adjacent rings is 0.34 nm. This is the same separation found between adjacent nucleotides of RNA and DNA. Smaller molecules will naturally attach themselves to the PAH rings. However PAH rings, while forming, tend to swivel around on one another, which will tend to dislodge attached compounds that would collide with those attached to those above and below. Therefore, it encourages preferential attachment of flat molecules such as pyrimidine and purine nucleobases, the key constituents (and information carriers) of RNA and DNA. These bases are similarly amphiphilic and so also tend to line up in similar stacks.\n\nAccording to the hypothesis, once the nucleobases are attached (via hydrogen bonds) to the PAH scaffolding, the inter-base distance would select for \"linker\" molecules of a specific size, such as small formaldehyde (methanal) oligomers, also taken from the prebiotic \"soup\", which will bind (via covalent bonds) to the nucleobases as well as each other to add a flexible structural backbone.\n\nA subsequent transient drop in the ambient pH (increase in acidity), for example as a result of a volcanic discharge of acidic gases such as sulfur dioxide or carbon dioxide, would allow the bases to break off from their PAH scaffolding, forming RNA-like molecules (with the formaldehyde backbone instead of the ribose-phosphate backbone used by \"modern\" RNA, but the same 0.34 nm pitch).\n\nThe hypothesis further speculates that once long RNA-like single strands are detached from the PAH stacks, and after ambient pH levels became less acidic, they would tend to fold back on themselves, with complementary sequences of nucleobases preferentially seeking out each other and forming hydrogen bonds, creating stable, at least partially double-stranded RNA-like structures, similar to ribozymes. The formaldehyde oligomers would eventually be replaced with more stable ribose-phosphate molecules for the backbone material, resulting in a starting milestone for the RNA world hypothesis, which speculates about further evolutionary developments from that point.\n\n"}
{"id": "24282630", "url": "https://en.wikipedia.org/wiki?curid=24282630", "title": "Parental abuse by children", "text": "Parental abuse by children\n\nAbuse of parents by their children, also known as child-to-parent violence (CPV), is a form of domestic violence, and is one of the most under-reported and under-researched subject areas in the field of psychology. Parents are quite often subject to levels of childhood aggression in excess of normal childhood aggressive outbursts, typically in the form of verbal or physical abuse. Parents feel a sense of shame and humiliation to have that problem, so they rarely seek help and there is usually little or no help available anyway.\n\nParent abuse is defined by Cottrell as ‘any harmful act of a teenager that physically harms another person, in this case the parents.' He goes on to say that although parental abuse is real, it is never really caused by the child but by the parents themselves. (This is not always the case though as some parents have done everything in their power to help their child.) Although many people try and convince themselves that someone else is to blame for their child's actions, yet they are only making it worse for the child. Parents must take the time to learn their child so they can have a meaningful relationship that the kid wants to keep healthy. \n\nAdolescent abuse towards parents and even grandparents is a problem in the United States as well as other countries around the world but it is something not often discussed or reported because most family abuse in general remains hidden from public view until law enforcement becomes involved. Child abuse and spousal abuse are discussed, but parents abused by their own offspring are still considered by many to be a taboo subject, according to some researchers. Reasons for this may be parents feel ashamed and/or think they should be able to handle the situation by themselves without outside assistance. In addition, some parents may feel it is not safe for them to attempt to control the situation for it might enrage their child more. But any form of abuse is harmful to the victim as well as the abuser and may lead to more serious consequences if ignored. Identifying or admitting there is a problem is the first step to finding a solution to adolescent parental abuse and seeking help through intervention is the next step to attempt to resolve problematic adolescent behavior.\n\nIt is difficult to ascertain the prevalence of the phenomenon due to the fact that it is hugely under reported by parents. Research carried out in Canada, United States and Oceania suggest that mothers, lone parents as well as parents facing social and family difficulties are more probable to experience parental abuse, especially if a child has experienced violence in the family.\n\nA unique factor in parental abuse is 'the culture of blame', which has deepened over the past decade.\n\nParental abuse by adolescents may be relatively common; an adolescent is a young person between the ages of 12 and 19. However, abusers can be younger or older; in fact, according to a review, 11% of abusers may be less than age 10.\n\nAccording to Cottrell and Bobic, abuse may appear in one or a combination of five forms; physical, verbal, psychological, emotional, and financial. Bobic mentioned only four of the five listed abuses; verbal abuse was not included in her 2004 article, \"Adolescent Violence Towards Parents.\"\n\nMany people consider parent abuse to be the result of bad parenting, neglect, or the child suffering abuse themselves, which some certainly have experienced, but other adolescent abusers have had \"normal\" upbringing and have not suffered from these situations. Children may be subjected to violence on TV, in movies and in music, and that violence may come to be considered \"normal.\" The breakdown of the family unit, poor or nonexistent relationships with an absent parent, as well as, debt, unemployment, and parental drug/alcohol abuse may all be contributing factors to abuse. Some other reasons for parental abuse according to several experts are:\n\n\nParental abuse is a relatively new term. In 1979, Harbin and Madden released a study using the term “parent battery” but juvenile delinquency, which is a major factor, has been studied since the late 19th century. Even though some studies have been done in the United States, Australia, Canada, and other countries, the lack of reporting of adolescent abuse toward parents makes it difficult to accurately determine the extent of it. Many studies have to rely on self-reporting by adolescents. In 2004, Robinson, of Brigham Young University, published: \"Parent Abuse on the Rise: A Historical Review\" in the American Association of Behavioral Social Science Online Journal, reporting results of the 1988 study performed by Evans and Warren-Sohlberg. The results reported that 57% of parental abuse was physical; using a weapon at 17%; throwing items at 5% and verbal abuse reported at 22%. With 82% of the abuse being against mothers (5 times greater than against fathers) and 11% of the abusers were under the age of 10 years. The highest rate of abuse happens within families with a single mother. Mothers are usually the primary caregiver; they spend more time with their children than fathers and have closer emotional connections to them. It can also be due to the size and strength of the abuser and women are often thought of as weaker and even powerless. Parental abuse can occur in any family and it is not necessarily associated with ethnic background, socio-economic class, or sexual orientation.\n\nNumerous studies concluded that gender does not play a role in the total number of perpetrators; however, males are more likely to inflict physical abuse and females are more likely to inflict emotional abuse. Studies from the United States estimate that violence among adolescents peaks at 15–17 years old. However, a Canadian study done by Barbara Cottrell in 2001 suggests the ages are 12–14 years old.\n\nParental abuse does not happen just inside the home but can be in public places, further adding to the humiliation of the parents. Abuse is not only a domestic affair but can be criminal as well. Most teenagers experience a normal transition in which they try to go from being dependent to independent, but there are some dynamics of unhealthy parental control that also play a direct part in the failure to properly raise a child in this regard. There will always be times of resistance toward parental authority. According to the Canadian National Clearinghouse on Family Violence the abuse generally begins with verbal abuse, but even then, some females can be very physically abusive towards a child who is smaller and more vulnerable than they are, and to cover their abuse, they often lie to the other parent about actual events that led to \"severe punishment.\" The child, adolescent or parent may show no remorse or guilt and feels justified in the behavior, but many times when the child is the one who is being abused, they are very remorseful for being forced to defend themselves, especially when they are not the aggressor. Parents must examine their children’s behavior and determine if it is acceptable or if it crosses the line of abusiveness, just as a parent has the responsibility as an adult who is supposed to know better should be responsible for his/her own abuses towards a child. Some teenagers can become aggressive as a result of parental abuses and dysfunction or psychological problems. Some children may have trouble dealing with their emotions, that is all part of growing up but there is a line that should not be crossed and parents may determine where that line is. Unfortunately, abused children are not afforded protections from abusive parents. This practice often helps discourage abusive behavior and show that it will not be tolerated.\n\nAccording to Spitzberg the typical interaction leading to parental abuse often seems to occur in the following sequence:\n\n\nThese types of aggressive behaviors are very important to recognize for appropriate treatment of adolescents and parents abused by the same. Yet the escalation of violence is an interactive process. When parents or others overreact and intervene emotionally, they can cause the adolescent’s aggression to escalate to a higher level, by exerting examples of violence and unreasonableness as a parent. The more tendency towards abuse and negative behaviors that the parent exemplifies, the more reactive the child will also be, more often in a negative manner. Balancing these two dynamics is the key to healthy family dynamics in reducing potential abuse within families, whether it be parental abuses or child abuses.\n\nIntervention is perhaps the best solution to confront adolescent parental abuse and the key to turn aggressive behavior by adolescents, teenagers, and young adults during its early stages and help prevent any other form of parental abuse from taking place.\n\nWhile Intervention is an option, it may not always work. There are times when the child does have a mental illness that does not allow the child, adolescent or teenager to understand what is exactly happening. Therefore, the individual acts out their emotions the only way they understand. This can present itself as violence, emotional abuse, destructive behaviors such as destroying personal property or self bodily injury. The United States currently protects abused children using Courts, Child Protective Services and other agencies. The US also has Adult Protective Services which is provided to abused, neglected, or exploited older adults and adults with significant disabilities. There are no agencies or programs that protect parents from abusive children, adolescents or teenagers other than giving up their Parental Rights to the state they live in.\n\n\n"}
{"id": "18059377", "url": "https://en.wikipedia.org/wiki?curid=18059377", "title": "Polar city", "text": "Polar city\n\nA polar city is a proposed sustainable polar retreat designed to house human beings in the future, in the event that global warming causes the equatorial and middle latitudes of the Earth to become uninhabitable for a long period of time. Although they have not been built yet, some futurists have been giving considerable thought to the concepts involved. High-population-density cities, to be built near the Arctic Rim and in Antarctica, New Zealand, Tasmania, and Patagonia, with sustainable energy and transportation infrastructure, will require substantial nearby agriculture. Boreal soils are largely poor in key nutrients like nitrogen and phosphorus, but nitrogen-fixing plants (such as the various alders) with the proper symbiotic microbes and mycorrhizal fungi can likely remedy such poverty without the need for petroleum-derived fertilizers. Regional probiotic soil improvement should perhaps rank high on any polar cities priority list. James Lovelock's notion of a widely distributed almanac of science knowledge and post-industrial survival skills also appears to have value.\n\nThe polar cities climate retreat living pod concept is a worst-case scenario prediction based on the ideas of British\nchemist and inventor James Lovelock: life in polar cities arrayed inland and\naround the shores of an ice-free Arctic Ocean in a greenhouse-warmed\nworld. Dr. Lovelock, who in 1972 conceived of the Earth's crust, climate,\nand veneer of life as a unified self-sustaining entity, foresees\nhumanity in full pole-bound retreat within a century as areas around\nthe tropics roast — a scenario far outside even the worst-case\nprojections of climate scientists.\n\nAfter reading a newspaper column in 2006 in which Dr. Lovelock predicted\ndisastrous warming, Danny Bloom, a freelance newspaper reporter and climate blogger, teamed up with Deng Cheng-hong, a Taiwanese\nartist, and set up websites showing designs for self-sufficient\nArctic communities. Mr. Bloom's intent is to conduct a non-threatening thought\nexperiment that might prod people out of their comfort zone on climate change.\n\nIn 2012, two books about polar cities were published. \"Polar City Red\", by Jim Laughter, is a climate fiction novel about life in\na polar city in the year 2075 in Alaska. \"Polar City Dreaming: How Climate Change Might Usher In The Age Of Polar Cities,\" by Stephan Malone, is a nonfiction history of polar city ideas.\n\nThe design of polar cities climate retreat living pods is currently (2008) driven entirely by volunteers under the name of \"The Polar City Project\". Danny Bloom is currently leading this effort. The defining design characteristics are efficiency, both for operational costs as well as construction costs, and a desire for the city to be a Zero energy building. The proposed agricultural module, for example, is a Sustainable agriculture vertical farm.\n\n"}
{"id": "30987765", "url": "https://en.wikipedia.org/wiki?curid=30987765", "title": "Principle of permanence", "text": "Principle of permanence\n\nIn mathematics, the principle of permanence is that a complex function (or functional equation) which is 0 on a set with a non-isolated point is 0 everywhere (or at least on the connected component of its domain which contains the point). There are various statements of the principle, depending on the type of function or equation considered.\n\nFor one variable, the principle of permanence states that if \"f\"(\"z\") is an analytic function defined on an open connected subset \"U\" of the complex numbers C, and there exists a convergent sequence {\"a\"} having a limit \"L\" which is in \"U\", such that \"f\"(\"a\") = 0 for all \"n\", then \"f\"(\"z\") is uniformly zero on \"U\".\n\nOne of the main uses of the principle of permanence is to show that a functional equation that holds for the real numbers also holds for the complex numbers.\n\nAs an example, the function e-ee=0 on the real numbers. By the principle of permanence for functions of two variables, this implies that e-ee=0 for all complex numbers, thus proving one of the laws of exponents for complex exponents.\n\n"}
{"id": "25765", "url": "https://en.wikipedia.org/wiki?curid=25765", "title": "RNA world", "text": "RNA world\n\nThe RNA world is a hypothetical stage in the evolutionary history of life on Earth, in which self-replicating RNA molecules proliferated before the evolution of DNA and proteins. The term also refers to the hypothesis that posits the existence of this stage.\n\nAlexander Rich first proposed the concept of the RNA world in 1962, and Walter Gilbert coined the term in 1986. Alternative chemical paths to life have been proposed, and RNA-based life may not have been the first life to exist. Even so, the evidence for an RNA world is strong enough that the hypothesis has gained wide acceptance.\n\nLike DNA, RNA can store and replicate genetic information; like protein enzymes, RNA enzymes (ribozymes) can catalyze (start or accelerate) chemical reactions that are critical for life. One of the most critical components of cells, the ribosome, is composed primarily of RNA. Ribonucleotide moieties in many coenzymes, such as Acetyl-CoA, NADH, FADH and F420, have long been thought of as surviving remnants of covalently bound coenzymes in an RNA world.\n\nAlthough RNA is fragile, some ancient RNAs may have evolved the ability to methylate other RNAs to protect them.\n\nIf the RNA world existed, it was probably followed by an age characterized by the evolution of ribonucleoproteins (RNP world), which in turn ushered in the era of DNA and longer proteins. DNA has better stability and durability than RNA; this may explain why it became the predominant storage molecule.\nProtein enzymes may have come to replace RNA-based ribozymes as biocatalysts because their greater abundance and diversity of monomers makes them more versatile. As some co-factors contain both nucleotide and amino-acid characteristics, it may be that amino acids, peptides and finally proteins initially were co-factors for ribozymes.\n\nOne of the challenges in studying abiogenesis is that the system of reproduction and metabolism utilized by all extant life involves three distinct types of interdependent macromolecules (DNA, RNA, and protein). This suggests that life could not have arisen in its current form, which has led researchers to hypothesize mechanisms whereby the current system might have arisen from a simpler precursor system. The concept of RNA as a primordial molecule can be found in papers by Francis Crick and Leslie Orgel, as well as in Carl Woese's 1967 book \"The Genetic Code\". In 1962, the molecular biologist Alexander Rich posited much the same idea in an article he contributed to a volume issued in honor of Nobel-laureate physiologist Albert Szent-Györgyi. Hans Kuhn in 1972 laid out a possible process by which the modern genetic system might have arisen from a nucleotide-based precursor, and this led Harold White in 1976 to observe that many of the cofactors essential for enzymatic function are either nucleotides or could have been derived from nucleotides. He proposed that these nucleotide cofactors represent \"fossils of nucleic acid enzymes\". The phrase \"RNA World\" was first used by Nobel laureate Walter Gilbert in 1986, in a commentary on how recent observations of the catalytic properties of various forms of RNA fit with this hypothesis.\n\nThe properties of RNA make the idea of the RNA world hypothesis conceptually plausible, though its general acceptance as an explanation for the origin of life requires further evidence. RNA is known to form efficient catalysts and its similarity to DNA makes clear its ability to store information. Opinions differ, however, as to whether RNA constituted the first autonomous self-replicating system or was a derivative of a still-earlier system. One version of the hypothesis is that a different type of nucleic acid, termed \"pre-RNA\", was the first one to emerge as a self-reproducing molecule, to be replaced by RNA only later. On the other hand, the discovery in 2009 that activated pyrimidine ribonucleotides can be synthesized under plausible prebiotic conditions suggests that it is premature to dismiss the RNA-first scenarios. Suggestions for 'simple' \"pre-RNA\" nucleic acids have included peptide nucleic acid (PNA), threose nucleic acid (TNA) or glycol nucleic acid (GNA). Despite their structural simplicity and possession of properties comparable with RNA, the chemically plausible generation of \"simpler\" nucleic acids under prebiotic conditions has yet to be demonstrated.\n\nRNA enzymes, or ribozymes, are found in today's DNA-based life and could be examples of living fossils. Ribozymes play vital roles, such as that of the ribosome, an RNA-protein complex responsible for protein synthesis. Many other ribozyme functions exist; for example, the hammerhead ribozyme performs self-cleavage and an RNA polymerase ribozyme can synthesize a short RNA strand from a primed RNA template.\n\nAmong the enzymatic properties important for the beginning of life are:\n\nRNA is a very similar molecule to DNA, with only two major chemical differences (the backbone of RNA uses ribose instead of deoxyribose and its nucleobases include uracil instead of thymine). The overall structure of RNA and DNA are immensely similar—one strand of DNA and one of RNA can bind to form a double helical structure. This makes the storage of information in RNA possible in a very similar way to the storage of information in DNA. However, RNA is less stable, being more prone to hydrolysis due to the presence of a hydroxyl group at the ribose 2' position.\n\nThe major difference between RNA and DNA is the presence of a hydroxyl group at the 2'-position of the ribose sugar in RNA (illustration, right). This group makes the molecule less stable because, when not constrained in a double helix, the 2' hydroxyl can chemically attack the adjacent phosphodiester bond to cleave the phosphodiester backbone. The hydroxyl group also forces the ribose into the C3'-\"endo\" sugar conformation unlike the C2'-\"endo\" conformation of the deoxyribose sugar in DNA. This forces an RNA double helix to change from a B-DNA structure to one more closely resembling A-DNA.\n\nRNA also uses a different set of bases than DNA—adenine, guanine, cytosine and uracil, instead of adenine, guanine, cytosine and thymine. Chemically, uracil is similar to thymine, differing only by a methyl group, and its production requires less energy. In terms of base pairing, this has no effect. Adenine readily binds uracil or thymine. Uracil is, however, one product of damage to cytosine that makes RNA particularly susceptible to mutations that can replace a GC base pair with a GU (wobble) or AU base pair.\n\nRNA is thought to have preceded DNA, because of their ordering in the biosynthetic pathways. The deoxyribonucleotides used to make DNA are made from ribonucleotides, the building blocks of RNA, by removing the 2'-hydroxyl group. As a consequence a cell must have the ability to make RNA before it can make DNA.\n\nThe chemical properties of RNA make large RNA molecules inherently fragile, and they can easily be broken down into their constituent nucleotides through hydrolysis. These limitations do not make use of RNA as an information storage system impossible, simply energy intensive (to repair or replace damaged RNA molecules) and prone to mutation. While this makes it unsuitable for current 'DNA optimised' life, it may have been acceptable for more primitive life.\n\nRiboswitches have been found to act as regulators of gene expression, particularly in bacteria, but also in plants and archaea. Riboswitches alter their secondary structure in response to the binding of a metabolite. This change in structure can result in the formation or disruption of a terminator, truncating or permitting transcription respectively. Alternatively, riboswitches may bind or occlude the Shine-Dalgarno sequence, affecting translation. It has been suggested that these originated in an RNA-based world. In addition, RNA thermometers regulate gene expression in response to temperature changes.\n\nThe RNA world hypothesis is supported by RNA's ability to store, transmit, and duplicate genetic information, as DNA does. RNA can act as a ribozyme, a special type of enzyme. Because it can perform the tasks of both DNA and enzymes, RNA is believed to have once been capable of supporting independent life forms. Some viruses use RNA as their genetic material, rather than DNA. Further, while nucleotides were not found in experiments based on Miller-Urey experiment, their formation in prebiotically plausible conditions was reported in 2009; the purine base known as adenine is merely a pentamer of hydrogen cyanide. Experiments with basic ribozymes, like Bacteriophage Qβ RNA, have shown that simple self-replicating RNA structures can withstand even strong selective pressures (e.g., opposite-chirality chain terminators).\n\nSince there were no known chemical pathways for the abiogenic synthesis of nucleotides from pyrimidine nucleobases cytosine and uracil under prebiotic conditions, it is thought by some that nucleic acids did not contain these nucleobases seen in life's nucleic acids. The nucleoside cytosine has a half-life in isolation of 19 days at and 17,000 years in freezing water, which some argue is too short on the geologic time scale for accumulation. Others have questioned whether ribose and other backbone sugars could be stable enough to find in the original genetic material, and have raised the issue that all ribose molecules would have had to be the same enantiomer, as any nucleotide of the wrong chirality acts as a chain terminator.\n\nPyrimidine ribonucleosides and their respective nucleotides have been prebiotically synthesised by a sequence of reactions that by-pass free sugars and assemble in a stepwise fashion by including nitrogenous and oxygenous chemistries. In a series of publications, John Sutherland and his team at the School of Chemistry, University of Manchester, have demonstrated high yielding routes to cytidine and uridine ribonucleotides built from small 2 and 3 carbon fragments such as glycolaldehyde, glyceraldehyde or glyceraldehyde-3-phosphate, cyanamide and cyanoacetylene. One of the steps in this sequence allows the isolation of enantiopure ribose aminooxazoline if the enantiomeric excess of glyceraldehyde is 60% or greater, of possible interest towards biological homochirality. This can be viewed as a prebiotic purification step, where the said compound spontaneously crystallised out from a mixture of the other pentose aminooxazolines. Aminooxazolines can react with cyanoacetylene in a mild and highly efficient manner, controlled by inorganic phosphate, to give the cytidine ribonucleotides. Photoanomerization with UV light allows for inversion about the 1' anomeric centre to give the correct beta stereochemistry; one problem with this chemistry is the selective phosphorylation of alpha-cytidine at the 2' position. However, in 2009, they showed that the same simple building blocks allow access, via phosphate controlled nucleobase elaboration, to 2',3'-cyclic pyrimidine nucleotides directly, which are known to be able to polymerise into RNA. Organic chemist Donna Blackmond described this finding as \"strong evidence\" in favour of the RNA world. However, John Sutherland said that while his team's work suggests that nucleic acids played an early and central role in the origin of life, it did not necessarily support the RNA world hypothesis in the strict sense, which he described as a \"restrictive, hypothetical arrangement\".\n\nThe Sutherland group's 2009 paper also highlighted the possibility for the photo-sanitization of the pyrimidine-2',3'-cyclic phosphates. A potential weakness of these routes is the generation of enantioenriched glyceraldehyde, or its 3-phosphate derivative (glyceraldehyde prefers to exist as its keto tautomer dihydroxyacetone).\n\nOn August 8, 2011, a report, based on NASA studies with meteorites found on Earth, was published suggesting building blocks of RNA (adenine, guanine and related organic molecules) may have been formed extraterrestrially in outer space. In 2017, a numerical model suggests that the RNA world may have emerged in warm ponds on the early Earth, and that meteorites were a plausible and probable source of the RNA building blocks (ribose and nucleic acids) to these environments. On August 29, 2012, astronomers at Copenhagen University reported the detection of a specific sugar molecule, glycolaldehyde, in a distant star system. The molecule was found around the protostellar binary \"IRAS 16293-2422\", which is located 400 light years from Earth. Because glycolaldehyde is needed to form RNA, this finding suggests that complex organic molecules may form in stellar systems prior to the formation of planets, eventually arriving on young planets early in their formation.\n\nNucleotides are the fundamental molecules that combine in series to form RNA. They consist of a nitrogenous base attached to a sugar-phosphate backbone. RNA is made of long stretches of specific nucleotides arranged so that their sequence of bases carries information. The RNA world hypothesis holds that in the primordial soup (or sandwich), there existed free-floating nucleotides. These nucleotides regularly formed bonds with one another, which often broke because the change in energy was so low. However, certain sequences of base pairs have catalytic properties that lower the energy of their chain being created, enabling them to stay together for longer periods of time. As each chain grew longer, it attracted more matching nucleotides faster, causing chains to now form faster than they were breaking down.\n\nThese chains have been proposed by some as the first, primitive forms of life. In an RNA world, different sets of RNA strands would have had different replication outputs, which would have increased or decreased their frequency in the population, i.e. natural selection. As the fittest sets of RNA molecules expanded their numbers, novel catalytic properties added by mutation, which benefitted their persistence and expansion, could accumulate in the population. Such an autocatalytic set of ribozymes, capable of self replication in about an hour, has been identified. It was produced by molecular competition (\"in vitro\" evolution) of candidate enzyme mixtures.\n\nCompetition between RNA may have favored the emergence of cooperation between different RNA chains, opening the way for the formation of the first protocell. Eventually, RNA chains developed with catalytic properties that help amino acids bind together (a process called peptide-bonding). These amino acids could then assist with RNA synthesis, giving those RNA chains that could serve as ribozymes the selective advantage. The ability to catalyze one step in protein synthesis, aminoacylation of RNA, has been demonstrated in a short (five-nucleotide) segment of RNA.\n\nIn March 2015, NASA scientists reported that, for the first time, complex DNA and RNA organic compounds of life, including uracil, cytosine and thymine, have been formed in the laboratory under conditions found only in outer space, using starting chemicals, like pyrimidine, found in meteorites. Pyrimidine, like polycyclic aromatic hydrocarbons (PAHs), may have been formed in giant red stars or in interstellar dust and gas clouds, according to the scientists.\n\nIn 2018, researchers at Georgia Institute of Technology identified three molecular candidates for the bases that might have formed an earliest version of proto-RNA: barbituric acid, melamine, and 2,4,6-triaminopyrimidine. These three molecules are simpler versions of the four bases in current RNA, which could have been present in larger amounts and could still be forwards compatibile with them, but may have been discarded by evolution in exchange for more optimal base pairs.\n\nOne of the problems with the RNA world hypothesis is to discover the pathway by which RNA became upgraded to the DNA system. Geoffrey Diemer and Ken Stedman, at Portland State University in Oregon, may have found a solution. While conducting a survey of viruses in a hot acidic lake in Lassen Volcanic National Park, California, they uncovered evidence that a simple DNA virus had acquired a gene from a completely unrelated RNA-based virus. Virologist Luis Villareal of the University of California Irvine also suggests that viruses capable of converting an RNA-based gene into DNA and then incorporating it into a more complex DNA-based genome might have been common in the Virus world during the RNA to DNA transition some 4 billion years ago. This finding bolsters the argument for the transfer of information from the RNA world to the emerging DNA world before the emergence of the last universal common ancestor. From the research, the diversity of this virus world is still with us.\n\nAdditional evidence supporting the concept of an RNA world has resulted from research on viroids, the first representatives of a novel domain of \"subviral pathogens\".\nViroids are mostly plant pathogens, which consist of short stretches (a few hundred nucleobases) of highly complementary, circular, single-stranded, and non-coding RNA without a protein coat. Compared with other infectious plant pathogens, viroids are extremely small in size, ranging from 246 to 467 nucleobases. In comparison, the genome of the smallest known viruses capable of causing an infection are about 2,000 nucleobases long.\n\nIn 1989, Diener proposed that, based on their characteristic properties, viroids are more plausible \"living relics\" of the RNA world than are introns or other RNAs then so considered. If so, viroids have attained potential significance beyond plant pathology to evolutionary biology, by representing the most plausible macromolecules known capable of explaining crucial intermediate steps in the evolution of life from inanimate matter (see: abiogenesis).\n\nApparently, Diener's hypothesis lay dormant until 2014, when Flores et al. published a review paper, in which Diener's evidence supporting his hypothesis was summarized. In the same year, a New York Times science writer published a popularized version of Diener's proposal, in which, however, he mistakenly credited Flores et al. with the hypothesis' original conception.\n\nPertinent viroid properties listed in 1989 are:\n\n\nThe existence, in extant cells, of RNAs with molecular properties predicted for RNAs of the RNA World constitutes an additional argument supporting the RNA World hypothesis.\n\nEigen \"et al\". and Woese proposed that the genomes of early protocells were composed of single-stranded RNA, and that individual genes corresponded to separate RNA segments, rather than being linked end-to-end as in present-day DNA genomes. A protocell that was haploid (one copy of each RNA gene) would be vulnerable to damage, since a single lesion in any RNA segment would be potentially lethal to the protocell (e.g. by blocking replication or inhibiting the function of an essential gene).\n\nVulnerability to damage could be reduced by maintaining two or more copies of each RNA segment in each protocell, i.e. by maintaining diploidy or polyploidy. Genome redundancy would allow a damaged RNA segment to be replaced by an additional replication of its homolog. However, for such a simple organism, the proportion of available resources tied up in the genetic material would be a large fraction of the total resource budget. Under limited resource conditions, the protocell reproductive rate would likely be inversely related to ploidy number. The protocell's fitness would be reduced by the costs of redundancy. Consequently, coping with damaged RNA genes while minimizing the costs of redundancy would likely have been a fundamental problem for early protocells.\n\nA cost-benefit analysis was carried out in which the costs of maintaining redundancy were balanced against the costs of genome damage. This analysis led to the conclusion that, under a wide range of circumstances, the selected strategy would be for each protocell to be haploid, but to periodically fuse with another haploid protocell to form a transient diploid. The retention of the haploid state maximizes the growth rate. The periodic fusions permit mutual reactivation of otherwise lethally damaged protocells. If at least one damage-free copy of each RNA gene is present in the transient diploid, viable progeny can be formed. For two, rather than one, viable daughter cells to be produced would require an extra replication of the intact RNA gene homologous to any RNA gene that had been damaged prior to the division of the fused protocell. The cycle of haploid reproduction, with occasional fusion to a transient diploid state, followed by splitting to the haploid state, can be considered to be the sexual cycle in its most primitive form. In the absence of this sexual cycle, haploid protocells with damage in an essential RNA gene would simply die.\n\nThis model for the early sexual cycle is hypothetical, but it is very similar to the known sexual behavior of the segmented RNA viruses, which are among the simplest organisms known. Influenza virus, whose genome consists of 8 physically separated single-stranded RNA segments, is an example of this type of virus. In segmented RNA viruses, \"mating\" can occur when a host cell is infected by at least two virus particles. If these viruses each contain an RNA segment with a lethal damage, multiple infection can lead to reactivation providing that at least one undamaged copy of each virus gene is present in the infected cell. This phenomenon is known as \"multiplicity reactivation\". Multiplicity reactivation has been reported to occur in influenza virus infections after induction of RNA damage by UV-irradiation, and ionizing radiation.\n\nPatrick Forterre has been working on a novel hypothesis, called \"three viruses, three domains\": that viruses were instrumental in the transition from RNA to DNA and the evolution of Bacteria, Archaea, and Eukaryota. He believes the last universal common ancestor was RNA-based and evolved RNA viruses. Some of the viruses evolved into DNA viruses to protect their genes from attack. Through the process of viral infection into hosts the three domains of life evolved. Another interesting proposal is the idea that RNA synthesis might have been driven by temperature gradients, in the process of thermosynthesis.\nSingle nucleotides have been shown to catalyze organic reactions.\n\nSteven Benner has argued that chemical conditions on the planet Mars, such as the presence of boron, molybdenum and oxygen, may have been better for initially producing RNA molecules than those on Earth. If so, life-suitable molecules, originating on Mars, may have later migrated to Earth via panspermia or similar process.\n\nThe hypothesized existence of an RNA world does not exclude a \"Pre-RNA world\", where a metabolic system based on a different nucleic acid is proposed to pre-date RNA. A candidate nucleic acid is peptide nucleic acid (PNA), which uses simple peptide bonds to link nucleobases. PNA is more stable than RNA, but its ability to be generated under prebiological conditions has yet to be demonstrated experimentally.\n\nThreose nucleic acid (TNA) has also been proposed as a starting point, as has glycol nucleic acid (GNA), and like PNA, also lack experimental evidence for their respective abiogenesis.\n\nAn alternative — or complementary — theory of RNA origin is proposed in the PAH world hypothesis, whereby polycyclic aromatic hydrocarbons (PAHs) mediate the synthesis of RNA molecules. PAHs are the most common and abundant of the known polyatomic molecules in the visible Universe, and are a likely constituent of the primordial sea. PAHs and fullerenes (also implicated in the origin of life) have been detected in nebulae.\n\nThe iron-sulfur world theory proposes that simple metabolic processes developed before genetic materials did, and these energy-producing cycles catalyzed the production of genes.\n\nSome of the difficulties of producing the precursors on earth are bypassed by another alternative or complementary theory for their origin, panspermia. It discusses the possibility that the earliest life on this planet was carried here from somewhere else in the galaxy, possibly on meteorites similar to the Murchison meteorite. This does not invalidate the concept of an RNA world, but posits that this world or its precursors originated not on Earth but rather another, probably older, planet.\n\nThere are hypotheses that are in direct conflict to the RNA world hypothesis. The relative chemical complexity of the nucleotide and the unlikelihood of it spontaneously arising, along with the limited number of combinations possible among four base forms, as well as the need for RNA polymers of some length before seeing enzymatic activity, have led some to reject the RNA world hypothesis in favor of a metabolism-first hypothesis, where the chemistry underlying cellular function arose first, along with the ability to replicate and facilitate this metabolism.\n\nAnother proposal is that the dual-molecule system we see today, where a nucleotide-based molecule is needed to synthesize protein, and a peptide-based (protein) molecule is needed to make nucleic acid polymers, represents the original form of life. This theory is called RNA-peptide coevolution, or the Peptide-RNA world, and offers a possible explanation for the rapid evolution of high-quality replication in RNA (since proteins are catalysts), with the disadvantage of having to postulate the coincident formation of two complex molecules, an enzyme (from peptides) and a RNA (from nucleotides). In this Peptide-RNA World scenario, RNA would have contained the instructions for life, while peptides (simple protein enzymes) would have accelerated key chemical reactions to carry out those instructions. The study leaves open the question of exactly how those primitive systems managed to replicate themselves — something neither the RNA World hypothesis nor the Peptide-RNA World theory can yet explain, unless polymerases (enzymes that rapidly assemble the RNA molecule) played a role.\n\nA research project completed in March 2015 by the Sutherland group found that a network of reactions beginning with hydrogen cyanide and hydrogen sulfide, in streams of water irradiated by UV light, could produce the chemical components of proteins and lipids, alongside those of RNA. The researchers used the term \"cyanosulfidic\" to describe this network of reactions. In November 2017, a team at the Scripps Research Institute identified reactions involving the compound diamidophosphate which could have linked the chemical components into short peptide and lipid chains as well as short RNA-like chains of nucleotides.\n\nThe RNA world hypothesis, if true, has important implications for the definition of life. For most of the time that followed Watson and Crick's elucidation of DNA structure in 1953, life was largely defined in terms of DNA and proteins: DNA and proteins seemed the dominant macromolecules in the living cell, with RNA only aiding in creating proteins from the DNA blueprint.\n\nThe RNA world hypothesis places RNA at center-stage when life originated. The RNA world hypothesis is supported by the observations that ribosomes are ribozymes: the catalytic site is composed of RNA, and proteins hold no major structural role and are of peripheral functional importance. This was confirmed with the deciphering of the 3-dimensional structure of the ribosome in 2001. Specifically, peptide bond formation, the reaction that binds amino acids together into proteins, is now known to be catalyzed by an adenine residue in the rRNA.\n\nRNAs are known to play roles in other cellular catalytic processes, specifically in the targeting of enzymes to specific RNA sequences. In eukaryotes, the processing of pre-mRNA and RNA editing take place at sites determined by the base pairing between the target RNA and RNA constituents of small nuclear ribonucleoproteins (snRNPs). Such enzyme targeting is also responsible for gene down regulation though RNA interference (RNAi), where an enzyme-associated guide RNA targets specific mRNA for selective destruction. Likewise, in eukaryotes the maintenance of telomeres involves copying of an RNA template that is a constituent part of the telomerase ribonucleoprotein enzyme. Another cellular organelle, the vault, includes a ribonucleoprotein component, although the function of this organelle remains to be elucidated.\n\n\n\n"}
{"id": "1996094", "url": "https://en.wikipedia.org/wiki?curid=1996094", "title": "Riding a rail", "text": "Riding a rail\n\nRiding the rail (also called being \"run out of town on a rail\") was a punishment most prevalent in the United States in the 18th and 19th centuries in which an offender was made to straddle a fence rail held on the shoulders of two or more bearers. The subject was then paraded around town or taken to the city limits and dumped by the roadside. \n\nBeing ridden on a rail was typically a form of extrajudicial punishment administered by a mob, sometimes in connection with tarring and feathering, intended to show community displeasure with the offender so they either conformed their behavior to the mob's demands or left the community.\n\nA story attributed to Abraham Lincoln has him quoting a victim of being ridden out of town on a rail as having said, \"If it weren't for the honor of the thing, I'd just as soon it happened to someone else.\"\n\nIn the film \"O Brother, Where Art Thou?\", Homer Stokes denounces the Soggy Bottom Boys as hostile to the social order, but the crowd is unimpressed and runs him out of town on a rail.\n\n\n"}
{"id": "3772506", "url": "https://en.wikipedia.org/wiki?curid=3772506", "title": "Role-playing game system", "text": "Role-playing game system\n\nA role-playing game system is a set of game mechanics used in a role-playing game (RPG) to determine the outcome of a character's in-game actions.\n\nBy the late 1970s, the Chaosium staff realized that Steve Perrin's \"RuneQuest\" system had the potential to become a \"house system\", where one set of game mechanics could be used for multiple games; Greg Stafford and Lynn Willis proved that theory by boiling down the RuneQuest rules into the thin 16-page \"Basic Role-Playing\" (1980). Hero Games used their \"Champions\" rules as the basis for their Hero System. The Pacesetter house system centered on a universal \"action table\" that used one chart to resolve all game actions. Steve Jackson became interested in publishing a new roleplaying system, designed by himself, with three goals: that it be detailed and realistic; logical and well-organized; and adaptable to any setting and any level of play; this system was eventually released as \"GURPS\" (1986). The \"D&D\"-derived Palladium house system ultimately encompassed all of the Palladium Books titles. \"Mekton II\" (1987) by R. Talsorian Games revealed for the first time the full-fledged Interlock System.\n\nIn 1990, Game Designers' Workshop released the \"\" second edition game system , and decided to turn it into their house system, an umbrella under which all future games would be designed. TSR's \"Amazing Engine\" was a universal game system, a simple beginner's system. In 1996, Hero Games partnered with R. Talsorian and decided to create a new, simpler rules system to attract new players, merging it with the Interlock game system and calling it Fuzion. \"\" (1996) was built on TSR's new SAGA storytelling game system, which centered on resource management (through cards) rather than die rolls. TSR published \"Alternity\" (1997), another universal system, this one directed only toward science-fiction games. West End Games' MasterBook system had failed to catch on as a house system, so they decided to publish another, the D6 System, based on their most well-known and well-tested game system, \"\" RPG.\n\nWhile early role-playing games relied heavily on either group consensus or the judgement of a single player (the \"Dungeon Master\" or Game Master) or on randomizers such as dice, later generations of narrativist games allow role-playing to influence the creative input and output of the players, so both acting out roles and employing rules take part in shaping the outcome of the game.\n\nAn RPG system also affects the game environment, which can take any of several forms. Generic role-playing game systems, such as \"Basic Role-Playing\", \"GURPS\", and \"Fate\", are not tied to a specific storytelling genre or campaign setting and can be used as a framework to play many different types of RPG. Others, such as \"Dungeons & Dragons\", are designed to depict a specific genre or style of play, and still others, such as \"Paranoia\", are not only genre-specific but come bundled with a specific campaign setting to which the game mechanics are inseparably tied. In fact, in more psychological games such as \"Call of Cthulhu\", \"King Arthur Pendragon\", \"Unknown Armies\", and \"Don't Rest Your Head\", aspects of the game system are designed to reinforce psychological or emotional dynamics that evoke a game world's specific atmosphere.\n\nMany role-playing game systems involve the generation of random numbers by which success or failure of an action is determined. This can be done using dice (probably the most common method) or cards (as in \"Castle Falkenstein\"), but other methods may be used depending on the system. The random result is added to an attribute which is then compared to a difficulty rating, although many variations on this game mechanic exist among systems. Some (such as the \"Storyteller\"/\"Storytelling System\" and the \"One-Roll Engine\") use dice pools instead of individual dice to generate a series of random numbers, some of which may be discarded or used to determine the magnitude of the result. However, some games (such as the \"Amber Diceless Roleplaying Game\" and \"Nobilis\") use no random factor at all. These instead use direct comparison of character ability scores to difficulty values, often supplemented with points from a finite but renewable pool. These \"resource points\" represent a character's additional effort or luck, and can be used strategically by the player to influence the success of an action.\n"}
{"id": "242844", "url": "https://en.wikipedia.org/wiki?curid=242844", "title": "Rose is a rose is a rose is a rose", "text": "Rose is a rose is a rose is a rose\n\nThe sentence \"Rose is a rose is a rose is a rose.\" was written by Gertrude Stein as part of the 1913 poem \"Sacred Emily\", which appeared in the 1922 book \"Geography and Plays\". In that poem, the first \"Rose\" is the name of a person. Stein later used variations on the sentence in other writings, and \"A rose is a rose is a rose\" is among her most famous quotations, often interpreted as meaning \"things are what they are\", a statement of the law of identity, \"A is A\". In Stein's view, the sentence expresses the fact that simply using the name of a thing already invokes the imagery and emotions associated with it, an idea also intensively discussed in the problem of universals debate where Peter Abelard and others used the \"rose\" as an example concept. As the quotation diffused through her own writing, and the culture at large, Stein once remarked, \"Now listen! I'm no fool. I know that in daily life we don't go around saying 'is a ... is a ... is a ...' Yes, I'm no fool; but I think that in that line the rose is red for the first time in English poetry for a hundred years.\" (\"Four in America\").\n\nShe herself said to an audience at Oxford University that the statement referred to the fact that when the Romantics used the word \"rose\", it had a direct relationship to an actual rose. For later periods in literature this would no longer be true. The eras following romanticism, notably the modern era, use the word rose to refer to the actual rose, yet they also imply, through the use of the word, the archetypical elements of the romantic era.\n\nThe following lines appear at widely separated places in \"Sacred Emily\":\n\n\n\n\n"}
{"id": "3005753", "url": "https://en.wikipedia.org/wiki?curid=3005753", "title": "Scottish units", "text": "Scottish units\n\nScottish or Scots units of measurement are the weights and measures peculiar to Scotland which were nominally replaced by English units in 1685 but continued to be used in unofficial contexts until at least the late 18th century. The system was based on the ell (length), stone (mass), and boll and firlot (volume). This official system coexisted with local variants, especially for the measurement of land area.\n\nThe system is said to have been introduced by David I of Scotland (1124–53), although there are no surviving records until the 15th century when the system was already in normal use. Standard measures and weights were kept in each burgh, and these were periodically compared against one another at \"assizes of measures\", often during the early years of the reign of a new monarch. Nevertheless, there was considerable local variation in many of the units, and the units of dry measure steadily increased in size from 1400 to 1700.\n\nThe Scots units of length were technically replaced by the English system by an Act of the Parliament of Scotland in 1685, and the other units by the Treaty of Union with England in 1706. However many continued to be used locally during the 18th and 19th centuries. The introduction of the Imperial system by the Weights and Measures Act 1824 saw the end of any formal use in trade and commerce, although some informal use as customary units continued into the 20th century. \"Scotch measure\" or \"Cunningham measure\" was brought to parts of Ulster in Ireland by Ulster Scots settlers, and used into the mid-19th century.\n\n\nA number of conflicting systems were used for area, sometimes bearing the same names in different regions, but working on different conversion rates. Because some of the systems were based on what land would produce, rather than the physical area, they are listed in their own section. Please see individual articles for more specific information. Because fertility varied widely, in many areas, production was considered a more practical measure.\n\n\"For information on the squared units, please see the appropriate articles in the length section\"\n\nEastern Scotland:\n\n\nIn western Scotland, including Galloway:\n\nDry volume measures were slightly different for various types of grain, but often bore the same name.\n\n\nWeight equivalents of one boll are given in a trade dictionary of 1863 as follows:\nFlour 140 pounds;\nPeas or beans 280 pounds;\nOats 264 pounds;\nBarley 320 pounds;\nOatmeal 140 pounds.\n\nNipperkin was also used, but perhaps not part of this more formal set.\n\nStandard Measures of Scotland before 1707:\n\nWeight was measured according to \"troy measure\" (Lanark) and \"tron measure\" (Edinburgh), which were standardised in 1661. In the Troy system these often bore the same name as imperial measures.\n\n\nVarious local measures all existed, often using local weighing stones.\n\nSee also the weight meanings of the boll under the dry volume section, above.\n\n\n\n"}
{"id": "3452822", "url": "https://en.wikipedia.org/wiki?curid=3452822", "title": "Social phenomenon", "text": "Social phenomenon\n\nSocial phenomena include all behavior that influences or is influenced by organisms sufficiently alive to respond to one another.\n\n\n\n"}
{"id": "22821130", "url": "https://en.wikipedia.org/wiki?curid=22821130", "title": "Sortal", "text": "Sortal\n\nSortal is a concept that has been used by some philosophers in discussing issues of identity, persistence, and change. The simplest property of a sortal is that it can be counted, i.e., can take numbers as modifiers. For example, \"pea\" is a sortal in the sentence \"I want two peas\", whereas \"water\" is not a sortal in the sentence \"I want water\". Countability is not the only criterion. Thus \"red thing\" in the sentence \"There are two red things on the shelf\" is not treated as a sortal by some philosophers who use the term. There is disagreement about the exact definition of the term as well as whether it is applied to linguistic things (such as predicates or words), abstract entities (such as properties), or psychological entities (such as concepts).\n\nAccording to the \"Stanford Encyclopedia of Philosophy\", the sortal/nonsortal distinction can be characterized in at least six different ways. It is said that a sortal:\n\n\nWhile some philosophers have argued that the notion of a sortal is similar to that of the idea of a \"secondary substance\" in Aristotle, the first actual use of the term 'sortal' did not appear until John Locke in his 1690 \"Essay Concerning Human Understanding\":\n\nGottlob Frege is also named as an antecedent to the present debate over sortals. Frege pointed out that in counting things, we need to know what kind of thing it is that we are counting; that is, there needs to be a \"criterion of identity\".\n\nIn contemporary philosophy, sortals make a return with the work of P. F. Strawson, W. V. O. Quine, Peter Geach, and David Wiggins. Strawson holds that sortals are universals, Quine thinks they are predicates, and Wiggins sees them as concepts. Geach did not use the exact term \"sortal\"; however, his idea of the \"substantival expression\" is identical or nearly so to that of \"sortal\".\n\n\n"}
{"id": "27848005", "url": "https://en.wikipedia.org/wiki?curid=27848005", "title": "Spatial distribution", "text": "Spatial distribution\n\nA spatial distribution is the arrangement of a phenomenon across the Earth's surface and a graphical display of such an arrangement is an important tool in geographical and environmental statistics. A graphical display of a spatial distribution may summarize raw data directly or may reflect the outcome of more sophisticated data analysis. Many different aspects of a phenomenon can be shown in a single graphical display by using a suitable choice of different colours to represent differences.\n\nOne example of such a display could be observations made to describe the geographic patterns of features, both physical and human across the earth. \n\nThe information included could be where units of something are, how many units of the thing there are per units of area, and how sparsely or densely packed they are from each other.\n\n"}
{"id": "2246590", "url": "https://en.wikipedia.org/wiki?curid=2246590", "title": "Specification and Description Language", "text": "Specification and Description Language\n\nSpecification and Description Language (SDL) is a specification language targeted at the unambiguous specification and description of the behaviour of reactive and distributed systems.\n\nThe ITU-T has defined SDL in Recommendations Z.100 to Z.106. SDL originally focused on telecommunication systems; its current areas of application include process control and real-time applications in general. Due to its nature it can be used to represent simulation systems without ambiguity and with a graphical notation.\n\nThe Specification and Description Language provides both a graphical \"Graphic Representation\" (SDL/GR) as well as a textual \"Phrase Representation\" (SDL/PR), which are both equivalent representations of the same underlying semantics. Models are usually shown in the graphical SDL/GR form, and SDL/PR is mainly used for exchanging models between tools. A system is specified as a set of interconnected abstract machines which are extensions of finite state machines (FSM).\n\nThe language is formally complete,\nso it can be used for code generation for either simulation or final targets.\n\nThe Specification and Description Language covers five main aspects: structure, communication, behavior, data, and inheritance. The behavior of components is explained by partitioning the system into a series of hierarchies. Communication between the components takes place through gates connected by channels. The channels are of delayed channel type, so communication is usually asynchronous, but when the delay is set to zero (that is, no delay) the communication becomes synchronous.\n\nThe first version of the language was released in 1976 using graphical syntax (SDL-76). This was revised in 1980 with some rudimentary semantics (SDL-80). The semantics were refined in 1984 (SDL-84), the textual form was introduced for machine processing and data was introduced. In 1988, SDL-88 was released with a formal basis for the language: an abstract grammar as well as a concrete grammar and a full formal definition. The version released in 1992 (SDL-92) introduced object-oriented concepts such as inheritance, abstract generic types etc., with the object-oriented features described by transformations into non-object oriented ones. SDL-2010 is the latest version, an updated version of SDL-2000 that was completely based on object-orientation, rather than description by transformations. This version is accompanied by a UML-Profile: ITU-T Recommendation Z.109 (04/12), SDL-2010 combined with UML. SDL-2010 also introduced the support of C data types as initially introduced by SDL-RT.\n\nThe Hierarchy level of SDL is structured as follows. \n\nAn SDL system is made of functional blocks and each block can be further decomposed in sub-blocks. The lowest level block is composed of one or several process described as finite state machines.\n\nBlocks are connected through channels that carry the messages (or signals) exchanged between the blocks. A block agent consists of process agents.\n\nEach process agent is a state machine that contributes to the action carried out by the system. A message stimulus coming from the environment or from another agent to an agent is called a signal. Signals received by a process agent are first placed in a queue (the input port). When the state machine is waiting in a state, if the first signal in the input port is enabled for that state it starts a transition leading to another state. Transitions can output signals to other agents or to the environment. A process agent is allowed to contain procedure types so that the same actions can be invoked from different places. It is also allowed to call a remote procedure type to invoke a procedure in another agent (or even another system) and wait for a response.\n\nIn this example MyVariable is of type INTEGER and is the only variable in the process. The first transition is the \"start\" transition that initializes the local variable. A connection request message \"conReq\" is sent, a 5 seconds timer \"conReqTimer\" is started, and the state machine goes to the \"connecting\" state. In the \"connecting\" state if the timer goes off -that is equivalent to a message receive- the connection request is sent again up to 10 times. If a connection confirmation is received the state machine goes to \"connected\" state. This is a typical telecommunication protocol scenario.\n\nAvailable symbols are:\n\nSDL Abstract Data Types (ADT) support basic data types such as INTEGER, REAL, CHARSTRING as well as structured ones such as structures (STRUCT), enumerated (LITERALS), constants (SYNONYMS). Syntax looks like the one from Pascal, for example an assignment is written ':='.\n\nThe most well-known SDL modelling tools are Telelogic Tau, PragmaDev Studio, Cinderella, Safire-SDL, and ObjectGeode (now out of the market). PragmaDev Studio supports both SDL and SDL-RT which is used to develop real-time and embedded software. There are also some open source projects relative to SDL modeling like JADE which is a Java-based specification environment, and OpenGEODE, a Python/Qt implementation of an SDL editor from the European Space Agency.\n\n\n\n\n"}
{"id": "140586", "url": "https://en.wikipedia.org/wiki?curid=140586", "title": "Specification language", "text": "Specification language\n\nA specification language is a formal language in computer science used during systems analysis, requirements analysis and systems design to describe a system at a much higher level than a programming language, which is used to produce the executable code for a system.\n\nSpecification languages are generally not directly executed. They are meant to describe the \"what\", not the \"how\". Indeed, it is considered as an error if a requirement specification is cluttered with unnecessary implementation detail.\n\nA common fundamental assumption of many specification approaches is that programs are modelled as algebraic or model-theoretic structures that include a collection of sets of data values together with functions over those sets. This level of abstraction coincides with the view that the correctness of the input/output behaviour of a program takes precedence over all its other properties.\n\nIn the \"property-oriented\" approach to specification (taken e.g. by CASL), specifications of programs consist mainly of logical axioms, usually in a logical system in which equality has a prominent role, describing the properties that the functions are required to satisfy - often just by their interrelationship.\nThis is in contrast to so-called model-oriented specification in frameworks like VDM and Z, which consist of a simple realization of the required behaviour.\n\nSpecifications must be subject to a process of \"refinement\" (the filling-in of implementation detail) before they can actually be implemented. The result of such a refinement process is an executable algorithm, which is either formulated in a programming language, or in an executable subset of the specification language at hand. For example, Hartmann pipelines, when\nproperly applied, may be considered a dataflow specification which \"is\" directly executable. Another example is the Actor model which has no specific application content and must be \"specialized\" to be executable.\n\nAn important use of specification languages is enabling the creation of proofs of program correctness (\"see theorem prover\").\n\n\n"}
{"id": "28870339", "url": "https://en.wikipedia.org/wiki?curid=28870339", "title": "Strength (mathematical logic)", "text": "Strength (mathematical logic)\n\nThe relative strength of two systems of formal logic can be defined via model theory. Specifically, a logic formula_1 is said to be as strong as a logic formula_2 if every elementary class in formula_2 is an elementary class in formula_1.\n\n\n \n"}
{"id": "50429881", "url": "https://en.wikipedia.org/wiki?curid=50429881", "title": "Sustainable development reserve (Brazil)", "text": "Sustainable development reserve (Brazil)\n\nA sustainable development reserve (, RDS) in Brazil is a type of protected area inhabited by a traditional population that seeks to preserve nature while maintaining and improving the life of the population through sustainable development.\n\nThe concept of Sustainable Development Reserves originated in the Projeto Mamirauá launched in the early 1990s by the Sociedade Civil Mamirauá.\nThe project followed the principle of management based on scientific research and controlled use of natural resources.\nThe local population participates actively in the planning process and in responsible for managing and monitoring the area.\nKey aspects are that the strategy can adapt to changes in the market, private property is maintained, plans are implemented to improve living conditions, and the local people partner with government agencies and NGOs to develop proposals for sustainable use.\n\nThe Mamirauá Sustainable Development Reserve was established in 1996, the first such reserve in Brazil.\nThe adjacent Amanã Sustainable Development Reserve was established in 1998 after the successful implementation of the Mamirauá reserve, and was fully supported by the local people.\nThey already recognized the importance of preserving the vegetation and animals that they depend upon for their livelihood.\nThe new category of protected area was included in the National Protected Areas System (SNUG), which defined types of protected area of Brazil in 2000.\n\nA Sustainable Development Reserve (RDS) holds traditional populations that live by sustainable exploitation of natural resources, developed over generations and adapted to the local ecology, and that protect nature and maintain biological diversity.\nThe goals are to preserve nature while preserving and improving the quality of life of the traditional populations, and to advance scientific knowledge and understanding of traditional techniques for managing the environment.\n\nThe land in an RDS is in the public domain, and may be expropriated.\nLand use is regulated according to Law 9985 article 23 (2000) and in specific regulations.\nA board of directors is chaired by the administrative agency and includes representatives of public bodies, civil society organizations and traditional populations living in the area.\nA management plan is created by the administrative agency, which recognises the dynamic balance between conservation goals and the size of the population.\nNatural ecosystems may be exploited sustainably, and cultivation of introduced species is allowed, subject to the management plan.\nThe plan defines areas where the ecology is fully protected, buffer zones, zones of sustainable use and ecological corridors.\nPublic visits to the RDS are allowed and encouraged where compatible with local interests and the management plan.\nScientific research and education with focus on conservation of nature are also encouraged.\n\n"}
{"id": "4163404", "url": "https://en.wikipedia.org/wiki?curid=4163404", "title": "Varkari", "text": "Varkari\n\nVarkari or Warkari(meaning \"a pilgrim\") is a sampradaya (religious movement) within the bhakti spiritual tradition of Vaishnavism part of Hinduism, geographically associated with the Indian state of Maharashtra. Varkaris worship Vitthal (also known as Vithoba), the presiding deity of Pandharpur, regarded as a form of Krishna. Saints and gurus of the bhakti movement associated with the Varkaris include Jñāneśvar, Namdev, Chokhamela, Eknath, and Tukaram, all of whom are accorded the title of Sant.\n\nThe Varkari movement includes the worship Vithoba and a duty-based approach towards life emphasising moral behavior and strict avoidance of alcohol and tobacco, the adoption of a strict lacto-vegetarian diet and fasting on \"Ekadashi\" day (twice a month), self-restraint (\"brahmacharya\") during student life, equality and humanity for all rejecting discrimination based on the caste system or wealth, the reading of Hindu texts, the recitation of the \"Haripath\" every day and the regular practice of \"bhajan\" and \"kirtan\".\n\nThe Varkari tradition has been part of Hindu culture in Maharashtra since the thirteenth-century CE, when it formed as a \"panth\" (community of people with shared spiritual beliefs and practices) during the Bhakti movement. Varkaris recognise around fifty poet-saints (\"sants\") whose works over a period of 500 years were documented in an eighteenth-century hagiography by Mahipati. The Varkari tradition regards these sants to have a common spiritual line of descent.\n\nVarkaris look upon God as the Ultimate Truth and ascertained grades of values in social life but accepted ultimate equality among men. Varkaris bow in front of each other because \"everybody is Brahma\" and stressed individual sacrifice, forgiveness, simplicity, peaceful co-existence, compassion, non-violence, love and humility in social life.\n\nThe Varkari poets put God-realisation (\"haripath\") in simple terms in small booklets of verse. Each saint extolled \"japa\", chanting the Lord's name. Dnyaneshwar, Namdev, Eknath, Tukaram, Santaji Jagnade, and other Marathi Bhakti saints of the sect tried to mould the attitude of the common people, which included low castes and women, to have a kind of detachment and the courage of one's convictions in the face of evil forces.\n\nVarkari people undertake an annual pilgrimage (\"vari\") to Pandharpur, gathering there on \"Ekadashi\" (the 11th day) of the Hindu lunar calendar month of Ashadha, corresponding to a date falling sometime between late June to July in the Gregorian calendar. Pilgrims carry Palkhi of the saints from their places of \"Samadhi\" (Enlightenment or \"spiritual birth\"). The tradition of carrying the \"paduka\" (sandals) of the sants in a Palkhi was started by the youngest son of Tukaram, Narayan Maharaj, in 1685. Further changes were brought to the pilgrimage by descendants of Tukaram in the 1820s and by Haibatravbaba, a courtier of the Scindias and devotee of Dnyaneshwar.\n\nDevotees of Vitthal were holding pilgrimages prior to the 14th century.In the present day, about 40 palkhis and their devotees from all over Maharashtra do so. Another pilgrimage is celebrated on the \"Ekadashi\" of the month of Kartika, which falls in November of the Gregorian Calendar.\n\nEvents such as \"Ringan\" and \"Dhava\" are held during the pilgrimage. During the \"Ringan\", an unmounted sacred horse called Maulincha Ashva, who is believed to be the soul of the saint whose idol is being carried in the litter, runs through the rows of pilgrims, who try catching the dust particles kicked off and smear their head with the same. \"Dhava\" is another kind of race where everyone wins and it is held to commemorate the manner in which Tukaram first saw the temple at Pandharpur and started running in sheer exhilaration.\n\nVarkari wear tulasi-mala, a rosary made from \"Ocimum tenuiflorum\". They are lacto-vegetarians and follow a sattvic diet. Furthermore, like many other Vaishnava sects, they refrain from using onion and garlic in their cooking. Sect members also refrain from intoxicating substances such as alcohol.\n"}
{"id": "35627827", "url": "https://en.wikipedia.org/wiki?curid=35627827", "title": "War on Women", "text": "War on Women\n\nWar on Women is a slogan in United States politics used to describe certain Republican Party policies and legislation as a wide-scale effort to restrict women's rights, especially reproductive rights. Prominent Democrats such as Nancy Pelosi and Barbara Boxer, as well as feminists, have used the phrase to criticize proponents of these laws as trying to force their social views on women through legislation. The slogan has been used to describe Republican policies in areas such as access to reproductive health services, particularly birth control and abortion services; the prosecution of criminal violence against women; the definition of rape for the purpose of the public funding of abortion; and workplace discrimination against women.\n\nWhile used in other contexts, and prior to 2010, it became a common slogan in American political discourse after the 2010 congressional elections. The term is often used to describe opposition to the contraceptive mandate in Obamacare and policies to defund women's health organizations that perform abortions, such as Planned Parenthood.\n\nThe concept again gained attention in the 2016 U.S. presidential election, when Republican nominee Donald Trump drew notice for a history of inflammatory statements and actions toward women.\n\nThe phrase and the concept have been criticized by Republicans and some pro-life Democrats. Republican National Committee chairman Reince Priebus described it as an over-simplified fiction advanced by Democrats and the media while other Republicans contended that such rhetoric was used as a distraction from President Barack Obama and the Democrats' handling of the economy. In August 2012, Todd Akin's controversial comments regarding pregnancy and rape sparked renewed media focus on the concept. Republicans have tried to turn the phrase against Democrats by using it to argue hypocrisy for not critiquing sex scandals of members within their Party who have cheated, sexted, and harassed women; and for not supporting bills to combat sex-selective abortion.\n\nIn 1989, radical feminist Andrea Dworkin wrote in a book introduction about \"war on women\" and, in 1997, she collected that and other writings in \"Life and Death\", for which the subtitle was \"Unapologetic Writings on the Continuing War Against Women\". Feminist Susan Faludi's 1991 book \"\", argued that throughout the 1980s the media created a \"backlash\" against the feminist advances of the 1970s. Former Republican political consultant Tanya Melich's 1996 memoir, \"The Republican War Against Women: An Insider's Report from Behind the Lines\", describes the incorporation of the pro-life movement and opposition to the Equal Rights Amendment by Republicans as a divergence from feminist causes.\n\nGeorge W. Bush's administration met with resistance from feminists and women's rights activists throughout his Presidency. In 2004 The Feminist Press published Laura Flanders' collection of essays \"The W Effect: Bush's War On Women\". In 2006 economist Barbara Finlay's critique of the Bush administration's treatment of women was published by Zed Books under the title \"George W. Bush and the War on Women: Turning Back the Clock on Progress\".\n\nIn the 2010 midterm elections, the Republican Party (GOP) won the majority in the House of Representatives. On January 4, 2011, the day after Congress convened, Kaili Joy Gray of the liberal Daily Kos wrote an opinion piece titled \"The Coming War on Women\". In the article, she outlined many of the measures that Republicans intended to push through the House of Representatives, including personhood laws, fetal pain laws, and the effort to defund Planned Parenthood. In February 2011, an AlterNet article by Sarah Seltzer and Lauren Kelley entitled \"9 New laws in the GOP's War on Women\" began to document state-level legislation restricting abortion access and rights. That same month, New York Representative Jerrold Nadler referred to the proposed No Taxpayer Funding for Abortion Act, one of the Congress's first actions and one that would have changed policy to allow only victims of \"forcible rape\" or child sex abuse to qualify for Medicaid funding for abortion, as \"an entirely new front in the war on women and their families\". Florida Representative and Chair of the Democratic National Committee Debbie Wasserman Schultz began using the term \"War on Women\" in March 2011.\n\nThe \"War on Women\" slogan was used often when describing the unprecedented rise in the passage of provisions related to women's health and reproductive rights in 2011 and 2012. In 2011, state legislatures across the United States introduced over 1100 provisions related to women's health and reproductive rights, and in the first quarter of 2012 an additional 944 provisions were introduced in state legislatures, half of which would restrict access to abortion. Legislation has focused on mandatory ultrasounds, narrowing the time when abortions may be performed and limiting insurance coverage of abortion.\n\nDemocratic strategist Zerlina Maxwell wrote an editorial for \"U.S. News & World Report\" in which she cited a Guttmacher Institute analysis showing state legislatures enacted 135 pieces of legislation affecting women's reproductive rights as evidence that the \"Republican 'War on Women' is no fiction.\" The analysis found that between 2000 and 2011, the number of states hostile to abortion rights have increased markedly, and that in 2011 there was an unprecedented rise in the number of provisions passed by state legislatures restricting abortion.\n\nMany states have adopted model legislation written by Americans United for Life, a pro-life advocacy group. In June 2011, Charmaine Yoest and Denise M. Burke of Americans United, acknowledged the expression in an op-ed for \"The Wall Street Journal\", writing that \"Indiana is being threatened with the loss of federal funding for health care and being held up to scorn as having 'declared war on women.'\"\n\nIn 2011 and 2012, \"War on Women\" was used to describe the legislation passed by many states requiring that women seeking abortions first undergo government-mandated ultrasounds. Some states require that women view the image of the fetus and others require that women be offered the opportunity to listen to the fetal heartbeat. Since many women's pregnancies are not far enough along to get an image via a traditional ultrasound, transvaginal ultrasounds, which involve the physician inserting a probe into the woman's vagina, may be required, but these requirements vary state to state. Critics have questioned the value of having a medically unnecessary procedure, and characterized it as similar to some states' legal definition of rape. Writer Megan Carpentier underwent the procedure and indicated that although it was not comparable to being raped, the process was \"uncomfortable to the point of being painful, emotionally triggering... and something that no government should force its citizens to undergo to make a political point.\" However, in an article critical of the assumptions of those on both sides of the issue, sociologist Tracy Weitz, who opposes mandatory ultrasound, notes that \"the use of trans-vaginal ultrasounds is routine among abortion providers.\"\n\nVirginia State legislators passed a bill in 2012 requiring women to have an ultrasound before having an abortion. The legislation, signed by Governor Bob McDonnell, would require that the provider of an abortion make a copy of the fetal image and include it in the file of the patient. In Louisiana, where pregnant women are already required to view ultrasounds of their fetuses before receiving an abortion, lawmakers proposed a bill that would require them to listen to the embryonic/fetal heartbeat as well. Pennsylvania Governor Tom Corbett drew criticism when he said of his state's new mandatory transvaginal ultrasound law that \"You can't make anybody watch, okay? Because you just have to close your eyes. As long as it's on the exterior and not the interior.\"\n\nIn June 2013, Representative Trent Franks of Arizona, passed a national bill in the House Judiciary Committee that would ban abortions after the 20th week of pregnancy. The bill did not include exceptions for rape, incest or health of the mother. In responding to the bill's lack of exception for rape victims, Franks stated that \"the incidence of rape resulting in pregnancy are very low,\" which was compared to the controversial statements made by Todd Akin; studies show that the incidence of pregnancy from rape is approximately equal to or higher than the rate from consensual sex. Afterwards, the House Rules Committee added exceptions for rape and incest. Georgia legislators passed HB 954, a \"fetal pain bill\" criminalizing abortions performed after the 20th week of pregnancy. The bill, which does not contain exemptions for rape or incest, has been referred to as the \"women as livestock bill\" by opponents after Representative Terry England made a comparison between women seeking abortions for stillborn fetuses to delivering calves and pigs on a farm.\n\nIn April 2012, Arizona passed legislation banning abortions occurring 20 weeks after a woman's last menstrual period. A judge from the District Court initially upheld this ban, but the Ninth Circuit Court of Appeals ruled in August 2012 that the ban could not be enforced until an appeal on the law had been decided. The Ninth Circuit then struck down the law as unconstitutional in May 2013. Eight other states, including Nebraska, Alabama, Georgia, Indiana, Idaho and Oklahoma, have passed such bills; unlike Arizona, the gestational age in these states is calculated from fertilization (20 weeks post-fertilization-which means 22 weeks LMP). In 2013, Idaho's ban was struck down as unconstitutional by a federal judge. States such as Ohio have proposed so-called \"heartbeat bills\" that would prohibit abortions when the heartbeat of the fetus can be detected. Fetal heartbeats can be detected as early as six weeks into a pregnancy.\n\nIn 2011, voters in Mississippi rejected Initiative 26, a measure that would have declared that human life begins at fertilization, which had drawn support from conservative Republicans and Democrats. Critics of the initiative indicated that the law would have made abortion illegal even in cases where the mother's life is in danger.\n\nSince the mid-1990s, the regulatory burden on abortion providers has increased. TRAP laws (Targeted Regulation of Abortion Providers) have been passed in numerous states. In 2015, the United States Supreme Court agreed to an emergency appeal regarding a Texas law that would have shut down 10 of the remaining 19 abortion clinics within the state. Sometime in the fall of 2015, the Supreme Court will decide whether or not to hear the clinics' full appeal of the ruling, which, if held, would be the largest abortion case before the Supreme Court in nearly 25 years.\n\nIn February 2011, South Dakota state legislators considered a bill that would expand that state's definition of justifiable homicide to include killings committed by a party other than a pregnant woman for the purpose of preventing harm to a fetus, a measure interpreted by critics as allowing the killing of abortion providers. Similar legislation was considered in Iowa.\n\nSeveral state legislatures have passed or are considering legislation to prevent parents from suing doctors who fail to warn them of fetal problems, which are sometimes known as wrongful birth lawsuits. Some of the laws, such as one proposed in Arizona, make exceptions for \"intentional or grossly negligent acts\", while others do not.\n\nA Kansas bill passed March 2012 requires doctors to warn women seeking abortions that they are linked to breast cancer, a claim that has been refuted by the medical community.\n\nIn April 2012, Wisconsin Governor Scott Walker signed into law a bill requiring doctors who prescribe the medical abortion pill to have three meetings with patients or be subject to felony charges. Planned Parenthood suspended non-surgical abortions in the state.\n\nOn January 20, 2012, Health and Human Services' Secretary Kathleen Sebelius announced a mandate requiring that all health plans provide coverage for all contraceptives approved by the FDA as part of preventive health services for women. Following complaints from Catholic bishops, an exception was created for religious institutions whereby an employee of a religious institution that does not wish to provide reproductive health care can seek it directly from the insurance company at no additional cost. Missouri Senator Roy Blunt proposed an amendment (the Blunt Amendment) that would have \"allowed employers to refuse to include contraception in health care coverage if it violated their religious or moral beliefs\", but it was voted down 51-48 by the U.S. Senate on March 1, 2012. A bill passed by the Arizona House would allow employers to exclude medication used for contraceptive purposes from their health insurance plans.\n\nIn February 2012, Republican Congressman Darrell Issa convened an all-male panel addressing religious freedom and contraceptive mandates for health insurers. He did not allow Sandra Fluke, a Georgetown University Law Center student who was proposed as a witness by the Democrats, to participate in the hearing, arguing that Fluke was not a member of the clergy. Democratic Representatives then staged a separate panel where Fluke was allowed to speak. Later that month, American conservative talk-show host Rush Limbaugh controversially called Sandra Fluke a \"slut\" and \"prostitute\" and continued in similar fashion for the next two days. Foster Friess, the billionaire supporting the candidacy of Rick Santorum, suggested in February 2012 that women put aspirin between their knees as a form of contraception. Limbaugh echoed the sentiment, saying he would \"buy all of the women at Georgetown University as much aspirin to put between their knees as they want.\" Nancy Pelosi circulated a petition and asked that Republicans in the House of Representatives disavow the comments by Friess and Limbaugh, which she called \"vicious and inappropriate\".\n\nSeveral Democrats used the phrase \"War on Women\" to criticize the Republican Party after House Republicans passed legislation to cut off funding for Planned Parenthood in February 2011. Texas, Indiana and Kansas have passed legislation in an effort to defund the organization. Arizona, Ohio and New Hampshire are considering similar legislation. In Texas, lawmakers reduced funds for family planning from $111M to $37M. The future of the Women's Health Program in Texas, which receives 90% of its funding from the federal government, is unclear. The Indiana legislature passed a bill restricting Medicaid funds for Planned Parenthood. Indiana Representative Bob Morris later referred to the Girl Scouts of the USA as a tactical arm of Planned Parenthood. A 2011 Kansas statute cut funding to Planned Parenthood.\n\nOn January 31, 2012, breast cancer organization Susan G. Komen for the Cure stopped funding Planned Parenthood, citing a congressional investigation by Rep. Cliff Stearns and a newly created internal rule about not funding organizations under any federal, state or local investigation. Four days later, Komen's Board of Directors reversed the decision and announced that it would amend the policy to \"make clear that disqualifying investigations must be criminal and conclusive in nature and not political\". Several top-level staff members resigned from Komen during the controversy.\n\nThe National Organization for Women (NOW), in the U.S., in 2011, stated its opinion that \"the 'war on women' isn't restricted to U.S. women\", saying that the House of Representatives planned to \"cut ... international family planning assistance... [to] include the elimination of all U.S. funds designated for UNFPA\" (now known as the United Nations Population Fund).\n\nIn January 2011, the No Taxpayer Funding for Abortion Act moved to change how rape is treated when used to determine whether abortions qualify for Medicaid funding. Under the language of the bill, only cases of \"forcible rape\" or child sexual abuse would have qualified. Political activist groups Moveon.org and Emily's List charged that this constituted a Republican attempt to \"redefine rape\".\n\nIn 2014, Michigan law prohibited all public and most private insurers from covering abortions including in cases of rape and incest. It requires women to buy separate insurance and has been called \"rape insurance\" by opponents because of the possibility that women will need to have separate insurance for an abortion resulting from rape.\n\nUnsuccessful Missouri Republican candidate to the U.S. Senate Todd Akin made controversial comments in August 2012 asserting (falsely) that women who are victims of \"legitimate rape\" rarely experience pregnancy from rape. While he issued an apology for his comments, they were widely criticized, and they sparked a renewed focus on Republican attitudes towards women and \"shift[ed] the national discussion to divisive social issues that could repel swing voters rather than economic issues that could attract them\".\n\nThere were multiple calls from Republicans for Akin to step down as nominee. \"The Washington Post\" reported a \"stampede\" of Republicans dissociating from Akin. NRSC chairman John Cornyn said the Republican Party would no longer provide him Senate election funding. A campaign spokesman for Mitt Romney and Paul Ryan said both disagreed with Akin's position and would not oppose abortion in instances of rape. Ryan reportedly called Akin to advise him to step aside. RNC Chairman Reince Priebus warned Akin not to attend the upcoming 2012 Republican convention and said he should resign the nomination. He described Akin's comments as \"biologically stupid\" and \"bizarre\" and said that \"This is not mainstream talk that he's referring to and his descriptions of whatever an illegitimate rape is.\"\n\nOther Republican candidates in the 2012 election also created controversy with their comments on rape. Indiana Senate candidate Richard Mourdock, when discussing his opposition to exceptions on abortion bans in cases of rape, said, \"I think even if life begins in that horrible situation of rape, that it is something that God intended to happen.\" Tom Smith, the Senate candidate in Pennsylvania, compared pregnancy from rape to pregnancy out of wedlock. Akin, Mourdock, and Smith all lost their races due to backlash from women voters.\n\nColumnist Margery Eagan has said that opposition to reforming the military in order to better prosecute sexual assaults constitutes a war on women. Senator Saxby Chambliss of Georgia was criticized for saying that part of the cause of the sexual assault was young officers' \"hormone level created by nature\".\n\nThe renewal of the Violence Against Women Act, which provides for community violence prevention programs and battered women's shelters, was fiercely opposed by conservative Republicans in 2012. The Act was originally passed in 1994 and has been reauthorized by Congress twice. Senate Minority leader Mitch McConnell, who has previously voted against renewal of the Act, said the bill was a distraction from a small business bill. However, in 2013 a strengthened version of the act was passed by Congress with bipartisan support.\n\nIn February 2011, Ms. magazine charged House Republicans with launching a new \"War on Women\" for their proposal to cut the WIC budget by 10%. The WIC program, which President Barack Obama has called a spending priority, is a federal assistance program for low-income pregnant women, breastfeeding women, and infants and children under the age of five. The program had been running a surplus, primarily due to decreases in the cost of milk, which make up 20% of WIC expenditures, and lower participation than expected. WIC's budget was later cut by 5.2% as part of the bipartisan budget sequestration in 2013.\n\nIn April 2012, Governor Scott Walker's repeal of Wisconsin's \"Equal Pay Enforcement Act\" was described by opponents as furthering the \"War on Women\", which became a big issue in his recall election. The \"Equal Pay Enforcement Act\" was passed in 2009 in response to the large gap between the wages of men and women in Wisconsin. Among other provisions, it allowed workplace discrimination victims redress in the less costly and more accessible state court system, rather than in federal court. Defending the repeal, Walker stated that the Act had essentially been nothing but a boon for trial lawyers, incentivizing them to sue job creators, including female business owners, and that the law was being used to clog up the legal system in his state. While it is still illegal in Wisconsin to pay women less on the basis of their sex, the repeal was criticized for reinforcing the gender pay gap, a recurrent theme in the struggle for women's rights. Republican State Senator Glenn Grothman said of the repeal, \"You could argue that money is more important for men. I think a guy in their first job, maybe because they expect to be a breadwinner someday, may be a little more money-conscious.\" Law student Sandra Fluke, criticized Grothman's comment, highlighting legislation that supports equal pay for equal work, such as the federal \"Lilly Ledbetter Fair Pay Act of 2009\".\n\nA May 2012 Kaiser Family Foundation poll found that 31 percent of women and 28 percent of men believed there was an ongoing and wide-scale effort to \"limit women's reproductive health choices and services\". 45 percent of women and 44 percent of men responded that some groups would like to limit these choices and services, but it's not wide‐scale. Democrats were more likely than Republicans to say there is a movement, but the largest gap was between liberal and conservative ideologies. Among those women believing these efforts to be wide-scale, 75 percent saw this as \"a bad thing\" against 16 percent who saw this as \"a good thing\". In the same poll, 42 percent of women and men have said they have taken some action in response to what they heard regarding reproductive health issues.\n\nIn the Colorado race of the 2014 midterm elections, the Republican candidate Cory Gardner unseated the incumbent Democratic Senator Mark Udall. NARAL Pro-Choice America gives Udall a 100% rating for abortion rights, and Gardner earned a 0% rating. Udall ran a number of TV ads highlighting his abortion stance, which critics said was a negative campaign that overplayed the \"war on women\" issue.\n\nSandra Fluke stood as a candidate in California, losing by 61 to 39.\n\nPolitical analysts have interpreted the 2016 Hillary Clinton presidential campaign as appealing to a female constituency, and have used the phrase \"war on women\" to describe Republican opposition. Republican presidential candidate Carly Fiorina said \"If Hillary Clinton were to face a female nominee, there are a whole set of things that she won't be able to talk about. She won't be able to talk about being the first woman president. She won't be able to talk about a war on women without being challenged. She won't be able to play the gender card.\"\n\nDonald Trump, a Republican candidate for the 2016 Presidency attended a Fox News debate in August 2015, where Megyn Kelly asked him about how he would respond to a Hillary Clinton campaign saying that he was waging a \"war on women\". In a later interview with Don Lemon on \"CNN Tonight\", Trump said that Kelly is a \"lightweight\" and had \"blood coming out of her eyes, blood coming out of her wherever\". Trump tweeted that his remark referred to Kelly's \"nose\" but was interpreted by critics as a reference to menstruation. RedState.com editor Erick Erickson cancelled Trump's invitation to a RedState meeting, saying \"there are just real lines of decency a person running for President should not cross\".\n\nCritics of the term have denied that a War on Women exists and some have suggested that it is a ploy to influence women voters. Reince Priebus, the Chairman of the RNC, referred to the War as a \"fiction\", saying \"If the Democrats said we had a war on caterpillars and every mainstream media outlet talked about the fact that Republicans have a war on caterpillars, then we'd have problems with caterpillars.\" Republican Representative Cathy McMorris Rodgers called the war a myth, saying \"It's an effort to drive a political wedge in an election year.\" Referring to the 2010 elections and Nancy Pelosi, she said that \"It could be argued that the women actually unelected the first woman Speaker of the House.\" South Carolina Governor Nikki Haley said in 2012 \"There is no war on women. Women are doing well.\" Republican Representative Paul Ryan mocked the idea of a Republican War on Women, saying \"Now it's a war on women; tomorrow it's going to be a war on left-handed Irishmen or something like that.\"\n\nSenator John McCain, when asked by journalist David Gregory if there was a Republican War on Women, said \"I think that there is a perception out there because of how this whole contraception issue played out — ah, we need to get off of that issue, in my view.\"\n\nRepublican Senator Lisa Murkowski countered the criticism from her fellow party members, challenging them to \"go home and talk to your wife and your daughters\" if they did not think there was a war on women, saying \"It makes no sense to make this attack on women.\"\n\nAfter the 2012 rape and pregnancy controversies, Republican strategists met with aides of Republican figures to advise them on how to run against female candidates.\n\nMembers of the Democratic Party, both prominent and local, have been accused of participating in the war on women. In a column for \"USA Today\", Glenn Reynolds wrote in July 2013 that \"most of the action in the war on women seems to be coming from the Democratic front,\" referring to the allegations of sexual harassment against San Diego mayor Bob Filner, the Anthony Weiner sexting scandal, and the Eliot Spitzer prostitution scandal. The Republican National Senatorial Committee has also used these scandals in press releases, tying Democratic Senators in Iowa and New Hampshire to the allegations.\n\nThe messaging from Republicans was described as unlikely to be effective by Garance Franke-Ruta in \"The Atlantic\" because \"[the War on Women] was an argument about Republican policies on women ... rather than about reprehensible individual behavior.\" Noting that many of the targets are not on upcoming ballots, Franke-Ruta continued by saying the Republican Party \"is going to need its own pro-active framework for thinking about what is happening in America and why women have been drawn to Democrats in numbers that matter in key elections.\"\n\nJonathan Alter characterized the phrase as an \"alliteratve but unfair notion\".\n\nDavid Weigel called for \"a moment of silence\" in his article entitled \"The 'War on Women' Is Over: The life cycle of a political talking point, from birth to adolescence to death.\" In it he explained his understanding of the stages in the \"life cycle\" of the Democratic \"talking point\".\n\nMolly Redden wrote an article for \"Mother Jones\" entitled \"The War on Women is Over -- and Women Lost\". She described the difficulties faced by abortion providers: \"Activists have been calling it the 'war on women.' But the onslaught of new abortion restrictions has been so successful, so strategically designed, and so well coordinated that the war in many places has essentially been lost.\"\n\nFeminist Camille Paglia has called the term \"War on Women\" a \"tired cliché that is as substance-less as a druggy mirage but that the inept GOP has never been able to counter.\"\n"}
{"id": "1181646", "url": "https://en.wikipedia.org/wiki?curid=1181646", "title": "War on drugs", "text": "War on drugs\n\nThe war on drugs is a campaign, led by the U.S. federal government, of drug prohibition, military aid, and military intervention, with the stated aim being to reduce the illegal drug trade in the United States. The initiative includes a set of drug policies that are intended to discourage the production, distribution, and consumption of psychoactive drugs that the participating governments and the UN have made illegal. The term was popularized by the media shortly after a press conference given on June 18, 1971, by President Richard Nixon—the day after publication of a special message from President Nixon to the Congress on Drug Abuse Prevention and Control—during which he declared drug abuse \"public enemy number one\". That message to the Congress included text about devoting more federal resources to the \"prevention of new addicts, and the rehabilitation of those who are addicted\", but that part did not receive the same public attention as the term \"war on drugs\". However, two years prior to this, Nixon had formally declared a \"war on drugs\" that would be directed toward eradication, interdiction, and incarceration. Today, the Drug Policy Alliance, which advocates for an end to the War on Drugs, estimates that the United States spends $51 billion annually on these initiatives.\n\nOn May 13, 2009, Gil Kerlikowske—the Director of the Office of National Drug Control Policy (ONDCP)—signaled that the Obama administration did not plan to significantly alter drug enforcement policy, but also that the administration would not use the term \"War on Drugs\", because Kerlikowske considers the term to be \"counter-productive\". ONDCP's view is that \"drug addiction is a disease that can be successfully prevented and treated... making drugs more available will make it harder to keep our communities healthy and safe\". One of the alternatives that Kerlikowske has showcased is the drug policy of Sweden, which seeks to balance public health concerns with opposition to drug legalization. The prevalence rates for cocaine use in Sweden are barely one-fifth of those in Spain, the biggest consumer of the drug.\n\nIn June 2011, the Global Commission on Drug Policy released a critical report on the War on Drugs, declaring: \"The global war on drugs has failed, with devastating consequences for individuals and societies around the world. Fifty years after the initiation of the UN Single Convention on Narcotic Drugs, and years after President Nixon launched the US government's war on drugs, fundamental reforms in national and global drug control policies are urgently needed.\" The report was criticized by organizations that oppose a general legalization of drugs.\n\nThe first U.S. law that restricted the distribution and use of certain drugs was the Harrison Narcotics Tax Act of 1914. The first local laws came as early as 1860. In 1919, the United States passed the 18th Amendment, prohibiting the sale, manufacture, and transportation of alcohol, with exceptions for religious and medical use. In 1920, the United States passed the National Prohibition Act (Volstead Act), enacted to carry out the provisions in law of the 18th Amendment.\n\nThe Federal Bureau of Narcotics was established in the United States Department of the Treasury by an act of June 14, 1930 (46 Stat. 585). In 1933, the federal prohibition for alcohol was repealed by passage of the 21st Amendment. In 1935, President Franklin D. Roosevelt publicly supported the adoption of the Uniform State Narcotic Drug Act. \"The New York Times\" used the headline \"Roosevelt Asks Narcotic War Aid\".\n\nIn 1937, the Marihuana Tax Act of 1937 was passed. Several scholars have claimed that the goal was to destroy the hemp industry, largely as an effort of businessmen Andrew Mellon, Randolph Hearst, and the Du Pont family. These scholars argue that with the invention of the decorticator, hemp became a very cheap substitute for the paper pulp that was used in the newspaper industry. These scholars believe that Hearst felt that this was a threat to his extensive timber holdings. Mellon, United States Secretary of the Treasury and the wealthiest man in America, had invested heavily in the DuPont's new synthetic fiber, nylon, and considered its success to depend on its replacement of the traditional resource, hemp. However, there were circumstances that contradict these claims. One reason for doubts about those claims is that the new decorticators did not perform fully satisfactorily in commercial production. To produce fiber from hemp was a labor-intensive process if you include harvest, transport and processing. Technological developments decreased the labor with hemp but not sufficient to eliminate this disadvantage.\n\nOn October 27, 1970, Congress passed the Comprehensive Drug Abuse Prevention and Control Act of 1970, which, among other things, categorized controlled substances based on their medicinal use and potential for addiction. In 1971, two congressmen released an explosive report on the growing heroin epidemic among U.S. servicemen in Vietnam; ten to fifteen percent of the servicemen were addicted to heroin, and President Nixon declared drug abuse to be \"public enemy number one\".\n\nAlthough Nixon declared \"drug abuse\" to be public enemy number one in 1971, the policies that his administration implemented as part of the Comprehensive Drug Abuse Prevention and Control Act of 1970 were a continuation of drug prohibition policies in the U.S., which started in 1914.\n\nIn 1973, the Drug Enforcement Administration was created to replace the Bureau of Narcotics and Dangerous Drugs.\n\nThe Nixon Administration also repealed the federal 2–10-year mandatory minimum sentences for possession of marijuana and started federal demand reduction programs and drug-treatment programs. Robert DuPont, the \"Drug czar\" in the Nixon Administration, stated it would be more accurate to say that Nixon ended, rather than launched, the \"war on drugs\". DuPont also argued that it was the proponents of drug legalization that popularized the term \"war on drugs\".\n\nIn 1982, Vice President George H. W. Bush and his aides began pushing for the involvement of the CIA and U.S. military in drug interdiction efforts.\nThe Office of National Drug Control Policy (ONDCP) was originally established by the National Narcotics Leadership Act of 1988, which mandated a national anti-drug media campaign for youth, which would later become the National Youth Anti-Drug Media Campaign. The director of ONDCP is commonly known as the Drug czar, and it was first implemented in 1989 under President George H. W. Bush, and raised to cabinet-level status by Bill Clinton in 1993. These activities were subsequently funded by the Treasury and General Government Appropriations Act of 1998. The Drug-Free Media Campaign Act of 1998 codified the campaign at .\n\nThe Global Commission on Drug Policy released a report on June 2, 2011, alleging that \"The War On Drugs Has Failed.\" The commissioned was made up of 22 self-appointed members including a number of prominent international politicians and writers. U.S. Surgeon General Regina Benjamin also released the first ever National Prevention Strategy.\n\nOn May 21, 2012, the U.S. Government published an updated version of its Drug Policy. The director of ONDCP stated simultaneously that this policy is something different from the \"War on Drugs\":\n\nAt the same meeting was a declaration signed by the representatives of Italy, the Russian Federation, Sweden, the United Kingdom and the United States in line with this:\n\"Our approach must be a balanced one, combining effective enforcement to restrict the supply of drugs, with efforts to reduce demand and build recovery; supporting people to live a life free of addiction.\"\n\nIn March 2016 the International Narcotics Control Board stated that the International Drug Control treaties do not mandate a \"war on drugs.\"\n\nAccording to Human Rights Watch, the War on Drugs caused soaring arrest rates that disproportionately targeted African Americans due to various factors. John Ehrlichman, an aide to Nixon, said that Nixon used the war on drugs to criminalize and disrupt black and hippie communities and their leaders.\n\nThe present state of incarceration in the U.S. as a result of the war on drugs arrived in several stages. By 1971, different stops on drugs had been implemented for more than 50 years (for e.g. since 1914, 1937 etc.) with only a very small increase of inmates per 100,000 citizens.\nDuring the first 9 years after Nixon coined the expression \"War on Drugs\", statistics showed only a minor increase in the total number of imprisoned.\n\nAfter 1980, the situation began to change. In the 1980s, while the number of arrests for all crimes had risen by 28%, the number of arrests for drug offenses rose 126%. The result of increased demand was the development of privatization and the for-profit prison industry. The US Department of Justice, reporting on the effects of state initiatives, has stated that, from 1990 through 2000, \"the increasing number of drug offenses accounted for 27% of the total growth among black inmates, 7% of the total growth among Hispanic inmates, and 15% of the growth among white inmates.\" In addition to prison or jail, the United States provides for the deportation of many non-citizens convicted of drug offenses.\n\nIn 1994, the \"New England Journal of Medicine\" reported that the \"War on Drugs\" resulted in the incarceration of one million Americans each year. In 2008, the \"Washington Post\" reported that of 1.5 million Americans arrested each year for drug offenses, half a million would be incarcerated. In addition, one in five black Americans would spend time behind bars due to drug laws.\n\nFederal and state policies also impose collateral consequences on those convicted of drug offenses, such as denial of public benefits or licenses, that are not applicable to those convicted of other types of crime. In particular, the passage of the 1990 Solomon–Lautenberg amendment led many states to impose mandatory driver's license suspensions (of at least 6 months) for persons committing a drug offense, regardless of whether any motor vehicle was involved. Approximately 191,000 licenses were suspended in this manner in 2016, according to a Prison Policy Initiative report.\n\nIn 1986, the U.S. Congress passed laws that created a 100 to 1 sentencing disparity for the trafficking \"or possession\" of crack when compared to penalties for \"trafficking\" of powder cocaine, which had been widely criticized as discriminatory against minorities, mostly blacks, who were more likely to use crack than powder cocaine. This 100:1 ratio had been required under federal law since 1986. Persons convicted in federal court of possession of 5 grams of crack cocaine received a minimum mandatory sentence of 5 years in federal prison. On the other hand, possession of 500 grams of powder cocaine carries the same sentence. In 2010, the Fair Sentencing Act cut the sentencing disparity to 18:1.\n\nAccording to Human Rights Watch, crime statistics show that—in the United States in 1999—compared to non-minorities, African Americans were far more likely to be arrested for drug crimes, and received much stiffer penalties and sentences.\n\nStatistics from 1998 show that there were wide racial disparities in arrests, prosecutions, sentencing and deaths. African-American drug users made up for 35% of drug arrests, 55% of convictions, and 74% of people sent to prison for drug possession crimes. Nationwide African-Americans were sent to state prisons for drug offenses 13 times more often than other races, even though they only supposedly comprised 13% of regular drug users.\n\nAnti-drug legislation over time has also displayed an apparent racial bias. University of Minnesota Professor and social justice author Michael Tonry writes, \"The War on Drugs foreseeably and unnecessarily blighted the lives of hundreds and thousands of young disadvantaged black Americans and undermined decades of effort to improve the life chances of members of the urban black underclass.\"\n\nIn 1968, President Lyndon B. Johnson decided that the government needed to make an effort to curtail the social unrest that blanketed the country at the time. He decided to focus his efforts on illegal drug use, an approach which was in line with expert opinion on the subject at the time. In the 1960s, it was believed that at least half of the crime in the U.S. was drug related, and this number grew as high as 90 percent in the next decade. He created the Reorganization Plan of 1968 which merged the Bureau of Narcotics and the Bureau of Drug Abuse to form the Bureau of Narcotics and Dangerous Drugs within the Department of Justice. The belief during this time about drug use was summarized by journalist Max Lerner in his work \"America as a Civilization\" (1957):\n\nAs a case in point we may take the known fact of the prevalence of reefer and dope addiction in Negro areas. This is essentially explained in terms of poverty, slum living, and broken families, yet it would be easy to show the lack of drug addiction among other ethnic groups where the same conditions apply.\n\nRichard Nixon became president in 1969, and did not back away from the anti-drug precedent set by Johnson. Nixon began orchestrating drug raids nationwide to improve his \"watchdog\" reputation. Lois B. Defleur, a social historian who studied drug arrests during this period in Chicago, stated that, \"police administrators indicated they were making the kind of arrests the public wanted\". Additionally, some of Nixon's newly created drug enforcement agencies would resort to illegal practices to make arrests as they tried to meet public demand for arrest numbers. From 1972 to 1973, the Office of Drug Abuse and Law Enforcement performed 6,000 drug arrests in 18 months, the majority of the arrested black.\n\nThe next two Presidents, Gerald Ford and Jimmy Carter, responded with programs that were essentially a continuation of their predecessors. Shortly after Ronald Reagan became President in 1981 he delivered a speech on the topic. Reagan announced, \"We're taking down the surrender flag that has flown over so many drug efforts; we're running up a battle flag.\"\n\nThen, driven by the 1986 cocaine overdose of black basketball star Len Bias, Reagan was able to pass the Anti-Drug Abuse Act through Congress. This legislation appropriated an additional $1.7 billion to fund the War on Drugs. More importantly, it established 29 new, mandatory minimum sentences for drug offenses. In the entire history of the country up until that point, the legal system had only seen 55 minimum sentences in total. A major stipulation of the new sentencing rules included different mandatory minimums for powder and crack cocaine. At the time of the bill, there was public debate as to the difference in potency and effect of powder cocaine, generally used by whites, and crack cocaine, generally used by blacks, with many believing that \"crack\" was substantially more powerful and addictive. Crack and powder cocaine are closely related chemicals, crack being a smokeable, freebase form of powdered cocaine hydrochloride which produces a shorter, more intense high while using less of the drug. This method is more cost effective, and therefore more prevalent on the inner-city streets, while powder cocaine remains more popular in white suburbia. The Reagan administration began shoring public opinion against \"crack\", encouraging DEA official Robert Putnam to play up the harmful effects of the drug. Stories of \"crack whores\" and \"crack babies\" became commonplace; by 1986, \"Time\" had declared \"crack\" the issue of the year. Riding the wave of public fervor, Reagan established much harsher sentencing for crack cocaine, handing down stiffer felony penalties for much smaller amounts of the drug.\n\nReagan protégé and former Vice-President George H. W. Bush was next to occupy the oval office, and the drug policy under his watch held true to his political background. Bush maintained the hard line drawn by his predecessor and former boss, increasing narcotics regulation when the First National Drug Control Strategy was issued by the Office of National Drug Control in 1989.\n\nThe next three presidents – Clinton, Bush and Obama – continued this trend, maintaining the War on Drugs as they inherited it upon taking office. During this time of passivity by the federal government, it was the states that initiated controversial legislation in the War on Drugs. Racial bias manifested itself in the states through such controversial policies as the \"stop and frisk\" police practices in New York city and the \"three strikes\" felony laws began in California in 1994.\n\nIn August 2010, President Obama signed the Fair Sentencing Act into law that dramatically reduced the 100-to-1 sentencing disparity between powder and crack cocaine, which disproportionately affected minorities.\n\nCommonly used illegal drugs include heroin, cocaine, methamphetamine, and, marijuana.\n\nHeroin is an opiate that is highly addictive. If caught selling or possessing heroin, a perpetrator can be charged with a felony and face two–four years in prison and could be fined to a maximum of $20,000.\n\nCrystal meth is composed of methamphetamine hydrochloride. It is marketed as either a white powder or in a solid (rock) form. The possession of crystal meth can result in a punishment varying from a fine to a jail sentence. As with other drug crimes, sentencing length may increase depending on the amount of the drug found in the possession of the defendant.\n\nCocaine possession is illegal across the U.S., with the cheaper crack cocaine incurring even greater penalties. Having possession is when the accused knowingly has it on their person, or in a backpack or purse. The possession of cocaine with no prior conviction, for the first offense, the person will be sentenced to a maximum of one year in prison or fined $1,000, or both. If the person has a prior conviction, whether it is a narcotic or cocaine, they will be sentenced to two years in prison, a $2,500 fine, or both. With two or more convictions of possession prior to this present offense, they can be sentenced to 90 days in prison along with a $5,000 fine.\n\nMarijuana is the most popular illegal drug worldwide. The punishment for possession of it is less than for the possession of cocaine or heroin. In some U.S. states, the drug is legal. Over 80 million Americans have tried marijuana. The \"Criminal Defense Lawyer\" article claims that, depending on the age of person and how much the person has been caught for possession, they will be fined and could plea bargain into going to a treatment program versus going to prison. In each state the convictions differ along with how much marijuana they have on their person.\n\nSome scholars have claimed that the phrase \"War on Drugs\" is propaganda cloaking an extension of earlier military or paramilitary operations. Others have argued that large amounts of \"drug war\" foreign aid money, training, and equipment actually goes to fighting leftist insurgencies and is often provided to groups who themselves are involved in large-scale narco-trafficking, such as corrupt members of the Colombian military.\n\nFrom 1963 to the end of the Vietnam War in 1975, marijuana usage became common among U.S. soldiers in non-combat situations. Some servicemen also used heroin. Many of the servicemen ended the heroin use after returning to the United States but came home addicted. In 1971, the U.S. military conducted a study of drug use among American servicemen and women. It found that daily usage rates for drugs on a worldwide basis were as low as two percent. However, in the spring of 1971, two congressmen released an alarming report alleging that 15% of the servicemen in Vietnam were addicted to heroin. Marijuana use was also common in Vietnam. Soldiers who used drugs had more disciplinary problems. The frequent drug use had become an issue for the commanders in Vietnam; in 1971 it was estimated that 30,000 servicemen were addicted to drugs, most of them to heroin.\n\nFrom 1971 on, therefore, returning servicemen were required to take a mandatory heroin test. Servicemen who tested positive upon returning from Vietnam were not allowed to return home until they had passed the test with a negative result. The program also offered a treatment for heroin addicts.\n\nElliot Borin's article \"The U.S. Military Needs its Speed\"—published in \"Wired\" on February 10, 2003—reports:\n\nOne of the first anti-drug efforts in the realm of foreign policy was President Nixon's Operation Intercept, announced in September 1969, targeted at reducing the amount of cannabis entering the United States from Mexico. The effort began with an intense inspection crackdown that resulted in an almost shutdown of cross-border traffic. Because the burden on border crossings was controversial in border states, the effort only lasted twenty days.\n\nOn December 20, 1989, the United States invaded Panama as part of Operation Just Cause, which involved 25,000 American troops. Gen. Manuel Noriega, head of the government of Panama, had been giving military assistance to Contra groups in Nicaragua at the request of the U.S. which, in exchange, tolerated his drug trafficking activities, which they had known about since the 1960s. When the Drug Enforcement Administration (DEA) tried to indict Noriega in 1971, the CIA prevented them from doing so. The CIA, which was then directed by future president George H. W. Bush, provided Noriega with hundreds of thousands of dollars per year as payment for his work in Latin America. When CIA pilot Eugene Hasenfus was shot down over Nicaragua by the Sandinistas, documents aboard the plane revealed many of the CIA's activities in Latin America, and the CIA's connections with Noriega became a public relations \"liability\" for the U.S. government, which finally allowed the DEA to indict him for drug trafficking, after decades of tolerating his drug operations. Operation Just Cause, whose purpose was to capture Noriega and overthrow his government; Noriega found temporary asylum in the Papal Nuncio, and surrendered to U.S. soldiers on January 3, 1990. He was sentenced by a court in Miami to 45 years in prison.\n\nAs part of its Plan Colombia program, the United States government currently provides hundreds of millions of dollars per year of military aid, training, and equipment to Colombia, to fight left-wing guerrillas such as the Revolutionary Armed Forces of Colombia (FARC-EP), which has been accused of being involved in drug trafficking.\n\nPrivate U.S. corporations have signed contracts to carry out anti-drug activities as part of Plan Colombia. DynCorp, the largest private company involved, was among those contracted by the State Department, while others signed contracts with the Defense Department.\n\nColombian military personnel have received extensive counterinsurgency training from U.S. military and law enforcement agencies, including the School of Americas (SOA). Author Grace Livingstone has stated that more Colombian SOA graduates have been implicated in human rights abuses than currently known SOA graduates from any other country. All of the commanders of the brigades highlighted in a 2001 Human Rights Watch report on Colombia were graduates of the SOA, including the III brigade in Valle del Cauca, where the 2001 Alto Naya Massacre occurred. US-trained officers have been accused of being directly or indirectly involved in many atrocities during the 1990s, including the Massacre of Trujillo and the 1997 Mapiripán Massacre.\n\nIn 2000, the Clinton administration initially waived all but one of the human rights conditions attached to Plan Colombia, considering such aid as crucial to national security at the time.\n\nThe efforts of U.S. and Colombian governments have been criticized for focusing on fighting leftist guerrillas in southern regions without applying enough pressure on right-wing paramilitaries and continuing drug smuggling operations in the north of the country. Human Rights Watch, congressional committees and other entities have documented the existence of connections between members of the Colombian military and the AUC, which the U.S. government has listed as a terrorist group, and that Colombian military personnel have committed human rights abuses which would make them ineligible for U.S. aid under current laws.\n\nIn 2010, the Washington Office on Latin America concluded that both Plan Colombia and the Colombian government's security strategy \"came at a high cost in lives and resources, only did part of the job, are yielding diminishing returns and have left important institutions weaker.\"\n\nA 2014 report by the RAND Corporation, which was issued to analyze viable strategies for the Mexican drug war considering successes experienced in Columbia, noted:\n\nBetween 1999 and 2002, the United States gave Colombia $2.04 billion in aid, 81 percent of which was for military purposes, placing Colombia just below Israel and Egypt among the largest recipients of U.S. military assistance. Colombia increased its defense spending from 3.2 percent of gross domestic product (GDP) in 2000 to 4.19 percent in 2005. Overall, the results were extremely positive. Greater spending on infrastructure and social programs helped the Colombian government increase its political legitimacy, while improved security forces were better able to consolidate control over large swaths of the country previously overrun by insurgents and drug cartels.\nIt also notes that, \"Plan Colombia has been widely hailed as a success, and some analysts believe that, by 2010, Colombian security forces had finally gained the upper hand once and for all.\"\n\nThe Mérida Initiative is a security cooperation between the United States and the government of Mexico and the countries of Central America. It was approved on June 30, 2008, and its stated aim is combating the threats of drug trafficking and transnational crime. The Mérida Initiative appropriated $1.4 billion in a three-year commitment (2008–2010) to the Mexican government for military and law enforcement training and equipment, as well as technical advice and training to strengthen the national justice systems. The Mérida Initiative targeted many very important government officials, but it failed to address the thousands of Central Americans who had to flee their countries due to the danger they faced everyday because of the war on drugs. There is still not any type of plan that addresses these people. No weapons are included in the plan.\n\nThe United States regularly sponsors the spraying of large amounts of herbicides such as glyphosate over the jungles of Central and South America as part of its drug eradication programs. Environmental consequences resulting from aerial fumigation have been criticized as detrimental to some of the world's most fragile ecosystems; the same aerial fumigation practices are further credited with causing health problems in local populations.\n\nIn 2012, the U.S. sent DEA agents to Honduras to assist security forces in counternarcotics operations. Honduras has been a major stop for drug traffickers, who use small planes and landing strips hidden throughout the country to transport drugs. The U.S. government made agreements with several Latin American countries to share intelligence and resources to counter the drug trade. DEA agents, working with other U.S. agencies such as the State Department, the CBP, and Joint Task Force-Bravo, assisted Honduras troops in conducting raids on traffickers' sites of operation.\n\nThe War on Drugs has been a highly contentious issue since its inception. A poll on October 2, 2008, found that three in four Americans believed that the War On Drugs was failing.\n\nIn 2014, a Pew Research Center poll found more than six in ten Americans state that state governments moving away from mandatory prison terms for drug law violations is a good thing. while three out of ten Americans say these policy changes are a bad thing. This a substantial shift from the same poll questions since 2001. In 2014 a Pew Research Center poll found that 67 percent of Americans feel that a movement towards treatment for drugs like cocaine and heroin is better versus the 26 percent who feel that prosecution is the better route.\n\nIn 2018, Rasmussen Report poll found that less than 10 percent of Americans think that the War on Drugs is being won and that 75 percent found that Americans believe that America is not winning the War on Drugs.\n\nMexican citizens, unlike American citizens, support the current measures their government were taking against drug cartels in the War on Drugs. A Pew Research Center poll in 2010 found that 80 percent supported the current use of the army in the War on Drugs to combat drug traffickers with about 55 percent saying that they have been making progress in the war. A year later in 2011 a Pew Research Center poll uncovered that 71 percent of Mexicans find that \"illegal drugs are a very big problem in their country\". 77 percent of Mexicans also found that drug cartels and the violence associated with them are as well a big challenge for Mexico. The poll also found that the percentages believing that that illegal drugs and violence related to the cartel where Higher in the North with 87 percent for illegal drug use and 94 percent cartel related violence being a problem. This compared to the other locations: South, Mexico City and the greater area of Mexico City, and Central Mexico which are all about 18 percent or lower than the North on Illegal drug use being a problem for the country. These perspective areas are also lower than the North by 19 percent or more on the issue of drug cartel related violence being an issue for the country.\n\nIn 2013 a Pew Research Center poll found that 74 percent of Mexican citizens would support the training of their police and military, the poll also found that another 55 percent would support the supplying of weapons and financial aid. Though the poll indicates a support of U.S. aid, 59 percent were against troops on the ground by the U.S. military. Also in 2013 Pew Research Center found in a poll that 56 percent of Mexican citizens believe that the United States and Mexico are both to blame for drug violence in Mexico. In that same poll 20 percent believe that the United States is solely to blame and 17 percent believe that Mexico is solely to blame.\n\nAt a meeting in Guatemala in 2012, three former presidents from Guatemala, Mexico and Colombia said that the war on drugs had failed and that they would propose a discussion on alternatives, including decriminalization, at the Summit of the Americas in April of that year. Guatemalan President Otto Pérez Molina said that the war on drugs was exacting too high a price on the lives of Central Americans and that it was time to \"end the taboo on discussing decriminalization\". At the summit, the government of Colombia pushed for the most far-reaching change to drugs policy since the war on narcotics was declared by Nixon four decades prior, citing the catastrophic effects it had had in Colombia.\n\nSeveral critics have compared the wholesale incarceration of the dissenting minority of drug users to the wholesale incarceration of other minorities in history. Psychiatrist Thomas Szasz, for example, wrote in 1997 \"Over the past thirty years, we have replaced the medical-political persecution of illegal sex users ('perverts' and 'psychopaths') with the even more ferocious medical-political persecution of illegal drug users.\"\n\nPenalties for drug crimes among American youth almost always involve permanent or semi-permanent removal from opportunities for education, strip them of voting rights, and later involve creation of criminal records which make employment more difficult. Thus, some authors maintain that the War on Drugs has resulted in the creation of a permanent underclass of people who have few educational or job opportunities, often as a result of being punished for drug offenses which in turn have resulted from attempts to earn a living in spite of having no education or job opportunities.\n\nAccording to a 2008 study published by Harvard economist Jeffrey A. Miron, the annual savings on enforcement and incarceration costs from the legalization of drugs would amount to roughly $41.3 billion, with $25.7 billion being saved among the states and over $15.6 billion accrued for the federal government. Miron further estimated at least $46.7 billion in tax revenue based on rates comparable to those on tobacco and alcohol ($8.7 billion from marijuana, $32.6 billion from cocaine and heroin, remainder from other drugs).\n\nLow taxation in Central American countries has been credited with weakening the region's response in dealing with drug traffickers. Many cartels, especially Los Zetas have taken advantage of the limited resources of these nations. 2010 tax revenue in El Salvador, Guatemala, and Honduras, composed just 13.53% of GDP. As a comparison, in Chile and the U.S., taxes were 18.6% and 26.9% of GDP respectively. However, direct taxes on income are very hard to enforce and in some cases tax evasion is seen as a national pastime.\n\nThe status of coca and coca growers has become an intense political issue in several countries, including Colombia and particularly Bolivia, where the president, Evo Morales, a former coca growers' union leader, has promised to legalise the traditional cultivation and use of coca. Indeed, legalization efforts have yielded some successes under the Morales administration when combined with aggressive and targeted eradication efforts. The country saw a 12–13% decline in coca cultivation in 2011 under Morales, who has used coca growers' federations to ensure compliance with the law rather than providing a primary role for security forces.\n\nThe coca eradication policy has been criticised for its negative impact on the livelihood of coca growers in South America. In many areas of South America the coca leaf has traditionally been chewed and used in tea and for religious, medicinal and nutritional purposes by locals. For this reason many insist that the illegality of traditional coca cultivation is unjust. In many areas the U.S. government and military has forced the eradication of coca without providing for any meaningful alternative crop for farmers, and has additionally destroyed many of their food or market crops, leaving them starving and destitute.\n\nThe CIA, DEA, State Department, and several other U.S. government agencies have been alleged to have relations with various groups which are involved in drug trafficking.\n\nSenator John Kerry's 1988 U.S. Senate Committee on Foreign Relations report on Contra drug links concludes that members of the U.S. State Department \"who provided support for the Contras are involved in drug trafficking... and elements of the Contras themselves knowingly receive financial and material assistance from drug traffickers.\" The report further states that \"the Contra drug links include... payments to drug traffickers by the U.S. State Department of funds authorized by the Congress for humanitarian assistance to the Contras, in some cases after the traffickers had been indicted by federal law enforcement agencies on drug charges, in others while traffickers were under active investigation by these same agencies.\"\n\nIn 1996, journalist Gary Webb published reports in the \"San Jose Mercury News\", and later in his book \"Dark Alliance\", detailing how Contras, had been involved in distributing crack cocaine into Los Angeles whilst receiving money from the CIA. Contras used money from drug trafficking to buy weapons.\n\nWebb's premise regarding the U.S. Government connection was initially attacked at the time by the media. It is now widely accepted that Webb's main assertion of government \"knowledge of drug operations, and collaboration with and protection of known drug traffickers\" was correct. In 1998, CIA Inspector General Frederick Hitz published a two-volume report that while seemingly refuting Webb's claims of knowledge and collaboration in its conclusions did not deny them in its body. Hitz went on to admit CIA improprieties in the affair in testimony to a House congressional committee. There has been a reversal amongst mainstream media of its position on Webb's work, with acknowledgement made of his contribution to exposing a scandal it had ignored.\n\nAccording to Rodney Campbell, an editorial assistant to Nelson Rockefeller, during World War II, the United States Navy, concerned that strikes and labor disputes in U.S. eastern shipping ports would disrupt wartime logistics, released the mobster Lucky Luciano from prison, and collaborated with him to help the mafia take control of those ports. Labor union members were terrorized and murdered by mafia members as a means of preventing labor unrest and ensuring smooth shipping of supplies to Europe.\n\nAccording to Alexander Cockburn and Jeffrey St. Clair, in order to prevent Communist party members from being elected in Italy following World War II, the CIA worked closely with the Sicilian Mafia, protecting them and assisting in their worldwide heroin smuggling operations. The mafia was in conflict with leftist groups and was involved in assassinating, torturing, and beating leftist political organizers.\n\nIn 1986, the US Defense Department funded a two-year study by the RAND Corporation, which found that the use of the armed forces to interdict drugs coming into the United States would have little or no effect on cocaine traffic and might, in fact, raise the profits of cocaine cartels and manufacturers. The 175-page study, \"Sealing the Borders: The Effects of Increased Military Participation in Drug Interdiction\", was prepared by seven researchers, mathematicians and economists at the National Defense Research Institute, a branch of the RAND, and was released in 1988. The study noted that seven prior studies in the past nine years, including one by the Center for Naval Research and the Office of Technology Assessment, had come to similar conclusions. Interdiction efforts, using current armed forces resources, would have almost no effect on cocaine importation into the United States, the report concluded.\n\nDuring the early-to-mid-1990s, the Clinton administration ordered and funded a major cocaine policy study, again by RAND. The Rand Drug Policy Research Center study concluded that $3 billion should be switched from federal and local law enforcement to treatment. The report said that treatment is the cheapest way to cut drug use, stating that drug treatment is twenty-three times more effective than the supply-side \"war on drugs\".\n\nThe National Research Council Committee on Data and Research for Policy on Illegal Drugs published its findings in 2001 on the efficacy of the drug war. The NRC Committee found that existing studies on efforts to address drug usage and smuggling, from U.S. military operations to eradicate coca fields in Colombia, to domestic drug treatment centers, have all been inconclusive, if the programs have been evaluated at all: \"The existing drug-use monitoring systems are strikingly inadequate to support the full range of policy decisions that the nation must make... It is unconscionable for this country to continue to carry out a public policy of this magnitude and cost without any way of knowing whether and to what extent it is having the desired effect.\" The study, though not ignored by the press, was ignored by top-level policymakers, leading Committee Chair Charles Manski to conclude, as one observer notes, that \"the drug war has no interest in its own results\".\n\nIn mid-1995, the US government tried to reduce the supply of methamphetamine precursors to disrupt the market of this drug. According to a 2009 study, this effort was successful, but its effects were largely temporary.\n\nDuring alcohol prohibition, the period from 1920 to 1933, alcohol use initially fell but began to increase as early as 1922. It has been extrapolated that even if prohibition had not been repealed in 1933, alcohol consumption would have quickly surpassed pre-prohibition levels. One argument against the War on Drugs is that it uses similar measures as Prohibition and is no more effective.\n\nIn the six years from 2000 to 2006, the U.S. spent $4.7 billion on Plan Colombia, an effort to eradicate coca production in Colombia. The main result of this effort was to shift coca production into more remote areas and force other forms of adaptation. The overall acreage cultivated for coca in Colombia at the end of the six years was found to be the same, after the U.S. Drug Czar's office announced a change in measuring methodology in 2005 and included new areas in its surveys. Cultivation in the neighboring countries of Peru and Bolivia increased, some would describe this effect like squeezing a balloon.\n\nRichard Davenport-Hines, in his book \"The Pursuit of Oblivion\", criticized the efficacy of the War on Drugs by pointing out that\n\nAlberto Fujimori, president of Peru from 1990 to 2000, described U.S. foreign drug policy as \"failed\" on grounds that \n\nfor 10 years, there has been a considerable sum invested by the Peruvian government and another sum on the part of the American government, and this has not led to a reduction in the supply of coca leaf offered for sale. Rather, in the 10 years from 1980 to 1990, it grew 10-fold.\n\nAt least 500 economists, including Nobel Laureates Milton Friedman, George Akerlof and Vernon L. Smith, have noted that reducing the supply of marijuana without reducing the demand causes the price, and hence the profits of marijuana sellers, to go up, according to the laws of supply and demand. The increased profits encourage the producers to produce more drugs despite the risks, providing a theoretical explanation for why attacks on drug supply have failed to have any lasting effect. The aforementioned economists published an open letter to President George W. Bush stating \"We urge...the country to commence an open and honest debate about marijuana prohibition... At a minimum, this debate will force advocates of current policy to show that prohibition has benefits sufficient to justify the cost to taxpayers, foregone tax revenues and numerous ancillary consequences that result from marijuana prohibition.\"\n\nThe declaration from the World Forum Against Drugs, 2008 state that a balanced policy of drug abuse prevention, education,\ntreatment, law enforcement, research, and supply reduction provides the most effective platform to reduce drug abuse and its associated harms and call on governments to consider demand reduction as one of their first priorities in the fight against drug abuse.\n\nDespite over $7 billion spent annually towards arresting and prosecuting nearly 800,000 people across the country for marijuana offenses in 2005 (FBI Uniform Crime Reports), the federally funded Monitoring the Future Survey reports about 85% of high school seniors find marijuana \"easy to obtain\". That figure has remained virtually unchanged since 1975, never dropping below 82.7% in three decades of national surveys. The Drug Enforcement Administration states that the number of users of marijuana in the U.S. declined between 2000 and 2005 even with many states passing new medical marijuana laws making access easier, though usage rates remain higher than they were in the 1990s according to the National Survey on Drug Use and Health.\n\nONDCP stated in April 2011 that there has been a 46 percent drop in cocaine use among young adults over the past five years, and a 65 percent drop in the rate of people testing positive for cocaine in the workplace since 2006. At the same time, a 2007 study found that up to 35% of college undergraduates used stimulants not prescribed to them.\n\nA 2013 study found that prices of heroin, cocaine and cannabis had decreased from 1990 to 2007, but the purity of these drugs had increased during the same time.\n\nThe War on Drugs is often called a policy failure.\n\nThe legality of the War on Drugs has been challenged on four main grounds in the U.S.\n\nSeveral authors believe that the United States' federal and state governments have chosen wrong methods for combatting the distribution of illicit substances. Aggressive, heavy-handed enforcement funnels individuals through courts and prisons; instead of treating the cause of the addiction, the focus of government efforts has been on punishment. By making drugs illegal rather than regulating them, the War on Drugs creates a highly profitable black market. Jefferson Fish has edited scholarly collections of articles offering a wide variety of public health based and rights based alternative drug policies.\n\nIn the year 2000, the United States drug-control budget reached 18.4 billion dollars, nearly half of which was spent financing law enforcement while only one sixth was spent on treatment. In the year 2003, 53 percent of the requested drug control budget was for enforcement, 29 percent for treatment, and 18 percent for prevention. The state of New York, in particular, designated 17 percent of its budget towards substance-abuse-related spending. Of that, a mere one percent was put towards prevention, treatment, and research.\n\nIn a survey taken by Substance Abuse and Mental Health Services Administration (SAMHSA), it was found that substance abusers that remain in treatment longer are less likely to resume their former drug habits. Of the people that were studied, 66 percent were cocaine users. After experiencing long-term in-patient treatment, only 22 percent returned to the use of cocaine. Treatment had reduced the number of cocaine abusers by two-thirds. By spending the majority of its money on law enforcement, the federal government had underestimated the true value of drug-treatment facilities and their benefit towards reducing the number of addicts in the U.S.\n\nIn 2004 the federal government issued the National Drug Control Strategy. It supported programs designed to expand treatment options, enhance treatment delivery, and improve treatment outcomes. For example, the Strategy provided SAMHSA with a $100.6 million grant to put towards their Access to Recovery (ATR) initiative. ATR is a program that provides vouchers to addicts to provide them with the means to acquire clinical treatment or recovery support. The project's goals are to expand capacity, support client choice, and increase the array of faith-based and community based providers for clinical treatment and recovery support services. The ATR program will also provide a more flexible array of services based on the individual's treatment needs.\n\nThe 2004 Strategy additionally declared a significant 32 million dollar raise in the Drug Courts Program, which provides drug offenders with alternatives to incarceration. As a substitute for imprisonment, drug courts identify substance-abusing offenders and place them under strict court monitoring and community supervision, as well as provide them with long-term treatment services. According to a report issued by the National Drug Court Institute, drug courts have a wide array of benefits, with only 16.4 percent of the nation's drug court graduates rearrested and charged with a felony within one year of completing the program (versus the 44.1% of released prisoners who end up back in prison within 1-year). Additionally, enrolling an addict in a drug court program costs much less than incarcerating one in prison. According to the Bureau of Prisons, the fee to cover the average cost of incarceration for Federal inmates in 2006 was $24,440. The annual cost of receiving treatment in a drug court program ranges from $900 to $3,500. Drug courts in New York State alone saved $2.54 million in incarceration costs.\n\nDescribing the failure of the War on Drugs, \"New York Times\" columnist Eduardo Porter noted:\n\nMany believe that the War on Drugs has been costly and ineffective largely because inadequate emphasis is placed on treatment of addiction. The United States leads the world in both recreational drug usage and incarceration rates. 70% of men arrested in metropolitan areas test positive for an illicit substance, and 54% of all men incarcerated will be repeat offenders.\n\n\nCovert activities and foreign policy\n\n\"Government agencies and laws\"\n\n\"Organizations opposing prohibition\"\n\n\"Organizations opposing drug legalization\"\n\n\n\n"}
