{"id": "48401710", "url": "https://en.wikipedia.org/wiki?curid=48401710", "title": "Airstrikes on hospitals in Yemen", "text": "Airstrikes on hospitals in Yemen\n\nA Saudi Arabian-led military intervention in Yemen began in 2015, in an attempt to influence the outcome of the Yemeni Civil War. Saudi Arabia, spearheading a coalition of nine Arab states, began carrying out airstrikes in neighbouring Yemen and imposing an aerial and naval blockade on 26 March 2015, heralding a military intervention code-named Operation Decisive Storm (). More than 70 health facilities in Yemen have been destroyed by a series of airstrikes conducted by the Saudi Arabian-led coalition since March 2015. Many of these have been public health hospitals staffed or supported by Doctors Without Borders (MSF). Critics of the assaults say the airstrikes are war crimes in violation of the protections of health care facilities afforded by the internationally recognized rules of war and have called for independent investigations.\n\nMany other civilians targets, including schools,and school buses in Yemen are also bombed by the Saudi-led coalition.\n\n1,500 schools were damaged and destroyed during Yemeni Civil War.\n\nThe UN accused the Saudi-led coalition of \"complete disregard for human life\".\n\nDoctors Without Borders reported that a Saudi Arabian-led coalition airstrike on 26 October 2015 had completely destroyed the Médecins Sans Frontières hospital in Saada, in northwestern Yemen, including the operating room. The first strike hit an unused part of the hospital, so the facility was completely evacuated at once. There were no direct casualties. The spokesman for the coalition forces, Brig-Gen Ahmed al-Asiri, disclaimed responsibility for the attack.\n\n\"With the hospital destroyed, at least 200,000 people now have no access to lifesaving medical care\", MSF said. \"This attack is another illustration of a complete disregard for civilians in Yemen, where bombings have become a daily routine,\" said Hassan Boucenine, MSF head of mission in Yemen. The GPS coordinates of the only hospital in the Haydan district were regularly shared with the Saudi-led coalition, and the roof of the facility was clearly identified with the MSF logo, he said. Abdallah al-Mouallimi, the Saudi ambassador to the United Nations, said the coordinates were inaccurate, although he admitted that the airstrike was \"a mistake\".\n\nThe UNICEF said the hospital in Saada was the 39th health center hit in Yemen since March, when the violence escalated. MSF reports that the Saudi-led coalition, supported by the British military, has been bombing hospitals across Yemen for the past 10 months. As many as 130 health facilities have been hit. \"More children in Yemen may well die from a lack of medicines and healthcare than from bullets and bombs,\" its executive director Anthony Lake said in a statement. He added that critical shortages of fuel, medication, electricity and water could mean many more will close. Amnesty International said the strike may amount to a war crime and called for an independent investigation.\n\nOn December 3, 2015 an airstrike by the Saudi Arabian-led intervention in Yemen hit a health center in Taiz, wounding nine people. Two hospital staff were among the wounded. \"The bombing of civilians and hospitals is a violation of international humanitarian law,\" said Jerome Alin, head of MSF head of mission in Yemen.\nOn January 10, 2016, Shiara Hospital, supported by MSF in Razeh district, Saada Governorate, Northern Yemen, was hit by a projectile and shrapnel from the Saudi-led coalition. Six people died and another 7 were injured, including three MSF staff, two of them in critical condition. Several buildings at the medical facility collapsed after the attack, although the critical areas of the hospital were not destroyed. The rocket hit a corridor leading from the main gate to the hospital buildings, with a metal fence alongside. The wounded were hit by shrapnel from the missile, and also by shards of metal from the fence. The injuries were brutal. Vickie Hawkins, Executive Director of MSF-UK, said, \"... there is a risk that \"errors\" in war situations will become normalised—just as \"collateral damage\" has been normalised in people’s minds since the first Gulf War. This would provide the perfect alibi for armies to shrug off accusations of war crimes and crimes against humanity. It perpetuates impunity.\n\nIn a separate attack by the Saudi-led coalition, an airstrike was reported to have hit a center for the blind in the capital Sana'a, resulting in multiple injuries. \n\nOn August 15, 2016, after the collapse of a UN-sponsored cease-fire, an airstrike by the Saudi Arabian-led intervention in Yemen destroyed a hospital operated by Yemen's Ministry of Health and supported by MSF and UNICEF in Abs District, Hajjah Governorate in northwestern Yemen. The bombardment struck the hospital's triage area near the emergency room and killed at least 19 and wounded 24 people. At the time of the attack, there were 23 patients in the surgery ward, 25 in the maternity ward, 13 newborns and 12 patients in the pediatric ward, MSF said. The hospital had a 14-bed emergency room, a maternity unit and a surgical unit. Hospital staff were among the dead and wounded. “There were no armed people there,” a witness said. The hospital was reportedly treating child victims of another airstrike on a school in the town of Haydan, in neighboring Saada province, in which 10 children died and another 30 were wounded, all between the ages of 8 and 15 years. MSF has now withdrawn its staff members from Haydan, Razeh, Al Gamouri and Yasnim hospitals in Saada governorate and Abs and Al Gamouri hospitals in Hajjah governorate. Ban Ki-moon, the United Nations secretary general, condemned the attack in a statement, emphasizing that antagonists in the Yemen conflict had damaged or destroyed more than 70 health facilities since the hostilities began 17 months ago.\n\nDoctors Without Borders reported that a Saudi Arabian coalition airstrike struck a new Médecins Sans Frontières cholera treatment center in Abs, in northwestern Yemen. Doctors Without Borders reported that they had provided GPS coordinates to The Kingdom of Saudi Arabia on twelve separate occasions, and had received nine written responses confirming receipt of those coordinates \n\nOn 2 August 2018, airstrikes on hospital, harbor and fish market in Al Hudaydah killed at least 55 people and wounded 124.\n\n"}
{"id": "7766835", "url": "https://en.wikipedia.org/wiki?curid=7766835", "title": "Anamorphism", "text": "Anamorphism\n\nIn category theory, the anamorphism (from the Greek \"upwards\" and \"form, shape\") of a coinductive type denotes the assignment of a coalgebra to its unique morphism to the final coalgebra of an endofunctor. These objects are used in functional programming as \"unfolds\". The categorical dual of the anamorphism is the catamorphism.\n\nIn functional programming, an anamorphism is a generalization of the concept of \"unfolds\" on coinductive lists. Formally, anamorphisms are generic functions that can corecursively construct a result of a certain type and which is parameterized by functions that determine the next single step of the construction.\n\nThe data type in question is defined as the greatest fixed point \"ν X . F X\" of a functor \"F\". By the universal property of final coalgebras, there is a unique coalgebra morphism \"A → ν X . F X\" for any other \"F\"-coalgebra \"a : A → F A\". Thus, one can define functions from a type \"A\" _into_ a coinductive datatype by specifying a coalgebra structure \"a\" on \"A\".\n\nAs an example, the type of potentially infinite lists (with elements of a fixed type \"value\") is given as the fixpoint \"[value] = ν X . value × X + 1\", i.e. a list consists either of a \"value\" and a further list, or it is empty. A (pseudo-)Haskell-Definition might look like this:\n\nIt is the fixed point of the functor codice_1, where:\n\nOne can easily check that indeed the type codice_2 is isomorphic to codice_3, and thus codice_2 is the fixed point.\n\nThe \"anamorphism\" for lists (then usually known as \"unfold\") would build a (potentially infinite) list from a state value. Typically, the unfold takes a state value codice_5 and a function codice_6 that yields either a pair of a value and a new state, or a singleton to mark the end of the list. The anamorphism would then begin with a first seed, compute whether the list continues or ends, and in case of a nonempty list, prepend the computed value to the recursive call to the anamorphism.\n\nA Haskell definition of an unfold, or anamorphism for lists, called codice_7, is as follows:\n\nWe can now implement quite general functions using \"ana\", for example a countdown:\n\nThis function will decrement an integer and output it at the same time, until it is negative, at which point it will mark the end of the list. Correspondingly, codice_8 will compute the list codice_9.\n\nAn anamorphism can be defined for any recursive type, according to a generic pattern, generalizing the second version of \"ana\" for lists.\n\nFor example, the unfold for the tree data structure\n\nis as follows\nTo better see the relationship between the recursive type and its anamorphism, note that codice_10 and codice_11 can be defined thus:\n\nThe analogy with codice_7 appears by renaming codice_13 in its type:\n\nWith these definitions, the argument to the constructor of the type has the same type as the return type of the first argument of codice_7, with the recursive mentions of the type replaced with codice_13.\n\nOne of the first publications to introduce the notion of an anamorphism in the context of programming was the paper \"Functional Programming with Bananas, Lenses, Envelopes and Barbed Wire\", by Erik Meijer \"et al.\", which was in the context of the Squiggol programming language.\n\nFunctions like codice_16 and codice_17 are examples of anamorphisms. codice_16 takes a pair of lists, say ['a','b','c'] and [1,2,3] and returns a list of pairs [('a',1),('b',2),('c',3)]. codice_19 takes a thing, x, and a function, f, from such things to such things, and returns the infinite list that comes from repeated application of f, i.e. the list [x, (f x), (f (f x)), (f (f (f x))), ...].\n\nTo prove this, we can implement both using our generic unfold, codice_7, using a simple recursive routine:\n\nIn a language like Haskell, even the abstract functions codice_21, codice_22 and codice_7 are merely defined terms, as we have seen from the definitions given above.\n\nIn category theory, anamorphisms are the categorical dual of catamorphisms (and catamorphisms are the categorical dual of anamorphisms).\n\nThat means the following. \nSuppose (\"A\", \"fin\") is a final \"F\"-coalgebra for some endofunctor \"F\" of some category into itself.\nThus, \"fin\" is a morphism from \"A\" to \"FA\", and since it is assumed to be final we know that whenever (\"X\", \"f\") is another \"F\"-coalgebra (a morphism \"f\" from \"X\" to \"FX\"), there will be a unique homomorphism \"h\" from (\"X\", \"f\") to (\"A\", \"fin\"), that is a morphism \"h\" from \"X\" to \"A\" such that \"fin . h = Fh . f\".\nThen for each such \"f\" we denote by ana f that uniquely specified morphism \"h\".\n\nIn other words, we have the following defining relationship, given some fixed \"F\", \"A\", and \"fin\" as above:\n\n\nA notation for ana \"f\" found in the literature is formula_3. The brackets used are known as lens brackets, after which anamorphisms are sometimes referred to as \"lenses\".\n\n\n"}
{"id": "48445299", "url": "https://en.wikipedia.org/wiki?curid=48445299", "title": "Anti-nesting principle", "text": "Anti-nesting principle\n\nIn the philosophy of consciousness, the anti-nesting principle states that one state of consciousness cannot exist within another.\n\nProponents of the anti-nesting principle include Giulio Tononi and Hilary Putnam.\n"}
{"id": "3017505", "url": "https://en.wikipedia.org/wiki?curid=3017505", "title": "Aristotelian ethics", "text": "Aristotelian ethics\n\nAristotle first used the term ethics to name a field of study developed by his predecessors Socrates and Plato. Philosophical ethics is the attempt to offer a rational response to the question of how humans should best live. Aristotle regarded ethics and politics as two related but separate fields of study, since ethics examines the good of the individual, while politics examines the good of the city-state.\n\nAristotle's writings have been read more or less continuously since ancient times, and his ethical treatises in particular continue to influence philosophers working today. Aristotle emphasized the importance of developing excellence (virtue) of character (Greek \"ēthikē aretē\"), as the way to achieve what is finally more important, excellent conduct (Greek \"energeia\"). As Aristotle argues in Book II of the \"Nicomachean Ethics\", the man who possesses character excellence does the right thing, at the right time, and in the right way. Bravery, and the correct regulation of one's bodily appetites, are examples of character excellence or virtue. So acting bravely and acting temperately are examples of excellent activities. The highest aims are living well and \"eudaimonia\" a Greek word often translated as well-being, happiness or \"human flourishing\". Like many ethicists, Aristotle regards excellent activity as pleasurable for the man of virtue. For example, Aristotle thinks that the man whose appetites are in the correct order actually takes pleasure in acting moderately.\n\nAristotle emphasized that virtue is practical, and that the purpose of ethics is to become good, not merely to know. Aristotle also claims that the right course of action depends upon the details of a particular situation, rather than being generated merely by applying a law. The type of wisdom which is required for this is called \"prudence\" or \"practical wisdom\" (Greek \"phronesis\"), as opposed to the wisdom of a theoretical philosopher (Greek \"sophia\"). But despite the importance of practical decision making, in the final analysis the original Aristotelian and Socratic answer to the question of how best to live, at least for the best types of human, was to live the life of philosophy.'\n\nThree Aristotelian ethical works survive today which are considered to be either by Aristotle, or from relatively soon after:\n\nThe exact origins of these texts is unclear, although they were already considered the works of Aristotle in ancient times. Textual oddities suggest that they may not have been put in their current form by Aristotle himself. For example, Books IV–VI of \"Eudemian Ethics\" also appear as Books V–VII of \"Nicomachean Ethics\". The authenticity of the \"Magna Moralia\" has been doubted, whereas almost no modern scholar doubts that Aristotle wrote the \"Nicomachean Ethics\" and the \"Eudemian Ethics\" himself, even if an editor also played some part in giving us those texts in their current forms.\n\nThe \"Nicomachean Ethics\" has received the most scholarly attention, and is the most easily available to modern readers in many different translations and editions. Some critics consider the \"Eudemian Ethics\" to be \"less mature,\" while others, such as Kenny (1978), contend that the \"Eudemian Ethics\" is the more mature, and therefore later, work.\n\nTraditionally it was believed that the \"Nicomachean Ethics\" and the \"Eudemian Ethics\" were either edited by or dedicated to Aristotle's son and pupil Nicomachus and his disciple Eudemus, respectively, although the works themselves do not explain the source of their names. Although Aristotle's father was also called Nicomachus, Aristotle's son was the next leader of Aristotle's school, the Lyceum, and in ancient times he was already associated with this work.\n\nA fourth treatise, Aristotle's \"Politics\", is often regarded as the sequel to the Ethics, in part because Aristotle closes the \"Nicomachean Ethics\" by saying that his ethical inquiry has laid the groundwork for an inquiry into political questions (\"NE\" X.1181b6-23). Aristotle's Ethics also states that the good of the individual is subordinate to the good of the city-state, or \"polis\".\n\nFragments also survive from Aristotle's \"Protrepticus\", another work which dealt with ethics.\n\nSome scholars regarded Aristotle as a Socratic thinker. Aristotle's ethics builds upon earlier Greek thought, particularly that of his teacher Plato and Plato's teacher, Socrates. While Socrates left no written works, and Plato wrote dialogues and a few letters, Aristotle wrote treatises in which he sets forth philosophical doctrines directly. To be more precise, Aristotle did write dialogues, but they unfortunately survive only in fragments.\n\nAccording to Aristotle in his \"Metaphysics\", Socrates was the first Greek philosopher to concentrate on ethics, although he apparently did not give it this name, as a philosophical inquiry concerning how people should best live. Aristotle dealt with this same question but giving it two names, \"the political\" (or Politics) and \"the ethical\" (Ethics), both with Politics being the name for the two together as the more important part. The original Socratic questioning on ethics started at least partly as a response to sophism, which was a popular style of education and speech at the time. Sophism emphasized rhetoric, and argument, and therefore often involved criticism of traditional Greek religion and flirtation with moral relativism.\n\nAristotle's ethics, or study of character, is built around the premise that people should achieve an excellent character (a virtuous character, \"ethikē aretē\" in Greek) as a pre-condition for attaining happiness or well-being (\"eudaimonia\"). It is sometimes referred to in comparison to later ethical theories as a \"character based ethics\". Like Plato and Socrates he emphasized the importance of reason for human happiness, and that there were logical and natural reasons for humans to behave virtuously, and try to become virtuous.\n\nAristotle's treatment of the subject is distinct in several ways from that found in Plato's Socratic dialogues.\n\n\nAristotle believed that ethical knowledge is not only a theoretical knowledge, but rather that a person must have \"experience of the actions in life\" and have been \"brought up in fine habits\" to become good (NE 1095a3 and b5). For a person to become virtuous, he can't simply study what virtue \"is\", but must actually do virtuous things.\n\nThe Aristotelian Ethics all aim to begin with approximate but uncontroversial starting points. In the \"Nicomachean Ethics\" Aristotle says explicitly that one must begin with what is familiar to us, and \"the that\" or \"the fact that\" (\"NE\" I.1095b2-13). Ancient commentators agree that what Aristotle means here is that his treatise must rely upon practical, everyday knowledge of virtuous actions as the starting points of his inquiry, and that he is supposing that his readers have some kind of experience-based understanding of such actions, and that they value noble and just actions to at least some degree.\n\nElsewhere, Aristotle also seems to rely upon common conceptions of how the world works. In fact, some regard his ethical inquiries as using a method that relies upon popular opinion (his so-called \"endoxic method\" from the Grk. \"endoxa\"). There is some dispute, however, about exactly how such common conceptions fit into Aristotle's method in his ethical treatises, particularly since he also makes use of more formal arguments, especially the so-called \"function argument,\" which is described below.\n\nAristotle describes popular accounts about what kind of life would be a happy one by classifying them into three most common types: a life dedicated to vulgar pleasure; a life dedicated to fame and honor; and a life dedicated to contemplation (\"NE\" I.1095b17-19). To reach his own conclusion about the best life, however, Aristotle tries to isolate the function of humans. The argument he develops here is accordingly widely known as \"the function argument,\" and is among the most-discussed arguments made by any ancient philosopher. He argues that while humans undergo nutrition and growth, so do other living things, and while humans are capable of perception, this is shared with animals (\"NE\" I.1098b22-1098a15). Thus neither of these characteristics is particular to humans. According to Aristotle, what remains and what is distinctively human is reason. Thus he concludes that the human function is some kind of excellent exercise of the intellect. And, since Aristotle thinks that practical wisdom rules over the character excellences, exercising such excellences is one way to exercise reason and thus fulfill the human function.\n\nOne common objection to Aristotle's function argument is that it uses descriptive or factual premises to derive conclusions about what is good. Such arguments are often thought to run afoul of the is-ought gap.\n\nMoral virtue, or excellence of character, is the disposition (Grk \"hexis\") to act excellently, which a person develops partly as a result of his upbringing, and partly as a result of his habit of action. Aristotle develops his analysis of character in Book II of the \"Nicomachean Ethics\", where he makes this argument that character arises from habit—likening ethical character to a skill that is acquired through practice, such as learning a musical instrument. In Book III of the \"Nicomachean Ethics\", Aristotle argues that a person's character is voluntary, since it results from many individual actions which are under his voluntary control.\n\nAristotle distinguishes the disposition to feel emotions of a certain kind from virtue and vice. But such emotional dispositions may also lie at a mean between two extremes, and these are also to some extent a result of up-bringing and habituation. Two examples of such dispositions would be modesty, or a tendency to feel shame, which Aristotle discusses in NE IV.9; and righteous indignation (\"nemesis\"), which is a balanced feeling of sympathetic pain concerning the undeserved pleasures and pains of others. Exactly which habitual dispositions are virtues or vices and which only concern emotions, differs between the different works which have survived, but the basic examples are consistent, as is the basis for distinguishing them in principle.\n\nSome people, despite intending to do the right thing, cannot act according to their own choice. For example, someone may choose to refrain from eating chocolate cake, but finds himself eating the cake contrary to his own choice. Such a failure to act in a way that is consistent with one's own decision is called \"akrasia\", and may be translated as weakness of will, incontinence, or lack of self-mastery.\n\nVices of courage must also be identified which are cowardice and recklessness. Soldiers who are not prudent act with cowardice, and soldiers who do not have temperance act with recklessness. One should not be unjust toward their enemy no matter the circumstance. On another note, one becomes virtuous by first imitating another who exemplifies such virtuous characteristics, practicing such ways in their daily lives, turning those ways into customs and habits by performing them each and every day, and finally, connecting or uniting the four of them together.\n\nOnly soldiers can exemplify such virtues because war demands soldiers to exercise disciplined and firm virtues, but war does everything in its power to shatter the virtues it demands. Since virtues are very fragile, they must be practiced always, for if they are not practiced they will weaken and eventually disappear. One who is virtuous has to avoid the enemies of virtue which are indifference or persuasion that something should not be done, self-indulgence or persuasion that something can wait and does not need to be done at that moment, and despair or persuasion that something simply cannot be accomplished anyway. In order for one to be virtuous they must display prudence, temperance, courage, and justice; moreover, they have to display all four of them and not just one or two to be virtuous.\n\nAristotle devotes Book V of the \"Nicomachean Ethics\" to justice (this is also Book IV of the \"Eudemian Ethics\"). In this discussion, Aristotle defines justice as having two different but related senses—general justice and particular justice. General justice is virtue expressed in relation to other people. Thus the just man in this sense deals properly and fairly with others, and expresses his virtue in his dealings with them—not lying or cheating or taking from others what is owed to them.\nParticular justice is the correct distribution of just deserts to others. For Aristotle, such justice is proportional—it has to do with people receiving what is proportional to their merit or their worth. In his discussion of particular justice, Aristotle says an educated judge is needed to apply just decisions regarding any particular case. This is where we get the image of the scales of justice, the blindfolded judge symbolizing blind justice, balancing the scales, weighing all the evidence and deliberating each particular case individually.\n\nIn his ethical works, Aristotle describes \"eudaimonia\" as the highest human good. In Book I of the \"Nicomachean Ethics\" he goes on to identify \"eudaimonia\" as the excellent exercise of the intellect, leaving it open whether he means practical activity or intellectual activity.\nWith respect to practical activity, in order to exercise any one of the practical excellences in the highest way, a person must possess all the others. Aristotle therefore describes several apparently different kinds of virtuous person as necessarily having all the moral virtues, excellences of character.\n\nAristotle also says, for example in NE Book VI, that such a complete virtue requires intellectual virtue, not only practical virtue, but also theoretical wisdom. Such a virtuous person, if they can come into being, will choose the most pleasant and happy life of all, which is the philosophical life of contemplation and speculation.\n\nAristotle claims that a human's highest functioning must include reasoning, being good at what sets humans apart from everything else. Or, as Aristotle explains it, \"The function of man is activity of soul in accordance with reason, or at least not without reason.\" He identifies two different ways in which the soul can engage: \"reasoning\" (both practical and theoretical) and \"following reasoning\". A person that does this is the happiest because they are fulfilling their purpose or nature as found in the rational soul.\n\nIn other words, the thinker is not only the 'best' person, but is also most like God.\n\nAristotle's writings were taught in the Academy in Athens until 529 CE when the Byzantine Emperor Justinian I closed down non-Christian schools of philosophy.\n\nAristotle's work however continued to be taught as a part of secular education. Aristotle's teachings spread through the Mediterranean and the Middle East, where some early Islamic regimes allowed rational philosophical descriptions of the natural world. Alfarabi was a major influence in all medieval philosophy and wrote many works which included attempts to reconcile the ethical and political writings of Plato and Aristotle. Later Avicenna, and later still Averroes, were Islamic philosophers who commented on Aristotle as well as writing their own philosophy in Arabic. Averroes, a European Muslim, was particularly influential in turn upon European Christian philosophers, theologians and political thinkers.\n\nIn the twelfth century, Latin translations of Aristotle's works were made, enabling the Dominican priest Albert the Great and his pupil Thomas Aquinas to synthesize Aristotle's philosophy with Christian theology. Later the medieval church scholasticism in Western Europe insisted on Thomist views and suppressed non-Aristotelian metaphysics. Aquinas' writings are full of references to Aristotle, and he wrote a commentary on Aristotle's \"Nicomachean Ethics\". Aquinas also departed from Aristotle in certain respects. In particular, his \"Summa Theologica\" argued that \"Eudaimonia\" or human flourishing was held to be a temporary goal for this life, but perfect happiness as the ultimate goal could only be attained in the next life by the virtuous. Aquinas also added new theological virtues to Aristotle's system: faith, hope and charity. And supernatural assistance could help people to achieve virtue. Nevertheless, much of Aristotle's ethical thought remained intact in Aquinas.\n\nIn modern times, Aristotle's writings on ethics remain among the most influential in his broad corpus, along with The Rhetoric, and The Poetics, while his scientific writings tend to be viewed as of more strictly historical interest. Modern science develops theories about the physical world based on experiments and careful observation—in particular, on the basis of exact measurements of time and distance. Aristotle, on the other hand, bases his science largely on qualitative and non-experimental observation. Accordingly, he made some inaccurate claims which have been overturned—such as the claim that objects of different mass accelerate at different rates due to gravity.\n\nOn the other hand, The \"Nicomachean Ethics\" continues to be relevant to philosophers today. In fact, virtue ethics takes its inspiration from Aristotle's approach to ethics—in particular, sharing his emphasis on character excellence, and ethical psychology. Some philosophers, in particular Bernard Williams, regard Aristotle's ethics as superior to the Utilitarian and Kantian traditions, which have come to be the dominant approaches to philosophical ethics. Aristotle's well-known function argument is less commonly accepted today, since he seems to use it in order to develop a claim about human perfection from an observation from what is distinctive about man. But the exact role of the function argument in Aristotle's ethical theory is itself a matter of dispute.\n\n\n"}
{"id": "39093307", "url": "https://en.wikipedia.org/wiki?curid=39093307", "title": "Chandy-Misra-Haas algorithm resource model", "text": "Chandy-Misra-Haas algorithm resource model\n\nThe Chandy-Misra-Haas algorithm resource model checks for deadlock in a distributed system. It was developed by K. Mani Chandy, Jayadev Misra and Laura M Haas.\n\nConsider the n processes \"P\", \"P\", \"P\", \"P\", \"P\", ... ,\"P\" which are performed in a single system(controller). \"P\" is locally dependent on \"P\", if \"P\" depends on \"P\", \"P\" on \"P\" so on and \"P\" on \"P\". That is, if formula_1, then formula_2 is locally dependent on formula_3. If \"P\" is said to be locally dependent to itself if it is locally dependent on \"P\" and \"P\" depends on \"P\": i.e. if formula_4, then formula_2 is locally dependent on itself.\n\nThe algorithm uses a message called probe(i,j,k) to transfer a message from controller of process \"P\" to controller of process \"P\". It specifies a message started by process \"P\" to find whether a deadlock has occurred or not. Every process \"P\" maintains a boolean array \"dependent\" which contains the information about the processes that depend on it. Initially the values of each array are all \"false\".\n\nBefore sending, the probe checks whether \"P\" is locally dependent on itself. If so, a deadlock occurs. Otherwise it checks whether \"P\", and \"P\" are in different controllers, are locally dependent and \"P\" is waiting for the resource that is locked by \"P\". Once all the conditions are satisfied it sends the probe.\n\nOn the receiving side, the controller checks whether \"P\" is performing a task. If so, it neglects the probe. Otherwise, it checks the responses given \"P\" to \"P\" and \"dependent\"(i) is false. Once it is verified, it assigns true to \"dependent\"(i). Then it checks whether k is equal to i. If both are equal, a deadlock occurs, otherwise it sends the probe to next dependent process.\n\nIn pseudocode, the algorithm works as follows:\n\n if \"P\" is locally dependent on itself\n\n if\n\n\"P\" initiates deadlock detection. \"C\" sends the probe saying \"P\" depends on \"P\". Once the message is received by \"C\", it checks whether \"P\" is idle. \"P\" is idle because it is locally dependent on \"P\" and updates \"dependent\"(2) to True.\n\nAs above, \"C\" sends probe to \"C\" and \"C\" sends probe to \"C\". At \"C\", \"P\" is idle so it update \"dependent\"(1) to True. Therefore, deadlock can be declared.\n\nConsider that there are \"m\" controllers and \"p\" process to perform, to declare whether a deadlock has occurred or not, the worst case for controllers and processes must be visited. Therefore, the solution is O(m+p). The time complexity is O(n).\n"}
{"id": "6449", "url": "https://en.wikipedia.org/wiki?curid=6449", "title": "Clock", "text": "Clock\n\nA clock is an instrument used to measure, keep, and indicate time. The clock is one of the oldest human inventions, meeting the need to measure intervals of time shorter than the natural units: the day, the lunar month, and the year. Devices operating on several physical processes have been used over the millennia.\n\nSome predecessors to the modern clock may be considered as \"clocks\" that are based on movement in nature: A sundial shows the time by displaying the position of a shadow on a flat surface. There is a range of duration timers, a well-known example being the hourglass. Water clocks, along with the sundials, are possibly the oldest time-measuring instruments. A major advance occurred with the invention of the verge escapement, which made possible the first mechanical clocks around 1300 in Europe, which kept time with oscillating timekeepers like balance wheels.\n\nTraditionally in horology, the term \"clock\" was used for a striking clock, while a clock that did not strike the hours audibly was called a timepiece. In general usage today, a \"clock\" refers to any device for measuring and displaying the time. Watches and other timepieces that can be carried on one's person are often distinguished from clocks.\nSpring-driven clocks appeared during the 15th century. During the 15th and 16th centuries, clockmaking flourished. The next development in accuracy occurred after 1656 with the invention of the pendulum clock. A major stimulus to improving the accuracy and reliability of clocks was the importance of precise time-keeping for navigation. The electric clock was patented in 1840. The development of electronics in the 20th century led to clocks with no clockwork parts at all.\n\nThe timekeeping element in every modern clock is a harmonic oscillator, a physical object (resonator) that vibrates or oscillates at a particular frequency.\nThis object can be a pendulum, a tuning fork, a quartz crystal, or the vibration of electrons in atoms as they emit microwaves.\n\nClocks have different ways of displaying the time. Analog clocks indicate time with a traditional clock face, with moving hands. Digital clocks display a numeric representation of time. Two numbering systems are in use; 24-hour time notation and 12-hour notation. Most digital clocks use electronic mechanisms and LCD, LED, or VFD displays. For the blind and use over telephones, speaking clocks state the time audibly in words. There are also clocks for the blind that have displays that can be read by touch. The study of timekeeping is known as horology.\n\nThe word \"clock\" is derived (via Dutch, Northern French, and Medieval Latin) from the Celtic words \"clagan\" and \"clocca\" meaning \"bell\".\n\nThe apparent position of the Sun in the sky moves over the course of each day, reflecting the rotation of the Earth. Shadows cast by stationary objects move correspondingly, so their positions can be used to indicate the time of day. A sundial shows the time by displaying the position of a shadow on a (usually) flat surface, which has markings that correspond to the hours. Sundials can be horizontal, vertical, or in other orientations. Sundials were widely used in ancient times. With the knowledge of latitude, a well-constructed sundial can measure local solar time with reasonable accuracy, within a minute or two. Sundials continued to be used to monitor the performance of clocks until the modern era.\n\nMany devices can be used to mark passage of time without respect to reference time (time of day, minutes, etc.) and can be useful for measuring duration or intervals. Examples of such duration timers are candle clocks, incense clocks and the hourglass. Both the candle clock and the incense clock work on the same principle wherein the consumption of resources is more or less constant allowing reasonably precise and repeatable estimates of time passages. In the hourglass, fine sand pouring through a tiny hole at a constant rate indicates an arbitrary, predetermined, passage of time. The resource is not consumed but re-used.\n\nWater clocks, also known as clepsydrae (sg: \"clepsydra\"), along with the sundials, are possibly the oldest time-measuring instruments, with the only exceptions being the vertical gnomon and the day counting tally stick. Given their great antiquity, where and when they first existed is not known and perhaps unknowable. The bowl-shaped outflow is the simplest form of a water clock and is known to have existed in Babylon and in Egypt around the 16th century BC. Other regions of the world, including India and China, also have early evidence of water clocks, but the earliest dates are less certain. Some authors, however, write about water clocks appearing as early as 4000 BC in these regions of the world.\n\nGreek astronomer Andronicus of Cyrrhus supervised the construction of the Tower of the Winds in Athens in the 1st century B.C. The Greek and Roman civilizations are credited for initially advancing water clock design to include complex gearing, which was connected to fanciful automata and also resulted in improved accuracy. These advances were passed on through Byzantium and Islamic times, eventually making their way back to Europe. Independently, the Chinese developed their own advanced water clocks（水鐘）in 725 AD, passing their ideas on to Korea and Japan.\n\nSome water clock designs were developed independently and some knowledge was transferred through the spread of trade. Pre-modern societies do not have the same precise timekeeping requirements that exist in modern industrial societies, where every hour of work or rest is monitored, and work may start or finish at any time regardless of external conditions. Instead, water clocks in ancient societies were used mainly for astrological reasons. These early water clocks were calibrated with a sundial. While never reaching the level of accuracy of a modern timepiece, the water clock was the most accurate and commonly used timekeeping device for millennia, until it was replaced by the more accurate pendulum clock in 17th-century Europe.\n\nIslamic civilization is credited with further advancing the accuracy of clocks with elaborate engineering. In 797 (or possibly 801), the Abbasid caliph of Baghdad, Harun al-Rashid, presented Charlemagne with an Asian Elephant named Abul-Abbas together with a \"particularly elaborate example\" of a water clock.\nPope Sylvester II introduced clocks to northern and western Europe around 1000AD\n\nIn the 13th century, Al-Jazari, an engineer from Mesopotamia (lived 1136–1206) who worked for Artuqid king of Diyar-Bakr, Nasir al-Din, made numerous clocks of all shapes and sizes. A book on his work described 50 mechanical devices in 6 categories, including water clocks. The most reputed clocks included the Elephant, Scribe and Castle clocks, all of which have been successfully reconstructed. As well as telling the time, these grand clocks were symbols of status, grandeur and wealth of the Urtuq State.\n\nThe word \"horologia\" (from the Greek ὥρα, hour, and λέγειν, to tell) was used to describe early mechanical clocks, but the use of this word (still used in several Romance languages) for all timekeepers conceals the true nature of the mechanisms. For example, there is a record that in 1176 Sens Cathedral installed a ‘horologe’ but the mechanism used is unknown. According to Jocelin of Brakelond, in 1198 during a fire at the abbey of St Edmundsbury (now Bury St Edmunds), the monks 'ran to the clock' to fetch water, indicating that their water clock had a reservoir large enough to help extinguish the occasional fire. The word \"clock\" (from the Celtic words \"clocca\" and \"clogan\", both meaning \"bell\"), which gradually supersedes \"horologe\", suggests that it was the sound of bells which also characterized the prototype mechanical clocks that appeared during the 13th century in Europe.\n\nA water-powered cogwheel clock was created in China in AD 725 by Yi Xing and Liang Lingzan. This is not considered an escapement mechanism clock as it was unidirectional, the Song dynasty polymath and genius Su Song (1020–1101) incorporated it into his monumental innovation of the astronomical clock-tower of Kaifeng in 1088. His astronomical clock and rotating armillary sphere still relied on the use of either flowing water during the spring, summer, autumn seasons and liquid mercury during the freezing temperature of winter (i.e. hydraulics). A mercury clock, described in the \"Libros del saber\", a Spanish work from 1277 consisting of translations and paraphrases of Arabic works, is sometimes quoted as evidence for Muslim knowledge of a mechanical clock. A mercury-powered cogwheel clock was created by Ibn Khalaf al-Muradi\n\nIn Europe, between 1280 and 1320, there is an increase in the number of references to clocks and horologes in church records, and this probably indicates that a new type of clock mechanism had been devised. Existing clock mechanisms that used water power were being adapted to take their driving power from falling weights. This power was controlled by some form of oscillating mechanism, probably derived from existing bell-ringing or alarm devices. This controlled release of power—the escapement—marks the beginning of the true mechanical clock, which differed from the previously mentioned cogwheel clocks. Verge escapement mechanism derived in the surge of true mechanical clocks, which didn't need any kind of fluid power, like water or mercury, to work.\n\nThese mechanical clocks were intended for two main purposes: for signalling and notification (e.g. the timing of services and public events), and for modeling the solar system. The former purpose is administrative, the latter arises naturally given the scholarly interests in astronomy, science, astrology, and how these subjects integrated with the religious philosophy of the time. The astrolabe was used both by astronomers and astrologers, and it was natural to apply a clockwork drive to the rotating plate to produce a working model of the solar system.\n\nSimple clocks intended mainly for notification were installed in towers, and did not always require faces or hands. They would have announced the canonical hours or intervals between set times of prayer. Canonical hours varied in length as the times of sunrise and sunset shifted. The more sophisticated astronomical clocks would have had moving dials or hands, and would have shown the time in various time systems, including Italian hours, canonical hours, and time as measured by astronomers at the time. Both styles of clock started acquiring extravagant features such as automata.\n\nIn 1283, a large clock was installed at Dunstable Priory; its location above the rood screen suggests that it was not a water clock. In 1292, Canterbury Cathedral installed a 'great horloge'. Over the next 30 years there are mentions of clocks at a number of ecclesiastical institutions in England, Italy, and France. In 1322, a new clock was installed in Norwich, an expensive replacement for an earlier clock installed in 1273. This had a large (2 metre) astronomical dial with automata and bells. The costs of the installation included the full-time employment of two clockkeepers for two years.\n\nBesides the Chinese astronomical clock of Su Song in 1088 mentioned above, in Europe there were the clocks constructed by Richard of Wallingford in St Albans by 1336, and by Giovanni de Dondi in Padua from 1348 to 1364. They no longer exist, but detailed descriptions of their design and construction survive, and modern reproductions have been made. They illustrate how quickly the theory of the mechanical clock had been translated into practical constructions, and also that one of the many impulses to their development had been the desire of astronomers to investigate celestial phenomena.\n\nWallingford's clock had a large astrolabe-type dial, showing the sun, the moon's age, phase, and node, a star map, and possibly the planets. In addition, it had a wheel of fortune and an indicator of the state of the tide at London Bridge. Bells rang every hour, the number of strokes indicating the time. Dondi's clock was a seven-sided construction, 1 metre high, with dials showing the time of day, including minutes, the motions of all the known planets, an automatic calendar of fixed and movable feasts, and an eclipse prediction hand rotating once every 18 years. It is not known how accurate or reliable these clocks would have been. They were probably adjusted manually every day to compensate for errors caused by wear and imprecise manufacture. Water clocks are sometimes still used today, and can be examined in places such as ancient castles and museums. The Salisbury Cathedral clock, built in 1386, is considered to be the world's oldest surviving mechanical clock that strikes the hours.\n\nClockmakers developed their art in various ways. Building smaller clocks was a technical challenge, as was improving accuracy and reliability. Clocks could be impressive showpieces to demonstrate skilled craftsmanship, or less expensive, mass-produced items for domestic use. The escapement in particular was an important factor affecting the clock's accuracy, so many different mechanisms were tried.\n\nSpring-driven clocks appeared during the 15th century, although they are often erroneously credited to Nuremberg watchmaker Peter Henlein (or Henle, or Hele) around 1511. The earliest existing spring driven clock is the chamber clock given to Phillip the Good, Duke of Burgundy, around 1430, now in the Germanisches Nationalmuseum. Spring power presented clockmakers with a new problem: how to keep the clock movement running at a constant rate as the spring ran down. This resulted in the invention of the \"stackfreed\" and the fusee in the 15th century, and many other innovations, down to the invention of the modern \"going barrel\" in 1760.\n\nEarly clock dials did not indicate minutes and seconds. A clock with a dial indicating minutes was illustrated in a 1475 manuscript by Paulus Almanus, and some 15th-century clocks in Germany indicated minutes and seconds.\nAn early record of a seconds hand on a clock dates back to about 1560 on a clock now in the Fremersdorf collection.\n\nDuring the 15th and 16th centuries, clockmaking flourished, particularly in the metalworking towns of Nuremberg and Augsburg, and in Blois, France. Some of the more basic table clocks have only one time-keeping hand, with the dial between the hour markers being divided into four equal parts making the clocks readable to the nearest 15 minutes. Other clocks were exhibitions of craftsmanship and skill, incorporating astronomical indicators and musical movements. The cross-beat escapement was invented in 1584 by Jost Bürgi, who also developed the remontoire. Bürgi's clocks were a great improvement in accuracy as they were correct to within a minute a day. These clocks helped the 16th-century astronomer Tycho Brahe to observe astronomical events with much greater precision than before.\n\nThe next development in accuracy occurred after 1656 with the invention of the pendulum clock. Galileo had the idea to use a swinging bob to regulate the motion of a time-telling device earlier in the 17th century. Christiaan Huygens, however, is usually credited as the inventor. He determined the mathematical formula that related pendulum length to time (about 99.4 cm or 39.1 inches for the one second movement) and had the first pendulum-driven clock made. The first model clock was built in 1657 in the Hague, but it was in England that the idea was taken up. The longcase clock (also known as the \"grandfather clock\") was created to house the pendulum and works by the English clockmaker William Clement in 1670 or 1671. It was also at this time that clock cases began to be made of wood and clock faces to utilize enamel as well as hand-painted ceramics.\n\nIn 1670, William Clement created the anchor escapement, an improvement over Huygens' crown escapement. Clement also introduced the pendulum suspension spring in 1671. The concentric minute hand was added to the clock by Daniel Quare, a London clockmaker and others, and the second hand was first introduced.\n\nIn 1675, Huygens and Robert Hooke invented the spiral balance spring, or the hairspring, designed to control the oscillating speed of the balance wheel. This crucial advance finally made accurate pocket watches possible. The great English clockmaker, Thomas Tompion, was one of the first to use this mechanism successfully in his pocket watches, and he adopted the minute hand which, after a variety of designs were trialled, eventually stabilised into the modern-day configuration. The rack and snail striking mechanism for striking clocks, was introduced during the 17th century and had distinct advantages over the 'countwheel' (or 'locking plate') mechanism. During the 20th century there was a common misconception that Edward Barlow invented \"rack and snail\" striking. In fact, his invention was connected with a repeating mechanism employing the rack and snail. The repeating clock, that chimes the number of hours (or even minutes) was invented by either Quare or Barlow in 1676. George Graham invented the deadbeat escapement for clocks in 1720.\n\nA major stimulus to improving the accuracy and reliability of clocks was the importance of precise time-keeping for navigation. The position of a ship at sea could be determined with reasonable accuracy if a navigator could refer to a clock that lost or gained less than about 10 seconds per day. This clock could not contain a pendulum, which would be virtually useless on a rocking ship. In 1714, the British government offered large financial rewards to the value of 20,000 pounds, for anyone who could determine longitude accurately. John Harrison, who dedicated his life to improving the accuracy of his clocks, later received considerable sums under the Longitude Act.\n\nIn 1735, Harrison built his first chronometer, which he steadily improved on over the next thirty years before submitting it for examination. The clock had many innovations, including the use of bearings to reduce friction, weighted balances to compensate for the ship's pitch and roll in the sea and the use of two different metals to reduce the problem of expansion from heat. The chronometer was tested in 1761 by Harrison's son and by the end of 10 weeks the clock was in error by less than 5 seconds.\n\nThe British had predominated in watch manufacture for much of the 17th and 18th centuries, but maintained a system of production that was geared towards high quality products for the elite. Although there was an attempt to modernise clock manufacture with mass production techniques and the application of duplicating tools and machinery by the British Watch Company in 1843, it was in the United States that this system took off. In 1816, Eli Terry and some other Connecticut clockmakers developed a way of mass-producing clocks by using interchangeable parts. Aaron Lufkin Dennison started a factory in 1851 in Massachusetts that also used interchangeable parts, and by 1861 was running a successful enterprise incorporated as the Waltham Watch Company.\n\nIn 1815, Francis Ronalds published the first electric clock powered by dry pile batteries. Alexander Bain, Scottish clockmaker, patented the electric clock in 1840. The electric clock's mainspring is wound either with an electric motor or with an electromagnet and armature. In 1841, he first patented the electromagnetic pendulum. By the end of the nineteenth century, the advent of the dry cell battery made it feasible to use electric power in clocks. Spring or weight driven clocks that use electricity, either alternating current (AC) or direct current (DC), to rewind the spring or raise the weight of a mechanical clock would be classified as an electromechanical clock. This classification would also apply to clocks that employ an electrical impulse to propel the pendulum. In electromechanical clocks the electricity serves no time keeping function. These types of clocks were made as individual timepieces but more commonly used in synchronized time installations in schools, businesses, factories, railroads and government facilities as a master clock and slave clocks.\n\nElectric clocks that are powered from the AC supply often use synchronous motors. The supply current alternates with a frequency of 50 hertz in many countries, and 60 hertz in others. The rotor of the motor rotates at a speed that is related to the alternation frequency. Appropriate gearing converts this rotation speed to the correct ones for the hands of the analog clock. The development of electronics in the 20th century led to clocks with no clockwork parts at all. Time in these cases is measured in several ways, such as by the alternation of the AC supply, vibration of a tuning fork, the behaviour of quartz crystals, or the quantum vibrations of atoms. Electronic circuits divide these high-frequency oscillations to slower ones that drive the time display. Even mechanical clocks have since come to be largely powered by batteries, removing the need for winding.\n\nThe piezoelectric properties of crystalline quartz were discovered by Jacques and Pierre Curie in 1880. The first crystal oscillator was invented in 1917 by Alexander M. Nicholson after which, the first quartz crystal oscillator was built by Walter G. Cady in 1921. In 1927 the first quartz clock was built by Warren Marrison and J.W. Horton at Bell Telephone Laboratories in Canada. The following decades saw the development of quartz clocks as precision time measurement devices in laboratory settings—the bulky and delicate counting electronics, built with vacuum tubes, limited their practical use elsewhere. The National Bureau of Standards (now NIST) based the time standard of the United States on quartz clocks from late 1929 until the 1960s, when it changed to atomic clocks. In 1969, Seiko produced the world's first quartz wristwatch, the Astron. Their inherent accuracy and low cost of production resulted in the subsequent proliferation of quartz clocks and watches.\n\nAs of the 2010s, atomic clocks are the most accurate clocks in existence. They are considerably more accurate than quartz clocks as they can be accurate to within a few seconds over thousands of years. Atomic clocks were first theorized by Lord Kelvin in 1879. In the 1930s the development of Magnetic resonance created practical method for doing this. A prototype ammonia maser device was built in 1949 at the U.S. National Bureau of Standards (NBS, now NIST). Although it was less accurate than existing quartz clocks, it served to demonstrate the concept. The first accurate atomic clock, a caesium standard based on a certain transition of the caesium-133 atom, was built by Louis Essen in 1955 at the National Physical Laboratory in the UK. Calibration of the caesium standard atomic clock was carried out by the use of the astronomical time scale \"ephemeris time\" (ET). As of 2013, the most stable atomic clocks are ytterbium clocks, which are stable to within less than two parts in 1 quintillion ().\n\nThe invention of the mechanical clock in the 13th century initiated a change in timekeeping methods from continuous processes, such as the motion of the gnomon's shadow on a sundial or the flow of liquid in a water clock, to periodic oscillatory processes, such as the swing of a pendulum or the vibration of a quartz crystal, which had the potential for more accuracy. All modern clocks use oscillation.\n\nAlthough the mechanisms they use vary, all oscillating clocks, mechanical, digital and atomic, work similarly and can be divided into analogous parts. They consist of an object that repeats the same motion over and over again, an \"oscillator\", with a precisely constant time interval between each repetition, or 'beat'. Attached to the oscillator is a \"controller\" device, which sustains the oscillator's motion by replacing the energy it loses to friction, and converts its oscillations into a series of pulses. The pulses are then counted by some type of \"counter\", and the number of counts is converted into convenient units, usually seconds, minutes, hours, etc. Finally some kind of \"indicator\" displays the result in human readable form.\n\nThe timekeeping element in every modern clock is a harmonic oscillator, a physical object (resonator) that vibrates or oscillates repetitively at a precisely constant frequency.\nThe advantage of a harmonic oscillator over other forms of oscillator is that it employs resonance to vibrate at a precise natural resonant frequency or 'beat' dependent only on its physical characteristics, and resists vibrating at other rates. The possible precision achievable by a harmonic oscillator is measured by a parameter called its Q, or quality factor, which increases (other things being equal) with its resonant frequency. This is why there has been a long term trend toward higher frequency oscillators in clocks. Balance wheels and pendulums always include a means of adjusting the rate of the timepiece. Quartz timepieces sometimes include a rate screw that adjusts a capacitor for that purpose. Atomic clocks are primary standards, and their rate cannot be adjusted.\n\nSome clocks rely for their accuracy on an external oscillator; that is, they are automatically synchronized to a more accurate clock:\n\nThis has the dual function of keeping the oscillator running by giving it 'pushes' to replace the energy lost to friction, and converting its vibrations into a series of pulses that serve to measure the time.\nIn mechanical clocks, the low Q of the balance wheel or pendulum oscillator made them very sensitive to the disturbing effect of the impulses of the escapement, so the escapement had a great effect on the accuracy of the clock, and many escapement designs were tried. The higher Q of resonators in electronic clocks makes them relatively insensitive to the disturbing effects of the drive power, so the driving oscillator circuit is a much less critical component.\n\nThis counts the pulses and adds them up to get traditional time units of seconds, minutes, hours, etc. It usually has a provision for \"setting\" the clock by manually entering the correct time into the counter.\n\nThis displays the count of seconds, minutes, hours, etc. in a human readable form.\n\nClocks can be classified by the type of time display, as well as by the method of timekeeping.\n\nAnalog clocks usually use a clock face which indicates time using rotating pointers called \"hands\" on a fixed numbered dial or dials. The standard clock face, known universally throughout the world, has a short \"hour hand\" which indicates the hour on a circular dial of 12 hours, making two revolutions per day, and a longer \"minute hand\" which indicates the minutes in the current hour on the same dial, which is also divided into 60 minutes. It may also have a \"second hand\" which indicates the seconds in the current minute. The only other widely used clock face today is the 24 hour analog dial, because of the use of 24 hour time in military organizations and timetables. Before the modern clock face was standardized during the Industrial Revolution, many other face designs were used throughout the years, including dials divided into 6, 8, 10, and 24 hours. During the French Revolution the French government tried to introduce a 10-hour clock, as part of their decimal-based metric system of measurement, but it didn't catch on. An Italian 6 hour clock was developed in the 18th century, presumably to save power (a clock or watch striking 24 times uses more power).\nAnother type of analog clock is the sundial, which tracks the sun continuously, registering the time by the shadow position of its gnomon. Because the sun does not adjust to daylight saving time, users must add an hour during that time. Corrections must also be made for the equation of time, and for the difference between the longitudes of the sundial and of the central meridian of the time zone that is being used (i.e. 15 degrees east of the prime meridian for each hour that the time zone is ahead of GMT). Sundials use some or part of the 24 hour analog dial. There also exist clocks which use a digital display despite having an analog mechanism—these are commonly referred to as flip clocks. Alternative systems have been proposed. For example, the \"Twelv\" clock indicates the current hour using one of twelve colors, and indicates the minute by showing a proportion of a circular disk, similar to a moon phase.\n\nDigital clocks display a numeric representation of time. Two numeric display formats are commonly used on digital clocks:\n\nMost digital clocks use electronic mechanisms and LCD, LED, or VFD displays; many other display technologies are used as well (cathode ray tubes, nixie tubes, etc.). After a reset, battery change or power failure, these clocks without a backup battery or capacitor either start counting from 12:00, or stay at 12:00, often with blinking digits indicating that the time needs to be set. Some newer clocks will reset themselves based on radio or Internet time servers that are tuned to national atomic clocks. Since the advent of digital clocks in the 1960s, the use of analog clocks has declined significantly.\n\nSome clocks, called 'flip clocks', have digital displays that work mechanically. The digits are painted on sheets of material which are mounted like the pages of a book. Once a minute, a page is turned over to reveal the next digit. These displays are usually easier to read in brightly lit conditions than LCDs or LEDs. Also, they do not go back to 12:00 after a power interruption. Flip clocks generally do not have electronic mechanisms. Usually, they are driven by AC-synchronous motors.\n\nClocks with analog quadrants, with a digital component, usually minutes and hours displayed analogously and seconds displayed in digital mode.\n\nFor convenience, distance, telephony or blindness, auditory clocks present the time as sounds. The sound is either spoken natural language, (e.g. \"The time is twelve thirty-five\"), or as auditory codes (e.g. number of sequential bell rings on the hour represents the number of the hour like the bell, Big Ben). Most telecommunication companies also provide a speaking clock service as well.\n\nWord clocks are clocks that display the time visually using sentences. E.g.: \"It’s about three o’clock.\" These clocks can be implemented in hardware or software.\n\nSome clocks, usually digital ones, include an optical projector that shines a magnified image of the time display onto a screen or onto a surface such as an indoor ceiling or wall. The digits are large enough to be easily read, without using glasses, by persons with moderately imperfect vision, so the clocks are convenient for use in their bedrooms. Usually, the timekeeping circuitry has a battery as a backup source for an uninterrupted power supply to keep the clock on time, while the projection light only works when the unit is connected to an A.C. supply. Completely battery-powered portable versions resembling flashlights are also available.\n\nAuditory and projection clocks can be used by people who are blind or have limited vision. There are also clocks for the blind that have displays that can be read by using the sense of touch. Some of these are similar to normal analog displays, but are constructed so the hands can be felt without damaging them. Another type is essentially digital, and uses devices that use a code such as Braille to show the digits so that they can be felt with the fingertips.\n\nSome clocks have several displays driven by a single mechanism, and some others have several completely separate mechanisms in a single case. Clocks in public places often have several faces visible from different directions, so that the clock can be read from anywhere in the vicinity; all the faces show the same time. Other clocks show the current time in several time-zones. Watches that are intended to be carried by travellers often have two displays, one for the local time and the other for the time at home, which is useful for making pre-arranged phone calls. Some equation clocks have two displays, one showing mean time and the other solar time, as would be shown by a sundial. Some clocks have both analog and digital displays. Clocks with Braille displays usually also have conventional digits so they can be read by sighted people.\n\nClocks are in homes, offices and many other places; smaller ones (watches) are carried on the wrist or in a pocket; larger ones are in public places, e.g. a railway station or church. A small clock is often shown in a corner of computer displays, mobile phones and many MP3 players.\n\nThe primary purpose of a clock is to \"display\" the time. Clocks may also have the facility to make a loud alert signal at a specified time, typically to waken a sleeper at a preset time; they are referred to as \"alarm clocks\". The alarm may start at a low volume and become louder, or have the facility to be switched off for a few minutes then resume. Alarm clocks with visible indicators are sometimes used to indicate to children too young to read the time that the time for sleep has finished; they are sometimes called \"training clocks\".\n\nA clock mechanism may be used to \"control\" a device according to time, e.g. a central heating system, a VCR, or a time bomb (see: digital counter). Such mechanisms are usually called timers. Clock mechanisms are also used to drive devices such as solar trackers and astronomical telescopes, which have to turn at accurately controlled speeds to counteract the rotation of the Earth.\n\nMost digital computers depend on an internal signal at constant frequency to synchronize processing; this is referred to as a clock signal. (A few research projects are developing CPUs based on asynchronous circuits.) Some equipment, including computers, also maintains time and date for use as required; this is referred to as time-of-day clock, and is distinct from the system clock signal, although possibly based on counting its cycles.\n\nIn Chinese culture, giving a clock (送鍾/送钟, sòng zhōng) is often taboo, especially to the elderly as the term for this act is a homophone with the term for the act of attending another's funeral (送終/送终, sòngzhōng). A UK government official Susan Kramer gave a watch to Taipei mayor Ko Wen-je unaware of such a taboo which resulted in some professional embarrassment and a pursuant apology.\n\nIt is undesirable to give someone a clock or (depending on the region) other timepiece as a gift. Traditional superstitions regard this as counting the seconds to the recipient's death. Another common interpretation of this is that the phrase \"to give a clock\" () in Chinese is pronounced \"sòng zhōng\" in Mandarin, which is a homophone of a phrase for \"terminating\" or \"attending a funeral\" (both can be written as (traditional) or (simplified)). Cantonese people consider such a gift as a curse.\n\nThis homonymic pair works in both Mandarin and Cantonese, although in most parts of China only clocks and large bells, and not watches, are called \"zhong\", and watches are commonly given as gifts in China.\n\nHowever, should such a gift be given, the \"unluckiness\" of the gift can be countered by exacting a small monetary payment so the recipient is buying the clock and thereby counteracting the (\"give\") expression of the phrase.\n\nFor some scientific work timing of the utmost accuracy is essential. It is also necessary to have a standard of the maximum accuracy against which working clocks can be calibrated. An ideal clock would give the time to unlimited accuracy, but this is not realisable. Many physical processes, in particular including some transitions between atomic energy levels, occur at exceedingly stable frequency; counting cycles of such a process can give a very accurate and consistent time—clocks which work this way are usually called atomic clocks. Such clocks are typically large, very expensive, require a controlled environment, and are far more accurate than required for most purposes; they are typically used in a standards laboratory.\n\nUntil advances in the late twentieth century, navigation depended on the ability to measure latitude and longitude. Latitude can be determined through celestial navigation; the measurement of longitude requires accurate knowledge of time. This need was a major motivation for the development of accurate mechanical clocks. John Harrison created the first highly accurate marine chronometer in the mid-18th century. The Noon gun in Cape Town still fires an accurate signal to allow ships to check their chronometers. Many buildings near major ports used to have (some still do) a large ball mounted on a tower or mast arranged to drop at a pre-determined time, for the same purpose. While satellite navigation systems such as the Global Positioning System (GPS) require unprecedentedly accurate knowledge of time, this is supplied by equipment on the satellites; vehicles no longer need timekeeping equipment.\n\n\n\n"}
{"id": "2502344", "url": "https://en.wikipedia.org/wiki?curid=2502344", "title": "Criminal punishment in Edo-period Japan", "text": "Criminal punishment in Edo-period Japan\n\nDuring the Edo period, Japan used various punishments against criminals. These can be categorized as follows:\n\n\nSerious crimes such as murder and arson were punished by death. The shogunate maintained execution grounds for Edo at Kozukappara, Suzugamori, and Itabashi. Kozukappara, also known as Kotsukappara or Kozukahara, is currently located near the southwest exit of Tokyo's Minami-Senju Station. It is estimated that between 100,000 and 200,000 people were executed here. Only part of the site remains, located next to Emmeiji temple, partly buried under the rail tracks and under a more-recent burial ground. Archaeological and morphological research was done by Tokyo University on the skulls found buried here which confirmed the execution methods. Another notable one was located at Suzugamori in Shinagawa. Both sites are still sparsely commemorated \"in situ\" with memorial plaques and tombstones.\n\nThe shogunate executed criminals in various ways:\n\n\nThe death penalty often carried collateral punishments. One was parading the criminal around town prior to execution. A similar one was public display of the criminal prior to execution. A third was public display of the severed head.\n\nSamurai were often sentenced to commit seppuku in lieu of these forms of punishment. Seppuku is a term of suicide for the samurai.\n\nDepending on the severity of the crime, magistrates could sentence convicts to incarceration in various forms:\n\nExclusion from the location of the crime was a penalty for both commoners and samurai.\n\nFor crimes requiring moderate punishment, convicts could be sent to work at labor camps such as the one on Ishikawa-jima in Edo Bay. More serious acts could result in being sent to work in the gold mine on the island of Sado. In 1590, Hideyoshi had banned \"unfree labor\" or slavery; but forms of contract and indentured labor persisted alongside the period penal codes' forced labor. For example, the Edo period penal laws prescribed \"non-free labor\" for the immediate family of executed criminals in Article 17 of the \"Gotōke reijō\" (Tokugawa House Laws), but the practice never became common. The 1711 \"Gotōke reijō\" was compiled from over 600 statutes promulgated between 1597 and 1696.\n\nIt was also common for female convicts to be sentenced to serve terms working as slaves and prostitutes in walled Red Light Districts, most notably Yoshiwara.\n\nA penalty that targeted merchants especially was kesshō, the confiscation of a business.\n\nHandcuffing allowed the government to punish a criminal while he was under house arrest. Depending on the severity of the crime, the sentence might last 30, 50, or 100 days.\n\nFlagellation was a common penalty for crimes such as theft and fighting. Amputation of the nose or ears replaced flogging as penalty early in the Edo period. The 8th Shōgun of Edo, Tokugawa Yoshimune introduced judicial Flogging Penalty, or \"tataki\", in 1720. A convicted criminal could be sentenced to a maximum of 100 lashes. \"Samurai\" and priests were exempt from flogging, and the penalty was applied only to commoners. The convict was stripped of all outer clothing and struck about the buttocks and back. The flogging penalty was used until 1867, though it fell out of favor from 1747 to 1795 intermittently. Both men and women could be sentenced to a flogging, though during one segment of the mid-Edo period, women were imprisoned rather than flogged.\n\nIn 757 A.D., the Chinese-influenced Yoro Ritsuryo (養老律令) legal system was enacted and introduced Five Judicial Penalties (五刑). Two of the Five Judicial Penalties involved Flogging. Light Flogging provided for 10 to 50 lashes, while Heavy \nFlogging stipulated 60 to 100 strokes. However, a slave could be sentenced to up a maximum of 200 lashes. These flogging penalties only applied to male commoners. Convicts of the nobility, along with female commoners, might be sentenced to the imposition of handcuffs or a fine. When a convicted criminal was flogged, half the number of lashes were typically applied to the back, half to the buttocks. At times, if the convict's request to change the lash target was sanctioned then the lashes would be applied only to the back or to the buttocks. By the Age of Warring States, flogging had been largely replaced by decapitation.\n\nKozukappara execution grounds\n\n\n"}
{"id": "1519875", "url": "https://en.wikipedia.org/wiki?curid=1519875", "title": "Disability studies", "text": "Disability studies\n\nDisability studies is an academic discipline that examines the meaning, nature, and consequences of disability. Initially the field focused on the division between \"impairment\" and \"disability\", where impairment was an impairment of an individual's mind or body, while disability was considered a social construct. This premise gave rise to two distinct models of disability: the social and medical models of disability. In 1999 the social model was universally accepted as the model preferred by the field. However, in recent years, the division between the social and medical models has been challenged. Additionally there has been an increased focus on interdisciplinary research. For example, recent investigations suggest using \"cross-sectional markers of stratification\" may help provide new insights on the non-random distribution of risk factors capable of acerbating disablement processes.\n\nDisability studies courses include work in disability history, theory, legislation, policy, ethics, and the arts. However, students are taught to focus on the lived experiences of individuals with disabilities in practical terms. The field is focused on increasing individuals with disabilities access to civil rights and improving their quality of life.\n\nDisability studies emerged in the 1980s primarily in the US, the UK, and Canada. In 1986, the Section for the Study of Chronic Illness, Impairment, and Disability of the Social Science Association (United States) was renamed the Society for Disability Studies. The first US disabilities studies program emerged in 1994, at Syracuse University. The first edition of the Disabilities Studies Reader (one of the first collections of academic papers related to disability studies) was published in 1997. The field grew rapidly over the next ten years. In 2005, the Modern Language Association established disability studies as a “division of study.” \n\nUniversities have long studied disabilities from a clinical perspective. In 1986 the Section for the Study of Chronic Illness, Impairment, and Disability of Social Science Association was renamed the Society for Disability Studies and its journal \"Disability Studies Quarterly\" was the first journal in disability studies. The first US disabilities studies program emerged in 1994, at Syracuse University. However, courses and programs were very few. In the 1997 first edition of the \"Disability Studies Reader\" Lennard J. Davis wrote that \"it had been virtually impossible to have someone teaching about disability within the humanities\". In the second edition, written ten years later, he writes that \"all that has changed\", but \"just because disability studies is on the map, does not mean that is easy to find\".\n\nStill the field continued to grow throughout the 2000s. In 2009 Disability Studies Quarterly published a A Multinational Review of English-language Disability Studies Degrees and Courses. They found that from 2003 to 2008 the number of disability studies stand alone studies courses in the US, UK, Australia, New Zealand and Canada grew from 56 to 108 and the number of degree granting courses grew from 212 to 420. A total of 17 degrees in disability studies were offered, with 11 programs in the US, 2 in the UK, 3 in Canada, and one in Australia.\n\nA 2014 New York Times article \"Disability Studies: A New Normal\" suggests that the expansion in disability studies programs is related to the 1990 passage of the Americans with Disabilities Act (ADA). Those raised after the passage of the ADA have entered colleges and the workforce, as Disability Studies has grown. In a 2014 article, Disability Studies Quarterly published an analysis on the relationships between student run groups and disability studies, from 2008 to 2012. Their article analyzes groups at four different universities and describes how professors have incorporated student activism into their curriculum and research.\n\nAccording to the transnational Society for Disability Studies:\n\"Using an interdisciplinary, multidisciplinary approach. Disability sits at the intersection of many overlapping disciplines in the humanities, sciences, and social sciences. Programs in Disability Studies should encourage a curriculum that allows students, activists, teachers, artists, practitioners, and researchers to engage the subject matter from various disciplinary perspectives.\n\n\nThe social model of disability is expanded to chronic illness and to the broader work of the medical humanities. Practitioners are working towards improving the healthcare for disabled people through disability studies. This multi-disciplinary field of inquiry draws on the experiences and perspectives of people with disabilities to address discrimination. Infinite Ability has done some preliminary work in India to introduce disability studies to medical students.\n\nFeminism introduces the inclusion of intersectionality in disability studies. It focuses on race, gender, sexuality, class and other related systems of oppression that can also intersect with having a disability. From a feminism standpoint, there is a large concern for grasping multiple positions and differences among social groups. Some research on intersectionality and disability has focused on the aspect of being part of two or more stigmatized groups and how these are contributing factors to multiple forms of harassment, the paradox known as \"Double Jeopardy\".\n\nRecent scholarship has included studies that explore the intersection between disability and race. Ellen Samuels explores gender, queer sexualities, and disability. Christopher Bell’s posthumous volume on \"Blackness and Disability\"; and the work of Robert McRuer both explore queerness and disability. These works engage with issues of neoliberal economic oppression. Scholars of Feminist Disability Studies include Rosemarie Garland-Thomson and Alison Kafer. The 2009 publication of Fiona Kumari Campbell’s \"Contours of Ableism: The Production of Disability and Abledness\" signalled a new direction of research — studies in ableism, moving beyond preoccupations with disability to explore the maintenance of abledness in sexed, raced and modified bodies. Similarly, recent work has focused on the intersections of race and ethnicity with disability in the field of education studies and attempted to bridge Critical Race Studies with Dis/ability studies. This work reflects an effort to deal with complex histories of marking racially \"othered\" bodies as physically, psychologically, or morally deficient, and traces this history of scientific racism to contemporary dynamics. Empirical studies show that minority students are disproportionately more likely to be removed from class or school for \"behavioral\" or academic reasons, and far more likely to be labeled with intellectual or learning disabilities. The authors propose a union of critical race and disability studies, DisCrit, as an intersectional approach designed to analyzing the interaction between ableism, sexism, and racism.\n\nIn addition to work by individual scholars, disability studies organizations have also begun to focus on disability and race and gender. The Society for Disability Studies created the Chris Bell Memorial Scholarship to honor Bell's commitment to diversity in disability studies. Postsecondary disability studies programs increasingly engage with the intersectionality of oppression. The University of Manitoba offers a course in on \"Women with disabilities\". Several recent masters' student research papers at York University focus on issues related to women with disabilities and people of African descent with disabilities.\n\nFeminism integrates the social and political aspects that makes a body oppressed while allowing empowerment to be present in acknowledging its culture. Rosemarie Garland-Thomson explains that these related systems of oppression pervades all aspects of culture by \"its structuring institutions, social identities, cultural practices, political positions, historical communities, and the shared human experience of embodiment\". Garland-Thomson further describes that \"identity based critical enterprises have enriched and complicated our understandings of social justice, subject formation, subjugated knowledges and collective action\". Feminism works towards accessibility for everyone regardless of which societal oppressive behavior makes them a minority. Although physical adjustments are most commonly fought for in disability awareness, psychological exclusion also plays a major role oppressing people with disabilities. Sara Ahmed elaborates the mental exclusiveness of privilege in \"Atmospheric Walls\", that there is an atmosphere that surrounds minority bodies which explains why an intersectionally privileged person could be made uncomfortable simply by being in the same room as a person of color, or in this case someone with a disability. Feminists and scholars also developed theories that put attention on the connection of gender and disability. Scholars like Thomas J. Gerschick argued that disability plays a big role on processing and experiencing gender, and people with disabilities often suffer stigmatization towards their gender, since their disabilities may make their body representation excluded by normative binary gender representation. Gerschick also argues that this stigmatization can affect the gendering process and self-representation of people with disabilities. Feminists also look into how people with disabilities are politically oppressed and powerless. Abby L. Wilkerson argues that people with disabilities are politically powerless because they are often desexualized, and the lack of sexual agency leads to the lack of political agency. Wilkerson also indicates that the erotophobia towards minority groups like people with disabilities further oppresses them, since it prevents these groups from gaining political power through sexual agency and power.\n\nThe intersectionality of disability and LGBTQ communities is a more modern subject of study in which the relevance, historically, came about mostly in the 1960s and the 1970s with the surge of advocates in the midst of the vocal liberation and civil rights movements. These communities on their own are topics of numerous deliberations, however they also often link in significance in many ways. The significance of the movements however, began to build momentum and most legal recognition in the 1980s. It was from this age of advocacy that the intersectionality of these two minority groups came into a more clear focus, though it is one continuously growing subject of discussion. It was only in 1973 that the American Psychiatric Association removed homosexuality from their list of mental disorders. In addition to this, it was about forty years later in 2013 that the Diagnostic and Statistical Manual of Mental Disorders, Fifth Edition (DSM-5) changed the listing of transgender to “gender dysphoria”.\n\nOne of the most notable circumstances where the case of these two minority rights come together was the court case \"In re Guardianship of Kowalski\", in which an accident that occurred in 1983 left a thirty-six year old Sharon Kowalski physically disabled with severe brain injuries. The court granted guardianship of her to her homophobic parents who refused visitation rights to her long time partner, Karen Thompson. The court case lasted nearly ten years and was resolved by granting Thompson custody in 1991. This was a major victory in the realm of gay rights but also called to attention the validity of rights for those who identified under the queer and disabled spectrum. Numerous support groups emerged from necessity to create safe spaces for those identifying in these specific minority groups such as the founding of the Rainbow Alliance of the Deaf in 1977, the Lesbian Disabled Veterans of America group in 1996 which then became the Gay, Lesbian, Bisexual and Transgender Disabled Veterans of America, and the San Francisco Gay Amputees group in 2006.\n\nA 2012 study showed that disability was more common in gay, lesbian, and bisexual individuals when compared to heterosexual peers. It was also shown that the gay, lesbian, and bisexual group with disabilities were noticeably younger in age than the heterosexual group. There was a significant difference in the prevalence of mental and physical health between the study groups, a higher statistic of those with queer identities having issues health wise than those straight identifying, reportedly due to the weight of social pressure and discrimination of minority sexual identities.\n\nThough these minority groups do not always intersect, the way they handle their own discrimination, can be compared in similar ways. Being that both the LGBTQ rights movement and the disability rights movements have been defined as minority discrimination movements, it has been said that they are a direct contrast to society normalizing the dominance of heterosexuality and able-bodiedness in its majority population. It is also a large topic of discussion to say that both groups have to undergo the same kind of \"coming out\" process in terms of their sexual identity, gender identity, and disability identity because of the lasting social stigma. \"Coming out\" through sexual identity, gender identity, and disability identity is one example of \"Double Jeopardy\", as they are part of more than one stigmatized group.\n\nQueer studies which emerged from women's studies, brings light towards the different kind of oppression queer and transgender people with disabilities have. For example, the Americans with Disabilities Act dismisses gender disphoria because of its moral case against transgender people. Queer studies are commonly associated with people with disabilities who identify as “Crip” and is commonly believed that queer politics must incorporate crip politics. Alison Kafer describes a first person experience of identifying queer and crip both bold reappropriated terms in Kafer's novel, \"Feminist Queer Crip\". Kafer describes the politics of the crip future and \"an insistence on thinking these imagined futures - and hence, these lived presents - differently\". Although many activists with disabilities find empowerment in appropriating the term crip, not all people with disabilities feel comfortable using that identity. There are many different terms used as an alternative to disability, for example Melwood, a nonprofit who uses the term \"differing abilities\", describes the label disability as \"a limitation in the ability to pursue an occupation because of a physical or mental impairment; a disqualification, restriction or disadvantage and a lack of legal qualification to do something, was an inadequate or limiting 'label' for a cross section of people\". Because the term disability has a history of inferiority, it is believed by many that substituting the term will help eliminate the ableism that is embedded within it. Susan Wendell describes ableism in society \"as a structure for people who have no weakness\", this also applies to anyone who has any intersectional disadvantages. Feminism identifies these disadvantages and strategizes how to deconstruct the system that supports marginalizing specific groups of people.\n\nWithin class comes multiple avenues for intersectionality through disability. Disability looks different from a middle class, upper class, and lower class perspective, as well as through race, gender, and ethnicity. One's social class can contribute to when a person becomes disabled, rather it be sooner or later. For example, where there is poverty we will find disability. This poverty can include social, economic, and cultural poverty. Having a disability can contribute to poverty just as poverty can contribute to having a disability. People with disabilities are more likely to live in poverty and be unemployed than those who do not, resulting in lower socioeconomic status.\n\nThe International Association of Accessibility Professionals recognizes six different models for conceptualizing disability: social, medical, cultural affiliation, economic, charity, and functional solutions. Once universally accepted in the field, the social model of disability has recently been challenged. In a 2014 \"Disability Studies Quarterly\" article, students involved in campus disability groups note that they actively seek cures for their chronic illnesses and \"question the rejection of the medical model\" of disability. The cultural affiliation model accepts the person's disability completely and uses it a point of pride in being associated with other people in a similar condition. The economic model recognizes the effect of bodily limitations on a person's ability to work, and there may be a need for economic support or accommodations for the person's disability while the charity model regards people with disabilities as unfortunate and in need of assistance from the outside, with those providing charity viewed as benevolent contributors to a needy population. The functional solutions model of disability is a practical perspective that identifies the limitations (or \"functional impairments\") due to disability, with the intent to create and promote solutions to overcome those limitations. The primary task is to eliminate, or at least reduce, the impact of the functional limitations of the body through technological or methodological innovation. The pragmatism of the functional solution model deemphasizes the sociopolitical aspects of disability, and instead prioritizes inventiveness and entrepreneurship. This is the prevailing opinion behind compliance literature that promotes self-efficacy and self-advocacy skills for people with disabilities preparing for transition to independent living.\n\nThere is discourse within disability studies to analyze the construction of \"mental illness.\" However, few post-structuralist disability scholars have focused their attention to impairments of the mind. According to Carol Thomas, the Reader in Sociology at the Institute for Health Research, Lancaster University, this may be because disability scholars have in the past considered only the barriers confronted by people with physical disabilities. The experience of impairment, cognitive disability, and mental illness had been absent from the discussion.\n\nIt is unclear exactly which perspective of disability scholarship “psychological impairment” can fall under, and this has led to a hesitation on the part of scholars. Scholars such as Peter Beresford (2002) suggest \"the development of a 'social model of madness and distress'\" which would consider impairments of the mind. Yet others may recommend the \"embodied approach,\" to the study of mental illnesses.\n\n\n\n\n"}
{"id": "194143", "url": "https://en.wikipedia.org/wiki?curid=194143", "title": "Double negative", "text": "Double negative\n\nA double negative is a grammatical construction occurring when two forms of negation are used in the same sentence. Multiple negation is the more general term referring to the occurrence of more than one negative in a clause. In some languages, double negatives cancel one another and produce an affirmative; in other languages, doubled negatives intensify the negation. Languages where multiple negatives affirm each other are said to have negative concord or emphatic negation. Portuguese, Persian, Russian, Spanish, Neapolitan, Italian, Japanese, Bulgarian, Czech, Polish, Afrikaans, Hebrew, and some dialects of English, such as African-American Vernacular English, are examples of negative-concord languages, while Latin and German do not have negative concord. It is cross-linguistically observed that negative-concord languages are more common than those without.\n\nLanguages without negative concord typically have negative polarity items that are used in place of additional negatives when another negating word already occurs. Examples are \"ever\", \"anything\" and \"anyone\" in the sentence \"I haven't ever owed anything to anyone\" (cf. \"I have\"n't\" \"never\" owed \"nothing\" to \"no one\"\" in negative-concord dialects of English, and \"\"Nunca\" devi \"nada\" a \"ninguém\"\" in Portuguese, lit. \"Never have I owed nothing to no one\", or \"\"Non\" ho \"mai\" dovuto \"nulla\" a \"nessuno\"\" in Italian). Note that negative polarity can be triggered not only by direct negatives such as \"not\" or \"never\", but also by words such as \"doubt\" or \"hardly\" (\"I doubt he has ever owed anything to anyone\" or \"He has hardly ever owed anything to anyone\").\n\nStylistically, in English, double negatives can sometimes be used for affirmation (e.g. \"I'm not feeling not good\"), an understatement of the positive (\"I'm feeling good\"). The rhetorical term for this is litotes.\n\nWhen two negatives are used in one independent clause, in standard English the negatives are understood to cancel one another and produce a weakened affirmative: this is known as litotes. However, depending on how such a sentence is constructed, in some dialects if a verb or adverb is in between two negatives then the latter negative is assumed to be intensifying the former thus adding weight or feeling to the negative clause of the sentence. For this reason, it is difficult to portray double negatives in writing as the level of intonation to add weight in one's speech is lost. A double negative intensifier does not necessarily require the prescribed steps, and can easily be ascertained by the mood or intonation of the speaker.\n\nvs.\n\nThese two sentences would be different in how they are communicated by speech. Any assumption would be correct, and the first sentence can be just as right or wrong in intensifying a negative as it is in cancelling it out; thereby rendering the sentence's meaning ambiguous. Since there is no adverb or verb to support the latter negative, the usage here is ambiguous and lies totally on the context behind the sentence. In light of punctuation, the second sentence can be viewed as the intensifier; and the former being a statement thus an admonishment.\n\nIn Standard English, two negatives are understood to resolve to a positive. This rule was observed as early as 1762, when Bishop Robert Lowth wrote \"A Short Introduction to English Grammar with Critical Notes\". For instance, \"I do not disagree\" could mean \"I certainly agree\", \"I agree\", \"I sort of agree\", \"I don't understand your point of view\", \"I have no opinion\", and so on; it is a form of \"weasel words\". Further statements are necessary to resolve which particular meaning was intended.\n\nThis is opposed to the single negative \"I do not agree\", which typically means \"I disagree\". However, the statement \"I do not completely disagree\" is a similar double negative to \"I do not disagree\" but needs little or no clarification.\n\nWith the meaning \"I completely agree\", Lowth would have been referring to litotes wherein two negatives simply cancel each other out. However, the usage of intensifying negatives and examples are presented in his work, which could also imply he wanted either usage of double negatives abolished. Because of this ambiguity, double negatives are frequently employed when making back-handed compliments. The phrase \"Mr. Jones was not incompetent.\" will seldom mean \"Mr. Jones was very competent\" since the speaker would have found a more flattering way to say so. Instead, some kind of problem is implied, though Mr. Jones possesses basic competence at his tasks.\n\nDiscussing English grammar, the term \"double negative\" is often though not universally applied to the non-standard use of a second negative as an intensifier to a negation.\n\nDouble negatives are usually associated with regional and ethnical dialects such as Southern American English, African American Vernacular English, and various British regional dialects. Indeed, they were used in Middle English. Historically, Chaucer made extensive use of double, triple, and even quadruple negatives in his \"Canterbury Tales\". About the Friar, he writes \"Ther nas no man no wher so vertuous\" (\"There never was no man nowhere so virtuous\"). About the Knight, \"He nevere yet no vileynye ne sayde / In all his lyf unto no maner wight\" (\"He never yet no vileness didn't say / In all his life to no manner of man\").\n\nFollowing the battle of Marston Moor, Oliver Cromwell quoted his nephew's dying words in a letter to the boy's father Valentine Walton: \"A little after, he said one thing lay upon his spirit. I asked him what it was. He told me it was that God had not suffered him to be no more the executioner of His enemies.\" Although this particular letter has often been reprinted, it is frequently changed to read \"not ... to be any more\" instead.\n\nWhereas some double negatives may resolve to a positive, in some dialects others resolve to intensify the negative clause within a sentence. For example:\n\nIn contrast, some double negatives become positives:\n\nThe key to understanding the former examples and knowing whether a double negative is intensive or negative is finding a verb between the two negatives. If a verb is present between the two, the latter negative becomes an intensifier which does not negate the former. In the first example, the verb \"to go\" separates the two negatives; therefore the latter negative does not negate the already negated verb. Indeed, the word 'nowhere' is thus being used as an adverb and does not negate the argument of the sentence. One interesting thing to note is that double negatives such as \"I don't want to know no more\" contrasts with Romance languages such as French in \"Je ne veux pas savoir.\" \n\nAn exception is when the second negative is stressed, as in \"I'm not doing ; I'm thinking.\" A sentence can otherwise usually only become positive through consecutive uses of negatives, such as those prescribed in the later examples, where a clause is void of a verb and lacks an adverb to intensify it. Two of them also use emphasis to make the meaning clearer. The last example is a popular example of a double negative that resolves to a positive. This is because the verb 'to doubt' has no intensifier which effectively resolves a sentence to a positive. Had we added an adverb thus:\n\nThen what happens is that the verb \"to doubt\" becomes intensified, which indeed deduces that the sentence is indeed false since nothing was resolved to a positive. The same applies to the third example, where the adverb 'more' merges with the prefix \"no-\" to become a negative word, which when combined with the sentence's former negative only acts as an intensifier to the verb \"hungry\". Where people think that the sentence \"I'm not hungry no more\" resolves to a positive is where the latter negative \"no\" becomes an adjective which only describes its suffix counterpart \"more\" which effectively becomes a noun, instead of an adverb. This is a valid argument since adjectives do indeed describe the nature of a noun; yet some fail to take into account that the phrase \"no more\" is only an adverb and simply serves as an intensifier. Another argument used to support the position double negatives aren't acceptable is a mathematical analogy: negating a negative number results in a positive one; e.g., ; therefore, it is argued, \"I did not go nowhere\" resolves to \"I went somewhere\".\n\nOther forms of double negatives, which are popular to this day and do strictly enhance the negative rather than destroying it, are described thus:\n\nPhilosophies aside, this form of double negative is still in use whereby the use of 'nor' enhances the negative clause by emphasizing what isn't to be. Opponents of double negatives would have preferred \"I'm not entirely familiar with Nihilism or Existentialism\"; however this renders the sentence somewhat empty of the negative clause being advanced in the sentence. This form of double negative along with others described are standard ways of intensifying as well as enhancing a negative. The use of 'nor' to emphasise the negative clause is still popular today, and has been popular in the past through works of Shakespeare and Milton:\n\nTo the common reader the negatives herein do not cancel each other out but simply emphasizes the negative clause.\nUp to the 18th century, double negatives were used to emphasize negation. \"Prescriptive grammarians\" recorded and codified a shift away from the double negative in the 1700s. Double negatives continue to be spoken by those of Vernacular English, such as those of Appalachian English and African American Vernacular English. To such speakers, they view double negatives as emphasizing the negative rather than cancelling out the negatives. Researchers have studied African American Vernacular English (AAVE) and trace its origins back to colonial English. This shows that double negatives were present in colonial English, and thus presumably English as a whole, and were acceptable at that time. English after the 18th century was changed to become more logical and double negatives became seen as canceling each other as in mathematics. The use of double negatives became associated with being uneducated and illogical.\n\nIn his \"Essay towards a practical English Grammar\" of 1711, James Greenwood first recorded the rule: \"Two Negatives, or two Adverbs of Denying do in English affirm\". Robert Lowth stated in his grammar textbook \"A Short Introduction to English Grammar\" (1762) that \"two negatives in English destroy one another, or are equivalent to an affirmative\". Grammarians have assumed that Latin was the model for Lowth and other early grammarians in prescribing against negative concord, as Latin does not feature it. Data indicates, however, that negative concord had already fallen into disuse in Standard English by the time of Lowth's grammar, and no evidence exists that the loss was driven by prescriptivism, which was well established by the time it appeared.\n\nDouble negatives have been employed in various films and television shows. In the film \"Mary Poppins\", the chimney sweep Bert employs a double negative when he says, \"If you don't want to go nowhere...\" Another is used by the bandits in the \"Stinking Badges\" scene of John Huston's \"The Treasure of the Sierra Madre\": \"Badges? We ain't got no badges. We don't need no badges!\".\n\nMore recently, the British television show \"EastEnders\" has received some publicity over the Estuary accent of character Dot Branning, who speaks with double and triple negatives (\"I ain't never heard of no licence.\").. In the Harry Enfield sketch \"Mr Cholmondley-Warner's Guide to the Working-Class\", a stereotypical Cockney employs a septuple-negative: \"Inside toilet? I ain't never not heard of one of them nor I ain't nor nothing.\"\n\nIn music, double negatives can be employed to similar effect (as in Pink Floyd's \"Another Brick in the Wall\", in which schoolchildren chant \"We don't need no education / We don't need no thought control\") or used to establish a frank and informal tone (as in The Rolling Stones' \"(I Can't Get No) Satisfaction\").\n\nDouble negation is uncommon in other West Germanic languages. A notable exception is Afrikaans, where it is mandatory (for example, \"He cannot speak Afrikaans\" becomes \"Hy kan nie Afrikaans praat nie\", \"He cannot Afrikaans speak not\"). Dialectal Dutch, French and San have been suggested as possible origins for this trait. Its proper use follows a set of fairly complex rules as in these examples provided by Bruce Donaldson:\n\nAnother point of view is that this construction is not really an example of a \"double negative\" but simply a grammatical template for negation. The second \"nie\" cannot be understood as a noun or adverb (as can, e.g., \"pas\" in French), and cannot be substituted by any part of speech other than itself with the sentence remaining grammatical. It is a grammatical particle with no independent meaning that happens to be spelled and pronounced the same as the embedded \"nie\", meaning \"not\", through historical accident.\n\nThe second \"nie\" is used if and only if the sentence or phrase doesn't already end with \"nie\" or another negating adverb.\n\nAfrikaans shares with English the property that two negatives make a positive. For example,\n\nWhile double negation is still found in the Low Franconian dialects of west Flanders (e.g., \"Ik ne willen da nie doen\", \"I do not want to do that\") and in some villages in the central Netherlands such as Garderen, it takes a different form than that found in Afrikaans. In Belgian Dutch dialects, however, there are still some widely used expressions like \"nooit niet\" (\"never not\") for \"never\".\n\nSimilar to some dialectal English, Bavarian employs both single and double negation, with the latter denoting special emphasis. For example, compare the Bavarian \"Des hob i no nia ned g'hört\" (\"This have I yet never not heard\") with the standard German \"Das habe ich noch nie gehört\". The German emphatic \"niemals!\" (roughly \"never ever\") corresponds to Bavarian \"(går) nia ned\" or even \"nie nicht\" in Standard German pronunciation.\n\nAnother exception is Yiddish. Due to Slavic influence, the double (and sometimes even triple) negative is quite common.\n\nA few examples would be:\n\nWhile in Latin a second negative word appearing along with \"non\" turns the meaning into a positive one: \"ullus\" means \"any\", \"nullus\" means \"no\", \"non...nullus\" (\"nonnullus\") means \"some\". In the same way, \"umquam\" means \"ever\", \"numquam\" means \"never\", \"non...numquam\" (\"nonnumquam\") means \"sometimes\", in many Romance languages a second term indicated a negative is required.\n\nIn French, the usual way to express negation is to employ two negatives, e.g. \"ne [verb] pas\", \"ne [verb] plus\", or \"ne [verb] jamais\", as in the sentences \"Je ne sais pas\" (\"I do not know\"), \"Il n'y a plus de baguettes\" (\"There aren't any more baguettes\"), and \"On ne sait jamais\" (\"one never knows\"). The second term was originally an emphatic; \"pas\", for example, derives from the Latin \"passus\", meaning \"step\", so that French \"Je ne marche pas\" and Catalan \"No camino pas\" originally meant \"I will not walk a single step.\" This initial usage spread so thoroughly that it became a necessary element of any negation in the modern French language and that, in fact, in contemporary French, the original actual negative \"ne\" is mostly left away in favour of \"pas\", as in \"Je sais pas\" \"I don't know\". In Northern Catalan, \"no\" may be omitted in colloquial language, and Occitan, which uses \"non\" only as a short answer to questions. In Venetian, the double negation \"no ... mìa\" can likewise lose the first particle and rely only on the second: \"magno mìa\" (\"I eat not\") and \"vegno mìa\" (\"I come not\"). These exemplify Jespersen's cycle.\n\nItalian, Portuguese and Romanian languages usually employ doubled negative correlatives. Portuguese \"Não vejo nada\", Romanian \"Nu văd nimic\" and Italian \"Non vedo niente\" (\"I do not see nothing\") are used to express \"No, I do not see anything\". In Italian, a second following negative particle \"non\" turns the phrase into a positive one, but with a slightly different meaning. For instance, while both \"Voglio mangiare\" (\"I want to eat\") and \"Non voglio non mangiare\" (\"I don't want not to eat\") mean \"I want to eat\", the latter phrase more precisely means \"I'd prefer to eat\".\n\nOther Romance languages employ double negatives less regularly. In Asturian, an extra negative particle is used with negative adverbs: \"Yo nunca nun lu viera\" (\"I had not never seen him\") means \"I have never seen him\" and \"A mi tampoco nun me presta\" (\"I neither do not like it\") means \"I do not like it either\". Standard Catalan and Galician also used to possess a tendency to double \"no\" with other negatives, so \"Jo tampoc no l'he vista\" or \"Eu tampouco non a vira\", respectively (\"I neither have not seen her\") meant \"I have not seen her either\". That practice is dying out.\n\nIn 1974, Italy held a referendum on whether to repeal a recent law that allowed divorce. Voters were said to have been confused in that in order to support divorce, they needed to vote 'no' on the referendum which was worded so that 'yes' would support repeal. And to reframe the fundamental underlying issue as being support/non-support of the continuation of marriage, then the vote was structured as a triple negative (with divorce as the negation of the continuation of marriage being the first negative). This referendum was defeated, and without this confusion, it was said that it would have been defeated more decisively.\n\nIn spoken Welsh, the word ddim (not) often occurs with a prefixed or mutated verb form that is negative in meaning: \"Dydy hi ddim yma\" (word-for-word, \"Not-is she not here\") expresses \"She is not here\" and \"Chaiff Aled ddim mynd\" (word-for-word, \"Not-will-get Aled not go\") expresses \"Aled is not allowed to go\".\n\nNegative correlatives can also occur with already negative verb forms. In literary Welsh, the mutated verb form is caused by an initial negative particle, ni or nid. The particle is usually omitted in speech but the mutation remains: \"[Ni] wyddai neb\" (word-for-word, \"[Not] not-knew nobody\") means \"Nobody knew\" and \"[Ni] chaiff Aled fawr o bres\" (word-for-word, \"[Not] not-will-get Aled lots of money\") means \"Aled will not get much money\". This is not usually regarded as three negative markers, however, because the negative mutation is really just an effect of the initial particle on the following word.\n\nDoubled negatives are perfectly correct in Ancient Greek. With few exceptions, a simple negative (οὐ or μή) following another negative (for example, οὐδείς, \"no one\") results in an affirmation: οὐδείς οὐκ ἔπασχε τι (\"No one was not suffering\") means more simply \"Everyone was suffering\". Meanwhile, a compound negative following a negative strengthens the negation: μὴ θορυβήσῃ μηδείς (\"Do not permit no one to raise an uproar\") means \"Let not a single one among them raise an uproar\".\n\nThose constructions apply only when the negatives all refer to the same word or expression. Otherwise, the negatives simply work independently of one another: οὐ διὰ τὸ μὴ ἀκοντίζειν οὐκ ἔβαλον αὐτόν means \"It was not on account of their not throwing that they did not hit him\", and one should not blame them for not trying.\n\nIn Modern Greek, negative concord is standard and more commonly used. For example, the sentence 'You (pl.) will not find anything' can be said in two ways: 'Δε θα βρείτε τίποτα' ('Not will find nothing') is more common than 'Δε θα βρείτε κάτι' ('Not will find something'). It depends simply on the mood of the speaker, and the latter being is considered slightly more polite. An exception to that rule is the (archaic) pronoun ουδείς, also meaning \"no one\", which does not allow negation of the verb that it governs.\n\nIn Slavic languages other than Slavonic, multiple negatives are grammatically correct ways to express negation, and a single negative is often incorrect. In complex sentences, every part that could be grammatically negated should be negative. For example, in the Serbo-Croatian, \"Ni(t)ko nikad(a) nigd(j)e ništa nije uradio\" (\"Nobody never did not do nothing nowhere\") means \"Nobody has ever done anything, anywhere\", and \"Nikad nisam tamo išao/išla\" (\"Never I did not go there\") means \"I have never been there\". In Czech it is also common to use three or more negations. For example, \"Nikdy jsem nikde nikoho neviděl\" (\"I have not never seen no one nowhere\"). In Russian, \"I know nothing\" is я ничего не знаю (\"ya nichevo nye znayu\"), lit. \"I nothing don't know.\"\n\nA single negation, while syntactically correct, may result in a very unusual meaning or make no sense at all. Saying \"I saw nobody\" in Polish (\"Widziałem nikogo\") instead of the more usual \"I did not see nobody\" (\"Nikogo nie widziałem\") might mean \"I saw an instance of nobody\" or \"I saw Mr. Nobody\" but it would not have its plain English meaning. Likewise, in Slovenian, saying \"I do not know anyone\" (') in place of \"I do not know no one\" (') has the connotation \"I do not know just \"anyone\"\": I know someone important or special.\n\nAs with most synthetic \"satem\" languages double negative is mandatory in Latvian and Lithuanian. Furthermore, all verbs and indefinite pronouns in a given statement must be negated, so it could be said that multiple negative is mandatory in Latvian.\n\nFor instance, a statement \"I have not ever owed anything to anyone\" would be rendered as \"es nekad nevienam neko neesmu bijis parādā\". The only alternative would be using a negating subordinate clause and subjunctive in the main clause, which could be approximated in English as \"there has not ever been an instance that I would have owed anything to anyone\" (\"nav bijis tā, ka es kādreiz būtu kādam bijis kaut ko parādā\"), where negative pronouns (\"nekad, neviens, nekas\") are replaced by indefinite pronouns (\"kādreiz, kāds, kaut kas\") more in line with the English \"ever, any\" indefinite pronoun structures.\n\nDouble or multiple negatives are grammatically required in Hungarian with negative pronouns: \"Nincs semmim\" (word for word: \"[doesn't-exists] [nothing-of-mine]\", and translates literally as \"I do not have nothing\") means \"I do not have anything\". Negative pronouns are constructed by means of adding the prefixes \"se-,\" \"sem-,\" and \"sen-\" to interrogative pronouns.\n\nSomething superficially resembling double negation is required also in Finnish, which uses the auxiliary verb \"ei\" to express negation. Negative pronouns are constructed by adding one of the suffixes \"-an,\" \"-än,\" \"-kaan,\" or \"-kään\" to interrogative pronouns: \"Kukaan ei soittanut minulle\" means \"No one called me\". These suffices are, however, never used alone, but always in connection with \"ei\". This phenomenon is commonplace in Finnish, where many words have alternatives that are required in negative expressions, for example \"edes\" for \"jopa\" (\"even\"), as in \"jopa niin paljon\" meaning \"even so much\", and \"ei edes niin paljoa\" meaning \"not even so much\".\n\nNegative verb forms are grammatically required in Turkish phrases with negative pronouns or adverbs that impart a negative meaning on the whole phrase. For example, \"Hiçbir şeyim yok\" (literally, word for word, \"Not-one thing-of-mine exists-not\") means \"I don't have anything\". Likewise, \"Asla memnun değilim\" (literally, \"Never satisfied not-I-am\") means \"I'm never satisfied\".\n\nJapanese employs litotes to phrase ideas in a more indirect and polite manner. Thus, one can indicate necessity by emphasizing that not doing something would not be proper. For instance, しなければならない (\"shinakereba naranai\", \"must\") literally means \"not doing [it] would not be proper\". しなければいけません (\"shinakereba ikemasen\", also \"must\") similarly means \"not doing [it] cannot go forward\".\n\nOf course, indirectness can also be employed to put an edge on one's rudeness as well. \"He has studied Japanese, so he should be able to write kanji\" can be phrased 彼は日本語を勉強したから漢字で書けないわけがありません (\"kare wa nihongo o benkyō shita kara kanji de kakenai wake ga arimasen\"), there is a rather harsher idea: \"As he has studied Japanese, the reasoning that he cannot write Kanji does not exist\".\n\nMandarin Chinese also employs litotes in a like manner. One common construction is 不得不 (Pinyin: \"bùdébù\", \"cannot not\"), which is used to express (or feign) a necessity more regretful and polite than that expressed by 必须 (\"bìxū\", \"must\"). Compared with \"我必须走\" (\"Wǒ bìxū zǒu\", \"I must go\"), \"我不得不走\" (\"Wǒ bùdébù zǒu\", \"I cannot not go\") tries to emphasize that the situation is out of the speaker's hands and that the speaker has no choice in the matter: \"Unfortunately, I have got to go\". Similarly, \"没有人不知道\" (\"Méiyǒu rén bù zhīdào\", \"There is not a person who does not know\") is a more emphatic way to express \"Everyone knows\".\n\nDouble negatives nearly always resolve to a positive meaning even in colloquial speech, while triple negatives resolve to a negative meaning. For example, \"我不相信没人不来\" (\"Wǒ bù xiāngxìn méi rén bù lái\", \"I do not believe no one will not come\") means \"I do not think everyone will come\". However, triple or multiple negatives are considered obscure and are typically avoided.\n\nMany languages, including all living Germanic languages, French, Welsh and some Berber and Arabic dialects, have gone through a process known as Jespersen's cycle, where an original negative particle is replaced by another, passing through an intermediate stage employing two particles (e.g. Old French \"jeo ne dis\" → Modern Standard French \"je ne dis pas\" → Modern Colloquial French \"je dis pas\" \"I don't say\").\n\nIn many cases, the original sense of the new negative particle is not negative \"per se\" (thus in French \"pas\" \"step\", originally \"not a step\" = \"not a bit\"), but in Germanic languages, such as English and German the intermediate stage was a case of double negation, as the current negatives \"not\" and \"nicht\" in these languages originally meant \"nothing\": e.g. Old English \"ic ne seah\" \"I didn't see\" » Middle English \"I ne saugh nawiht\", lit. \"I didn't see nothing\" » Early Modern English \"I saw not\".\n\nA similar development to a circumfix from double negation can be seen in non-Indo-European languages, too: for example, in Maltese, \"kiel\" \"he ate\" is negated as \"ma kielx\" \"he did not eat\", where the verb is preceded by a negative particle \"ma\"- \"not\" and followed by the particle -\"x\", which was originally a shortened form of \"xejn\" \"nothing\" - thus, \"he didn't eat nothing\".\n\n"}
{"id": "2427526", "url": "https://en.wikipedia.org/wiki?curid=2427526", "title": "Euler diagram", "text": "Euler diagram\n\nThe first use of \"Eulerian circles\" is commonly attributed to Swiss mathematician Leonhard Euler (1707–1783). In the United States, both Venn and Euler diagrams were incorporated as part of instruction in set theory as part of the new math movement of the 1960s. Since then, they have also been adopted by other curriculum fields such as reading as well as organizations and businesses.\n\nEuler diagrams consist of simple closed shapes in a two dimensional plane that each depict a set or category. How or if these shapes overlap demonstrates the relationships between the sets. There are only 3 possible relationships between any 2 sets; completely inclusive, partially inclusive, and exclusive. This is also referred to as containment, overlap or neither or, especially in mathematics, it may be referred to as subset, intersection and disjoint.\n\nEach Euler curve divides the plane into two regions or \"zones\": the interior, which symbolically represents the elements of the set, and the exterior, which represents all elements that are not members of the set. Curves whose interior zones do not intersect represent disjoint sets. Two curves whose interior zones intersect represent sets that have common elements; the zone inside both curves represents the set of elements common to both sets (the intersection of the sets). A curve that is contained completely within the interior zone of another represents a subset of it.\nVenn diagrams are a more restrictive form of Euler diagrams. A Venn diagram must contain all 2 logically possible zones of overlap between its \"n\" curves, representing all combinations of inclusion/exclusion of its constituent sets. Regions not part of the set are indicated by coloring them black, in contrast to Euler diagrams, where membership in the set is indicated by overlap as well as color. When the number of sets grows beyond 3 a Venn diagram becomes visually complex, especially compared to the corresponding Euler diagram. The difference between Euler and Venn diagrams can be seen in the following example. Take the three sets:\n\n\nThe Euler and the Venn diagrams of those sets are:\n\nIn a logical setting, one can use model theoretic semantics to interpret Euler diagrams, within a universe of discourse. In the examples below, the Euler diagram depicts that the sets \"Animal\" and \"Mineral\" are disjoint since the corresponding curves are disjoint, and also that the set \"Four Legs\" is a subset of the set of \"Animal\"s. The Venn diagram, which uses the same categories of \"Animal\", \"Mineral\", and \"Four Legs\", does not encapsulate these relationships. Traditionally the \"emptiness\" of a set in Venn diagrams is depicted by shading in the region. Euler diagrams represent \"emptiness\" either by shading or by the absence of a region.\n\nOften a set of well-formedness conditions are imposed; these are topological or geometric constraints imposed on the structure of the diagram. For example, connectedness of zones might be enforced, or concurrency of curves or multiple points might be banned, as might tangential intersection of curves. In the adjacent diagram, examples of small Venn diagrams are transformed into Euler diagrams by sequences of transformations; some of the intermediate diagrams have concurrency of curves. However, this sort of transformation of a Venn diagram with shading into an Euler diagram without shading is not always possible. There are examples of Euler diagrams with 9 sets that are not drawable using simple closed curves without the creation of unwanted zones since they would have to have non-planar dual graphs.\n\nAs shown in the illustration to the right, Sir William Hamilton in his posthumously published \"Lectures on Metaphysics and Logic\" (1858–60) erroneously asserts that the original use of circles to \"sensualize ... the abstractions of Logic\" (p. 180) was not Leonhard Paul Euler (1707–1783) but rather Christian Weise (1642–1708) in his \"Nucleus Logicae Weisianae\" that appeared in 1712 posthumously, however, the latter book was actually written by Johann Christian Lange rather than Weise. He references Euler's \"Letters to a German Princess\" [Partie ii., Lettre XXXV., ed. Cournot. – ED.]\n\nIn Hamilton's illustration the four categorical propositions that can occur in a syllogism as symbolized by the drawings A, E, I and O are:\n\nIn his 1881 \"Symbolic Logic\" Chapter V \"Diagrammatic Representation\", John Venn (1834–1923) comments on the remarkable prevalence of the Euler diagram:\n\nBut nevertheless, he contended, \"the inapplicability of this scheme for the purposes of a really general Logic\" (page 100) and on page 101 observed that, \"It fits in but badly even with the four propositions of the common Logic to which it is normally applied.\" Venn ends his chapter with the observation illustrated in the examples below—that their use is based on practice and intuition, not on a strict algorithmic practice:\n\nFinally, in his Chapter XX HISTORIC NOTES Venn gets to a crucial criticism (italicized in the quote below); observe in Hamilton's illustration that the O (\"Particular Negative\") and I (\"Particular Affirmative\") are simply rotated:\n\n(Sandifer 2003 reports that Euler makes such observations too; Euler reports that his figure 45 (a simple intersection of two circles) has 4 different interpretations). Whatever the case, armed with these observations and criticisms, Venn then demonstrates (pp. 100–125) how he derived what has become known as his Venn diagrams from the \"...old-fashioned Euler diagrams.\" In particular he gives an example, shown on the left.\n\nBy 1914, Louis Couturat (1868–1914) had labeled the terms as shown on the drawing on the right. Moreover, he had labeled the \"exterior region\" (shown as a'b'c') as well. He succinctly explains how to use the diagram – one must \"strike out\" the regions that are to vanish:\n\nGiven the Venn's assignments, then, the unshaded areas \"inside\" the circles can be summed to yield the following equation for Venn's example:\n\nIn Venn the 0th term, x'y'z', i.e. the background surrounding the circles, does not appear. Nowhere is it discussed or labeled, but Couturat corrects this in his drawing. The correct equation must include this unshaded area shown in boldface:\n\nIn modern usage the Venn diagram includes a \"box\" that surrounds all the circles; this is called the universe of discourse or the domain of discourse.\n\nCouturat now observes that, in a direct algorithmic (formal, systematic) manner, one cannot derive reduced Boolean equations, nor does it show how to arrive at the conclusion \"No X is Z\". Couturat concluded that the process \"has ... serious inconveniences as a method for solving logical problems\": \n\nThus the matter would rest until 1952 when Maurice Karnaugh (1924– ) would adapt and expand a method proposed by Edward W. Veitch; this work would rely on the truth table method precisely defined in Emil Post's 1921 PhD thesis \"Introduction to a general theory of elementary propositions\" and the application of propositional logic to switching logic by (among others) Claude Shannon, George Stibitz, and Alan Turing. For example, in chapter \"Boolean Algebra\" Hill and Peterson (1968, 1964) present sections 4.5ff \"Set Theory as an Example of Boolean Algebra\" and in it they present the Venn diagram with shading and all. They give examples of Venn diagrams to solve example switching-circuit problems, but end up with this statement:\nIn Chapter 6, section 6.4 \"Karnaugh Map Representation of Boolean Functions\" they begin with:\n\nThe history of Karnaugh's development of his \"chart\" or \"map\" method is obscure. Karnaugh in his 1953 referenced Veitch 1951, Veitch referenced Claude E. Shannon 1938 (essentially Shannon's Master's thesis at M.I.T.), and Shannon in turn referenced, among other authors of logic texts, Couturat 1914. In Veitch's method the variables are arranged in a rectangle or square; as described in Karnaugh map, Karnaugh in his method changed the order of the variables to correspond to what has become known as (the vertices of) a hypercube.\n\nThis example shows the Euler and Venn diagrams and Karnaugh map deriving and verifying the deduction \"No \"X\"s are \"Z\"s\".\nIn the illustration and table the following logical symbols are used:\n\nGiven a proposed conclusion such as \"No \"X\" is a \"Z\"\", one can test whether or not it is a correct deduction by use of a truth table. The easiest method is put the starting formula on the left (abbreviate it as \"P\") and put the (possible) deduction on the right (abbreviate it as \"Q\") and connect the two with logical implication i.e. \"P\" → \"Q\", read as IF \"P\" THEN \"Q\". If the evaluation of the truth table produces all 1s under the implication-sign (→, the so-called \"major connective\") then \"P\" → \"Q\" is a tautology. Given this fact, one can \"detach\" the formula on the right (abbreviated as \"Q\") in the manner described below the truth table.\n\nGiven the example above, the formula for the Euler and Venn diagrams is:\nAnd the proposed deduction is:\n\nSo now the formula to be evaluated can be abbreviated to:\nAt this point the above implication \"P\" → \"Q\" (i.e. ~(y & z) & (x → y) ) → ~(x & z) ) is still a formula, and the deduction – the \"detachment\" of \"Q\" out of \"P\" → \"Q\" – has not occurred. But given the demonstration that \"P\" → \"Q\" is tautology, the stage is now set for the use of the procedure of modus ponens to \"detach\" Q: \"No \"X\"s are \"Z\"s\" and dispense with the terms on the left. \n\"Modus ponens\" (or \"the fundamental rule of inference\") is often written as follows: The two terms on the left, \"P\" → \"Q\" and \"P\", are called \"premises\" (by convention linked by a comma), the symbol ⊢ means \"yields\" (in the sense of logical deduction), and the term on the right is called the \"conclusion\": \n\nFor the modus ponens to succeed, both premises P → Q and P must be \"true\". Because, as demonstrated above the premise \"P\" → \"Q\" is a tautology, \"truth\" is always the case no matter how x, y and z are valued, but \"truth\" will only be the case for \"P\" in those circumstances when \"P\" evaluates as \"true\" (e.g. rows OR OR OR : x'y'z' + x'y'z + x'yz' + xyz' = x'y' + yz'). \n\nOne is now free to \"detach\" the conclusion \"No \"X\"s are \"Z\"s\", perhaps to use it in a subsequent deduction (or as a topic of conversation).\n\nThe use of tautological implication means that other possible deductions exist besides \"No \"X\"s are \"Z\"s\"; the criterion for a successful deduction is that the 1s under the sub-major connective on the right \"include\" all the 1s under the sub-major connective on the left (the \"major\" connective being the implication that results in the tautology). For example, in the truth table, on the right side of the implication (→, the major connective symbol) the bold-face column under the sub-major connective symbol \" ~ \" has the all the same 1s that appear in the bold-faced column under the left-side sub-major connective & (rows , , and ), plus two more (rows and ).\n\n<br><br><br><br><br><br><br><br>\n\n\nBy date of publishing:\n\n\n"}
{"id": "56913918", "url": "https://en.wikipedia.org/wiki?curid=56913918", "title": "Evolution of Cognition", "text": "Evolution of Cognition\n\nEvolution of cognition is the idea that life on earth has gone from organisms with little to no cognitive function to a greatly varying display of cognitive function that we see in organisms today. Animal cognition is largely studied by observing behavior, which makes studying extinct species difficult. The definition of cognition varies by discipline; psychologists tend define cognition by human behaviors, while ethologists have widely varying definitions. Ethological definitions of cognition range from only considering cognition in animals to be behaviors exhibited in humans, while others consider anything action involving a nervous system to be cognitive.\n\nStudying the evolution of cognition is accomplished through a comparative cognitive approach where a cognitive ability and comparing it between closely related species and distantly related species. For example, a researcher may want to analyze the connection between spatial memory and food caching behavior. By examining two closely related animals (chickadees and jays) and/or two distantly related animals (jays and chipmunks), hypotheses could be generated about when and how this cognitive ability evolved.\n\nHigher cognitive processes have evolved in many closely and distantly related animals. Some of these examples are considered convergent evolution, while others most likely shared a common ancestor that possessed higher cognitive function. For example, apes humans, and cetaceans most likely had a common ancestor with high levels of cognition, and as these species diverged they all possessed this trait. Corvids (the crow family) and apes show similar cognitive abilities in some areas such as tool use. This ability is most likely an example of convergent evolution, due to their distant relatedness. \n\nSocial living is thought to have co-evolved with higher cognitive processes. It is hypothesized that higher cognitive function evolved to mitigate the negative effects of living in social groups. For example, the ability to recognize individual groups members could solve the problem of cheating behavior. If individuals within the group can keep track of the cheaters, then they can punish or exclude them from the group. There is also a positive correlation between relative brain size and aspects of sociality in some species There are many benefits to living in social groups such as division of labor and protect, but in order to reap these benefits the animals tend to possess high levels of cognition.\n\nMany animals have complex mating rituals require higher levels of cognition to evaluate. Birds are well known for their intense mating displays including swan dances that can last hours or even days.\n\nHigher levels of cognition may have evolved to facilitate the formation of longer lasting relationships. Animals that form pair bonds and share parental responsibilities produce offspring that are more likely to survive and reproduce, which increases the fitness of these individuals. The cognitive requirements for this type of mating include the ability the differentiate individuals from their group and resolve social conflicts.\n\nAnother hypothesis for the evolution of cognition is that cognition allowed individuals access to food and resources that were previously unavailable. For example, the genetic mutation for color vision allowed for a greatly increased efficiency in finding and foraging fruit. Food caching behavior displayed in some birds and mammals is an example of a behavior that may have co-evolved with higher cognitive processes. This ability to store food for later consumption allows these animals to take advantage of temporary surpluses in food availability. Corvids have displayed incredible abilities to create and remember the locations of up to hundreds of caches. In addition, there is evidence that this is not just an instinctual behavior, but an example of future planning. Jays have been found to diversify the types of food they cache, possible indicating they understand the need to eat a variety of food. Some supporters of this hypothesis suggest that higher cognitive processes require a large brain to body ratio. This higher brain to body size ratio in turn requires a large metabolic input to function. The idea is that the two processes (greater access to food and the brain's growing need for energy) may have snowballed the evolution of these two features.\n\nThe cognitive ability to use tools and pass information from one generation to the next is thought to have been a driving force of the evolution of cognition. Many animals use tools including: primates, elephants, cetaceans, birds, fish, and some invertebrates. Tool use varies widely depending on the species. For example, sea otters have been observed using a rock to break open snail shells, while primates and New Caledonian crows have demonstrated an ability to fashion a new tool for a specific use. The ability to use tools seems to provide animals with a fitness advantage, usually in the form of access to food previously unavailable, which allows a competitive advantage for these individuals.\n\nSome animals have demonstrated the ability to pass information from one generation to the next (culture) including: primates, cetaceans, and birds. Primates and birds can pass information of specific tool use strategies on to their offspring who can, in turn, pass it on to their offspring. In this way, the information can remain in a group on individuals even after the original users are gone. One famous example of this is in a group of macaque monkeys in Japan. Researchers studying this species observed these monkeys feeding behavior in a population in Japan. The researchers witnessed one female, named Imo, realize that by washing potatoes in the nearby river you could remove much more sand and dirt then by simply wiping it off. Over the next few generations the researcher saw this behavior begin to appear in other individuals throughout the group.\n\n"}
{"id": "59104434", "url": "https://en.wikipedia.org/wiki?curid=59104434", "title": "Fair division experiments", "text": "Fair division experiments\n\nFair division has mostly been researched theoretically. But in addition to theory, various experiments have been made for checking various fair division procedures.\n\n1. Flood describes a division of a gift containing 5 parcels: whiskey, prunes, eggs, suitcase, etc. The division was done using the Steinhaus-Banach-Knaster procedure. The resulting division was fair, but in retrospect it was found that coalitions could gain from manipulation.\n\n2. When Mary Anna Lee Paine Winsor died at the age of 93, her estate included two trunks of silver, that had to be divided among her 8 grandchildren. It was divided using a decentralized, fair and efficient allocation procedure, which combined market equilibrium and a Vickrey auction. Although most participants did not fully understand the algorithm or the preference information desired, it handled the major considerations well and was regarded as equitable.\n\nIn California, the law says that public school classrooms should be shared fairly among all public school pupils, including those in charter schools. Schools have dichotomous preferences: each school demands a certain number of classes, it is happy if it got all of them and unhappy otherwise. A new algorithm, devloped by , allocates classrooms to schools using a non-trivial implementation of the \"randomized leximin mechanism.\" Unfortunately it was not deployed in practice, but it was tested using computer simulations based on real school data. While the problem is computationally-hard, simulations show that the implementation scales gracefully in terms of running time: even when there are 300 charter schools, it terminates in a few minutes on average. Moreover, while theoretically the algorithm guarantees only 1/4 of the maximum number of allocated classrooms, in the simulations it satisfies on average at least 98% of the maximum number of charter schools that can possibly be satisfied, and allocates on average at least 98% of the maximum number of classrooms that can possibly be allocated. \n\nThe partial collaboration with the school district lead to several practical desiderata in deploying fair division solutions in practice. First, the simplicity of the mechanism, and the intuitiveness of the properties of proportionality, envy-freeness, Pareto optimality, and strategyproofness, have made the approach more likely to be adopted. On the other hand, the use of randomization, though absolutely necessary in order to guarantee fairness in allocating indivisible goods such as classrooms, has been a somewhat harder sell: the term \"lottery\" raised negative connotations and legal objections.\n\nThe adjusted winner procedure is a protocol for simultaneously resolving several issues under conflict, such that the agreement is envy-free, equitable, and Pareto efficient. It has been commercialized through the FairOutcomes website. While there are no account of it actually being used to resolve disputes, there are several counterfactual studies checking what would have been the results of using this procedure to solve international disputes: \n\n\nRental harmony is the problem of simultaneously allocating rooms in an apartment and the rent of the apartment among the housemates. It has several solutions. Some of these solutions were implemented in the Spliddit.org website and tested on real users.\n\nWhen different agents cooperate, there is an economic surplus in welfare. Cooperative game theory studies the question of how this surplus should be allocated, taking into account the various coalitional options of the players. Several cases of such cooperation has been studied, in light of concepts such as the Shapley value.\n\nFlood analyzed several cases of bargaining between a buyer and a seller on the price of purchasing a good (e.g. a car). He found that the \"split-the-difference\" principle was acceptable by both participants. The same cooperative principle was found in more abstract non-cooperative games. However, in some cases, bidders in an auction did not find a cooperative sollution.\n\nWalsh developed several algorithms for online fair cake-cutting. He tested them using a computerized simulation: valuation functions for each agent were generated by dividing the cake into random segments, and assigning a random value to each segment, normalizing the total value of the cake. The egalitarian welfare and the utilitarian welfare of various algorithms were compared.\n\nCavallo developed an improvemeot of the Vickrey–Clarke–Groves mechanism in which money is redistributed in order to increase social welfare. He tested his mechanism using simulations. He generated piecewise-constant valuation functions, whose constants were selected at random from the uniform distribution. He also tried Gaussian distributions and got similar results.\n\nIt is known that, when items are indivisible, fair allocations might not exist. However, this is a worst-case scenario; in realistic settings, fair allocations often do exist. Computerized simulations show that, when the number of goods is larger than the number of agents by a logarithmic factor, fair allocations often exist.\n\nMoreover, computerized simulations show that, in many cases, there exist allocations that are \"necessarily fair\" based on a certain convexity assumption on the agents' preferences.\n\nSeveral experiments were conducted with people, in order to find out what is the relative importance of several desiderata in choosing an allocation.\n\nSometimes, there are only two possible allocations: one is fair (e.g. envy-free division) but inefficient, while the other is efficient (e.g. Pareto-optimal) but unfair. Which division do people prefer? This was tested in several lab experiments. \n\n1. Subjects were given several possible allocations of money, and were asked which allocation they prefer. One experiment found that the most important factors were Pareto-efficiency and Rawlsian motive for helping the poor (maximin principle). However, a later experiment found that these conclusions only hold for students of economics and business, who train to acknowledge the importance of efficiency. In the general population, the most important factors are selfishness and inequality aversion. \n\n2. Subjects were asked to answer questionnaires regarding the division of indivisible items between two people. The subjects were shown the subjective value that each (virtual) person attaches to each item. The predominant aspect considered was equity - satisfying each individual's preferences. The efficiency aspect was secondary. This effect was slightly more pronounced in economics students, and less pronounced in law students (who chose a Pareto-efficient allocation more frequently). \n\n3. Subjects were divided into pairs and asked to negotiate and decide how to divide a set of 4 items between them. Each combination of items had a pre-specified monetary value, which was \"different\" between the two subjects. Each subject knew both his own values and the partner's values. After the division, each subject could redeem the items for their monetary value. The items could be divided in several ways: some divisions were equitable (e.g., giving each partner a value of 45), while other divisions were Pareto efficient (e.g., giving one partner 46 and another partner 75). The interesting question was whether people prefer the equitable or the efficient division. The results showed that people preferred the more efficient division only if it was not \"too unfair\". A difference of 2-3 value units was considered sufficiently small for most subjects, so they preferred the efficient allocation. But a difference of 20-30 units (such as in the 45:45 vs. 46:75 example) was perceived as too large: 51% preferred the 45:45 division. The effect were less pronounced when the subjects were only shown the \"rank\" of the item combinations for each of them, rather than the full monetary value. This experiment also revealed a recurring process which was used during the negotiation: subjects first find the most equitable division of the goods. They take it as a reference point and try to find Pareto improvements. An improvement is implemented only if the inequality it causes is not too large. This process is called CPIES: Conditioned Pareto Improvement from Equal Split.\n\nWhat is the importance of \"intra-personal\" fairness criteria (such as envy-freeness, where each person compares bundles based only on his own utility-function), vs. \"inter-personal\" fairness criteria (such as equitability, where each person views the utilities of all other agents)? Using a free-form bargaining experiment, it was found that inter-personal fairness (e.g. equitability) is more important. Intra-personal fairness (such as envy-freeness) are relevant only as a secondary criterion.\n\nDivide and choose (DC) is a fair and very simple procedure. There are more sophisticated procedures that have better fairness guarantees. The question of which were more satisfactory was tested in several lab experiments.\n\n1. Divide-and-choose vs Knaster-Brams-Taylor. Several pairs of players had to divide among them 3 indivisible goods (a ballpoint pen, a lighter and a mug) and some money. Three procedures were used: the simple DC, and the more complicated \"Adjusted Knaster\" (an improvement of adjusted winner) and \"Proportional Knaster\". The authors asked the subjects to select their favorite procedure. Then, they let them play the procedure in two modes: binding (strict adherence to the protocol rules) and non-binding (possible renegotiation afterwards). They compared the procedures performance in terms of efficiency, envy-freeness, equitability and truthfulness. Their conclusions are: (a) The sophisticated mechanisms are advantageous only in the binding case; when renegotiation is possible, their performance drops to the baseline level of DC. (b) The preference for a procedure depends not only on the expected utility calculations of the negotiators, but also on their psychological profile: the more \"antisocial\" a person is, the more likely he is to opt for a procedure with a compensatory mechanism. The more risk-averse a person is, the more likely he is to opt for a straightforward procedure like DC. (c) The final payoff of a participant in a procedure depends a lot on the implementation. If participants cannot divide the goods under a procedure of their own choice, they are more eager to maximize their payoff. A shortened time horizon is equally detrimental.\n\n2. Structured procedures vs. Genetic algorithms. Two pairs of players had to divide between them 10 indivisible goods. A genetic algorithm was used to search for the best division candidates: out of the 1024 possible divisions, a subset of 20 divisions was shown to the players, and they were asked to grade their satisfaction about the candidate division on a scale ranging from 0 (not satisfied at all) to 1 (fully satisfied). Then, for each subject, a new population of 20 divisions was created using a genetic algorithm. This procedure continued for 15 iterations until a best surviving allocation was found. The results were compared to five provably-fair division algorithms: Sealed Bid Knaster, Adjusted Winner, Adjusted Knaster, Division by Lottery and Descending Demand. Often, the best divisions found by the genetic algorithm were rated as more mutually satisfactory than the ones derived from the algorithms. Two possible reasons for that were: (a) \"Temporal fluctuation of preferences\" - the valuations of humans change from the point they report their valuations to the point they see the final allocation. Most fair division procedures ignore this issue, but the genetic algorithm captures it naturally. (b) \"Non-additivity of preferences\". Most division procedures assume that valuations are additive, but in reality they are not; the genetic algorithm works just as well with non-additive valuations.\n\n3. Simple procedures vs. Strongly-fair procedures. 39 player-pairs were given 6 indivisible gift-certificates of the same value ($10) but from different vendors (e.g. Esso, Starbucks, etc.). Before the procedure, each participant was shown all the 64 possible allocations, and was asked to grade the \"satisfaction\" and \"fairness\" of each of them between 0 (bad) and 100 (good). Then, they were taught seven different procedures, with different levels of fairness guarantees: Strict Alternation and Balanced Alternation (no guarantees), Divide and Choose (only envy-freeness), Compensation Procedure and Price Procedure (envy-freeness and Pareto-efficiency), Adjusted Knaster and Adjusted Winner (envy-freeness, Pareto-efficiency and equitability). They practiced each of these against a computer. Then, they did an actual division against another human subject. After the procedure, they were asked again to grade the satisfaction and fairness of the outcome; the goal was to distinguish procedural fairness from distributional fairness. The results showed that: (a) procedural fairness had no significant impact; satisfaction was mainly determined by distributional fairness. (b) the results of simpler procedures (strict alternation, balanced alternation and DC) were considered fairer and more satisfactory. They explain this couter-intuitive result by showing that humans care about \"object equality\" - giving each agent the same number of objects (though this does not entail any mathematical fairness criterion).\n\nConsider two agents that have to bargain on a deal, such as how to divide goods among them. Often, if they sincerely reveal their preferences, they can attain a win-win deal. However, if they strategically misrepresent their preferences in an attempt to gain, they might actually lose the deal. What negotiation procedure is most efficient in terms of attaining good deals? Several bargaining procedures were studied in the lab.\n\n1. Sealed bid auction: a simple one-shot negotiation procedure. In the lab, information-advantaged players aggressively exploited asymmetric information, and drastically misrepresented their true valuation through strategic bidding. This often resulted in a reduced bargaining zone, forgone deals and low economic efficiency. In one experiment, deals were made on only 52% of all trials, while 77% of all trials had a positive bargaining zone. \n\n2. Bonus procedure: a procedure that gives a bonus was given to participants making a deal. This bonus is calculated such that it is optimal for players to reveval their true preferences. Lab experiments show that this does not help: subjects still strategize, although it is bad for them. \n\n3. Adjusted Winner (AW): a procedure that allocates divisible objects in order to maximize the total utility. In the lab, subjects bargained in pairs over two divisible objects. Each of the two objects was assigned a random value drawn from a commonly known prior distribution. Each player had complete information about their own values, but incomplete information about their co-bargainer’s values. There were three information conditions: (1) Competing Preferences: Players know that the preferences of their co-bargainer are similar to their own; (2) Complementary Preferences: Players know that the preferences of their co-bargainer are diametrically opposed to their own; (3) Unknown (Random) Preferences: Players do not know what their co-bargainer values most relative to their own preferences. In condition (1), the bilateral decisions converge toward efficient outcomes, yet only one-third are \"envy-free\". In condition (2), while players dramatically misrepresent their true valuation for objects, both efficiency and envy-freeness approach maximum levels. In condition (3), pronounced strategic bidding emerges, yet the result is twice as many envy-free outcomes, with increased levels of efficiency (relative to condition 1). In all cases, the structured AW procedure was quite successful in attaining a win-win solution - about 3/2 times more than unstructured negotiation. The key to its success is that it forces players out of the ‘fixed pie myth’.\n\n4. Conflict-resolution algorithm: Hortala-Vallve and lorente-Saguer describe a simple mechanism for solving several issues simultaneously (analogous to Adjusted Winner). They observe that equilibrium play increases over time, and truthful play decreases over time - agents manipulate more often when they learn their partners' preferences. Fortunately, the deviations from equilibrium do not cause much damage to the social welfare - the final welfare is close to the theoretic optimum.\n\nIn the lab, children were paired to \"rich\" and \"poor\" and were asked to share objects. There were differences in the perception of \"initial belongings\" vs. \"things that have to be shared\": young children (up to 7) did not distinguish them while older children (above 11) did. \n\n"}
{"id": "17788343", "url": "https://en.wikipedia.org/wiki?curid=17788343", "title": "Felicity (given name)", "text": "Felicity (given name)\n\nFelicity is an English feminine given name meaning \"happiness\". It is derived from the Latin word \"felicitas\" meaning \"luck, good fortune\". It is also used as a form of the Latin name Felicitas, taken from the name of the Ancient Roman goddess Fortuna. It was also the name of Saint Felicity of Rome, a 2nd-century saint venerated by the Roman Catholic Church. \"Felicia\", a related name, is a feminine form of the name \"Felix\", which is derived from an Ancient Roman cognomen meaning \"lucky,\" or \"successful.\"\n\nFelicity was the 236th most popular name for girls born in England and Wales in 2007. The name was the 706th most popular name for girls born in the United States in 2007, down from 619th place in 2006. The name was most popular in the United States in 1999, the year after the television show \"Felicity\" debuted. It was the 390th most popular name for girls in 1999, rising from 818th place in 1998, the year it debuted on the list of 1,000 most popular names for girls in the United States. Felicity Merriman is a red-headed Colonial doll produced by the American Girl company. The doll, which has a tie-in book series, movies, and a number of accessories, was introduced in the United States in 1991.\n\n\n\n"}
{"id": "2086749", "url": "https://en.wikipedia.org/wiki?curid=2086749", "title": "Fibred category", "text": "Fibred category\n\nFibred categories (or fibered categories) are abstract entities in mathematics used to provide a general framework for descent theory. They formalise the various situations in geometry and algebra in which \"inverse images\" (or \"pull-backs\") of objects such as vector bundles can be defined. As an example, for each topological space there is the category of vector bundles on the space, and for every continuous map from a topological space \"X\" to another topological space \"Y\" is associated the pullback functor taking bundles on \"Y\" to bundles on \"X\". Fibred categories formalise the system consisting of these categories and inverse image functors. Similar setups appear in various guises in mathematics, in particular in algebraic geometry, which is the context in which fibred categories originally appeared. Fibered categories are used to define stacks, which are fibered categories (over a site) with \"descent\". Fibrations also play an important role in categorical semantics of type theory, and in particular that of dependent type theories.\n\nFibred categories were introduced by , and developed in more detail by .\n\nThere are many examples in topology and geometry where some types of objects are considered to exist \"on\" or \"above\" or \"over\" some underlying \"base space\". The classical examples include vector bundles, principal bundles and sheaves over topological spaces. Another example is given by \"families\" of algebraic varieties parametrised by another variety. Typical to these situations is that to a suitable type of a map \"f\": \"X\" → \"Y\" between base spaces, there is a corresponding \"inverse image\" (also called \"pull-back\") operation \"f\" taking the considered objects defined on \"Y\" to the same type of objects on \"X\". This is indeed the case in the examples above: for example, the inverse image of a vector bundle \"E\" on \"Y\" is a vector bundle \"f\"(\"E\") on \"X\".\n\nMoreover, it is often the case that the considered \"objects on a base space\" form a category, or in other words have maps (morphisms) between them. In such cases the inverse image operation is often compatible with composition of these maps between objects, or in more technical terms is a functor. Again, this is the case in examples listed above.\n\nHowever, it is often the case that if \"g\": \"Y\" → \"Z\" is another map, the inverse image functors are not \"strictly\" compatible with composed maps: if \"z\" is an object \"over\" \"Z\" (a vector bundle, say), it may well be that\n\nInstead, these inverse images are only naturally isomorphic. This introduction of some \"slack\" in the system of inverse images causes some delicate issues to appear, and it is this set-up that fibred categories formalise.\n\nThe main application of fibred categories is in descent theory, concerned with a vast generalisation of \"glueing\" techniques used in topology. In order to support descent theory of sufficient generality to be applied in non-trivial situations in algebraic geometry the definition of fibred categories is quite general and abstract. However, the underlying intuition is quite straightforward when keeping in mind the basic examples discussed above.\n\nThere are two essentially equivalent technical definitions of fibred categories, both of which will be described below. All discussion in this section ignores the set-theoretical issues related to \"large\" categories. The discussion can be made completely rigorous by, for example, restricting attention to small categories or by using universes.\n\nIf φ: \"F\" → \"E\" is a functor between two categories and \"S\" is an object of \"E\", then the subcategory of \"F\" consisting of those objects \"x\" for which φ(\"x\")=\"S\" and those morphisms \"m\" satisfying φ(\"m\")=id, is called the \"fibre category\" (or \"fibre\") \"over S\", and is denoted \"F\". The morphisms of \"F\" are called \"S-morphisms\", and for \"x\",\"y\" objects of \"F\", the set of \"S\"-morphisms is denoted by Hom(\"x\",\"y\"). The image by φ of an object or a morphism in \"F\" is called its \"projection\" (by φ). If f is a morphism of \"E\", then those morphisms of \"F\" that project to \"f\" are called \"f-morphisms\", and the set of \"f\"-morphisms between objects \"x\" and \"y\" in \"F\" is denoted by Hom(\"x\",\"y\"). A functor φ: \"F\" → \"E\" is also called an \"E-category\", or said to make \"F\" into an \"E\"-category or a category \"over\" \"E\". An \"E\"-functor from an \"E\"-category φ: \"F\" → \"E\" to an \"E\"-category ψ: \"G\" → \"E\" is a functor α: \"F\" → \"G\" such that ψ ∘ α = φ. \"E\"-categories form in a natural manner a 2-category, with 1-morphisms being \"E\"-functors, and 2-morphisms being natural transformations between \"E\"-functors whose components lie in some fibre.\n\nA morphism \"m\": \"x\" → \"y\" in \"F\" is called \"φ-cartesian\" (or simply \"cartesian\") if it satisfies the following condition:\nA cartesian morphism \"m\": \"x\" → \"y\" is called an \"inverse image\" of its projection \"f\" = φ(\"m\"); the object \"x\" is called an \"inverse image\" of \"y\" \"by f\".\n\nThe cartesian morphisms of a fibre category \"F\" are precisely the isomorphisms of \"F\". There can in general be more than one cartesian morphism projecting to a given morphism \"f\": \"T\" → \"S\", possibly having different sources; thus there can be more than one inverse image of a given object \"y\" in \"F\" by \"f\". However, it is a direct consequence of the definition that two such inverse images are isomorphic in \"F\".\n\nAn \"E\"-functor between two \"E\"-categories is called a \"cartesian functor\" if it takes cartesian morphisms to cartesian morphisms. Cartesian functors between two \"E\"-categories \"F\",\"G\" form a category Cart(\"F\",\"G\"), with natural transformations as morphisms. A special case is provided by considering \"E\" as an \"E\"-category via the identity functor: then a cartesian functor from \"E\" to an \"E\"-category \"F\" is called a \"cartesian section\". Thus a cartesian section consists of a choice of one object \"x\" in \"F\" for each object \"S\" in \"E\", and for each morphism \"f\": \"T\" → \"S\" a choice of an inverse image \"m\": \"x\" → \"x\". A cartesian section is thus a (strictly) compatible system of inverse images over objects of \"E\". The category of cartesian sections of \"F\" is denoted by\n\nIn the important case where \"E\" has a terminal object \"e\" (thus in particular when \"E\" is a topos or the category \"E\" of arrows with target \"S\" in \"E\") the functor\n\nis fully faithful (Lemma 5.7 of Giraud (1964)).\n\nThe technically most flexible and economical definition of fibred categories is based on the concept of cartesian morphisms. It is equivalent to a definition in terms of \"cleavages\", the latter definition being actually the original one presented in Grothendieck (1959); the definition in terms of cartesian morphisms was introduced in Grothendieck (1971) in 1960–1961.\n\nAn \"E\" category φ: \"F\" → \"E\" is a \"fibred category\" (or a \"fibred E-category\", or a \"category fibred over E\") if each morphism \"f\" of \"E\" whose codomain is in the range of projection has at least one inverse image, and moreover the composition \"m ∘ n\" of any two cartesian morphisms \"m\",\"n\" in \"F\" is always cartesian. In other words, an \"E\"-category is a fibred category if inverse images always exist (for morphisms whose codomains are in the range of projection) and are \"transitive\".\n\nIf \"E\" has a terminal object \"e\" and if \"F\" is fibred over \"E\", then the functor ε from cartesian sections to \"F\" defined at the end of the previous section is an equivalence of categories and moreover surjective on objects.\n\nIf \"F\" is a fibred \"E\"-category, it is always possible, for each morphism \"f\": \"T\" → \"S\" in \"E\" and each object \"y\" in \"F\", to choose (by using the axiom of choice) precisely one inverse image \"m\": \"x\" → \"y\". The class of morphisms thus selected is called a \"cleavage\" and the selected morphisms are called the \"transport morphisms\" (of the cleavage). A fibred category together with a cleavage is called a \"cleaved category\". A cleavage is called \"normalised\" if the transport morphisms include all identities in \"F\"; this means that the inverse images of identity morphisms are chosen to be identity morphisms. Evidently if a cleavage exists, it can be chosen to be normalised; we shall consider only normalised cleavages below.\n\nThe choice of a (normalised) cleavage for a fibred \"E\"-category \"F\" specifies, for each morphism \"f\": \"T\" → \"S\" in \"E\", a \"functor\" \"f\": \"F\" → \"F\": on objects \"f\" is simply the inverse image by the corresponding transport morphism, and on morphisms it is defined in a natural manner by the defining universal property of cartesian morphisms. The operation which associates to an object \"S\" of \"E\" the fibre category \"F\" and to a morphism \"f\" the \"inverse image functor\" \"f\" is \"almost\" a contravariant functor from \"E\" to the category of categories. However, in general it fails to commute strictly with composition of morphisms. Instead, if \"f\": \"T\" → \"S\" and \"g\": \"U\" → \"T\" are morphisms in \"E\", then there is an isomorphism of functors\nThese isomorphisms satisfy the following two compatibilities:\nIt can be shown (see Grothendieck (1971) section 8) that, inversely, any collection of functors \"f\": \"F\" → \"F\" together with isomorphisms \"c\" satisfying the compatibilities above, defines a cleaved category. These collections of inverse image functors provide a more intuitive view on fibred categories; and indeed, it was in terms of such compatible inverse image functors that fibred categories were introduced in Grothendieck (1959).\n\nThe paper by Gray referred to below makes analogies between these ideas and the notion of fibration of spaces.\n\nThese ideas simplify in the case of groupoids, as shown in the paper of Brown referred to below, which obtains a useful family of exact sequences from a fibration of groupoids.\n\nA (normalised) cleavage such that the composition of two transport morphisms is always a transport morphism is called a \"splitting\", and a fibred category with a splitting is called a \"split\" (fibred) \"category\". In terms of inverse image functors the condition of being a splitting means that the composition of inverse image functors corresponding to composable morphisms \"f,g\" in \"E\" \"equals\" the inverse image functor corresponding to \"f ∘ g\". In other words, the compatibility isomorphisms \"c\" of the previous section are all identities for a split category. Thus split \"E\"-categories correspond exactly to true functors from \"E\" to the category of categories.\n\nUnlike cleavages, not all fibred categories admit splittings. For an example, see below.\n\nOne can invert the direction of arrows in the definitions above to arrive at corresponding concepts of co-cartesian morphisms, co-fibred categories and split co-fibred categories (or co-split categories). More precisely, if φ: \"F\" →\"E\" is a functor, then a morphism \"m\": \"x\" → \"y\" in \"F\" is called \"co-cartesian\" if it is cartesian for the opposite functor φ: \"F\" → \"E\". Then \"m\" is also called a \"direct image\" and \"y\" a direct image of \"x\" for \"f\" = φ(\"m\"). A \"co-fibred\" \"E\"-category is an\"E\"-category such that direct image exists for each morphism in \"E\" and that the composition of direct images is a direct image. A \"co-cleavage\" and a \"co-splitting\" are defined similarly, corresponding to \"direct image functors\" instead of inverse image functors.\n\nThe categories fibred over a fixed category \"E\" form a 2-category Fib(\"E\"), where the \"category\" of morphisms between two fibred categories \"F\" and \"G\" is defined to be the category Cart(\"F\",\"G\") of cartesian functors from \"F\" to \"G\".\n\nSimilarly the split categories over \"E\" form a 2-category Scin(\"E\") (from French \"catégorie scindée\"), where the category of morphisms between two split categories \"F\" and \"G\" is the full sub-category Scin(\"F\",\"G\") of \"E\"-functors from \"F\" to \"G\" consisting of those functors that transform each transport morphism of \"F\" into a transport morphism of \"G\". Each such \"morphism of split E-categories\" is also a morphism of \"E\"-fibred categories, i.e., Scin(\"F\",\"G\") ⊂ Cart(\"F\",\"G\").\n\nThere is a natural forgetful 2-functor \"i\": Scin(\"E\") → Fib(\"E\") that simply forgets the splitting.\n\nWhile not all fibred categories admit a splitting, each fibred category is in fact \"equivalent\" to a split category. Indeed, there are two canonical ways to construct an equivalent split category for a given fibred category \"F\" over \"E\". More precisely, the forgetful 2-functor \"i\": Scin(\"E\") → Fib(\"E\") admits a right 2-adjoint \"S\" and a left 2-adjoint \"L\" (Theorems 2.4.2 and 2.4.4 of Giraud 1971), and \"S\"(\"F\") and \"L\"(\"F\") are the two associated split categories. The adjunction functors \"S\"(\"F\") → \"F\" and \"F\" → \"L\"(\"F\") are both cartesian and equivalences (\"ibid\".). However, while their composition \"S\"(\"F\") → \"L\"(\"F\") is an equivalence (of categories, and indeed of fibred categories), it is \"not\" in general a morphism of split categories. Thus the two constructions differ in general. The two preceding constructions of split categories are used in a critical way in the construction of the stack associated to a fibred category (and in particular stack associated to a pre-stack).\n\n\n\n\n"}
{"id": "152900", "url": "https://en.wikipedia.org/wiki?curid=152900", "title": "Finitism", "text": "Finitism\n\nFinitism is a philosophy of mathematics that accepts the existence only of finite mathematical objects. It is best understood in comparison to the mainstream philosophy of mathematics where infinite mathematical objects (e.g., infinite sets) are accepted as legitimate.\n\nThe main idea of finitistic mathematics is not accepting the existence of infinite objects such as infinite sets. While all natural numbers are accepted as existing, the set of all natural numbers is not considered to exist as a mathematical object. Therefore quantification over infinite domains is not considered meaningful. The mathematical theory often associated with finitism is Thoralf Skolem's primitive recursive arithmetic.\n\nThe introduction of infinite mathematical objects occurred a few centuries ago when the use of infinite objects was already a controversial topic among mathematicians. The issue entered a new phase when Georg Cantor in 1874 introduced what is now called naive set theory and used it as a base for his work on transfinite numbers. When paradoxes such as Russell's paradox, Berry's paradox and the Burali-Forti paradox were discovered in Cantor's naive set theory, the issue became a heated topic among mathematicians.\n\nThere were various positions taken by mathematicians. All agreed about finite mathematical objects such as natural numbers. However there were disagreements regarding infinite mathematical objects. \nOne position was the intuitionistic mathematics that was advocated by L. E. J. Brouwer, which rejected the existence of infinite objects until they are constructed.\n\nAnother position was endorsed by David Hilbert: finite mathematical objects are concrete objects, infinite mathematical objects are ideal objects, and accepting ideal mathematical objects does not cause a problem regarding finite mathematical objects. More formally, Hilbert believed that it is possible to show that any theorem about finite mathematical objects that can be obtained using ideal infinite objects can be also obtained without them. Therefore allowing infinite mathematical objects would not cause a problem regarding finite objects. This led to Hilbert's program of proving consistency of set theory using finitistic means as this would imply that adding ideal mathematical objects is conservative over the finitistic part. Hilbert's views are also associated with formalist philosophy of mathematics. Hilbert's goal of proving the consistency of set theory or even arithmetic through finitistic means turned out to be an impossible task due to Kurt Gödel's incompleteness theorems. However, by Harvey Friedman's grand conjecture most mathematical results should be provable using finitistic means.\n\nHilbert did not give a rigorous explanation of what he considered finitistic and refer to as elementary. However, based on his work with Paul Bernays some experts such as William Tait have argued that the primitive recursive arithmetic can be considered an upper bound on what Hilbert considered finitistic mathematics.\n\nIn the years following Gödel's theorems, as it became clear that there is no hope of proving consistency of mathematics, and with development of axiomatic set theories such as Zermelo–Fraenkel set theory and the lack of any evidence against its consistency, most mathematicians lost interest in the topic. Today most classical mathematicians are considered Platonist and readily use infinite mathematical objects and a set-theoretical universe.\n\nIn her book \"Philosophy of Set Theory\", Mary Tiles characterized those who allow \"potentially infinite\" objects as classical finitists, and those who do not allow potentially infinite objects as strict finitists: for example, a classical finitist would allow statements such as \"every natural number has a successor\" and would accept the meaningfulness of infinite series in the sense of limits of finite partial sums, while a strict finitist would not. Historically, the written history of mathematics was thus classically finitist until Cantor discovered the hierarchy of transfinite cardinals at the end of the 19th century.\n\nLeopold Kronecker remained a strident opponent to Cantor's set theory:\nReuben Goodstein is another proponent of finitism. Some of his work involved building up to analysis from finitist foundations.\n\nAlthough he denied it, much of Ludwig Wittgenstein's writing on mathematics has a strong affinity with finitism.\n\nIf finitists are contrasted with transfinitists (proponents of e.g. Georg Cantor's hierarchy of infinities), then also Aristotle may be characterized as a strict finitist. Aristotle especially promoted the potential infinity as a middle option between strict finitism and actual infinity (the latter being an actualization of something never-ending in nature, in contrast with the Cantorist actual infinity consisting of the transfinite cardinal and ordinal numbers, which have nothing to do with the things in nature):\n\nUltrafinitism (also known as ultraintuitionism) has an even more conservative attitude towards mathematical objects than finitism, and has objections to the existence of finite mathematical objects when they are too large.\n\nTowards the end of the 20th century John Penn Mayberry developed a system of finitary mathematics which he called \"Euclidean Arithmetic\". The most striking tenet of his system is a complete and rigorous rejection of the special foundational status normally accorded to iterative processes, including in particular the construction of the natural numbers by the iteration \"+1\". Consequently Mayberry is in sharp dissent from those who would seek to equate finitary mathematics with Peano Arithmetic or any of its fragments such as primitive recursive arithmetic.\n\n\n"}
{"id": "11728902", "url": "https://en.wikipedia.org/wiki?curid=11728902", "title": "Fixation (psychology)", "text": "Fixation (psychology)\n\n\"Fixation\" () is a concept (in human psychology) that was originated by Sigmund Freud (1905) to denote the persistence of anachronistic sexual traits. The term subsequently came to denote object relationships with attachments to people or things in general persisting from childhood into adult life.\n\nIn \"Three Essays on the Theory of Sexuality\" (1905), Freud distinguished the fixations of the libido on an incestuous object from a fixation upon a specific, partial \"aim\", such as voyeurism.\n\nFreud theorized that some humans may develop psychological fixation due to one or more of the following:\n\nAs Freud's thought developed, so did the range of possible 'fixation points' he saw as significant in producing particular neuroses. However, he continued to view fixation as \"the manifestation of very early linkages – linkages which it is hard to resolve – between instincts and impressions and the objects involved in those impressions\".\n\nPsychoanalytic therapy involved producing a new transference fixation in place of the old one. The new fixation – for example a father-transference onto the analyst – may be very different from the old, but will absorb its energies and enable them eventually to be released for non-fixated purposes.\n\n\nMelanie Klein saw fixation as inherently pathological – a blocking of potential sublimation by way of repression.\n\nErik H. Erikson distinguished fixation to zone – oral or anal, for example – from fixation to mode, such as taking in, as with his instance of the man who \"may eagerly absorb the 'milk of wisdom' where he once desired more tangible fluids from more sensuous containers\". Eric Berne, developed his insight further as part of transactional analysis, suggesting that \"particular games and scripts, and their accompanying physical symptoms, are based in appropriate zones and modes\".\n\nHeinz Kohut saw the grandiose self as a fixation upon a normal childhood stage; while other post-Freudians explored the role of fixation in aggression and criminality.\n\n\n"}
{"id": "15749202", "url": "https://en.wikipedia.org/wiki?curid=15749202", "title": "For Want of a Nail", "text": "For Want of a Nail\n\n\"For Want of a Nail\" is a proverb, having numerous variations over several centuries, reminding that seemingly unimportant acts or omissions can have grave and unforeseen consequences.\n\nThe proverb has come down in many variations over the centuries. It describes a situation in which a failure to anticipate or correct some initially small dysfunction leads by successively more critical stages to an egregious outcome. The rhyme's implied small difference in initial conditions is the lack of a spare horseshoe nail, relative to a condition of its availability. At a more literal level, it expresses the importance of military logistics in warfare.\n\nSuch chains of causality are perceived only in hindsight. No one ever lamented, upon seeing his unshod horse, that the kingdom would eventually fall because of it.\n\nRelated sayings are is \"A stitch, in time, saves nine\" and \"An ounce of prevention is worth a pound of cure\". A somewhat similar idea is referred to in the metaphor known as the camel's nose.\n\n \n\nThe proverb is found in a number of forms, beginning as early as the 13th century:\n\nAlong with the long history of the proverb listed above, it has continued to be referenced since the mid 20th century in modern culture. Examples include:\n\n\n\n\n\n\n\n"}
{"id": "13508973", "url": "https://en.wikipedia.org/wiki?curid=13508973", "title": "Freedom versus license", "text": "Freedom versus license\n\nIn moral and legal philosophy, there exists a distinction between the concepts of freedom and licence. The former deals with the rights of the individual; the latter covers the expressed permission (or lack thereof) for more than one individual to engage in an activity.\n\nAs a result, freedoms usually include rights which are usually recognized (often, not always, in an unconditional manner) by the government (and access to which is theoretically enforced against any and all interferences). Licenses, on the other hand, are distributed to individuals who make use of a specific item, expressing the permission to use the item or service under specified, conditional terms and boundaries of usage.\n\n\n"}
{"id": "881176", "url": "https://en.wikipedia.org/wiki?curid=881176", "title": "Fruit of the poisonous tree", "text": "Fruit of the poisonous tree\n\nFruit of the poisonous tree is a legal metaphor in the United States used to describe evidence that is obtained illegally. The logic of the terminology is that if the source (the \"tree\") of the evidence or evidence itself is tainted, then anything gained (the \"fruit\") from it is tainted as well.\n\nThe doctrine underlying the name was first described in \"Silverthorne Lumber Co. v. United States\", 251 U.S. 385 (1920). The term's first use was by Justice Felix Frankfurter in \"Nardone v. United States\" (1939).\n\nSuch evidence is not generally admissible in court. For example, if a police officer conducted an unconstitutional (Fourth Amendment) search of a home and obtained a key to a train station locker, and evidence of a crime came from the locker, that evidence would most likely be excluded under the fruit of the poisonous tree legal doctrine. The testimony of a witness who is discovered through illegal means would not necessarily be excluded, however, , which allows certain evidence or testimony to be admitted in court if the link between the illegal police conduct and the resulting evidence or testimony is sufficiently attenuated. It is believed that a witness who freely and voluntarily testifies is enough of an independent intervening factor to sufficiently \"attenuate\" the connection between the government's illegal discovery of the witness and the witness's voluntary testimony itself. (\"United States v. Ceccolini\", 435 U.S. 268 (1978))\n\nThe \"fruit of the poisonous tree\" doctrine is an extension of the exclusionary rule, which, subject to some exceptions, prevents evidence obtained in violation of the Fourth Amendment from being admitted in a criminal trial. Like the exclusionary rule, the fruit of the poisonous tree doctrine is intended to deter police from using illegal means to obtain evidence.\n\nThe doctrine is subject to four main exceptions. The tainted evidence is admissible if:\n\nThis doctrine was also used by the European Court of Human Rights in \"Gäfgen v. Germany\". In certain cases continental European countries have similar laws (e.g. in cases of torture), while the doctrine itself is generally not known. Illegally obtained evidence is used by the courts to ensure that the judgment is factually correct, however the person obtaining the illegal evidence typically faces independent consequences.\n\n"}
{"id": "56421869", "url": "https://en.wikipedia.org/wiki?curid=56421869", "title": "Girls of Enghelab Street", "text": "Girls of Enghelab Street\n\nGirls of Enghelab Street (Persian: دختران خیابان انقلاب) is a series of protests against compulsory hijab in Iran. The protests were inspired by Vida Movahed (), an Iranian woman known as the Girl of Enghelab Street (), who stood in the crowd on a utility box in the Enghelab Street (Revolution Street) of Tehran on 28 December 2017, tied her hijab, a white headscarf, to a stick, and waved it to the crowd as a flag. She was arrested on that day and was released temporary on bail a month later, on 28 January 2018. Some people believe that Movahed's action was based on Masih Alinejad's call for White Wednesdays, a protest movement that the presenter at VOA Persian Television started in early 2017. Other women later re-enacted her protest and posted photos of their actions on social media. These women are described as the \"Girls of Enghelab Street\" and \"The Girls Of Revolution Street\" in English sources. Some of the protesters claim that they didn't follow Masih Alinejad's call.\n\nIn the Islamic law of Iran imposed shortly after the 1979 Islamic revolution, article 638 of 5th book of Islamic Penal Code (called Sanctions and deterrent penalties) women who do not wear a hijab may be imprisoned from ten days to two months, and/or required to pay fines from 50,000 up to 500,000 rials. Fines are recalculated in the courts as the value of Iranian rials drops every year since 1979.\n\nArticle 639 of the same book says, two type of people shall be sentenced to one year to 10 years imprisonment, first a person who establishes or directs a place of immorality or prostitution, second a person who facilitates or encourages people to commit immorality or prostitution.\n\nThese are some of the laws under which some protesters were charged.\n\nBefore the Iranian Islamic Revolution of 1979 (during the reign of Mohammad Reza Pahlavi, the last Shah of Iran) the hijab was not compulsory, though some Iranian women during this period wore headscarfs or chador.\n\nAfter the 1979 Islamic revolution, the hijab gradually became compulsory. In 1979, Ruhollah Khomeini announced that women should observe Islamic dress code; His statement sparked demonstrations, which were met by government assurances that the statement was only a recommendation. Hijab was subsequently made mandatory in government and public offices in 1980, and in 1983 it became mandatory for all women.\n\nIn 2018, a government run survey dating back to 2014, was released by President Hassan Rouhani, showing that 49.8% of Iranians were against compulsory or mandatory hijab. The report was released by the Center for Strategic Studies, the research arm of the Iranian President's office, and was titled \"Report of the first hijab special meeting\" on July 2014 in a PDF format.\n\nOn 2 February 2018 a poll conducted by the Center for International and Security Studies at Maryland (CISSM) showed that a few Iranians agreed with \"changing Iran's political system or relaxing strict Islamic law\".\n\nIran is the only country in the world that requires non-Muslim women to wear a headscarf. For example, in January 2018, a Chinese female musician was forcibly veiled in the middle of her concert performance.\n\nOn 28 December 2017, pictures and videos of Movahed waving her scarf went viral through the hashtag \"Where_is_she?\" (, \"Where is the girl of Enghelab Street\" in Persian) on social media. While at first she was unknown, days later, Nasrin Sotoudeh, the human rights activist and lawyer who has also been arrested, found out that the girl is 31 years old and was arrested on the spot with her 19-month-old baby.\n\nOn 28 January 2018, according to Nasrin Sotoudeh, the lawyer investigating the case, Vida Movahed was released; temporary on bail.\n\nOn 29 January 2018, a woman was arrested in Tehran after re-enacting Movahed's protest by standing on the same utility box in Enqelab Street, taking off her white Hijab, and holding it up on a stick. Photos has been posted on social media show that at least three other women re-enacted Movahed's protest in Tehran on 29 January, including one near Ferdowsi Square.\n\nAccording to Nasrin Sotoudeh on 30 January 2018, the second woman who was arrested on 29 January 2018 was Narges Hosseini (); her age is 32.\n\nOn 30 January 2018, several more women, but also men, protested against the compulsory hijab law by re-enacting Movahed's protest. This took place in Tehran, as well as other cities as well including Esfahan and Shiraz.\n\nOn 1st February 2018 the Iranian Police department announced they had arrested 29 women for taking off their hijab.\n\nAccording to Nasrin Sotoudeh, the Iranian lawyer, Narges Hosseini known as the second girl of Enghelab street, who is 32 years old was unable to pay the $135,000 USD bail set by the judge presiding over her case, facing a possible 10 years in prison and up to 74 lashes on charges including openly committing a sinful act.\n\nNew photos and video shared on social media shows another woman re-enacting Movahed's protest on the same street, Enghelab Street (Revolution Street) on 15 February 2018 was identified as Azam Jangravi (), videos shows that the police took her down aggressively. According to her latest Instagram picture, she said that she a part of Iranian women Reformists and Executives of Construction Party and has taken no orders from neither someone from inside nor outside the country, she said she has done that to protest against compulsory hijab.\n\nNarges Hosseini and Azam Jangravi were released from the custody temporary on bail.\n\nAnother female protester named Shaparak Shajarizadeh () was arrested protesting with a white scarf on Wednesday, 21 February 2018 in Gheytarieh street; eyewitnesses said that the police attacked her from behind and took her in custody.\n\nPhotos shared on social media shows that the government was placing an inverted v-shaped iron structure on the utility boxes so as to inhibit anyone standing on top of the boxes. She was sentenced to two years in prison in addition to an 18-year suspended prison term. In addition, she stated that she left Iran.\n\nAnother woman named Maryam Shariatmadari () was protesting compulsory hijab in the afternoon on a utility box; the police asked her to come down and the woman refused and questioned the police what's her crime, \"disturbing the peace\" the police replied. Then, as she was violently ejected by the police, she was injured and her leg was broken.\n\nShaparak Shajarizadeh was beaten up in custody. She was released later temporary on bail.\n\nEyewitnesses said that another women named Hamraz Sadeghi () was protesting compulsory hijab on Saturday, 24 February 2018 when suddenly she was attacked by an unknown security force, her arm was broken and was arrested.\n\nOn July 8, 2018, Iranian teenager Maedeh Hojabri was arrested after she posted videos of herself dancing to Western and Iranian music on her Instagram account without her headscarf. She was among several popular Instagram users, with more than 600,000 followers. Her videos were shared by hundreds of people. Several Iranian women posted videos of themselves dancing to protest her arrest.\n\nOn 27 October 2018, students in Islamic Azad University, Central Tehran Branch protested after a morality police van entered the campus and attempted to arrest several women for improper hijab. Videos showed a student standing in front of the van, attempting to block its exit, which lead to the driver of the van attempting to run her over.\n\nOn 29 October 2018, an Iranian woman stood on the dome of Enghelab square in Tehran, and removed her headscarf in protest to the compulsory hijab. She was arrested minutes later by the police.\n\n\n\n\n\n\n\n"}
{"id": "3424213", "url": "https://en.wikipedia.org/wiki?curid=3424213", "title": "God and gender in Hinduism", "text": "God and gender in Hinduism\n\nIn Hinduism, there are diverse approaches to conceptualizing God and gender. Many Hindus focus upon impersonal Absolute (Brahman) which is genderless. Other Hindu traditions conceive God as androgynous (both female and male), alternatively as either male or female, while cherishing gender henotheism, that is without denying the existence of other Gods in either gender.\n\nThe Shakti tradition conceives of God as a female. Other Bhakti traditions of Hinduism have both male and female gods. In ancient and medieval Indian mythology, each masculine deva of the Hindu pantheon is partnered with a feminine who is often a devi.\n\nMale and female deities are extensively mentioned in the Vedas. The earliest \"mandalas\" (\"Books\"; the authorship of each \"mandala\" is traditionally ascribed to a particular \"rishi\" or that \"rishi\"'s family) of the Rigveda, estimated to have been composed sometime in the 2nd millennium BCE, invoke and praise both gods and goddesses. \"Ushas\" (\"Goddess of Dawns\") is praised in twenty Hymns of Chapters VI.64, VI.65, VII.78 and X.172, with Hymn VI.64.5 declaring goddess Ushas as the one who must be worshipped first.\n\nGoddesses, other than Ushas, mentioned in early Vedic literature include \"Prthivi\" (earth), \"Aditi\" (mother of gods, abundance), \"Sarasvati\" (river, nourishment), \"Vac\" (sound and speech), and \"Nirrti\" (death, destruction). Similarly male gods feature prominently in the Vedas, with \"Indra\" (rain, lightning), \"Agni\" (fire), \"Varuna\" (rta, law), \"Dyaus\" (sky, virility), \"Savitr\" (\"Surya\", sun), and \"Soma\" (drink) some of the most mentioned. The two deities most mentioned in Rigveda are Indra and Agni, both male. Surya is the third most revered god, again a male. Each is mentioned, anywhere rain and fire is evoked. They are profusely praised, with ceremonies and prayers to all gods and goddesses symbolically organized around fire (Agni yajna). The hymns seek strengthening of fire, and it is god Indra who increases the energy of the fire, while god Surya increases his brightness. Max Muller states that, while there are difference in frequency of mentions, gods and goddesses in Rig veda are \"neither superior nor inferior; almost every one is represented as supreme and absolute\".\n\nGross states that ancient and medieval Hindu literature is richly endowed with gods, goddesses and androgynous representations of God. This, states Gross, is in contrast with several monotheistic religions, where God is often synonymous with \"He\" and theism is replete with male anthropomorphisms. In Hinduism, goddess-imagery does not mean loss of male-god, rather the ancient literature presents the two genders as balancing each other and complementary. The Goddesses in Hinduism, states Gross, are strong, beautiful and confident, symbolizing their vitality in cycle of life. While masculine Gods are symbolically represented as those who act, the feminine Goddesses are symbolically portrayed as those who inspire action. Goddesses in Hinduism are envisioned as the patrons of arts, culture, nurture, learning, arts, joys, spirituality and liberation.\n\nGod is not either male or female concept in ancient Indian literature. Androgynous concepts of god are common place as well.\n\nMost major schools of Hindu philosophy focus their philosophical discourse on the Universal Absolute, called Brahman, which is a grammatically genderless noun. This Universal Absolute, states Zimmer, is \"beyond the differentiating qualifications of sex, beyond any and all limitations, individualizing characteristics whatsoever\". The \"Brahman\" is the Great Cosmic Spirit, the Ultimate True Reality, the Supreme Self. It is a transcendental concept that includes all virtues, forms, genders, characteristics, capacities, knowledge and being-ness. The history of the genderless concept of Brahman, as the omnipresent Absolute Spirit and Supreme Self, can be traced back to Vedas, and extensively in the earliest Upanishads, such as hymns 1.4.10 and 4.4.5 of Brihadaranyaka Upanishad, and hymn 6.2.1 of Chandogya Upanishad 6.2.1.\n\nZimmer clarifies the notion of gender in Sanskrit language and its relation to the concepts of Brahman and God in Hinduism, as follows:\n\nHindu mythology incorporates numerous devas (gods) and devis (goddesses). These are symbolic stories that synthesize God and gender, with ideas and values. The Vishnu Purana, for example, recites one such myth describes male gods and female goddesses with names that is loaded with symbolism. An excerpt of the story is as follows,\n\nThe Smarta tradition, which by and large, follows Advaita philosophy believes all forms, male and female, to be different forms of the impersonal Absolute, Brahman which is of neuter gender and can never be defined.\nBrahman is viewed as without personal attributes (Nirguna Brahman) or with attributes (Saguna Brahman, equated with Ishvara) as God. In Advaita Vedanta, Ishvara is Brahman. Thus according to Smarta views, the divine can be with attributes, Saguna Brahman, and also be viewed with whatever attributes, (e.g., a goddess) a devotee conceives.\n\nIn Vaishnavism and Shaivism, God, Vishnu or Shiva respectively, is personified as male. God, however, transcends gender in these sub-schools, and the male form is used as an icon to help focus the Puja (worship). The use of icons is not restricted to male forms. It takes various forms and shapes. The Shaivites and Vaishnavites worship God in non-anthropomorphic, symbolic male-female images as well, such as the linga-yoni and Saligram respectively. In their literature, the principle of God's true nature as sexless is emphasized as in the Vishnu sahasranama.\n\nThus, the first few names, of Vishnu sahasranama, in particular, do not describe features of Vishnu in detail and hence are not anthropomorphic.\n\nShaktism, on the other hand, is a denomination of Hinduism that worships Shakti, or Devi Mata—the Hindu name for the Great Divine Mother—in all of her forms whilst not rejecting the importance of masculine and neuter divinity (which are however deemed to be inactive in the absence of the Shakti). In pure Shaktism, the Great Goddess, or Devi, is worshiped. N. N. Bhattacharyya explained that \"[those] who worship the Supreme Deity exclusively as a Female Principle are called Shakta.\n\nAlternative interpretations of Shaktism, however—primarily those of Shaivite scholars, such as Satguru Sivaya Subramuniyaswami—argue that the feminine manifest is ultimately only the vehicle through which the masculine Un-manifest Parasiva is ultimately reached.\n\nThe common separation of Sakti and Saktiman, i.e. Female and Male principle in god arrives at the conclusion Sakti and Saktiman are the same. Each and every god has its partner, 'better-half' or Sakti and without this Sakti he is sometimes viewed being without essential power. In some Bhakti schools, devotees of Hinduism worship both genders as a God-pair, rather than a specific gender.\n\nFrom the Vaishnava point of view the divine feminine energy (\"Shakti\") implies a divine source of energy, i.e. God as \"shaktiman\". \"Sita relates to Rama; Lakshmi belongs to Narayana; Radha has Her Krishna.\" The female, in these pairs, is viewed as the source of energy and essence of the male.\n\nOne of the prominent features of Vaishnavism in Manipur, for example, is the worship of the two genders together. Devotees do not worship Krishna alone, or Radha alone, but Radha-Krishna. \"Rasa\" and other dances are a feature of the regional folk and religious tradition and often, for example, a female dancer will portray both 'male' Krishna and his consort, Radha, in the same piece.\n\n\n"}
{"id": "14308109", "url": "https://en.wikipedia.org/wiki?curid=14308109", "title": "Good moral character", "text": "Good moral character\n\nGood moral character is an ideal state of a person’s beliefs and values that is considered most beneficial to society. In United States law, good moral character can, depending on the assessor, include honesty, trustworthiness, diligence, reliability, respect for the law, integrity, candor, discretion, observance of fiduciary duty, respect for the rights of others, absence of hatred and racism, fiscal responsibility, mental and emotional stability, profession-specific criteria such as pledging to honor the constitution and uphold the law, and the absence of a criminal conviction. Since the moral character of a person is an intrinsic psychological characteristic and cannot be measured directly, some scholars and statutes have used the phrase “behaved as a person of good moral character.”\n\nPeople must have good moral character determined as a fact of law in predominately two contexts - (1) state-issued licensure that allows one to work and practice a regulated profession and (2) federal government-issued U.S. citizenship certificates whereby an immigrant undergoes naturalization to become a citizen. Many laws create a paradox by placing the burden of proof of good moral character on the applicant while such a proof, but not the law, necessitates that the evaluators assess the beliefs and values of the applicant.\n\nGood moral character is the opposite of moral turpitude, another legal concept in the United States used in similar instances.\n\nGood moral character can be proven through the presence of several positive moral findings, having no-to-minimal negative moral findings, and by the absence of legal violations. Positive evidence of good moral character can include letters of reccomendation, pursuing education, working seven days a week, owning one’s home, attending church every Sunday, marrying one’s high-school sweetheart, having strong ties to one’s nuclear family, coaching little league teams, teaching English above all other languages in one’s home, paying taxes, paying bills on time, and volunteering in the community. If one volunteers to help others, they may be considered a better person if something bad, uncontrollable, and unexpected happens to them while they are working. For example, a man who was stung by a bee while mowing the lawn for an elderly neighbor would often be rated as having a better moral character than a similar man who was not stung by a bee.\n\nNegative findings of moral character can include having children without being married, not paying taxes, receiving government support, and advocating for racism. The presence of any negative finding can outweigh several positive findings.\n\nEven minor violations of the law can be the sole reason for denying citizenship. The United States Citizenship and Immigration Services describes \"good moral character\" as an absence of involvement in the following activities:\n\n\nAdditionally, several other prior activities can disqualify a person from having a current “good moral character\"\n\n\nTeachers, nurses, physicians, attorneys, barbers, liquor salespersons, pharmacists, and many other professionals require a state-issued license to perform their job. In order to obtain a license to work, one must meet the regular non-moral requirements such as years of education and also convince the state board that the applicant has good moral character. However, the criteria used to determine good moral character can vary significantly. Background checks are a type of verification of good moral character and they are often accompanied by drug testing. For admission to the bar in the United States, lawyers must go through extensive moral character checks as part of the application process.\n\nA 1975 survey by Camenisch included responses from 19 state medical board presidents (respondents) wherein they placed eight characteristics of good moral character in order of importance from most-to-least.\n\nCamenisch writes, “The grouping of the eight elements in the order of importance makes it clear that ‘good moral character’ in the minds of the respondents emphatically has more to do with the professional's obligations to a limited number of specific individuals, to his patients, than to the society at large, to the entire population of those needing health care... this second area ... includes such matters as the distribution of health care and the mode and ease of patient access to it. ... this low ranking of societal issues and responsibilities is of special interest in light of the fact that these same respondents, when asked to indicate the major differences between professional and other occupational licensing, gave highest place to ‘the degree of dedication to the public well-being expected of the licensee.’”\n"}
{"id": "6548700", "url": "https://en.wikipedia.org/wiki?curid=6548700", "title": "Half-truth", "text": "Half-truth\n\nA half-truth is a deceptive statement that includes some element of truth. The statement might be partly true, the statement may be totally true but only part of the whole truth, or it may use some deceptive element, such as improper punctuation, or double meaning, especially if the intent is to deceive, evade, blame or misrepresent the truth.\n\nThe purpose and or consequence of a half-truth is to make something that is really only a belief appear to be knowledge, or a truthful statement to represent the whole truth, or possibly lead to a false conclusion. According to the justified true belief theory of knowledge, in order to know that a given proposition is true, one must not only believe in the relevant true proposition, but one must also have a good reason for doing so. A half-truth deceives the recipient by presenting something believable and using those aspects of the statement that can be shown to be true as good reason to believe the statement is true in its entirety, or that the statement represents the whole truth. A person deceived by a half-truth considers the proposition to be knowledge and acts accordingly.\n\n\nSome forms of half-truths are an inescapable part of politics in representative democracies. The reputation of a political candidate can be irreparably damaged if they are exposed in a lie, so a complex style of language has evolved to minimise the chance of this happening. If someone has not said something, they cannot be accused of lying. As a consequence, politics has become a world where half-truths are expected, and political statements are rarely accepted at face value.\n\nWilliam Safire defines a half-truth, for political purposes, as \"a statement accurate enough to require an explanation; and the longer the explanation, the more likely a public reaction of half-belief\".\n\nIn his 1990 work \"The Magic Lantern: The Revolution of 1989 Witnessed in Warsaw, Budapest, Berlin, and Prague\", Timothy Garton Ash responded to Václav Havel's call for \"living in truth\":\nPhilosopher Alfred North Whitehead was quoted as saying: \"There are no whole truths; all truths are half-truths. It is trying to treat them as whole truths that plays the devil\". If this is true, statements, or truths, which according to Whitehead are all half-truths, are susceptible to creating deceptive and false conclusions.\n\nRichard Brodie links half-truths to memes \"the truth of any proposition depends on the assumptions you make in considering it—the distinct memes you use in thinking about it\". Brodie considers half-truths a necessary part of human interaction because they allow practical application of ideas when it is impractical to convey all the information needed to make a fully informed decision, although some half-truths can lead to a false conclusions or inferences in the world of logic.\n\nThe notion of half-truths has existed in various cultures, giving rise to several epigrammatic sayings.\n\n\n"}
{"id": "1652134", "url": "https://en.wikipedia.org/wiki?curid=1652134", "title": "Interactive media", "text": "Interactive media\n\nInteractive media normally refers to products and services on digital computer-based systems which respond to the user's actions by presenting content such as text, moving image, animation, video, audio, and video games.\n\nInteractive media is a method of communication in which the output from the media comes from the input of the users.\nInteractive media works with the user's participation. The media still has the same purpose but the user's input adds interaction and brings interesting features to the system for better enjoyment.\n\nThe analogue videodisc developed by NV Philips was the pioneering technology for interactive media. Additionally, there are several elements that encouraged the development of interactive media including the following: \nAll of the prior elements contributed in the development of the main hardware and software systems used in interactive media.\n\nThough the word \"media\" is plural, the term is often used as a singular noun.\n\nInteractive media is related to the concepts interaction design, new media, interactivity, human computer interaction, cyberculture, digital culture, interactive design, and includes augmented reality.\n\nAn essential feature of interactivity is that it is mutual: user and machine each take an active role (see interaction). Most interactive computing systems are for some human purpose and interact with humans in human contexts. Manovich complains that 'In relation to computer-based media, the concept of interactivity is a tautology. ... Therefore, to call computer media “interactive” is meaningless – it simply means stating the most basic fact about computers.'. Nevertheless, the term is useful to denote an identifiable body of practices and technologies.\n\nInteractive media are an instance of a computational method influenced by the sciences of cybernetics, autopoiesis and system theories, and challenging notions of reason and cognition, perception and memory, emotions and affection.\n\nAny form of interface between the end user/audience and the medium may be considered interactive. Interactive media is not limited to electronic media or digital media. Board games, pop-up books, gamebooks, flip books and constellation wheels are all examples of printer interactive media. Books with a simple table of contents or index may be considered interactive due to the non-linear control mechanism in the medium, but are usually considered non-interactive since the majority of the user experience is non-interactive reading.\n\nInteractive media is helpful in the four development dimensions in which young children learn: social and emotional, language development, cognitive and general knowledge, and approaches toward learning. Using computers and educational computer software in a learning environment helps children increase communication skills and their attitudes about learning. Children who use educational computer software are often found using more complex speech patterns and higher levels of verbal communication. A study found that basic interactive books that simply read a story aloud and highlighted words and phrases as they were spoken were beneficial for children with lower reading abilities. Children have different styles of learning, and interactive media helps children with visual, verbal, auditory, and tactile learning styles.\n\nInteractive media makes technology more intuitive to use. Interactive products such as smartphones, iPad's/iPod's, interactive whiteboards and websites are all easy to use. The easy usage of these products encourages consumers to experiment with their products rather than reading instruction manuals.\n\nInteractive media promotes dialogic communication. This form of communication allows senders and receivers to build long term trust and cooperation. This plays a critical role in building relationships. Organizations also use interactive media to go further than basic marketing and develop more positive behavioral relationships.\n\nThe introduction of interactive media has greatly affected the lives and inner workings of families, with many family activities having integrated with technology quite seamlessly, allowing both children and parents to adapt to it as they see fit. However, parents have also become increasingly worried about the impact that it will have on their family lives. This is not necessarily because they are opposed to technology, but because they fear that it will lessen the time that they get to spend with their children. Studies have shown that although interactive media is able to connect families together when they are unable to physically, the dependence on this media also continues to persist even when there are opportunities for family time, which often leads the adults to believe that it distracts children more than it benefits them.\n\nThe media which allows several geographically remote users to interact synchronously with the media application/system is known as Distributed Interactive Media. Some common examples of this type of Media include Online Gaming, Distributed Virtual Environment, Whiteboards which are used for interactive conferences and many more.\n\nA couple of basic examples of interactive media (also known as a company in Azerbaijan) are video games and websites. Websites, especially social networking websites provide the interactive use of text and graphics to its users, who interact with each other in various ways such as chatting, playing online games, sharing posts that may include their thoughts and/or pictures and so forth. Video games are also one of the common examples of Interactive Media as the players make use of the joystick/controller to interactively respond to the actions and changes taking place on the game screen generated by the game application, which in turn reacts to the response of the players through the joystick/controller.\n\nInteractive media can be implemented using a variety of platforms and applications that use technology. Some examples include mobile platforms such as touch screen smartphones and tablets, as well as other interactive mediums that are created exclusively to solve a unique problem or set of problems. Interactive media is not limited to a professional environment, it can be used for any technology that responds to user actions. This can include the use of JavaScript and AJAX in web pages, but can also be used in programming languages or technology that has similar functionality.\n\nOne of the most recent innovations to use interactivity that solves a problem that individuals have on a daily basis is Delta Airlines's \"Photon Shower\". This device was developed as a collaboration between Delta Airlines and Professor Russell Foster of Cambridge University. The device is designed to reduce the effect of jet lag on customers that often take long flights across time zones. The interactivity is evident because of how it solves this problem. By observing what time zones a person has crossed and matching those to the basic known sleep cycles of the individual, the machine is able to predict when a person's body is expecting light, and when it is expecting darkness. It then stimulates the individual with the appropriate light source variations for the time, as well as an instructional card to inform them of what times their body expects light and what times it expects darkness. Growth of interactive media continues to advance today, with the advent of more and more powerful machines the limit to what can be input and manipulated on a display in real time is become virtually non-existent.\n\n\n"}
{"id": "42007077", "url": "https://en.wikipedia.org/wiki?curid=42007077", "title": "Kshetrajna", "text": "Kshetrajna\n\nKshetrajna (Devnagari: क्षेत्रज्ञ) means the One who knows of the body, soul, spirituality, conscious principle in the corporeal frame. In the thirteenth chapter of the Bhagavad Gita, Krishna explains the distinction between the Kshetra (known) and the Kshetrajna (knower).\n\nThe Kshetra or the field refers to the body which is material, mutable, transitory and perishable, the Kshetrajna refers to the conscious knower of the body who is of the same essence as Knowledge, immutable, eternal and imperishable, the knower of the body is the soul residing in the body. Kshetra is Prakrti or matter which is insentient, and the knower of the Kshetra is the Purusha who is sentient. True knowledge is knowing and understanding both these two factors, the insentient and sentient. The knowledge of Prakrti only, is called the Apara Vidya or Lower knowledge, and that pertaining to the Purusha is called the Para Vidya or Higher knowledge. in the Bhagavad Gita, Arjuna is told that the distinctive nature of God is eight-fold constituted by the five primordial elements, mind, intellect and the ego-sense, but that is the lower nature which is inferior, impure, troublesome, whose essence is bondage; the higher nature, which is the pure essential nature of God, is the higher living being, the Kshetrajna, the field-knower, the cause leading to the assumption of vital force by which the world is penetrated and upheld.\n\nIn the opening Sloka of Chapter XIII of the Bhagavad Gita, Krishna defines Kshetra and establishes the identity of the individual soul, the conscious knower of the Kshetra, with the Universal Soul.\n\nThereafter, He explains that - the five elements, the ego, the intellect, the Unmanifest (Primordial Matter), the ten organs (of perception and action), the mind, and the five objects of sense (sound, touch, colour, taste and smell); also desire, aversion, pleasure, pain, the physical body, consciousness, firmness, this is the Kshetra with its evolutes (XIII.5-6).\n\nArjuna is told that – absence of pride, freedom from hypocrisy, non-violence, forbearance, straightness of the body, speech and mind, devout service of the preceptor, internal and external purity, steadfastness of mind and control of body, mind and the senses, dispassion towards the objects of enjoyment of this world and the next, and also absence of egotism, pondering again and again on the pain and evils inherent in birth, death, old age and disease; absence of attachment and the feeling of mineness in respect of son, wife, home etc., and constant equipoise of mind both in favourable and unfavourable circumstances; unflinching devotion to God through exclusive attachment, living in secluded and holy places, and finding no enjoyment in the company of men; fixity in self-knowledge and seeing God as the object of true knowledge – all this is declared as knowledge; and what is other than this is called ignorance (XIII 7-11).\n\nSankara in his Bhasya explains that devotion inspired by conviction that wavers not, is unwavering devotion, which devotion is knowledge. Spiritual knowledge is that of the Self, meditation on it is the perception of the content of philosophical knowledge. Knowledge is what ought to be known and the knowable is that by which one attains immortality.429-430\n\nKrishna tells Arjuna that:-\n\nHaving identified Himself as the Kshetrajna, Krishna proceeds to describe in detail Him who is sat (being) and asat (non-being) both, the Sole Witness who is eternal and present everywhere and in all things, and failing to reach Whom, speech together with the mind returneth (Taittiriya Upanishad II.9) – this indescribable entity is Brahman, ज्ञेयम् - the object worth knowing, विज्ञाय मद्भावाय उपपद्यते – in whose being the devout knowing which reality enter (merge) (VIII.18). Krishna directs that one should know Prakrti and Purusha to be beginningless – that the former is responsible for bringing forth the evolutes and the instruments, and the latter, who is the individual soul seated in Prakrti, is declared to be the cause of experience of joys and sorrows, and attachment to Gunas is the cause of birth in sat-asat-yoni. And that - देहेऽस्मिन् पुरुषः परः – Purusha, the individual soul dwelling in the body, is the same as the Supreme Soul Brahman. Thus, Kshetrajna is the pure conscious spirit that is purusha and atman. Krishna does not describe Kshetrajna, He describes that which is required to be known. When ज्ञेयम् - Jneya is realized, there disappears all duality and separateness in the form of knower, knowledge and what is to be known Every Kshetra does not have a separate Kshetrajna, the Lord is the supreme Kshetrajna in all the Kshetras.\n\nThe concept of “Sameness” or “Oneness” of the individual soul and the Universal Soul emphasized by the Upanishads, the Brahma Sutras, the Bhagavad Gita and other allied texts is but an echo of what was long ago revealed to the Vedic Rishis and has lingered on. Ātman (Hinduism) Dirghatamas one of the Angirasa Rishis of the Rig Veda in Mantra 4 of Sukta I.163 addressed to Agni states:-\n\nThis unique awareness of Sameness which is actually the awareness of Oneness is the knowledge of Reality, the true knowledge of existence, gaining which knowledge the true seeker of knowledge ceases to see difference in this wide world which difference is seen only as so many names echoing and re-echoing persistently in one’s mind.\n\nSankara, in his commentary on Chapter XIII of the Bhagavad Gita with regard to the distinction between Ishvara and Jiva since the identification of the prajna (the self in deep sleep-state) with Ishvara is problematic, states:-\n“Now as to the objections that Ishvara would be a samsarin if He be one with Kshetrajna, and that if Kshetrajnas be one with Ishvara there can be no samsara because there is no samsarin: these objections have been met by saying that knowledge and ignorance are distinct in kind and effects, - that all that is knowable is the Kshetra, and Kshetrajna is the knower and none else.” \nAnd, therefore, avarna dosha (obscuration of intellect) which is the basic feature of deep sleep affects only individual beings and not God.\n"}
{"id": "44683200", "url": "https://en.wikipedia.org/wiki?curid=44683200", "title": "Legal consciousness", "text": "Legal consciousness\n\nLegal consciousness is a \"collection\" of understood and/or imagined to have understood, legal awareness of ideas, views, feelings and traditions imbibed through legal socialization; which reflects as legal culture among given individual, or a group, or a given society at large. The legal consciousness evaluates the existing law and also bears in mind an image of the desired or ideal law.\n\nConsciousness is not an individual trait nor solely ideational; legal consciousness is a type of social practice reflecting and forming social structures. The study of legal Consciousness documents the forms of participation and interpretation through which act or sustain, reproduce, or amend the circulating contested or hegemonic structures of meanings concerning law. Legal consciousness is the way in which law is experienced and interpreted by specific individuals as they engage, avoid, resist or just assume the law and legal meanings.\n\nLegal consciousness is a state of being, legal socialisation is the process to Legal consciousness; where as legal awareness & legal mobilisation are means to achieve the same.\n\nThe Great Soviet Encyclopaedia (1979) defined legal consciousness as \" \"the sum of views and ideas expressing the attitude of people toward law, legality, and justice and their concept of what is lawful and unlawful. Legal consciousness is a form of social consciousness. Legal ideology, the system of legal views based on certain social and scientific viewpoints, is a concentrated expression of legal consciousness. The customs and feelings of people in relation to legal phenomena constitute the psychological aspect of legal consciousness; among these are a sense of justice and a loathing of crimes and illegal actions\" \"\n\nLegal consciousness is defined by Ewick and Silbey as the process by which people make sense of their experiences by relying on legal categories and concepts. People do this even when they are not familiar with the details and minutia of law or the legal system. They explain that there are cultural schemas provided by law that people use to make sense of their experiences. They refer to this as legality. The concept of legality includes \"the meanings, sources, authority and cultural practices that are commonly recognized as legal, regardless of who employs them or for what ends.\" These meanings and sources and different ways of knowing and understanding enable people to make sense of what happens to them and what that might mean in terms of their rights and options. This process of understanding legal experiences occurs within a larger ecosystem in which there are disputes over meaning and values. Seron and Munger explain that \"\"in addition, class may affect legal consciousness: Law may mean different things depending on an individual's location in the various hierarchies of status, prestige, and knowledge associated with membership in a social class.\n\nlegal consciousness narratives\n\n"}
{"id": "21755295", "url": "https://en.wikipedia.org/wiki?curid=21755295", "title": "Military expression", "text": "Military expression\n\nMilitary expression is an area of military law pertaining to the United States military that relates to the free speech rights of its service members. While \"military free speech\" was the term used during the Vietnam War era, \"military expression\" has become a niche area of military law since 2001. Besides media references relating to specific cases, the term was used at military whistleblower committee hearings with members of the United States House of Representatives and Senate on May 14, 2008. Transcripts of the hearings show that attorney Mike Lebowitz was identified as testifying as a legal expert in \"military expression\". That hearing also included references by U.S. Representative Sheila Jackson Lee (D-TX) who also referred to the area of law as \"military expression\".\n\nWhile the civilian population of the United States is afforded the right to free expression under the First Amendment, the U.S. Supreme Court has affirmed the notion that service members have a reduced level of free speech. While the Court acknowledged that service members do have First Amendment rights, these rights are limited:\n\nWith the advent of the Iraq War in 2003, the issue of military expression was again in the public eye as a relatively small amount of service members and veterans began demonstrating. One case revolved around a former Marine (still under contract with the IRR) who was photographed by the \"Washington Post\" wearing a partial uniform during an anti-war demonstration in Washington, D.C. The individual faced disciplinary action for his participation in this demonstration, as well as for a politically charged email he sent to a Marine officer. However, in this case, the service member avoided the other than honorable discharge being sought by the military due to the First Amendment arguments posed on his behalf. That case, which was argued by attorney Mike Lebowitz in representation of anti-war and political activist Adam Kokesh, is regarded as the first military expression case of its kind to result generally favorably for the service member.\n\nPolitical speech, to include being active in a political party, also has become an issue as the Internet and email permits easier participation despite rules against such activity.\n\n\n[[Category:United States military law]]\n[[Category:First Amendment to the United States Constitution]]\n[[Category:Freedom of expression]]\n[[Category:Military sociology]]"}
{"id": "22157068", "url": "https://en.wikipedia.org/wiki?curid=22157068", "title": "Multislice", "text": "Multislice\n\nThe multislice algorithm is a method for the simulation of the interaction of an electron beam with matter, including all multiple elastic scattering effects. The method is reviewed in the book by Cowley. The algorithm is used in the simulation of high resolution Transmission electron microscopy micrographs, and serves as a useful tool for analyzing experimental images. Here we describe relevant background information, the theoretical basis of the technique, approximations used, and several software packages that implement this technique. Moreover, we delineate some of the advantages and limitations of the technique and important considerations that need to be taken into account for real-world use.\n\nThe multislice method has found wide application in electron crystallography. The mapping from a crystal structure to its image or diffraction pattern has been relatively well understood and documented. However, the reverse mapping from electron micrograph images to the crystal structure is generally more complicated. The fact that the images are two-dimensional projections of three-dimensional crystal structure makes it tedious to compare these projections to all plausible crystal structures. Hence, the use of numerical techniques in simulating results for different crystal structure is integral to the field of electron microscopy and crystallography. Several software packages exist to simulate electron micrographs.\n\nThere are two widely used simulation techniques that exist in literature: the Bloch wave method, derived from Hans Bethe's original theoretical treatment of the Davisson-Germer experiment, and the multislice method. In this paper, we will primarily focus on the multislice method for simulation of diffraction patterns, including multiple elastic scattering effects. Most of the packages that exist implement the multislice algorithm along with Fourier analysis to incorporate electron lens aberration effects to determine electron microscope image and address aspects such as phase contrast and diffraction contrast. For electron microscope samples in the form of a thin crystalline slab in the transmission geometry, the aim of these software packages is to provide a map of the crystal potential, however this inversion process is greatly complicated by the presence of multiple elastic scattering.\n\nThe first description of what is now known as the multislice theory was given in the classic paper by Cowley and Moodie . In this work, the authors describe scattering of electrons using a physical optics approach without invoking quantum mechanical arguments. Many other derivations of these iterative equations have since been given using alternative methods, such as Greens functions, differential equations, scattering matrices or path integral methods.\n\nA summary of the development of a computer algorithm from the multislice theory of Cowley and Moodie for numerical computation was reported by Goodman and Moodie. They also discussed in detail the relationship of the multislice to the other formulations. Specifically, using Zassenhaus's theorem, this paper gives the mathematical path from multislice to 1. Schroedingers equation (derived from the multislice), 2. Darwin's differential equations, widely used for diffraction contrast TEM image simulations - the Howie-Whelan equations derived from the multislice. 3. Sturkey's scattering matrix method. 4. the free-space propagation case, 5. The phase grating approximation, 6. A new \"thick-phase grating\" approximation, which has never been used, 7. Moodie's polynomial expression for multiple scattering, 8. The Feynman path-integral formulation, and 9. relationship of multislice to the Born series. The relationship between algorithms is summarized in Section 5.11 of Spence (2013), (see Figure 5.9).\n\nThe form of multislice algorithm presented here has been adapted from Peng, Dudarev and Whelan 2003. The multislice algorithm is an approach to solving the Schrödinger wave equation:\n\nformula_1\n\nIn 1957, Cowley and Moodie showed that the Schrödinger equation can be solved analytically to evaluate the amplitudes of diffracted beam. Subsequently, the effects of dynamical diffraction can be calculated and the resulting simulated image will exhibit good similarities with the actual image taken from a microscope under dynamical conditions. Furthermore, the multislice algorithm does not make any assumption about the periodicity of the structure, as a result this method can be used to simulate HREM images of aperiodic systems as well.\n\nThe following section will include a mathematical formulation of the Multislice algorithm. The Schrödinger equation can also be represented in the form of incident and scattered wave as:\n\nformula_2\n\nwhere formula_3 is the Green’s function that represents the amplitude of the electron wave function at a point formula_4 due to a source at point formula_5.\n\nHence for an incident plane wave of the form formula_6 the Schrödinger equation can be written as:\n\nformula_7\n\nWe then choose the coordinate axis in such a way that the incident beam hits the sample at (0,0,0) in the formula_8-direction. Now we consider wave-function with a modulation function formula_9 for the amplitude of the wave-function. Hence, the modulation function can be represented as:\n\nformula_10\n\nNow we make substitutions with regards to the coordinate system we have adhered.\n\nformula_11\n\nformula_12 and convergence. This program is good to use if one already has structure files for a material that have been used in other calculations (for example, Density Functional Theory). These structure files can be used to general X-Ray structure factors which are then used as input for the PTBV routine in NUMIS. Microscope parameters can be changed through the MICROVB routine.\n\nhttp://www.numis.northwestern.edu/ourwiki/index.php/Multislice\n\nThis software is specifically developed to run in Mac OS X by Dr. Roar Kilaas of Lawrence Berkeley National Laboratory. It is designed to have a user-friendly user interface and has been well-maintained relative to many other codes (last update May 2013). It is available (for a fee) from http://www.totalresolution.com.\n\nThis is a software for multislice simulation was written in FORTRAN 77 by Dr. J. M. Zuo, while he was a postdoc research fellow at Arizona State University under the guidance of Prof. John C. H. Spence. The source code was published in the book of Electron Microdiffraction. A comparison between multislice and Bloch wave simulations for ZnTe was also published in the book. A separate comparison between several multislice algorithms at the year of 2000 was reported in \n\nElectron and Ion Microscopy, written by Professor Christopher Koch of Ulm University in Germany. Allows simulation of HAADF, ADF, ABF-STEM, as well as conventional TEM and CBED. The executable and source code is available as a free download at the Koch group website: https://archive.is/20130620044224/http://elim.physik.uni-ulm.de/?page_id=834\n\nThis is a code written by Dr Vincenzo Grillo of the Institute for Nanoscience (CNR) in Italy. This code is essentially a graphical frontend to the multislice code written by Kirkland, with more additional features. These include tools to generate complex crystalline structures, simulate HAADF images and model the STEM probe, as well as modeling of strain in materials. Tools for image analysis (e.g. GPA) and filtering are also available.\nThe code is updated quite often with new features and a user mailing list is maintained. Freely available at \nhttp://tem-s3.nano.cnr.it/?page_id=2\n\nMulti-slice image simulations for high-resolution scanning and coherent imaging transmission electron microscopy written by Dr. Juri Barthel from the Ernst Ruska-Centre at the Jülich Research Centre. The software comprises a graphical user interface version for direct visualization of STEM image calculations, as well as a bundle of command-line modules for more comprehensive calculation tasks. The programs have been written using Visual C++, Fortran 90, and Perl. Executable binaries for Microsoft Windows 32-bit and 64-bit operating systems are available for free from the website:\nhttp://www.er-c.org/barthel/drprobe/\n\n"}
{"id": "307372", "url": "https://en.wikipedia.org/wiki?curid=307372", "title": "Mīmāṃsā", "text": "Mīmāṃsā\n\nMīmāṃsā is a Sanskrit word that means \"reflection\" or \"critical investigation\" and thus refers to a tradition of brahminical thought which reflected on the meanings of certain Vedic texts. This tradition is also known as Pūrva-Mīmāṃsā because of its focus on the earlier (\"pūrva\") Vedic texts dealing with ritual actions, and similarly as Karma-Mīmāṃsā due to its focus on ritual action (\"karma\"). It is one of six Vedic \"affirming\" (āstika) schools of Hinduism. This particular school is known for its philosophical theories on the nature of dharma, based on hermeneutics of the Vedas, especially the Brāḥmanas and Saṃhitas. The Mīmāṃsā school was foundational and influential for the vedāntic schools, which were also known as Uttara-Mīmāṃsā for their focus on the \"later\" (\"uttara\") portions of the Vedas, the Upaniṣads. While both \"earlier\" and \"later\" Mīmāṃsā investigate the aim of human action, they do so with different attitudes towards the necessity of ritual praxis.\n\nMīmāṃsā has several sub-schools, each defined by its epistemology. The Prābhākara sub-school, which takes its name from the seventh-century philosopher Prabhākara, described the five epistemically reliable means to gaining knowledge: \"pratyakṣa\" or perception; \"anumāna\" or inference; \"upamāṇa\", by comparison and analogy; \"arthāpatti\", the use of postulation and derivation from circumstances; and \"śabda\", the word or testimony of past or present reliable experts. The Bhāṭṭa sub-school, from philosopher Kumārila Bhaṭṭa, added a sixth means to its canon; \"anupalabdhi\" meant non-perception, or proof by the absence of cognition (e.g., the \"lack\" of gunpowder on a suspect's hand)\n\nThe school of Mīmāṃsā consists of both atheistic and theistic doctrines, but the school showed little interest in systematic examination of the existence of Gods. Rather, it held that the soul is an eternal, omnipresent, inherently active spiritual essence, and focused on the epistemology and metaphysics of \"dharma\". For the Mīmāṃsā school, \"dharma\" meant rituals and social duties, not \"devas\", or gods, because gods existed only in name. The Mīmāṃsakas also held that Vedas are \"eternal, author-less, [and] infallible\", that Vedic \"vidhi\", or injunctions and mantras in rituals are prescriptive \"kārya\" or actions, and the rituals are of primary importance and merit. They considered the Upaniṣads and other texts related to self-knowledge and spirituality as subsidiary, a philosophical view that Vedānta disagreed with.\n\nMīmāṃsā gave rise to the study of philology and the philosophy of language. While their deep analysis of language and linguistics influenced other schools of Hinduism, their views were not shared by others. Mīmāṃsakas considered the purpose and power of language was to clearly \"prescribe\" the proper, correct and right. In contrast, Vedāntins extended the scope and value of language as a tool to also \"describe\", \"develop\" and \"derive\". Mīmāṃsakas considered orderly, law driven, procedural life as central purpose and noblest necessity of dharma and society, and divine (theistic) sustenance means to that end.\n\nThe Mīmāṃsā school is a form of philosophical realism. A key text of the Mīmāṃsā school is the Mīmāṃsā Sūtra of Jaimini.\n\nMīmāṃsā, also romanized Mimansa or Mimamsa, means \"reflection, consideration, profound thought, investigation, examination, discussion\" in Sanskrit. It also refers to the \"examination of the Vedic text\" and to a school of Hindu philosophy that is also known as (\"prior\" inquiry, also ), in contrast to (\"posterior\" inquiry, also ) – the opposing school of Vedanta. This division is based on classification of the Vedic texts into ', the early sections of the Veda treating of mantras and rituals (Samhitas and Brahmanas), and the ' dealing with the meditation, reflection and knowledge of Self, Oneness, Brahman (the Upaniṣads). Between the \"Samhitas\" and \"Brahmanas\", the Mīmāṃsā school places greater emphasis to the Brahmanas - the part of Vedas that is a commentary on Vedic rituals.\n\nDonald Davis translates Mīmāṃsā as the \"desire to think\", and in colloquial historical context as \"how to think and interpret things\". In the last centuries of the first millennium BCE, the word Mīmāṃsā began to denote the thoughts on and interpretation of the Vedas, first as \"Pūrva-Mīmāṃsā\" for rituals portions in the earlier layers of texts in the Vedas, and as \"Uttara-Mīmāṃsā\" for the philosophical portions in the last layers. Over time, Pūrva-Mīmāṃsā was just known as the Mīmāṃsā school, and the Uttara-Mīmāṃsā as the Vedanta school.\n\nMīmāṃsā scholars are referred to as \"Mīmāṃsāka\"s.\n\nMīmānsā is one of the six classical Hindu \"darśanas\". It is among the earliest schools of Hindu philosophies. It has attracted relatively less scholarly study, although its theories and particularly its questions on exegesis and theology have been highly influential on all classical Indian philosophies. Its analysis of language has been of central importance to the legal literature of India.\n\nAncient Mīmānsā's central concern was epistemology (\"pramana\"), that is what are the reliable means to knowledge. It debated not only \"how does man ever learn or know, whatever he knows\", but also whether the nature of all knowledge is inherently circular, whether those such as foundationalists who critique the validity of any \"justified beliefs\" and knowledge system make flawed presumptions of the very premises they critique, and how to correctly interpret and avoid incorrectly interpreting dharma texts such as the Vedas. It asked questions such as \"what is \"devata\" (god)?\", \"are rituals dedicated to \"devatas\" efficacious?\", \"what makes anything efficacious?\", and \"can it be proved that the Vedas, or any canonical text in any system of thought, fallible or infallible (\"svatah pramanya\", intrinsically valid)?, if so, how?\" and others. To Mīmānsā scholars, the nature of non-empirical knowledge and human means to it are such that one can never demonstrate certainty, one can only falsify knowledge claims, in some cases. According to Francis Clooney, a professor at Harvard Divinity School specializing on Hinduism, the Mīmānsā school is \"one of the most distinctively Hindu forms of thinking; it is without real parallel elsewhere in the world\".\n\nThe central text of the Mīmānsā school is Jamini's \"Mīmānsā Sutras\", along with the historically influential commentaries on this sutra by Sabara and by Kumarila Bhatta. Together, these texts develop and apply the rules of language analysis (such as the rules of contradiction), asserting that one must not only examine injunctive propositions in any scripture, but also examine the alternate related or reverse propositions for better understanding. They suggested that to reach correct and valid knowledge it is not only sufficient to demand proof of a proposition, it is important to give proof of a proposition's negative as well as declare and prove one's own preferred propositions. Further, they asserted that whenever perception is not the means of direct proof and knowledge, one cannot prove such non-empirical propositions to be \"true or not true\", rather one can only prove a non-empirical proposition is \"false, not false, or uncertain\".\n\nFor example, Mīmānsākas welcome not only the demand for proof of an injunctive proposition such as \"agnihotra ritual leads one to heaven\", but suggest that one must examine and prove alternate propositions such as \"ritual does not lead one to heaven\", \"something else leads one to heaven\", \"there is heaven\", \"there is no heaven\" and so on. Mīmānsā literature states that if satisfactory, verifiable proof for all of such propositions cannot be found by its proponents and its opponents, then the proposition needs to be accepted as a part of a \"belief system\". Beliefs, such as those in the scriptures (Vedas), must be accepted to be true unless its opponents can demonstrate the proof of validity of their own texts or teacher(s) these opponents presume to be \"prima facie justified\", and until these opponents can demonstrate that the scriptures they challenge are false. If they do not try to do so, it is hypocrisy; if they try to do so, it can only lead to infinite regress, according to Mīmānsākas. Any historic scripture with widespread social acceptance, according to Mīmānsāka, is an activity of communication (\"vyavaharapravrtti\") and is accepted as authoritative because it is socially validated practice, unless perceptually verifiable evidence emerges that proves parts or all of it as false or harmful.\n\nMīmānsākas were predominantly concerned with the central motivation of human beings, the highest good, and actions that make this possible. They stated that human beings seek \"niratisaya priti\" (unending ecstatic pleasure, joy, happiness) in this life and the next. They argued that this highest good is the result of one's own ethical actions (\"dharma\"), that such actions are what the Vedic sentences contain and communicate, and therefore it important to properly interpret and understand Vedic sentences, words and meaning. Mīmānsā scholarship was centrally concerned with the philosophy of language, how human beings learn and communicate with each other and across generations with language in order to act in a manner that enables them to achieve that which motivates them. The Mīmānsā school focussed on \"dharma\", deriving ethics and activity from the \"karma-kanda\" (rituals) part of the Vedas, with the argument that ethics for this life and efficacious action for \"svarga\" (heaven) cannot be derived from sense-perception, and can only be derived from experience, reflection and understanding of past teachings.\n<poem>\nIn every human activity, the motivating force to perform an action is his innate longing for \"priti\" (pleasure, happiness),\nwhether at the lowest level or the highest level.\nAt the highest level, it is nothing but an unsurpassed state of \"priti\",\nwhich is ensured only by performing ethical actions.\n</poem>\n– Sabara, 2nd century Mīmānsā scholar\nAccording to Daniel Arnold, Mīmānsā scholarship has \"striking affinities\" with that of William Alston, the 20th century Western philosopher, along with some notable differences. The Mīmānsākas subjected to a radical critique, more than two thousand years ago, states Francis Clooney, the notions such as \"God,\" the \"sacred text,\" the \"author\" and the \"anthropocentric ordering of reality\".\n\nIn the field of epistemology, later Mīmāṃsākas made some notable contributions. Unlike the Nyaya or the Vaisheshika systems, the sub-school of Mīmāṃsā recognizes five means of valid knowledge (Skt. \"pramāṇa\"). The sub-school of Mīmāṃsā recognizes one additional sixth, namely \"anuapalabdhi\", just like Advaita Vedanta school of Hinduism. These six epistemically reliable means of gaining knowledge are:\n\nMain article : Pratyaksha\n\n\"Pratyakṣa\" (प्रत्यक्ष्य) means perception. It is of two types in Mīmānsā and other schools of Hinduism: external and internal. External perception is described as that arising from the interaction of five senses and worldly objects, while internal perception is described by this school as that of inner sense, the mind. The ancient and medieval Indian texts identify four requirements for correct perception: \"Indriyarthasannikarsa\" (direct experience by one's sensory organ(s) with the object, whatever is being studied), \"Avyapadesya\" (non-verbal; correct perception is not through hearsay, according to ancient Indian scholars, where one's sensory organ relies on accepting or rejecting someone else's perception), \"Avyabhicara\" (does not wander; correct perception does not change, nor is it the result of deception because one's sensory organ or means of observation is drifting, defective, suspect) and \"Vyavasayatmaka\" (definite; correct perception excludes judgments of doubt, either because of one's failure to observe all the details, or because one is mixing inference with observation and observing what one wants to observe, or not observing what one does not want to observe). Some ancient scholars proposed \"unusual perception\" as \"pramana\" and called it internal perception, a proposal contested by other Indian scholars. The internal perception concepts included \"pratibha\" (intuition), \"samanyalaksanapratyaksa\" (a form of induction from perceived specifics to a universal), and \"jnanalaksanapratyaksa\" (a form of perception of prior processes and previous states of a 'topic of study' by observing its current state). Further, some schools of Hinduism considered and refined rules of accepting uncertain knowledge from \"Pratyakṣa-pramana\", so as to contrast \"nirnaya\" (definite judgment, conclusion) from \"anadhyavasaya\" (indefinite judgment).\n\nMain article : Anumana\n\n\"Anumāṇa\" (अनुमान) means inference. It is described as reaching a new conclusion and truth from one or more observations and previous truths by applying reason. Observing smoke and inferring fire is an example of \"Anumana\". In all except one Hindu philosophies, this is a valid and useful means to knowledge. The method of inference is explained by Indian texts as consisting of three parts: \"pratijna\" (hypothesis), \"hetu\" (a reason), and \"drshtanta\" (examples). The hypothesis must further be broken down into two parts, state the ancient Indian scholars: \"sadhya\" (that idea which needs to proven or disproven) and \"paksha\" (the object on which the \"sadhya\" is predicated). The inference is conditionally true if \"sapaksha\" (positive examples as evidence) are present, and if \"vipaksha\" (negative examples as counter-evidence) are absent. For rigor, the Indian philosophies also state further epistemic steps. For example, they demand \"Vyapti\" - the requirement that the \"hetu\" (reason) must necessarily and separately account for the inference in \"all\" cases, in both \"sapaksha\" and \"vipaksha\". A conditionally proven hypothesis is called a \"nigamana\" (conclusion).\n\nMain article : Upamāṇa\n\n\"Upamāṇa\" means comparison and analogy. Some Hindu schools consider it as a proper means of knowledge. \"Upamana\", states Lochtefeld, may be explained with the example of a traveller who has never visited lands or islands with endemic population of wildlife. He or she is told, by someone who has been there, that in those lands you see an animal that sort of looks like a cow, grazes like cow but is different from a cow in such and such way. Such use of analogy and comparison is, state the Indian epistemologists, a valid means of conditional knowledge, as it helps the traveller identify the new animal later. The subject of comparison is formally called \"upameyam\", the object of comparison is called \"upamanam\", while the attribute(s) are identified as \"samanya\". Thus, explains Monier Monier-Williams, if a boy says \"her face is like the moon in charmingness\", \"her face\" is \"upameyam\", the moon is \"upamanam\", and charmingness is \"samanya\". The 7th century text Bhaṭṭikāvya in verses 10.28 through 10.63 discusses many types of comparisons and analogies, identifying when this epistemic method is more useful and reliable, and when it is not. In various ancient and medieval texts of Hinduism, 32 types of \"Upanama\" and their value in epistemology are debated.\n\n\"Arthāpatti\" (अर्थापत्ति) means postulation, derivation from circumstances. In contemporary logic, this \"pramāṇa\" is similar to circumstantial implication. As example, if a person left in a boat on river earlier, and the time is now past the expected time of arrival, then the circumstances support the truth postulate that the person has arrived. Many Indian scholars considered this \"pramāṇa\" as invalid or at best weak, because the boat may have gotten delayed or diverted. However, in cases such as deriving the time of a future sunrise or sunset, this method was asserted by the proponents to be reliable. Another common example for \"arthāpatti\" found in the texts of Mīmāṃsā and other schools of Hinduism is, that if \"Devadatta is fat\" and \"Devadatta does not eat in day\", then the following must be true: \"Devadatta eats in the night\". This form of postulation and deriving from circumstances is, claim the Indian scholars, a means to discovery, proper insight and knowledge. The Hindu schools that accept this means of knowledge state that this method is a valid means to conditional knowledge and truths about a subject and object in original premises or different premises. The schools that do not accept this method, state that postulation, extrapolation and circumstantial implication is either derivable from other \"pramāṇas\" or flawed means to correct knowledge, instead one must rely on direct perception or proper inference.\n\nMain article : Anupalabdhi, See also: Abhava\n\n\"Anupalabdi\" (अनुपलब्धि), accepted only by Kumarila Bhatta sub-school of Mīmāṃsā, means non-perception, negative/cognitive proof. \"Anupalabdhi pramana\" suggests that knowing a negative, such as \"there is no jug in this room\" is a form of valid knowledge. If something can be observed or inferred or proven as non-existent or impossible, then one knows more than what one did without such means. In the two schools of Hinduism that consider \"Anupalabdhi\" as epistemically valuable, a valid conclusion is either \"sadrupa\" (positive) or \"asadrupa\" (negative) relation - both correct and valuable. Like other \"pramana\", Indian scholars refined \"Anupalabdi\" to four types: non-perception of the cause, non-perception of the effect, non-perception of object, and non-perception of contradiction. Only two schools of Hinduism accepted and developed the concept \"non-perception\" as a \"pramana\". The schools that endorsed \"Anupalabdi\" affirmed that it as valid and useful when the other five \"pramanas\" fail in one's pursuit of knowledge and truth.\n\n\"Abhava\" (अभाव) means non-existence. Some scholars consider \"Anupalabdi\" to be same as \"Abhava\", while others consider \"Anupalabdi\" and \"Abhava\" as different. \"Abhava-pramana\" has been discussed in ancient Hindu texts in the context of \"Padārtha\" (पदार्थ, referent of a term). A \"Padartha\" is defined as that which is simultaneously \"Astitva\" (existent), \"Jneyatva\" (knowable) and \"Abhidheyatva\" (nameable). Specific examples of \"padartha\", states Bartley, include \"dravya\" (substance), \"guna\" (quality), \"karma\" (activity/motion), \"samanya/jati\" (universal/class property), \"samavaya\" (inherence) and \"vishesha\" (individuality). \"Abhava\" is then explained as \"referents of negative expression\" in contrast to \"referents of positive expression\" in \"Padartha\". An absence, state the ancient scholars, is also \"existent, knowable and nameable\", giving the example of negative numbers, silence as a form of testimony, \"asatkaryavada\" theory of causation, and analysis of deficit as real and valuable. \"Abhava\" was further refined in four types, by the schools of Hinduism that accepted it as a useful method of epistemology: \"dhvamsa\" (termination of what existed), \"atyanta-abhava\" (impossibility, absolute non-existence, contradiction), \"anyonya-abhava\" (mutual negation, reciprocal absence) and \"pragavasa\" (prior, antecedent non-existence).\n\n\"Śabda\" (शब्द) means relying on word, testimony of past or present reliable experts. Hiriyanna explains \"Sabda-pramana\" as a concept which means reliable expert testimony. The schools of Hinduism which consider it epistemically valid suggest that a human being needs to know numerous facts, and with the limited time and energy available, he can learn only a fraction of those facts and truths directly. He must rely on others, his parent, family, friends, teachers, ancestors and kindred members of society to rapidly acquire and share knowledge and thereby enrich each other's lives. This means of gaining proper knowledge is either spoken or written, but through \"Sabda\" (words). The reliability of the source is important, and legitimate knowledge can only come from the \"Sabda\" of reliable sources. The disagreement between the schools of Hinduism has been on how to establish reliability. Some schools, such as Carvaka, state that this is never possible, and therefore \"Sabda\" is not a proper pramana. Other schools debate means to establish reliability.\n\nAn interesting feature of the Mīmāṃsā school of philosophy is its unique epistemological theory of the intrinsic validity of all cognition as such. It is held that all knowledge is \"ipso facto\" true (Skt. \"svataḥ prāmāṇyavāda\"). Thus, what is to be proven is not the truth of a cognition, but its falsity. The Mīmāṃsākas advocate the self-validity of knowledge both in respect of its origin (\"utpatti\") and ascertainment (\"jñapti\"). Not only did the Mīmāṃsākas make the very great use of this theory to establish the unchallengeable validity of the Vedas, but later Vedantists also drew freely upon this particular Mīmāṃsā contribution.\n\nThe core tenets of are ritualism (orthopraxy), anti-asceticism and anti-mysticism. The central aim of the school is elucidation of the nature of \"dharma\", understood as a set ritual obligations and prerogatives to be performed properly.\n\nMīmāṃsā theorists decided that the evidence allegedly proving the existence of God was insufficient. They argue that there was no need to postulate a maker for the world, just as there was no need for an author to compose the Vedas or a God to validate the rituals. Mīmāṃsā argues that the Gods named in the Vedas have no existence apart from the \"mantras\" that speak their names. To that regard, the power of the mantras is what is seen as the power of Gods.\n\nDharma as understood by Pūrva Mīmāṃsā can be loosely translated into English as \"virtue\", \"morality\" or \"duty\". The Pūrva Mīmāṃsā school traces the source of the knowledge of dharma neither to sense-experience nor inference, but to verbal cognition (i.e. knowledge of words and meanings) according to Vedas. In this respect it is related to the Nyāya school, the latter, however, accepts only four sources of knowledge (\"pramāṇa\") as valid.\n\nThe Pūrva Mīmāṃsā school held dharma to be equivalent to following the prescriptions of the Saṃhitās and their Brāhmaṇa commentaries relating the correct performance of Vedic rituals. Seen in this light, Pūrva Mīmāṃsā is essentially ritualist (orthopraxy), placing great weight on the performance of karma or action as enjoined by the Vedas.\n\nEmphasis of Yajnic Karmakāṇḍas in Pūrva Mīmāṃsā is erroneously interpreted by some to be an opposition to Jñānakāṇḍa of Vedānta and Upaniṣads. Pūrva Mīmāṃsā does not discuss topics related to Jñānakāṇḍa, such as salvation (\"mokṣa\"), but it never speaks against mokṣa. Vedānta quotes Jaimini's belief in Brahman as well as in mokṣa:\n\nIn Uttara-Mīmāṃsā or Vedānta (4.4.5-7), Bāḍarāyaṇa cites Jaimini as saying (ब्राह्मेण जैमिनिरूपन्यासादिभ्यः) \"(The mukta Puruṣa is united with the Brahman) as if it were like the Brahman, because descriptions (in Śruti etc) prove so\".\n\nIn Vedānta (1.2.28), Bāḍarāyaṇa cites Jaimini as saying that \"There is no contradiction in taking Vaishvānara as the supreme Brahman\".\n\nIn 1.2.31, Jaimini is again quoted by Bāḍarāyana as saying that the nirguna (attribute-less) Brahman can manifest itself as having a form.\n\nIn 4.3.12, Bādarāyana again cites Jaimini as saying that the mukta Purusha attains Brahman.\n\nIn Pūrva Mīmāṃsā too, Jaimini emphasises the importance of faith in and attachment to the Omnipotent Supreme Being Whom Jaimini calls \"The Omnipotent Pradhaana\" (The Main):\n\nPūrva Mīmāṃsā 6.3.1: \"sarvaśaktau pravṛttiḥ syāt tathābhūtopadeśāt\" (सर्वशक्तौ प्रवृत्तिः स्यात् तथाभूतोपदेशात्). The term \"upadeśa\" here means instructions of the śāstras as taught. We should tend towards the omnipotent supreme being. In the context of Pūrva Mīmāṃsā 6.3.1 shown above, next two sutras becomes significant, in which this Omnipotent Being is termed as \"pradhāna\", and keeping away from Him is said to be a \"doṣa\", hence all beings are asked to get related (\"abhisambandhāt\" in tadakarmaṇi ca doṣas tasmāt tato viśeṣaḥ syāt pradhānenābhisambandhāt; Jaimini 6, 3.3) to the \"Omnipotent Main Being\" (api vāpy ekadeśe syāt pradhāne hy arthanirvṛttir guṇamātram itarat tadarthatvāt; Jaimini 6, 3.2). Karma-Mīmāṃsā supports the Vedas, and Rgveda says that one Truth is variously named by the sages. It is irrelevant whether we call Him as Pradhāna or Brahman or Vaishvānara or Shiva or God.\n\nThe school's origins lie in the scholarly traditions of the final centuries BCE, when the priestly ritualism of Vedic sacrifice was being marginalized by Buddhism and Vedanta. To counteract this challenge, several groups emerged dedicated to demonstrating the validity of the Vedic texts by rigid formulation of rules for their interpretation. The school gathers momentum in the Gupta period with Śābara, and reaches its apex in the 7th to 8th centuries with and Prabhākara.\n\nThe school for some time in the Early Middle Ages exerted near-dominant influence on learned Hindu thought, and is credited as a major force contributing to the decline of Buddhism in India, but it has fallen into decline in the High Middle Ages and today is all but eclipsed by Vedanta.\n\nThe foundational text for the Mīmāṃsā school is the Purva Mīmāṃsā Sutras of Jaimini (ca. 5th to 4th century BCE). A major commentary was composed by Śābara in ca. the 1th century BC. The school reaches its height with and (fl. ca. 700 CE). Both Kumarila Bhatta and Prabhākara (along with , whose work is no more extant) have written extensive commentaries on Śābara's Mīmāṃsāsūtrabhāṣyam.\nKumārila Bhaṭṭa, Mandana Miśra, Pārthasārathi Miśra, Sucarita Miśra, Ramakrishna Bhatta, Madhava Subhodini, Sankara Bhatta, Krsnayajvan, Anantadeva, Gaga Bhatta, Ragavendra Tirtha, VijayIndhra Tirtha, Appayya Dikshitar, Paruthiyur Krishna Sastri, Mahomahapadyaya Sri Ramsubba Sastri, Sri Venkatsubba Sastri, Sri A. Chinnaswami Sastri, Sengalipuram Vaidhyanatha Dikshitar were some of Mīmānsā scholars.\n\nThe ' of Jaimini (c. 3rd century BCE) has summed up the general rules of ' for Vedic interpretation. The text has 12 chapters, of which the first chapter is of philosophical value. The commentaries on the ' by , , Hari and are no more extant. (c. 1st century BCE) is the first commentator of the ', whose work is available to us. His ' is the basis of all later works of '. (7th century CE), the founder of the first school of the ' commented on both the ' and its '. His treatise consists of 3 parts, the ', the ' and the '. (8th century CE) was a follower of , who wrote \"Vidhiviveka\" and '. There are several commentaries on the works of . wrote a ' (commentary) on the '. wrote ', also known as ', a commentary on the '. wrote ' (1300 CE), another commentary on the '. He also wrote ', an independent work on the ' and \"Tantraratna\". ’s ' is a commentary on the '. (8th century CE), the originator of the second school of the ' wrote his commentary ' on the '. ’s ' (ninth century CE) is a commentary on the '. His ' is an independent work of this school and the ' is a brief explanation of the '. ’s ' deals with the views of this school in details. The founder of the third school of the ' was , whose works have not reached us.\n\n\n\n"}
{"id": "1405792", "url": "https://en.wikipedia.org/wiki?curid=1405792", "title": "Neuromantic (philosophy)", "text": "Neuromantic (philosophy)\n\nAccording to the anthropologist Bradd Shore, the neuromantic refers to the cybernetic frame of mind among excited computer enthusiasts as they experience what Michael Heim called \"the all-at-once simultaneity of totalizing presentness\". It is part of Shore's discourse on the embodied cognitive dimensions of the cultural transformation produced by the emergence of word processing, which he maintained has overcome the spatial limitations of the written text since \"text modules can be combined, reconfigured, reduced, expanded, appended, or deleted from other units at the touch of a finger.\" This phenomenon is said to elicit passion that is also identified as an impulse unique to the modern world. Shore explains that \"the sense of mastery over language resources that word processing bestows on the experienced user is intimately related to Martin Heidegger's notion of enframing \"(Bestellen\"), a subjection of the world to human will that Heidegger saw as a characteristic of all modern technology.\" \n\nThere are sources that claim the concept of neuromantic is not only confined to computer enthusiasts. For instance, there is the so-called neuromantic imagination that inspire architects. Here, the cybernetic frame of mind is fired by the concept of the \"third nature,\" which is occupied by those who inhabit not the actual terrain in which we live, work, and play but the virtual space of media flows that enter the \"unconscious\". See also Flowstates. \n"}
{"id": "27656741", "url": "https://en.wikipedia.org/wiki?curid=27656741", "title": "Newcastle Personality Assessor", "text": "Newcastle Personality Assessor\n\nThe Newcastle Personality Assessor (NPA) is a personality test designed to measure the test-taker's personality on five dimensions: Extroversion, Neuroticism, Conscientious, Agreeableness, and Openness. The 10-questions assessor was developed by Daniel Nettle, a behavioral scientist at the Centre for Behaviour & Evolution, Newcastle University.\n"}
{"id": "38860861", "url": "https://en.wikipedia.org/wiki?curid=38860861", "title": "Pedro Guanikeyu Torres", "text": "Pedro Guanikeyu Torres\n\nPedro Guanikeyu Torres, also known as Peter Guanikeyu Torres is a Taino Native American civil rights activist, tribal leader, educator, language teacher, tribal historian, Actor and a Taino Indian Nationalist of Puerto Rico.\n\nTorres's academic background includes studies in Cultural Anthropology, Puerto Rican studies, Art and Latin American studies. Torres graduated from Rutgers University, the State University of New Jersey at Livingston College in June 1977.\n\nIn the late 1960s and early 1970s Torres did research aimed at documenting Taino artifacts and history through the CPI Comite Pro Indigenismo. In the mid 1970s Torres had translated the very first theater script from the Spanish language to the Taino language as a former Actor and member of El Grupo Guazabara (The Guazabara Theater Group). In 1993 he founded the Taino Inter-Tribal Council (TITC, Inc.), a not-for-profit Taino cultural-educational organization. He worked at various archaeological sites, including Puerto De Tierra in the 1970s. For over five (5) decades and many years, Torres has advocated the reclaiming and repatriation of artifacts pertaining to the past Pre-Columbian historical territory of the principal regional Chief Orocobix of the Jatibonicu Taino tribe, a tribe located in the central mountain region of Puerto Rico. It should be mentioned that Torres is credited and recognized as being a Taino Native Civil Rights activist and one of the original founding Fathers of the 1968 Taino Indian Movement of Puerto Rico.\n\nHe is an activist for the official government recognition of the Taino American Indian people and his Jatibonicu Taino tribal community of Puerto Rico and in New Jersey. As tribal leader, Torres is politically active in Taino tribal council government, national and international political affairs.\n\nOften known as Don Pedro or Chief Guanikeyu, in 1996 Torres called for the \"Taino national unity of all the Taino Indian people. In 2000, Torres represented the Jatibonicu Taino Tribal Nation before the U.S. Census Bureau. As part of his Taino tribal activism he has written various papers and articles on the relevancy of Taino culture and the history of Tainos in Puerto Rico, Florida and adjacent areas.\n\nTorres also performed as a Taino language teacher and researcher. As a tribal leader, he conducted the traditional Taino Guatiao (naming ceremony) and has bestowed the Taino name on many modern Taino people today. He also promoted the Taino language when he was chosen to name a crater on planet Venus that he named Nanichi \"My Love or My Heart\" in the year 2000.\n\nDue to failing health, in 2001, Guanikeyu Torres withdrew from the leading role of his Taino tribal community and assumed the post of tribal elder.\n\n"}
{"id": "1985954", "url": "https://en.wikipedia.org/wiki?curid=1985954", "title": "Poka-yoke", "text": "Poka-yoke\n\nMore broadly, the term can refer to any behavior-shaping constraint designed into a process to prevent incorrect operation by the user. \n\nA simple poka-yoke example is demonstrated when a driver of the car equipped with a manual gearbox must press on the clutch pedal (a process step, therefore a poka-yoke) prior to starting an automobile. The interlock serves to prevent unintended movement of the car. Another example of poka-yoke would be the car equipped with an automatic transmission, which has a switch that requires the car to be in \"Park\" or \"Neutral\" before the car can be started (some automatic transmissions require the brake pedal to be depressed as well). These serve as behavior-shaping constraints as the action of \"car in Park (or Neutral)\" or \"foot depressing the clutch/brake pedal\" must be performed before the car is allowed to start. The requirement of a depressed brake pedal to shift most of the cars with an automatic transmission from \"Park\" to any other gear is yet another example of a poka-yoke application. Over time, the driver's behavior is conformed with the requirements by repetition and habit.\n\nThe term poka-yoke was applied by Shigeo Shingo in the 1960s to industrial processes designed to prevent human errors. Shingo redesigned a process in which factory workers, while assembling a small switch, would often forget to insert the required spring under one of the switch buttons. In the redesigned process, the worker would perform the task in two steps, first preparing the two required springs and placing them in a placeholder, then inserting the springs from the placeholder into the switch. When a spring remained in the placeholder, the workers knew that they had forgotten to insert it and could correct the mistake effortlessly.\n\nShingo distinguished between the concepts of inevitable human mistakes and defects in the production. Defects occur when the mistakes are allowed to reach the customer. The aim of poka-yoke is to design the process so that mistakes can be detected and corrected immediately, eliminating defects at the source.\n\nPoka-yoke can be implemented at any step of a manufacturing process where something can go wrong or an error can be made. For example, a fixture that holds pieces for processing might be modified to only allow pieces to be held in the correct orientation, or a digital counter might track the number of spot welds on each piece to ensure that the worker executes the correct number of welds.\n\nShigeo Shingo recognized three types of poka-yoke for detecting and preventing errors in a mass production system:\n\n\nEither the operator is alerted when a mistake is about to be made, or the poka-yoke device actually prevents the mistake from being made. In Shingo's lexicon, the former implementation would be called a \"warning\" poka-yoke, while the latter would be referred to as a \"control\" poka-yoke.\n\nShingo argued that errors are inevitable in any manufacturing process, but that if appropriate poka-yokes are implemented, then mistakes can be caught quickly and prevented from resulting in defects. By eliminating defects at the source, the cost of mistakes within a company is reduced.\n\nA methodic approach to build up poka-yoke countermeasures has been proposed by the Applied Problem Solving (APS) methodology, which consists of a three-step analysis of the risks to be managed:\nThis approach can be used to emphasize the technical aspect of finding effective solutions during brainstorming sessions.\n\nA typical feature of poka-yoke solutions is that they don't let an error in a process happen. But that is just one of their advantages. Others include:\n\n\n\n"}
{"id": "41111937", "url": "https://en.wikipedia.org/wiki?curid=41111937", "title": "Political appointments in the United States", "text": "Political appointments in the United States\n\nAccording to the United States Office of Government Ethics, a political appointee is \"any employee who is appointed by the President, the Vice President, or agency head\". As of 2016, there are around 4,000 political appointment positions which an incoming administration needs to review, and fill or confirm, of which about 1,200 require Senate confirmation.\n\nThese positions are published in the \"United States Government Policy and Supporting Positions\" (Plum Book), a new edition of which is released after each United States presidential election. \n\nThere are four basic categories of political appointments:\n\n\n\n\nUnlike the presidential appointments, the non-career SES and Schedule C appointments tend to be made within each agency and then approved by the Office of Presidential Personnel.\n\nPolitical appointees, referring broadly to anyone appointed by the President, the Vice President, or agency head, are subject to more ethics restrictions than regular executive-branch employees. There are two categories of appointees, and each category is subject to additional and slightly different ethics restrictions.\nPolitical appointees are required to take an ethics pledge not to accept gifts from lobbyists. This is because of Executive Order 13490 Under of Executive Order 12674, political appointees who are appointed by the president are not allowed to receive any income from outside employment or activities. Exceptions to the gift rule include:\nPolitical appointees sometimes attempt to transfer to a career position in the competitive service, excepted service, or Senior Executive Service. This practice, known as \"burrowing in\", is desired by employees due to increased pay and job security, as career positions do not end when a presidential administration changes. As these appointed positions are selected noncompetitively, while career employees are supposed to be selected on the basis of merit and without political influence, these conversions are subject to extra scrutiny. Since 2010, such conversions require advance approval from OPM, and the Government Accountability Office (GAO) periodically audits the conversions. In 2008, members of Congress criticized the Department of Homeland Security and Department of Justice for improperly allowing political employees to convert to career positions.\n\nIn the politics of the United States, the system of political appointments comes from a history of the spoils system (also known as a patronage system) which is a practice where a political party, after winning an election, gives government jobs to its supporters, friends and relatives as a reward for working toward victory. The term was derived from the phrase \"to the victor belong the spoils\" by New York Senator William L. Marcy, referring to the victory of the Jackson Democrats in the election of 1828, with the term \"spoils\" meaning goods or benefits taken from the loser in a competition, election or military victory. Though it is commonly assumed that the patronage system in the United States first came into general use during Andrew Jackson's presidency, it actually has an older history. President Thomas Jefferson, a Democratic-Republican, favored a policy of keeping rival Federalists out of government.\n\nThe patronage system thrived in the U.S. federal government until 1883. In 1820 Congress limited federal administrators to four-year terms, which led to constant turnover; by the 1860s and the Civil War, patronage had led to widespread inefficiency and political corruption. Although it used to be confined to cabinet positions, department heads, and foreign ambassadorships, by the 1860s patronage had spread to low-level government positions. This meant that when the incumbent political party lost a presidential election, the federal government underwent wholesale turnover.\n\nOn July 2, 1881, Charles J. Guiteau, a disaffected and mentally unstable political office seeker, assassinated President James Garfield. This highlighted how much the patronage problem had gotten out of control, and shifted public opinion, convincing the United States that the President of the United States had more important things to do than to engage in patronage. Congress was eventually spurred to pass the Pendleton Civil Service Reform Act of 1883, which created a Civil Service Commission and advocated a merit system for selecting government employees.\n\nIn addition, passage of the Hatch Act of 1939 forbade the intimidation or bribery of voters and restricted political campaign activities by federal employees. It prohibited using any public funds designated for relief or public works for electoral purposes. It forbade officials paid with federal funds from using promises of jobs, promotion, financial assistance, contracts, or any other benefit to coerce campaign contributions or political support, which restricted most partisan political activities of federal employees. By 1980, 90% of federal positions had become part of the civil service system, which lead to state and local governments to employed large patronage systems. Big-city political machines in places such as New York City, Boston, and Chicago thrived in the late nineteenth century. Being as a patronage system not only rewarded political supporters for past support, it also encouraged future support, because persons who have a patronage job would try to retain it by campaigning for the party at the next election. Large-scale patronage systems declined steadily during the twentieth century. During the Progressive Era (1900–1920), \"good government\" reformers overthrew political machines and installed civil service systems. Chicago, under Mayor Richard J. Daley, remained the last bastion of patronage, existing in its purest form until the late 1970s. In today’s more observant standards, politicians wanted to pull away from the negative intonation of the name casting of Patronage, and Spoil system which led them to rename the process “Political Appointments”.\n\nThe United States has more political appointees in government than any other industrialized democracy in the world. Even though the United States has one of the largest populations of political appointees in the system, the efficiency of political appointees is constantly shifting. Political appointees are engraved in everyday decisions even making the final call on major events. As of 2013, the Centers for Medicare and Medicaid Services was lacking a Senate-confirmed adminitrator since 2006 when Marilyn Tavenner was acting administrator. At least 60 positions for appointment remain vacant and 45 positions have remained vacant for more than a year.\n\nJudicial vacancies have also become a problem as well with numerous open seats for circuit and district judges left to be filled. At the end of President Obama's first term, thirteen percent of presidential-appointee positions had not been filled. Political appointees also come under heat for their own actions including Ben Bernanke's involvement with private banks and also Michael Brown's involvement in Hurricane Katrina. One study published in the \"Journal of Public Administration Research and Theory \"by Nick Gallo and David Lewis evaluated more than 350 managers with a program assessment rating tool (\"PART\") to determine efficacy and found that programs run by political appointees tended to be less effective. Furthermore, those with previous government experience or appointees who had not worked for a political campaign tended be more effective than appointees with experience in the business or non-profit sectors. Gallo and Lewis stated that they thought careerist and appointees should work in a balanced atmosphere to be more productive and share skills. Professional rapport between careerists and appointees is considered in a study of presidential environmental appointees by Matthew Auer. Auer found that oft-mentioned problems in the appointment system, such as short time-in-office and lack of government experience were less pronounced among top federal environmental appointees, across both Republican and Democratic administrations.\n\n"}
{"id": "13403445", "url": "https://en.wikipedia.org/wiki?curid=13403445", "title": "Rate of reinforcement", "text": "Rate of reinforcement\n\nIn behaviorism, rate of reinforcement is number of reinforcements per time, usually per minute. Symbol of this rate is usually \"Rf\". Its first major exponent was B.F. Skinner (1939). It is used in the Matching Law.\n\n\"Rf\" = \"# of reinforcements/unit of time\" = \"S\"/\"t\"\n\n\n"}
{"id": "3446583", "url": "https://en.wikipedia.org/wiki?curid=3446583", "title": "Referent power", "text": "Referent power\n\nReferent power is gained by a leader who has strong interpersonal relationship skills.\n\nReferent power, as an aspect of personal power, becomes particularly important as organizational leadership is increasingly about collaboration and influence rather than command and control.\n\nIn an organizational setting, referent power is most easily seen in the charismatic leader who excels in making others feel comfortable in his or her presence. Staff typically express their excitement about work in terms of their attraction to their leader's personal characteristics and charisma. The reason they commit to their work is because of the leader's likability and they base their self-esteem and sense of accomplishment on their leader's approval.\n\n\n"}
{"id": "35158738", "url": "https://en.wikipedia.org/wiki?curid=35158738", "title": "Right to sexuality", "text": "Right to sexuality\n\nThe right to sexuality incorporates the right to express one's sexuality and to be free from discrimination on the grounds of sexual orientation. In specific, it relates to the human rights of people of diverse sexual orientation, including lesbian, gay, bisexual and transgender (LGBT) people, and the protection of those rights, although it is equally applicable to heterosexuality. The right to sexuality and freedom from discrimination on the grounds of sexual orientation is based on the universality of human rights and the inalienable nature of rights belonging to every person by virtue of being human.\n\nNo right to sexuality exists explicitly in international human rights law; rather, it is found in a number of international human rights instruments including the Universal Declaration of Human Rights, the International Covenant on Civil and Political Rights and the International Covenant on Economic, Social and Cultural Rights.\n\nThe concept of the right to sexuality is difficult to define, as it comprises various rights from within the framework of international human rights law.\n\nSexual orientation is defined in the Preamble to the Yogyakarta Principles as \"each person’s capacity for profound emotional, affectional and sexual attraction to, and intimate and sexual relations with, individuals of a different gender or the same gender or more than one gender\".\n\nFreedom from discrimination on the grounds of sexual orientation is found in the Universal Declaration of Human Rights (UDHR) and the International Covenant on Civil and Political Rights (ICCPR).\n\nThe UDHR provides for non-discrimination in Article 2, which states that:\n\n\"Everyone is entitled to all the rights and freedoms set forth in this Declaration, without distinction of any kind, such as race, colour, sex, language, religion, political or other opinion, national or social origin, property, birth or other status. Furthermore, no distinction shall be made on the basis of the political, jurisdictional or international status of the country or territory to which a person belongs, whether it be independent, trust, non-self-governing or under any other limitation of sovereignty.\"\nSexual orientation can be read into Article 2 as \"other status\" or alternatively as falling under \"sex\".\n\nIn the ICCPR, Article 2 sets out a similar provision for non-discrimination:\n\n\"Each State Party to the present Covenant undertakes to respect and to ensure to all individuals within its territory and subject to its jurisdiction the rights recognized in the present Covenant, without distinction of any kind, such as race, colour, sex, language, religion, political or other opinion, national or social origin, property, birth or other status.\"\nIn Toonen v Australia the United Nations Human Rights Committee (UNHRC) found that the reference to \"sex\" in Article 2 of the ICCPR included sexual orientation, thereby making sexual orientation prohibited grounds of distinction in respect of the enjoyment of rights under the ICCPR.\n\nThe right to be free from discrimination is the basis of the right to sexuality, but it is closely related to the exercise and protection of other fundamental human rights.\n\nIndividuals of diverse sexual orientation have been discriminated against historically and continue to be a \"vulnerable\" group in society today. Forms of discrimination experienced by people of diverse sexual orientation include the denial of the right to life, the right to work and the right to privacy, non-recognition of personal and family relationships, interference with human dignity, interference with security of the person, violations of the right to be free from torture, discrimination in access to economic, social and cultural rights, including housing, health and education, and pressure to remain silent and invisible.\n\nSeventy-eight countries maintain laws that make same-sex consensual sex between adults a criminal offence, and seven countries (or parts thereof) impose the death penalty for same-sex consensual sex. They are Iran, Saudi Arabia, Yemen, Mauritania, Sudan, the twelve northern states of Nigeria, and the southern parts of Somalia.\n\nThe right to sexuality has only relatively recently become the subject of international concern, with the regulation of sexuality traditionally falling within the jurisdiction of the nation state. Today numerous international non-governmental organisations and intergovernmental organisations are engaged in the protection of the rights of people of diverse sexual orientation as it is increasingly recognised that discrimination on grounds of sexual orientation is widespread and an unacceptable violation of human rights.\n\nActs of violence against LGBT people are often especially vicious compared to other bias-motivated crimes and include killings, kidnappings, beatings, rape, and psychological violence, including threats, coercion and arbitrary depravations of liberty.\n\nExamples of violent acts against people of diverse sexual orientation are too numerous to account here, and they occur in all parts of the world. A particularly distressing example is the sexual assault and murder of fifteen lesbians in Thailand in March 2012. In that example, two lesbian couples were killed by men who objected to their relationship and who were embarrassed when they were unable to convince the women into heterosexual relationships with themselves.\n\nOften acts of violence against people of diverse sexual orientation are perpetrated by the victim's own family. In a case in Zimbabwe, the multiple rape of a lesbian was organised by her own family in an attempt to \"cure\" her of homosexuality.\n\nIn those cases, as in many other cases of violence against people of diverse sexual orientation, State law enforcement authorities are complicit in human rights abuses for failing to persecute violators of rights.\n\nThe right to privacy is a protected freedom under the UDHR, and the ICCPR which reflects the \"widespread, if not universal, human need to pursue certain activities within an intimate sphere, free of outside interference. The possibility to do so is fundamental to personhood.\" Intimate relationships, whether between two people of the same sex or of different sexes, are among those activities that are subject to a right of privacy.\n\nIt has been successfully argued in a number of cases that criminalization of homosexual relationships is an interference with the right to privacy, including decisions in the European Court of Human Rights and the UNHRC.\n\nThe freedom to decide on one's own consensual adult relationships, including the gender of that person, without the interference of the State is a fundamental human right. To prohibit the relationships of people of diverse sexual orientation is a breach of the right to sexuality and the right to privacy.\n\nEvery person, by virtue of their individual autonomy, is free to express themselves, assemble and join in association with others. Freedom of expression is a protected human right under Article 19 of the UDHR and Article 19 of the ICCPR, as is the right to freedom of assembly under Article 20 of the UDHR and Article 21 of the ICCPR.\n\nLGBT people are discriminated against in respect of their ability to defend and promote their rights. Gay pride marches, peaceful demonstrations and other events promoting LGBT rights are often banned by State governments.\n\nIn 2011 gay pride marches were banned in Serbia and another march in Moscow was broken up by police, who arrested thirty leading gay rights activists.\n\nIn 2005, twenty-nine experts undertook the drafting of the Yogyakarta Principles on the Application of International Human Rights Law in relation to Sexual Orientation and Gender Identity. The document was intended to set out experiences of human rights violations against people of diverse sexual orientation and transgender people, the application of international human rights law to those experiences and the nature of obligations on States in respect of those experiences.\n\nThe Principles can be broadly categorised into the following:\n\nThe Yogyakarta Principles is an instrument of soft law and is therefore not binding. But it does provide an important standard for States in their obligation to protect the rights of individuals of diverse sexual orientation.\n\nOn June 17, 2011 the United Nations Human Rights Council in a Resolution on Human Rights, Sexual Orientation and Gender Identity, adopted by a vote of 23 in favour, 19 against, and 3 abstentions, requested the commission of a study to document discriminatory laws and acts of violence against people based on their sexual orientation and gender identity.\n\nThe 2011 Resolution was intended to shed light on how international human rights could be used to prevent acts of violence and discrimination against people of diverse sexual orientation.\n\nOn 15 December 2011 the first Report on human rights of LGBT people was released by the Office of the United Nations High Commissioner for Human Rights.\n\nThe Report made the following recommendations. In order to prevent such acts of violence occurring, United Nations Member States are recommended to:\n\nFurther action is yet to be taken by the United Nations, although a proposed declaration on sexual orientation and gender identity was brought before the United Nations General Assembly in 2008. However, that declaration has not been officially adopted by the General Assembly and remains open for signatories.\n\n\n"}
{"id": "2220565", "url": "https://en.wikipedia.org/wiki?curid=2220565", "title": "Rudder ratio", "text": "Rudder ratio\n\nRudder ratio refers to a value that is monitored by the computerized flight control systems in modern aircraft. The ratio relates the aircraft airspeed to the rudder deflection setting that is in effect at the time. As an aircraft accelerates, the deflection of the rudder needs to be reduced proportionately within the range of the rudder pedal depression by the pilot. This automatic reduction process is needed because if the rudder is fully deflected when the aircraft is in high-speed flight, it will cause the plane to sharply and violently yaw, or swing from side to side, leading to loss of control and rudder, tail and other damages, even causing the aircraft to crash.\n\n"}
{"id": "743785", "url": "https://en.wikipedia.org/wiki?curid=743785", "title": "Ruling class", "text": "Ruling class\n\nThe ruling class is the social class of a given society that decides upon and sets that society's political agenda.\n\nSociologist C. Wright Mills (1916–1962) argued that the ruling class differs from the power elite. The latter simply refers to the small group of people with the most political power. Many of them are politicians, hired political managers and/or military leaders. The ruling class are people who directly influence politics, education and government with the use of wealth or power.\n\nAnalogous to the class of the major capitalists, other modes of production give rise to different ruling classes: under feudalism it was the feudal lords while under slavery it was the slave-owners. Under the feudal society, feudal lords had power over the vassals because of their control of the fiefs. This gave them political and military power over the people. In slavery, because complete rights of the person's life belonged to the slave owner, they could and did every implementation that would help the production in the farm.\n\nIn his recent studies on elites in contemporary societies, Mattei Dogan has argued that because of their complexity and their heterogeneity and particularly because of the social division of work and the multiple levels of stratification, there is not, or can not be, a coherent ruling class, even if in the past there were solid examples of ruling classes as in the Russian and Ottoman Empires and the more recent totalitarian regimes of the 20th century (Communist and Fascist).\n\nMilovan Djilas said that in a Communist regime the nomenklatura form a ruling class, which \"benefited from the use, enjoyment, and disposition of material goods\", thus controls all of the property and thus all of the wealth of the nation. Furthermore, he argued, the Communist bureaucracy was not an accidental mistake, but the central inherent aspect of the Communist system since a Communist regime would not be possible without the system of bureaucrats.\n\nGlobalization theorists argue that today a transnational capitalist class has emerged.\n\nThere are several examples of ruling class systems in movies, novels and television shows. The 2005 American independent film \"The American Ruling Class\" written by former \"Harper's Magazine\" editor Lewis Lapham and directed by John Kirby is a semi-documentary that examines how the American economy is structured and for whom. The 2017 Philippine political crime-suspense epic \"Wildflower\" is about a rich influential and corrupt political family the Ardientes ruling over a town where a wave of murders and crimes which they have committed washed over.\n\nSociety, in the novel \"Brave New World\", by Aldous Huxley, is eusocial with a genetically engineered caste system. The alpha++ class is the ruling class having been bred as scientists and administrators and control the World State in the novel. This situation can also be found in the George Orwell novel \"Nineteen Eighty-Four\" where the inner party as symbolized by the fictitious Big Brother literally controls what everyone in the outer party hears, sees and learns, albeit without genetic engineering and on the model of Stalinist communism having taken over the Anglosphere (Oceania). In Oceania, the ignorant masses (\"proles\") are relatively free as they pose no threat to oligarchical collectivism (\"Big Brother\").\n\nExamples in movies include \"Gattaca\", where the genetically-born were superior and the ruling class; and \"V for Vendetta\", which depicted a powerful totalitarian government in Britain. The comedic film \"The Ruling Class\" was a satire of British aristocracy, depicting nobility as self-serving and cruel, juxtaposed against an insane relative who believes that he is Jesus Christ, whom they identify as a \"bloody Bolshevik\".\n\n"}
{"id": "4430300", "url": "https://en.wikipedia.org/wiki?curid=4430300", "title": "Sociocultural system", "text": "Sociocultural system\n\nA sociocultural system is a \"human population viewed (1) in its ecological context and (2) as one of the many subsystems of a larger ecological system\".\n\nIn 1979, Marvin Harris outlined a universal structure of sociocultural systems. He mentioned infrastructure (production and population), structure (which is behavioural, like corporations, political organizations, hierarchies, castes), and a superstructure (which is mental, like beliefs, values, norms).\n\n"}
{"id": "22635341", "url": "https://en.wikipedia.org/wiki?curid=22635341", "title": "Southern Regional Council", "text": "Southern Regional Council\n\nThe Southern Regional Council (SRC) is a reform-oriented organization created in 1944 to avoid racial violence and promote racial equality in the Southern United States. Voter registration and political-awareness campaigns are used toward this end. The SRC evolved in 1944 from the Commission on Interracial Cooperation. It is headquartered in Atlanta, Georgia.\n\nThe Commission on Interracial Cooperation (CIC) was formed in 1919. The CIC formed in response to the increased tensions between white Americans and black soldiers returning from fighting in Europe after World War I. Although most African Americans still lived in the South, the Great Migration had started to the North and Midwestern industrial cities, and thousands of blacks were living in new urban environments. They often had to compete with immigrants and ethnic whites for jobs and housing. In the summer of 1919, race riots erupted in numerous major cities as whites attacked blacks. African-American veterans and others resisted being treated as second-class citizens and fought back, especially in Chicago and Washington, DC. during what has been called \"Red Summer\" because of all the violence. Black veterans in the South were confronted with expectations they would submit to Jim Crow and lynchings of black men rose after the war, including of some veterans in uniform.\n\nDuring World War II, members of the CIC realized that the same problem could recur during and following that war. In 1943, leaders from the CIC, including sociologist Howard W. Odum, held a series of conferences in Durham, North Carolina; Richmond, Virginia, and Atlanta, Georgia. As a result, they formed the Southern Regional Council, with Odum selected as its leader. The CIC was disbanded, essentially being merged with the new SRC in 1944. The SRC was formed \"to attain through research and action the ideals and practices of equal opportunity for all peoples of the region.\"\n\nThe SRC urged whites, particularly those with more liberal political attitudes, to help black people obtain equal rights. Like the CIC before it, the SRC was a coalition of lawyers, Christian ministers, and newspaper editors from thirteen southern states. Although the group was bi-racial and included both men and women, the majority of its members were white.\n\nInitially, Odum sought to bring about racial equality in the Southern US by improving economic, social and political conditions. The SRC avoided taking a public stand against legal segregation, on the belief that this would hinder progress toward its economic-planning goals. Critics of this approach, such as activist author Lillian Smith, believed that the SRC should condemn the state-imposed legal segregation. In 1949 the SRC declared in a resolution that segregation \"in and of itself constitutes discrimination and inequality of treatment.\" As a result, many whites left the SRC, resulting in a decline of membership by almost half by 1954.\n\nOften partners with other groups involved in the Civil Rights Movement, the SRC used communications and analysis to try to reach people through facts and education. It published literature related to racial justice, released studies on race relations, and acted as a think tank for issues concerning the movement.\n\nSince 1944, the SRC has published some form of journal. The Council's first publication, \"Southern Frontier\", was published for two years before being reformatted and renamed as \"New South\". In 1974, \"New South\" and a companion tabloid \"South Today\" were merged into a color glossy magazine, \"Southern Voices\". This published for ten months but ceased because of financial issues.\n\nThe SRC journal \"Southern Changes\" was published between 1978 and 2003. Emory University, in partnership with the Library of Congress, has digitally preserved the journal, described as \"an alternative and groundbreaking news outlet for stories on social justice in the South.\"\n\nThe Council publishes various issues briefs, position papers, and legislative reviews, including the annuals \"Southern States Legislative Review\" and \"State of the South Report\".\n\nThe SRC served as a liaison between a number of southern organizations and northern foundations, providing resources and opportunities for mutual understanding. The organization created the Voter Education Project, building on an idea from U.S. Attorney General Robert F. Kennedy during the Kennedy administration; the project was run by the SRC from its inception on April 1, 1962 until it was made an independent organization on June 1, 1971. The Voter Education Project did not actually register voters; instead, it acted as a conduit between philanthropic grants and civil rights organizations conducting voter registration drives or voting-related research. For example, the Project funded voter-registration work by the National Urban League; in October 1962, the Jefferson County (Alabama) Voters Campaign received assistance with a registration effort from the League.\n\nThe Lillian Smith Book Award was established by the SRC in 1968, shortly after writer Lillian Smith died, to \"recognize authors whose writing extends the legacy of this outspoken writer, educator and social critic who challenged her fellow Southerners and all Americans on issues of social and racial justice.\"\n\n"}
{"id": "12563101", "url": "https://en.wikipedia.org/wiki?curid=12563101", "title": "Speech production", "text": "Speech production\n\nSpeech production is the process by which thoughts are translated into speech. This includes the selection of words, the organization of relevant grammatical forms, and then the articulation of the resulting sounds by the motor system using the vocal apparatus. Speech production can be spontaneous such as when a person creates the words of a conversation, reactive such as when they name a picture or read aloud a written word, or imitative, such as in speech repetition. Speech production is not the same as language production since language can also be produced manually by signs.\n\nIn ordinary fluent conversation people pronounce roughly four syllables, ten or twelve phonemes and two to three words out of their vocabulary (that can contain 10 to 100 thousand words) each second. Errors in speech production are relatively rare occurring at a rate of about once in every 900 words in spontaneous speech. Words that are commonly spoken or learned early in life or easily imagined are quicker to say than ones that are rarely said, learnt later in life, or are abstract.\n\nNormally speech is created with pulmonary pressure provided by the lungs that generates sound by phonation through the glottis in the larynx that then is modified by the vocal tract into different vowels and consonants. However speech production can occur without the use of the lungs and glottis in alaryngeal speech by using the upper parts of the vocal tract. An example of such alaryngeal speech is Donald Duck talk.\n\nThe vocal production of speech may be associated with the production of hand gestures that act to enhance the comprehensibility of what is being said.\n\nThe development of speech production throughout an individual's life starts from an infant's first babble and is transformed into fully developed speech by the age of five. The first stage of speech doesn't occur until around age one (holophrastic phase). Between the ages of one and a half and two and a half the infant can produce short sentences (telegraphic phase). After two and a half years the infant develops systems of lemmas used in speech production. Around four or five the child's lemmas are largely increased, this enhances the child's production of correct speech and they can now produce speech like an adult. An adult now develops speech in four stages: Activation of lexical concepts, select lemmas needed, morphologically and phonologically encode speech, and the word is phonetically encoded.\n\nThe production of spoken language involves three major levels of processing: conceptualization, formulation, and articulation.\n\nThe first is the processes of conceptualization or conceptual preparation, in which the intention to create speech links a desired concept to the particular spoken words to be expressed. Here the preverbal intended messages are formulated that specify the concepts to be expressed.\n\nThe second stage is formulation in which the linguistic form required for the expression of the desired message is created. Formulation includes grammatical encoding, morpho-phonological encoding, and phonetic encoding. Grammatical encoding is the process of selecting the appropriate syntactic word or lemma. The selected lemma then activates the appropriate syntactic frame for the conceptualized message. Morpho-phonological encoding is the process of breaking words down into syllables to be produced in overt speech. Syllabification is dependent on the preceding and proceeding words, for instance:\n\"I-com-pre-hend\" vs. \"I-com-pre-hen-dit\".\nThe final part of the formulation stage is phonetic encoding. This involves the activation of articulatory gestures dependent on the syllables selected in the morpho-phonological process, creating an articulatory score as the utterance is pieced together and the order of movements of the vocal apparatus is completed.\n\nThe third stage of speech production is articulation, which is the execution of the articulatory score by the lungs, glottis, larynx, tongue, lips, jaw and other parts of the vocal apparatus resulting in speech.\n\nThe motor control for speech production in right handed people depends mostly upon areas in the left cerebral hemisphere. These areas include the bilateral supplementary motor area, the left posterior inferior frontal gyrus, the left insula, the left primary motor cortex and temporal cortex. There are also subcortical areas involved such as the basal ganglia and cerebellum. The cerebellum aids the sequencing of speech syllables into fast, smooth and rhythmically organized words and longer utterances.\n\nSpeech production can be affected by several disorders:\n\n\n\nUntil the late 1960s research on speech was focused on comprehension. As researchers collected greater volumes of speech error data, they began to investigate the psychological processes responsible for the production of speech sounds and to contemplate possible processes for fluent speech. Findings from speech error research were soon incorporated into speech production models. Evidence from speech error data supports the following conclusions about speech production.\n\nSome of these ideas include:\n\n\nModels of speech production must contain specific elements to be viable. These include the elements from which speech is composed, listed below. The accepted models of speech production discussed in more detail below all incorporate these stages either explicitly or implicitly, and the ones that are now outdated or disputed have been criticized for overlooking one or more of the following stages.\n\nThe attributes of accepted speech models are:\n\na) a conceptual stage where the speaker abstractly identifies what they wish to express.\n\nb) a syntactic stage where a frame is chosen that words will be placed into, this frame is usually sentence structure.\n\nc) a lexical stage where a search for a word occurs based on meaning. Once the word is selected and retrieved, information about it becomes available to the speaker involving phonology and morphology.\n\nd) a phonological stage where the abstract information is converted into a speech like form.\n\ne) a phonetic stage where instructions are prepared to be sent to the muscles of articulation.\n\nAlso, models must allow for forward planning mechanisms, a buffer, and a monitoring mechanism.\n\nFollowing are a few of the influential models of speech production that account for or incorporate the previously mentioned stages and include information discovered as a result of speech error studies and other disfluency data, such as tip-of-the-tongue research.\n\nThe Utterance Generator Model was proposed by Fromkin (1971). It is composed of six stages and was an attempt to account for the previous findings of speech error research. The stages of the Utterance Generator Model were based on possible changes in representations of a particular utterance. The first stage is where a person generates the meaning they wish to convey. The second stage involves the message being translated onto a syntactic structure. Here, the message is given an outline. The third stage proposed by Fromkin is where/when the message gains different stresses and intonations based on the meaning. The fourth stage Fromkin suggested is concerned with the selection of words from the lexicon. After the words have been selected in Stage 4, the message undergoes phonological specification. The fifth stage applies rules of pronunciation and produces syllables that are to be outputted. The sixth and final stage of Fromkin's Utterance Generator Model is the coordination of the motor commands necessary for speech. Here, phonetic features of the message are sent to the relevant muscles of the vocal tract so that the intended message can be produced. Despite the ingenuity of Fromkin's model, researchers have criticized this interpretation of speech production. Although The Utterance Generator Model accounts for many nuances and data found by speech error studies, researchers decided it still had room to be improved.\n\nA more recent (than Fromkin's) attempt to explain speech production was published by Garrett in 1975. Garrett also created this model by compiling speech error data and there are many overlaps between this model and the Fromkin model off which it was based, but he did add a few things to the Fromkin model that filled some of the gaps being pointed out by other researchers. The Garrett Model and the Fromkin model both distinguish between three levels—a conceptual level, and sentence level, and a motor level. These three levels are common to contemporary understanding of Speech Production.\n\nIn 1994, Dell proposed a model of the lexical network that became fundamental in the understanding of the way speech is produced. This model of the lexical network attempts to symbolically represent the lexicon, and in turn, explain how people choose the words they wish to produce, and how those words are to be organized into speech. Dell's model was composed of three stages, semantics, words, and phonemes. The words in the highest stage of the model represent the semantic category. (In the image, the words representing semantic category are winter, footwear, feet, and snow represent the semantic categories of boot and skate.) The second level represents the words that describe the semantic category (In the image, boot and skate). And, the third level represents the phonemes ( syllabic information including onset, vowels, and codas).\n\nLevelt further refined the lexical network proposed by Dell. Through the use of speech error data, Levelt recreated the three levels in Dell's model. The conceptual stratum, the top and most abstract level, contains information a person has about ideas of particular concepts. The conceptual stratum also contains ideas about how concepts relate to each other. This is where word selection would occur, a person would choose which words they wish to express. The next, or middle level, the lemma-stratum, contains information about the syntactic functions of individual words including tense and function. This level functions to maintain syntax and place words correctly into sentence structure that makes sense to the speaker. The lowest and final level is the form stratum which, similarly to the Dell Model, contains syllabic information. From here, the information stored at the form stratum level is sent to the motor cortex where the vocal apparatus are coordinated to physically produce speech sounds.\n\nThe physical structure of the human nose, throat, and vocal cords allows for the productions of many unique sounds, these areas can be further broken down into places of articulation. Different sounds are produced in different areas, and with different muscles and breathing techniques. Our ability to utilize these skills to create the various sounds needed to communicate effectively is essential to our speech production. Speech is a psychomotor activity. Speech between two people is a conversation - they can be casual, formal, factual, or transactional, and the language structure/ narrative genre employed differs depending upon the context. Affect is a significant factor that controls speech, manifestations that disrupt memory in language use due to affect include feelings of tension, states of apprehension, as well as physical signs like nausea. Language level manifestations that affect brings could be observed with the speaker's hesitations, repetitions, false starts, incompletion, syntactic blends, etc. Difficulties in manner of articulation can contribute to speech difficulties and impediments. It is suggested that infants are capable of making the entire spectrum of possible vowel and consonant sounds. IPA has created a system for understanding and categorizing all possible speech sounds, which includes information about the way in which the sound is produced, and where the sounds is produced. This is extremely useful in the understanding of speech production because speech can be transcribed based on sounds rather than spelling, which may be misleading depending on the language being spoken. Average speaking rates are in the 120 to 150 words per minute (wpm) range, and same is the recommended guidelines for recording audiobooks. As people grow accustomed to a particular language they are prone to lose not only the ability to produce certain speech sounds, but also to distinguish between these sounds.\n\nArticulation, often associated with speech production, is the term used to describe how people physically produced speech sounds. For people who speak fluently, articulation is automatic and allows 15 speech sounds to be produced per second.\n\nAn effective articulation of speech include the following elements – fluency, complexity, accuracy, and comprehensibility.\n\n\n\n\n\nBefore even producing a sound, infants imitate facial expressions and movements. Around 7 months of age, infants start to experiment with communicative sounds by trying to coordinate producing sound with opening and closing their mouths.\n\nUntil the first year of life infants cannot produce coherent words, instead they produce a reoccurring babbling sound. Babbling allows the infant to experiment with articulating sounds without having to attend to meaning. This repeated babbling starts the initial production of speech. Babbling works with object permanence and understanding of location to support the networks of our first lexical items or words. The infant’s vocabulary growth increases substantially when they are able to understand that objects exist even when they are not present.\n\nThe first stage of meaningful speech does not occur until around the age of one. This stage is the holophrastic phase. The holistic stage refers to when infant speech consists of one word at a time (i.e. papa).\n\nThe next stage is the telegraphic phase. In this stage infants can form short sentences (i.e., Daddy sit, or Mommy drink). This typically occurs between the ages of one and a half and two and a half years old. This stage is particularly noteworthy because of the explosive growth of their lexicon. During this stage, infants must select and match stored representations of words to the specific perceptual target word in order to convey meaning or concepts. With enough vocabulary, infants begin to extract sound patterns, and they learn to break down words into phonological segments, increasing further the number of words they can learn. At this point in an infant's development of speech their lexicon consists of 200 words or more and they are able to understand even more than they can speak.\nWhen they reach two and a half years their speech production becomes increasingly complex, particularly in its semantic structure. With a more detailed semantic network the infant learns to express a wider range of meanings, helping the infant develop a complex conceptual system of lemmas.\n\nAround the age of four or five the child lemmas have a wide range of diversity, this helps them select the right lemma needed to produce correct speech. Reading to infants enhances their lexicon. At this age, children who have been read to and are exposed to more uncommon and complex words have 32 million more words than a child who is linguistically impoverished. At this age the child should be able to speak in full complete sentences, similar to an adult.\n\n\n"}
{"id": "6105873", "url": "https://en.wikipedia.org/wiki?curid=6105873", "title": "Tarski–Grothendieck set theory", "text": "Tarski–Grothendieck set theory\n\nTarski–Grothendieck set theory (TG, named after mathematicians Alfred Tarski and Alexander Grothendieck) is an axiomatic set theory. It is a non-conservative extension of Zermelo–Fraenkel set theory (ZFC) and is distinguished from other axiomatic set theories by the inclusion of Tarski's axiom, which states that for each set there is a Grothendieck universe it belongs to (see below). Tarski's axiom implies the existence of inaccessible cardinals, providing a richer ontology than that of conventional set theories such as ZFC. For example, adding this axiom supports category theory.\n\nThe Mizar system and Metamath use Tarski–Grothendieck set theory for formal verification of proofs.\n\nTarski–Grothendieck set theory starts with conventional Zermelo–Fraenkel set theory and then adds \"Tarski's axiom\". We will use the axioms, definitions, and notation of Mizar to describe it. Mizar's basic objects and processes are fully formal; they are described informally below. First, let us assume that:\n\n\nTG includes the following axioms, which are conventional because they are also part of ZFC:\n\nIt is Tarski's axiom that distinguishes TG from other axiomatic set theories. Tarski's axiom also implies the axioms of infinity, choice, and power set. It also implies the existence of inaccessible cardinals, thanks to which the ontology of TG is much richer than that of conventional set theories such as ZFC.\n\n\n- formula_7 itself;\n\n- every subset of every member of formula_10;\n\n- the power set of every member of formula_10;\n\n- every subset of formula_10 of cardinality less than that of formula_10.\n\nMore formally:\n\nwhere \"formula_17\" denotes the power class of \"x\" and \"formula_18\" denotes equinumerosity. What Tarski's axiom states (in the vernacular) for each set formula_7 there is a Grothendieck universe it belongs to.\n\nThere y looks much like a \"universal set\" for x - it not only has as members the powerset of x, and all subsets of x, it also has the powerset of that powerset and so on - its members are closed under the operations of taking powerset or taking a subset. It's like a \"universal set\" except that of course it is not a member of itself and is not a set of all sets. That's the guaranteed Grothendieck universe it belongs to. And then any such y is itself a member of an even larger \"almost universal set\" and so on. It's one of the strong cardinality axioms guaranteeing vastly more sets than one normally assumes to exist.\n\nThe Mizar language, underlying the implementation of TG and providing its logical syntax, is typed and the types are assumed to be non-empty. Hence, the theory is implicitly taken to be non-empty. The existence axioms, e.g. the existence of the unordered pair, is also implemented indirectly by the definition of term constructors.\n\nThe system includes equality, the membership predicate and the following standard definitions:\n\nThe Metamath system supports arbitrary higher-order logics, but it is typically used with the \"set.mm\" definitions of axioms. The ax-groth axiom adds Tarski's axiom, which in Metamath is defined as follows:\n\n⊢ ∃y(x ∈ y ∧ ∀z ∈ y (∀w(w ⊆ z → w ∈ y) ∧ ∃w ∈ y ∀v(v ⊆ z → v ∈ w)) ∧ ∀z(z ⊆ y → (z ≈ y ∨ z ∈ y)))\n\n\n\n"}
{"id": "37784373", "url": "https://en.wikipedia.org/wiki?curid=37784373", "title": "Tensor Contraction Engine", "text": "Tensor Contraction Engine\n\nThe Tensor Contraction Engine (TCE) is a compiler for a domain-specific language that allows chemists to specify the computation in a high-level Mathematica-style language. It transforms tensor summation expressions to low-level code (C/Fortran) for specific hardware being mindful of memory availability, communication costs, loop fusion and ordering, etc. It is used primarily in computational chemistry.\n\n"}
{"id": "163766", "url": "https://en.wikipedia.org/wiki?curid=163766", "title": "The City of the Sun", "text": "The City of the Sun\n\nThe City of the Sun (; ) is a philosophical work by the Italian Dominican philosopher Tommaso Campanella. It is an important early utopian work. The work was written in Italian in 1602, shortly after Campanella's imprisonment for heresy and sedition. A Latin version was written in 1613–1614 and published in Frankfurt in 1623.\n\nThe book is presented as a dialogue between \"a Grandmaster of the Knights Hospitaller and a Genoese Sea-Captain\". Inspired by Plato's \"Republic\" and the description of Atlantis in \"Timaeus\", it describes a theocratic society where goods, women and children are held in common. It also resembles the City of Adocentyn in the \"Picatrix\", an Arabic grimoire of astrological magic. In the final part of the work, Campanella prophesies—in the veiled language of astrology—that the Spanish kings, in alliance with the Pope, are destined to be the instruments of a Divine Plan: the final victory of the True Faith and its diffusion in the whole world. While one could argue that Campanella was simply thinking of the conquest of the New World, it seems that this prophecy should be interpreted in the light of a work written shortly before \"The City of the Sun\", \"The Monarchy in Spain\", in which Campanella exposes his vision of a unified, peaceful world governed by a theocratic monarchy.\n\nProtected and defended by seven circles of walls, constructed of palaces that serve as dwellings for the citizens, the city is located in a place with an ideal climate, conducive to physical health, and on the slope of a hillside because the air there is lighter and purer. One of the most significant aspects of this community is the distribution of work. Once again Campanella engages in an explicit polemic with Aristotle, who had excluded artisans, peasants and those involved in manual labor from the category of full citizenship and from the highest levels of virtue.\n\nIn the City of the Sun no occupation is vile or base, and all are of equal dignity—in fact, those workers who are required to expend greater effort, such as artisans and builders, receive more praise. Everyone must be acquainted with all lines of work, and then each person practices the one for which he shows the greatest aptitude. They have no servants, and no service is regarded as unworthy. The only thing that they consider to be despicable is idleness, and in this way they come to privilege the dignity of work and to overturn an absurd conception of nobility, linked to inactivity and vice.\n\nThanks to the equal division of labour, it is sufficient for each person to spend only four hours a day working; but it is essential that they all work, because the idleness of one would have repercussions on the profit and the effort of the others. The citizens possess nothing; instead, everything is held in common, from food to houses, from the acquisition of knowledge to the exercise of activities, from honors to amusements, from women to children.\n\nThere are “officials” in charge of the distribution of each thing, who keep an eye out and make sure that this happens justly, but no one can appropriate anything for himself. According to them, possession of a house or a family reinforces “self-love”, with all the dire consequences this generates. They live “like philosophers in common” because they are aware of the negative impact, not only on the social but also on the moral level, of an unequal distribution of goods.\n\nOne of the most spectacular and imaginative aspects of The City of the Sun, which immediately struck its readers, are the painted walls of the city. Apart from enclosing and protecting the city, the walls are also the curtains of an extraordinary theater and the pages of an illustrated encyclopedia of knowledge. The walls of the palaces are painted with images of all the arts and sciences.\n\nStarting with the wall that holds up the columns of the temple and gradually descending in large circles, following the order of the planets from Mercury to Saturn, we encounter illustrations of the heavens and the stars, of mathematical figures, of every country on earth and of all the marvels and secrets of the mineral, vegetable and animal worlds, until we arrive at mankind: on the internal wall of the sixth circle the mechanical arts and their inventors are represented.\n\nCampanella was greatly interested in all ingenious discoveries, and in The City of the Sun he provides many examples of curious inventions, such as vessels able to navigate without wind and without sails, and stirrups that make it possible to guide a horse using only one's feet, leaving one's hands free. On the external wall legislators are depicted; and it is here, in “a place of great honor”—but along with Moses, Osiris, Jove, Mercury and Muhammad—that the Genoese sailor recognizes Christ and the twelve apostles. Knowledge is not enclosed in books kept in separate places such as libraries but is openly on show to everyone's eyes. Visualizing in this manner promotes a quicker, easier and more efficient form of learning, in that it is connected to the art of memory, which underlines the evocative and emotive power of images. From a tender age children run around in this theater of knowledge, appropriately guided and following correct itineraries, so that they learn joyously, as if playing a game, without effort or pain.\n\nIn addition to the community of goods and the painted walls, another characteristic feature of the City of the Sun, one that is more difficult and disconcerting and that Campanella himself describes as “hard and arduous”, is the community of wives. This is the solution adopted by the citizens to the problem of generation. Echoing the teaching of the Pythagorean Ocellus Lucanus, Campanella says that they are amazed that humans are preoccupied by the breeding of horses and dogs while neglecting their own. The act of generation entails a large responsibility of the part of the parents; and if it is exercised in an incorrect manner, it can give rise to a long chain of suffering.\n\nMoreover, there is a close connection between a person's natural “complexion” or character, which is inborn and not afterwards modifiable, and moral virtue, which needs a suitable terrain in order to take root and prosper. Generation should therefore respect precise norms and not be entrusted to chance nor to individual sentiments. The citizens distinguish between love and sex. Affection between men and women, based on friendship and respect more than sexual attraction, is expressed in acts that are far removed from sexuality, such as exchanges of gifts, conversation and dancing. Sexual generation, on the other hand, must obey strict rules regarding the physical and moral qualities of the parents and the choice of a propitious time for conception, determined by an astrologer. Such a union is not the expression of a personal, emotional or passionate relationship, but rather is connected to the social responsibility of generation and to love for the collective community.\n\nThe religious beliefs of the citizenry, even though they include fundamental principles of Christianity (such as the immortality of the soul and divine providence), form a natural religion that establishes a sort of osmosis between the city and the stars. The temple is open and not surrounded by walls. In one of his poems Campanella promises: “I shall make the heavens a temple and the stars an altar”. On the vault of the temple's dome the stars are depicted together with their influence on earthly affairs. The altar, on which are placed a celestial and a terrestrial globe, is in the form of the sun. Prayers are directed toward the heavens. The task of the twenty-four priests, who live in cells located in the highest part of the temple, is to observe the stars and, using astronomical instruments, to take account of all their movements. It is their job to indicate the times most favorable for generation and for agricultural labors, acting in this way as intermediaries between God and human beings.\n\nIn Trento's Civic Library, there is kept a 1602 manuscript of \"The City of the Sun\" (shelf mark BCT1-1538), discovered in 1943 by Italian historian Luigi Firpo. It is considered the most ancient manuscript copy that has survived to present time. The text arrived at the Library through the baron Antonio Mazzetti 's (1781–1841) bequest. He was a book collector and bibliophile and, as written in his will, he donated his book heritage to the Civic Library. \n\nThe manuscript was restored in 1980. It is made of parchment tied on paperboard. It consists of two codicological unities joined together years after their writing: the first is a Venetian historical chronicle from 1297 to 1582, followed by a list of \"Hospedali di Venezia\" (\"Hospitals in Venice\"). At the bottom, it is sewed to a small-sized booklet independently enumerated: it's a copy by anonymous author of \"The City of the Sun\". The transcription is meticulous and there only are a few insignificant mistakes.\n\nIn 2016, the manuscript was uploaded to the digital library Wikisource.\n\n\n\n\n"}
{"id": "34038330", "url": "https://en.wikipedia.org/wiki?curid=34038330", "title": "Theta model", "text": "Theta model\n\nThe theta model, or Ermentrout–Kopell canonical model, is a biological neuron model originally developed to model neurons in the animal Aplysia, and later used in various fields of computational neuroscience. The model is particularly well suited to describe neuron bursting, which are rapid oscillations in the membrane potential of a neuron interrupted by periods of relatively little oscillation. Bursts are often found in neurons responsible for controlling and maintaining steady rhythms. For example, breathing is controlled by a small network of bursting neurons in the brain stem. Of the three main classes of bursting neurons (square wave bursting, parabolic bursting, and elliptic bursting), the theta model describes parabolic bursting. Parabolic bursting is characterized by a series of bursts that are regulated by a slower external oscillation. This slow oscillation changes the frequency of the faster oscillation so that the frequency curve of the burst pattern resembles a parabola.\n\nThe model has just one state variable which describes the membrane voltage of a neuron. In contrast, the Hodgkin–Huxley model consists of four state variables (one voltage variable and three gating variables) and the Morris–Lecar model is defined by two state variables (one voltage variable and one gating variable). The single state variable of the theta model, and the elegantly simple equations that govern its behavior allow for analytic, or closed-form solutions (including an explicit expression for the phase response curve). The dynamics of the model take place on the unit circle, and are governed by two cosine functions and a real-valued input function.\n\nSimilar models include the quadratic integrate and fire (QIF) model, which differs from the theta model by only by a change of variables and Plant's model, which consists of Hodgkin–Huxley type equations and also differs from the theta model by a series of coordinate transformations.\n\nDespite its simplicity, the theta model offers enough complexity in its dynamics that it has been used for a wide range of theoretical neuroscience research as well as in research beyond biology, such as in artificial intelligence.\n\nBursting is \"an oscillation in which an observable [part] of the system, such as voltage or chemical concentration, changes periodically between an active phase of rapid spike oscillations (the fast sub-system) and a phase of quiescence\". Bursting comes in three distinct forms: square wave bursting, parabolic bursting, and elliptic bursting. There exist some models that do not fit neatly into these categories by qualitative observation, but it is possible to sort such models by their topology (i.e. such models can be sorted \"by the structure of the fast subsystem\").\n\nAll three forms of bursting are capable of beating and periodic bursting. Periodic bursting (or just bursting) is of more interest because many phenomena are controlled by, or arise from, bursting. For example, bursting due to a changing membrane potential is common in various neurons, including but not limited to cortical chattering neurons, thalamacortical neurons, and pacemaker neurons. Pacemakers in general are known to burst and synchronize as a population, thus generating a robust rhythm that can maintain repetitive tasks like breathing, walking, and eating. Beating occurs when a cell bursts continuously with no periodic quiescent periods, but beating is often considered to be an extreme case and is rarely of primary interest.\n\nBursting cells are important for motor generation and synchronization. For example, the pre-Bötzinger complex in the mammalian brain stem contains many bursting neurons that control autonomous breathing rhythms. Various neocortical neurons (i.e. cells of the neocortex) are capable of bursting, which \"contribute significantly to [the] network behavior [of neocortical neurons]\". The R15 neuron of the abdominal ganglion in \"Aplyisa\", hypothesized to be a [pneurosecretory[] cell (i.e. a cell that produces hormones), is known to produce bursts characteristic of neurosecretory cells. In particular, it is known to produce parabolic bursts.\n\nSince many biological processes involve bursting behavior, there is a wealth of various bursting models in scientific literature. For instance, there exist several models for interneurons and cortical spiking neurons. However, the literature on parabolic bursting models is relatively scarce.\n\nParabolic bursting models are mathematical models that mimic parabolic bursting in real biological systems. Each burst of a parabolic burster has a characteristic feature in the burst structure itself – the frequency at the beginning and end of the burst is low relative to the frequency in the middle of the burst. A frequency plot of one burst resembles a parabola, hence the name \"parabolic burst\". Furthermore, unlike elliptic or square-wave bursting, there is a slow modulating wave which, at its peak, excites the cell enough to generate a burst and inhibits the cell in regions near its minimum. As a result, the neuron periodically transitions between bursting and quiescence.\n\nParabolic bursting has been studied most extensively in the R15 neuron, which is one of six types of neurons of the \"Aplysia\" abdominal ganglion and one of thirty neurons comprising the abdominal ganglion. The \"Aplysia\" abdominal ganglion was studied and extensively characterized because its relatively large neurons and proximity of the neurons to the surface of the ganglion made it an ideal and \"valuable preparation for cellular electrophysical studies\".\n\nEarly attempts to model parabolic bursting were for specific applications, often related to studies of the R15 neuron. This is especially true of R. E. Plant and Carpenter, whose combined works comprise the bulk of parabolic bursting models prior to Ermentrout and Kopell's canonical model.\n\nThough there was no specific mention of the term \"parabolic bursting\" in Plant's papers, Plant's model(s) do involve a slow, modulating oscillation which control bursting in the model(s). This is, by definition, parabolic bursting. Both of Plant's papers on the topic involve a model derived from the Hodgkin–Huxley equations and include extra conductances, which only add to the complexity of the model.\n\nCarpenter developed her model primarily for a square wave burster. The model was capable of producing a small variety of square wave bursts and produced parabolic bursts as a consequence of adding an extra conductance. However, the model applied to only spatial propagation down axons and not situations where oscillations are limited to a small region in space (i.e. it was not suited for \"space-clamped\" situations).\n\nThe lack of a simple, generalizable, space-clamped, parabolic bursting model motivated Ermentrout and Kopell to develop the theta model.\n\nIt is possible to describe a multitude of parabolic bursting cells by deriving a simple mathematical model, called a canonical model. Derivation of the Ermentrout and Kopell canonical model begins with the general form for parabolic bursting, and notation will be fixed to clarify the discussion. The letters formula_1, formula_2, formula_3, formula_4 are reserved for functions; formula_5, formula_6, formula_7 for state variables; formula_8, formula_9, and formula_10 for scalars.\n\nIn the following generalized system of equations for parabolic bursting, the values of formula_1 describe the membrane potential and ion channels, typical of many conductance-based biological neuron models. Slow oscillations are controlled by formula_3, and ultimately described by formula_6. These slow oscillations can be, for example, slow fluctuations in calcium concentration inside a cell. The function formula_2 couples formula_15 to formula_16, thereby allowing the second system, formula_15, to influence the behavior of the first system, formula_16. In more succinct terms, \"formula_5 generates the spikes and formula_6 generates the slow waves\". The equations are:\n\nwhere formula_5 is a vector with formula_9 entries (i.e. formula_25), formula_6 is a vector with formula_10 entries (i.e. formula_28), formula_8 is small and positive, and formula_1, formula_2, formula_3 are smooth (i.e. infinitely differentiable). Additional constraints are required to guarantee parabolic bursting. First, formula_33 must produce a circle in phase space that is invariant, meaning it does not change under certain transformations. This circle must also be attracting in formula_34 with a critical point located at formula_35. The second criterion requires that when formula_36, there exists a stable limit cycle solution. These criteria can be summarized by the following points:\n\n\nThe theta model can be used in place of any parabolic bursting model that satisfies the assumptions above.\n\nThe theta model is a reduction of the generalized system from the previous section and takes the form,\n\nThis model is one of the simplest excitable neuron models. The state variable formula_7 represents the angle in radians, and the input function, formula_44, is typically chosen to be periodic. Whenever formula_7 reaches the value formula_46, the model is said to produce a spike.\n\nThe theta model is capable of a single saddle-node bifurcation and can be shown to be the \"normal form for the saddle-node on a limit cycle bifurcation\" (SNIC). When formula_47, the system is excitable, i.e., given an appriate perturbation the system will produce a spike. Incidentally, when viewed in the plane (formula_34), the unstable critical point is actually a saddle point because formula_49 is attracting in formula_34. When formula_51, formula_52 is also positive, and the system will give rise to a limit cycle. Therefore, the bifurcation point is located at formula_53.\n\nNear the bifurcation point, the theta model resembles the quadratic integrate and fire model:\n\nFor I > 0, the solutions \"blow up\" rather quickly. By resetting the trajectory formula_55 to formula_56 when it reaches formula_57, the total period is then\n\nTherefore, the period diverges as formula_59 and the frequency converges to zero.\n\nWhen formula_44 is some slow wave which can be both negative and positive, the system is capable of producing parabolic bursts. Consider the simple example formula_61, where formula_62 is relatively small. Then for formula_63, formula_44 is strictly positive and formula_7 makes multiple passes through the angle formula_66, resulting in multiple bursts. Note that whenever formula_67 is near zero or formula_66, the theta neuron will spike at relatively a low frequency, and whenever formula_67 is near formula_70 the neuron will spike with very high frequency. When formula_71, the frequency of spikes is zero since the period is infinite since formula_7 can no longer pass through formula_46. Finally, for formula_74, the neuron is excitable and will no longer burst. This qualitative description highlights the characteristics that make the theta model a parabolic bursting model. Not only does the model have periods of quiescence between bursts which are modulated by a slow wave, but the frequency of spikes at the beginning and end of each burst is high relative to the frequency at the middle of the burst.\n\nThe derivation comes in the form of two lemmas in Ermentrout and Kopell (1986). Lemma 1, in summary, states that when viewing the general equations above in a subset formula_75, the equations take the form:\n\nBy lemma 2 in Ermentrout and Kopell 1986, \"There exists a change of coordinates... and a constant, c, such that in new coordinates, the two equations above converge pointwise as formula_78 to the equations\n\nfor all formula_81. Convergence is uniform except near formula_46.\" (Ermentrout and Kopell, 1986). By letting formula_83, resemblance to the theta model is obvious.\n\nIn general, given a scalar phase model of the form\n\nwhere formula_85 represents the perturbation current, a closed form solution of the phase response curve (PRC) does not exist.\n\nHowever, the theta model is a special case of such an oscillator and happens to have a closed-form solution for the PRC. The theta model is recovered by defining formula_1 and formula_2 as\n\nIn the appendix of Ermentrout 1996, the PRC is shown to be formula_90.\n\nThe authors of Soto-Treviño et al. (1996) discuss in great detail the similarities between Plant's (1976) model and the theta model. At first glance, the mechanisms of bursting in both systems are very different: In Plant's model, there are two slow oscillations – one for conductance of a specific current and one for the concentration of calcium. The calcium oscillations are active only when the membrane potential is capable of oscillating. This contrasts heavily against the theta model in which one slow wave modulates the burst of the neuron and the slow wave has no dependence upon the bursts. Despite these differences, the theta model is shown to be similar to Plant's (1976) model by a series of coordinate transformations. In the process, Soto-Trevino, et al. discovered that the theta model was more general than originally believed.\n\nThe quadratic integrate-and-fire (QIF) model was created by Latham et al. in 2000 to explore the many questions related to networks of neurons with low firing rates. It was unclear to Latham et al. why networks of neurons with \"standard\" parameters were unable to generate sustained low frequency firing rates, while networks with low firing rates were often seen in biological systems.\n\nAccording to Gerstner and Kistler (2002), the quadratic integrate-and-fire (QIF) model is given by the following differential equation:\n\nwhere formula_92 is a strictly positive scalar, formula_93 is the membrane potential, formula_94 is the resting potential formula_95 is the minimum potential necessary for the membrane to produce an action potential, formula_96 is the membrane resistance, formula_97 the membrane time constant and formula_98. When there is no input current (i.e. formula_99), the membrane potential quickly returns to rest following a perturbation. When the input current, formula_4, is large enough, the membrane potential (formula_93) surpasses its firing threshold and rises rapidly (indeed, it reaches arbitrarily large values in finite time); this represents the peak of the action potential. To simulate the recovery after the action potential, the membrane voltage is then reset to a lower value formula_102. To avoid dealing with arbitrarily large values in simulation, researchers will often set an upper limit on the membrane potential, above which the membrane potential will be reset; for example Latham et al. (2000) reset the voltage from +20 mV to −80 mV. This voltage reset constitutes an action potential.\n\nThe theta model is very similar to the QIF model since the theta model differs from the QIF model by means of a simple coordinate transform. By scaling the voltage appropriately and letting formula_103 be the change in current from the minimum current required to elicit a spike, the QIF model can be rewritten in the form\n\nSimilarly, the theta model can be rewritten as\n\nThe following proof will show that the QIF model becomes the theta model given an appropriate choice for the coordinate transform.\n\nDefine formula_106. Recall that formula_107, so taking the derivative yields\n\nAn additional substitution and rearranging in terms of formula_7 yields\n\nUsing the trigonometric identities formula_111, formula_112 and formula_52 as defined above, we have that\n\nTherefore, there exists a change of coordinates, namely formula_106, which transforms the QIF model into the theta model. The reverse transformation also exists, and is attained by taking the inverse of the first transformation.\n\nThough the theta model was originally used to model slow cytoplasmic oscillations that modulate fast membrane oscillations in a single cell, Ermentrout and Kopell found that the theta model could be applied just as easily to systems of two electrically coupled cells such that the slow oscillations of one cell modulates the bursts of the other. Such cells serve as the central pattern generator (CPG) of the pyloric system in the lobster stomatograstic ganglion. In such a system, a slow oscillator, called the anterior burster (AB) cell, modulates the bursting cell called the pyloric dilator (PD), resulting in parabolic bursts.\n\nA group led by Boergers, used the theta model to explain why exposure to multiple simultaneous stimuli can reduce the response of the visual cortex below the normal response from a single (preferred) stimulus. Their computational results showed that this may happen due to strong stimulation of a large group of inhibitory neurons. This effect not only inhibits neighboring populations, but has the extra consequence of leaving the inhibitory neurons in disarray, thus increasing the effectiveness of inhibition.\n\nOsan et al. (2002) found that in a network of theta neurons, there exist two different types of waves that propagate smoothly over the network, given a sufficiently large coupling strength. Such traveling waves are of interest because they are frequently observed in pharmacologically treated brain slices, but are hard to measure in intact animals brains. The authors used a network of theta models in favor of a network of leaky integrate-and-fire (LIF) models due to two primary advantages: first, the theta model is continuous, and second, the theta model retains information about \"the delay between the crossing of the spiking threshold and the actual firing of an action potential\". The LIF fails to satisfy both conditions.\n\nThe theta model can also be applied to research beyond the realm of biology. McKennoch et al. (2008) derived a steepest gradient descent learning rule based on theta neuron dynamics. Their model is based on the assumption that \"intrinsic neuron dynamics are sufficient to achieve consistent time coding, with no need to involve the precise shape of postsynaptic currents...\" contrary to similar models like SpikeProp and Tempotron, which depend heavily on the shape of the postsynaptic potential (PSP). Not only could the multilayer theta network perform just about as well as Tempotron learning, but the rule trained the multilayer theta network to perform certain tasks neither SpikeProp nor Tempotron were capable of.\n\nAccording to Kopell and Ermentrout (2004), a limitation of the theta lies in its relative difficulty in electrically coupling two theta neurons. It is possible to create large networks of theta neurons – and much research has been done with such networks – but it may be advantageous to use Quadratic Integrate-and-Fire (QIF) neurons, which allow for electrical coupling in a \"straightforward way\".\n\n\n\n"}
{"id": "146996", "url": "https://en.wikipedia.org/wiki?curid=146996", "title": "Tournament", "text": "Tournament\n\nA tournament is a competition involving a relatively large number of competitors, all participating in a sport or game. More specifically, the term may be used in either of two overlapping senses:\n\nThese two senses are distinct. All golf tournaments meet the first definition, but while match play tournaments meet the second, stroke play tournaments do not, since there are no distinct matches within the tournament. In contrast, association football leagues like the Premier League are tournaments in the second sense, but not the first, having matches spread across many stadia over a period of up to a season. Many tournaments meet both definitions; for example, the Wimbledon tennis championship. Tournaments \"are temporally demarcated events, participation in which confers levels of status and prestige amongst all participating members\".\n\nA tournament-match (or tie or fixture or heat) may involve multiple game-matches (or rubbers or legs) between the competitors. For example, in the Davis Cup tennis tournament, a tie between two nations involves five rubbers between the nations' players. The team that wins the most rubbers wins the tie. In the later rounds of UEFA Champions League , each fixture is played over two legs. The scores of each leg are added, and the team with the higher aggregate score wins the fixture, with away goals used as a tiebreaker and a penalty shoot out if away goals cannot determine a winner of the game.\n\nA knockout tournament or elimination tournament is divided into successive rounds; each competitor plays in at least one fixture per round. The top-ranked competitors in each fixture progress to the next round. As rounds progress, the number of competitors and fixtures decreases. The final round, usually known as the final or cup final, consists of just one fixture; the winner of which is the overall champion.\n\nIn a single-elimination tournament, only the top-ranked competitors in a fixture progress; in 2-competitor games, only the winner progresses. All other competitors are eliminated. This ensures a winner is decided with the minimum number of fixtures. However, most competitors will be eliminated after relatively few matches; a single bad or unlucky performance can nullify many preceding excellent ones.\n\nA double-elimination tournament may be used in 2-competitor games to allow each competitor a single loss without being eliminated from the tournament. All losers from the main bracket enter a losers' bracket, the winner of which plays off against the main bracket's winner.\n\nA triple elimination tournament allows a competitor to lose two games, and creates a third bracket or fourth bracket, and are usually followed by a playoff. It is usually used in curling tournaments. \n\nSome elimination tournaments are in a best-of-\"x\" series, allowing a competitor to lose at most four times (to the same competitor) before being eliminated.\n\nSome formats use a repechage, allowing losers to play extra rounds before re-entering the main competition in a later round. Rowing regattas often have repechage rounds for the \"fastest losers\" from the heats. The winners of these progress, but are at a disadvantage in later rounds owing to the extra effort expended during the repechage.\n\nA family of tournament systems that grew from a system devised for the Victorian Football League, the historic predecessor to the Australian Football League (AFL), allow the teams with the best record before the playoffs to lose a game without being eliminated, whereas lesser qualifiers are not. Several of the most prominent leagues in Australia use such a system, such as the AFL and the National Rugby League in rugby league. The A-League of association football also used such a system through its 2011–12 season, but now uses a pure knockout playoff. Similar systems are used in cricket's Indian Premier League and most curling tournaments, and were also used by the Super League of European rugby league before being scrapped after the 2014 season.\n\nIn athletics meetings, fastest losers may progress in a running event held over several rounds; e.g. the qualifiers for a later round might be the first 4 from each of 6 heats, plus the 8 fastest losers from among the remaining runners.\n\nAn extreme form of the knockout tournament is the stepladder format where the strongest team (or individual, depending on the sport) is assured of a berth at the final round while the next strongest teams are given byes according to their strength/seeds; for example, in a four team tournament, the fourth and third seed figure in the first round, then the winner goes to the semifinals against the second seed, while the survivor faces the first seed at the final. Four American sports organizations either currently use this format, or have in the past:\n\nA group tournament, league, division or conference involves all competitors playing a number of \"fixtures\" (again, a fixture is one name for a tournament-match that determines who, out of two or three or more, will advance; a fixture may consist of one or more game-matches between competitors). Points are awarded for each fixture, with competitors ranked based either on total number of points or average points per fixture. Usually each competitor plays an equal number of fixtures, in which case rankings by total points and by average points are equivalent. The English County Championship in cricket did not require an equal number of matches prior to 1963.\n\nIn a round-robin tournament, each competitor plays all the others an equal number of times, once in a single round-robin tournament and twice in a double round-robin tournament. This is often seen as producing the most reliable rankings. However, for large numbers of competitors it may require an unfeasibly large number of rounds. A Swiss system tournament attempts to determine a winner reliably, based on a smaller number of fixtures. Fixtures are scheduled one round at a time; a competitor will play another who has a similar record in previous rounds of the tournament. This allows the top (and bottom) competitors to be determined with fewer rounds than a round-robin, though the middle rankings are unreliable.\n\nThere may be other considerations besides reliability of rankings. In some professional team sports, weaker teams are given an easier slate of fixtures as a form of handicapping. Sometimes schedules are weighted in favour of local derbies or other traditional rivalries. For example, NFL teams play two games against each of the other three teams in their division, one game against half of the other twelve teams in their conference, and one game against a quarter of the sixteen teams in the other conference.\n\nAmerican sports are also unusual in providing fixtures between competitors who are, for ranking purposes, in different groups. Another, systematic, example of this was the 2006 Women's Rugby World Cup: each of the teams in Group A played each of the teams in Group B, with the groups ranked separately based on the results. (Groups C and D intertwined similarly.) An elaboration of this system is the Mitchell movement in duplicate bridge, discussed below, where North-South pairs play East-West pairs.\nIn 2-competitor games where ties are rare or impossible, competitors are typically ranked by number of wins, with ties counting half; each competitors' listings are usually ordered Wins–Losses(–Ties). Where ties are more common, this may be 2 points for a win and 1 for a tie, which is mathematically equivalent but avoids having too many half-points in the listings. These are usually ordered Wins–Ties–Losses. If there are more than two competitors per fixture, points may be ordinal (for example, 3 for first, 2 for second, 1 for third).\n\nMany tournaments are held in multiple stages, with the top teams in one stage progressing to the next. American professional team sports have a \"regular season\" (group tournament) acting as qualification for the \"post season\" or \"playoffs\" (single-elimination tournament). A group stage (also known as pool play or the pool stage) is a round-robin stage in a multi-stage tournament. The competitors are divided into multiple groups, which play separate round-robins in parallel. Measured by a points-based ranking system, the top competitors in each group qualify for the next stage. In most editions of the FIFA World Cup finals tournament, the first round has been a group stage with groups of four teams, the top two qualifying for the \"knockout stage\" played as a single-elimination tournament. This format is common in many international team events, such as World Cups or Olympic tournaments. Some tournaments have two group stages, for example the 1982 FIFA World Cup or the 1999–2000 UEFA Champions League. As well as a fixed number of qualifiers from each group, some may be determined by comparing between different groups: at the 1986 FIFA World Cup the best four of six third-place sides qualified; at the 1999 Rugby World Cup the best one of five third-place sides did so.\n\nSometimes, results from an earlier phase are carried over into a later phase. In the Cricket World Cup, the second stage, known as the Super Eight since 2007 and before that the Super Six, features two teams from each of four preliminary groups (previously three teams from two preliminary groups), who do not replay the teams they have already played, but instead reuse the original results in the new league table. Formerly in the Swiss Football League, teams played a double round-robin, at which point they were split into a top \"championship\" group and a bottom \"relegation\" group; each played a separate double round-robin, with results of all 32 matches counting for ranking each group. A similar system is also used by the Scottish Premiership and its historic predecessor, the Scottish Premier League, since 2000. After 33 games, when every club has played every other club three times, the division is split into two halves. Clubs play a further 5 matches, against the teams in their half of the division. This can (and often does) result in the team placed 7th having a higher points total than the team placed 6th (because their final 5 games are considerably easier), nevertheless, a team in the bottom half never receives a higher final ranking than a team which qualified for the top half.\n\nA multi-stage pool system was implemented by Curling Canada for the Canadian championship curling tournaments (the Scotties Tournament of Hearts for women and the Tim Hortons Brier for men) starting in 2018. The change was intended to allow the expansion of the main stage of the tournament from twelve to sixteen teams while keeping the round robin at eleven games. The teams are seeded using a ranking system in which points are calculated based on the teams' results in all competitive bonspiels using a complicated formula. Seeds 1, 4, 5, 8, 9, 12, 13 and 16 and placed in Pool A while seeds 2, 3, 6, 7, 10, 11, 14 and 15 are placed in Pool B. After each team has played seven games, the top four teams from each pool advance to the \"Championship Pool.\" Carrying over their entire round robin records with them, Championship Pool teams play one game against each of the four teams in the opposite pool, with the top four teams qualifying for the page playoffs. In contrast, teams that fail to qualify for the Championship Pool play only one additional \"Placement Round\" game against the team that finished in the same position in the opposite pool for the purposes of determining final tournament ranking. For these teams, there is little else to play for since there is no form of relegation (and, with the expansion of the field to sixteen teams, no \"Pre-Qualifying Tournament\") and seeding is based solely on the performances of the participating teams and not the past results of the provinces and territories they represent.\n\nThe top Slovenian basketball league has a unique system. In its first phase, 12 of the league's 13 clubs compete in a full home-and-away season, with the country's representative in the Euroleague (an elite pan-European club competition) exempt. The league then splits. The top seven teams are joined by the Euroleague representative for a second home-and-away season, with no results carrying over from the first phase. These eight teams compete for four spots in a final playoff. The bottom five teams play their own home-and-away league, but their previous results \"do\" carry over. These teams are competing to avoid relegation, with the bottom team automatically relegated and the second-from-bottom team forced to play a mini-league with the second- and third-place teams from the second level for a place in the top league.\n\nWhere the number of competitors is larger than a tournament format permits, there may be multiple tournaments held in parallel, with competitors assigned to a particular tournament based on their ranking. In chess, Scrabble, and many other individual games, many tournaments over one or more years contribute to a player's ranking. However, many team sports involve teams in only one major tournament per year. In European sport, including football, this constitutes the sole ranking for the following season; the top teams from each division of the league are promoted to a higher division, while the bottom teams from a higher division are relegated to a lower one.\n\nThis promotion and relegation occurs mainly in league tournaments, but also features in Davis Cup and Fed Cup tennis:\n\nThe hierarchy of divisions may be linear, or tree-like, as with the English football league pyramid.\n\nIn contract bridge a \"tournament\" is a tournament in the first sense above, composed of multiple \"events\", which are tournaments in the second sense. Some events may be single-elimination, double-elimination, or Swiss style. However, \"Pair events\" are the most widespread. In these events, a number of deals (or \"boards\") are each played several times by different players. For each such board the score achieved by each North-South (NS) pair is then measured against all the other NS pairs playing the same board. Thus pairs are rewarded for playing the same cards better than others have played them. There is a predetermined schedule of fixtures depending on the number of pairs and boards to be played, to ensure a good mix of opponents, and that no pair plays the same board or the same opponents twice (see duplicate bridge movements).\n\nIn poker tournaments, as players are eliminated, the number of tables is gradually reduced, with the remaining players redistributed among the remaining tables. Play continues until one player has won all of the chips in play. Finishing order is determined by the order in which players are eliminated: last player remaining gets 1st place, last player eliminated gets 2nd, previous player eliminated gets 3rd, etc.\n\nIn a \"shootout\" tournament, players do not change tables until every table has been reduced to one player.\n\nWhile tournament structures attempt to provide an objective format for determining the best competitor in a game or sport, other methods exist.\n\n\n\n\nTournaments of value have come to legitimise what are often seen as marginalised practices that sit outside of popular culture. For example, the Grammy Award ceremony helped to shape country music as a viable commercial field, and Booker Prize ceremony helped to create new fields of literary fiction. Tournaments of value go beyond game show and simple contests as the journey itself emerges as being more significant, bestowing status and prestige on the winner and, in the process, shaping industry practices and acting as institutional mechanisms for shaping social fields. \n\n"}
{"id": "30802", "url": "https://en.wikipedia.org/wiki?curid=30802", "title": "Tragedy of the commons", "text": "Tragedy of the commons\n\nThe tragedy of the commons is a term used in social science to describe a situation in a shared-resource system where individual users acting independently according to their own self-interest behave contrary to the common good of all users by depleting or spoiling that resource through their collective action. The concept and phrase originated in an essay written in 1833 by the British economist William Forster Lloyd, who used a hypothetical example of the effects of unregulated grazing on common land (also known as a \"common\") in Great Britain and Ireland. The concept became widely known over a century later due to an article written by the American ecologist and philosopher Garrett Hardin in 1968. In this modern economic context, commons is taken to mean any shared and unregulated resource such as atmosphere, oceans, rivers, fish stocks, or even an office refrigerator.\n\nIt has been argued that the very term 'tragedy of the Commons' is a misnomer, since 'the commons' referred to land resources with rights jointly owned by members of a community, and no individual outside the community had any access to the resource. However, the term is now used in social science and economics when describing a problem where \"all\" individuals have equal and open access to a resource. Hence, 'tragedy of open access regimes' or simply 'the open access problem' are more apt terms.\n\nThe 'tragedy of the commons' is often cited in connection with sustainable development, meshing economic growth and environmental protection, as well as in the debate over global warming. It has also been used in analyzing behavior in the fields of economics, evolutionary psychology, anthropology, game theory, politics, taxation and sociology.\n\nAlthough common resource systems have been known to collapse due to overuse (such as in over-fishing), many examples have existed and still do exist where members of a community with access to a common resource co-operate or regulate to exploit those resources prudently without collapse. Elinor Ostrom was awarded the Nobel Prize in economics for demonstrating exactly this concept in her book Governing the Commons, which included examples of how local communities were able to do this without top-down regulations.\n\nIn 1833, the English economist William Forster Lloyd published a pamphlet which included a hypothetical example of over-use of a common resource. This was the situation of cattle herders sharing a common parcel of land on which they are each entitled to let their cows graze, as was the custom in English villages. He postulated that if a herder put more than his allotted number of cattle on the common, overgrazing could result. For each additional animal, a herder could receive additional benefits, but the whole group shared damage to the commons. If all herders made this individually rational economic decision, the common could be depleted or even destroyed, to the detriment of all.\n\nIn 1968, ecologist Garrett Hardin explored this social dilemma in his article \"The Tragedy of the Commons\", published in the journal \"Science\". The essay derived its title from the pamphlet by Lloyd, which he cites, on the over-grazing of common land.\n\nHardin discussed problems that cannot be solved by technical means, as distinct from those with solutions that require \"a change only in the techniques of the natural sciences, demanding little or nothing in the way of change in human values or ideas of morality\". Hardin focused on human population growth, the use of the Earth's natural resources, and the welfare state. Hardin argued that if individuals relied on themselves alone, and not on the relationship of society and man, then the number of children had by each family would not be of public concern. Parents breeding excessively would leave fewer descendants because they would be unable to provide for each child adequately. Such negative feedback is found in the animal kingdom. Hardin said that if the children of improvident parents starved to death, if overbreeding was its own punishment, then there would be no public interest in controlling the breeding of families. Hardin blamed the welfare state for allowing the tragedy of the commons; where the state provides for children and supports overbreeding as a fundamental human right, Malthusian catastrophe is inevitable. Consequently, in his article, Hardin lamented the following proposal from the United Nations:\nIn addition, Hardin also pointed out the problem of individuals acting in rational self-interest by claiming that if all members in a group used common resources for their own gain and with no regard for others, all resources would still eventually be depleted. Overall, Hardin argued against relying on conscience as a means of policing commons, suggesting that this favors selfish individuals – often known as free riders – over those who are more altruistic.\n\nIn the context of avoiding over-exploitation of common resources, Hardin concluded by restating Hegel's maxim (which was quoted by Engels), \"freedom is the recognition of necessity\". He suggested that \"freedom\" completes the tragedy of the commons. By recognizing resources as commons in the first place, and by recognizing that, as such, they require management, Hardin believed that humans \"can preserve and nurture other and more precious freedoms\".\n\nHardin's article was the start of the modern use of \"Commons\" as a term connoting a shared resource. As Frank van Laerhoven & Elinor Ostrom have stated: \"Prior to the publication of Hardin’s article on the tragedy of the commons (1968), titles containing the words 'the commons', 'common pool resources,' or 'common property' were very rare in the academic literature.\" They go on to say: \"In 2002, Barrett and Mabry conducted a major survey of biologists to determine which publications in the twentieth century had become classic books or benchmark publications in biology. They report that Hardin’s 1968 article was the one having the greatest career impact on biologists and is the most frequently cited\".\n\nLike Lloyd and Thomas Malthus before him, Hardin was primarily interested in the problem of human population growth. But in his essay, he also focused on the use of larger (though finite) resources such as the Earth's atmosphere and oceans, as well as pointing out the \"negative commons\" of pollution (i.e., instead of dealing with the deliberate privatization of a positive resource, a \"negative commons\" deals with the deliberate commonization of a negative cost, pollution).\n\nAs a metaphor, the tragedy of the commons should not be taken too literally. The \"tragedy\" is not in the word's conventional or theatric sense, nor a condemnation of the processes that lead to it. Similarly, Hardin's use of \"commons\" has frequently been misunderstood, leading him to later remark that he should have titled his work \"The Tragedy of the Unregulated Commons\".\n\nThe metaphor illustrates the argument that free access and unrestricted demand for a finite resource ultimately reduces the resource through over-exploitation, temporarily or permanently. This occurs because the benefits of exploitation accrue to individuals or groups, each of whom is motivated to maximize use of the resource to the point in which they become reliant on it, while the costs of the exploitation are borne by all those to whom the resource is available (which may be a wider class of individuals than those who are exploiting it). This, in turn, causes demand for the resource to increase, which causes the problem to snowball until the resource collapses (even if it retains a capacity to recover). The rate at which depletion of the resource is realized depends primarily on three factors: the number of users wanting to consume the common in question, the consumptiveness of their uses, and the relative robustness of the common.\n\nThe same concept is sometimes called the \"tragedy of the fishers\", because fishing too many fish before or during breeding could cause stocks to plummet.\n\nThe \"tragedy of the commons\" can be considered in relation to environmental issues such as sustainability. The commons dilemma stands as a model for a great variety of resource problems in society today, such as water, forests, fish, and non-renewable energy sources such as oil and coal.\n\nSituations exemplifying the \"tragedy of the commons\" include the overfishing and destruction of the Grand Banks, the destruction of salmon runs on rivers that have been dammed – most prominently in modern times on the Columbia River in the Northwest United States, and historically in North Atlantic rivers – the devastation of the sturgeon fishery – in modern Russia, but historically in the United States as well – and, in terms of water supply, the limited water available in arid regions (e.g., the area of the Aral Sea) and the Los Angeles water system supply, especially at Mono Lake and Owens Lake.\n\nIn economics, an externality is a cost or benefit that affects a party who did not choose to incur that cost or benefit. Negative externalities are a well-known feature of the \"tragedy of the commons\". For example, driving cars has many negative externalities; these include pollution, carbon emissions, and traffic accidents. Every time 'Person A' gets in a car, it becomes more likely that 'Person Z' and millions of others will suffer in each of those areas. Economists often urge the government to adopt policies that \"internalize\" an externality.\n\nMore general examples (some alluded to by Hardin) of potential and actual tragedies include:\n\nA parallel was drawn recently between the tragedy of the commons and the competing behaviour of parasites that through acting selfishly eventually diminish or destroy their common host. The idea has also been applied to areas such as the evolution of virulence or sexual conflict, where males may fatally harm females when competing for matings. It is also raised as a question in studies of social insects, where scientists wish to understand why insect workers do not undermine the \"common good\" by laying eggs of their own and causing a breakdown of the society.\n\nThe idea of evolutionary suicide, where adaptation at the level of the individual causes the whole species or population to be driven extinct, can be seen as an extreme form of an evolutionary tragedy of the commons. From an evolutionary point of view, the creation of the tragedy of the commons in pathogenic microbes may provide us with advanced therapeutic methods.\n\nThe \"commons dilemma\" is a specific class of social dilemma in which people's short-term selfish interests are at odds with long-term group interests and the common good. In academia, a range of related terminology has also been used as shorthand for the theory or aspects of it, including \"resource dilemma\", \"take-some dilemma\", and \"common pool resource\".\n\nCommons dilemma researchers have studied conditions under which groups and communities are likely to under- or over-harvest common resources in both the laboratory and field. Research programs have concentrated on a number of motivational, strategic, and structural factors that might be conducive to management of commons.\n\nIn game theory, which constructs mathematical models for individuals' behavior in strategic situations, the corresponding \"game\", developed by Hardin, is known as the Commonize Costs – Privatize Profits Game (CC–PP game).\n\nKopelman, Weber, & Messick (2002), in a review of the experimental research on cooperation in commons dilemmas, identify nine classes of independent variables that influence cooperation in commons dilemmas: social motives, gender, payoff structure, uncertainty, power and status, group size, communication, causes, and frames. They organize these classes and distinguish between psychological individual differences (stable personality traits) and situational factors (the environment). Situational factors include both the task (social and decision structure) and the perception of the task.\n\nEmpirical findings support the theoretical argument that the cultural group is a critical factor that needs to be studied in the context of situational variables. Rather than behaving in line with economic incentives, people are likely to approach the decision to cooperate with an appropriateness framework. An expanded, four factor model of the Logic of Appropriateness, suggests that the cooperation is better explained by the question: \"What does a person like me (identity) do (rules) in a situation like this (recognition) given this culture (group)?\"\n\nStrategic factors also matter in commons dilemmas. One often-studied strategic factor is the order in which people take harvests from the resource. In simultaneous play, all people harvest at the same time, whereas in sequential play people harvest from the pool according to a predetermined sequence – first, second, third, etc. There is a clear order effect in the latter games: the harvests of those who come first – the leaders – are higher than the harvest of those coming later – the followers. The interpretation of this effect is that the first players feel entitled to take more. With sequential play, individuals adopt a first come-first served rule, whereas with simultaneous play people may adopt an equality rule. Another strategic factor is the ability to build up reputations. Research found that people take less from the common pool in public situations than in anonymous private situations. Moreover, those who harvest less gain greater prestige and influence within their group.\n\nMuch research has focused on when and why people would like to structurally rearrange the commons to prevent a tragedy. Hardin stated in his analysis of the tragedy of the commons that \"Freedom in a commons brings ruin to all.\" One of the proposed solutions is to appoint a leader to regulate access to the common. Groups are more likely to endorse a leader when a common resource is being depleted and when managing a common resource is perceived as a difficult task. Groups prefer leaders who are elected, democratic, and prototypical of the group, and these leader types are more successful in enforcing cooperation. A general aversion to autocratic leadership exists, although it may be an effective solution, possibly because of the fear of power abuse and corruption.\n\nThe provision of rewards and punishments may also be effective in preserving common resources. Selective punishments for overuse can be effective in promoting domestic water and energy conservation – for example, through installing water and electricity meters in houses. Selective rewards work, provided that they are open to everyone. An experimental carpool lane in the Netherlands failed because car commuters did not feel they were able to organize a carpool. The rewards do not have to be tangible. In Canada, utilities considered putting \"smiley faces\" on electricity bills of customers below the average consumption of that customer`s neighborhood.\n\nArticulating solutions to the tragedy of the commons is one of the main problems of political philosophy. In many situations, locals implement (often complex) social schemes that work well. The best governmental solution may be to do nothing. When these fail, there are many possible governmental solutions such as privatization, internalizing the externalities, and regulation.\n\nSometimes the best governmental solution may be to do nothing. Robert Axelrod contends that even self-interested individuals will often find ways to cooperate, because collective restraint serves both the collective and individual interests. Anthropologist G. N. Appell criticized those who cited Hardin to \"impos[e] their own economic and environmental rationality on other social systems of which they have incomplete understanding and knowledge.\"\n\nPolitical scientist Elinor Ostrom, who was awarded 2009's Nobel Memorial Prize in Economic Sciences for her work on the issue, and others revisited Hardin's work in 1999. They found the tragedy of the commons not as prevalent or as difficult to solve as Hardin maintained, since locals have often come up with solutions to the commons problem themselves. For example, it was found that a commons in the Swiss Alps has been run by a collective of farmers there to their mutual and individual benefit since 1517, in spite of the farmers also having access to their own farmland. In general, it is in the users of a commons interests to keep the common running and complex social schemes are often invented by the users for maintaining them at optimum efficiency.\n\nSimilarly, geographer Douglas L. Johnson remarks that many nomadic pastoralist societies of Africa and the Middle East in fact \"balanced local stocking ratios against seasonal rangeland conditions in ways that were ecologically sound\", reflecting a desire for lower risk rather than higher profit; in spite of this, it was often the case that \"the nomad was blamed for problems that were not of his own making and were a product of alien forces.\" Independently finding precedent in the opinions of previous scholars such as Ibn Khaldun as well as common currency in antagonistic cultural attitudes towards non-sedentary peoples, governments and international organizations have made use of Hardin's work to help justify restrictions on land access and the eventual sedentarization of pastoral nomads despite its weak empirical basis. Examining relations between historically nomadic Bedouin Arabs and the Syrian state in the 20th century, Dawn Chatty notes that \"Hardin's argument […] was curiously accepted as the fundamental explanation for the degradation of the steppe land\" in development schemes for the arid interior of the country, downplaying the larger role of agricultural overexploitation in desertification as it melded with prevailing nationalist ideology which viewed nomads as socially backward and economically harmful.\n\nElinor Ostrom, and her colleagues looked at how real-world communities manage communal resources, such as fisheries, land irrigation systems, and farmlands, and they identified a number of factors conducive to successful resource management. One factor is the resource itself; resources with definable boundaries (e.g., land) can be preserved much more easily. A second factor is resource dependence; there must be a perceptible threat of resource depletion, and it must be difficult to find substitutes. The third is the presence of a community; small and stable populations with a thick social network and social norms promoting conservation do better. A final condition is that there be appropriate community-based rules and procedures in place with built-in incentives for responsible use and punishments for overuse. When the commons is taken over by non-locals, those solutions can no longer be used.\n\nGovernmental solutions may be necessary when the above conditions are not met (such as a community being too big or too unstable to provide a thick social network). Examples of government regulation include privatization, regulation, and internalizing the externalities.\n\nOne solution for some resources is to convert common good into private property, giving the new owner an incentive to enforce its sustainability. Libertarians and classical liberals cite the tragedy of the commons as an example of what happens when Lockean property rights to homestead resources are prohibited by a government. They argue that the solution to the tragedy of the commons is to allow individuals to take over the property rights of a resource, that is, to privatize it.\n\nIn England, this solution was attempted in the Inclosure Acts.\n\nIn a typical example, governmental regulations can limit the amount of a common good that is available for use by any individual. Permit systems for extractive economic activities including mining, fishing, hunting, livestock raising and timber extraction are examples of this approach. Similarly, limits to pollution are examples of governmental intervention on behalf of the commons. This idea is used by the United Nations Moon Treaty, Outer Space Treaty and Law of the Sea Treaty as well as the UNESCO World Heritage Convention which involves the international law principle that designates some areas or resources the Common Heritage of Mankind.\n\nIn Hardin's essay, he proposed that the solution to the problem of overpopulation must be based on \"mutual coercion, mutually agreed upon\" and result in \"relinquishing the freedom to breed\". Hardin discussed this topic further in a 1979 book, \"Managing the Commons,\" co-written with John A. Baden. He framed this prescription in terms of needing to restrict the \"reproductive right\", to safeguard all other rights. Several countries have a variety of population control laws in place.\n\nGerman historian Joachim Radkau thought Hardin advocates strict management of common goods via increased government involvement or international regulation bodies. An asserted impending \"tragedy of the commons\" is frequently warned of as a consequence of the adoption of policies which restrict private property and espouse expansion of public property.\n\nPrivatization works when the person who owns the property (or rights of access to that property) pays the full price of its exploitation. As discussed above negative externalities (negative results, such as air or water pollution, that do not proportionately affect the user of the resource) is often a feature driving the tragedy of the commons. \"Internalizing the externalities\", in other words ensuring that the users of resource pay for all of the consequences of its use, can provide an alternate solution between privatization and regulation. One example is gasoline taxes which are intended to include both the cost of road maintenance and of air pollution. This solution can provide the flexibility of privatization while minimizing the amount of government oversight and overhead that is needed.\n\nThe environmentalist Derrick Jensen claims the tragedy of the commons is used as propaganda for private ownership. He says it has been used by the political right wing to hasten the final enclosure of the \"common resources\" of third world and indigenous people worldwide, as a part of the Washington Consensus. He argues that in true situations, those who abuse the commons would have been warned to desist and if they failed would have punitive sanctions against them. He says that rather than being called \"The Tragedy of the Commons\", it should be called \"the Tragedy of the Failure of the Commons\".\n\nHardin's work was also criticised as historically inaccurate in failing to account for the demographic transition, and for failing to distinguish between common property and open access resources. In a similar vein, Carl Dahlman argues that commons were effectively managed to prevent overgrazing. Likewise, Susan Jane Buck Cox argues that the common land example used to argue this economic concept is on very weak historical ground, and misrepresents what she terms was actually the \"triumph of the commons\": the successful common usage of land for many centuries. She argues that social changes and agricultural innovation, and not the behaviour of the commoners, led to the demise of the commons. \n\nSome authors, like Yochai Benkler, say that with the rise of the Internet and digitalisation, an economics system based on commons becomes possible again. He wrote in his book \"The Wealth of Networks\" in 2006 that cheap computing power plus networks enable people to produce valuable products through non-commercial processes of interaction: \"as human beings and as social beings, rather than as market actors through the price system\". He uses the term 'networked information economy' to describe a \"system of production, distribution, and consumption of information goods characterized by decentralized individual action carried out through widely distributed, nonmarket means that do not depend on market strategies.\" He also coined the term 'commons-based peer production' to describe collaborative efforts based on sharing information. Examples of commons-based peer production are free and open source software and open-source hardware.\n\nIn certain cases, exploiting a resource more may be a good thing. Carol M. Rose, in a 1986 article, discussed the concept of the \"comedy of the commons\", where the public property in question exhibits \"increasing returns to scale\" in usage (hence the phrase, \"the more the merrier\"), in that the more people use the resource, the higher the benefit to each one. Rose cites as examples commerce and group recreational activities. According to Rose, public resources with the \"comedic\" characteristic may suffer from under-investment rather than over usage.\n\n\n"}
{"id": "1135563", "url": "https://en.wikipedia.org/wiki?curid=1135563", "title": "Transmutation of species", "text": "Transmutation of species\n\nTransmutation of species and transformism are 19th-century evolutionary ideas for the altering of one species into another that preceded Charles Darwin's theory of natural selection. The French \"Transformisme\" was a term used by Jean Baptiste Lamarck in 1809 for his theory, and other 19th century proponents of pre-Darwinian evolutionary ideas included Étienne Geoffroy Saint-Hilaire, Robert Grant, and Robert Chambers, the anonymous author of the book \"Vestiges of the Natural History of Creation\". Opposition in the scientific community to these early theories of evolution, led by influential scientists like the anatomists Georges Cuvier and Richard Owen and the geologist Charles Lyell, was intense. The debate over them was an important stage in the history of evolutionary thought and would influence the subsequent reaction to Darwin's theory.\n\nTransmutation was one of the names commonly used for evolutionary ideas in the 19th century before Charles Darwin published \"On The Origin of Species\" (1859). Transmutation had previously been used as a term in alchemy to describe the transformation of base metals into gold. Other names for evolutionary ideas used in this period include \"the development hypothesis\" (one of the terms used by Darwin) and \"the theory of regular gradation\", used by William Chilton in the periodical press such as \"The Oracle of Reason\". \"Transformation\" is another word used quite as often as transmutation in this context. These early 19th century evolutionary ideas played an important role in the history of evolutionary thought.\n\nThe proto-evolutionary thinkers of the 18th and early 19th century had to invent terms to label their ideas, but it was first Joseph Gottlieb Kölreuter who used the term \"transmutation\" to refer to species who have had biological changes through hybridization.\n\nThe terminology did not settle down until some time after the publication of the \"Origin of Species\". The word \"evolution\" was quite a late-comer: it can be seen in Herbert Spencer's \"Social Statics\" of 1851, and there is at least one earlier example, but it was not in general use until about 1865-70.\n\nJean-Baptiste Lamarck proposed a hypothesis on the transmutation of species in \"Philosophie Zoologique\" (1809). Lamarck did not believe that all living things shared a common ancestor. Rather he believed that simple forms of life were created continuously by spontaneous generation. He also believed that an innate life force, which he sometimes described as a nervous fluid, drove species to become more complex over time, advancing up a linear ladder of complexity that was related to the great chain of being. Lamarck also recognized that species were adapted to their environment. He explained this observation by saying that the same nervous fluid driving increasing complexity, also caused the organs of an animal (or a plant) to change based on the use or disuse of that organ, just as muscles are affected by exercise. He argued that these changes would be inherited by the next generation and produce slow adaptation to the environment. It was this secondary mechanism of adaptation through the inheritance of acquired characteristics that became closely associated with his name and would influence discussions of evolution into the 20th century. \n\nA radical British school of comparative anatomy (the Edinburgh school) which included the surgeon Robert Knox and the anatomist Robert Grant was closely in touch with Lamarck's school of French \"Transformationism\", which contained scientists such as Étienne Geoffroy Saint-Hilaire. Grant developed Lamarck's and Erasmus Darwin's ideas of transmutation and evolutionism, investigating homology to prove common descent. As a young student Charles Darwin joined Grant in investigations of the life cycle of marine animals. He also studied geology under professor Robert Jameson whose journal published an anonymous paper in 1826 praising \"Mr. Lamarck\" for explaining how the higher animals had \"evolved\" from the \"simplest worms\" – this was the first use of the word \"evolved\" in a modern sense. Jameson's course closed with lectures on the \"Origin of the Species of Animals\".\n\nThe computing pioneer Charles Babbage published his unofficial \"Ninth Bridgewater Treatise\" in 1837, putting forward the thesis that God had the omnipotence and foresight to create as a divine legislator, making laws (or programs) which then produced species at the appropriate times, rather than continually interfering with \"ad hoc\" miracles each time a new species was required. In 1844 the Scottish publisher Robert Chambers anonymously published an influential and extremely controversial book of popular science entitled \"Vestiges of the Natural History of Creation\". This book proposed an evolutionary scenario for the origins of the solar system and life on earth. It claimed that the fossil record showed an ascent of animals with current animals being branches off a main line that leads progressively to humanity. It implied that the transmutations led to the unfolding of a preordained orthogenetic plan woven into the laws that governed the universe. In this sense it was less completely materialistic than the ideas of radicals like Robert Grant, but its implication that humans were just the last step in the ascent of animal life incensed many conservative thinkers. Both conservatives like Adam Sedgwick, and radical materialists like Thomas Henry Huxley, who disliked Chambers' implications of preordained progress, were able to find scientific inaccuracies in the book that they could disparage. Darwin himself openly deplored the author's \"poverty of intellect\", and dismissed it as a \"literary curiosity.\" However, the high profile of the public debate over \"Vestiges\", with its depiction of evolution as a progressive process, and its popular success, would greatly influence the perception of Darwin's theory a decade later. It also influenced some younger naturalists, including Alfred Russel Wallace, to take an interest in the idea of transmutation.\n\nIdeas about the transmutation of species were strongly associated with the radical materialism of the Enlightenment and were greeted with hostility by more conservative thinkers. Cuvier attacked the ideas of Lamarck and Geoffroy Saint-Hilaire, agreeing with Aristotle that species were immutable. Cuvier believed that the individual parts of an animal were too closely correlated with one another to allow for one part of the anatomy to change in isolation from the others, and argued that the fossil record showed patterns of catastrophic extinctions followed by re-population, rather than gradual change over time. He also noted that drawings of animals and animal mummies from Egypt, which were thousands of years old, showed no signs of change when compared with modern animals. The strength of Cuvier's arguments and his reputation as a leading scientist helped keep transmutational ideas out of the scientific mainstream for decades. \n\nIn Britain, where the philosophy of natural theology remained influential, William Paley wrote the book \"Natural Theology\" with its famous watchmaker analogy, at least in part as a response to the transmutational ideas of Erasmus Darwin. Geologists influenced by natural theology, such as Buckland and Sedgwick, made a regular practice of attacking the evolutionary ideas of Lamarck and Grant, and Sedgwick wrote a famously harsh review of \"The Vestiges of the Natural History of Creation\". Although the geologist Charles Lyell opposed scriptural geology, he also believed in the immutability of species, and in his \"Principles of Geology\" (1830–1833) he criticized and dismissed Lamarck's theories of development. Instead, he advocated a form of progressive creation, in which each species had its \"centre of creation\" and was designed for this particular habitat, but would go extinct when this habitat changed.\nAnother source of opposition to transmutation was a school of naturalists who were influenced by the German philosophers and naturalists associated with idealism, such as Goethe, Hegel and Lorenz Oken. Idealists such as Louis Agassiz and Richard Owen believed that each species was fixed and unchangeable because it represented an idea in the mind of the creator. They believed that relationships between species could be discerned from developmental patterns in embryology, as well as in the fossil record, but that these relationships represented an underlying pattern of divine thought, with progressive creation leading to increasing complexity and culminating in humanity. Owen developed the idea of \"archetypes\" in the divine mind that would produce a sequence of species related by anatomical homologies, such as vertebrate limbs. Owen was concerned by the political implications of the ideas of transmutationists like Robert Grant, and he led a public campaign by conservatives that successfully marginalized Grant in the scientific community. In his famous 1841 paper, which coined the term dinosaur for the giant reptiles discovered by Buckland and Gideon Mantell, Owen argued that these reptiles contradicted the transmutational ideas of Lamarck because they were more sophisticated than the reptiles of the modern world. Darwin would make good use of the homologies analyzed by Owen in his own theory, but the harsh treatment of Grant, along with the controversy surrounding \"Vestiges\", would be factors in his decision to ensure that his theory was fully supported by facts and arguments before publishing his ideas.\n\n"}
{"id": "1979061", "url": "https://en.wikipedia.org/wiki?curid=1979061", "title": "Two-fluid model", "text": "Two-fluid model\n\nTwo-fluid model is a macroscopic traffic flow model to represent traffic in a town/city or metropolitan area, put forward in the 1970s by Ilya Prigogine and Robert Herman.\n\nThere is also a two-fluid model which helps explain the behavior of superfluid helium. This model states that there will be two components in liquid helium below its lambda point (the temperature where superfluid forms). These components are a normal fluid and a superfluid component. Each liquid has a different density and together their sum makes the total density, which remains constant. The ratio of superfluid density to the total density increases as the temperature approaches absolute zero.\n\n"}
