{"id": "7321377", "url": "https://en.wikipedia.org/wiki?curid=7321377", "title": "Angle of loll", "text": "Angle of loll\n\nAngle of loll is the state of a ship that is unstable when upright (i.e. has a negative metacentric height) and therefore takes on an angle of heel to either port or starboard.\n\nWhen a vessel has negative metacentric height (GM) \"i.e.\", is in unstable equilibrium, any external force applied to the vessel will cause it to start heeling. As it heels, the moment of inertia of the vessel's waterplane (a plane intersecting the hull at the water's surface) increases, which increases the vessel's BM (distance from the center of \"B\"uoyancy to the \"M\"etacenter). Since there is relatively little change in KB (distance from the \"K\"eel to the center of \"B\"uoyancy) of the vessel, the KM (distance from \"K\"eel to the \"M\"etacenter) of the vessel increases.\n\nAt some angle of heel (say 10°), KM will increase sufficiently equal to KG (distance from the keel to the center of gravity), thus making GM of vessel equal to zero. When this occurs, the vessel goes to neutral equilibrium, and the angle of heel at which it happens is called \"angle of loll\".\nIn other words, when an unstable vessel heels over towards a progressively increasing angle of heel, at a certain angle of heel, the center of buoyancy (B) may fall vertically below the center of gravity (G). Angle of list should not be confused with angle of loll. Angle of list is caused by unequal loading on either side of center line of vessel.\n\nAlthough a vessel at angle of loll does display features of stable equilibrium, this is a dangerous situation and rapid remedial action is required to prevent the vessel from capsizing.\n\nIt is often caused by the influence of a large free surface or the loss of stability due to damaged compartments. It is different from list in that the vessel is not induced to heel to one side or the other by the distribution of weight, it is merely incapable of maintaining a zero heel attitude.\n"}
{"id": "928", "url": "https://en.wikipedia.org/wiki?curid=928", "title": "Axiom", "text": "Axiom\n\nAn axiom or postulate is a statement that is taken to be true, to serve as a premise or starting point for further reasoning and arguments. The word comes from the Greek \"axíōma\" () 'that which is thought worthy or fit' or 'that which commends itself as evident.'\n\nThe term has subtle differences in definition when used in the context of different fields of study. As defined in classic philosophy, an axiom is a statement that is so evident or well-established, that it is accepted without controversy or question. As used in modern logic, an axiom is a premise or starting point for reasoning. \nAs used in mathematics, the term \"axiom\" is used in two related but distinguishable senses: \"logical axioms\" and \"non-logical axioms\". Logical axioms are usually statements that are taken to be true within the system of logic they define (e.g., (\"A\" and \"B\") implies \"A\"), often shown in symbolic form, while non-logical axioms (e.g., ) are actually substantive assertions about the elements of the domain of a specific mathematical theory (such as arithmetic). When used in the latter sense, \"axiom\", \"postulate\", and \"assumption\" may be used interchangeably. In general, a non-logical axiom is not a self-evident truth, but rather a formal logical expression used in deduction to build a mathematical theory. To axiomatize a system of knowledge is to show that its claims can be derived from a small, well-understood set of sentences (the axioms). There are typically multiple ways to axiomatize a given mathematical domain.\n\nAny axiom is a statement that serves as a starting point from which other statements are logically derived. Whether it is meaningful (and, if so, what it means) for an axiom to be \"true\" is a subject of debate in the philosophy of mathematics.\n\nThe word \"axiom\" comes from the Greek word (\"axíōma\"), a verbal noun from the verb (\"axioein\"), meaning \"to deem worthy\", but also \"to require\", which in turn comes from (\"áxios\"), meaning \"being in balance\", and hence \"having (the same) value (as)\", \"worthy\", \"proper\". Among the ancient Greek philosophers an axiom was a claim which could be seen to be true without any need for proof.\n\nThe root meaning of the word \"postulate\" is to \"demand\"; for instance, Euclid demands that one agree that some things can be done, e.g. any two points can be joined by a straight line, etc.\n\nAncient geometers maintained some distinction between axioms and postulates. While commenting on Euclid's books, Proclus remarks that, \"Geminus held that this [4th] Postulate should not be classed as a postulate but as an axiom, since it does not, like the first three Postulates, assert the possibility of some construction but expresses an essential property.\" Boethius translated 'postulate' as \"petitio\" and called the axioms \"notiones communes\" but in later manuscripts this usage was not always strictly kept.\n\nThe logico-deductive method whereby conclusions (new knowledge) follow from premises (old knowledge) through the application of sound arguments (syllogisms, rules of inference), was developed by the ancient Greeks, and has become the core principle of modern mathematics. Tautologies excluded, nothing can be deduced if nothing is assumed. Axioms and postulates are the basic assumptions underlying a given body of deductive knowledge. They are accepted without demonstration. All other assertions (theorems, if we are talking about mathematics) must be proven with the aid of these basic assumptions. However, the interpretation of mathematical knowledge has changed from ancient times to the modern, and consequently the terms \"axiom\" and \"postulate\" hold a slightly different meaning for the present day mathematician, than they did for Aristotle and Euclid.\n\nThe ancient Greeks considered geometry as just one of several sciences, and held the theorems of geometry on par with scientific facts. As such, they developed and used the logico-deductive method as a means of avoiding error, and for structuring and communicating knowledge. Aristotle's posterior analytics is a definitive exposition of the classical view.\n\nAn \"axiom\", in classical terminology, referred to a self-evident assumption common to many branches of science. A good example would be the assertion that \"When an equal amount is taken from equals, an equal amount results.\"\n\nAt the foundation of the various sciences lay certain additional hypotheses which were accepted without proof. Such a hypothesis was termed a \"postulate\". While the axioms were common to many sciences, the postulates of each particular science were different. Their validity had to be established by means of real-world experience. Indeed, Aristotle warns that the content of a science cannot be successfully communicated, if the learner is in doubt about the truth of the postulates.\n\nThe classical approach is well-illustrated by Euclid's Elements, where a list of postulates is given (common-sensical geometric facts drawn from our experience), followed by a list of \"common notions\" (very basic, self-evident assertions).\n\nA lesson learned by mathematics in the last 150 years is that it is useful to strip the meaning away from the mathematical assertions (axioms, postulates, propositions, theorems) and definitions. One must concede the need for primitive notions, or undefined terms or concepts, in any study. Such abstraction or formalization makes mathematical knowledge more general, capable of multiple different meanings, and therefore useful in multiple contexts. Alessandro Padoa, Mario Pieri, and Giuseppe Peano were pioneers in this movement.\n\nStructuralist mathematics goes further, and develops theories and axioms (e.g. field theory, group theory, topology, vector spaces) without \"any\" particular application in mind. The distinction between an \"axiom\" and a \"postulate\" disappears. The postulates of Euclid are profitably motivated by saying that they lead to a great wealth of geometric facts. The truth of these complicated facts rests on the acceptance of the basic hypotheses. However, by throwing out Euclid's fifth postulate we get theories that have meaning in wider contexts, hyperbolic geometry for example. We must simply be prepared to use labels like \"line\" and \"parallel\" with greater flexibility. The development of hyperbolic geometry taught mathematicians that postulates should be regarded as purely formal statements, and not as facts based on experience.\n\nWhen mathematicians employ the field axioms, the intentions are even more abstract. The propositions of field theory do not concern any one particular application; the mathematician now works in complete abstraction. There are many examples of fields; field theory gives correct knowledge about them all.\n\nIt is not correct to say that the axioms of field theory are \"propositions that are regarded as true without proof.\" Rather, the field axioms are a set of constraints. If any given system of addition and multiplication satisfies these constraints, then one is in a position to instantly know a great deal of extra information about this system.\n\nModern mathematics formalizes its foundations to such an extent that mathematical theories can be regarded as mathematical objects, and mathematics itself can be regarded as a branch of logic. Frege, Russell, Poincaré, Hilbert, and Gödel are some of the key figures in this development.\n\nIn the modern understanding, a set of axioms is any collection of formally stated assertions from which other formally stated assertions follow by the application of certain well-defined rules. In this view, logic becomes just another formal system. A set of axioms should be consistent; it should be impossible to derive a contradiction from the axiom. A set of axioms should also be non-redundant; an assertion that can be deduced from other axioms need not be regarded as an axiom.\n\nIt was the early hope of modern logicians that various branches of mathematics, perhaps all of mathematics, could be derived from a consistent collection of basic axioms. An early success of the formalist program was Hilbert's formalization of Euclidean geometry, and the related demonstration of the consistency of those axioms.\n\nIn a wider context, there was an attempt to base all of mathematics on Cantor's set theory. Here the emergence of Russell's paradox, and similar antinomies of naïve set theory raised the possibility that any such system could turn out to be inconsistent.\n\nThe formalist project suffered a decisive setback, when in 1931 Gödel showed that it is possible, for any sufficiently large set of axioms (Peano's axioms, for example) to construct a statement whose truth is independent of that set of axioms. As a corollary, Gödel proved that the consistency of a theory like Peano arithmetic is an unprovable assertion within the scope of that theory.\n\nIt is reasonable to believe in the consistency of Peano arithmetic because it is satisfied by the system of natural numbers, an infinite but intuitively accessible formal system. However, at present, there is no known way of demonstrating the consistency of the modern Zermelo–Fraenkel axioms for set theory. Furthermore, using techniques of forcing (Cohen) one can show that the continuum hypothesis (Cantor) is independent of the Zermelo–Fraenkel axioms. Thus, even this very general set of axioms cannot be regarded as the definitive foundation for mathematics.\n\nAxioms play a key role not only in mathematics, but also in other sciences, notably in theoretical physics. In particular, the monumental work of Isaac Newton is essentially based on Euclid's axioms, augmented by a postulate on the non-relation of spacetime and the physics taking place in it at any moment.\n\nIn 1905, Newton's axioms were replaced by those of Albert Einstein's special relativity, and later on by those of general relativity.\n\nAnother paper of Albert Einstein and coworkers (see EPR paradox), almost immediately contradicted by Niels Bohr, concerned the interpretation of quantum mechanics. This was in 1935. According to Bohr, this new theory should be probabilistic, whereas according to Einstein it should be deterministic. Notably, the underlying quantum mechanical theory, i.e. the set of \"theorems\" derived by it, seemed to be identical. Einstein even assumed that it would be sufficient to add to quantum mechanics \"hidden variables\" to enforce determinism. However, thirty years later, in 1964, John Bell found a theorem, involving complicated optical correlations (see Bell inequalities), which yielded measurably different results using Einstein's axioms compared to using Bohr's axioms. And it took roughly another twenty years until an experiment of Alain Aspect got results in favour of Bohr's axioms, not Einstein's. (Bohr's axioms are simply: The theory should be probabilistic in the sense of the Copenhagen interpretation.)\n\nAs a consequence, it is not necessary to explicitly cite Einstein's axioms, the more so since they concern subtle points on the \"reality\" and \"locality\" of experiments.\n\nRegardless, the role of axioms in mathematics and in the above-mentioned sciences is different. In mathematics one neither \"proves\" nor \"disproves\" an axiom for a set of theorems; the point is simply that in the conceptual realm identified by the axioms, the theorems logically follow. In contrast, in physics a comparison with experiments always makes sense, since a falsified physical theory needs modification.\n\nIn the field of mathematical logic, a clear distinction is made between two notions of axioms: \"logical\" and \"non-logical\" (somewhat similar to the ancient distinction between \"axioms\" and \"postulates\" respectively).\n\nThese are certain formulas in a formal language that are universally valid, that is, formulas that are satisfied by every assignment of values. Usually one takes as logical axioms \"at least\" some minimal set of tautologies that is sufficient for proving all tautologies in the language; in the case of predicate logic more logical axioms than that are required, in order to prove logical truths that are not tautologies in the strict sense.\n\nIn propositional logic it is common to take as logical axioms all formulae of the following forms, where formula_1, formula_2, and formula_3 can be any formulae of the language and where the included primitive connectives are only \"formula_4\" for negation of the immediately following proposition and \"formula_5\" for implication from antecedent to consequent propositions:\n\n\nEach of these patterns is an \"axiom schema\", a rule for generating an infinite number of axioms. For example, if formula_9, formula_10, and formula_11 are propositional variables, then formula_12 and formula_13 are both instances of axiom schema 1, and hence are axioms. It can be shown that with only these three axiom schemata and \"modus ponens\", one can prove all tautologies of the propositional calculus. It can also be shown that no pair of these schemata is sufficient for proving all tautologies with \"modus ponens\".\n\nOther axiom schemas involving the same or different sets of primitive connectives can be alternatively constructed.\n\nThese axiom schemata are also used in the predicate calculus, but additional logical axioms are needed to include a quantifier in the calculus.\n\nThis means that, for any variable symbol formula_14 the formula formula_15 can be regarded as an axiom. Also, in this example, for this not to fall into vagueness and a never-ending series of \"primitive notions\", either a precise notion of what we mean by formula_15 (or, for that matter, \"to be equal\") has to be well established first, or a purely formal and syntactical usage of the symbol formula_17 has to be enforced, only regarding it as a string and only a string of symbols, and mathematical logic does indeed do that.\n\nAnother, more interesting example axiom scheme, is that which provides us with what is known as Universal Instantiation:\n\nWhere the symbol formula_18 stands for the formula formula_1 with the term formula_20 substituted for formula_21. (See Substitution of variables.) In informal terms, this example allows us to state that, if we know that a certain property formula_22 holds for every formula_21 and that formula_20 stands for a particular object in our structure, then we should be able to claim formula_25. Again, \"we are claiming that the formula\" formula_26 \"is valid\", that is, we must be able to give a \"proof\" of this fact, or more properly speaking, a \"metaproof\". Actually, these examples are \"metatheorems\" of our theory of mathematical logic since we are dealing with the very concept of \"proof\" itself. Aside from this, we can also have Existential Generalization:\n\nNon-logical axioms are formulas that play the role of theory-specific assumptions. Reasoning about two different structures, for example the natural numbers and the integers, may involve the same logical axioms; the non-logical axioms aim to capture what is special about a particular structure (or set of structures, such as groups). Thus non-logical axioms, unlike logical axioms, are not \"tautologies\". Another name for a non-logical axiom is \"postulate\".\n\nAlmost every modern mathematical theory starts from a given set of non-logical axioms, and it was thought that in principle every theory could be axiomatized in this way and formalized down to the bare language of logical formulas.\n\nNon-logical axioms are often simply referred to as \"axioms\" in mathematical discourse. This does not mean that it is claimed that they are true in some absolute sense. For example, in some groups, the group operation is commutative, and this can be asserted with the introduction of an additional axiom, but without this axiom we can do quite well developing (the more general) group theory, and we can even take its negation as an axiom for the study of non-commutative groups.\n\nThus, an \"axiom\" is an elementary basis for a formal logic system that together with the rules of inference define a deductive system.\n\nThis section gives examples of mathematical theories that are developed entirely from a set of non-logical axioms (axioms, henceforth). A rigorous treatment of any of these topics begins with a specification of these axioms.\n\nBasic theories, such as arithmetic, real analysis and complex analysis are often introduced non-axiomatically, but implicitly or explicitly there is generally an assumption that the axioms being used are the axioms of Zermelo–Fraenkel set theory with choice, abbreviated ZFC, or some very similar system of axiomatic set theory like Von Neumann–Bernays–Gödel set theory, a conservative extension of ZFC. Sometimes slightly stronger theories such as Morse–Kelley set theory or set theory with a strongly inaccessible cardinal allowing the use of a Grothendieck universe are used, but in fact most mathematicians can actually prove all they need in systems weaker than ZFC, such as second-order arithmetic.\n\nThe study of topology in mathematics extends all over through point set topology, algebraic topology, differential topology, and all the related paraphernalia, such as homology theory, homotopy theory. The development of \"abstract algebra\" brought with itself group theory, rings, fields, and Galois theory.\n\nThis list could be expanded to include most fields of mathematics, including measure theory, ergodic theory, probability, representation theory, and differential geometry.\n\nThe Peano axioms are the most widely used \"axiomatization\" of first-order arithmetic. They are a set of axioms strong enough to prove many important facts about number theory and they allowed Gödel to establish his famous second incompleteness theorem.\n\nWe have a language formula_27 where formula_28 is a constant symbol and formula_29 is a unary function and the following axioms:\n\n\nThe standard structure is formula_35 where formula_36 is the set of natural numbers, formula_29 is the successor function and formula_28 is naturally interpreted as the number 0.\n\nProbably the oldest, and most famous, list of axioms are the 4 + 1 Euclid's postulates of plane geometry. The axioms are referred to as \"4 + 1\" because for nearly two millennia the fifth (parallel) postulate (\"through a point outside a line there is exactly one parallel\") was suspected of being derivable from the first four. Ultimately, the fifth postulate was found to be independent of the first four. Indeed, one can assume that exactly one parallel through a point outside a line exists, or that infinitely many exist. This choice gives us two alternative forms of geometry in which the interior angles of a triangle add up to exactly 180 degrees or less, respectively, and are known as Euclidean and hyperbolic geometries. If one also removes the second postulate (\"a line can be extended indefinitely\") then elliptic geometry arises, where there is no parallel through a point outside a line, and in which the interior angles of a triangle add up to more than 180 degrees.\n\nThe objectives of study are within the domain of real numbers. The real numbers are uniquely picked out (up to isomorphism) by the properties of a \"Dedekind complete ordered field\", meaning that any nonempty set of real numbers with an upper bound has a least upper bound. However, expressing these properties as axioms requires use of second-order logic. The Löwenheim–Skolem theorems tell us that if we restrict ourselves to first-order logic, any axiom system for the reals admits other models, including both models that are smaller than the reals and models that are larger. Some of the latter are studied in non-standard analysis.\n\nA deductive system consists of a set formula_39 of logical axioms, a set formula_40 of non-logical axioms, and a set formula_41 of \"rules of inference\". A desirable property of a deductive system is that it be complete. A system is said to be complete if, for all formulas formula_1,\n\nformula_43\nthat is, for any statement that is a \"logical consequence\" of formula_40 there actually exists a \"deduction\" of the statement from formula_40. This is sometimes expressed as \"everything that is true is provable\", but it must be understood that \"true\" here means \"made true by the set of axioms\", and not, for example, \"true in the intended interpretation\". Gödel's completeness theorem establishes the completeness of a certain commonly used type of deductive system.\n\nNote that \"completeness\" has a different meaning here than it does in the context of Gödel's first incompleteness theorem, which states that no \"recursive\", \"consistent\" set of non-logical axioms formula_40 of the Theory of Arithmetic is \"complete\", in the sense that there will always exist an arithmetic statement formula_1 such that neither formula_1 nor formula_49 can be proved from the given set of axioms.\n\nThere is thus, on the one hand, the notion of \"completeness of a deductive system\" and on the other hand that of \"completeness of a set of non-logical axioms\". The completeness theorem and the incompleteness theorem, despite their names, do not contradict one another.\n\nEarly mathematicians regarded axiomatic geometry as a model of physical space, and obviously there could only be one such model. The idea that alternative mathematical systems might exist was very troubling to mathematicians of the 19th century and the developers of systems such as Boolean algebra made elaborate efforts to derive them from traditional arithmetic. Galois showed just before his untimely death that these efforts were largely wasted. Ultimately, the abstract parallels between algebraic systems were seen to be more important than the details and modern algebra was born. In the modern view axioms may be any set of formulas, as long as they are not known to be inconsistent.\n\n\n\n"}
{"id": "15989285", "url": "https://en.wikipedia.org/wiki?curid=15989285", "title": "Bitlaha", "text": "Bitlaha\n\nBitlaha is a South Asian concept used as a social punishment for violating the norms of exogamy and endogamy.\n\nThe concept has been used by the Santals of India and the Satars of Nepal, who call themselves \"hod\", meaning “human beings”. The hod in Nepal translate \"bitlaha\" as an outcast, disorder, polluted or unclean person. “It is a temporary state that affects both the village and members…Bitlaha is contagious, affecting a bitlahas parents, guardians and whole village” (Skinner 207). Once a person is considered bitlaha they are no longer considered a member of their ethnic group and they are shunned and exiled from their community. The pancha, a male politician in Hod society, gives the bitlaha a chance to remove the derogatory name and reenter the ethnic group by paying a severe penalty.\n\nMost bitlaha pay their penalty or their parents do so they will cease to be shunned from festivals, carnivals, celebrations and religious rituals. Those who decide to remain a bitlaha live in an isolated non-hod community for the rest of their lives. Even though bitlaha do not have to return to their villages, almost all of them do because of kinship networks and strong friendships within the village. Santals use bitlaha as a way to outcast those who have sexually misbehaved. Santals believe that exposing these culprits to corruption and public ridicule will make individuals stray from sexual misbehaving. “The tribe asserts itself as the supreme body regulating the conduct of its members”, which motivates individuals to behave properly and not shame their Hods.\n\n"}
{"id": "780856", "url": "https://en.wikipedia.org/wiki?curid=780856", "title": "Black-tailed deer", "text": "Black-tailed deer\n\nTwo forms of black-tailed deer or blacktail deer that occupy coastal woodlands in the Pacific Northwest are subspecies of the mule deer (\"Odocoileus hemionus\"). They have sometimes been treated as a species, but virtually all recent authorities maintain they are subspecies.\nThe Columbian black-tailed deer (\"Odocoileus hemionus columbianus\") is found in western North America, from Northern California into the Pacific Northwest and coastal British Columbia. The Sitka deer (\"Odocoileus hemionus sitkensis\") is found coastally in British Columbia, Southeast Alaska and Southcentral Alaska (as far as Kodiak Island).\n\nBlack-tailed deer once lived at least as far east as Wyoming. In Francis Parkman's \"The Oregon Trail,\" an eyewitness account of his 1846 trek across the early West, while within a two-day ride from Fort Laramie, Parkman writes of shooting what he believes to be an elk, only to discover he has killed a black-tailed deer.\n\nThe black-tailed deer is currently common in California, western Oregon, Washington, in coastal and interior British Columbia, and north into the Alaskan panhandle. It is a popular game animal.\n\nThough it has been argued that the black-tailed deer is a species, virtually all recent authorities maintain it as a subspecies of the mule deer (\"Odocoileus hemionus\"). Strictly speaking, the black-tailed deer group consists of two subspecies, as it also includes \"O. h. sitkensis\" (the Sitka deer). The black-tailed deer group and the mule deer group (\"sensu stricto\") hybridize, and mule deer appear to have evolved from the black-tailed deer group. Despite this, the mtDNA of the white-tailed deer and mule deer are similar, but differ from that of the black-tailed deer. This may be the result of introgression, although hybrids between the mule deer and white-tailed deer are rare in the wild (apparently more common locally in West Texas), and the hybrid survival rate is low even in captivity.\n\nThese two subspecies thrive on the edge of the forest, as the dark forest lacks the underbrush and grasslands the deer prefer as food, and completely open areas lack the hiding spots and cover they prefer for harsh weather. One of the plants that black-tailed deer browse is western poison oak, despite its irritant content. This deer often is most active at dawn and dusk, and is frequently involved in collisions with automobiles.\n\nDeer are browsers. During the winter and early spring, they feed on Douglas-fir, western red cedar, red huckleberry, salal, deer fern, and lichens growing on trees. Late spring to fall, they consume grasses, blackberries, apples, fireweed, pearly everlasting, forbs, salmonberry, salal, and maple. The mating or 'rutting' season occurs during November and early December. Bucks can be observed running back and forth across the roads in the pursuit of does. After the rut, the bucks tend to hide and rest, often nursing wounds. They suffer broken antlers, and have lost weight. They drop their antlers between January and March. Antlers on the forest floor provide a source of calcium and other nutrients to other forest inhabitants. Bucks regrow their antlers beginning in April through to August.\n\nThe gestation period for does is six to seven months, with fawns being born in late May and into June. Twins are the rule, although young does often have only single fawns. Triplets can also occur. Fawns weigh and have no scent for the first week or so. This enables the mother to leave the fawn hidden while she goes off to browse and replenish her body after giving birth. She must also eat enough to produce enough milk to feed her fawns. Although does are excellent mothers, fawn mortality rate is 45 to 70%. Does are very protective of their young and humans are viewed as predators.\n\nDeer communicate with the aid of scent and pheromones from several glands located on the lower legs. The metatarsal (outside of lower leg) produces an alarm scent, the tarsal (inside of hock) serves for mutual recognition and the interdigital (between the toes) leave a scent trail when deer travel. Deer have excellent sight and smell. Their large ears can move independently of each other and pick up any unusual sounds that may signal danger.\n\nAt dawn and dusk and moonlit nights, deer are seen browsing on the roadside. Wooded areas with forests on both sides of the road and open, grassy areas, i.e. golf courses, attract deer. Caution when driving is prudent because often as one deer crosses, another one or two follow.\n\nIn Southeast Alaska, the Sitka deer is the primary prey of the rare Alexander Archipelago wolf (\"Canis lupus ligoni\"), which is endemic to the region. In the mid-1990s, the United States Fish and Wildlife Service evaluated a petition to list this wolf subspecies as threatened, and decided a listing was not warranted in August 1997, largely on the basis of provisions the Forest Service had included to protect the viability of the wolf subspecies in its Forest Plan for the Tongass National Forest, adopted three months earlier. The Tongass NF is important in wolf conservation because it includes about 80% of the region's land area. The protections for the wolf included a standard and guideline intended to retain, in the face of logging losses, enough habitat carrying capacity for deer in winter to assure the viability of the Alexander Archipelago wolf and an adequate supply of deer for hunters. The needed carrying capacity was originally specified as 13 deer per square mile, but was corrected in 2000 to 18. Use of a deer model is specified for determining carrying capacity, and is the only tool available for the purpose.\n\nHowever, the Forest Service's implementation of the deer provision in the Tongass wolf standard and guideline has been controversial for many years, and led to a lawsuit by Greenpeace and Cascadia Wildlands in 2008, over four logging projects. The data set the Forest Service was using in the deer model was known through the agency's own study (done in 2000) to generally overestimate the carrying capacity for deer and underestimate the impacts of logging. The study showed the data set (called Vol-Strata) is not correlated to habitat quality. Also, a conversion factor, known as the \"deer multiplier\" (used in calculating carrying capacity) was incorrectly applied, causing — by itself – a 30% overestimation of carrying capacity and corresponding underestimation of impacts. The combined effect of the two errors is variable because Vol-Strata is not correlated to habitat quality. Regarding the Traitors Cove Timber Sales project, in 2011 the plaintiffs noted in oral arguments before the 9th Circuit Court of Appeals that the difference is between a claimed 21 deer per square mile carrying capacity in the project EIS, and 9.5 deer per square mile (about half of the Tongass Forest Plan's requirement) according to unpublished corrections the agency made in 2008.\n\nThe 9th Circuit panel ruled unanimously on August 2, 2011 in favor of the plaintiffs, remanding the four timber sale decisions to the Forest Service and giving guidance for what is necessary during reanalysis of impacts to deer. The ruling says in part:\n\nWe do not think that USFS has adequately explained its decision to approve the four logging projects in the Tongass. ... USFS has failed to explain how it ended up with a table that identifies 100 deer per square mile as a maximum carrying capacity, but allows 130 deer per square mile as a potential carrying capacity. 'The agency is obligated to articulate a rational connection between the facts found and the choices made,' which the agency has not done here. Pac. Coast Fed'n of Fisherman's Ass'ns v. U.S. Bureau of Reclamation, 426 F.3d 1082, 1091 (9th Cir. 2005)...\"\n\nWe have similar questions about USFS's use of VolStrata data, which identifies total timber volume and not forest structure, to approve the projects, where forest structure—and not total timber volume—is relevant to the habitability of a piece of land. USFS itself has recognized the limitations in the VolStrata data. ... Because we must remand to the agency to re-examine its Deer Model, we need not decide whether the use of the VolStrata data was arbitrary and capricious. We anticipate that, in reviewing the proposed projects, USFS will use the best available data ...\n\nIn a statement to the press, a spokesman for the plaintiffs said the errors in this lawsuit apply to every significant Tongass timber sale decision between 1996 and 2008, before the Forest Service corrected errors in the deer model when the agency issued its revised Tongass Forest Plan in 2008. But he said despite those corrections, the agency still fails to address cumulative impacts to deer, especially on Prince of Wales Island, as is being challenged in the Logjam timber sale lawsuit, by ignoring substantial logging on nonfederal lands. In September 2013, under the same litigation, the U.S. District Court in Anchorage made a second remand to the Forest Service because the agency's further work under the first remand had not resolved the modeling issues. Activity on the four timber sales involved in the litigation has been suspended since 2008.\n\n"}
{"id": "3146632", "url": "https://en.wikipedia.org/wiki?curid=3146632", "title": "Bland–Altman plot", "text": "Bland–Altman plot\n\ncodice_1\nA Bland–Altman plot (Difference plot) in analytical chemistry or biomedicine is a method of data plotting used in analyzing the agreement between two different assays. It is identical to a Tukey mean-difference plot, the name by which it is known in other fields, but was popularised in medical statistics by J. Martin Bland and Douglas G. Altman.\n\nBland and Altman make the point that any two methods that are designed to measure the same parameter (or property) should have good correlation when a set of samples are chosen such that the property to be determined varies considerably. A high correlation for any two methods designed to measure the same property could thus in itself just be a sign that one has chosen a widespread sample. A high correlation does not necessarily imply that there is good agreement between the two methods.\n\nConsider a set of formula_1 samples (for example, objects of unknown volume). Both assays (for example, different methods of volume measurement) are performed on each sample, resulting in formula_2 data points. Each of the formula_1 samples is then represented on the graph by assigning the mean of the two measurements as the formula_4-value, and the difference between the two values as the formula_5-value.\n\nThe Cartesian coordinates of a given sample formula_6 with values of formula_7 and formula_8 determined by the two assays is\n\nFor comparing the dissimilarities between the two sets of samples independently from their mean values, it is more appropriate to look at the ratio of the pairs of measurements. Log transformation (base 2) of the measurements before the analysis will enable the standard approach to be used; so the plot will be given by the following equation:\n\nThis version of the plot is used in MA plot.\n\nOne primary application of the Bland–Altman plot is to compare two clinical measurements that each produce some error in their measures. It can also be used to compare a new measurement technique or method with a gold standard, as even a gold standard does not - and should not - imply it to be without error. See Analyse-it, MedCalc, NCSS, GraphPad Prism, R, or StatsDirect for software providing Bland–Altman plots.\n\nBland-Altman plots are extensively used to evaluate the agreement among two different instruments or two measurements techniques. Bland-Altman plots allow identification of any systematic difference between the measurements (i.e., fixed bias) or possible outliers. The mean difference is the estimated bias, and the SD of the differences measures the random fluctuations around this mean. If the mean value of the difference differs significantly from 0 on the basis of a 1-sample t-test, this indicates the presence of fixed bias. If there is a consistent bias, it can be adjusted for by subtracting the mean difference from the new method. It is common to compute 95% limits of agreement for each comparison (average difference ± 1.96 standard deviation of the difference), which tells us how far apart measurements by 2 methods were more likely to be for most individuals. If the differences within mean ± 1.96 SD are not clinically important, the two methods may be used interchangeably. The 95% limits of agreement can be unreliable estimates of the population parameters especially for small sample sizes so, when comparing methods or assessing repeatability, it is important to calculate confidence intervals for 95% limits of agreement. This can be done by Bland and Altman's approximate method or by more precise methods.\n\nBland-Altman plots were also used to investigate any possible relationship of the discrepancies between the measurements and the true value (i.e., proportional bias). The existence of proportional bias indicates that the methods do not agree equally through the range of measurements (i.e., the limits of agreement will depend on the actual measurement). To evaluate this relationship formally, the difference between the methods should be regressed on the average of the 2 methods. When a relationship between the differences and the true value was identified (i.e., a significant slope of the regression line), regression-based 95% limits of agreement should be provided.\n\nA similar method was proposed in 1981 by Eksborg. This method was based on Deming regression—a method introduced by Adcock in 1878.\n\nBland and Altman's Lancet paper was number 29 in a list of the top 100 most-cited papers of all time with over 23,000 citations.\n\n"}
{"id": "3875858", "url": "https://en.wikipedia.org/wiki?curid=3875858", "title": "Brauer algebra", "text": "Brauer algebra\n\nIn mathematics, a Brauer algebra is an algebra introduced by used in the representation theory of the orthogonal group. It plays the same role that the symmetric group does for the representation theory of the general linear group in Schur–Weyl duality.\n\nThe Brauer algebra depends on the choice of a positive integer \"n\" and a number \"d\" (which in practice is often the dimension of the fundamental representation of an orthogonal group \"O\"). The Brauer algebra has dimension (2\"n\")!/2\"n\"! = (2\"n\" − 1)(2\"n\" − 3) ··· 5·3·1 and has a basis consisting of all pairings on a set of 2\"n\" elements \"X\", ..., \"X\", \"Y\", ..., \"Y\" (that is, all perfect matchings of a complete graph \"K\": any two of the 2\"n\" elements may be matched to each other, regardless of their symbols). The elements \"X\" are usually written in a row, with the elements \"Y\" beneath them. The product of two basis elements \"A\" and \"B\" is obtained by first identifying the endpoints in the bottom row of \"A \" and the top row of \"B \" (Figure \"AB \" in the diagram), then deleting the endpoints in the middle row and joining endpoints in the remaining two rows if they are joined, directly or by a path, in \"AB \" (Figure \"AB=nn \" in the diagram).\n\nIf \"O\"(R) is the orthogonal group acting on \"V\" = R, then \nthe Brauer algebra has a natural action on the space of polynomials on \"V\" commuting with the action of the orthogonal group.\n\n\n"}
{"id": "47278", "url": "https://en.wikipedia.org/wiki?curid=47278", "title": "Cognitive bias", "text": "Cognitive bias\n\nA cognitive bias is a systematic pattern of deviation from norm or rationality in judgment. Individuals create their own \"subjective social reality\" from their perception of the input. An individual's construction of social reality, not the objective input, may dictate their behaviour in the social world. Thus, cognitive biases may sometimes lead to perceptual distortion, inaccurate judgment, illogical interpretation, or what is broadly called irrationality.\n\nSome cognitive biases are presumably adaptive. Cognitive biases may lead to more effective actions in a given context. Furthermore, allowing cognitive biases enable faster decisions which can be desirable when timeliness is more valuable than accuracy, as illustrated in heuristics. Other cognitive biases are a \"by-product\" of human processing limitations, resulting from a lack of appropriate mental mechanisms (bounded rationality), or simply from a limited capacity for information processing.\n\nA continually evolving list of cognitive biases has been identified over the last six decades of research on human judgment and decision-making in cognitive science, social psychology, and behavioral economics. Kahneman and Tversky (1996) argue that cognitive biases have efficient practical implications for areas including clinical judgment, entrepreneurship, finance, and management.\n\nBias arises from various processes that are sometimes difficult to distinguish. These include\n\n\nThe notion of cognitive biases was introduced by Amos Tversky and Daniel Kahneman in 1972 and grew out of their experience of people's \"innumeracy\", or inability to reason intuitively with the greater orders of magnitude. Tversky, Kahneman and colleagues demonstrated several replicable ways in which human judgments and decisions differ from rational choice theory. Tversky and Kahneman explained human differences in judgement and decision making in terms of heuristics. Heuristics involve mental shortcuts which provide swift estimates about the possibility of uncertain occurrences. Heuristics are simple for the brain to compute but sometimes introduce \"severe and systematic errors.\"\n\nFor example, the representativeness heuristic is defined as the tendency to \"judge the frequency or likelihood\" of an occurrence by the extent of which the event \"resembles the typical case\". The \"Linda Problem\" illustrates the representativeness heuristic (Tversky & Kahneman, 1983). Participants were given a description of \"Linda\" that suggests Linda might well be a feminist (e.g., she is said to be concerned about discrimination and social justice issues). They were then asked whether they thought Linda was more likely to be a \"(a) bank teller\" or a \"(b) bank teller and active in the feminist movement\". A majority chose answer (b). This error (mathematically, answer (b) cannot be more likely than answer (a)) is an example of the \"conjunction fallacy\"; Tversky and Kahneman argued that respondents chose (b) because it seemed more \"representative\" or typical of persons who might fit the description of Linda. The representativeness heuristic may lead to errors such as activating stereotypes and inaccurate judgments of others (Haselton et al., 2005, p. 726).\n\nAlternatively, critics of Kahneman and Tversky such as Gerd Gigerenzer argue that heuristics should not lead us to conceive of human thinking as riddled with irrational cognitive biases, but rather to conceive rationality as an adaptive tool that is not identical to the rules of formal logic or the probability calculus. Nevertheless, experiments such as the \"Linda problem\" grew into the heuristics and biases research program which spread beyond academic psychology into other disciplines including medicine and political science.\n\nBiases can be distinguished on a number of dimensions. \nFor example,\nSome biases reflect a subject's \"motivation\", for example, the desire for a positive self-image leading to egocentric bias and the avoidance of unpleasant cognitive dissonance. Other biases are due to the particular way the brain perceives, forms memories and makes judgments. This distinction is sometimes described as \"hot cognition\" versus \"cold cognition\", as motivated reasoning can involve a state of arousal.\n\nAmong the \"cold\" biases,\n\n\nThe fact that some biases reflect motivation, and in particular the motivation to have positive attitudes to oneself accounts for the fact that many biases are self-serving or self-directed (e.g., illusion of asymmetric insight, self-serving bias). There are also biases in how subjects evaluate in-groups or out-groups; evaluating in-groups as more diverse and \"better\" in many respects, even when those groups are arbitrarily-defined (ingroup bias, outgroup homogeneity bias).\n\nSome cognitive biases belong to the subgroup of attentional biases which refer to the paying of increased attention to certain stimuli. It has been shown, for example, that people addicted to alcohol and other drugs pay more attention to drug-related stimuli. Common psychological tests to measure those biases are the Stroop task and the dot probe task.\n\nIndividuals' susceptibility to some types of cognitive biases can be measured by the Cognitive Reflection Test (CRT) developed by Frederick (2005).\n\nThe following is a list of the more commonly studied cognitive biases:\n\nA 2012 \"Psychological Bulletin\" article suggests that at least 8 seemingly unrelated biases can be produced by the same information-theoretic generative mechanism. It is shown that noisy deviations in the memory-based information processes that convert objective evidence (observations) into subjective estimates (decisions) can produce regressive conservatism, the belief revision (Bayesian conservatism), illusory correlations, illusory superiority (better-than-average effect) and worse-than-average effect, subadditivity effect, exaggerated expectation, overconfidence, and the hard–easy effect.\n\nMany social institutions rely on individuals to make rational judgments.\n\nThe securities regulation regime largely assumes that all investors act as perfectly rational persons. In truth, actual investors face cognitive limitations from biases, heuristics, and framing effects.\n\nA fair jury trial, for example, requires that the jury ignore irrelevant features of the case, weigh the relevant features appropriately, consider different possibilities open-mindedly and resist fallacies such as appeal to emotion. The various biases demonstrated in these psychological experiments suggest that people will frequently fail to do all these things. However, they fail to do so in systematic, directional ways that are predictable.\n\nCognitive biases are also related to the persistence of superstition, to large social issues such as prejudice, and they also work as a hindrance in the acceptance of scientific non-intuitive knowledge by the public.\n\nHowever, in some academic disciplines, the study of bias is very popular. For instance, bias is a wide spread phenomenon and well studied, because most decisions that concern the minds and hearts of entrepreneurs are computationally intractable\n\nSimilar to Gigerenzer (1996), Haselton et al. (2005) state the content and direction of cognitive biases are not \"arbitrary\" (p. 730). Moreover, cognitive biases can be controlled. Debiasing is a technique which aims to decrease biases by encouraging individuals to use controlled processing compared to automatic processing. In relation to reducing the FAE, monetary incentives and informing participants they will be held accountable for their attributions have been linked to the increase of accurate attributions. Training has also shown to reduce cognitive bias. Morewedge and colleagues (2015) found that research participants exposed to one-shot training interventions, such as educational videos and debiasing games that taught mitigating strategies, exhibited significant reductions in their commission of six cognitive biases immediately and up to 3 months later.\n\nCognitive bias modification refers to the process of modifying cognitive biases in healthy people and also refers to a growing area of psychological (non-pharmaceutical) therapies for anxiety, depression and addiction called cognitive bias modification therapy (CBMT). CBMT is sub-group of therapies within a growing area of psychological therapies based on modifying cognitive processes with or without accompanying medication and talk therapy, sometimes referred to as applied cognitive processing therapies (ACPT). Although cognitive bias modification can refer to modifying cognitive processes in healthy individuals, CBMT is a growing area of evidence-based psychological therapy, in which cognitive processes are modified to relieve suffering from serious depression, anxiety, and addiction. CBMT techniques are technology assisted therapies that are delivered via a computer with or without clinician support. CBM combines evidence and theory from the cognitive model of anxiety, cognitive neuroscience, and attentional models.\n\nThere are criticisms against theories of cognitive biases based on the fact that both sides in a debate often claim each other's thoughts to be in human nature and the result of cognitive bias, while claiming their own viewpoint as being the correct way to \"overcome\" cognitive bias. This is not due simply to debate misconduct but is a more fundamental problem that stems from psychology's making up of multiple opposed cognitive bias theories that can be non-falsifiably used to explain away any viewpoint.\n\n\n"}
{"id": "6198", "url": "https://en.wikipedia.org/wiki?curid=6198", "title": "Convention on Biological Diversity", "text": "Convention on Biological Diversity\n\nThe Convention on Biological Diversity (CBD), known informally as the Biodiversity Convention, is a multilateral treaty. The Convention has three main goals including: the conservation of biological diversity (or biodiversity); the sustainable use of its components; and the fair and equitable sharing of benefits arising from genetic resources.\n\nIn other words, its objective is to develop national strategies for the conservation and sustainable use of biological diversity. It is often seen as the key document regarding sustainable development. The Convention was opened for signature at the Earth Summit in Rio de Janeiro on 5 June 1992 and entered into force on 29 December 1993. At the 2010 10th Conference of Parties (COP) to the Convention on Biological Diversity in October in Nagoya, Japan, the Nagoya Protocol was adopted.\n\nThe notion of an international convention on bio-diversity was conceived at a United Nations Environment Programme (UNEP) Ad Hoc Working Group of Experts on Biological Diversity in November 1988. The subsequent year, the Ad Hoc Working Group of Technical and Legal Experts was established for the drafting of a legal text which addressed the conservation and sustainable use of biological diversity, as well as the sharing of benefits arising from their utilization with sovereign states and local communities.\nIn 1991, an intergovernmental negotiating committee was established, tasked with finalizing the convention's text.\n\nA Conference for the Adoption of the Agreed Text of the Convention on Biological Diversity was held in Nairobi, Kenya, in 1992, and its conclusions were distilled in the Nairobi Final Act. The Convention's text was opened for signature on 5 June 1992 at the United Nations Conference on Environment and Development (the Rio \"Earth Summit\"). By its closing date, 4 June 1993, the convention had received 168 signatures. It entered into force on 29 December 1993.\n\nThe convention recognized for the first time in international law that the conservation of biodiversity is \"a common concern of humankind\" and is an integral part of the development process. The agreement covers all ecosystems, species, and genetic resources. It links traditional conservation efforts to the economic goal of using biological resources sustainably. It sets principles for the fair and equitable sharing of the benefits arising from the use of genetic resources, notably those destined for commercial use. It also covers the rapidly expanding field of biotechnology through its Cartagena Protocol on Biosafety, addressing technology development and transfer, benefit-sharing and biosafety issues. Importantly, the Convention is legally binding; countries that join it ('Parties') are obliged to implement its provisions.\n\nThe convention reminds decision-makers that natural resources are not infinite and sets out a philosophy of sustainable use. While past conservation efforts were aimed at protecting particular species and habitats, the Convention recognizes that ecosystems, species and genes must be used for the benefit of humans. However, this should be done in a way and at a rate that does not lead to the long-term decline of biological diversity.\n\nThe convention also offers decision-makers guidance based on the precautionary principle which demands that where there is a threat of significant reduction or loss of biological diversity, lack of full scientific certainty should not be used as a reason for postponing measures to avoid or minimize such a threat. The Convention acknowledges that substantial investments are required to conserve biological diversity. It argues, however, that conservation will bring us significant environmental, economic and social benefits in return.\n\nThe Convention on Biological Diversity of 2010 banned some forms of geoengineering.\n\nSome of the many issues dealt with under the convention include:\n\n\nThe Cartagena Protocol on Biosafety of the Convention, also known as the Biosafety Protocol, was adopted in January 2000. The Biosafety Protocol seeks to protect biological diversity from the potential risks posed by living modified organisms resulting from modern biotechnology.\n\nThe Biosafety Protocol makes clear that products from new technologies must be based on the precautionary principle and allow developing nations to balance public health against economic benefits. It will for example let countries ban imports of a genetically modified organism if they feel there is not enough scientific evidence the product is safe and requires exporters to label shipments containing genetically modified commodities such as corn or cotton.\n\nThe required number of 50 instruments of ratification/accession/approval/acceptance by countries was reached in May 2003. In accordance with the provisions of its Article 37, the Protocol entered into force on 11 September 2003.\n\nIn April 2002, the parties of the UN CBD adopted the recommendations of the Gran Canaria Declaration Calling for a Global Plant Conservation Strategy, and adopted a 16-point plan aiming to slow the rate of plant extinctions around the world by 2010.\n\nAs of 2016, the Convention has 196 parties, which includes 195 states and the European Union. All UN member states—with the exception of the United States—have ratified the treaty. Non-UN member states that have ratified are the Cook Islands, Niue, and the State of Palestine. The Holy See and the states with limited recognition are non-parties. The US has signed but not ratified the treaty, and has not announced plans to ratify it.\n\nConference of the parties:\nThe convention's governing body is the Conference of the parties (COP), consisting of all governments (and regional economic integration organizations) that have ratified the treaty. This ultimate authority reviews progress under the Convention, identifies new priorities, and sets work plans for members. The COP can also make amendments to the Convention, create expert advisory bodies, review progress reports by member nations, and collaborate with other international organizations and agreements.\n\nThe Conference of the Parties uses expertise and support from several other bodies that are established by the Convention. In addition to committees or mechanisms established on an ad hoc basis, two main organs are:\n\nSecretariat:\nThe CBD Secretariat, based in Montreal, operates under the United Nations Environment Programme. Its main functions are to organize meetings, draft documents, assist member governments in the implementation of the programme of work, coordinate with other international organizations, and collect and disseminate information.\n\nSubsidiary body for Scientific, Technical and Technological Advice (SBSTTA):\nThe SBSTTA is a committee composed of experts from member governments competent in relevant fields. It plays a key role in making recommendations to the COP on scientific and technical issues.\n\nThe 13th Meeting of the SBSTTA, or SBSTTA-13, held from 18–22 February 2008 at the FAO headquarters in Rome, Italy. SBSTTA-13 delegates finalized and adopted recommendations on the in-depth reviews of the work programmes on agricultural and forest biodiversity and SBSTTA's modus operandi for the consideration of new and emerging issues, as well as on inland waters biodiversity, marine biodiversity, invasive alien species, and biodiversity and climate change.\n\nThe current chairperson of the SBSTTA is Dr. Theresa Mundita Lim of the Philippines.\n\n\"National Biodiversity Strategies and Action Plans (NBSAPs) are the principal instruments for implementing the Convention at the national level (Article 6). The Convention requires countries to prepare a national biodiversity strategy (or equivalent instrument) and to ensure that this strategy is mainstreamed into the planning and activities of all those sectors whose activities can have an impact (positive and negative) on biodiversity. To date [2012-02-01], 173 Parties have developed NBSAPs in line with Article 6.\"\n\nFor example, the United Kingdom, New Zealand and Tanzania have carried out elaborate responses to conserve individual species and specific habitats. The United States of America, a signatory who has not yet ratified the treaty, has produced one of the most thorough implementation programs through species Recovery Programs and other mechanisms long in place in the USA for species conservation.\n\nSingapore has also established a detailed \"National Biodiversity Strategy and Action Plan\". The \"National Biodiversity Centre\" of Singapore represents Singapore in the Convention for Biological Diversity.\n\nIn accordance with Article 26 of the Convention, Parties prepare national reports on the status of implementation of the Convention.\n\nThe current executive secretary is Cristiana Pașca Palmer, who took up this post on 17 March 2017. Braulio Ferreira de Souza Dias was the previous executive secretary.\n\nThe Nagoya Protocol on Access to Genetic Resources and the Fair and Equitable Sharing of Benefits Arising from their Utilization to the Convention on Biological Diversity is a supplementary agreement to the Convention on Biological Diversity. It provides a transparent legal framework for the effective implementation of one of the three objectives of the CBD: the fair and equitable sharing of benefits arising out of the utilization of genetic resources. The Protocol was adopted on 29 October 2010 in Nagoya, Aichi Province, Japan, and entered into force on 12 October 2014. Its objective is the fair and equitable sharing of benefits arising from the utilization of genetic resources, thereby contributing to the conservation and sustainable use of biodiversity.\n\nThe first ordinary meeting of the parties to the convention took place in November and December 1994, in Nassau, Bahamas.\n\nThe second ordinary meeting of the parties to the convention took place in November 1995, in Jakarta, Indonesia.\n\nThe third ordinary meeting of the parties to the convention took place in November 1996, in Buenos Aires, Argentina.\n\nThe fourth ordinary meeting of the parties to the convention took place in May 1998, in Bratislava, Slovakia.\n\nThe First Extraordinary Meeting of the Conference of the Parties took place in February 1999, in Cartagena, Colombia.\n\nThe fifth ordinary meeting of the parties to the convention took place in May 2000, in Nairobi, Kenya.\n\nThe sixth ordinary meeting of the parties to the convention took place in April 2002, in The Hague, Netherlands.\n\nThe seventh ordinary meeting of the parties to the convention took place in February 2004, in Kuala Lumpur, Malaysia.\n\nThe eighth ordinary meeting of the parties to the convention took place in March 2006, in Curitiba, Brazil.\n\nThe ninth ordinary meeting of the parties to the convention took place in May 2008, in Bonn, Germany.\n\nThe tenth ordinary meeting of the parties to the convention took place in October 2010, in Nagoya, Japan.\n\nLeading up to the Conference of the Parties (COP 11) meeting on biodiversity in Hyderabad, India 2012, preparations for a World Wide Views on Biodiversity has begun, involving old and new partners and building on the experiences from the World Wide Views on Global Warming.\n\nUnder the theme, \"Biodiversity for Sustainable Development,\" thousands of representatives of governments, NGOs, indigenous peoples, scientists and the private sector gathered in Pyeongchang, Republic of Korea in October 2014 for the 12th meeting of the Conference of the Parties to the Convention on Biological Diversity (COP 12).\n\nFrom 6–17 October 2014, Parties discussed the implementation of the Strategic Plan for Biodiversity 2011-2020 and its Aichi Biodiversity Targets, which are to be achieved by the end of this decade. The results of Global Biodiversity Outlook 4, the flagship assessment report of the CBD informed the discussions.\n\nThe conference gave a mid-term evaluation to the UN Decade on Biodiversity (2011-2020) initiative, which aims to promote the conservation and sustainable use of nature.\nAt the end of the meeting, the meeting adopted the \"Pyeongchang Road Map,\" which addresses ways to achieve biodiversity through technology cooperation, funding and strengthening the capacity of developing countries.\n\nThe thirteenth ordinary meeting of the parties to the convention took place between 2 and 17 December 2016 in Cancun, Mexico.\n\n2010 was the International Year of Biodiversity and the Secretariat of the CBD was its focal point. Following a recommendation of CBD signatories during COP 10 at Nagoya in October 2010, the UN, on 22 December 2010, declared 2011 to 2020 as the United Nations Decade on Biodiversity.\n\nThere have been criticisms against CBD that the Convention has been weakened in implementation due to the resistance of Western countries to the implementation of the pro-South provisions of the Convention. CBD is also regarded as a case of a hard treaty gone soft in the implementation trajectory The argument to enforce the treaty as a legally binding multilateral instrument with the Conference of Parties reviewing the infractions and non-compliance is also gaining strength\n\nAlthough the convention explicitly states that all forms of life are covered by its provisions, examination of reports and of national biodiversity strategies and action plans submitted by participating countries shows that in practice this is not happening. The fifth report of the European Union, for example, makes frequent reference to animals (particularly fish) and plants, but does not mention bacteria, fungi or protists at all. The International Society for Fungal Conservation has assessed more than 100 of these CBD documents for their coverage of fungi using defined criteria to place each in one of six categories. No documents were assessed as good or adequate, less than 10% as nearly adequate or poor, and the rest as deficient, seriously deficient or totally deficient.\nFurthermore, intensive monoculture and human overpopulation are the two most pertinent biodiversity issues to address.\n\nScientists working with biodiversity and medical research are expressing fears that the Nagoya Protocol is counterproductive, and will will hamper disease prevention and conservation efforts, and that the threat of imprisonment of scientists will have a chilling effect on research. Non-commercial researchers and institutions such as natural history museums fear maintaining biological reference collections and exchanging material between institutions will become difficult, and medical researchers have expressed alarm at plans to expand the protocol to make it illegal to publicly share genetic information, e.g. via GenBank.\n\n\n\"This article is partly based on the relevant entry in the CIA World Factbook, edition.\"\n\nThere are indeed several comprehensive publications on the subject, the given reference covers only one small aspect\n\n"}
{"id": "16704670", "url": "https://en.wikipedia.org/wiki?curid=16704670", "title": "Courant minimax principle", "text": "Courant minimax principle\n\nIn mathematics, the Courant minimax principle gives the eigenvalues of a real symmetric matrix. It is named after Richard Courant.\n\nThe Courant minimax principle gives a condition for finding the eigenvalues for a real symmetric matrix. The Courant minimax principle is as follows:\n\nFor any real symmetric matrix \"A\",\n\nwhere \"C\" is any (\"k\" − 1) × \"n\" matrix.\n\nNotice that the vector \"x\" is an eigenvector to the corresponding eigenvalue \"λ\".\n\nThe Courant minimax principle is a result of the maximum theorem, which says that for \"q\"(\"x\") = <\"Ax\",\"x\">, \"A\" being a real symmetric matrix, the largest eigenvalue is given by \"λ\" = max\"q\"(\"x\") = \"q\"(\"x\"), where \"x\" is the corresponding eigenvector. Also (in the maximum theorem) subsequent eigenvalues \"λ\" and eigenvectors \"x\" are found by induction and orthogonal to each other; therefore, \"λ\" = max \"q\"(\"x\") with <\"x\",\"x\"> = 0, \"j\" < \"k\".\n\nThe Courant minimax principle, as well as the maximum principle, can be visualized by imagining that if ||\"x\"|| = 1 is a hypersphere then the matrix \"A\" deforms that hypersphere into an ellipsoid. When the major axis on the intersecting hyperplane are maximized — i.e., the length of the quadratic form \"q\"(\"x\") is maximized — this is the eigenvector, and its length is the eigenvalue. All other eigenvectors will be perpendicular to this.\n\nThe minimax principle also generalizes to eigenvalues of positive self-adjoint operators on Hilbert spaces, where it is commonly used to study the Sturm–Liouville problem.\n\n\n"}
{"id": "144793", "url": "https://en.wikipedia.org/wiki?curid=144793", "title": "Denying the antecedent", "text": "Denying the antecedent\n\nDenying the antecedent, sometimes also called inverse error or fallacy of the inverse, is a formal fallacy of inferring the inverse from the original statement. It is committed by reasoning in the form:\n\nwhich may also be phrased as\n\nArguments of this form are invalid. Informally, this means that arguments of this form do not give good reason to establish their conclusions, even if their premises are true.\n\nThe name \"denying the antecedent\" derives from the premise \"not \"P\"\", which denies the \"if\" clause of the conditional premise.\n\nOne way to demonstrate the invalidity of this argument form is with a counterexample that has true premises but an obviously false conclusion. For example:\n\nThat argument is intentionally bad, but arguments of the same form can sometimes seem superficially convincing, as in the following example offered by Alan Turing in the article \"Computing Machinery and Intelligence\":\n\nHowever, men could still be machines that do not follow a definite set of rules. Thus, this argument (as Turing intends) is invalid.\n\nIt is possible that an argument that denies the antecedent could be valid, if the argument instantiates some other valid form. For example, if the claims \"P\" and \"Q\" express the same proposition, then the argument would be trivially valid, as it would beg the question. In everyday discourse, however, such cases are rare, typically only occurring when the \"if-then\" premise is actually an \"if and only if\" claim (i.e., a biconditional/equality). For example:\n\nThe above argument is not valid, but would be if the first premise ended thus: \"...and if I can veto Congress, then I am the U.S. President\" (as is in fact true). More to the point, the validity of the new argument stems not from denying the antecedent, but \"modus tollens\" (denying the consequent).\n\n\n"}
{"id": "487627", "url": "https://en.wikipedia.org/wiki?curid=487627", "title": "Domain (ring theory)", "text": "Domain (ring theory)\n\nIn mathematics, and more specifically in algebra, a domain is a nonzero ring in which implies or . (Sometimes such a ring is said to \"have the zero-product property\".) Equivalently, a domain is a ring in which 0 is the only left zero divisor (or equivalently, the only right zero divisor). A commutative domain is called an integral domain. Mathematical literature contains multiple variants of the definition of \"domain\".\n\n\nOne way of proving that a ring is a domain is by exhibiting a filtration with special properties.\n\nTheorem: If \"R\" is a filtered ring whose associated graded ring gr(\"R\") is a domain, then \"R\" itself is a domain.\n\nThis theorem needs to be complemented by the analysis of the graded ring gr(\"R\").\n\nSuppose that \"G\" is a group and \"K\" is a field. Is the group ring a domain? The identity\n\nshows that an element \"g\" of finite order induces a zero divisor in \"R\". The zero divisor problem asks whether this is the only obstruction; in other words,\n\nNo counterexamples are known, but the problem remains open in general (as of 2017).\n\nFor many special classes of groups, the answer is affirmative. Farkas and Snider proved in 1976 that if \"G\" is a torsion-free polycyclic-by-finite group and then the group ring \"K\"[\"G\"] is a domain. Later (1980) Cliff removed the restriction on the characteristic of the field. In 1988, Kropholler, Linnell and Moody generalized these results to the case of torsion-free solvable and solvable-by-finite groups. Earlier (1965) work of Michel Lazard, whose importance was not appreciated by the specialists in the field for about 20 years, had dealt with the case where \"K\" is the ring of p-adic integers and \"G\" is the \"p\"th congruence subgroup of .\n\nZero divisors have a topological interpretation, at least in the case of commutative rings: a ring \"R\" is an integral domain if and only if it is reduced and its spectrum Spec \"R\" is an irreducible topological space. The first property is often considered to encode some infinitesimal information, whereas the second one is more geometric.\n\nAn example: the ring , where \"k\" is a field, is not a domain, since the images of \"x\" and \"y\" in this ring are zero divisors. Geometrically, this corresponds to the fact that the spectrum of this ring, which is the union of the lines and , is not irreducible. Indeed, these two lines are its irreducible components.\n\n\n"}
{"id": "5186633", "url": "https://en.wikipedia.org/wiki?curid=5186633", "title": "Energy (psychological)", "text": "Energy (psychological)\n\nMental energy or psychic energy is a concept in some psychological theories or models of a postulated unconscious mental functioning on a level between biology and consciousness.\n\nThe idea harks back to Aristotle's conception of \"actus et potentia\". \"Energy\" here used in the literal meaning of \"activity\" or \"operation\". Henry More, in his 1642 \"Psychodia platonica; or a platonicall song of the soul\", defined an \"energy of the soul\" as including \"every phantasm of the soul\". Julian Sorell Huxley defines \"mental energy\" as \"the driving forces of the psyche, emotional as well as intellectual\" (\"On living in a revolution\" xv.192, 1944).\n\nIn 1874, the concept of \"psychodynamics\" was proposed with the publication of \"Lectures on Physiology\" by German physiologist Ernst Wilhelm von Brücke who, in coordination with physicist Hermann von Helmholtz, one of the formulators of the first law of thermodynamics (conservation of energy), supposed that all living organisms are energy-systems also governed by this principle. During this year, at the University of Vienna, Brücke served as supervisor for first-year medical student Sigmund Freud who adopted this new \"dynamic\" physiology. In his \"Lectures on Physiology\", Brücke set forth the then-radical view that the living organism is a dynamic system to which the laws of chemistry and physics apply.\n\nIn \"The Ego and the Id\", Freud argued that the id was the source of the personality's desires, and therefore of the psychic energy that powered the mind. Freud defined libido as the instinct energy or force. Freud later added the death drive (also contained in the id) as a second source of mental energy. The origins of Freud's basic model, based on the fundamentals of chemistry and physics, according to John Bowlby, stems from Brücke, Meynert, Breuer, Helmholtz, and Herbart.\n\nIn 1928, Carl Jung published a seminal essay entitled \"On Psychic Energy\". Later, the theory of psychodynamics and the concept of \"psychic energy\" was developed further by those such as Alfred Adler and Melanie Klein.\n\nJust as physical energy acts upon physical objects, psychological energy would act upon psychological entities, i.e. thoughts. Psychological energy and force are the basis of an attempt to formulate a scientific theory according to which psychological phenomena would be subject to precise laws akin to how physical objects are subject to Newton's laws. This concept of psychological energy is completely separate and distinct from (or even opposed to) the mystical eastern concept of spiritual energy.\n\nThe divides people into 16 categories based on whether certain activities leave them feeling energized or drained of energy.\n\nMental energy has been repeatedly compared to or connected with the physical quantity energy.\n\nStudies of the 1990s to 2000s (and earlier) have found that mental effort can be measured in terms of increased metabolism in the brain. The modern neuroscientific view is that brain metabolism, measured by functional magnetic resonance imaging or positron emission tomography, is a physical correlate of mental activity.\n\n\n\n"}
{"id": "36161211", "url": "https://en.wikipedia.org/wiki?curid=36161211", "title": "Exploitation of women in mass media", "text": "Exploitation of women in mass media\n\nThe exploitation of women in mass media is the use or portrayal of women in mass media (such as television, film and advertising) to increase the appeal of media or a product to the detriment of, or without regard to, the interests of the women portrayed, or women in general. This process includes the presentation of woman as sexual objects and the setting of standards of beauty that women are expected to reflect. Feminists and other advocates of women's rights have criticized such exploitation. The most often criticized aspect of the use of women in mass media is sexual objectification, but dismemberment can be a part of the objectification as well.\n\nRobert Jensen, Sut Jhally and other cultural critics accuse mass media of using sex in advertising that promotes the objectification of women to help sell their goods and services.\n\nIn \"Gender Advertisements\", Erving Goffman sought to uncover the covert ways that popular media constructs masculinity and femininity in a detailed analysis of more than 500 advertisements. The relationship between men and women, Goffman argued, was portrayed as a parent–child relationship, one characterized by male power and female subordination.\n\nMany contemporary studies of gender and sexualization in popular culture take as their starting point Goffman's analysis in \"Gender Advertisements.\" Among them, later research which expanded empirical framework by analyzing the aspects of women's sexualization and objectification in advertisements, M.-E Kang examined the advertisements in women's magazines between 1979 and 1991 and found out there are still showing the same stereotyped images of women: Nude or partially nude images of women increased nearly 30% from 1979 to 1991. Lindner further developed Kang's analytical framework in a study of women in advertisements and found out magazines rely on gender stereotypes, but in different ways, particularly in terms of sexualization. For example, in \"Vogue\", sexualized images of women are the primary way of portraying women in positions of inferiority and low social power.\n\nResearch conducted by Eric Hatton and Mary Nell Trautner included a longitudinal content analysis of images of women and men on more than four decades of \"Rolling Stone\" magazine covers (1967–2009). It found that the frequency of sexualized images of men and women has increased, though the intensity of sexualization between men and women is different in that women are increasingly likely to be hypersexualized, but men are not. Researchers argue that the simple presence of images of sexualized men does not signal equality in media representations of women and men. Sexualized images may legitimize or exacerbate violence against women and girls, sexual harassment, and anti-women attitudes among men. They concluded that similarly sexualized images can suggest victimization for women but confidence for men, consider the implications when women are sexualized at the same rate as men are not sexualized, as they were on the covers of \"Rolling Stone\" in the 2000s.\n\nClothing designer Calvin Klein was criticized for using images of young, sexualized girls and women in his advertisments, having said:\n\nCalvin Klein has also received media attention for its controversial advertisements in the mid-1990s. Several of Calvin Klein's advertisements featured images of teenage models, some \"who were reportedly as young as 15\" in overly sexual and provocative poses.\n\nIn a recent analysis, it was found that almost 30% of the clothing items available for pre-teen girls on the websites of 15 national stores had sexualizing characteristics. The clothing emphasized or revealed a sexualized body part (e.g., bikinis and push-up bras), or had characteristics associated with sexiness (e.g., red satin lingerie-like dresses). This exploitation of women is being seen in younger girls.\n\nThe overt use of sexuality to promote breast cancer awareness, through fundraising campaigns like \"I Love Boobies\" and \"Save the Ta-tas\", angers and offends breast cancer survivors and older women, who are at higher risk of developing breast cancer. Women who have breast cancer say that these advertising campaigns suggest that having sexy breasts is more important than saving their lives, which devalues them as human beings.\n\nAnother trend that has been studied in advertising is the victimization of women. A study conducted in 2008 found that women were represented as victims in 9.51% of the advertisements they were present in. Separate examination by subcategory found that the highest frequency of this is in women's fashion magazines where 16.57% of the ads featuring women present them as victims.\n\nIn considering the way that films are put together, many feminist film critics have pointed to the \"male gaze\" that predominates in classical Hollywood film-making. Budd Boetticher summarises the view thus: \"What counts is what the heroine provokes, or rather what she represents. She is the one, or rather the love or fear she inspires in the hero, or else the concern he feels for her, who makes him act the way he does. In herself the woman has not the slightest importance.\" Laura Mulvey's germinal essay \"Visual Pleasure and Narrative Cinema\" (written in 1973 and published in 1975) expands on this conception of the passive role of women in cinema to argue that film provides visual pleasure through scopophilia and identification with the on-screen male actor. She asserts: \"In their traditional exhibitionist role women are simultaneously looked at and displayed, with their appearance coded for strong visual and erotic impact so that they can be said to connote \"to-be-looked-at-ness\",\" and as a result contends that in film a woman is the \"bearer of meaning, not maker of meaning\". Mulvey argues that Lacan's psychoanalytic theory is the key to understanding how film creates such a space for female sexual objectification and exploitation through the combination of the patriarchal order of society, and 'looking' in itself as a pleasurable act of voyeurism, as \"the cinema satisfies a primordial wish for pleasurable looking\".\n\nAdditionally, the sexual objectification of women in film has a detrimental effect on girls and young women. Research shows that when girls had extended exposure to films in which female super heroes were dressed in over-sexualized costumes, they became more aware of their own body competence. Additionally, the exposure impacted their view of the female gender and female roles. Such over-sexualization in popular Hollywood films takes away from girl's self-esteem and encourages them to want to alter their bodies to look more like the actresses in films and media.\n\nThe Geena Davis Institute on Gender in Media is an organisation that has been lobbying the industry for years to expand the roles of women in film. \n\nResearch into the social implications of the presentation of women in film and its effect on the African-American community indicates that young black girls are exposed to a stereotyped portrayal of black females which goes beyond sexual objectification. Young black girls are presented with only one type of depiction: an angry black woman who is obnoxious, ignorant, confrontational and loud. Not only do they struggle with internalizing these fixed notions of who they are, they are also faced with definitions of beauty for African American girls that are measured against white standards of what beauty should be. Film and social media reflect an idea of female beauty based on features closely resembling those of women of European origin, which is nearly impossible for a black girl to attain, or indeed any young girl. At the same time black characters are typically depicted in films in occupational roles such as athletes, servants, musicians and criminals, roles which hold a lower status than the roles of white characters.\n\nA survey conducted as a part of the Human Use of Music Information Retrieval Systems (HUMIRS) project found that 73.1% of respondents identified themselves as being \"avid listeners\" of music. Popular music often contains messages about women that involve misogyny, sexual violence and abuse. \n\nListeners are often absorbing messages exploiting women without it being obvious. There are multiple online articles that seek to identify songs that have misogynistic undertones woven throughout them. For example, an article in the online US women's magazine \"Bustle\" provided a clip of lyrics from the song \"Fine China\" by Chris Brown. He sings \"It's alright, I'm not dangerous / When you're mine, I'll be generous / You're irreplaceable; Collectible / Just like fine China.\" The article went on to conclude that the song was demeaning to women by referring to them as objects or possessions.\n\nMusic is a key factor in the socialization of children. Children and adolescents often turn to music lyrics as an outlet away from loneliness or as a source of advice and information. The results of a study through \"A Kaiser Family Foundation Study\" in 2005 showed that 85% of youth ages 8-18 listen to music each day. While music is commonly thought of as only a means of entertainment, studies have found that music is often chosen by youth because it mirrors their own feelings and the content of the lyrics is important to them. Numerous studies have been conducted to research how music influences listeners behaviors and beliefs. For example, a study featured in the \"Journal of Youth and Adolescence\" found that when compared to adolescent males who did not like heavy metal music, those who liked heavy metal had a higher occurrence of deviant behaviors. These behaviors included sexual misconduct, substance abuse and family issues.\n\nGan, Zillmann and Mitrook found that exposure to sexually explicit rap promotes unfavorable evaluations of black women. Following exposure to sexual rap, as compared with exposure to romantic music or to no music, the assessment of the female performers' personality resulted in a general downgrading of positive traits and a general upgrading of negative ones. A 2008 study by Zhang et al. showed that exposure to sexually explicit music videos was associated with stronger endorsement of sexual double standards (e.g., belief that it is less acceptable for women to be sexually experienced than for men). Exposure to sexual content was also associated with more permissive attitudes toward premarital sex, regardless of gender, overall television viewing, and previous sexual experience. However, Gad Saad argues that the premise that music videos yield harmful effects and that the harm would be sex-specific (e.g., women's self-concepts will be negatively affected) has not been supported by research. \n\nA survey found that 72.2% of black, 68.0% of white, and 69.2% of Hispanic youths agree with the suggestion that rap music videos contain \"too many\" references to sex. \n\nDespite the lack of adequate research linking music videos to negative self perception by young girls, research has shown adolescents have a higher susceptibility rate than other age brackets. More importantly, music videos are one of the many significant mediums that perpetuate sexual objectification of females, implicitly creating fixed gender norms. The perpetuation of females being nothing more than seductive \"creatures\" to men can presumably lead to young girls internalizing their self worth as nothing more than mere objects.\n\nIn her article, \"Negative effect of media on girls,\" Monique Smith discusses the evolution of acceptable female figures throughout time. The transition between sexy meaning curvaceous to sexy meaning thin made it difficult for women to keep up with the ideal feminine figure. Striving for the virtually unattainable perfect body, women were viewed as a new way to make money. The use of size 0 in advertisements and products of the clothing industry has been met with criticism. For example, Dawn Porter, a reporter from the UK who had been challenged to go on an extreme celebrity 'size zero' diet for a new BBC programme, Super Slim Me, logged her experiences about her journey to a size zero.\n\nA study conducted in the UK found evidence that anorexia nervosa is a socially transmitted disease and exposure to skinny models may be a contributing factor in the cause of anorexia nervosa.\n\nFor decades most rape cases in the modeling industry go unreported. Anand Jon Alexander was a successful designer also seen on Americas Next Top Model, was sentenced to 59 years in prison having been found guilty of 16 counts, including sexual battery and performing lewd acts on a child. According to sources, Anand had raped models who worked for him and some of them were even underage.According to model, Sarah Ziff, many rape cases in the modeling industry go unreported as the people with power look out for each other. Modeling agencies view their models as independent contractors and not employees so models cannot unionize to fight the injustices they face everyday. In the modeling world there is no set fee therefore these women can be subjected to underpayment. They are asked to exploit themselves and their bodies for little respect and bad working conditions often times in return. Models are denied food on shoots to keep them thin according to model Vanessa Perron. Fernanda Ly a famous pink haired model who has walked for successful designers such as Louis Vuitton and Dior has her own similar issues she has faced in the modeling industry. She was once felt up while shooting a look book by the stylist trying to dress her at a young age. This pain still haunts her today. Due to significant lack of regulation the inhumane treatment of models in society can make these abuses completely legal. An agency in Florida got young women who were desperate for modeling work and drugged them. The models in this case were also used to create pornographic films. According to Carolyn Cramer, unless you are a supermodel you are treated like royalty but 99% of the rest of the models in the industry are treated like garbage. The lack of regulation makes it easy for bad agencies to thrive and treat workers as nothing more than a source for profit. Modeling agencies have fought back to consider these claims incorrect and state that models work at odd hours for different clients. This means they can't actually be considered employees and that they can't properly unionize to fight these injustices.Models sign on to management companies and not the other way around and for this reason they are treated as if they are being owned. Guided by a partnership between the American Guild of Musical Artists and Actors’ Equity Association, the Model Alliance provides its members with protection, advice and support in their efforts to be treated as workers in an industry that profits from their exploitation.\n\nIn \"Effects of Prolonged Consumption of Pornography\", a review of pornography research conducted for the Surgeon General in 1986, Dolf Zillmann noted that some inconsistencies in the literature on pornography exist, but overall concluded that extensive viewing of pornographic material may produce some negative sociological effects, including a decreased respect for long-term, monogamous relationships, and an attenuated desire for procreation. He describes the theoretical basis for these conclusions stating:The values expressed in pornography clash so obviously with the family concept, and they potentially undermine the traditional values that favor marriage, family, and children... Pornographic scripts dwell on sexual engagements of parties who have just met, who are in no way attached or committed to each other, and who will part shortly, never to meet again... Sexual gratification in pornography is not a function of emotional attachment, of kindness, of caring, and especially not of continuance of the relationship, as such continuance would translate into responsibilities, curtailments, and costs...Another study conducted by Svedin, Åkermana, and Priebe concluded that male partners' use of pornography might be integrated within the objectification theory framework for women, considering that pornography is a socialization agent for sexual attitudes and behavior. It often portrays men objectifying women via gazing at women's breasts and/or labia, non-permitted aggressive and sexualized touching of women's body parts, making sexual and derogatory remarks about women's body parts, and engaging in forceful oral and anal sex despite women gagging and crying. As pornography portrays women succumbing to this objectification, male viewers may internalize a view that these behaviors are acceptable. According to the tenets of social learning theory, men who view pornography may learn and transfer the objectifying behaviors they view in pornography to sexual encounters with their female partners. Men's pornography use may correspond to higher levels of experienced sexual objectification by their female partners. Pornography usage may also enable men to treat their female partners in objectifying ways and believe that it is acceptable to do so.\n\nPartner's use of pornography can also be negatively linked to women's well-being. Qualitative studies of women whose male partners heavily use pornography have revealed that these women reported lower relational and psychological well-being. The women perceived that their partner's pornography use was connected to their inability to be intimately and authentically open and vulnerable within their relationships. Women from this qualitative research also reported a personal struggle regarding the implications of their male partners pornography use for their own self-worth and value. These women were feeling less attractive and desirable after becoming aware of their male partner's pornography use. Similarly, women view their partners in a new way. The general conclusion that women feel is that their partner is not who they originally thought he/she was. The mate is seen as a sexually questionable and degraded being since the partner seeks sexual fulfilment through the objectification and sometimes degradation of women.\n\nSocial media has a prominent effect on people’s lives, especially those who use social media platforms more frequently than others. A study conducted in 2006 found inverse relationships between the frequency of social media usage and the relationships adolescents formed with the impact it had on their sense of self. When social media usage increased, adolescents began to form stronger relationships online while their sense of self was impacted negatively. According to a study conducted by Xinyan Zhao, Mengqi Zhan, and Brooke F. Liu, social media content that weaves emotional components in a positive manner appears to have the benefit of also increasing one’s online influence. Positive social media content results in increased presence on networking sites among adolescent users.\n\nDigital social media platforms such as Twitter, Instagram, and Snapchat allow individuals to establish their influence through sharing opinions, insights, experiences and perspectives with others. In the 2000s, these platforms have emerged as integral communities for publics to voice their opinions, resulting in a changed online behavior associated largely with misinformation. One example of these behaviors is displayed in a 2017 Dutch study conducted by Johanna M. F. van Oosten. This study found that adolescents play out stereotypical gender roles in their self-presentations in social media. Results of this study show that it is predominantly women that feel pressured to conform to hyper femininity and stereotypical gender roles online, including personality traits, domestic behaviors, occupations, and physical appearances.\n\nThe prevalence of social media and its influence on self-perception among adolescents, especially young girls, is undeniable. Research has shown a significant scientific link between social media and depression among young girls. In addition, this link between depression and social media perceptions has been connected to obesity among young girls. The negative implications social media poses on women associated with their appearance or how they carry themselves reveals a chain reaction; the depression related to negative social media experiences can manifest itself in the form of poor academic performance and further mental and physical health issues. \n\nSuch evidence of substantial mental and physical harm suggests that the root of the problem can be found not only within social media advertising and usage, but in the way young girls are taught to internalized responses on various social media platforms. \n\nTelevision is often subject to criticism for the sexual exploitation of women on screen, particularly when teenagers are involved. In 2013, the Parents Television Council released a report that found that it was increasingly more likely for a scene to be exploitative when a teenage girl was involved. The report also found that 43 percent of teen girls on television are the targets of sexually exploitative jokes compared to 33 percent of adult women. Rev. Delman Coates, a PTC board member said, \"young people are having difficulty managing the distinction between appropriate and inappropriate sexual conduct\". This report is of a series that's about media sexualization of young girls.\n\nThe researchers from the study claim that \"[i]f media images communicate that sexual exploitation is neither serious nor harmful, the environment is being set for sexual exploitation to be viewed as trivial and acceptable. As long as there are media producers who continue to find the degradation of women to be humorous, and media outlets that will air the content, the impact and seriousness of sexual exploitation will continue to be understated and not meaningfully addressed in our society.\"\n\nA 2012 study led by sociologist Stacy L. Smith found that in both prime-time television and family films, women were highly likely to be depicted as thin and scantily clad. They were also vastly underrepresented in STEM fields when compared to their male counterparts, and had less speaking roles. According to this study, only 28.3 percent of characters in family films, 30.8 percent of characters in children's shows, and 38.9 percent of characters on prime time television were women.\n\nAccording to a report by the Women's Media Center (WMC), it found that the gender gap has not declined and that in some industries it has gotten worse. In television, it found the percentage of female TV characters has decreased and that the ones who make it on-screen are not likely to get the lead roles compared to the male characters. \"According to the Center for the Study of Women in Television & Film's 'Boxed In' report, CW Television Network is the only TV network where women can be seen in accurate proportion to their representation in the U.S. population\".\n\nAccording to a report done by the Entertainment Software Association in 2013, 55% of game players are male and 45% are female. Women's roles in many modern games usually are less important to the game and rely heavily on stereotypes. Video games' female characters also tend be lighter skinned individuals, as are their male counterparts. Furthermore many of the female characters found in video games intentionally depict woman to be sultry and enhance the body form of females in an effort to appeal to men's desires Although not demonstrating blatantly racist stereotypes, many games practice racism through omission of racially diverse characters.\n\nVideo games have been found to offer a smaller range of roles to female characters compared to male characters, and these roles tend to involve being victims or prizes to be won. The majority of female characters are also not playable. These roles for women have been found to have a negative impact on the perception of women in gaming and even main playable female characters are found to be unrealistically proportioned with revealing clothing. If a sexualized female character is the main protagonist and portrayed in a positive light, studies have shown a potential negative effect if the character is hyper-sexualized in a stereotypical manner. A recent Ohio State University Study has found that sexist and violent content in games cause male gamers to identify with the male lead, and find less empathy with female victims of violence, although a 2017 review of this paper suggested several flaws and a reanalysis of the dataset using different statistical methods found no sexist effect, concluding \"These results call into question whether use of “sexist” video games is a causal factor in the development of reduced empathy toward girls and women among adolescents\". Similarly, the results of a 2015 study suggested that \"sexist video game play is related to men perceiving women in a stereotypic and sexist way\", but found that the same correlation did not occur with female players.\n\nA German longitudinal study from 2011 to 2015 explored the connection between gaming and sexist attitudes. The results of this study concluded both that playing video games was not predictive of sexist beliefs and that sexist beliefs were not predicative of video game play. The researchers stressed, however, that the study did not, nor was intended to, disprove the existence of sexist attitudes in general. A 2012 study also raised concerns about the correlation between video games and individual attitudes. Focusing on the Singaporean subjects playing the game Grand Theft Auto, the study found some evidence of \"first order cultivation effects\" – which relate to the perceptions of situations and issues – but found that second order effects, relating to beliefs and issues, were provided with only limited support by the study. This led the authors to conclude that previous studies on cultivation effects from television may not directly relate to effects from video game playing.\n\nThe trend of portraying sex-typed images of women and violence against women in popular video games continues to proliferate and promulgate in video games. Video games depicting sexual objectification of women and violence against women resulted in statistically significant increased rape myths acceptance for male study participants but not for female participants. A 2016 study by Fox and Potocki had similar findings, in which they ran a survey which found that \"video game consumption throughout the life span is associated with interpersonal aggression, hostile sexism, and RMA [Rape Myth Acceptance]\".\n\nOut of the top 10 video games listed midyear 2010 (New Super Mario Brothers; Call Of Duty: Modern Warfare; Battlefield: Bad Company 2; Final Fantasy XIII; Wii Fit Plus; God of War III; Pokémon SoulSilver; Wii Sports Resort, Mass Effect 2, Pokémon HeartGold Version; Morris, 2010), most have violent content, including violence against women, and some contain sexual objectification of women. Not only are gamers increasingly being exposed to video games containing sexual objectification of and violence against women, but research also indicates that such exposure can be excessive. A national sample of youth aged 8 to 18 found that \"8.5 percent of video game players exhibited pathological patterns of play,\" which is \"very similar to the prevalence demonstrated in many other studies of this age group, including across nations\".\n\nCritics of the prevalent portrayals of women in the mass media observe possible negative consequences for various segments of the population, such as:\nAccording to Muehlenkamp and Saris–Baglama, self-objectification of women can lead to depression, noting that \"the relationship between self-objectification and depression can be explained by the anxiety and powerlessness women may experience as a result of not knowing when or where they will encounter objectification. These feelings may increase women's vulnerability to depressive symptoms. Once a woman starts to self-objectify and compare her body to others, it may be a risk factor for holistic human functioning, and may also lead to impairment in multiple life tasks, such as forming meaningful interpersonal relationships and achieving academic success.\"\n\nIn addition, it can lead to sexual dysfunction. Engaging in sexual activity involves another person focusing attention on one's body and during sexual relations a woman can be distracted by thoughts about her body rather than experiencing sexual pleasure.\n\nMany studies have shown the negative effects that this exploitation of women in the media has on the mental health of young women, but recently the studies have focused on aging women in western societies. It has been observed that the exploitation of young attractive women in the media causes aging women to feel a variety of emotions including sadness, anger, concern, envy, desensitization, marginalization, and discomfort that their appearance was being judged by others.\n\nA study done in 1994 about the effects of media on young and middle-aged women found that of adolescent girls aged 11–17, the primary desire was to \"lose weight and keep it off.\" The results were not different for older women. When asked what they'd most like to change about their lives, the answer for over half of them was their body and weight.\n\nA recent study done by Vanderbilt University illustrated how sexist commercials have a greater impact on wellbeing than commercials that do not exploit women. The study was designed with three different groups: one was exposed to sexist media, one was exposed to neutral media, and the control group was not exposed to media at all. Of the women exposed to sexist advertising, there was a substantial difference. The women in this group expressed having a body larger than it was in actuality and expressed feeling a greater disparity between their own body and the \"ideal body.\" Following exposure to this kind of media, there was an immediate negative effect on their mood. It was also concluded that adolescent girls exposed to sexist media are the most highly impacted demographic.\n\nStatistically, a significant number of young children are exposed to sexualized media forms from early within their childhood: influence upon girls' self-image has been reported within girls as young as 5 or 6. According to the social cognitive theory, modeling such behaviors outlined within popular media have long-lasting effects upon the self-awareness and self-identity of young girls. \n\nA study conducted by the Department of Psychology at Knox College provided insight into risk factors such as media consumption hours, maternal self-objectification, maternal religiosity, and television mediation; each has been shown to affect rates of media influence and rates of self-internalization of their potential negative influence.\n\nSupport has shown that the effects of media exploitation vary for women of different ethnicities. Research has depicted that these implications often resonate beyond cultural boundaries, to cause significant differences among African American, Latina, and Asian American women. \n\nAccording to the American Psychological Association, when comparing one's body to the sexualized cultural ideals, this significantly impaired the ability for women of these ethnicities to regulate cognitive functions, including logical reasoning and spatial skills.\n\nSpanish-language TV in the United States statistically projects more stereotypical roles for Latina women, often portraying them as 'exoticized' and 'overly sexual'; meanwhile, more Latina youth, on average, watch more television than that of the standard caucasian American child. This combination projects increased rates of the acceptance of the negative effects within minority women within the US, leading to a greater acceptance of standard gender roles and negative stereotypes projected by Latina characters. However, studies have shown that Latina women who watch more black-oriented television shows see a general increase of body acceptance over time. \n\nGallup & Robinson, an advertising and marketing research firm, has reported that in more than 50 years of testing advertising effectiveness, it has found the use of the erotic to be a significantly above-average technique in communicating with the marketplace, \"...although one of the more dangerous for the advertiser. Weighted down with taboos and volatile attitudes, sex is a Code Red advertising technique ... handle with care ... seller beware; all of which makes it even more intriguing.\" This research has led to the popular idea that \"sex sells\".\n\nCamille Paglia holds that \"Turning people into sex objects is one of the specialties of our species.\" In her view, objectification is closely tied to (and may even be identical with) the highest human faculties toward conceptualization and aesthetics.\n\nDanish criminologist Berl Kutchinsky's \"Studies on Pornography and sex crimes in Denmark\" (1970), a scientific report ordered by the Presidential Commission on Obscenity and Pornography, found that the legalizing of pornography in Denmark had not (as expected) resulted in an increase of sex crimes. Since then, many other experiments have been conducted, either supporting or opposing the findings of Berl Kutchinsky, who would continue his study into the social effects of pornography until his death in 1995. His life's work was summed up in the publication \"Law, Pornography, and Crime: The Danish Experience\" (1999). Milton Diamond from the University of Hawaii found that the number of reported cases of child sex abuse dropped markedly immediately after the ban on sexually explicit materials was lifted in 1989.\n\nSome researchers, such as Susan Bordo and Rosalind Gill, argue against using the phrase \"sexual objectification\" to describe such images because they often depict women as active, confident, and/or sexually desirous. For this argument, there have been several refutations that intensity of women's sexualization suggests that \"sexual object\" may indeed be the only appropriate label. The accumulation of sexualized attributes in these images leaves little room for observers to interpret them in any way other than as instruments of sexual pleasure and visual possession for a heterosexual male audience. Yet, some scholars have criticized such statements as overly homogenizing because they render invisible differences in this process of sexualization.\n\nSome social conservatives have agreed with aspects of the feminist critique of sexual objectification. In their view however, the increase in the sexual objectification of both sexes in Western culture is one of the negative legacies of the sexual revolution. These critics, notably Wendy Shalit, advocate a return to pre-sexual revolution standards of sexual morality, which Shalit refers to as a \"return to modesty\", as an antidote to sexual objectification.\n"}
{"id": "38846583", "url": "https://en.wikipedia.org/wiki?curid=38846583", "title": "Gender disparities in health", "text": "Gender disparities in health\n\nWHO has defined health as \"a state of complete physical, mental, and social well-being and not merely the absence of disease or infirmity.\" Identified by the \"2012 World Development Report\" as one of two key human capital endowments, health can influence an individual's ability to reach his or her full potential in society. Yet while gender equality has made the most progress in areas such as education and labor force participation, health inequality between men and women continues to plague many societies today. While both males and females face health disparities, girls and women experience a majority of health disparities. This comes from the fact that many cultural ideologies and practices have structured society in a way whereby women are more vulnerable to abuse and mistreatment, making them more prone to illnesses and early death. Women are also restricted from receiving many opportunities, such as education and paid labor, that can help improve their accessibility to better health care resources.\n\nHealth disparity has been defined by the World Health Organization as the differences in health care received by different groups of people that are not only unnecessary and avoidable but unfair and unjust. The existence of health disparity implies that health equity does not exist in many parts of the world. Equity in health refers to the situation whereby every individual has a fair opportunity to attain their full health potential. Overall, the term \"health disparities\", or \"health inequalities\", is widely understood as the differences in health between people who are situated in different positions in a socioeconomic hierarchy.\n\nThe social structures of many countries perpetuate the marginalization and oppression of women in the form of cultural norms and legal codes. As a result of this unequal social order, women are usually relegated into positions where they have less access and control over healthcare resources, making women more vulnerable to suffering from health problems than men. For example, women living in areas with a patriarchal system are often less likely to receive tertiary education or to be employed in the paid labor market due to gender discrimination. As a result, female life expectancy at birth and nutritional well-being, and immunity against communicable and non-communicable diseases, are often lower than those of men.\n\nWhile a majority of the global health gender disparities is weighted against women, there are situations in which men tend to fare poorer. One such instance is armed conflicts, where men are often the immediate victims. A study of conflicts in 13 countries from 1955 to 2002 found that 81% of all violent war deaths were male. Apart from armed conflicts, areas with high incidence of violence, such as regions controlled by drug cartels, also see men experiencing higher mortality rates. This stems from social beliefs that associate ideals of masculinity with aggressive, confrontational behavior. Lastly, sudden and drastic changes in economic environments and the loss of social safety nets, in particular social subsidies and food stamps, have also been linked to higher levels of alcohol consumption and psychological stress among men, leading to a spike in male mortality rates. This is because such situations often makes it harder for men to provide for their family, a task that has been long regarded as the \"essence of masculinity.\" A retrospective analyses of people infected with the common cold found that doctors underrate the symptoms of men, and are more willing to attribute symptoms and illness to women than men. Women live longer than men in all countries, and across all age groups, for which reliable records exist. \n\nAt birth, boys outnumber girls with the ratio of 105 or 106 male to 100 female children. However, after conception, biology favors women. Research has shown that if men and women received similar nutrition, medical attention, and general health care, women would live longer than men. This is because women, on a whole, are more resistant to diseases and less prone to debilitating genetic conditions. However, despite medical and scientific research that shows that when given the same care as males, females tend to have better survival rates than males, the ratio of women to men in developing regions such as South Asia, West Asia, and China can be as low as 0.94, or even lower. This deviation from the natural male to female sex ratio has been described by Indian philosopher and economist Amartya Sen as the \"missing women\" phenomenon. According to the 2012 World Development Report, the number of missing women is estimated to be about 1.5 million women per year, with a majority of the women missing in India and China.\n\nIn many developing regions, women experience high levels of mortality. Many of these deaths result from maternal mortality and HIV/AIDS infection. Although only 1,900 maternal deaths were recorded in high-income nations in 2008, India and Sub-Saharan Africa experienced a combined total of 266,000 deaths from pregnancy-related causes. In Somalia and Chad, one in every 14 women die from causes related to child birth. In addition, the HIV/AIDS epidemic also contributes significantly to female mortality. The case is especially true for Sub-Saharan Africa, where women account for 60% of all adult HIV infections.\n\nWomen tend to have poorer health outcomes than men for several reasons ranging from sustaining greater risk to diseases to experiencing higher mortality rates. In the Population Studies Center Research Report by Rachel Snow that compares the disability-adjusted life years (DALY) of both male and females, the global DALYs lost to females for sexually transmitted diseases such as gonorrhea and chlamydia are more than ten times greater than those of the males. Moreover, the female DALYs to male DALYs ratio for malnutrition-related diseases such as Iron-Deficiency Anemia are often close to 1.5, suggesting that poor nutrition impacts women at a much higher level than men. Additionally, in terms of mental illnesses, women are also two to three times more likely than men to be diagnosed with depression. With regards to suicidal rates, up to 80% of those who committed suicide or attempted suicide in Iran are women.\n\nIn developed countries with more social and legal gender equality, overall health outcomes can disfavor men. For example, in the United States, as of 2001, men's life expectancy is 5 years lower than women's (down from 1 year in 1920), and men die at higher rates from all top 10 causes of death, especially heart disease and stroke. Men die from suicide more frequently, though women more frequently have suicidal thoughts and the suicide attempt rate is the same for men and women (see Gender differences in suicide). Men may suffer from undiagnosed depression more frequently, due to gender differences in the expression of emotion. American men are more likely to consume alcohol, smoke, engage in risky behaviors, and defer medical care.\n\nIncidence of melanoma has strong gender-related differences which vary by age.\n\nWomen outlive men in 176 countries. Data from 38 countries shows women having higher life expediencies than men for all years both at birth and at age 50. Men are more likely to die from 13 of the 15 major causes of death in the U.S. However, women are more likely to suffer from disease than men and miss work due to illness throughout life. This is called the mortality-morbidity paradox This is explained by an excess of psychological, rather than physical, distress among women, as well as higher smoking rates among men. Androgens also contribute to the male deficit in longevity. \n\nWomen tend to have poorer access to health care resources than men. In certain regions of Africa, many women often lack access to malaria treatment as well as access to resources that could protect them against Anopheles mosquitoes during pregnancy. As a result of this, pregnant women who are residing in areas with low levels of malaria transmission are still placed at two to three times higher risk than men in terms of contracting a severe malaria infection. These disparities in access to healthcare are often compounded by cultural norms and expectations imposed on women. For example, certain societies forbid women from leaving their homes unaccompanied by a male relative, making it harder for women to receive healthcare services and resources when they need it most.\n\nGender factors, such as women's status and empowerment (i.e., in education, employment, intimate partner relationships, and reproductive health), are linked with women's capacity to access and use maternal health services, a critical component of maternal health. Still, family planning is typically viewed as the responsibility of women, with programs targeting women and overlooking the role of men—even though men's dominance in decision making, including contraceptive use, has significant implications for family planning and access to reproductive health services.\n\nIn order to promote equity in access to reproductive health care, health programs and services should conduct analyses to identify gender inequalities and barriers to health, and determine the programmatic implications. The analyses will help inform decisions about how to design, implement, and scale up health programs that meet the differential needs of women and men.\n\nCultural norms and practices are two of the main reasons why gender disparities in health exist and continue to persist. These cultural norms and practices often influence the roles and behaviors that men and women adopt in society. It is these gender differences between men and women, which are regarded and valued differently, that give rise to gender inequalities as they work to systematically empower one group and oppress the other. Both gender differences and gender inequalities can lead to disparities in health outcomes and access to health care. Some of the examples provided by the World Health Organization of how cultural norms can result in gender disparities in health include a woman's inability to travel alone, which can prevent them from receiving the necessary health care that they need. Another societal standard is a woman's inability to insist on condom use by her spouse or sex partners, leading to a higher risk of contracting HIV. \n\nOne of the better documented cultural norms that augment gender disparities in health is the preference for sons. In India, for instance, the 2001 census recorded only 93 girls per 100 boys. This is a sharp decline from 1961, when the number of girls per 100 boys was nearly 98. In certain parts of India, such as Kangra and Rohtak the number of girls for every 100 boys can be as low as in the 70s. Additionally, low female to male numbers have also been recorded in other Asian countries – most notably in China where, according to a survey in 2005, only 84 girls were born for every 100 boys. Although this was a slight increase from 81 during 2001–2004, it is still much lower than the 93 girls per 100 boys in the late 1980s. The increasing number of unborn girls in the late 20th century has been attributed to technological advances that made pre-birth sex determination, also known as prenatal sex discernment, such as the ultrasound test more affordable and accessible to a wider population. This allowed parents who prefer a son to determine the sex of their unborn child during the early stages of pregnancy. By having early identification of their unborn child's sex, parents could practice sex-selective abortion, where they would abort the fetus if it was not the preferred sex, which in most cases is that of the female.\n\nAdditionally, the culture of son preference also extends beyond birth in the form of preferential treatment of boys. Economic benefits of having a son in countries like India also explain the preferential treatment of boys over girls. For example, in Indian culture it is the sons that provide care and economic stability to their parents as they age, so having a boy helps to ensure the futures of many Indian families. This preferential care can be manifested in many ways, such as through differential provision of food resources, attention, and medical care. Data from household surveys over the past 20 years has indicated that the female disadvantage has persisted in India and may have even worsened in some other countries such as Nepal and Pakistan.\n\nHarmful cultural practices such as female genital mutilation (FGM) also cause girls and women to face health risks. Millions of females are estimated to have undergone FGM, which involves partial or total removal of the external female genitalia for non-medical reasons. It is estimated that 92.5 million females over 10 years of age in Africa are living with the consequences of FGM. Of these, 12.5 million are girls between 10 and 14 years of age. Each year, about three million girls in Africa are subjected to FGM.\n\nOften performed by traditional practitioners using unsterile techniques and devices, FGM can have both immediate and late complications.<ref name=\"Care of women with female genital mutilation/cutting\"></ref> These include excessive bleeding, urinary tract infections, wound infection, and in the case of unsterile and reused instruments, hepatitis and HIV. In the long run, scars and keloids can form, which can obstruct and damage the urinary and genital tracts. According to a 2005 UNICEF report on FGM, it is unknown how many girls and women die from the procedure because of poor record keeping and a failure to report fatalities.<ref name=\"Changing a Harmful Social Convention: Female Genital Mutilation/Cutting\"></ref> FGM may also complicate pregnancy and place women at a higher risk for obstetrical problems, such as prolonged labor. According to a 2006 study by the WHO involving 28,393 women, neonatal mortality increases when women have experienced FGM; an additional ten to twenty babies were estimated to die per 1,000 deliveries.\n\nPsychological complications are related to cultural context. Women who undergo FGM might be emotionally affected when they move outside their traditional circles and are confronted with the view that mutilation is not the norm.\n\nViolence against women is a widespread global occurrence with serious public health implications. This is a result of social and gender bias.<ref name=\"World Health Organization: Violence and injuries to/against women\"></ref> Many societies in developing nations function on a patriarchal framework, where women are often viewed as a form of property and as socially inferior to men. This unequal standing in the social hierarchy has led women to be physically, emotionally, and sexually abused by men, both as children and adults. These abuses usually constitute some form of violence. Although children of both sexes do suffer from physical maltreatment, sexual abuse, and other forms of exploitation and violence, studies have indicated that young girls are far more likely than boys to experience sexual abuse. In a 2004 study on child abuse, 25.3% of all girls surveyed experienced some form of sexual abuse, a percentage that is three times higher than that of boys (8.7%).\n\nSuch violence against women, especially sexual abuse, is increasingly being documented in areas experiencing armed conflicts. Presently, women and girls bear the brunt of social turmoil worldwide, making up an estimated 65% of the millions who are displaced and affected. Some of these places which are facing such problems include Rwanda, Kosovo, and the Democratic Republic of the Congo. This comes as a result of both the general instability around the region, as well as a tactic of warfare to intimidate enemies. Often being placed in emergency and refugee settings, girls and women alike are highly vulnerable to abuse and exploitation by military combatants, security forces, and members of rival communities.\n\nThe sexual violence and abuse of both young and adult women have both short and long-term consequences, contributing significantly to a myriad of health issues into adulthood. These range from debilitating physical injuries, reproductive health issues, substance abuse, and psychological trauma. Examples of the above categories include depression and post-traumatic stress disorder, alcohol and drug use and dependence, sexually transmitted diseases, and suicide attempts.\n\nAbused women often have higher rates of unplanned and problematic pregnancies, abortions, neonatal and infant health issues, sexually transmitted infections (including HIV), and mental disorders (such as depression, anxiety disorders, sleep disorders and eating disorders) as compared to their non-abused peers. During peacetime, most violence against women is perpetrated by either male individuals whom them know or intimate male partners. An eleven-country study conducted by WHO between 2000 and 2003 found that, depending on the country, between 15% and 71% of women have experienced physical or sexual violence by a husband or partner in their lifetime, and 4% to 54% within the previous year. Partner violence may also be fatal. Studies from Australia, Canada, Israel, South Africa and the United States show that between 40% and 70% of female murders were carried out by intimate partners.\n\nOther forms of violence against women include sexual harassment and abuse by authority figures (such as teachers, police officers or employers), trafficking for forced labour or sex, and traditional practices such as forced child marriages and dowry-related violence. At its most extreme, violence against women can result in female infanticide and violent death. Despite the size of the problem, many women do not report their experience of abuse and do not seek help. As a result, violence against women remains a hidden problem with great human and health care costs. Worldwide men account for 79% of all victims of homicide. Homicide statistics by gender\n\nPoverty is another factor that facilitates the continual existence of gender disparities in health. Poverty often works in tandem with various cultural norms to indirectly impact women's health. While many communities and households are not opposed to helping women attain better health through education, better nutrition, and financial stability, poverty often act as a form of barrier against gender equity in health for women. Oftentimes, due to financial constraints and limited capital, only a select few are able to receive opportunities, like education and employment, that might help them attain better health outcomes. However, cultural norms would often prioritized men in receiving these opportunities. This prioritization of males stems from the societal perception that the potential returns to both the household and the community is higher for men than women.\n\nThe World Health Organization defines health systems as \"all the activities whose primary purpose is to promote, restore, or maintain health\". However, factors outside of healthcare systems can influence the impact healthcare systems have on the health of different demographics within a population. This is because healthcare systems are known to be influenced by social, cultural and economic frameworks. As a result, health systems are regarded as not only \"producers of health and health care\", but also as \"purveyors of a wider set of societal norms and values,\" many of which are biased against women\n\nIn the Women and Gender Equity Knowledge Network's Final Report to the WHO Commission on Social Determinants of Health in 2007, health systems in many countries were noted to have been unable to deliver adequately on gender equity in health. One explanation for this issue is that many healthcare systems tend to neglect the fact that men and women's health needs can be very different. In the report, studies have found evidence that the healthcare system can promote gender disparities in health through the lack of gender equity in terms of the way women are regarded - as both consumers (users) and producers (carers) of health care services. For instance, healthcare systems tend to regard women as objects rather than subjects, where services are often provided to women as a means of something else rather on the well-being of women. In the case of reproductive health services, these services are often provided as a form of fertility control rather than as care for women's well-being. Additionally, although the majority of the workforce in health care systems are female, many of the working conditions remain discriminatory towards women. Many studies have shown that women are often expected to conform to male work models that ignore their special needs, such as childcare or protection from violence. This significantly reduces the ability and efficiency of female caregivers providing care to patients, particularly female ones.<ref name=\"Expanding the care continuum for HIV/AIDS: bringing carers into focus\"></ref>\n\nStructural gender inequalities in the allocation of resources, such as income, education, health care, nutrition and political voice, are strongly associated with poor health and reduced well-being. Very often, such structural gender discrimination of women in many other areas has an indirect impact on women's health. For example, because women in many developing nations are less likely to be part of the formal labor market, they often lack access to job security and the benefits of social protection, including access to health care. Additionally, within the formal workforce, women often face challenges related to their lower status, where they suffer workplace discrimination and sexual harassment. Studies have shown that this expectation of having to balance the demands of paid work and work at home often give rise to work-related fatigue, infections, mental ill-health and other problems, which results in women faring poorer in health.\n\nWomen's health is also put at a higher level of risk as a result of being confined to certain traditional responsibilities, such as cooking and water collection. Being confined to unpaid domestic labor not only reduces women's opportunities to education and formal job employment (both of which can indirectly contribute to better health in the long run), but also potentially expose women to higher risk of health issues. For instance, in developing regions where solid fuels are used for cooking, women are exposed to a higher level of indoor air pollution due to extended periods of cooking and preparing meals for the family. Breathing air tainted by the burning of solid fuels is estimated to be responsible for 641,000 of the 1.3 million deaths of women worldwide each year due to chronic obstructive pulmonary disorder (COPD).\n\nIn some settings, structural gender inequity is associated with particular forms of violence, marginalization, and oppression against females. This includes violent assault by men, child sexual abuse, strict regulation of women's behavior and movement, female genital mutilation, and exploitative, forced labor. Women and girls are also vulnerable to less well-documented forms of abuse or exploitation, such as human trafficking or \"honor killings\" for perceived behavioral transgressions and deviation of their social roles. These acts are associated with a wide range of health problems in women such as physical injuries, unwanted pregnancies, abortions, mental disorders such as depression, and anxiety, substance abuse, and sexually transmitted infections, all of which can potentially lead to premature death.\n\nThe ability of women to utilize health care is also heavily influenced by other forms of structural gender inequalities. These include unequal restriction on one's mobility and behavior, as well as unequal control over financial resources. Many of these social gender inequalities can impact the way women's health is regarded, which can in turn determine the level of access women have to healthcare services and the extent by which households and the larger community are willing to invest in women's health issues.\n\nApart from gender discrimination, other axes of oppression also exist in society to further marginalize certain groups of women, especially those who are living in poverty or of minority status in which they live.\n\nRace is a well known axis of oppression, where people of color tend to suffer more from structural violence. For people of color, race can serve as a factor, in addition to gender, that can further influence one's health negatively. Studies have shown that in both high-income and low-income countries, levels of maternal mortality may be up to three times higher among women of disadvantaged ethnic groups than among white women. In a study on race and mother-death within the US, the maternal mortality rate for African Americans is close to four times higher than that of white women. Similarly in South Africa, the maternal mortality rate for black/African women and women of color is approximately 10 and 5 times greater respectively than that of white/European women.\n\nAlthough women around the world share many similarities in terms of the health-impacting challenges, there are also many distinct differences that arise from their varying states of socioeconomic conditions. The type of living conditions in which women live is largely associated with not only their own socioeconomic status, but also that of their nation.\n\nAt every single age category, women in high income countries tend to live longer and are less likely to suffer from ill health than and premature mortality than those in low income countries. Death rates in high-income countries are also very low among children and younger women, where most deaths occur after the age of 60 years. In low-income countries however, the death rates at young ages are much higher, with most death occurring among girls, adolescents, and younger adult women. Data from 66 developing countries show that child mortality rates among the poorest 20% of the population are almost double those in the top 20%.\n\nThe situation is similar within countries as well, where the health of both girls and women is critically affected by social and economic factors. Those who are living in poverty or of lower socioeconomic status tend to perform poorly in terms of health outcomes. In almost all countries, girls and women living in wealthier households experience lower levels of mortality and higher usage of health care services than those living in the poorer households. Such socioeconomic status-related health disparities is present in every nation around the world, including developed regions.\n\nThe Fourth World Conference on Women asserts that men and women share the same right to the enjoyment of the highest attainable standard of physical and mental health. However, women are disadvantaged due to social, cultural, political and economic factors that directly influence their health and impede their access to health-related information and care. In the 2008 World Health Report, the World Health Organization stressed that strategies to improve women's health must take full account of the underlying determinants of health, particularly gender inequality. Additionally, specific socioeconomic and cultural barriers that hamper women in protecting and improving their health must also be addressed.\n\nGender mainstreaming was established as a major global strategy for the promotion of gender equality in the Beijing Platform for Action from the Fourth United Nations World Conference on Women in Beijing in 1995. Gender mainstreaming is defined by the United Nations Economic and Social Council in 1997 as follows: “Mainstreaming a gender perspective is the process of assessing the implications for women and men of any planned action, including legislation, policies or programmes, in all areas and at all levels. It is a strategy for making women’s as well as men’s concerns and experiences an integral dimension of the design, implementation, monitoring and evaluation of policies and programmes in all political, economic and societal spheres so that women and men benefit equally and inequality is not perpetuated. The ultimate aim is to achieve gender equality.”\n\nOver the past few years, \"gender mainstreaming\" has become a preferred approach for achieving greater health parity between men and women. It stems from the recognition that while technical strategies are necessary, they are not sufficient in alleviating gender disparities in health unless the gender discrimination, bias and inequality that in organizational structures of governments and organizations – including health systems - are being challenged and addressed. The gender mainstreaming approach is a response to the realisation that gender concerns must be dealt with in every aspect of policy development and programming, through systematic gender analyses and the implementation of actions that address the balance of power and the distribution of resources between women and men. In order to address gender health disparities, gender mainstreaming in health employs a dual focus. First, it seeks to identify and address gender-based differences and inequalities in all health initiatives; and second, it works to implement initiatives that address women's specific health needs that are a result either of\nbiological differences between women and men (e.g. maternal health) or of gender-based\ndiscrimination in society (e.g. gender-based violence; poor access to health services).\n\nSweden’s new public health policy, which came into force in 2003, has been identified as a key example of mainstreaming gender in health policies. According to the World Health Organization, Sweden’s public health policy is designed to address not only the broader social determinants of health, but also the way in which gender is woven into the public health strategy. The policy specifically highlights its commitment to address and reduce gender-based inequalities in health.\n\nThe United Nations has identified the enhancement of women's involvement as way to achieve gender equality in the realm of education, work, and health. This is because women play critical roles as caregivers, formally and informally, in both the household and the larger community. Within the United States, an estimated 66% of all caregivers are female, with one-third of all female caregivers taking care of two or more people According to the World Health Organization, it is important that approaches and frameworks that are being implemented to address gender disparities in health acknowledge the fact that majority of the care work is provided by women. A meta-analysis of 40 different women's empowerment projects found that increased female participation have led to a broad range of quality of life improvements. These improvements include increases in women's advocacy demands and organization strengths, women-centered policy and governmental changes, and improved economic conditions for lower class women.\n\nIn Nepal, a community-based participatory intervention to identify local birthing problems and formulating strategies has been shown to be effective in reducing both neonatal and maternal mortality in a rural population. Community-based programs in Malaysia and Sri Lanka that used well-trained midwives as front-line health workers also produced rapid declines in maternal mortality.\n"}
{"id": "54015466", "url": "https://en.wikipedia.org/wiki?curid=54015466", "title": "Holy Lie", "text": "Holy Lie\n\nA Holy Lie is a philosophical concept coined by Friedrich Nietzsche in his note manuscript \"The Will to Power\". It is also mentioned in another of Nietzsche's books, \"The Antichrist\".\n\nThe idea of a holy lie is seen in part one of the second book of \"The Will to Power,\" which consists of two critiques. The two critiques are \"the critique of religion\" and \"the critique of morality\" respectively.\n\nA holy lie, according to Friedrich Nietzsche, is the means by which priests and philosophers obtain the piety of their audiences.\n\nNietzsche argues that the purpose of human action is altered by the holy lie since the original moral standards of the general public is influenced by the moral standards preached by the priests and philosophers. Consequently, the human faculty of moral judgment based on the \"beneficial\" versus the \"harmful\" is in dysfunction. People who accept the holy lie do not evaluate the fairness of a thing by what Nietzsche calls \"The natural notion\". Instead, the notions, standards and doctrines preached by the preachers(priests and philosophers) substitutes the natural notion.\nIn his book \"The Antichrist\", Nietzsche further explains his idea with an example of Paul the Apostle and the downfall of the Roman empire.\n"}
{"id": "1264398", "url": "https://en.wikipedia.org/wiki?curid=1264398", "title": "Hyperbolic coordinates", "text": "Hyperbolic coordinates\n\nIn mathematics, hyperbolic coordinates are a method of locating points in quadrant I of the Cartesian plane\n\nHyperbolic coordinates take values in the hyperbolic plane defined as:\n\nThese coordinates in \"HP\" are useful for studying logarithmic comparisons of direct proportion in \"Q\" and measuring deviations from direct proportion.\n\nFor formula_3 in formula_4 take\n\nand\n\nThe parameter \"u\" is the hyperbolic angle to (\"x, y\") and \"v\" is the geometric mean of \"x\" and \"y\".\n\nThe inverse mapping is\n\nThe function formula_8 is a continuous mapping, but not an analytic function.\n\nSince \"HP\" carries the metric space structure of the Poincaré half-plane model of hyperbolic geometry, the bijective correspondence \nformula_9 brings this structure to \"Q\". It can be grasped using the notion of hyperbolic motions. Since geodesics in \"HP\" are semicircles with centers on the boundary, the geodesics in \"Q\" are obtained from the correspondence and turn out to be rays from the origin or petal-shaped curves leaving and re-entering the origin. And the hyperbolic motion of \"HP\" given by a left-right shift corresponds to a squeeze mapping applied to \"Q\".\n\nSince hyperbolas in \"Q\" correspond to lines parallel to the boundary of \"HP\", they are horocycles in the metric geometry of \"Q\".\n\nIf one only considers the Euclidean topology of the plane and the topology inherited by \"Q\", then the lines bounding \"Q\" seem close to \"Q\". Insight from the metric space \"HP\" shows that the open set \"Q\" has only the origin as boundary when viewed through the correspondence. Indeed, consider rays from the origin in \"Q\", and their images, vertical rays from the boundary \"R\" of \"HP\". Any point in \"HP\" is an infinite distance from the point \"p\" at the foot of the perpendicular to \"R\", but a sequence of points on this perpendicular may tend in the direction of \"p\". The corresponding sequence in \"Q\" tends along a ray toward the origin. The old Euclidean boundary of \"Q\" is no longer relevant.\n\nFundamental physical variables are sometimes related by equations of the form \"k\" = \"x y\". For instance, \"V\" = \"I R\" (Ohm's law), \"P\" = \"V I\" (electrical power), \"P V\" = \"k T\" (ideal gas law), and \"f\" λ = \"v\" (relation of wavelength, frequency, and velocity in the wave medium). When the \"k\" is constant, the other variables lie on a hyperbola, which is a horocycle in the appropriate \"Q\" quadrant.\n\nFor example, in thermodynamics the isothermal process explicitly follows the hyperbolic path and work can be interpreted as a hyperbolic angle change. Similarly, a given mass \"M\" of gas with changing volume will have variable density δ = \"M / V\", and the ideal gas law may be written \"P = k T\" δ so that an isobaric process traces a hyperbola in the quadrant of absolute temperature and gas density.\n\nFor hyperbolic coordinates in the theory of relativity see the History section.\n\n\nThere are many natural applications of hyperbolic coordinates in economics:\nThe unit currency sets formula_10. The price currency corresponds to formula_11. For\n\nwe find formula_13, a positive hyperbolic angle. For a \"fluctuation\" take a new price\n\nThen the change in \"u\" is:\n\nQuantifying exchange rate fluctuation through hyperbolic angle provides an objective, symmetric, and consistent measure. The quantity formula_16 is the length of the left-right shift in the hyperbolic motion view of the currency fluctuation.\n\nThe geometric mean is an ancient concept, but hyperbolic angle was developed in this configuration by Gregoire de Saint-Vincent. He was attempting to perform quadrature with respect to the rectangular hyperbola \"y\" = 1/\"x\". That challenge was a standing open problem since Archimedes performed the quadrature of the parabola. The curve passes through (1,1) where it is opposite the origin (mathematics) in a unit square. The other points on the curve can be viewed as rectangles having the same area as this square. Such a rectangle may be obtained by applying a squeeze mapping to the square. Another way to view these mappings is via hyperbolic sectors. Starting from (1,1) the hyperbolic sector of unit area ends at (e, 1/e), where e is 2.71828…, according to the development of Leonhard Euler in \"Introduction to the Analysis of the Infinite\" (1748).\n\nTaking (e, 1/e) as the vertex of rectangle of unit area, and applying again the squeeze that made it from the unit square, yields formula_17 Generally n squeezes yields formula_18 A. A. de Sarasa noted a similar observation of G. de Saint Vincent, that as the abscissas increased in a geometric series, the sum of the areas against the hyperbola increased in arithmetic series, and this property corresponded to the logarithm already in use to reduce multiplications to additions. Euler’s work made the natural logarithm a standard mathematical tool, and elevated mathematics to the realm of transcendental functions. The hyperbolic coordinates are formed on the original picture of G. de Saint-Vincent, which provided the quadrature of the hyperbola, and transcended the limits of algebraic functions.\n\nIn special relativity the focus is on the 3-dimensional hypersurface in the future of spacetime where various velocities arrive after a given proper time. Scott Walter explains that in November 1907 Hermann Minkowski alluded to a well-known three-dimensional hyperbolic geometry while speaking to the Göttingen Mathematical Society, but not to a four-dimensional one.\nIn tribute to Wolfgang Rindler, the author of a standard introductory university-level textbook on relativity, hyperbolic coordinates of spacetime are called Rindler coordinates.\n\n"}
{"id": "43996093", "url": "https://en.wikipedia.org/wiki?curid=43996093", "title": "International Berthing and Docking Mechanism", "text": "International Berthing and Docking Mechanism\n\nThe International Berthing and Docking Mechanism (IBDM) is the European androgynous low impact docking mechanism that is capable of docking and berthing large and small spacecraft. The development of the IBDM is under ESA contract with QinetiQ Space as prime contractor.\n\nThe IBDM development was initiated as a joint development programme with NASA JSC. The first application of the IBDM was intended to be the ISS Crew Return Vehicle (CRV). In the original Agency to Agency agreement, it was decided to develop an Engineering Development Unit (EDU) to demonstrate the feasibility of the system and the associated technologies. NASA JSC were responsible for the system and avionics designs and ESA for the mechanical design. However, since the cancellation of the CRV program, the two Agencies have independently progressed with the docking system development.\n\nThe IBDM is now designed to be compatible with the International Docking System Standard (IDSS) and is hence compatible with the future ISS International Docking Adapters (IDA) on the US side of the ISS.\n\nThe European Space Agency has now started a cooperation with SNC to provide the IBDM for attaching this new vehicle to the ISS in the future. After SNC was selected as a commercial contractor to resupply the International Space Station in January 2016, ESA decided to spend 33 million euros ($36 million) to complete the design of the IBDM and build a flight model for Dream Chaser’s first mission.\n\nThe IBDM provides both docking and berthing capability. The docking mechanism comprises a Soft Capture Mechanism (SCS), and a structural mating system called the Hard Capture System (HCS), explained in more detail below. The IBDM avionics runs in hot redundancy.\n\nThe SCS utilizes active control using 6 servo-actuated legs from RUAG Space (Switzerland) which are coordinated to control the SCS ring in its 6 degrees of freedom. The leg forces are measured to modify the compliance of the SCS ring to facilitate alignment of the active platform during capture. A large range of vehicle mass properties can be handled. Mechanical latches achieve soft capture.\n\nThe HCS uses structural hook mechanisms to close the sealed mated interface. QinetiQ Space has developed several generations of latches and hooks to come to the final hook design.\nSENER (Spain) will be responsible for the further development and qualification of the HCS subsystem.\n\nThe key feature of IBDM is that it is a fully computer controlled mechanism having in the following SAFE advantages:\n\n\nThe American company Sierra Nevada Corporation (SNC) is developing the Dream Chaser, which is a small reusable spacecraft that is selected to transport cargo and/or crew to the ISS. The European Space Agency has started a cooperation with SNC to potentially provide the IBDM for attaching this new vehicle to the ISS in the future. The IBDM will be mounted to the unpressurised cargo module, which will be ejected before reentry.\n\nThe IBDM development has successfully passed the Critical Design Review (December 2015). \nAn engineering model of the mechanism and its hot-redundant avionics has been developed and successfully tested (March 2016). The performance of the system has been verified at the certified SDTS docking test facility at NASA JSC.\nA proposal for the qualification and flight manufacturing of the complete IBDM system has been submitted to ESA (April 2016).\n"}
{"id": "51718506", "url": "https://en.wikipedia.org/wiki?curid=51718506", "title": "John Houston Burrus", "text": "John Houston Burrus\n\nJohn Houston Burrus (February 22, 1849 - March 27, 1917) was an educator in Nashville, Tennessee and Lorman, Mississippi. He was a member of the first class of students at Fisk University in Nashville and when that class graduated became among the first group of African-Americans to graduate from a liberal arts college south of the Mason–Dixon line. He was a professor of mathematics at Fisk and in 1883 became the second president of Alcorn Agricultural and Mechanical College, a position he held until 1893.\n\nJohn Houston Burrus was born February 22, 1849 to William C. J. Burrus and his slave, Nancy near Murfreesboro, Tennessee in Rutherford County. William was a planter and lawyer and had been a whig member of the General Assembly of Tennessee. Nancy was mixed-race: African-American, Native American, and white. William and Nancy had two other sons, James Dallas Burrus and Preston Robert Burrus and lived together as husband and wife, with William never marrying and James later remembering their relationship as affectionate and respectful. William died in 1860 and the Burrus family went to a brother of their master, the mother as a cook and the brothers as body servants, serving their master while he was a soldier in the US Civil War (1861-1865).\n\nAt the end of the War, Burrus, was with his two brothers, mother, and Braxton Bragg's Army in Marshall, Texas. Finally free, they were brought to Shreveport, Louisiana, then to New Orleans, and then Memphis, Tennessee where John took a job as a cook on a stern-wheel steamboat. He remained in Memphis for a short time, and moved in 1866 to Nashville where he took a job as a hotel waiter along with James. The pair studied nights with two ladies boarding at the hotel where they worked. During this time they saved enough and learned enough so that by 1867 they were able to enroll at Fisk University along with America W. Robinson and Virginia Walker, who were the schools first students. During his first year at Fisk John converted to the Congregational Church. As a student, Burrus would teach during the summer until his eyesight began to weaken. In the summers of 1873 and 1874 he gave religious presentations with a panorama he bought. At Fisk, John studied Greek and James studied math. John, James, and Virginia Walker graduated in May, 1875, becoming the first blacks to graduate from a liberal arts college south of the Mason–Dixon line. America would join the Fisk Jubilee Singers and become fiance to James, although they did not marry.\n\nAfter graduation, Burrus became a teacher at a school in the suburbs of Nashville and was quickly promoted to principal. In 1876 he was selected by the Republican State committee as a delegate to the Republican National Convention. At the convention, he initially gave his support to Oliver P. Morton, but finally supported Rutherford B. Hayes on the last ballot, which nominated Hays. That fall he accepted principalship of the Yazoo city school In Yazoo, Mississippi, and in June 1877, he was offered a position of instructor of mathematics at Fisk University in place of his brother who had just resigned from the position. He taught for two years at Fisk and received an A.M. (masters) degree in May 1878 along with Virginia Walker (now Virginia Walker Broughton) and America Robinson. In 1879, he resigned in favor of his younger brother who had just graduated. In 1878 he was elected permanent secretary of the Tennessee Republican State convention and was secretary and treasurer of the State executive committee for the next two years. He was elected to the board of school directors for his district for consecutive three year terms from 1878 to 1884. The district had two other directors, both white, and 17 teachers, of whom nine were white. He served as chairman of the board and succeeded in equalizing the salaries of the white and black teachers. In 1880, he spoke at the State Teachers Institute convention in Nashville about unequal funding in schools. This was based on the requirements of the Morrill Act of 1862 which funded land-grant institutions and resulted in new scholarships for black students at Fisk University. He was chosen alternate delegate to the 1880 Republican National Convention. In 1882 he was a candidate for register in Davidson County, Tennessee in August, and in November he was a candidate for the Tennessee House of Representatives. While at Fisk, Burrus began to study law and he was admitted to the bar in 1881. He also began to work as a correspondent for several newspapers and did some real estate work, forming the Laborer's Cooperative Building and Endowment Association.\n\nIn August 1883 he was offered the presidency of Alcorn Agricultural and Mechanical College in Lorman, Mississippi, following Hiram Rhodes Revels in that position. James Burrus had become professor of mathematics and superintendent of the college farm in 1882, and had played a key role in promoting John for the position. He held the position until 1893 until forced to retire due to health issues. He was very successful and the enrollment at Alcorn increased greatly during his tenure In 1898 he attended the National Educational Association Convention in Nashville\n\nAfter leaving Alcorn, he continued to be active in education. In 1903, he wrote to the Nashville \"American\" again criticizing inequal treatment of blacks when federal moneys given to Tennessee under the Morrill Acts were dispersed. He also continued to practice law until his health further deteriorated. He then purchased a farm on Brick Church pike in Nashville. He died of bronchial pneumonia on March 27, 1917 in Nashville. His funeral was at Howard Congregational Church and was buried in Greenwood Cemetery.\n"}
{"id": "25479975", "url": "https://en.wikipedia.org/wiki?curid=25479975", "title": "Kairosis", "text": "Kairosis\n\nKairosis is the literary effect of fulfillment in time. This effect is normally associated with the epic/novel genre of literature, and can be understood by the analogy \"as \"catharsis\" is to the dramatic, so \"kenosis\" is to the lyric, so \"kairosis\" is to the epic/novel.\" It is derived from Kermode's usage of kairos in literary aesthetics, and is part of an ongoing debate within literary aesthetics about the limitations of the rhetorical use of the term kairos.\n\nKairosis is the feeling of integration experienced by the reader of the novel or epic form; it is experienced by the reader as the central protagonist's character and characterisation faces crisis and resolves itself into an explored and integrated personality. This typically occurs by challenging unique and interesting characters with typical and stereotyped actions that are generally applicable to all people.\n\nIn the novel Emma, kairosis is developed when an individualistic young woman attempts to play matchmaker to the world. As her personality is challenged by the stereotypical activity of courtship and matchmaking in a young woman's life, she experiences a crisis in her character as contradictions within her personality become increasingly antagonistic. When Emma integrates these conflicts, by performing the stereotyped action of completing a successful match, the reader experiences kairosis as Emma's character reaches a moment of synthesis that embodies the previous dialectical contradictions.\n\nEmma's problem was not to gain a social status of marriage: we are always aware that she will be married by the end of the novel and there is no catharsis experienced as an unexpected climactic action. Rather, we do not know if Emma is worth marrying. This kind of moral-psychological question is at the heart of Austen's work, and the modern novel. Kairosis is experienced by the reader of the modern novel when the character reaches a moment of psychological integration in time.\n\nKairosis has been used as part of an attempt in role playing game (rpg) theory to grapple with the issue of immersion.\n\n"}
{"id": "33674235", "url": "https://en.wikipedia.org/wiki?curid=33674235", "title": "Language complexity", "text": "Language complexity\n\nLanguage complexity is a topic in linguistics which can be divided into several sub-topics such as phonological, morphological, syntactic, and semantic complexity.\n\nLanguage complexity has been studied less than many other traditional fields of linguistics. While the consensus is turning towards recognizing that complexity is a suitable research area, a central focus has been on methodological choices. Some languages, particularly pidgins and creoles, are considered simpler than most other languages, but there is no direct ranking, and no universal method of measurement although several possibilities are now proposed within different schools of analysis.\n\nThroughout the 19th century, differential complexity was taken for granted. The classical languages Latin and Greek, as well as Sanskrit, were considered to possess qualities which could be achieved by the rising European national languages only through an elaboration that would give them the necessary structural and lexical complexity that would meet the requirements of an advanced civilization. At the same time, languages described as 'primitive' were naturally considered to reflect the simplicity of their speakers. On the other hand, Friedrich Schlegel noted that some nations \"which appear to be at the very lowest grade of intellectual culture\", such as Basque, Sámi and some native American languages, possess a striking degree of elaborateness.\n\nDarwin considered the apparent complexity of many non-Western languages as problematic for evolution theory which in his time held that less advanced people should have less complex languages. Darwin's suggestion was that simplicity and irregularities were the result of extensive language contact while \"the extremely complex and regular construction of many barbarous languages\" should be seen as an utmost perfection of the one and same evolutionary process.\n\nDuring the 20th century, linguists and anthropologists adopted a standpoint that would reject any nationalist ideas about superiority of the languages of establishment. The first known quote that puts forward the idea that all languages are equally complex comes from Rulon S. Wells III, 1954, who attributes it to Charles F. Hockett. Within a year, the same idea found its way to Encyclopædia Britannica:\nWhile laymen never ceased to consider certain languages as simple and others as complex, such a view was erased from official contexts. For instance, the 1971 edition of Guinness Book of World Records featured Saramaccan, a creole language, as \"the world's least complex language\". According to linguists, this claim was \"not founded on any serious evidence\", and it was removed from later editions. Apparent complexity differences in certain areas were explained with a balancing force by which the simplicity in one area would be compensated with the complexity of another; e.g. David Crystal, 1987:\nIn 2001 the compensation hypothesis was eventually refuted by the African-American creolist John McWhorter who pointed out the absurdity of the idea that, as languages change, each would have to include a mechanism that calibrates it according to the complexity of all the other 6,000 or so languages around the world; but linguistics has no knowledge of any such a mechanism.\n\nRevisiting the idea of differential complexity, McWhorter argued that it is indeed creole languages, such as Saramaccan, that are structurally \"much simpler than all but very few older languages\". In McWhorter's notion this is not problematic in terms of the equality of creole languages because simpler structures convey logical meanings in the most straightforward manner, while increased language complexity is largely a question of features which may not add much to the functionality, or improve usefulness, of the language. Examples of such features are inalienable possessive marking, switch-reference marking, syntactic asymmetries between matrix and subordinate clauses, grammatical gender, and other secondary features which are most typically absent in creoles.\n\nDuring the years following McWhorter's article, several books and dozens of articles were published on the topic. As to date, there have been research projects on language complexity, and several workshops for researchers have been organised by various universities.\n\nAt a general level, language complexity can be characterized as the number and variety of elements, and the elaborateness of their interrelational structure. This general characterisation can be broken down into sub-areas:\n\nMeasuring complexity is considered difficult, and the comparison of whole natural languages as a daunting task. On a more detailed level, it is possible to demonstrate that some structures are more complex than others. Phonology and morphology are areas where such comparisons have traditionally been made. For instance, linguistics has tools for the assessment of the phonological system of any given language. As for the study of syntactic complexity, grammatical rules have been proposed as a basis, but generative frameworks, such as Minimalist Program and Simpler Syntax, have been less successful in defining complexity and its predictions than non-formal ways of description.\n\nMany researchers suggest that several different concepts may be needed when approaching complexity: entropy, size, description length, effective complexity, information, connectivity, irreducibility, low probability, syntactic depth etc. Research suggests that while methodological choices affect the results, even rather crude analytic tools may provide a feasible starting point for measuring grammatical complexity.\n\nGuy (1994) illustrates the point by comparing two Santo languages he has worked on that are about as closely related as French and Spanish, Tolomako and Sakao, both spoken in the village of Port Olry, Vanuatu. Because these languages are very similar to each other, and equally distant from English, he holds that neither is inherently biased as being seen as more easy or difficult by an English speaker (see difficulty of learning languages).\n\nSakao has more, and more difficult, vowel distinctions than Tolomako:\nIn addition, it has more and more difficult consonant distinctions:\nTolomako has a simple syllable structure, maximally consonant–vowel–vowel. It is not clear if Sakao even has syllables; that is, whether trying to divide Sakao words into meaningful syllables is even possible.\nWith inalienably possessed nouns, Tolomako inflections are consistently regular, whereas Sakao is full of irregular nouns:\n\nHere Tolomako \"mouth\" is invariably ' and \"hair\" invariably \",\" whereas Sakao \"mouth\" is variably ' and \"hair\" variably \".\"\n\nWith deixis, Tolomako has three degrees (here/this, there/that, yonder/yon), whereas Sakao has seven.\n\nTolomako has a preposition to distinguish the object of a verb from an instrument; indeed, a single preposition, \"ne,\" is used for all relationships of space and time. Sakao, on the other hand, treats both as objects of the verb, with a transitive suffix \"\" that shows the verb has two objects, but letting context disambiguate which is which:\n\nThe Sakao strategy involves polysynthetic syntax, as opposed to the isolating syntax of Tolomako:\nHere ' \"the bow\" is the instrumental of ' \"to shoot\", and ' \"the sea\" is the direct object of ' \"to follow\", which because they are combined into a single verb, are marked as ditransitive with the suffix \".\" Because ' \"to shoot\" has the incorporated object ' \"fish\", the first consonant geminates for \"\"; \",\" being part of one word, then reduces to \".\" And indeed, the previous example of killing a pig could be put more succinctly, but grammatically more complexly, in Sakao by incorporating the object 'pig' into the verb:\n\nGuy asks rhetorically, \"Which of the two languages spoken in Port-Olry do you think the Catholic missionaries learnt and used? Could that possibly be because it was easier than the other?\"\n\nIt is generally acknowledged that, as young languages, creoles are necessarily simpler than non-creoles. Guy believes this to be untrue ; after a comparison with Antillean Creole, he writes, \"I assure you that it is far, far more complex than Tolomako!\", despite being based on his native language, French.\n\n"}
{"id": "9753150", "url": "https://en.wikipedia.org/wiki?curid=9753150", "title": "Law, Legislation and Liberty", "text": "Law, Legislation and Liberty\n\nLaw, Legislation and Liberty is the 1973 work in three volumes by Nobel laureate economist and political philosopher Friedrich Hayek. In it, Hayek further develops the philosophical principles he discussed earlier in \"The Road to Serfdom\", \"The Constitution of Liberty\", and other writings. \"Law, Legislation and Liberty\" is more abstract than Hayek's earlier work, and it focuses on the conflicting views of society as either a design, a made order (\"taxis\"), on the one hand, or an emergent system, a grown order (\"cosmos\"), on the other. These ideas are then connected to two different forms of law: law proper, or \"nomos\" coinciding more or less with the traditional concept of natural law, which is an emergent property of social interaction, and legislation, or \"thesis\", which is properly confined to the administration of non-coercive government services, but is easily confused with the occasional acts of legislature that do actually straighten out flaws in the nomos.\n\"Vol. 1 : Rules and Order\" (1973)\n\"Vol. 2 : The Mirage of Social Justice\" (1976) \n\"Vol. 3 : The Political Order of a Free People\" (1979)\n\n"}
{"id": "218445", "url": "https://en.wikipedia.org/wiki?curid=218445", "title": "Lean manufacturing", "text": "Lean manufacturing\n\nLean manufacturing or lean production, often simply \"lean\", is a systematic method for waste minimization (\"Muda\") within a manufacturing system without sacrificing productivity, which can cause problems. Lean also takes into account waste created through overburden (\"Muri\") and waste created through unevenness in work loads (\"Mura\"). Working from the perspective of the client who consumes a product or service, \"value\" is any action or process that a customer would be willing to pay for.\n\nLean manufacturing makes obvious what adds value, by reducing everything else (which is not adding value). This management philosophy is derived mostly from the Toyota Production System (TPS) and identified as \"lean\" only in the 1990s. TPS is renowned for its focus on reduction of the original Toyota \"seven wastes\" to improve overall customer value, but there are varying perspectives on how this is best achieved. The steady growth of Toyota, from a small company to the world's largest automaker, has focused attention on how it has achieved this success.\n\nLean principles are derived from the Japanese manufacturing industry. The term was first coined by John Krafcik in his 1988 article, \"Triumph of the Lean Production System\", based on his master's thesis at the MIT Sloan School of Management. Krafcik had been a quality engineer in the Toyota-GM NUMMI joint venture in California before joining MIT for MBA studies. Krafcik's research was continued by the International Motor Vehicle Program (IMVP) at MIT, which produced the international best-selling book co-authored by James P. Womack, Daniel Jones, and Daniel Roos called \"The Machine That Changed the World.\" A complete historical account of the IMVP and how the term \"lean\" was coined is given by Holweg (2007).\n\nFor many, lean is the set of \"tools\" that assist in the identification and steady elimination of waste. As waste is eliminated quality improves while production time and cost are reduced.\nA non exhaustive list of such tools would include: SMED, value stream mapping, Five S, \"Kanban\" (pull systems), \"poka-yoke\" (error-proofing), total productive maintenance, elimination of time batching, mixed model processing, rank order clustering, single point scheduling, redesigning working cells, multi-process handling and control charts (for checking mura).\n\nThere is a second approach to lean manufacturing, which is promoted by Toyota, called The Toyota Way, in which the focus is upon improving the \"flow\" or smoothness of work, thereby steadily eliminating \"mura\" (\"unevenness\") through the system and not upon 'waste reduction' per se. Techniques to improve flow include production leveling, \"pull\" production (by means of \"kanban\") and the \"Heijunka box\". This is a fundamentally different approach from most improvement methodologies, and requires considerably more persistence than basic application of the tools, which may partially account for its lack of popularity.\n\nThe difference between these two approaches is not the goal itself, but rather the prime approach to achieving it. The implementation of smooth flow exposes quality problems that already existed, and thus waste reduction naturally happens as a consequence. The advantage claimed for this approach is that it naturally takes a system-wide perspective, whereas a waste focus sometimes wrongly assumes this perspective.\n\nBoth lean and TPS can be seen as a loosely connected set of potentially competing principles whose goal is cost reduction by the elimination of waste. These principles include: pull processing, perfect first-time quality, waste minimization, continuous improvement, flexibility, building and maintaining a long term relationship with suppliers, autonomation, load leveling and production flow and visual control. The disconnected nature of some of these principles perhaps springs from the fact that the TPS has grown pragmatically since 1948 as it responded to the problems it saw within its own production facilities. Thus what one sees today is the result of a 'need' driven learning to improve where each step has built on previous ideas and not something based upon a theoretical framework.\n\nToyota's view is that the main method of lean is not the tools, but the reduction of three types of waste: \"muda\" (non-value-adding work), \"muri\" (overburden), and \"mura\" (unevenness), to expose problems systematically and to use the tools where the ideal cannot be achieved. From this perspective, the tools are workarounds adapted to different situations, which explains any apparent incoherence of the principles above.\n\nLean implementation emphasizes the importance of optimizing work flow through strategic operational procedures while minimizing waste and being adaptable. Flexibility is required to allow production leveling (Heijunka) using tools such as SMED, but have their analogues in other processes such as research and development (R&D). However, adaptability is often constrained, and therefore may not require significant investment. More importantly, all of these concepts have to be acknowledged by employees who develop the products and initiate processes that deliver value. The cultural and managerial aspects of lean are arguably more important than the actual tools or methodologies of production itself. There are many examples of lean tool implementation without sustained benefit, and these are often blamed on weak understanding of lean throughout the whole organization.\n\nLean aims to enhance productivity by simplifying the operational structure enough to understand, perform and manage the work environment. To achieve these three goals simultaneously, one of Toyota's mentoring methodologies (loosely called \"Senpai\" and \"Kohai\" which is Japanese for senior and junior), can be used to foster lean thinking throughout the organizational structure from the ground up. The closest equivalent to Toyota's mentoring process is the concept of \"\"Lean Sensei\",\" which encourages companies, organizations, and teams to seek third-party experts that can provide advice and coaching.\n\nIn 1999, Spear and Bowen identified four rules which characterize the \"Toyota DNA\":\n\n\nMost of the basic goals of lean manufacturing and waste reduction were derived from Benjamin Franklin through documented examples. \"Poor Richard's Almanack\" says of wasted time, \"He that idly loses 5s. worth of time, loses 5s., and might as prudently throw 5s. into the river.\" He added that avoiding unnecessary costs could be more profitable than increasing sales: \"A penny saved is two pence clear. A pin a-day is a groat a-year. Save and have.\"\n\nAgain Franklin's \"The Way to Wealth\" says the following about carrying unnecessary inventory. \"You call them goods; but, if you do not take care, they will prove evils to some of you. You expect they will be sold cheap, and, perhaps, they may [be bought] for less than they cost; but, if you have no occasion for them, they must be dear to you. Remember what Poor Richard says, 'Buy what thou hast no need of, and ere long thou shalt sell thy necessaries.' In another place, he says, 'Many have been ruined by buying good penny worths'.\" Henry Ford cited Franklin as a major influence on his own business practices, which included Just-in-time manufacturing.\n\nThe accumulation of waste and energy within the work environment was noticed by motion efficiency expert Frank Gilbreth, who witnessed the inefficient practices of masons who often bend over to gather bricks from the ground. The introduction of a non-stooping scaffold, which delivered the bricks at waist level, allowed masons to work about three times as quickly, and with the least amount of effort.\n\nFrederick Winslow Taylor, the father of scientific management, introduced what are now called standardization and best practice deployment. In \"Principles of Scientific Management\", (1911), Taylor said: \"And whenever a workman proposes an improvement, it should be the policy of the management to make a careful analysis of the new method, and if necessary conduct a series of experiments to determine accurately the relative merit of the new suggestion and of the old standard. And whenever the new method is found to be markedly superior to the old, it should be adopted as the standard for the whole establishment.\"\n\nTaylor also warned explicitly against cutting piece rates (or, by implication, cutting wages or discharging workers) when efficiency improvements reduce the need for raw labor: \"...after a workman has had the price per piece of the work he is doing lowered two or three times as a result of his having worked harder and increased his output, he is likely entirely to lose sight of his employer's side of the case and become imbued with a grim determination to have no more cuts if soldiering [marking time, just doing what he is told] can prevent it.\"\n\nShigeo Shingo, the best-known exponent of single minute exchange of die and error-proofing or poka-yoke, cites \"Principles of Scientific Management\" as his inspiration.\n\nAmerican industrialists recognized the threat of cheap offshore labor to American workers during the 1910s, and explicitly stated the goal of what is now called lean manufacturing as a countermeasure. Henry Towne, past President of the American Society of Mechanical Engineers, wrote in the Foreword to Frederick Winslow Taylor's \"Shop Management\" (1911), \"We are justly proud of the high wage rates which prevail throughout our country, and jealous of any interference with them by the products of the cheaper labor of other countries. To maintain this condition, to strengthen our control of home markets, and, above all, to broaden our opportunities in foreign markets where we must compete with the products of other industrial nations, we should welcome and encourage every influence tending to increase the efficiency of our productive processes.\"\n\nHenry Ford initially ignored the impact of waste accumulation while developing his mass assembly manufacturing system. Charles Buxton Going wrote in 1915:\n\nFord, in \"My Life and Work\" (1922), provided a single-paragraph description that encompasses the entire concept of waste:\n\nPoor arrangement of the workplace—a major focus of the modern kaizen—and doing a job inefficiently out of habit—are major forms of waste even in modern workplaces.\n\nFord also pointed out how easy it was to overlook material waste. A former employee, Harry Bennett, wrote:\n\nIn other words, Ford saw the rust and realized that the steel plant was not recovering all of the iron.\nFord's early success, however, was not sustainable. As James P. Womack and Daniel Jones pointed out in \"Lean Thinking\", what Ford accomplished represented the \"special case\" rather than a robust lean solution. The major challenge that Ford faced was that his methods were built for a steady-state environment, rather than for the dynamic conditions firms increasingly face today. Although his rigid, top-down controls made it possible to hold variation in work activities down to very low levels, his approach did not respond well to uncertain, dynamic business conditions; they responded particularly badly to the need for new product innovation. This was made clear by Ford's precipitous decline when the company was forced to finally introduce a follow-on to the Model T.\n\nDesign for Manufacture (DFM) is a concept derived from Ford which emphasizes the importance of standardizing individual parts as well as eliminating redundant components in \"My Life and Work\". This standardization was central to Ford's concept of mass production, and the manufacturing \"tolerances\", or upper and lower dimensional limits that ensured interchangeability of parts became widely applied across manufacturing. Decades later, the renowned Japanese quality guru, Genichi Taguchi, demonstrated that this \"goal post\" method of measuring was inadequate. He showed that \"loss\" in capabilities did not begin only after exceeding these tolerances, but increased as described by the Taguchi Loss Function at any condition exceeding the nominal condition. This became an important part of W. Edwards Deming's quality movement of the 1980s, later helping to develop improved understanding of key areas of focus such as cycle time variation in improving manufacturing quality and efficiencies in aerospace and other industries.\n\nWhile Ford is renowned for his production line, it is often not recognized how much effort he put into removing the fitters' work to make the production line possible. Previous to the use, Ford's car's components were fitted and reshaped by a skilled engineer at the point of use, so that they would connect properly. By enforcing very strict specification and quality criteria on component manufacture, he eliminated this work almost entirely, reducing manufacturing effort by between 60-90%. However, Ford's mass production system failed to incorporate the notion of \"pull production\" and thus often suffered from overproduction.\n\nToyota's development of ideas that later became lean may have started at the turn of the 20th century with Sakichi Toyoda, in a textile factory with looms that stopped themselves when a thread broke. This became the seed of autonomation and \"Jidoka\". Toyota's journey with just-in-time (JIT) may have started back in 1934 when it moved from textiles to produce its first car. Kiichiro Toyoda, founder of Toyota Motor Corporation, directed the engine casting work and discovered many problems in their manufacturing. He decided he must stop the repairing of poor quality by intense study of each stage of the process. In 1936, when Toyota won its first truck contract with the Japanese government, his processes hit new problems and he developed the \"Kaizen\" improvement teams.\n\nLevels of demand in the Post War economy of Japan were low and the focus of mass production on lowest cost per item via economies of scale therefore had little application. Having visited and seen supermarkets in the USA, Taiichi Ohno recognised the scheduling of work should not be driven by sales or production targets but by actual sales. Given the financial situation during this period, over-production had to be avoided and thus the notion of Pull (build to order rather than target driven Push) came to underpin production scheduling.\n\nIt was with Taiichi Ohno at Toyota that these themes came together. He built on the already existing internal schools of thought and spread their breadth and use into what has now become the Toyota Production System (TPS). It is principally from the TPS (which was widely referred to in the 1980s as \"just-in-time manufacturing\"), but now including many other sources, that lean production is developing. Norman Bodek wrote the following in his foreword to a reprint of Ford's \"Today and Tomorrow:\"\n\nAlthough the elimination of waste may seem like a simple and clear subject, it is noticeable that waste is often very conservatively identified. This then hugely reduces the potential of such an aim. The elimination of waste is the goal of lean, and Toyota defined three broad types of waste: \"muda\", \"muri\" and \"mura\"; for many lean implementations this list shrinks to the first waste type only with reduced corresponding benefits.\n\nTo illustrate the state of this thinking Shigeo Shingo observed that only the last turn of a bolt tightens it—the rest is just movement. This ever finer clarification of waste is key to establishing distinctions between value-adding activity, waste and non-value-adding work. Non-value adding work is waste that must be done under the present work conditions. One key is to measure, or estimate, the size of these wastes, to demonstrate the effect of the changes achieved and therefore the movement toward the goal.\n\nThe \"flow\" (or smoothness) based approach aims to achieve JIT, by removing the variation caused by work scheduling and thereby provide a driver, rationale or target and priorities for implementation, using a variety of techniques. The effort to achieve JIT exposes many quality problems that are hidden by buffer stocks; by forcing smooth flow of only value-adding steps, these problems become visible and must be dealt with explicitly.\n\n\"Muri\" is all the unreasonable work that management imposes on workers and machines because of poor organization, such as carrying heavy weights, moving things around, dangerous tasks, even working significantly faster than usual. It is pushing a person or a machine beyond its natural limits. This may simply be asking a greater level of performance from a process than it can handle without taking shortcuts and informally modifying decision criteria. Unreasonable work is almost always a cause of multiple variations.\n\nTo link these three concepts is simple in TPS and thus lean. Firstly, \"muri\" focuses on the preparation and planning of the process, or what work can be avoided proactively by design. Next, \"mura\" then focuses on how the work design is implemented and the elimination of fluctuation at the scheduling or operations level, such as quality and volume. \"Muda\" is then discovered after the process is in place and is dealt with reactively. It is seen through variation in output. It is the role of management to examine the \"muda\", in the processes and eliminate the deeper causes by considering the connections to the \"muri\" and \"mura\" of the system. The \"muda\" and \"mura\" inconsistencies must be fed back to the \"muri\", or planning, stage for the next project.\n\nA typical example of the interplay of these wastes is the corporate behaviour of \"making the numbers\" as the end of a reporting period approaches. Demand is raised to 'make plan,' increasing (\"mura\"), when the \"numbers\" are low, which causes production to try to squeeze extra capacity from the process, which causes routines and standards to be modified or stretched. This stretch and improvisation leads to \"muri\"-style waste, which leads to downtime, mistakes and back flows, and waiting, thus the muda of waiting, correction and movement.\n\nThe original seven \"mudas\" are:\n\nEventually, an eighth \"muda\" was defined by Womack et al. (2003); it was described as manufacturing goods or services that do not meet customer demand or specifications. Many others have added the \"waste of unused human talent\" to the original seven wastes. For example, Six Sigma includes the waste of Skills, referred to as \"under-utilizing capabilities and delegating tasks with inadequate training\". Other additional wastes added were for example \"space\". These wastes were not originally a part of the seven deadly wastes defined by Taiichi Ohno in TPS, but were found to be useful additions in practice. In 1999 Geoffrey Mika in his book, \"Kaizen Event Implementation Manual\" added three more forms of waste that are now universally accepted; The waste associated with working to the wrong metrics or no metrics, the waste associated with not utilizing a complete worker by not allowing them to contribute ideas and suggestions and be part of Participative Management, and lastly the waste attributable to improper use of computers; not having the proper software, training on use and time spent surfing, playing games or just wasting time. For a complete listing of the \"old\" and \"new\" wastes see Bicheno and Holweg (2009)\n\nThe identification of non-value-adding work, as distinct from wasted work, is critical to identifying the assumptions behind the current work process and to challenging them in due course. Breakthroughs in SMED and other process changing techniques rely upon clear identification of where untapped opportunities may lie if the processing assumptions are challenged.\n\nThe role of the leaders within the organization is the fundamental element of sustaining the progress of lean thinking. Experienced kaizen members at Toyota, for example, often bring up the concepts of \"Senpai\", \"Kohai\", and \"Sensei\", because they strongly feel that transferring of Toyota culture down and across Toyota can only happen when more experienced Toyota Sensei continuously coach and guide the less experienced lean champions.\n\nOne of the dislocative effects of lean is in the area of key performance indicators (KPI). The KPIs by which a plant/facility are judged will often be driving behaviour, because the KPIs themselves assume a particular approach to the work being done. This can be an issue where, for example a truly lean, Fixed Repeating Schedule (FRS) and JIT approach is adopted, because these KPIs will no longer reflect performance, as the assumptions on which they are based become invalid. It is a key leadership challenge to manage the impact of this KPI chaos within the organization.\n\nSimilarly, commonly used accounting systems developed to support mass production are no longer appropriate for companies pursuing lean. Lean accounting provides truly lean approaches to business management and financial reporting.\n\nAfter formulating the guiding principles of its lean manufacturing approach in the Toyota Production System (TPS), Toyota formalized in 2001 the basis of its lean management: the key managerial values and attitudes needed to sustain continuous improvement in the long run. These core management principles are articulated around the twin pillars of Continuous Improvement (relentless elimination of waste) and Respect for People (engagement in long term relationships based on continuous improvement and mutual trust).\n\nThis formalization stems from problem solving. As Toyota expanded beyond its home base for the past 20 years, it hit the same problems in getting TPS properly applied that other western companies have had in copying TPS. Like any other problem, it has been working on trying a series of countermeasures to solve this particular concern. These countermeasures have focused on culture: how people behave, which is the most difficult challenge of all. Without the proper behavioral principles and values, TPS can be totally misapplied and fail to deliver results. As with TPS, the values had originally been passed down in a master-disciple manner, from boss to subordinate, without any written statement on the way. Just as with TPS, it was internally argued that formalizing the values would stifle them and lead to further misunderstanding. However, as Toyota veterans eventually wrote down the basic principles of TPS, Toyota set to put the Toyota Way into writing to educate new joiners.\n\nContinuous Improvement breaks down into three basic principles:\n\n\nRespect For People is less known outside of Toyota, and essentially involves two defining principles:\n\n\nWhile lean is seen by many as a generalization of the Toyota Production System into other industries and contexts, there are some acknowledged differences that seem to have developed in implementation:\n\n\nLean principles have been successfully applied to various sectors and services, such as call centers and healthcare. In the former, lean's waste reduction practices have been used to reduce handle time, within and between agent variation, accent barriers, as well as attain near perfect process adherence. In the latter, several hospitals have adopted the idea of \"lean hospital\", a concept that priorizes the patient, thus increasing the employee commitment and motivation, as well as boosting medical quality and cost effectiveness.\n\nLean principles also have applications to software development and maintenance as well as other sectors of information technology (IT). More generally, the use of lean in information technology has become known as Lean IT. Lean methods are also applicable to the public sector, but most results have been achieved using a much more restricted range of techniques than lean provides.\n\nThe challenge in moving lean to services is the lack of widely available reference implementations to allow people to see how directly applying lean manufacturing tools and practices can work and the impact it does have. This makes it more difficult to build the level of belief seen as necessary for strong implementation. However, some research does relate widely recognized examples of success in retail and even airlines to the underlying principles of lean. Despite this, it remains the case that the direct manufacturing examples of 'techniques' or 'tools' need to be better 'translated' into a service context to support the more prominent approaches of implementation, which has not yet received the level of work or publicity that would give starting points for implementors. The upshot of this is that each implementation often 'feels its way' along as must the early industrial engineering practices of Toyota. This places huge importance upon sponsorship to encourage and protect these experimental developments.\n\nLean management is nowadays implemented also in non-manufacturing processes and administrative processes. In non-manufacturing processes is still huge potential for optimization and efficiency increase.\n\nThe espoused goals of lean manufacturing systems differ between various authors. While some maintain an internal focus, e.g. to increase profit for the organization, others claim that improvements should be done for the sake of the customer.\n\nSome commonly mentioned goals are:\n\nThe strategic elements of lean can be quite complex, and comprise multiple elements. Four different notions of lean have been identified:\n\n\nLean production has been adopted into other industries to promote productivity and efficiency in an ever changing market. In global supply chain and outsource scale, Information Technology is necessary and can deal with most of hard lean practices to synchronise pull system in supply chains and value system. The manufacturing industry can renew and change strategy of production just in time.\n\nThe supply chains take changes in deploying second factory or warehouse near their major markets in order to react consumers’ need promptly instead of investing manufacturing factories on the lost-cost countries. For instance, Dell sells computers directly from their website, cutting franchised dealers out of their supply chains. Then, the firm use outsourced partners to produce its components, deliver components to their assembly plants on these main markets around the world, like America and China.\n\nZara made decision of speeding their fashion to the consumers market by fast-producing cloths within five weeks with their local partners in Spain and never involved in mass production to pursue new styles and keep products fresh.\n\nThe other way to avoid market risk and control the supply efficiently is to cut down in stock. P&G has done the goal to co-operate with Walmart and other wholesales companies by building the response system of stocks directly to the suppliers companies.\n\nWith the improvement of global scale supply chains, firms apply lean practices (JIT, supplier partnership, and customer involvement) built between global firms and suppliers intensively to connect with consumers markets efficiently.\n\nAfter years of success of Toyota’s Lean Production, the consolidation of supply chain networks has brought Toyota to the position of being the world's biggest carmaker in the rapid expansion. In 2010, the crisis of safety-related problems in Toyota made other carmakers that duplicated Toyota’s supply chain system wary that the same recall issue might happen to them.\n\nJames Womack had warned Toyota that cooperating with single outsourced suppliers might bring unexpected problems. Indeed, the crisis cost a great fortune and left Toyota thinking whether the JIT practice has problems about outsourced suppliers without enough experience and senior engineers could not achieve the monitoring job close to their suppliers out of Japan. That is proven as the economy of scale becomes global, the soft-learn practices become more important in their outsourced suppliers, if they could keep good Sensei relationship with their partners and constantly modify production process to perfection. Otherwise, Toyota begins to consider whether to have more choices of suppliers of producing the same component, it might bring more safety on risk-control and reduce the huge cost that might happen in the future.\n\nThe appliance of JIT in supply chain system is the key issue of Lean implementation in global scale. How do the supply partners avoid causing production flow? Global firms should make more suppliers who can compete with each other in order to get the best quality and lower the risk of production flow at the same time.\n\nThe following steps should be implemented to create the ideal lean manufacturing system:\n\n\nA fundamental principle of lean manufacturing is demand-based flow manufacturing. In this type of production setting, inventory is only pulled through each production center when it is needed to meet a customer's order. The benefits of this goal include:\n\n\nA continuous improvement mindset is essential to reach the company's goals. The term \"continuous improvement\" means incremental improvement of products, processes, or services over time, with the goal of reducing waste to improve workplace functionality, customer service, or product performance. Lean is founded on the concept of continuous and incremental improvements on product and process while eliminating redundant activities. \"The value of adding activities are simply only those things the customer is willing to pay for, everything else is waste, and should be eliminated, simplified, reduced, or integrated\" (Rizzardo, 2003). Improving the flow of material through new ideal system layouts at the customer's required rate would reduce waste in material movement and inventory.\n\nOverall equipment effectiveness (OEE) is a set of performance metrics that fit well in a lean environment. Also, PMTS, methods-time measurement, cost analysis and perhaps time study can be used to evaluate the wastes and IT effectiveness in the operational processes. For example, Jun-Ing Ker and Yichuan Wang analyze two prescribing technologies, namely no carbon required (NCR) and digital scanning technologies to quantify the advantages of the medication ordering, transcribing, and dispensing process in a multi-hospital health system. With comparison between these two technologies, the statistical analysis results show a significant reduction on process times by adopting digital scanning technology. The results indicated a reduction of 54.5% in queue time, 32.4% in order entry time, 76.9% in outgoing delay time, and 67.7% in outgoing transit time with the use of digital scanning technology.\n\nOne criticism of lean is that its practitioners may focus on tools and methodologies rather than on the philosophy and culture of lean. Consequently, adequate management is needed in order to avoid failed implementation of lean methodologies. Another pitfall is that management decides what solution to use without understanding the true problem and without consulting shop floor personnel. As a result, lean implementations often look good to the manager but fail to improve the situation.\n\nIn addition, many of the popular lean initiatives, coming from the TPS, are solutions to specific problems that Toyota was facing. Toyota, having an undesired current condition, determined what the end state would look like. Through much study, the gap was closed, which resulted in many of the tools in place today. Often, when a tool is implemented outside of TPS, a company believes that the solution lay specifically within one of the popular lean initiatives. The tools which were the solution to a specific problem for a specific company may not be able to be applied in exactly the same manner as designed. Thus, the solution does not fit the problem and a temporary solution is created vs. the actual root cause.\n\nThe lean philosophy aims to reduce costs while optimizing and improving performance. Value stream mapping (VSM) and 5S are the most common approaches companies take on their first steps towards making their organisation leaner. Lean actions can be focused on the specific logistics processes, or cover the entire supply chain. For example, you might start from analysis of SKUs (stock keeping units), using several days to identify and draw each SKUs path, evaluating all the participants from material suppliers to the consumer. Conducting a gap analysis determines the company's 'must take' steps to improve the value stream and achieve the objective. Based on that evaluation, the improvement group conducts the failure mode effects analysis (FMEA), in order to identify and prevent risk factors. It is crucial for front-line workers to be involved in VSM activities since they understood the process and can directly increase the efficiency. Although the impact may be small and limited for each lean activity, implementing a series small improvements incrementally along the supply chain can bring forth enhanced productivity.\n\nAfter adopting the lean approach, both managers and employees experience change. Therefore, decisive leaders are needed when starting on a lean journey. There are several requirements to control the lean journey. First and most importantly, experts recommend that the organization have its own lean plan, developed by the lean Leadership. In other words, the lean team provides suggestions for the leader who then makes the actual decisions about what to implement. Second, coaching is recommended when the organization starts off on its lean journey. They will impart their knowledge and skills to shopfloor staff and the lean implementation will be much more efficient. Third, the metrics or measurements used for measuring lean and improvements are extremely important. It will enable collection of the data required for informed decision-making by a leader. One cannot successfully implement lean without sufficient aptitude at measuring the process and outputs. To control and improve results going forward, one must see and measure, i.e. map, what is happening now.\n\nLean manufacturing is different from lean enterprise. Recent research reports the existence of several lean manufacturing processes but of few lean enterprises. One distinguishing feature opposes lean accounting and standard cost accounting. For standard cost accounting, SKUs are difficult to grasp. SKUs include too much hypothesis and variance, i.e., SKUs hold too much indeterminacy. Manufacturing may want to consider moving away from traditional accounting and adopting lean accounting. In using lean accounting, one expected gain is activity-based cost visibility, i.e., measuring the direct and indirect costs at each step of an activity rather than traditional cost accounting that limits itself to labor and supplies.\n\n\n"}
{"id": "6057472", "url": "https://en.wikipedia.org/wiki?curid=6057472", "title": "List of waste management concepts", "text": "List of waste management concepts\n\nThis is a list of waste management concepts.\n\n\n"}
{"id": "3512082", "url": "https://en.wikipedia.org/wiki?curid=3512082", "title": "Low-g condition", "text": "Low-g condition\n\nLow-\"g\" condition is a phase of aerodynamic flight where the airframe is temporarily unloaded. The pilot—and the airframe—feel temporarily \"weightless\" because the aircraft is in free-fall or decelerating vertically at the top of a climb. It may also occur during an excessively rapid entry into autorotation. This can have a disastrous effect on the aircraft, particularly in the case of helicopters, some of which need the rotor to constantly be under a non-zero amount of load.\n\nIn smaller airplanes\n\nMost smaller airplanes and gliders have no problems with 0\"g\" conditions. In fact, it can be enjoyable to have zero gravity in the cockpit. To produce 0\"g\", the aircraft has to follow a ballistic flight path, which is essentially an upside down parabola.\nThis is the only method to simulate zero gravity for humans on earth. \n\nIn helicopters\n\nIn contrast, low-g conditions can be disastrous for helicopters. In such a situation their rotors may flap beyond normal limits. The excessive flapping can cause the root of the blades to exceed the limit of their hinges and this condition, known as mast bumping, can cause the separation of the blades from the hub or for the mast to shear, and hence detach the whole system from the aircraft, falling from the sky. This is especially true for helicopters with teetering rotors, such as the two-bladed design seen on Robinson helicopters.\nThis effect was first discovered when many accidents with Bell UH-1 and AH-1 helicopters occurred. These particular helicopters simply crashed without any obvious cause. Later, it was found that these accidents usually happened during low terrain flight after passing a ridge and initiating a dive from the previous climb.\nArticulated and rigid rotor systems do not lose controlling forces up to 0\"g\", but may encounter this depending on their flapping hinge offset from the mast. However, dangerous situations, as with a teetering rotor, may not occur.\n\nOn fixed-wing aircraft\n\nLow-\"g\" conditions can also affect fixed-wing aircraft in some instances, mainly by disrupting the airflow over the wings, making them difficult or impossible to control via the aerodynamic surfaces.\n\nThe controllability of an airplane by the control surfaces only depends on airspeed. So, if one keeps airspeed, control is retained. Usually the controllability is even increased, because there is no need to produce lift. 0\"g\" forces are a minimal problem for fixed wing aircraft, but there are exceptions, including, but not limited to, airplanes with gravity-fed fuel systems.\n\nTo simulate 0-\"g\" conditions some space agencies uses a modified passenger aircraft to simulate a low-\"g\" condition. The ESA uses an Airbus A300, for example. NASA has the Vomit Comet. One upside down parabola simulates 0\"g\" for about 25 s.\n"}
{"id": "11880292", "url": "https://en.wikipedia.org/wiki?curid=11880292", "title": "Manvantara", "text": "Manvantara\n\nManvantara or Manuvantara or \"Manvanter\" , or age of a Manu, the Hindu progenitor of humanity, is an astronomical period of time measurement. Manvantara is a Sanskrit word, a compound of \"manu\" and \"antara\", manu-antara or manvantara, literally meaning the duration of a Manu, or his life span.\n\nEach Manvantara is created and ruled by a specific Manu, who in turn is created by Brahma, the Creator himself. Manu creates the world, and all its species during that period of time, each Manvantara lasts the lifetime of a Manu, upon whose death, Brahma creates another Manu to continue the cycle of Creation or Shristi, Vishnu on his part takes a new Avatar, and also a new Indra and Saptarishis are appointed.\n\nFourteen Manus and their respective Manvantaras constitute one Kalpa, Aeon, or a ‘Day of Brahma’, according to the Hindu Time Cycles and also the Vedic timeline. Thereafter, at the end of each Kalpa, there is a period - same as Kalpa - of dissolution or Pralaya, wherein the world (earth and all life forms, but not the entire universe itself) is destroyed and lies in a state of rest, which is called the, ‘Night of Brahma’.\n\nAfter that the creator, Brahma starts his cycle of creation all over again, in an endless cycle of creation followed by Absorption for which Shiva, Hindu God of Absorption, and also renewal, is invoked towards the end of each such cycle.\n\nThe 14 appointed Indras for each kalpa are: Visvabhuk, Vipascit, Sukirti, Sibi, Vibhu, Manobhuva, Ojasvin, the powerful Bali, Adbhuta, Santi, Ramya, Devavara, Vrsa Rtadhaman, Divassvamin and Suci. These are the fourteen sakras(indras).\n\nThe actual duration of a Manvantara, according to the Vishnu Purana is seventy one times the number of years contained in the four Yugas, with some additional years, adding up to 852,000 divine years, or 306,720,000 human years. Seven Rishis, certain (secondary) divinities, Indra, Manu, the king and his sons, are created and perish in one interval (called a Manvantara) equal to seventy-one times the number of years contained in the four Yugas, with some additional years: this is the duration of the Manu, the (attendant) divinities, and the rest, which is equal to 852,000 divine years, or to 306,720,000 years of mortals, independent of the additional period. Fourteen times this period constitutes a Bráhma day, that is, a day of Brahmá; the term (Bráhma) being the derivative form. The Brahma life span is 100 Brahma varshas. The following table will illustrate clearly the link to our years and Brahma years.\n\n1 human year (in Hindu calendar) = 1 Deva Ahoratra for God (1 day and 1 night)\n\n360 Deva Ahoratras = 1 Deva Vatsara\n\n12,000 Deva Vatsara = 1 Chaturyuga\n\n71 Chaturyugas = 1 Manvantaram (1 life span of Manu)\n\n14 Manvantaras = 1 kalpa (1 day of Brahma)\n\n2 Kalpas = 1 day + 1 Brahma ratra\n\n360 days of Brahma = 1 Brahma varsha\n\nSaptarshis (सप्तर्षि) list: Marichi, Atri, Angiras, Pulaha, Kratu, Pulastya, and Vashishtha.\nIn Svayambhuva-manvantara, Lord Vishnu's avatar was called Yajna.\n\nThe first Manu was Svayambhuva Manu. His three daughters, namely Akuti, Devahuti and Prasuti. Devahuti gave birth to one son name kapila and 9 daughters while Prasuti have given birth Yajna and Akuti has given birth to one son and one daughter. Svayambhuva Manu, along with his wife, Satarupa, went into the forest to practice austerities on the bank of the River Sunanda. At some point in time, Rakshasas attacked them, but Yajna, accompanied by his sons the Yamas and the demigods, killed them. Then Yajna personally took the post of Indra, the King of the heavenly planets.\n\nSaptarshis list: Urjastambha, Agni, Prańa, Danti, Rishabha, Nischara, and Charvarivan.\nIn Svarocisha-manvantara, Lord Vishnu's avatar was called Vibhu.\n\nThe second Manu, whose name was Svarocisha, was the son of Agni, and His sons were headed by Dyumat, Sushena and Rochishmat. In the age of this Manu, Rochana became Indra, the ruler of the heavenly planets, and there were many demigods, headed by Tushita. There were also many saintly persons, such as Urjastambha. Among them was Vedasira, whose wife, Tushita, gave birth to Vibhu. Vibhu instructed eighty-eight thousand dridha-vratas, or saintly persons, on self-control and austerity.\n\nSaptarshis list: Kaukundihi, Kurundi, Dalaya, Śankha, Praváhita, Mita, and Sammita.\nIn Uttama-manvantara, Lord Vishnu's avatar was called Satyasena.\n\nUttama, the son of Priyavrata, was the third Manu. Among his sons were Pavana, Srinjaya and Yajnahotra. During the reign of this Manu, the sons of Vashista, headed by Pramada, became the seven saintly persons. The Satyas, Devasrutas and Bhadras became the demigods, and Sushanti became Indra. From the womb of Sunrita, the wife of Dharma, the Lord appeared as Satyasena, and He killed all the Yakshas and Rakshasas who were fighting with Satyajit.\n\nSaptarshis list: Jyotirdhama, Prithu, Kavya, Chaitra, Agni, Vanaka, and Pivara.\nIn Tapasa-manvantara, Lord Vishnu's avatar was called Hari.\n\nHe is named Tapasa because he was born during Tapassu (deep meditation)\n\nTapasa/Tamasa, the brother of the third Manu, was the fourth Manu, and he had ten sons, including Prithu, Khyati, Nara and Ketu. During his reign, the Satyakas, Haris, Viras and others were demigods, the seven great saints were headed by Jyotirdhama, and Trisikha became Indra. Harimedha begot a son named Hari by his wife Harini. Hari saved the devotee Gajendra.\n\nSaptarshis list: Hirannyaroma, Vedasrí, Urddhabahu, Vedabahu, Sudhaman, Parjanya, and Mahámuni.\nIn Raivata-manvantara, Lord Vishnu's avatar was called Vaikuntha.\n\nVaikuntha came as Raivata Manu, the twin brother of Tamasa. His sons were headed by Arjuna, Bali and Vindhya. Among the demigods were the Bhutarayas, and among the seven brahmanas who occupied the seven planets were Hiranyaroma, Vedasira and Urdhvabahu.\n\nSaptarshis list: Sumedhas, Virajas, Havishmat, Uttama, Madhu, Abhináman, and Sahishnnu.\nIn Chakshusha-manvantara, Lord Vishnu's avatar was called Ajita.\n\nAjita came as Chakshsusa Manu, the son of the demigod Chakshu. He had many sons, headed by Puru, Purusa and Sudyumna. During the reign of Chakshusa Manu, the King of heaven was known as Mantradruma. Among the demigods were the Apyas, and among the great sages were Havisman and Viraka.\n\nSaptarshis list: Kashyapa, Atri, Vashista, Vishvamitra, Gautama, Agastya, Bharadvaja. During Vaivasvata-manvantara, Lord Vishnu's avatar is called Vamana\n\nThe seventh Manu, who is the son of Vivasvan, is known as Sraddhadeva(or satyavrata ) or Vaivasvat(son of Vivasvan). He has ten sons, named Ikshvaku, Nabhaga, Dhrsta, Saryati, Narisyanta, Dista, Tarusa, Prsadhra and Vasuman. In this manvantara, or reign of Manu, among the demigods are the Adityas, Vasus, Rudras, Visvedevas, Maruts, Asvini-kumaras and Rbhus. The king of heaven, Indra, is known as Purandara, and the seven sages are known as Kashyapa, Atri, Vashista, Angira, Gautama, Agastya and Bharadwaja. During this period of Manu, Lord Vishnu took birth from the womb of Aditi, the wife of Kashyapa.\n\nSaptarshis list: Diptimat, Galava, Parasurama, Kripa, Drauni or Ashwatthama, Vyasa, and Rishyasringa. In Savarnya-manvantara, Lord Vishnu's avatar will be called Sarvabhauma.\n\nIn the period of the eighth Manu, the Manu is Surya Savarnika Manu. His sons are headed by Nirmoka, and among the demigods are the Sutapas. Bali, the son of Virochana, is Indra, and Galava and Parasurama are among the seven sages. In the age of this Manu, Lord Vishnu's avatar will be called Sarvabhauma, the son of Devaguhya and Sarasvati.\n\nSaptarshis list: Savana, Dyutimat, Bhavya, Vasu, Medhatithi, Jyotishmán, and Satya.\nIn Daksha-savarnya-manvantara, Lord Vishnu's avatar will be called Rishabha.\n\nThe ninth Manu is Daksha-savarni. His sons are headed by Bhutaketu, and among the demigods are the Maricigarbhas. Adbhuta is Indra, and among the seven sages is Dyutiman. Rishabha would be born of Ayushman and Ambudhara.\n\nSaptarshis list: Havishmán, Sukriti, Satya, Apámmúrtti, Nábhága, Apratimaujas, and Satyaket.\nIn Brahma-savarnya-manvantara, Lord Vishnu's avatar will be called Vishvaksena.\n\nIn the period of the tenth Manu, the Manu is Brahma-savarni. Among his sons is Bhurishena, and the seven sages are Havishman and others. Among the demigods are the Suvasanas, and Sambhu is Indra. Vishvaksena would be a friend of Sambhu and will be born from the womb of Vishuci in the house of a brahmana named Visvasrashta.\n\nSaptarshis list: Niśchara, Agnitejas, Vapushmán, Vishńu, Áruni, Havishmán, and Anagha.\nIn Dharma-savarnya-manvantara, Lord Vishnu's avatar will be called Dharmasetu.\n\nIn the period of the eleventh Manu, the Manu is Dharma-savarni, who has ten sons, headed by Satyadharma. Among the demigods are the Vihangamas, Indra is known as Vaidhrita, and the seven sages are Aruna and others. Dharmasetu will be born of Vaidhrita and Aryaka.\n\nSaptarshis list: Tapaswí, Sutapas, Tapomúrtti, Taporati, Tapodhriti, Tapodyuti, and Tapodhan.\nIn Rudra-savarnya-manvantara, Lord Vishnu's avatar will be called Sudhama.\n\nIn the period of the twelfth Manu, the Manu is Rudra-savarni, whose sons are headed by Devavan. The demigods are the Haritas and others, Indra is Ritadhama, and the seven sages are Tapomurti and others.Sudhama, or Svadhama, who will be born from the womb of Sunrita, wife of a Satyasaha.\n\nSaptarshis list: Nirmoha, Tatwadersín, Nishprakampa, Nirutsuka, Dhritimat, Avyaya, and Sutapas.\nIn Deva-savarnya-manvantara, Lord Vishnu's avatar will be called Yogeshwara.\n\nIn the period of the thirteenth Manu, the Manu is Deva-savarni. Among his sons is Chitrasena, the demigods are the Sukarmas and others, Indra is Divaspati, and Nirmoka is among the sages. Yogeshwara will be born of Devahotra and Brihati.\n\nSaptarshis list: Agnibáhu, Śuchi, Śhukra, Magadhá, Gridhra, Yukta, and Ajita.\nIn Indra-savarnya-manvantara, Lord Vishnu's avatar will be called Brihadbhanu.\n\nIn the period of the fourteenth Manu, the Manu is Indra-savarni. Among his sons are Uru and Gambhira, the demigods are the Pavitras and others, Indra is Suci, and among the sages are Agni and Bahu. Brihadbhanuwill be born of Satrayana from the womb of Vitana.\n\nAlmost all literature refers to the first 9 Manus with the same names but there is a lot of disagreement on names after that, although all of them agree with a total of 14.\n\nModern scientific astronomy estimates the Age of the Universe as around 13 Billion years (13 * 10 years). Conversion of 1 day of Brahma into human years yields 8.58816 * 10 years (derived as 2 kalpas * 14 Manvantaras * 71 Chaturyugas * 12,000 Deva vatsaras * 360 human years). According to Vedas, there are 504 000 Manus manifested during the lifetime of one Brahmā (311,040,000,000,000 human Earthly years), 5040 Manus in one year of Brahma, and 420 Manus in one month of Brahma. (See for more details: List of numbers in Hindu scriptures.)\n"}
{"id": "44719851", "url": "https://en.wikipedia.org/wiki?curid=44719851", "title": "Maryanne Cline Horowitz", "text": "Maryanne Cline Horowitz\n\nMaryanne Cline Horowitz is an American Historian of The Renaissance and \nof History of ideas. She is \nProfessor of History at Occidental College,\nAssociate of the UCLA Center for Medieval and Renaissance Studies,\nand an affiliate of the USC Institute for Early Modern Studies.\nHorowitz is best known as the author of\n\"Seeds of Virtue and Knowledge\", \nwhich won the Jacques Barzun Prize in Cultural History in 1999 from the\nAmerican Philosophical Society. \nDr. Horowitz served as Editor-in-Chief of the \"New Dictionary of the History of Ideas\" (6 volumes or E-book) \nwhich the American Library Association division RUSA declared an Outstanding Reference Source 2005.\nShe is an innovator in women's and gender history and was\na Research Associate in Women's Studies in Religion at the Harvard Divinity School, 1979-80.\n\nShe serves on the Board of Editors of the Journal of the History of ideas, \nand edited two books in their series Library of the History of Ideas\nAs President of the Renaissance Conference of Southern California, she hosted a national \nconference for the Renaissance Society of America in 1985 at The Huntington Library, \nOccidental College, and the J. Paul Getty Museum, \ncommemorated in the co-edited book \"Renaissance Rereadings: Intertext and Context\".\n\n\nMaryanne Horowitz has published numerous articles and several books, a selection:\n\n"}
{"id": "17318000", "url": "https://en.wikipedia.org/wiki?curid=17318000", "title": "Meaning (existential)", "text": "Meaning (existential)\n\nMeaning in existentialism is \"descriptive\"; therefore it is unlike typical, \"prescriptive\" conceptions of \"the meaning of life\". Due to the methods of existentialism, prescriptive or declarative statements about meaning are unjustified.. The root of the word \"meaning\" is \"mean\", which is the way someone or something is conveyed, interpreted, or represented. Each individual has his or her own form of unique perspective; meaning is, therefore, purely subjective. Meaning is the way something is understood by an individual; in turn, this subjective meaning is also how the individual may identify it. Meaning is the personal significance of something physical or abstract. This would include the assigning of value(s) to such significance. \n\nFor Kierkegaard, meaning does not equal knowledge, although both are important. Meaning, for Kierkegaard, is a lived experience, a quest to find one's values, beliefs, and purpose in a meaningless world. As a Christian, Kierkegaard finds his meaning in the Word of God, but for those who are not Christian, Kierkegaard wishes them well in their search. However, it is noted that Kierkegaard was not a friend to the church so to claim him as a man of prayer is neglectful. He in fact hated the governance of the church and is religious in the sense that he attempted to understand happiness through his three stages of existence, marking himself on the second stage.\n\n\"Existence precedes essence\" means that humans exist first before they have meaning in life. Meaning is not given, and must be achieved.\nWith objects—say a knife for example, there is some creator who conceives of an idea or purpose of an object, and then creates it with the essence of the object already present. The essence of what the knife will be exists before the actual knife itself. Sartre, who was an atheist, believed that if there is no God to have conceived of our essence or nature, then we must come into existence first, and then create our own essence out of interaction with our surroundings and ourselves. With this come serious implications of self-responsibility over who we are and what our lives mean. For this reason, meaning is something without representation or bearing in anything or anyone else. It is something truly unique to each personseparate, independent.\n\nLogotherapy is a type of psychological analysis that focuses on a will to meaning as opposed to a Nietzschean/Adlerian doctrine of \"will to power\" or Freud's \"will to pleasure\". Frankl also noted the barriers to humanity's quest for meaning in life. He warns against \"...affluence, hedonism, [and] materialism...\" in the search for meaning.\n\nThe following list of tenets represents Frankl's basic principles of Logotherapy:\n\nWe can find meaning in life in three different ways:\n\nLogotherapy was developed by psychiatrist and Holocaust survivor Viktor Frankl.\n\n"}
{"id": "2214559", "url": "https://en.wikipedia.org/wiki?curid=2214559", "title": "Mediated reference theory", "text": "Mediated reference theory\n\nA mediated reference theory (also indirect reference theory) is any semantic theory that posits that words refer to something in the external world, but insists that there is more to the meaning of a name than simply the object to which it refers. It thus stands opposed to the theory of direct reference. Gottlob Frege is a well-known advocate of mediated reference theories. Similar theories were widely held in the middle of the twentieth century by philosophers such as Peter Strawson and John Searle.\n\nMediated reference theories are contrasted with theories of direct reference.\n\nSaul Kripke, a proponent of direct reference theory, in his \"Naming and Necessity\" dubbed mediated reference theory the Frege–Russell view and criticized it. Subsequent scholarship refuted the claim that Bertrand Russell's views on reference theory were the same as Frege's, since Russell was also a proponent of direct reference theory.\n\n\n"}
{"id": "5731748", "url": "https://en.wikipedia.org/wiki?curid=5731748", "title": "Mindfulness-based cognitive therapy", "text": "Mindfulness-based cognitive therapy\n\nMindfulness-based cognitive therapy (MBCT) is an approach to psychotherapy that uses cognitive behavioral therapy (CBT) methods in collaboration with mindfulness meditative practices and similar psychological strategies. It was originally created to be a relapse-prevention treatment for individuals with major depressive disorder (MDD). Focus on MDD and cognitive processes distinguish MBCT from other mindfulness-based therapies. Mindfulness-based stress reduction (MBSR), for example, is a more generalized program that also utilizes the practice of mindfulness.\n\nCBT-inspired methods are used in MBCT, such as educating the participant about depression and the role that cognition plays within it. MBCT takes practices from CBT and applies aspects of mindfulness to the approach. One example would be \"decentering\", a focus on becoming aware of all incoming thoughts and feelings and accepting them, but not attaching or reacting to them. This process aims to aid an individual in regard to disengaging from self-criticism, rumination, and dysphoric moods that can arise when reacting to negative thinking patterns.\n\nLike CBT, MBCT functions on the etiological theory that when individuals who have historically had depression become distressed, they return to automatic cognitive processes that can trigger a depressive episode. The goal of MBCT is to interrupt these automatic processes and teach the participants to focus less on reacting to incoming stimuli, and instead accepting and observing them without judgment. Like MBSR, this mindfulness practice encourages the participant to notice when automatic processes are occurring and to alter their reaction to be more of a reflection. It is theorized that this aspect of MBCT is responsible for the observed clinical outcomes.\n\nBeyond the use of MBCT to reduce depressive symptoms, research additionally supports the effectiveness of mindfulness meditation in reducing cravings for individuals with substance abuse issues. Addiction is known to involve interference with the prefrontal cortex that ordinarily allows for delaying of immediate gratification for longer term benefits by the limbic and paralimbic brain regions. The nucleus accumbens, together with the ventral tegmental area, constitutes the central link in the reward circuit. The nucleus accumbens is also one of the brain structures that is most closely involved in drug dependency. Mindfulness meditation of smokers over a two-week period totaling five hours of meditation decreased smoking by about 60% and reduced their cravings, even for those smokers in the experiment who had no prior intentions to quit. Neuroimaging of those who practice mindfulness meditation reveals increased activity in the prefrontal cortex, a sign of greater self-control.\n\nIn 1991 Philip Barnard and John Teasdale created a multilevel concept of the mind called \"Interacting Cognitive Subsystems\" (ICS). The ICS model is based on Barnard and Teasdale's concept that the mind has multiple modes that are responsible for receiving and processing new information cognitively and emotionally. Barnard and Teasdale's (1991) concept associates an individual's vulnerability to depression with the degree to which he/she relies on only one of the modes of mind, inadvertently blocking the other modes. The two main modes of mind include the \"doing\" mode and \"being\" mode. The \"doing\" mode is also known as the driven mode. This mode is very goal-oriented and is triggered when the mind develops a discrepancy between how things are versus how the mind wishes things to be. The second main mode of mind is the \"being\" mode. \"Being\" mode, is not focused on achieving specific goals, instead the emphasis is on \"accepting and allowing what is,\" without any immediate pressure to change it. The central component of Barnard and Teasdale's ICS is metacognitive awareness. Metacognitive awareness is the ability to experience negative thoughts and feelings as mental events that pass through the mind, rather than as a part of the self. Individuals with high metacognitive awareness are able to avoid depression and negative thought patterns more easily during stressful life situations, in comparison to individuals with low metacognitive awareness. Metacognitive awareness is regularly reflected through an individual's ability to decenter. Decentering is the ability to perceive thoughts and feelings as both impermanent and objective occurrences in the mind.\n\nBased on Barnard and Teasdale's (1991) model, mental health is related to an individual's ability to disengage from one mode or to easily move among the modes of mind. Therefore, individuals that are able to flexibly move between the modes of mind based on the conditions in the environment are in the most favorable state. The ICS model theorizes that the \"being\" mode is the most likely mode of mind that will lead to lasting emotional changes. Therefore, for prevention of relapse in depression, cognitive therapy must promote this mode. This led Teasdale to the creation of MBCT, which promotes the \"being\" mode.\n\nThis therapy was also created by Zindel Segal and Mark Williams, and was partially based on the mindfulness-based stress reduction program, developed by Jon Kabat-Zinn. Theories behind these mindfulness-based approaches to psychological issues function on the idea that being aware of things in the present, and not focusing on the past or the future, will allow the client to be more apt to deal with current stressors and distressing feelings with a flexible and accepting mindset, rather than avoiding, and, therefore, prolonging them.\n\nThe MBCT program is a group intervention that lasts eight weeks. During these eight weeks, there is a weekly course, which lasts two hours, and one day-long class after the fifth week. However, much of the practice is done outside of classes, where the participant uses guided meditations and attempts to cultivate mindfulness in their daily lives.\n\nMBCT prioritizes learning how to pay attention or concentrate with purpose, in each moment and most importantly, without judgment. Through mindfulness, clients can recognize that holding onto some of these feelings is ineffective and mentally destructive. Mindfulness is also thought by Fulton et al. to be useful for the therapists as well during therapy sessions.\n\nMBCT is an intervention program developed to specifically target vulnerability to depressive relapse.Throughout the program, patients learn mind management skills leading to heightened metacognitive awareness, acceptance of negative thought patterns and an ability to respond in skillful ways. During MBCT patients learn to decenter their negative thoughts and feelings, allowing the mind to move from an automatic thought pattern to conscious emotional processing. MBCT can be used as an alternative to maintenance antidepressant treatment, though it may be no more effective.\n\nAlthough the primary purpose of MBCT is to prevent relapse in depressive symptomology, clinicians have been formulating ways in which MBCT can be used to treat physical symptoms of other diseases such as diabetes, cancer, etc. Clinicians are also discovering ways to use MBCT to treat the anxiety and weariness associated with these diseases. \n\nA meta-analysis by Jacob Piet and Esben Hougaard of the University of Aarhus, Denmark Research found that MBCT could be a viable option for individuals with major depressive disorder (MDD) in preventing a relapse. Various studies have shown that it is most effective with individuals who have a history of at least three or more past episodes of MDD. Within that population, participants with life-event triggered depressive episodes were least receptive to MBCT. According to a 2017 meta analysis, mindfulness-based interventions support the decrease in depressive and anxious symptoms in addition to overall level of patient stress. \n\nA mindfulness program based on MBCT offered by the Tees, Esk, and Wear Valleys NHS Foundation Trust, showed that measures of psychological distress, risk of burnout, self-compassion, anxiety, worry, mental well-being, and compassion to others all showed significant improvements after completing the program. Research supports that MBCT results in increased self-reported mindfulness which suggests increased present-moment awareness, decentering, and acceptance, in addition to decreased maladaptive cognitive processes such as judgment, reactivity, rumination, and thought suppression. Results of a 2017 meta-analysis highlight the importance of home practice and its relation to conducive outcomes for mindfulness-based interventions. \n\n\n\n"}
{"id": "2204838", "url": "https://en.wikipedia.org/wiki?curid=2204838", "title": "Morya (Theosophy)", "text": "Morya (Theosophy)\n\nMorya is one of the \"Masters of the Ancient Wisdom\" within modern Theosophical beliefs. He is one of the Mahatmas who inspired the founding of the Theosophical Society and was engaged in a correspondence with two English Theosophists living in India, A. P. Sinnett and A. O. Hume. The correspondence was published in 1923 by A. Trevor Barker, in the book The Mahatma Letters to A. P. Sinnett.\n\nH. P. Blavatsky, originally described the existence of a spiritual master whom she considered her guru, and who went by, among other names, Morya. Blavatsky said that Morya and another master, Koot Hoomi, were her primary guides in establishing the Theosophical Society. Blavatsky also wrote that Masters Morya and Koot Hoomi belonged to a group of highly developed humans known to some as the Great White Brotherhood or the White Lodge (though this is not how they described themselves). Master Morya's personality has been depicted in some detail by various theosophical authors. A man \"living on the earth, but possessed of developed senses that laughed at time and space.\" On the other hand author P. Jenkins challenges that there is little evidence that Blavatsky's Masters, including Morya, ever existed. Author K. Paul Johnson wrote that Blavatsky gave conflicting versions of her meeting with Morya and suggests Blavatsky fictionalized the story, basing it on her encounter with an Italian political activist. Blavatsky's published works have been praised by New York papers as exhibiting immense research, in referring to her book Isis Unveiled the New York Sun writes, \"the strange part of this is, as I and many others can testify as eye witnesses to the production of the book, that the writer had no library in which to make researches and possessed no notes of investigation or reading previously done. All was written straight out of hand. And yet it is full of references to books in the British Museum and other great libraries, and every reference is correct. Either, then, we have, as to that book [referring to Isis Unveiled], a woman who was capable of storing in her memory a mass of facts, dates, numbers, titles, and subjects such as no other human being ever was capable of, or her claim to help from unseen beings is just.\" \n\nAfter Blavatsky's death, theosophists and others continued claiming to have met Morya or to have received communications from him. William Quan Judge, the leader of the American Section of the Theosophical Society, stated privately that he had received letters from Morya and other Adepts. Annie Besant, head of the European Section and co-head of the Esoteric Section with Judge, made public statements supporting the genuineness of those letters; but she later accused Judge of falsifying them, asserting that her suspicions of him were confirmed by the visitation of a Mahatma, presumably Master Morya, to whom she was linked. The ensuing controversy led to the break-up of the Society in 1895, but leaders in the increasingly fragmented movement continued making claims about having received communications and visitations from the Masters connected with the cause. Theosophical writings offered vivid descriptions of Morya, his role in the Brotherhood, and his past lives.\n\nMorya's earliest notable claimed incarnation is recorded by Annie Besant and C.W. Leadbeater (from, the source states, their research into the \"akashic records\" at the Theosophical Society headquarters in Adyar (Tamil Nadu), India conducted in the summer of 1910) as having been the Emperor of Atlantis in 220,000 BC, ruling from his palace in the capital city, the \"City of the Golden Gates\".\n\nAccording to the Ascended Masters teachings, some of the later incarnations that Morya is said to have had include:\n\n\nStudents of Ascended Master Activities believe that Morya ascended in 1898, becoming an Ascended Master and Chohan of the First Ray, and that his spiritual retreat is located at Darjeeling, India.\n\n\n\n"}
{"id": "47926105", "url": "https://en.wikipedia.org/wiki?curid=47926105", "title": "Open Energy Modelling Initiative", "text": "Open Energy Modelling Initiative\n\nThe Open Energy Modelling Initiative (openmod) is a grass roots community of energy system modellers from universities and research institutes across Europe and elsewhere. The initiative promotes the use of open-source software and open data in energy system modelling for research and policy advice. The Open Energy Modelling Initiative documents a variety of open-source energy models and addresses practical and conceptual issues regarding their development and application. The initiative runs an email list, an internet forum, and a wiki and hosts occasional academic workshops. A statement of aims is available.\n\nThe application of open-source development to energy modelling dates back to around 2010. This section provides some background for the growing interest in open methods.\n\nJust two active open energy modelling projects were cited in a 2011 paper: OSeMOSYS and TEMOA. Balmorel was also open at that time, having been made public in 2001.\n, the openmod wiki lists 24 such undertakings.\n\nAn innovative 2012 paper presents the case for using \"open, publicly accessible software and data as well as crowdsourcing techniques to develop robust energy analysis tools\". The paper claims that these techniques can produce high-quality results and are particularly relevant for developing countries.\n\nThere is an increasing call for the energy models and datasets used for energy policy analysis and advice to be made public in the interests of transparency and quality. A 2010 paper concerning energy efficiency modeling argues that \"an open peer review process can greatly support model verification and validation, which are essential for model development\". One 2012 study argues that the source code and datasets used in such models should be placed under publicly accessible version control to enable third-parties to run and check specific models. Another 2014 study argues that the public trust needed to underpin a rapid transition in energy systems can only be built through the use of transparent open-source energy models. The UK TIMES project (UKTM) is open source, according to a 2014 presentation, because \"energy modelling must be replicable and verifiable to be considered part of the scientific process\" and because this fits with the \"drive towards clarity and quality assurance in the provision of policy insights\". In 2016, the Deep Decarbonization Pathways Project (DDPP) is seeking to improve its modelling methodologies, a key motivation being \"the intertwined goals of transparency, communicability and policy credibility.\" A 2016 paper argues that model-based energy scenario studies, wishing to influence decision-makers in government and industry, must become more comprehensible and more transparent. To these ends, the paper provides a checklist of transparency criteria that should be completed by modelers. The authors note however that they \"consider open source approaches to be an extreme case of transparency that does not automatically facilitate the comprehensibility of studies for policy advice.\" An editorial from 2016 opines that closed energy models providing public policy support \"are inconsistent with the open access movement [and] funded research\". A 2017 paper lists the benefits of open data and models and the reasons that many projects nonetheless remain closed. The paper makes a number of recommendations for projects wishing to transition to a more open approach. The authors also conclude that, in terms of openness, energy research has lagged behind other fields, most notably physics, biotechnology, and medicine. Moreover:\n\nA one-page opinion piece in \"Nature News\" from 2017 advances the case for using open energy data and modeling to build public trust in policy analysis. The article also argues that scientific journals have a responsibility to require that data and code be submitted alongside text for scrutiny, currently only \"Energy Economics\" makes this practice mandatory within the energy domain.\n\nIssues surrounding copyright remain at the forefront with regard to open energy data. Most energy datasets are collated and published by official or semi-official sources, for example, national statistics offices, transmission system operators, and electricity market operators. The doctrine of open data requires that these datasets be available under free licenses (such as ) or be in the public domain. But most published energy datasets carry proprietary licenses, limiting their reuse in numerical and statistical models, open or otherwise. Measures to enforce market transparency have not helped because the associated information is normally licensed to preclude downstream usage. Recent transparency measures include the 2013 European energy market transparency regulation 543/2013 and a 2016 amendment to the German Energy Industry Act to establish a nation energy information platform, slated to launch on 1July 2017. Energy databases are protected under general database law, irrespective of the copyright status of the information they hold.\n\nIn December 2017, participants from the Open Energy Modelling Initiative and allied research communities made a written submission to the European Commission on the of public sector information. The document provides a comprehensive account of the data issues faced by researchers engaged in open energy system modeling and energy market analysis and quoted extensively from a German legal opinion.\n\nIn May 2016 the European Union announced that \"all scientific articles in Europe must be freely accessible as of 2020\". This is a step in the right direction, but the new policy makes no mention of open software and its importance to the scientific process. In August 2016, the United States government announced a new federal source code policy which mandates that at least 20% of custom source code developed by or for any agency of the federal government be released as open-source software (OSS). The US Department of Energy (DOE) is participating in the program. The project is hosted on a dedicated website and subject to a three-year pilot. Open-source campaigners are using the initiative to advocate that European governments adopt similar practices. In 2017 the Free Software Foundation Europe (FSFE) issued a position paper calling for free software and open standards to be central to European science funding, including the flagship EU program Horizon2020. The position paper focuses on open data and open data processing and the question of open modeling is not traversed perse.\n\nThe Open Energy Modelling Initiative participants take turns to host regular academic workshops.\n\n\nRelated to openmod\n\n\nOpen energy data\n\n\nSimilar initiatives\n\n\nOther\n\n"}
{"id": "17876651", "url": "https://en.wikipedia.org/wiki?curid=17876651", "title": "Predictor–corrector method", "text": "Predictor–corrector method\n\nIn numerical analysis, predictor–corrector methods belong to a class of algorithms designed to integrate ordinary differential equationsto find an unknown function that satisfies a given differential equation. All such algorithms proceed in two steps: \n\n\nWhen considering the numerical solution of ordinary differential equations (ODEs), a predictor–corrector method typically uses an explicit method for the predictor step and an implicit method for the corrector step.\n\nA simple predictor–corrector method (known as Heun's method) can be constructed from the Euler method (an explicit method) and the trapezoidal rule (an implicit method).\n\nConsider the differential equation\n\nand denote the step size by formula_2.\n\nFirst, the predictor step: starting from the current value formula_3, calculate an initial guess value formula_4 via the Euler method,\n\nNext, the corrector step: improve the initial guess using trapezoidal rule,\n\nThat value is used as the next step.\n\nThere are different variants of a predictor–corrector method, depending on how often the corrector method is applied. The Predict–Evaluate–Correct–Evaluate (PECE) mode refers to the variant in the above example:\n\nIt is also possible to evaluate the function \"f\" only once per step by using the method in Predict–Evaluate–Correct (PEC) mode:\n\nAdditionally, the corrector step can be repeated in the hope that this achieves an even better approximation to the true solution. If the corrector method is run twice, this yields the PECECE mode:\n\nThe PECEC mode has one fewer function evaluation. More generally, if the corrector is run \"k\" times, the method is in P(EC)\nor P(EC)E mode. If the corrector method is iterated until it converges, this could be called PE(CE).\n\n\n\n"}
{"id": "33452109", "url": "https://en.wikipedia.org/wiki?curid=33452109", "title": "Quasi-relative interior", "text": "Quasi-relative interior\n\nIn topology, a branch of mathematics, the quasi-relative interior of a subset of a vector space is a refinement of the concept of the interior. Formally, if formula_1 is a linear space then the quasi-relative interior of formula_2 is\nwhere formula_4 denotes the closure of the conic hull.\n\nLet formula_1 is a normed vector space, if formula_6 is a convex finite-dimensional set then formula_7 such that formula_8 is the relative interior.\n\n"}
{"id": "2916375", "url": "https://en.wikipedia.org/wiki?curid=2916375", "title": "Reterritorialization", "text": "Reterritorialization\n\nReterritorialization () is the restructuring of a place or territory that has experienced deterritorialization. Deterritorialization is a term created by Deleuze and Guattari in their philosophical project \"Capitalism and Schizophrenia\" (1972–1980). They distinguished that relative deterritorialization is always accompanied by reterritorialization. It is the design of the new power. For example, when the Spanish (Hernán Cortés) conquered the Aztecs, and after the Spanish deterritorialized by eliminating the symbols of the Aztecs' beliefs and rituals, the Spanish then reterritorialized by putting up their own beliefs and rituals. This form of propaganda established their takeover of the land. Propaganda is an attempt to reterritorialize by influencing people's ideas through information distributed on a large scale. For example, during World War I, the U.S. put up posters everywhere to encourage young men to join the military and fight.\n\nReterritorialization is when people within a place start to produce an aspect of popular culture themselves, doing so in the context of their local culture and making it their own. An example would be the Indonesian Hip Hop. Although hip hop and rap grew out of the inner cities of New York City and Los Angeles during the 1980s and 1990s, by the time it reached Indonesia through Europe and Central Asia, it had already lost some of its original characteristics. Imported hip hop diffused first to a small group of people in Indonesia; then, Indonesians began to create hip hop music. Although the music was hip hop, the local artists integrated their local culture with the practises of the “foreign” hip hop to create a hybrid that was no longer foreign.\n\nMost current work in human geography uses anthropological definitions of culture and often views the practice associated with popular culture as cultural expressions that may reveal or create aspects of place, space landscape, and identity. The continuous cycles of deterritorialization and reterritorialization through axiomatization makes up one of the basic rhythms of capitalist society. Karl Marx referred to this as the constant revolution of the means of production and uninterrupted disturbances of all social conditions that distinguish the bourgeois era from all the previous. The fundamental mechanism of capital accompanies the process of deterritorialization and reterritorialization. It conjoins deterritorialized resources and appropriates the surplus from their reterritorialized conjunction.\n\nDeterritorialization and reterritorialization presuppose and reinforce the notice of a common essence of desire and labor. This refers to the detachment and reattachment of the energies of production in general of investments of all kinds, whether conventionally considered psychological or economical.\n\nSince the introduction of the mass media, reterritorialization has become more prevalent. The mass media have expedited the process of deterritorialization and reterritorialization and allowed it to occur at a global level. Communications technology has connected the entire world and, in a sense, created a global culture that encompasses everyone who has access to these communications technologies. Anyone who has the internet is part of this culturally diffused community.\nOnce a local culture is part of the global community the process of deterritorialization and reterritorialization continues as the global culture takes from and feeds to all the communities that take part in it. A pop culture example that comments on global reterritorialization is the song \"Californication\" by the Red Hot Chili Peppers. The song is about how California's culture influences the world; a trend that is picked up in California will likely be picked up everywhere in the global community. One of the final verses of the song mentions the destruction that takes place during deterritorialization, but how that opens up the opportunity for reterritorialization: “Destruction leads to a very rough road but it also breeds creation, and earthquakes are to a girl's guitar, they're just another good vibration, and tidal waves couldn't save the world from Californication.” These lyrics capture the essence of reterritorialization at a global level. California is, in a sense, a cultural node in the global community; a place where international trends begin. Deterritorialization and reterritorialization are a continuous part of the evolution of the global culture, and the mass media is its catalyst.\n\n"}
{"id": "302790", "url": "https://en.wikipedia.org/wiki?curid=302790", "title": "Ring laser gyroscope", "text": "Ring laser gyroscope\n\nA ring laser gyroscope (RLG) consists of a ring laser having two independent counter-propagating resonant modes over the same path; the difference in the frequencies is used to detect rotation. It operates on the principle of the Sagnac effect which shifts the nulls of the internal standing wave pattern in response to angular rotation. Interference between the counter-propagating beams, observed externally, results in motion of the standing wave pattern, and thus indicates rotation.\n\nThe first experimental ring laser gyroscope was demonstrated in the US by Macek and Davis in 1963. Various organizations worldwide subsequently developed ring-laser technology further. Many tens of thousands of RLGs are operating in inertial navigation systems and have established high accuracy, with better than 0.01°/hour bias uncertainty, and mean time between failures in excess of 60,000 hours.\n\nRing laser gyroscopes can be used as the stable elements (for one degree of freedom each) in an inertial reference system. The advantage of using an RLG is that there are no moving parts (apart from the dither motor assembly, see further description below and laser-lock), compared to the conventional spinning gyroscope. This means there is no friction, which in turn means there will be no inherent drift terms. Additionally, the entire unit is compact, lightweight and highly durable, making it suitable for use in aircraft and also satellites. Unlike a mechanical gyroscope, the device does not resist changes to its orientation.\n\nContemporary applications of the Ring Laser Gyroscope (RLG) include an embedded GPS capability to further enhance accuracy of RLG Inertial Navigation Systems (INS)s on military aircraft, commercial airliners, ships and spacecraft. These hybrid INS/GPS units have replaced their mechanical counterparts in most applications. Where ultra accuracy is needed however, spin gyro based INSs are still in use today.\n\nA certain rate of rotation induces a small difference between the time it takes light to traverse the ring in the two directions according to the Sagnac effect. This introduces a tiny separation between the frequencies of the counter-propagating beams, a motion of the standing wave pattern within the ring, and thus a beat pattern when those two beams are interfered outside the ring. Therefore, the net shift of that interference pattern follows the rotation of the unit in the plane of the ring.\n\nRLGs, while more accurate than mechanical gyroscopes, suffer from an effect known as \"lock-in\" at very slow rotation rates. When the ring laser is hardly rotating, the frequencies of the counter-propagating laser modes become almost identical. In this case, crosstalk between the counter-propagating beams can allow for injection locking so that the standing wave \"gets stuck\" in a preferred phase, thus locking the frequency of each beam to each other rather than responding to gradual rotation.\n\nForced dithering can largely overcome this problem. The ring laser cavity is rotated clockwise and anti-clockwise about its axis using a mechanical spring driven at its resonance frequency. This ensures that the angular velocity of the system is usually far from the lock-in threshold. Typical rates are 400 Hz, with a peak dither velocity of 1 arc-second per second. Dither does not fix the lock-in problem completely, as each time the direction of rotation is reversed, a short time interval exists in which the rotation rate is near zero and lock-in can briefly occur. If a pure frequency oscillation is maintained, these small lock-in intervals can accumulate. This was remedied by introducing noise to the 400 Hz vibration.\n\nA related device is the fibre optic gyroscope which also operates on the basis of the Sagnac effect, but in which the ring is not a part of the laser. Rather, an external laser injects counter-propagating beams into an optical fiber ring, where rotation causes a relative phase shift between those beams when interfered after their pass through the fiber ring. The phase shift is proportional to the rate of rotation. This is less sensitive in a single traverse of the ring than the RLG in which the externally observed phase shift is proportional to the accumulated rotation itself, not its derivative. However the sensitivity of the fiber optic gyro is enhanced several times over the RLG by having a long optical fiber, coiled for compactness, in which the Sagnac effect is multiplied according to the number of turns.\n\n\n\n"}
{"id": "43082197", "url": "https://en.wikipedia.org/wiki?curid=43082197", "title": "School for Conflict Analysis and Resolution", "text": "School for Conflict Analysis and Resolution\n\nThe School for Conflict Analysis and Resolution (S-CAR) is a division of George Mason University based near Washington, D.C., United States with locations in Arlington, Fairfax, and Lorton, Virginia.\n\nS-CAR was founded in 1981 as the Center for Conflict Analysis, later named the Center for Conflict Analysis and Resolution (CCAR) and began offering a master's degree in Conflict Analysis and Resolution in 1983. In 1988 it became the first academic institution to grant PhD's in Conflict Analysis and Resolution and rose to the status of Institute, becoming ICAR in 1989. In 2010, after a decade of growth and development, including the introduction of the undergraduate program and graduate certificate programs, it became the School for Conflict Analysis and Resolution (S-CAR).\n\n\n\n\n\n"}
{"id": "39157279", "url": "https://en.wikipedia.org/wiki?curid=39157279", "title": "Set inversion", "text": "Set inversion\n\nIn mathematics, set inversion is the problem of characterizing the preimage \"X\" of a set \"Y\" by a function \"f\", i.e., \"X\" = \"f\"(\"Y\") = {\"x\" ∈ R | \"f\"(\"x\") ∈ \"Y\"}. It can also be viewed as the problem of describing the solution set of the quantified constraint \"Y(f(x))\", where Y(y) is a constraint, for example, an inequality, describing the set Y.\n\nIn most applications, \"f\" is a function from R to R and the set \"Y\" is a box of R (i.e. a Cartesian product of \"p\" intervals of R). \n\nWhen \"f\" is nonlinear the set inversion problem can be solved \nusing interval analysis combined with a branch-and-bound algorithm.\nThe main idea consists in building a paving of R made with non-overlapping boxes. For each box [\"x\"], we perform the following tests:\n\nTo check the two first tests, we need an interval extension (or an inclusion function) [\"f\"] for \"f\". Classified boxes are stored into subpavings, i.e., union of non overlapping boxes. \nThe algorithm can be made more efficient by replacing the inclusion tests by contractors.\n\nThe set \"X\" = \"f\"([4,9]) where \"f\"(\"x\", \"x\") = \"x\" + \"x\" is represented on the figure. \n\nFor instance, since [−2,1] + [4,5] = [0,4] + [16,25] = [16,29] does not intersect the interval [4,9], we conclude that the box [-2,1] × [4,5] is outside \"X\". Since [−1,1] + [2,] = [0,1] + [4,5] = [4,6] is inside [4,9], we conclude that the whole box [-1,1] × [2,] is inside \"X\". \n\nSet inversion is mainly used for path planning, for nonlinear parameter set estimation \n\n, for localization \nor for the characterization of stability domains of linear dynamical systems.\n"}
{"id": "44044572", "url": "https://en.wikipedia.org/wiki?curid=44044572", "title": "Sexual violence in the Iraqi insurgency", "text": "Sexual violence in the Iraqi insurgency\n\nThe Islamic State of Iraq and the Levant (ISIL) has employed sexual violence against women and men in a manner that has been described as \"terrorism\". Sexual violence, as defined by The World Health Organization includes “any sexual act, attempt to obtain a sexual act, unwanted sexual comments or advances, or acts to traffic, or otherwise directed, against a person’s sexuality using coercion, by any person regardless of their relationship to the victim, in any setting, including but not limited to home and work.” ISIL has used sexual violence to undermine a sense of security within communities, and to raise funds through the sale of captives into sexual slavery.\n\nIn October 2014, in its digital magazine \"Dabiq\", ISIL explicitly claimed religious justification for enslaving Yazidi women. Specifically, ISIL argued that the Yazidi were idol worshipers and appealed to the shariah practice of spoils of war. ISIL asserts that certain Hadith and Qur’anic verses support their right to enslave and rape captive non-Muslim women. ISIL appealed to apocalyptic beliefs and \"claimed justification by a Hadith that they interpret as portraying the revival of slavery as a precursor to the end of the world.\" According to \"Dabiq,\" \"enslaving the families of the kuffar and taking their women as concubines is a firmly established aspect of the Sharia’s that if one were to deny or mock, he would be denying or mocking the verses of the Qur'an and the narration of the Prophet … and thereby apostatizing from Islam.\" In late 2014 ISIL released a pamphlet that focused on the treatment of female slaves. It says fighters are allowed to have sex with adolescent girls and to beat slaves as discipline. The pamphlet's guidelines also allow fighters to trade slaves, including for sex, as long as they have not been impregnated by their owner. Charlie Winter, a researcher at the counter-extremist think tank Quilliam, described the pamphlet as \"abhorent\". \"The New York Times\" said in August 2015 that \"[t]he systematic rape of women and girls from the Yazidi religious minority has become deeply enmeshed in the organization and the radical theology of the Islamic State in the year since the group announced it was reviving slavery as an institution.\"\n\nISIL has received widespread criticism from Muslim scholars and others in the Muslim world for using part of the Qur'an to derive a ruling in isolation, rather than considering the entire Qur’an and Hadith. In late September 2014 a group of 126 Islamic scholars had signed an open letter to the Islamic State's leader Abu Bakr al-Baghdadi, rejecting his group's interpretations of the Qur'an and hadith to justify its actions. The letter accused the group of instigating fitna—sedition—by instituting slavery under its rule in contravention of the anti-slavery consensus of the Islamic scholarly community. According to Martin Williams in \"The Citizen\", some hard-line Salafists apparently regard extramarital sex with multiple partners as a legitimate form of holy war and it is \"difficult to reconcile this with a religion where some adherents insist that women must be covered from head to toe, with only a narrow slit for the eyes\". According to Mona Siddiqui, ISIL's \"narrative may well be wrapped up in the familiar language of jihad and 'fighting in the cause of Allah', but it amounts to little more than destruction of anything and anyone who doesn't agree with them\"; she describes ISIL as reflecting a \"lethal mix of violence and sexual power\" and a \"deeply flawed view of manhood\". In response to the ISIL pamphlet on the treatment of slaves Abbas Barzegar, a religion professor at Georgia State University, said Muslims around the world find ISIL's \"alien interpretation of Islam grotesque and abhorrent\".<ref name=\"Botelho - 12/12/2014\">Greg Botelho, \"ISIS: Enslaving, having sex with 'unbelieving' women, girls is OK,\" \"CNN\", December 13, 2014</ref> Muslim leaders and scholars from around the world have rejected the validity of these claims, claiming that the reintroduction of slavery is unislamic, that they are required to protect ‘People of the Scripture’ including Christians, Jews, Muslims and Yazidis, and that ISIL's fatwas are invalid due to their lack of religious authority and the fatwas' inconsistency with Islam.\n\nAn article in \"Foreign Policy\" suggests the existence of “a bias against covering rape and sexual assault, since they tend to be viewed by some as 'women’s issues' versus 'mainstream' insurgent tactics.\" Others warn that sexual violence should not be categorized as an act of terror, because such a categorization could provoke dangerous consequences.\nCatherine M. Russell, Ambassador-at-Large for Global Women’s Issues affirms “the de-humanization of women and girls is central to ISIL’s campaign of terror, through which it destroys communities, rewards its fighters and feeds its evil. A coalition that fights ISIL must also fight this particularly egregious form of brutality.”\nOn September 6, 2014, Defend International launched a worldwide campaign entitled \"Save The Yazidis: The World Has To Act Now\" to raise awareness about the tragedy of the Yazidis in Sinjar; coordinate activities related to intensifying efforts aimed at rescuing Yazidi and Christian women and girls captured by ISIL, and building a bridge between potential partners and communities whose work is relevant to the campaign, including individuals, groups, communities, and organizations active in the areas of women’s and girls’ rights, inter alia, as well as actors involved in ending modern-day slavery and violence against women and girls\n\nOn October 14, 2014, Dr. Widad Akrawi of Defend International dedicated her 2014 International Pfeffer Peace Award to the Yazidis, Christians and all residents of Kobane because, she said, facts on the ground demonstrate that these peaceful people are not safe in their enclaves and therefore in urgent need for immediate attention from the global community. She asked the international community to make sure that the victims are not forgotten; they should be rescued, protected, fully assisted and compensated fairly. On November 4, 2014, Dr. Akrawi said that “the international community should define what’s happening to the Yezidis as a crime against humanity, crime against cultural heritage of the region and ethnic cleansing,” adding that Yazidi females are being subjected to a systematic gender-based violence and that slavery and rape are being used by ISIL as weapons of war.” On 3 November 2014, the horrifying “price list” for Yazidi and Christian females issued by ISIL surfaced online, and Dr. Akrawi and her team were the first to verify the authenticity of the document. On 4 November 2014, a translated version of the document was shared by Dr. Akrawi. On 4 August 2015, the same document was confirmed as genuine by a UN official.\n\nA United Nations report issued on October 2, 2014, based on 500 interviews with witnesses, said that ISIL took 450–500 women and girls to Iraq's Nineveh region in August where \"150 unmarried girls and women, predominantly from the Yazidi and Christian communities, were reportedly transported to Syria, either to be given to ISIL fighters as a reward or to be sold as sex slaves\". In mid-October, the UN confirmed that 5,000–7,000 Yazidi women and children had been abducted by ISIL and sold into slavery. In November 2014 the U.N. Commission of Inquiry on Syria said that ISIS was committing crimes against humanity. In 2016 the Commission for International Justice and Accountability said they had identified 34 senior ISIL members who were instrumental in the systematic sex slave trade and planned to prosecute them after the end of hostilities.\n\nAccording to one report, ISIL's capture of Iraqi cities in June 2014 was accompanied by an upsurge in crimes against women, including kidnap and rape. \"The Guardian\" reported that ISIL's extremist agenda extended to women's bodies and that women living under their control were being captured and raped. Fighters are told that they are free to have sex and rape non-Muslim captive women. Hannaa Edwar, a leading women’s rights advocate in Baghdad who runs an NGO called Iraqi Al-Amal Association (IAA), said that none of her contacts in Mosul were able to confirm any cases of rape. However, another Baghdad-based women's rights activist, Basma al-Khateeb, said that a culture of violence existed in Iraq against women generally and felt sure that sexual violence against women was happening in Mosul involving not only ISIL but all armed groups.\n\nIn a press release by the United Nations Iraq on August 12, 2014, representatives report “atrocious accounts on the abduction and detention of Yazidi, Christian, as well as Turkomen and Shabak women, girls and boys, and reports of savage rapes, are reaching us in an alarming manner.” Instances of sexual violence appear to be increasing, with some estimates totaling 1,500 Yazidi and Christian captives forced into sexual slavery.\nAmnesty International infers that ISIL has “launched a systematic campaign of ethnic cleansing in northern Iraq,” where “many of those held by IS have been threatened with rape or sexual assault or pressured to convert to Islam. In some cases entire families have been abducted”. Thus, these crimes extend beyond gender-based violence as males, in addition to females, are being targeted. In this case sexual violence is employed to achieve a political goal, religious conversion to Islam as interpreted by ISIL.\n\nYazidi girls in Iraq allegedly raped by ISIL fighters have committed suicide by jumping to their death from Mount Sinjar, as described in a witness statement.\n\nHaleh Esfandiari from the Woodrow Wilson International Center for Scholars has highlighted the abuse of local women by ISIL militants after they have captured an area. \"They usually take the older women to a makeshift slave market and try to sell them. The younger girls ... are raped or married off to fighters\", she said, adding, \"It's based on temporary marriages, and once these fighters have had sex with these young girls, they just pass them on to other fighters.\" Speaking of Yazidi women captured by ISIS, Nazand Begikhani said \"[t]hese women have been treated like cattle... They have been subjected to physical and sexual violence, including systematic rape and sex slavery. They've been exposed in markets in Mosul and in Raqqa, Syria, carrying price tags.\" \"Dabiq\" describes \"this large-scale enslavement\" of non-Muslims as \"probably the first since the abandonment of Shariah law\".\n\n\"The Guardian\" reported on September 29, 2014, that ISIL extended its recruitment efforts to Western females, asking them to join the movement in order to bear children for the new caliphate. Hundreds of females, predominantly between 16–24 years old, have been radicalized and abandoned their families, homes, and countries to join the jihad in the name of ISIL. At least one is as young as 13 years old.\n\nIn December 2014 the Iraqi Ministry of Human Rights announced that the Islamic State of Iraq and the Levant had killed over 150 women and girls in Fallujah who refused to participate in sexual jihad.\n\nShortly after the death of US hostage Kayla Mueller was confirmed on 10 February 2015, several media outlets reported that the US intelligence community believed she may have been given as a wife to an ISIL fighter. In August 2015 it was confirmed that she had been forced into marriage to Abu Bakr al-Baghdadi, who raped her repeatedly. \"The Washington Post\" reported that \"[t]he leader of the Islamic State personally kept a 26-year-old American woman [Mueller] as a hostage and raped her repeatedly.\" The Mueller family was informed by the U.S. Federal Bureau of Investigation (FBI) that Abu Bakr al-Baghdadi had sexually abused Ms. Mueller, and that Ms. Mueller had also been tortured. Abu Sayyaf's widow, Umm Sayyaf, confirmed that it was her husband who had been Mueller's primary abuser.\n\n"}
{"id": "1002256", "url": "https://en.wikipedia.org/wiki?curid=1002256", "title": "Shot clock", "text": "Shot clock\n\nA shot clock is used in basketball to quicken the pace of the game. It is usually displayed above the backboard behind each goal. The shot clock times a play and provides that a team on offense that does not promptly try to score points loses possession of the ball. It is distinct from the game clock, which times the entire game. The shot clock may be referred to by its initial value. For example, in the National Basketball Association, it may be called the \"24-second clock\".\n\nShot clocks are also used in snooker, pro lacrosse, water polo, korfball, and ten-pin bowling. It is analogous with the play clock used in American and Canadian football.\n\nThe shot clock is set to a certain number of seconds when possession of the ball switches to the other team. The initial setting varies by country, level of play, and league; see the table below. In special cases, such as situations where the new offensive team does not have to travel the entire length of the court, the initial setting may be lower.\n\nThe shot clock counts down the seconds. In some leagues, the shot clock displays tenths of seconds. When the count reaches zero, a buzzer or horn sounds.\n\nThe offensive team must shoot the ball before the shot clock expires. The shot must either touch the rim or enter the basket. If the shot clock expires before the ball leaves the player's hand, the team has committed a shot clock violation that results in a turnover to their opponents. The buzzer may sound after the ball leaves the shooter's hand; this is not a violation.\n\nWhen the ball touches the rim or goes into the basket, the shot clock is reset to its initial value. However, it does not start to count down until a player achieves control of the ball, or in the case of a made basket, a player achieves control of the in-bounds pass.\n\nThe shot clock operator sits at the scorer's table. He or she is usually a different person from the scoreboard operator, as the task requires concentration during and after the shot attempt.\n\nThe National Basketball Association has had a 24-second limit since first introducing the clock in the 1950s; and college basketball for both men and women has a 30-second limit. The WNBA had a 30-second clock originally; since 2006, the limit is 24 seconds.\n\nThe NBA (National Basketball Association) had problems attracting fans (and positive media coverage) before the shot clock's inception. This was due to teams running out the clock once they were leading in a game; without the shot clock, teams passed the ball nearly endlessly without penalty. If one team chose to stall, the other team (especially if behind) would often commit fouls to get the ball back following the free throw. Very low-scoring games with many fouls were common, which bored fans. The most extreme case occurred on November 22, 1950, when the Fort Wayne Pistons defeated the Minneapolis Lakers by a record-low score of 19–18, including 3–1 in the fourth quarter. The Pistons held the ball for minutes at a time without shooting (they attempted 13 shots for the game) in order to limit the impact of the Lakers' dominant George Mikan. The Pistons' performance led the \"St. Paul Dispatch\" to write \"[The Pistons] gave pro basketball a great black eye.\" NBA President Maurice Podoloff said, \"In our game, with the number of stars we have, we of necessity run up big scores.\" A few weeks after the Pistons/Lakers game, the Rochester Royals and Indianapolis Olympians played a six-overtime game with only one shot in each overtime: in each overtime period, the team that had the ball first held it for the entirety of the period before attempting a last-second shot. The NBA tried several rule changes in the early 1950s to speed up the game and reduce fouls before eventually adopting the shot clock.\n\nThe shot clock first came to use in 1954 in Syracuse, New York, where Syracuse Nationals (now the Philadelphia 76ers) owner Danny Biasone and general manager Leo Ferris experimented using a 24-second version during a scrimmage game. Jack Andrews, longtime basketball writer for The Syracuse Post-Standard, often recalled how Ferris would sit at Danny Biasone's Eastwood bowling alley, scribbling potential shot clock formulas onto a napkin. According to Biasone, \"I looked at the box scores from the games I enjoyed, games where they didn't screw around and stall. I noticed each team took about 60 shots. That meant 120 shots per game. So I took 2,880 seconds (48 minutes) and divided that by 120 shots. The result was 24 seconds per shot.\" Ferris was singled out by business manager Bob Sexton at the 1954 team banquet for pushing the shot clock rule. Biasone and Ferris then convinced the NBA to adopt it for the 1954–55 season, a season in which the Nationals won the NBA Championship.\n\nWhen it was first introduced by the NBA, the 24-second shot clock made players so nervous that it hardly came into play, as players were taking fewer than 20 seconds to shoot. According to Syracuse star Dolph Schayes, \"We thought we had to take quick shots – a pass and a shot was it – maybe 8–10 seconds... But as the game went on, we saw the inherent genius in Danny's 24 seconds – you could work the ball around [the offensive zone] for a good shot.\"\n\nThe shot clock, together with some rule changes concerning fouls, revolutionized NBA basketball. In the last pre-clock season (1953–54), teams averaged 79 points per game; in the first year with the clock (1954–55), the average was 93 points, which went up to 107 points by its fourth year in use (1957–58). The advent of the shot clock (and the resulting increase in scoring) coincided with an increase in attendance, which increased 40% within a few years to an average of 4,800 per game.\n\nThe shot clock received near-universal praise for its role in improving the style of play in the NBA. Coach and referee Charley Eckman said, \"Danny Biasone saved the NBA with the 24-second rule.\" Boston Celtic all-star Bob Cousy said, \"Before the new rule, the last quarter could be deadly. The team in front would hold the ball indefinitely, and the only way you could get it was by fouling somebody. In the meantime, nobody dared take a shot and the whole game slowed up. With the clock, we have constant action. I think it saved the NBA at that time. It allowed the game to breathe and progress.\" League president Maurice Podoloff called the adoption of the shot clock \"the most important event in the NBA.\" The league itself states, \"Biasone's invention rescue[d] the league.\"\n\nTwo later pro leagues that rivaled the NBA adopted a modified version of the shot clock. The American Basketball League used a 30-second shot clock for its two years in existence (1961–1963). The American Basketball Association also adopted a 30-second clock when it launched in 1967–68, switching to the NBA's 24-second length for its final season (1975–76).\n\nIn the 1969–70 season, women's collegiate basketball (at the time sanctioned by the Commission on Intercollegiate Athletics for Women) used a 30-second shot clock on an experimental basis, officially adopting it for the 1970–71 season. Unlike the women's side, there was initial resistance to the implementation of a shot clock for men's NCAA basketball, due to fears that smaller colleges would be unable to compete with powerhouses in a running game. However, after extreme results like an 11–6 Tennessee win over Temple in 1973, support for a men's shot clock began to build. The NCAA introduced a 45-second shot clock for the men's game in the 1985–86 season, reducing it to 35 seconds in the 1993–94 season and 30 seconds in the 2015–16 season. The NAIA also reduced the shot clock to 30 seconds for men's basketball starting in 2015–16.\n\nFrom its inception in 1975, the Philippine Basketball Association adopted a 25-second shot clock. This was because the shot clocks then installed at the league's main venues, the Araneta Coliseum and Rizal Memorial Coliseum (the latter no longer used by the league), could only be set at 5-second intervals. The league later adopted a 24-second clock starting from the 1995 season. The Metropolitan Basketball Association in the Philippines used the 23-second clock from its maiden season in 1998. In Filipino college basketball, the NCAA Basketball Championship (Philippines) and the UAAP Basketball Championship adopted a 30-second clock; they switched to 24 seconds starting with the 2001–02 season, the first season to start after the FIBA rule change in 2001.\n\nIn the NBA (since 1954), Women's National Basketball Association (since 2006), and FIBA play (since 2000; 30-second from 1956 to 2000), the shot clock counts down 24 seconds. If a shot is attempted and hits or enters the rim, or if the defensive team gains possession via a rebound, steal, or out-of-bounds play, the shot clock resets. Failure by the offense to attempt a shot that hits the rim within the prescribed time results in a \"shot clock violation\" and a loss of possession to the other team. Three signals indicate when the shot clock expires—a shot clock signal, illuminated lights on the shot clock (NBA, FIBA, Euroleague, and many venues using an NBA-style transparent shot clock), and in the NBA and FIBA play (starting in July 2018), a yellow LED light strip on the backboard. In the 2011–12 NBA season and 2014–15 Euroleague, the last five seconds of the shot clock were modified to include tenths of a second, allowing offensive players to see precisely how much time they have to shoot and officials to determine any last-second shots easily. The rule has been adopted by FIBA starting in 2018. In the 2016-17 NBA season, a new 'official timekeeper' deal for the NBA with Swiss watch manufacturer Tissot introduced a new united official game clock/shot clock system, putting both timing systems under the same system for the first time. Tissot also became official timekeeper for the WNBA in the 2017 season. \n\nIf the offensive team is fouled and the penalty does not include free throws but just an in-bounds pass, the shot clock is reset. There are several cases where the play has already begun to develop and the offense does not need another full 24 seconds. These exceptions work by resetting the shot clock to a given number of seconds if it showed fewer seconds, topping it up to give the offense the given number of seconds as a minimum. Time is never taken off the shot clock by rule.\n\nOne such exception, in the NBA since 1998 and in FIBA after 2010, is that on a foul in the frontcourt, the shot clock is increased to 14.\n\nIn FIBA and the WNBA, the shot clock is also reset to 14 seconds after an offensive rebound.\n\nOn a jump ball, the state of the shot clock depends on which team wins the jump. If the offensive team retains the ball, the shot clock resumes from the time at which it stopped, because there was no change of possession. If the defensive team acquires possession, the shot clock is reset. However:\n\nAmerican college basketball uses a 30-second shot clock, while Canadian university basketball uses a 24-second clock. The American women's game has used a 30-second clock since the 1970s, but the men's game did not adopt a shot clock until 1985. The men's limit was originally 45 seconds, and was shortened to 35 seconds in 1993 before going to 30 seconds in 2015.\n\nThe National Federation of State High School Associations (NFHS), which sets rules for high school basketball in the U.S., does not mandate the use of a shot clock, instead leaving the choice to use a clock and its duration up to each individual state association. Proposals to adopt a national shot clock for high school basketball have been voted down by the NFHS as recently as 2011.\n\nCurrently, eight U.S. states require the use of a shot clock of 30/35 seconds in high school competition: California, Maryland, Massachusetts, New York, North Dakota, Rhode Island, South Dakota, and Washington. The District of Columbia also uses a 30-second shot clock for public school (DCIAA) competition and for the DCSAA State Tournament, where public, private, and charter schools compete for the championship of the District of Columbia.\n\nA related rule to speed up play is that the offensive team has a limited time to advance the ball across the half-court line (the \"time line\"). Failure to do so is a \"backcourt violation\" resulting in a turnover to the other team. This rule was introduced in 1933, predating the shot clock by over 2 decades.\n\nIn men's college basketball, the interval is 10 seconds. FIBA and the NBA specified 10 seconds, but adopted an 8-second limit in 2000 and 2001, respectively. The violation may be referred to using this limit: a \"10-second violation\" or \"8-second violation\".\n\nGenerally, the time limit is not marked off by the shot clock; a referee counts the seconds through a visible motion of his hand or arm. However, women's college basketball introduced the 10-second limit in 2013–2014, and provided that officials will not count the ten seconds but \"will use the shot clock to determine if a 10-second violation has occurred.\" The referee calls a violation if the offense still has the ball in the backcourt when the shot clock has counted down from 30 to 20 and now shows 19 (which first occurs at 19.9 seconds left).\n\n\n"}
{"id": "13059113", "url": "https://en.wikipedia.org/wiki?curid=13059113", "title": "Socioeconomic status", "text": "Socioeconomic status\n\nSocioeconomic status (SES) is an economic and sociological combined total measure of a person's work experience and of an individual's or family's economic and social position in relation to others, based on income, education, and occupation. When analyzing a family's SES, the household income, earners' education, and occupation are examined, as well as combined income, whereas for an individual's SES only their own attributes are assessed. However, SES is more commonly used to depict an economic difference in society as a whole.\n\nSocioeconomic status is typically broken into three levels (high, middle, and low) to describe the three places a family or an individual may fall into. When placing a family or individual into one of these categories, any or all of the three variables (income, education, and occupation) can be assessed.\n\nAdditionally, low income and education have been shown to be strong predictors of a range of physical and mental health problems, including respiratory viruses, arthritis, coronary disease, and schizophrenia. These problems may be due to environmental conditions in their workplace, or, in the case of disabilities or mental illnesses, may be the entire cause of that person's social predicament to begin with.\n\nEducation in higher socioeconomic families is typically stressed as much more important, both within the household as well as the local community. In poorer areas, where food, shelter and safety are priority, education can take a backseat. Youth audiences are particularly at risk for many health and social problems in the United States, such as unwanted pregnancies, drug abuse, and obesity.\n\n\"Income\" refers to wages, salaries, profits, rents, and any flow of earnings received. Income can also come in the form of unemployment or worker's compensation, social security, pensions, interests or dividends, royalties, trusts, alimony, or other governmental, public, or family financial assistance.\n\nIncome can be looked at in two terms, relative and absolute. Absolute income, as theorized by economist John Maynard Keynes, is the relationship in which as income increases, so will consumption, but not at the same rate. Relative income dictates a person or family’s savings and consumption based on the family’s income in relation to others. Income is a commonly used measure of SES because it is relatively easy to figure for most individuals.\n\nIncome inequality is most commonly measured around the world by the Gini coefficient, where 0 corresponds to perfect equality and 1 means perfect inequality. Low income families focus on meeting immediate needs and do not accumulate wealth that could be passed on to future generations, thus increasing inequality. Families with higher and expendable income can accumulate wealth and focus on meeting immediate needs while being able to consume and enjoy luxuries and weather crises.\n\nEducation also plays a role in income. Median earnings increase with each level of education. As conveyed in the chart, the highest degrees, professional and doctoral degrees, make the highest weekly earnings while those without a high school diploma earn less. Higher levels of education are associated with better economic and psychological outcomes (i.e.: more income, more control, and greater social support and networking).\n\nEducation plays a major role in skill sets for acquiring jobs, as well as specific qualities that stratify people with higher SES from lower SES. Annette Lareau speaks on the idea of concerted cultivation, where middle class parents take an active role in their children’s education and development by using controlled organized activities and fostering a sense of entitlement through encouraged discussion. Laureau argues that families with lower income do not participate in this movement, causing their children to have a sense of constraint. An interesting observation that studies have noted is that parents from lower SES households are more likely to give orders to their children in their interactions while parents with a higher SES are more likely to interact and play with their children. A division in education attainment is thus born out of these two differences in child rearing. Research has shown how children who are born in lower SES households have weaker language skills compared to children raised in higher SES households. These language skills affect their abilities to learn and thus exacerbate the problem of education disparity between low and high SES neighborhoods. Lower income families can have children who do not succeed to the levels of the middle income children, who can have a greater sense of entitlement, be more argumentative, or be better prepared for adult life.\n\nResearch shows that lower SES students have lower and slower academic achievement as compared with students of higher SES. When teachers make judgments about students based on their class and SES, they are taking the first step in preventing students from having an equal opportunity for academic achievement. Educators need to help overcome the stigma of poverty. A student of low SES and low self-esteem should not be reinforced by educators. Teachers need to view students as individuals and not as a member of an SES group. Teachers looking at students in this manner will help them to not be prejudiced towards students of certain SES groups. Raising the level of instruction can help to create equality in student achievement. Teachers relating the content taught to students' prior knowledge and relating it to real world experiences can improve achievement. Educators also need to be open and discuss class and SES differences. It is important that all are educated, understand, and be able to speak openly about SES.\n\nOccupational prestige, as one component of SES, encompasses both income and educational attainment. Occupational status reflects the educational attainment required to obtain the job and income levels that vary with different jobs and within ranks of occupations. Additionally, it shows achievement in skills required for the job. Occupational status measures social position by describing job characteristics, decision making ability and control, and psychological demands on the job.\n\nOccupations are ranked by the Census (among other organizations) and opinion polls from the general population are surveyed. Some of the most prestigious occupations are physicians and surgeons, lawyers, chemical and biomedical engineers, university professors, and communications analysts. These jobs, considered to be grouped in the high SES classification, provide more challenging work and greater control over working conditions but require more ability. The jobs with lower rankings include food preparation workers, counter attendants, bartenders and helpers, dishwashers, janitors, maids and housekeepers, vehicle cleaners, and parking lot attendants. The jobs that are less valued also offer significantly lower wages, and often are more laborious, very hazardous, and provide less autonomy.\n\nOccupation is the most difficult factor to measure because so many exist, and there are so many competing scales. Many scales rank occupations based on the level of skill involved, from unskilled to skilled manual labor to professional, or use a combined measure using the education level needed and income involved.\n\nIn sum, the majority of researchers agree that income, education and occupation together best represent SES, while some others feel that changes in family structure should also be considered. With the definition of SES more clearly defined, it is now important to discuss the effects of SES on students' cognitive abilities and academic success. Several researchers have found that SES affects students' abilities.\n\n\"Wealth\", a set of economic reserves or assets, presents a source of security providing a measure of a household's ability to meet emergencies, absorb economic shocks, or provide the means to live comfortably. Wealth reflects intergenerational transitions as well as accumulation of income and savings.\n\nIncome, age, marital status, family size, religion, occupation, and education are all predictors for wealth attainment.\n\nThe wealth gap, like income inequality, is very large in the United States. There exists a racial wealth gap due in part to income disparities and differences in achievement resulting from institutional discrimination. According to Thomas Shapiro, differences in savings (due to different rates of incomes), inheritance factors, and discrimination in the housing market lead to the racial wealth gap. Shapiro claims that savings increase with increasing income, but African Americans cannot participate in this, because they make significantly less than Americans of European descent (whites). Additionally, rates of inheritance dramatically differ between African Americans and Americans of European descent. The amount a person inherits, either during a lifetime or after death, can create different starting points between two different individuals or families. These different starting points also factor into housing, education, and employment discrimination. A third reason Shapiro offers for the racial wealth gap are the various discriminations African Americans must face, like redlining and higher interest rates in the housing market. These types of discrimination feed into the other reasons why African Americans end up having different starting points and therefore fewer assets.\n\nRecently, there has been increasing interest from epidemiologists on the subject of economic inequality and its relation to the health of populations. Socioeconomic status has long been related to health, those higher in the social hierarchy typically enjoy better health than do those below. Socioeconomic status is an important source of health inequity, as there is a very robust positive correlation between socioeconomic status and health, other than for male homosexuals. This correlation suggests that it is not only the poor who tend to be sick when everyone else is healthy, but that there is a continual gradient, from the top to the bottom of the socio-economic ladder, relating status to health. Parents with a low socioeconomic status cannot afford many of the health care resources which is the reason that their children may have a more advanced illness because of the lack of treatment. This phenomenon is often called the \"SES Gradient\" or according to the World Health Organisation the \"Social Gradient\". Lower socioeconomic status has been linked to chronic stress, heart disease, ulcers, type 2 diabetes, rheumatoid arthritis, certain types of cancer, and premature aging.\n\nThere is debate regarding the cause of the SES Gradient. Researchers see a definite link between economic status and mortality due to the greater economic resources of the wealthy, but they find little correlation due to social status differences.\n\nOther researchers such as Richard G. Wilkinson, J. Lynch, and G.A. Kaplan have found that socioeconomic status strongly affects health even when controlling for economic resources and access to health care. Most famous for linking social status with health are the Whitehall studies—a series of studies conducted on civil servants in London. The studies found that although all civil servants in England have the same access to health care, there was a strong correlation between social status and health. The studies found that this relationship remained strong even when controlling for health-affecting habits such as exercise, smoking and drinking. Furthermore, it has been noted that no amount of medical attention will help decrease the likelihood of someone getting type 2 diabetes or rheumatoid arthritis—yet both are more common among populations with lower socioeconomic status.\n\nThere is no significant relationship between SES and stress during pregnancy, while there is a significant relationship with a husband's occupational status. Also, there is no significant relationship between income and mother's education and the rate of pregnancy stress\n\nPolitical scientists have established a consistent relationship between SES and political participation.\n\nThe environment of low SES children is characterized by less dialogue from parents, minimal amounts of book reading, and few instances of joint attention, the shared focus of the child and adult on the same object or event, when compared to the environment of high SES children. In contrast, infants from high SES families experience more child-directed speech. At 10 months, children of high SES hear on average 400 more words than their low SES peers.\n\nLanguage ability differs sharply as a function of SES, for example, the average vocabulary size of 3-year-old children from professional families was more than twice as large as for those on welfare.\n\nChildren from lower income households had greater media access in their bedrooms but lower access to portable play equipment compared to higher income children. This eventually leads children from lower socioeconomic backgrounds to be at a disadvantage when comparing them with their counterparts in terms of access to physical activities.\n\nIn addition to the amount of language input from parents, SES heavily influences the type of parenting style a family chooses to practice. These different parenting styles shape the tone and purpose of verbal interactions between parent and child. For example, parents of high SES tend toward more authoritative or permissive parenting styles. These parents pose more open-ended questions to their children to encourage the latter’s speech growth. In contrast, parents of low SES tend toward more authoritarian styles of address. Their conversations with their children contain more imperatives and yes/no questions that inhibits child responses and speech development.\n\nParental differences in addressing children may be traced to the position of their respective groups within society. Working class individuals often hold low power, subordinate positions in the occupational world. This standing in the social hierarchy requires a personality and interaction style that is relational and capable of adjusting to circumstances. An authoritarian style of address prepares children for these types of roles, which require a more accommodating and compliant personality. Therefore, low SES parents see the family as more hierarchical, with the parents at the top of the power structure, which shapes verbal interaction. This power differential emulates the circumstances of the working class world, where individuals are ranked and discouraged from questioning authority.\n\nConversely, high SES individuals occupy high power positions that call for greater expressivity. High SES parents encourage their children to question the world around them. In addition to asking their children more questions, these parents push their children to create questions of their own. In contrast with low SES parents, these individuals often view the power disparity between parent and child as detrimental to the family. Opting instead to treat children as equals, high SES conversations are characterized by a give and take between parent and child. These interactions help prepare these children for occupations that require greater expressivity.\n\nThe linguistic environment of low and high SES children differs substantially, which affects many aspects of language and literacy development such as semantics, syntax, morphology, and phonology.\n\nSemantics is the study of the meaning of words and phrases. Semantics covers vocabulary, which is affected by SES. \nChildren of high SES have larger expressive vocabularies by the age of 24 months due to more efficient processing of familiar words. By age 3, there are significant differences in the amount of dialogue and vocabulary growth between children of low and high SES. A lack of joint attention in children contributes to poor vocabulary growth when compared to their high SES peers. Joint attention and book reading are important factors that affect children’s vocabulary growth. With joint attention, a child and adult can focus on the same object, allowing the child to map out words. For example, a child sees an animal running outside and the mom points to it and says, \"Look, a dog.\" The child will focus its attention to where its mother is pointing and map the word dog to the pointed animal. Joint attention thus facilitates word learning for children.\n\nSyntax refers to the arrangement of words and phrases to form sentences. SES affects the production of sentence structures. Although 22- to 44-month-old children’s production of simple sentence structures does not vary by SES, low SES does contribute to difficulty with complex sentence structures. Complex sentences include sentences that have more than one verb phrase. An example of a complex sentence is, \"I want you to sit there\". The emergence of simple sentence structures is seen as a structure that is obligatory in everyday speech. Complex sentence structures are optional and can only be mastered if the environment fosters its development.\n\nThis lag in the sentence formation abilities of low SES children may be caused by less frequent exposure to complex syntax through parental speech. Low SES parents ask fewer response-coaxing questions of their children which limits the opportunities of these children to practice more complex speech patterns. Instead, these parents give their children more direct orders, which has been found to negatively influence the acquisition of more difficult noun and verb phrases. In contrast, high SES households ask their children broad questions to cultivate speech development. Exposure to more questions positively contributes to children’s vocabulary growth and complex noun phrase constructions.\n\nChildren’s grasp of morphology, the study of how words are formed, is affected by SES. Children of high SES have advantages in applying grammatical rules, such as the pluralization of nouns and adjectives compared to children of low SES. Pluralizing nouns consists of understanding that some nouns are regular and -s denotes more than one, but also understanding how to apply different rules to irregular nouns. Learning and understanding how to use plural rules is an important tool in conversation and writing. In order to communicate successfully that there is more than one dog running down the street, an -s must be added to dog. Research also finds that the gap in ability to pluralize nouns and adjectives does not diminish by age or schooling because low SES children’s reaction times to pluralize nouns and adjectives do not decrease.\n\nPhonological awareness, the ability to recognize that words are made up different sound units, is also affected by SES. Children of low SES between the second and sixth grades are found to have low phonological awareness. The gap in phonological awareness increases by grade level. This gap is even more problematic if children of low SES are already born with low levels of phonological awareness and their environment does not foster its growth. Children who have high phonological awareness from an early age are not affected by SES.\n\nGiven the large amount of research on the setbacks children of low SES face, there is a push by child developmental researchers to steer research to a more positive direction regarding low SES. The goal is to highlight the strengths and assets low income families possess in raising children. For example, African American preschoolers of low SES exhibit strengths in oral narrative, or storytelling, that may promote later success in reading. These children have better narrative comprehension when compared to peers of higher SES.\n\nA gap in reading growth exists between low SES and high SES children, which widens as children move on to higher grades. Reading assessments that test reading growth include measures on basic reading skills (i.e., print familiarity, letter recognition, beginning and ending sounds, rhyming sounds, word recognition), vocabulary (receptive vocabulary), and reading comprehension skills (i.e., listening comprehension, words in context). The reading growth gap is apparent between the spring of kindergarten and the spring of first-grade, the time when children rely more on the school for reading growth and less on their parents. Initially, high SES children begin as better readers than their low SES counterparts. As children get older, high SES children progress more rapidly in reading growth rates than low SES children. These early reading outcomes affect later academic success. The further children fall behind, the more difficult it is to catch up and the more likely they will continue to fall behind. By the time students enter high school in the United States, low SES children are considerably behind their high SES peers in reading growth.\n\nHome environment is one of the leading factors of a child's well being. Children living in a poor home with inadequate living conditions are more likely to be susceptible to illness and injuries. The disparities in experiences in the home environment children of high and low SES affect reading outcomes. The home environment is considered the main contributor to SES reading outcomes. Children of low SES status are read to less often and have fewer books in the home than their high SES peers, which suggests an answer to why children of low SES status have lower initial reading scores than their high SES counterparts upon entering kindergarten. \nThe home environment makes the largest contribution to the prediction of initial kindergarten reading disparities. Characteristics of the home environment include home literacy environment and parental involvement in school. Home literacy environment is characterized by the frequency with which parents engage in joint book reading with the child, the frequency with which children read books outside of school, and the frequency with which household members visited the library with the child. Parental involvement in school is characterized by attending a parent–teacher conference, attending a parent–teacher association (PTA) meeting, attending an open house, volunteering, participating in fundraising, and attending a school event. Resources, experiences, and relationships associated with the family are most closely associated with reading gaps when students reading levels are first assessed in kindergarten. The influence of family factors on initial reading level may be due to children experiencing little schooling before kindergarten—they mainly have their families to rely on for their reading growth.\n\nFamily SES is also associated with reading achievement growth during the summer. Students from high SES families continue to grow in their ability to read after kindergarten and students from low SES families fall behind in their reading growth at a comparable amount. Additionally, the summer setback disproportionately affects African American and Hispanic students because they are more likely than White students to come from low SES families. Also, low SES families typically lack the appropriate resources to continue reading growth when school is not in session.\n\nThe neighborhood setting in which children grow up contributes to reading disparities between low and high SES children. These neighborhood qualities include but are not limited to garbage or litter in the street, individuals selling or using drugs in the street, burglary or robbery in the area, violent crime in the area, vacant homes in the area, and how safe it is to play in the neighborhood. Low SES children are more likely to grow up in such neighborhood conditions than their high SES peers. Community support for the school and poor physical conditions surrounding the school are also associated with children’s reading. Neighborhood factors help explain the variation in reading scores in school entry, and especially as children move on to higher grades. As low SES children in poor neighborhood environments get older, they fall further behind their high SES peers in reading growth and thus have a more difficult time developing reading skills at grade level.\n\nIn a study by M. Keels, it was determined that when low-income families are moved from poor neighborhoods to suburban neighborhoods, there are reductions in delinquency in children. When comparing different social statuses of families, the environment of a neighborhood turns out to be major factor in contributing to the growth of a child.\n\nSchool characteristics, including characteristics of peers and teachers, contribute to reading disparities between low and high SES children. For instance, peers play a role in influencing early reading proficiency. In low SES schools, there are higher concentrations of less skilled, lower SES, and minority peers who have lower gains in reading. The number of children reading below grade and the presence of low-income peers were consistently associated with initial achievement and growth rates. Low SES peers tend to have limited skills and fewer economic resources than high SES children, which makes it difficult for children to grow in their reading ability. The most rapid growth of reading ability happens between the spring of kindergarten and the spring of first grade. Teacher experience (number of years teaching at a particular school and the number of years teaching a particular grade level), teacher preparation to teach (based on the number of courses taken on early education, elementary education, and child development), the highest degree earned, and the number of courses taken on teaching reading all determine whether or not a reading teacher is qualified. Low SES students are more likely to have less qualified teachers, which is associated with their reading growth rates being significantly lower than the growth rates of their high SES counterparts.\n\nMichael Kraus and Dacher Keltner, in their study published in the December 2008 issue of \"Psychological Science,\" found that children of parents with a high SES tended to express more disengagement behaviors than their peers of low SES. In this context, disengagement behaviors included self-grooming, fidgeting with nearby objects, and doodling while being addressed. In contrast, engagement behaviors included head nods, eyebrow raises, laughter and gazes at one’s partner. These cues indicated an interest in one’s partner and the desire to deepen and enhance the relationship. Participants of low SES tended to express more engagement behaviors toward their conversational partners, while their high SES counterparts displayed more disengagement behaviors. Authors hypothesized that, as SES rises, the capacity to fulfill one’s needs also increases. This may lead to greater feelings of independence, making individuals of high SES less inclined to gain rapport with conversational partners because they are less likely to need their assistance in the future.\n\n"}
{"id": "3665948", "url": "https://en.wikipedia.org/wiki?curid=3665948", "title": "Switching (film)", "text": "Switching (film)\n\nSwitching is the first ever Danish interactive movie directed by Morten Schjødt, produced by Oncotype and released in 2003. It was financed by, and in cooperation with, the Danish Film Institute, with the support of the \"Development Fund of the Ministry of Culture\", the MEDIA Programme of the European Union and the DFI Film Workshop. It is distributed by SF FILM A/S.\n\n\"Switching\" is a different type of film that paves the way for new storytelling methods. Switching was specially developed for DVD video. The fascination with interactive expression basically originates in its potential to depict a more fragmented form of reality. The user of this film enters a narrative labyrinth simultaneously unfolding and disrupting the story. \n\nThere are three elements that explain why \"Switching\" departs from the linear form:\n\nFrida and Simon are in a relationship, but something has changed where they must make a decision. They find themselves wishing for change, yet that wish stands in the way of their wish to hold on to the past. The viewer is caught in this moment and through their choices they uniquely determine their own film experience.\n\n"}
{"id": "13251869", "url": "https://en.wikipedia.org/wiki?curid=13251869", "title": "Theatrix (role-playing game)", "text": "Theatrix (role-playing game)\n\nTheatrix is a role playing game produced by the defunct Backstage Press and no longer in print. Primarily diceless, the game includes rules for using dice to resolve actions. The game applies cinematic concepts to role-playing—the players are \"actors\" and the GM is the \"director.\" The games attempts to frame adventures in the model of screenplays, which have a structured plot consisting of a number of agreed upon acts, scenes, and \"pinch-points\". Players use plot points to guarantee success of an action or take minor control of the story. This was a relatively new and controversial concept being championed by few in mainstream RPGs at the time of its release.\n\nIn addition to their numerically-rated attributes and skills, characters possess non-numerically-rated personality traits, and unrated 'descriptors'. Descriptors can act as advantages, but in order to use them so, the player must spend plot points. They can also act as disadvantages, and when they do so, the player gains plot points. The game encourages collaborative roleplaying, using what it calls \"Distributed Directing\", whereby the players may introduce subplots though the GM remains responsible for directing the main plot. Theatrix has many methods of resolving action and conflict without dice. Most notable is the use of extensive flow-charts, printed on card-stock.\n\n"}
{"id": "539928", "url": "https://en.wikipedia.org/wiki?curid=539928", "title": "Totenkopf", "text": "Totenkopf\n\nTotenkopf (i.e. \"skull\", literally \"dead's head\") is the German word for the skull and crossbones and death's head symbols. The \"Totenkopf\" symbol is an old international symbol for death, the defiance of death, danger, or the dead, as well as piracy. It consists usually of the human skull with or without the mandible and often includes two crossed long-bones (femurs), most often depicted with the crossbones being \"behind\" some part of the skull. \n\nIt is commonly associated with 19th- and 20th-century German military use.\n\n\"Toten-Kopf\" translates literally to \"dead's head\", meaning exactly \"dead person's head\". Semantically, it refers to a skull, literally a \"Schädel\". As a term, \"Totenkopf\" connotes the human skull as a symbol, typically one with crossed thigh bones as part of a grouping.\n\nContemporary German language meaning of the word \"Totenkopf\" has not changed for at least two centuries. For example, the German poet Clemens Brentano (b. 1778 – d. 1842) wrote in the story \"Baron Hüpfenstich\":<br> \"Lauter Totenbeine und Totenköpfe, die standen oben herum ...\" (i.e. \"A lot of bones and skulls, they were placed above ...\").\n\nThe common translation of \"Totenkopf\" as \"death's head\" is incorrect; it would be \"Todeskopf\", but no such word is in use. The English term death squad is called \"Todesschwadron\", not \"Totenschwadron\". It would be a logical fallacy to conclude that usage varies only because of the German naming of the Death's-head Hawkmoth, which is called \"Skull Hawkmoth\" (\"Totenkopfschwärmer\") in German, in the same way that it would be a fallacy to conclude that the German word \"Nachtkerze\" (i.e. night candle) would mean Willowherb, just because the Willowherb Hawkmoth (\"Proserpinus proserpina\") is called \"Night Candle Hawkmoth\" (\"Nachtkerzenschwärmer\", \"Proserpinus proserpina\") in German.\n\nUse of the \"Totenkopf\" as a military emblem began under Frederick the Great, who formed a regiment of Hussar cavalry in the Prussian army commanded by Colonel von Ruesch, the Husaren-Regiment Nr. 5 (von Ruesch). It adopted a black uniform with a \"Totenkopf\" emblazoned on the front of its mirlitons and wore it on the field in the War of Austrian Succession and in the Seven Years' War. The \"Totenkopf\" remained a part of the uniform when the regiment was reformed into Leib-Husaren Regiments Nr.1 and Nr.2 in 1808. \n\nIn 1809 during the War of the Fifth Coalition, Frederick William, Duke of Brunswick-Wolfenbüttel raised a force of volunteers to fight Napoleon Bonaparte, who had conquered the Duke's lands. The Brunswick corps was provided with black uniforms, giving rise to their nickname, the Black Brunswickers. Both hussar cavalry and infantry in the force wore a \"Totenkopf\" badge, either in mourning for the duke's father, Charles William Ferdinand, Duke of Brunswick-Wolfenbüttel, who had been killed at the Battle of Jena–Auerstedt in 1806, or according to some sources, as a sign of revenge against the French. After fighting their way through Germany, the Black Brunswickers entered British service and fought with them in the Peninsular War and at the Battle of Waterloo. The Brunswick corps was eventually incorporated into the Prussian Army in 1866.\n\nThe skull continued to be used by the Prussian and Brunswick armed forces until 1918, and some of the stormtroopers that led the last German offensives on the Western Front in 1918 used skull badges. \"Luftstreitkräfte\" fighter pilots Georg von Hantelmann and Kurt Adolf Monnington are just two of a number of Central Powers military pilots who used the \"Totenkopf\" as their personal aircraft insignia.\n\nThe \"Totenkopf\" was used in Germany throughout the inter-war period, most prominently by the \"Freikorps\". In 1933, it was in use by the regimental staff and the 1st, 5th, and 11th squadrons of the \"Reichswehr\"s 5th Cavalry Regiment as a continuation of a tradition from the \"Kaiserreich\".\n\nIn the early days of the NSDAP, Julius Schreck, the leader of the \"Stabswache\" (Adolf Hitler's bodyguard unit), resurrected the use of the \"Totenkopf\" as the unit's insignia. This unit grew into the \"Schutzstaffel\" (SS), which continued to use the \"Totenkopf\" as insignia throughout its history. According to a writing by Reichsführer-SS Heinrich Himmler the \"Totenkopf\" had the following meaning:\n\nThe \"Skull\" is the reminder that you shall always be willing to put your self at stake for the life of the whole community.\n\nThe \"Totenkopf\" was also used as the unit insignia of the \"Panzer\" forces of the German \"Heer\" (Army), and also by the \"Panzer\" units of the Luftwaffe, including those of the elite Fallschirm-Panzer Division 1 Hermann Göring.\n\nBoth the 3rd SS Panzer Division of the Waffen-SS, and the World War II era Luftwaffe's 54th Bomber Wing \"Kampfgeschwader 54\" were given the unit name \"\"Totenkopf\", and used a strikingly similar-looking graphic skull-crossbones insignia as the SS units of the same name. The 3rd SS Panzer Division also had skull patches on their uniform collars instead of the SS sieg rune.\n\n\n\nIn the United States, the skull & crossbones symbol has often been used to indicate a poisonous substance.\n\n\n"}
{"id": "1906107", "url": "https://en.wikipedia.org/wiki?curid=1906107", "title": "Transtheoretical model", "text": "Transtheoretical model\n\nThe transtheoretical model of behavior change is an integrative theory of therapy that assesses an individual's readiness to act on a new healthier behavior, and provides strategies, or processes of change to guide the individual. The model is composed of constructs such as: stages of change, processes of change, levels of change, self-efficacy, and decisional balance.\n\nThe transtheoretical model is also known by the abbreviation \"TTM\" and sometimes by the term \"stages of change\", although this latter term is a synecdoche since the stages of change are only one part of the model along with processes of change, levels of change, etc. Several self-help books—\"Changing for Good\" (1994), \"Changeology\" (2012), and \"Changing to Thrive\" (2016)—and articles in the news media have discussed the model. It has been called \"arguably the dominant model of health behaviour change, having received unprecedented research attention, yet it has simultaneously attracted criticism\".\nJames O. Prochaska of the University of Rhode Island, and Carlo Di Clemente and colleagues developed the transtheoretical model beginning in 1977. It is based on analysis and use of different theories of psychotherapy, hence the name \"transtheoretical\".\n\nProchaska and colleagues refined the model on the basis of research that they published in peer-reviewed journals and books.\n\nThis construct refers to the temporal dimension of behavioural change. In the transtheoretical model, change is a \"process involving progress through a series of stages\":\n\nIn addition, the researchers conceptualized \"Relapse\" (recycling) which is not a stage in itself but rather the \"return from Action or Maintenance to an earlier stage\".\n\nThe quantitative definition of the stages of change (see below) is perhaps the most notorious feature of the model. However it is also one of the most critiqued, even in the field of smoking cessation, where it was originally formulated. It has been said that such quantitative definition (i.e. a person is in preparation if it intends to change within a month) does not reflect the nature of behaviour change, that it does not have better predictive power than simpler questions (i.e. \"do you have plans to change...\"), and that it has problems regarding its classification reliability.\n\nCommunication theorist and sociologist Everett Rogers suggested that the stages of change are analogues of the stages of the innovation adoption process in Rogers' theory of diffusion of innovations.\n\nStage 1: Precontemplation (not ready)\n\nPeople at this stage do not intend to start the healthy behavior in the near future (within 6 months), and may be unaware of the need to change. People here learn more about healthy behavior: they are encouraged to think about the pros of changing their behavior and to feel emotions about the effects of their negative behavior on others.\n\nPrecontemplators typically underestimate the pros of changing, overestimate the cons, and often are not aware of making such mistakes.\n\nOne of the most effective steps that others can help with at this stage is to encourage them to become more mindful of their decision making and more conscious of the multiple benefits of changing an unhealthy behavior.\n\nStage 2: Contemplation (getting ready)\n\nAt this stage, participants are intending to start the healthy behavior within the next 6 months. While they are usually now more aware of the pros of changing, their cons are about equal to their Pros. This ambivalence about changing can cause them to keep putting off taking action.\n\nPeople here learn about the kind of person they could be if they changed their behavior and learn more from people who behave in healthy ways.\n\nOthers can influence and help effectively at this stage by encouraging them to work at reducing the cons of changing their behavior.\n\nStage 3: Preparation (ready)\n\nPeople at this stage are ready to start taking action within the next 30 days. They take small steps that they believe can help them make the healthy behavior a part of their lives. For example, they tell their friends and family that they want to change their behavior.\n\nPeople in this stage should be encouraged to seek support from friends they trust, tell people about their plan to change the way they act, and think about how they would feel if they behaved in a healthier way. Their number one concern is: when they act, will they fail? They learn that the better prepared they are, the more likely they are to keep progressing.\n\nStage 4: Action (current action)\n\nPeople at this stage have changed their behavior within the last 6 months and need to work hard to keep moving ahead. These participants need to learn how to strengthen their commitments to change and to fight urges to slip back.\n\nPeople in this stage progress by being taught techniques for keeping up their commitments such as substituting activities related to the unhealthy behavior with positive ones, rewarding themselves for taking steps toward changing, and avoiding people and situations that tempt them to behave in unhealthy ways.\n\nStage 5: Maintenance (monitoring)\n\nPeople at this stage changed their behavior more than 6 months ago. It is important for people in this stage to be aware of situations that may tempt them to slip back into doing the unhealthy behavior—particularly stressful situations.\n\nIt is recommended that people in this stage seek support from and talk with people whom they trust, spend time with people who behave in healthy ways, and remember to engage in healthy activities to cope with stress instead of relying on unhealthy behavior.\n\nRelapse (recycling)\n\nRelapse in the TTM specifically applies to individuals who successfully quit smoking or using drugs or alcohol, only to resume these unhealthy behaviors. Individuals who attempt to quit highly addictive behaviors such as drug, alcohol, and tobacco use are at particularly high risk of a relapse. Achieving a long-term behavior change often requires ongoing support from family members, a health coach, a physician, or another motivational source. Supportive literature and other resources can also be helpful to avoid a relapse from happening.\n\nThe 10 processes of change are \"covert and overt activities that people use to progress through the stages\".\n\nTo progress through the early stages, people apply cognitive, affective, and evaluative processes. As people move toward Action and Maintenance, they rely more on commitments, conditioning, contingencies, environmental controls, and support.\n\nProchaska and colleagues state that their research related to the transtheoretical model shows that interventions to change behavior are more effective if they are \"stage-matched\", that is, \"matched to each individual's stage of change\".\n\nIn general, for people to progress they need:\n\nThe ten \"processes of change\" include:\n\nHealth researchers have extended Prochaska's and DiClemente's 10 original processes of change by an additional 21 processes. In the first edition of \"Planning Health Promotion Programs\", Bartholomew et al. (2006) summarised the processes that they identified in a number of studies; however, their extended list of processes was removed from later editions of the text. The additional processes of Bartholomew et al. were:\n\nWhile most of these processes are associated with health interventions such as smoking cessation and other addictive behaviour, some of them are also used in travel interventions. Depending on the target behaviour the effectiveness of the process should differ. Also some processes are recommended in a specific stage, while others can be used in one or more stages. Recently, these processes have been identified in travel interventions, broadening the scope of TTM in other research domains.\n\nThis core construct \"reflects the individual's relative weighing of the pros and cons of changing\". Decision making was conceptualized by Janis and Mann as a \"decisional balance sheet\" of comparative potential gains and losses. Decisional balance measures, the pros and the cons, have become critical constructs in the transtheoretical model. The pros and cons combine to form a decisional \"balance sheet\" of comparative potential gains and losses. The balance between the pros and cons varies depending on which stage of change the individual is in.\n\nSound decision making requires the consideration of the potential benefits (pros) and costs (cons) associated with a behavior's consequences. TTM research has found the following relationships between the pros, cons, and the stage of change across 48 behaviors and over 100 populations studied.\n\nThe evaluation of pros and cons is part of the formation of attitudes. Attitude is defined as a \"psychological tendency that is expressed by evaluating a particular entity with some degree of favour or disfavour\". This means that by evaluating pros and cons we form a positive or negative attitude about something or someone. During the change process individuals gradually shift from cons to pros, forming a more positive attitude towards the target behaviour. Attitudes are one of the core constructs explaining behaviour and behaviour change in various research domains. Other behaviour models, such as the theory of planned behavior (TPB) and the stage model of self-regulated change, also emphasise attitude as an important determinant of behaviour. The progression through the different stages of change is reflected in a gradual change in attitude before the individual acts. Most of the processes of change aim at evaluating and reevaluating as well as reinforcing specific elements of the current and target behaviour. The processes of change contribute to a great degree on attitude formation.\n\nDue to the synonymous use of decisional balance and attitude, travel behaviour researchers have begun to combine the TTM with the TPB. Forward uses the TPB variables to better differentiate the different stages. Especially all TPB variables (attitude, perceived behaviour control, descriptive and subjective norm) are positively show a gradually increasing relationship to stage of change for bike commuting. As expected, intention or willingness to perform the behaviour increases by stage. Similarly, Bamberg uses various behavior models, including the transtheoretical model, theory of planned behavior and norm-activation model, to build the stage model of self-regulated behavior change (SSBC). Bamberg claims that his model is a solution to criticism raised towards the TTM. Some researchers in travel, dietary, and environmental research have conducted empirical studies, showing that the SSBC might be a future path for TTM-based research.\n\nThis core construct is \"the situation-specific confidence people have that they can cope with high-risk situations without relapsing to their unhealthy or high risk-habit\". The construct is based on Bandura's self-efficacy theory and conceptualizes a person's perceived ability to perform on a task as a mediator of performance on future tasks. In his research Bandura already established that greater levels of perceived self-efficacy leads to greater changes in behavior. Similarly, Ajzen mentions the similarity between the concepts of self-efficacy and perceived behavioral control. This underlines the integrative nature of the transtheoretical model which combines various behavior theories. A change in the level of self-efficacy can predict a lasting change in behavior if there are adequate incentives and skills. The transtheoretical model employs an overall confidence score to assess an individual's self-efficacy. Situational temptations assess how tempted people are to engage in a problem behavior in a certain situation.\n\nThis core construct identifies the depth or complexity of presenting problems according to five levels of increasing complexity. Different therapeutic approaches are recommended for each level as well as for each stage of change. The levels are:\n\n\nThe outcomes of the TTM computerized tailored interventions administered to participants in pre-Action stages are outlined below.\n\nA national sample of pre-Action adults was provided a stress management intervention. At the 18-month follow-up, a significantly larger proportion of the treatment group (62%) was effectively managing their stress when compared to the control group. The intervention also produced statistically significant reductions in stress and depression and an increase in the use of stress management techniques when compared to the control group. Two additional clinical trials of TTM programs by Prochaska et al. and Jordan et al. also found significantly larger proportions of treatment groups effectively managing stress when compared to control groups.\n\nOver 1,000 members of a New England group practice who were prescribed antihypertensive medication participated in an adherence to antihypertensive medication intervention. The vast majority (73%) of the intervention group who were previously pre-Action were adhering to their prescribed medication regimen at the 12-month follow-up when compared to the control group.\n\nMembers of a large New England health plan and various employer groups who were prescribed a cholesterol lowering medication participated in an adherence to lipid-lowering drugs intervention. More than half of the intervention group (56%) who were previously pre-Action were adhering to their prescribed medication regimen at the 18-month follow-up. Additionally, only 15% of those in the intervention group who were already in Action or Maintenance relapsed into poor medication adherence compared to 45% of the controls. Further, participants who were at risk for physical activity and unhealthy diet were given only stage-based guidance. The treatment group doubled the control group in the percentage in Action or Maintenance at 18 months for physical activity (43%) and diet (25%).\n\nParticipants were 350 primary care patients experiencing at least mild depression but not involved in treatment or planning to seek treatment for depression in the next 30 days. Patients receiving the TTM intervention experienced significantly greater symptom reduction during the 9-month follow-up period. The intervention's largest effects were observed among patients with moderate or severe depression, and who were in the Precontemplation or Contemplation stage of change at baseline. For example, among patients in the Precontemplation or Contemplation stage, rates of reliable and clinically significant improvement in depression were 40% for treatment and 9% for control. Among patients with mild depression, or who were in the Action or Maintenance stage at baseline, the intervention helped prevent disease progression to Major Depression during the follow-up period.\n\nFifty-hundred-and-seventy-seven overweight or moderately obese adults (BMI 25-39.9) were recruited nationally, primarily from large employers. Those randomly assigned to the treatment group received a stage-matched multiple behavior change guide and a series of tailored, individualized interventions for three health behaviors that are crucial to effective weight management: healthy eating (i.e., reducing calorie and dietary fat intake), moderate exercise, and managing emotional distress without eating. Up to three tailored reports (one per behavior) were delivered based on assessments conducted at four time points: baseline, 3, 6, and 9 months. All participants were followed up at 6, 12, and 24 months. Multiple Imputation was used to estimate missing data. Generalized Labor Estimating Equations (GLEE) were then used to examine differences between the treatment and comparison groups. At 24 months, those who were in a pre-Action stage for healthy eating at baseline and received treatment were significantly more likely to have reached Protons or Maintenance than the comparison group (47.5% vs. 34.3%). The intervention also impacted a related, but untreated behavior: fruit and vegetable consumption. Over 48% of those in the treatment group in a pre-Action stage at baseline progressed to Action or Maintenance for eating at least 5 servings a day of fruit and vegetables as opposed to 39% of the comparison group. Individuals in the treatment group who were in a pre-Action stage for exercise at baseline were also significantly more likely to reach Action or Maintenance (44.9% vs. 38.1%). The treatment also had a significant effect on managing emotional distress without eating, with 49.7% of those in a pre-Action stage at baseline moving to Action or Maintenance versus 30.3% of the comparison group. The groups differed on weight lost at 24 months among those in a pre-action stage for healthy eating and exercise at baseline. Among those in a pre-Action stage for both healthy eating and exercise at baseline, 30% of those randomized to the treatment group lost 5% or more of their body weight vs.18.6% in the comparison group. Coaction of behavior change occurred and was much more pronounced in the treatment group with the treatment group losing significantly more than the comparison group. This study demonstrates the ability of TTM-based tailored feedback to improve healthy eating, exercise, managing emotional distress, and weight on a population basis. The treatment produced the highest population impact to date on multiple health risk behaviors.\n\nMultiple studies have found individualized interventions tailored on the 14 TTM variables for smoking cessation to effectively recruit and retain pre-Action participants and produce long-term abstinence rates within the range of 22% – 26%. These interventions have also consistently outperformed alternative interventions including best-in-class action-oriented self-help programs, non-interactive manual-based programs, and other common interventions. Furthermore, these interventions continued to move pre-Action participants to abstinence even after the program ended. For a summary of smoking cessation clinical outcomes, see Velicer, Redding, Sun, & Prochaska, 2007 and Jordan, Evers, Spira, King & Lid, 2013.\n\nIn the treatment of smoke control, TTM focuses on each stage to monitor and to achieve a progression to the next stage.\n\nIn each stage, a patient may have multiple sources that could influence their behavior. These may include: friends, books, and interactions with their healthcare providers. These factors could potentially influence how successful a patient may be in moving through the different stages. This stresses the importance to have continuous monitoring and efforts to maintain progress at each stage. TTM helps guide the treatment process at each stage, and may assist the healthcare provider in making an optimal therapeutic decision.\n\nThe use of TTM in travel behaviour interventions is rather novel. A number of cross-sectional studies investigated the individual constructs of TTM, e.g. stage of change, decisional balance and self-efficacy, with regards to transport mode choice. The cross-sectional studies identified both motivators and barriers at the different stages regarding biking, walking and public transport. The motivators identified were e.g. liking to bike/walk, avoiding congestion and improved fitness. Perceived barriers were e.g. personal fitness, time and the weather. This knowledge was used to design interventions that would address attitudes and misconceptions to encourage an increased use of bikes and walking. These interventions aim at changing people's travel behaviour towards more sustainable and more active transport modes. In health-related studies, TTM is used to help people walk or bike more instead of using the car. Most intervention studies aim to reduce car trips for commute to achieve the minimum recommended physical activity levels of 30 minutes per day. Other intervention studies using TTM aim to encourage sustainable behaviour. By reducing single occupied motor vehicle and replacing them with so called sustainable transport (public transport, car pooling, biking or walking), greenhouse gas emissions can be reduced considerably. A reduction in the number of cars on our roads solves other problems such as congestion, traffic noise and traffic accidents. By combining health and environment related purposes, the message becomes stronger. Additionally, by emphasising personal health, physical activity or even direct economic impact, people see a direct result from their changed behaviour, while saving the environment is a more general and effects are not directly noticeable.\n\nDifferent outcome measures were used to assess the effectiveness of the intervention. Health-centred intervention studies measured BMI, weight, waist circumference as well as general health. However, only one of three found a significant change in general health, while BMI and other measures had no effect. Measures that are associated with both health and sustainability were more common. Effects were reported as number of car trips, distance travelled, main mode share etc. Results varied due to greatly differing approaches. In general, car use could be reduced between 6% and 55%, while use of the alternative mode (walking, biking and/or public transport) increased between 11% and 150%. These results indicate a shift to action or maintenance stage, some researchers investigated attitude shifts such as the willingness to change. Attitudes towards using alternative modes improved with approximately 20% to 70%. Many of the intervention studies did not clearly differentiate between the five stages, but categorised participants in pre-action and action stage. This approach makes it difficult to assess the effects per stage. Also, interventions included different processes of change; in many cases these processes are not matched to the recommended stage. It highlights the need to develop a standardised approach for travel intervention design. Identifying and assessing which processes are most effective in the context of travel behaviour change should be a priority in the future in order to secure the role of TTM in travel behaviour research.\n\nThe TTM has been called \"arguably the dominant model of health behaviour change, having received unprecedented research attention, yet it has simultaneously attracted criticism\". Depending on the field of application (e.g. smoking cessation, substance abuse, condom use, diabetes treatment, obesity and travel) somewhat different criticisms have been raised.\n\nIn a systematic review, published in 2003, of 23 randomized controlled trials, the authors found that \"stage based interventions are no more effective than non-stage based interventions or no intervention in changing smoking behaviour. However, it was also mentioned that stage based interventions are often used and implemented inadequately in practice. Thus, criticism is directed towards the use rather the effectiveness of the model itself. Looking at interventions targeting smoking cessation in pregnancy found that stage-matched interventions were more effective than non-matched interventions. One reason for this was the greater intensity of stage-matched interventions. Also, the use of stage-based interventions for smoking cessation in mental illness proved to be effective. Further studies, e.g. a randomized controlled trial published in 2009, found no evidence that a TTM based smoking cessation intervention was more effective than a control intervention not tailored to stage of change. The study claims that those not wanting to change (i.e. precontemplators) tend to be responsive to neither stage nor non-stage based interventions. Since stage-based interventions tend to be more intensive they appear to be most effective at targeting contemplators and above rather than pre-contemplators. A 2010 systematic review of smoking cessation studies under the auspices of the Cochrane Collaboration found that \"stage-based self-help interventions (expert systems and/or tailored materials) and individual counselling were neither more nor less effective than their non-stage-based equivalents.\n\nMain criticism is raised regarding the \"arbitrary dividing lines\" that are drawn between the stages. West claimed that a more coherent and distinguishable definition for the stages is needed. Especially the fact that the stages are bound to a specific time interval is perceived to be misleading. Additionally, the effectiveness of stage-based interventions differs depending on the behavior. A continuous version of the model has been proposed, where each process is first increasingly used, and then decreases in importance, as smokers make progress along some latent dimension. This proposal suggests the use of processes without reference to stages of change.\n\nThe model \"assumes that individuals typically make coherent and stable plans\", when in fact they often do not.\n\nWithin research on prevention of pregnancy and sexually transmitted diseases a systematic review from 2003 comes to the conclusion that \"no strong conclusions\" can be drawn about the effectiveness of interventions based on the transtheoretical model. Again this conclusion is reached due to the inconsistency of use and implementation of the model. This study also confirms that the better stage-matched the intervention the more effect it has to encourage condom use.\n\nWithin the health research domain, a 2005 systematic review of 37 randomized controlled trials claims that \"there was limited evidence for the effectiveness of stage-based interventions as a basis for behavior change. Studies with which focused on increasing physical activity levels through active commute however showed that stage-matched interventions tended to have slightly more effect than non-stage matched interventions. Since many studies do not use all constructs of the TTM, additional research suggested that the effectiveness of interventions increases the better it is tailored on all core constructs of the TTM in addition to stage of change. In diabetes research the \"existing data are insufficient for drawing conclusions on the benefits of the transtheoretical model\" as related to dietary interventions. Again, studies with slightly different design, e.g. using different processes, proved to be effective in predicting the stage transition of intention to exercise in relation to treating patients with diabetes.\n\nTTM has generally found a greater popularity regarding research on physical activity, due to the increasing problems associated with unhealthy diets and sedentary living, e.g. obesity, cardiovascular problems. A 2011 Cochrane Systematic Review found that there is little evidence to suggest that using the Transtheoretical Model Stages of Change (TTM SOC) method is effective in helping obese and overweight people lose weight. Earlier in a 2009 paper, the TTM was considered to be useful in promoting physical activity. In this study, the algorithms and questionnaires that researchers used to assign people to stages of change lacked standardisation to be compared empirically, or validated.\n\nSimilar criticism regarding the standardisation as well as consistency in the use of TTM is also raised in a recent review on travel interventions. With regard to travel interventions only stages of change and sometimes decisional balance constructs are included. The processes used to build the intervention are rarely stage-matched and short cuts are taken by classifying participants in a pre-action stage, which summarises the precontemplation, contemplation and preparation stage, and an action/maintenance stage. More generally, TTM has been criticised within various domains due to the limitations in the research designs. For example, many studies supporting the model have been cross-sectional, but longitudinal study data would allow for stronger causal inferences. Another point of criticism is raised in a 2002 review, where the model's stages were characterized as \"not mutually exclusive\". Furthermore, there was \"scant evidence of sequential movement through discrete stages\". While research suggests that movement through the stages of change is not always linear, a study conducted in 1996 demonstrated that the probability of forward stage movement is greater than the probability of backward stage movement. Due to the variations in use, implementation and type of research designs, data confirming TTM are ambiguous. More care has to be taken in using a sufficient amount of constructs, trustworthy measures, and longitudinal data.\n\n\nThe following notes summarize major differences between the well-known 1983, 1992, and 1997 versions of the model. Other published versions may contain other differences. For example, Prochaska, Prochaska, and Levesque (2001) do not mention the Termination stage, Self-efficacy, or Temptation.\n\n"}
{"id": "223321", "url": "https://en.wikipedia.org/wiki?curid=223321", "title": "Worse is better", "text": "Worse is better\n\nWorse is better, also called New Jersey style, was conceived by Richard P. Gabriel in an essay \"Worse is better\" to describe the dynamics of software acceptance, but it has broader application. It is the idea that quality does not necessarily increase with functionality—that there is a point where less functionality (\"worse\") is a preferable option (\"better\") in terms of practicality and usability. Software that is limited, but simple to use, may be more appealing to the user and market than the reverse.\n\nAs to the oxymoronic title, Gabriel calls it a caricature, declaring the style bad in comparison with \"The Right Thing\". However he also states that \"it has better survival characteristics than the-right-thing\" development style and is superior to the \"MIT Approach\" with which he contrasted it in the original essay.\n\nGabriel was a Lisp programmer when he formulated the concept in 1989, presenting it in his essay \"Lisp: Good News, Bad News, How to Win Big\". A section of the article, titled \"The Rise of 'Worse is Better'\", was widely disseminated beginning in 1991, after Jamie Zawinski found it in Gabriel's files at Lucid Inc. and e-mailed it to friends and colleagues.\n\nIn \"The Rise of Worse is Better\", Gabriel claimed that \"Worse-is-Better\" is a model of software design and implementation which has the following characteristics (in approximately descending order of importance):\n\n\nGabriel argued that early Unix and C, developed by Bell Labs, are examples of this design approach.\n\nGabriel contrasted his philosophy with what he called the \"MIT/Stanford style of design\" or \"MIT approach\" (also known as \"the Right Thing\"), which he described as follows. Contrasts are in bold:\n\n\nGabriel argued that \"Worse is better\" produced more \"successful\" software than the MIT approach: As long as the initial program is basically good, it will take much less time and effort to implement initially and it will be easier to adapt to new situations. Porting software to new machines, for example, becomes far easier this way. Thus its use will spread rapidly, long before a program developed using the MIT approach has a chance to be developed and deployed (first-mover advantage). Once it has spread, there will be pressure to improve its functionality, but users have already been conditioned to accept \"worse\" rather than the \"right thing\". \"Therefore, the worse-is-better software first will gain acceptance, second will condition its users to expect less, and third will be improved to a point that is almost the right thing. In concrete terms, even though Lisp compilers in 1987 were about as good as C compilers, there are many more compiler experts who want to make C compilers better than want to make Lisp compilers better.\"\n\nGabriel credits Jamie Zawinski for excerpting the worse-is-better sections of \"Lisp: Good News, Bad News, How to Win Big\" and e-mailing them to his friends at Carnegie Mellon University, who sent them to their friends at Bell Labs, \"who sent them to their friends everywhere\". He apparently connected these ideas to those of Richard Stallman and saw related ideas that are important in the design philosophy of Unix, and more generally in the open-source movement, both of which were central to the development of Linux.\n\nGabriel later answered his earlier essay with one titled \"Worse Is Better Is Worse\" under the pseudonym \"Nickieben Bourbaki\" (an allusion to Nicolas Bourbaki).\n\n\n"}
