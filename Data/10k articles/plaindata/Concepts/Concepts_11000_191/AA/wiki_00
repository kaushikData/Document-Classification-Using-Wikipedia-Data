{"id": "12267953", "url": "https://en.wikipedia.org/wiki?curid=12267953", "title": "6-sphere coordinates", "text": "6-sphere coordinates\n\nIn mathematics, 6-sphere coordinates are the coordinate system created by inverting the Cartesian coordinates across the unit sphere. They are so named because the loci where one coordinate is constant form spheres tangent to the origin from one of six sides (depending on which coordinate is held constant and whether its value is positive or negative).\n\nThe three coordinates are\nSince inversion is its own inverse, the equations for \"x\", \"y\", and \"z\" in terms of \"u\", \"v\", and \"w\" are similar:\nThis coordinate system is formula_3-separable for the 3-variable Laplace equation.\n\n\n"}
{"id": "41711392", "url": "https://en.wikipedia.org/wiki?curid=41711392", "title": "Abhasavada", "text": "Abhasavada\n\nAbhasavada (Sanskrit: आभासवाद) is the term derived from the word Abhasa meaning mere or fallacious appearance, reflection, looking like, light, semblance of reason, intention. In Hindu philosophy this term refers to the Theory of Appearance, both of the Shaivite school and the Advaita Vedanta, though with differing connotations.\n\nThe Shaivites rely on Maheshvaraya (Sovereignty of Will) of Shiva, the creator-sustainer-destroyer to explain Creation. Jnanadikara deals with two theories a) Svatantryavada and b) Abhasavada to explain Shiva’s volitional power. The whole creation or manifestation is the result of the Kriya Sakti of the Lord who becomes Nirmana Sakti (constituent power) owing to the operation of three laws viz. the law of Division (bheda-bheda), the law of Perception (mana-tat-phala-meya) and the law of Causation (Karya karana, Kriya Sakti). Svatantryavada or the universal voluntarism is the chief doctrine of the Pratyabhijna system; it is the doctrine of self-dependence or sovereignty of the Lord’s Will which imparts impetus to the process of the world. It explains the creative power in Nature and multiplication of effect. This theory replaced Arambhavada (theory of Realistic creation), Parinamavada (theory of transformation) and Vivartavada (theory of Manifestation). Abhasavada is the Pratyabhijna’s theory of Manifestation, propounded by Utpalacarya and influenced Abhinavagupta, which explains Monism and holds the world objects as manifestations or Abhasas, and the view that it is the very nature of Shiva, the Supreme Cause (Parma Shiva), to manifest Himself in diverse forms of the universe, that the whole universe is an abhasa of Shiva. It recognizes the truth that appearance as appearance or as process of the world, is real, the appearance is not a superimposition on Shiva actively involved in free spontaneous kriya of creation. Prakrti is projection of the free-will of Shiva.\n\nIn the Advaita Vedanta version, Abhasavada, the theory of appearance advocated by Suresvara, holds that the individual soul is merely an illusory appearance – a projection – of Brahman-intelligence. According to this school of thought championed by Sankara, at the level of Consciousness Jiva and Ishvara are considered to be mere reflection or appearance of the One Impartite Brahman; because they are identical with Brahman they have no separate identity of their own. Suresvara maintains that Jivas are as real as Brahman, they being primary appearances in and through avidya, while the objects of the world are unreal, they being secondary appearances, the mere reflections of the primary appearances. Reality thus appearing in Avidya is the cause of all further outward appearances by way of phenomenal or empirical entities, recognized as illusions. Sankara holds the view that Avidya or Maya, the metaphysical Ignorance, is of the nature of a superimposition of Self on the Not-self (Anatman), the real on the real and vice versa, there cannot be superimposition on the empty void. The creation of the universe is nothing but self-creation (Brahma Sutra I.iv.26); Brahman creates all things by transforming Itself into all things. Pratibimbavada, the theory of reflection, evolved from Abhasavada. Padamapada had as basis the fact that Awareness is identical to the original as in Tat Tvam Asi in which mahavakya there is the identification of anidamamsa (pure Awareness) with Brahman.\n"}
{"id": "18143904", "url": "https://en.wikipedia.org/wiki?curid=18143904", "title": "Aboutness", "text": "Aboutness\n\nAboutness is a term used in library and information science (LIS), linguistics, philosophy of language, and philosophy of mind. In LIS, it is often considered synonymous with subject (documents). In the philosophy of mind it has been often considered synonymous with intentionality, perhaps since John Searle (1983). In the philosophy of logic and language it is understood as the way a piece of text relates to a subject matter or topic.\n\nR. A. Fairthorne (1969) is credited with coining the exact term \"aboutness\", which became popular in LIS since the late 1970s, perhaps due to arguments put forward by William John Hutchins (1975, 1977, 1978). Hutchins argued that \"aboutness\" was to be preferred to \"subject\" because it removed some epistemological problems. Birger Hjørland (1992, 1997) argued, however, that the same epistemological problems also were present in Hutchins' proposal, why \"aboutness\" and \"subject\" should be considered synonymous.\n\nWhile information scientists may well be concerned with the literary aboutness (John Hutchins, 1975, 1977, 1978), philosophers of mind and psychologists with the psychological or intentional aboutness (John Searle, 1983) and language of thought (Jerry Fodor, 1975), and semantic externalists with the external state of affairs (Hilary Putnam, 1975). These seminal perspectives are respectively analogous to Ogden and Richards' \"literary, psychological, and external contexts\" (1923), as well as Karl Popper's World 1, 2, and 3 (1977).\n\n\n"}
{"id": "41890659", "url": "https://en.wikipedia.org/wiki?curid=41890659", "title": "Aishvarya", "text": "Aishvarya\n\nAishvarya (Sanskrit: ऐश्वर्य) which is a noun, means lordship or sovereignty, prosperity or royal or exalted rank. Prosperity, power and recognition by society are the three aspects of man’s life that constitute \"aishvarya\" which term also refers to the \"aishvarya\" or greatness of God and of Brahman.\n\nThe word \"Aishvarya\" is derived from the word ईश meaning supreme, powerful, lord or master or God as in the phrase - ईशावास्यमिदं सर्वं – God certainly resides in all this (Isha Upanishad Mantra 1). It is directly connected with one’s ego at the individual level, and with the assumed nature of God.\n\nProsperity, power and recognition by society are the three aspects of man’s life that constitute \"aishvarya\". They are the basic needs and aspirations of man about which he dreams and he plans and for achieving which objectives he performs varying deeds. He also eagerly strives to know about the timing and extent of their fulfillment for which he has devised different means and methods. In Hindu astrology, \"Vaibhava\" ('opulence'), which includes \"Prabhava\" ('influence'), \"Dhana\" ('wealth') and \"Aishvarya\" ('magnificence'), is represented by the 6th house, the 9th house and the 11th house from the lagna and their respective lords. Mantreswara in the Chapter XX of his Phaladeepika states that during the dasha of the strong lord of the 6th house one gains \"aishvarya\" and crushes foes, during the \"dasha\" of the strong lord of the 9th house a person enjoys \"aishvarya\", and during the \"dasha\" of the strong lord of the 11th house one experiences constant increase of \"aishvarya\".\n\nShakti as Goddess Annapoorna fulfills the most basic physiological need of man that of food items and clothes; as Goddess Durga Shakti fulfills the needs such as shelter and safety from natural and man-made threats; as Goddess Lakshmi and Goddess Saraswati Shakti fulfills the social needs such as education, social status and recognition in the society; out of the eight forms of Lakshmi, \"Aishvarya Lakshmi\" refers to riches and \"Dhana Lakshmi\", to gold and money.\n\nThe sage of the Shvetashvatara Upanishad describes the all-pervading aishvarya or greatness of the Lord (Brahman) in the following words:\n\nThe wheel is the \"Brahma-chakra\", the repeated cycle of origination and destruction of inanimate things, and of birth and death of all living beings; the rim is the singular non-dual support of the \"Karya Jagat\" which is the unreal whole world of phenomenon of effects and has Maya as its source. The \"Karya Jagat\" is covered by the three Gunasi.e. by (Sattva, Rajas and Tamas), and their sixteen transformations or manifestations (the five primordial elements, the mind, the five sense organs and the five organs of action) which give satisfaction and pleasure through contacts with objects and constitute the Prakrti ashtakam (existence and awareness of objects), the Dhātu ashtakam (the contact of senses with the objects) and the Aishvarya ashtakam (the psychological forces which bind and cause one to rotate in samsara) which are the three kinds of bondages. The fifty spokes are the fifty psychological forces or misconceptions due to Maya. The one bondage of infinite forms is the fundamental bondage consisting of Vāsanā i.e. desire or emotion. There are the eight siddhis or successes. \"Righteousness\", \"Unrighteousness\" and \"Knowledge\" are the three paths, and virtue and vice are the two factors that cause delusion.\n\nBhagavad Gita in its own way, presents Krishna as the Supreme Godhead and the supreme object of worship. Krishna tells Arjuna:-\n\n\"“Nor do beings exist (in reality) in Me – Behold My Divine Yoga supporting all beings, but not dwelling in them, I am My Self, the efficient cause of all beings.” - Bhagavad Gita IX.5.\n\nIn this regard, Chinmayananda explains that –“In Pure Awareness, in Its Infinite Nature of sheer Knowledge, there never was, never is and never can be any world of pluralistic embodiment. Pure Consciousness, Divine and Eternal, is the substratum that sustains and illumines the entire panorama of the ever-changing plurality.” and Prabhupada explains that the Lord is everywhere present by His personal representation, the diffusion of His different energies because of which creation takes place and therefore all things rests on Him but He is different from all things, this is the \"yogam aisvaram\", the mystic power of God. Jayadayal Goyandaka explains that when a man realizes God, then nothing exists in his conception of God; therefore in the eyes of him who has attained this state, the world does not exist in God, in reality nothing exists but God. The words \"Aisvaram yogam\" denote the wonderful power of God, that of remaining absolutely detached from everything, and the words \"MaMa Atma\" refer to His qualified, formless aspect.\n\nThe Shaivites know \"aishvarya\" or \"Aishvarya-tattva\" as the \"Ishvara-tattva\", the \"tattva\" of realizing what constitutes the Lordliness and the Glory of the Divine Being. It is the stage which succeeds Sada-Shiva Tattva as the stage of making a full survey of, identification with, what constitutes the state of the Experiencer, of the pure and undivided \"this\" aspect of his being as a whole, of the Ideal Universe hitherto lurking as an indistinct picture in the background of the Being. The \"Sadakhya\" or the \"Sada-Shiva Tattva\" state is Jnana, the power of being conscious or the experience of the \"I\", and \"Aishvarya\" or the \"Ishvara-tattva\" is true identification or the experience of being identified with and merged into the \"this\" of the \"vakya\" \"I am This\". It is the fourth step in the evolution of mental aspects of universal manifestation.\n\nAishvarya is the Sanskrit term derived from Ishvara that refers to the opulence and power of God. According to Gaudiya Vaishnavism, God has both a public face and a private face. Manifesting his power and majesty (\"aishvarya\"), he is known as Narayana and is served in awe and reverence, when his beauty and sweetness (\"madhurya\") overshadows his majesty he is known as \"Krishna-aishvarya\" (God’s supreme divinity and power) is one of the general dimensions of Krishna’s Divinity described by Chaitanya school, the other two being – \"madhurya\" ('divine tenderness and intimacy') and \"karunya\" ('compassion and protection'). This school favours the \"madhurya\" aspect rather than \"aishvarya\" aspect in which the Jiva does not experience a love for God as exalted as is experienced by the devotee in \"madhurya\" aspect. Manifestations of the \"aishvarya\" aspect assist in the realization of Krishna’s \"madhurya\" aspect.\nAccording to Vishnu Purana, \"Aishvarya\" ('Omnipotence or Controlling power or Transcendent majesty' ) is one of the six folds of God’s majesty, the other five being – Dharma or Virya ('Virtue or Potency or Creative power'), Yasha ('Glory, Fame, Universal honour'), Sri ('Beauty, Prosperity or Radiant beauty'), Jnana ('Omniscience, Knowledge, Omniscient knowledge') and Vairagya ('Non-affectedness or Dispassion or Renunciation or Serene dispassion'), which attributes eternally reside in God in the relation known as \"samavaya-sambandha\" ('perpetual co-inherence'), the inseparable relation that exists between substance and quality. God’s divine potency, inconceivable to human mind, is natural to Him and constitutes His essence; God’s relation with his Divine potency is one of inconceivable difference in non-difference known as \"achintya-bhedabheda\", the recognition of the nature of which relation is Chaitanya’s philosophy of \"achintya bhedabheda-vada\".\nGod’s \"aishvarya\" includes his embodied life as the universe and his avatars descending into it. Govinda which term means the Indra of cows and the rescuer of earth that was taken to a secret place also refers to the late evening and Phalguna-masa (month) as does Madhava which refers to early morning and Magha-masa (month). Govinda is \"aishvarya\" and Madhava is \"virya\".\n"}
{"id": "10800111", "url": "https://en.wikipedia.org/wiki?curid=10800111", "title": "Bilingual communes in Poland", "text": "Bilingual communes in Poland\n\nThe bilingual status of gminas (municipalities) in Poland is regulated by the \"Act of 6 January 2005 on National and Ethnic Minorities and on the Regional Languages\", which permits certain gminas with significant linguistic minorities to introduce a second, auxiliary language to be used in official contexts alongside Polish. So far 44 gminas have done this:\n\nPolish/German bilingual gminas (\"Gemeinden\") in\n\nOther gminas in Opole Voivodeship and Silesian Voivodeship which would be permitted by the Act to make German an auxiliary language are Olesno and Pawłowiczki.\n\nPolish/Kashubian bilingual gminas in Pomeranian Voivodeship:\n\nPolish/Lithuanian bilingual gmina in Podlaskie Voivodeship:\n\nPolish/Belarusian bilingual gmina in Podlaskie Voivodeship:\n\n\nPolish/Lemko bilingual names of localities in Małopolskie Voivodeship:\n\n"}
{"id": "7344", "url": "https://en.wikipedia.org/wiki?curid=7344", "title": "Cogito, ergo sum", "text": "Cogito, ergo sum\n\nThis proposition became a fundamental element of Western philosophy, as it purported to form a secure foundation for knowledge in the face of radical doubt. While other knowledge could be a figment of imagination, deception, or mistake, Descartes asserted that the very act of doubting one's own existence served—at minimum—as proof of the reality of one's own mind; there must be a thinking entity—in this case the self—for there to be a thought.\n\nDescartes first wrote the phrase in French in his 1637 \"Discourse on the Method\". He referred to it in Latin without explicitly stating the familiar form of the phrase in his 1641 \"Meditations on First Philosophy\". The earliest written record of the phrase in Latin is in his 1644 \"Principles of Philosophy\", where, in a margin note (see below), he provides a clear explanation of his intent: \"[W]e cannot doubt of our existence while we doubt\". Fuller forms of the phrase are attributable to other authors.\n\nThe phrase first appeared (in French) in Descartes's 1637 \"Discourse on the Method\" in the first paragraph of its fourth part:\n\nIn 1641, Descartes published (in Latin) \"Meditations on first philosophy\" in which he referred to the proposition, though not explicitly as \"cogito, ergo sum\" in Meditation II:\n\nIn 1644, Descartes published (in Latin) his \"Principles of Philosophy\" where the phrase \"ego cogito, ergo sum\" appears in Part 1, article 7:\n\nDescartes's margin note for the above paragraph is:\n\nDescartes, in a lesser-known posthumously published work dated as written ca. 1647 and titled (\"The Search for Truth by Natural Light\"), wrote:\n\nThe proposition is sometimes given as . This fuller form was penned by the eloquent French literary critic, Antoine Léonard Thomas, in an award-winning 1765 essay in praise of Descartes, where it appeared as In English, this is \"Since I doubt, I think; since I think, I exist\"; with rearrangement and compaction, \"I doubt, therefore I think, therefore I am\", or in Latin, \"dubito, ergo cogito, ergo sum\".\n\nA further expansion, (\"…—a thinking thing\") extends the \"cogito\" with Descartes's statement in the subsequent Meditation, , or, in English, \"I am a thinking (conscious) thing, that is, a being who doubts, affirms, denies, knows a few objects, and is ignorant of many …\". This has been referred to as \"the expanded \"cogito\"\".\n\nNeither nor indicate whether the verb form corresponds to the English simple present or progressive aspect. Translation needs a larger context to determine aspect.\n\nFollowing John Lyons (1982), Vladimir Žegarac notes, \"The temptation to use the simple present is said to arise from the lack of progressive forms in Latin and French, and from a misinterpretation of the meaning of \"cogito\" as habitual or generic.\" (Cf. gnomic aspect.) Ann Banfield writes (also following Lyons), \"In order for the statement on which Descartes's argument depends to represent certain knowledge, … its tense must be a true present—in English, a progressive, … not as 'I think' but as 'I am thinking, in conformity with the general translation of the Latin or French present tense in such nongeneric, nonstative contexts.\" Or in the words of Simon Blackburn, \"Descartes’s premise is not ‘I think’ in the sense of ‘I ski’, which can be true even if you are not at the moment skiing. It is supposed to be parallel to ‘I am skiing’.\"\n\nFumitaka Suzuki (2012) writes \"Taking consideration of Cartesian theory of continuous creation, which theory was developed especially in the Meditations and in the Principles, we would assure that 'I am thinking, therefore I am/exist' is the most appropriate English translation of 'ego cogito, ergo sum'.\"\n\nThe similar “I am thinking, therefore I exist” appears in the CSMK translation of Descartes's correspondence in French (“, ”) to colleagues at CSMK III 247.\n\nThe earliest known translation as \"I am thinking, therefore I am\" is from 1872 by Charles Porterfield Krauth.\n\nAs put compactly by Prof. Krauth (1872), \"That cannot doubt which does not think, and that cannot think which does not exist. I doubt, I think, I exist.\"\n\nThe phrase \"cogito, ergo sum\" is not used in Descartes's \"Meditations on First Philosophy\" but the term \"the \"cogito\"\" is used to refer to an argument from it. In the \"Meditations\", Descartes phrases the conclusion of the argument as \"that the proposition, \"I am, I exist,\" is necessarily true whenever it is put forward by me or conceived in my mind.\" (\"Meditation\" II)\n\nAt the beginning of the second meditation, having reached what he considers to be the ultimate level of doubt—his argument from the existence of a deceiving god—Descartes examines his beliefs to see if any have survived the doubt. In his belief in his own existence, he finds that it is impossible to doubt that he exists. Even if there were a deceiving god (or an evil demon), one's belief in their own existence would be secure, for there is no way one could be deceived unless one existed in order to be deceived.\n\nBut I have convinced myself that there is absolutely nothing in the world, no sky, no earth, no minds, no bodies. Does it now follow that I, too, do not exist? No. If I convinced myself of something [or thought anything at all], then I certainly existed. But there is a deceiver of supreme power and cunning who deliberately and constantly deceives me. In that case, I, too, undoubtedly exist, if he deceives me; and let him deceive me as much as he can, he will never bring it about that I am nothing, so long as I think that I am something. So, after considering everything very thoroughly, I must finally conclude that the proposition, \"I am, I exist,\" is necessarily true whenever it is put forward by me or conceived in my mind. (AT VII 25; CSM II 16–17)\n\nThere are three important notes to keep in mind here. First, he claims only the certainty of \"his own\" existence from the first-person point of view — he has not proved the existence of other minds at this point. This is something that has to be thought through by each of us for ourselves, as we follow the course of the meditations. Second, he does not say that his existence is necessary; he says that \"if he thinks\", then necessarily he exists (see the instantiation principle). Third, this proposition \"I am, I exist\" is held true not based on a deduction (as mentioned above) or on empirical induction but on the clarity and self-evidence of the proposition. Descartes does not use this first certainty, the \"cogito\", as a foundation upon which to build further knowledge; rather, it is the firm ground upon which he can stand as he works to discover further truths As he puts it:\n\nArchimedes used to demand just one firm and immovable point in order to shift the entire earth; so I too can hope for great things if I manage to find just one thing, however slight, that is certain and unshakable. (AT VII 24; CSM II 16)\n\nAccording to many Descartes specialists, including Étienne Gilson, the goal of Descartes in establishing this first truth is to demonstrate the capacity of his criterion — the immediate clarity and distinctiveness of self-evident propositions — to establish true and justified propositions despite having adopted a method of generalized doubt. As a consequence of this demonstration, Descartes considers science and mathematics to be justified to the extent that their proposals are established on a similarly immediate clarity, distinctiveness, and self-evidence that presents itself to the mind. The originality of Descartes's thinking, therefore, is not so much in expressing the \"cogito\" — a feat accomplished by other predecessors, as we shall see — but on using the \"cogito\" as demonstrating the most fundamental epistemological principle, that science and mathematics are justified by relying on clarity, distinctiveness, and self-evidence.\nBaruch Spinoza in \"Principia philosophiae cartesianae\" at its \"Prolegomenon\" identified \"cogito ergo sum\" the \"ego sum cogitans\" (I am a thinking being) as the thinking substance with his ontological interpretation. It can also be considered that \"Cogito ergo sum\" is needed before any living being can go further in life\".\n\nAlthough the idea expressed in \"cogito, ergo sum\" is widely attributed to Descartes, he was not the first to mention it. Plato spoke about the \"knowledge of knowledge\" (Greek νόησις νοήσεως \"nóesis noéseos\") and Aristotle explains the idea in full length:\nBut if life itself is good and pleasant (...) and if one who sees is conscious that he sees, one who hears that he hears, one who walks that he walks and similarly for all the other human activities there is a faculty that is conscious of their exercise, so that whenever we perceive, we are conscious that we perceive, and whenever we think, we are conscious that we think, and to be conscious that we are perceiving or thinking is to be conscious that we exist... (\"Nicomachean Ethics\", 1170a25 ff.)\n\nAugustine of Hippo in \"De Civitate Dei\" writes \"Si […] fallor, sum\" (\"If I am mistaken, I am\") (book XI, 26), and also anticipates modern refutations of the concept. Furthermore, in the \"Enchiridion\" Augustine attempts to refute skepticism by stating, \"[B]y not positively affirming that they are alive, the skeptics ward off the appearance of error in themselves, yet they do make errors simply by showing themselves alive; one cannot err who is not alive. That we live is therefore not only true, but it is altogether certain as well\" (Chapter 7 section 20). In 1640 correspondence, Descartes thanked two colleagues for drawing his attention to Augustine and notes similarity and difference. (See CSMK III 159, 161.)\n\nAnother predecessor was Avicenna's \"Floating Man\" thought experiment on human self-awareness and self-consciousness.\n\nThe 8th century Hindu philosopher Adi Shankara wrote in a similar fashion, No one thinks, 'I am not', arguing that one's existence cannot be doubted, as there must be someone there to doubt. . The central idea of \"cogito, ergo sum\" is also the topic of Mandukya Upanishad. \n\nSpanish philosopher Gómez Pereira in his 1554 work \"De Inmortalitate Animae\", published in 1749, wrote \"nosco me aliquid noscere, & quidquid noscit, est, ergo ego sum\" (\"I know that I know something, anyone who knows exists, then I exist\").\n\nIn \"Descartes, The Project of Pure Enquiry\", Bernard Williams provides a history and full evaluation of this issue. Apparently, the first scholar who raised the \"I\" problem was Pierre Gassendi. He \"points out that recognition that one has a set of thoughts does not imply that one is a particular thinker or another. Were we to move from the observation that there is thinking occurring to the attribution of this thinking to a particular agent, we would simply assume what we set out to prove, namely, that there exists a particular person endowed with the capacity for thought\". In other words, \"the only claim that is indubitable here is the agent-independent claim that there is cognitive activity present\". The objection, as presented by Georg Lichtenberg, is that rather than supposing an entity that is thinking, Descartes should have said: \"thinking is occurring.\" That is, whatever the force of the \"cogito\", Descartes draws too much from it; the existence of a thinking thing, the reference of the \"I,\" is more than the \"cogito\" can justify. Friedrich Nietzsche criticized the phrase in that it presupposes that there is an \"I\", that there is such an activity as \"thinking\", and that \"I\" know what \"thinking\" is. He suggested a more appropriate phrase would be \"it thinks\" wherein the \"it\" could be an impersonal subject as in the sentence \"It is raining.\"\n\nThe Danish philosopher Søren Kierkegaard calls the phrase a tautology in his \"Concluding Unscientific Postscript\". He argues that the \"cogito\" already presupposes the existence of \"I\", and therefore concluding with existence is logically trivial. Kierkegaard's argument can be made clearer if one extracts the premise \"I think\" into the premises \"'x' thinks\" and \"I am that 'x'\", where \"x\" is used as a placeholder in order to disambiguate the \"I\" from the thinking thing.\n\nHere, the \"cogito\" has already assumed the \"I\"'s existence as that which thinks. For Kierkegaard, Descartes is merely \"developing the content of a concept\", namely that the \"I\", which already exists, thinks. As Kierkegaard argues, the proper logical flow of argument is that existence is already assumed or presupposed in order for thinking to occur, not that existence is concluded from that thinking.\n\nBernard Williams claims that what we are dealing with when we talk of thought, or when we say \"I am thinking,\" is something conceivable from a third-person perspective; namely objective \"thought-events\" in the former case, and an objective thinker in the latter. He argues, first, that it is impossible to make sense of \"there is thinking\" without relativizing it to \"something.\" However, this something cannot be Cartesian egos, because it is impossible to differentiate objectively between things just on the basis of the pure content of consciousness. The obvious problem is that, through introspection, or our experience of consciousness, we have no way of moving to conclude the existence of any third-personal fact, to conceive of which would require something above and beyond just the purely subjective contents of the mind.\n\nAs a critic of Cartesian subjectivity, Heidegger sought to ground human subjectivity in death as that certainty which individualizes and authenticates our being. As he wrote in 1927:\n\n\"This certainty, that \"I myself am in that I will die,\" is the basic certainty of Dasein itself. It is a genuine statement of Dasein, while \"cogito sum\" is only the semblance of such a statement. If such pointed formulations mean anything at all, then the appropriate statement pertaining to Dasein in its being would have to be \"sum moribundus\" [I am in dying], \"moribundus\" not as someone gravely ill or wounded, but insofar as I am, I am \"moribundus\". The \"MORIBUNDUS\" first gives the \"SUM\" its sense.\"\n\nThe Scottish philosopher John Macmurray rejects the \"cogito\" outright in order to place action at the center of a philosophical system he entitles the Form of the Personal. \"We must reject this, both as standpoint and as method. If this be philosophy, then philosophy is a bubble floating in an atmosphere of unreality.\" The reliance on thought creates an irreconcilable dualism between thought and action in which the unity of experience is lost, thus dissolving the integrity of our selves, and destroying any connection with reality. In order to formulate a more adequate \"cogito\", Macmurray proposes the substitution of \"I do\" for \"I think\", ultimately leading to a belief in God as an agent to whom all persons stand in relation.\n\n\n"}
{"id": "17220101", "url": "https://en.wikipedia.org/wiki?curid=17220101", "title": "Cognitive and linguistic theories of composition", "text": "Cognitive and linguistic theories of composition\n\nCognitive science and linguistic theory have played an important role in providing empirical research into the writing process and serving the teaching of composition. As composition theories, there is some dispute concerning the appropriateness of tying these two schools of thought together into one theory of composition. However, their empirical basis for research and ties to the process theory of composition and cognitive science can be thought to warrant some connection.\n\nThe cognitive theory of composition (hereafter referred to as “cognitive theory”) can trace its roots to psychology and cognitive science. Lev Vygotsky's and Jean Piaget's contributions to the theories of cognitive development and developmental psychology could be found in early work linking these sciences with composition theory (see Ann E. Berthoff). Linda Flower and John Hayes published “A Cognitive Process Theory of Writing” in 1981, providing the groundwork for further research into how thought processes influence the writing process.\n\nLinguistic theories of composition found their roots in the debate surrounding grammar's importance in composition pedagogy. Scholars, such as Janet Emig, Patrick Hartwell, Martha J. Kolln, Robert Funk, Stephen Witte, and Lester Faigley continued this line of thought around the same time that a cognitive theory of composition was being developed by Flower and Hayes. These scholars, like scholars researching cognitive-oriented composition theory, focused on research providing insight into the writing process, but were also committed to providing pedagogical advancements addressing deficiencies, trends, and insights gained from their linguistic research.\n\nA cognitive theory is focused on gaining insight into the writing process through the writer’s thought processes. Composition theorists have attacked the problem of accessing writers’ thoughts in various ways. Flower and Hayes’ essay, “A Cognitive Process Theory of Writing” sought to outline the writer’s choice-making throughout the writing process, and how those choices constrained or influenced other choices down the line. Other research has focused on capturing the cognitive processes of writers during the writing process through note-taking or speaking aloud, while some early research by Birdwell, Nancrow, and Ross was done with computers to record writers’ keystrokes during the writing process.\n\nLinguistic composition theory has traditionally focused on sentence and paragraph-level composition, with the goal of providing instructors insights into the way students write at various proficiency levels. Stephen Witte and Lester Faigley utilized detailed syntactic analysis to redefine the importance of cohesion and coherence in judging writing quality. Paul Rodgers and Richard Braddock focused on paragraph structure, in separate studies, in order to dispel common misjudgments about the importance of traditional paragraph structure.\n\nApplied linguistics, specifically EFL/ESL studies, has played a large role in development linguistic theories of composition. Liz Hamp-Lyons’ research in ESL/EFL writing assessment is valuable in informing ESL composition pedagogy. Paul Kei Matsuda, has illustrated the deficiency in ESL composition research, and recent compilations by Matsuda and others have attempted to bridge the gap between ESL instruction and composition theory by presenting pedagogical, theoretical, and assessment frameworks in the ESL composition classroom.\n\nCognitive and linguistic theories of composition are heavily tied to process theory. Cognitive and linguistic theories have been instrumental in providing respected empirical research to the field of composition theory, but tend to stay away from making pedagogical suggestions. Instead, research in these fields is typically intended to inform process theory by providing data analysis regarding the writing process, and by bringing scientific research to the field.\n"}
{"id": "542168", "url": "https://en.wikipedia.org/wiki?curid=542168", "title": "Cohesion (computer science)", "text": "Cohesion (computer science)\n\nIn computer programming, cohesion refers to the \"degree to which the elements inside a module belong together\". In one sense, it is a measure of the strength of relationship between the methods and data of a class and some unifying purpose or concept served by that class. In another sense, it is a measure of the strength of relationship between the class’s methods and data themselves.\n\nCohesion is an ordinal type of measurement and is usually described as “high cohesion” or “low cohesion”. Modules with high cohesion tend to be preferable, because high cohesion is associated with several desirable traits of software including robustness, reliability, reusability, and understandability. In contrast, low cohesion is associated with undesirable traits such as being difficult to maintain, test, reuse, or even understand.\n\nCohesion is often contrasted with coupling, a different concept. High cohesion often correlates with loose coupling, and vice versa. The software metrics of coupling and cohesion were invented by Larry Constantine in the late 1960s as part of Structured Design, based on characteristics of “good” programming practices that reduced maintenance and modification costs. Structured Design, cohesion and coupling were published in the article and the book ; the latter two subsequently became standard terms in software engineering.\n\nIn object-oriented programming, if the methods that serve a class tend to be similar in many aspects, then the class is said to have high cohesion. In a highly cohesive system, code readability and reusability is increased, while complexity is kept manageable.\n\nCohesion is increased if:\n\nAdvantages of high cohesion (or \"strong cohesion\") are:\n\nWhile in principle a module can have perfect cohesion by only consisting of a single, atomic element – having a single function, for example – in practice complex tasks are not expressible by a single, simple element. Thus a single-element module has an element that either is too complicated, in order to accomplish a task, or is too narrow, and thus tightly coupled to other modules. Thus cohesion is balanced with both unit complexity and coupling.\n\nCohesion is a qualitative measure, meaning that the source code to be measured is examined using a rubric to determine a classification. Cohesion types, from the worst to the best, are as follows:\n\n\n\n\n\n\n\n\n\nAlthough cohesion is a ranking type of scale, the ranks do not indicate a steady progression of improved cohesion. Studies by various people including Larry Constantine, Edward Yourdon, and Steve McConnell indicate that the first two types of cohesion are inferior; communicational and sequential cohesion are very good; and functional cohesion is superior.\n\nWhile functional cohesion is considered the most desirable type of cohesion for a software module, it may not be achievable. There are cases where communicational cohesion is the highest level of cohesion that can be attained under the circumstances.\n\n\n"}
{"id": "47021992", "url": "https://en.wikipedia.org/wiki?curid=47021992", "title": "Cologne Diocesan Feud", "text": "Cologne Diocesan Feud\n\nThe Cologne Diocesan Feud (, or \"Stiftsfehde zu Köln\"), also called the Neuss War (\"Neusser Krieg\") or Burgundian War (\"Burgundischer Krieg\"), was a conflict, which began in 1473, between the Archbishop of Cologne, Ruprecht of the Palatinate and the \"Landstände\" of his archbishopric that began in 1473. As a result of the involvement of Charles the Bold of Burgundy and, eventually, the Holy Roman Empire the matter at times assumed a European dimension. It finally ended when Ruprecht died in 1480.\n\nThe background was that, after the death of , the estates in the secular dominion (\"Archbishopric\") of the Archbishop of Cologne banded together to form so-called \"hereditary estate agreements\" (\"Erblandesvereinigungen\"). The \"Erblandesvereinigung\" in the Diocese itself also joined the ecclesiastical territory of Vest Recklinghausen. In the Duchy of Westphalia the estates agreed their own \"Erblandesvereinigung\". These agreements had henceforth to be sworn by new archbishops in their role as territorial rulers. The \"Erblandesvereinigung\" envisaged that, in this case of important fiscal and public policy issues, the sovereign had to seek the permission of estates or \"Landstände\". Although Ruprecht came from the centre of the cathedral chapter, it was not long before he ignored the \"Erblandesvereinigung\" he had sworn. Instead he hired mercenaries from the Palatinate, with whom he intended to recapture the estates enfeoffed by previous archbishops. When he let a dispute occur over the raising of a poll and hearth tax on the town of Zons, which was enfeoffed to the cathedral chapter, the conflict broke out openly. He also tried to take the town of Neuss by force. The \"Landstände\" saw the actions of the Archbishop as an infraction of \"Erblandesvereinigung\", relied on their right to opposition and deposed Ruprecht. In his place, in the spring of 1473, they elected Hermann of Hesse as the administrator (\"Stiftsverweser\") of the Diocese. The estates had strong support from the cities of Cologne and Neuss.\n\nRuprecht did not accept this and was also supported by the small and middle-ranking estates. In 1473, Hessian troops under Johann Schenk zu Schweinsberg, sent to support the administrator, failed in their attempt to capture the town of Brilon in the Duchy of Westphalia, but then played an important role in the defence of Neuss. Ruprecht's situation improved as a result, because he was able to win the backing of Charles the Bold. The latter was even appointed as the \"hereditary advocate\" (\"Erbvogt\") of the Diocese.\n\nFor his part, Charles the Bold saw a favourable opportunity to extend his power at the expense of the Diocese. The majority of the neighbouring territories were already in Burgundian hands. In addition, the Duchy of Cleves belonged to the allies. In 1473 after Charles the Bold also came into possession of the Duchy of Guelders, the existence of the Diocese became threatened. The same year there was an attempt at the \"Trier Meeting\" (\"Trierer Treffen\") to clarify the position of Charles the Bold and the Emperor and the prince-electors. This attempt at negotiation proved unsuccessful. In April and May 1474, Ahrweiler was besieged by troops sympathetic to Ruprecht. The town walls and guns fended the attacks off.\n\nCharles the Bold marched out with an army that was one of the largest and best equipped of its time. In his support was Frederick the Victorious of the Palatinate, brother of Archbishop Ruprecht, and the dukes of Guelders and Cleves. The allied troops jointly numbered about 13,000 to 20,000 men. Instead of marching on Cologne as Ruprecht had thought, the army advanced on Neuss, a town defended by Hermann of Hesse and his 4,000-man body of troops. Neuss was besieged in 1474/75 by the enemy forces (see the Siege of Neuss). The siege was ended by the delivery of an imperial ban by Emperor Frederick III. This ended the Neuss War, but not the diocesan feud.\n\nAfter the withdrawal of Burgundian troops, Ruprecht of the Palatinate still had available several supporters in the Upper Diocese (\"Oberstift\") and in the Duchy of Westphalia. So he did not give up. However, his position was severely weakened by the death of Frederick the Victorious in 1476 and Charles the Bold a year later. In the territory of the Diocese itself he was now only able to control Kempen and Altenahr whilst, outside, he still had several Westphalian estates. In 1478, he was taken prisoner by Hessian troops. However, initially the Hessians were unable to make gains in Westphalia. The Duke of Cleves, who fought on Ruprecht's side, was even able to occupyArnsberg and Eversberg for a time.\n\nAfter his capture, Ruprecht announced he was prepared to give up the office of archbishop. However, confirmation from the Pope in light of the difficult church legal situation - Ruprecht was not just the sovereign, but first and foremost a bishop - was taking a long time to arrive. Ruprecht's renunciation of the archbishop's office was also questionable, because he had come under external pressure. The death of Ruprecht on 26 July 1480 ended this tricky situation.\n\n\n\n"}
{"id": "2381958", "url": "https://en.wikipedia.org/wiki?curid=2381958", "title": "Conceptual model", "text": "Conceptual model\n\nA conceptual model is a representation of a system, made of the composition of concepts which are used to help people know, understand, or simulate a subject the model represents. It is also a set of concepts. Some models are physical objects; for example, a toy model which may be assembled, and may be made to work like the object it represents.\n\nThe term \"conceptual model\" may be used to refer to models which are formed after a conceptualization or generalization process. Conceptual models are often abstractions of things in the real world whether physical or social. Semantic studies are relevant to various stages of concept formation. Semantics is basically about concepts, the meaning that thinking beings give to various elements of their experience.\n\nThe term \"conceptual model\" is normal. It could mean \"a model of concept\" or it could mean \"a model that is conceptual.\" A distinction can be made between \"what models are\" and \"what models are made of\". With the exception of iconic models, such as a scale model of Winchester Cathedral, most models are concepts. But they are, mostly, intended to be models of real world states of affairs. The value of a model is usually directly proportional to how well it corresponds to a past, present, future, actual or potential state of affairs. A model of a concept is quite different because in order to be a good model it need not have this real world correspondence. In artificial intelligence conceptual models and conceptual graphs are used for building expert systems and knowledge-based systems; here the analysts are concerned to represent expert opinion on what is true not their own ideas on what is true.\n\nConceptual models (models that are conceptual) range in type from the more concrete, such as the mental image of a familiar physical object, to the formal generality and abstractness of mathematical models which do not appear to the mind as an image. Conceptual models also range in terms of the scope of the subject matter that they are taken to represent. A model may, for instance, represent a single thing (e.g. the \"Statue of Liberty\"), whole classes of things (e.g. \"the electron\"), and even very vast domains of subject matter such as \"the physical universe.\" The variety and scope of conceptual models is due to then variety of purposes had by the people using them.\nConceptual modeling is the activity of formally describing some aspects of the physical and social world around us for the purposes of understanding and communication.\"\n\nA conceptual model's primary objective is to convey the fundamental principles and basic functionality of the system which it represents. Also, a conceptual model must be developed in such a way as to provide an easily understood system interpretation for the models users. A conceptual model, when implemented properly, should satisfy four fundamental objectives.\n\n\nThe conceptual model plays an important role in the overall system development life cycle. Figure 1 below, depicts the role of the conceptual model in a typical system development scheme. It is clear that if the conceptual model is not fully developed, the execution of fundamental system properties may not be implemented properly, giving way to future problems or system shortfalls. These failures do occur in the industry and have been linked to; lack of user input, incomplete or unclear requirements, and changing requirements. Those weak links in the system design and development process can be traced to improper execution of the fundamental objectives of conceptual modeling. The importance of conceptual modeling is evident when such systemic failures are mitigated by thorough system development and adherence to proven development objectives/techniques.\n\nAs systems have become increasingly complex, the role of conceptual modelling has dramatically expanded. With that expanded presence, the effectiveness of conceptual modeling at capturing the fundamentals of a system is being realized. Building on that realization, numerous conceptual modeling techniques have been created. These techniques can be applied across multiple disciplines to increase the users understanding of the system to be modeled. A few techniques are briefly described in the following text, however, many more exist or are being developed. Some commonly used conceptual modeling techniques and methods include: workflow modeling, workforce modeling, rapid application development, object-role modeling, and the Unified Modeling Language (UML).\n\nData flow modeling (DFM) is a basic conceptual modeling technique that graphically represents elements of a system. DFM is a fairly simple technique, however, like many conceptual modeling techniques, it is possible to construct higher and lower level representative diagrams. The data flow diagram usually does not convey complex system details such as parallel development considerations or timing information, but rather works to bring the major system functions into context. Data flow modeling is a central technique used in systems development that utilizes the structured systems analysis and design method (SSADM).\n\nEntity-relationship modeling (ERM) is a conceptual modeling technique used primarily for software system representation. Entity-relationship diagrams, which are a product of executing the ERM technique, are normally used to represent database models and information systems. The main components of the diagram are the entities and relationships. The entities can represent independent functions, objects, or events. The relationships are responsible for relating the entities to one another. To form a system process, the relationships are combined with the entities and any attributes needed to further describe the process. Multiple diagramming conventions exist for this technique; IDEF1X, Bachman, and EXPRESS, to name a few. These conventions are just different ways of viewing and organizing the data to represent different system aspects.\n\nThe event-driven process chain (EPC) is a conceptual modeling technique which is mainly used to systematically improve business process flows. Like most conceptual modeling techniques, the event driven process chain consists of entities/elements and functions that allow relationships to be developed and processed. More specifically, the EPC is made up of events which define what state a process is in or the rules by which it operates. In order to progress through events, a function/ active event must be executed. Depending on the process flow, the function has the ability to transform event states or link to other event driven process chains. Other elements exist within an EPC, all of which work together to define how and by what rules the system operates. The EPC technique can be applied to business practices such as resource planning, process improvement, and logistics.\n\nThe dynamic systems development method uses a specific process called JEFFF to conceptually model a systems life cycle. JEFFF is intended to focus more on the higher level development planning that precedes a projects initialization. The JAD process calls for a series of workshops in which the participants work to identify, define, and generally map a successful project from conception to completion. This method has been found to not work well for large scale applications, however smaller applications usually report some net gain in efficiency.\n\nAlso known as Petri nets, this conceptual modeling technique allows a system to be constructed with elements that can be described by direct mathematical means. The petri net, because of its nondeterministic execution properties and well defined mathematical theory, is a useful technique for modeling concurrent system behavior, i.e. simultaneous process executions.\n\nState transition modeling makes use of state transition diagrams to describe system behavior. These state transition diagrams use distinct states to define system behavior and changes. Most current modeling tools contain some kind of ability to represent state transition modeling. The use of state transition models can be most easily recognized as logic state diagrams and directed graphs for finite-state machines.\n\nBecause the conceptual modeling method can sometimes be purposefully vague to account for a broad area of use, the actual application of concept modeling can become difficult. To alleviate this issue, and shed some light on what to consider when selecting an appropriate conceptual modeling technique, the framework proposed by Gemino and Wand will be discussed in the following text. However, before evaluating the effectiveness of a conceptual modeling technique for a particular application, an important concept must be understood; Comparing conceptual models by way of specifically focusing on their graphical or top level representations is shortsighted. Gemino and Wand make a good point when arguing that the emphasis should be placed on a conceptual modeling language when choosing an appropriate technique. In general, a conceptual model is developed using some form of conceptual modeling technique. That technique will utilize a conceptual modeling language that determines the rules for how the model is arrived at. Understanding the capabilities of the specific language used is inherent to properly evaluating a conceptual modeling technique, as the language reflects the techniques descriptive ability. Also, the conceptual modeling language will directly influence the depth at which the system is capable of being represented, whether it be complex or simple.\n\nBuilding on some of their earlier work, Gemino and Wand acknowledge some main points to consider when studying the affecting factors: the content that the conceptual model must represent, the method in which the model will be presented, the characteristics of the models users, and the conceptual model languages specific task. The conceptual models content should be considered in order to select a technique that would allow relevant information to be presented. The presentation method for selection purposes would focus on the techniques ability to represent the model at the intended level of depth and detail. The characteristics of the models users or participants is an important aspect to consider. A participant's background and experience should coincide with the conceptual models complexity, else misrepresentation of the system or misunderstanding of key system concepts could lead to problems in that systems realization. The conceptual model language task will further allow an appropriate technique to be chosen. The difference between creating a system conceptual model to convey system functionality and creating a system conceptual model to interpret that functionality could involve to completely different types of conceptual modeling languages.\n\nGemino and Wand go on to expand the affected variable content of their proposed framework by considering the focus of observation and the criterion for comparison. The focus of observation considers whether the conceptual modeling technique will create a \"new product\", or whether the technique will only bring about a more intimate understanding of the system being modeled. The criterion for comparison would weigh the ability of the conceptual modeling technique to be efficient or effective. A conceptual modeling technique that allows for development of a system model which takes all system variables into account at a high level may make the process of understanding the system functionality more efficient, but the technique lacks the necessary information to explain the internal processes, rendering the model less effective.\n\nWhen deciding which conceptual technique to use, the recommendations of Gemino and Wand can be applied in order to properly evaluate the scope of the conceptual model in question. Understanding the conceptual models scope will lead to a more informed selection of a technique that properly addresses that particular model. In summary, when deciding between modeling techniques, answering the following questions would allow one to address some important conceptual modeling considerations.\n\n\nAnother function of the simulation conceptual model is to provide a rational and factual basis for assessment of simulation application appropriateness.\n\nIn cognitive psychology and philosophy of mind, a mental model is a representation of something in the mind, but a mental model may also refer to a nonphysical external model of the mind itself.\n\nA metaphysical model is a type of conceptual model which is distinguished from other conceptual models by its proposed scope; a metaphysical model intends to represent reality in the broadest possible way. This is to say that it explains the answers to fundamental questions such as whether matter and mind are one or two substances; or whether or not humans have free will.\n\nAn epistemological model is a type of conceptual model whose proposed scope is the known and the knowable, and the believed and the believable.\n\nIn logic, a model is a type of interpretation under which a particular statement is true. Logical models can be broadly divided into ones which only attempt to represent concepts, such as mathematical models; and ones which attempt to represent physical objects, and factual relationships, among which are scientific models.\n\nModel theory is the study of (classes of) mathematical structures such as groups, fields, graphs, or even universes of set theory, using tools from mathematical logic. A system that gives meaning to the sentences of a formal language is called a model for the language. If a model for a language moreover satisfies a particular sentence or theory (set of sentences), it is called a model of the sentence or theory. Model theory has close ties to algebra and universal algebra.\n\nMathematical models can take many forms, including but not limited to dynamical systems, statistical models, differential equations, or game theoretic models. These and other types of models can overlap, with a given model involving a variety of abstract structures.\n\nA more comprehensive type of mathematical model uses a linguistic version of category theory to model a given situation. Akin to entity-relationship models, custom categories or sketches can be directly translated into database schemas. The difference is that logic is replaced by category theory, which brings powerful theorems to bear on the subject of modeling, especially useful for translating between disparate models (as functors between categories).\n\nA scientific model is a simplified abstract view of a complex reality. A scientific model represents empirical objects, phenomena, and physical processes in a logical way. Attempts to formalize the principles of the empirical sciences use an interpretation to model reality, in the same way logicians axiomatize the principles of logic. The aim of these attempts is to construct a formal system for which reality is the only interpretation. The world is an interpretation (or model) of these sciences, only insofar as these sciences are true.\n\nA statistical model is a probability distribution function proposed as generating data. In a parametric model, the probability distribution function has variable parameters, such as the mean and variance in a normal distribution, or the coefficients for the various exponents of the independent variable in linear regression. A nonparametric model has a distribution function without parameters, such as in bootstrapping, and is only loosely confined by assumptions. Model selection is a statistical method for selecting a distribution function within a class of them; e.g., in linear regression where the dependent variable is a polynomial of the independent variable with parametric coefficients, model selection is selecting the highest exponent, and may be done with nonparametric means, such as with cross validation.\n\nIn statistics there can be models of mental events as well as models of physical events. For example, a statistical model of customer behavior is a model that is conceptual (because behavior is physical), but a statistical model of customer satisfaction is a model of a concept (because satisfaction is a mental not a physical event).\n\nIn economics, a model is a theoretical construct that represents economic processes by a set of variables and a set of logical and/or quantitative relationships between them. The economic model is a simplified framework designed to illustrate complex processes, often but not always using mathematical techniques. Frequently, economic models use structural parameters. Structural parameters are underlying parameters in a model or class of models. A model may have various parameters and those parameters may change to create various properties.\n\nA system model is the conceptual model that describes and represents the structure, behavior, and more views of a system. A system model can represent multiple views of a system by using two different approaches. The first one is the non-architectural approach and the second one is the architectural approach. The non-architectural approach respectively picks a model for each view. The architectural approach, also known as system architecture, instead of picking many heterogeneous and unrelated models, will use only one integrated architectural model.\n\nIn business process modelling the enterprise process model is often referred to as the \"business process model\". Process models are core concepts in the discipline of process engineering. Process models are:\nThe same process model is used repeatedly for the development of many applications and thus, has many instantiations.\n\nOne possible use of a process model is to prescribe how things must/should/could be done in contrast to the process itself which is really what happens. A process model is roughly an anticipation of what the process will look like. What the process shall be will be determined during actual system development.\n\nConceptual models of human activity systems are used in soft systems methodology (SSM), which is a method of systems analysis concerned with the structuring of problems in management. These models are models of concepts; the authors specifically state that they are not intended to represent a state of affairs in the physical world. They are also used in information requirements analysis (IRA) which is a variant of SSM developed for information system design and software engineering.\n\nLogico-linguistic modeling is another variant of SSM that uses conceptual models. However, this method combines models of concepts with models of putative real world objects and events. It is a graphical representation of modal logic in which modal operators are used to distinguish statement about concepts from statements about real world objects and events.\n\nIn software engineering, an entity-relationship model (ERM) is an abstract and conceptual representation of data. Entity-relationship modeling is a database modeling method, used to produce a type of conceptual schema or semantic data model of a system, often a relational database, and its requirements in a top-down fashion. Diagrams created by this process are called entity-relationship diagrams, ER diagrams, or ERDs.\n\nEntity-relationship models have had wide application in the building of information systems intended to support activities involving objects and events in the real world. In these cases they are models that are conceptual. However, this modeling method can be used to build computer games or a family tree of the Greek Gods, in these cases it would be used to model concepts.\n\nA domain model is a type of conceptual model used to depict the structural elements and their conceptual constraints within a domain of interest (sometimes called the problem domain). A domain model includes the various entities, their attributes and relationships, plus the constraints governing the conceptual integrity of the structural model elements comprising that problem domain. A domain model may also include a number of conceptual views, where each view is pertinent to a particular subject area of the domain or to a particular subset of the domain model which is of interest to a stakeholder of the domain model.\n\nLike entity-relationship models, domain models can be used to model concepts or to model real world objects and events.\n\n\n"}
{"id": "14616475", "url": "https://en.wikipedia.org/wiki?curid=14616475", "title": "Conspiracy against rights", "text": "Conspiracy against rights\n\nConspiracy against rights is a federal offense in the United States of America under :\n\nThe Supreme Court held that a conviction under a related statute, 18 U.S.C. §242 required proof of the defendant's specific intent to deprive the victim of a constitutional right. In United States v. Guest, the Supreme Court read this same requirement into §241, the conspiracy statute.\n"}
{"id": "3291957", "url": "https://en.wikipedia.org/wiki?curid=3291957", "title": "Don't repeat yourself", "text": "Don't repeat yourself\n\nIn software engineering, don't repeat yourself (DRY) is a principle of software development aimed at reducing repetition of software patterns, replacing it with abstractions or using data normalization to avoid redundancy.\n\nThe DRY principle is stated as \"Every piece of knowledge must have a single, unambiguous, authoritative representation within a system\". The principle has been formulated by Andy Hunt and Dave Thomas in their book \"The Pragmatic Programmer\". They apply it quite broadly to include \"database schemas, test plans, the build system, even documentation\". When the DRY principle is applied successfully, a modification of any single element of a system does not require a change in other logically unrelated elements. Additionally, elements that are logically related all change predictably and uniformly, and are thus kept in sync. Besides using methods and subroutines in their code, Thomas and Hunt rely on code generators, automatic build systems, and scripting languages to observe the DRY principle across layers.\n\nViolations of DRY are typically referred to as WET solutions, which is commonly taken to stand for either \"write everything twice\", \"we enjoy typing\" or \"waste everyone's time\". WET solutions are common in multi-tiered architectures where a developer may be tasked with, for example, adding a comment field on a form in a web application. The text string \"comment\" might be repeated in the label, the HTML tag, in a read function name, a private variable, database DDL, queries, and so on. A DRY approach eliminates that redundancy by using frameworks that reduce or eliminate all those edit tasks excepting the most important one, leaving the extensibility of adding new knowledge variables in one place.\n\n\n"}
{"id": "1130867", "url": "https://en.wikipedia.org/wiki?curid=1130867", "title": "Emasculation", "text": "Emasculation\n\nEmasculation of a human male is to deprive (a man) of his male role or identity.\n\nThe word also has other meanings which are more commonly used. See below.\n\nIn Medieval Europe, emasculation was used as a form of punishment. It was sometimes done when a person was hanged, drawn, and quartered (a form of execution by torture).\n\nIn ancient China, emasculation was performed as a punishment up until the Sui Dynasty and Tang Dynasty. Additionally, some men underwent the procedure as means of becoming employed as an imperial servant or bureaucrat. In English, the word eunuch is generally used to refer to these Chinese people who underwent emasculation, and they are often referred to as having been \"castrated\" rather than \"emasculated\". As of the Qing Dynasty, emasculation was still performed in China. In the 19th century, the rebel Yaqub Beg and all of his sons and grandsons were punished by being emasculated and enslaved. The last Imperial eunuch was Sun Yaoting, who died in 1996. For more information on emasculation in China, see Castration#China.\n\nThe ancient Vietnamese adopted China's practice of emasculation and the use of eunuchs as servants and slaves for the monarchy. The procedure was reportedly very painful as both the testicles and penis were removed. In 1838, the Emperor of Vietnam made a law that said only adult men of high social standing could be emasculated. In the end, most eunuchs ended up being men who had been born with genital abnormalities and then handed over to the authorities. During the late 19th century, the French used the existence of eunuchs in Vietnam to degrade the Vietnamese. For more information on emasculation in Vietnam, see Castration#Vietnam.\n\nThe Khitan people of northeast Asia adopted the Chinese practice of emasculating slaves.\n\nIn 19th century Russia, the Skoptsy sect of Christianity performed emasculation, which they termed the \"greater seal\".\n\nIn the Arab slave trade, enslaved men and boys from East Africa were often \"castrated\" by removing both the penis and testicles.\n\nIn The Indian Subcontinent, some members of Hijra communities reportedly undergo emasculation. It is called \"nirwaan\" and seen as a rite of passage.\n\nIn the United States, males in the Nullo subculture voluntarily undergo emasculation.\n\nIn the Old Testament:\n\n\"“No one whose testicles are crushed or whose male organ is cut off shall enter the assembly of the LORD.\"\n\nBy extension, the word \"emasculation\" has also come to mean rendering a male less masculine, including by humiliation. It can also mean to deprive anything of vigour or effectiveness. This figurative usage has become more common than the literal meaning. For example: \"William Lewis Hughes voted for Folkestone’s amendment to Curwen’s emasculated reform bill, 12 June 1809...\"\n\nIn horticulture, the removal of male (pollen) parts of a plant, largely for controlled pollination and breeding purposes, is also called emasculation.\n\n"}
{"id": "56101", "url": "https://en.wikipedia.org/wiki?curid=56101", "title": "Ethnic conflict", "text": "Ethnic conflict\n\nAn ethnic conflict is a conflict between two or more contending ethnic groups. While the source of the conflict may be political, social, economic or religious, the individuals in conflict must expressly fight for their ethnic group's position within society. This final criterion differentiates ethnic conflict from other forms of struggle.\n\nEthnic conflict does not necessarily have to be violent. In a multi-ethnic society where freedom of speech is protected, ethnic conflict can be an everyday feature of plural democracies. For example, ethnic conflict might be a non-violent struggle for resources divided among ethnic groups. However, the subject of the confrontation must be either directly or symbolically linked with an ethnic group. In healthy multi-ethnic democracies, these conflicts are usually institutionalized and \"channeled through parliaments, assemblies and bureaucracies or through non-violent demonstrations and strikes.\" While democratic countries cannot always prevent ethnic conflict flaring up into violence, institutionalized ethnic conflict does ensure that ethnic groups can articulate their demands in a peaceful manner, which reduces the likelihood of violence. On the other hand, in authoritarian systems, ethnic minorities are often unable to express their grievances. Grievances are instead allowed to fester which might lead to long phases of ethnic silence followed by a violent outburst. Therefore, ethnic peace is an absence of violence, not an absence of conflict. Another consequence is that violent ethnic rebellions often result in political rights for previously marginalized groups.\n\nAcademic explanations of ethnic conflict generally fall into one of three schools of thought: primordialist, instrumentalist or constructivist. Recently, several political scientists have argued for either top-down or bottom-up explanations for ethnic conflict. Intellectual debate has also focused on whether ethnic conflict has become more prevalent since the end of the Cold War, and on devising ways of managing conflicts, through instruments such as consociationalism and federalisation.\n\nThe causes of ethnic conflict are debated by political scientists and sociologists. Explanations generally fall into one of three schools of thought: primordialist, instrumentalist, and constructivist. More recent scholarship draws on all three schools.\n\nProponents of primordialist accounts argue that \"[e]thnic groups and nationalities exist because there are traditions of belief and action towards primordial objects such as biological features and especially territorial location\". Primordialist accounts rely on strong ties of kinship among members of ethnic groups. Donald L. Horowitz argues that this kinship \"makes it possible for ethnic groups to think in terms of family resemblances\".\n\nClifford Geertz, a founding scholar of primordialism, asserts that each person has a natural connection to perceived kinsmen. In time and through repeated conflict, essential ties to one's ethnicity will coalesce and will interfere with ties to civil society. Ethnic groups will consequently always threaten the survival of civil governments but not the existence of nations formed by one ethnic group. Thus, when considered through a primordial lens, ethnic conflict in multi-ethnic society is inevitable.\n\nA number of political scientists argue that the root causes of ethnic conflict do not involve ethnicity \"per se\" but rather institutional, political, and economic factors. These scholars argue that the concept of ethnic war is misleading because it leads to an essentialist conclusion that certain groups are doomed to fight each other when in fact the wars between them that occur are often the result of political decisions.\n\nMoreover, primordial accounts do not account for the spatial and temporal variations in ethnic violence. If these \"ancient hatreds\" are always simmering under the surface and are at the forefront of people's consciousness, then ethnic groups should constantly be ensnared in violence. However, ethnic violence occurs in sporadic outbursts. For example, Varshney points out that although Yugoslavia broke up due to ethnic violence in the 1990s, it had enjoyed a long peace of decades before the USSR collapsed. Therefore, some scholars claim that it is unlikely that primordial ethnic differences alone caused the outbreak of violence in the 1990s.\n\nPrimordialists have reformulated the \"ancient hatreds\" hypothesis and have focused more on the role of human nature. Peterson argues that the existence of hatred and animosity does not have to be rooted in history for it to play a role in shaping human behavior and action: \"If \"ancient hatred\" means a hatred consuming the daily thoughts of great masses of people, then the \"ancient hatreds\" argument deserves to be readily dismissed. However, if hatred is conceived as a historically formed \"schema\" that guides action in some situations, then the conception should be taken more seriously.\"\n\nAnthony Smith notes that the instrumentalist account \"came to prominence in the 1960s and 1970s in the United States, in the debate about (white) ethnic persistence in what was supposed to have been an effective melting pot\". This new theory sought explained persistence as the result of the actions of community leaders, \"who used their cultural groups as sites of mass mobilization and as constituencies in their competition for power and resources, because they found them more effective than social classes\". In this account of ethnic identification, ethnicity and race are viewed as instrumental means to achieve particular ends.\n\nWhether ethnicity is a fixed perception or not is not crucial in the instrumentalist accounts. Moreover, the scholars of this school generally do not oppose the view that ethnic difference plays a part in many conflicts. They simply claim that ethnic difference is not sufficient to explain conflicts.\n\nMass mobilization of ethnic groups can only be successful if there are latent ethnic differences to be exploited, otherwise politicians would not even attempt to make political appeals based on ethnicity and would focus instead on economic or ideological appeals. Hence, it is difficult to completely discount the role of inherent ethnic differences. Additionally, ethnic entrepreneurs, or elites, could be tempted to mobilize ethnic groups in order to gain their political support in democratizing states. Instrumentalists theorists especially emphasize this interpretation in ethnic states in which one ethnic group is promoted at the expense of other ethnicities.\n\nFurthermore, ethnic mass mobilization is likely to be plagued by collective action problems, especially if ethnic protests are likely to lead to violence. Instrumentalist scholars have tried to respond to these shortcomings. For example, Hardin argues that ethnic mobilization faces problems of coordination and not collective action. He points out that a charismatic leader acts as a focal point around which members of an ethnic group coalesce. The existence of such an actor helps to clarify beliefs about the behavior of others within an ethnic group.\n\nA third, constructivist, set of accounts stress the importance of the socially constructed nature of ethnic groups, drawing on Benedict Anderson's concept of the imagined community. Proponents of this account point to Rwanda as an example because the Tutsi/Hutu distinction was codified by the Belgian colonial power in the 1930s on the basis of cattle ownership, physical measurements and church records. Identity cards were issued on this basis, and these documents played a key role in the genocide of 1994.\n\nSome argue that constructivist narratives of historical master cleavages are unable to account for local and regional variations in ethnic violence. For example, Varshney highlights that in the 1960s \"racial violence in the USA was heavily concentrated in northern cities; southern cities though intensely politically engaged, did not have riots\". A constructivist master narrative is often a country level variable whereas we often have to study incidences of ethnic violence at the regional and local level.\n\nScholars of ethnic conflict and civil wars have introduced theories that draw insights from all three traditional schools of thought. In \"The Geography of Ethnic Violence\", for example, Monica Duffy Toft shows how ethnic group settlement patterns, socially constructed identities, charismatic leaders, issue indivisibility, and state concern with precedent setting can lead rational actors to escalate a dispute to violence, even when doing so is likely to leave contending groups much worse off. Such research addresses empirical puzzles that are difficult to explain using primordialist, instrumentalist, or constructivist approaches alone. As Varshney notes, \"pure essentialists and pure instrumentalists do not exist anymore\".\n\nThe end of the Cold War thus sparked interest in two important questions about ethnic conflict: whether ethnic conflict was on the rise and whether given that some ethnic conflicts had escalated into serious violence, what, if anything, could scholars of large-scale violence (security studies, strategic studies, interstate politics) offer by way of explanation.\n\nOne of the most debated issues relating to ethnic conflict is whether it has become more or less prevalent in the post–Cold War period. At the end of the Cold War, academics including Samuel P. Huntington and Robert D. Kaplan predicted a proliferation of conflicts fueled by civilisational clashes, Tribalism, resource scarcity and overpopulation.\n\nThe post–Cold War period has witnessed a number of ethnically-informed secessionist movements, predominantly within the former communist states. Conflicts have involved secessionist movements in the former Yugoslavia, Transnistria in Moldova, Armenians in Azerbaijan, Abkhaz and Ossetians in Georgia. Outside the former communist bloc, ethno-separatist strife in the same period has occurred in areas such as Sri Lanka, West Papua, Chiapas, East Timor, the Basque Country, Catalonia, Southern Sudan and Hazaras in Afghanistan under the Taliban.\n\nHowever, some theorists contend that this does not represent a rise in the incidence of ethnic conflict, because many of the proxy wars fought during the Cold War as ethnic conflicts were actually hot spots of the Cold War. Research shows that the fall of Communism and the increase in the number of capitalist states were accompanied by a decline in total warfare, interstate wars, ethnic wars, revolutionary wars, and the number of refugees and displaced persons. Indeed, some scholars have questioned whether the concept of ethnic conflict is useful at all. Others have attempted to test the \"clash of civilisations\" thesis, finding it to be difficult to operationalise and that civilisational conflicts have not risen in intensity in relation to other ethnic conflicts since the end of the Cold War.\n\nA key question facing scholars who attempt to adapt their theories of interstate violence to explain or predict large-scale ethnic violence is whether ethnic groups could be considered \"rational\" actors.\nPrior to the end of the Cold War, the consensus among students of large-scale violence was that ethnic groups should be considered irrational actors, or semi-rational at best. If true, general explanations of ethnic violence would be impossible. In the years since, however, scholarly consensus has shifted to consider that ethnic groups may in fact be counted as rational actors, and the puzzle of their apparently irrational actions (for example, fighting over territory of little or no intrinsic worth) must therefore be explained in some other way. As a result, the possibility of a general explanation of ethnic violence has grown, and collaboration between comparativist and international-relations sub-fields has resulted in increasingly useful theories of ethnic conflict.\n\nA major source of ethnic conflict in multi-ethnic democracies is over the access to state patronage. Conflicts over state resources between ethnic groups can increase the likelihood of ethnic violence. In ethnically divided societies, demand for public goods decreases as each ethnic group derives more utility from benefits targeted at their ethnic group in particular. These benefits would be less valued if all other ethnic groups had access to them.Targeted benefits are more appealing because ethnic groups can solidify or heighten their social and economic status relative to other ethnic groups whereas broad programmatic policies will not improve their relative worth. Politicians and political parties in turn, have an incentive to favor co-ethnics in their distribution of material benefits. Over the long run, ethnic conflict over access to state benefits is likely to lead to the ethnification of political parties and the party system as a whole where the political salience of ethnic identity increase leading to a self-fulfilling equilibrium: If politicians only distribute benefits on an ethnic basis, voters will see themselves primarily belonging to an ethnic group and view politicians the same way. They will only vote for the politician belonging to the same ethnic group. In turn, politicians will refrain from providing public goods because it will not serve them well electorally to provide services to people not belonging to their ethnic group. In democratizing societies, this could lead to ethnic outbidding and lead to extreme politicians pushing out moderate co-ethnics. Patronage politics and ethnic politics eventually reinforce each other, leading to what Chandra terms a \"patronage democracy\".\n\nThe existence of patronage networks between local politicians and ethnic groups make it easier for politicians to mobilize ethnic groups and instigate ethnic violence for electoral gain since the neighborhood or city is already polarized along ethnic lines. The dependence of ethnic groups on their co-ethnic local politician for access to state resources is likely to make them more responsive to calls of violence against other ethnic groups. Therefore, the existence of these local patronage channels generates incentives for ethnic groups to engage in politically motivated violence.\n\nWhile the link between ethnic heterogeneity and under provision of public goods is generally accepted, there is little consensus around the causal mechanism underlying this relationship. To identify possible causal stories, Humphreys and Habyarimana ran a serious of behavioral games in Kampala, Uganda that involved several local participants completing joint tasks and allocating money amongst them. Contrary to the conventional wisdom, they find that participants did not favor the welfare of their co-ethnics disproportionately. It was only when anonymity was removed and everyone's ethnicity was known did co-ethnics decide to favor each other. Humphreys and Habyarimana argue that cooperation among co-ethnics is primarily driven by reciprocity norms that tend to be stronger among co-ethnics. The possibility of social sanctions compelled those who would not otherwise cooperate with their co-ethnics to do so. The authors find no evidence to suggest that co-ethnics display a greater degree of altruism towards each other or have the same preferences. Ethnic cooperation takes place because co-ethnics have common social networks and therefore can monitor each other and can threaten to socially sanction any transgressors.\n\nA number of scholars have attempted to synthesize the methods available for the resolution, management or transformation of their ethnic conflict. John Coakley, for example, has developed a typology of the methods of conflict resolution that have been employed by states, which he lists as: indigenization, accommodation, assimilation, acculturation, population transfer, boundary alteration, genocide and ethnic suicide. John McGarry and Brendan O'Leary have developed a taxonomy of eight macro-political ethnic conflict regulation methods, which they note are often employed by states in combination with each other. They include a number of methods that they note are clearly morally unacceptable.\n\nWith increasing interest in the field of ethnic conflict, many policy analysts and political scientists theorized potential resolutions and tracked the results of institutional policy implementation. As such, theories often focus on which Institutions are the most appropriate for addressing ethnic conflict.\n\nConsociationalism is a power sharing agreement which coopts the leaders of ethnic groups into the central state's government. Each nation or ethnic group is represented in the government through a supposed spokesman for the group. In the power sharing agreement, each group has veto powers to varying degrees, dependent on the particular state. Moreover, the norm of proportional representation is dominant: each group is represented in the government in a percentage that reflects the ethnicity's demographic presence in the state. Another requirement for Arend Lijphart is that the government must be composed of a \"grand coalition\" of the ethnic group leaders which supposes a top-down approach to conflict resolution.\n\nIn theory, this leads to self governance and protection for the ethnic group. Many scholars maintain that since ethnic tension erupts into ethnic violence when the ethnic group is threatened by a state, then veto powers should allow the ethnic group to avoid legislative threats. Switzerland is often characterized as a successful consociationalist state.\n\nA recent example of a consociational government is the post-conflict Bosnian government that was agreed upon in the Dayton Accords in 1995. A tripartite presidency was chosen and must have a Croat, a Serb, and a Bosniak. The presidents take turns acting as the forefront executive in terms of 8 months for 4 years. Many have credited this compromise of a consociational government in Bosnia for the end of the violence and the following long-lasting peace.\n\nIn contrast to Lijphart, several political scientists and policy analysts have condemned consociationalism. One of the many critiques is that consociationalism locks in ethnic tensions and identities. This assumes a primordial stance that ethnic identities are permanent and not subject to change. Furthermore, this does not allow for any \"others\" that might want to partake in the political process. As of 2012 a Jewish Bosnian is suing the Bosnian government from precluding him from running for presidential office since only a Croat, Serb, or Bosniak can run under the consociational government. Determining ethnic identities in advance and implementing a power sharing system on the basis of these fixed identities is inherently discriminatory against minority groups that might be not be recognized. Moreover, it discriminates against those who do not choose to define their identity on an ethnic or communal basis. In power sharing-systems that are based on pre-determined identities, there is a tendency to rigidly fix shares of representation on a permanent basis which will not reflect changing demographics over time. The categorization of individuals in particular ethnic groups might be controversial anyway and might in fact fuel ethnic tensions.\n\nThe inherent weaknesses in using pre-determined ethnic identities to form power sharing systems has led Ljiphart to argue that adopting a constructivist approach to consociationalism can increase its likelihood of success. The self-determination of ethnic identities is more likely to be \"non-discriminatory, neutral, flexible and self-adjusting.\" For example, in South Africa, the toxic legacy of apartheid meant that successful consociation could only be built on the basis of the self-determination of groups. Ljiphart claims that because ethnic identities are often \"unclear, fluid and flexible,\" self-determination is likely to be more successful than pre-determination of ethnic groups. A constructivist approach to consociational theory can therefore strengthen its value as a method to resolve ethnic conflict.\n\nAnother critique points to the privileging of ethnic identity over personal political choice. Howard has deemed consociationalism as a form of ethnocracy and not a path to true pluralistic democracy. Consociationalism assumes that a politician will best represent the will of his co-ethnics above other political parties. This might lead to the polarization of ethnic groups and the loss of non-ethnic ideological parties.\n\nHorowitz has argued that a single transferable vote system could prevent the ethnification of political parties because voters cast their ballots in order of preference. This means that a voter could cast some of his votes to parties other than his co-ethnic party. This in turn would compel political parties to broaden their manifestos to appeal to voters across the ethnic divide to hoover up second and third preference votes.\n\nThe theory of implementing federalism in order to curtail ethnic conflict assumes that self-governance reduces \"demands for sovereignty\". Hechter argues that some goods such as language of education and bureaucracy must be provided as local goods, instead of statewide, in order to satisfy more people and ethnic groups. Some political scientists such as Stroschein contend that ethnofederalism, or federalism determined along ethnic lines, is \"asymmetric\" as opposed to the equal devolution of power found in non-ethnic federal states, such as the United States. In this sense, special privileges are granted to specific minority groups as concessions and incentives to end violence or mute conflict.\n\nThe Soviet Union divided its structure into ethno-federal sub-states termed Union Republics. The sub-state was named after a titular minority who inhabited the area as a way to Sovietize nationalist sentiments during the 1920s. Brubaker asserts that these titular republics were formed in order to absorb any potential elite led nationalist movements against the Soviet center by incentivizing elite loyalty through advancement in the Soviet political structure.\n\nThus, federalism provides some self-governance for local matters in order to satisfy some of the grievances which might cause ethnic conflict among the masses. Moreover, federalism brings in the elites and ethnic entrepreneurs into the central power structure; this prevents a resurgence of top-down ethnic conflict.\n\nNevertheless, after the fall of the USSR many critiques of federalism as an institution to resolve ethnic conflict emerged. The devolution of power away from the central state can weaken ties to the central state. Moreover, the parallel institutions created to serve a particular nation or ethnic group might provide significant resources for Secession from the central state. As most states are unwilling to give up an integral portion of their territory, secessionist movements may trigger violence.\n\nFurthermore, some competing elite political players may not be in power; they would remain unincorporated into the central system. These competing elites can gain access through federal structures and their resources to solidify their political power in the structure. According to V.P. Gagnon this was the case in the former Yugoslavia and its violent disintegration into its ethno-federal sub-states. Ethnic entrepreneurs were able to take control of the institutionally allocated resources to wage war on other ethnic groups.\n\nA recent theory of ethnic tension resolution is non-territorial autonomy or NTA. NTA has emerged in recent years as an alternative solution to ethnic tensions and grievances in places that are likely to breed conflict. For this reason, NTA has been promoted as a more practical and state building solution than consociationalism. NTA, alternatively known as non-cultural autonomy (NCA), is based on the difference of \"jus solis\" and \"jus sanguinis,\" the principles of territory versus that of personhood. It gives rights to ethnic groups to self-rule and govern matters potentially concerning but limited to: education, language, culture, internal affairs, religion, and the internally established institutions needed to promote and reproduce these facets. In contrast to federalism, the ethnic groups are not assigned a titular sub-state, but rather the ethnic groups are dispersed throughout the state unit. Their group rights and autonomy are not constrained to a particular territory within the state. This is done in order not to weaken the center state such as in the case of ethnofederalism.\n\nThe origin of NTA can be traced back to the Marxists works of Otto Bauer and Karl Renner. NTA was employed during the interwar period, and the League of Nations sought to add protection clauses for national minorities in new states. In the 1920s, Estonia granted some cultural autonomy to the German and Jewish populations in order to ease conflicts between the groups and the newly independent state.\n\nIn Europe, most notably in Belgium, NTA laws have been enacted and created parallel institutions and political parties in the same country. In Belgium, NTA has been integrated within the federal consociational system. Some scholars of ethnic conflict resolution claim that the practice of NTA will be employed dependent on the concentration and size of the ethnic group asking for group rights.\n\nOther scholars, such as Clarke, argue that the successful implementation of NTA rests on the acknowledgement in a state of \"universal\" principles: true Rule of Law, established human rights, stated guarantees to minorities and their members to use their own quotidien language, religion, and food practices, and a framework of anti-discrimination legislation in order to enforce these rights. Moreover, no individual can be forced to adhere, identify, or emphasize a particular identity (such as race, gender, sexuality, etc.) without their consent in order for NTA to function for its purpose.\n\nNonetheless, Clarke critiques the weaknesses of NTA in areas such as education, a balance between society wide norms and intracommunity values; policing, for criminal matters and public safety; and political representation, which limits the political choices of an individual if based solely on ethnicity. Furthermore, the challenge in evaluating the efficacy of NTA lies in the relatively few legal implementations of NTA.\n\nEmphasizing the limits of approaches that focus mainly on institutional answers to ethnic conflicts—which are essentially driven by ethnocultural dynamics of which political and/or economic factors are but elements—Gregory Paul Meyjes urges the use of intercultural communication and cultural-rights based negotiations as tools with which to effectively and sustainably address inter-ethnic strife. Meyjes argues that to fully grasp, preempt, and/or resolve such conflicts—whether with or without the aid of territorial or non-territorial institutional mechanism(s) -- a cultural rights approach grounded in intercultural knowledge and skill is essential.\n\nInstitutionalist arguments for resolving ethnic conflict often focus on national-level institutions and do not account for regional and local variation in ethnic violence within a country. Despite similar levels of ethnic diversity in a country, some towns and cities have often found to be especially prone to ethnic violence. For example, Ashutosh Varshney, in his study of ethnic violence in India, argues that strong inter-ethnic engagement in villages often disincentivizes politicians from stoking ethnic violence for electoral gain. Informal interactions include joint participation in festivals, families from different communities eating together or allowing their children to play with one another. Every day engagement between ethnic groups at the village level can help to sustain the peace in the face of national level shocks like an ethnic riot in another part of the country. In times of ethnic tension, these communities can quell rumors, police neighborhoods and come together to resist any attempts by politicians to polarize the community. The stronger the inter-ethnic networks are, the harder it is for politicians to polarize the community even if it may be in their political interest to do so.\n\nHowever, in cities, where the population tends to be much higher, informal interactions between ethnic groups might not be sufficient to prevent violence. This is because many more links are needed to connect everyone, and therefore it is much more difficult to form and strengthen inter-ethnic ties. In cities, formal inter-ethnic associations like trade unions, business associations and professional organizations are more effective in encouraging inter-ethnic interactions that could prevent ethnic violence in the future. These organizations force ethnic groups to come together based on shared economic interests that overcomes any pre-existing ethnic differences. For example, inter-ethnic business organizations serve to connect the business interests of different ethnic groups which would increase their desire to maintain ethnic harmony. Any ethnic tension or outbreak of violence will go against their economic interests and therefore, over time, the salience of ethnic identity diminishes.\n\nInteractions between ethnic groups in formal settings can also help countries torn apart by ethnic violence to recover and break down ethnic divisions. Paula Pickering, a political scientist, who studies peace-building efforts in Bosnia, finds that formal workplaces are often the site where inter-ethnic ties are formed. She claims that mixed workplaces lead to repeated inter-ethnic interaction where norms of professionalism compel everyone to cooperate and to treat each other with respect, making it easier for individuals belonging to the minority group to reach out and form relationships with everyone else. Nevertheless, Giuliano's research in Russia has shown that economic grievances, even in a mixed workplace, can be politicized on ethnic lines.\n\n\n"}
{"id": "52217", "url": "https://en.wikipedia.org/wiki?curid=52217", "title": "Free-rider problem", "text": "Free-rider problem\n\nIn economics, the free-rider problem occurs when those who benefit from resources, public goods, or services do not pay for them, which results in an underprovision of those goods or services. For example, a free-rider may frequently ask for available parking lots (public goods) from those who have already paid for them, in order to benefit from free parking. That is, the free-rider may use the parking even more than the others without paying a penny. The free-rider problem is the question of how to limit free riding and its negative effects in these situations. The free-rider problem may occur when property rights are not clearly defined and imposed.\n\nThe free-rider problem is common with goods which are non-excludable, including public goods and situations of the Tragedy of the Commons.\n\nAlthough the term \"free rider\" was first used in economic theory of public goods, similar concepts have been applied to other contexts, including collective bargaining, antitrust law, psychology and political science. For example, some individuals in a team or community may reduce their contributions or performance if they believe that one or more other members of the group may free ride.\n\nFree riding is a problem of economic inefficiency when it leads to the under-production or over-consumption of a good. For example, when people are asked how much they value a particular public good, with that value measured in terms of how much money they would be willing to pay, their tendency is to under report their valuations.\n\nGoods which are subject to free riding are usually characterized by the inability to exclude non-payers. This problem is sometimes compounded by the fact that common-property goods are characterized by rival consumption. Not only can consumers of common-property goods benefit without payment, but consumption by one imposes an opportunity cost on others. This will lead to overconsumption and even possibly exhaustion or destruction of the common-property good. If too many people start to free ride, a system or service will eventually not have enough resources to operate.\n\nThe other problem of free-riding is experienced when the production of goods does not consider the external costs, particularly the use of ecosystem services.\n\nAn assurance contract is a contract in which participants make a binding pledge to contribute to building a public good, contingent on a quorum of a predetermined size being reached. Otherwise the good is not provided and any monetary contributions are refunded.\n\nA dominant assurance contract is a variation in which an entrepreneur creates the contract and refunds the initial pledge plus an additional sum of money if the quorum is not reached. (The entrepreneur profits by collecting a fee if the quorum is reached and the good is provided.) In game-theoretic terms this makes pledging to build the public good a dominant strategy: the best move is to pledge to the contract regardless of the actions of others.\nA \"Coasian solution\", named for the economist Ronald Coase, proposes that potential beneficiaries of a public good can negotiate to pool their resources and create it, based on each party's self-interested willingness to pay. His treatise, \"The Problem of Social Cost\" (1960), argued that if the transaction costs between potential beneficiaries of a public good are low—that it is easy for potential beneficiaries to find each other and organize a pooling their resources based upon the good's value to each of them—that public goods could be produced without government action.\n\nMuch later, Coase himself wrote that while what had become known as the Coase Theorem had explored the implications of zero transaction costs, he had actually intended to use this construct as a stepping-stone to understand the real world of positive transaction costs, corporations, legal systems and government actions:I examined what would happen in a world in which transaction costs were assumed to be zero. My aim in doing so was not to describe what life would be like in such a world but to provide a simple setting in which to develop the analysis and, what was even more important, to make clear the fundamental role which transaction costs do, and should, play in the fashioning of the institutions which make up the economic system.Coase also wrote:The world of zero transaction costs has often been described as a Coasian world. Nothing could be further from the truth. It is the world of modern economic theory, one which I was hoping to persuade economists to leave. What I did in \"The Problem of Social Cost\" was simply to shed light on some of its properties. I argued in such a world the allocation of resources would be independent of the legal position, a result which Stigler dubbed the \"Coase theorem\".Thus, while Coase himself appears to have considered the \"Coase theorem\" and Coasian solutions as simplified constructs to ultimately consider the real 20th-century world of governments and laws and corporations, these concepts have become attached to a world where transaction costs were much lower, and government intervention would unquestionably be less necessary.\n\nA minor alternative, especially for information goods, is for the producer to refuse to release a good to the public until payment to cover costs is met. Author Stephen King, for instance, authored chapters of a new novel downloadable for free on his website while stating that he would not release subsequent chapters unless a certain amount of money was raised. Sometimes dubbed \"holding for ransom\", this method of public goods production is a modern application of the street performer protocol for public goods production. Unlike assurance contracts, its success relies largely on social norms to ensure (to some extent) that the threshold is reached and partial contributions are not wasted.\n\nOne of the purest Coasian solutions today is the new phenomenon of Internet crowdfunding. Here rules are enforced by computer algorithms and legal contracts as well as social pressure. For example, on the Kickstarter site, each funder authorizes a credit card purchase to buy a new product or receive other promised benefits, but no money changes hands until the funding goal is met. Because automation and the Internet so reduce the transaction costs for pooling resources, project goals of only a few hundred dollars are frequently crowdfunded, far below the costs of soliciting traditional investors.\n\nIf voluntary provision of public goods will not work, then the solution is making their provision involuntary. This saves each of us from our own tendency to be a free rider, while also assuring us that no one else will be allowed to free ride. One frequently proposed solution to the problem is for governments or states to impose taxation to fund the production of public goods. This does not actually solve the theoretical problem because \"good government\" is itself a public good. Thus it is difficult to ensure the government has an incentive to provide the optimum amount even if it were possible for the government to determine precisely what amount would be optimum. These issues are studied by public choice theory and public finance.\n\nSometimes the government provides public goods using \"unfunded mandates\". An example is the requirement that every car be fit with a catalytic converter. This may be executed in the private sector, but the end result is predetermined by the state: the individually involuntary provision of the public good clean air. Unfunded mandates have also been imposed by the U.S. federal government on the state and local governments, as with the Americans with Disabilities Act, for example.\n\nRegardless the role of the government is provide vital goods to all individuals, some of which they cannot obtain on themselves. In order to ensure that government services are properly funded taxes and other government controlled entities are enforced. Although enforced taxes deter the free-rider problem many contend that some goods should be excluded and made into private goods. However, this not possible with all goods such as pure public goods that are inseparable and inclusive, thus require \"provision by public means\". In short, the government has a responsibility to ensure that the social welfare of individuals is met as opposed to privatized goods.\n\nA government may subsidize production of a public good in the private sector. Unlike government provision, subsidies may result in some form of a competitive market. The potential for cronyism (for example, an alliance between political insiders and the businesses receiving subsidies) can be limited with secret bidding for the subsidies or application of the subsidies following clear general principles. Depending on the nature of a public good and a related subsidy, principal–agent problems can arise between the citizens and the government or between the government and the subsidized producers; this effect and counter-measures taken to address it can diminish the benefits of the subsidy.\n\nSubsidies can also be used in areas with a potential for non-individualism: For instance, a state may subsidize devices to reduce air pollution and appeal to citizens to cover the remaining costs.\n\nSimilarly, a joint-product model analyzes the collaborative effect of joining a private good to a public good. For example, a tax deduction (private good) can be \"tied\" to a donation to a charity (public good). It can be shown that the provision of the public good increases when tied to the private good, as long as the private good is provided by a monopoly (otherwise the private good would be provided by competitors without the link to the public good).\n\nThe study of collective action shows that public goods are still produced when one individual benefits more from the public good than it costs him to produce it; examples include benefits from individual use, intrinsic motivation to produce, and business models based on selling complement goods. A group that contains such individuals is called a privileged group. A historical example could be a downtown entrepreneur who erects a street light in front of his shop to attract customers; even though there are positive external benefits to neighboring nonpaying businesses, the added customers to the paying shop provide enough revenue to cover the costs of the street light.\n\nThe existence of privileged groups may not be a complete solution to the free rider problem, however, as underproduction of the public good may still result. The street light builder, for instance, would not consider the added benefit to neighboring businesses when determining whether to erect his street light, making it possible that the street light isn't built when the cost of building is too high for the single entrepreneur even when the total benefit to all the businesses combined exceeds the cost.\n\nAn example of the privileged group solution could be the Linux community, assuming that users derive more benefit from contributing than it costs them to do it. For more discussion on this topic see also Coase's Penguin.\n\nAnother example is those musicians and writers who create music and writings for their own personal enjoyment, and publish because they enjoy having an audience. Financial incentives are not necessary to ensure the creation of these public goods. Whether this creates the correct production level of writings and music is an open question.\n\nAnother method of overcoming the free rider problem is to simply eliminate the profit incentive for free riding by buying out all the potential free riders. A property developer that owned an entire city street, for instance, would not need to worry about free riders when erecting street lights since he owns every business that could benefit from the street light without paying. Implicitly, then, the property developer would erect street lights until the marginal social benefit met the marginal social cost. In this case, they are equivalent to the private marginal benefits and costs.\n\nWhile the purchase of all potential free riders may solve the problem of underproduction due to free riders in smaller markets, it may simultaneously introduce the problem of underproduction due to monopoly. Additionally, some markets are simply too large to make a buyout of all beneficiaries feasible—this is particularly visible with public goods that affect everyone in a country.\n\nAnother solution, which has evolved for information goods, is to introduce exclusion mechanisms which turn public goods into club goods. One well-known example is copyright and patent laws. These laws, which in the 20th century came to be called intellectual property laws, attempt to remove the natural non-excludability by prohibiting reproduction of the good. Although they can address the free rider problem, the downside of these laws is that they imply private monopoly power and thus are not Pareto-optimal.\n\nFor example, in the United States, the patent rights given to pharmaceutical companies encourage them to charge high prices (above marginal cost) and to advertise to convince patients to persuade their doctors to prescribe the drugs. Likewise, copyright provides an incentive for a publisher to act like The Dog in the Manger, taking older works out of print so as not to cannibalize revenue from the publisher's own new works.\n\nThe laws also end up encouraging patent and copyright owners to sue even mild imitators in court and to lobby for the extension of the term of the exclusive rights in a form of rent seeking.\n\nThese problems with the club-good mechanism arise because the underlying marginal cost of giving the good to more people is low or zero, but, because of the limits of price discrimination those who are unwilling or unable to pay a profit-maximizing price do not gain access to the good.\n\nIf the costs of the exclusion mechanism are not higher than the gain from the collaboration, club goods can emerge naturally. James M. Buchanan showed in his seminal paper that clubs can be an efficient alternative to government interventions.\n\nOn the other hand, the inefficiencies and inequities of club goods exclusions sometimes cause potentially excludable club goods to be treated as public goods, and their production financed by some other mechanism. Examples of such \"natural\" club goods include natural monopolies with very high fixed costs, private golf courses, cinemas, cable television and social clubs. This explains why many such goods are often provided or subsidized by governments, co-operatives or volunteer associations, rather than being left to be supplied by profit-minded entrepreneurs. These goods are often known as \"social goods\".\n\nJoseph Schumpeter claimed that the \"excess profits\", or profits over normal profit, generated by the copyright or patent monopoly will attract competitors that will make technological innovations and thereby end the monopoly. This is a continual process referred to as \"Schumpeterian creative destruction\", and its applicability to different types of public goods is a source of some controversy. The supporters of the theory point to the case of Microsoft, for example, which has been increasing its prices (or lowering its products' quality), predicting that these practices will make increased market shares for Linux and Apple largely inevitable.\n\nA nation can be seen as a club whose members are its citizens. Government would then be the manager of this club. This is further studied in the Theory of the State.\n\nWhen enough people do not think like free-riders, the private and voluntary provision of public goods may be successful. For example, a free rider might come to a public park to enjoy its beauty, yet discard litter that makes it less enjoyable for others. Other public-spirited individuals don't do this and might even pick up existing litter. Reasons for the act could be that the person derives pleasure from helping their community, feels ashamed if their neighbors or friends saw them, or could be emotionally attached to the public good. People unconsciously adapt their behavior to that of their peers; this is conformity. Even people who engaged in free-riding by littering elsewhere are less likely to if they see others hold on to their trash.\n\nSocial norms can be observed wherever people interact, not only in physical spaces but in virtual communities on the Internet. For example, if a disabled person boards a crowded bus, everyone expects that some able-bodied person will volunteer their seat. The same social norm, although executed in a different environment, can also be applied to the Internet. If a user enters a discussion in a chat room and continues to use all capital letters or to make personal attacks (\"flames\") when addressing other users, the culprit may realize he or she has been blocked by other participants. As in real life, users learning to adapt to the social norms of cyberspace communities provide a public good—here, not suffering disruptive online behavior—for all the participants.\n\nExperimental literature suggests that free riding can be overcome without any state intervention. Peer-to-peer punishment, that is, members sanction those members that do not contribute to the public good at a cost, is sufficient to establish and maintain cooperation. Such punishment is often considered altruistic, because it comes at a cost to the punisher, however, the exact nature remains to be explored. Whether costly punishment can explain cooperation is disputed. Recent research finds that costly punishment is less effective in real world environments. For example, punishment works relatively badly under imperfect information, where people cannot observe the behavior of others perfectly.\n\nOrganizations such as the Red Cross, public radio and television or a volunteer fire department provide public goods to the majority at the expense of a minority who voluntarily participate or contribute funds. Contributions to online collaborative media like Wikipedia and other wiki projects, and free software projects such as Linux are another example of relatively few contributors providing a public good (information) freely to all readers or software users.\n\nProposed explanations for altruistic behavior include biological altruism and reciprocal altruism. For example, voluntary groups such as labor unions and charities often have a federated structure, probably in part because voluntary collaboration emerges more readily in smaller social groups than in large ones (e.g., see Dunbar's number).\n\nWhile both biological and reciprocal altruism are observed in other animals, our species' complex social behaviors take these raw materials much farther. Philanthropy by wealthy individuals—some, such as Andrew Carnegie giving away their entire vast fortunes—have historically provided a multitude of public goods for others. One major impact was the Rockefeller Foundation's development of the \"Green Revolution\" hybrid grains that probably saved many millions of people from starvation in the 1970s. Christian missionaries, who typically spend large parts of their lives in remote, often dangerous places, have had disproportionate impact compared with their numbers worldwide for centuries. Communist revolutionaries in the 20th century had similar dedication and outsized impacts. International relief organizations such as Doctors Without Borders, Save the Children and Amnesty International have benefited millions, while also occasionally costing workers their lives. For better and for worse, humans can conceive of, and sacrifice for, an almost infinite variety of causes in addition to their biological kin.\n\nVoluntary altruistic organizations often motivate their members by encouraging deep-seated personal beliefs, whether religious or other (such as social justice or environmentalism) that are taken \"on faith\" more than proved by rational argument. When individuals resist temptations to free riding (e.g., stealing) because they hold these beliefs (or because they fear the disapproval of others who do), they provide others with public goods that might be difficult or impossible to \"produce\" by administrative coercion alone.\n\nOne proposed explanation for the ubiquity of religious belief in human societies is multi-level selection: altruists often lose out within groups, but groups with more altruists win. A group whose members believe a \"practical reality\" that motivates altruistic behavior may out-compete other groups whose members' perception of \"factual reality\" causes them to behave selfishly. A classic example is a soldier's willingness to fight for his tribe or country. Another example given in evolutionary biologist David Sloan Wilson's \"Darwin's Cathedral\" is the early Christian church under the late Roman Empire; because Roman society was highly individualistic, during frequent epidemics many of the sick died not of the diseases \"per se\" but for lack of basic nursing. Christians, who believed in an afterlife, were willing to nurse the sick despite the risks. Although the death rate among the nurses was high, the average Christian had a \"much better chance\" of surviving an epidemic than other Romans did, and the community prospered.\n\nReligious and non-religious traditions and ideologies (such as nationalism and patriotism) are in full view when a society is in crisis and public goods such as defense are most needed. Wartime leaders invoke their God's protection and claim that their society's most hallowed traditions are at stake. For example, according to President Abraham Lincoln's Gettysburg Address during the American Civil War, the Union was fighting so \"that government of the people, by the people, for the people, shall not perish from the earth\". Such voluntary, if exaggerated, exhortations complement forcible measures—taxation and conscription—to motivate people to make sacrifices for their cause.\n\n\n"}
{"id": "4414154", "url": "https://en.wikipedia.org/wiki?curid=4414154", "title": "Frozen conflict", "text": "Frozen conflict\n\nIn international relations, a frozen conflict is a situation in which active armed conflict has been brought to an end, but no peace treaty or other political framework resolves the conflict to the satisfaction of the combatants. Therefore, legally the conflict can start again at any moment, creating an environment of insecurity and instability.\n\nThe term has been commonly used for post-Soviet conflicts, but it has also often been applied to other perennial territorial disputes. The \"de facto\" situation that emerges may match the \"de jure\" position asserted by one party to the conflict; for example, Russia claims and effectively controls Crimea following the 2014 Crimean crisis despite Ukraine's continuing claim to the region. Alternatively, the \"de facto\" situation may not match either side's official claim. The division of Korea is an example of the latter situation: both the Republic of Korea and the Democratic People's Republic of Korea officially assert claims to the entire peninsula; however, there exists a well-defined border between the two countries' areas of control.\n\nFrozen conflicts sometimes result in partially recognized states. For example, the Republic of South Ossetia, a product of the frozen Georgian–Ossetian conflict, is recognized by eight other states, including five UN members; the other three of these entities are partially recognized states themselves.\n\nFollowing the breakup of the Soviet Union in 1991, a number of conflicts arose in areas of some of the post-Soviet states, usually where the new international borders did not match the ethnic affiliations of local populations. These conflicts have largely remained \"frozen\", with disputed areas under the \"de facto\" control of entities other than the countries to which they are internationally recognized as belonging, and which still consider those areas to be part of their territory.\n\nSince the ceasefire which ended the Transnistria War (1990–1992), the Russian-influenced breakaway republic of Transnistria has controlled the easternmost strip of the territory of Moldova. The republic is internationally unrecognized, and Moldova continues to claim the territory.\n\nNagorno-Karabakh is a disputed territory, internationally recognized as part of Azerbaijan, but most of the region is governed by the Republic of Artsakh (formerly named Nagorno-Karabakh Republic), a \"de facto\" independent state with Armenian ethnic majority established on the basis of the Nagorno-Karabakh Autonomous Oblast of the Azerbaijan Soviet Socialist Republic. Azerbaijan has not exercised political authority over the region since the advent of the Karabakh movement in 1988. Since the end of the Nagorno-Karabakh War in 1994, representatives of the governments of Armenia and Azerbaijan have been holding peace talks mediated by the OSCE Minsk Group on the region's disputed status.\n\nThe Abkhaz–Georgian conflict and Georgian–Ossetian conflict have led to the creation of two largely unrecognized states within the internationally recognized territory of Georgia. The 1991–92 South Ossetia War and the War in Abkhazia (1992–93), followed by the Russo-Georgian War of August 2008, have left the Russian-backed Republic of South Ossetia and Republic of Abkhazia in \"de facto\" control of the South Ossetia and Abkhazia regions in north and northwest Georgia.\n\nIn 2014, Crimea was occupied by the Russian troops without insignia while Ukraine was still recovering from large scale violence in the capital, and soon afterwards was admitted into the Russian Federation. This is widely regarded as an annexation of the peninsula by Russia, and is considered likely to result in another post-Soviet frozen conflict. While there are similarities between Transnistria, Abkhazia and the 2014–2015 War in Donbass, where the unrecognized Donetsk People's Republic and Lugansk People's Republic have taken \"de facto\" control of areas in the Donbass region in eastern Ukraine, the conflict in Donbass is not a frozen conflict yet as ceasefire violations are keeping the fighting on a low burner. However, some experts predict a frozen future for this conflict as well.\n\nIndia and Pakistan have fought at least three wars over the disputed region of Kashmir in 1947, 1965, and 1999. India claims the entire area of the former princely state of Kashmir and Jammu, of which it administers approximately 43%. Its claims are contested by Pakistan, which controls approximately 37% of the region and urges for plebiscite in Kashmir. The remaining territory is controlled by the People's Republic of China, with which India is again in dispute and have fought the Sino-Indian War.\n\nThe conflict between mainland China and Taiwan remains frozen since 1949. Both the People's Republic of China in the Mainland and the Republic of China in Taiwan consider the other to be part of its territory. While the latter especially is not recognized by a majority of countries and states internationally, it remains a \"de facto\" independent administration in Taiwan, Penghu, Kinmen and Matsu, and PRC's \"de facto\" administration remains in the Mainland.\n\nThe division of Korea was frozen from 1953, when a ceasefire ended the Korean War; until 27 April 2018, when the two countries agreed to end the war formally. Both North Korea and South Korea governments claim the entire peninsula, while \"de facto\" control is divided along the military demarcation line in the Korean Demilitarized Zone.\n\nHostilities of the 1991 Gulf War ended when the United Nations and Iraq signed a ceasefire agreement on April 3, 1991; Kuwait was liberated from being annexed by Iraq and its sovereignty was recognized by the latter. Due to sporadic conflicts through the Iraqi no-fly zones, the war remained frozen for the time being until 12 years later when the United States and its \"Coalition of the willing\" launched the invasion of Iraq and removed dictator Saddam Hussein from power over the non-compliance with UN Resolutions passed against Iraq following the 1991 war.\n\nThe Arab–Israeli conflict is a perennial conflict between Israel and its Arab neighbours, including the Palestinian territories. Israel refuses to recognize Palestinian statehood without an assured peace deal, while some Arab countries and groups refuse to recognize Israel. Israel has \"de facto\" control of East Jerusalem and claims it as its integral territory, although it is not internationally recognized as such. Similarly most of the Golan Heights are currently under \"de facto\" Israeli control and civil administration, whereas the international community rejects that claim.\n\nThe dispute over the status of Kosovo remains frozen since the end of the Kosovo War, fought in 1998–1999 between Yugoslav forces (FR Yugoslavia) and the ethnically Albanian Kosovo Liberation Army. The Kosovo region has been administered independently since the war, and the Republic of Kosovo declared its independence from Serbia in 2008, but it is not recognized by all countries worldwide, and Serbia still considers Kosovo part of its territory.\n\nThe Cyprus dispute has been frozen since 1974. The northern part of Cyprus is under the \"de facto\" control of the Turkish Republic of Northern Cyprus, but this is not recognized internationally except by Turkey.\n\nThe Western Sahara conflict has been largely frozen since a ceasefire in 1991, although various disturbances have broken out since then. Control of the territory of Western Sahara remains divided between Morocco and the Polisario Front.\n\n"}
{"id": "10415136", "url": "https://en.wikipedia.org/wiki?curid=10415136", "title": "Generali Foundation", "text": "Generali Foundation\n\nThe Generali Foundation was established in 1988 by the Generali Group Austria as a private and non-profit-making art association for the promotion of contemporary art. Situated in Vienna, Austria, it is one of the important museums specialised in collecting and exhibiting conceptual and performance art pieces in different media (documents, video art, installation art etc.). \n\nThe collection of the Generali Foundation contains 2100 works by 170 international artists, mainly from the 1960s to today. The Generali Foundation has also edited numerous books and catalogues on conceptual art, performance art, and other contemporary art practices.\n\n"}
{"id": "634216", "url": "https://en.wikipedia.org/wiki?curid=634216", "title": "Hard problem of consciousness", "text": "Hard problem of consciousness\n\nThe hard problem of consciousness is the problem of explaining how and why sentient organisms have qualia or phenomenal experiences—how and why it is that some internal states are felt states, such as heat or pain, rather than unfelt states, as in a thermostat or a toaster. The philosopher David Chalmers, who introduced the term \"hard problem\" of consciousness, contrasts this with the \"easy problems\" of explaining the ability to discriminate, integrate information, report mental states, focus attention, etc. Easy problems are easy because all that is required for their solution is to specify a mechanism that can perform the function. That is, their proposed solutions, regardless of how complex or poorly understood they may be, can be entirely consistent with the modern materialistic conception of natural phenomena. Chalmers claims that the problem of experience is distinct from this set and that the problem of experience will \"persist even when the performance of all the relevant functions is explained\".\n\nThe existence of a \"hard problem\" is controversial and has been disputed by philosophers such as Daniel Dennett and cognitive neuroscientists such as Stanislas Dehaene.\n\nIn \"Facing Up to the Problem of Consciousness\" (1995), Chalmers wrote:\n\nIt is undeniable that some organisms are subjects of experience. But the question of how it is that these systems are subjects of experience is perplexing. Why is it that when our cognitive systems engage in visual and auditory information-processing, we have visual or auditory experience: the quality of deep blue, the sensation of middle C? How can we explain why there is something it is like to entertain a mental image, or to experience an emotion? It is widely agreed that experience arises from a physical basis, but we have no good explanation of why and how it so arises. Why should physical processing give rise to a rich inner life at all? It seems objectively unreasonable that it should, and yet it does.\n\nIn the same paper, he also wrote:\n\nThe really hard problem of consciousness is the problem of experience. When we think and perceive there is a whir of information processing, but there is also a subjective aspect.\n\nThe philosopher Raamy Majeed noted in 2016 that the hard problem is, in fact, associated with two \"explanatory targets\":\n\nThe first fact concerns the relationship between the physical and the phenomenal (i.e., how and why are some physical states felt states), whereas the second concerns the very nature of the phenomenal itself (i.e., what does the felt state feel like?). Most responses to the hard problem are aimed at explaining either one of these facts or both.\n\nChalmers contrasts the hard problem with a number of (relatively) easy problems that consciousness presents. He emphasizes that what the easy problems have in common is that they all represent some ability, or the performance of some function or behavior. Examples of easy problems include:\n\n\nOther formulations of the \"hard problem\" include:\n\nThe hard problem has scholarly antecedents considerably earlier than Chalmers, as Chalmers himself has pointed out.\n\nThe physicist and mathematician Isaac Newton wrote in a 1672 letter to Henry Oldenburg:\n\nto determine by what modes or actions light produceth in our minds the phantasm of colour is not so .\n\nIn \"An Essay Concerning Human Understanding\" (1690), the philosopher and physician John Locke argued:\n\nDivide matter into as minute parts as you will (which we are apt to imagine a sort of spiritualizing or making a thinking thing of it) vary the figure and motion of it as much as you please—a globe, cube, cone, prism, cylinder, etc., whose diameters are but 1,000,000th part of a gry, will operate not otherwise upon other bodies of proportionable bulk than those of an inch or foot diameter—and you may as rationally expect to produce sense, thought, and knowledge, by putting together, in a certain figure and motion, gross particles of matter, as by those that are the very minutest that do anywhere exist. They knock, impel, and resist one another, just as the greater do; and that is all they can do... [I]t is impossible to conceive that matter, either with or without motion, could have originally in and from itself sense, perception, and knowledge; as is evident from hence that then sense, perception, and knowledge must be a property eternally inseparable from matter and every particle of it.\n\nThe polymath and philosopher Gottfried Leibniz wrote in 1714, as an example also known as Leibniz's gap:\n\nMoreover, it must be confessed that perception and that which depends upon it are inexplicable on mechanical grounds, that is to say, by means of figures and motions. And supposing there were a machine, so constructed as to think, feel, and have perception, it might be conceived as increased in size, while keeping the same proportions, so that one might go into it as into a mill. That being so, we should, on examining its interior, find only parts which work one upon another, and never anything by which to explain a perception.\n\nThe philosopher and political economist J.S. Mill wrote in \"A System of Logic\" (1843), Book V, Chapter V, section 3:\n\nNow I am far from pretending that it may not be capable of proof, or that it is not an important addition to our knowledge if proved, that certain motions in the particles of bodies are the conditions of the production of heat or light; that certain assignable physical modifications of the nerves may be the conditions not only of our sensations or emotions, but even of our thoughts; that certain mechanical and chemical conditions may, in the order of nature, be sufficient to determine to action the physiological laws of life. All I insist upon, in common with every thinker who entertains any clear idea of the logic of science, is, that it shall not be supposed that by proving these things one step would be made towards a real explanation of heat, light, or sensation; or that the generic peculiarity of those phenomena can be in the least degree evaded by any such discoveries, however well established. Let it be shown, for instance, that the most complex series of physical causes and effects succeed one another in the eye and in the brain to produce a sensation of colour; rays falling on the eye, refracted, converging, crossing one another, making an inverted image on the retina, and after this a motion—let it be a vibration, or a rush of nervous fluid, or whatever else you are pleased to suppose, along the optic nerve—a propagation of this motion to the brain itself, and as many more different motions as you choose; still, at the end of these motions, there is something which is not motion, there is a feeling or sensation of colour. Whatever number of motions we may be able to interpolate, and whether they be real or imaginary, we shall still find, at the end of the series, a motion antecedent and a colour consequent. The mode in which any one of the motions produces the next, may possibly be susceptible of explanation by some general law of motion: but the mode in which the last motion produces the sensation of colour, cannot be explained by any law of motion; it is the law of colour: which is, and must always remain, a peculiar thing. Where our consciousness recognises between two phenomena an inherent distinction; where we are sensible of a difference which is not merely of degree, and feel that no adding one of the phenomena to itself would produce the other; any theory which attempts to bring either under the laws of the other must be false; though a theory which merely treats the one as a cause or condition of the other, may possibly be true.\n\nThe biologist T.H. Huxley wrote in 1868:\n\nBut what consciousness is, we know not; and how it is that anything so remarkable as a state of consciousness comes about as the result of irritating nervous tissue, is just as unaccountable as the appearance of the Djin when Aladdin rubbed his lamp in the story, or as any other ultimate fact of nature.\n\nThe philosopher Thomas Nagel argued in 1974:\n\nIf physicalism is to be defended, the phenomenological features must themselves be given a physical account. But when we examine their subjective character it seems that such a result is impossible. The reason is that every subjective phenomenon is essentially connected with a single point of view, and it seems inevitable that an objective, physical theory will abandon that point of view.\n\nSince 1990, researchers including the molecular biologist Francis Crick and the neuroscientist Christof Koch have made significant progress toward identifying which neurobiological events occur concurrently to the experience of subjective consciousness. These postulated events are referred to as \"neural correlates of consciousness\" or NCCs. However, this research arguably addresses the question of \"which\" neurobiological mechanisms are linked to consciousness but not the question of \"why\" they should give rise to consciousness at all, the latter being the hard problem of consciousness as Chalmers formulated it. In \"On the Search for the Neural Correlate of Consciousness\", Chalmers said he is confident that, granting the principle that something such as what he terms global availability can be used as an indicator of consciousness, the neural correlates will be discovered \"in a century or two\". Nevertheless, he stated regarding their relationship to the hard problem of consciousness:\n\nOne can always ask why these processes of availability should give rise to consciousness in the first place. As yet we cannot explain why they do so, and it may well be that full details about the processes of availability will still fail to answer this question. Certainly, nothing in the standard methodology I have outlined answers the question; that methodology assumes a relation between availability and consciousness, and therefore does nothing to explain it. [...] So the hard problem remains. But who knows: Somewhere along the line we may be led to the relevant insights that show why the link is there, and the hard problem may then be solved.\n\nThe neuroscientist and Nobel laureate Eric Kandel wrote that locating the NCCs would not solve the hard problem, but rather one of the so-called easy problems to which the hard problem is contrasted. Kandel went on to note Crick and Koch's suggestion that once the binding problem—understanding what accounts for the unity of experience—is solved, it will be possible to solve the hard problem empirically. However, neuroscientist Anil Seth argued that emphasis on the so-called hard problem is a distraction from what he calls the \"real problem\": understanding the neurobiology underlying consciousness, namely the neural correlates of various conscious processes. This more modest goal is the focus of most scientists working on consciousness. Psychologist Susan Blackmore believes, by contrast, that the search for the neural correlates of consciousness is futile and itself predicated on an erroneous belief in the hard problem of consciousness.\n\nIntegrated information theory (IIT), developed by the neuroscientist and psychiatrist Giulio Tononi in 2004 and more recently also advocated by Koch, is one of the most discussed models of consciousness in neuroscience and elsewhere. The theory proposes an identity between consciousness and integrated information, with the latter item (denoted as Φ) defined mathematically and thus in principle measurable. The hard problem of consciousness, write Tononi and Koch, may indeed be intractable when working from matter to consciousness. However, because IIT inverts this relationship and works from phenomenological axioms to matter, they say it could be able to solve the hard problem. In this vein, proponents have said the theory goes beyond identifying human neural correlates and can be extrapolated to all physical systems. Tononi wrote (along with two colleagues):\n\nWhile identifying the “neural correlates of consciousness” is undoubtedly important, it is hard to see how it could ever lead to a satisfactory explanation of what consciousness is and how it comes about. As will be illustrated below, IIT offers a way to analyze systems of mechanisms to determine if they are properly structured to give rise to consciousness, how much of it, and of which kind.\n\nAs part of a broader critique of IIT, Michael Cerullo suggested that the theory's proposed explanation is in fact for what he dubs (following Scott Aaronson) the \"Pretty Hard Problem\" of methodically inferring which physical systems are conscious—but would not solve Chalmers' hard problem. \"Even if IIT is correct,\" he argues, \"it does not explain why integrated information generates (or is) consciousness.\"\n\nSome philosophers, including David Chalmers in the late 20th century and Alfred North Whitehead earlier in the 1900s, argued that conscious experience is a fundamental constituent of the universe, a form of panpsychism sometimes referred to as panexperientialism. Chalmers argued that a \"rich inner life\" is not logically reducible to the functional properties of physical processes. He states that consciousness must be described using nonphysical means. This description involves a fundamental ingredient capable of clarifying phenomena that have not been explained using physical means. Use of this fundamental property, Chalmers argues, is necessary to explain certain functions of the world, much like other fundamental features, such as mass and time, and to explain significant principles in nature.\n\nThe philosopher Thomas Nagel posited in 1974 that experiences are essentially subjective (accessible only to the individual undergoing them—i.e., felt only by the one feeling them), while physical states are essentially objective (accessible to multiple individuals). So at this stage, he argued, we have no idea what it could even mean to claim that an essentially subjective state just \"is\" an essentially non-subjective state (i.e., how and why a felt state is just a functional state). In other words, we have no idea of what reductivism really amounts to.\n\nNew mysterianism, such as that of the philosopher Colin McGinn, proposes that the human mind, in its current form, will not be able to explain consciousness.\n\nSome philosophers, such as Daniel Dennett and Peter Hacker oppose the idea that there is a hard problem. These theorists have argued that once we really come to understand what consciousness is, we will realize that the hard problem is unreal. For instance, Dennett asserts that the so-called hard problem will be solved in the process of answering the \"easy\" ones (which, as he has clarified, he does not consider \"easy\" at all). In contrast with Chalmers, he argues that consciousness is not a fundamental feature of the universe and instead will eventually be fully explained by natural phenomena. Instead of involving the nonphysical, he says, consciousness merely plays tricks on people so that it appears nonphysical—in other words, it simply seems like it requires nonphysical features to account for its powers. In this way, Dennett compares consciousness to stage magic and its capability to create extraordinary illusions out of ordinary things.\n\nTo show how people might be commonly fooled into overstating the powers of consciousness, Dennett describes a normal phenomenon called change blindness, a visual process that involves failure to detect scenery changes in a series of alternating images. He uses this concept to argue that the overestimation of the brain's visual processing implies that the conception of our consciousness is likely not as pervasive as we make it out to be. He claims that this error of making consciousness more mysterious than it is could be a misstep in any developments toward an effective explanatory theory. Critics such as Galen Strawson reply that, in the case of consciousness, even a mistaken experience retains the essential face of experience that needs to be explained, contra Dennett.\n\nTo address the question of the hard problem, or how and why physical processes give rise to experience, Dennett states that the phenomenon of having experience is nothing more than the performance of functions or the production of behavior, which can also be referred to as the easy problems of consciousness. He states that consciousness itself is driven simply by these functions, and to strip them away would wipe out any ability to identify thoughts, feelings, and consciousness altogether. So, unlike Chalmers and other dualists, Dennett says that the easy problems and the hard problem cannot be separated from each other. To him, the hard problem of experience is included among—not separate from—the easy problems, and therefore they can only be explained together as a cohesive unit.\n\nCritics of Dennett's approach, such as Chalmers and Nagel, argue that Dennett's argument misses the point of the inquiry by merely re-defining consciousness as an external property and ignoring the subjective aspect completely. This has led detractors to refer to Dennett's book \"Consciousness Explained\" as \"Consciousness Ignored\" or \"Consciousness Explained Away\". Dennett discussed this at the end of his book with a section entitled \"Consciousness Explained or Explained Away?\"\n\nLike Dennett, Hacker argues that the hard problem is fundamentally incoherent and that \"consciousness studies\", as it exists today, is \"literally a total waste of time\":\n\nThe whole endeavour of the consciousness studies community is absurd—they are in pursuit of a chimera. They misunderstand the nature of consciousness. The conception of consciousness which they have is incoherent. The questions they are asking don't make sense. They have to go back to the drawing board and start all over again.\n\nThough the most common arguments against deflationary accounts and eliminative materialism are the argument from qualia and the argument that conscious experiences are irreducible to physical states—or that current popular definitions of \"physical\" are incomplete—the objection has been posed that the one and same reality can appear in different ways, and that the numerical difference of these ways is consistent with a unitary mode of existence of the reality. Critics of the deflationary approach object that qualia are a case where a single reality cannot have multiple appearances. For example, the philosopher John Searle pointed out: \"where consciousness is concerned, the existence of the appearance is the reality\".\n\nA notable deflationary account is the higher-order theories of consciousness. In 2005, the philosopher Peter Carruthers wrote about \"recognitional concepts of experience\", that is, \"a capacity to recognize [a] type of experience when it occurs in one's own mental life\", and suggested that such a capacity does not depend upon qualia.\n\nThe philosophers Glenn Carruthers and Elizabeth Schier said in 2012 that the main arguments for the existence of a hard problem—philosophical zombies, Mary's room, and Nagel's bats—are only persuasive if one already assumes that \"consciousness must be independent of the structure and function of mental states, i.e. that there is a hard problem\". Hence, the arguments beg the question. The authors suggest that \"instead of letting our conclusions on the thought experiments guide our theories of consciousness, we should let our theories of consciousness guide our conclusions from the thought experiments\".\n\nIn 2013, the philosopher Elizabeth Irvine argued that both science and folk psychology do not treat mental states as having phenomenal properties, and therefore \"the hard problem of consciousness may not be a genuine problem for non-philosophers (despite its overwhelming obviousness to philosophers), and questions about consciousness may well 'shatter' into more specific questions about particular capacities\".\n\nThe philosopher Massimo Pigliucci distances himself from eliminativism, but he said in 2013 that the hard problem is still misguided, resulting from a \"category mistake\":\n\nOf course an explanation isn't the same as an experience, but that's because the two are completely independent categories, like colors and triangles. It is obvious that I cannot experience what it is like to be you, but I can potentially have a complete explanation of how and why it is possible to be you.\n\nIn 2017, the philosopher Marco Stango, in a paper on John Dewey's approach to the problem of consciousness (which preceded Chalmers' formulation of the hard problem by over half a century), noted that Dewey's approach would see the hard problem as the consequence of an unjustified assumption that feelings and functional behaviors are not the same physical process: \"For the Deweyan philosopher, the 'hard problem' of consciousness is a 'conceptual fact' only in the sense that it is a : the mistake of failing to see that the physical can be had as an episode of immediate sentiency.\"\n\nA complete reductionistic or mechanistic theory of consciousness must include the description of a mechanism by which the subjective aspect of consciousness is perceived and reported by people. Philosophers such as Chalmers or Nagel have rejected reductionist theories of consciousness because they believe that the reports of subjective experience constitute a vast and important body of empirical evidence which is ignored by modern reductionist theories of consciousness.\n\nDennett argued that solving the easy problem of consciousness, that is finding out how the brain works, will eventually lead to the solution of the hard problem of consciousness. In particular, the solution can be achieved by identifying the stimuli and neurological pathways whose operation generates evidence of subjective experience.\n\nNeuroscientist Michael Graziano, in his book \"Consciousness and the Social Brain\", advocates what he calls attention schema theory, in which our perception of being conscious is merely an error in perception, held by brains which evolved to hold erroneous and incomplete models of their own internal workings, just as they hold erroneous and incomplete models of their own bodies and of the external world.\n\nCognitive neuroscientist Stanislas Dehaene, in his 2014 book \"Consciousness and the Brain\", summarized the previous decades of experimental consciousness research involving reports of subjective experience, and argued that Chalmers' \"easy problems\" of consciousness are actually the hard problems and the \"hard problems\" are based only upon ill-defined intuitions that, according to Dehaene, are continually shifting as understanding evolves:\n\nOnce our intuitions are educated by cognitive neuroscience and computer simulations, Chalmers' hard problem will evaporate. The hypothetical concept of qualia, pure mental experience, detached from any information-processing role, will be viewed as a peculiar idea of the prescientific era, much like vitalism... [Just as science dispatched vitalism] the science of consciousness will keep eating away at the hard problem of consciousness until it vanishes.\n"}
{"id": "480259", "url": "https://en.wikipedia.org/wiki?curid=480259", "title": "Indecent assault", "text": "Indecent assault\n\nIndecent assault is an offence of aggravated assault in some common law-based jurisdictions. It is characterised as a sex crime and has significant overlap with offences referred to as sexual assault.\n\nIndecent assault was a broadly defined offence under sections 14 and 15 of the Sexual Offences Act 1956. It was superseded by the new offence of sexual assault under section 3 of the Sexual Offences Act 2003. Other aspects of the actus reus were merged into other offences, including rape (section 1), whose scope was expanded. Individuals may still be prosecuted for indecent assault under the 1956 Act for offences committed before the new law came into force. \n\nThe mens rea and actus reus of indecent assault were similar to that for common law assault and/or battery. However, there was an additional element of \"indecent circumstances\". \"Indecent circumstances\" were present if a \"reasonable person\" would believe it indecent, whatever the belief of the accused.\n\nIn India it is punishable under section 354 of the Indian Penal Code.\n\nIn New South Wales, the offence of indecent assault is punishable under Section 61L of the Crimes Act 1900.\n\nThe mens rea and actus reus are the same for the common law offence of assault, the only distinction being that the act committed must have a sexual connotation.\n\n"}
{"id": "15111", "url": "https://en.wikipedia.org/wiki?curid=15111", "title": "Interplanetary spaceflight", "text": "Interplanetary spaceflight\n\nInterplanetary spaceflight or interplanetary travel is travel between planets, usually within a single planetary system. In practice, spaceflights of this type are confined to travel between the planets of the Solar System.\n\nRemotely guided space probes have flown by all of the planets of the Solar System from Mercury to Neptune, with the New Horizons probe having flown by the dwarf planet Pluto and the Dawn spacecraft currently orbiting the dwarf planet Ceres. The most distant spacecraft, Voyager 1, has left the Solar System, while Pioneer 10, Pioneer 11, Voyager 2 and New Horizons are on course to leave it.\n\nIn general, planetary orbiters and landers return much more detailed and comprehensive information than fly-by missions. Space probes have been placed into orbit around all the five planets known to the ancients: first Mars (Mariner 9, 1971), then Venus (Venera 9, 1975; but landings on Venus and atmospheric probes were performed even earlier), Jupiter (Galileo, 1995), Saturn (Cassini/Huygens, 2004), and most recently Mercury (MESSENGER, March 2011), and have returned data about these bodies and their natural satellites.\n\nThe NEAR Shoemaker mission in 2000 orbited the large near-Earth asteroid 433 Eros, and was even successfully landed there, though it had not been designed with this maneuver in mind. The Japanese ion-drive spacecraft Hayabusa in 2005 also orbited the small near-Earth asteroid 25143 Itokawa, landing on it briefly and returning grains of its surface material to Earth. Another powerful ion-drive mission, Dawn, has orbited the large asteroid Vesta (July 2011 – September 2012) and later moved on to the dwarf planet Ceres, arriving in March 2015.\n\nRemotely controlled landers such as Viking, Pathfinder and the two Mars Exploration Rovers have landed on the surface of Mars and several Venera and Vega spacecraft have landed on the surface of Venus. The Huygens probe successfully landed on Saturn's moon, Titan.\n\nNo manned missions have been sent to any planet of the Solar System. NASA's Apollo program, however, landed twelve people on the Moon and returned them to Earth. The American Vision for Space Exploration, originally introduced by President George W. Bush and put into practice through the Constellation program, had as a long-term goal to eventually send human astronauts to Mars. However, on February 1, 2010, President Barack Obama proposed cancelling the program in Fiscal Year 2011. An earlier project which received some significant planning by NASA included a manned fly-by of Venus in the Manned Venus Flyby mission, but was cancelled when the Apollo Applications Program was terminated due to NASA budget cuts in the late 1960s.\n\nThe costs and risk of interplanetary travel receive a lot of publicity — spectacular examples include the malfunctions or complete failures of unmanned probes such as Mars 96, Deep Space 2 and Beagle 2 (the article List of Solar System probes gives a full list).\n\nMany astronomers, geologists and biologists believe that exploration of the Solar System provides knowledge that could not be gained by observations from Earth's surface or from orbit around Earth. But they disagree about whether manned missions make a useful scientific contribution — some think robotic probes are cheaper and safer, while others argue that either astronauts advised by Earth-based scientists, or spacefaring scientists advised by Earth-based scientists, can respond more flexibly and intelligently to new or unexpected features of the region they are exploring.\n\nThose who pay for such missions (primarily in the public sector) are more likely to be interested in benefits for themselves or for the human race as a whole. So far the only benefits of this type have been \"spin-off\" technologies which were developed for space missions and then were found to be at least as useful in other activities (NASA publicizes spin-offs from its activities).\n\nOther practical motivations for interplanetary travel are more speculative, because our current technologies are not yet advanced enough to support test projects. But science fiction writers have a fairly good track record in predicting future technologies — for example geosynchronous communications satellites (Arthur C. Clarke) and many aspects of computer technology (Mack Reynolds).\n\nMany science fiction stories (notably Ben Bova's Grand Tour stories) feature detailed descriptions of how people could extract minerals from asteroids and energy from sources including orbital solar panels (unhampered by clouds) and the very strong magnetic field of Jupiter. Some point out that such techniques may be the only way to provide rising standards of living without being stopped by pollution or by depletion of Earth's resources (for example peak oil).\n\nFinally, colonizing other parts of the Solar System would prevent the whole human species from being exterminated by any one of a number of possible events (see Human extinction). One of these possible events is an asteroid impact like the one which may have resulted in the Cretaceous–Paleogene extinction event. Although various Spaceguard projects monitor the Solar System for objects that might come dangerously close to Earth, current asteroid deflection strategies are crude and untested. To make the task more difficult, carbonaceous chondrites are rather sooty and therefore very hard to detect. Although carbonaceous chondrites are thought to be rare, some are very large and the suspected \"dinosaur-killer\" may have been a carbonaceous chondrite.\n\nSome scientists, including members of the Space Studies Institute, argue that the vast majority of mankind eventually will live in space and will benefit from doing this.\n\nOne of the main challenges in interplanetary travel is producing the very large velocity changes necessary to travel from one body to another in the Solar System.\n\nDue to the Sun's gravitational pull, a spacecraft moving farther from the Sun will slow down, while a spacecraft moving closer will speed up. Also, since any two planets are at different distances from the Sun, the planet from which the spacecraft starts is moving around the Sun at a different speed than the planet to which the spacecraft is travelling (in accordance with Kepler's Third Law). Because of these facts, a spacecraft desiring to transfer to a planet closer to the Sun must decrease its speed with respect to the Sun by a large amount in order to intercept it, while a spacecraft traveling to a planet farther out from the Sun must increase its speed substantially. Then, if additionally the spacecraft wishes to enter into orbit around the destination planet (instead of just flying by it), it must match the planet's orbital speed around the Sun, usually requiring another large velocity change.\n\nSimply doing this by brute force – accelerating in the shortest route to the destination and then matching the planet's speed – would require an extremely large amount of fuel. And the fuel required for producing these velocity changes has to be launched along with the payload, and therefore even more fuel is needed to put both the spacecraft and the fuel required for its interplanetary journey into orbit. Thus, several techniques have been devised to reduce the fuel requirements of interplanetary travel.\n\nAs an example of the velocity changes involved, a spacecraft travelling from low Earth orbit to Mars using a simple trajectory must first undergo a change in speed (also known as a delta-v), in this case an increase, of about 3.8 km/s. Then, after intercepting Mars, it must change its speed by another 2.3 km/s in order to match Mars' orbital speed around the Sun and enter an orbit around it. For comparison, launching a spacecraft into low Earth orbit requires a change in speed of about 9.5 km/s.\n\nFor many years economical interplanetary travel meant using the Hohmann transfer orbit. Hohmann demonstrated that the lowest energy route between any two orbits is an elliptical \"orbit\" which forms a tangent to the starting and destination orbits. Once the spacecraft arrives, a second application of thrust will re-circularize the orbit at the new location. In the case of planetary transfers this means directing the spacecraft, originally in an orbit almost identical to Earth's, so that the aphelion of the transfer orbit is on the far side of the Sun near the orbit of the other planet. A spacecraft traveling from Earth to Mars via this method will arrive near Mars orbit in approximately 8.5 months, but because the orbital velocity is greater when closer to the center of mass (i.e. the Sun) and slower when farther from the center, the spacecraft will be traveling quite slowly and a small application of thrust is all that is needed to put it into a circular orbit around Mars. If the manoeuver is timed properly, Mars will be \"arriving\" under the spacecraft when this happens.\n\nThe Hohmann transfer applies to any two orbits, not just those with planets involved. For instance it is the most common way to transfer satellites into geostationary orbit, after first being \"parked\" in low Earth orbit. However, the Hohmann transfer takes an amount of time similar to ½ of the orbital period of the outer orbit, so in the case of the outer planets this is many years – too long to wait. It is also based on the assumption that the points at both ends are massless, as in the case when transferring between two orbits around Earth for instance. With a planet at the destination end of the transfer, calculations become considerably more difficult.\n\nThe gravitational slingshot technique uses the gravity of planets and moons to change the speed and direction of a spacecraft without using fuel. In typical example, a spacecraft is sent to a distant planet on a path that is much faster than what the Hohmann transfer would call for. This would typically mean that it would arrive at the planet's orbit and continue past it. However, if there is a planet between the departure point and the target, it can be used to bend the path toward the target, and in many cases the overall travel time is greatly reduced. A prime example of this are the two crafts of the Voyager program, which used slingshot effects to change trajectories several times in the outer Solar System. It is difficult to use this method for journeys in the inner part of the Solar System, although it is possible to use other nearby planets such as Venus or even the Moon as slingshots in journeys to the outer planets.\n\nThis maneuver can only change an object's velocity relative to a third, uninvolved object, – possibly the “centre of mass” or the Sun. There is no change in the velocities of the two objects involved in the maneuver relative to each other. The Sun cannot be used in a gravitational slingshot because it is stationary compared to rest of the Solar System, which orbits the Sun. It may be used to send a spaceship or probe into the galaxy because the Sun revolves around the center of the Milky Way.\n\nA powered slingshot is the use of a rocket engine at or around closest approach to a body (periapsis). The use at this point multiplies up the effect of the delta-v, and gives a bigger effect than at other times.\n\nComputers did not exist when Hohmann transfer orbits were first proposed (1925) and were slow, expensive and unreliable when gravitational slingshots were developed (1959). Recent advances in computing have made it possible to exploit many more features of the gravity fields of astronomical bodies and thus calculate even lower-cost trajectories. Paths have been calculated which link the Lagrange points of the various planets into the so-called Interplanetary Transport Network. Such \"fuzzy orbits\" use significantly less energy than Hohmann transfers but are much, much slower. They aren't practical for manned missions because they generally take years or decades, but may be useful for high-volume transport of low-value commodities if humanity develops a space-based economy.\n\nAerobraking uses the atmosphere of the target planet to slow down. It was first used on the Apollo program where the returning spacecraft did not enter Earth orbit\nbut instead used a S-shaped vertical descent profile (starting with an initially steep descent, followed by a leveling out, followed by a slight climb, followed by a return to a positive rate of descent continuing to splash-down in the ocean) through Earth's atmosphere to reduce its speed until the parachute system could be deployed enabling a safe landing. Aerobraking does not require a thick atmosphere – for example most Mars landers use the technique, and Mars' atmosphere is only about 1% as thick as Earth's.\n\nAerobraking converts the spacecraft's kinetic energy into heat, so it requires a heatshield to prevent the craft from burning up. As a result, aerobraking is only helpful in cases where the fuel needed to transport the heatshield to the planet is less than the fuel that would be required to brake an unshielded craft by firing its engines. This can be addressed by creating heatshields from material available near the target\n\nSeveral technologies have been proposed which both save fuel and provide significantly faster travel than the traditional methodology of using Hohmann transfers. Some are still just theoretical, but over time, several of the theoretical approaches have been tested on spaceflight missions. For example, the Deep Space 1 mission was a successful test of an ion drive. These improved technologies typically focus on one or more of:\n\nBesides making travel faster or cost less, such improvements could also allow greater design \"safety margins\" by reducing the imperative to make spacecraft lighter.\n\nAll rocket concepts are limited by the rocket equation, which sets the characteristic velocity available as a function of exhaust velocity and mass ratio, of initial (\"M\", including fuel) to final (\"M\", fuel depleted) mass. The main consequence is that mission velocities of more than a few times the velocity of the rocket motor exhaust (with respect to the vehicle) rapidly become impractical.\n\nIn a nuclear thermal rocket or solar thermal rocket a working fluid, usually hydrogen, is heated to a high temperature, and then expands through a rocket nozzle to create thrust. The energy replaces the chemical energy of the reactive chemicals in a traditional rocket engine. Due to the low molecular mass and hence high thermal velocity of hydrogen these engines are at least twice as fuel efficient as chemical engines, even after including the weight of the reactor.\n\nThe US Atomic Energy Commission and NASA tested a few designs from 1959 to 1968. The NASA designs were conceived as replacements for the upper stages of the Saturn V launch vehicle, but the tests revealed reliability problems, mainly caused by the vibration and heating involved in running the engines at such high thrust levels. Political and environmental considerations make it unlikely such an engine will be used in the foreseeable future, since nuclear thermal rockets would be most useful at or near the Earth's surface and the consequences of a malfunction could be disastrous. Fission-based thermal rocket concepts produce lower exhaust velocities than the electric and plasma concepts described below, and are therefore less attractive solutions. For applications requiring high thrust-to-weight ratio, such as planetary escape, nuclear thermal is potentially more attractive.\n\nElectric propulsion systems use an external source such as a nuclear reactor or solar cells to generate electricity, which is then used to accelerate a chemically inert propellant to speeds far higher than achieved in a chemical rocket. Such drives produce feeble thrust, and are therefore unsuitable for quick maneuvers or for launching from the surface of a planet. But they are so economical in their use of\nreaction mass that they can keep firing continuously for days or weeks, while chemical rockets use up reaction mass so quickly that they can only fire for seconds or minutes. Even a trip to the Moon is long enough for an electric propulsion system to outrun a chemical rocket – the Apollo missions took 3 days in each direction.\n\nNASA's Deep Space One was a very successful test of a prototype ion drive, which fired for a total of 678 days and enabled the probe to run down Comet Borrelly, a feat which would have been impossible for a chemical rocket. Dawn, the first NASA operational (i.e., non-technology demonstration) mission to use an ion drive for its primary propulsion, is currently on track to explore and orbit the large main-belt asteroids 1 Ceres and 4 Vesta. A more ambitious, nuclear-powered version was intended for an unmanned Jupiter mission, the Jupiter Icy Moons Orbiter (JIMO), originally planned for launch sometime in the next decade. Due to a shift in priorities at NASA that favored manned space missions, the project lost funding in 2005. A similar mission is currently under discussion as the US component of a joint NASA/ESA program for the exploration of Europa and Ganymede.\n\nA NASA multi-center Technology Applications Assessment Team led from the Johnson Spaceflight Center, has as of January 2011 described \"Nautilus-X\", a concept study for a multi-mission space exploration vehicle useful for missions beyond low Earth orbit (LEO), of up to 24 months duration for a crew of up to six. Although Nautilus-X is adaptable to a variety of mission-specific propulsion units of various low-thrust, high specific impulse (I) designs, nuclear ion-electric drive is shown for illustrative purposes. It is intended for integration and checkout at the International Space Station (ISS), and would be suitable for deep-space missions from the ISS to and beyond the Moon, including Earth/Moon L1, Sun/Earth L2, near-Earth asteroidal, and Mars orbital destinations. It incorporates a reduced-g centrifuge providing artificial gravity for crew health to ameliorate the effects of long-term 0g exposure, and the capability to mitigate the space radiation environment.\n\nThe electric propulsion missions already flown, or currently scheduled, have used solar electric power, limiting their capability to operate far from the Sun, and also limiting their peak acceleration due to the mass of the electric power source. Nuclear-electric or plasma engines, operating for long periods at low thrust and powered by fission reactors, can reach speeds much greater than chemically powered vehicles.\n\nFusion rockets, powered by nuclear fusion reactions, would \"burn\" such light element fuels as deuterium, tritium, or He. Because fusion yields about 1% of the mass of the nuclear fuel as released energy, it is energetically more favorable than fission, which releases only about 0.1% of the fuel's mass-energy. However, either fission or fusion technologies can in principle achieve velocities far higher than needed for Solar System exploration, and fusion energy still awaits practical demonstration on Earth.\n\nOne proposal using a fusion rocket was Project Daedalus. Another fairly detailed vehicle system, designed and optimized for crewed Solar System exploration, \"Discovery II\", based on the DHe reaction but using hydrogen as reaction mass, has been described by a team from NASA's Glenn Research Center. It achieves characteristic velocities of >300 km/s with an acceleration of ~1.7•10 \"g\", with a ship initial mass of ~1700 metric tons, and payload fraction above 10%.\n\nSee the spacecraft propulsion article for a discussion of a number of other technologies that could, in the medium to longer term, be the basis of interplanetary missions. Unlike the situation with interstellar travel, the barriers to fast interplanetary travel involve engineering and economics rather than any basic physics.\n\nSolar sails rely on the fact that light reflected from a surface exerts pressure on the surface. The radiation pressure is small and decreases by the square of the distance from the Sun, but unlike rockets, solar sails require no fuel. Although the thrust is small, it continues as long as the Sun shines and the sail is deployed.\n\nThe original concept relied only on radiation from the Sun – for example in Arthur C. Clarke's 1965 story \"Sunjammer\". More recent light sail designs propose to boost the thrust by aiming ground-based lasers or masers at the sail. Ground-based lasers or masers can also help a light-sail spacecraft to \"decelerate\": the sail splits into an outer and inner section, the outer section is pushed forward and its shape is changed mechanically to focus reflected radiation on the inner portion, and the radiation focused on the inner section acts as a brake.\n\nAlthough most articles about light sails focus on interstellar travel, there have been several proposals for their use within the Solar System.\n\nCurrently, the only spacecraft to use a solar sail as the main method of propulsion is IKAROS which was launched by JAXA on May 21, 2010. It has since been successfully deployed, and shown to be producing acceleration as expected. Many ordinary spacecraft and satellites also use solar collectors, temperature-control panels and Sun shades as light sails, to make minor corrections to their attitude and orbit without using fuel. A few have even had small purpose-built solar sails for this use (for example Eurostar E3000 geostationary communications satellites built by EADS Astrium).\n\nIt is possible to put stations or spacecraft on orbits that cycle between different planets, for example a Mars cycler would synchronously cycle between Mars and Earth, with very little propellant usage to maintain the trajectory. Cyclers are conceptually a good idea, because massive radiation shields, life support and other equipment only need to be put onto the cycler trajectory once. A cycler could combine several roles: habitat (for example it could spin to produce an \"artificial gravity\" effect); mothership (providing life support for the crews of smaller spacecraft which hitch a ride on it). Cyclers could also possibly make excellent cargo ships for resupply of a colony.\n\nA space elevator is a theoretical structure that would transport material from a planet's surface into orbit. The idea is that, once the expensive job of building the elevator is complete, an indefinite number of loads can be transported into orbit at minimal cost. Even the simplest designs avoid the vicious circle of rocket launches from the surface, wherein the fuel needed to travel the last 10% of the distance into orbit must be lifted all the way from the surface, requiring even more fuel, and so on. More sophisticated space elevator designs reduce the energy cost per trip by using counterweights, and the most ambitious schemes aim to balance loads going up and down and thus make the energy cost close to zero. Space elevators have also sometimes been referred to as \"beanstalks\", \"space bridges\", \"space lifts\", \"space ladders\" and \"orbital towers\". \n\nA terrestrial space elevator is beyond our current technology, although a lunar space elevator could theoretically be built using existing materials.\n\nA skyhook is a theoretical class of orbiting tether propulsion intended to lift payloads to high altitudes and speeds. Proposals for skyhooks include designs that employ tethers spinning at hypersonic speed for catching high speed payloads or high altitude aircraft and placing them in orbit. In addition, it has been suggested that the rotating skyhook is \"not engineeringly feasible using presently available materials\".\n\nThe ITS launch vehicle—with first launch slated to be no earlier than 2020—is designed to be fully and rapidly reusable, making use of the SpaceX reusable technology that was developed during 2011–2016 for Falcon 9 and Falcon Heavy launch vehicles.\n\nThe SpaceX CEO, Elon Musk estimates that the reusability capability alone, on both the launch vehicle and the two spacecraft associated with the ITS launch vehicle— the \"Interplanetary Spaceship\" and \"ITS tanker\"—will reduce overall system costs per tonne delivered to Mars by at least two orders of magnitude over what NASA had previously achieved:\n\nWhen launching interplanetary probes from the surface of Earth, carrying all energy needed for the long-duration mission, payload quantities are necessarily extremely limited, due to the basis mass limitations described theoretically by the rocket equation. One alternative to transport more mass on interplanetary trajectories is to use up nearly all of the upper stage propellant on launch, and then refill propellants in Earth orbit before firing the rocket to escape velocity for a heliocentric trajectory. These propellants could be stored on orbit at a propellant depot, or carried to orbit in a propellant tanker to be directly transferred to the interplanetary spacecraft. For returning mass to Earth, a related option is to mine raw materials from a solar system celestial object, refine, process, and store the reaction products (propellant) on the Solar System body until such time as a vehicle needs to be loaded for launch.\n\nSpaceX is currently developing a system in which a reusable first stage vehicle would transport a manned interplanetary spacecraft to earth orbit, detach, return to its launch pad where a tanker spacecraft would be mounted atop it, then both fueled, then launched again to rendezvous with the waiting manned spacecraft. The tanker would then transfer its fuel to the manned spacecraft for use on its interplanetary voyage. The SpaceX \"Interplanetary Spaceship\" and \"ITS tanker\" are carbon-fiber-structure spacecraft propelled by nine Raptor engines operating on densified methane/oxygen propellants. Both are very large spacecraft—-long; maximum hull diameter of 12 m, and is -diameter at its widest point—and is capable of transporting up to of cargo and passengers per trip to Mars, with on-orbit propellant refill before the interplanetary part of the journey.\n\nAs an example of a funded project current under development, a key part of the system SpaceX has designed for Mars in order to radically decrease the cost of spaceflight to interplanetary destinations is the placement and operation of a physical plant on Mars to handle production and storage of the propellant components necessary to launch and fly the \"Interplanetary Spaceships\" back to Earth, or perhaps to increase the mass that can be transported onward to destinations in the outer Solar System.\n\nThe first \"Interplanetary Spaceship\" to Mars will carry a small propellant plant as a part of its cargo load. The plant will be expanded over multiple synods as more equipment arrives, is installed, and placed into mostly-autonomous production.\n\nThe SpaceX propellant plant will take advantage of the large supplies of carbon dioxide and water resources on Mars, mining the water (HO) from subsurface ice and collecting CO from the atmosphere. A chemical plant will process the raw materials by means of electrolysis and the Sabatier process to produce oxygen (O) and methane (CH), and then liquefy it to facilitate long-term storage and ultimate use.\n\nCurrent space vehicles attempt to launch with all their fuel (propellants and energy supplies) on board that they will need for their entire journey, and current space structures are lifted from the Earth's surface. Non-terrestrial sources of energy and materials are mostly a lot further away, but most would not require lifting out of a strong gravity field and therefore should be much cheaper to use in space in the long term.\n\nThe most important non-terrestrial resource is energy, because it can be used to transform non-terrestrial materials into useful forms (some of which may also produce energy). At least two fundamental non-terrestrial energy sources have been proposed: solar-powered energy generation (unhampered by clouds), either directly by solar cells or indirectly by focusing solar radiation on boilers which produce steam to drive generators; and electrodynamic tethers which generate electricity from the powerful magnetic fields of some planets (Jupiter has a very powerful magnetic field).\n\nWater ice would be very useful and is widespread on the moons of Jupiter and Saturn:\n\nOxygen is a common constituent of the moon's crust, and is probably abundant in most other bodies in the Solar System. Non-terrestrial oxygen would be valuable as a source of water ice only if an adequate source of hydrogen can be found. Possible uses include:\n\n\nUnfortunately hydrogen, along with other volatiles like carbon and nitrogen, are much less abundant than oxygen in the inner Solar System.\n\nScientists expect to find a vast range of organic compounds in some of the planets, moons and comets of the outer Solar System, and the range of possible uses is even wider. For example, methane can be used as a fuel (burned with non-terrestrial oxygen), or as a feedstock for petrochemical processes such as making plastics. And ammonia could be a valuable feedstock for producing fertilizers to be used in the vegetable gardens of orbital and planetary bases, reducing the need to lift food to them from Earth.\n\nEven unprocessed rock may be useful as rocket propellant if mass drivers are employed.\n\nLife support systems must be capable of supporting human life for weeks, months or even years. A breathable atmosphere of at least 35 kPa (5psi) must be maintained, with adequate amounts of oxygen, nitrogen, and controlled levels of carbon dioxide, trace gases and water vapor.\n\nIn October 2015, the NASA Office of Inspector General issued a health hazards report related to human spaceflight, including a human mission to Mars.\nOnce a vehicle leaves low Earth orbit and the protection of Earth's magnetosphere, it enters the Van Allen radiation belt, a region of high radiation. Once through there the radiation drops to lower levels, with a constant background of high energy cosmic rays which pose a health threat. These are dangerous over periods of years to decades.\n\nScientists of Russian Academy of Sciences are searching for methods of reducing the risk of radiation-induced cancer in preparation for the mission to Mars. They consider as one of the options a life support system generating drinking water with low content of deuterium (a stable isotope of hydrogen) to be consumed by the crew members. Preliminary investigations have shown that deuterium-depleted water features certain anti-cancer effects. Hence, deuterium-free drinking water is considered to have the potential of lowering the risk of cancer caused by extreme radiation exposure of the Martian crew.\n\nIn addition, coronal mass ejections from the Sun are highly dangerous, and are fatal within a very short timescale to humans unless they are protected by massive shielding.\n\nAny major failure to a spacecraft en route is likely to be fatal, and even a minor one could have dangerous results if not repaired quickly, something difficult to accomplish in open space. The crew of the Apollo 13 mission survived despite an explosion caused by a faulty oxygen tank (1970).\n\nFor astrodynamics reasons, economic spacecraft travel to other planets is only practical within certain time windows. Outside these windows the planets are essentially inaccessible from Earth with current technology. This constrains flights and limits rescue options in the case of an emergency.\n"}
{"id": "582687", "url": "https://en.wikipedia.org/wiki?curid=582687", "title": "Jnana", "text": "Jnana\n\nIn Indian philosophy and religion, jñāna (Sanskrit, pronounced gya:nə) (Pali: \"ñāṇa\") or gyan/gian (Hindi: \"jñān\") is \"knowledge\".\n\nThe idea of jnana centers on a cognitive event which is recognized when experienced. It is knowledge inseparable from the total experience of reality, especially a total or divine reality (Brahman). \n\nThe root jñā- is cognate to English \"know\", as well as to the Greek γνώ- (as in γνῶσις \"gnosis\") and Russian \"знание\". Its antonym is \"ajñāna\" \"ignorance\".\n\nIn Tibetan Buddhism, it refers to pure awareness that is free of conceptual encumbrances, and is contrasted with vijnana, which is a moment of 'divided knowing'. Entrance to, and progression through the ten stages of Jnana/Bhimis, will lead one to complete enlightenment and nirvana.\n\nIn the Vipassanā tradition of Buddhism there are the following ñanas according to Mahasi Sayadaw. As a person meditates these ñanas or \"knowledges\" will be experienced in order. The experience of each may be brief or may last for years and the subjective intensity of each is variable. Each ñana could also be considered a jhāna although many are not stable and the mind has no way to remain embedded in the experience. Experiencing all the ñanas will lead to the first of the Four stages of enlightenment then the cycle will start over at a subtler level.\n\nSahu explains:\nJnana yoga (Yoga of Knowledge) is one of the three main paths (margas), which are supposed to lead towards moksha (liberation) from material miseries. The other two main paths are Karma yoga and Bhakti Yoga. Rāja yoga (classical yoga) which includes several yogas, is also said to lead to moksha. It is said that each path is meant for a different temperament of personality.\n\nAccording to the Jain texts like Tattvārthsūtra and Sarvārthasiddhi, knowledge is of five kinds:\n\n\"Gyan\" or \"Gian\" refers to spiritual knowledge. It is mentioned throughout the Guru Granth Sahib.\n\n\n"}
{"id": "51889715", "url": "https://en.wikipedia.org/wiki?curid=51889715", "title": "John Arthur Stokes", "text": "John Arthur Stokes\n\nJohn Arthur Stokes was a prominent figure in the civil rights movement. He grew up in the Jim Crow South and attended Robert Russa Moton High School, a segregated school for African Americans. Recognizing the inequalities he and his classmates faced, Stokes staged a walk out and refused to return to class until the school was rebuilt. He attended Virginia State University and became an educator in Baltimore, MD.\n"}
{"id": "42083967", "url": "https://en.wikipedia.org/wiki?curid=42083967", "title": "K-theory of a category", "text": "K-theory of a category\n\nIn algebraic K-theory, the K-theory of a category \"C\" (usually equipped with some kind of additional data) is a sequence of abelian groups \"K\"(\"C\") associated to it. If \"C\" is an abelian category, there is no need for extra data, but in general it only makes sense to speak of K-theory after specifying on \"C\" a structure of an exact category, or of a Waldhausen category, or of a dg-category, or possibly some other variants. Thus, there are several constructions of those groups, corresponding to various kinds of structures put on \"C\". Traditionally, the K-theory of \"C\" is \"defined\" to be the result of a suitable construction, but in some contexts there are more conceptual definitions. For instance, the K-theory is a 'universal additive invariant' of dg-categories and small stable ∞-categories.\n\nThe motivation for this notion comes from algebraic K-theory of rings. For a ring \"R\" Daniel Quillen in introduced two equivalent ways to find the higher K-theory. The plus construction expresses \"K\"(\"R\") in terms of \"R\" directly, but it's hard to prove properties of the result, including basic ones like functoriality. The other way is to consider the exact category of projective modules over \"R\" and to set \"K\"(\"R\") to be the K-theory of that category, defined using the Q-construction. This approach proved to be more useful, and could be applied to other exact categories as well. Later Friedhelm Waldhausen in extended the notion of K-theory even further, to very different kinds of categories, including the category of topological spaces. \n\nIn algebra, the S-construction is a construction in algebraic K-theory that produces a model that can be used to define higher K-groups. It is due to Friedhelm Waldhausen and concerns a category with cofibrations and weak equivalences; such a category is called a Waldhausen category and generalizes Quillen's exact category. A cofibration can be thought of as analogous to a monomorphism, and a category with cofibrations is one in which, roughly speaking, monomorphisms are stable under pushouts. According to Waldhausen, the \"S\" was chosen to stand for Graeme B. Segal.\n\nUnlike the Q-construction, which produces a topological space, the S-construction produces a simplicial set.\n\nThe arrow category formula_1 of a category \"C\" is a category whose objects are morphisms in \"C\" and whose morphisms are squares in \"C\". Let a finite ordered set formula_2 be viewed as a category in the usual way.\n\nLet \"C\" be a category with cofibrations and let formula_3 be a category whose objects are functors formula_4 such that, for formula_5, formula_6, formula_7 is a cofibration, and formula_8 is the pushout of formula_7 and formula_10. The category formula_3 defined in this manner is itself a category with cofibrations. One can therefore iterate the construction, forming the sequenceformula_12. This sequence is a spectrum called the K-theory spectrum of \"C\".\n\nMost basic properties of algebraic K-theory of categories are consequences of the following important theorem. There are versions of it in all available settings. Here's a statement for Waldhausen categories. Notably, it's used to show that the sequence of spaces obtained by the iterated S-construction is an Ω-spectrum. \n\nLet \"C\" be a Waldhausen category. The category of extensions formula_13 has as objects the sequences formula_14 in \"C\", where the first map is a cofibration, and formula_15 is a quotient map, i.e. a pushout of the first one along the zero map \"A\" → \"0\". This category has a natural Waldhausen structure, and the forgetful functor formula_16 from formula_13 to \"C\" × \"C\" respects it. The additivity theorem says that the induced map on K-theory spaces formula_18 is a homotopy equivalence.\n\nFor dg-categories the statement is similar. Let \"C\" be a small pretriangulated dg-category with a semiorthogonal decomposition formula_19. Then the map of K-theory spectra K(\"C\") → K(\"C\") ⊕ K(\"C\") is a homotopy equivalence. In fact, K-theory is a universal functor satisfying this additivity property and Morita invariance.\n\nConsider the category of pointed finite sets. This category has an object formula_20 for every natural number \"k\", and the morphisms in this category are the functions formula_21 which preserve the zero element. A theorem of Barratt, Priddy and Quillen says that the algebraic K-theory of this category is a sphere spectrum.\n\nMore generally in abstract category theory, the K-theory of a category is a type of decategorification in which a set is created from an equivalence class of objects in a stable (∞,1)-category, where the elements of the set inherit an Abelian group structure from the exact sequences in the category.\n\nThe Grothendieck group construction is a functor from the category of rings to the category of abelian groups. The higher \"K\"-theory should then be a functor from the category of rings but to the category of higher objects such as simplicial abelian groups.\n\nWaldhausen introduced the idea of a trace map from the algebraic \"K\"-theory of a ring to its Hochschild homology; by way of this map, information can be obtained about the \"K\"-theory from the Hochschild homology. Bökstedt factorized this trace map, leading to the idea of a functor known as the topological Hochschild homology of the ring's Eilenberg–MacLane spectrum.\n\nIf \"R\" is a constant simplicial ring, then this is the same thing as \"K\"-theory of a ring. \n\n\nFor the recent ∞-category approach, see\n"}
{"id": "32390285", "url": "https://en.wikipedia.org/wiki?curid=32390285", "title": "LABoral Centro de Arte y Creación Industrial", "text": "LABoral Centro de Arte y Creación Industrial\n\nLABoral Centro de Arte y Creación Industrial (Art and Industrial Creation Centre) is an exhibition centre in Gijón, Spain, for art, science, technology and advanced visual industries. It is also a venue for artistic and technological production, research investigation and training; and for the dissemination of new forms of art and industrial creation.\n\nThe museum's programme of activities mirrors the changes society is undergoing and its immersion in contemporary visual culture.\n\n\nArtists whose works have been exhibited at the center include: Aram Bartholl, Aaron Koblin, Ai Weiwei, Martin Parr, Roy Arden, Yael Bartana, Julian Opie among others.\n\n"}
{"id": "34980327", "url": "https://en.wikipedia.org/wiki?curid=34980327", "title": "Lady Macbeth effect", "text": "Lady Macbeth effect\n\nThe supposed Lady Macbeth effect or \"Macbeth\" effect is a priming effect said to occur when response to a cleaning cue is increased after having been induced by a feeling of shame. The effect is named after the Lady Macbeth character in the Shakespeare play \"Macbeth\"; she imagined bloodstains on her hands after committing murder.\n\nIn one experiment, different groups of participants were asked to recall a good or bad past deed, after which they were asked to fill in the letters of three incomplete words: \"W_ _H\", \"SH_ _ER\" and \"S_ _P\". Those who had been asked to recall a bad deed were about 60% more likely to respond with cleansing-related words like \"wash\", \"shower\" and \"soap\" instead of alternatives such as \"wish\", \"shaker\" or \"stop\".\n\nIn another experiment, experimenters were able to reduce choice-supportive bias by having subjects engage in forms of self-cleaning. \n\nThe effect is apparently localized enough that those who had been asked to lie verbally preferred an oral cleaning product and those asked to lie in writing preferred a hand cleaning product over the other kind of cleanser and other control items.\n\nOther researchers have been unable to replicate the basic effect using larger samples. Replication difficulties have emerged for three out of four of Zhong and Liljenquist's original studies (i.e., Study 2, Study 3, and Study 4).\n\n"}
{"id": "2632698", "url": "https://en.wikipedia.org/wiki?curid=2632698", "title": "Lapsus", "text": "Lapsus\n\nA lapsus (Latin for \"lapse, slip, error\") is an involuntary mistake made while writing or speaking, something long studied in philology.\n\nIn 1895 an investigation into verbal slips was undertaken by a philologist and a psychologist, Rudolf Meringer and Karl Meyer, who collected a large number of examples and divided them into separate types.\n\nFreud was to become interested in such mistakes from 1897 onwards, developing an interpretation of slips in terms of their unconscious meaning. Subsequently followers of his like Ernest Jones developed the theme of lapsus in connection with writing, typing, and misprints.\n\nAccording to Freud's early psychoanalytic theory, a lapsus represents a bungled act that hides an unconscious desire: “the phenomena can be traced back to incompletely suppressed psychical material...pushed away by consciousness”.\n\nJacques Lacan would thoroughly endorse the Freudian interpretation of unconscious motivation in the slip, arguing that “in the \"lapsus\" it is...clear that every unsuccessful act is a successful, not to say 'well-turned', discourse”.\n\nIn the seventies Sebastiano Timpanaro would controversially take up the question again, by offering a mechanistic explanation of all such slips, in opposition to Freud's theories.\n\nIn literature, a number of different types of lapsus are named depending on the mode of correspondence:\n\nSlips of the tongue can happen on any level:\n\nAdditionally, each of these five levels of error may take various forms:\n\nMeringer and Meyer highlighted the role of familiar associations and similarities of words and sounds in producing the lapsus. Freud objected that such factors did not cause but only “\"favour\" slips of the tongue...in the immense majority of cases my speech is not disturbed by the circumstance that the words I am using recall others with a similar sound...or that familiar associations branch off from them”.\n\nTimpanaro later reignited the debate, by maintaining that any given slip can always be explained mechanically without a need for deeper motivation.\n\nJ. L. Austin had independently seen slips not as revealing a particular complex, but as an ineluctable feature of the human condition, necessitating a continual preparation for excuses and remedial work.\n\n"}
{"id": "170167", "url": "https://en.wikipedia.org/wiki?curid=170167", "title": "Maxwell–Boltzmann statistics", "text": "Maxwell–Boltzmann statistics\n\nIn statistical mechanics, Maxwell–Boltzmann statistics describes the average distribution of non-interacting material particles over various energy states in thermal equilibrium, and is applicable when the temperature is high enough or the particle density is low enough to render quantum effects negligible.\n\nThe expected number of particles with energy formula_1 for Maxwell–Boltzmann statistics is\n\nwhere:\n\nEquivalently, the number of particles is sometimes expressed as\n\nwhere the index \"i\" now specifies a particular state rather than the set of all states with energy formula_1, and formula_12.\n\nMaxwell–Boltzmann statistics may be used to derive the Maxwell–Boltzmann distribution (for an ideal gas of classical particles in a three-dimensional box). However, they apply to other situations as well. Maxwell–Boltzmann statistics can be used to extend that distribution to particles with a different energy–momentum relation, such as relativistic particles (Maxwell–Jüttner distribution). In addition, hypothetical situations can be considered, such as particles in a box with different numbers of dimensions (four-dimensional, two-dimensional, etc.).\n\nMaxwell–Boltzmann statistics are often described as the statistics of \"distinguishable\" classical particles. In other words, the configuration of particle \"A\" in state 1 and particle \"B\" in state 2 is different from the case in which particle \"B\" is in state 1 and particle \"A\" is in state 2. This assumption leads to the proper (Boltzmann) statistics of particles in the energy states, but yields non-physical results for the entropy, as embodied in the Gibbs paradox.\n\nAt the same time, there are no real particles which have the characteristics required by Maxwell–Boltzmann statistics. Indeed, the Gibbs paradox is resolved if we treat all particles of a certain type (e.g., electrons, protons, etc.) as indistinguishable, and this assumption can be justified in the context of quantum mechanics. Once this assumption is made, the particle statistics change.\nQuantum particles are either bosons (following instead Bose–Einstein statistics) or fermions (subject to the Pauli exclusion principle, following instead Fermi–Dirac statistics). Both of these quantum statistics approach the Maxwell–Boltzmann statistics in the limit of high temperature and low particle density, without the need for any ad hoc assumptions. The Fermi–Dirac and Bose–Einstein statistics give the energy level occupation as:\nIt can be seen that the condition under which the Maxwell–Boltzmann statistics are valid is when\nwhere formula_15 is the lowest (minimum) value of formula_1.\n\nMaxwell–Boltzmann statistics are particularly useful for studying gases that are not very dense. Note, however, that all of these statistics assume that the particles are non-interacting and have static energy states.\n\nMaxwell–Boltzmann statistics can be derived in various statistical mechanical thermodynamic ensembles:\nIn each case it is necessary to assume that the particles are non-interacting, and that multiple particles can occupy the same state and do so independently.\n\nSuppose we have a container with a huge number of very small particles all with identical physical characteristics (such as mass, charge, etc.). Let's refer to this as the \"system\". Assume that though the particles have identical properties, they are distinguishable. For example, we might identify each particle by continually observing their trajectories, or by placing a marking on each one, e.g., drawing a different number on each one as is done with lottery balls.\n\nThe particles are moving inside that container in all directions with great speed. Because the particles are speeding around, they possess some energy. The Maxwell–Boltzmann distribution is a mathematical function that speaks about how many particles in the container have a certain energy. More precisely, the Maxwell–Boltzmann distribution gives the non-normalized probability that the state corresponding to a particular energy is occupied.\n\nIn general, there may be many particles with the same amount of energy formula_17. Let the number of particles with the same energy formula_18 be formula_19, the number of particles possessing another energy formula_20 be formula_21, and so forth for all the possible energies {formula_1 | i=1,2,3...}. To describe this situation, we say that formula_23 is the \"occupation number\" of the \"energy level\" formula_24 If we know all the occupation numbers {formula_23 | i=1,2,3...}, then we know the total energy of the system. However, because we can distinguish between \"which\" particles are occupying each energy level, the set of occupation numbers {formula_23 | i=1,2,3...} does not completely describe the state of the system. To completely describe the state of the system, or the \"microstate\", we must specify exactly which particles are in each energy level. Thus when we count the number of possible states of the system, we must count each and every microstate, and not just the possible sets of occupation numbers.\n\nTo begin with, let's ignore the degeneracy problem: assume that there is only one way to put formula_23 particles into the energy level formula_28 . What follows next is a bit of combinatorial thinking which has little to do in accurately describing the reservoir of particles.\n\nThe number of different ways of performing an ordered selection of one single object from \"N\" objects is obviously \"N\". The number of different ways of selecting two objects from \"N\" objects, in a particular order, is thus \"N\"(\"N\" − 1) and that of selecting \"n\" objects in a particular order is seen to be \"N\"!/(\"N\" − \"n\")<nowiki>!</nowiki>. It is divided by the number of permutations, \"n\"!, if order does not matter. The binomial coefficient, \"N\"!/(\"n\"!(\"N\" − \"n\")!), is, thus, the number of ways to pick \"n\" objects from formula_29. If we now have a set of boxes labelled \"a, b, c, d, e, ..., k\", then the number of ways of selecting \"N\" objects from a total of \"N\" objects and placing them in box \"a\", then selecting \"N\" objects from the remaining \"N\" − \"N\" objects and placing them in box \"b\", then selecting \"N\" objects from the remaining \"N\" − \"N\" − \"N\" objects and placing them in box \"c\", and continuing until no object is left outside is\n\nand because not even a single object is to be left outside the boxes, implies that the sum made of the terms \"N, N, N, N, N, ..., N\" must equal \"N\", thus the term \"(N - N - N - N - ... - N - N)!\" in the relation above evaluates to \"0!\". (0!=1) which makes possible to write down that relation as\nThis is just the multinomial coefficient, the number of ways of arranging \"N\" items into \"k\" boxes, the \"i\"-th box holding \"N\" items, ignoring the permutation of items in each box.\n\nNow going back to the degeneracy problem which characterizes the reservoir of particles. If the \"i\"-th box has a \"degeneracy\" of formula_6, that is, it has formula_6 \"sub-boxes\", such that any way of filling the \"i\"-th box where the number in the sub-boxes is changed is a distinct way of filling the box, then the number of ways of filling the \"i\"-th box must be increased by the number of ways of distributing the formula_23 objects in the formula_6 \"sub-boxes\". The number of ways of placing formula_23 distinguishable objects in formula_6 \"sub-boxes\" is formula_38 (the first object can go into any of the formula_6 boxes, the second object can also go into any of the formula_6 boxes, and so on). Thus the number of ways formula_41 that a total of formula_29 particles can be classified into energy levels according to their energies, while each level formula_28 having formula_6 distinct states such that the \"i\"-th level accommodates formula_23 particles is:\n\nThis is the form for \"W\" first derived by Boltzmann. Boltzmann's fundamental equation formula_47 relates the thermodynamic entropy \"S\" to the number of microstates \"W\", where \"k\" is the Boltzmann constant. It was pointed out by Gibbs however, that the above expression for \"W\" does not yield an extensive entropy, and is therefore faulty. This problem is known as the Gibbs paradox. The problem is that the particles considered by the above equation are not indistinguishable. In other words, for two particles (\"A\" and \"B\") in two energy sublevels the population represented by [A,B] is considered distinct from the population [B,A] while for indistinguishable particles, they are not. If we carry out the argument for indistinguishable particles, we are led to the Bose–Einstein expression for \"W\":\n\nThe Maxwell–Boltzmann distribution follows from this Bose–Einstein distribution for temperatures well above absolute zero, implying that formula_49. The Maxwell–Boltzmann distribution also requires low density, implying that formula_50. Under these conditions, we may use Stirling's approximation for the factorial:\n\nto write:\n\nUsing the fact that formula_53 for formula_50 we can again use Stirlings approximation to write:\n\nThis is essentially a division by \"N!\" of Boltzmann's original expression for \"W\", and this correction is referred to as .\n\nWe wish to find the formula_23 for which the function formula_41 is maximized, while considering the constraint that there is a fixed number of particles formula_58 and a fixed energy formula_59 in the container. The maxima of formula_41 and formula_61 are achieved by the same values of formula_23 and, since it is easier to accomplish mathematically, we will maximize the latter function instead. We constrain our solution using Lagrange multipliers forming the function:\n\nFinally\n\nIn order to maximize the expression above we apply Fermat's theorem (stationary points), according to which local extrema, if exist, must be at critical points (partial derivatives vanish):\n\nBy solving the equations above (formula_67) we arrive to an expression for formula_23:\n\nSubstituting this expression for formula_23 into the equation for formula_71 and assuming that formula_72 yields:\n\nor, rearranging:\n\nBoltzmann realized that this is just an expression of the Euler-integrated fundamental equation of thermodynamics. Identifying \"E\" as the internal energy, the Euler-integrated fundamental equation states that :\n\nwhere \"T\" is the temperature, \"P\" is pressure, \"V\" is volume, and μ is the chemical potential. Boltzmann's famous equation formula_47 is the realization that the entropy is proportional to formula_71 with the constant of proportionality being Boltzmann's constant. Using the ideal gas equation of state (\"PV=NkT\"), It follows immediately that formula_78 and formula_79 so that the populations may now be written:\n\nNote that the above formula is sometimes written:\n\nwhere formula_82 is the absolute activity.\n\nAlternatively, we may use the fact that\n\nto obtain the population numbers as\n\nwhere \"Z\" is the partition function defined by:\n\nIn an approximation where \"ε\" is considered to be a continuous variable, the Thomas-Fermi approximation yields a continuous degeneracy g proportional to formula_86 so that:\n\nwhich is just the Maxwell-Boltzmann distribution for the energy.\n\nIn the above discussion, the Boltzmann distribution function was obtained via directly analysing the multiplicities of a system. Alternatively, one can make use of the canonical ensemble. In a canonical ensemble, a system is in thermal contact with a reservoir. While energy is free to flow between the system and the reservoir, the reservoir is thought to have infinitely large heat capacity as to maintain constant temperature, \"T\", for the combined system.\n\nIn the present context, our system is assumed to have the energy levels formula_88 with degeneracies formula_6. As before, we would like to calculate the probability that our system has energy formula_1.\n\nIf our system is in state formula_91, then there would be a corresponding number of microstates available to the reservoir. Call this number formula_92. By assumption, the combined system (of the system we are interested in and the reservoir) is isolated, so all microstates are equally probable. Therefore, for instance, if formula_93, we can conclude that our system is twice as likely to be in state formula_91 than formula_95. In general, if formula_96 is the probability that our system is in state formula_97,\n\nSince the entropy of the reservoir formula_99, the above becomes\n\nNext we recall the thermodynamic identity (from the first law of thermodynamics):\n\nIn a canonical ensemble, there is no exchange of particles, so the formula_102 term is zero. Similarly, formula_103 This gives\n\nwhere formula_105 and formula_106 denote the energies of the reservoir and the system at formula_107, respectively. For the second equality we have used the conservation of energy. Substituting into the first equation relating formula_108:\n\nwhich implies, for any state \"s\" of the system\n\nwhere \"Z\" is an appropriately chosen \"constant\" to make total probability 1. (\"Z\" is constant provided that the temperature \"T\" is invariant.)\n\nwhere the index \"s\" runs through all microstates of the system. \"Z\" is sometimes called the Boltzmann sum over states (or \"Zustandsumme\" in the original German). If we index the summation via the energy eigenvalues instead of all possible states, degeneracy must be taken into account. The probability of our system having energy formula_88 is simply the sum of the probabilities of all corresponding microstates:\n\nwhere, with obvious modification,\n\nthis is the same result as before.\n\nComments on this derivation:\n\n\n"}
{"id": "47077309", "url": "https://en.wikipedia.org/wiki?curid=47077309", "title": "Negative-dimensional space", "text": "Negative-dimensional space\n\nIn topology, a discipline within mathematics, a negative-dimensional space is an extension of the usual notion of space, allowing for negative dimensions.\n\nSuppose that is a compact space of Hausdorff dimension , which is an element of a scale of compact spaces embedded in each other and parametrized by (). Such scales are considered \"equivalent\" with respect to if the compact spaces constituting them coincide for . It is said that the compact space is the \"hole\" in this equivalent set of scales, and is the negative dimension of the corresponding equivalence class.\n\nBy the 1940s, the science of topology had developed and studied a thorough basic theory of topological spaces of positive dimension. Motivated by computations, and to some extent aesthetics, topologists searched\nfor mathematical frameworks that extended our notion of space to allow for negative dimensions. Such dimensions, as well as the fourth and higher dimensions, are hard to imagine since we are not able to directly observe them. It wasn’t until the 1960s that a special topological framework was constructed—the category of spectra. A spectrum is a generalization of space that allows for negative dimensions. The concept of negative-dimensional spaces is applied, for example, to analyze linguistic statistics.\n\n\n"}
{"id": "314692", "url": "https://en.wikipedia.org/wiki?curid=314692", "title": "One-form", "text": "One-form\n\nIn linear algebra, a one-form on a vector space is the same as a linear functional on the space. The usage of \"one-form\" in this context usually distinguishes the one-forms from higher-degree multilinear functionals on the space. For details, see linear functional.\n\nIn differential geometry, a one-form on a differentiable manifold is a smooth section of the cotangent bundle. Equivalently, a one-form on a manifold \"M\" is a smooth mapping of the total space of the tangent bundle of \"M\" to formula_1 whose restriction to each fibre is a linear functional on the tangent space. Symbolically,\n\nwhere \"α\" is linear.\n\nOften one-forms are described locally, particularly in local coordinates. In a local coordinate system, a one-form is a linear combination of the differentials of the coordinates:\n\nwhere the \"f\" are smooth functions. From this perspective, a one-form has a covariant transformation law on passing from one coordinate system to another. Thus a one-form is an order 1 covariant tensor field.\n\nMany real-world concepts can be described as one-forms:\n\n\n\n\nThe most basic non-trivial differential one-form is the \"change in angle\" form formula_6 This is defined as the derivative of the angle \"function\" formula_7 (which is only defined up to a constant), which can be explicitly defined in terms of the atan2 function formula_8 Taking the derivative yields the following formula for the total derivative:\nWhile the angle \"function\" cannot be continuously defined – the function atan2 is discontinuous along the negative \"y\"-axis – which reflects the fact that angle cannot be continuously defined, this derivative is continuously defined except at the origin, reflecting the fact that infinitesimal (and indeed local) \"changes\" in angle can be defined everywhere except the origin. Integrating this derivative along a path gives the total change in angle over the path, and integrating over a closed loop gives the winding number.\n\nIn the language of differential geometry, this derivative is a one-form, and it is closed (its derivative is zero) but not exact (it is not the derivative of a 0-form, i.e., a function), and in fact it generates the first de Rham cohomology of the punctured plane. This is the most basic example of such a form, and it is fundamental in differential geometry.\n\nLet formula_10 be open (e.g., an interval formula_11), and consider a differentiable function formula_12, with derivative \"f\"'. The differential \"df\" of \"f\", at a point formula_13, is defined as a certain linear map of the variable \"dx\". Specifically, formula_14. (The meaning of the symbol \"dx\" is thus revealed: it is simply an argument, or independent variable, of the function \"df\".) Hence the map formula_15 sends each point \"x\" to a linear functional \"df(x,dx)\". This is the simplest example of a differential (one-)form.\n\nIn terms of the de Rham complex, one has an assignment from zero-forms (scalar functions) to one-forms i.e., formula_16.\n\n"}
{"id": "13065509", "url": "https://en.wikipedia.org/wiki?curid=13065509", "title": "Paired opposites", "text": "Paired opposites\n\nPaired opposites are an ancient, pre-Socratic method of establishing thesis, antithesis and synthesis in terms of a standard for what is right and proper in natural philosophy.\n\nScalar ranges and coordinate systems are paired opposites within sets. Incorporating dimensions of positive and negative numbers and exponents, or expanding x, y and z coordinates, by adding a fourth dimension of time allows a resolution of position relative to the standard of the scale which is often taken as 0,0,0,0 with additional dimensions added as referential scales are expanded from space and time to mass and energy.\n\nAncient systems frequently scaled their degree of opposition by rate of increase or rate of decrease. Linear increase was enhanced by doubling systems. An acceleration in the rate of increase or decrease could be analyzed arithmetrically, geometrically, or through a wide range of other numerical and physical analysis. Arithmetic and geometric series, and other methods of rating proportionate expansion or contraction could be thought of as convergent or divergent toward a position.\n\nThough unit quantities were first defined by spatial dimensions, and then expanded by adding coordinates of time, the weight or mass a given spatial dimension could contain was also considered and even in antiquity, conditions under which the standard would be established such as at a given temperature, distance from sea level, or density were added.\n\nRates of change over time were then considered as either indexes of production or depletion\n\nPaired opposites are used as poetic diction meaning \"everything\". Common phrases incorporated paired opposites in English include \"all creatures great and small,\" \"working for the man every night and day,\" \"more things in heaven and Earth\" \"searching high and low\" \"in sickness and in health\". In Greek literature, Homer uses the device when he lets Telemachus say, \"I know all things, the good and the evil\" (Od.20:309-10).\nThe same phrase is used in Hebrew in text of Genesis, referring to the Tree of the knowledge of good and evil.\n\nIn quantum mechanics, as well as some fields of mathematics, conjugate variables are a form of paired opposites, in which knowledge of one precludes knowledge of the other. A standard example is the relation between position (x) and momentum (p), which can be expressed in terms of the uncertainty principle as formula_1.\n"}
{"id": "360094", "url": "https://en.wikipedia.org/wiki?curid=360094", "title": "Pale of Settlement", "text": "Pale of Settlement\n\nThe Pale of Settlement (, ', , ', , ') was a western region of Imperial Russia with varying borders that existed from 1791 to 1917, in which permanent residency by Jews was allowed and beyond which Jewish residency, permanent or temporary, was mostly forbidden. Most Jews were still excluded from residency in a number of cities within the Pale as well. A limited number of Jews were allowed to live outside the area, including those with university education, the ennobled, members of the most affluent of the merchant guilds and particular artisans, some military personnel and some services associated with them, including their families, and sometimes the servants of these. The archaic English term \"pale\" is derived from the Latin word ', a stake, extended to mean the area enclosed by a fence or boundary.\n\nThe Pale of Settlement included all of Belarus, Lithuania and Moldova, much of present-day Ukraine, parts of eastern Latvia, eastern Poland, and some parts of western Russia, roughly corresponding to the Kresy macroregion and modern-day western border of Russia. It extended from the eastern \"pale\", or demarcation line, to the Russian border with the Kingdom of Prussia (later the German Empire) and the Austro-Hungarian Empire. Furthermore, it comprised about 20% of the territory of European Russia and largely corresponded to historical lands of the former Polish–Lithuanian Commonwealth, Cossack Hetmanate, and the Ottoman Empire (with Crimean Khanate).\n\nThe Russian Empire in the period of the existence of the Pale was predominantly Orthodox Christian. The area included in the Pale, with its large Jewish, Uniate and Catholic populations, was acquired through a series of military conquests and diplomatic maneuvers, between 1654 and 1815. While the religious nature of the edicts creating the Pale are clear (conversion to Russian Orthodoxy, the state religion, released individuals from the strictures), historians argue that the motivations for its creation and maintenance were primarily economic and nationalistic in nature.\n\nThe end of the enforcement and formal demarcation of the Pale coincided with the beginning of the First World War, and ultimately with the February and October Revolutions of 1917, i.e., the fall of the Russian Empire.\n\nThe Pale was first created by Catherine the Great in 1791, after several failed attempts by her predecessors, notably the Empress Elizabeth, to remove Jews from Russia entirely, unless they converted to Russian Orthodoxy, the state religion. \n\nThe institution of the Pale became more significant following the Second Partition of Poland in 1793, since, until then, Russia's Jewish population had been rather limited; the dramatic westward expansion of the Russian Empire through the annexation of Polish-Lithuanian territory substantially increased the Jewish population. At its height, the Pale, including the new Polish and Lithuanian territories, had a Jewish population of over five million, and represented the largest component (40 percent) of the world Jewish population at that time.\n\nFrom 1791 to 1835, and until 1917, there were differing reconfigurations of the boundaries of the Pale, such that certain areas were variously open or shut to Jewish residency, such as the Caucasus. At times, Jews were forbidden to live in agricultural communities, or certain cities, (as in Kiev, Sevastopol and Yalta), and were forced to move to small provincial towns, thus fostering the rise of the \"shtetls.\" Jewish merchants of the First Guild (, the wealthiest \"sosloviye\" of merchants in the Russian Empire), people with higher or special education, University students, artisans, army tailors, ennobled Jews, soldiers (drafted in accordance with the Recruit Charter of 1810), and their families had the right to live outside the Pale of Settlement. In some periods, special dispensations were given for Jews to live in the major imperial cities, but these were tenuous, and several thousand Jews were expelled to the Pale from Saint Petersburg and Moscow as late as 1891.\n\nDuring World War I, the Pale lost its rigid hold on the Jewish population when large numbers of Jews fled into the Russian interior to escape the invading German army. On March 20 (April 2 N.S.), 1917, the Pale was abolished by the Provisional Government decree, \"On abolition of confessional and national restrictions\" (). A large portion of the Pale, together with its Jewish population, became part of Poland. Subsequently, most of this population would perish in The Holocaust one generation later.\n\nJewish life in the shtetls ( \"\" \"little towns\") of the Pale of Settlement was hard and poverty-stricken. Following the Jewish religious tradition of \"tzedakah\" (charity), a sophisticated system of volunteer Jewish social welfare organizations developed to meet the needs of the population. Various organizations supplied clothes to poor students, provided kosher food to Jewish soldiers conscripted into the Tsar's army, dispensed free medical treatment for the poor, offered dowries and household gifts to destitute brides, and arranged for technical education for orphans. According to historian Martin Gilbert's \"Atlas of Jewish History\", no province in the Pale had less than 14% of Jews on relief; Lithuanian and Ukrainian Jews supported as much as 22% of their poor populations.\n\nThe concentration of Jews in the Pale, coupled with Tzar Alexander III's \"fierce hatred of the Jews\", and the rumors that Jews had been involved in the assassination of his father Tzar Alexander II made them easy targets for pogroms and anti-Jewish riots by the majority population. These, along with the repressive May Laws, often devastated whole communities. Though attacks occurred throughout the existence of the Pale, particularly devastating anti-Jewish pogroms occurred from 1881–83 and from 1903–1906, targeting hundreds of communities, assaulting thousands of Jews, and causing considerable property damage.\n\nOne outgrowth of the concentration of Jews in a circumscribed area was the development of the modern yeshiva system. Until the beginning of the 19th century, each town supported its own advanced students who learned in the local synagogue with the rabbinical head of the community. Each student would eat his meals in a different home each day, a system known as \"\" (\"eating days\").\n\nAfter 1886, the Jewish quota was applied to education, with the percentage of Jewish students limited to no more than 10% within the Pale, 5% outside the Pale and 3% in the capitals of Moscow, St. Petersburg, and Kiev. The quotas in the capitals, however, were increased slightly in 1908 and 1915.\n\nAmidst the difficult conditions in which the Jewish population lived and worked, the courts of Hasidic dynasties flourished in the Pale. Thousands of followers of rebbes such as the Gerrer Rebbe Yehudah Aryeh Leib Alter (known as the \"Sfas Emes\"), the Chernobyler Rebbe, and the Vizhnitzer Rebbe flocked to their towns for the Jewish holidays and followed their rebbes' \"minhagim\" (Jewish practices) in their own homes.\n\nThe tribulations of Jewish life in the Pale of Settlement were immortalized in the writings of Yiddish authors such as humorist Sholom Aleichem, whose novel \"\" (Tevye the Milkman) (in the form of the narration of Tevye from a fictional shtetl of Anatevka to the author) form the basis of the theatrical (and subsequent film) production \"Fiddler on the Roof\". Because of the harsh conditions of day-to-day life in the Pale, some two million Jews emigrated from there between 1881 and 1914, mainly to the United States.\n\nThe Pale of Settlement included the following areas.\n\nThe ukase of Catherine the Great of December 23, 1791 limited the Pale to:\n\n\nAfter the Second Partition of Poland, the ukase of June 23, 1794, the following areas were added:\n\nAfter the Third Partition of Poland, the following areas were added:\n\nAfter 1805 the Pale gradually shrank, and became limited to the following areas:\n\nCongress Poland did not belong to the Pale of Settlement\n\nRural areas for from the western border were closed for new settlement of the Jews.\n\nAccording to the 1897 census, the \"guberniyas\" had the following percentages of Jews:\n\n\n\nOthers:\n\nIn 1882 it was forbidden for Jews to settle in rural areas.\n\nThe following cities within the Pale were excluded from it:\n\n\n\n\n"}
{"id": "49769402", "url": "https://en.wikipedia.org/wiki?curid=49769402", "title": "Paper abortion", "text": "Paper abortion\n\nPaper abortion, also known as a financial abortion or a statutory abort, is the proposed ability of the biological father, before the birth of the child, to opt out of any rights, privileges, and responsibilities toward the child, including financial support. By this means, before a child is born, a man would be able to absolve himself of both the privileges and demands of fatherhood.\n\nIn a 1996 article \"Abortion and Fathers' Rights,\" philosopher Steven Hales made an argument that presupposes the following assertions:\n\n\nHales contends that the conjunction of these three principles is prima facie inconsistent and that this inconsistency should be eradicated by firstly acknowledging that men have no absolute duty to provide material support for their children, and secondly by admitting that fathers have the right of refusal.\n\nLaurie Shrage, professor of philosophy and women’s and gender studies, questions whether men should be 'penalized for being sexually active', and she puts the subject in the perspective of feminists who had to fight the same idea with different gender portent, namely that consenting to sexual intercourse is not the same as consenting to parenthood. Furthermore, both men and children are punished, according to professor Shrage; children have to live with an absent father who never 'voluntarily' became a parent.\n\nAt most, according to Brake, men should be responsible for helping with the medical expenses and other costs of a pregnancy for which they are partly responsible.\n\nPaper abortion has met opposition by those who see it as an excuse for men to shirk their responsibilities as a father. Critics say that men should use birth control (either contraception or sterilization) or practice abstinence if they want to avoid the financial and personal responsibilities of fatherhood. Critics also argue that a father's paper abortion is different from a female abortion since a child is born. Thus the best interests of the child should weigh more than equal opportunity to deny parenthood.\n\nThe concept of a paper abortion was first introduced in Denmark in 2000 by the socioeconomicist Henrik Platz. He says that it is necessary from an egalitarian perspective, to ensure that women and men have equal rights under the law. According to a Gallup poll from 2014 and earlier polls, between 40% and 70% of Danes agree with legalizing paper abortion.\n\nSociologist Karen Sjørup, who conducted research on the topic argues that it would give women more freedom by allowing those who want to become mothers without having to share the rights and duties of parenthood with men an additional way to do so. She also suggests that it could decrease the abortion rate because it would prevent men who wished to avoid fatherhood from pressuring women to abort.\n\nAdvocates argue that just as women are able to choose whether to have a child or not, men should also be able to choose whether to assume paternity or not. Allowing men to have the opportunity to renounce the economic, social and legal responsibility for an unborn child during the first three months of pregnancy would give men and women as close to equal opportunities as possible.\n\nIn 2016, a regional branch of the Swedish Liberal Youth Party decided to support paper abortion for men until the 18th week of pregnancy, the time limit on abortions for women. The proposition was supported by some commentators, but not by the LYP's parent party.\n\n"}
{"id": "53933", "url": "https://en.wikipedia.org/wiki?curid=53933", "title": "Permittivity", "text": "Permittivity\n\nIn electromagnetism, absolute permittivity, often simply called permittivity , usually denoted by the Greek letter ε (epsilon), is the measure of capacitance that is encountered when forming an electric field in a particular medium. More specifically, permittivity describes the amount of charge needed to generate one unit of electric flux in a particular medium. Accordingly, a charge will yield more electric flux in a medium with low permittivity than in a medium with high permittivity. Permittivity is the measure of a material's ability to store an electric field in the polarization of the medium.\n\nThe SI unit for permittivity is farad per meter (F/m or F·m).\n\nThe lowest possible permittivity is that of a vacuum. Vacuum permittivity, sometimes called the electric constant, is represented by ε and has a value of approximately 8.85×10 F/m.\n\nThe permittivity of a dielectric medium is often represented by the ratio of its absolute permittivity to the electric constant. This dimensionless quantity is called the medium’s relative permittivity, sometimes also called \"permittivity\". Relative permittivity is also commonly referred to as the \"dielectric constant\", a term which has been deprecated in physics and engineering as well as in chemistry.\n\nBy definition, a perfect vacuum has a relative permittivity of exactly 1. The difference in permittivity between a vacuum and air can often be considered negligible, as κ = 1.0006.\n\nRelative permittivity is directly related to electric susceptibility (χ), which is a measure of how easily a dielectric polarizes in response to an electric field, given by\n\notherwise written as\n\nThe standard SI unit for permittivity is Farad per meter (F/m or F·m).\n\nIn electromagnetism, the electric displacement field represents how an electric field influences the organization of electric charges in a given medium, including charge migration and electric dipole reorientation. Its relation to permittivity in the very simple case of \"linear, homogeneous, isotropic\" materials with \"\"instantaneous\" response\" to changes in electric field is\n\nwhere the permittivity is a scalar. If the medium is anisotropic, the permittivity is a second rank tensor.\n\nIn general, permittivity is not a constant, as it can vary with the position in the medium, the frequency of the field applied, humidity, temperature, and other parameters. In a nonlinear medium, the permittivity can depend on the strength of the electric field. Permittivity as a function of frequency can take on real or complex values.\n\nIn SI units, permittivity is measured in farads per meter (F/m or A·s·kg·m). The displacement field is measured in units of coulombs per square meter (C/m), while the electric field is measured in volts per meter (V/m). and describe the interaction between charged objects. is related to the \"charge densities\" associated with this interaction, while is related to the \"forces\" and \"potential differences\".\n\nThe vacuum permittivity (also called permittivity of free space or the electric constant) is the ratio in free space. It also appears in the Coulomb force constant,\n\nIts value is\n\nwhere\nThe constants and are defined in SI units to have exact numerical values, shifting responsibility of experiment to the determination of the meter and the ampere. (The approximation in the second value of above stems from being an irrational number.)\n\nThe linear permittivity of a homogeneous material is usually given relative to that of free space, as a relative permittivity (also called dielectric constant, although this term is deprecated and sometimes only refers to the static, zero-frequency relative permittivity). In an anisotropic material, the relative permittivity may be a tensor, causing birefringence. The actual permittivity is then calculated by multiplying the relative permittivity by :\n\nwhere (frequently written ) is the electric susceptibility of the material.\n\nThe susceptibility is defined as the constant of proportionality (which may be a tensor) relating an electric field to the induced dielectric polarization density such that\n\nwhere is the electric permittivity of free space.\n\nThe susceptibility of a medium is related to its relative permittivity by\n\nSo in the case of a vacuum,\n\nThe susceptibility is also related to the polarizability of individual particles in the medium by the Clausius-Mossotti relation.\n\nThe electric displacement is related to the polarization density by\n\nThe permittivity and permeability of a medium together determine the phase velocity of electromagnetic radiation through that medium:\n\nThe capacitance of a capacitor is based on its design and architecture, meaning it will not change with charging and discharging. The formula for capacitance is written as\n\nwhere formula_15 is the area of one plate, formula_16 is the distance between the plates, and formula_17 is the permittivity of the medium between the two plates. For a capacitor with relative permittivity formula_18, it can be said that\n\nPermittivity is connected to electric flux (and by extension electric field) through Gauss's law. Gauss's law states that for a closed Gaussian surface, s\n\nwhere formula_21 is the net electric flux passing through the surface, formula_22 is the charge enclosed in the Gaussian surface, formula_23 is the electric field vector at a given point on the surface, and formula_24 is a differential area vector on the Gaussian surface.\n\nIf the Gaussian surface uniformly encloses an insulated, symmetrical charge arrangement, the formula can be simplified to\n\nwhere formula_26 represents the angle between the electric field vector and the area vector.\n\nIf all of the electric field lines cross the surface at 90°, the formula can be further simplified to\n\nBecause the surface area of a sphere is formula_28, the electric field a distance formula_29 away from a uniform, spherical charge arrangement is\n\nwhere formula_32 is Coulomb's constant (formula_33). This formula applies to the electric field due to a point charge, outside of a conducting sphere or shell, outside of a uniformly charged insulating sphere, or between the plates of a spherical capacitor.\n\nIn general, a material cannot polarize instantaneously in response to an applied field, and so the more general formulation as a function of time is\n\nThat is, the polarization is a convolution of the electric field at previous times with time-dependent susceptibility given by . The upper limit of this integral can be extended to infinity as well if one defines for . An instantaneous response would correspond to a Dirac delta function susceptibility .\n\nIt is convenient to take the Fourier transform with respect to time and write this relationship as a function of frequency. Because of the convolution theorem, the integral becomes a simple product,\n\nThis frequency dependence of the susceptibility leads to frequency dependence of the permittivity. The shape of the susceptibility with respect to frequency characterizes the dispersion properties of the material.\n\nMoreover, the fact that the polarization can only depend on the electric field at previous times (i.e. effectively for ), a consequence of causality, imposes Kramers–Kronig constraints on the susceptibility .\n\nAs opposed to the response of a vacuum, the response of normal materials to external fields generally depends on the frequency of the field. This frequency dependence reflects the fact that a material's polarization does not change instantaneously when an electric field is applied. The response must always be \"causal\" (arising after the applied field), which can be represented by a phase difference. For this reason, permittivity is often treated as a complex function of the (angular) frequency of the applied field:\n\n(since complex numbers allow specification of magnitude and phase). The definition of permittivity therefore becomes\n\nwhere\n\nThe response of a medium to static electric fields is described by the low-frequency limit of permittivity, also called the static permittivity (also ):\n\nAt the high-frequency limit, the complex permittivity is commonly referred to as . At the plasma frequency and below, dielectrics behave as ideal metals, with electron gas behavior. The static permittivity is a good approximation for alternating fields of low frequencies, and as the frequency increases a measurable phase difference emerges between and . The frequency at which the phase shift becomes noticeable depends on temperature and the details of the medium. For moderate field strength (), and remain proportional, and\n\nSince the response of materials to alternating fields is characterized by a complex permittivity, it is natural to separate its real and imaginary parts, which is done by convention in the following way:\n\nwhere\n\nThe choice of sign for time-dependence, , dictates the sign convention for the imaginary part of permittivity. The signs used here correspond to those commonly used in physics, whereas for the engineering convention one should reverse all imaginary quantities.\n\nThe complex permittivity is usually a complicated function of frequency , since it is a superimposed description of dispersion phenomena occurring at multiple frequencies. The dielectric function must have poles only for frequencies with positive imaginary parts, and therefore satisfies the Kramers–Kronig relations. However, in the narrow frequency ranges that are often studied in practice, the permittivity can be approximated as frequency-independent or by model functions.\n\nAt a given frequency, the imaginary part, , leads to absorption loss if it is positive (in the above sign convention) and gain if it is negative. More generally, the imaginary parts of the eigenvalues of the anisotropic dielectric tensor should be considered.\n\nIn the case of solids, the complex dielectric function is intimately connected to band structure. The primary quantity that characterizes the electronic structure of any crystalline material is the probability of photon absorption, which is directly related to the imaginary part of the optical dielectric function . The optical dielectric function is given by the fundamental expression:\n\nIn this expression, represents the product of the Brillouin zone-averaged transition probability at the energy with the joint density of states, ; is a broadening function, representing the role of scattering in smearing out the energy levels. In general, the broadening is intermediate between Lorentzian and Gaussian; for an alloy it is somewhat closer to Gaussian because of strong scattering from statistical fluctuations in the local composition on a nanometer scale.\n\nAccording to the Drude model of magnetized plasma, a more general expression which takes into account the interaction of the carriers with an alternating electric field at millimeter and microwave frequencies in an axially magnetized semiconductor requires the expression of the permittivity as a non-diagonal tensor. (see also Electro-gyration).\n\nIf vanishes, then the tensor is diagonal but not proportional to the identity and the medium is said to be a uniaxial medium, which has similar properties to a uniaxial crystal.\n\nMaterials can be classified according to their complex-valued permittivity , upon comparison of its real and imaginary components (or, equivalently, conductivity, , when accounted for in the latter). A \"perfect conductor\" has infinite conductivity, , while a \"perfect dielectric\" is a material that has no conductivity at all, ; this latter case, of real-valued permittivity (or complex-valued permittivity with zero imaginary component) is also associated with the name \"lossless media\". Generally, when we consider the material to be a \"low-loss dielectric\" (although not exactly lossless), whereas is associated with a \"good conductor\"; such materials with non-negligible conductivity yield a large amount of loss that inhibit the propagation of electromagnetic waves, thus are also said to be \"lossy media\". Those materials that do not fall under either limit are considered to be general media.\n\nIn the case of a lossy medium, i.e. when the conduction current is not negligible, the total current density flowing is:\n\nwhere\n\nThe size of the displacement current is dependent on the frequency ω of the applied field \"E\"; there is no displacement current in a constant field.\n\nIn this formalism, the complex permittivity is defined as:\n\nIn general, the absorption of electromagnetic energy by dielectrics is covered by a few different mechanisms that influence the shape of the permittivity as a function of frequency:\n\nThe above effects often combine to cause non-linear effects within capacitors. For example, dielectric absorption refers to the inability of a capacitor that has been charged for a long time to completely discharge when briefly discharged. Although an ideal capacitor would remain at zero volts after being discharged, real capacitors will develop a small voltage, a phenomenon that is also called \"soakage\" or \"battery action\". For some dielectrics, such as many polymer films, the resulting voltage may be less than 1–2% of the original voltage. However, it can be as much as 15–25% in the case of electrolytic capacitors or supercapacitors.\n\nIn terms of quantum mechanics, permittivity is explained by atomic and molecular interactions.\n\nAt low frequencies, molecules in polar dielectrics are polarized by an applied electric field, which induces periodic rotations. For example, at the microwave frequency, the microwave field causes the periodic rotation of water molecules, sufficient to break hydrogen bonds. The field does work against the bonds and the energy is absorbed by the material as heat. This is why microwave ovens work very well for materials containing water. There are two maxima of the imaginary component (the absorptive index) of water, one at the microwave frequency, and the other at far ultraviolet (UV) frequency. Both of these resonances are at higher frequencies than the operating frequency of microwave ovens.\n\nAt moderate frequencies, the energy is too high to cause rotation, yet too low to affect electrons directly, and is absorbed in the form of resonant molecular vibrations. In water, this is where the absorptive index starts to drop sharply, and the minimum of the imaginary permittivity is at the frequency of blue light (optical regime).\n\nAt high frequencies (such as UV and above), molecules cannot relax, and the energy is purely absorbed by atoms, exciting electron energy levels. Thus, these frequencies are classified as ionizing radiation.\n\nWhile carrying out a complete \"ab initio\" (that is, first-principles) modelling is now computationally possible, it has not been widely applied yet. Thus, a phenomenological model is accepted as being an adequate method of capturing experimental behaviors. The Debye model and the Lorentz model use a first-order and second-order (respectively) lumped system parameter linear representation (such as an RC and an LRC resonant circuit).\n\nThe relative permittivity of a material can be found by a variety of static electrical measurements. The complex permittivity is evaluated over a wide range of frequencies by using different variants of dielectric spectroscopy, covering nearly 21 orders of magnitude from 10 to 10 hertz. Also, by using cryostats and ovens, the dielectric properties of a medium can be characterized over an array of temperatures. In order to study systems for such diverse excitation fields, a number of measurement setups are used, each adequate for a special frequency range.\n\nVarious microwave measurement techniques are outlined in Chen \"et al.\". Typical errors for the Hakki-Coleman method employing a puck of material between conducting planes are about 0.3%.\n\nAt infrared and optical frequencies, a common technique is ellipsometry. Dual polarisation interferometry is also used to measure the complex refractive index for very thin films at optical frequencies.\n\n\n\n"}
{"id": "3224696", "url": "https://en.wikipedia.org/wiki?curid=3224696", "title": "Photodegradation", "text": "Photodegradation\n\nPhotodegradation is the alteration of materials by light. Typically, the term refers to the combined action of sunlight and air. Photodegradation is usually oxidation and hydrolysis. Often photodegradation is avoided, since it destroys paintings and other artifacts. It is however partly responsible for remineralization of biomass and is used intentionally in some disinfection technologies. Photodegradation does not apply to how materials may be aged or degraded via infrared light or heat, but does include degradation in all of the ultraviolet light wavebands.\n\nThe protection of food from photodegradation is very important. Some nutrients, like those found in beer for example, are affected by degradation when being exposed to sunlight. In the case of beer, the UV radiation causes a process, which entails the degradation of hop bitter compounds to 3-Methyl-2-buten-1-thiol and therefore changes the taste. As amber glass has the ability to absorb UV radiation, beer bottles are often made from this glass type to avoid this process.\n\nPaints, inks and dyes that are organic are more susceptible to photodegradation than those that are not. Ceramics are almost universally colored with non-organic origin materials so as to allow the material to resist photodegradation even under the most relentless conditions, maintaining its color.\n\nThe photodegradation of pesticides is of great interest because of the scale of agriculture and the intensive use of chemicals. Pesticides are however selected in part not to photodegrade readily in sunlight in order to allow them to exert their biocidal activity. Thus, additional modalities are implemented to enhance their photodegradation, including the use of photosensitizers, photocatalysts (e.g., titanium dioxide), and the addition of reagents such as hydrogen peroxide that would generate hydroxyl radicals that would attack the pesticides.\n\nThe photodegradation of pharmaceuticals is of interest because they are found in many water supplies. They have deleterious effects on aquatic organisms including toxicity, endocrine disruption, genetic damage. But also in the primary packaging material the photodegradation of pharmaceuticals has to be prevented. For this, amber glasses like Fiolax amber and Corning 51-L are commonly used to protect the pharmaceutical from UV radiations. Iodine (in the form of Lugol's solution) and collodial silver are universally used in packaging that lets through very little UV light so as to avoid degradation.\n\nCommon synthetic polymers that can be attacked include polypropylene and LDPE, where tertiary carbon bonds in their chain structures are the centres of attack. Ultraviolet rays interact with these bonds to form free radicals, which then react further with oxygen in the atmosphere, producing carbonyl groups in the main chain. The exposed surfaces of products may then discolour and crack, and in extreme cases, complete product disintegration can occur. \n\nIn fibre products like rope used in outdoor applications, product life will be low because the outer fibres will be attacked first, and will easily be damaged by abrasion for example. Discolouration of the rope may also occur, thus giving an early warning of the problem. \n\nPolymers which possess UV-absorbing groups such as aromatic rings may also be sensitive to UV degradation. Aramid fibres like Kevlar, for example, are highly UV-sensitive and must be protected from the deleterious effects of sunlight.\n\nMany organic chemicals are thermodynamically unstable in the presence of oxygen, however, their rate of spontaneous oxidation is slow at room temperature. In the language of physical chemistry, such reactions are kinetically limited. This kinetic stability allows the accumulation of complex environmental structures in the environment. Upon the absorption of light, triplet oxygen converts to singlet oxygen, a highly reactive form of the gas, which effects spin-allowed oxidations. In the atmosphere, the organic compounds are degraded by hydroxyl radicals, which are produced from water and ozone.\n\nPhotochemical reactions are initiated by the absorption of a photon, typically in the wavelength range 290-700 nm (at the surface of the Earth). The energy of an absorbed photon is transferred to electrons in the molecule and briefly changes their configuration (i.e., promotes the molecule from a ground state to an excited state). The excited state represents what is essentially a new molecule. Often excited state molecules are not kinetically stable in the presence of O or HO and can spontaneously decompose (oxidize or hydrolyze). Sometimes molecules decompose to produce high energy, unstable fragments that can react with other molecules around them. The two processes are collectively referred to as direct photolysis or indirect photolysis, and both mechanisms contribute to the removal of pollutants.\n\nThe United States federal standard for testing plastic for photo-degradation is 40 CFR Ch. I (7–1–03 Edition)PART 238\n\nPhotodegradation of plastics and other materials can be inhibited with additives, which are widely used. These additives include antioxidants, which interrupt degradation processes. Typical antioxidants are derivatives of aniline. Another type of additive are UV-absorbers. These agents capture the photon and convert it to heat. Typical UV-absorbers are hydroxy-substituted benzophenones, related to the chemicals used in sunscreen.\n\n\n"}
{"id": "952842", "url": "https://en.wikipedia.org/wiki?curid=952842", "title": "Principle of compositionality", "text": "Principle of compositionality\n\nIn mathematics, semantics, and philosophy of language, the principle of compositionality is the principle that the meaning of a complex expression is determined by the meanings of its constituent expressions and the rules used to combine them. This principle is also called Frege's principle, because Gottlob Frege is widely credited for the first modern formulation of it. However, the idea appears already among Indian philosophers of grammar such as Yāska, and also in Plato's work such as in \"Theaetetus\".\nBesides, the principle was never explicitly stated by Frege, and it was arguably already assumed by George Boole decades before Frege’s work.\n\nThe principle of compositionality states that in a meaningful sentence, if the lexical parts are taken out of the sentence, what remains will be the rules of composition. Take, for example, the sentence \"Socrates was a man\". Once the meaningful lexical items are taken away—\"Socrates\" and \"man\"—what is left is the pseudo-sentence, \"S was a M\". The task becomes a matter of describing what the connection is between S and M.\n\nIt is frequently taken to mean that every operation of the syntax should be associated with an operation of the semantics that acts on the meanings of the constituents combined by the syntactic operation. As a guideline for constructing semantic theories, this is generally taken, as in the influential work on the philosophy of language by Donald Davidson, to mean that every construct of the syntax should be associated by a clause of the T-schema with an operator in the semantics that specifies how the meaning of the whole expression is built from constituents combined by the syntactic rule. In some general mathematical theories (especially those in the tradition of Montague grammar), this guideline is taken to mean that the interpretation of a language is essentially given by a homomorphism between an algebra of syntactic representations and an algebra of semantic objects.\n\nThe principle of compositionality also exists in a similar form in the compositionality of programming languages.\n\nThe principle of compositionality has been the subject of intense debate. Indeed, there is no general agreement as to how the principle is to be interpreted, although there have been several attempts to provide formal definitions of it. (Szabó, 2012)\n\nScholars are also divided as to whether the principle should be regarded as a factual claim, open to empirical testing; an analytic truth, obvious from the nature of language and meaning; or a methodological principle to guide the development of theories of syntax and semantics. The Principle of Compositionality has been attacked in all three spheres, although so far none of the criticisms brought against it have been generally regarded as compelling. Most proponents of the principle, however, make certain exceptions for idiomatic expressions in natural language. (Szabó, 2012)\n\nFurther, in the context of the philosophy of language, the principle of compositionality does not explain all of meaning. For example, you cannot infer sarcasm purely on the basis of words and their composition, yet a phrase used sarcastically means something completely different from the same phrase uttered straightforwardly. Thus, some theorists argue that the principle has to be revised to take into account linguistic and extralinguistic context, which includes the tone of voice used, common ground between the speakers, the intentions of the speaker, and so on. (Szabó, 2012)\n\n\n"}
{"id": "20538184", "url": "https://en.wikipedia.org/wiki?curid=20538184", "title": "Privacy protocol", "text": "Privacy protocol\n\nPrivacy protocols are guildlines intended to allow computation while still protecting the individuals involved. It can be developed from just two individuals trying to discover if they both know the same secret, without leaking information about the secret itself. In this case, after the protocol runs, both individuals will either know that they share the secret, or know that they do not share it; they will have gained no additional information about the other's secret.\n\nFor example, say the secret is a name of a person. One protocol is to use a random phone number, such as 555-111-2222, then replace the last n digits of the phone number with the secret such as 555-111-JOHN. Then the first person calls the number and leaves a message with the person on the other end for the second person. Next the second person calls the number of their secret and asks if there are any messages for him. One issue with this protocol is that the phone number created might not exist.\n\nAnother protocol without this issue is to designate an airline, destination and date, and have the first person make a reservation using the name of their secret, then the second person goes and cancels the reservation using the name of their secret. If the second person is not successful, then they don't share the secret.\n\nA simple protocol that does not rely on a human third party involves password changing. This works anywhere one has to type in new passwords the same twice before the password is changed. The first individual will type their secret in the first box, and the second person will type their secret in the second box, if the password is successfully changed then the secret is shared. However the computer is still a third party and must be trusted not to have a key logger.\n\nA more involved protocol that does not involve any reliance on a third party, human or machine, involves \"n\" cups, each with a label of the name of a person that could be the secret. Each individual will then place a slip of paper under each cup, one slip of paper will say 'yes' on it and will go under the cup with the name of the secret on it, all the other slips will say 'no'. Then the labels will be removed, the cups shuffled, then flipped over to reveal the slips of paper. If there is a cup with both slips of paper saying 'yes' on them then they share the secret.\n\nThere are many other protocols that involve two individuals.\n"}
{"id": "154839", "url": "https://en.wikipedia.org/wiki?curid=154839", "title": "Pro bono", "text": "Pro bono\n\nPro bono publico (; usually shortened to pro bono) is a Latin phrase for professional work undertaken voluntarily and without payment. Unlike traditional volunteerism, it is service that uses the specific skills of professionals to provide services to those who are unable to afford them. \n\n\"Pro bono publico\" is also used in the United Kingdom to describe the central motivation of large organizations, such as the National Health Service and various NGOs which exist \"for the public good\" rather than for shareholder profit, but it equally or even more applies to the private sector where professionals like lawyers and bankers offer their specialist skills for the benefit of the community or NGOs.\n\nPro bono legal counsel may assist an individual or group on a legal case by filing government applications or petitions. A judge may occasionally determine that the loser should compensate a winning pro bono counsel.\n\nSouth Korean lawyers are required to do at least 30 hours of pro bono work per year, (the local bar associations can reduce the hours to 20). Those who have a good reason not to fulfill the requirement may pay ₩20,000–30,000 (USD 17-26) per hour instead.\n\nSince 2003, many UK law firms and law schools have celebrated an annual Pro Bono Week, which encourages solicitors and barristers to offer pro bono services and increases general awareness of pro bono service.\nLawWorks (the operating name for the Solicitors Pro Bono Group) is a national charity that works with solicitors and law students, encouraging and supporting them in carrying out legal pro bono work. It also acts as a clearing house for pro bono casework. Individuals and community groups may apply to the charity for free legal advice and mediation, where they could not otherwise afford to pay and are not entitled to legal aid. \nAdvocates for International Development, which exclusively brokers international pro bono contributing towards the Sustainable Development Goals, operates from a London base.\n\nLawyers in the United States are recommended under American Bar Association (ABA) ethical rules to contribute at least 50 hours of pro bono service per year. Some state bar associations, however, may recommend fewer hours. Rule 6.1 of the New York Rules of Professional Conduct strongly encourages lawyers to aspire to provide at least 50 hours of pro bono service each year and quantifies the minimal financial contributions that lawyers should aspire to make to organizations providing legal services to the poor and underserved. In contrast, other states, such as Illinois, do not have recommended hours, yet require annual disclosure of voluntary pro bono hours and contributions made to pro bono organizations. \n\nThe Chief Judge of New York has also instituted a requirement that applicants who plan to be admitted in 2015 and onward must complete 50 hours of pro bono service in order to qualify. All attorneys who register must report their voluntary pro bono hours and/or voluntary contributions.\nThe ABA has conducted four national surveys of pro bono service: one released in August 2005, \nthe second in February 2009, the third in March 2013 and the fourth in April 2018.\n\nThe ABA Standing Committee on Pro Bono and Public Service and its project, the Center for Pro Bono, are a national source of information, resources and assistance to support, facilitate, and expand the delivery of pro bono legal help. The ABA Standing Committee also sponsors Pro Bono Week during the week of October 23–29. The ABA Standing Committee on Legal Assistance for Military Personnel and Section of Litigation jointly sponsor the ABA Military Pro Bono Project, which delivers pro bono legal assistance to enlisted, active-duty military personnel.\n\nIn an October 2007 press conference reported in \"The Wall Street Journal\" and \"The New York Times\", the law student group Building a Better Legal Profession released its first annual ranking of top law firms by average billable hours, pro bono participation, and demographic diversity. The report found that most large firms fall short of their pro bono targets. The group has sent the information to top law schools around the country, encouraging students to take this data into account when choosing where to work after graduation.\n\nCorporate pro bono efforts generally focus on building the capacity of local nonprofits or mentoring local businesses. There are many models that businesses use and tailor to their specific strengths. They may loan employees, provide coaching and mentoring, complete a service marathon, create standardized team projects, engage in open-ended outsourcing, provide sector-wise solutions, perform general contracting, or work on a signature issue.\n\nIn this model companies are effectively donating a fully trained and paid-for employee to the non-profit. Employees apply for the coveted opportunity to pursue a pro bono interest by lending their knowledge and experience. They use their workplace skills in a hands-on and/or consulting role to build the partner nonprofit's capacity.\n\nEmployees match up with their nonprofit peers, form a relationship, and share functional expertise. They may connect them with assets for growth or revise their business models. For commodities and service-based businesses, coaching and mentoring is a fresh way for them to do philanthropy. It builds a stronger market for the businesses by strengthening the local economy and cultivates important skills for the service providers and recipients.\n\nAn event where people meet up to do pro bono work for many hours.\n\nIndividuals are placed on teams, each with specific roles and responsibilities. Each project is scoped and structured around a standard deliverable based on the needs of the nonprofit partners. Team projects are meant as fun team-building activities or as highly competitive competitions to examine leadership abilities in employees.\n\nA company makes its services available to a specific number of nonprofit organizations on an ongoing, as needed basis. Volunteers act in a mentor capacity to fill a non-profit’s need. Often employees use workplace skills to provide services that non-profits do not have the resources to fund.\n\nA company creates a deliverable pro bono resource that can be applicable to all nonprofits across the sector. Similar to creating products for consumers, this pro bono model advocates creating products that will be distributed for free or at a greatly reduced cost. Often these are software or other tech services.\n\nAn entity coordinates and oversees internal and external resources, promoting cross-sector collaboration to address a specific social problem. Contracting is generally done in an ad-hoc capacity and by intermediary organizations such as Taproot Foundation, Common Impact, or Points of Light.\n\nSignature issues combine corporate assets with pro bono work to fight social problems. This is as much a corporate branding initiative as it is an altruistic endeavor. Pro bono volunteers that come en masse from a company become associated with that cause while combating social issues.\n\n\n"}
{"id": "51220189", "url": "https://en.wikipedia.org/wiki?curid=51220189", "title": "Recall elections in Wisconsin", "text": "Recall elections in Wisconsin\n\nA recall election in the state of Wisconsin is a procedure by which voters can remove an elected official from office through a direct vote before his or her term has ended.\n\nIn 1911, newly elected Governor Francis E. McGovern laid out his progressive vision for Wisconsin, which included a proposal for a recall. The next week, State Senator Paul O. Husting introduced Senate Joint Resolution 9, which allowed for the recall of every office holder in the state, including both those elected and appointed. Several senators did not like that the recall also applied to judges, and attached an exemption for judges. The bill passed the Senate 20-7. The bill was then passed by the Assembly 64-1.\n\nSince proposed amendments to the Wisconsin Constitution must pass two consecutive legislatures before going to the people for a vote, Husting introduced his bill again on February 11, 1913. The bill passed the Senate 26-1, and the Assembly 72-17. The proposed amendment then when to the voters, who, in November 1914, voted it down by a margin of 64 percent to 36 percent.\n\nA second attempt at a recall amendment came in 1923, when Senator Henry Huber introduced Senate Joint Resolution 39, which allowed for the recall of public officials. It passed the Senate 17-12, and the Assembly 60-10. In 1924, Huber was elected Lieutenant Governor of Wisconsin, so the second introduction of the resolution was taken up by Max W. Heck. It passed the Senate 22-8, and the Assembly 70-22. The resolution was scheduled to appear on the November 2, 1926 ballot.\n\nThere was strong opposition to the proposed amendment, because it allowed for the recall of state judges. Organizations, such as the State Bar of Wisconsin, the Milwaukee Bar Association, Archbishop Sebastian Gebhard Messmer and the editorial boards of the Milwaukee Journal and the Milwaukee Sentinel.\n\nThe proposed amendment also had its supporters, including the editorial board of the Wisconsin State Journal, the Wisconsin State Federation of Labor, and U.S. Senator Robert M. La Follette Jr.\n\nThe amendment passed, by a margin of 50.6 percent to 49.4 percent.\n\nIn 2011, State Senator Jim Holperin became the first state legislator to be subject to a recall in two different legislative bodies: the Assembly in 1990, and the State Senate in 2011.\n\nIn 2012, Governor Scott Walker became the first governor in U.S. history to survive a recall.\n\nThe recall was officially created in November 1926 by constitutional amendment. The rules established in the constitution are:\n\nIn 1981, the constitution was amended, which changed the rules for recalls. This amendment changed the rules, which follow:\n\n\n\n\n"}
{"id": "25163261", "url": "https://en.wikipedia.org/wiki?curid=25163261", "title": "Reflective disclosure", "text": "Reflective disclosure\n\nReflective disclosure is a model of social criticism proposed and developed by philosopher Nikolas Kompridis. It is partly based on Martin Heidegger's insights into the phenomenon of world disclosure, which Kompridis applies to the field of political theory. The term refers to practices through which we can imagine and articulate meaningful alternatives to current social and political conditions, by acting back on their conditions of intelligibility. This could uncover possibilities that were previously suppressed or untried, or make us insightfully aware of a problem in a way that allows us to go on differently with our institutions, traditions and ideals.\n\nIn his book \"Critique and Disclosure: Critical Theory between Past and Future\", Kompridis describes a set of heterogeneous social practices he believes can be a source of significant ethical, political, and cultural transformation. Highlighting the work of theorists such as Hannah Arendt, Charles Taylor, Michel Foucault and others, Kompridis calls such practices examples of \"reflective disclosure\" after Martin Heidegger's insights into the phenomenon of world disclosure. He also argues that social criticism or critique, and in particular critical theory, ought to incorporate Heidegger's insights about this phenomenon and reorient itself around practices of reflective disclosure if it is, as he puts it, \"to have a future worthy of its past\".\n\nThese practices, according to Kompridis, constitute what Charles Taylor calls a \"new department\" of reason which is distinct from instrumental reason, from reason understood merely as the slave of the passions (Hume), and from the idea of reason as public justification (Rawls). In contrast to theories of social and political change that emphasize socio-historical contradictions (i.e., Marxist and neo-Marxist), theories of recognition and self-realization, and theories that try to make sense of change in terms of processes that are outside the scope of human agency, Kompridis' paradigm for critical theory, with reflective disclosure at the centre, is to help reopen the future by disclosing alternative possibilities for speech and action, self-critically expanding what he calls the normative and logical \"space of possibility\".\n\nKompridis contrasts his own vision of critical theory with a Habermasian emphasis on the procedures by which we can reach agreement in modern democratic societies. He claims the latter has ignored the utopian concerns that previously animated critical theory, and narrowed its scope in a way that brings it closer to liberal and neo-Kantian theories of justice.\n\n\n"}
{"id": "11347064", "url": "https://en.wikipedia.org/wiki?curid=11347064", "title": "Reversed electrodialysis", "text": "Reversed electrodialysis\n\nReverse electrodialysis (RED) is the salinity gradient energy retrieved from the difference in the salt concentration between seawater and river water. A method of utilizing the energy produced by this process by means of a heat engine was invented by Prof. Sidney Loeb in 1977 at the Ben-Gurion University of the Negev.\n--United States Patent US4171409\n\nIn reverse electrodialysis a salt solution and fresh water are let through a stack of alternating cation and anion exchange membranes. The chemical potential difference between salt and fresh water generates a voltage over each membrane and the total potential of the system is the sum of the potential differences over all membranes. The process works through difference in ion concentration instead of an electric field, which has implications for the type of membrane needed.\n\nIn RED, as in a fuel cell, the cells are stacked. A module with a capacity of 250 kW has the size of a shipping container.\n\nIn the Netherlands, for example, more than 3,300 m³ fresh water runs into the sea per second on average. The membrane halves the pressure differences which results in a water column of approximately 135 meters. The energy potential is therefore e=mgΔh=3.3*10 kg/s*10 m/s*135 meters ca.= 4.5*10 Joule per second, Power=4.5 gigawatts.\nIn 2006 a 50 kW plant was located at a coastal test site in Harlingen, the Netherlands, the focus being on prevention of biofouling of the anode, cathode, and membranes and increasing the membrane performance. In 2007 the Directorate for Public Works and Water Management, Redstack, and ENECO signed a declaration of intent for development of a pilot plant on the Afsluitdijk in the Netherlands. The plant was put into service on 26 November 2014 and produces 50 kW of electricity to show the technical feasibility in real-life conditions using fresh IJsselmeer water and salt water from the Wadden Sea. Theoretically, with 1m/s river water and an equal amount of sea water, approximately 1 MW of renewable electricity can be recovered at this location by upscaling the plant. It is to be expected that after this phase the installation could be further expanded to a final capacity of 200 MW.\n\n\n"}
{"id": "45594620", "url": "https://en.wikipedia.org/wiki?curid=45594620", "title": "Schulte table", "text": "Schulte table\n\nA Schulte table is a grid with randomly distributed numbers or letters used for development of speed reading, peripheral vision, attention and visual perception.\n\nGenerally 5x5 table used, while there are possible variations with different dimensions, coloured cells and values.\n\nFocus on the grid centre and find all the numbers (letters) with your peripheral vision, i.e. without moving your eyes.\n\nCompletion speed and number of mistakes are the measurement of the efficiency. From the results of each table the “exhausting curve” could be built which reflects attention stability and peripheral vision functionality.\n\nOther measurements which could be calculated based on Schulte Table performance include:\n\n\"WE = (T1 + T2 + … + Tn) / n\", where \"Ti\" - completion time of table \"i\".\n\n\"WU = T1 / WE\"\n\nThe result of 1,0 and lower shows good warming-up, while 1,0 and more means that one needs more time to prepare for the main work (warm-up).\n\n\"PS = Tn-1 / WE\"\n\nThe result of 1,0 and less shown good psychological stability.\n\nPositive effects include attention stability, improved visual perception, improved peripheral vision, and development of speed reading.\n\nThe Schulte Table was developed originally as a psycho-diagnostic test to study the properties of attention, by German psychiatrist and psychotherapist Walter Schulte (1910 — 1972)). From 1962 to 1972 Professor Schulte worked in Tübingen, where he worked in psychopathology and psychotherapy research. Initially, the sample was developed in engineering psychology, it has been used to assess the efficiency and speed of search movements of the vision.\n"}
{"id": "48941272", "url": "https://en.wikipedia.org/wiki?curid=48941272", "title": "Serious case review", "text": "Serious case review\n\nA serious case review (SCR) in England is held after a child or vulnerable adult dies or is seriously injured under circumstances where abuse or neglect are thought to have been involved. Its purpose is to learn lessons to help prevent future similar incidents. Similar procedures in other countries of the UK are called child practice reviews in Wales, case management reviews in Northern Ireland, and significant case reviews in Scotland. An SCR should be held if abuse or neglect is suspected to have been involved, a child has died or been seriously harmed and if there are concerns about the way organisations or professionals worked together to safeguard the child.\n\nThe Local Safeguarding Children Boards (LSCB) follow statutory guidance for conducting a serious case review in which the different professionals and organisations involved and the family are represented. The SCR should be completed within six months.\n\nAn SCR may also be commissioned following the death or injury of a vulnerable adult. For example, in 2010 Warwickshire County Council commissioned an SCR following the death of 27-year old Gemma Hayter, because \"a vulnerable adult had died and abuse or neglect is known or\nsuspected to be a factor in the death; and the case gives rise to concerns about the way in which local professionals and/or services work together to safeguard vulnerable adults\".\n"}
{"id": "3431840", "url": "https://en.wikipedia.org/wiki?curid=3431840", "title": "Sparsity-of-effects principle", "text": "Sparsity-of-effects principle\n\nIn the statistical analysis of the results from factorial experiments, the sparsity-of-effects principle states that a system is usually dominated by main effects and low-order interactions. Thus it is most likely that main (single factor) effects and two-factor interactions are the most significant responses in a factorial experiment. In other words, higher order interactions such as three-factor interactions are very rare. This is sometimes referred to as the \"hierarchical ordering principle\". The sparsity-of-effects principle actually refers to the idea that only a few effects in a factorial experiment will be statistically significant.\n\nThis principle is only valid on the assumption of a factor space far from a stationary point.\n\n"}
{"id": "30038", "url": "https://en.wikipedia.org/wiki?curid=30038", "title": "Thomas Henry Huxley", "text": "Thomas Henry Huxley\n\nThomas Henry Huxley (4 May 1825 – 29 June 1895) was an English biologist specialising in comparative anatomy. He is known as \"Darwin's Bulldog\" for his advocacy of Charles Darwin's theory of evolution.\n\nThe stories regarding Huxley's famous debate in 1860 with Samuel Wilberforce were a key moment in the wider acceptance of evolution and in his own career, although historians think that the surviving story of the debate is a later fabrication. Huxley had been planning to leave Oxford on the previous day, but, after an encounter with Robert Chambers, the author of \"Vestiges\", he changed his mind and decided to join the debate. Wilberforce was coached by Richard Owen, against whom Huxley also debated about whether humans were closely related to apes.\n\nHuxley was slow to accept some of Darwin's ideas, such as gradualism, and was undecided about natural selection, but despite this he was wholehearted in his public support of Darwin. Instrumental in developing scientific education in Britain, he fought against the more extreme versions of religious tradition.\n\nOriginally coining the term in 1869, Huxley elaborated on \"agnosticism\" in 1889 to frame the nature of claims in terms of what is knowable and what is not. Huxley statesAgnosticism, in fact, is not a creed, but a method, the essence of which lies in the rigorous application of a single principle... the fundamental axiom of modern science... In matters of the intellect, follow your reason as far as it will take you, without regard to any other consideration... In matters of the intellect, do not pretend that conclusions are certain which are not demonstrated or demonstrable. Use of that term has continued to the present day (see Thomas Henry Huxley and agnosticism). Much of Huxley's agnosticism is influenced by Kantian views on human perception and the ability to rely on rational evidence rather than belief systems.\n\nHuxley had little formal schooling and was virtually self-taught. He became perhaps the finest comparative anatomist of the later 19th century. He worked on invertebrates, clarifying relationships between groups previously little understood. Later, he worked on vertebrates, especially on the relationship between apes and humans. After comparing \"Archaeopteryx\" with \"Compsognathus\", he concluded that birds evolved from small carnivorous dinosaurs, a theory widely accepted today.\n\nThe tendency has been for this fine anatomical work to be overshadowed by his energetic and controversial activity in favour of evolution, and by his extensive public work on scientific education, both of which had significant effects on society in Britain and elsewhere.\n\nThomas Henry Huxley was born in Ealing, which was then a village in Middlesex. He was the second youngest of eight children of George Huxley and Rachel Withers. Like some other British scientists of the nineteenth century such as Alfred Russel Wallace, Huxley was brought up in a literate middle-class family which had fallen on hard times. His father was a mathematics teacher at Ealing School until it closed, putting the family into financial difficulties. As a result, Thomas left school at age 10, after only two years of formal schooling. Huxley’s parents were Anglicans, although it was against organized religion Huxley sympathized with the town’s Nonconformist.\n\nDespite this unenviable start, Huxley was determined to educate himself. He became one of the great autodidacts of the nineteenth century. At first he read Thomas Carlyle, James Hutton's \"Geology\", and Hamilton's \"Logic\". In his teens he taught himself German, eventually becoming fluent and used by Charles Darwin as a translator of scientific material in German. He learned Latin, and enough Greek to read Aristotle in the original.\n\nLater on, as a young adult, he made himself an expert, first on invertebrates, and later on vertebrates, all self-taught. He was skilled in drawing and did many of the illustrations for his publications on marine invertebrates. In his later debates and writing on science and religion his grasp of theology was better than most of his clerical opponents. Huxley, a boy who left school at ten, became one of the most knowledgeable men in Britain.\n\nHe was apprenticed for short periods to several medical practitioners: at 13 to his brother-in-law John Cooke in Coventry, who passed him on to Thomas Chandler, notable for his experiments using mesmerism for medical purposes. Chandler's practice was in London's Rotherhithe amidst the squalor endured by the Dickensian poor. Here Thomas would have seen poverty, crime and rampant disease at its worst. Next, another brother-in-law took him on: John Salt, his eldest sister's husband. Now 16, Huxley entered Sydenham College (behind University College Hospital), a cut-price anatomy school whose founder, Marshall Hall, discovered the reflex arc. All this time Huxley continued his programme of reading, which more than made up for his lack of formal schooling.\n\nA year later, buoyed by excellent results and a silver medal prize in the Apothecaries' yearly competition, Huxley was admitted to study at Charing Cross Hospital, where he obtained a small scholarship. At Charing Cross, he was taught by Thomas Wharton Jones, Professor of Ophthalmic Medicine and Surgery at University College London. Jones had been Robert Knox's assistant when Knox bought cadavers from Burke and Hare. The young Wharton Jones, who acted as go-between, was exonerated of crime, but thought it best to leave Scotland. He was a fine teacher, up-to-date in physiology and also an ophthalmic surgeon. In 1845, under Wharton Jones' guidance, Huxley published his first scientific paper demonstrating the existence of a hitherto unrecognised layer in the inner sheath of hairs, a layer that has been known since as Huxley's layer. No doubt remembering this, and of course knowing his merit, later in life Huxley organised a pension for his old tutor.\n\nAt twenty he passed his First M.B. examination at the University of London, winning the gold medal for anatomy and physiology. However, he did not present himself for the final (Second M.B.) exams and consequently did not qualify with a university degree. His apprenticeships and exam results formed a sufficient basis for his application to the Royal Navy.\n\nAged 20, Huxley was too young to apply to the Royal College of Surgeons for a licence to practise, yet he was 'deep in debt'. So, at a friend's suggestion, he applied for an appointment in the Royal Navy. He had references on character and certificates showing the time spent on his apprenticeship and on requirements such as dissection and pharmacy. Sir William Burnett, the Physician General of the Navy, interviewed him and arranged for the College of Surgeons to test his competence (by means of a \"viva voce\").\n\nFinally Huxley was made Assistant Surgeon ('surgeon's mate', but in practice marine naturalist) to HMS \"Rattlesnake\", about to set sail on a voyage of discovery and surveying to New Guinea and Australia. The \"Rattlesnake\" left England on 3 December 1846 and, once they had arrived in the southern hemisphere, Huxley devoted his time to the study of marine invertebrates. He began to send details of his discoveries back to England, where publication was arranged by Edward Forbes FRS (who had also been a pupil of Knox). Both before and after the voyage Forbes was something of a mentor to Huxley.\n\nHuxley's paper \"On the anatomy and the affinities of the family of Medusae\" was published in 1849 by the Royal Society in its \"Philosophical Transactions\". Huxley united the Hydroid and Sertularian polyps with the Medusae to form a class to which he subsequently gave the name of \"Hydrozoa\". The connection he made was that all the members of the class consisted of two cell layers, enclosing a central cavity or stomach. This is characteristic of the phylum now called the \"Cnidaria\". He compared this feature to the serous and mucous structures of embryos of higher animals. When at last he got a grant from the Royal Society for the printing of plates, Huxley was able to summarise this work in \"The Oceanic Hydrozoa\", published by the Ray Society in 1859.\n\nThe value of Huxley's work was recognised and, on returning to England in 1850, he was elected a Fellow of the Royal Society. In the following year, at the age of twenty-six, he not only received the Royal Society Medal but was also elected to the Council. He met Joseph Dalton Hooker and John Tyndall, who remained his lifelong friends. The Admiralty retained him as a nominal assistant-surgeon, so he might work on the specimens he collected and the observations he made during the voyage of the \"Rattlesnake\". He solved the problem of \"Appendicularia\", whose place in the animal kingdom Johannes Peter Müller had found himself wholly unable to assign. It and the Ascidians are both, as Huxley showed, tunicates, today regarded as a sister group to the vertebrates in the phylum \"Chordata\". Other papers on the morphology of the cephalopods and on brachiopods and rotifers are also noteworthy. The \"Rattlesnake\"'s official naturalist, John MacGillivray, did some work on botany, and proved surprisingly good at notating Australian aboriginal languages. He wrote up the voyage in the standard Victorian two volume format.\n\nHuxley effectively resigned from the navy (by refusing to return to active service) and, in July 1854, he became Professor of Natural History at the Royal School of Mines and naturalist to the British Geological Survey in the following year. In addition, he was Fullerian Professor at the Royal Institution 1855–58 and 1865–67; Hunterian Professor at the Royal College of Surgeons 1863–69; President of the British Association for the Advancement of Science 1869–1870; President of the Quekett Microscopical Club 1878; President of the Royal Society 1883–85; Inspector of Fisheries 1881–85; and President of the Marine Biological Association 1884–1890.\n\nThe thirty-one years during which Huxley occupied the chair of natural history at the Royal School of Mines included work on vertebrate palaeontology and on many projects to advance the place of science in British life. Huxley retired in 1885, after a bout of depressive illness which started in 1884. He resigned the presidency of the Royal Society in mid-term, the Inspectorship of Fisheries, and his chair (as soon as he decently could) and took six months' leave. His pension was a fairly handsome £1200 a year.\n\nIn 1890, he moved from London to Eastbourne where he edited the nine volumes of his \"Collected Essays\". In 1894 he heard of Eugene Dubois' discovery in Java of the remains of \"Pithecanthropus erectus\" (now known as \"Homo erectus\"). Finally, in 1895, he died of a heart attack (after contracting influenza and pneumonia), and was buried in North London at St Marylebone. This small family plot had been purchased upon the death of his beloved youngest son Noel, who died of scarlet fever in 1860; Huxley's wife Henrietta Anne née Heathorn and son Noel are also buried there. No invitations were sent out, but two hundred people turned up for the ceremony; they included Joseph Dalton Hooker, William Henry Flower, Mulford B. Foster, Edwin Lankester, Joseph Lister and, apparently, Henry James.\n\nHuxley and his wife had five daughters and three sons:\n\nNoel Huxley (1856–1860), died aged 4.\nJessie Oriana Huxley (1856–1927), married architect Fred Waller in 1877.\nMarian Huxley (1859–1887), married artist John Collier in 1879.\nLeonard Huxley (1860–1933), married Julia Arnold.\nRachel Huxley (1862–1934), married civil engineer Alfred Eckersley in 1884.\nHenrietta (Nettie) Huxley (1863–1940), married Harold Roller, travelled Europe as a singer.\nHenry Huxley (1865–1946), became a fashionable general practitioner in London.\nEthel Huxley (1866–1941) married artist John Collier (widower of sister) in 1889.\n\nFrom 1870 onwards, Huxley was to some extent drawn away from scientific research by the claims of public duty. He served on eight Royal Commissions, from 1862 to 1884. From 1871 to 1880 he was a Secretary of the Royal Society and from 1883 to 1885 he was president. He was president of the Geological Society from 1868 to 1870. In 1870, he was president of the British Association at Liverpool and, in the same year was elected a member of the newly constituted London School Board. He was president of the Quekett Microscopical Club from 1877 to 1879. He was the leading person amongst those who reformed the Royal Society, persuaded government about science, and established scientific education in British schools and universities. Before him, science was mostly a gentleman's occupation; after him, science was a profession.\n\nHe was awarded the highest honours then open to British men of science. The Royal Society, who had elected him as Fellow when he was 25 (1851), awarded him the Royal Medal the next year (1852), a year before Charles Darwin got the same award. He was the youngest biologist to receive such recognition. Then later in life came the Copley Medal in 1888 and the Darwin Medal in 1894; the Geological Society awarded him the Wollaston Medal in 1876; the Linnean Society awarded him the Linnean Medal in 1890. There were many other elections and appointments to eminent scientific bodies; these and his many academic awards are listed in the \"Life and Letters\". He turned down many other appointments, notably the Linacre chair in zoology at Oxford and the Mastership of University College, Oxford.\n\nIn 1873 the King of Sweden made Huxley, Hooker and Tyndall Knights of the Order of the Polar Star: they could wear the insignia but not use the title in Britain. Huxley collected many honorary memberships of foreign societies, academic awards and honorary doctorates from Britain and Germany. He also became foreign member of the Royal Netherlands Academy of Arts and Sciences in 1892.\n\nAs recognition of his many public services he was given a pension by the state, and was appointed Privy Councillor in 1892.\n\nDespite his many achievements he was given no award by the British state until late in life. In this he did better than Darwin, who got no award of any kind from the state. (Darwin's proposed knighthood was vetoed by ecclesiastical advisers, including Wilberforce) Perhaps Huxley had commented too often on his dislike of honours, or perhaps his many assaults on the traditional beliefs of organised religion made enemies in the establishment—he had vigorous debates in print with Benjamin Disraeli, William Ewart Gladstone and Arthur Balfour, and his relationship with Lord Salisbury was less than tranquil.\n\nHuxley was for about thirty years evolution's most effective advocate, and for some Huxley was \"\"the\" premier advocate of science in the nineteenth century [for] the whole English-speaking world\".\n\nThough he had many admirers and disciples, his retirement and later death left British zoology somewhat bereft of leadership. He had, directly or indirectly, guided the careers and appointments of the next generation, but none were of his stature. The loss of Francis Balfour in 1882, climbing the Alps just after he was appointed to a chair at Cambridge, was a tragedy. Huxley thought he was \"the only man who can carry out my work\": the deaths of Balfour and W. K. Clifford were \"the greatest losses to science in our time\".\n\nThe first half of Huxley's career as a palaeontologist is marked by a rather strange predilection for 'persistent types', in which he seemed to argue that evolutionary advancement (in the sense of major new groups of animals and plants) was rare or absent in the Phanerozoic. In the same vein, he tended to push the origin of major groups such as birds and mammals back into the Palaeozoic era, and to claim that no order of plants had ever gone extinct.\n\nMuch paper has been consumed by historians of science ruminating on this strange and somewhat unclear idea. Huxley was wrong to pitch the loss of orders in the Phanerozoic as low as 7%, and he did not estimate the number of new orders which evolved. Persistent types sat rather uncomfortably next to Darwin's more fluid ideas; despite his intelligence, it took Huxley a surprisingly long time to appreciate some of the implications of evolution. However, gradually Huxley moved away from this conservative style of thinking as his understanding of palaeontology, and the discipline itself, developed.\n\nHuxley's detailed anatomical work was, as always, first-rate and productive. His work on fossil fish shows his distinctive approach: whereas pre-Darwinian naturalists collected, identified and classified, Huxley worked mainly to reveal the evolutionary relationships between groups.\n\nThe lobed-finned fish (such as coelacanths and lung fish) have paired appendages whose internal skeleton is attached to the shoulder or pelvis by a single bone, the humerus or femur. His interest in these fish brought him close to the origin of tetrapods, one of the most important areas of vertebrate palaeontology.\n\nThe study of fossil reptiles led to his demonstrating the fundamental affinity of birds and reptiles, which he united under the title of \"Sauropsida\". His papers on \"Archaeopteryx\" and the origin of birds were of great interest then and still are.\n\nApart from his interest in persuading the world that man was a primate, and had descended from the same stock as the apes, Huxley did little work on mammals, with one exception. On his tour of America Huxley was shown the remarkable series of fossil horses, discovered by O. C. Marsh, in Yale's Peabody Museum. Marsh was part palaeontologist, part robber baron, a man who had hunted buffalo and met Red Cloud (in 1874). Funded by his uncle George Peabody, Marsh had made some remarkable discoveries:\nthe huge Cretaceous aquatic bird \"Hesperornis\", and the dinosaur footprints along the Connecticut River were worth the trip by themselves, but the horse fossils were really special.\n\nThe collection at that time went from the small four-toed forest-dwelling \"Orohippus\" from the Eocene through three-toed species such as \"Miohippus\" to species more like the modern horse. By looking at their teeth he could see that, as the size grew larger and the toes reduced, the teeth changed from those of a browser to those of a grazer. All such changes could be explained by a general alteration in habitat from forest to grassland. And, it is now known, that is what did happen over large areas of North America from the Eocene to the Pleistocene: the ultimate causative agent was global temperature reduction (see Paleocene–Eocene Thermal Maximum). The modern account of the evolution of the horse has many other members, and the overall appearance of the tree of descent is more like a bush than a straight line.\n\nThe horse series also strongly suggested that the process was gradual, and that the origin of the modern horse lay in North America, not in Eurasia. If so, then something must have happened to horses in North America, since none were there when Europeans arrived. The experience was enough for Huxley to give credence to Darwin's gradualism, and to introduce the story of the horse into his lecture series.\n\nHuxley was originally not persuaded of \"development theory\", as evolution was once called. This can be seen in his savage review of Robert Chambers' \"Vestiges of the Natural History of Creation\", a book which contained some quite pertinent arguments in favour of evolution. Huxley had also rejected Lamarck's theory of transmutation, on the basis that there was insufficient evidence to support it. All this scepticism was brought together in a lecture to the Royal Institution, which made Darwin anxious enough to set about an effort to change young Huxley's mind. It was the kind of thing Darwin did with his closest scientific friends, but he must have had some particular intuition about Huxley, who was from all accounts a most impressive person even as a young man.\n\nHuxley was therefore one of the small group who knew about Darwin's ideas before they were published (the group included Joseph Dalton Hooker and Charles Lyell). The first publication by Darwin of his ideas came when Wallace sent Darwin his famous paper on natural selection, which was presented by Lyell and Hooker to the Linnean Society in 1858 alongside excerpts from Darwin's notebook and a Darwin letter to Asa Gray. Huxley's famous response to the idea of natural selection was \"How extremely stupid not to have thought of that!\" However, he never conclusively made up his mind about whether natural selection was the main method for evolution, though he did admit it was a hypothesis which was a good working basis.\n\nLogically speaking, the prior question was whether evolution had taken place at all. It is to this question that much of Darwin's \"On the Origin of Species\" was devoted. Its publication in 1859 completely convinced Huxley of evolution and it was this and no doubt his admiration of Darwin's way of amassing and using evidence that formed the basis of his support for Darwin in the debates that followed the book's publication.\n\nHuxley's support started with his anonymous favourable review of the \"Origin\" in the \"Times\" for 26 December 1859, and continued with articles in several periodicals, and in a lecture at the Royal Institution in February 1860. At the same time, Richard Owen, whilst writing an extremely hostile anonymous review of the \"Origin\" in the \"Edinburgh Review\", also primed Samuel Wilberforce who wrote one in the \"Quarterly Review\", running to 17,000 words. The authorship of this latter review was not known for sure until Wilberforce's son wrote his biography. So it can be said that, just as Darwin groomed Huxley, so Owen groomed Wilberforce; and both the proxies fought public battles on behalf of their principals as much as themselves. Though we do not know the exact words of the Oxford debate, we do know what Huxley thought of the review in the \"Quarterly\":\n\nSince Lord Brougham assailed Dr Young, the world has seen no such specimen of the insolence of a shallow pretender to a Master in Science as this remarkable production, in which one of the most exact of observers, most cautious of reasoners, and most candid of expositors, of this or any other age, is held up to scorn as a \"flighty\" person, who endeavours \"to prop up his utterly rotten fabric of guess and speculation,\" and whose \"mode of dealing with nature\" is reprobated as \"utterly dishonourable to Natural Science.\"\n\nIf I confine my retrospect of the reception of the \"Origin of Species\" to a twelvemonth, or thereabouts, from the time of its publication, I do not recollect anything quite so foolish and unmannerly as the \"Quarterly Review\" article...\n\nHuxley said \"I am Darwin's bulldog\". While the second half of Darwin's life was lived mainly within his family, the younger combative Huxley operated mainly out in the world at large. A letter from Huxley to Ernst Haeckel (2 November 1871) states: \"The dogs have been snapping at [Darwin's] heels too much of late.\" At Oxford and Cambridge Universities, \"Bulldog\" was and still is student slang for a university policeman, whose job was to corral errant students and maintain their moral rectitude.\n\nFamously, Huxley responded to Wilberforce in the debate at the British Association meeting, on Saturday 30 June 1860 at the Oxford University Museum. Huxley's presence there had been encouraged on the previous evening when he met Robert Chambers, the Scottish publisher and author of \"Vestiges\", who was walking the streets of Oxford in a dispirited state, and begged for assistance. The debate followed the presentation of a paper by John William Draper, and was chaired by Darwins's former botany tutor John Stevens Henslow. Darwin's theory was opposed by the Lord Bishop of Oxford, Samuel Wilberforce, and those supporting Darwin included Huxley and their mutual friends Hooker and Lubbock. The platform featured Brodie and Professor Beale, and Robert FitzRoy, who had been captain of HMS \"Beagle\" during Darwin's voyage, spoke against Darwin.\n\nWilberforce had a track record against evolution as far back as the previous Oxford B.A. meeting in 1847 when he attacked Chambers' \"Vestiges\". For the more challenging task of opposing the \"Origin\", and the implication that man descended from apes, he had been assiduously coached by Richard Owen – Owen stayed with him the night before the debate. On the day Wilberforce repeated some of the arguments from his \"Quarterly Review\" article (written but not yet published), then ventured onto slippery ground. His famous jibe at Huxley (as to whether Huxley was descended from an ape on his mother's side or his father's side) was probably unplanned, and certainly unwise. Huxley's reply to the effect that he would rather be descended from an ape than a man who misused his great talents to suppress debate—the exact wording is not certain—was widely recounted in pamphlets and a spoof play.\n\nThe letters of Alfred Newton include one to his brother giving an eye-witness account of the debate, and written less than a month afterwards. Other eyewitnesses, with one or two exceptions (Hooker especially thought \"he\" had made the best points), give similar accounts, at varying dates after the event. The general view was and still is that Huxley got much the better of the exchange though Wilberforce himself thought he had done quite well. In the absence of a verbatim report differing perceptions are difficult to judge fairly; Huxley wrote a detailed account for Darwin, a letter which does not survive; however, a letter to his friend Frederick Daniel Dyster does survive with an account just three months after the event.\n\nOne effect of the debate was to increase hugely Huxley's visibility amongst educated people, through the accounts in newspapers and periodicals. Another consequence was to alert him to the importance of public debate: a lesson he never forgot. A third effect was to serve notice that Darwinian ideas could not be easily dismissed: on the contrary, they would be vigorously defended against orthodox authority. A fourth effect was to promote professionalism in science, with its implied need for scientific education. A fifth consequence was indirect: as Wilberforce had feared, a defence of evolution did undermine literal belief in the Old Testament, especially the Book of Genesis. Many of the liberal clergy at the meeting were quite pleased with the outcome of the debate; they were supporters, perhaps, of the controversial \"Essays and Reviews\". Thus both on the side of science, and on the side of religion, the debate was important, and its outcome significant. (see also below)\n\nThat Huxley and Wilberforce remained on courteous terms after the debate (and able to work together on projects such as the Metropolitan Board of Education) says something about both men, whereas Huxley and Owen were never reconciled.\n\nFor nearly a decade his work was directed mainly to the relationship of man to the apes. This led him directly into a clash with Richard Owen, a man widely disliked for his behaviour whilst also being admired for his capability. The struggle was to culminate in some severe defeats for Owen. Huxley's Croonian Lecture, delivered before the Royal Society in 1858 on \"The Theory of the Vertebrate Skull\" was the start. In this, he rejected Owen's theory that the bones of the skull and the spine were homologous, an opinion previously held by Goethe and Lorenz Oken.\n\nFrom 1860–63 Huxley developed his ideas, presenting them in lectures to working men, students and the general public, followed by publication. Also in 1862 a series of talks to working men was printed lecture by lecture as pamphlets, later bound up as a little green book; the first copies went on sale in December. Other lectures grew into Huxley's most famous work \"Evidence as to Man's place in Nature\" (1863) where he addressed the key issues long before Charles Darwin published his \"Descent of Man\" in 1871.\n\nAlthough Darwin did not publish his \"Descent of Man\" until 1871, the general debate on this topic had started years before (there was even a precursor debate in the 18th century between Monboddo and Buffon). Darwin had dropped a hint when, in the conclusion to the \"Origin\", he wrote: \"In the distant future... light will be thrown on the origin of man and his history\". Not so distant, as it turned out. A key event had already occurred in 1857 when Richard Owen presented (to the Linnean Society) his theory that man was marked off from all other mammals by possessing features of the brain peculiar to the genus \"Homo\". Having reached this opinion, Owen separated man from all other mammals in a subclass of its own. No other biologist held such an extreme view. Darwin reacted \"Man...as distinct from a chimpanzee [as] an ape from a platypus... I cannot swallow that!\" Neither could Huxley, who was able to demonstrate that Owen's idea was completely wrong.\nThe subject was raised at the 1860 BA Oxford meeting, when Huxley flatly contradicted Owen, and promised a later demonstration of the facts. In fact, a number of demonstrations were held in London and the provinces. In 1862 at the Cambridge meeting of the B.A. Huxley's friend William Flower gave a public dissection to show that the same structures (the posterior horn of the lateral ventricle and hippocampus minor) were indeed present in apes. The debate was widely publicised, and parodied as the \"Great Hippocampus Question\". It was seen as one of Owen's greatest blunders, revealing Huxley as not only dangerous in debate, but also a better anatomist.\n\nOwen conceded that there was something that could be called a hippocampus minor in the apes, but stated that it was much less developed and that such a presence did not detract from the overall distinction of simple brain size.\n\nHuxley's ideas on this topic were summed up in January 1861 in the first issue (new series) of his own journal, the \"Natural History Review\": \"the most violent scientific paper he had ever composed\". This paper was reprinted in 1863 as chapter 2 of \"Man's Place in Nature\", with an addendum giving his account of the Owen/Huxley controversy about the ape brain. In his \"Collected Essays\" this addendum was removed.\n\nThe extended argument on the ape brain, partly in debate and partly in print, backed by dissections and demonstrations, was a landmark in Huxley's career. It was highly important in asserting his dominance of comparative anatomy, and in the long run more influential in establishing evolution amongst biologists than was the debate with Wilberforce. It also marked the start of Owen's decline in the esteem of his fellow biologists.\n\nThe following was written by Huxley to Rolleston before the BA meeting in 1861:\n\nDuring those years there was also work on human fossil anatomy and anthropology. In 1862 he examined the Neanderthal skull-cap, which had been discovered in 1857. It was the first pre-\"sapiens\" discovery of a fossil man, and it was immediately clear to him that the brain case was surprisingly large.\n\nPerhaps less productive was his work on physical anthropology, a topic which fascinated the Victorians. Huxley classified the human races into nine categories, and discussed them under four headings as: Australoid, Negroid, Xanthocroic and Mongoloid types. Such classifications depended mainly on appearance and anatomical characteristics.\n\nHuxley was certainly not slavish in his dealings with Darwin. As shown in every biography, they had quite different and rather complementary characters. Important also, Darwin was a field naturalist, but Huxley was an anatomist, so there was a difference in their experience of nature. Lastly, Darwin's views on science were different from Huxley's views. For Darwin, natural selection was the best way to explain evolution because it explained a huge range of natural history facts and observations: it solved problems. Huxley, on the other hand, was an empiricist who trusted what he could see, and some things are not easily seen. With this in mind, one can appreciate the debate between them, Darwin writing his letters, Huxley never going quite so far as to say he thought Darwin was right.\n\nHuxley's reservations on natural selection were of the type \"until selection and breeding can be seen to give rise to varieties which are infertile with each other, natural selection cannot be proved\". Huxley's position on selection was agnostic; yet he gave no credence to any other theory. Despite this concern about evidence, Huxley saw that if evolution came about through variation, reproduction and selection then other things would also be subject to the same pressures. This included ideas because they are invented, imitated and selected by humans: ‘The struggle for existence holds as much in the intellectual as in the physical world. A theory is a species of thinking, and its right to exist is coextensive with its power of resisting extinction by its rivals.’ This is the same idea as meme theory put forward by Richard Dawkins in 1976.\n\nDarwin's part in the discussion came mostly in letters, as was his wont, along the lines: \"The empirical evidence you call for is both impossible in practical terms, and in any event unnecessary. It's the same as asking to see every step in the transformation (or the splitting) of one species into another. My way so many issues are clarified and problems solved; no other theory does nearly so well\".\n\nHuxley's reservation, as Helena Cronin has so aptly remarked, was contagious: \"it spread itself for years among all kinds of doubters of Darwinism\". One reason for this doubt was that comparative anatomy could address the question of descent, \"but not the question of mechanism\".\n\nHuxley was a pallbearer at the funeral of Charles Darwin on 26 April 1882.\n\nIn November 1864, Huxley succeeded in launching a dining club, the X Club, composed of like-minded people working to advance the cause of science; not surprisingly, the club consisted of most of his closest friends. There were nine members, who decided at their first meeting that there should be no more. The members were: Huxley, John Tyndall, J. D. Hooker, John Lubbock (banker, biologist and neighbour of Darwin), Herbert Spencer (social philosopher and sub-editor of the Economist), William Spottiswoode (mathematician and the Queen's Printer), Thomas Hirst (Professor of Physics at University College London), Edward Frankland (the new Professor of Chemistry at the Royal Institution) and George Busk, zoologist and palaeontologist (formerly surgeon for HMS \"Dreadnought\"). All except Spencer were Fellows of the Royal Society. Tyndall was a particularly close friend; for many years they met regularly and discussed issues of the day. On more than one occasion Huxley joined Tyndall in the latter's trips into the Alps and helped with his investigations in glaciology.\n\nThere were also some quite significant X-Club satellites such as William Flower and George Rolleston, (Huxley protegés), and liberal clergyman Arthur Stanley, the Dean of Westminster. Guests such as Charles Darwin and Hermann von Helmholtz were entertained from time to time.\n\nThey would dine early on first Thursdays at a hotel, planning what to do; high on the agenda was to change the way the Royal Society Council did business. It was no coincidence that the Council met later that same evening. First item for the Xs was to get the Copley Medal for Darwin, which they managed after quite a struggle.\n\nThe next step was to acquire a journal to spread their ideas. This was the weekly \"Reader\", which they bought, revamped and redirected. Huxley had already become part-owner of the \"Natural History Review\" bolstered by the support of Lubbock, Rolleston, Busk and Carpenter (X-clubbers and satellites). The journal was switched to pro-Darwinian lines and relaunched in January 1861. After a stream of good articles the \"NHR\" failed after four years; but it had helped at a critical time for the establishment of evolution. The \"Reader\" also failed, despite its broader appeal which included art and literature as well as science. The periodical market was quite crowded at the time, but most probably the critical factor was Huxley's time; he was simply over-committed, and could not afford to hire full-time editors. This occurred often in his life: Huxley took on too many ventures, and was not so astute as Darwin at getting others to do work for him.\n\nHowever, the experience gained with the \"Reader\" was put to good use when the X Club put their weight behind the founding of \"Nature\" in 1869. This time no mistakes were made: above all there was a permanent editor (though not full-time), Norman Lockyer, who served until 1919, a year before his death. In 1925, to celebrate his centenary, \"Nature\" issued a supplement devoted to Huxley.\n\nThe peak of the X Club's influence was from 1873 to 1885 as Hooker, Spottiswoode and Huxley were Presidents of the Royal Society in succession. Spencer resigned in 1889 after a dispute with Huxley over state support for science. After 1892 it was just an excuse for the surviving members to meet. Hooker died in 1911, and Lubbock (now Lord Avebury) was the last surviving member.\n\nHuxley was also an active member of the Metaphysical Society, which ran from 1869 to 1880. It was formed around a nucleus of clergy and expanded to include all kinds of opinions. Tyndall and Huxley later joined The Club (founded by Dr. Johnson) when they could be sure that Owen would not turn up.\n\nWhen Huxley himself was young there were virtually no degrees in British universities in the biological sciences and few courses. Most biologists of his day were either self-taught, or took medical degrees. When he retired there were established chairs in biological disciplines in most universities, and a broad consensus on the curricula to be followed. Huxley was the single most influential person in this transformation.\n\nIn the early 1870s the Royal School of Mines moved to new quarters in South Kensington; ultimately it would become one of the constituent parts of Imperial College London. The move gave Huxley the chance to give more prominence to laboratory work in biology teaching, an idea suggested by practice in German universities. In the main, the method was based on the use of carefully chosen types, and depended on the dissection of anatomy, supplemented by microscopy, museum specimens and some elementary physiology at the hands of Foster.\n\nThe typical day would start with Huxley lecturing at 9am, followed by a program of laboratory work supervised by his demonstrators. Huxley's demonstrators were picked men—all became leaders of biology in Britain in later life, spreading Huxley's ideas as well as their own. Michael Foster became Professor of Physiology at Cambridge; E. Ray Lankester became Jodrell Professor of Zoology at University College London (1875–91), Professor of Comparative Anatomy at Oxford (1891–98) and Director of the Natural History Museum (1898–1907); S.H. Vines became Professor of Botany at Cambridge; W.T. Thiselton-Dyer became Hooker's successor at Kew (he was already Hooker's son-in-law!); T. Jeffery Parker became Professor of Zoology and Comparative Anatomy at University College, Cardiff; and William Rutherford became the Professor of Physiology at Edinburgh. William Flower, Conservator to the Hunterian Museum, and THH's assistant in many dissections, became Sir William Flower, Hunterian Professor of Comparative Anatomy and, later, Director of the Natural History Museum. It's a remarkable list of disciples, especially when contrasted with Owen who, in a longer professional life than Huxley, left no disciples at all. \"No one fact tells so strongly against Owen... as that he has never reared one pupil or follower\".\n\nHuxley's courses for students were so much narrower than the man himself that many were bewildered by the contrast: \"The teaching of zoology by use of selected animal types has come in for much criticism\"; Looking back in 1914 to his time as a student, Sir Arthur Shipley said \"Darwin's later works all dealt with living organisms, yet our obsession was with the dead, with bodies preserved, and cut into the most refined slices\". E.W MacBride said \"Huxley... would persist in looking at animals as material structures and not as living, active beings; in a word... he was a necrologist. To put it simply, Huxley preferred to teach what he had actually seen with his own eyes.\n\nThis largely morphological program of comparative anatomy remained at the core of most biological education for a hundred years until the advent of cell and molecular biology and interest in evolutionary ecology forced a fundamental rethink. It is an interesting fact that the methods of the field naturalists who led the way in developing the theory of evolution (Darwin, Wallace, Fritz Müller, Henry Bates) were scarcely represented at all in Huxley's program. Ecological investigation of life in its environment was virtually non-existent, and theory, evolutionary or otherwise, was at a discount. Michael Ruse finds no mention of evolution or Darwinism in any of the exams set by Huxley, and confirms the lecture content based on two complete sets of lecture notes.\n\nSince Darwin, Wallace and Bates did not hold teaching posts at any stage of their adult careers (and Műller never returned from Brazil) the imbalance in Huxley's program went uncorrected. It is surely strange that Huxley's courses did not contain an account of the evidence collected by those naturalists of life in the tropics; evidence which they had found so convincing, and which caused their views on evolution by natural selection to be so similar. Desmond suggests that \"[biology] had to be simple, synthetic and assimilable [because] it was to train teachers and had no other heuristic function\". That must be part of the reason; indeed it does help to explain the stultifying nature of much school biology. But zoology as taught at all levels became far too much the product of one man.\n\nHuxley was comfortable with comparative anatomy, at which he was the greatest master of the day. He was not an all-round naturalist like Darwin, who had shown clearly enough how to weave together detailed factual information and subtle arguments across the vast web of life. Huxley chose, in his teaching (and to some extent in his research) to take a more straightforward course, concentrating on his personal strengths.\n\nHuxley was also a major influence in the direction taken by British schools: in November 1870 he was voted onto the London School Board. In primary schooling, he advocated a wide range of disciplines, similar to what is taught today: reading, writing, arithmetic, art, science, music, etc. In secondary education he recommended two years of basic liberal studies followed by two years of some upper-division work, focusing on a more specific area of study. A practical example of the latter is his famous 1868 lecture \"On a Piece of Chalk\" which was first published as an essay in \"Macmillan's Magazine\" in London later that year. The piece reconstructs the geological history of Britain from a simple piece of chalk and demonstrates science as \"organized common sense\".\n\nHuxley supported the reading of the Bible in schools. This may seem out of step with his agnostic convictions, but he believed that the Bible's significant moral teachings and superb use of language were relevant to English life. \"I do not advocate burning your ship to get rid of the cockroaches\".\nHowever, what Huxley proposed was to create an \"edited version\" of the Bible, shorn of \"shortcomings and errors... statements to which men of science absolutely and entirely demur... These tender children [should] not be taught that which you do not yourselves believe\". The Board voted against his idea, but it also voted against the idea that public money should be used to support students attending church schools. Vigorous debate took place on such points, and the debates were minuted in detail. Huxley said \"I will never be a party to enabling the State to sweep the children of this country into denominational schools\". The Act of Parliament which founded board schools permitted the reading of the Bible, but did not permit any denominational doctrine to be taught.\n\nIt may be right to see Huxley's life and work as contributing to the secularisation of British society which gradually occurred over the following century. Ernst Mayr said \"It can hardly be doubted that [biology] has helped to undermine traditional beliefs and value systems\"  — and Huxley more than anyone else was responsible for this trend in Britain. Some modern Christian apologists consider Huxley the father of antitheism, though he himself maintained that he was an agnostic, not an atheist. He was, however, a lifelong and determined opponent of almost all organised religion throughout his life, especially the \"Roman Church... carefully calculated for the destruction of all that is highest in the moral nature, in the intellectual freedom, and in the political freedom of mankind\". In the same line of thought, in an article in \"Popular Science\", Huxley used the expression \"the so-called Christianity of Catholicism,\" explaining: \"I say 'so-called' not by way of offense, but as a protest against the monstruous assumption that Catholic Christianity is explicitly or implicitly contained in any trust-worthy record of the teaching of Jesus of Nazareth.\"\n\nIn 1893, during preparation for the second Romanes Lecture, Huxley expressed his disappointment at the shortcomings of 'liberal' theology, describing its doctrines as 'popular illusions', and the teachings they replaced 'faulty as they are, appear to me to be vastly nearer the truth'.\n\nVladimir Lenin remarked (in \"Materialism and empirio-criticism\") \"In Huxley's case... agnosticism serves as a fig-leaf for materialism\" (see also the Debate with Wilberforce above).\n\nHuxley's interest in education went still further than school and university classrooms; he made a great effort to reach interested adults of all kinds: after all, he himself was largely self-educated. There were his lecture courses for working men, many of which were published afterwards, and there was the use he made of journalism, partly to earn money but mostly to reach out to the literate public. For most of his adult life he wrote for periodicals—the \"Westminster Review\", the \"Saturday Review\", the \"Reader\", the \"Pall Mall Gazette\", \"Macmillan's Magazine\", the \"Contemporary Review\". Germany was still ahead in formal science education, but interested people in Victorian Britain could use their initiative and find out what was going on by reading periodicals and using the lending libraries.\n\nIn 1868 Huxley became Principal of the South London Working Men's College in Blackfriars Road. The moving spirit was a portmanteau worker, Wm. Rossiter, who did most of the work; the funds were put up mainly by F.D. Maurice's Christian Socialists. At sixpence for a course and a penny for a lecture by Huxley, this was some bargain; and so was the free library organised by the college, an idea which was widely copied. Huxley thought, and said, that the men who attended were as good as any country squire.\n\nThe technique of printing his more popular lectures in periodicals which were sold to the general public was extremely effective. A good example was \"The physical basis of life\", a lecture given in Edinburgh on 8 November 1868. Its theme — that vital action is nothing more than \"the result of the molecular forces of the protoplasm which displays it\" — shocked the audience, though that was nothing compared to the uproar when it was published in the \"Fortnightly Review\" for February 1869. John Morley, the editor, said \"No article that had appeared in any periodical for a generation had caused such a sensation\". The issue was reprinted seven times and protoplasm became a household word; \"Punch\" added 'Professor Protoplasm' to his other soubriquets.\n\nThe topic had been stimulated by Huxley seeing the cytoplasmic streaming in plant cells, which is indeed a sensational sight. For these audiences Huxley's claim that this activity should not be explained by words such as vitality, but by the working of its constituent chemicals, was surprising and shocking. Today we would perhaps emphasise the extraordinary structural arrangement of those chemicals as the key to understanding what cells do, but little of that was known in the nineteenth century.\n\nWhen the Archbishop of York thought this 'new philosophy' was based on Auguste Comte's positivism, Huxley corrected him: \"Comte's philosophy [is just] Catholicism minus Christianity\" (Huxley 1893 vol 1 of Collected Essays \"Methods & Results\" 156). A later version was \"[positivism is] sheer Popery with M. Comte in the chair of St Peter, and with the names of the saints changed\". (lecture on \"The scientific aspects of positivism\" Huxley 1870 \"Lay Sermons, Addresses and Reviews\" p. 149). Huxley's dismissal of positivism damaged it so severely that Comte's ideas withered in Britain.\n\nDuring his life, and especially in the last ten years after retirement, Huxley wrote on many issues relating to the humanities.\n\nPerhaps the best known of these topics is \"Evolution and Ethics\", which deals with the question of whether biology has anything particular to say about moral philosophy. Both Huxley and his grandson Julian Huxley gave Romanes Lectures on this theme. For a start, Huxley dismisses religion as a source of moral authority. Next, he believes the mental characteristics of man are as much a product of evolution as the physical aspects. Thus, our emotions, our intellect, our tendency to prefer living in groups and spend resources on raising our young are part and parcel of our evolution, and therefore inherited.\n\nDespite this, the \"details\" of our values and ethics are not inherited: they are partly determined by our culture, and partly chosen by ourselves. Morality and duty are often at war with natural instincts; ethics cannot be derived from the \"struggle for existence\": \"Of moral purpose I see not a trace in nature. That is an article of exclusively human manufacture.\" It is therefore our responsibility to make ethical choices (see Ethics and Evolutionary ethics). This seems to put Huxley as a compatibilist in the Free Will vs Determinism debate. In this argument Huxley is diametrically opposed to his old friend Herbert Spencer.\n\nHuxley's dissection of Rousseau's views on man and society is another example of his later work. The essay undermines Rousseau's ideas on man as a preliminary to undermining his ideas on the ownership of property. Characteristic is: \"The doctrine that all men are, in any sense, or have been, at any time, free and equal, is an utterly baseless fiction.\"\n\nHuxley's method of argumentation (his strategy and tactics of persuasion in speech and print) is itself much studied. His career included controversial debates with scientists, clerics and politicians; persuasive discussions with Royal Commissions and other public bodies; lectures and articles for the general public, and a mass of detailed letter-writing to friends and other correspondents. A large number of textbooks have excerpted his prose for anthologies.\n\nHuxley worked on ten Royal and other commissions (titles somewhat shortened here). The Royal Commission is the senior investigative forum in the British constitution. A rough analysis shows that five commissions involved science and scientific education; three involved medicine and three involved fisheries. Several involve difficult ethical and legal issues. All deal with possible changes to law and/or administrative practice.\n\n\n\nIn 1855, he married Henrietta Anne Heathorn (1825–1915), an English émigrée whom he had met in Sydney. They kept correspondence until he was able to send for her. They had five daughters and three sons:\n\n\nHuxley's relationships with his relatives and children were genial by the standards of the day—so long as they lived their lives in an honourable manner, which some did not. After his mother, his eldest sister Lizzie was the most important person in his life until his own marriage. He remained on good terms with his children, more than can be said of many Victorian fathers. This excerpt from a letter to Jessie, his eldest daughter is full of affection:\n\nHuxley's descendants include children of Leonard Huxley:\n\n\nOther significant descendants of Huxley, such as Sir Crispin Tickell, are treated in the Huxley family.\n\nBiographers have sometimes noted the occurrence of mental illness in the Huxley family. His father became \"sunk in worse than childish imbecility of mind\", and later died in Barming Asylum; brother George suffered from \"extreme mental anxiety\" and died in 1863 leaving serious debts. Brother James, a well known psychiatrist and Superintendent of Kent County Asylum, was at 55 \"as near mad as any sane man can be\"; and there is more. His favourite daughter, the artistically talented Mady (Marian), who became the first wife of artist John Collier, was troubled by mental illness for years. She died of pneumonia in her mid-twenties.\n\nAbout Huxley himself we have a more complete record. As a young apprentice to a medical practitioner, aged thirteen or fourteen, Huxley was taken to watch a post-mortem dissection. Afterwards he sank into a 'deep lethargy' and though Huxley ascribed this to dissection poisoning, Bibby and others may be right to suspect that emotional shock precipitated the depression. Huxley recuperated on a farm, looking thin and ill.\n\nThe next episode we know of in Huxley's life when he suffered a debilitating depression was on the third voyage of HMS \"Rattlesnake\" in 1848. Huxley had further periods of depression at the end of 1871, and again in 1873. Finally, in 1884 he sank into another depression, and this time it precipitated his decision to retire in 1885, at the age of only 60. This is enough to indicate the way depression (or perhaps a moderate bi-polar disorder) interfered with his life, yet unlike some of the other family members, he was able to function extremely well at other times.\n\nThe problems continued sporadically into the third generation. Two of Leonard's sons suffered serious depression: Trevennen committed suicide in 1914 and Julian suffered a breakdown in 1913, and five more later in life.\n\nDarwin's ideas and Huxley's controversies gave rise to many cartoons and satires. It was the debate about man's place in nature that roused such widespread comment: cartoons are so numerous as to be almost impossible to count; Darwin's head on a monkey's body is one of the visual clichés of the age. The \"Great Hippocampus Question\" attracted particular attention:\n\n\n\n\n\n\n \n"}
{"id": "14703193", "url": "https://en.wikipedia.org/wiki?curid=14703193", "title": "Type inhabitation", "text": "Type inhabitation\n\nIn type theory, a branch of mathematical logic, in a given typed calculus, the type inhabitation problem for this calculus is the following problem: given a type formula_1 and a typing environment formula_2, does there exist a formula_3-term M such that formula_4? With an empty type environment, such an M is said to be an inhabitant of formula_1.\n\nIn the case of simply typed lambda calculus, a type has an inhabitant if and only if its corresponding proposition is a tautology of minimal implicative logic. Similarly, a System F type has an inhabitant if and only if its corresponding proposition is a tautology of second-order logic.\n\nFor most typed calculi, the type inhabitation problem is very hard. Richard Statman proved that for simply typed lambda calculus the type inhabitation problem is PSPACE-complete. For other calculi, like System F, the problem is even undecidable.\n\n"}
{"id": "14236462", "url": "https://en.wikipedia.org/wiki?curid=14236462", "title": "Warwick Collins", "text": "Warwick Collins\n\nWarwick Collins (born 14 December 1948 - 10 February 2013) was a British novelist, screenwriter, yacht designer, and evolutionary theorist.\nCollins was born in Johannesburg to English-speaking parents. His father, Robin Collins, was a novelist who wrote under the nom-de-plume Robin Cranford. Robin Collins's novels were written from a liberal perspective and one of them, \"My City Fears Tomorrow\", was banned by the South African apartheid regime. When Warwick Collins was eleven, his family moved to England, and Collins entered The King's School, Canterbury. He continued his education at the University of Sussex, where he read Biology. He lived for many years in the Hampshire town of Lymington where he set two of his novels.\n\nHis early poetry was featured in \"Encounter\" between 1968 and 1971.\n\nCollins studied biology at The University of Sussex, where his tutor was the leading theoretical biologist John Maynard Smith. In 1975 Collins voiced to Maynard Smith the view that natural selection could not drive evolution because it always acted to reduce variation in favour of an optimum type for any environment, whereas the central story of evolution was that of increasing variation and complexity. Collins quoted Charles Darwin in \"The Origin of Species\" (\"... unless profitable variations do occur, natural selection can do nothing.\"), and argued that if variation must always occur before natural selection can act, then variation, and not natural selection, drives evolution. He asked Maynard Smith whether he could search for a \"strong\" theory of variation. Maynard Smith warned Collins that he could not support his efforts to pursue a rival theory to the theory that natural selection drives evolution. Collins replied that he thought the object of science was to question and examine everything, including hallowed theories such as the theory of natural selection. Maynard Smith asserted that, on the contrary, the strength of science was its capacity to agree on certain principles, and act collectively to pursue agreed aims. This difference of view with his tutor made Collins give up his scientific career and pursue other interests instead.\n\nAfter leaving university, Collins became a yacht designer and invented and patented the tandem keel, which was conceived to create high performance at low draft, but which also remains one of the radical keels in the America's Cup. He continued his interest in yacht design with an innovation in hull design called the Universal Hull. This fused together two classic hull types (the long, thin, easily driven hull and the beamy commodious hull) in a form which yielded the chief virtues of both types of hull. The two hulls are joined above the waterline by a ledge which also acts as a spray ledge. The resulting shape is easily driven because of the long, thin underwater shape but enjoys the accommodation space (above the waterline) of a beamy hull.\n\nIn the 1990s Collins turned to fiction, publishing three sailing novels and then a series of more wide-ranging novels, including two (\"The Rationalist\" and \"The Marriage of Souls\") which are set in 18th century Lymington. He published ten novels in all.\n\nCollins's political views were liberal and libertarian, but (in 1979) he was asked by Keith Joseph to join a Conservative party think tank chaired by John Hoskyns (who became Chief Political Adviser to Margaret Thatcher) to work on issues such as privatisation. Collins, though left of centre politically, always believed, in common with \"classical liberals\" such as Gladstone, that the free market is a superior means of distributing wealth than the state.\n\nCollins's political views manifested themselves in his novel \"Gents\" (1996) which has recently been republished by The Friday Project, and was reviewed as an all-time classic in the \"Times\" (8 September 2007). \"Gents\", which describes the lives of three West Indian immigrants who run a public urinal in London, is considered to be a leading fiction on tolerance. Collins claimed it was stimulated in part by his memories of apartheid when he lived as a child in South Africa.\n\nCollins's other fictions include the somewhat luridly entitled \"Fuckwoman\", a spoof on the superhero genre which details the adventures of a feminist vigilante who hunts down men who commit crimes against women. Set in Los Angeles, it also satirises the movie industry, contrasting Hollywood's emphasis on the image over reality. It has been published in French, German and Italian translations and recently in English as \"F-Woman\".\n\nHis last novel was \"The Sonnets\", a fictional account of William Shakespeare's life from 1592–4, when the London theatres were closed by threat of plague, during which time many scholars believe that the main body of Shakespeare's sonnets were written.\n\nWarwick Collins maintained an occasional blog at \"www.publicpoems.com\".\n\nFiction\n\n\nNon-fiction\n\n\n"}
{"id": "33787", "url": "https://en.wikipedia.org/wiki?curid=33787", "title": "Wrecking (shipwreck)", "text": "Wrecking (shipwreck)\n\nWrecking is the practice of taking valuables from a shipwreck which has foundered or run aground close to shore. Often an unregulated activity of opportunity in coastal communities, wrecking has been subjected to increasing regulation and evolved into what is now known as marine salvage. Wrecking is no longer economically significant; however, as recently as the 19th century in some parts of the world, it was the mainstay of otherwise economically marginal coastal communities.\n\nA traditional legend is of wreckers deliberately decoying ships on to coasts using tricks (in particular false lights), so that they run ashore and can be plundered. While this has been depicted in many stories and legends, it is uncertain that this has ever happened.\n\nThere are legends that some ships were deliberately lured into danger by a display of false lights. John Viele, retired U. S. Navy officer and author of a history of wrecking in the Florida Keys, states that such tricks simply would not work. He points out that mariners interpret a light as indicating land, and so avoid them if they cannot identify them. Moreover, oil lanterns cannot be seen very far over water at night, unless they are large, fitted with mirrors or lenses, and mounted at a great height (i.e., in a lighthouse). In hundreds of admiralty court cases heard in Key West, Florida, no captain of a wrecked ship ever charged that he had been led astray by a false light. A Bahamian wrecker, when asked if he and his crewmates made beacons on shore or showed their lights to warn ships away from the land at night, is reported to have said, \"No, no [laughing]; we always put them out for a better chance by night\".\n\nLegend maintains that the town of Nags Head, North Carolina takes its name from wreckers or \"Bankers\" deploying false lights. The Nags Head urban legend states that in the 18th century, wreckers would hang lanterns from the necks of mules (colloquially called \"nags\" at the time) and walk the animals very slowly up and down the beach. The alleged intent was to fool mariners into believing that the slow-moving lights were ships drifting at rest or at anchor, prompting the ships to change course and subsequently run aground. In 1860, a writer for \"Harper’s New Monthly Magazine\" corroborates the story of the \"Bankers\" who gave Nags Head its name.\n\nAs soon as the Spanish began sending home the treasures they found in the New World, some of the treasure was lost in shipwrecks. By the 1540s Indians along the coast of Florida, where many of the Spanish treasure ships wrecked, were diving on the wrecks and recovering significant amounts of gold and silver. By that time the Spanish had been using first Indians (the Lucayans from the Bahamas were particularly prized for the task) and then Africans to dive for pearls around the islands near present-day Venezuela. The Spanish began using these divers to recover treasure from shipwrecks. The Spanish kept salvage ships with crews of African divers on-call in major ports around the Caribbean, ready to sail as soon as word of a wreck was received. In the course of the 16th through the 18th centuries the Spanish recovered more than 100,000,000 pesos worth of treasure by such means. Spanish salvage efforts had varying success. Although the Spanish carried out salvage operations on the wrecks of the 1715 Treasure Fleet for four years, they recovered less than half of the treasure recorded as sent on the fleet. On the other hand, the Spanish recovered more treasure from the 1733 treasure fleet than had been officially registered on it.\n\nIn the 16th and 17th centuries Spanish ships returning to Spain from the Caribbean rode the Gulf Stream to Cape Canaveral and then aimed for Bermuda. Raising Bermuda was essential to Spanish ships for verifying their position before setting course for the Azores. As a result, some Spanish ships wrecked on Bermuda. After the English settled on Bermuda in the early 17th century, they quickly took up \"wracking\" on Bermuda, and then extended their search for wrecks to all of the Caribbean. Later in the 17th century the center for English \"wracking\" in the Caribbean shifted to Port Royal in Jamaica. William Phips went there to recruit the divers he used to salvage treasure from a Spanish wreck on the north shore of Hispaniola, where he recovered the largest amount of treasure from a single wreck before the 20th century.\nWrecking (or \"wracking\") was an important activity in the Bahamas from its first settlement in 1648. A company of religious dissidents from Bermuda, the Eleutheran Adventurers, established a colony on Eleutheria. Their governing document, the \"Articles and Orders\", included regulations of wrecking, providing that any salvaged ordnance would be held in common for the defense of the colony, and all other salvaged goods would be delivered to designated agents, \"made fit for sale\" and then sold, with one-third of the proceeds going to the wreckers.\n\nWhile the Eleutheran Adventurers were primarily farmers, seamen from Bermuda began settling on New Providence in the 1660s, attracted by ambergris, wrecks and salt. There were vessels dedicated to wrecking from this time, but wrecking was a secondary occupation for most men. These seamen, who called themselves \"wrackers\" or \"wreckers\", pursued wrecking aggressively, regarding all salvage as their property. They were rumored to have killed people who had inconveniently survived a shipwreck. They drove Spanish salvors away from Spanish wrecks, and even took goods that the Spanish had already salvaged. Spain regarded the Bahamian wreckers as pirates, and retaliated by attacking the wreckers' ships, kidnapping farmers from New Providence, and burning the capital, Charles Town.\n\nThe Bahamian government eventually exerted control over the wreckers. The wreckers were required to carry salvaged goods to Nassau, where they were auctioned. However, goods useful on a ship or in a wrecker's home were often diverted with a blind eye turned by government officials. Increased shipping after the end of the Napoleonic Wars in 1815 led to more wrecks. Vessels specifically designed for wrecking were built in the Bahamas. A U.S law of 1825 required that all goods salvaged from wrecks in U.S. waters be taken to an American port of entry (which, for the Bahamians, meant Key West, Florida). Many Bahamian wreckers eventually moved to Key West and became U.S. citizens.\n\nWrecking was a mainstay of the Bahamian economy through most of the 19th century. In 1856 there were 302 ships and 2,679 men (out of a total population of 27,000) licensed as wreckers in the Bahamas. In that year salvaged wreck cargo brought to Nassau was valued at £96,304, more than half of all imports to the Bahamas. More than two-thirds of exports from the Bahamas were salvaged goods. The government normally took 15% customs duty on salvaged goods. If the salvaged cargo was not claimed, the Vice Admiralty Court took 30%, and the Governor took another 10%. Shore workers (warehouse workers, agents and laborers) usually received around 14% of the value. The wreckers themselves usually received 40% to 60% of the value of the salvaged goods. Even so, the average annual income of an ordinary seaman on a wrecker was about ₤20.\n\nThe American Civil War sharply cut the volume of shipping around the Bahamas, and the wreckers suffered with far fewer wrecks to salvage. The end of the Civil War brought back increased shipping and wrecks. In 1865, the last year of the Civil War, £28,000 worth of salvaged goods were taken to Nassau. In 1866 that rose to £108,000, and peaked at £154,000 in 1870. Wrecking then entered a decline, and was nearly gone by the end of the 19th century. More lighthouses (eventually numbering 37 in the Bahamas), better charts, more ships powered by steam, better qualified ship's officers, and more seaworthy ships all contributed to fewer wrecks.\n\nFor several centuries wrecking was an important economic activity in the Florida Keys. During the 19th century wrecking in the Keys became a highly organized and regulated industry, with dozens of vessels and hundreds of men active in the trade at any given time. The Florida Keys form a long arc of islands extending from the southern end of the east coast of Florida to the Dry Tortugas. A line of shallow coral reefs, the Florida Reef, runs parallel to the Keys from east of Cape Florida to southwest of Key West, with dangerous shoals stretching west from Key West to the Dry Tortugas. This chain of reefs and shoals is approximately long, separated from the Keys by the narrow and relatively shallow Hawk Channel. The Gulf Stream passes close to the Florida Reef through the Straits of Florida, which is the major route for shipping between the eastern coast of the United States and ports in the Gulf of Mexico and the western Caribbean Sea. The combination of heavy shipping and a powerful current flowing close to dangerous reefs made the Florida Keys the site of a great many wrecks, especially during the 19th century. Ships were wrecking on the Florida Reef at the rate of almost once a week in the middle of the 19th century (the collector of customs in Key West reported a rate of 48 wrecks a year in 1848). For a period of almost 100 years, wrecking captains and wrecking vessels in the Keys had to hold a license issued by the Federal court. In 1858 there were 47 boats and ships licensed as wreckers.\n\nShips began wrecking along the Florida Reef almost as soon as Europeans reached the New World. From early in the 16th century, Spanish ships returning from the New World to Spain sailed from Havana to catch the Gulf Stream, which meant they passed close to the Florida Reef, with some wrecking. The first wreckers in the Keys were Indians; when Hernando de Escalante Fontaneda's ship was wrecked in 1549, he was taken prisoner by Indians who were experienced in plundering wrecked ships. In 1622, six ships of the Spanish treasure fleet wrecked during a hurricane in the lower Keys. Spanish operations to recover the gold and silver from the lost ships continued intermittently for 21 years, but the Spanish lost track of the \"Nuestra Señora de Atocha\", which was finally found and excavated in the 20th century. In 1733, 19 ships of the Spanish treasure fleet wrecked during a hurricane in the middle and upper keys, and salvage operations lasted four years. The Spanish used dragged chains, grapnels, free divers and even an early diving bell to find and recover goods from the wrecked ships.\n\nStarting in the 18th century ships from The Bahamas began frequenting the Florida Keys. The Bahamians were opportunists, fishing, turtling, logging tropical hardwoods on the Keys, and salvaging wrecks as the opportunity arose. When the Spanish were salvaging the wrecks of the 1733 treasure fleet, the Spanish commander of the operation expressed concern that the Bahamians would try to salvage some of the treasure on their own. By 1775, George Gauld, who produced a chart of the Keys that was still being used 75 years later, advised mariners to stay with their ships if they wrecked, so that the Bahamian wreckers could assist them. Although the Keys were at various times part of Spanish Florida, the British colony of East Florida and the U.S. Florida Territory, the Bahamians took goods salvaged from ships wrecked in the Keys to Nassau for adjudication, rather than to the Florida port of entry, St. Augustine. After the end of the Napoleonic Wars and the War of 1812 in 1815, increased shipping through the Straights of Florida resulted in an increase in wrecks on the Keys, and the Crown's share from the auction of salvaged goods became the major support of the economy of Nassau.\n\nAfter 1815, fishing boats from New England began visiting the Florida Keys in the winter to fish for the Havana market. These fishermen engaged in wrecking when the opportunity arose. With the acquisition of Florida by the United States in 1821 and the settlement of Key West in 1822, the New England fishermen-wreckers began moving their homes to Key West. Conflicts quickly developed with the Bahamian wreckers. U.S. Navy ships stopped and boarded Bahamian wreckers to check papers, and arrested two Bahamian captains on suspicion of smuggling slaves. American wreckers became increasingly hostile to Bahamian wreckers, and in 1825 the U.S. Congress passed a law requiring all goods salvaged in U.S. waters to be taken to an American port of entry. This measure created a great inconvenience for the Bahamian wreckers, as they had to take salvaged goods and ships to Key West before they could return home to the Bahamas. Some of them soon moved to Key West and acquired U.S. citizenship.\n\nKey West had become a port of entry in 1822. In the same year the U.S. Navy chose Key West as its base for suppressing piracy in the West Indies. The city quickly developed into Florida's most important port. By the 1830s Key West accounted for 60% to 90% of imports and exports for the Territory. Most of this traffic was due to the activities of the wreckers. Warehouses for storing salvaged goods, shipyards for repairing damaged ships that had been removed from the reefs and for building vessels to be used in wrecking, and ship chandlers for refitting ships all contributed to the city's prosperity.\n\nIn the 1820s and 1830s Indian Key functioned as a secondary center for the wrecking industry in the Keys. Closer to most of the reefs off the keys than Key West, Indian Key enjoyed a brief prosperity before being destroyed in a raid by Seminoles in 1840.\n\nWrecking in the Florida Keys was conducted from sailing vessels. Numerous vessels would patrol along the Florida Reef looking for wrecks. The wreckers would normally anchor at night in protected anchorages along the Keys, and then sail out in the morning to see if any ships had wrecked during the night. As a result, a ship that ran on the reef during the night might attract a dozen wreckers by the afternoon of the next day. The first wrecking captain to reach a stranded ship became the wreck master, determining how many wreckers he needed to help salvage the ship, and directing the operation. Wreckers had an obligation to save passengers and crew of the wrecked ship (for which they received no compensation), and to salvage as much of the cargo as possible, and the ship, as well. If the judge in Federal court decided that a wrecking crew had not done everything possible to salvage cargo and ship, he would reduce the award.\n\nThe salvaged cargo and the ship, if it could be saved, were taken to Key West where they were appraised or auctioned. The wrecking vessels and crews that participated in the operations would then be awarded a share of the salvage value. Half of the salvage award went to the owners of the wrecking vessels, divided among the boats on a tonnage basis. The other half went to the wrecker crews, proportional to the number of crewmen on each vessel. Ordinary crewmen received one share, \"boys\" a half-share, cooks, one-and-a-quarter shares, and captains one to three shares, depending on the size of the vessel. Divers, who dove into the flooded holds of ships to retrieve cargo, received extra shares. By the time a salvage award was divided this way, individual shares were often quite small. Contemporary observers estimated that wrecking crews on average made no more than an ordinary seaman.\n\nIn the first few years after Florida was acquired by the United States, salvage awards were determined either by prior agreement between the wreck master and the captain of the wrecked ship, or by arbitration. As the persons available to serve as arbitrators usually had ties to the wrecking industry, if not a direct business relationship with the wreck master and/or the owners of the wrecking vessels, the process was often abused, with awards as high as 90% of the salvaged value. In 1829 a United States District Court was established in Key West with admiralty jurisdiction, after which most salvage cases were decided in court. Court awards for a wrecking operation averaged about 25% of the salvage value. Private agreements and arbitration remained an option, however, particularly when the judge was not available. A visitor to Key West in the 1880s reported that the United States District Court was in session almost every week, and had heard more than 700 Admiralty law cases during the preceding year.\n\nWreckers were required by the Federal law to carry equipment that might be needed to save cargo and ships. Such equipment included heavy anchors for kedging (hauling) ships off reefs, heavy hawsers and chain, fenders and blocks and tackle. Wreckers also had to be prepared to make emergency repairs to ships to refloat them or keep them afloat while they were sailed or towed back to Key West. By the middle of the 19th century windmill-powered pumps, and later a steam-powered pump, were kept in Key West. If the wreckers were not able to pump out a ship fast enough to float it using the ship's own pumps, they could rent one of the large pumps from Key West. As the wrecking vessels could not always directly approach wrecked ships, they had to carry sturdy boats.\n\nCargoes saved by wreckers varied tremendously. Cotton was perhaps the most valuable bulk cargo. A bale of cotton might be worth US$50 or $60 in Charleston, South Carolina, but a bale pulled from a flooded hold would be saturated with water, and weigh as much as half a ton. Unusual cargoes salvaged by wreckers included the fossilized \"Hydrarchos\" skeleton collected in Alabama by Albert Koch, and a locomotive. In 1827 \"Guerrero\", a Spanish slave-runner carrying 500 African captives, and the Royal Navy warship HMS \"Nimble\" ran onto the Florida Reef during a running gun battle. Wreckers went to the aid of both ships. After most of the Africans and the Spanish crewmen had been transferred to wrecker vessels, the Spanish crewmen commandeered two of the ships and sailed to Cuba with most of the Africans. The remaining 120 Africans were taken to Key West, and then to St. Augustine. After Congress passed a special law the next year, 96 surviving Africans were sent to Liberia.\n\nIn an effort to reduce the number of wrecks along the Florida Reef, the United States government funded the construction of lighthouses. Lighthouses were built in the 1820s at Cape Florida, Key West (both on the island itself and on nearby Sand Key, and on Garden Key in the Dry Tortugas. A lightship was stationed at Carysfort Reef. Mariners complained that the lights were not visible enough. There were also long interruptions. The Cape Florida lighthouse was burned by Seminoles in 1836 and remained dark for ten years. It was also dark while the tower was made higher in 1855. It was put out of commission again in 1860 by Confederate sympathizers and remained dark until the end of the American Civil War in 1865. The Key West and Sand Key lighthouses were destroyed by a hurricane in 1846. A lightship was placed at Sand Key until the lighthouses could be rebuilt. Beginning in 1852 lighthouses were built directly on the Florida Reef, but it was 1880 before mariners could rely on having a lighthouse in sight at all times while sailing along the Florida Reef.\n\nThe wreckers were unhappy about the lights, expecting them to reduce the number of wrecks and their livelihood. Initially, however, the lights did not greatly reduce the number of wrecks. Some ships wrecked when their captains became confused about which lights they were seeing, mistaking lights on the Florida Reef for lights on the Bahama Banks. Some wrecks may have been deliberate, as well. On a few occasions wreckers trying to refloat flooded ships discovered that holes had been bored through the hull below the water line. The captain of a ship that had wrecked stated that the wreck was not to be greatly regretted, as there were too many ships in the freight business. Judge Marvin of the Federal court in Key West told a navy officer in 1860 there was \"a great deal of wrecking by design.\"\n\nShipping through the Straits of Florida, and therefore the number of wrecks on the Florida Reef, declined sharply during the Civil War. Following the Civil War, the number of wrecks did not increase as fast as the ship traffic through the Straits. More lighthouses were in place, better charts were available, and more ships were powered by steam and thus less vulnerable to being pushed onto reefs by unfavorable winds. Steam-powered vessels began to enter the wrecking trade. Eventually ocean-going tugboats took over what became known as marine salvage operations. By the end of the 19th century wrecks were infrequent. The last major wrecking operation was in 1905, when 77 small vessels and 500 men salvaged cargo from the steamer \"Alicia\". Salvage work was abandoned when divers refused to continue, as contaminated water in the hold was causing them to become blind for 24 hours after a dive. The salvage award was US$17,690. The last local wrecker was bought out by a New York company in 1920. The Federal court closed the book of wrecking licenses the next year.\n\nResidents of the isolated communities along the outer banks of North Carolina and the Islands of Hatteras, Ocracoke, and Portsmouth and in Newfoundland and the islands and coastline of the Gulf of Saint Lawrence and engaged in wrecking for many years. In Nova Scotia, Seal Island and especially Sable Island were known for wrecking. Reports of violent wreckers on Sable helped spur efforts by the colony of Nova Scotia to establish a rescue station in Sable Island in 1801.\n\nWrecking was well known in Devon and Cornwall where the rocky coastline, and strong prevailing onshore winds helped wreck many merchant ships and warships. It is rumoured that ships were sometimes deliberately attracted: false lights on the shore were said to be used sometimes to lead ships into disaster. However, there is no evidence to support these ideas.\n\nIn 1735 a law was passed to make it an offence to make false lights, but no one was prosecuted as a result. In 1769 William Pearse was hanged at Launceston in Cornwall for stealing from a wreck. It was not until after a case in the Court of Appeal in 1870 that rewards were made for rescuing people.\n\nWrecking was a major industry in the 19th century, and as far back as the 16th century, especially of ships returning from the New World using the Gulf Stream, which passes by the south west of England. This would help to speed these ships on their way to France and Spain and put them out of position. Wreckers would attempt to frighten off the curious, suspicious or unwanted visitors, by spreading wild rumours concerning supernatural activity, ghosts and cannibals (as occurred in Clovelly) near their wrecking sites.\n\nWrecking was a major activity of the inhabitants of Stroma Island in the Pentland Firth off the north of Scotland. It was also well known on the Goodwin Sands off the south east of England where over 2000 wrecks have occurred. The boatmen of Deal, who took supplies to ships at anchor off the coast, would plunder any wrecked vessel. Another area where wrecking was prevalent was the Wirral Peninsula, near Liverpool, where wrecking continued to be reported into the early twentieth century.\n\nA 2005 BBC documentary, \"Coast\", successfully replicated the conditions of false light wrecking in an experiment which suggested that a single-candle lantern onshore would be sufficient to lure a boat into dangerous water on a dark night. It should be noted that the boat crew did not see the light until they got within 150m of it.\n\nIn 2007 the container ship MSC \"Napoli\" went aground off Branscombe beach in Devon. Some of its cargo was washed ashore and many wreckers plundered the cargo in spite of attempts to prevent this. People came long distances to retrieve such things as BMW motorcycles. Goods from wrecks are supposed by law to be reported to the \"Receiver of Wreck\" and finders will then be given a reward.\n\nWrecking has been practised a long way back in Denmark. The long shorelines, the heavy international marine traffic in combination with some difficult and often harsh waters, has produced many wrecks here. It was only recently, that the light signalling was fully developed and deep sea-bed canals were dug in the often very shallow waters here, making seafaring relatively safe. Skagen's Vippefyr was constructed in 1627, along with several other rudimentary lighting signals in the following years, after complaints. Sea floor canals came centuries later. It was and still is common practice to hire skilled Danish pilots to help navigate in and out of the Baltic Sea.\n\nSo-called wreck masters used to be employed in the Danish coastal communities, in order to oversee, report on and collect valuables from new wrecks. A former wreck masters home can be experienced at the Wreck Master's Farm near Rubjerg Knude on the North Atlantic coast for example. The last wreck master here left in 1992. A few wreck masters are still appointed in Denmark, but nowadays the job also includes observations and reporting on oil spills, pollutions, vandalism, etc., and they work in close cooperation with the police.\n\nWreckers have been featured in a number of works of fiction, including a references in \"The Shipping News\" by E. Annie Proulx, \"Jamaica Inn\" by Daphne du Maurier, \"Shipwrecks\" by Akira Yoshimura, the film \"The Light at the Edge of the World\" based on the novel \"Le Phare du bout du monde\" by Jules Verne, and also in the opening chapter of Verne's \"The Archipelago on Fire\".\n\nThe plot of Compton Mackenzie's novel \"Whisky Galore\" revolves around the inhabitants of a small Scottish island recovering as-yet-untaxed whisky from a shipwreck and their subsequent efforts at evading government officials.\n\nDame Ethel Smyth's opera, \"The Wreckers\", is set in Cornwall—the plot revolves around deliberate wrecking.\n\nIn 1942, the Technicolor Movie \"Reap The Wild Wind \" by Cecil B. DeMille depicted life in the wrecking business in the Nineteenth Century around Key West, Florida. It garnered an Academy Award for underwater Special Effects.\n\nIn 1962, the \"Walt Disney's Wonderful World of Color\" TV series aired a two-episode live action adventure film entitled \"The Mooncussers\" about the investigation and exposure of a gang of wreckers.\n\nEnid Blyton, specifically in the \"Famous Five\" books, writes often about the treasures of the wreckers.\n\n\"The Wreckers\", by Iain Lawrence, is a book for younger readers about \"The Isle of Skye\" (a London vessel) being shipwrecked along the shores of Pendennis, Cornwall.\n\nCanadian progressive rock band Rush included a song on their 2012 album \"Clockwork Angels\" titled \"The Wreckers\", the lyrics of which were inspired by historical tales of wreckers luring ships to their demise.\n\n\"Crimson Shore\" is part of the Agent Pendergast series by Douglas Preston and Lincoln Child. The main story involves a ship, the \"Pembroke Castle\", being purposefully wrecked on the rocks off the coast of Massachusetts. \n\n\"Coot Club\", the fifth of Arthur Ransome's Swallows and Amazons series of books, features wreckers on the Norfolk Broads.\n\n\"The Wreck of the Zanzibar\" is a Whitbread Award-winning children's novel by Michael Morpurgo, set on the Isles of Scilly.\n\n\"H.M.S. Dolores\" is a 2016 board game whose plot centers around rival crews of wreckers.\n\n\n\n\n\n"}
{"id": "18820615", "url": "https://en.wikipedia.org/wiki?curid=18820615", "title": "Zulu golden mole", "text": "Zulu golden mole\n\nThe Zulu golden mole (\"Amblysomus hottentotus iris\") is a golden mole native to Transvaal, South Africa.\n"}
