{"id": "690512", "url": "https://en.wikipedia.org/wiki?curid=690512", "title": "Akaike information criterion", "text": "Akaike information criterion\n\nThe Akaike information criterion (AIC) is an estimator of the relative quality of statistical models for a given set of data. Given a collection of models for the data, AIC estimates the quality of each model, relative to each of the other models. Thus, AIC provides a means for model selection.\n\nAIC is founded on information theory. When a statistical model is used to represent the process that generated the data, the model will almost never be exact; so some information will be lost by using the model to represent the process. AIC estimates the relative information lost by a given model: the less information a model loses, the higher the quality of that model. (In making an estimate of the information lost, AIC deals with the trade-off between the goodness of fit of the model and the simplicity of the model.)\n\nThe Akaike information criterion is named after the statistician Hirotugu Akaike, who formulated it. It now forms the basis of a paradigm for the foundations of statistics; as well, it is widely used for statistical inference.\n\nSuppose that we have a statistical model of some data. Let be the number of estimated parameters in the model. Let formula_1 be the maximum value of the likelihood function for the model. Then the AIC value of the model is the following.\n\nGiven a set of candidate models for the data, the preferred model is the one with the minimum AIC value. Thus, AIC rewards goodness of fit (as assessed by the likelihood function), but it also includes a penalty that is an increasing function of the number of estimated parameters. The penalty discourages overfitting, because increasing the number of parameters in the model almost always improves the goodness of the fit.\n\nAIC is founded in information theory. Suppose that the data is generated by some unknown process \"f\". We consider two candidate models to represent \"f\": \"g\" and \"g\". If we knew \"f\", then we could find the information lost from using \"g\" to represent \"f\" by calculating the Kullback–Leibler divergence, ; similarly, the information lost from using \"g\" to represent \"f\" could be found by calculating . We would then choose the candidate model that minimized the information loss.\n\nWe cannot choose with certainty, because we do not know \"f\". showed, however, that we can estimate, via AIC, how much more (or less) information is lost by \"g\" than by \"g\". The estimate, though, is only valid asymptotically; if the number of data points is small, then some correction is often necessary (see AICc, below).\n\nNote that AIC does not provide a test of a model in the sense of testing a null hypothesis. It tells nothing about the absolute quality of a model, only the quality relative to other models. Thus, if all the candidate models fit poorly, AIC will not give any warning of that.\n\nTo apply AIC in practice, we start with a set of candidate models, and then find the models' corresponding AIC values. There will almost always be information lost due to using a candidate model to represent the \"true model\" (i.e. the process that generated the data). We wish to select, from among the candidate models, the model that minimizes the information loss. We cannot choose with certainty, but we can minimize the estimated information loss.\n\nSuppose that there are \"R\" candidate models. Denote the AIC values of those models by AIC, AIC, AIC, …, AIC. Let AIC be the minimum of those values. Then the quantity exp((AIC − AIC)/2) can be interpreted as being proportional to the probability that the \"i\"th model minimizes the (estimated) information loss.\n\nAs an example, suppose that there are three candidate models, whose AIC values are 100, 102, and 110. Then the second model is exp((100 − 102)/2) = 0.368 times as probable as the first model to minimize the information loss. Similarly, the third model is exp((100 − 110)/2) = 0.007 times as probable as the first model to minimize the information loss.\n\nIn this example, we would omit the third model from further consideration. We then have three options: (1) gather more data, in the hope that this will allow clearly distinguishing between the first two models; (2) simply conclude that the data is insufficient to support selecting one model from among the first two; (3) take a weighted average of the first two models, with weights proportional to 1 and 0.368, respectively, and then do statistical inference based on the weighted multimodel.\n\nThe quantity exp((AIC − AIC)/2) is known as the \"relative likelihood\" of model \"i\". It is closely related to the likelihood ratio used in the likelihood-ratio test. Indeed, if all the models in the candidate set have the same number of parameters, then using AIC might at first appear to be very similar to using the likelihood-ratio test. There are, however, important distinctions. In particular, the likelihood-ratio test is valid only for nested models, whereas AIC (and AICc) has no such restriction.\n\nWhen the sample size is small, there is a substantial probability that AIC will select models that have too many parameters, i.e. that AIC will overfit. To address such potential overfitting, AICc was developed: AICc is AIC with a correction for small sample sizes.\n\nThe formula for AICc depends upon the statistical model. Assuming that the model is univariate, is linear in its parameters, and has normally-distributed residuals (conditional upon regressors), then the formula for AICc is as follows.\n\n—where denotes the sample size and denotes the number of parameters. Thus, AICc is essentially AIC with an extra penalty term for the number of parameters. Note that as , the extra penalty term converges to 0, and thus AICc converges to AIC.\n\nIf the assumption that the model is univariate and linear with normal residuals does not hold, then the formula for AICc will generally be different from the formula above. For some models, the formula can be difficult to determine. For every model that has AICc available, though, the formula for AICc is given by AIC plus terms that includes both and . In comparison, the formula for AIC includes but not . In other words, AIC is a first-order estimate (of the information loss), whereas AICc is a second-order estimate.\n\nFurther discussion of the formula, with examples of other assumptions, is given by and by . In particular, with other assumptions, bootstrap estimation of the formula is often feasible.\n\nTo summarize, AICc has the advantage of tending to be more accurate than AIC (especially for small samples), but AICc also has the disadvantage of sometimes being much more difficult to compute than AIC. Note that if all the candidate models have the same and the same formula for AICc, then AICc and AIC will give identical (relative) valuations; hence, there will be no disadvantage in using AIC, instead of AICc. Furthermore, if is many times larger than , then the extra penalty term will be negligible; hence, the disadvantage in using AIC, instead of AICc, will be negligible.\n\nThe Akaike information criterion was formulated by the statistician Hirotugu Akaike; it was originally named \"an information criterion\". It was first announced by Akaike at a 1971 symposium, the proceedings of which were published in 1973. The 1973 publication, though, was only an informal presentation of the concepts. The first formal publication was a 1974 paper by Akaike. , the 1974 paper had received more than 14000 citations in the Web of Science: making it the 73rd most-cited research paper of all time.\n\nNowadays, AIC has become common enough that it is often used without citing Akaike's 1974 paper. Indeed, there are over 150,000 scholarly articles/books that use AIC (as assessed by Google Scholar).\n\nThe initial derivation of AIC relied upon some strong assumptions. showed that the assumptions could be made much weaker. Takeuchi's work, however, was in Japanese and was not widely known outside Japan for many years.\n\nAICc was originally proposed for linear regression (only) by . That instigated the work of , and several further papers by the same authors, which extended the situations in which AICc could be applied.\n\nThe first general exposition of the information-theoretic approach was the volume by . It includes an English presentation of the work of Takeuchi. The volume led to far greater use of AIC, and it now has more than 42000 citations on Google Scholar.\n\nAkaike called his approach an \"entropy maximization principle\", because the approach is founded on the concept of entropy in information theory. Indeed, minimizing AIC in a statistical model is effectively equivalent to maximizing entropy in a thermodynamic system; in other words, the information-theoretic approach in statistics is essentially applying the Second Law of Thermodynamics. As such, AIC has roots in the work of Ludwig Boltzmann on entropy. For more on these issues, see and .\n\nA statistical model must fit all the data points. Thus, a straight line, on its own, is not a model of the data, unless all the data points lie exactly on the line. We can, however, choose a model that is \"a straight line plus noise\"; such a model might be formally described thus:\n\"y\" = \"b\" + \"b\"\"x\" + ε. Here, the ε are the residuals from the straight line fit. If the ε are assumed to be i.i.d. Gaussian (with zero mean), then the model has three parameters:\n\"b\", \"b\", and the variance of the Gaussian distributions.\nThus, when calculating the AIC value of this model, we should use \"k\"=3. More generally, for any least squares model with i.i.d. Gaussian residuals, the variance of the residuals' distributions should be counted as one of the parameters.\n\nAs another example, consider a first-order autoregressive model, defined by\n\"x\" = \"c\" + \"φx\" + ε, with the ε being i.i.d. Gaussian (with zero mean). For this model, there are three parameters: \"c\", \"φ\", and the variance of the ε. More generally, a \"p\"th-order autoregressive model has \"p\" + 2 parameters. (If, however, \"c\" is not estimated from the data, but instead given in advance, then there are only \"p\" + 1 parameters.)\n\nThe AIC values of the candidate models must all be computed with the same data set. Sometimes, though, we might want to compare a model of the response variable, , with a model of the logarithm of the response variable, . More generally, we might want to compare a model of the data with a model of transformed data. Following is an illustration of how to deal with data transforms (adapted from : \"Investigators should be sure that all hypotheses are modeled using the same response variable\").\n\nSuppose that we want to compare two models: one with a normal distribution of and one with a normal distribution of . We should \"not\" directly compare the AIC values of the two models. Instead, we should transform the normal cumulative distribution function to first take the logarithm of . To do that, we need to perform the relevant integration by substitution: thus, we need to multiply by the derivative of the (natural) logarithm function, which is . Hence, the transformed distribution has the following probability density function:\n—which is the probability density function for the log-normal distribution. We then compare the AIC value of the normal model against the AIC value of the log-normal model.\n\nSome statistical software will report the value of AIC or the maximum value of the log-likelihood function, but the reported values are not always correct.\nTypically, any incorrectness is due to a constant in the log-likelihood function being omitted. For example,\nthe log-likelihood function for independent identical normal distributions is\n—this is the function that is maximized, when obtaining the value of AIC. Some software, however, omits the constant term , and so reports erroneous values for the log-likelihood maximum—and thus for AIC. Such errors do not matter for AIC-based comparisons, \"if\" all the models have their residuals as normally-distributed: because then the errors cancel out. In general, however, the constant term needs to be included in the log-likelihood function. Hence, before using software to calculate AIC, it is generally good practice to run some simple tests on the software, to ensure that the function values are correct.\n\nThe formula for the Bayesian information criterion (BIC) is similar to the formula for AIC, but with a different penalty for the number of parameters. With AIC the penalty is , whereas with BIC the penalty is .\n\nA comparison of AIC/AICc and BIC is given by , with follow-up remarks by . The authors show that AIC/AICc can be derived in the same Bayesian framework as BIC, just by using different prior probabilities. In the Bayesian derivation of BIC, though, each candidate model has a prior probability of 1/\"R\" (where \"R\" is the number of candidate models); such a derivation is \"not sensible\", because the prior should be a decreasing function of . Additionally, the authors present a few simulation studies that suggest AICc tends to have practical/performance advantages over BIC.\n\nA point made by several researchers is that AIC and BIC are appropriate for different tasks. In particular, BIC is argued to be appropriate for selecting the \"true model\" (i.e. the process that generated the data) from the set of candidate models, whereas AIC is not appropriate. To be specific, if the \"true model\" is in the set of candidates, then BIC will select the \"true model\" with probability 1, as ; in contrast, when selection is done via AIC, the probability can be less than 1. Proponents of AIC argue that this issue is negligible, because the \"true model\" is virtually never in the candidate set. Indeed, it is a common aphorism in statistics that \"all models are wrong\"; hence the \"true model\" (i.e. reality) cannot be in the candidate set.\n\nAnother comparison of AIC and BIC is given by . Vrieze presents a simulation study—which allows the \"true model\" to be in the candidate set (unlike with virtually all real data). The simulation study demonstrates, in particular, that AIC sometimes selects a much better model than BIC even when the \"true model\" is in the candidate set. The reason is that, for finite , BIC can have a substantial risk of selecting a very bad model from the candidate set. This reason can arise even when is much larger than . With AIC, the risk of selecting a very bad model is minimized.\n\nIf the \"true model\" is not in the candidate set, then the most that we can hope to do is select the model that best approximates the \"true model\". AIC is appropriate for finding the best approximating model, under certain assumptions. (Those assumptions include, in particular, that the approximating is done with regard to information loss.)\n\nComparison of AIC and BIC in the context of regression is given by . In regression, AIC is asymptotically optimal for selecting the model with the least mean squared error, under the assumption that the \"true model\" is not in the candidate set. BIC is not asymptotically optimal under the assumption. Yang additionally shows that the rate at which AIC converges to the optimum is, in a certain sense, the best possible.\n\nLeave-one-out cross-validation is asymptotically equivalent to AIC, for ordinary linear regression models. Asymptotic equivalence to AIC also holds for mixed-effects models.\n\nSometimes, each candidate model assumes that the residuals are distributed according to independent identical normal distributions (with zero mean). That gives rise to least squares model fitting.\n\nWith least squares fitting, the maximum likelihood estimate for the variance of a model's residuals distributions is formula_6, where formula_7 is the residual sum of squares: formula_8. Then, the maximum value of a model's log-likelihood function is \n—where is a constant independent of the model, and dependent only on the particular data points, i.e. it does not change if the data does not change.\n\nThat gives AIC = . Because only differences in AIC are meaningful, the constant can be ignored, which allows us to conveniently take AIC = for model comparisons. Note that if all the models have the same , then selecting the model with minimum AIC is equivalent to selecting the model with minimum —which is the usual objective of model selection based on least squares.\n\nMallows's \"C\" is equivalent to AIC in the case of (Gaussian) linear regression.\n\n\n"}
{"id": "20602246", "url": "https://en.wikipedia.org/wiki?curid=20602246", "title": "Atmatusti", "text": "Atmatusti\n\nAtmatusti is translated into English as being “what is pleasing to one's self” – it is important to recognize \"what is pleasing to one's self, oneself is not equal one's self\", for without knowing one's self one cannot know what is pleasing to self. For instance, editing this Wikipedia line is pleasing to me, I'm self, re-editing it and leaving it to the prior state is also pleasing to me, thus only one who knows one's self must edit, all others are not following Atmatusti, thus Atmatusti is not meant for those who do not know one's self, taking oneself to be one's self is the root of all misery. Other scholars have also used different terms to describe atmatusti. For example, Derrett’s translation from French to English of Lingat’s ‘‘The Classical Law of India’’ has coined the term “inner contentment” in reference to atmatusti. Lingat states that inner contentment can be best understood as \"the approval of one's conscious\". Only Manu and Yājñavalkya refer to atmatusti as the fourth source of dharma within the Hindu Law tradition. Scholars reject atmatusti as a fourth source of dharma because of this. Textual accounts of Manu's and Yajnavalkya's placement of atmatusti as a fourth source of dharma can be found in The Law Code of Manu 2.6 and The Law Code of Yajnavalkya 1.7. Also, atmatusti does not share the same authority as sruti, smriti, and acara. Atmatusti differs significantly from the other three sources of dharma in that it is not based on an \"authority exterior to man\"; in other words, an individual is able to create their own authority for any issue not covered under sruti, smriti, and acara. The first three sources of law are rooted in the vedas whereas atmatusti is not. It is because of this that atmatusti, as a fourth source, is not recognized by most scholars due to the lack of legitimacy.\n\n\nThere are only two instances where Atmatusti is designated as a fourth source of dharma within the dharmasastras.\n\nThe first instance comes from The Law Code of Manu or Manava Dharmasastra (MDh). The Laws of Manu are commentaries on the dharmasastras by a sage named Manu and therefore is considered apart of smriti. Because of this, The Law Code of Manu has a great amount of authority. However, Manu’s view of Atmatusti as a fourth source of dharma seemingly was not shared universally with other sages; except a sage named Yajnavalkya. This is understood through the lack of other smriti texts or dharmasastra commentaries in which Atmatusti is designated as a fourth source. Manu lists atmatusti along with sruti, smriti, and acara as being “the four visible marks of the law”. Manu's use of the phrase \"the four visible marks of law\", however, is not explained in relation to atmatusti legitimately being a fourth source of dharma. The significance here is therefore based interpretation. The first textual account of Atmatusti being listed as a fourth source of dharma is as follows:\n\nThe second instance comes from The Law Code of Yajnavalkya (YDh). Here Yajnavalkya, in addition to listing Atmatusti as a fourth source, also lists a fifth source of dharma: \"the desire born of proper intention\". However, Yajnavalkya is the only person to list this fifth source; therefore, its recognition among scholars is almost non-existent. The lack of support within the dharmasastras as a whole shows that the Hindu community, for the most part, did not find Yajnavalkya's fifth source of dharma correctly listed or legitimate. The second textual account of Atmatusti being listed as a fourth source of dharma along with the listing of a fifth source is as follows:\n\nThe appointment of atmatusti as a fourth source can be understood by looking at the hierarchy of the sources. The authority of each source is outlined along with the authoritative relationship the sources have on one another. Sruti, the first source, is superior and has more authority than Smriti and Acara. Smriti, the second source, in turn has authority over Acara, the third source. This is understandable in that the vedic texts are superior to tradition. Hence, it is only when the vedic texts do not provide the necessary dharma needed; will tradition or the secondary vedic texts be sought out. And, only when tradition does not provide the dharma on a specific topic should customary laws be looked into. Therefore, Manu appoints atmatusti as a last resort and fourth source of dharma for instances where the vedas, tradition, and customs all do not provide the necessary dharma or law.\n\nAtmatusti creates a guide in which a person does not obey laws because it is what they were told to do, but because the person has reverence for the law. It can be used to do what is a right in the context of legal manners, but it is generally not viewed as a moral compass that believers should follow. However, the legal aims of atmatusti are usually consistent with the Veda, seeing as the person's inner-self is partly built on by the individual's education. Therefore, the atmatusti is closely connected with a more highly acknowledged source of dharma.\n\n“When the Mimamsa method came to be applied to the texts of the smrti it left very little room for atmatusti. ([Lingat] 1973:6)”.\n\n"}
{"id": "634877", "url": "https://en.wikipedia.org/wiki?curid=634877", "title": "Bocca della Verità", "text": "Bocca della Verità\n\nThe Mouth of Truth ( ) is a marble mask in Rome, Italy, which stands against the left wall of the portico of the Santa Maria in Cosmedin church, at the Piazza della Bocca della Verità, the site of the ancient Forum Boarium (the ancient cattle market). It attracts visitors who audaciously stick their hand in the mouth.\n\nThe massive marble mask weighs about 1300 kg and probably depicts the face of the sea god Oceanus. The eyes, nostrils and mouth are open. Historians aren't quite certain what the original purpose of the disc was. It was possibly used as a drain cover in the nearby Temple of Hercules Victor, which had an oculus—a round open space in the middle of the roof, similar to that of the Pantheon. Hence, it could rain inside. It is also thought that cattle merchants used it to drain the blood of cattle sacrificed to the god Hercules. In the thirteenth century the disc was probably removed from the temple and placed against the wall of the Santa Maria in Cosmedin. In the seventeenth century it eventually moved to its current location inside the portico of the church.\n\nThe Mouth of Truth is now known mostly from its appearance in the 1953 film \"Roman Holiday\". The film also uses the Mouth of Truth as a storytelling device since both Hepburn's and Peck's characters are not initially truthful with each other.\nIn \"Het geheim van de afgebeten vingers\" by Dutch writer Rindert Kromhout, the fingers of lying children are cut off by a skeleton with a scythe who lives in the Capuchin Crypt in the Santa Maria della Concezione dei Cappuccini.\n\nThere are a number of \"Bocca della Verità\" replicas and derivative works. A full-size reproduction sits in the Alta Vista Gardens in California and one of Jules Blanchard's sculptures in the Luxembourg Garden in Paris depicts a woman with her hand in the sculpture's mouth. Coin-operated fortune teller machines have been developed and installed in different parts of the world, including one on display in the Musee Mecanique.\n\nThe Mouth of Truth has also been featured as a theme in historical European art. Lucas Cranach the Elder, a German painter during the Renaissance period, created two paintings depicting a woman placing her hand in the mouth of a statue of a lion while onlookers watched, a subject which was drawn by Albrecht Altdorfer and made into a woodcut by the Dutch printmaker Lucas van Leyden.\n\nThe Mouth of Truth is referenced in one chapter of Devilman Lady, by a Devil Beast known as Mita, who utters its Italian name alongside an illustration of the Mouth, before transforming into a spider-like creature with a mouth full of fangs on its back, which rapidly descends and bites off the top of the head of its victims before they can react.\n\nThe Mouth of Truth is depicted as a teacher in the japanese manga Shishunki Renaissance David-kun, the teacher uses his mouth to verify if students are telling the truth or not.\n\n\n<br>\n"}
{"id": "826956", "url": "https://en.wikipedia.org/wiki?curid=826956", "title": "Clausius–Mossotti relation", "text": "Clausius–Mossotti relation\n\nThe Clausius–Mossotti relation expresses the dielectric constant (relative permittivity, ε) of a material in terms of the atomic polarizibility, α, of the material's constituent atoms and/or molecules, or a homogeneous mixture thereof. It is named after Ottaviano-Fabrizio Mossotti and Rudolf Clausius. It is equivalent to the Lorentz–Lorenz equation. It may be expressed as:\n\nformula_1\n\nwhere\n\nIn the case that the material consists of a mixture of two or more species, the right hand side of the above equation would consist of the sum of the molecular polarizability contribution from each species, indexed by \"i\" in the following form: (see Lorrain and Corson - Electromagnetic Field and Waves, 1962, 2nd Edition, page 116)\n\nformula_6\n\nIn the CGS system of units the Clausius–Mossotti relation is typically rewritten to show the molecular polarizability \"volume\" formula_7 which has units of volume (m). Confusion may arise from the practice of using the shorter name \"molecular polarizability\" for both formula_5 and formula_9 within literature intended for the respective unit system.\n\nThe Lorentz–Lorenz equation is similar to the Clausius–Mossotti relation, except that it relates the refractive index (rather than the dielectric constant) of a substance to its polarizability. The Lorentz–Lorenz equation is named after the Danish mathematician and scientist Ludvig Lorenz, who published it in 1869, and the Dutch physicist Hendrik Lorentz, who discovered it independently in 1878.\n\nThe most general form of the Lorentz–Lorenz equation is\nwhere formula_11 is the refractive index, formula_4 is the number of molecules per unit volume, and formula_13 is the mean polarizability. \nThis equation is approximately valid for homogeneous solids as well as liquids and gases.\n\nWhen the square of the refractive index is formula_14, as it is for many gases, the equation reduces to:\nor simply\n\nThis applies to gases at ordinary pressures. The refractive index formula_11 of the gas can then be expressed in terms of the molar refractivity formula_18 as:\nwhere formula_20 is the pressure of the gas, formula_21 is the universal gas constant, and formula_22 is the (absolute) temperature, which together determine the number density formula_4.\n\n"}
{"id": "3197311", "url": "https://en.wikipedia.org/wiki?curid=3197311", "title": "David Held", "text": "David Held\n\nDavid Held (born 1951) is a British political scientist specialising in political theory and international relations. He currently holds a joint appointment as Professor of Politics and International Relations, and Master of University College, at Durham University. He is also a visiting Professor of Political Science at Libera Università Internazionale degli Studi Sociali Guido Carli. Previously he was the Graham Wallas chair of Political Science and the co-director of the Centre for the Study of Global Governance at the London School of Economics.\n\nTogether with Daniele Archibugi, Held has been prominent in the development of cosmopolitanism, and of cosmopolitan democracy in particular. He has been an active scholar on issues of globalisation, global governance and is joint editor-in-chief of the academic journal \"Global Policy\".\n\nDavid Held was born in Britain where he spent most of his childhood. He was educated in Britain, France, Germany and the United States. Upon completing his doctoral studies at the Massachusetts Institute of Technology he conducted post-doctoral research at Cambridge University. He has held numerous Visiting Appointments in the United States, Australia, Canada, Spain and Italy, among other places.\n\nDavid Held co-founded Polity press in 1984, which has become a leading publisher in the social sciences and humanities across the world. He is also the General Editor for \"Global Policy\", an academic journal started in 2008 that focuses on bridging the gap between academics and practitioners on issues of global significance.\n\nIn January 2012, he succeeded Professor Maurice Tucker as Master of University College, Durham, alongside his chair in the School of Government and International Affairs at Durham University.\n\nSince his first book was published in 1980 (\"Introduction to Critical Theory\"), David Held has been pursuing a multilevel inquiry into the nature and changing form of national and international politics. This approach has involved three kinds of work. First, it has involved extensive empirical enquiry into the dynamic character, structural elements and governance failures of contemporary society. The empirical dimensions of his work have included books such as \"Global Transformations\" (1999), \"Globalization/Anti-globalization\" (2007), \"Global Inequality\" (2007) and \"Gridlock: Why Global Cooperation is Failing When We Need it Most\" (2013). These books map the changing global context of politics, how the world has become increasingly interconnected, and how failures of leadership and negotiation at the global level are creating a breakdown of multilateralism and global governance.\n\nSecond, he has been investigating the changing nature and form of the modern state and the locus of the political good. Held examines the question of whether the nation state alone, as typically assumed by political theory, can be the sole home of democracy, accountability and the rule of law. This has entailed a critical evaluation of the concepts of democracy, sovereignty, governance and cosmopolitanism, among other concepts. Books that have explored these themes include: \"Democracy in the Global Order\" (1999), \"Models of Democracy\" (2006), \"Cosmopolitanism: ideals and realities\" (2010). \n\nThe third element of Prof. Held’s current work is to explore how and in what ways one can move beyond the crises and dilemmas of politics and governance in the contemporary world. Books such as \"Global Covenant\" (2004), \"Debating Globalization\" (2005) and a wide range of academic articles set out the contours of a multiactor, multilevel democratic politics framed by the fundamental principles of democracy, justice and sustainability.\n\nAccordingly, Held’s work explores, on the one hand, the shift in politics from nation states to what he calls a world of ‘overlapping communities of fate' (where the fortunes of countries are increasingly enmeshed) and, on the other hand, how democratic standards and cosmopolitan values can be entrenched in the global order. In pursuing this multilevel approach Prof. Held sees himself working within the classic tradition of political theory which has always been concerned with how to characterize the world in which we live, how to develop and reach normative goals such as liberty, democracy and social justice, and how to move from where we are to where we might like to be. Prof. Held’s response to this challenge is to explore the way globalization has altered the landscape of politics, how cosmopolitanism provides ideals that enable one to rethink politics and the political good, and to pursue political stepping stones that could help embed this agenda. He offers a contribution to a pressing dialogue of our times: how to resolve collective action problems, nationally and globally, through institutions and governance arrangements that enhance democracy, social justice and the participation of all citizens in a democratic public life.\n\nIn March, 2011, Held's name came to be linked with the LSE Libya Links controversy. Held was an advisor of Saif al-Islam Gaddafi, son of Libyan leader Muammar Gaddafi, who received his PhD from LSE in 2008.\n\n\n\n"}
{"id": "21665752", "url": "https://en.wikipedia.org/wiki?curid=21665752", "title": "Derby's dose", "text": "Derby's dose\n\nDerby's dose was a form of torture used in Jamaica to punish slaves who attempted to escape or committed other offenses like stealing food. According to Malcolm Gladwell in his 2008 book \"Outliers\", \"The runaway would be beaten, and salt pickle, lime juice, and bird pepper would be rubbed into his or her open wounds. Another slave would defecate into the mouth of the miscreant, who would then be gagged for four to five hours.\" The punishment was invented by Thomas Thistlewood, a slave overseer and named for the slave, Derby, who was made to undergo this punishment when he was caught eating young sugar cane stalks in the field on 25 May 1756. \n\nThistlewood recorded this punishment as well as a further punishment of Derby in August of that same year in his diary. \n\nOn 18 November 2013 British television host Martin Bashir discredited a comparison made by U.S. politician Sarah Palin between the United States' debt to China and slavery by referring to Derby's dose. In pointing out how cruel and barbaric slavery was, Bashir used Derby's dose as an example; at the end of the segment, he finished by saying that \"if anyone truly qualified for a dose of discipline from Thomas Thistlewood, [Palin] would be the outstanding candidate\". He was criticized for this comment, and ultimately resigned.\n"}
{"id": "3367515", "url": "https://en.wikipedia.org/wiki?curid=3367515", "title": "Dowry death", "text": "Dowry death\n\nDowry deaths are deaths of women who are murdered or driven to suicide by continuous harassment and torture by husbands and in-laws in an effort to extort an increased dowry.\n\nDowry deaths are found predominantly in India, Pakistan, Bangladesh, and Iran. India reports the highest total number of dowry deaths with 8,391 such deaths reported in 2010, 1.4 deaths per 100,000 women. Adjusted for population, Pakistan, with 2,000 reported such deaths per year, has the highest rate of dowry death at 2.45 per 100,000 women.\n\nDowry death is considered one of the many categories of violence against women, alongside rape, bride burning, eve teasing, and acid throwing.\n\nMost dowry deaths occur when the young woman, unable to bear the harassment and torture, commits suicide. Most of these suicides are by hanging, poisoning or by fire. Sometimes the woman is killed by setting her on fire by her husband or inlaws ; this is known as \"bride burning\", and sometimes disguised as suicide or accident. Death by burning of Indian women have been more frequently attributed to dowry conflicts. In dowry deaths, the groom's family is the perpetrator of murder or suicide.\n\nIndia has by far the highest number of dowry related deaths in the world according to Indian National Crime Record Bureau. In 2012, 18,233 dowry death cases were reported across India. This means a bride was burned every 90 minutes, or dowry issues cause 1.4 deaths per year per 100,000 women in India.Crime statistics in India , Government of India (2011)\n\nAccording to a 1996 report by Indian police, every year it receives over 2,500 reports of bride-burning. The Indian National Crime Records Bureau (NCRB) reports that there were about 8331 dowry death cases registered in India in 2011. Incidents of dowry deaths during the year 2008 (8172) have increased by 14.4 per cent over 1998 level (7146), while India's population grew at 17.6% over the 10-year period. The accuracy of these figures have received a great deal of scrutiny from critics who believe dowry deaths are consistently under-reported.\n\nDowry deaths in India is not limited to any specific religion. The ratio of dowry deaths are about the same as the ratio of population in India by religions.\n\nThe \"Dowry Prohibition Act\" of 1961 prohibits the request, payment or acceptance of a dowry, \"as consideration for the marriage\", where \"dowry\" is defined as a gift demanded or given as a precondition for a marriage. Gifts given without a precondition are not considered dowry, and are legal. Asking or giving of dowry can be punished by an imprisonment of up to six months, or a fine of up to . It replaced several pieces of anti-dowry legislation that had been enacted by various Indian states. Murder and suicide under compulsion are addressed by India's criminal penal code.\n\nIndian women's rights activists campaigned for more than 40 years for laws to contain dowry deaths, such as the \"Dowry Prohibition Act\" 1961 and the more stringent Section 498a of Indian Penal Code (enacted in 1983). Under the \"Protection of Women from Domestic Violence Act 2005\" (PWDVA), a woman can put a stop to the dowry harassment by approaching a domestic violence protection officer.\n\nAlthough Indian laws against dowries have been in effect for decades, they have been largely criticised as being ineffective. The practice of dowry deaths and murders continues to take place unchecked in many parts of India and this has further added to the concerns of enforcement.\n\nIn Pakistan, the giving and expectation of a dowry (called \"Jahez\") is part of the culture, with over 95% of marriages in every region of Pakistan involving transfer of a dowry from the bride's family to a groom's family.\n\nDowry deaths have been rising in Pakistan for decades. Dowry-related violence and deaths have been widespread for many decades. At over 2000 dowry-related deaths per year, and annual rates exceeding 2.45 deaths per 100,000 women from dowry-related violence, Pakistan has the highest reported number of dowry death rates per 100,000 women in the world.\n\nThere is some controversy on the dowry death rates in Pakistan. Some publications suggest Pakistan officials do not record dowry deaths, the death rates are culturally under-reported and may be significantly higher. For example, Nasrullah reports total average annual stove burn rates of 33 per 100,000 women in Pakistan, of which 49% were intentional, or an average annual rate of about 16 per 100,000 women.\n\nPakistan's \"Dowry and Marriage Gifts (Restriction) Bill\", 2008, restricts dowry to PKR 30,000 (~US$300) while the total value of bridal gifts is limited to PKR 50,000. The law made demands for a dowry by the groom's family illegal, as well as public display of dowry before or during the wedding. However, this and similar anti-dowry laws of 1967, 1976 and 1998, as well as \"Family Court Act\" of 1964 have proven to be unenforceable. Activists such as SACHET, Pakistan claim the police refuse to register and prosecute allegations of dowry-related domestic violence and fatal injuries.\n\nVarious military and democratically elected civil governments in Pakistan have tried to outlaw traditional display of dowry and expensive parties (\"walima\"). One such attempt was the Act of 1997, Ordinance (XV) of 1998 and Ordinance (III) of 1999. These were challenged in the Supreme Court of Pakistan. The petitioner cited a number of hadiths under religious Sharia laws to demonstrate that Islam encouraged \"walima\" and related customary practices. The petitioner claimed that the Pakistan government's effort to enact these laws are against the injunctions of Islam. The Supreme Court ruled these laws and ordinances unconstitutional.\n\nIn Bangladesh, dowry is called joutuk, and a significant cause of deaths as well. Between 0.6 and 2.8 brides per year per 100,000 women are reported to die because of dowry-related violence in recent years. The methods of death include suicides, fire and other forms of domestic violence. In 2013, Bangladesh reported 4,470 women were victims of dowry-related violence over a 10-month period, or dowry violence victimized about 7.2 brides per year per 100,000 women in Bangladesh.\n\nDowry is an ancient custom of Persia, and locally called \"jahâz\" (sometimes spelled \"jahiziyeh\"). Dowry-related violence and deaths in Iran are reported in Iranian newspapers, some of which appear in English media. Kiani et al., in a 2014 study, report dowry deaths in Iran.\n\nReports of incidents of dowry deaths have attracted public interest and sparked a global activist movement seeking to end the practice. Of this activist community, the United Nations (UN) has played a pivotal role in combating violence against women, including dowry deaths.\n\nThe United Nations has been an advocate for women's rights since its inception in 1945, explicitly stating so in its Charter's Preamble, the Universal Declaration of Human Rights (adopted in 1948), the International Covenant on Civil and Political Rights (adopted in 1966), the International Covenant on Economic, Social and Cultural Rights (also adopted in 1966) (these three documents are known collectively as the International Bill of Rights) and the Convention on the Elimination of All Forms of Discrimination Against Women (CEDAW) (2012).\n\nThe United Nations Children's Fund (UNICEF), though predominately focused on improving the quality of education available to children globally, has also taken a proactive stance against dowry death. On March 9 (International Women's Day), 2009, at a press conference in Washington D.C., UNICEF's Executive Director, Ann M. Veneman, publicly condemned dowry deaths and the legislative systems which allow the culprits to go unpunished. In 2009, UNICEF launched its first Strategic Priority Action Plan for Gender Equality, which was followed by a second Action Plan in 2010. The aim of these plans has been to make gender equality a higher priority within all international UNICEF programs and functions.\n\nAmnesty International, in an effort to educate the public, has cited dowry deaths as a major contributor to global violence against women. Also, in their annual human rights evaluations, Amnesty International criticizes India for the occurrences of dowry deaths as well as the impunity provided to its perpetrators.\n\nHuman Rights Watch has also criticized the Indian government for its inability to make any progress towards eliminating dowry deaths and its lackluster performance for bringing its perpetrators to justice in 2011. In 2004, the Global Fund for Women launched its \"Now or Never\" funding project. This campaign hopes to raise funds domestically and consequently finance the efforts of feminist organizations across the globe - including Indian women's rights activists. the Now or Never fund has raised and distributed about $7 million.\n\nA relatively smaller organization, V-Day, has dedicated itself to ending violence against women. By arranging events such as plays, art shows, and workshops in communities and college campuses across the United States, V-Day raises funds and educates the public on topics of gender-based violence including dowry death. Full-length plays on dowry deaths include 'The Bride Who Would Not Burn'\n\n\n\n"}
{"id": "1073078", "url": "https://en.wikipedia.org/wiki?curid=1073078", "title": "Felix Adler (professor)", "text": "Felix Adler (professor)\n\nFelix Adler (August 13, 1851 – April 24, 1933) was a German American professor of political and social ethics, rationalist, influential lecturer on euthanasia, religious leader and social reformer who founded the Ethical Culture movement.\n\nFelix Adler was born in Alzey, Rhenish Hesse, Grand Duchy of Hesse, Germany, the son of a rabbi, Samuel Adler, a leading figure in European Reform Judaism. The family immigrated to the United States from Germany when Felix was six years old so that his father could accept the appointment as head rabbi at Temple Emanu-El in New York.\n\nAdler attended Columbia Grammar School and graduated from Columbia University in 1870 with honors. He continued at Heidelberg University where he studied as part of his training to become a rabbi. He received a PhD from Heidelberg in 1873. While in Germany, he was strongly influenced by neo-Kantianism, especially the notions that one cannot prove or disprove the existence of a deity or immortality, and that morality can be established independently of theology.\n\nWhen Adler returned to New York at the age of twenty-three, he was asked to give a sermon at Temple Emanu-El, where he was meant to follow in his father's footsteps as rabbi of the congregation. His sermon, \"The Judaism of the Future\", shocked the congregation, as he did not once mention God. Adler introduced his concept of Judaism as a universal religion of morality for all of mankind. The sermon was his first and last at Temple Emanu-El.\n\nIn 1874, after it had become clear that he would not become a rabbi, members of his father's congregation helped Adler gain a teaching position at Cornell University as a nonresident Professor of Hebrew and Oriental literature. He was popular with his students, with whom he discussed his novel religious ideas while illuminating contemporary labor struggles and power politics. He was attacked as an atheist for his views, and in 1876 Cornell declined to accept the grant that had paid Adler's salary. In 1902 Adler was given the chair of political and social ethics at Columbia University, where he taught until his death in 1933.\n\nIn 1876, Adler at age 26 was invited to give a lecture expanding upon his themes first presented in the sermon at Temple Emanu-El. On May 15, 1876 he reiterated the need for a religion, without the trappings of ritual or creed, that united all of mankind in moral social action. To do away with theology and to unite theists, atheists, agnostics and deists, all in the same religious cause, was a revolutionary idea at the time. A few weeks after the sermon, Adler started a series of weekly Sunday lectures. In February 1877, aided by Joseph Seligman, president of Temple Emanu-El, Adler incorporated the Society of Ethical Culture.\n\nAdler talked about \"deed, not creed\"; his belief was that good works were the basis of ethical culture. In 1877 the Society founded the District Nursing Department, which organized a team of nurses who visited the homebound sick in poor districts. A year later, in 1878, the Society established a Free Kindergarten for working people's children. Because it served the working poor, the kindergarten provided basic necessities for the children when needed, such as clothing and hot meals. It evolved over time into the Ethical Culture Fieldston School.\n\nWell known as a lecturer and writer, Adler served as rector for the Ethical Culture School until his death in 1933. Throughout his life, he always looked beyond the immediate concerns of family, labor, and race to the long-term challenge of reconstructing institutions, such as schools and government, to promote greater justice in human relations. Cooperation rather than competition was the higher social value. He gave a series of six lectures on \"The Ethics of Marriage\" for the Lowell Institute's 1896–97 season.\n\nAdler was the founding chairman of the National Child Labor Committee in 1904. Lewis Hine was hired as the committee's photographer in 1908. In 1917 Adler served on the Civil Liberties Bureau, which later became the American Civil Liberties Bureau and then the American Civil Liberties Union (ACLU). In 1928 he became president of the Eastern division of the American Philosophical Association. He served on the first Executive Board of the National Urban League.\n\nAs a member of the New York State Tenement House Commission, Adler was concerned not only with overcrowding but also by the increase in contagious disease caused by overcrowding. Though not a proponent of free public housing, Adler spoke out about tenement reform and the rents, which he considered exorbitant. Jacob Riis wrote that Adler had \"clear incisive questions that went through all subterfuges to the root of things.\"\n\nIn 1885 Adler and others created the Tenement House Building Company in order to build \"model\" affordable tenements; they rented for $8–$14/month. By 1887 the company had completed six model buildings on the Lower East Side of Manhattan for the sum of $155,000. Critics favored legislation and regulations to improve tenement conditions, but the model tenement was a progressive step.\n\nBy the late 1890s, with the increase in international conflicts, Adler switched his concern from domestic issues to the question of American foreign policy. While some contemporaries viewed the 1898 Spanish–American War as an act to liberate the Cubans from Spanish rule, others perceived the U.S. victories in the Caribbean and the Philippines as the beginning of an expansionist empire. Adler at first supported the war but later expressed concern about American sovereignty over the Philippines and Puerto Rico. He believed that an imperialistic rather than a democratic goal was guiding U.S. foreign policy. Ethical Culture affirms \"the supreme worth of the person\", and Adler superimposed this tenet on international relations, believing that no single group could lay claim to superior institutions and lifestyle.\n\nUnlike many of his contemporaries during World War I, Adler did not believe that the defeat of the German Empire would make the world safe for democracy. He thought peace depended on representative democratic governments being non-imperialistic and their curbing the arms race. He opposed the Versailles Treaty and the League of Nations. As an alternative, Adler proposed a \"Parliament of Parliaments\", elected by the legislative bodies of the different nations and representing different classes of people, rather than special interests, so that common and not national differences would prevail.\n\nAdler developed an original philosophy with a foundation in those of Immanuel Kant and G.W.F. Hegel that developed and transformed these roots. He considered philosophy not just a guide of life but key to improving society and the human condition appropriate to a respect for essential human dignity. Rejecting Kant's metaphysics he embraced his stress on the intrinsic worth and dignity of the person. Combining a supreme moral principle similar to Kant's with his own detailed ideas of self-realization, he emphasized free development of the individual in relation to societal concerns and fellowship. He preceded John Dewey in a concern for the \"problems of men\" instead of philosophical technicalities. While his ideas shared some aspects of pragmatism he was better described as \"an ethical idealist with great practical reforming zeal\" he promoted an idealist version of moral perfectionism. He was however realistic and not sentimental acknowledging that man has done evil knowingly and deliberately.\n\nAdler's ethics combined an appeal to universal principles with moral particularism, which holds that the unique circumstances of a particular case must be carefully considered to determine the moral choice in that case. Adler believed that moral laws could not be applied in the same way to varied and unique individuals but that moral principles applied to all. He saw a need to balance essential general principles with consideration of the particular specific circumstances. He developed his own version of what he called Kant's \"formula\" which was, \"Treat every [person] as a spiritual means to thine own spiritual end and conversely.\"\n\nHe proposed a \"supreme ethical rule\" which he stated as follows: \"So act as to elicit the unique personality in others, and thereby in thyself\", or \"Act so as to elicit the best in others and thereby in thyself.\" He thought by doing so one would transcend both egoism and altruism. His position was that virtue is and must be its own reward or else it is not really virtue. He characterized a virtuous act as one, \"in which the ends of self and of the other are respected and promoted jointly\", coordinating Kantian universalistic imperative ethics with a type of perfectionism. He took ethics seriously and felt it \"must run like a golden thread through the whole of a [person's] life\". He felt consequentialism particularly utilitarianism was inappropriate in ethics as it attempts to apply quantitative measures to something of a qualitative nature.\n\nAdler's social philosophy opposed commercialism. He claimed, \"The root disease that afflicts the world at the present day is the supremacy of the commercial point of view.\" His thought prized public works and the use of reason to develop ultimate ethical standards. Adler published such works as \"Creed and Deed\" (1878), \"Moral Instruction of Children\" (1892), \"Life and Destiny\" (1905), \"The Religion of Duty\" (1906), \"Essentials of Spirituality\" (1908), \"An Ethical Philosophy of Life\" (1918), \"The Reconstruction of the Spiritual Ideal\" (1925), and \"Our Part in this World.\" He made use of ideas from Judaism, as well as the philosophies of Kant and Ralph Waldo Emerson, mixed with certain socialist ideas of his time. He believed that the concept of a \"personal god\" was unnecessary and thought that the human personality was the central force of religion. He believed that different people's interpretations of religions were to be respected as religious in themselves. The Ethical Culture movement was open to people of diverse beliefs. Ethical Culture societies were formed in the late nineteenth century in numerous cities in the United States, for instance, Philadelphia and St. Louis.\n\n\n\n\n\n"}
{"id": "50669033", "url": "https://en.wikipedia.org/wiki?curid=50669033", "title": "Fundación Mujeres en Igualdad", "text": "Fundación Mujeres en Igualdad\n\nThe Fundación Mujeres en Igualdad (MEI), known in English as the Women in Equality Foundation, is an Argentine NGO created in March 1990. It has been awarded consultative status with United Nations ECOSOC. The foundation sets out to combat gender based violence and discrimination against women by promoting welfare, participation and empowerment in the political, economic, social and cultural spheres. From its inception Women in Equality promoted the use of the new technologies intensively, being the first women´s NGO in Argentina to have a website. Through such initiatives it has networked and created partnerships with NGOs and with the women's movement both at the national and international levels.\n\nMujeres en Igualdad Foundation has its offices in Florida, Province of Buenos Aires. The founder of Women in Equality was Zita Montes de Oca, with Monique Thiteux- Altschul as its executive director.\n\n\n\nMujeres en Igualdad began its “About Representatives and Represented” Project breakfasts in 1993, and has since held 176 breakfasts, in the City of Buenos Aires and in numerous provinces. An average of 70 sundry participants attend each month, including women senators, deputees ad legislators, judges, lawyers, functionaries, journalists, academics, union representatives, members of NGOs and international agencies, embassies, aboriginal people´s organizations, politicians, and grass roots organizations. These monthly meetings are meant to make town halls debate women's issues from the political agenda as well as special topics that allow the women present to pool their thoughts and coordinate their work.\n\nMujeres en Igualdad Foundation has organized three forums and prepares its fourth Forum to be held in October 2016 in order to analyze and draft public policies and gendered budgets to counteract actions used by corruption to threaten women´s human rights. The third Forum was held in 2008 in the School of Law in the University Buenos Aires, within the framework of Women for Equality and Transparency (UNIFEM – UNDEF, United Nations), in order to debate and share experiences fostering transparency.\n\n"}
{"id": "50021", "url": "https://en.wikipedia.org/wiki?curid=50021", "title": "Gift", "text": "Gift\n\nA gift or a present is an item given to someone without the expectation of payment or anything in return. An item is not a gift if that item is already owned by the one to whom it is given. Although gift-giving might involve an expectation of reciprocity, a gift is meant to be free. In many countries, the act of mutually exchanging money, goods, etc. may sustain social relations and contribute to social cohesion. Economists have elaborated the economics of gift-giving into the notion of a gift economy. By extension the term \"gift\" can refer to any item or act of service that makes the other happier or less sad, especially as a favor, including forgiveness and kindness. Gifts are also first and foremost presented on occasions such as birthdays and holidays.\n\nIn many cultures gifts are traditionally packaged in some way. For example, in Western cultures, gifts are often wrapped in wrapping paper and accompanied by a gift note which may note the occasion, the recipient's name and the giver's name. In Chinese culture, red wrapping connotes luck. Although inexpensive gifts are common among colleagues, associates and acquaintances, expensive or amorous gifts are considered more appropriate among close friends, romantic interests or relatives.\n\nGift-giving occasions may be:\n\nPromotional gifts vary from the normal gifts. The recipients of the gifts may be either employee of a company or the clients. Promotional gifts are mainly used for advertising purposes. They are used to promote the brand name and increase its awareness among the people. In promotional gifting procedures, the quality and presentation of the gifts hold more value than the gifts itself since it will act as a gateway to acquire new clients or associates.\n\nGiving a gift to someone is not necessarily just an altruistic act. It may be given in the hope that the receiver reciprocates in a particular way. It may take the form of positive reinforcement as a reward for compliance, possibly for an underhand manipulative and abusive purpose.\n\nA significant fraction of gifts are unwanted, or the giver pays more for the item than the recipient values it, resulting in a misallocation of economic resources known as a deadweight loss. Unwanted gifts are often regifted, donated to charity, or thrown away. A gift that actually imposes a burden on the recipient, either due to maintenance or storage or disposal costs, is known as a white elephant. \n\nOne means of reducing the mismatch between the buyer and receivers' tastes is advance coordination, often undertaken in the form of a wedding registry or Christmas list. Wedding registries in particular are often kept at a single store, which can designate the exact items to be purchased (resulting in matching housewares), and to coordinate purchases so the same gift is not purchased by different guests. One study found that wedding guests who departed from the registry typically did so because they wished to signal a closer relationship to the couple by personalizing a gift, and also found that as a result of not abiding by the recipients' preferences, their gifts were appreciated less often.\n\nAn estimated $3.4 billion was spent on unwanted Christmas gifts in the United States in 2017. The day after Christmas is typically the busiest day for returns in countries with large Christmas gift giving traditions. The total unredeemed value of gift cards purchased in the U.S. each year is estimated to be about a billion dollars.\n\nAt common law, for a gift to have legal effect, it was required that there be (1) intent by the donor to give a gift, and (2) delivery to the recipient of the item to be given as a gift.\n\nIn some countries, certain types of gifts above a certain monetary amount are subject to taxation. For the United States, see Gift tax in the United States.\n\nIn some contexts, gift giving can be construed as bribery. This tends to occur in situations where the gift is given with an implicit or explicit agreement between the giver of the gift and its receiver that some type of service will be rendered (often outside of normal legitimate methods) because of the gift. Some groups, such as government workers, may have strict rules concerning gift giving and receiving so as to avoid the appearance of impropriety.\n\nLewis Hyde remarks in \"The Gift\" that Christianity considers the Incarnation and subsequent death of Jesus to be the greatest gift to humankind, and that the Jataka contains a tale of the Buddha in his incarnation as the Wise Hare giving the ultimate alms by offering himself up as a meal for Sakka. (Hyde, 1983, 58-60)\n\nIn the Eastern Orthodox Church, the bread and wine that are consecrated during the Divine Liturgy are referred to as \"the Gifts.\" They are first of all the gifts of the community (both individually and corporately) to God, and then, after the epiklesis, the Gifts of the Body and Blood of Christ to the Church.\n\nRitual sacrifices can be seen as return gifts to a deity.\n\n"}
{"id": "900695", "url": "https://en.wikipedia.org/wiki?curid=900695", "title": "Gold digging", "text": "Gold digging\n\nGold digging is a type of transactional relationship in which people, especially women, engage in relationships for money rather than love. When it turns into marriage, it is a type of marriage of convenience. \n\nPeggy Hopkins Joyce was in the 1920s considered the perfect example of a gold digger, with some claims existing that the term was even coined to describe her.\n\nA popular association between chorus girls and gold diggers was established in 1919 by \"The Gold Diggers\" play, association which was also present in the subsequent film four years later, \"The Gold Diggers\".\n\nIn 1920s and 1930s American cinema the \"gold digger\" was the type of \"femme fatale\" that gradually replaced the \"vamp\". The character type would be featured, for example, in \"How to Marry a Millionaire\", a 1953 film starring Marilyn Monroe, alongside Schatze Page and Loco Dempsey. \n\nIn the analysis of rap music it has been theorized that the \"gold digger script\" is one of a few prevalent sexual scripts present for young African American women. \n\nKanye West's \"Gold Digger\" references gold digging.\n\n\n"}
{"id": "24050502", "url": "https://en.wikipedia.org/wiki?curid=24050502", "title": "Google Maps Road Trip", "text": "Google Maps Road Trip\n\nGoogle Maps Road Trip was a live-streaming documentary produced by Marc Horowitz and Peter Baldes between August 10 and August 18, 2009. The event represented the first virtual live-streaming broadcast cross-country road trip: using only Google Maps, the pair drove from Los Angeles to Richmond, VA. They were interviewed by NPR Weekend Edition for their innovative \"vacation\". They were also mentioned in The New York Times, India's The Economic Times, The Faster Times and Readymade Magazine's blog. They streamed live on ustream.tv.\n\nGoogle Maps Road Trip was part of the 6th annual Conflux festival in 2009 held at New York University in the Steinhardt School of Culture, Education, and Human Development as part of Psy-Geo-Conflux.\n"}
{"id": "1634790", "url": "https://en.wikipedia.org/wiki?curid=1634790", "title": "Grothendieck group", "text": "Grothendieck group\n\nIn mathematics, the Grothendieck group construction in abstract algebra constructs an abelian group from a commutative monoid \"M\" in the most universal way in the sense that any abelian group containing a homomorphic image of \"M\" will also contain a homomorphic image of the Grothendieck group of \"M\". The Grothendieck group construction takes its name from the more general construction in category theory, introduced by Alexander Grothendieck in his fundamental work of the mid-1950s that resulted in the development of K-theory, which led to his proof of the Grothendieck–Riemann–Roch theorem. This article treats both constructions.\n\nGiven a commutative monoid \"M\", we want to construct \"the most general\" abelian group \"K\" that arises from \"M\" by introducing additive inverses. Such an abelian group \"K\" always exists; it is called the Grothendieck group of \"M\". It is characterized by a certain universal property and can also be concretely constructed from \"M\".\n\nLet \"M\" be a commutative monoid. Its Grothendieck group \"K\" is an abelian group with the following universal property: There exists a monoid homomorphism\n\nsuch that for any monoid homomorphism\n\nfrom the commutative monoid \"M\" to an abelian group \"A\", there is a unique group homomorphism\n\nsuch that\n\nThis expresses the fact that any abelian group \"A\" that contains a homomorphic image of \"M\" will also contain a homomorphic image of \"K\", \"K\" being the \"most general\" abelian group containing a homomorphic image of \"M\".\n\nTo construct the Grothendieck group \"K\" of a commutative monoid \"M\", one forms the Cartesian product formula_5 The two coordinates are meant to represent a positive part and a negative part, so (\"m\", \"m\") corresponds to \"m\" − \"m\" in \"K\".\n\nAddition on \"M\" × \"M\" is defined coordinate-wise:\n\nNext we define an equivalence relation on \"M\" × \"M\". We say that (\"m\", \"m\") is equivalent to (\"n\", \"n\") if, for some element \"k\" of \"M\", \"m\" + \"n\" + \"k\" = \"m\" + \"n\" + \"k\" (the element \"k\" is necessary because the cancellation law does not hold in all monoids). The equivalence class of the element (\"m\", \"m\") is denoted by [(\"m\", \"m\")]. We define \"K\" to be the set of equivalence classes. Since the addition operation on \"M\" × \"M\" is compatible with our equivalence relation, we obtain an addition on \"K\", and \"K\" becomes an abelian group. The identity element of \"K\" is [(0, 0)], and the inverse of [(\"m\", \"m\")] is [(\"m\", \"m\")]. The homomorphism formula_6 sends the element \"m\" to [(\"m\", 0)].\n\nAlternatively, the Grothendieck group \"K\" of \"M\" can also be constructed using generators and relations: denoting by formula_7 the free abelian group generated by the set \"M\", the Grothendieck group \"K\" is the quotient of formula_8 by the subgroup generated by formula_9. (Here +′ and −′ denote the addition and subtraction in the free abelian group formula_8 while + denotes the addition in the monoid \"M\".) This construction has the advantage that it can be performed for any semigroup \"M\" and yields a group which satisfies the corresponding universal properties for semigroups, i.e. the \"most general and smallest group containing a homomorphic image of \"M\"\". This is known as the \"group completion of a semigroup\" or \"group of fractions of a semigroup\".\n\nIn the language of category theory, any universal construction gives rise to a functor; we thus obtain a functor from the category of commutative monoids to the category of abelian groups which sends the commutative monoid \"M\" to its Grothendieck group \"K\". This functor is left adjoint to the forgetful functor from the category of abelian groups to the category of commutative monoids.\n\nFor a commutative monoid \"M\", the map \"i\" : \"M\"→\"K\" is injective if and only if \"M\" has the cancellation property, and it is bijective if and only if \"M\" is already a group.\n\nThe easiest example of a Grothendieck group is the construction of the integers formula_11 from the natural numbers formula_12 First one observes that the natural numbers (including 0) together with the usual addition indeed form a commutative monoid formula_13 Now when we use the Grothendieck group construction we obtain the formal differences between natural numbers as elements \"n\" − \"m\" and we have the equivalence relation\n\nNow define\n\nThis defines the integers formula_11 Indeed, this is the usual construction to obtain the integers from the natural numbers. See \"Construction\" under Integers for a more detailed explanation.\n\nThe Grothendieck group is the fundamental construction of K-theory. The group formula_17 of a compact manifold \"M\" is defined to be the Grothendieck group of the commutative monoid of all isomorphism classes of vector bundles of finite rank on \"M\" with the monoid operation given by direct sum. This gives a contravariant functor from manifolds to abelian groups. This functor is studied and extended in topological K-theory.\n\nThe zeroth algebraic K group formula_18 of a (not necessarily commutative) ring \"R\" is the Grothendieck group of the monoid consisting of isomorphism classes of finitely generated projective modules over \"R\", with the monoid operation given by the direct sum. Then formula_19 is a covariant functor from rings to abelian groups.\n\nThe two previous examples are related: consider the case where \"R\" is the ring formula_20 of complex-valued smooth functions on a compact manifold \"M\". In this case the projective \"R\"-modules are dual to vector bundles over \"M\" (by the Serre-Swan theorem). Thus formula_18 and formula_17 are the same group.\n\nAnother construction that carries the name Grothendieck group is the following: Let \"R\" be a finite-dimensional algebra over some field \"k\" or more generally an artinian ring. Then define the Grothendieck group formula_23 as the abelian group generated by the set formula_24 of isomorphism classes of finitely generated \"R\"-modules and the following relations: For every short exact sequence\n\nof \"R\"-modules add the relation\n\nNote that the proposed definition of Grothendieck group formula_23 is well-defined. Let \"R\" be an Artinian ring, and suppose formula_28 and formula_29 are isomorphic finitely generated \"R\"-modules. Then there exists the following short exact sequence\n\nThe exact sequence hence implies that formula_31. Since formula_32, it follows that formula_33. The proposed definition also implies that for any two finitely generated \"R\"-modules \"M\" and \"N\", formula_34. This follows from the given split short exact sequence.\n\nLet \"K\" be a field. Then the Grothendieck group formula_36 is an abelian group generated by symbols formula_37 for any finite dimensional \"K\"-vector space \"V\". In fact, formula_36 is isomorphic to formula_39 whose generator is the element formula_40. Here, the symbol formula_37 for a finite \"K\"-vector space \"V\" is defined as formula_42, the dimension of the vector space \"V\". Suppose we have the following short exact sequence of \"K\"-vector spaces.\n\nSince any short exact sequence of vector spaces splits, it holds that formula_44. In fact, for any two finite dimensional vector spaces \"V\" and \"W\" the following holds.\n\nThe above equality hence satisfies the condition of the symbol formula_37 in the Grothendieck group.\n\nNote that any two isomorphic finite dimensional \"K\"-vector space has the same dimension. Also, any two finite dimensional \"K\"-vector space \"V\" and \"W\" of same dimension are isomorphic to each other. In fact, every finite \"n\"-dimensional \"K\"-vector space \"V\" is isomorphic to formula_48. The observation from the previous paragraph hence proves the following equation. \n\nHence, every symbol formula_37 is generated by the element formula_40 with integer coefficients, which implies that formula_36 is isomorphic to formula_11 with the generator formula_40.\n\nMore generally, let formula_11 be the set of integers. The Grothendieck group formula_56 is an abelian group generated by symbols formula_57 for any finitely generated abelian groups \"A\". We first note that any finite abelian group \"G\" satisfies that formula_58. The following short exact sequence holds, where the map formula_59 is multiplication by \"n\".\n\nThe exact sequence implies that formula_61, so every cyclic group has its symbol equal to 0. This in turn implies that every finite abelian group \"G\" satisfies formula_58 by the Fundamental Theorem of Finite Abelian groups. \n\nObserve that by the Fundamental Theorem of Finitely Generated Abelian Groups, every abelian group is isomorphic to a direct sum of a torsion subgroup and a torsion-free abelian group isomorphic to formula_63 for some non-negative integer \"r\". Note that the integer \"r\" is defined as the rank of the abelian group \"A\". Define the symbol formula_57 as formula_65. Then the Grothendieck group formula_56 is isomorphic to formula_11 with generator formula_68 Indeed, the observation made from the previous paragraph shows that every abelian group \"A\" has its symbol formula_57 the same to the symbol formula_70 where formula_71. Furthermore, the rank of the abelian group satisfies the conditions of the symbol formula_57 of the Grothendieck group. Suppose we have the following short exact sequence of abelian groups.\n\nThen tensoring with the rational numbers formula_74 implies the following equation.\n\nSince the above is a short exact sequence of formula_74-vector spaces, the sequence splits. Therefore, we have the following equation.\n\nOn the other hand, we also have the following relation. For more information, see: Rank of Abelian Group.\n\nTherefore, the following equation holds.\n\nHence we have shown that formula_56 is isomorphic to formula_11 with generator formula_68\n\nGrothendieck group satisfies a universal property. We make a preliminary definition: A function formula_83 from the set of isomorphism classes to an abelian group formula_84 is called \"additive\" if, for each exact sequence formula_85, we have formula_86 Then, for any additive function formula_87, there is a \"unique\" group homomorphism formula_88 such that formula_83 factors through \"f\" and the map that takes each object of formula_90 to the element representing its isomorphism class in formula_91 Concretely this means that formula_92 satisfies the equation formula_93 for every finitely generated formula_94-module formula_95 and formula_92 is the only group homomorphism that does that.\n\nExamples of additive functions are the character function from representation theory: If formula_94 is a finite-dimensional formula_98-algebra, then we can associate the character formula_99 to every finite-dimensional formula_94-module formula_101 is defined to be the trace of the formula_98-linear map that is given by multiplication with the element formula_103 on formula_95.\n\nBy choosing a suitable basis and writing the corresponding matrices in block triangular form one easily sees that character functions are additive in the above sense. By the universal property this gives us a \"universal character\" formula_105 such that formula_106.\n\nIf formula_107 and formula_94 is the group ring formula_109 of a finite group formula_110 then this character map even gives a natural isomorphism of formula_111 and the character ring formula_112. In the modular representation theory of finite groups formula_98 can be a field formula_114 the algebraic closure of the finite field with \"p\" elements. In this case the analogously defined map that associates to each formula_115-module its Brauer character is also a natural isomorphism formula_116 onto the ring of Brauer characters. In this way Grothendieck groups show up in representation theory.\n\nThis universal property also makes formula_23 the 'universal receiver' of generalized Euler characteristics. In particular, for every bounded complex of objects in formula_118\n\nwe have a canonical element\n\nIn fact the Grothendieck group was originally introduced for the study of Euler characteristics.\n\nA common generalization of these two concepts is given by the Grothendieck group of an exact category formula_121. Simply put, an exact category is an additive category together with a class of distinguished short sequences \"A\" → \"B\" → \"C\". The distinguished sequences are called \"exact sequences\", hence the name. The precise axioms for this distinguished class do not matter for the construction of the Grothendieck group.\n\nThe Grothendieck group is defined in the same way as before as the abelian group with one generator [\"M\"] for each (isomorphism class of) object(s) of the category formula_121 and one relation\n\nfor each exact sequence\n\nAlternatively one can define the Grothendieck group using a similar universal property: An abelian group \"G\" together with a mapping formula_125 is called the Grothendieck group of formula_121 iff every \"additive\" map formula_127 from formula_121 into an abelian group \"X\" (\"additive\" in the above sense, i.e. for every exact sequence formula_124 we have formula_130) factors uniquely through φ.\n\nEvery abelian category is an exact category if we just use the standard interpretation of \"exact\". This gives the notion of a Grothendieck group in the previous section if we choose formula_131-mod the category of finitely generated \"R\"-modules as formula_121. This is really abelian because \"R\" was assumed to be artinian and (hence noetherian) in the previous section.\n\nOn the other hand, every additive category is also exact if we declare those and only those sequences to be exact that have the form formula_133 with the canonical inclusion and projection morphisms. This procedure produces the Grothendieck group of the commutative monoid formula_134 in the first sense (here formula_135 means the \"set\" [ignoring all foundational issues] of isomorphism classes in formula_121.)\n\nGeneralizing even further it is also possible to define the Grothendieck group for triangulated categories. The construction is essentially similar but uses the relations [\"X\"] - [\"Y\"] + [\"Z\"] = 0 whenever there is a distinguished triangle \"X\" → \"Y\" → \"Z\" → \"X\"[1].\n\n\n\n\n\n\n\n"}
{"id": "19328052", "url": "https://en.wikipedia.org/wiki?curid=19328052", "title": "Hatred", "text": "Hatred\n\nHatred or hate is a human emotion. Hatred could invoke feelings of animosity, anger or resentment, which can be directed against certain individuals, groups, entities, objects, behaviors, concepts, or ideas.\n\nHatred is often associated with feelings of anger, disgust and a disposition towards the source of hostility.\n\nAs an emotion, hatred can be short-lived or long-lasting. It can be of low intensity - 'I hate broccoli' - or high intensity: 'I hate the whole world'.\n\nRobert Steinberg saw three main elements in hatred: \nThe important self-protective function, to be found in hatred, can be illustrated by Steinberg's analysis of 'mutinous' hatred, whereby a dependent relationship is repudiated in a quest for autonomy. \n\nSigmund Freud defined hate as an ego state that wishes to destroy the source of its unhappiness, stressing that it was linked to the question of self-preservation. Donald Winnicott highlighted the developmental step involved in hatred, with its recognition of an outside object: \"As compared to magical destruction, aggressive ideas and behaviour take on a positive value, and hate becomes a sign of civilization\".\n\nIn his wake, Object relations theory has emphasised the importance of recognising hate in the analytic setting: the analyst acknowledges his own hate (as revealed in the strict time-limits and the fee charged), which in turn may make it possible for the patient to acknowledge and contain \"their\" previously concealed hate for the analyst.\n\nAdam Phillips went so far as to suggest that true kindness is impossible in a relationship without hating and being hated, so that an unsentimental acknowledgement of interpersonal frustrations and their associated hostilities can allow real fellow-feeling to emerge.\n\nIn the English language, a hate crime (also known as a \"bias-motivated crime\") generally refers to criminal acts which are seen to have been motivated by hate. Those who commit hate crimes target victims because of their perceived membership in a certain social group, usually defined by race, gender, religion, sexual orientation, mental disorder, disability, class, ethnicity, nationality, age, gender identity, or political affiliation. Incidents may involve physical assault, destruction of property, bullying, harassment, verbal abuse or insults, or offensive graffiti or letters (hate mail).\n\nHate speech is speech perceived to disparage a person or group of people based on their social or ethnic group, such as race, sex, age, ethnicity, nationality, religion, sexual orientation, gender identity, mental disorder, disability, language ability, ideology, social class, occupation, appearance (height, weight, skin color, etc.), mental capacity, and any other distinction that might be considered a liability. The term covers written as well as oral communication and some forms of behaviors in a public setting. It is also sometimes called antilocution and is the first point on Allport's scale which measures prejudice in a society. In many countries, deliberate use of hate speech is a criminal offence prohibited under \"incitement to hatred\" legislation. It is often alleged that the criminalization of hate speech is sometimes used to discourage legitimate discussion of negative aspects of voluntary behavior (such as political persuasion, religious adherence and philosophical allegiance). There is also some question as to whether or not hate speech falls under the protection of freedom of speech in some countries.\n\nBoth of these classifications have sparked debate, with counter-arguments such as, but not limited to, a difficulty in distinguishing motive and intent for crimes, as well as philosophical debate on the validity of valuing targeted hatred as a greater crime than general misanthropy and contempt for humanity being a potentially equal crime in and of itself.\n\nThe neural correlates of hate have been investigated with an fMRI procedure. In this experiment, people had their brains scanned while viewing pictures of people they hated. The results showed increased activity in the middle frontal gyrus, right putamen, bilaterally in the premotor cortex, in the frontal pole, and bilaterally in the medial insular cortex of the human brain.\n\nHate, like love, takes different shapes and forms in different languages. While it may be fair to say that one single emotion exists in English, French (haine), and German (Hass), hate is historically situated and culturally constructed: it varies in the forms in which it is manifested. Thus a certain relationless hatred is expressed in the French expression \"J'ai la haine\", which has no precise equivalent in English; while for English-speakers, loving and hating invariably involve an object, or a person, and therefore, a relationship with something or someone, \"J'ai la haine\" (literally, I have hate) precludes the idea of an emotion directed at a person. This is a form of frustration, apathy and animosity which churns within the subject but establishes no relationship with the world, other than an aimless desire for destruction. \n\nFrench forms of anti-Americanism have been seen as a specific form of cultural resentment, registering joy-in-hate.\n\nThe Hebrew word describing David's \"perfect hatred\" (KJV) means that it \"brings a process to completion\".\n\n<poem>I study hatred with great diligence,\nFor that's a passion in my own control,\nA sort of besom that can clear the soul\nOf everything that is not mind or sense.\"</poem>\n\n\n"}
{"id": "1574433", "url": "https://en.wikipedia.org/wiki?curid=1574433", "title": "Homesickness", "text": "Homesickness\n\nHomesickness is the distress caused by being away from home. Its cognitive hallmark is preoccupying thoughts of home and attachment objects. Sufferers typically report a combination of depressive and anxious symptoms, withdrawn behavior and difficulty focusing on topics unrelated to home.\n\nIn its mild form, homesickness prompts the development of coping skills and motivates healthy attachment behaviors, such as renewing contact with loved ones. Indeed, nearly all people miss something about home when they are away, making homesickness a nearly universal experience. However, intense homesickness can be painful and debilitating.\n\nHomesickness is an ancient phenomenon, mentioned in both the Old Testament books of \"Exodus\" and \"Psalm\" 137:1 (\"By the rivers of Babylon, there we sat down, yea, we wept, when we remembered Zion\") as well as Homer's \"Odyssey\", whose opening scene features Athena arguing with Zeus to bring Odysseus home because he is homesick (\"...longing for his wife and his homecoming...\"). The Greek physician Hippocrates (ca. 460–377 BC) believed that homesickness—also called \"heimveh\" (old German word for \"Heimweh\") or a \"nostalgic reaction\"—was caused by a surfeit of black bile in the blood. In recent history homesickness is first mentioned specifically with Swiss people being abroad in Europe (\"Heimweh\") for a longer period of time in a document dating back to 1691. A normal phenomenon amongst the many common Swiss mercenaries serving in different countries and many rulers across Europe at that time. It was not uncommon for them staying many years away from home and, if lucky enough, return home if still alive. This phenomenon at that time was first only thought to affect Swiss people until this was revised, probably caused by big migration streams across Europe suggesting the same symptoms and thus homesickness found its way into general German medical literature in the 19th century. American contemporary histories, such as Susan J. Matt's \"Homesickness: An American History\" eloquently describe experiences of homesickness in colonists, immigrants, gold miners, soldiers, explorers and others spending time away from home. First understood as a brain lesion, homesickness is now known to be a form of normative psychopathology that reflects the strength of a person's attachment to home, native culture and loved ones, as well as their ability to regulate their emotions and adjust to novelty. Cross-cultural research, with populations as diverse as refugees and boarding school students, suggests considerable agreement on the definition of homesickness. Additional historical perspectives on homesickness and place attachment can be found in books by van Tilburg & Vingerhoets, Matt, and Williams.\n\nWhereas separation anxiety disorder is characterized by \"inappropriate and excessive fear or anxiety concerning separation from those to whom the individual is attached\" symptoms of homesickness are most prominent \"after \"a separation and include \"both \"depression \"and\" anxiety. In DSM terms, homesickness may be related to Separation Anxiety Disorder, but it is perhaps best categorized as either an Adjustment Disorder with mixed anxiety and depressed mood (309.28) or, for immigrants and foreign students as a V62.4, Acculturation Difficulty. As noted above, researchers use the following definition: \"Homesickness is the distress or impairment caused by an actual or anticipated separation from home. Its cognitive hallmark is preoccupying thoughts of home and attachment objects.\" Recent pathogenic models support the possibility that homesickness reflects both insecure attachment and a variety of emotional and cognitive vulnerabilities, such as little previous experience away from home and negative attitudes about the novel environment.\n\nThe prevalence of homesickness varies greatly, depending on the population studied and the way homesickness is measured. One way to conceptualize homesickness prevalence is as a function of severity. Nearly all people miss something about home when they are away, so the absolute prevalence of homesickness is close to 100%, mostly in a mild form. Roughly 20% of university students and children at summer camp rate themselves at or above the midpoint on numerical rating scales of homesickness severity. And only 5–7% of students and campers report intense homesickness associated with severe symptoms of anxiety and depression. However, in adverse or painful environments, such as the hospital or the battlefield, intense homesickness is far more prevalent. In one study, 50% of children scored themselves at or above the midpoint on a numerical homesickness intensity scale (compared to 20% of children at summer camp). Soldiers report even more intense homesickness, sometimes to the point of suicidal misery. Naturally, aversive environmental elements, such as the trauma associated with war, exacerbate homesickness and other mental health problems.\n\nIn sum, homesickness is a normative pathology that can take on clinical relevance in its moderate and severe forms.\n\nRisk factors (constructs which increase the likelihood or intensity of homesickness) and protective factors (constructs that decrease the likelihood or intensity of homesickness) vary by population. For example, a seafarers on board, the environmental stressors associated with a hospital, a military boot camp or a foreign country may exacerbate homesickness and complicate treatment. Generally speaking, however, risk and protective factors transcend age and environment.\n\nThe risk factors for homesickness fall into five categories: experience, personality, family, attitude and environment. More is known about some of these factors in adults—especially personality factors—because more homesickness research has been performed with older populations. However, a growing body of research is elucidating the etiology of homesickness in younger populations, including children at summer camp, hospitalized children and students.\n\n\nFactors which mitigate the prevalence or intensity of homesickness are essentially the inverse of the risk factors cited above. Effective coping (reviewed in the following section) also diminishes the intensity of homesickness over time. Prior to a separation, however, key protective factors can be identified. Positive adjustment to separation from home is generally associated with the following factors:\n\n\nHow people—especially young people—cope with homesickness deserves careful study for at least three reasons. First, homesickness is experienced by millions of people who spend time away from home (see McCann, 1941, for an early review) including children at boarding schools, residential summer camps and hospitals.\n\nSecond, severe homesickness is associated with significant distress and impairment. There is evidence that homesick persons present with non-traumatic physical ailments significantly more than their non-homesick peers. Homesick boys and girls complain about somatic problems and exhibit more internalizing and externalizing behaviors problems than their nonhomesick peers. First-year college students are three times more likely to drop out of school than their nonhomesick peers. Other data have pointed to concentration and academic problems in homesick students. And maladjustment to separation from home has been documented in hospitalized young people and is generally associated with slower recovery. See Thurber & Walton (2012) for a review.\n\nThird, learning more about how people cope with homesickness is a helpful guide to designing treatment programs. By complementing existing theories of depression, anxiety and attachment, a better theoretical understanding of homesickness can shape applied interventions. Among the most relevant theories that could shape interventions are those concerned with Learned Helplessness and Control Beliefs.\n\nLearned helplessness predicts that persons who develop a belief that they cannot influence or adjust to their circumstance of separation from home will become depressed and make fewer attempts to change that circumstance. Control beliefs theory predicts that negative affect is most likely in persons who perceive personal incompetence in the separation environment (e.g., poor social skills at a summer camp or university) and who perceive contingency uncertainty (e.g., uncertainty about whether friendly behavior will garner friends). Although these are not the only broad etiologic theories that inform homesickness, note that both theories hinge on control, the perception of which \"reflects the fundamental human need for competence\" (Skinner, 1995, p. 8). This is particularly relevant to coping, because people's choice of \"how\" to respond to a stressor hinges partly on their perception of a stressor's controllability.\n\nAn equally important coping factor is social connection, which for many people is the antidote to homesickness. As the results of several studies have suggested, social connection is a powerful mediator of homesickness intensity.\n\nThe most effective way of coping with homesickness is mixed and layered. Mixed coping is that which involves both primary goals (changing circumstances) and secondary goals (adjusting to circumstances). Layered coping is that which involves more than one method. This kind of sophisticated coping is learned through experience, such as brief periods away from home without parents. As an example of mixed and layered coping, one study revealed the following method-goal combinations to be the most frequent and effective ways for boys and girls:\n\n\nSometimes, people will engage in wishful thinking, attempt to arrange a shorter stay or (rarely) break rules or act violently in order to be sent home. These ways of coping are rarely effective and can produce unintended negative side effects.\n\nHomesickness is a major theme of the film \"Brooklyn\" (2015). One critic said that the protagonist's depiction of homesickness \"as a physical, implacable reality is acute, and it's backed up by what we see around her.\"\n\n\n"}
{"id": "4944254", "url": "https://en.wikipedia.org/wiki?curid=4944254", "title": "Hope (virtue)", "text": "Hope (virtue)\n\nHope (lat. \"spes\") is one of the three theological virtues in Christian tradition. Hope being a combination of the desire for something and expectation of receiving it, the virtue is hoping for Divine union and so eternal happiness. While faith is a function of the intellect, hope is an act of the will.\n\nAquinas defines hope as \"\"...a future good, difficult but possible to attain...by means of the Divine Assistance...on Whose help it leans\". Hope is, by its very nature, always concerned with something in the future. Like the theological virtues of faith and charity, hope finds its \"origin, motive, and object\" in God. In Hebrews 10:23, St. Paul says, \"Let us hold unwaveringly to our confession that gives us hope, for he who made the promise is trustworthy.\" Like the other theological virtues, hope is an infused virtue. It is not, like good habits in general, the outcome of repeated acts or the product of our own industry. Hope is bestowed by God at baptism.\n\nIn the Christian tradition, \"hope in Christ\" and faith in Christ are closely linked, with hope having a connotation that means the one with hope has a firm assurance, through the witness of the Holy Spirit, that Christ has promised a better world to those who are His. The Christian sees death not just as the end of a passing life, but as the gateway to a future life without end and in all fullness. Pope Benedict XVI states: \"Whoever believes in Christ has a future. For God has no desire for what is withered, dead, ersatz, and finally discarded: he wants what is fruitful and alive, he wants life in its fullness and he gives us life in its fullness\"\n\nHope can thus sustain one through trials of faith, human tragedies or difficulties that may otherwise seem overwhelming. Hope is seen as \"an anchor of the soul\" as referenced in the Epistle to the Hebrews of the New Testament. Hebrews 7:19 also describes the \"better hope\" of the New Covenant in Christ rather than the Old Covenant of the Jewish law.\n\nHope is opposed to the sins of despair and presumption; refraining from them is adhering to the \"negative precept\" of hope. The \"positive precept\" is required when exercising some duties, as in prayer or penance.\n\nSome forms of Quietism have denied that a human being should desire anything whatsoever to such an extent that they denied that hope was a virtue. Quietism was condemned as heresy by Pope Innocent XI in 1687 in the papal bull \"Coelestis Pastor\".\n\n\n\n\n\nO my God, relying on Your almighty power and infinite mercy and promises, I hope to obtain pardon of my sins, the help of Your grace, and life everlasting through the merits of Jesus Christ, my Lord and Redeemer. Amen. \n\n\n"}
{"id": "26764", "url": "https://en.wikipedia.org/wiki?curid=26764", "title": "International System of Units", "text": "International System of Units\n\nThe International System of Units (SI, abbreviated from the French \"\") is the modern form of the metric system, and is the most widely used system of measurement. It comprises a coherent system of units of measurement built on seven base units, which are the ampere, kelvin, second, metre, kilogram, candela, mole, and a set of twenty prefixes to the unit names and unit symbols that may be used when specifying multiples and fractions of the units. The system also specifies names for 22 derived units, such as lumen and watt, for other common physical quantities.\n\nThe base units are derived from invariant constants of nature, such as the speed of light in vacuum and the triple point of water, which can be observed and measured with great accuracy, and one physical artefact. The artefact is the international prototype kilogram, certified in 1889, and consisting of a cylinder of platinum-iridium, which nominally has the same mass as one litre of water at the freezing point. Its stability has been a matter of significant concern, culminating in a proposed revision of the definition of the base units entirely in terms of constants of nature, scheduled to be put into effect on 20 May 2019.\n\nDerived units may be defined in terms of base units or other derived units. They are adopted to facilitate measurement of diverse quantities. The SI is intended to be an evolving system; units and prefixes are created and unit definitions are modified through international agreement as the technology of measurement progresses and the precision of measurements improves. The most recent derived unit, the katal, was defined in 1999.\n\nThe reliability of the SI depends not only on the precise measurement of standards for the base units in terms of various physical constants of nature, but also on precise definition of those constants. The set of underlying constants is modified as more stable constants are found, or may be more precisely measured. For example, in 1983 the metre was redefined as the distance that light propagates in vacuum in a given fraction of a second, thus making the value of the speed of light in terms of the defined units exact.\nThe motivation for the development of the SI was the diversity of units that had sprung up within the centimetre–gram–second (CGS) systems (specifically the inconsistency between the systems of electrostatic units and electromagnetic units) and the lack of coordination between the various disciplines that used them. The General Conference on Weights and Measures (French: \"\" – CGPM), which was established by the Metre Convention of 1875, brought together many international organisations to establish the definitions and standards of a new system and standardise the rules for writing and presenting measurements. The system was published in 1960 as a result of an initiative that began in 1948. It is based on the metre–kilogram–second system of units (MKS) rather than any variant of the CGS. Since then, the SI has been adopted by all countries except the United States, Liberia and Myanmar.\n\nThe International System of Units consists of a set of base units, derived units, and a set of decimal-based multipliers that are used as prefixes. The units, excluding prefixed units, form a coherent system of units, which is based on a system of quantities in such a way that the equations between the numerical values expressed in coherent units have exactly the same form, including numerical factors, as the corresponding equations between the quantities. For example, 1 N = 1 kg × 1 m/s says that \"one\" newton is the force required to accelerate a mass of \"one\" kilogram at \"one\" metre per second squared, as related through the principle of coherence to the equation relating the corresponding quantities: .\n\nDerived units apply to derived quantities, which may by definition be expressed in terms of base quantities, and thus are not independent; for example, electrical conductance is the inverse of electrical resistance, with the consequence that the siemens is the inverse of the ohm, and similarly, the ohm and siemens can be replaced with a ratio of an ampere and a volt, because those quantities bear a defined relationship to each other. Other useful derived quantities can be specified in terms of the SI base and derived units that have no named units in the SI system, such as acceleration, which is defined in SI units as m/s.\n\nThe SI base units are the building blocks of the system and all the other units are derived from them. When Maxwell first introduced the concept of a coherent system, he identified three quantities that could be used as base units: mass, length and time. Giorgi later identified the need for an electrical base unit, for which the unit of electric current was chosen for SI. Another three base units (for temperature, amount of substance and luminous intensity) were added later.\n\nThe early metric systems defined a unit of weight as a base unit, while the SI defines an analogous unit of mass. In everyday use, these are mostly interchangeable, but in scientific contexts the difference matters. Mass, strictly the inertial mass, represents a quantity of matter. It relates the acceleration of a body to the applied force via Newton's law, : force equals mass times acceleration. A force of 1 N (newton) applied to a mass of 1 kg will accelerate it at 1 m/s. This is true whether the object is floating in space or in a gravity field e.g. at the Earth's surface. Weight is the force exerted on a body by a gravitational field, and hence its weight depends on the strength of the gravitational field. Weight of a 1 kg mass at the Earth's surface is ; mass times the acceleration due to gravity, which is 9.81 newtons at the Earth's surface and is about 3.5 newtons at the surface of Mars. Since the acceleration due to gravity is local and varies by location and altitude on the Earth, weight is unsuitable for precision measurements of a property of a body, and this makes a unit of weight unsuitable as a base unit.\n\nThe derived units in the SI are formed by powers, products or quotients of the base units and are unlimited in number. Derived units are associated with derived quantities; for example, velocity is a quantity that is derived from the base quantities of time and length, and thus the SI derived unit is metre per second (symbol m/s). The dimensions of derived units can be expressed in terms of the dimensions of the base units.\n\nCombinations of base and derived units may be used to express other derived units. For example, the SI unit of force is the newton (N), the SI unit of pressure is the pascal (Pa)—and the pascal can be defined as one newton per square metre (N/m).\n\nPrefixes are added to unit names to produce multiples and sub-multiples of the original unit. All of these are integer powers of ten, and above a hundred or below a hundredth all are integer powers of a thousand. For example, \"kilo-\" denotes a multiple of a thousand and \"milli-\" denotes a multiple of a thousandth, so there are one thousand millimetres to the metre and one thousand metres to the kilometre. The prefixes are never combined, so for example a millionth of a metre is a \"micrometre\", not a millimillimetre. Multiples of the kilogram are named as if the gram were the base unit, so a millionth of a kilogram is a \"milligram\", not a microkilogram. When prefixes are used to form multiples and submultiples of SI base and derived units, the resulting units are no longer coherent.\n\nThe BIPM specifies twenty prefixes for the International System of Units (SI):\n\nMany non-SI units continue to be used in the scientific, technical, and commercial literature. Some units are deeply embedded in history and culture, and their use has not been entirely replaced by their SI alternatives. The CIPM recognised and acknowledged such traditions by compiling a list of non-SI units accepted for use with SI, which are grouped as follows:\n\n\nThe basic units of the metric system, as originally defined, represented common quantities or relationships in nature. They still do – the modern precisely defined quantities are refinements of definition and methodology, but still with the same magnitudes. In cases where laboratory precision may not be required or available, or where approximations are good enough, the original definitions may suffice.\n\n\nThe symbols for the SI units are intended to be identical, regardless of the language used, but unit names are ordinary nouns and use the character set and follow the grammatical rules of the language concerned. Names of units follow the grammatical rules associated with common nouns: in English and in French they start with a lowercase letter (e.g., newton, hertz, pascal), even when the symbol for the unit begins with a capital letter. This also applies to \"degrees Celsius\", since \"degree\" is the unit. The official British and American spellings for certain SI units differ – British English, as well as Australian, Canadian and New Zealand English, uses the spelling \"deca-\", \"metre\", and \"litre\" whereas American English uses the spelling \"deka-\", \"meter\", and \"liter\", respectively.\n\nAlthough the writing of unit names is language-specific, the writing of unit symbols and the values of quantities is consistent across all languages and therefore the SI Brochure has specific rules in respect of writing them. The guideline produced by the National Institute of Standards and Technology (NIST) clarifies language-specific areas in respect of American English that were left open by the SI Brochure, but is otherwise identical to the SI Brochure.\n\nGeneral rules for writing SI units and quantities apply to text that is either handwritten or produced using an automated process:\n\n\nThe rules covering printing of quantities and units are part of ISO 80000-1:2009.\n\nFurther rules are specified in respect of production of text using printing presses, word processors, typewriters and the like.\n\nThe denominator \"hour\" (h) is often translated to the country language:\n\nCountries with historical ties to the United States often mix up the international \"km/h\" with the American \"MPH\":\nThe quantities and equations that provide the context in which the SI units are defined are now referred to as the \"International System of Quantities\" (ISQ).\nThe system is based on the quantities underlying each of the seven base units of the SI. Other quantities, such as area, pressure, and electrical resistance, are derived from these base quantities by clear non-contradictory equations. The ISQ defines the quantities that are measured with the SI units. The ISQ is defined in the international standard ISO/IEC 80000, and was finalised in 2009 with the publication of ISO 80000-1.\n\nMetrologists carefully distinguish between the definition of a unit and its realisation. The definition of each base unit of the SI is drawn up so that it is unique and provides a sound theoretical basis on which the most accurate and reproducible measurements can be made. The realisation of the definition of a unit is the procedure by which the definition may be used to establish the value and associated uncertainty of a quantity of the same kind as the unit. A description of the \"mise en pratique\" of the base units is given in an electronic appendix to the SI Brochure.\n\nThe published \"mise en pratique\" is not the only way in which a base unit can be determined: the SI Brochure states that \"any method consistent with the laws of physics could be used to realise any SI unit.\" In the current (2016) exercise to overhaul the definitions of the base units, various consultative committees of the CIPM have required that more than one \"mise en pratique\" shall be developed for determining the value of each unit. In particular:\n\nThe International Bureau of Weights and Measures (BIPM) has described SI as \"the modern metric system\". Changing technology has led to an evolution of the definitions and standards that has followed two principal strands – changes to SI itself, and clarification of how to use units of measure that are not part of SI but are still nevertheless used on a worldwide basis.\n\nSince 1960 the CGPM has made a number of changes to the SI to meet the needs of specific fields, notably chemistry and radiometry. These are mostly additions to the list of named derived units, and include the \"mole\" (symbol mol) for an amount of substance, the \"pascal\" (symbol Pa) for pressure, the \"siemens\" (symbol S) for electrical conductance, the \"becquerel\" (symbol Bq) for \"activity referred to a radionuclide\", the \"gray\" (symbol Gy) for ionising radiation, the \"sievert\" (symbol Sv) as the unit of dose equivalent radiation, and the \"katal\" (symbol kat) for catalytic activity.\n\nAcknowledging the advancement of precision science at both large and small scales, the range of defined prefixes pico- (10) to tera- (10) was extended to 10 to 10.\n\nThe 1960 definition of the standard metre in terms of wavelengths of a specific emission of the krypton 86 atom was replaced with the distance that light travels in a vacuum in exactly second, so that the speed of light is now an exactly specified constant of nature.\n\nA few changes to notation conventions have also been made to alleviate lexicographic ambiguities. An analysis under the aegis of CSIRO, published in 2009 by the Royal Society, has pointed out the opportunities to finish the realisation of that goal, to the point of universal zero-ambiguity machine readability.\n\nAfter the metre was redefined in 1960, the kilogram remained the only SI base unit directly based on a specific physical artefact, the international prototype of the kilogram (IPK), for its definition and thus the only unit that was still subject to periodic comparisons of national standard kilograms with the IPK. During the 2nd and 3rd Periodic Verification of National Prototypes of the Kilogram, a significant divergence had occurred between the mass of the IPK and all of its official copies stored around the world: the copies had all noticeably increased in mass with respect to the IPK. During \"extraordinary verifications\" carried out in 2014 preparatory to redefinition of metric standards, continuing divergence was not confirmed. Nonetheless, the residual and irreducible instability of a physical IPK undermined the reliability of the entire metric system to precision measurement from small (atomic) to large (astrophysical) scales.\n\nA proposal was made that:\n\nThe redefinitions were adopted at the 26th CGPM in November 2018, and will come into effect in May 2019. The CODATA task group on fundamental constants has announced special submission deadlines for data to compute the values that will be announced at this event.\n\nThe units and unit magnitudes of the metric system which became the SI were improvised piecemeal from everyday physical quantities starting in the mid-18th century. Only later were they moulded into an orthogonal coherent decimal system of measurement.\n\nThe degree centigrade as a unit of temperature resulted from the scale devised by Swedish astronomer Anders Celsius in 1742. His scale counter-intuitively designated 100 as the freezing point of water and 0 as the boiling point. Independently, in 1743, the French physicist Jean-Pierre Christin described a scale with 0 as the freezing point of water and 100 the boiling point. The scale became known as the centi-grade, or 100 gradations of temperature, scale.\n\nThe metric system was developed from 1791 onwards by a committee of the French Academy of Sciences, commissioned to create a unified and rational system of measures. The group, which included preeminent French men of science, used the same principles for relating length, volume, and mass that had been proposed by the English clergyman John Wilkins in 1668 and the concept of using the Earth's meridian as the basis of the definition of length, originally proposed in 1670 by the French abbot Mouton.\n\nIn March 1791, the Assembly adopted the committee's proposed principles for the new decimal system of measure including the metre defined to be 1/10,000,000th of the length of the quadrant of earth's meridian passing through Paris, and authorised a survey to precisely establish the length of the meridian. In July 1792, the committee proposed the names \"metre\", \"are\", \"litre\" and \"grave\" for the units of length, area, capacity, and mass, respectively. The committee also proposed that multiples and submultiples of these units were to be denoted by decimal-based prefixes such as \"centi\" for a hundredth and \"kilo\" for a thousand.\nLater, during the process of adoption of the metric system, the Latin \"gramme\" and \"kilogramme\", replaced the former provincial terms \"gravet\" (1/1000 \"grave\") and \"grave\". In June 1799, based on the results of the meridian survey, the standard \"mètre des Archives\" and \"kilogramme des Archives\" were deposited in the French National Archives. Subsequently, that year, the metric system was adopted by law in France. \n\nDuring the first half of the 19th century there was little consistency in the choice of preferred multiples of the base units: typically the myriametre ( metres) was in widespread use in both France and parts of Germany, while the kilogram ( grams) rather than the myriagram was used for mass.\n\nIn 1832, the German mathematician Carl Friedrich Gauss, assisted by Wilhelm Weber, implicitly defined the second as a base unit when he quoted the Earth's magnetic field in terms of millimetres, grams, and seconds. Prior to this, the strength of the Earth's magnetic field had only been described in relative terms. The technique used by Gauss was to equate the torque induced on a suspended magnet of known mass by the Earth's magnetic field with the torque induced on an equivalent system under gravity. The resultant calculations enabled him to assign dimensions based on mass, length and time to the magnetic field.\n\nA candlepower as a unit of illuminance was originally defined by an 1860 English law as the light produced by a pure spermaceti candle weighing pound (76 grams) and burning at a specified rate. Spermaceti, a waxy substance found in the heads of sperm whales, was once used to make high-quality candles. At this time the French standard of light was based upon the illumination from a Carcel oil lamp. The unit was defined as that illumination emanating from a lamp burning pure rapeseed oil at a defined rate. It was accepted that ten standard candles were about equal to one Carcel lamp.\n\nA French-inspired initiative for international cooperation in metrology led to the signing in 1875 of the Metre Convention, also called Treaty of the Metre, by 17 nations. Initially the convention only covered standards for the metre and the kilogram. In 1921, the Metre Convention was extended to include all physical units, including the ampere and others thereby enabling the CGPM to address inconsistencies in the way that the metric system had been used.\n\nA set of 30 prototypes of the metre and 40 prototypes of the kilogram, in each case made of a 90% platinum-10% iridium alloy, were manufactured by British metallurgy specialty firm and accepted by the CGPM in 1889. One of each was selected at random to become the International prototype metre and International prototype kilogram that replaced the \"mètre des Archives\" and \"kilogramme des Archives\" respectively. Each member state was entitled to one of each of the remaining prototypes to serve as the national prototype for that country.\n\nThe treaty also established a number of international organisations to oversee the keeping of international standards of measurement:\n\nIn the 1860s, James Clerk Maxwell, William Thomson (later Lord Kelvin) and others working under the auspices of the British Association for the Advancement of Science, built on Gauss' work and formalised the concept of a coherent system of units with base units and derived units christened the centimetre–gram–second system of units in 1874. The principle of coherence was successfully used to define a number of units of measure based on the CGS, including the erg for energy, the dyne for force, the barye for pressure, the poise for dynamic viscosity and the stokes for kinematic viscosity.\n\nIn 1879, the CIPM published recommendations for writing the symbols for length, area, volume and mass, but it was outside its domain to publish recommendations for other quantities. Beginning in about 1900, physicists who had been using the symbol \"μ\" (mu) for \"micrometre\" or \"micron\", \"λ\" (lambda) for \"microlitre\", and \"γ\" (gamma) for \"microgram\" started to use the symbols \"μm\", \"μL\" and \"μg\".\n\nAt the close of the 19th century three different systems of units of measure existed for electrical measurements: a CGS-based system for electrostatic units, also known as the Gaussian or ESU system, a CGS-based system for electromechanical units (EMU) and an International system based on units defined by the Metre Convention. for electrical distribution systems. \nAttempts to resolve the electrical units in terms of length, mass, and time using dimensional analysis was beset with difficulties—the dimensions depended on whether one used the ESU or EMU systems. This anomaly was resolved in 1901 when Giovanni Giorgi published a paper in which he advocated using a fourth base unit alongside the existing three base units. The fourth unit could be chosen to be electric current, voltage, or electrical resistance. Electric current with named unit 'ampere' was chosen as the base unit, and the other electrical quantities derived from it according to the laws of physics. This became the foundation of the MKS system of units.\n\nIn the late 19th and early 20th centuries, a number of non-coherent units of measure based on the gram/kilogram, centimetre/metre and second, such as the \"Pferdestärke\" (metric horsepower) for power, the darcy for permeability and \"millimetres of mercury\" for barometric and blood pressure were developed or propagated, some of which incorporated standard gravity in their definitions.\n\nAt the end of the Second World War, a number of different systems of measurement were in use throughout the world. Some of these systems were metric system variations; others were based on customary systems of measure, like the U.S customary system and Imperial system of the UK and British Empire.\n\nIn 1948, the 9th CGPM commissioned a study to assess the measurement needs of the scientific, technical, and educational communities and \"to make recommendations for a single practical system of units of measurement, suitable for adoption by all countries adhering to the Metre Convention\". This working document was \"Practical system of units of measurement\". Based on this study, the 10th CGPM in 1954 defined an international system derived from six base units including units of temperature and optical radiation in addition to those for the MKS system mass, length, and time units and Giorgi's current unit. Six base units were recommended: the metre, kilogram, second, ampere, degree Kelvin, and candela.\n\nThe 9th CGPM also approved the first formal recommendation for the writing of symbols in the metric system when the basis of the rules as they are now known was laid down. These rules were subsequently extended and now cover unit symbols and names, prefix symbols and names, how quantity symbols should be written and used and how the values of quantities should be expressed.\n\nIn 1960, the 11th CGPM synthesised the results of the 12-year study into a set of 16 resolutions. The system was named the \"International System of Units\", abbreviated SI from the French name, .\n\n\n\n\n"}
{"id": "27655523", "url": "https://en.wikipedia.org/wiki?curid=27655523", "title": "Job demands-resources model", "text": "Job demands-resources model\n\nThe job demands-resources model or JD-R model is an occupational stress model that suggests strain is a response to imbalance between demands on the individual and the resources he or she has to deal with those demands.\n\nThe JD-R was introduced as an alternative to other models of employee well-being, such as the demand-control model and the effort-reward imbalance model. The authors of the JD-R model argue that these models \"have been restricted to a given and limited set of predictor variables that may not be relevant for all job positions\" (p. 309) Therefore, the JD-R incorporates a wide range of working conditions into the analyses of organizations and employees. Furthermore, instead of focusing solely on negative outcome variables (e.g., burnout, ill health, and repetitive strain) the JD-R model includes both negative and positive indicators and outcomes of employee well-being.\n\nThe JD-R model can be summarized with a short list of assumptions/premises:\n\n\n\nThe JD-R model assumes that whereas every occupation may have its own specific working characteristics, these characteristics can be classified in two general categories (i.e. job demands and job resources), thus constituting an overarching model that may be applied to various occupational settings, irrespective of the particular demands and resources involved. The central assumption of the JD-R model is that job strain develops – irrespective of the type of job or occupation – when (certain) job demands are high and when (certain) job resources are limited. In contrast, work engagement is most likely when job resources are high (also in the face of high job demands). This implies that the JD-R model can be used as a tool for human resource management.\n\nThe most recent article written by the authors of the original JD-R paper proposes that the interactions of demands and resources are nuanced and not clearly understood. Here Bakker and Demerouti suggest that demands may sometimes actually have a positive influence on the employee, by providing a challenge to be overcome rather than an insurmountable obstacle. In this same article, the authors describe a cumulative effect of demands and resources in their suggestion of gain and loss spirals. They conclude that these issues and that of workplace aggression may all be part of the JD-R framework.\n\n"}
{"id": "2593764", "url": "https://en.wikipedia.org/wiki?curid=2593764", "title": "Mancuerda", "text": "Mancuerda\n\nMancuerda was a method of torture. A tight cord was wound around the arms of the condemned. The executioner would then throw his entire weight backwards, or the pressure would be exerted by a lever. \n\nThe cord cut through skin and muscle directly to the bone. Additional pain was produced by the fact that the body of the prisoner was stretched as in a rack, and the belt or girdle attached to the waist also contributed further to the suffering.\n\nThe procedure was repeated six or eight times, on different parts of the arms. People subjected to such torture usually fainted from its effects.\n\nGiovanni II Bentivoglio, tyrant of Bologna, is said to have subjected the astrologer Luca Gaurico to this method of torture after he was unhappy with a prediction that Gaurico had made. \n\n"}
{"id": "871169", "url": "https://en.wikipedia.org/wiki?curid=871169", "title": "Marty McSorley", "text": "Marty McSorley\n\nMartin James McSorley (born May 18, 1963) is a Canadian former professional hockey player, who played in the National Hockey League from 1983 until 2000. A versatile player, he was able to play both the forward and defense positions.\n\nA former head coach of the Springfield Falcons of the American Hockey League (2002–04), aside from his hockey career, McSorley has worked as an actor, appearing in several film and television roles. McSorley was a valued teammate of Wayne Gretzky during their years playing together for the Edmonton Oilers and Los Angeles Kings, where he served as an enforcer. In 2000, his on-ice assault of Donald Brashear with his stick, in which Brashear suffered a severe concussion, led to McSorley's suspension and eventual retirement from the NHL.\n\nMcSorley was born in Hamilton, Ontario, but grew up near Cayuga, Haldimand County, Ontario. He made his NHL debut in October 1983 with the Pittsburgh Penguins, but rose to fame after a trade in September 1985 brought him to the Edmonton Oilers. His arrival and physical presence soon made Edmonton's incumbent enforcer Dave Semenko expendable, and McSorley inherited the title of \"Wayne Gretzky's bodyguard\".\n\nThis title would follow him to Los Angeles in 1988, when both he and Gretzky, along with Mike Krushelnyski, were obtained by the Kings. With the Kings, McSorley's bruising style made him a fan favorite; but he strove to improve his game beyond being primarily known as an enforcer, earning great respect around the league for his hard work ethic, his fine team play, and his articulate intelligence off the ice.\n\nIn the 1992–93 NHL regular season, McSorley led all defensemen in shorthanded goals with three.\n\nThe Kings reached the 1993 Stanley Cup Finals against the Montreal Canadiens, but in Game 2 with the Kings up 2–1, McSorley was caught with an illegal stick, contributing to the Canadiens game-tying goal. Montreal ending up winning that game in overtime and ultimately took the series in five games. McSorley otherwise had ten points in the playoffs, and was the only King to score during the final game. Some suggested that he was the second most dominant King after Gretzky in the playoffs.\n\nMcSorley remained with the Kings until an August 1993 trade sent him to Pittsburgh in exchange for offensive forward Shawn McEachern; however, his stay in Pittsburgh would be brief (only 47 games). The Kings re-acquired him on February 16, 1994. Back with the Kings, he assisted on Gretzky's goal which broke Gordie Howe's all-time goal-scoring record. On March 14, 1996, McSorley left the Kings' organization for good, traded to the New York Rangers as part of a multi-player deal.\n\nAfter completing the 1995–96 season with the Rangers, McSorley returned to the West Coast after being acquired by the San Jose Sharks in August 1996. He spent two injury-plagued seasons with the Sharks before returning to Edmonton as a free agent in October 1998. Confined to a part-time role in his second stint in Edmonton, he left after one season and signed with the Boston Bruins in December 1999. As a Bruin, his NHL career would come to a sudden and infamous end in a game against the Vancouver Canucks on February 21, 2000.\n\nIn a game between the Bruins and the Canucks in Vancouver on February 21, 2000, McSorley swung his stick and hit Donald Brashear in the head with 4.6 seconds left in the game. Brashear fell backwards and hit his head hard on the ice, losing consciousness and suffering a Grade III concussion. McSorley was charged with assault and suspended by the NHL for the remainder of the 1999–2000 season (including the playoffs) missing 23 games. On October 6, 2000, Judge William Kitchen of the Provincial Court of British Columbia found him guilty of assault with a weapon for his attack on Brashear. He was sentenced to 18 months probation. The trial was the first for an on-ice attack by an NHL player since Dino Ciccarelli's 1988 trial.\n\nAfter his assault conviction, his NHL suspension was extended to one full year through February 21, 2001. McSorley would never play in another NHL game.\n\nDuring his suspension, he attempted to continue playing hockey in the United Kingdom with the London Knights, where his elder brother Chris was coaching, but this move was blocked by the International Ice Hockey Federation, in deference to the NHL suspension. A similar intention to play in Germany for the Munich Barons also failed, but he then played for the Grand Rapids Griffins in their final IHL season, dressing for 14 games.\n\nIn the autumn of 2001, following the completion of his suspension, McSorley again looked towards the other side of the Atlantic. He considered purchasing the then struggling Cardiff Devils team with his brother, in order to pursue a new player-coach role and to develop interest in the sport in the UK.\n\nMcSorley appeared as a guest player for both Great Britain and the Cardiff Devils during a series of games in November 2001, but the business deal failed to materialise.\n\nMcSorley coached the American Hockey League team the Springfield Falcons between 2002 and 2004.\n\nFrom 1995 to 1997, McSorley also appeared in four movies in small roles: \"Bad Boys\" (1995), \"Forget Paris\" (1995), \"Con Air\" (1997) and \"Do Me A Favor\" (1997).\n\nDuring the 2005–06 NHL season, McSorley worked for Fox Sports West in Los Angeles, providing in-studio analysis of games involving the Los Angeles Kings or the Mighty Ducks of Anaheim. He provided color commentary for the San Jose Sharks games on FSN Bay Area during 2006–07 NHL season. McSorley's time in that role ended mysteriously midway through the Sharks playoff series with Detroit, when the Sharks announced McSorley would not return for a Game 3 broadcast for personal reasons. No further explanation was given.\n\nHe appeared in one episode of \"CSI: Miami\" in 2005 as rink manager Andrew Greven. On July 30, 2007, McSorley guest starred on ABC Family's \"Greek\" as himself playing a hockey goaltender. In February 2008, McSorley was featured as one of the pros on \"Pros vs Joes\" on Spike TV.\n\nCanadian singer-songwriter Kathleen Edwards referred to McSorley in her song \"I Make the Dough, You Get the Glory\", with the lyric, \"You're the Great One, I'm Marty McSorley..., I make the dough, but you get the glory.\" McSorley appears in the song's music video.\n\nMcSorley is currently a TV analyst for Sportsnet and occasionally \"Hockey Night in Canada\". He is a regular at Staples Center during Kings hockey games.\n\nMcSorley currently resides in Hermosa Beach, California. He married beach volleyball player Leanne Schuster in August 2002. They have three children.\n\n\n\n\n"}
{"id": "47620343", "url": "https://en.wikipedia.org/wiki?curid=47620343", "title": "Maucha diagram", "text": "Maucha diagram\n\nA Maucha diagram, or Maucha symbol, is a graphical representation of the major cations and anions in a chemical sample. R. Maucha published the symbol in 1932.\n\nIt is mainly used by biologists and chemists for quickly recognising samples by their chemical composition. The symbol is similar in concept to the Stiff diagram. It conveys similar ionic information to the Piper diagram, though in a more compact format that is suitable as a map symbol or for showing changes with time. The Maucha diagram is a special case of the Radar chart and overcomes some of the limitations of the Pie chart by having equal angles for all variables and consistently showing each variable in the same position.\n\nThe star shape comprises eight kite-shaped polygons, the area of each of which is proportional to the concentration of an ion in milliequivalents per litre. The anions carbonate, bicarbonate, chloride and sulphate are on the left, while the cations potassium, sodium, calcium and magnesium are on the right. The total ionic concentration adds up to the area of the background circle, the total anion concentration adds up to the left semicircle and the total cation concentration adds up to the right semicircle. A method for drawing the diagram in R is available on GitHub .\n\nBroch and Yake modified Maucha's original fixed-size diagram by scaling for concentration.\n\nFurther scaling using the logarithm of the ionic concentration enables the plotting of a wide range of concentrations on a single map.\n"}
{"id": "305303", "url": "https://en.wikipedia.org/wiki?curid=305303", "title": "Multiset", "text": "Multiset\n\nIn mathematics, a multiset (aka bag or mset) is a modification of the concept of a set that, unlike a set, allows for multiple instances for each of its elements. The positive integer number of instances, given for each element is called the multiplicity of this element in the multiset. As a consequence, an infinite number of multisets exist, which contain only elements and , but vary by the multiplicity of their elements:\n\nThese objects are all different, when viewed as multisets, although they are the same set, since they all consist of the same elements. As with sets, and in contrast to tuples, order does not matter in discriminating multisets, so and denote the same multiset. To distinguish between sets and multisets, a notation that incorporates square brackets is sometimes used: the multiset can be denoted as .\n\nThe cardinality of a multiset is constructed by summing up the multiplicities of all its elements. For example, in the multiset the multiplicities of the members , , and are respectively 2, 3, and 1, and therefore the cardinality of this multiset is 6.\n\nNicolaas Govert de Bruijn coined the word \"multiset\" in the 1970s, according to Donald Knuth.\nHowever, the use of the concept for multisets predates the coinage of word \"multiset\" by many centuries. Knuth himself attributes the first study of multisets to the Indian mathematician Bhāskarāchārya, who described permutations of multisets around 1150. Knuth also lists other names that were proposed or used for this concept, including \"list\", \"bunch\", \"bag\", \"heap\", \"sample\", \"weighted set\", \"collection\", and \"suite\".\n\nWayne Blizard traced multisets back to the very origin of numbers, arguing that “in ancient times, the number \"n\" was often represented by a collection of \"n\" strokes, tally marks, or units.” These and similar collections of objects are multisets, because strokes, tally marks, or units are considered indistinguishable. This shows that people implicitly used multisets even before mathematics emerged.\n\nPractical needs for this structure have caused multisets to be rediscovered several times, appearing in literature under different names. For instance, they were important in early AI languages, such as QA4, where they were referred to as \"bags,\" a term attributed to Peter Deutsch. A multiset has been also called an aggregate, heap, bunch, sample, weighted set, occurrence set, and fireset (finitely repeated element set).\n\nAlthough multisets were used implicitly from ancient times, their explicit exploration happened much later. The first known study of multisets is attributed to the Indian mathematician Bhāskarāchārya circa 1150, who described permutations of multisets. The work of Marius Nizolius (1498–1576) contains another early reference to the concept of multisets. Athanasius Kircher found the number of multiset permutations when one element can be repeated. Jean Prestet published a general rule for multiset permutations in 1675. John Wallis explained this rule in more detail in 1685.\n\nMultisets appeared explicitly in the work of Richard Dedekind.\n\nOther mathematicians formalized multisets and began to study them as precise mathematical structures in the 20th century. For example, Whitney (1933) described \"generalized sets\" (\"sets\" whose characteristic functions may take any integer value - positive, negative or zero). Monro (1987) investigated the category Mul of multisets and their morphisms, defining a \"multiset\" as a set with an equivalence relation between elements \"of the same \"sort\"\", and a \"morphism\" between multisets as a function which respects \"sorts\". He also introduced a \"multinumber\": a function \"f(x)\" from a multiset to the natural numbers, giving the \"multiplicity\" of element \"x\" in the multiset. Monro argued that the concepts of multiset and multinumber are often mixed indiscriminately, though both are useful.\n\nOne of the simplest and most natural examples is the multiset of prime factors of a number . Here the underlying set of elements is the set of prime divisors of . For example, the number 120 has the prime factorization\nwhich gives the multiset .\n\nA related example is the multiset of solutions of an algebraic equation. A quadratic equation, for example, has two solutions. However, in some cases they are both the same number. Thus the multiset of solutions of the equation could be , or it could be . In the latter case it has a solution of multiplicity 2. More generally, the fundamental theorem of algebra asserts that the complex solutions of a polynomial equation of degree always form a multiset of cardinality .\n\nA special case of the above are the eigenvalues of a matrix, if these are defined as the multiset of roots of its characteristic polynomial. However a choice is made here: the (usual) definition of eigenvalues does not refer to the characteristic polynomial, and this give rise to another notion of multiplicity, and therefore to another multiset. The geometric multiplicity of an eigenvalue of a matrix is the dimension of the kernel of ; the geometric multiplicity of an eigenvalue is not larger and often smaller than its \"algebraic multiplicity\", which is the multiplicity as a root of the characteristic polynomial. The geometric multiplicity of an eigenvalue is its multiplicity as a root of the minimal polynomial of . Therefore, the multiset of eigenvalues with geometric multiplicities is the multiset of roots of the minimal polynomial.\n\nA multiset may be formally defined as a 2-tuple where is the \"underlying set\" of the multiset, formed from its distinct elements, and formula_2 is a function from to the set of the positive integers, giving the \"multiplicity\", that is, the number of occurrences, of the element in the multiset as the number .\n\nRepresenting the function by its graph, that is the set of ordered pairs formula_3 allows for writing the multiset as , and the multiset as . This notation is however not commonly used and more compact notations are employed.\n\nIf formula_4 is a finite set, the multiset is often represented as \n\nwhere upper indices equal to 1 are omitted. For example, the multiset may be written formula_7 or formula_8 If the elements of the multiset are numbers, a confusion is possible with ordinary arithmetic operations, those normally can be excluded from the context. On the other hand, the latter notation is coherent with the fact that the prime factorization of a positive integer is a uniquely defined multiset, as asserted by the fundamental theorem of arithmetic. Also, a monomial is a multiset of indeterminates.\n\nA multiset corresponds to an ordinary set if the multiplicity of every element is one (as opposed to some larger natural number). An indexed family, , where varies over some index-set \"I\", may define a multiset, sometimes written . In this view the underlying set of the multiset is given by the image of the family, and the multiplicity of any element is the number of index values such that formula_9. In this article the multiplicities are considered to be finite, i.e. no element occurs infinitely many times in the family: even in an infinite multiset, the multiplicities are finite numbers.\n\nIt is possible to extend the definition of a multiset by allowing multiplicities of individual elements to be infinite cardinals instead of natural numbers, but not all properties carry over to this generalization.\n\nElements of a multiset are generally taken in a fixed set , sometimes called a \"universe\", which is typically the set of natural numbers. An element of that does not belong to a given multiset is said to have a multiplicity 0 in this multiset. This extends the multiplicity function of the multiset to a function from to the set formula_10 of nonnegative integers. This defines a one to one correspondence between these functions and the multisets that have their elements in .\n\nThis extended multiplicity function is commonly called simply the multiplicity function, and suffices for defining multisets, when the universe containing the elements has been fixed. This multiplicity function is a generalization of the indicator function of a subset, and shares some properties with it.\n\nThe support of a multiset formula_11 in a universe is the underlying set of the multiset. Using the multiplicity function formula_12, it is characterized as \n\nA multiset is \"finite\" if its support is finite, or, equivalently, if its cardinality\nis finite. The \"empty multiset\" is the unique multiset with an empty support (underlying set), and thus a cardinality 0.\n\nThe usual operations of sets may be extended to multisets by using the multiplicity function, in a similar way as using the indicator function for subsets. In the following, and are multisets in a given universe , with multiplicity functions formula_15 and formula_16\n\n\nTwo multisets are \"disjoint\" if their supports are disjoint sets. This is equivalent to saying that their intersection is the empty multiset or that their sum equals their union.\n\nThe number of multisets of cardinality , with elements taken from a finite set of cardinality , is called the multiset coefficient or multiset number. This number is written by some authors as formula_21, a notation that is meant to resemble that of binomial coefficients; it is used for instance in (Stanley, 1997), and could be pronounced \" multichoose \" to resemble \" choose \" for formula_22. Unlike for binomial coefficients, there is no \"multiset theorem\" in which multiset coefficients would occur, and they should not be confused with the unrelated multinomial coefficients that occur in the multinomial theorem.\n\nThe value of multiset coefficients can be given explicitly as\nwhere the second expression is as a binomial coefficient; many authors in fact avoid separate notation and just write binomial coefficients. So, the number of such multisets is the same as the number of subsets of cardinality in a set of cardinality . The analogy with binomial coefficients can be stressed by writing the numerator in the above expression as a rising factorial power\nto match the expression of binomial coefficients using a falling factorial power:\n\nThere are for example 4 multisets of cardinality 3 with elements taken from the set of cardinality 2 ( = 2, = 3), namely , , , . There are also 4 subsets of cardinality 3 in the set of cardinality 4 (), namely , , , .\n\nOne simple way to prove the equality of multiset coefficients and binomial coefficients given above, involves representing multisets in the following way. First, consider the notation for multisets that would represent (6 s, 2 s, 3 s, 7 s) in this form:\n\nThis is a multiset of cardinality = 18 made of elements of a set of cardinality = 4. The number of characters including both dots and vertical lines used in this notation is 18 + 4 − 1. The number of vertical lines is 4 − 1. The number of multisets of cardinality 18 is then the number of ways to arrange the 4 − 1 vertical lines among the 18 + 4 − 1 characters, and is thus the number of subsets of cardinality 4 − 1 in a set of cardinality 18 + 4 − 1. Equivalently, it is the number of ways to arrange the 18 dots among the 18 + 4 − 1 characters, which is the number of subsets of cardinality 18 of a set of cardinality 18 + 4 − 1. This is\nthus is the value of the multiset coefficient and its equivalencies:\n\nOne may define a generalized binomial coefficient\nin which is not required to be a nonnegative integer, but may be negative or a non-integer, or a non-real complex number. (If  = 0, then the value of this coefficient is 1 because it is the empty product.) Then the number of multisets of cardinality in a set of cardinality is\n\nA recurrence relation for multiset coefficients may be given as\nwith\n\nThe above recurrence may be interpreted as follows.\nLet  := formula_36 be the source set. There is always exactly one (empty) multiset of size 0, and if  = 0 there are no larger multisets, which gives the initial conditions.\n\nNow, consider the case in which  > 0. A multiset of cardinality with elements from might or might not contain any instance of the final element . If it does appear, then by removing once, one is left with a multiset of cardinality  − 1 of elements from , and every such multiset can arise, which gives a total of\n\nIf does not appear, then our original multiset is equal to a multiset of cardinality with elements from , of which there are\n\nThus,\n\nThe generating function of the multiset coefficients is very simple, being\nAs multisets are in one to one correspondence with monomials, formula_41 is also the number of monomials of degree in indeterminates. Thus, above series is also the Hilbert series of the polynomial ring formula_42\n\nAs formula_41 is a polynomial in , it is defined for any complex value of .\n\nThe multiplicative formula allows the definition of multiset coefficients to be extended by replacing \"n\" by an arbitrary number \"α\" (negative, real, complex):\n\nWith this definition one has a generalization of the negative binomial formula (with one of the variables set to 1), which justifies calling the formula_45 negative binomial coefficients:\n\nThis Taylor series formula is valid for all complex numbers \"α\" and \"X\" with  < 1. It can also be interpreted as an identity of formal power series in \"X\", where it actually can serve as definition of arbitrary powers of series with constant coefficient equal to 1; the point is that with this definition all identities hold that one expects for exponentiation, notably\n\nand formulas such as these can be used to prove identities for the multiset coefficients.\n\nIf \"α\" is a nonpositive integer \"n\", then all terms with \"k\" > −\"n\" are zero, and the infinite series becomes a finite sum. However, for other values of \"α\", including positive integers and rational numbers, the series is infinite.\n\nMultisets have various applications. They are becoming fundamental in combinatorics. Multisets have become an important tool in database theory, which often uses the synonym \"bag\". For instance, multisets are often used to implement relations in database systems. Multisets also play an important role in computer science.\n\nThere are also other applications. For instance, Richard Rado used multisets as a device to investigate the properties of families of sets. He wrote, \"The notion of a set takes no account of multiple occurrence of any one of its members, and yet it is just this kind of information which is frequently of importance. We need only think of the set of roots of a polynomial f(x) or the spectrum of a linear operator.\"\n\nDifferent generalizations of multisets have been introduced, studied and applied to solving problems.\n\n"}
{"id": "10432412", "url": "https://en.wikipedia.org/wiki?curid=10432412", "title": "Natural resource management", "text": "Natural resource management\n\nNatural resource management refers to the management of natural resources such as land, water, soil, plants and animals, with a particular focus on how management affects the quality of life for both present and future generations (stewardship).\n\nNatural resource management deals with managing the way in which people and natural landscapes interact. It brings together land use planning, water management, biodiversity conservation, and the future sustainability of industries like agriculture, mining, tourism, fisheries and forestry. It recognises that people and their livelihoods rely on the health and productivity of our landscapes, and their actions as stewards of the land play a critical role in maintaining this health and productivity.\n\nNatural resource management specifically focuses on a scientific and technical understanding of resources and ecology and the life-supporting capacity of those resources. Environmental management is also similar to natural resource management. In academic contexts, the sociology of natural resources is closely related to, but distinct from, natural resource management.\n\nThe emphasis on sustainability can be traced back to early attempts to understand the ecological nature of North American rangelands in the late 19th century, and the resource conservation movement of the same time. This type of analysis coalesced in the 20th century with recognition that preservationist conservation strategies had not been effective in halting the decline of natural resources. A more integrated approach was implemented recognising the intertwined social, cultural, economic and political aspects of resource management. A more holistic, national and even global form evolved, from the Brundtland Commission and the advocacy of sustainable development.\n\nIn 2005 the government of New South Wales, established a \"Standard for Quality Natural Resource Management\", to improve the consistency of practice, based on an adaptive management approach.\n\nIn the United States, the most active areas of natural resource management are wildlife management often associated with ecotourism and rangeland management. In Australia, water sharing, such as the Murray Darling Basin Plan and catchment management are also significant.\n\nNatural resource management approaches can be categorised according to the kind and right of stakeholders, natural resources:\n\n\n\n\n\n\nStakeholder analysis originated from business management practices and has been incorporated into natural resource management in ever growing popularity. Stakeholder analysis in the context of natural resource management identifies distinctive interest groups affected in the utilisation and conservation of natural resources.\n\nThere is no definitive definition of a stakeholder as illustrated in the table below. Especially in natural resource management as it is difficult to determine who has a stake and this will differ according to each potential stakeholder.\n\nDifferent approaches to who is a stakeholder:\nTherefore, it is dependent upon the circumstances of the stakeholders involved with natural resource as to which definition and subsequent theory is utilised.\n\nBillgrena and Holme identified the aims of stakeholder analysis in natural resource management:\n\n\nThis gives transparency and clarity to policy making allowing stakeholders to recognise conflicts of interest and facilitate resolutions.\nThere are numerous stakeholder theories such as Mitchell et al. however Grimble created a framework of stages for a Stakeholder Analysis in natural resource management. Grimble designed this framework to ensure that the analysis is specific to the essential aspects of natural resource management.\n\nStages in Stakeholder analysis:\n\nApplication:\n\nGrimble and Wellard established that Stakeholder analysis in natural resource management is most relevant where issued can be characterised as;\n\n\nCase studies:\n\nIn the case of the Bwindi Impenetrable National Park, a comprehensive stakeholder analysis would have been relevant and the Batwa people would have potentially been acknowledged as stakeholders preventing the loss of people's livelihoods and loss of life.\n\nNepal, Indonesia and Koreas' community forestry are successful examples of how stakeholder analysis can be incorporated into the management of natural resources. This allowed the stakeholders to identify their needs and level of involvement with the forests.\n\nCriticisms:\n\n\nAlternatives/ Complementary forms of analysis:\n\n\nNatural resource management issues are inherently complex. They involve the ecological cycles, hydrological cycles, climate, animals, plants and geography, etc. All these are dynamic and inter-related. A change in one of them may have far reaching and/or long term impacts which may even be irreversible. In addition to the natural systems, natural resource management also has to manage various stakeholders and their interests, policies, politics, geographical boundaries, economic implications and the list goes on. It is a very difficult to satisfy all aspects at the same time. This results in conflicting situations.\n\nAfter the United Nations Conference for the Environment and Development (UNCED) held in Rio de Janeiro in 1992, most nations subscribed to new principles for the integrated management of land, water, and forests. Although program names vary from nation to nation, all express similar aims.\n\nThe various approaches applied to natural resource management include:\n\n\nThe community-based natural resource management (CBNRM) approach combines conservation objectives with the generation of economic benefits for rural communities. The three key assumptions being that: locals are better placed to conserve natural resources, people will conserve a resource only if benefits exceed the costs of conservation, and people will conserve a resource that is linked directly to their quality of life. When a local people's quality of life is enhanced, their efforts and commitment to ensure the future well-being of the resource are also enhanced. Regional and community based natural resource management is also based on the principle of subsidiarity.\n\nThe United Nations advocates CBNRM in the Convention on Biodiversity and the Convention to Combat Desertification. Unless clearly defined, decentralised NRM can result an ambiguous socio-legal environment with local communities racing to exploit natural resources while they can e.g. forest communities in central Kalimantan (Indonesia).\n\nA problem of CBNRM is the difficulty of reconciling and harmonising the objectives of socioeconomic development, biodiversity protection and sustainable resource utilisation. The concept and conflicting interests of CBNRM, show how the motives behind the participation are differentiated as either people-centred (active or participatory results that are truly empowering) or planner-centred (nominal and results in passive recipients). Understanding power relations is crucial to the success of community based NRM. Locals may be reluctant to challenge government recommendations for fear of losing promised benefits.\n\nCBNRM is based particularly on advocacy by nongovernmental organizations working with local groups and communities, on the one hand, and national and transnational organizations, on the other, to build and extend new versions of environmental and social advocacy that link social justice and environmental management agendas with both direct and indirect benefits observed including a share of revenues, employment, diversification of livelihoods and increased pride and identity. Ecological and societal successes and failures of CBNRM projects have been documented. CBNRM has raised new challenges, as concepts of community, territory, conservation, and indigenous are worked into politically varied plans and programs in disparate sites. Warner and Jones address strategies for effectively managing conflict in CBNRM.\n\nThe capacity of indigenous communities to conserve natural resources has been acknowledged by the Australian Government with the Caring for Country Program. Caring for our Country is an Australian Government initiative jointly administered by the Australian Government Department of Agriculture, Fisheries and Forestry and the Department of the Environment, Water, Heritage and the Arts. These Departments share responsibility for delivery of the Australian Government's environment and sustainable agriculture programs, which have traditionally been broadly referred to under the banner of ‘natural resource management’. These programs have been delivered regionally, through 56 State government bodies, successfully allowing regional communities to decide the natural resource priorities for their regions.\n\nMore broadly, a research study based in Tanzania and the Pacific researched what motivates communities to adopt CBNRM's and found that aspects of the specific CBNRM program, of the community that has adopted the program, and of the broader social-ecological context together shape the why CBNRM's are adopted. However, overall, program adoption seemed to mirror the relative advantage of CBNRM programs to local villagers and villager access to external technical assistance. There have been socioeconomic critiques of CBNRM in Africa, but ecological effectiveness of CBNRM measured by wildlife population densities has been shown repeatedly in Tanzania. \n\nGovernance is seen as a key consideration for delivering community-based or regional natural resource management. In the State of NSW, the 13 catchment management authorities (CMAs) are overseen by the Natural Resources Commission (NRC), responsible for undertaking audits of the effectiveness of regional natural resource management programs.\n\nThe primary methodological approach adopted by catchment management authorities (CMAs) for regional natural resource management in Australia is adaptive management.\n\nThis approach includes recognition that adaption occurs through a process of ‘plan-do-review-act’. It also recognises seven key components that should be considered for quality natural resource management practice:\n\nIntegrated natural resource management (INRM) is a process of managing natural resources in a systematic way, which includes multiple aspects of natural resource use (biophysical, socio-political, and economic) meet production goals of producers and other direct users (e.g., food security, profitability, risk aversion) as well as goals of the wider community (e.g., poverty alleviation, welfare of future generations, environmental conservation). It focuses on sustainability and at the same time tries to incorporate all possible stakeholders from the planning level itself, reducing possible future conflicts. The conceptual basis of INRM has evolved in recent years through the convergence of research in diverse areas such as sustainable land use, participatory planning, integrated watershed management, and adaptive management. INRM is being used extensively and been successful in regional and community based natural management.\n\nThere are various frameworks and computer models developed to assist natural resource management.\n\nGeographic Information Systems (GIS)\n\nGIS is a powerful analytical tool as it is capable of overlaying datasets to identify links. A bush regeneration scheme can be informed by the overlay of rainfall, cleared land and erosion. In Australia, Metadata Directories such as NDAR provide data on Australian natural resources such as vegetation, fisheries, soils and water. These are limited by the potential for subjective input and data manipulation.\n\nNatural Resources Management Audit Frameworks\n\nThe NSW Government in Australia has published an audit framework for natural resource management, to assist the establishment of a performance audit role in the governance of regional natural resource management. This audit framework builds from other established audit methodologies, including performance audit, environmental audit and internal audit. Audits undertaken using this framework have provided confidence to stakeholders, identified areas for improvement and described policy expectations for the general public.\n\nThe Australian Government has established a framework for auditing greenhouse emissions and energy reporting, which closely follows Australian Standards for Assurance Engagements.\n\nThe Australian Government is also currently preparing an audit framework for auditing water management, focussing on the implementation of the Murray Darling Basin Plan.\n\n\nThe issue of biodiversity conservation is regarded as an important element in natural resource management. What is biodiversity? Biodiversity is a comprehensive concept, which is a description of the extent of natural diversity. Gaston and Spicer (p. 3) point out that biodiversity is \"the variety of life\" and relate to different kinds of \"biodiversity organization\". According to Gray (p. 154), the first widespread use of the definition of biodiversity, was put forward by the United Nations in 1992, involving different aspects of biological diversity.\n\n\nThe \"threats\" wreaking havoc on biodiversity include; habitat fragmentation, putting a strain on the already stretched biological resources; forest deterioration and deforestation; the invasion of \"alien species\" and \"climate change\"( p. 2). Since these threats have received increasing attention from environmentalists and the public, the precautionary management of biodiversity becomes an important part of natural resources management. According to Cooney, there are material measures to carry out precautionary management of biodiversity in natural resource management.\n\n\nCooney claims that the policy making is dependent on \"evidences\", relating to \"high standard of proof\", the forbidding of special \"activities\" and \"information and monitoring requirements\". Before making the policy of precaution, categorical evidence is needed. When the potential menace of \"activities\" is regarded as a critical and \"irreversible\" endangerment, these \"activities\" should be forbidden. For example, since explosives and toxicants will have serious consequences to endanger human and natural environment, the South Africa Marine Living Resources Act promulgated a series of policies on completely forbidding to \"catch fish\" by using explosives and toxicants.\n\n\nAccording to Cooney, there are 4 methods to manage the precaution of biodiversity in natural resources management;\n\n\n\nIn order to have a sustainable environment, understanding and using appropriate management strategies is important. In terms of understanding, Young emphasises some important points of land management:\n\n\nDale et al. (2000) study has shown that there are five fundamental and helpful ecological principles for the land manager and people who need them. The ecological principles relate to time, place, species, disturbance and the landscape and they interact in many ways.It is suggested that land managers could follow these guidelines:\n\n"}
{"id": "4033255", "url": "https://en.wikipedia.org/wiki?curid=4033255", "title": "Near-field (mathematics)", "text": "Near-field (mathematics)\n\nIn mathematics, a near-field is an algebraic structure similar to a division ring, except that it has only one of the two distributive laws. Alternatively, a near-field is a near-ring in which there is a multiplicative identity, and every non-zero element has a multiplicative inverse.\n\nA near-field is a set formula_1, together with two binary operations, formula_2 (addition) and formula_3 (multiplication), satisfying the following axioms:\n\n\n\nThe concept of a near-field was first introduced by Leonard Dickson in 1905. He took division rings and modified their multiplication, while leaving addition as it was, and thus produced the first known examples of near-fields that were not division rings. The near-fields produced by this method are known as Dickson near-fields; the near-field of order 9 given above is a Dickson near-field.\nHans Zassenhaus proved that all but 7 finite near-fields are either fields or Dickson near-fields.\n\nThe earliest application of the concept of near-field was in the study of geometries, such as projective geometries. Many projective geometries can be defined in terms of a coordinate system over a division ring, but others can not. It was found that by allowing coordinates from any near-ring the range of geometries which could be coordinatized was extended. For example, Marshall Hall used the near-field of order 9 given above to produce a Hall plane, the first of a sequence of such planes based on Dickson near-fields of order the square of a prime. In 1971 T. G. Room and P.B. Kirkpatrick provided an alternative development.\n\nThere are numerous other applications, mostly to geometry. A more recent application of near-fields is in the construction of ciphers for data-encryption, such as Hill ciphers.\n\nLet formula_29 be a near field. Let formula_44 be its multiplicative group and let formula_45 be its additive group. Let formula_46 act on formula_47 by formula_48. The axioms of a near field show that this is a right group action by group automorphisms of formula_45, and the nonzero elements of formula_45 form a single orbit with trivial stabilizer.\n\nConversely, if formula_51 is an abelian group and formula_52 is a subgroup of formula_53 which acts freely and transitively on the nonzero elements of formula_51, then we can define a near field with additive group formula_51 and multiplicative group formula_52. Choose an element in formula_51 to call formula_58 and let formula_59 be the bijection formula_60. Then we define addition on formula_51 by the additive group structure on formula_51 and define multiplication by formula_63.\n\nA Frobenius group can be defined as a finite group of the form formula_64 where formula_52 acts without stabilizer on the nonzero elements of formula_51. Thus, near fields are in bijection with Frobenius groups where formula_67.\n\nAs described above, Zassenhaus proved that all finite near fields either arise from a construction of Dickson or are one of seven exceptional examples. We will describe this classification by giving pairs formula_68 where formula_51 is an abelian group and formula_52 is a group of automorphisms of formula_51 which acts freely and transitively on the nonzero elements of formula_51.\n\nThe construction of Dickson proceeds as follows. Let formula_73 be a prime power and choose a positive integer formula_74 such that all prime factors of formula_74 divide formula_76 and, if formula_77, then formula_74 is not divisible by formula_79. Let formula_80 be the finite field of order formula_81 and let formula_51 be the additive group of formula_80. The multiplicative group of formula_80, together with the Frobenius automorphism formula_85 generate a group of automorphisms of formula_80 of the form formula_87, where formula_88 is the cyclic group of order formula_89. The divisibility conditions on formula_74 allow us to find a subgroup of formula_87 of order formula_92 which acts freely and transitively on formula_51. The case formula_94 is the case of commutative finite fields; the nine element example above is formula_95, formula_96.\n\nIn the seven exceptional examples, formula_51 is of the form formula_98. This table, including the numbering by Roman numerals, is taken from Zassenhaus's paper.\n\nThe binary tetrahedral, octahedral and icosahedral groups are central extensions of the rotational symmetry groups of the platonic solids; these rotational symmetry groups are formula_99, formula_100 and formula_101 respectively. formula_102 and formula_103 can also be described as formula_104 and formula_105.\n\n\n"}
{"id": "2795942", "url": "https://en.wikipedia.org/wiki?curid=2795942", "title": "Negative cutting", "text": "Negative cutting\n\nNegative cutting (also known as negative matching and negative conforming) is the process of cutting motion picture negative to match precisely the final edit as specified by the film editor. Original camera negative (OCN) is cut with scissors and joined using a film splicer and film cement. Negative cutting is part of the post-production process and occurs after editing and prior to striking internegatives and release prints. The process of negative cutting has changed little since the beginning of cinema in the early 20th century. In the early 1980s computer software was first used to aid the cutting process. Kodak introduced barcode on motion picture negative in the mid-1990s. This enabled negative cutters to more easily track shots and identify film sections based on keykode.\n\nToward the late 1990s and early 2000s negative cutting changed due to the advent of digital cinema technologies such as digital intermediate (DI), digital projection and high-definition television. In some countries, due to the high cost of online suites, negative cutting is still used for commercials by reducing footage. Increasingly feature films are bypassing the negative cutting process altogether and are being scanned directly from the uncut rushes.\n\nThe existence of digital intermediates (DI) has created a new demand for negative cutters to extract selected takes which are cut from the rushes and re-spliced into new rolls (in edit order) to reduce the volume of footage for scanning.\n\nAfter a film shoot, the original camera negative (OCN) is sent to a film laboratory for processing. Two or three camera rolls are spliced together to create a lab roll approximately long. After developing the lab roll, it is put through a telecine to create a rushes transfer tape. This rushes transfer tape is of lower quality than film and is used for editing purposes only.\n\nThe rushes tape is sent to the Editor who loads it into an offline edit suite. The lab rolls are sent to the negative cutter for logging and storage.\n\nAfter the Editor finishes the Edit it is exported to an offline EDL list and the EDL list is sent to the negative cutter. The negative cutter will translate the Timecode in the EDL list to edge numbers (keykode) using specially designed negative cutting software to find which shot is needed from the rushes negative.\n\nTraditionally a negative cutter would then fine cut the negative to match the Editor's final edit frame accurately. Negative would be spliced together to create rolls less than which would then be sent to the film laboratory to print release prints.\n\nToday most feature films are extracted full takes (as selected takes) and scanned digitally as a digital intermediate. Television series and commercials shot on film follow the same extraction process but are sent for telecine. Each required shot is extracted from the lab roll as a full take and respliced together to create a new selected roll of negative. This reduces the negative required by up to 1/10 of the footage shot, saving considerable time during scanning or telecine. The negative cutter will create a new Online EDL list replacing the rushes roll timecode with the new selected roll timecode.\n\nIn the case of feature films the selected roll and Online EDL are sent to a post production facility for scanning as a digital intermediate. For television commercials or series the selected takes and EDL are sent to a post production facility for re-telecine and compiled in an Online Suite for final grading.\n\nThere have been a number of dedicated software systems that have been developed for and by negative cutters to manage the process of cutting motion picture negative. A number of individual proprietary software systems have been developed starting in the early 1980s. Stan Sztaba developed a system for World Cinevision Services Inc (New York) in 1983 using Apple II DOS and then ProDOS, this system is still used today. Elliott Gamson of Immaculate Matching (New York) developed a system using MS-DOS. Computamatch was one of the first MS-DOS-based systems developed and is still in use today in several countries.\n\nThe first commercially available software product was OSC/R (pronounced \"Oscar\"), a DOS-based application developed in Toronto, Canada by The Adelaide Works. OSC/R was very widely used and at the time was the only negative cutting software on the market until Adelaide Works ceased operation in 1993. OSC/R is still used today in some negative cutting facilities but has been mostly replaced by newer and more advanced systems. Excalibur was a later Windows 98 based product developed by FilmLab Engineering in Britain. Film Fusion is one of the most recent developments and is a Windows XP and Vista based system developed in Sydney, Australia by Popsoft IT.\n\nNegative cutters use various hardware tools such as film synchronizers, re-winders, film splicers, scissors, film cement and film keykode readers. DigiSync, a purpose built keykode reader is used by most negative cutters in conjunction with software for logging the keykode from film. DigiSync was developed by Research In Motion and in 1998 it won a Technical Achievement Academy Award for the design and development of the DigiSync Film Keykode Reader. Research In Motion later moved on to bigger things and invented the BlackBerry Wireless Email Phone and is now a publicly listed company. Other brands of barcode scanners are also in use.\n\n"}
{"id": "31434142", "url": "https://en.wikipedia.org/wiki?curid=31434142", "title": "Numerical semigroup", "text": "Numerical semigroup\n\nIn mathematics, a numerical semigroup is a special kind of a semigroup. Its underlying set is the set of all nonnegative integers except a finite number and the binary operation is the operation of addition of integers. Also, the integer 0 must be an element of the semigroup. For example, while the set {0, 2, 3, 4, 5, 6, ...} is a numerical semigroup, the set {0, 1, 3, 5, 6, ...} is not because 1 is in the set and 1 + 1 = 2 is not in the set. Numerical semigroups are commutative monoids and are also known as numerical monoids. \n\nThe definition of numerical semigroup is intimately related to the problem of determining nonnegative integers that can be expressed in the form \"x\"\"n\" + \"x\" \"n\" + ... + \"x\" \"n\" for a given set {\"n\", \"n\", ..., \"n\"} of positive integers and for arbitrary nonnegative integers \"x\", \"x\", ..., \"x\". This problem had been considered by several mathematicians like Frobenius (1849 – 1917) and Sylvester (1814 – 1897) at the end of the 19th century. During the second half of the twentieth century, interest in the study of numerical semigroups resurfaced because of their applications in algebraic geometry.\n\nLet \"N\" be the set of nonnegative integers. A subset \"S\" of \"N\" is called a numerical semigroup if the following conditions are satisfied.\n\n\nThere is a simple method to construct numerical semigroups. Let \"A\" = {\"n\", \"n\", ..., \"n\"} be a nonempty set of positive integers. The set of all integers of the form \"x\" \"n\" + \"x\" \"n\" + ... + \"x\" \"n\" is the subset of \"N\" generated by \"A\" and is denoted by 〈 \"A\" 〉. The following theorem fully characterizes numerical semigroups.\n\nLet \"S\" be the subsemigroup of \"N\" generated by \"A\". Then \"S\" is a numerical semigroup if and only if gcd (\"A\") = 1. Moreover, every numerical semigroup arises in this way.\n\nThe following subsets of \"N\" are numerical semigroups.\n\nThe set \"A\" is a set of generators of the numerical semigroup 〈 \"A\" 〉. A set of generators of a numerical semigroup is a minimal system\nof generators if none of its proper subsets generates the numerical semigroup. It is known that \nevery numerical semigroup \"S\" has a unique minimal system of generators and also that this minimal system of generators is finite. The cardinality of the minimal set of generators is called the \"embedding dimension\" of the numerical semigroup \"S\" and is denoted by \"e\"(\"S\"). The smallest member in the minimal system of generators is called the \"multiplicity\" of the numerical semigroup \"S\" and is denoted by \"m\"(\"S\").\n\nThere are several notable numbers associated with a numerical semigroup \"S\".\n\nLet \"S\" = 〈 5, 7, 9 〉. Then we have:\nNumerical semigroups with small Frobenius number or genus\n\nThe following general results were known to Sylvester. Let \"a\" and \"b\" be positive integers such that gcd (\"a\", \"b\") = 1. Then \n\nThere is no known general formula to compute the Frobenius number of numerical semigroups having embedding dimension three or more. No polynomial formula can be found to compute the Frobenius number or genus of a numerical semigroup with embedding dimension three. Every positive integer is the Frobenius number of some numerical semigroup with embedding dimension three.\n\nThe following algorithm, known as Rödseth's algorithm,\ncan be used to compute the Frobenius number of a numerical semigroup \"S\" generated by {\"a\", \"a\", \"a\"} where \"a\" < \"a\" < \"a\" and gcd ( \"a\", \"a\", \"a\") = 1. Its worst-case complexity is not as good as Greenberg's algorithm\n\nbut it is much simpler to describe.\n\nAn \"irreducible numerical semigroup\" is a numerical semigroup such that it cannot be written as the intersection of two numerical semigroups properly containing it. A numerical semigroup \"S\" is irreducible if and only if \"S\" is maximal, with respect to set inclusion, in the collection of all numerical semigroups with Frobenius number \"F\"(\"S\"). \n\nA numerical semigroup \"S \" is \"symmetric\" if it is irreducible and its Frobenius number \"F\"(\"S\") is odd. We say that \"S\" is \"pseudo-symmetric\" provided that \"S\" is irreducible and F(S) is even. Such numerical semigroups have simple characterizations in terms of Frobenius number and genus:\n\n"}
{"id": "44903985", "url": "https://en.wikipedia.org/wiki?curid=44903985", "title": "Phenomenological model", "text": "Phenomenological model\n\nA phenomenological model is a scientific model that describes the empirical relationship of phenomena to each other, in a way which is consistent with fundamental theory, but is not directly derived from theory. In other words, a phenomenological model is not derived from first principles. A phenomenological model foregoes any attempt to explain why the variables interact the way they do, and simply attempts to describe the relationship, with the assumption that the relationship extends past the measured values. Regression analysis is sometimes used to create statistical models that serve as phenomenological models.\n\nPhenomenological models have been characterized as being completely independent of theories, though many phenomenological models, while failing to be derivable from a theory, incorporate principles and laws associated with theories. The liquid drop model of the atomic nucleus, for instance, portrays the nucleus as a liquid drop and describes it as having several properties (surface tension and charge, among others) originating in different theories (hydrodynamics and electrodynamics, respectively). Certain aspects of these theories—though usually not the complete theory—are then used to determine both the static and dynamical properties of the nucleus.\n"}
{"id": "3518067", "url": "https://en.wikipedia.org/wiki?curid=3518067", "title": "Pneumatherapy", "text": "Pneumatherapy\n\nPneumatherapy is the belief that the state of one's spirit (\"pneuma\", ) influences physical health. It is influenced by pneumatology.\n"}
{"id": "44749915", "url": "https://en.wikipedia.org/wiki?curid=44749915", "title": "Praise", "text": "Praise\n\nPraise is a form of social interaction expressing recognition, reassurance or admiration.\nPraise is expressed verbally as well as by body language (facial expression and gestures).\n\nVerbal praise consists of a positive evaluations of another's attributes or actions, where the evaluator presumes the validity of the standards on which the evaluation is based. \n\nAs a form of social manipulation, praise is a form of reward and furthers behavioral reinforcement by conditioning.\nThe influence of praise on an individual can depend on many factors, including the context, the meanings the praise may convey, and the characteristics and interpretations of the recipient. \nWhile praise may share some predictive relationships (both positive and negative) with tangible (material) rewards, praise tends to be less salient and expected, conveys more information about competence, and is typically given more immediately after the desired behavior.\n\nPraise is distinct from acknowledgement or feedback, which are more neutral forms of recognition, and encouragement, which is expressedly future oriented. \n\nPraise is given across social hierarchy, and both within the ingroup and towards an outgroup; it is an important aspect in the regulation of social hierarchy and the maintenance of group cohesion, influencing the potential for political action and social upheaval. When given by a dominant individual it takes the form of recognition and reassurance, reducing the potential of political action aiming for \nwhen given by a submissive to a dominant individual it takes the form of deference, admiration or exultation, or in the extreme case of deification (prayer of praise).\n\nThe concept of praise as a means of behavioral reinforcement is rooted in B.F. Skinner's model of operant conditioning. Through this lens, praise has been viewed as a means of positive reinforcement, wherein an observed behavior is made more likely to occur by contingently praising said behavior. Hundreds of studies have demonstrated the effectiveness of praise in promoting positive behaviors, notably in the study of teacher and parent use of praise on child in promoting improved behavior and academic performance, but also in the study of work performance. Praise has also been demonstrated to reinforce positive behaviors in non-praised adjacent individuals (such as a classmate of the praise recipient) through vicarious reinforcement. Praise may be more or less effective in changing behavior depending on its form, content and delivery. In order for praise to effect positive behavior change, it must be contingent on the positive behavior (i.e., only administered after the targeted behavior is enacted), must specify the particulars of the behavior that is to be reinforced, and must be delivered sincerely and credibly.\n\nAcknowledging the effect of praise as a positive reinforcement strategy, numerous behavioral and cognitive behavioral interventions have incorporated the use of praise in their protocols. The strategic use of praise is recognized as an evidence-based practice in both classroom management and parenting training interventions, though praise is often subsumed in intervention research into a larger category of positive reinforcement, which includes strategies such as strategic attention and behavioral rewards.\n\nAlthough the majority of early research on the influences of praise focused on behavior implications, more recent investigations have highlighted important implications in other domains. Praise may have cognitive influences on an individual, by attracting attention to the self, or by conveying information about the values and expectations of the praiser to the recipient. Effective praise (i.e., praise that is welcomed or accepted by the recipient) may also have positive emotional effects by generating a positive affective state (e.g., happiness, joy, pride). Praise is also thought to convey that one has surpassed a noteworthy evaluative standard, and if the recipient of the praise is likely to experience a sense of pleasure stemming from a positive self-perception. Contrastingly, praise may create negative emotional consequences if it appears disingenuous or manipulative.\n\nAlternative views of the effects of praise on motivation exist. In one camp, praise is thought to decrease intrinsic motivation by increasing the presence of external control. However, praise has also been argued to define standards and expectations, which in turn may motivate an individual to exert effort to meet those standards. Lastly, praise may serve to influence interpersonal relations. For example, strong pressures to reciprocate praise have been found. It is thought that the mutual praise may serve to increase attraction and strengthen the interpersonal relationship, and this process may underlie the use of praise in ingratiation.\n\nOver the past several decades, researchers have distinguished between praise that is directed at a person's general abilities and qualities (e.g., \"You're such a good drawer.\") and praise that is directed at the process of performance (e.g., \"You are working so hard at that drawing.\"). This distinction between person versus process praise is sometimes referred to as ability versus effort praise, though ability and effort statements can be seen as subcategories of person and process statements, respectively.\n\nTraditionally, person(trait)-oriented praise was thought to instill a child's belief that they have the capacity to succeed, and thus help motivate them to learn. However, social-cognitive theorists have more recently suggested that person-oriented (as opposed to process-oriented) praise may have detrimental impacts on a child's self-perceptions, motivation and learning. For example, praising children for their personal attributes, rather than specifics about their performance, may teach them to make interferences about their global worth, and may thus undermine their intrinsic motivation. In a study of person- versus process-oriented praise, Kamins and Dweck found that children who received person-oriented praise displayed more \"helpless\" responses following a failure including self-blame, than those in the process condition. Henderlong and Lepper suggest that person-oriented praise may function like tangible rewards, in that they produce desired outcomes in the short-run, but may undermine intrinsic motivation and subsequent perseverance. However, Skipper & Douglas found that although person- versus process-oriented praise (and an objective feedback control group) predicted more negative responses to the first failure, all three groups demonstrated similarly negative responses to the second failure. Thus, the long-term negative consequences of person-oriented praise are still unclear.\n\nPerson and process (or performance) praise may also foster different attributional styles such that person-oriented praise may lead one to attribute success and failure to stable ability, which in turn may foster helplessness reactions in the face of setbacks. Contrastingly, process praise may foster attributions regarding effort or strategy, such that children attribute their success (or failure) to these variables, rather than their stable trait or ability. This attributional style can foster more adaptive reactions to both success and failure. In support of this notion, Muller and Dweck experimentally found praise for child intelligence to be more detrimental to 5th graders' achievement motivation than praise for effort. Following a failure, the person-praised students displayed less task persistence, task enjoyment, and displayed worse task performance than those praised for effort. These findings are in line with personal theories of achievement striving, in which in the face of failure, performance tends to improve when individuals make attributions to a lack of effort, but worsen when they attribute their failure to a lack of ability.\n\nIn the studies mentioned above, person-oriented praise was found to be less beneficial than process-oriented praise, but this is not always found to be the case. Particularly, effort-oriented praise may be detrimental when given during tasks that are exceptionally easy. This may be especially apparent for older children as they see effort and ability to be inversely related and thus an overemphasis on effort may suggest a lack of ability.\n\nProponents of cognitive evaluation theory (Deci & Ryan ) have focused on two aspects of praise thought to influence a child's self-determination: information and control. Taking this perspective, the informational aspect of praise is thought to promote a perceived internal locus of control (and thus greater self-determination) while the controlling aspects promote a perceived external locus of control and thus extrinsic compliance or defiance. Thus, Deci & Ryan suggest that the effect of praise is moderated by the salience of informational versus controlling aspects of praise.\n\nThe theory that informational praise enhances self-determination over controlling praise has been supported by several empirical studies. In a metanalysis including five studies distinguishing informational from controlling praise, Deci, Koestner & Ryan found that informational-based praise related to greater intrinsic motivation (as measured by free-choice behavior and self-reported interest) while controlling praise was associated with less intrinsic motivation. For example, Pittman and colleagues found that adults demonstrated more free-choice engagement with a task after receiving informational (\"e.g., \"Compared to most of my subjects, you're doing really well.\"), rather than controlling (e.g., \"I haven't been able to use most of the data I've gotten so far, but you're doing really well, and if you keep it up I'll be able to use yours.\") praise.\n\nSeveral complexities of informational versus controlling praise have been acknowledged. First, though the differences between information and controlling praise have been well-established, it is difficult to determine whether the net effects of these forms of praise will be positive, negative or neutral compared to a control condition. In addition, it is often difficult to determine the extent to which informational, controlling, or both, which may muddy interpretations of results.\n\nSocial comparison is a psychological process that is widely prevalent, particularly so in educational settings. In Festinger's social comparison theory, he noted that people engage in social comparison as a means to reduce ambiguity and accurately evaluate their own qualities and abilities. However, controversy exists over whether providing children with social-comparison praise has beneficial impact on their motivation and performance. Some studies have demonstrated that students who received social-comparison praise (e.g., \"you're doing better than most students\" or \"you're performance is amongst the best we've had\") demonstrated greater motivation compared to no-praise or other control groups. Sarafino, Russo, Barker, Consentino and Titus found that students who received social-comparison voluntarily engaged in the task more so than those who received feedback that they performed similar to others. Though these studies demonstrate the possible positive influence of social-comparison praise, they have been criticized for inadequate control groups. For example, a control group given feedback that they are average may be seen as negative, rather than neutral. In addition, most social-comparison studies do not examine motivation or behavior following a subsequent unsuccessful task.\n\nBeyond methodology, the primary criticism to social-comparison praise is that it teaches children to evaluate themselves on the basis of the performance of others, and may therefore lead to maladaptive coping in situations in which one is outperformed by others individuals. Social-comparison praise has been hypothesized to decrease intrinsic motivation for the praised children because they may then view their behaviors as externally controlled. Contrastingly, it is suggested that praise that focused on a child's competence (mastery) rather than social comparison may be important for fostering motivation. This area is relatively understudied, though some interesting findings have emerged. In a study of adults, Koestner, Zuckerman, and Olsson found that gender moderated the influence of social-comparison and mastery praise, where women were more intrinsically motivated following mastery praise, while men were more motivated following social-comparison praise. In a study of children, Henderlong Corpus, Ogle & Love-Geiger found that social-comparison praise lead to decreased motivation following ambiguous feedback for all children, and also decreased motivation following positive feedback for girls only. Thus, mastery praise may be more conducive than social-comparison to fostering intrinsic motivation, particularly for females, though more research is needed to tease apart these relationships.\n\nThe function of praise on child performance and motivation may likely vary as a function of age. Few studies have directly examined developmental differences in praise, though some evidence has been found. Henderlong Corpus & Lepper found person praise (as opposed to process praise) to negatively influence motivation for older girls (4th/5th grade), while for preschool-age children, there were no differences in the effects of process, person and product praise, though all three forms of praise were associated with increased motivation as compared to neutral feedback. In a different study, Henderlong found that for older children, process praise enhanced post-failure motivation more so than person praise, and person praise decreased motivation as compared to neutral feedback. Contrastingly, for preschool-age children process praise enhanced post-failure motivation more than person praise, but both were better than neutral feedback. Some posit that younger children do not experience the negative effects of certain types of praise because they do not yet make causal attributions in complex ways, and they are more literal in their interpretations of adult speech.\n\nThe function of praise on child behavior and motivation has also found to vary as a function of child gender. Some researchers have shown that females are more susceptible to the negative effects of certain types of praise (person-oriented praise, praise that limits autonomy). For example, Koestner, Zuckerman & Koestner found that girls were more negatively influenced by praise that diminished perceived autonomy. Henderlong Corpus and Lepper found that process praise was more beneficial to motivation than person praise, but only for girls. This difference was found for older children, but not preschool-aged children.\n\nOthers have found young girls to be more negatively influenced by the evaluations of adults more generally. Some have posited that this gender difference is due to girls more often attributing failure to lack of ability rather than a lack of motivation or effort. Gender differences may be attributable to normative socialization practices, in which people generally emphasize dependence and interpersonal relationships for girls, but achievement and independence for boys.\n\nCulture has been referred to as a \"blind spot\" in the praise literature. Yet, there is reason to believe that cultural differences in the effects of praise exist. Much of the discussion on culture and praise has focused on differences between independent and interdependent cultures (e.g.). Stated briefly, independent cultures, common in Western cultures, generally value and seek to promote individualism and autonomy, while interdependent cultures promote fundamental connectedness and harmony in interpersonal relationships. Looking through this cultural lens, clear differences in the use and impact of praise can be found. In comparison to the United States, praise is rarely in China and Japan (e.g.), as praise may be thought to be harmful to a child's character. In interdependent cultures, individuals are generally motivated by self-improvement. This cultural difference has also been found experimentally. Heine, Lehman, Markus & Katayama found that Canadian students persisted longer after positive than negative performance feedback, while the opposite was true for Japanese students. Some posit that individuals from independent and interdependent cultures largely express different models of praise (independence-supportive and interdependence-supportive praise.\n\n"}
{"id": "10273917", "url": "https://en.wikipedia.org/wiki?curid=10273917", "title": "Projective cover", "text": "Projective cover\n\nIn the branch of abstract mathematics called category theory, a projective cover of an object \"X\" is in a sense the best approximation of \"X\" by a projective object \"P\". Projective covers are the dual of injective envelopes.\n\nLet formula_1 be a category and \"X\" an object in formula_1. A projective cover is a pair (\"P\",\"p\"), with \"P\" a projective object in formula_1 and \"p\" a superfluous epimorphism in Hom(\"P\", \"X\").\n\nIf \"R\" is a ring, then in the category of \"R\"-modules, a superfluous epimorphism is then an epimorphism formula_4 such that the kernel of \"p\" is a superfluous submodule of \"P\".\n\nProjective covers and their superfluous epimorphisms, when they exist, are unique up to isomorphism. The isomorphism need not be unique, however, since the projective property is not a full fledged universal property.\n\nThe main effect of \"p\" having a superfluous kernel is the following: if \"N\" is any proper submodule of \"P\", then formula_5. Informally speaking, this shows the superfluous kernel causes \"P\" to cover \"M\" optimally, that is, no submodule of \"P\" would suffice. This does not depend upon the projectivity of \"P\": it is true of all superfluous epimorphisms.\n\nIf (\"P\",\"p\") is a projective cover of \"M\", and \"P' \" is another projective module with an epimorphism formula_6, then there is a split epimorphism α from \"P' \" to \"P\" such that formula_7\n\nUnlike injective envelopes and flat covers, which exist for every left (right) \"R\"-module regardless of the ring \"R\", left (right) \"R\"-modules do not in general have projective covers. A ring \"R\" is called left (right) perfect if every left (right) \"R\"-module has a projective cover in \"R\"-Mod (Mod-\"R\").\n\nA ring is called semiperfect if every finitely generated left (right) \"R\"-module has a projective cover in \"R\"-Mod (Mod-\"R\"). \"Semiperfect\" is a left-right symmetric property.\n\nA ring is called \"lift/rad\" if idempotents lift from \"R\"/\"J\" to \"R\", where \"J\" is the Jacobson radical of \"R\". The property of being lift/rad can be characterized in terms of projective covers: \"R\" is lift/rad if and only if direct summands of the \"R\" module \"R\"/\"J\" (as a right or left module) have projective covers.\n\nIn the category of \"R\" modules:\n\n\n"}
{"id": "2685819", "url": "https://en.wikipedia.org/wiki?curid=2685819", "title": "Psychometric function", "text": "Psychometric function\n\nA psychometric function is an inferential model applied in detection and discrimination tasks. It models the relationship between a given feature of a physical stimulus, e.g. velocity, duration, brightness, weight etc., and forced-choice responses of a human test subject. The psychometric function therefore is a specific application of the generalized linear model (GLM) to psychophysical data. The probability of response is related to a linear combination of predictors by means of a sigmoid link function (e.g. probit, logit, etc.). \n\nDepending on the number of choices, the psychophysical experimental paradigms classify as simple forced choice (also known as yes-no task), two-alternative forced choice (2AFC), and n-alternative forced choice. The number of alternatives in the experiment determine the lower asymptote of the function. \n\nA common example is visual acuity testing with an eye chart. The person sees symbols of different sizes (the size is the relevant physical stimulus parameter) and has to decide which symbol it is. Usually, there is one line on the chart where a subject can identify some, but not all, symbols. This is equal to the transition range of the psychometric function and the sensory threshold corresponds to visual acuity. (Strictly speaking, a typical optometric measurement does not exactly yield the sensory threshold due to biases in the standard procedure.)\n\nTwo different types of psychometric plots are in common use: \nThe second way of plotting psychometric functions is often preferable, as it is more easily amenable to principled quantitative analysis using tools such as probit analysis (fitting of cumulative Gaussian distributions). However, it also has important drawbacks. First, the threshold estimation is based only on p(yes), namely on \"Hit\" in Signal Detection Theory terminology. Second, and consequently, it is not bias free or criterion free. Third, the threshold is identified with the p(yes) = .5, which is just a conventional and arbitrary choice. \n\n"}
{"id": "543116", "url": "https://en.wikipedia.org/wiki?curid=543116", "title": "Rankit", "text": "Rankit\n\nIn statistics, rankits of a set of data are the expected values of the order statistics of a sample from the standard normal distribution the same size as the data. They are primarily used in the normal probability plot, a graphical technique for normality testing.\n\nThis is perhaps most readily understood by means of an example. If an i.i.d. sample of six items is taken from a normally distributed population with expected value 0 and variance 1 (the standard normal distribution) and then sorted into increasing order, the expected values of the resulting order statistics are:\n\nSuppose the numbers in a data set are\n\nThen one may sort these and line them up with the corresponding rankits; in order they are\nwhich yields the points:\nThese points are then plotted as the vertical and horizontal coordinates of a scatter plot.\n\nAlternatively, rather than \"sort\" the data points, one may \"rank\" them, and \"rearrange\" the rankits accordingly. This yields the same pairs of numbers, but in a different order.\n\nFor:\nthe corresponding ranks are:\n\ni.e., the number appearing first is the 5th-smallest, the number appearing second is 6th-smallest, the number appearing third is smallest, the number appearing fourth is 2nd-smallest, etc. One rearranges the expected normal order statistics accordingly, getting the rankits of this data set:\n\nA graph plotting the rankits on the horizontal axis and the data points on the vertical axis is called a rankit plot or a normal probability plot. Such a plot is necessarily nondecreasing. In large samples from a normally distributed population, such a plot will approximate a straight line. Substantial deviations from straightness are considered evidence against normality of the distribution. \n\nRankit plots are usually used to visually demonstrate whether data are from a specified probability distribution.\n\nA rankit plot is a kind of Q-Q plot – it plots the order statistics (quantiles) of the sample against certain quantiles (the rankits) of the assumed normal distribution. Q-Q plots may use other quantiles for the normal distribution, however.\n\nThe rankit plot and the word rankit was introduced by the biologist and statistician Chester Ittner Bliss (1899–1979).\n\n\n"}
{"id": "51065855", "url": "https://en.wikipedia.org/wiki?curid=51065855", "title": "RepRap Ormerod", "text": "RepRap Ormerod\n\nThe RepRap Ormerod is an open-source fused deposition modeling 3D printer and is part of the RepRap project. The RepRap Ormerod is named after the English entomologist Eleanor Anne Ormerod, it was designed by RepRapPro. There have been two versions of the Ormerod, the Ormerod 1 was released in December 2013 and the Ormerod 2 released in December 2014.\n\nThe RepRap Ormerod has a 200 mm build volume, uses a Bowden extruder, it also has a micro SD card and USB and Ethernet connections allowing it to be connected to a network. The printer was praised for the simplicity of construction and its low cost.\n\n\n"}
{"id": "219243", "url": "https://en.wikipedia.org/wiki?curid=219243", "title": "Right to keep and bear arms", "text": "Right to keep and bear arms\n\nThe right to keep and bear arms (often referred to as the right to bear arms) is the people's right to possess weapons (arms) for their own defense. Only few countries recognize people's right to keep and bear arms and protect it on statutory level, and even fewer protect the right on constitutional level.\n\nIn Old English, \"beran\" (past tense \"bær\") means to bear, bring, bring forth, or produce; to endure or sustain; or to wear.\n\nThe Bill of Rights 1689 allowed Protestant citizens of England to \"have Arms for their Defence suitable to their Conditions and as allowed by Law\" and restricted the ability of the English Crown to have a standing army or to interfere with Protestants' right to bear arms \"when Papists were both Armed and Imployed contrary to Law\" and established that Parliament, not the Crown, could regulate the right to bear arms.\n\nSir William Blackstone wrote in the 18th century that the right to have arms was auxiliary to the \"natural right of resistance and self-preservation\" subject to suitability and allowance by law. The term \"arms\" as used in the 1600s, the term refers to the process of equipping for war. It is commonly used as a synonym for weapon.\n\nInclusion of this right in a written constitution is uncommon. In 1875, 17 percent of constitutions included a right to bear arms. Since the early twentieth century, \"the proportion has been less than 9 percent and falling\". In their historical survey and comparative analysis of constitutions dating back to 1789, Tom Ginsburg and colleagues \"identified only 15 constitutions (in nine countries) that had ever included an explicit right to bear arms. Almost all of these constitutions have been in Latin America, and most were from the 19th century\".\n\nGenerally, where modern constitutions refer to arms at all, the purpose is \"to allow the government to regulate their use or to compel military service, not to provide a right to bear them\". Constitutions which historically guaranteed a right to bear arms are those of Bolivia, Colombia, Costa Rica, Guatemala, Honduras, Liberia, Mexico, Nicaragua and the United States of America. Nearly all of the Latin American examples were modelled on that of the United States. At present, out of the world’s nearly 200 constitutions, three still include a right to bear arms: Guatemala, Mexico, and the United States; of these three, only the last does not include explicit restrictive conditions.\n\nWhile protecting the right to keep arms, Guatemalan constitution specifies that this right extends only to \"weapons not prohibited by law\". \n"}
{"id": "4106285", "url": "https://en.wikipedia.org/wiki?curid=4106285", "title": "Self-referential encoding", "text": "Self-referential encoding\n\nEvery day, people are presented with endless amounts of information, and in an effort to help keep track and organize this information, people must be able to recognize, differentiate and store information. One way to do that is to organize information as it pertains to the self. The overall concept of self-reference suggests that people interpret incoming information in relation to themselves, using their self-concept as a background for new information. Examples include being able to attribute personality traits to oneself or to identify recollected episodes as being personal memories of the past. The implications of self-referential processing are evident in many psychological phenomena. For example, the \"cocktail party effect\" notes that people attend to the sound of their names even during other conversation or more prominent, distracting noise. Also, people tend to evaluate things related to themselves more positively (This is thought to be an aspect of implicit self-esteem). For example, people tend to prefer their own initials over other letters. The self-reference effect (SRE) has received the most attention through investigations into memory. The concepts of self-referential encoding and the SRE rely on the notion that relating information to the self during the process of encoding it in memory facilitates recall, hence the effect of self-reference on memory. In essence, researchers have investigated the potential mnemonic properties of self-reference.\n\nResearch includes investigations into self-schema, self-concept and self-awareness as providing the foundation for self-reference's role in memory. Multiple explanations for the self-reference effect in memory exist, leading to a debate about the underlying processes involved in the self-reference effect. In addition, through the exploration of the self-reference effect, other psychological concepts have been discovered or supported, including simulation theory and the effect.\nAfter researchers developed a concrete understanding of the self-reference effect, many expanded their investigations to consider the self-reference effect in particular groups like those with autism spectrum disorders or those experiencing depression.\n\nSelf-knowledge can be categorized by structures in memory or schemata. A self-schema is a set of facts or beliefs that one has about themselves. For any given trait, an individual may or may not be \"schematic\"; that is, the individual may or may not think about themselves as to where they stand on that trait. For example, people who think of themselves as very overweight or who identify themselves to a greater extent based on their body weight would be considered \"schematic\" on the attribute of body weight. Thus, many everyday events, such as going out for a meal or discussing a friend's eating habits, could induce thoughts about the self. When people relate information to something that has to do with the self, it facilitates memory. Self-descriptive adjectives that fit into one's self-schema are easier to remember than adjectives not viewed as related to the self. Thus, the self-schema is an aspect of oneself that is used as an encoding structure that brings upon memory of information consistent with one's self-schema. Memories that are elaborate and well encoded are usually the result of self-referent correlations during the process of remembering. During the process of encoding, trait representations are encoded in long term memory either directly or indirectly. When they are directly encoded, it is in terms of relating to the self, and when it is indirectly encoded it is done through spouts of episodic information instead of information about the self.\n\nSelf-schema is often used as somewhat of a database for encoding personal data. The self-schema is also used by paying selective attention to outside information and internalizing that information more deeply in one's memory depending on how much that information relates to their schema. When self-schema is engaged, traits that go along with one's view of themselves are better remembered and recalled. These traits are also often recalled much better when processed with respect to the self. Similarly, items that are encoded with the self are based on one's self-schema. Processing the information should balance out when recalled for individuals who have a self-schema that goes along with the information.\n\nSelf-schemas do not necessarily only involve individual traits. People self-categorize at different levels that range from more personal to more social. Self-schemas have three main categories which play a role: the personal self, the relational self, and the collective self. The personal self deals with individual level characteristics, the relational self deals with intimate relationship partners, and the collective self deals with group identities, relating to self-important social groups to which one belongs (e.g., one's family or university). Information that is related to any type of self-schema, including group-related knowledge structures facilitates memory.\n\nIn order for the self to be an effective encoding mechanism, it must be a uniform, consistent, well-developed schema. It has been shown that identity exploration leads to the development of self-knowledge which facilitates self-judgments. Identity exploration led to shorter decision times, higher confidence ratings and more intrusions in memory tasks. Previous researchers hypothesized that words compatible with a person's self-schema are easily accessible in memory and are more likely than incompatible words to intrude on a schema-irrelevant memory task. In one experiment, when participants were asked to decide if certain adjectives were \"like me\" or \"not like me,\" they made the decisions faster when the words were compatible with their self-schema.\n\nHowever, despite the existence of the self-reference effect when considering schemata consistent adjectives, the connection between the self and memory can lead to a larger number of mistakes in recognition, commonly referred to as false alarms. Rogers et al. (1979) found that people are more likely to falsely recognize adjectives they had previously designated to be self-descriptive. Expanding on this, Strube et al. (1986) found that false alarms occurred more for self-schema consistent content, presumably because the presence of such words in the schema makes them more accessible in memory.\n\nIn addition to investigating the self-reference effect in regards to schemata consistent information, Strube et al. discussed how counter schemata information relates to this framework. They noted that the pattern of making correct decisions more rapidly did not hold when considering words that countered a person's self-schema, presumably because they were difficult to integrate into memory due to lack of a preexisting structure. That is, they lacked the organizational structure of encoding because they did not fall into the \"like me\" category, and elaboration would not work because prior connections to the adjective did not exist.\n\nTwo of the most common functions of the self receiving significant attention in research are the self-acting to organize the individual's understanding of the social environment, and the self functioning to regulate behavior through self-evaluation. The concept of self-awareness is considered to be the foundational principle for both functions of the self. Some research presents self-awareness in terms of self-focused attention whereas Hull and Levy suggest that self-awareness refers to the encoding of information based on its relevance to the self. Based on the latter interpretation of self-awareness, individuals must identify the aspects of situations that are relevant to themselves and their behavior will be shaped accordingly. Hull and Levy suggest that self-awareness corresponds to the encoding of information cued by self-symbolic stimuli, and examine the idea of self-awareness as a method of encoding. They structured an investigation that examined self-referent encoding in individuals with different levels of self-awareness, predicting that individuals with higher levels of self-consciousness would encode self-relevant information more deeply than other information, and that they would encode it more deeply than individuals with low levels of self-consciousness. The results of their investigation supported their hypothesis that self-focused attention is not enough to explain the role of self-awareness on attribution. Their results suggest that self-awareness leads to increased sensitivity to the situationally defined meanings of behavior, and therefore organizes the individual's understanding of the social environment. The research presented by Hull and Levy led to future research on the encoding of information associated with self-awareness.\n\nIn later research, Hull and colleagues examined the associations between self-referential encoding, self-consciousness and the extent to which a stimulus is consistent with self-knowledge. They first assumed that the encoding of a stimulus is facilitated if an individual's working memory already contains information consistent with the stimulus, and suggested that self-consciousness as an encoding mechanism relies on an individual's self-knowledge. It is known that situational and dispositional factors may activate certain pools of knowledge, moving them into working memory, and guiding the processing of certain stimulus information.\n\nIn order to better understand the idea of activating information in memory, Hull et al. presented an example of how information is activated. They referred to the sentence \"The robber took the money from the bank\". In English, the word bank has two applicable meanings in the context of this sentence (monetary institution and river shore). However, the monetary institution meaning of the word is more highly activated in this context due to the addition of the words robber and money to the sentence, because they are associatively relevant and therefore pull the monetary institution definition for bank into working memory. Once information is added to working memory, meanings and associations are more easily drawn. Therefore, the meaning of this example sentence is almost universally understood.\n\nIn reference to self-consciousness and self-reference, the connection between self-consciousness and self-referent encoding relies on such information activation. Research suggests that self-consciousness activates knowledge relating to the self, thereby guiding the processing of self-relevant information. Three experiments conducted by Hull and colleagues provided evidence that a manipulation of accessible self-knowledge impacts self-referent encoding based on the self-relevance of such information, individual differences in the accessibility of self-knowledge (self-consciousness) impacts perception, and a mediation relationship exists between self-consciousness and individual differences in self-referential encoding.\n\nSimilar to how self-awareness impacts the availability of self-knowledge and the encoding of self-relevant information, through the development of the self-schema, people develop and maintain certain personality characteristics leading to a variety of behavior patterns. Research has been done on the differences between Type A and Type B behavior patterns, focusing on how people in each group respond to environmental information and their interpretation of the performance of others and themselves. It has been found that Type A behavior is characterized by competitive achievement striving, time urgency and hostility, whereas Type B is usually defined as an absence of Type A characteristics. When investigating causal attributions for hypothetical positive and negative outcomes, Strube et al. found that Type A individuals were more self-serving, in that they took greater responsibility for positive than negative effects. Strube and colleagues argued that this could be a result of the fact that schema-consistent information is more easily remembered and the ease with which past successes and failures are recalled, determined by self-schema, would impact attributions. It is reasonable to believe that Type A's might recall successes more easily and hence be more self-serving.\n\nInfluential psychologists Craik and Lockhart laid the groundwork for research focused on self-referential encoding and memory. In 1972 they proposed their Depth of Processing framework which suggests that memory retention depends on how the stimulus material was encoded in memory. Their original research considered structural, phonemic, and semantic encoding tasks, and showed that semantic encoding is the best method to aid in recall. They asked participants to rate 40 descriptive adjectives on one of four tasks; Structural (Big font or small font?), Phonemic (Rhymes with xxx?), Semantic (Means same as xxx?), or Self-reference (Describes you?). This was then followed by an \"incidental recall task\". This is where participants are asked, without prior warning, to recall as many of the words they had seen as possible within a given time limit. Craik and Tulving's original experiment showed that structural and phonemic tasks lead only to \"shallow\" encoding, while the semantic tasks lead to \"deep\" encoding and resulted in better recall.\n\nHowever, in 1977, it was shown that self-relevant or self-descriptive encoding leads to even better recall than semantic tasks. Experts suggest that the call on associative memory required by semantic tasks is what provides the advantage over structural or phonemic tasks, but is not enough to surpass the benefit provided by self-referential encoding. The fact that self-reference was shown to be a stronger memory encoding method than semantic tasks is what led to more significant interest in the field One early and significant experiment aimed to place self-reference on Craik and Lockhart's depth of processing hierarchy, and suggested that self-reference was a more beneficial encoding method than semantic tasks. In this experiment, participants filled out self-ratings on 84 adjectives. Months later, these participants were revisited and were randomly shown 42 of those words. They then had to select the group of 42 \"revisited\" words out of the total original list. The researchers argued that if the \"self\" was involved in memory retrieval, participants would incorrectly recognize words that were more self-descriptive In another experiment, subjects answered yes or no to cue questions about 40 adjective in 4 tasks (structural, phonemic, semantic and self-referential) and later had to recall the adjectives. This experiment validated the strength of self-reference as an encoding method, and indicated it developed a stronger memory trace than the semantic task.\n\nResearchers are implementing a new strategy by developing different encoding tasks that enhance memory very similarly to self-referential encoding. Symons (1990) had findings that went against the norm when he was unable to find evidence of self-schematicity in the self-reference effect. Another finding was that when referencing gender and religion, there was a low memory recall when compared with referencing the self. A meta-analysis by Symons and Johnson (1997) showed self-reference resulting in better memory in comparison to tasks relying on semantic encoding or other-referent encoding. According to Symons and Johnson, self-referencing questions elicit elaboration and organization in memory, both of which creating a deeper encoding and thus facilitate memory.\n\nTheorists that favor the view that the self has a special role believe that the self leads to more in depth processing, leading to easier recall during self-reference tasks. Theorists also promote the self-schema as being one of the sole inhibitors that allow for recall from deep memory. Thorndyke and Hayes-Roth had the goal of focusing on the process made by the active memory schemata. Sex-typed individuals recall trait adjectives that go along with their sex role more quickly than trait adjectives that are not. During the process of free recall, these individuals also showed more patterns for gender clustering than other sexually typed individuals.\n\nAs research on self-referential encoding became more prolific, some psychologists took an opportunity to delineate specific self-referential encoding tasks. It is noted that descriptive tasks are those that require participants to determine if a stimulus word can be classified as \"self-descriptive.\" Autobiographical tasks are those that require participants to use the stimulus word as a cue to recall an autobiographical memory. Results from experiments that differentiated between these types of self-referential encoding found that they both produced better recall than semantic tasks, and neither was more advantageous than the other. However, research does suggest that the two types of self-referential encoding do rely on different processes to facilitate memory. In most experiments discussed, these types of self- referential encoding were not differentiated.\n\nIn a typical self-reference task, adjectives are presented and classified as either self-descriptive or not. For example, in a study by Dobson and Shaw, adjectives about the self that were preselected were given to the participants and they decide whether or not the adjectives are self-descriptive. The basis for making certain judgments, decisions, inferences and decisions is a self-referent encoding task. If two items are classified as self-descriptive there is no reason one trait would not be equally as easy to retrieve as the other on a self-reference task.\n\nWhile a significant amount of research supports the existence of the self-reference effect, the processes behind it are not well understood. However, multiple hypotheses have been introduced, and two main arguments have been developed: the elaborative processing hypothesis and the organizational processing hypothesis. Encodings in reference to the self are so elaborate because of the information one has about the self. Information encoded with the self is better remembered than information encoded with reference to something else.\n\nElaboration refers to the encoding of a single word by forming connections between it and other material already stored in memory. By creating these connections between the stimulus word and other material already in memory, multiple routes for retrieval of the stimulus word are formed. Based on the depth of processing framework, memory retention increases as elaboration during encoding increases. The Elaborative Processing Hypothesis would suggest that any encoding task that leads to the development of the most trace elaboration or associations is the best for memory retention. Additional research on the depth of processing hierarchy suggests that self-reference is the superior method of information encoding. The elaborative hypothesis would suggest this is because self-reference creates the most elaborate trace, due to the many links that can be made between the stimulus and information about the self already in memory.\n\nThe organizational processing hypothesis was proposed by Klein and Kihlstrom. This hypothesis suggests that encoding is best prompted by considering stimulus words in relation to one another. This thought process and relational thinking creates word to word associations. These inter-item associations are paths in memory that can be used during retrieval. Also, the category labels that define the relations between stimulus items can be used as item cues. Evidence of the organizational component of encoding is demonstrated through the clustering of words during recall. Word clustering during recall indicates that relational information was used to store the words in memory. Rogers, Kuiper and Kirker showed that self-referential judgments were more likely to encourage organization than semantic ones. Therefore, they suggested the self-reference effect was likely due to the organizational processing endured by self-referential encoding.\n\nStructural, phonemic and semantic tasks within the depth of processing paradigm require words to be considered individually, and lend themselves to an elaborative approach. As such, it can be argued that self-referential encoding is superior because it leads to an indirect division of words into categories: words that describe me versus words that do not. Due to this connection between self-reference and organizational processing, further research has been done on this area. Klein and Kihlstrom's research suggests first that, like previous research, self-reference led to better recall than semantic and structural encoding. Second, they found that self-referentially encoded words were more clustered in recall than words from other tasks, suggesting higher levels of organizational processing. From this they concluded that the organization, not encoding task, is what makes self-referential encoding superior \n\nPsychologists Einstein and Hunt showed that both elaborative processing and organizational processing facilitate recall. However, their research argues that the effectiveness of either approach depends on how related the stimulus words are to one another. A list of highly related stimulus words would be better encoded using the elaborative method. The relations between the words would be evident to subjects; therefore, they would not gain any additional pathways for retrieval by encoding the words based on their categorical membership. Instead, the other information gained through elaborative processing would be more beneficial. On the other hand, a list of stimulus words with little relation would be better stored to memory through the organizational method. Since the words have no obvious connection to one another, subjects would likely encode them individually, using an elaborative approach. Since relational information wouldn't be readily detected, focusing on it would add to memory by creating new traces for retrieval. Superior recall was better explained by a combination of elaboration and organization.\nUltimately, the exact processes behind self-referential encoding that makes it superior to other encoding tasks are still under debate. Research suggests that if elaborative processing is behind self-referential encoding, a self-referential task should have the same effect as an elaborative task, whereas if organizational processing underlies the self-reference effect self-referential encoding tasks should function like organizational tasks. To test this, Klein and Loftus ran a 3x2 study testing organizational, elaborative and self-referential encoding with lists of 30 related or unrelated words. When participants were asked to memorize the unrelated list, recall and clustering were higher for the organizational task, which produced almost equal results to the self-referential task, suggesting that has an organizational basis. For the list of related words, the elaborative task led to better recall and had matched results to the self-reference task, suggesting an elaborative basis. This research, then, suggests that the self-reference effect cannot be explained by a single type of processing. Instead, self-referential encoding must lead to information in memory that incorporates item specific and relational information.\n\nOverall, the SRE relies on the unique mnemonic aspects of the self. Ultimately, if the research is suggesting that the self has superior elaborative or organizational properties, information related to the self should be more easily remembered and recalled. The research presented suggests that self-referential encoding is superior because it promotes organization and elaboration simultaneously, and provides self-relevant categories that promote recall.\n\nThe field of social brain science is aimed at examining the neural foundations of social behavior. Neuroimaging and neuropsychology have led to the examination of neuroanatomy and its connection to psychological topics. Through this research, neuropsychologists have found a connection between social cognitive functioning and the medial prefrontal cortex (mPFC). In addition, the mPFC has been connected to reflection and introspection about personal mental states. Supporting these findings, it has been shown that damage to the mPFC is connected to impairments with self-reflection, introspection and daydreaming, as well as social competence, but not other areas of functioning. As such, the mPFC has been connected to self-referential processing.\n\nThe research discussed by those focusing on the neuroanatomy of self-referential processing included similar tasks to the memory and depth of processing research discussed previously. When participants were asked to judge adjectives based in whether or not they were self-descriptive, it was noted that the more self-relevant the trait, the stronger the activation of the mPFC. In addition, it was shown that the mPFC was activated during the appraisal of one's own personality traits, as well as during trait retrieval. One study showed that the more activity in the mPFC during self-referential judgments, the more likely the word was to be remembered on a subsequent surprise memory test. These results suggest that the mPFC is involved in both self-referential processing and in creating self-relevant memories.\n\nMedial prefrontal cortex (mPFC) activation occurs during processing of self-relevant information. When self-referent judgment is more relatable and less negative, the mFPC is activated. Finding support clear cut circuits that have high levels of activation when cognitive and emotional aspects of self-reflection are present. The caudate nucleus has not been associated with self-reference before, however, Fossati and colleagues found activity while participants were retrieving self-relevant trait adjectives. The ventral anterior cingulate cortex (vACC) is also a part of the brain that becomes activated when there are signs of self-referencing and processing. The vACC is activated when self-descriptive information is negative. There is also pCC (posterior cingulate cortex) activity seen in neuroimaging studies during self-referential processing.\n\nGiven all of the neurological support for the effect of self-reference on encoding and memory, there is still a debate in the psychological community about whether or not the self-reference effect signifies a special functional role played by the self in cognition. Generally, this question is met by people that have two opposing views on the processes behind self-reference. On one side of the debate, people believe that the self has special mnemonic abilities because it is a unique cognitive structure. On the other side, people support the arguments described above that suggest there is no special structure, but instead, the self-reference effect is simply a part of the standard depth of processing hierarchy. Since the overall hypothesis is the same for both sides of the debate, that self-relevant material leads to enhanced memory, it is difficult to test them using strictly behavioral measures. Therefore, PET and fMRI scans have been used to see the neural marker of self-referential mental activity.\n\nPrevious studies have shown that areas of the left prefrontal cortex are activated during semantic encoding. Therefore, if the self-reference effect works the same way, as part of the depth of processing hierarchy, the same brain region should be activated when judging traits related to the self. However, if the self has unique mnemonic properties, then self-referential tasks should activate brain regions distinct from those activated during semantic tasks. The field is still at is infancy, but future work on this hypothesis might help to settle the debate about the underlying processes of self-referential encoding.\n\nWhile not able to completely settle the debate over the foundation of self-referential processing, studies on the neurological aspect of personality trait judgments did lead to a related, significant result. It has been shown that judging personality traits about oneself and a close friend activated overlapping brain regions, and the activated regions have all been implicated in self-reference. Noting the similarity between making self-judgments and judgments about close others led to the introduction of the simulation theory of empathy. Simulation theory rests on the idea that one can make inferences about others by using the knowledge they have about themselves. In essence, the theory suggests that people use self-reflection to understand or predict the mental state of others. The more similar a person perceives another to be, the more active the mPFC has shown to be, suggesting more deep or intricate self-reference. However, this effect can cause people to make inaccurate judgments about others or to believe that their own opinions are representative of others in general. This misrepresentation is referred to as the false-consensus effect.\n\nIn addition to simulation theory, other expansions of the self-reference effect have been examined. Through studying the self, researchers have found that the self consists of many independent cognitive representations. For example, the personal self composed of individual characteristics is separate from the relational self which is based on relationships with significant others. These two forms of self are again separate from the collective self which corresponds to a particular group identity. Noting the existence of the collective self and the different group identities that combine to form such a self-representation led researchers to question if information stored in reference to a social group identity has the same effects in memory as information stored in reference to the individual self. In essence, researchers questioned if the self-reference effect can be extended to include situations where the self is more socially defined, producing a group-reference effect.\n\nPrevious research supports the idea that the group-reference effect should exist from a theoretical standpoint. First, the self-expansion model argues that individuals incorporate characteristics of their significant others (or other in-group members into the development of their self-concept. From this model, it is reasonable to conclude that characteristics that are common to both oneself and their significant others (or in-group members) would be more accessible. Second, the previous research discussed suggests that the self-reference effect is due to some combination of organizational, elaborative, mental cueing or evaluative properties of self-referential encoding tasks. Given that we have significant stores of knowledge about our social identities, and such collective identities provide an organizational framework, it is reasonable to assume that a group-reference task would operate similar to that of a self-reference task.\n\nIn order to test these claims, Johnson and colleagues aimed to test whether the self-reference effect generalized to group level identities. Their first study was structured to simply assess if group-reference influenced subsequent memory. In their experiment, they used membership at a particular university as the group of reference. They included group-reference, self-reference and semantic tasks. The experiment replicated the self-reference effect, consistent with previous research. In addition, evidence for a group-reference effect was found. Group-referenced encoding produced better recall than the semantic tasks, and the level of recall from the group-referenced task was not significantly different from the self-referenced task.\n\nDespite finding evidence of a group-reference effect, Johnson and colleagues pointed out that people identify with numerous groups, each with unique characteristics. Therefore, in order to reach conclusive evidence of a group-reference effect, alternative group targets need to be considered. In a second experiment by Johnson et al., the group of reference was modified to be the family of the individual. This group has fewer exemplars than the pool of university students, and affective considerations of the family as a group should be strong. No specific instructions or definitions were provided for family, allowing individuals to consider either the group as a whole (prototype) or specific exemplars (group). When the experiment was repeated using family as the group of reference, group-reference produced recall as much as self-reference. The mean number of recall for the group-reference was higher than self-reference. Participants indicated that they considered both the prototype and individual exemplars when responding to the questions, suggesting that the magnitude of the group-reference effect might not be dependent on the number of exemplars in the target group.\nBoth experiments presented by Johnson et al. found evidence for the group-reference effect. However, these conclusions are limited to the target groups of university students and family. Other research included gender (males and females) and religion (Jewish) as the reference groups and the group-reference effect on memory was not as evident. The group-reference recall for these two groups was not significantly more advantageous than the semantic task. Questioning what characteristics of reference groups that lead to the group-reference effect, a meta-analysis of all four group-reference conditions was performed. This analysis found that self-reference emerged as the most powerful encoding device; however, evidence was found to support the existence of a group-reference effect. The size of the reference groups and number of specific, individual exemplars was hypothesized to influence the existence of the group-reference effect. In addition, accessibility and level of knowledge about group members may also impact such an effect. So, while university students is a much larger group than family, individual exemplars may be more readily accessible than those in a religious group. Similarly, different cognitive representations were hypothesized to influence the group-reference effect. When a larger group is considered, people may be more likely to consider a prototype which may lead to fewer elaborations and cues later on. Smaller groups may lead to relying on the prototype and specific exemplars. Finally, desirability judgments that influence later processing may be influenced by self-reference and certain group-reference tasks. Individuals may be more sensitive to evaluative implications for the personal self and some group identities, but not others.\n\nGroups are also a major part of the self; therefore we attribute the role that different groups play in our self-concept also play a role in the self-reference effect. We process information about group members similarly to how we process for ourselves. Recall of remarks referencing our home and our self and group to familiarity of those aspects of our self. Reference to the self and social group and the identity that comes along with being a part of a social group are equally affective for memory. This is especially true when the groups are small, rather than large.\n\nUltimately, the group-reference effect provides evidence to explain the tendency to notice or pay attention to and remember statements made in regard to our home when traveling in a foreign place. Considering the proposal that groups form part of the self, this phenomenon can be considered an extension of the self-reference effect. Similar to the memorable nature of references to a person's individual self, references to social identities are seemed to be privileged in memory as well.\n\nOnce the foundation of research on self-referential encoding was established, psychologists began to explore how the concept applied to different groups of people, and connected to different phenomena.\n\nIndividuals diagnosed with autism spectrum disorders (ASDs) can display a wide range of symptoms. Some of the most common characteristics of individuals with ASDs include impairments with social functioning, language and communication difficulties, repetitive behaviors and restricted interests. In addition, it is often noted that these individuals are more \"self-focused.\" That is, they have difficulty seeing things from another's perspective. Despite being self-focused, though, research has shown that individuals with ASD's often have difficulty identifying or describing their emotions or the emotions of others. When asked to describe their daily experiences, responses from individuals on the autism spectrum tended to focus more on physical descriptions rather than mental and emotional states. In regards to their social interactions and behavior differences, it is thought that these individuals lack top down control, and therefore, their bottom up decisions remain unchecked. This simply suggests that these individuals cannot use their prior knowledge and memory to make sense of new input, but instead react to each new input individually, compiling them to make a whole picture \n\nNoting the difficulty individuals with ASDs experience with self-awareness, it was thought that they might have difficulty with self-related memory processes. Psychologists questioned if these individuals would show the typical self-reference effect in memory. In one Depth of Processing Study, participants were asked questions about the descriptiveness of certain stimulus words. However, unlike previous DOP studies that focused on phonemic, structural, semantic and self-referential tasks, the tasks were altered for this experiment. To test the referential abilities of individuals with ASD's, the encoding tasks were divided into: \"the self,\" asking to what extent a stimulus word described oneself, \"similar close other,\" asking to what extent a stimulus word was descriptive of one's best friend, \"dissimilar non-close other,\" asking to what extent a stimulus word was descriptive of Harry Potter, and a control group that was asked to determine the number of syllables in each word. Following these encoding tasks, participants were given thirty minutes before a surprise memory task. It was found that individuals with ASD's had no impairment in memory for words encoded in the syllable or dissimilar non-close other condition. However, they had decreased memory for words related to the self.\n\nTherefore, while research suggests that self-referentially encoded information is encoded more deeply than other information, the research on individuals with ASD's showed no advantage for memory recognition with self-reference tasks over semantic encoding tasks. This suggests that individuals with ASD's don't preferentially encode self-relevant information. Psychologists have investigated the biological basis for the decreased self-reference effect among individuals with Autism Spectrum Disorders and have suggested that it may be due to less specialized neural activity in the mPFC for those individuals. However, while individuals with ASD's showed smaller self-reference effects than the control group, some evidence of a self-reference effect was evident in some cases. This indicates that self-referent impairments are a matter of degree, not total absence.\n\nLombardo and his colleagues measured empathy among individuals with ASD's, and showed that these individuals scored lower than the control group on all empathy measures. This may be a result of the difficulty for these individuals to understand or take the perspective of others, in conjunction with their difficulty identifying emotions. This has implications for simulation theory, because these individuals are unable to use their self-knowledge to make conclusions about similar others.\n\nUltimately, the research suggests that people with ASD's might benefit from being more self-focused. The better their ability to reflect on themselves, the better the can mentalize with others.\n\nThere are three possible relations between cognitive processes and anxiety and depression. The first is whether cognitive processes are actually caused by the onset of clinically diagnosed symptoms of major depression or just generalized sadness or anxiousness. The second is whether emotional disorders such as depression and anxiety are able to be considered as caused by cognitions. And the third is whether different specific cognitive processes are able to be considered associates of different disorders. Kovacs and Beck (1977) posited a schematic model of depression where an already depressed self was primed by outside prompts that negatively impacted cognitive illusions of the world in the eye of oneself. These prompts only led participants to a more depressive series of emotions and behavior. The results from the study done by Derry and Kuiper supported Beck's theory that a negative self-schema is present in people, especially those with depressive disorder. Depressed individuals attribute depressive adjectives to themselves more than nondepressive adjectives. Those suffering from a more mild case of depression have trouble deciphering between the traits of themselves and others which results in a loss of their self-esteem and their negative self-evaluation. A depressive schema is what causes the negativity reported by those suffering from depression. Kuiper and Derry found that self-referent recall enhancement was limited only to nondepressed content.\n\nGenerally, self-focus is association with negative emotions. In particular private self-focus is more strongly associated with depression than public self-focus. Results from brain-imaging studies shows\nthat during self-referential processing, those with major depressive disorder show greater activation in the medial prefrontal cortex, suggesting that depressed individuals may be exhibiting greater cognitive control than\nnon-depressed individuals when processing self-relevant information.\n"}
{"id": "3054008", "url": "https://en.wikipedia.org/wiki?curid=3054008", "title": "Sensitization", "text": "Sensitization\n\nSensitization is a non-associative learning process in which repeated administration of a stimulus results in the progressive amplification of a response. Sensitization often is characterized by an enhancement of response to a whole class of stimuli in addition to the one that is repeated. For example, repetition of a painful stimulus may make one more responsive to a loud noise.\nEric Kandel was one of the first to study the neural basis of sensitization, conducting experiments in the 1960s and 1970s on the gill withdrawal reflex of the seaslug \"Aplysia\". Kandel and his colleagues first habituated the reflex, weakening the response by repeatedly touching the animal's siphon. They then paired noxious electrical stimulus to the tail with a touch to the siphon, causing the gill withdrawal response to reappear. After this sensitization, a light touch to the siphon alone produced a strong gill withdrawal response, and this sensitization effect lasted for several days. (After Squire and Kandel, 1999). In 2000, Eric Kandel was awarded the Nobel Prize in Physiology or Medicine for his research in neuronal learning processes.\n\nThe neural basis of behavioral sensitization is often not known, but it typically seems to result from a cellular receptor becoming more likely to respond to a stimulus. Several examples of neural sensitization include:\n\nCross-sensitization is a phenomenon in which sensitization to a stimulus is generalized to a related stimulus, resulting in the amplification of a particular response to both the original stimulus and the related stimulus. For example, cross-sensitization to the neural and behavioral effects of addictive drugs are well characterized, such as sensitization to the locomotor response of a stimulant resulting in cross-sensitization to the motor-activating effects of other stimulants. Similarly, reward sensitization to a particular addictive drug often results in reward cross-sensitization, which entails sensitization to the rewarding property of other addictive drugs in the same drug class or even certain natural rewards.\n\nIn animals, \"cross-sensitization\" has been established between the consumption of many different types of drugs of abuse - in line with the gateway drug theory - and also between sugar consumption and the self-administration of drugs of abuse.\n\nSensitization has been implied as a causal or maintaining mechanism in a wide range of apparently unrelated pathologies including addiction, allergies, asthma, overactive bladder and some medically unexplained syndromes such as fibromyalgia and multiple chemical sensitivity. Sensitization may also contribute to psychological disorders such as post-traumatic stress disorder, panic anxiety and mood disorders.\n\n"}
{"id": "22513037", "url": "https://en.wikipedia.org/wiki?curid=22513037", "title": "Simulation algorithms for coupled DEVS", "text": "Simulation algorithms for coupled DEVS\n\nGiven a coupled DEVS model, simulation algorithms are methods to generate the model's \"legal\" behaviors, which are a set of trajectories not to reach illegal states. (see behavior of a Coupled DEVS model.) [Zeigler84] originally introduced the algorithms that handle time variables related to \"lifespan\" formula_1 and \"elapsed time\" formula_2 by introducing two other time variables, \"last event time\", formula_3, and \"next event time\" formula_4 with the following relations: formula_5\n\nand\n\nformula_6\n\nwhere formula_7 denotes the \"current time\". And the \"remaining time\",\n\nformula_9, apparently formula_10.\nBased on these relationships, the algorithms to simulate the behavior of a given Coupled DEVS are written as follows.\n\n DEVS-coordinator\n\n\n"}
{"id": "11389841", "url": "https://en.wikipedia.org/wiki?curid=11389841", "title": "Single-subject research", "text": "Single-subject research\n\nSingle-subject research is a group of research methods that are used extensively in the experimental analysis of behavior and applied behavior analysis with both human and non-human participants. Principal methods in this type of research are: A-B-A-B designs, Multi-element designs, Multiple Baseline designs, Repeated acquisition designs, Brief experimental designs and Combined designs.\n\nThese methods form the heart of the data collection and analytic code of behavior analysis. Behavior analysis is data driven, inductive, and disinclined to hypothetico-deductive methods. Statistical methods have been largely ignored.\n\nExperimental questions are decisive in determining the nature of the experimental design to be selected. There are four basic types of experimental questions: demonstration, comparison, parametric, and component. A demonstration is \"Does A cause or influence B?\". A comparison is \"Does A1 or A2 cause or influence B more?\". A parametric question is \"How much of A will cause how much change or influence on B?\". A component question is \"Which part of A{1,2,3} - A1 or A2 or A3... - causes or influences B?\" where A is composed of parts which can be separated and tested.\n\nThe A-B-A-B design is useful for demonstration questions.\n\nAn AB design is a two part or phase design composed of a baseline (\"A\" phase) with no changes, and a treatment or intervention (\"B\") phase. If there is a change then the treatment may be said to have had an effect. However, it is subject to many possible competing hypotheses, making strong conclusions difficult. Variants on the AB design introduce ways to control for the competing hypotheses to allow for stronger conclusions.\n\nThe reversal design is the most powerful of the single-subject research designs showing a strong reversal from baseline (\"A\") to treatment (\"B\") and back again. If the variable returns to baseline measure without a treatment then resumes its effects when reapplied, the researcher can have greater confidence in the efficacy of that treatment. However, many interventions cannot be reversed, some for ethical reasons (e.g., involving self-injurious behavior, smoking) and some for practical reasons (they cannot be unlearned, like a skill).\n\nFurther ethics notes: It may be unethical to end an experiment on a baseline measure if the treatment is self-sustaining and highly beneficial and/or related to health. Control condition participants may also deserve the benefits of research once all data has been collected. It is a researcher's ethical duty to maximize benefits and to ensure that all participants have access to those benefits when possible.\n\nThe A-B-C design is a variant that allows for the extension of research questions around component, parametric and comparative questions.\n\nMulti-element designs sometimes referred to as alternating-treatment designs are used in order to ascertain the comparative effect of two treatments. Two treatments are alternated in rapid succession and correlated changes are plotted on a graph to facilitate comparison.\n\nThe multiple baseline design was first reported in 1960 as used in basic operant research. It was applied in the late 1960s to human experiments in response to practical and ethical issues that arose in withdrawing apparently successful treatments from human subjects. In it two or more (often three) behaviors, people or settings are plotted in a staggered graph where a change is made to one, but not the other two, and then to the second, but not the third behavior, person or setting. Differential changes that occur to each behavior, person or in each setting help to strengthen what is essentially an AB design with its problematic competing hypotheses.\n\nIn addition to multiple baseline designs, a way to deal with problematic reversibility is the use of repeated acquisitions.\n\nA designed favored by applied settings researchers where logistical challenges, time and other limits make research difficult are variants of multi-element and A-B-A-B type designs.\n\nThe combined design has arisen from a need to obtain answers to more complex research questions. Combining two or more single-case designs, such as A-B-A-B and multiple baseline, may produce such answers.\n\nPopular in Verbal Behavior research, the multipleprobe research design has elements of the other research designs.\n\nIn a changing-criterion research design a criterion for reinforcement is changed across the experiment to demonstrate a functional relation between the reinforcement and the behavior.\n\n"}
{"id": "29248", "url": "https://en.wikipedia.org/wiki?curid=29248", "title": "Space colonization", "text": "Space colonization\n\nSpace colonization (also called space settlement, or extraterrestrial colonization) is permanent human habitation off the planet Earth.\n\nMany arguments have been made for and against space colonization. The two most common in favor of colonization are survival of human civilization and the biosphere in the event of a planetary-scale disaster (natural or man-made), and the availability of additional resources in space that could enable expansion of human society. The most common objections to colonization include concerns that the commodification of the cosmos may be likely to enhance the interests of the already powerful, including major economic and military institutions, and to exacerbate pre-existing detrimental processes such as wars, economic inequality, and environmental degradation.\n\nNo space colonies have been built so far. Currently, the building of a space colony would present a set of huge technological and economic challenges. Space settlements would have to provide for nearly all (or all) the material needs of hundreds or thousands of humans, in an environment out in space that is very hostile to human life. They would involve technologies, such as controlled ecological life support systems, that have yet to be developed in any meaningful way. They would also have to deal with the as-yet unknown issue of how humans would behave and thrive in such places long-term. Because of the present cost of sending anything from the surface of the Earth into orbit (around $2,500 per-pound to orbit, expected to further decrease), a space colony would currently be a massively expensive project.\n\nThere are yet no plans for building space colonies by any large-scale organization, either government or private. However, many proposals, speculations, and designs for space settlements have been made through the years, and a considerable number of space colonization advocates and groups are active. Several famous scientists, such as Freeman Dyson, have come out in favor of space settlement.\n\nOn the technological front, there is ongoing progress in making access to space cheaper (reusable launch systems could reach $10 per-pound to orbit), and in creating automated manufacturing and construction techniques.\n\nThe primary argument calling for space colonization is the long-term survival of human civilization. By developing alternative locations off Earth, the planet's species, including humans, could live on in the event of natural or man-made disasters on our own planet.\n\nOn two occasions, theoretical physicist and cosmologist Stephen Hawking has argued for space colonization as a means of saving humanity. In 2001, Hawking predicted that the human race would become extinct within the next thousand years, unless colonies could be established in space. In 2006, he stated that humanity faces two options: either we colonize space within the next two hundred years and build residential units on other planets, or we will face the prospect of long-term extinction.\n\nIn 2005, then NASA Administrator Michael Griffin identified space colonization as the ultimate goal of current spaceflight programs, saying:\nLouis J. Halle, formerly of the United States Department of State, wrote in \"Foreign Affairs\" (Summer 1980) that the colonization of space will protect humanity in the event of global nuclear warfare. The physicist Paul Davies also supports the view that if a planetary catastrophe threatens the survival of the human species on Earth, a self-sufficient colony could \"reverse-colonize\" Earth and restore human civilization. The author and journalist William E. Burrows and the biochemist Robert Shapiro proposed a private project, the Alliance to Rescue Civilization, with the goal of establishing an off-Earth \"backup\" of human civilization.\n\nBased on his Copernican principle, J. Richard Gott has estimated that the human race could survive for another 7.8 million years, but it is not likely to ever colonize other planets. However, he expressed a hope to be proven wrong, because \"colonizing other worlds is our best chance to hedge our bets and improve the survival prospects of our species\".\n\nResources in space, both in materials and energy, are enormous. The Solar System alone has, according to different estimates, enough material and energy to support anywhere from several thousand to over a billion times that of the current Earth-based human population. Outside the Solar System, several hundred billion other stars in the observable universe provide opportunities for both colonization and resource collection, though travel to any of them is impossible on any practical time-scale without interstellar travel by use of generation ships or revolutionary new methods of travel, such as faster-than-light (FTL).\n\nAsteroid mining will also be a key player in space colonization. Water and materials to make structures and shielding can be easily found in asteroids. Instead of resupplying on Earth, mining and fuel stations need to be established on asteroids to facilitate better space travel. Optical mining is the term NASA uses to describe extracting materials from asteroids. NASA believes by using propellant derived from asteroids for exploration to the moon, Mars, and beyond will save $100 billion. If funding and technology come sooner than estimated, asteroid mining might be possible within a decade.\n\nAll these planets and other bodies offer a virtually endless supply of resources providing limitless growth potential. Harnessing these resources can lead to much economic development.\n\nExpansion of humans and technological progress has usually resulted in some form of environmental devastation, and destruction of ecosystems and their accompanying wildlife. In the past, expansion has often come at the expense of displacing many indigenous peoples, the resulting treatment of these peoples ranging anywhere from encroachment to genocide. Because space has no known life, this need not be a consequence, as some space settlement advocates have pointed out.\n\nAnother argument for space colonization is to mitigate the negative effects of overpopulation.\nIf the resources of space were opened to use and viable life-supporting habitats were built, Earth would no longer define the limitations of growth. Although many of Earth's resources are non-renewable, off-planet colonies could satisfy the majority of the planet's resource requirements. With the availability of extraterrestrial resources, demand on terrestrial ones would decline.\n\nAdditional goals cite the innate human drive to explore and discover, a quality recognized at the core of progress and thriving civilizations.\n\nNick Bostrom has argued that from a utilitarian perspective, space colonization should be a chief goal as it would enable a very large population to live for a very long period of time (possibly billions of years), which would produce an enormous amount of utility (or happiness). He claims that it is more important to reduce existential risks to increase the probability of eventual colonization than to accelerate technological development so that space colonization could happen sooner. In his paper, he assumes that the created lives will have positive ethical value despite the problem of suffering.\n\nIn a 2001 interview with Freeman Dyson, J. Richard Gott and Sid Goldstein, they were asked for reasons why some humans should live in space. Their answers were:\n\nAlthough some items of the infrastructure requirements above can already be easily produced on Earth and would therefore not be very valuable as trade items (oxygen, water, base metal ores, silicates, etc.), other high value items are more abundant, more easily produced, of higher quality, or can only be produced in space. These would provide (over the long-term) a very high return on the initial investment in space infrastructure.\n\nSome of these high-value trade goods include precious metals, gemstones, power, solar cells, ball bearings, semi-conductors, and pharmaceuticals.\n\nThe mining and extraction of metals from a small asteroid the size of 3554 Amun or (6178) 1986 DA, both small near-Earth asteroids, would be 30 times as much metal as humans have mined throughout history. A metal asteroid this size would be worth approximately US$20 trillion at 2001 market prices.\n\nSpace colonization is seen as a long-term goal of some national space programs. Since the advent of the 21st-century commercialization of space, which saw greater cooperation between NASA and the private sector, several private companies have announced plans toward the colonization of Mars. Among entrepreneurs leading the call for space colonization are Elon Musk, Dennis Tito and Bas Lansdorp.\n\nThe main impediments to commercial exploitation of these resources are the very high cost of initial investment, the very long period required for the expected return on those investments (\"The Eros Project\" plans a 50-year development), and the fact that the venture has never been carried out before — the high-risk nature of the investment.\n\nMajor governments and well-funded corporations have announced plans for new categories of activities: space tourism and hotels, prototype space-based solar-power satellites, heavy-lift boosters and asteroid mining—that create needs and capabilities for humans to be present in space.\n\nBuilding colonies in space would require access to water, food, space, people, construction materials, energy, transportation, communications, life support, simulated gravity, radiation protection and capital investment. It is likely the colonies would be located near the necessary physical resources. The practice of space architecture seeks to transform spaceflight from a heroic test of human endurance to a normality within the bounds of comfortable experience. As is true of other frontier-opening endeavors, the capital investment necessary for space colonization would probably come from governments, an argument made by John Hickman and Neil deGrasse Tyson.\n\nColonies on the Moon, Mars, or asteroids could extract local materials. The Moon is deficient in volatiles such as argon, helium and compounds of carbon, hydrogen and nitrogen. The LCROSS impacter was targeted at the Cabeus crater which was chosen as having a high concentration of water for the Moon. A plume of material erupted in which some water was detected. Mission chief scientist Anthony Colaprete estimated that the Cabeus crater contains material with 1% water or possibly more. Water ice should also be in other permanently shadowed craters near the lunar poles. Although helium is present only in low concentrations on the Moon, where it is deposited into regolith by the solar wind, an estimated million tons of He-3 exists over all. It also has industrially significant oxygen, silicon, and metals such as iron, aluminum, and titanium.\n\nLaunching materials from Earth is expensive, so bulk materials for colonies could come from the Moon, a near-Earth object (NEO), Phobos, or Deimos. The benefits of using such sources include: a lower gravitational force, no atmospheric drag on cargo vessels, and no biosphere to damage. Many NEOs contain substantial amounts of metals. Underneath a drier outer crust (much like oil shale), some other NEOs are inactive comets which include billions of tons of water ice and kerogen hydrocarbons, as well as some nitrogen compounds.\n\nFarther out, Jupiter's Trojan asteroids are thought to be rich in water ice and other volatiles.\n\nRecycling of some raw materials would almost certainly be necessary.\n\nSolar energy in orbit is abundant, reliable, and is commonly used to power satellites today. There is no night in free space, and no clouds or atmosphere to block sunlight. Light intensity obeys an inverse-square law. So the solar energy available at distance \"d\" from the Sun is \"E\" = 1367/\"d\" W/m, where \"d\" is measured in astronomical units (AU) and 1367 watts/m is the energy available at the distance of Earth's orbit from the Sun, 1 AU.\n\nIn the weightlessness and vacuum of space, high temperatures for industrial processes can easily be achieved in solar ovens with huge parabolic reflectors made of metallic foil with very lightweight support structures. Flat mirrors to reflect sunlight around radiation shields into living areas (to avoid line-of-sight access for cosmic rays, or to make the Sun's image appear to move across their \"sky\") or onto crops are even lighter and easier to build.\n\nLarge solar power photovoltaic cell arrays or thermal power plants would be needed to meet the electrical power needs of the settlers' use. In developed parts of Earth, electrical consumption can average 1 kilowatt/person (or roughly 10 megawatt-hours per person per year.) These power plants could be at a short distance from the main structures if wires are used to transmit the power, or much farther away with wireless power transmission.\n\nA major export of the initial space settlement designs was anticipated to be large solar power satellites that would use wireless power transmission (phase-locked microwave beams or lasers emitting wavelengths that special solar cells convert with high efficiency) to send power to locations on Earth, or to colonies on the Moon or other locations in space. For locations on Earth, this method of getting power is extremely benign, with zero emissions and far less ground area required per watt than for conventional solar panels. Once these satellites are primarily built from lunar or asteroid-derived materials, the price of SPS electricity could be lower than energy from fossil fuel or nuclear energy; replacing these would have significant benefits such as the elimination of greenhouse gases and nuclear waste from electricity generation.\n\nTransmitting solar energy wirelessly from the Earth to the Moon and back is also an idea proposed for the benefit of space colonization and energy resources. Physicist Dr. David Criswell, who worked for NASA during the Apollo missions, came up with the idea of using power beams to transfer energy from space. These beams, microwaves with a wavelength of about 12 cm, will be almost untouched as they travel through the atmosphere. They can also be aimed at more industrial areas to keep away from humans or animal activities. This will allow for safer and more reliable methods of transferring solar energy.\n\nIn 2008, scientists were able to send a 20 watt microwave signal from a mountain in Maui to the island of Hawaii. Since then JAXA and Mitsubishi has teamed up on a $21 billion project in order to place satellites in orbit which could generate up to 1 gigawatt of energy. These are the next advancements being done today in order to make energy be transmitted wirelessly for space-based solar energy.\n\nHowever, the value of SPS power delivered wirelessly to other locations in space will typically be far higher than to Earth. Otherwise, the means of generating the power would need to be included with these projects and pay the heavy penalty of Earth launch costs. Therefore, other than proposed demonstration projects for power delivered to Earth, the first priority for SPS electricity is likely to be locations in space, such as communications satellites, fuel depots or \"orbital tugboat\" boosters transferring cargo and passengers between low-Earth orbit (LEO) and other orbits such as geosynchronous orbit (GEO), lunar orbit or highly-eccentric Earth orbit (HEEO). The system will also rely on satellites and receiving stations on Earth to convert the energy into electricity. Because of this energy can be transmitted easily from dayside to nightside meaning power is reliable 24/7.\n\nNuclear power is sometimes proposed for colonies located on the Moon or on Mars, as the supply of solar energy is too discontinuous in these locations; the Moon has nights of two Earth weeks in duration. Mars has nights, relatively high gravity, and an atmosphere featuring large dust storms to cover and degrade solar panels. Also, Mars' greater distance from the Sun (1.5 astronomical units, AU) translates into \"E/(1.5 = 2.25)\" only ½-⅔ the solar energy of Earth orbit. Another method would be transmitting energy wirelessly to the lunar or Martian colonies from solar power satellites (SPSs) as described above; the difficulties of generating power in these locations make the relative advantages of SPSs much greater there than for power beamed to locations on Earth. In order to also be able to fulfill the requirements of a moon base and energy to supply life support, maintenance, communications, and research, a combination of both nuclear and solar energy will be used in the first colonies.\n\nFor both solar thermal and nuclear power generation in airless environments, such as the Moon and space, and to a lesser extent the very thin Martian atmosphere, one of the main difficulties is dispersing the inevitable heat generated. This requires fairly large radiator areas.\n\nIn space settlements, a life support system must recycle or import all the nutrients without \"crashing.\" The closest terrestrial analogue to space life support is possibly that of a nuclear submarine. Nuclear submarines use mechanical life support systems to support humans for months without surfacing, and this same basic technology could presumably be employed for space use. However, nuclear submarines run \"open loop\"—extracting oxygen from seawater, and typically dumping carbon dioxide overboard, although they recycle existing oxygen. Recycling of the carbon dioxide has been approached in the literature using the Sabatier process or the Bosch reaction.\n\nAlthough a fully mechanistic life support system is conceivable, a closed ecological system is generally proposed for life support. The Biosphere 2 project in Arizona has shown that a complex, small, enclosed, man-made biosphere can support eight people for at least a year, although there were many problems. A year or so into the two-year mission oxygen had to be replenished, which strongly suggests that they achieved atmospheric closure.\n\nThe relationship between organisms, their habitat and the non-Earth environment can be:\n\nA combination of the above technologies is also possible.\n\nCosmic rays and solar flares create a lethal radiation environment in space. In Earth orbit, the Van Allen belts make living above the Earth's atmosphere difficult. To protect life, settlements must be surrounded by sufficient mass to absorb most incoming radiation, unless magnetic or plasma radiation shields were developed.\n\nPassive mass shielding of four metric tons per square meter of surface area will reduce radiation dosage to several mSv or less annually, well below the rate of some populated high natural background areas on Earth. This can be leftover material (slag) from processing lunar soil and asteroids into oxygen, metals, and other useful materials. However, it represents a significant obstacle to maneuvering vessels with such massive bulk (mobile spacecraft being particularly likely to use less massive active shielding). Inertia would necessitate powerful thrusters to start or stop rotation, or electric motors to spin two massive portions of a vessel in opposite senses. Shielding material can be stationary around a rotating interior.\n\nSpace manufacturing could enable self-replication. Some think it's the ultimate goal because it allows an exponential increase in colonies, while eliminating costs to and dependence on Earth. It could be argued that the establishment of such a colony would be Earth's first act of self-replication. Intermediate goals include colonies that expect only information from Earth (science, engineering, entertainment) and colonies that just require periodic supply of light weight objects, such as integrated circuits, medicines, genetic material and tools.\n\nThe monotony and loneliness that comes from a prolonged space mission can leave astronauts susceptible to cabin fever or having a psychotic break. Moreover, lack of sleep, fatigue, and work overload can affect an astronaut's ability to perform well in an environment such as space where every action is critical.\n\nIn 2002, the anthropologist John H. Moore estimated that a population of 150–180 would permit a stable society to exist for 60 to 80 generations — equivalent to 2000 years.\n\nA much smaller initial population of as little as two women should be viable as long as human embryos are available from Earth. Use of a sperm bank from Earth also allows a smaller starting base with negligible inbreeding.\n\nResearchers in conservation biology have tended to adopt the \"50/500\" rule of thumb initially advanced by Franklin and Soule. This rule says a short-term effective population size (\"N\") of 50 is needed to prevent an unacceptable rate of inbreeding, whereas a long‐term \"N\" of 500 is required to maintain overall genetic variability. The \"N\" = 50 prescription corresponds to an inbreeding rate of 1% per generation, approximately half the maximum rate tolerated by domestic animal breeders. The \"N\" = 500 value attempts to balance the rate of gain in genetic variation due to mutation with the rate of loss due to genetic drift.\n\nAssuming a journey of 6,300 years, the astrophysicist Frédéric Marin and the particle physicist Camille Beluffi calculated that the minimum viable population for a generation ship to reach Proxima Centauri would be 98 settlers at the beginning of the mission (then the crew will breed until reaching a stable population of several hundred settlers within the ship) .\n\nLocation is a frequent point of contention between space colonization advocates. The location of colonization can be on a physical body planet, dwarf planet, natural satellite, or asteroid or orbiting one. For colonies not on a body see also space habitat.\n\nDue to its proximity and familiarity, Earth's Moon is discussed as a target for colonization. It has the benefits of proximity to Earth and lower escape velocity, allowing for easier exchange of goods and services. A drawback of the Moon is its low abundance of volatiles necessary for life such as hydrogen, nitrogen, and carbon. Water-ice deposits that exist in some polar craters could serve as a source for these elements. An alternative solution is to bring hydrogen from near-Earth asteroids and combine it with oxygen extracted from lunar rock.\n\nThe Moon's low surface gravity is also a concern, as it is unknown whether 1/6g is enough to maintain human health for long periods. \n\nThe Moon's lack of atmosphere provides no protection from space radiation or meteoroids. The early Moon colonies may shelter in ancient Lunar lava tubes to gain protection. The two-week day/night cycle makes use of solar power more difficult.\n\nAnother near-Earth possibility are the five Earth–Moon Lagrange points. Although they would generally also take a few days to reach with current technology, many of these points would have near-continuous solar power because their distance from Earth would result in only brief and infrequent eclipses of light from the Sun. However, the fact that the Earth–Moon Lagrange points and tend to collect dust and debris, whereas - require active station-keeping measures to maintain a stable position, make them somewhat less suitable places for habitation than was originally believed. Additionally, the orbit of – takes them out of the protection of the Earth's magnetosphere for approximately two-thirds of the time, exposing them to the health threat from cosmic rays.\n\nThe five Earth–Sun Lagrange points would totally eliminate eclipses, but only and would be reachable in a few days' time. The other three Earth–Sun points would require months to reach.\n\nSee space habitat\n\nColonizing Mercury would involve similar challenges as the Moon as there are few volatile elements, no atmosphere and the surface gravity is lower than Earth's. However, the planet also receives almost seven times the solar flux as the Earth/Moon system.\n\nGeologist Stephen Gillett suggested in 1996 that this could make Mercury an ideal place to build and launch solar sail spacecraft, which could launch as folded up \"chunks\" by mass driver from Mercury's surface. Once in space the solar sails would deploy. Since Mercury's solar constant is 6.5 times higher than Earth's, energy for the mass driver should be easy to come by, and solar sails near Mercury would have 6.5 times the thrust they do near Earth. This could make Mercury an ideal place to acquire materials useful in building hardware to send to (and terraform) Venus. Vast solar collectors could also be built on or near Mercury to produce power for large scale engineering activities such as laser-pushed lightsails to nearby star systems.\n\nColonization of asteroids would require space habitats. The asteroid belt has significant overall material available, the largest object being Ceres, although it is thinly distributed as it covers a vast region of space. Unmanned supply craft should be practical with little technological advance, even crossing 500 million kilometers of space. The colonists would have a strong interest in assuring their asteroid did not hit Earth or any other body of significant mass, but would have extreme difficulty in moving an asteroid of any size. The orbits of the Earth and most asteroids are very distant from each other in terms of delta-v and the asteroidal bodies have enormous momentum. Rockets or mass drivers can perhaps be installed on asteroids to direct their path into a safe course.\n\nThe Artemis Project designed a plan to colonize Europa, one of Jupiter's moons. Scientists were to inhabit igloos and drill down into the Europan ice crust, exploring any sub-surface ocean. This plan discusses possible use of \"air pockets\" for human habitation. Europa is considered one of the more habitable bodies in the Solar System and so merits investigation as a possible abode for life.\n\nNASA performed a study called \"HOPE\" (Revolutionary Concepts for Human Outer Planet Exploration) regarding the future exploration of the Solar System. The target chosen was Callisto due to its distance from Jupiter, and thus the planet's harmful radiation. It could be possible to build a surface base that would produce fuel for further exploration of the Solar System.\n\nThree of the Galilean moons (Europa, Ganymede, Callisto) have an abundance of volatiles that may support colonization efforts.\n\nTitan is suggested as a target for colonization, because it is the only moon in the Solar System to have a dense atmosphere and is rich in carbon-bearing compounds. Titan has ice water and large methane oceans. Robert Zubrin identified Titan as possessing an abundance of all the elements necessary to support life, making Titan perhaps the most advantageous locale in the outer Solar System for colonization, and saying \"In certain ways, Titan is the most hospitable extraterrestrial world within our solar system for human colonization\".\n\nEnceladus is a small, icy moon orbiting close to Saturn, notable for its extremely bright surface and the geyser-like plumes of ice and water vapor that erupt from its southern polar region. If Enceladus has liquid water, it joins Mars and Jupiter's moon Europa as one of the prime places in the Solar System to look for extraterrestrial life and possible future settlements.\n\nOther large satellites: Rhea, Iapetus, Dione, Tethys, and Mimas, all have large quantities of volatiles, which can be used to support settlements.\n\nThe Kuiper belt is estimated to have 70,000 bodies of 100 km or larger.\n\nFreeman Dyson has suggested that within a few centuries human civilization will have relocated to the Kuiper belt.\n\nThe Oort cloud is estimated to have up to a trillion comets.\n\nLooking beyond the Solar System, there are up to several hundred billion potential stars with possible colonization targets. The main difficulty is the vast distances to other stars: roughly a hundred thousand times further away than the planets in the Solar System. This means that some combination of very high speed (some percentage of the speed of light), or travel times lasting centuries or millennia, would be required. These speeds are far beyond what current spacecraft propulsion systems can provide.\n\nMany scientific papers have been published about interstellar travel. Given sufficient travel time and engineering work, both unmanned and generational voyages seem possible, though representing a very considerable technological and economic challenge unlikely to be met for some time, particularly for manned probes.\n\nSpace colonization technology could in principle allow human expansion at high, but sub-relativistic speeds, substantially less than the speed of light, \"c\".  An interstellar colony ship would be similar to a space habitat, with the addition of major propulsion capabilities and independent energy generation.\n\nHypothetical starship concepts proposed both by scientists and in hard science fiction include:\n\nThe above concepts all appear limited to high, but still sub-relativistic speeds, due to fundamental energy and reaction mass considerations, and all would entail trip times which might be enabled by space colonization technology, permitting self-contained habitats with lifetimes of decades to centuries. Yet human interstellar expansion at average speeds of even 0.1% of \"c\"  would permit settlement of the entire Galaxy in less than one half of a galactic rotation period of ~250,000,000 years, which is comparable to the timescale of other galactic processes. Thus, even if interstellar travel at near relativistic speeds is never feasible (which cannot be clearly determined at this time), the development of space colonization could allow human expansion beyond the Solar System without requiring technological advances that cannot yet be reasonably foreseen. This could greatly improve the chances for the survival of intelligent life over cosmic timescales, given the many natural and human-related hazards that have been widely noted.\n\nIf humanity does gain access to a large amount of energy, on the order of the mass-energy of entire planets, it may eventually become feasible to construct Alcubierre drives. These are one of the few methods of superluminal travel which may be possible under current physics. However it is probable that such a device could never exist, due to the fundamental challenges posed. For more on this see Difficulties of making and using an Alcubierre Drive.\n\nLooking beyond the Milky Way, there are at least 2 trillion other galaxies in the observable universe. The distances between galaxies are on the order of a million times farther than those between the stars. Because of the speed of light limit on how fast any material objects can travel in space, intergalactic travel would either have to involve voyages lasting millions of years, or a possible faster than light propulsion method based on speculative physics, such as the Alcubierre drive. There are, however, no scientific reasons for stating that intergalactic travel is impossible in principle.\n\nSpace colonization can roughly be said to be possible when the necessary methods of space colonization become cheap enough (such as space access by cheaper launch systems) to meet the cumulative funds that have been gathered for the purpose.\n\nAlthough there are no immediate prospects for the large amounts of money required for space colonization to be available given traditional launch costs,\nthere is some prospect of a radical reduction to launch costs in the 2010s, which would consequently lessen the cost of any efforts in that direction. With a published price of per launch of up to payload to low Earth orbit, SpaceX Falcon 9 rockets are already the \"cheapest in the industry\". Advancements currently being developed as part of the SpaceX reusable launch system development program to enable reusable Falcon 9s \"could drop the price by an order of magnitude, sparking more space-based enterprise, which in turn would drop the cost of access to space still further through economies of scale.\" If SpaceX is successful in developing the reusable technology, it would be expected to \"have a major impact on the cost of access to space\", and change the increasingly competitive market in space launch services.\n\nThe President's Commission on Implementation of United States Space Exploration Policy suggested that an inducement prize should be established, perhaps by government, for the achievement of space colonization, for example by offering the prize to the first organization to place humans on the Moon and sustain them for a fixed period before they return to Earth.\n\nThe most famous attempt to build an analogue to a self-sufficient colony is Biosphere 2, which attempted to duplicate Earth's biosphere. BIOS-3 is another closed ecosystem, completed in 1972 in Krasnoyarsk, Siberia.\n\nMany space agencies build testbeds for advanced life support systems, but these are designed for long duration human spaceflight, not permanent colonization.\n\nRemote research stations in inhospitable climates, such as the Amundsen–Scott South Pole Station or Devon Island Mars Arctic Research Station, can also provide some practice for off-world outpost construction and operation. The Mars Desert Research Station has a habitat for similar reasons, but the surrounding climate is not strictly inhospitable.\n\nThe first known work on space colonization was \"The Brick Moon\", a work of fiction published in 1869 by Edward Everett Hale, about an inhabited artificial satellite.\n\nThe Russian schoolmaster and physicist Konstantin Tsiolkovsky foresaw elements of the space community in his book \"Beyond Planet Earth\" written about 1900. Tsiolkovsky had his space travelers building greenhouses and raising crops in space. Tsiolkovsky believed that going into space would help perfect human beings, leading to immortality and peace.\n\nOthers have also written about space colonies as Lasswitz in 1897 and Bernal, Oberth, Von Pirquet and Noordung in the 1920s. Wernher von Braun contributed his ideas in a 1952 \"Colliers\" article. In the 1950s and 1960s, Dandridge M. Cole published his ideas.\n\nAnother seminal book on the subject was the book \"The High Frontier: Human Colonies in Space\" by Gerard K. O'Neill in 1977 which was followed the same year by \"Colonies in Space \" by T. A. Heppenheimer.\n\nM. Dyson wrote \"Home on the Moon; Living on a Space Frontier\" in 2003; Peter Eckart wrote \"Lunar Base Handbook\" in 2006 and then Harrison Schmitt's \"Return to the Moon\" written in 2007.\n\n, Bigelow Aerospace is the only private commercial spaceflight company that has launched two experimental space station modules, Genesis I (2006) and Genesis II (2007), into Earth-orbit, and has indicated that their first production model of the space habitat, the BA 330, could be launched by 2017.\n\nRobotic spacecraft to Mars are required to be sterilized, to have at most 300,000 spores on the exterior of the craft—and more thoroughly sterilized if they contact \"special regions\" containing water, otherwise there is a risk of contaminating not only the life-detection experiments but possibly the planet itself.\n\nIt is impossible to sterilize human missions to this level, as humans are host to typically a hundred trillion microorganisms of thousands of species of the human microbiome, and these cannot be removed while preserving the life of the human. Containment seems the only option, but it is a major challenge in the event of a hard landing (i.e. crash). There have been several planetary workshops on this issue, but with no final guidelines for a way forward yet. Human explorers would also be vulnerable to back contamination to Earth if they become carriers of microorganisms.\n\nA corollary to the Fermi paradox—\"nobody else is doing it\"—is the argument that, because no evidence of alien colonization technology exists, it is statistically unlikely to even be possible to use that same level of technology ourselves.\n\nColonizing space would require massive amounts of financial, physical, and human capital devoted to research, development, production, and deployment. Earth's natural resources do not increase to a noteworthy extent (which is in keeping with the \"only one Earth\" position of environmentalists). Thus, considerable efforts in colonizing places outside Earth would appear as a hazardous waste of the Earth's limited resources for an aim without a clear end.\n\nThe fundamental problem of public things, needed for survival, such as space programs, is the free rider problem. Convincing the public to fund such programs would require additional self-interest arguments: If the objective of space colonization is to provide a \"backup\" in case everyone on Earth is killed, then why should someone on Earth pay for something that is only useful after they are dead? This assumes that space colonization is not widely acknowledged as a sufficiently valuable social goal.\n\nSeen as a relief to the problem of overpopulation even as early as 1758, and listed as one of Stephen Hawking's reasons for pursuing space exploration, it has become apparent that space colonisation in response to overpopulation is unwarranted. Indeed, the birth rates of many developed countries, specifically spacefaring ones, are at or below replacement rates, thus negating the need to use colonisation as a means of population control.\n\nOther objections include concerns that the forthcoming colonization and commodification of the cosmos may be likely to enhance the interests of the already powerful, including major economic and military institutions e.g. the large financial institutions, the major aerospace companies and the military–industrial complex, to lead to new wars, and to exacerbate pre-existing exploitation of workers and resources, economic inequality, poverty, social division and marginalization, environmental degradation, and other detrimental processes or institutions.\n\nAdditional concerns include creating a culture in which humans are no longer seen as human, but rather as material assets. The issues of human dignity, morality, philosophy, culture, bioethics, and the threat of megalomaniac leaders in these new \"societies\" would all have to be addressed in order for space colonization to meet the psychological and social needs of people living in isolated colonies.\n\nAs an alternative or addendum for the future of the human race, many science fiction writers have focused on the realm of the 'inner-space', that is the computer-aided exploration of the human mind and human consciousness—possibly en route developmentally to a Matrioshka Brain.\n\nRobotic exploration is proposed as an alternative to gain many of the same scientific advantages without the limited mission duration and high cost of life support and return transportation involved in manned missions.\n\nAnother concern is the potential to cause interplanetary contamination on planets that may harbor hypothetical extraterrestrial life.\n\nAn additional concern is the health of the humans who may participate in a colonization venture, including a range of physical, mental and emotional health risks.\n\nOrganizations that contribute to space colonization include:\n\nAlthough established space colonies are a stock element in science fiction stories, fictional works that explore the themes, social or practical, of the settlement and occupation of a habitable world are much rarer.\n\n"}
{"id": "49651604", "url": "https://en.wikipedia.org/wiki?curid=49651604", "title": "Subterminal object", "text": "Subterminal object\n\nIn category theory, a branch of mathematics, a subterminal object is an object \"X\" of a category \"C\" with the property that every object of \"C\" has at most one morphism into \"X\". If \"X\" is subterminal, then the pair of identity morphisms (1, 1) makes \"X\" into the product of \"X\" and \"X\". If \"C\" has a terminal object 1, then an object \"X\" is subterminal if and only if it is a subobject of 1, hence the name. The category of categories with subterminal objects and functors preserving them is not accessible.\n"}
{"id": "5670581", "url": "https://en.wikipedia.org/wiki?curid=5670581", "title": "System of systems engineering", "text": "System of systems engineering\n\nSystem of systems engineering (SoSE) is a set of developing processes, tools, and methods for designing, re-designing and deploying solutions to system-of-systems challenges.\n\nSystem of Systems Engineering (SoSE) methodology is heavily used in U.S. Department of Defense applications, but is increasingly being applied to non-defense related problems such as architectural design of problems in air and auto transportation, healthcare, global communication networks, search and rescue, space exploration and many other System of Systems application domains. SoSE is more than systems engineering of monolithic, complex systems because design for System-of-Systems problems is performed under some level of uncertainty in the requirements and the constituent systems, and it involves considerations in multiple levels and domains (as per and ). Whereas systems engineering focuses on building the system right, SoSE focuses on choosing the right system(s) and their interactions to satisfy the requirements.\n\nSystem-of-Systems Engineering and Systems Engineering are related but different fields of study. Whereas systems engineering addresses the development and operations of monolithic products, SoSE addresses the development and operations of evolving programs. In other words, traditional systems engineering seeks to optimize an individual system (i.e., the product), while SoSE seeks to optimize network of various interacting legacy and new systems brought together to satisfy multiple objectives of the program. SoSE should enable the decision-makers to understand the implications of various choices on technical performance, costs, extensibility and flexibility over time; thus, effective SoSE methodology should prepare decision-makers to design informed architectural solutions for System-of-Systems problems.\n\nDue to varied methodology and domains of applications in existing literature, there does not exist a single unified consensus for processes involved in System-of-Systems Engineering. One of the proposed SoSE frameworks, by Dr. Daniel A. DeLaurentis, recommends a three-phase method where a SoS problem is defined (understood), abstracted, modeled and analyzed for behavioral patterns. More information on this method and other proposed methods can be found in the listed SoSE focused organizations and SoSE literature in the subsequent sections.\n\n\n\n"}
{"id": "36038511", "url": "https://en.wikipedia.org/wiki?curid=36038511", "title": "Technobiophilia", "text": "Technobiophilia\n\nTechnobiophilia is 'the innate tendency to focus on life and lifelike processes as they appear in technology'.\n\nThe concept was devised by Sue Thomas as an extrapolation of the Biophilia hypothesis introduced by biologist Edward O. Wilson in his book \"Biophilia\" (1984). where he defines biophilia as 'the innate tendency to focus on life and lifelike processes.'\n\nTechnobiophilic practices and artefacts have one or more of the following features. They\n\nTimothy Beatley, Teresa Heinz Professor of Sustainable Communities in the Department of Urban and Environmental Planning at the University of Virginia School of Architecture, has written of technobiophilia \"We can look forward to the promise and potential of technobiophilic cities, that at once commit to restoring and enjoying actual nature, but acknowledge the realities of life in cities (much of it inside, and behind a screen), and the powerful ways in which our digital technologies could underpin and help to reinforce our nature-ful commitments and experiences and our biophilic tendencies.\" \n\n"}
{"id": "1067753", "url": "https://en.wikipedia.org/wiki?curid=1067753", "title": "Telos", "text": "Telos\n\nA telos (from the Greek τέλος for \"end\", \"purpose\", or \"goal\") is an end or purpose, in a fairly constrained sense used by philosophers such as Aristotle. It is the root of the term \"teleology\", roughly the study of purposiveness, or the study of objects with a view to their aims, purposes, or intentions. Teleology figures centrally in Aristotle's biology and in his theory of causes. It is central to nearly all philosophical theories of history, such as those of Hegel and Marx. One running debate in modern philosophy of biology is to what extent teleological language (as in the \"purposes\" of various organs or life-processes) is unavoidable, or is simply a shorthand for ideas that can ultimately be spelled out non-teleologically. Philosophy of action also makes essential use of teleological vocabulary: on Davidson's account, an action is just something an agent does with an intention—that is, looking forward to some end to be achieved by the action.\n\nIn contrast to telos, techne is the rational method involved in producing an object or accomplishing a goal or objective; however, the two methods are not mutually exclusive in principle.\n\n\n"}
{"id": "5479856", "url": "https://en.wikipedia.org/wiki?curid=5479856", "title": "Transitional justice", "text": "Transitional justice\n\nTransitional justice consists of judicial and non-judicial measures implemented in order to redress legacies of human rights abuses. Such measures \"include criminal prosecutions, truth commissions, reparations programs, and various kinds of institutional reforms\". Transitional justice is enacted at a point of political transition from violence and repression to societal stability and it is informed by a society’s desire to rebuild social trust, repair a fractured justice system, and build a democratic system of governance. The core value of transitional justice is the very notion of justice—which does not necessarily mean criminal justice. This notion and the political transformation, such as regime change or transition from conflict are thus linked toward a more peaceful, certain, and democratic future.\n\nTransitional justice has recently received greater attention by both academics and policymakers. It has also generated interest in the fields of political and legal discourse, especially in transitional societies. In period of political transitions, from authoritarian, dictatorial regimes or from civil conflicts to democracy, transitional justice has often provided opportunities for such societies to address past human rights abuses, mass atrocities, or other forms of severe trauma in order to facilitate a smooth transition into a more democratic or peaceful future.\n\nThe origins of the transitional justice field can be traced back to the post-World War II period in Europe with the establishment of the International Military Tribunal at Nuremberg and the various de-Nazification programs in Germany and the trials of Japanese soldiers at Tokyo Tribunal. What became known as the \"Nuremberg Trials\", when the victorious allied forces extended criminal justice to Japanese and German soldiers and their leaders for war crimes committed during the war, marked the genesis of transitional justice. The field gained momentum and coherence during the 1980s and onwards, beginning with the trials of former members of the military juntas in Greece (1975), and Argentina (Trial of the Juntas, 1983). The focus of transitional justice in the 1970s and 1980s was on criminal justice with a focus on human rights promotion. This led to a worldwide focus and progressive rise of human rights regime culminating in the establishments of international human rights laws and conventions.\n\nThe emphasis of transitional justice was on how abuses of human rights get treated during political transition: legal and criminal prosecution. As noted earlier, the universal conceptions of \"justice\" became the platform on which transitional justice was premised. The field in its early epistemology, thus, assumed jurisprudence of human rights. As a result, the initial literature on transitional justice was dominated by lawyers, law, and legal rights: defining laws, and processes on how to deal with human rights abuse and holding people accountable. Thus, transitional justice has its roots in both the human rights movement and in international human rights and humanitarian law. These origins in the human rights movement have rendered transitional justice “self-consciously victim-centric”.\n\nThe late 1980s and early 1990s saw a shift in the focus of transitional justice. Informed by the worldwide wave of democratization, particularly the third wave, transitional justice reemerged as a new field of study in democratization. Transitional justice broadened its scope from more narrow questions of jurisprudence to political considerations of developing stable democratic institutions and renewing civil society. Studies by scholars on the transition from autocratic regimes to democratic ones have integrated the transitional justice framework into an examination of the political processes inherent to democratic change. The challenges of democratization in transitional periods are many: settling past accounts without derailing democratic progress, developing judicial or third-party fora capable of resolving conflicts, reparations, and creating memorials and developing educational curricula that redress cultural lacunae and unhealed trauma.\n\nIt is clear that elements of transitional justice have broken the initial mold of post-war jurisprudence. The transitional justice framework has benefited from democratic activists who sought to bolster fledgling democracies and bring them into line with the moral and legal obligations articulated in the international human rights consensus.\n\nOne particular innovation is the appearance of truth commissions. Beginning with Argentina in 1983, Chile in 1990, and South Africa in 1995, truth commissions have become a symbol of transitional justice, appearing in transitional societies in Latin America, Africa, Asia, and Eastern Europe. However, several attempts to create a regional truth commission in the former Yugoslavia (REKOM) have failed due to political obstacles. Recent years have also seen proposals for truth and reconciliation commissions in conflict zones of the Middle East and it is likely that these transitional justice institutions will someday figure prominently in Israel and Palestine, Iraq, Lebanon, and the Kurdish regions.\n\nAnother major institutional innovation is the appearance of the variety of lustration programs in Central and Eastern Europe since the 1990s. While most countries pursued programs based on dismissals of compromised personnel and comprehensive screening tools, other countries implemented more inclusive methods allowing discredited personnel a second chance.\n\nAs a link between transition and justice, the concept of transitional justice transformed in the late 1940s to assume a broader perspective of comprehensive examination of the society in transition from a retrospective to a prospective position with democratic consolidation as one of the primary objectives. Scholars and practitioners of democratization have come to a common conclusion on the general principles of a transitional justice framework: that national strategies to confront past abuses, depending on the specific nature and context of the country in question, can contribute to accountability, an end to impunity, reconstruct state–citizen relations, and the creation of democratic institutions.\n\nThe primary objective of a transitional justice policy is to end the culture of impunity and establish the rule of law in a context of democratic governance. The legal and human rights protection roots of transitional justice impute certain legal obligations on states undergoing transitions. It challenges such societies to strive for a society where respect for human rights is the core and accountability is routinely practiced as the main goals. In the context of these goals, transitional justice aims at:\n\n\nIn general, therefore, one can identify eight broad objectives that transitional justice aims to serve: establishing the truth, providing victims a public platform, holding perpetrators accountable, strengthening the rule of law, providing victims with compensation, effectuating institutional reform, promoting reconciliation, and promoting public deliberation.\n\nIn order to be effective, transitional justice measures should be part of a holistic approach. There are five broad strategies or forms of transitional justice: 1) Criminal prosecutions, 2) Truth commissions, 3) Reparation programs, 4) Security sector reform, 5) Memorialization efforts. The needs of the survivors are essential and including gender justice is necessary to ensure women have equal access to the mechanisms.\n\nThe investigation and prosecution of serious international crimes, such as genocide, crimes against humanity, and war crimes helps to strengthen the rule of law by sanctioning those who violate laws with criminal penalties. It also demonstrates that crime will not be tolerated, and that human rights abusers will be held accountable for their actions. From its historical roots in the Nuremberg Trials, recent examples have included International Criminal Tribunal for Rwanda and International Criminal Tribunal for the former Yugoslavia, hybrid courts such as Special Court for Sierra Leone, Special Panels of the Dili District Court, Extraordinary Chambers in the Courts of Cambodia, Court of Bosnia and Herzegovina, and the establishment of the International Criminal Court (ICC), assuming a universal jurisdiction. The ICC and Hybrid Courts/Tribunals are key components of prosecution initiatives:\n\nThe International Criminal Court (ICC) was established by the Rome Statute in 1998. It is the first international criminal court that helps end impunity for perpetrators of severe crimes. It was established to investigate and try leaders of genocide, war crimes, and crimes against humanity in cases where countries are unable or unwilling to do so.\n\nHybrid courts and tribunals have emerged as “third generation” courts established to investigate and prosecute human rights offenses. They follow the “first generation” Nuremberg and Tokyo tribunals and the “second generation” International Criminal Court and International Criminal Tribunals for the former Yugoslavia (ICTY) and Rwanda (ICTR). These courts consist of both international and domestic justice actors. They attempt to deliver justice that the domestic justice systems cannot provide alone due to lack of capacity or political will. Furthermore, hybrid courts attempt to strengthen domestic capacities to prosecute human rights abuses through the transfer of international legal skills and expertise. Examples include the Special Court for Sierra Leone and the Extraordinary Chambers in the Courts of Cambodia. A step beyond within a more societal and traditional scope of justice could be the Gacaca for Rwanda.\n\nReparations aim to repair the suffering of victims of human rights abuses. They seek to make amends with victims, help them overcome the consequences of abuse, and provide rehabilitation. They may include financial payments, social services including health care or education, or symbolic compensation such as public apologies. One example is the Canadian government’s apology “Statement of Reconciliation” to indigenous Canadian families for removing their children and placing them in church-run Indian Residential Schools. The Canadian government also created a $350 million fund to help those affected by the schools.\n\nTruth-seeking encompasses initiatives allowing actors in a country to investigate past abuses and seek redress for victims. These processes aim to enable societies to examine and come to terms with past crimes and human rights violations in order to prevent their recurrence. They help create documentation that prevents repressive regimes from rewriting history and denying the past. They can also help victims obtain closure by knowing the truth about what actually happened (such as to “disappeared” people) and understanding the atrocities they endured. Truth-seeking measures may include freedom of information legislation, declassification of archives, investigations, and truth commissions.\n\nTruth commissions are non-judicial commissions of inquiry that aim to discover and reveal past abuses by a government or non-state actors; about forty official truth commissions have been created worldwide. One example is the Truth and Reconciliation Commission in South Africa, which was established to help overcome apartheid and reconcile tensions in the country.\n\nMemorials seek to preserve memories of people or events. In the context of transitional justice, they serve to honor those who died during conflict or other atrocities, examine the past, address contemporary issues and show respect to victims. They can help create records to prevent denial and help societies move forward. Memorials may include commemoration activities, such as architectural memorials, museums, and other commemorative events. One example includes the monuments, annual prayer ceremony, and mass grave in northern Uganda, created in response to the war conducted by and against the Lord’s Resistance Army there.\n\nPublic institutions, including the police, military, and judiciary, often contribute to repression and other human rights violations. When societies undergo a transition, these institutions must be reformed in order to create accountability and prevent the recurrence of abuse. Institutional reform includes the process of restructuring these state actors to ensure that they respect human rights and abide by the rule of law.\n\nReforms can include measures such as vetting, lustration, and Disarmament, Demobilization and Reintegration (DDR). Vetting is the process of eliminating corrupt or abusive officials from public service employment. For instance, in Afghanistan, election candidates in the 2009 and 2010 elections were vetted. While similar to lustration, \"vetting\" is the broader category referring to processes aimed at screening and excluding human rights abusers from public institutions while \"lustration\" refers specifically to the vetting processes and laws that were implemented in the former communist countries in Eastern and Central Europe after the end of the Cold War. Vetting does not necessarily imply dismissals from the state apparatus. Several countries developed alternative personnel systems that provide for the inclusion of inherited personnel in exchange for their exposure or confession. DDR programs assist ex‑combatants in rejoining society.\n\nOne example of institutional reform is the removal of court officials involved in crimes of the fallen Tunisian regime. Under Ben Ali’s rule, courts often facilitated corruption. The removal of implicated officials is a part of the government’s efforts to reconcile this abuse.\n\nStates in times of transition to democracy, since the early 1980s, have been using a variety of transitional justice mechanisms as part of measures to account for the past and build a future democratic state. Mechanisms, such as trials, truth commissions, reparations, lustration, museums, and other memory sites have been employed either single-handedly or in a combined form to address past human rights violations. Diverse studies ranging from the decision-making process of a choice of strategy through to the implementation of the transitional justice policy and impacts on the transition and future stability of the society in question have been produced by scholars in recent years. One illuminating study in particular that has documented the dramatic new trend of transitional justice and democratization is by Kathryn Sikkink and Carrie Booth Walling (2006). In their research paper described as the \"justice cascade\", Sikkink and Walling conducting analysis of truth commissions and human rights trials occurring throughout the world from 1979 to 2004 revealed a significant increase in the judicialization of world politics both regionally and internationally. Of the 192 countries surveyed, 34 have used truth commissions, and 50 had at least one transitional human rights trial.\n\nMore than two-thirds of the approximately 85 new and/or transitional countries during that period used either trials or truth commissions as a transitional justice mechanism; over half tried some form of judicial proceedings. Thus, the use of a truth commission and/or human rights trials among transitional countries is not an isolated or marginal practice, but a widespread social practice occurring in the bulk of transitional countries.\n\nSince its emergence, transitional justice has encountered numerous challenges such as identifying victims, deciding whether to punish superiors or middle agents, avoiding a \"victor’s justice\", and finding adequate resources for compensation, trial, or institutional reform. Also, the transitional period may only result in a tenuous peace or fragile democracy. As has been noted in the discourse on transition to democracy, the dilemma has always been for new regimes to promote accountability for past abuses without risking a smooth transition to democracy. In addition, existing judicial system might be weak, corrupt, or ineffective and in effect make achieving any viable justice difficult. Observers of transitional justice application and processes, such as Makau W. Mutua (2000) emphasized on the difficulties of achieving actual justice through one of the most prominent mechanisms of transitional justice, trials. Commenting on the international tribunal established in Rwanda in 1994, he argued that it “serves to deflect responsibility, to assuage the consciences of states which were unwilling to stop the genocide... [and] largely masks the illegitimacy of the Tutsi regime”. In sum, Matua argues that criminal tribunals such as those in Rwanda and Yugoslavia are “less meaningful if they cannot be applied or enforced without prejudice to redress transgressions or unless they have a deterrent effect such as behavior modification on the part of would be perpetrators”.\n\nMore recently, Lyal S. Sunga has argued that unless truth commissions are set up and conducted according to international human rights law, international criminal law and international humanitarian law, they risk conflicting or undermining criminal prosecutions, whether these prosecutions are supposed to be carried out at the national or international levels. He contends that this risk is particularly pronounced where truth commissions employ amnesties, and especially blanket amnesties to pardon perpetrators of serious crimes. On the other hand, criminal prosecutions should be better tailored to focus on victims and to place events in proper perspective. Sunga therefore proposes ten principles for making truth and national reconciliation commissions fully complementary to criminal prosecutions in a way that conforms fully to international law.\n\nThis type of critique of transitional justice mechanisms could cause some scholars and policymakers to wonder which of the objectives outlined above are most important to achieve, and even if they are achievable. Truth commissions could be characterized as a second-best alternative and also an affront to rule of law, because of the possibility that amnesty and indemnities will be made exchange for truth. These sets of challenges can raise critical questions for transitional justice in its application. Questions and issues, such as: Can the \"truth\" ever really be established? Can all victims be given compensation or a public platform? Can all perpetrators be held accountable? Or is it sufficient to acknowledge that atrocities were committed and that victims should be compensated for their suffering?\n\nAlso, one might argue that too narrow a focus on the challenges of the field runs the risk of making transitional justice seem meaningless. However transitional justice aims at an ongoing search for truth, justice, forgiveness, and healing, and efforts undertaken within it help people to live alongside former enemies. Simply put, “the past must be addressed in order to reach the future”. Thus, even if the impact or reach of transitional justice seems marginal, the end result is worth the effort.\n\nAnother way of assessing attempts at transitional justice is to say that decision makers may have less control over the methods used to pursue such policies than they imagine. In fact, whatever their wishes, they may not be able to prevent such policies at all. As A. James McAdams has demonstrated in his book, \"Judging the Past in Unified Germany\" (2001), West German policymakers such as former chancellor Helmut Kohl wanted to close public access to the files of East Germany's secret police, the Stasi, but pressures from East German dissidents prevented them from doing so.\n\nAnother challenge is the tension between peace and justice, which arises the conflicting goals of achieving peace and justice in the aftermath of a society’s emergence from conflict. Though it is generally unanimous that both goals are integral to achieving reconciliation, practitioners often disagree about which goal should be pursued first: justice or peace? Proponents of the “justice” school of thought argue that if all perpetrators of human rights abuses do not stand trial, impunity for crimes will continue into the new regime, preventing it from fully completing a transition from conflict. The \"peace\" school of thought, however, argues that the only way to effectively end violence is by granting amnesties and brokering negotiations to persuade criminals to lay down their arms. Examples such as Northern Ireland illustrate how selective amnesties can cease conflict.\n\nRecent trends in the post-conflict field have tended to favor the “justice” school of thought, maintaining that only if justice is dutifully served to victims of the conflict can civil war will be prevented from recurring. A 2011 debate in \"The Economist\" determined in its concluding polls that 76% of the debate participants agreed with the motion that achieving peace can occur only through implementing justice mechanisms.\n\nLiterary scholars and historians have begun to use the concept of transitional justice to reexamine historical events and texts. Christopher N. Warren, for instance, has applied transitional justice to pre-Restoration England, claiming that it helps explain how Anglican royalists convinced Presbyterians to assent to a restoration of the monarchy. Warren also argues that English poet John Milton “can be seen as an early critic of transitional justice,” using the allegory of Sin and Death in his epic poem Paradise Lost to complicate “overly-rosy” depictions of transitional justice.\n\nAlthough transitional justice is engulfed by many critical challenges in addition to the difficulty in measuring its impact, given the number of other factors in any given country’s experience over time, human rights trials or truth commissions need not have a negative effect on human rights practices. This makes transitional justice viable, especially in this age of state-building and democracy promotion in post-conflict societies. In fact, Sikkink and Walling’s comparison of human rights conditions before and after trials in Latin American countries with two or more trial years showed that eleven of the fourteen countries had better Political Terror Scale (PTS) ratings after trials. Latin American countries that had both a truth commission and human rights trials improved more on their PTS ratings than countries that only had trials. These statistics indicate that transitional justice mechanisms are associated with countries’ improving their human rights practices. Each state that employs transitional justice mechanisms will have to determine which mechanisms to use to best achieve the targeted goals. In order to avoid causing disappointment amongst victims, the state should also ensure that the public is well-informed about the goals and limits of those mechanisms.\n\nTransitional justice shows no signs of decreasing in use. Indeed, the incorporation of transitional justice policies, tools and programs in peacebuilding and democratization process operations by the United Nations (UN) and in the programs by many local and international democracy promotion organizations, including, the Stockholm-based International Institute for Electoral Assistance and Democracy (International IDEA) and a host of others as well as the establishments of other international non-governmental organizations (INGOs) and networks such as the International Center for Transitional Justice (ICTJ) and the African Transitional Justice Research Network (ATJRN) and research centers like the Transitional Justice Institute are strong manifestations of how well placed transitional justice has become a feature in the discourse of transitional politics in the 21st century. Academic publications such as the International Journal of Transitional Justice are also contributing towards building an interdisciplinary field with the hope that future innovations are tailored for a specific state’s situation and will contribute towards political transitions that address the past as well as establish guarantees for respect of human rights and democracy.\n\nThe World Bank's \"2011 World Development Report on Conflict, Security, and Development” links transitional justice to security and development. It explores how countries can avoid cycles of violence and emphasizes the importance of transitional justice, arguing that it is one of the “signaling mechanisms” that governments can use to show that they are breaking away from past practices. It also argues that transitional justice measures can send signals about the importance of accountability and to improve institutional capacity.\n\nIn September 2011, the International Center for Transitional Justice (ICTJ) published a report advocating the need to understand traditional transitional justice measures from a child's perspective. The report identifies children as a large demographic too often excluded from traditional transitional justice measures. In order to correct this imbalance, a new child-centered perspective is needed to incorporate children into the larger scope of transitional justice.\n\n\n\n\n"}
{"id": "37514369", "url": "https://en.wikipedia.org/wiki?curid=37514369", "title": "Violence-free zone", "text": "Violence-free zone\n\nViolence-Free Zones, or Violence-Free Zone Initiatives, are community-based interventions for gang members and youth. Zones attempt to stem violence by providing mentorship, guidance, social development, job training and an effective environment for learning, among other tools, to help gang members and at-risk youth break free and become successful in life, crime-free and violence-free. The initiatives operate in urban schools with high levels of crime and violence. \n\nThe model for Violence-Free Zones was developed by the Center for Neighborhood Enterprise and is based on the principle that lack of parental involvement and guidance is at the root of violence among youth in these high-risk communities.\n\nParticipating cities include Washington, D.C., Hartford, Indianapolis, Los Angeles, Dallas, Houston, Milwaukee, Richmond (Virginia) and others. \n\nThe model has also been adopted as a strategy to prevent HIV/AIDS transmission in schools.\n\nEach Violence-Free Zone engages youth-oriented organizations in the area to become \"Community Partners.\" These organizations must have the trust and confidence of local youth. Key to the program are Youth Advisors, young adults from the neighborhood who have overcome similar obstacles and have the respect of students. The Advisors work in the schools, coaching and mediating, and serve as mentors to the most troubled students. Community Partners and the school work together to provide guidance, social development, job readiness programs. Training, oversight, and technical assistance for the program administrators is provided by the Center for Neighborhood Enterprise.\n\nThe initiative has been found to have measurable success in each targeted metric, from school absences and incidents to police arrests. Independent evaluations, including a study by Baylor University, have shown declines in each zone from 4% to 61%.\n"}
{"id": "34515483", "url": "https://en.wikipedia.org/wiki?curid=34515483", "title": "Wheat Street Baptist Church", "text": "Wheat Street Baptist Church\n\nWheat Street Baptist Church is located at 359 Auburn Ave. in the Sweet Auburn neighborhood of Atlanta, Georgia. The building is part of the Martin Luther King, Jr. Historic District; the church and its leader at the time, Rev. William Holmes Borders, played a leading role in the Civil Rights Movement. The congregation dates back to 1869, with the present building dating from 1921. From 1898-1929, Rev. P. James Bryant served as pastor.\n"}
{"id": "17972680", "url": "https://en.wikipedia.org/wiki?curid=17972680", "title": "Winders", "text": "Winders\n\nThe term “winders” was originally coined in 2008 by the sociologist John W. Leigh, in his article \"Moving towards new forms of social success\", describing the new forms of social success in the United States, and in Western societies. The term (a contraction of the expression “windy winners”) goes back to the original way of experiencing social success by individuals uninhibited with regards to their own success, not looking as much to reconcile rival existential expectations (such as the bobos, for example) but rather to juxtapose them in a way which is not seeking to constitute a system.\n\nThe analysis of this new group is positioned along the same lines as the social success models embodied respectively by yuppies, “hardcore winners” (traders and other “golden boys” of the 1980s), and then bobos. It borrows from the critical sociology of Michael Hartmann, as well as employment sociologists Peter Meiksins and Peter Whalley, whose work \"Putting work in its place: a quiet revolution\"\ndetails the paradigm shift borne by the winders within the American employment market. As J. W. Leigh puts it: “In cultural terms, for example, he is a \"multi-consumer\": capable of frequenting art galleries or the screens of a public cinema, and listening to Haydn and Bach as much as Beyonce or Michael Jackson.” To this regard, the winder is considered a “cultural omnivore”\n\nFrom this point of view, the “winder” is one of the many avatars of the ability of superior social classes to legitimise their own situation via positive and always renewing value systems.\n\n\n"}
{"id": "33912", "url": "https://en.wikipedia.org/wiki?curid=33912", "title": "Working memory", "text": "Working memory\n\nWorking memory is a cognitive system with a limited capacity that is responsible for temporarily holding information available for processing. Working memory is important for reasoning and the guidance of decision-making and behavior. Working memory is often used synonymously with short-term memory, but some theorists consider the two forms of memory distinct, assuming that working memory allows for the manipulation of stored information, whereas short-term memory only refers to the short-term storage of information. Working memory is a theoretical concept central to cognitive psychology, neuropsychology, and neuroscience.\n\nThe term \"working memory\" was coined by Miller, Galanter, and Pribram, and was used in the 1960s in the context of theories that likened the mind to a computer. In 1968, Atkinson and Shiffrin used the term to describe their \"short-term store\". What we now call working memory was formerly referred to variously as a \"short-term store\" or short-term memory, primary memory, immediate memory, operant memory, and provisional memory. Short-term memory is the ability to remember information over a brief period (in the order of seconds). Most theorists today use the concept of working memory to replace or include the older concept of short-term memory, marking a stronger emphasis on the notion of manipulating information rather than mere maintenance.\n\nThe earliest mention of experiments on the neural basis of working memory can be traced back to more than 100 years ago, when Hitzig and Ferrier described ablation experiments of the prefrontal cortex (PFC); they concluded that the frontal cortex was important for cognitive rather than sensory processes. In 1935 and 1936, Carlyle Jacobsen and colleagues were the first to show the deleterious effect of prefrontal ablation on delayed response.\n\nNumerous models have been proposed for how working memory functions, both anatomically and cognitively. Of those, the two that have been most influential are summarized below.\n\nIn 1974, Baddeley and Hitch introduced the multicomponent model of working memory. The theory proposed a model containing three components: the central executive, the phonological loop, and the visuospatial sketchpad with the central executive functioning as a control center of sorts, directing info between the phonological and visuospatial components. The central executive is responsible \"inter alia\" for directing attention to relevant information, suppressing irrelevant information and inappropriate actions, and coordinating cognitive processes when more than one task is simultaneously performed. A \"central executive\" is responsible for supervising the integration of information and for coordinating \"slave systems\" that are responsible for the short-term maintenance of information. One slave system, the phonological loop (PL), stores phonological information (that is, the sound of language) and prevents its decay by continuously refreshing it in a rehearsal loop. It can, for example, maintain a seven-digit telephone number for as long as one repeats the number to oneself again and again. The other slave system, the visuospatial sketchpad, stores visual and spatial information. It can be used, for example, for constructing and manipulating visual images and for representing mental maps. The sketchpad can be further broken down into a visual subsystem (dealing with such phenomena as shape, colour, and texture), and a spatial subsystem (dealing with location).\n\nIn 2000, Baddeley extended the model by adding a fourth component, the episodic buffer, which holds representations that integrate phonological, visual, and spatial information, and possibly information not covered by the slave systems (e.g., semantic information, musical information). The episodic buffer is also the link between working memory and long-term memory. The component is episodic because it is assumed to bind information into a unitary episodic representation. The episodic buffer resembles Tulving's concept of episodic memory, but it differs in that the episodic buffer is a temporary store.\n\nAnders Ericsson and Walter Kintsch have introduced the notion of \"long-term working memory\", which they define as a set of \"retrieval structures\" in long-term memory that enable seamless access to the information relevant for everyday tasks. In this way, parts of long-term memory effectively function as working memory. In a similar vein, Cowan does not regard working memory as a separate system from long-term memory. Representations in working memory are a subset of representations in long-term memory. Working memory is organized into two embedded levels. The first consists of long-term memory representations that are activated. There can be many of these—there is theoretically no limit to the activation of representations in long-term memory. The second level is called the focus of attention. The focus is regarded as having a limited capacity and holds up to four of the activated representations.\n\nOberauer has extended Cowan's model by adding a third component, a more narrow focus of attention that holds only one chunk at a time. The one-element focus is embedded in the four-element focus and serves to select a single chunk for processing. For example, four digits can be held in mind at the same time in Cowan's \"focus of attention\". When the individual wishes to perform a process on each of these digits—for example, adding the number two to each digit—separate processing is required for each digit since most individuals cannot perform several mathematical processes in parallel. Oberauer's attentional component selects one of the digits for processing and then shifts the attentional focus to the next digit, continuing until all digits have been processed.\n\nWorking memory is generally considered to have limited capacity. An early quantification of the capacity limit associated with short-term memory was the \"magical number seven\" suggested by Miller in 1956. He claimed that the information-processing capacity of young adults is around seven elements, which he called \"chunks\", regardless of whether the elements are digits, letters, words, or other units. Later research revealed this number depends on the category of chunks used (e.g., span may be around seven for digits, six for letters, and five for words), and even on features of the chunks within a category. For instance, span is lower for long than short words. In general, memory span for verbal contents (digits, letters, words, etc.) depends on the phonological complexity of the content (i.e., the number of phonemes, the number of syllables), and on the lexical status of the contents (whether the contents are words known to the person or not). Several other factors affect a person's measured span, and therefore it is difficult to pin down the capacity of short-term or working memory to a number of chunks. Nonetheless, Cowan proposed that working memory has a capacity of about four chunks in young adults (and fewer in children and old adults).\n\nWhereas most adults can repeat about seven digits in correct order, some individuals have shown impressive enlargements of their digit span—up to 80 digits. This feat is possible by extensive training on an encoding strategy by which the digits in a list are grouped (usually in groups of three to five) and these groups are encoded as a single unit (a chunk). For this to succeed, participants must be able to recognize the groups as some known string of digits. One person studied by Ericsson and his colleagues, for example, used an extensive knowledge of racing times from the history of sports in the process of coding chunks: several such chunks could then be combined into a higher-order chunk, forming a hierarchy of chunks. In this way, only some chunks at the highest level of the hierarchy must be retained in working memory, and for retrieval the chunks are unpacked. That is, the chunks in working memory act as retrieval cues that point to the digits they contain. Practicing memory skills such as these does not expand working memory capacity proper: it is the capacity to transfer (and retrieve) information from long-term memory that is improved, according to Ericsson and Kintsch (1995; see also Gobet & Simon, 2000).\n\nWorking memory capacity can be tested by a variety of tasks. A commonly used measure is a dual-task paradigm, combining a memory span measure with a concurrent processing task, sometimes referred to as \"complex span\". Daneman and Carpenter invented the first version of this kind of task, the \"reading span\", in 1980. Subjects read a number of sentences (usually between two and six) and tried to remember the last word of each sentence. At the end of the list of sentences, they repeated back the words in their correct order. Other tasks that do not have this dual-task nature have also been shown to be good measures of working memory capacity. Whereas Daneman and Carpenter believed that the combination of \"storage\" (maintenance) and processing is needed to measure working memory capacity, we know now that the capacity of working memory can be measured with short-term memory tasks that have no additional processing component. Conversely, working memory capacity can also be measured with certain processing tasks that don't involve maintenance of information. The question of what features a task must have to qualify as a good measure of working memory capacity is a topic of ongoing research.\n\nMeasures of working-memory capacity are strongly related to performance in other complex cognitive tasks, such as reading comprehension, problem solving, and with measures of intelligence quotient.\n\nSome researchers have argued that working-memory capacity reflects the efficiency of executive functions, most notably the ability to maintain multiple task-relevant representations in the face of distracting irrelevant information; and that such tasks seem to reflect individual differences in the ability to focus and maintain attention, particularly when other events are serving to capture attention. Both working memory and executive functions rely strongly, though not exclusively, on frontal brain areas.\n\nOther researchers have argued that the capacity of working memory is better characterized as the ability to mentally form relations between elements, or to grasp relations in given information. This idea has been advanced, among others, by Graeme Halford, who illustrated it by our limited ability to understand statistical interactions between variables. These authors asked people to compare written statements about the relations between several variables to graphs illustrating the same or a different relation, as in the following sentence: \"If the cake is from France, then it has more sugar if it is made with chocolate than if it is made with cream, but if the cake is from Italy, then it has more sugar if it is made with cream than if it is made of chocolate\". This statement describes a relation between three variables (country, ingredient, and amount of sugar), which is the maximum most individuals can understand. The capacity limit apparent here is obviously not a memory limit (all relevant information can be seen continuously) but a limit to how many relationships are discerned simultaneously.\n\nThere are several hypotheses about the nature of the capacity limit. One is that a limited pool of cognitive resources needed to keep representations active and thereby available for processing, and for carrying out processes. Another hypothesis is that memory traces in working memory decay within a few seconds, unless refreshed through rehearsal, and because the speed of rehearsal is limited, we can maintain only a limited amount of information. Yet another idea is that representations held in working memory interfere with each other.\n\nThe assumption that the contents of short-term or working memory decay over time, unless decay is prevented by rehearsal, goes back to the early days of experimental research on short-term memory. It is also an important assumption in the multi-component theory of working memory. The most elaborate decay-based theory of working memory to date is the \"time-based resource sharing model\". This theory assumes that representations in working memory decay unless they are refreshed. Refreshing them requires an attentional mechanism that is also needed for any concurrent processing task. When there are small time intervals in which the processing task does not require attention, this time can be used to refresh memory traces. The theory therefore predicts that the amount of forgetting depends on the temporal density of attentional demands of the processing task—this density is called \"cognitive load\". The cognitive load depends on two variables, the rate at which the processing task requires individual steps to be carried out, and the duration of each step. For example, if the processing task consists of adding digits, then having to add another digit every half second places a higher cognitive load on the system than having to add another digit every two seconds. In a series of experiments, Barrouillet and colleagues have shown that memory for lists of letters depends neither on the number of processing steps nor the total time of processing but on cognitive load.\n\nResource theories assume that the capacity of working memory is a limited resource that must be shared between all representations that need to be maintained in working memory simultaneously. Some resource theorists also assume that maintenance and concurrent processing share the same resource; this can explain why maintenance is typically impaired by a concurrent processing demand. Resource theories have been very successful in explaining data from tests of working memory for simple visual features, such as colors or orientations of bars. An ongoing debate is whether the resource is a continuous quantity that can be subdivided among any number of items in working memory, or whether it consists of a small number of discrete \"slots\", each of which can be assigned to one memory item, so that only a limited number of about 3 items can be maintained in working memory at all.\n\nSeveral forms of interference have been discussed by theorists. One of the oldest ideas is that new items simply replace older ones in working memory. Another form of interference is retrieval competition. For example, when the task is to remember a list of 7 words in their order, we need to start recall with the first word. While trying to retrieve the first word, the second word, which is represented in proximity, is accidentally retrieved as well, and the two compete for being recalled. Errors in serial recall tasks are often confusions of neighboring items on a memory list (so-called transpositions), showing that retrieval competition plays a role in limiting our ability to recall lists in order, and probably also in other working memory tasks. A third form of interference is the distortion of representations by superposition: When multiple representations are added on top of each other, each of them is blurred by the presence of all the others. A fourth form of interference assumed by some authors is feature overwriting. The idea is that each word, digit, or other item in working memory is represented as a bundle of features, and when two items share some features, one of them steals the features from the other. The more items are held in working memory, and the more their features overlap, the more each of them will be degraded by the loss of some features.\n\nNone of these hypotheses can explain the experimental data entirely. The resource hypothesis, for example, was meant to explain the trade-off between maintenance and processing: The more information must be maintained in working memory, the slower and more error prone concurrent processes become, and with a higher demand on concurrent processing memory suffers. This trade-off has been investigated by tasks like the reading-span task described above. It has been found that the amount of trade-off depends on the similarity of the information to be remembered and the information to be processed. For example, remembering numbers while processing spatial information, or remembering spatial information while processing numbers, impair each other much less than when material of the same kind must be remembered and processed. Also, remembering words and processing digits, or remembering digits and processing words, is easier than remembering and processing materials of the same category. These findings are also difficult to explain for the decay hypothesis, because decay of memory representations should depend only on how long the processing task delays rehearsal or recall, not on the content of the processing task. A further problem for the decay hypothesis comes from experiments in which the recall of a list of letters was delayed, either by instructing participants to recall at a slower pace, or by instructing them to say an irrelevant word once or three times in between recall of each letter. Delaying recall had virtually no effect on recall accuracy. The interference theory seems to fare best with explaining why the similarity between memory contents and the contents of concurrent processing tasks affects how much they impair each other. More similar materials are more likely to be confused, leading to retrieval competition.\n\nThe capacity of working memory increases gradually over childhood and declines gradually in old age.\n\nMeasures of performance on tests of working memory increase continuously between early childhood and adolescence, while the structure of correlations between different tests remains largely constant. Starting with work in the Neo-Piagetian tradition, theorists have argued that the growth of working-memory capacity is a major driving force of cognitive development. This hypothesis has received substantial empirical support from studies showing that the capacity of working memory is a strong predictor of cognitive abilities in childhood. Particularly strong evidence for a role of working memory for development comes from a longitudinal study showing that working-memory capacity at one age predicts reasoning ability at a later age. Studies in the Neo-Piagetian tradition have added to this picture by analyzing the complexity of cognitive tasks in terms of the number of items or relations that have to be considered simultaneously for a solution. Across a broad range of tasks, children manage task versions of the same level of complexity at about the same age, consistent with the view that working memory capacity limits the complexity they can handle at a given age.\n\nWorking memory is among the cognitive functions most sensitive to decline in old age. Several explanations have been offered for this decline in psychology. One is the processing speed theory of cognitive aging by Tim Salthouse. Drawing on the finding of general slowing of cognitive processes as people grow older, Salthouse argues that slower processing leaves more time for working-memory contents to decay, thus reducing effective capacity. However, the decline of working-memory capacity cannot be entirely attributed to slowing because capacity declines more in old age than speed. Another proposal is the inhibition hypothesis advanced by Lynn Hasher and Rose Zacks. This theory assumes a general deficit in old age in the ability to inhibit irrelevant, or no-longer relevant, information. Therefore, working memory tends to be cluttered with irrelevant contents that reduce the effective capacity for relevant content. The assumption of an inhibition deficit in old age has received much empirical support but, so far, it is not clear whether the decline in inhibitory ability fully explains the decline of working-memory capacity. An explanation on the neural level of the decline of working memory and other cognitive functions in old age has been proposed by West. He argued that working memory depends to a large degree on the pre-frontal cortex, which deteriorates more than other brain regions as we grow old.\n\nTorkel Klingberg was the first to investigate whether intensive training of working memory has beneficial effects on other cognitive functions. His pioneering study suggested that working memory can be improved by training in ADHD patients through computerized programs. This study has found that a period of working memory training increases a range of cognitive abilities and increases IQ test scores. Another study of the same group has shown that, after training, measured brain activity related to working memory increased in the prefrontal cortex, an area that many researchers have associated with working memory functions. It has been shown in one study that working memory training increases the density of prefrontal and parietal dopamine receptors (specifically, DRD1) in test persons. However, subsequent work with the same training program has failed to replicate the beneficial effects of training on cognitive performance. A meta-analytic summary of research with Klingberg's training program up to 2011 shows that this training has at best a negligible effect on tests of intelligence and of attention\n\nIn another influential study, training with a working memory task (the dual n-back task) has improved performance on a fluid intelligence test in healthy young adults. The improvement of fluid intelligence by training with the n-back task was replicated in 2010, but two studies published in 2012 failed to reproduce the effect. The combined evidence from about 30 experimental studies on the effectiveness of working-memory training has been evaluated by several meta-analyses. The authors of these meta-analyses disagree in their conclusions as to whether or not working-memory training improves intelligence. Yet, these meta-analyses agree in their estimate of the size of the effect of working-memory training: If there is such an effect, it is likely to be small.\n\nThe first insights into the neuronal and neurotransmitter basis of working memory came from animal research. The work of Jacobsen and Fulton in the 1930s first showed that lesions to the PFC impaired spatial working memory performance in monkeys. The later work of Joaquin Fuster recorded the electrical activity of neurons in the PFC of monkeys while they were doing a delayed matching task. In that task, the monkey sees how the experimenter places a bit of food under one of two identical-looking cups. A shutter is then lowered for a variable delay period, screening off the cups from the monkey's view. After the delay, the shutter opens and the monkey is allowed to retrieve the food from under the cups. Successful retrieval in the first attempt – something the animal can achieve after some training on the task – requires holding the location of the food in memory over the delay period. Fuster found neurons in the PFC that fired mostly during the delay period, suggesting that they were involved in representing the food location while it was invisible. Later research has shown similar delay-active neurons also in the posterior parietal cortex, the thalamus, the caudate, and the globus pallidus. The work of Goldman-Rakic and others showed that principal sulcal, dorsolateral PFC interconnects with all of these brain regions, and that neuronal microcircuits within PFC are able to maintain information in working memory through recurrent excitatory glutamate networks of pyramidal cells that continue to fire throughout the delay period. These circuits are tuned by lateral inhibition from GABAergic interneurons. The neuromodulatory arousal systems markedly alter PFC working memory function; for example, either too little or too much dopamine or norepinephrine impairs PFC network firing and working memory performance.\n\nThe research described above on persistent firing of certain neurons in the delay period of working memory tasks shows that the brain has a mechanism of keeping representations active without external input. Keeping representations active, however, is not enough if the task demands maintaining more than one chunk of information. In addition, the components and features of each chunk must be bound together to prevent them from being mixed up. For example, if a red triangle and a green square must be remembered at the same time, one must make sure that \"red\" is bound to \"triangle\" and \"green\" is bound to \"square\". One way of establishing such bindings is by having the neurons that represent features of the same chunk fire in synchrony, and those that represent features belonging to different chunks fire out of sync. In the example, neurons representing redness would fire in synchrony with neurons representing the triangular shape, but out of sync with those representing the square shape. So far, there is no direct evidence that working memory uses this binding mechanism, and other mechanisms have been proposed as well. It has been speculated that synchronous firing of neurons involved in working memory oscillate with frequencies in the theta band (4 to 8 Hz). Indeed, the power of theta frequency in the EEG increases with working memory load, and oscillations in the theta band measured over different parts of the skull become more coordinated when the person tries to remember the binding between two components of information.\n\nLocalization of brain functions in humans has become much easier with the advent of brain imaging methods (PET and fMRI). This research has confirmed that areas in the PFC are involved in working memory functions. During the 1990s much debate has centered on the different functions of the ventrolateral (i.e., lower areas) and the dorsolateral (higher) areas of the PFC. A human lesion study provides additional evidence for the role of the dorsolateral prefrontal cortex in working memory. One view was that the dorsolateral areas are responsible for spatial working memory and the ventrolateral areas for non-spatial working memory. Another view proposed a functional distinction, arguing that ventrolateral areas are mostly involved in pure maintenance of information, whereas dorsolateral areas are more involved in tasks requiring some processing of the memorized material. The debate is not entirely resolved but most of the evidence supports the functional distinction.\n\nBrain imaging has also revealed that working memory functions are not limited to the PFC. A review of numerous studies shows areas of activation during working memory tasks scattered over a large part of the cortex. There is a tendency for spatial tasks to recruit more right-hemisphere areas, and for verbal and object working memory to recruit more left-hemisphere areas. The activation during verbal working memory tasks can be broken down into one component reflecting maintenance, in the left posterior parietal cortex, and a component reflecting subvocal rehearsal, in the left frontal cortex (Broca's area, known to be involved in speech production).\n\nThere is an emerging consensus that most working memory tasks recruit a network of PFC and parietal areas. A study has shown that during a working memory task the connectivity between these areas increases. Another study has demonstrated that these areas are necessary for working memory, and not simply activated accidentally during working memory tasks, by temporarily blocking them through transcranial magnetic stimulation (TMS), thereby producing an impairment in task performance.\n\nA current debate concerns the function of these brain areas. The PFC has been found to be active in a variety of tasks that require executive functions. This has led some researchers to argue that the role of PFC in working memory is in controlling attention, selecting strategies, and manipulating information in working memory, but not in maintenance of information. The maintenance function is attributed to more posterior areas of the brain, including the parietal cortex. Other authors interpret the activity in parietal cortex as reflecting executive functions, because the same area is also activated in other tasks requiring attention but not memory.\n\nA 2003 meta-analysis of 60 neuroimaging studies found left frontal cortex was involved in low-task demand verbal working memory and right frontal cortex for spatial working memory. Brodmann's areas (BAs) 6, 8, and 9, in the superior frontal cortex was involved when working memory must be continuously updated and when memory for temporal order had to be maintained. Right Brodmann 10 and 47 in the ventral frontal cortex were involved more frequently with demand for manipulation such as dual-task requirements or mental operations, and Brodmann 7 in the posterior parietal cortex was also involved in all types of executive function.\n\nWorking memory has been suggested to involve two processes with different neuroanatomical locations in the frontal and parietal lobes. First, a selection operation that retrieves the most relevant item, and second an updating operation that changes the focus of attention made upon it. Updating the attentional focus has been found to involve the transient activation in the caudal superior frontal sulcus and posterior parietal cortex, while increasing demands on selection selectively changes activation in the rostral superior frontal sulcus and posterior cingulate/precuneus.\n\nArticulating the differential function of brain regions involved in working memory is dependent on tasks able to distinguish these functions. Most brain imaging studies of working memory have used recognition tasks such as delayed recognition of one or several stimuli, or the n-back task, in which each new stimulus in a long series must be compared to the one presented n steps back in the series. The advantage of recognition tasks is that they require minimal movement (just pressing one of two keys), making fixation of the head in the scanner easier. Experimental research and research on individual differences in working memory, however, has used largely recall tasks (e.g., the reading span task, see below). It is not clear to what degree recognition and recall tasks reflect the same processes and the same capacity limitations.\n\nBrain imaging studies have been conducted with the reading span task or related tasks. Increased activation during these tasks was found in the PFC and, in several studies, also in the anterior cingulate cortex (ACC). People performing better on the task showed larger increase of activation in these areas, and their activation was correlated more over time, suggesting that their neural activity in these two areas was better coordinated, possibly due to stronger connectivity.\n\nOne approach to model the neurophysiology and the functioning of working memory is the prefrontal cortex basal ganglia working memory (PBWM).\n\nWorking memory is impaired by acute and chronic psychological stress. This phenomenon was first discovered in animal studies by Arnsten and colleagues, who have shown that stress-induced catecholamine release in PFC rapidly decreases PFC neuronal firing and impairs working memory performance through feedforward, intracellular signaling pathways. Exposure to chronic stress leads to more profound working memory deficits and additional architectural changes in PFC, including dendritic atrophy and spine loss, which can be prevented by inhibition of protein kinase C signaling. fMRI research has extended this research to humans, and confirms that reduced working memory caused by acute stress links to reduced activation of the PFC, and stress increased levels of catecholamines. Imaging studies of medical students undergoing stressful exams have also shown weakened PFC functional connectivity, consistent with the animal studies. The marked effects of stress on PFC structure and function may help to explain how stress can cause or exacerbate mental illness.\nThe more stress in one's life, the lower the efficiency of working memory in performing simple cognitive tasks. Students who performed exercises that reduced the intrusion of negative thoughts showed an increase in their working memory capacity. Mood states (positive or negative) can have an influence on the neurotransmitter dopamine, which in turn can affect problem solving.\n\nAlcohol abuse can result in brain damage which impairs working memory. Alcohol has an effect on the blood-oxygen-level-dependent (BOLD) response. The BOLD response correlates increased blood oxygenation with brain activity, which makes this response a useful tool for measuring neuronal activity. The BOLD response affects regions of the brain such as the basal ganglia and thalamus when performing a working memory task. Adolescents who start drinking at a young age show a decreased BOLD response in these brain regions. Alcohol dependent young women in particular exhibit less of a BOLD response in parietal and frontal cortices when performing a spatial working memory task. Binge drinking, specifically, can also affect one's performance on working memory tasks, particularly visual working memory. Additionally, there seems to be a gender difference in regards to how alcohol affects working memory. While women perform better on verbal working memory tasks after consuming alcohol compared to men, they appear to perform worse on spatial working memory tasks as indicated by less brain activity. Finally, age seems to be an additional factor. Older adults are more susceptible than others to the effects of alcohol on working memory.\n\nIndividual differences in working-memory capacity are to some extent heritable; that is, about half of the variation between individuals is related to differences in their genes. The genetic component of variability of working-memory capacity is largely shared with that of fluid intelligence.\n\nLittle is known about which genes are related to the functioning of working memory. Within the theoretical framework of the multi-component model, one candidate gene has been proposed, namely ROBO1 for the hypothetical phonological loop component of working memory.\n\nWorking memory capacity is correlated with learning outcomes in literacy and numeracy. Initial evidence for this relation comes from the correlation between working-memory capacity and reading comprehension, as first observed by Daneman and Carpenter (1980) and confirmed in a later meta-analytic review of several studies. Subsequent work found that working memory performance in primary school children accurately predicted performance in mathematical problem solving. One longitudinal study showed that a child's working memory at 5 years old is a better predictor of academic success than IQ.\n\nIn a large-scale screening study, one in ten children in mainstream classrooms were identified with working memory deficits. The majority of them performed very poorly in academic achievements, independent of their IQ. Similarly, working memory deficits have been identified in national curriculum low-achievers as young as seven years of age. Without appropriate intervention, these children lag behind their peers. A recent study of 37 school-age children with significant learning disabilities has shown that working memory capacity at baseline measurement, but not IQ, predicts learning outcomes two years later. This suggests that working memory impairments are associated with low learning outcomes and constitute a high risk factor for educational underachievement for children. In children with learning disabilities such as dyslexia, ADHD, and developmental coordination disorder, a similar pattern is evident.\n\nThere is some evidence that optimal working memory performance links to the neural ability to focus attention on task-relevant information and to ignore distractions, and that practice-related improvement in working memory is due to increasing these abilities. One line of research suggests a link between the working memory capacities of a person and their ability to control the orientation of attention to stimuli in the environment. Such control enables people to attend to information important for their current goals, and to ignore goal-irrelevant stimuli that tend to capture their attention due to their sensory saliency (such as an ambulance siren). The direction of attention according to one's goals is assumed to rely on \"top-down\" signals from the pre-frontal cortex (PFC) that biases processing in posterior cortical areas. Capture of attention by salient stimuli is assumed to be driven by \"bottom-up\" signals from subcortical structures and the primary sensory cortices. The ability to override \"bottom-up\" capture of attention differs between individuals, and this difference has been found to correlate with their performance in a working-memory test for visual information. Another study, however, found no correlation between the ability to override attentional capture and measures of more general working-memory capacity.\n\nAn impairment of working memory functioning is normally seen in several neural disorders:\n\nADHD: Several authors have proposed that symptoms of ADHD arise from a primary deficit in a specific executive function (EF) domain such as working memory, response inhibition or a more general weakness in executive control. A meta-analytical review cites several studies that found significant lower group results for ADHD in spatial and verbal working memory tasks, and in several other EF tasks. However, the authors concluded that EF weaknesses neither are necessary nor sufficient to cause all cases of ADHD.\n\nSeveral neurotransmitters, such as dopamine and glutamate may be both involved in ADHD and working memory. Both are associated with the frontal brain, self-direction and self-regulation, but cause–effect have not been confirmed, so it is unclear whether working memory dysfunction leads to ADHD, or ADHD distractibility leads to poor functionality of working memory, or if there is some other connection.\n\nParkinson's disease: Patients with Parkinson's show signs of a reduced verbal function of working memory. They wanted to find if the reduction is due to a lack of ability to focus on relevant tasks, or a low amount of memory capacity. Twenty-one patients with Parkinson's were tested in comparison to the control group of 28 participants of the same age.The researchers found that both hypotheses were the reason working memory function is reduced which did not fully agree with their hypothesis that it is either one or the other.\n\nAlzheimer's disease: As Alzheimer's disease becomes more serious, less working memory functions. There is one study that focuses on the neural connections and fluidity of working memory in mice brains. Half of the mice were given an injection that is similar to Alzheimer's effects, and the other half were not. Then they were expected to go through a maze that is a task to test working memory. The study help answer questions about how Alzheimer's can deteriorate the working memory and ultimately obliterate memory functions.\n\nHuntington's disease: A group of researchers hosted a study that researched the function and connectivity of working memory over a 30-month longitudinal experiment. It found that there were certain places in the brain where most connectivity was decreased in pre-Huntington diseased patients, in comparison to the control group that remained consistently functional.\n\n\n"}
{"id": "23714788", "url": "https://en.wikipedia.org/wiki?curid=23714788", "title": "Writing system", "text": "Writing system\n\nA writing system is any conventional method of visually representing verbal communication. While both writing and speech are useful in conveying messages, writing differs in also being a reliable form of information storage and transfer. The processes of encoding and decoding writing systems involve shared understanding between writers and readers of the meaning behind the sets of characters that make up a script. Writing is usually recorded onto a durable medium, such as paper or electronic storage, although non-durable methods may also be used, such as writing on a computer display, on a blackboard, in sand, or by skywriting.\n\nThe general attributes of writing systems can be placed into broad categories such as alphabets, syllabaries, or logographies. Any particular system can have attributes of more than one category. In the alphabetic category, there is a standard set of letters (basic written symbols or graphemes) of consonants and vowels that encode based on the general principle that the letters (or letter pair/groups) represent speech sounds. In a syllabary, each symbol correlates to a syllable or mora. In a logography, each character represents a word, morpheme, or other semantic units. Other categories include abjads, which differ from alphabets in that vowels are not indicated, and abugidas or alphasyllabaries, with each character representing a consonant–vowel pairing. Alphabets typically use a set of 20-to-35 symbols to fully express a language, whereas syllabaries can have 80-to-100, and logographies can have several hundreds of symbols.\n\nMost systems will typically have an ordering of its symbol elements so that groups of them can be coded into larger clusters like words or acronyms (generally lexemes), giving rise to many more possibilities (permutations) in meanings than the symbols can convey by themselves. Systems will also enable the stringing together of these smaller groupings (sometimes referred to by the generic term 'character strings') in order to enable a full expression of the language. The reading step can be accomplished purely in the mind as an internal process, or expressed orally. A special set of symbols known as punctuation is used to aid in structure and organization of many writing systems and can be used to help capture nuances and variations in the message's meaning that are communicated verbally by cues in timing, tone, accent, inflection or intonation. A writing system will also typically have a method for formatting recorded messages that follows the spoken version's rules like its grammar and syntax so that the reader will have the meaning of the intended message accurately preserved.\n\nWriting systems were preceded by proto-writing, which used pictograms, ideograms and other mnemonic symbols. Proto-writing lacked the ability to capture and express a full range of thoughts and ideas. The invention of writing systems, which dates back to the beginning of the Bronze Age in the late Neolithic Era of the late 4th millennium BC, enabled the accurate durable recording of human history in a manner that was not prone to the same types of error to which oral history is vulnerable. Soon after, writing provided a reliable form of long distance communication. With the advent of publishing, it provided the medium for an early form of mass communication.\n\nThe creation of a new alphabetic writing system for a language with an existing logographic writing system is called alphabetization, as when the People's Republic of China studied the prospect of alphabetizing the Chinese languages with Latin script, Cyrillic script, Arabic script, and even numbers, although the most common instance of it, converting to Latin script, is usually called romanization.\n\nWriting systems are distinguished from other possible symbolic communication systems in that a writing system is always associated with at least one spoken language. In contrast, visual representations such as drawings, paintings, and non-verbal items on maps, such as contour lines, are not language-related. Some symbols on information signs, such as the symbols for male and female, are also not language related, but can grow to become part of language if they are often used in conjunction with other language elements. Some other symbols, such as numerals and the ampersand, are not directly linked to any specific language, but are often used in writing and thus must be considered part of writing systems.\n\nEvery human community possesses language, which many regard as an innate and defining condition of humanity. However, the development of writing systems, and the process by which they have supplanted traditional oral systems of communication, have been sporadic, uneven and slow. Once established, writing systems generally change more slowly than their spoken counterparts. Thus they often preserve features and expressions which are no longer current in the spoken language. One of the great benefits of writing systems is that they can preserve a permanent record of information expressed in a language.\n\nAll writing systems require:\n\n\nIn the examination of individual scripts, the study of writing systems has developed along partially independent lines. Thus, the terminology employed differs somewhat from field to field.\n\nThe generic term \"text\" refers to an instance of written or spoken material with the latter having been transcribed in some way. The act of composing and recording a text may be referred to as \"writing\", and the act of viewing and interpreting the text as \"reading\". \"Orthography\" refers to the method and rules of observed writing structure (literal meaning, \"correct writing\"), and particularly for alphabetic systems, includes the concept of \"spelling\".\n\nA \"grapheme\" is a specific base unit of a writing system. Graphemes are the \"minimally significant\" elements which taken together comprise the set of \"building blocks\" out of which texts made up of one or more writing systems may be constructed, along with rules of correspondence and use. The concept is similar to that of the phoneme used in the study of spoken languages. For example, in the Latin-based writing system of standard contemporary English, examples of graphemes include the majuscule and minuscule forms of the twenty-six letters of the alphabet (corresponding to various phonemes), marks of punctuation (mostly non-phonemic), and a few other symbols such as those for numerals (logograms for numbers).\n\nAn individual grapheme may be represented in a wide variety of ways, where each variation is visually distinct in some regard, but all are interpreted as representing the \"same\" grapheme. These individual variations are known as \"allographs\" of a grapheme (compare with the term allophone used in linguistic study). For example, the minuscule letter \"a\" has different allographs when written as a cursive, block, or typed letter. The choice of a particular allograph may be influenced by the medium used, the writing instrument, the stylistic choice of the writer, the preceding and following graphemes in the text, the time available for writing, the intended audience, and the largely unconscious features of an individual's handwriting.\n\nThe terms \"glyph\", \"sign\" and \"character\" are sometimes used to refer to a grapheme. Common usage varies from discipline to discipline; compare cuneiform sign, Maya glyph, Chinese character. The glyphs of most writing systems are made up of lines (or strokes) and are therefore called linear, but there are glyphs in non-linear writing systems made up of other types of marks, such as Cuneiform and Braille.\n\nWriting systems may be regarded as \"complete\" according to the extent to which they are able to represent all that may be expressed in the spoken language, while a \"partial\" writing system is limited in what it can convey.\n\nWriting systems can be independent from languages, one can have multiple writing systems for a language, e.g., Hindi and Urdu; and one can also have one writing system for multiple languages, e.g., the Arabic script. Chinese characters were also borrowed by variant countries as their early writing systems, e.g., the early writing systems of Vietnamese language until the beginning of the 20th century.\n\nTo represent a conceptual system, one uses one or more languages, e.g., mathematics is a conceptual system and one may use first-order logic and a natural language together in representation.\n\nWriting systems were preceded by proto-writing, systems of ideographic and/or early mnemonic symbols. The best known examples are:\n\n\nThe invention of the first writing systems is roughly contemporary with the beginning of the Bronze Age in the late Neolithic of the late 4th millennium BC. The Sumerian archaic cuneiform script and the Egyptian hieroglyphs are generally considered the earliest writing systems, both emerging out of their ancestral proto-literate symbol systems from 3400 to 3200 BC with earliest coherent texts from about 2600 BC. It is generally agreed that Sumerian writing was an independent invention; however, it is debated whether Egyptian writing was developed completely independently of Sumerian, or was a case of cultural diffusion.\n\nA similar debate exists for the Chinese script, which developed around 1200 BC. Chinese script are probably an independent invention, because there is no evidence of contact between China and the literate civilizations of the Near East, and because of the distinct differences between the Mesopotamian and Chinese approaches to logography and phonetic representation.\n\nThe pre-Columbian Mesoamerican writing systems (including among others Olmec and Maya scripts) are generally believed to have had independent origins.\n\nA hieroglyphic writing system used by pre-colonial Mi'kmaq, that was observed by missionaries from the 17th to 19th centuries, is thought to have developed independently. Although, there is some debate over whether or not this was a fully formed system or just a series of mnemonic pictographs.\n\nIt is thought that the first consonantal alphabetic writing appeared before 2000 BC, as a representation of language developed by Semitic tribes in the Sinai-peninsula (see History of the alphabet). Most other alphabets in the world today either descended from this one innovation, many via the Phoenician alphabet, or were directly inspired by its design.\n\nThe first true alphabet is the Greek script which consistently represents vowels since 800 BC. The Latin alphabet, a direct descendant, is by far the most common writing system in use.\n\nSeveral approaches have been taken to classify writing systems, the most common and basic one is a broad division into three categories: \"logographic\", \"syllabic\", and \"alphabetic\" (or \"segmental\"); however, all three may be found in any given writing system in varying proportions, often making it difficult to categorise a system uniquely. The term \"complex system\" is sometimes used to describe those where the admixture makes classification problematic. Modern linguists regard such approaches, including Diringer's\nas too simplistic, often considering the categories to be incomparable.\nHill split \"writing\" into three major categories of linguistic analysis, one of which covers discourses and is not usually considered writing proper:\nSampson draws a distinction between \"semasiography\" and \"glottography\"\nDeFrancis, criticizing Sampson's introduction of \"semasiographic writing\" and \"featural alphabets\" stresses the phonographic quality of writing proper\nFaber categorizes phonographic writing by two levels, linearity and coding:\n\nA \"logogram\" is a single written character which represents a complete grammatical word. Most traditional Chinese characters are classified as logograms.\n\nAs each character represents a single word (or, more precisely, a morpheme), many logograms are required to write all the words of language. The vast array of logograms and the memorization of what they mean are major disadvantages of logographic systems over alphabetic systems. However, since the meaning is inherent to the symbol, the same logographic system can theoretically be used to represent different languages. In practice, the ability to communicate across languages only works for the closely related varieties of Chinese, as differences in syntax reduce the crosslinguistic portability of a given logographic system. Japanese uses Chinese logograms extensively in its writing systems, with most of the symbols carrying the same or similar meanings. However, the grammatical differences between Japanese and Chinese are significant enough that a long Chinese text is not readily understandable to a Japanese reader without any knowledge of basic Chinese grammar, though short and concise phrases such as those on signs and newspaper headlines are much easier to comprehend.\n\nWhile most languages do not use wholly logographic writing systems, many languages use some logograms. A good example of modern western logograms are the Hindu-Arabic numerals: everyone who uses those symbols understands what \"1\" means whether they call it \"one\", \"eins\", \"uno\", \"yi\", \"ichi\", \"ehad\", \"ena\", or \"jedan\". Other western logograms include the ampersand \"&\", used for \"and\", the at sign \"@\", used in many contexts for \"at\", the percent sign \"%\" and the many signs representing units of currency ($, ¢, €, £, ¥ and so on.)\n\nLogograms are sometimes called ideograms, a word that refers to symbols which graphically represent abstract ideas, but linguists avoid this use, as Chinese characters are often semantic–phonetic compounds, symbols which include an element that represents the meaning and a phonetic complement element that represents the pronunciation. Some nonlinguists distinguish between lexigraphy and ideography, where symbols in lexigraphies represent words and symbols in ideographies represent words or morphemes.\n\nThe most important (and, to a degree, the only surviving) modern logographic writing system is the Chinese one, whose characters have been used with varying degrees of modification in varieties of Chinese, Japanese, Korean, Vietnamese, and other east Asian languages. Ancient Egyptian hieroglyphs and the Mayan writing system are also systems with certain logographic features, although they have marked phonetic features as well and are no longer in current use. Vietnamese speakers switched to the Latin alphabet in the 20th century and the use of Chinese characters in Korean is increasingly rare. The Japanese writing system includes several distinct forms of writing including logography.\n\n\"Another type of writing system with systematic syllabic linear symbols, the abugidas, is discussed below as well.\n\nAs logographic writing systems use a single symbol for an entire word, a \"syllabary\" is a set of written symbols that represent (or approximate) syllables, which make up words. A symbol in a syllabary typically represents a consonant sound followed by a vowel sound, or just a vowel alone.\n\nIn a \"true syllabary\", there is no systematic graphic similarity between phonetically related characters (though some do have graphic similarity for the vowels). That is, the characters for , and have no similarity to indicate their common \"k\" sound (voiceless velar plosive). More recent creations such as the Cree syllabary embody a system of varying signs, which can best be seen when arranging the syllabogram set in an onset–coda or onset–rime table.\n\nSyllabaries are best suited to languages with relatively simple syllable structure, such as Japanese. The English language, on the other hand, allows complex syllable structures, with a relatively large inventory of vowels and complex consonant clusters, making it cumbersome to write English words with a syllabary. To write English using a syllabary, every possible syllable in English would have to have a separate symbol, and whereas the number of possible syllables in Japanese is around 100, in English there are approximately 15,000 to 16,000.\n\nHowever, syllabaries with much larger inventories do exist. The Yi script, for example, contains 756 different symbols (or 1,164, if symbols with a particular tone diacritic are counted as separate syllables, as in Unicode). The Chinese script, when used to write Middle Chinese and the modern varieties of Chinese, also represents syllables, and includes separate glyphs for nearly all of the many thousands of syllables in Middle Chinese; however, because it primarily represents morphemes and includes different characters to represent homophonous morphemes with different meanings, it is normally considered a logographic script rather than a syllabary.\n\nOther languages that use true syllabaries include Mycenaean Greek (Linear B) and Indigenous languages of the Americas such as Cherokee. Several languages of the Ancient Near East used forms of cuneiform, which is a syllabary with some non-syllabic elements.\n\nAn \"alphabet\" is a small set of \"letters\" (basic written symbols), each of which roughly represents or represented historically a phoneme of a spoken language. The word \"alphabet\" is derived from alpha and beta, the first two symbols of the Greek alphabet.\n\nThe first type of alphabet that was developed was the abjad. An abjad is an alphabetic writing system where there is one symbol per consonant. Abjads differ from other alphabets in that they have characters only for consonantal sounds. Vowels are not usually marked in abjads.\n\nAll known abjads (except maybe Tifinagh) belong to the Semitic family of scripts, and derive from the original Northern Linear Abjad. The reason for this is that Semitic languages and the related Berber languages have a morphemic structure which makes the denotation of vowels redundant in most cases.\n\nSome abjads, like Arabic and Hebrew, have markings for vowels as well. However, they use them only in special contexts, such as for teaching. Many scripts derived from abjads have been extended with vowel symbols to become full alphabets. Of these, the most famous example is the derivation of the Greek alphabet from the Phoenician abjad. This has mostly happened when the script was adapted to a non-Semitic language.\n\nThe term \"abjad\" takes its name from the old order of the Arabic alphabet's consonants 'alif, bā', jīm, dāl, though the word may have earlier roots in Phoenician or Ugaritic. \"Abjad\" is still the word for alphabet in Arabic, Malay and Indonesian.\nAn abugida is an alphabetic writing system whose basic signs denote consonants with an inherent vowel and where consistent modifications of the basic sign indicate other following vowels than the inherent one.\n\nThus, in an abugida there may or may not be a sign for \"k\" with no vowel, but also one for \"ka\" (if \"a\" is the inherent vowel), and \"ke\" is written by modifying the \"ka\" sign in a way that is consistent with how one would modify \"la\" to get \"le\". In many abugidas the modification is the addition of a vowel sign, but other possibilities are imaginable (and used), such as rotation of the basic sign, addition of diacritical marks and so on.\n\nThe contrast with \"true syllabaries\" is that the latter have one distinct symbol per possible syllable, and the signs for each syllable have no systematic graphic similarity. The graphic similarity of most abugidas comes from the fact that they are derived from abjads, and the consonants make up the symbols with the inherent vowel and the new vowel symbols are markings added on to the base symbol.\n\nIn the Ge'ez script, for which the linguistic term \"abugida\" was named, the vowel modifications do not always appear systematic, although they originally were more so. Canadian Aboriginal syllabics can be considered abugidas, although they are rarely thought of in those terms. The largest single group of abugidas is the Brahmic family of scripts, however, which includes nearly all the scripts used in India and Southeast Asia.\n\nThe name \"abugida\" is derived from the first four characters of an order of the Ge'ez script used in some contexts. It was borrowed from Ethiopian languages as a linguistic term by Peter T. Daniels.\n\nA \"featural\" script represents finer detail than an alphabet. Here symbols do not represent whole phonemes, but rather the elements (features) that make up the phonemes, such as voicing or its place of articulation. Theoretically, each feature could be written with a separate letter; and abjads or abugidas, or indeed syllabaries, could be featural, but the only prominent system of this sort is Korean hangul. In hangul, the featural symbols are combined into alphabetic letters, and these letters are in turn joined into syllabic blocks, so that the system combines three levels of phonological representation.\n\nMany scholars, e.g. John DeFrancis, reject this class or at least labeling hangul as such. The Korean script is a conscious script creation by literate experts, which Daniels calls a \"sophisticated grammatogeny\". These include stenographies and constructed scripts of hobbyists and fiction writers (such as Tengwar), many of which feature advanced graphic designs corresponding to phonologic properties. The basic unit of writing in these systems can map to anything from phonemes to words. It has been shown that even the Latin script has sub-character \"features\".\n\nMost writing systems are not purely one type. The English writing system, for example, includes numerals and other logograms such as #, $, and &, and the written language often does not match well with the spoken one. As mentioned above, all logographic systems have phonetic components as well, whether along the lines of a syllabary, such as Chinese (\"logo-syllabic\"), or an abjad, as in Egyptian (\"logo-consonantal\").\n\nSome scripts, however, are truly ambiguous. The semi-syllabaries of ancient Spain were syllabic for plosives such as \"p\", \"t\", \"k\", but alphabetic for other consonants. In some versions, vowels were written redundantly after syllabic letters, conforming to an alphabetic orthography. Old Persian cuneiform was similar. Of 23 consonants (including null), seven were fully syllabic, thirteen were purely alphabetic, and for the other three, there was one letter for /C\"u\"/ and another for both /C\"a\"/ and /C\"i\"/. However, all vowels were written overtly regardless; as in the Brahmic abugidas, the /C\"a\"/ letter was used for a bare consonant.\n\nThe zhuyin phonetic glossing script for Chinese divides syllables in two or three, but into onset, medial, and rime rather than consonant and vowel. Pahawh Hmong is similar, but can be considered to divide syllables into either onset-rime or consonant-vowel (all consonant clusters and diphthongs are written with single letters); as the latter, it is equivalent to an abugida but with the roles of consonant and vowel reversed. Other scripts are intermediate between the categories of alphabet, abjad and abugida, so there may be disagreement on how they should be classified.\n\nPerhaps the primary graphic distinction made in classifications is that of \"linearity\". Linear writing systems are those in which the characters are composed of lines, such as the Latin alphabet and Chinese characters. Chinese characters are considered linear whether they are written with a ball-point pen or a calligraphic brush, or cast in bronze. Similarly, Egyptian hieroglyphs and Maya glyphs were often painted in linear outline form, but in formal contexts they were carved in bas-relief. The earliest examples of writing are linear: the Sumerian script of c. 3300 BC was linear, though its cuneiform descendants were not. Non-linear systems, on the other hand, such as braille, are not composed of lines, no matter what instrument is used to write them.\n\nCuneiform was probably the earliest non-linear writing. Its glyphs were formed by pressing the end of a reed stylus into moist clay, not by tracing lines in the clay with the stylus as had been done previously. The result was a radical transformation of the appearance of the script.\n\nBraille is a non-linear adaptation of the Latin alphabet that completely abandoned the Latin forms. The letters are composed of raised bumps on the writing substrate, which can be leather (Louis Braille's original material), stiff paper, plastic or metal.\n\nThere are also transient non-linear adaptations of the Latin alphabet, including Morse code, the manual alphabets of various sign languages, and semaphore, in which flags or bars are positioned at prescribed angles. However, if \"writing\" is defined as a potentially permanent means of recording information, then these systems do not qualify as writing at all, since the symbols disappear as soon as they are used. (Instead, these transient systems serve as signals.)\n\nScripts are also graphically characterized by the direction in which they are written. Egyptian hieroglyphs were written either left to right or right to left, with the animal and human glyphs turned to face the beginning of the line. The early alphabet could be written in multiple directions: horizontally (side to side), or vertically (up or down). Prior to standardization, alphabetical writing was done both left-to-right (LTR or sinistrodextrally) and right-to-left (RTL or dextrosinistrally). It was most commonly written boustrophedonically: starting in one (horizontal) direction, then turning at the end of the line and reversing direction.\n\nThe Greek alphabet and its successors settled on a left-to-right pattern, from the top to the bottom of the page. Other scripts, such as Arabic and Hebrew, came to be written right-to-left. Scripts that incorporate Chinese characters have traditionally been written vertically (top-to-bottom), from the right to the left of the page, but nowadays are frequently written left-to-right, top-to-bottom, due to Western influence, a growing need to accommodate terms in the Latin script, and technical limitations in popular electronic document formats. Chinese characters sometimes, as in signage, especially when signifying something old or traditional, may also be written from right to left. The Old Uyghur alphabet and its descendants are unique in being written top-to-bottom, left-to-right; this direction originated from an ancestral Semitic direction by rotating the page 90° counter-clockwise to conform to the appearance of vertical Chinese writing. Several scripts used in the Philippines and Indonesia, such as Hanunó'o, are traditionally written with lines moving away from the writer, from bottom to top, but are read horizontally left to right; however, Kulitan, another Philippine script, is written top to bottom and right to left. Ogham is written bottom to top and read vertically, commonly on the corner of a stone.\n\nIn computers and telecommunication systems, writing systems are generally not codified as such, but graphemes and other grapheme-like units that are required for text processing are represented by \"characters\" that typically manifest in encoded form. There are many , such as ISO/IEC 8859-1 (a character repertoire and encoding scheme oriented toward the Latin script), CJK (Chinese, Japanese, Korean) and bi-directional text. Today, many such standards are re-defined in a collective standard, the ISO/IEC 10646 \"Universal Character Set\", and a parallel, closely related expanded work, \"The Unicode Standard\". Both are generally encompassed by the term Unicode. In Unicode, each character, in every language's writing system, is (simplifying slightly) given a unique identification number, known as its \"code point\". Computer operating systems use code points to look up characters in the font file, so the characters can be displayed on the page or screen.\n\nA keyboard is the device most commonly used for writing via computer. Each key is associated with a standard code which the keyboard sends to the computer when it is pressed. By using a combination of alphabetic keys with modifier keys such as Ctrl, Alt, Shift and AltGr, various character codes are generated and sent to the CPU. The operating system intercepts and converts those signals to the appropriate characters based on the keyboard layout and input method, and then delivers those converted codes and characters to the running application software, which in turn looks up the appropriate glyph in the currently used font file, and requests the operating system to draw these on the screen.\n\n"}
