{"id": "910851", "url": "https://en.wikipedia.org/wiki?curid=910851", "title": "A Letter Concerning Toleration", "text": "A Letter Concerning Toleration\n\nA Letter Concerning Toleration by John Locke was originally published in 1689. Its initial publication was in Latin, though it was immediately translated into other languages. Locke's work appeared amidst a fear that Catholicism might be taking over England, and responds to the problem of religion and government by proposing religious toleration as the answer. This \"letter\" is addressed to an anonymous \"Honored Sir\": this was actually Locke's close friend Philipp van Limborch, who published it without Locke's knowledge.\n\nIn the wake of discovery of the Rye House Plot and Charles II's persecution of the Whigs, Locke fled England to Amsterdam, Holland in September 1683. Throughout his life, Locke had taken an interest in the debate about religious toleration. The question was much debated in Holland during Locke's stay and in October 1685 Louis XIV of France Revoked the Edict of Nantes that had guaranteed religious toleration for French Protestants.\n\nIn Holland, Locke met Philipp van Limborch, a Professor of Divinity, and it was to be a discussion with Limborch that persuaded Locke to temporarily put aside his work on \"An Essay Concerning Human Understanding\" and put forth his ideas on toleration. Locke wrote the \"Letter\" during the winter of 1685-86.\n\nOne of the founders of Empiricism, Locke develops a philosophy that is contrary to the one expressed by Thomas Hobbes in \"Leviathan\", in supporting toleration for various Christian denominations. Hobbes did allow for individuals to maintain their own religious beliefs as long as they outwardly expressed those of the state, however, and it has been argued that Locke's rejection of Catholic Imperialism was the ultimate basis for his rejection of government's interest in spiritual salvation.\n\nUnlike Hobbes, who saw uniformity of religion as the key to a well-functioning civil society, Locke argues that more religious groups actually prevent civil unrest. Locke argues that civil unrest results from confrontations caused by any magistrate's attempt to prevent different religions from being practiced, rather than tolerating their proliferation. Locke's primary goal is to \"distinguish exactly the business of civil government from that of religion.\" He seeks to persuade the reader that government is instituted to promote external interests, relating to life, liberty, and the general welfare, while the church exists to promote internal interests, i.e., salvation. The two serve separate functions, and so, must be considered to be separate institutions.\n\nFor Locke, the only way a Church can gain genuine converts is through persuasion and not through violence. This relates to his central conclusion, namely, that the government should not involve itself in care of souls. In support of this argument he presents three main reasons: (1) individuals, according to Locke, cannot divest control over their souls to secular forces, as God does not appoint the magistrate; (2) force cannot create the change necessary for salvation, because while it can coerce obedience, it cannot change one's beliefs; and (3) even if coercion could persuade someone of a notion, it would not help with ensuring salvation, because there is no reason to believe that magistrates are reliable judges of religious truth.\n\nLocke argued that those who believed that \"faith need not be kept with heretics\" and that \"kings excommunicated forfeit their kingdoms\"\nhad \"no right to be tolerated by the magistrate\". Neither did \"those who refuse to teach that dissenters from their own religion should be tolerated\". This was because those who believed such doctrines would, given the opportunity,\nattack the laws and the liberty and property of the citizen. These people, Locke argued, sought religious toleration \"only until they have supplies and forces enough to make the attempt\" on liberty. The doctrines that \"faith need not be kept with heretics\" and that \"kings excommunicated forfeit their kingdoms\" were commonly held to be Catholic beliefs by Protestants. During his visit to France in 1676, Locke recorded that the belief that \"faith does not have to be kept with heretics\" was an important factor in the intolerance shown to the Protestant Huguenots.\n\n\"That church can have no right to be tolerated by the magistrate,\" Locke argued, \"which is so constituted that all who enter it \"ipso facto\" pass into the allegiance and service of another prince\". If this were to be tolerated, \"the magistrate would make room for a foreign jurisdiction in his own territory and...allow for his own people to be enlisted as soldiers against his own government\". This has been interpreted by historians as a reference to the Catholic Church, with the Pope being the prince to whom Catholics owed allegiance.\n\nHowever, more recently scholars have challenged the idea that Locke opposed the toleration of Catholics in all circumstances. Mark Goldie argues that the traditional interpretation of Locke's position on Catholics \"needs finessing, since he did not, in fact, exclude the theoretical possibility of tolerating Catholics...if Catholics could discard their uncivil beliefs, they could then be tolerated\". Goldie asserts that Locke was opposed not to Catholicism as such but antinomianism, the belief that ordinary moral laws are superseded by religious truth. Scott Sowerby also claims that Locke left open the possibility that Catholics could be tolerated if they adopted tolerant principles and rejected political allegiance to the Pope.\n\nJohn Marshall has argued that a number of passages in the \"Letter\" demonstrate that Locke believed that Catholics \"in their terms of worship and religious speculative beliefs...deserved their worship to be free\". Marshall also notes that \"The combination of Locke’s comments in the Letter suggest that during [its] composition ... Locke was once again struggling over how to discriminate between the series of associated political principles which for him made Catholics intolerable, and the religious worship and other religious beliefs of Catholics which deserved toleration.\"\n\nLocke argued that atheists should not be tolerated because \"Promises, covenants, and oaths, which are the bonds of human society, can have no hold upon or sanctity for an atheist\". There is, however, a passage added in a later edition of the \"Essay concerning Human Understanding\", where Locke perhaps questioned \"whether 'atheism' was necessarily inimical to political obedience.\"\n\nToleration is central to Locke's political philosophy. Consequently, only churches that teach toleration are to be allowed in his society. Locke’s view on the difficulty of knowing the one true religion may suggest that religion is not personally important to Locke, but it also may point to the deep uncertainties surrounding religious belief in a time of political and intellectual conflict. In contrast, Locke’s view on atheism suggests that he was far from considering religion as unimportant. As an empiricist, he took practical considerations into account, such as how the peace of civil society will be affected by religious toleration. A close reading of the text also reveals that Locke relies on Biblical analysis at several key points in his argument.\n\nThere were immediate responses from the High Church Anglican clergy, published by Thomas Long and Jonas Proast. Long believed the letter was written by an atheistically disguised Jesuit plot for the Roman Catholic Church to gain dominance by bringing chaos and ruin to church and state. Proast attacked the Letter and defended the view that the government has the right to use force to cause dissenters to reflect on the merits of Anglicanism, the True Religion. Locke's reply to Proast developed into an extended, controversial exchange.\n\n\n"}
{"id": "52922616", "url": "https://en.wikipedia.org/wiki?curid=52922616", "title": "Access to public information in Armenia", "text": "Access to public information in Armenia\n\nAccess to public information and freedom of information (FOI) refer to the right to access information held by public bodies also known as \"right to know\". Access to public information is considered of fundamental importance for the effective functioning of democratic systems, as it enhances governments' and public officials' accountability, boosting people participation and allowing their informed participation into public life. The fundamental premise of the right to access public information is that the information held by governmental institutions is in principle public and may be concealed only on the basis of legitimate reasons which should be detailed in the law. Access to public information builds on the principle that in a democratic system people should be in the condition of accessing a wide range of information in order to effectively participate in public life as well as on matters affecting them.\n\nIn Armenia the right of access to information is guaranteed and defined by the Articles 27 and 27.1 of the Constitution. Also, Armenia has a specific law on freedom of information providing the basic foundations for the realization of this right. According to the Constitution and the Law, in Armenia everybody, in spite of the fact he/she is a citizen of the country or not, has a right to have access to information.\n\nThe Law on Freedom of Information was adopted in September 2003 and was widely considered a progressive document, greatly appreciated by international experts. Since the adoption of the law, civil society and the journalists community, have advocated for the need to pass the regulations that would have supported and improved the implementation of the law. Such regulations were finally adopted in October 2015, when the Armenian government approved a procedure for improving the effectiveness of the procedure to access public information by streamlining the classification, maintenance and provision of information from the government to the public. Prior to the adoption of these regulations, in several cases public officials used to cite the lack of these implementation rules when refusing requests for information.\n\nAccording to the law, people can obtain information from: central government bodies, self-governing bodies, state institutions, organizations financed from central or local government finances, organizations with public functions, i.e. organisations providing public services (for instance, private universities, schools, hospitals, energy provider companies, etc.) Each body has a person that is responsible for freedom of information and is in charge of dealing with information requests.\n\nTo submit a request to access information, applicants have to apply to the information holder with a written or oral inquiry to be addressed to specific departments in the appropriate body or organization. Civil society organizations advocating for access to information suggest to apply with a written inquiry so that a compliant can be done in case of rejection. To be taken into consideration, both written and oral inquiries have to meet the requirements stated by the law.\n\nThe inquirer is not obliged to justify the enquiry and to state the reason for it or how the information sought is going to be used.\n\nIf the body/organizations does not possess all the details concerning the information required, the information holder has to provide the available parts and mention the name and the location of other possible information holders who might be in possess of other details concerning the information sought.\n\nInformation are provided free of charge. Only technical expenses of information provision (photocopies, electronic supports, etc.) can be charged.\n\nFreedom of information can be restricted only on grounds well defined by the law. Information holder may refuse to provide information if it contains state, official, bank or trade secret; infringe the privacy of individuals and his family; contains pre-investigation data which cannot be made public; concerns data with accessibility limitation (such as business, notary or medical secrets); infringes copyright and associated rights.\n\nIn case of rejection, the information holder has to inform the applicant within 5 days in a written form and explain the reasons for the refusal as well as the relevant appealing procedures. Denials can be appealed to the Ombudsman or the court by administrative procedure. Citizens can also apply directly to the concerned bodies without appealing by an administrative procedure. Also, it is possible to apply to non-governmental organizations which provide legal advisory.\n\nIn practice, despite the good legal framework, citizens and media professionals face some problems in accessing documents, such as incomplete disclosure of the information required, extensions of the time frame for obtaining information prescribed by the law, the refusal of some government bodies and officials to implement the law, and so forth. According to the NGO Freedom of Information Center of Armenia and the Transparency International Anticorruption Center, the main challenge is the behaviour of some public officials and public bodies that continue to provide answers that are either irrelevant or incomplete. This has resulted in major difficulties for NGOs being less and less successful in bringing freedom of information cases before the courts than in previous years, as judges rejected their cases on the grounds that public offices had provided the answers, regardless of their content.\n\nIn Armenia, NGOs have played a significant role in educating the public on how to exercise their \"right to know\" and have organised trainings for public officials. By 2012, the NGO Freedom of Information Center Armenia has distributed around 2050 bulletin boards in several urban and rural communities in order to inform the public on the right to access public information and how to realize it.\n\n"}
{"id": "53125280", "url": "https://en.wikipedia.org/wiki?curid=53125280", "title": "Access to public information in Bulgaria", "text": "Access to public information in Bulgaria\n\nAccess to public information and freedom of information (FOI) refer to the right of access to information held by public bodies also known as \"right to know\". Access to public information is considered of fundamental importance for the effective functioning of democratic systems, as it enhances governments' and public officials' accountability, boosting people participation and allowing their informed participation into public life. The fundamental premise of the right of access to public information is that the information held by governmental institutions is in principle public and may be concealed only on the basis of legitimate reasons which should be detailed in the law.\n\nAccess to Public Information in Bulgaria is a right guaranteed by the 1991 Constitution. It is regulated by the Access to Public Information Act first introduced in 2000 and amended in December 2015. The amendments enhanced proactive disclosure, encouraged electronic requests and facilitate re-use of information, in line with Directive 2013/37/EU on the Re-Use of Public Sector Information.\n\nIn 2014, a public consultation on the amendments to be introduced in the access to information legislation was initiated. In the Summer of 2014, a working group was set up at the Ministry of Transport Information Technologies and Communications with the mandate of drafting amendments to the Bulgarian legislation regarding the introduction of the revised Directive 2013/37/EU on the Re-use of Public Sector Information of the European Parliament and the Council of 26 June 2013. The amended Directive enlarged the scope of the re-use of public sector information by including archives, libraries and museums.\n\nThe initiative of amending the Bulgarian law on access to public information was also prepared in the framework of drafting and discussing the National Plan under the Open Government Partnership initiative.\n\nIn November 2015, the Bulgarian National Assembly adopted amendments to the Access to Public Information Act to enhance proactive disclosure, encourage electronic requests and facilitate re-use of information.\n\nAccording to Professor Georgi Lozanov, a former member of the Council for Electronic Media, the introduction of the Access to Information Law could help in making more transparent the media sector, in particular media ownership and sources of funds which are largely opaque in Bulgaria.\n\nArticle 41 of the Bulgarian Constitution of 1991 states that everyone shall be entitled to seek, receive and impart information, provided that this right shall not be exercised to the detriment of national security, public order, public health and morality. Article 41 entitles citizens to obtain information from state bodies and agencies on any matter of legitimate public interest which is not a state or other secret prescribed by law and does not affect the rights of other.\n\nIn Bulgaria access to public information is regulated by the Access to Public Information Act enacted in 2000, and amended in 2008 and 2015. The Law entitles any person or legal entity to the right of access to public information in any form held by state institutions and other entities financed by state budget and exercising public functions.\n\nThe amended legal framework on access to public information introduced an extended the list of categories of information which are subject to proactive disclosure. Also, amendments introduced an explicit duty to publish any information that have been provided on requests more than three times, along with a broader obligation to publish online information of public interest.\n\nThe amendments also aimed at encouraging the submission of electronic requests, which can be sent with no need of electronic signature.\n\nAnother novelty concerns the presumption of third-party consent, meaning that if a public authority asks for a third-party consent for the disclosure of requested information affecting it, the lack of response within 14 days will be presumed as consent and the information should be completely provided, Thanks to the amendment, thus, the requested information is not considered, as it was before the amendment, a dissent by the third party which obliged the public body to provide only partial access.\n\nThe amendments also introduced the Directive 2013/37/UE, revising the first Directive on the Re-Use of Public Sector Information (2003) which was transposed in 2007 in the access to Public Information Act. In line with the revised directive, the amended law extended the re-use regime by prohibiting any exclusive clauses in giving rights to use whole databases coherently with the Directive provision that obliges any public body in the EU to provide equal availability of such databases on equal conditions and with costs for accessing calculated in transparent way.\n\nThe new legal framework adopts a broader definition of the \"public law organizations\" that are subject to both access to information requests and information re-use. Libraries, museums and archives have been included among the institutions that have to disclose information for re-use. According to the amended law, public sector bodies are obliged to make their documents available in a user-friendly manner and in open and machine-readable format, together with their metadata in a government Open Data Portal.\n\nUnder the Bulgarian law on Access to Public Information, all state bodies, including their directorates/regional/local offices/territorial units, etc. are subject to access to information. Also, the law applies to local government authorities, including mayors and municipal councils. Moreover, other entities are subject to the access to information law, in particular all authorities which perform public functions prescribed by the law, named \"public law entities\", for instance the Electronic Media Council, the National Health Insurance Fund and the like, and individuals and legal entities financed with funds from the state budget or the European Union (both subsidies and EU projects and programmes).\n\nThe law applies to any information generated or held by state authorities, local authorities and other entities obliged under the law. The right of access to public information is restricted in case of information:\n\n\nHowever, according to the law, even if there is a reason for refusal, information shall be disclosed if there is an overriding public interest which disclosure contributes to enhance.\n\nInformation requests may be submitted either orally or in written form. According to the law, each authority is obliged to appoint for its office an official with the responsibility of handling the requests.\n\nIn line with international best practices, access to information in Bulgaria is free of charge. Paying a fee can only be required to cover the actual costs of reproducing the document (copies, prints, etc.)\n\nUnder the Bulgarian law, in case requesters do not receive the information sought they may appeal to the court. The law does not establish an institution in charge of overseeing the implementation of the law in access to public information.\n\nAccording to a study conducted by the NGO Access to Information Program (AIP) in 2015, every third institution in Bulgaria does not make public its regulatory acts and half of them do not do this with regard to the general administrative acts. Among the institutions which fail to publish their regulatory acts there are the Ministry of Foreign Affairs and many municipalities.\n\nThe study also finds out there is a bit more transparency in relation to the publication of databases and registers. The same holds good for the publication of programs and strategic documents, such as development strategies, and the like.\n\nAs for financial transparency, the report acknowledges an improvement since 2013, but still about half of the institutions do not publish their budgets on the Internet as required by the law.A good level of transparency has been registered on disclosure of information for public procurement tenders: 94% of institutions scrutinized by the AIP has a dedicated section on their websites.\n\nThe report found out that Bulgarian institutions do not comply with the new requirements of the Access to Public Information Act when it establishes that they are obliged to publish the information that they have provided to citizens more than three times on the grouds of freedom of information applications. According to the AIP survey, only 4% out of 565 institutions scrutinized had complied with this requirement at the time the survey was conducted during 2015. However, almost all institutions, around 97%, complied with the new requirement that introduced electronic applications for freedom of information requests.\n\nThe 2015 AIP survey revealed that the Ministry of Education and Science is the most transparent ministry in Bulgaria, while the Ministry of Foreign Affairs is the least transparent.\n\n"}
{"id": "57505663", "url": "https://en.wikipedia.org/wiki?curid=57505663", "title": "Alena Kupčíková", "text": "Alena Kupčíková\n\nAlena Kupčíková (born 22 October 1976) is a Czech artist.\n\nShe was born in Šumperk and studied at the in Prague, the Technikon Natal in Durban, the École nationale supérieure des Beaux-Arts in Paris and the Academy of Fine Arts, Prague, receiving a PhD from the latter institution.\n\nShe also includes hair from both human and animal sources in her drawings. Her multimedia works also incorporate recordings of various sounds.\n\nKupčíková is dyslexic and founded a non-profit organization in support of dyslexic children. She created a special multimedia work for dyslexic children which received an award at the 2014 International Exhibition of Inventions (IEIK) in Kunshan.\n\nIn 2003, she was awarded a LMVH prize for her work. Kupčíková is a member of the FEMLINK international women's art group which was founded in Paris in 2006.\n\nHer work is included in the collection of the National Gallery in Prague and in private collections in the Czech Republic, Slovakia, Germany, Poland, the Netherlands, England, Switzerland, Japan and the United States.\n"}
{"id": "6190251", "url": "https://en.wikipedia.org/wiki?curid=6190251", "title": "Argument map", "text": "Argument map\n\nIn informal logic and philosophy, an argument map or argument diagram is a visual representation of the structure of an argument. An argument map typically includes the key components of the argument, traditionally called the \"conclusion\" and the \"premises\", also called \"contention\" and \"reasons\". Argument maps can also show co-premises, objections, counterarguments, rebuttals, and lemmas. There are different styles of argument map but they are often functionally equivalent and represent an argument's individual claims and the relationships between them.\n\nArgument maps are commonly used in the context of teaching and applying critical thinking. The purpose of mapping is to uncover the logical structure of arguments, identify unstated assumptions, evaluate the support an argument offers for a conclusion, and aid understanding of debates. Argument maps are often designed to support deliberation of issues, ideas and arguments in wicked problems.\n\nAn argument map is not to be confused with a concept map or a mind map, two other kinds of node–link diagram which have different constraints on nodes and links.\n\nA number of different kinds of argument map have been proposed but the most common, which Chris Reed and Glenn Rowe called the \"standard diagram\", consists of a tree structure with each of the reasons leading to the conclusion. There is no consensus as to whether the conclusion should be at the top of the tree with the reasons leading up to it or whether it should be at the bottom with the reasons leading down to it. Another variation diagrams an argument from left to right.\n\nAccording to Doug Walton and colleagues, an argument map has two basic components: \"One component is a set of circled numbers arrayed as points. Each number represents a proposition (premise or conclusion) in the argument being diagrammed. The other component is a set of lines or arrows joining the points. Each line (arrow) represents an inference. The whole network of points and lines represents a kind of overview of the reasoning in the given argument...\" With the introduction of software for producing argument maps, it has become common for argument maps to consist of boxes containing the actual propositions rather than numbers referencing those propositions.\n\nThere is disagreement on the terminology to be used when describing argument maps, but the \"standard diagram\" contains the following structures:\n\nDependent premises or co-premises, where at least one of the joined premises requires another premise before it can give support to the conclusion: An argument with this structure has been called a \"linked\" argument.\n\nIndependent premises, where the premise can support the conclusion on its own: Although independent premises may jointly make the conclusion more convincing, this is to be distinguished from situations where a premise gives no support unless it is joined to another premise. Where several premises or groups of premises lead to a final conclusion the argument might be described as \"convergent\". This is distinguished from a \"divergent\" argument where a single premise might be used to support two separate conclusions.\n\nIntermediate conclusions or sub-conclusions, where a claim is supported by another claim that is used in turn to support some further claim, i.e. the final conclusion or another intermediate conclusion: In the following diagram, statement 4 is an intermediate conclusion in that it is a conclusion in relation to statement 5 but is a premise in relation to the final conclusion, i.e. statement 1. An argument with this structure is sometimes called a \"complex\" argument. If there is a single chain of claims containing at least one intermediate conclusion, the argument is sometimes described as a \"serial\" argument or a \"chain\" argument.\n\nEach of these structures can be represented by the equivalent \"box and line\" approach to argument maps. In the following diagram, the \"contention\" is shown at the top, and the boxes linked to it represent supporting \"reasons\", which comprise one or more \"premises\". The green arrow indicates that the two \"reasons\" support the \"contention\":\n\nArgument maps can also represent counterarguments. In the following diagram, the two \"objections\" weaken the \"contention\", while the \"reasons\" support the \"premise\" of the objection:\n\nA written text can be transformed into an argument map by following a sequence of steps. Monroe Beardsley's 1950 book \"Practical Logic\" recommended the following procedure:\n\nBeardsley gave the first example of a text being analysed in this way:\n\nBeardsley said that the conclusion in this example is statement ②. Statement ④ needs to be rewritten as a declarative sentence, e.g. \"Academic monstrosities [were] produced by the official Nazi painters.\" Statement ① points out that the conclusion isn't accepted by everyone, but statement ① is omitted from the diagram because it doesn't support the conclusion. Beardsley said that the logical relation between statement ③ and statement ④ is unclear, but he proposed to diagram statement ④ as supporting statement ③.\n\nMore recently, philosophy professor Maralee Harrell recommended the following procedure:\n\nArgument maps are useful not only for representing and analyzing existing writings, but also for thinking through issues as part of a problem-structuring process or writing process. The use of such argument analysis for thinking through issues has been called \"reflective argumentation\".\n\nAn argument map, unlike a decision tree, does not tell how to make a decision, but the process of choosing a coherent position (or reflective equilibrium) based on the structure of an argument map can be represented as a decision tree.\n\nIn the \"Elements of Logic\", which was published in 1826 and issued in many subsequent editions, Archbishop Richard Whately gave probably the first form of an argument map, introducing it with the suggestion that \"many students probably will find it a very clear and convenient mode of exhibiting the logical analysis of the course of argument, to draw it out in the form of a Tree, or Logical Division\".\n\nHowever, the technique did not become widely used, possibly because for complex arguments, it involved much writing and rewriting of the premises.\nLegal philosopher and theorist John Henry Wigmore produced maps of legal arguments using numbered premises in the early 20th century, based in part on the ideas of 19th century philosopher Henry Sidgwick who used lines to indicate relations between terms.\n\nDealing with the failure of formal reduction of informal argumentation, English speaking argumentation theory developed diagrammatic approaches to informal reasoning over a period of fifty years.\n\nMonroe Beardsley proposed a form of argument diagram in 1950. His method of marking up an argument and representing its components with linked numbers became a standard and is still widely used. He also introduced terminology that is still current describing \"convergent\", \"divergent\" and \"serial\" arguments.\n\nStephen Toulmin, in his groundbreaking and influential 1958 book \"The Uses of Argument\", identified several elements to an argument which have been generalized. The Toulmin diagram is widely used in educational critical teaching. Whilst Toulmin eventually had a significant impact on the development of informal logic he had little initial impact and the Beardsley approach to diagramming arguments along with its later developments became the standard approach in this field. Toulmin introduced something that was missing from Beardsley's approach. In Beardsley, \"arrows link reasons and conclusions (but) no support is given to the implication itself between them. There is no theory, in other words, of inference distinguished from logical deduction, the passage is always deemed not controversial and not subject to support and evaluation\". Toulmin introduced the concept of \"warrant\" which \"can be considered as representing the reasons behind the inference, the backing that authorizes the link\".\n\nBeardsley's approach was refined by Stephen N. Thomas, whose 1973 book \"Practical Reasoning In Natural Language\" introduced the term \"linked\" to describe arguments where the premises necessarily worked together to support the conclusion. However, the actual distinction between dependent and independent premises had been made prior to this. The introduction of the linked structure made it possible for argument maps to represent missing or \"hidden\" premises. In addition, Thomas suggested showing reasons both \"for\" and \"against\" a conclusion with the reasons \"against\" being represented by dotted arrows. Thomas introduced the term \"argument diagram\" and defined \"basic reasons\" as those that were not supported by any others in the argument and the \"final conclusion\" as that which was not used to support any further conclusion.\nMichael Scriven further developed the Beardsley-Thomas approach in his 1976 book \"Reasoning\". Whereas Beardsley had said \"At first, write out the statements...after a little practice, refer to the statements by number alone\" Scriven advocated clarifying the meaning of the statements, listing them and then using a tree diagram with numbers to display the structure. Missing premises (unstated assumptions) were to be included and indicated with an alphabetical letter instead of a number to mark them off from the explicit statements. Scriven introduced counterarguments in his diagrams, which Toulmin had defined as rebuttal. This also enabled the diagramming of \"balance of consideration\" arguments.\n\nIn 1998 a series of large-scale argument maps released by Robert E. Horn stimulated widespread interest in argument mapping.\n\nHuman–computer interaction pioneer Douglas Engelbart, in a famous 1962 technical report on intelligence augmentation, envisioned in detail something like argument-mapping software as an integral part of future intelligence-augmenting computer interfaces: \n\nIn the middle to late 1980s, hypertext software applications that supported argument visualization were developed, including NoteCards and gIBIS; the latter generated an on-screen graphical hypertextual map of an issue-based information system, a model of argumentation developed by Werner Kunz and Horst Rittel in the 1970s. In the 1990s, Tim van Gelder and colleagues developed a series of software applications that permitted an argument map's premises to be fully stated and edited in the diagram, rather than in a legend. Van Gelder's first program, Reason!Able, was superseded by two subsequent programs, bCisive and Rationale.\n\nThroughout the 1990s and 2000s, many other software applications were developed for argument visualization. By 2013, more than 60 such software systems existed. One of the differences between these software systems is whether collaboration is supported. Single-user argumentation systems include Convince Me, iLogos, LARGO, Athena, Araucaria, and Carneades; small group argumentation systems include Digalo, QuestMap, Compendium, Belvedere, and AcademicTalk; community argumentation systems include Debategraph and Collaboratorium.\n\nArgument maps have been applied in many areas, but foremost in educational, academic and business settings, including design rationale. Argument maps are also used in forensic science, law, and artificial intelligence. It has also been proposed that argument mapping has a great potential to improve how we understand and execute democracy, in reference to the ongoing evolution of e-democracy.\n\nIt has traditionally been hard to separate teaching critical thinking from the philosophical tradition of teaching logic and method, and most critical thinking textbooks have been written by philosophers. Informal logic textbooks are replete with philosophical examples, but it is unclear whether the approach in such textbooks transfers to non-philosophy students. There appears to be little statistical effect after such classes. Argument mapping, however, has a measurable effect according to many studies. For example, instruction in argument mapping has been shown to improve the critical thinking skills of business students.\n\nThere is empirical evidence that the skills developed in argument-mapping-based critical thinking courses substantially transfer to critical thinking done without argument maps. Alvarez's meta-analysis found that such critical thinking courses produced gains of around 0.70 SD, about twice as much as standard critical-thinking courses. The tests used in the reviewed studies were standard critical-thinking tests.\n\nThe use of argument mapping has occurred within a number of disciplines, such as philosophy, management reporting, military and intelligence analysis, and public debates.\n\n\nThe Argument Interchange Format, AIF, is an international effort to develop a representational mechanism for exchanging argument resources between research groups, tools, and domains using a semantically rich language. AIF-RDF is the extended ontology represented in the Resource Description Framework Schema (RDFS) semantic language. Though AIF is still something of a moving target, it is settling down.\n\nThe Legal Knowledge Interchange Format (LKIF) was developed in the European ESTRELLA project and designed with the goal of becoming a standard for representing and interchanging policy, legislation and cases, including their justificatory arguments, in the legal domain. LKIF builds on and uses the Web Ontology Language (OWL) for representing concepts and includes a reusable basic ontology of legal concepts.\n\n"}
{"id": "5407398", "url": "https://en.wikipedia.org/wiki?curid=5407398", "title": "C date and time functions", "text": "C date and time functions\n\nThe C date and time functions are a group of functions in the standard library of the C programming language implementing date and time manipulation operations. They provide support for time acquisition, conversion between date formats, and formatted output to strings.\n\nThe C date and time operations are defined in the codice_1 header file (codice_2 header in C++).\n\nThe following C source code prints the current time to the standard output stream.\nThe output is:\n"}
{"id": "3663289", "url": "https://en.wikipedia.org/wiki?curid=3663289", "title": "Capacitor analogy", "text": "Capacitor analogy\n\nThere are several formal analogies that can be made between electricity, which is invisible to the eye, and more familiar physical notions, such as the flowing of water or the motion of mechanical devices. \n\nIn the case of capacitance, one analogy to a capacitor in mechanical rectilineal terms is a spring where the compliance of the spring is analogous to the capacitance. Thus in electrical engineering, a capacitor may be defined as an ideal electrical component which satisfies this equation V = (1/C)ʃIdt where:\n\nV is voltage measured at the terminals of the capacitor\n\nC is the capacitance of the capacitor\n\nI is the current flowing between the terminals of the capacitor\n\nt is time\n\nThe equation quoted above has the same form as that describing an ideal massless spring:\nF = (1/k)ʃvdt where:\n\nF is the force applied between the two ends of the spring\n\nk is the compliance of the spring defined as displacement/force\n\nv is the speed (or velocity) of one end of the spring, the other end being fixed.\n\nNote that in the electrical case, current (I) is defined as the rate of change of charge (Q) with respect to time:\n\nI = dQ/dt\n\nWhile in the mechanical case, velocity (v) is defined as the rate of change of displacement (d) with respect to time:\n\nv = dd/dt\n\nThus, in this analogy:\n\n\nAlso, these analogous relationships apply:\n\n\nThis analogy of the capacitor forms part of the more comprehensive impedance analogy of mechanical to electrical systems.\n\n\n"}
{"id": "34570096", "url": "https://en.wikipedia.org/wiki?curid=34570096", "title": "Civil rights referendum", "text": "Civil rights referendum\n\nA civil rights referendum or human rights referendum is any act of direct democracy which allows for a vote on the granting or amendment of current civil rights, liberties or associations as recognized by a government. Such referenda have frequently been proposed as a means by which the majority of the voting public in a polity, rather than the judicial or legislative chambers of government, could determine what the state should recognize or carry out, while such referenda have been strongly criticized by civil rights organizations and professional bodies as means by which the majority of the public could vote on the rights of a vulnerable minority according to contemporary prejudices.\n\nCivil rights referenda have been frequently proposed by those in ideological rejection against lesbian, gay, bisexual or transgender rights, most often due to Abrahamic religious objections against homosexuality. In countries where governments have passed, activists have frequently sought to put either a repeal of the new LGBT-affirmative law or a (constitutional or statutory) ban on LGBT-affirmative activities or relationships, and often rely upon a core religious constituency in order to drive the advocacy for such a referendum.\n\nIn the United States, civil rights referenda were held in the latter 1900s in order to prohibit same-sex unions (including marriage) and repeal amendments to human rights ordinances which included sexual orientations and gender identities as protected classes. The climax of such legislation was the passage of a record number of U.S. state constitutional amendments banning same-sex unions by referendum in 2004, which coincided with a large turnout for the re-election of George W. Bush to the presidency and Republican legislators to control of both houses of Congress.\n\nAmong those who advocate for LGBT rights, the delegation of marriage and other rights to the \"will of the people\" has propelled the notion of preventing civil rights-related laws and proposals from going to the ballot. This notion was underscored in the aftermath of the passage of California Proposition 8 in California.\n\nIn a 2000 Alabama referendum on repealing the 1901 state constitutional ban on interracial marriage, over 40% of the participating electorate voted against repealing the ban. While the ban was rendered unenforceable following \"Loving v. Virginia\", the 40.51% of the populace voting against the repeal . In 1998, South Carolina voters voted 61.95%-38.05% in favor of repealing their own constitutional ban. Harvard University professor Werner Sollors intimated that the laws took so long after \"Loving\" to be repealed because of the complex clauses which required large majorities in order to repeal them.\n\nOn January 26, 2012, in remarks accompanying his decision to veto the legalization of same-sex marriage by the State Legislature and call for a referendum on the matter, New Jersey governor Chris Christie remarked that \"The fact of the matter is, I think people would have been happy to have a referendum on civil rights rather than fighting and dying in the streets in the South. It was our political institutions that were holding things back.\" He was strongly criticized by politicians and activists of both African-American and other ancestries from both inside and outside of New Jersey, with Newark mayor Cory Booker stating \"...dear God, we should not put civil rights issues to a popular vote to be subject to the sentiments and passions of the day. No minority should have their civil rights subject to the passions and sentiments of the majority.\" Christie initially criticized the response of many legislators as a Democratic partisan ploy, but walked back his comments by apologizing for offense on February 1 while continuing to back his call for a referendum.\n\nWomen's suffrage was first brought forward as proposed legislation in the Kansas women's suffrage referendum, 1867. The proposal was defeated twice before passing.\n"}
{"id": "7472170", "url": "https://en.wikipedia.org/wiki?curid=7472170", "title": "Composition of relations", "text": "Composition of relations\n\nIn the mathematics of binary relations, the composition relations is a concept of forming a new relation from two given relations \"R\" and \"S\". The composition of relations is called relative multiplication in the calculus of relations. The composition is then the relative product of the factor relations. Composition of functions is a special case of composition of relations.\n\nThe words uncle and aunt indicate a compound relation: for a person to be an uncle, he must be a brother of a parent (or a sister for an aunt). In algebraic logic it is said that the relation of Uncle ( \"xUz\" ) is the composition of relations \"is a brother of\" ( \"xBy\" ) and \"is a parent of\" ( \"yPz\" ).\n\nBeginning with Augustus De Morgan, the traditional form of reasoning with by syllogism has been subsumed by relational logical expressions and their composition.\n\nIf formula_2 and formula_3 are two binary relations, then\ntheir composition formula_4 is the relation\n\nIn other words, formula_6 is defined by the rule that says formula_7 if and only if there is an element formula_8 such that formula_9 (i.e. formula_10 and formula_11).\n\nIn particular fields, authors might denote by what is defined here to be .\nThe convention chosen here is such that function composition (with the usual notation) is obtained as a special case, when \"R\" and \"S\" are functional relations. Some authors prefer to write formula_12 and formula_13 explicitly when necessary, depending whether the left or the right relation is the first one applied.\n\nA further variation encountered in computer science is the Z notation: formula_14 is used to denote the traditional (right) composition, but ⨾ ; (a fat open semicolon with Unicode code point U+2A3E) denotes left composition. This use of semicolon coincides with the notation for function composition used (mostly by computer scientists) in category theory, as well as the notation for dynamic conjunction within linguistic dynamic semantics. The semicolon notation (with this semantic) was introduced by Ernst Schröder in 1895.\n\nThe binary relations formula_2 are sometimes regarded as the morphisms formula_16 in a category Rel which has the sets as objects. In Rel, composition of morphisms is exactly composition of relations as defined above. The category Set of sets is a subcategory of Rel that has the same objects but fewer morphisms. A generalization of this is found in the theory of allegories.\n\n\n\nFinite homogeneous binary relations are represented by logical matrices. The entries of these matrices are either zero or one, depending on whether the relation represented is false or true for the row and column corresponding to compared objects. Working with such matrices involves the Boolean arithmetic with 1 + 1 = 1 and 1 × 1 = 1. An entry in the matrix product of two logical matrices will be 1, then, only if the row and column multiplied have a corresponding 1. Thus the logical matrix of a composition of relations can be found by computing the matrix product of the matrices representing the factors of the composition. \"Matrices constitute a method for \"computing\" the conclusions traditionally drawn by means of hypothetical syllogisms and sorites.\"\n\nConsider a heterogeneous relation \"R\" ⊆ \"A\" × \"B\". Then using composition of relation \"R\" with its converse \"R\", there are homogeneous relations \"R R\" (on \"A\") and \"R\" \"R\" (on \"B\").\n\nIf ∀\"x\" ∈ \"A\" ∃\"b\" ∈ B \"aRb\" (\"R\" is a total relation), then ∀\"x\" \"xRR\"\"x\" so that \"R R\" is a reflexive relation or I ⊆ \"R R\" where I is the identity relation {\"x\"I\"x\" : \"x\" ∈ \"A\"}. Similarly, if \"R\" is a surjective relation then \n\nThe composition formula_18 is used to distinguish relations of Ferrer's type, which satisfy formula_19\n\nLet \"A\" = { France, Germany, Italy, Switzerland } and \"B\" = { French, German, Italian } with the relation \"R\" given by \"aRb\" when \"b\" is a national language of \"a\". The logical matrix for \"R\" is given by\n\nFor a given set \"V\", the collection of all binary relations on \"V\" forms a Boolean lattice ordered by inclusion (⊆). Recall that complementation reverses inclusion:\nformula_23 In the calculus of relations it is common to represent the complement of a set by an overbar: formula_24\n\nIf \"S\" is a binary relation, let formula_25 represent the converse relation, also called the \"transpose\". Then the Schröder rules are\nVerbally, one equivalence can be obtained from another: select the first or second factor and transpose it; then complement the other two relations and permute them.\n\nThough this transformation of an inclusion of a composition of relations was detailed by Ernst Schröder, in fact Augustus De Morgan first articulated the transformation as Theorem K in 1860. He wrote \n\nWith Schröder rules and complementation one can solve for an unknown relation \"X\" in relation inclusions such as \nFor instance, by Schröder rule formula_29 and complementation gives formula_30 which is called the right residual of S by R .\n\nJust as composition of relations is a type of multiplication resulting in a product, so some compositions compare to division and produce quotients. Three quotients are exhibited here: left residual, right residual, and symmetric quotient. The left residual of two relations is defined presuming that they have the same domain (source), and the right residual presumes the same codomain (range, target). The symmetric quotient presumes two relations share a domain.\n\nDefinitions:\n\nUsing Schröder's rules, \"AX\" ⊆ \"B\" is equivalent to \"X\" ⊆ \"A\"\\\"B\". Thus the left residual is the greatest relation satisfying \"AX\" ⊆ \"B\". Similarly, the inclusion \"YC\" ⊆ \"D\" is equivalent to \"Y\" ⊆ \"D\"/\"C\", and the right residual is the greatest relation satisfying \"YC\" ⊆ \"D\".\n\nOther forms of composition of relations, which apply to general \"n\"-place relations instead of binary relations, are found in the \"join\" operation of relational algebra. The usual composition of two binary relations as defined here can be obtained by taking their join, leading to a ternary relation, followed by a projection that removes the middle component. For example, in the query language SQL there is the operation Join (SQL).\n\n\n"}
{"id": "49068860", "url": "https://en.wikipedia.org/wiki?curid=49068860", "title": "Conscientious Objectors Commemorative Stone", "text": "Conscientious Objectors Commemorative Stone\n\nThe Conscientious Objectors' Commemorative Stone is on the north side of Tavistock Square, Bloomsbury, in the London Borough of Camden.\n\nIn 1994 a stone commemorating \"men and women conscientious objectors all over the world and in every age\" by Hugh Court was unveiled in Tavistock Square.\n\nThe erection of this massive slate stone to commemorate the struggle of conscientious objectors past and present was co-ordinated by the Peace Pledge Union (PPU) and unveiled by Sir Michael Tippett, President of the PPU and a former Conscientious Objector, on 15 May 1994 in Tavistock Square. \n\nThe idea of having a stone dedicated to conscientious objectors (COs) to war began in 1976 at the funeral of a conscientious objector, Joseph Brett, who had been imprisoned in 1916. Bill McIlroy, then Secretary of the National Secular Society (NSS), officiated, and happened to remark that perhaps one day COs would be commemorated, as armed forces personnel are, both nationally and locally in the places where they lived.\n\nEdna Mathieson, niece of Joseph Brett, and a member of the Inner London Education Authority (sub group of the Greater London Council), approached the GLC for help, and in 1981 the GLC Labour Group accepted the idea and the Greater London Regional Labour Party agreed. But when, after the abolition of the GLC in 1986, the London Residuary Board took over the GLC’s commitments, agreement to a physical commemoration of COs was lost. Mathieson turned to Lambeth Council, and Lambeth initiated a competition in all London art schools for a suitable piece of sculpture, to be set up next to County Hall (GLC's headquarters), situated in Lambeth. Lambeth Council then found that, because of cuts, it could not afford to sponsor the arrangements for such a monument.\n\nFinally, Mathieson approached the PPU, who agreed to help with the whole concept, including design, location and raising finances. Camden Council (its local council) was asked for permission to erect a stone in Tavistock Square. On 15 May 1993, International Conscientious Objectors' Day, a public appeal was launched by means of a letter in \"The Guardian\", headed by Sir Michael Tippett.\n\nGranite had been considered as material, as it is the strongest stone – symbolic of those who refused against the odds to fight. However, when the PPU’s architectural adviser, Hugh Court, and sculptor, Paul Wehrle, went to Cumbria in search of a stone, they were so attracted to a naturally shaped piece of grey-green volcanic slate, some 400 million years old and rather larger than the size originally envisaged, they chose that.\n\nThe PPU decided upon the form of words for the stone with an addition of “Their foresight and courage give us hope”, written by a Dublin wordsmith.\n\nIn 1998 Mathieson pressed for the rest of Bill McIlroy's idea to be implemented - an annual event for the commemoration of all COs. A planning group was established at a Conway Hall meeting (in the Bertrand Russell Room) and Jess Hodgkins, a Unitarian, suggested the group be called the Right to Refuse to Kill Group (RRK). Ever since there has been an annual memorial event every year at noon on 15 May, International Conscientious Objectors' Day.\n\nAround the left, top, and right edges:\nIn the centre:\nAlong the bottom edge:\nAlso in Tavistock Square is a statue of Mahatma Gandhi, sculpted by Fredda Brilliant and installed in 1968 and a cherry tree planted in 1967 in memory of the victims of the nuclear bombing of Hiroshima and Nagasaki.\n\nWith the Conscientious Objectors Commemorative Stone, the three features have led to the square unofficially being regarded by some as a peace park or garden. Annual ceremonies are held at each of these memorials including International Conscientious Objectors Day, 15 May.\n\n\n"}
{"id": "14659441", "url": "https://en.wikipedia.org/wiki?curid=14659441", "title": "Covariation model", "text": "Covariation model\n\nKelley's covariation model (1967, 1971, 1972, 1973) is an attribution theory in which people make causal inferences to explain why other people and ourselves behave in a certain way. It is concerned with both social perception and self-perception (Kelley, 1973).\n\nThe covariation principle states that, \"an effect is attributed to the one of its possible causes with which, over time, it covaries\" (Kelley, 1973:108). That is, a certain behaviour is attributed to potential causes that appear at the same time. This principle is useful when the individual has the opportunity to observe the behaviour over several occasions. Causes of an outcome can be attributed to the person (internal), the stimulus (external), the circumstance, or some combination of these factors (Hewstone et al., 1973). Attributions are made based on three criteria: Consensus, Distinctiveness, and Consistency (Kelley, 1973).\n\nConsensus is the co-variation of behavior across different people. If lots of people find Lisa attractive, consensus is high. If only Arnab finds Lisa attractive, consensus is low. High consensus is attributed to the stimulus (in the above example, to Lisa), while low consensus is attributed to the person (in this case, Arnab).\n\nDistinctiveness refers to how unique the behavior is to the particular situation. There is a low distinctiveness if an individual behaves similarly in all situations, and there exists a high distinctiveness when the person only shows the behaviour in particular situations. If the distinctiveness is high, one will attribute this behaviour more to the circumstance instead of person (Gilovich et al., 2005).\n\nReferring to the example of Dr. Stanton's complimenting Barry's work, if Dr. Stanton almost never compliments other people's work, he shows high distinctiveness. But if he compliments everybody's work, this is low distinctiveness, and one will attribute the behaviour to the person, in this case, Dr. Stanton (Orvis et al., 1975).\n\nConsistency is the covariation of behavior across time. If Jane is generous all the time, she shows high consistency. If Jane is rarely generous or is generous only at specific times, perhaps around the holidays, she shows low consistency. High consistency is attributed to the person (Jane is a generous person), while low consistency is attributed to the circumstance (the holidays make people generous).\n\nAccording to Hewstone and Jaspars (1987), we are able to determine whether a person would likely make a personal (internal), stimulus (external) or circumstantial attribution by assessing the levels of consensus, distinctiveness, and consistency in a given situation: \n\nLow Consensus, Low Distinctiveness, High Consistency = Personal Attribution \nHigh Consensus, High Distinctiveness, High Consistency = Stimulus Attribution \nHigh Consensus, Low Distinctiveness, Low Consistency = Circumstance Attribution \n\nIn reference to McArthur's study (1972), let us consider the following example: \"John laughs at the comedian\" This outcome could be caused by something in the person (John), the stimulus (the comedian) the circumstances (the comedy club on that night), or some combination of these factors (Hewstone et al., 1987).\n\nIf John is the only person laughing at the comedian (low consensus), he laughs at the comedian at other comedy clubs (high consistency), and he laughs at other comedians (low distinctiveness), then the effect is seen as caused by something in the person (John).\n\nIf everyone is laughing at the comedian (high consensus), John laughs at the comedian at other comedy clubs (high consistency), and he does not laugh at other comedians (high distinctiveness), then the effect is seen as caused by something in the stimulus (the comedian).\n\nIf everyone is laughing at the comedian (high consensus), John doesn't laugh at the comedian at other comedy clubs (low consistency), and he laughs at other comedians at the club (low distinctiveness) then the effect is seen as caused by something in the circumstance (the comedy club on that night).\n\nA causal schema refers to the way a person thinks about plausible causes in relation to a given effect. It provides him or her with the means of making causal attributions when the information provided is limited. The three causal schemata recognized by Kelley are \"Multiple Sufficient Causes\", \"Multiple Necessary Causes\", and \"Causal Schema for Compensatory Causes\" (Kelley, 1973).\n\nMultiple Sufficient Causes: He or she may believe that either cause A or cause B suffices to produce a given effect (Kelley et al., 1980). For example, if an athlete fails a drug test (effect), we reason that he or she may be attempting to cheat (cause A) or may have been tricked into taking a banned substance (cause B). Either cause sufficiently attributes to the effect (McLeod, 2010).\n\nMultiple Necessary Causes: Both A and B are necessary to produce a given effect (Kelley et al., 1980). For example, if an athlete wins a marathon (effect), we reason that he or she must be very fit (cause A), and highly motivated (cause B) (McLeod, 2010).\n\nCausal Schema for Compensatory Causes: The effect occurs if either A or B is maximally present, or if both A and B are moderately present. For example, success (effect) depends on high ability (cause A) or low task difficulty (cause B). Success will occur if either cause is highly present or if both are moderately present (Kelley 1973).\n\nKelley's covariation model also has its limitations. The critique of the model mainly concerns the lack of distinction between intentional and unintentional behavior, and between reason and cause explanations (Malle, 1999).\n\nIntentional behavior occurs when there is a desire for an outcome, together with a belief that a certain behavior will lead to the desired outcome. These beliefs and desires are mental states acting as reasons behind an intention to act. When behavior is unintentional, the behavior is not explained by reasons, but rather by cause explanations not related to mental states of desire and belief. Malle (1999) found that whether behavior is intentional or unintentional predicts the type of explanation, and that the type of explanation presented predicts the judgement of intentionality.\n\nMalle (1999) also pointed at the differential effect of being an actor versus observer, the effect of the self-serving bias and the distinction between subjective and rational reasoning as important factors acting on attributions of behavior. This is not accounted for by the covariation model. Malle offers a new theoretical framework to give a broader and more comprehensive understanding of attributions of behavior.\n\n"}
{"id": "4505581", "url": "https://en.wikipedia.org/wiki?curid=4505581", "title": "Dot-probe paradigm", "text": "Dot-probe paradigm\n\nThe dot-probe paradigm is a test used by cognitive psychologists to assess selective attention.\n\nDeveloped by Halkiopoulos (1981), the method initially examined attentional biases to threatening auditory information, when threatening and non-threatening information was presented simultaneously to both ears in a dichotic listening task (). The method was then adapted to the visual modality (also known as the visual-probe task) by MacLeod, Mathews and Tata (1986). In many cases, the dot-probe paradigm is used to assess selective attention to threatening stimuli in individuals diagnosed with anxiety disorders. Biases have also been investigated in other disorders via this paradigm, including depression, post-traumatic stress disorder and chronic pain. Attention biases toward positive stimuli have been associated with a number of positive outcomes such as increased social engagement, increased prosocial behavior, decreased externalizing disorders, and decreased emotionally withdrawn behavior.\n\nDuring the dot-probe task, participants are situated in front of a computer screen with their chin securely placed on a chin rest. Participants are asked to stare at a fixation cross on the center of the screen. Two stimuli, one of which is neutral and one of which is threatening, appear randomly on either side of the screen. The stimuli are presented for a predetermined length of time (most commonly 500ms), before a dot is presented in the location of one former stimulus. Participants are instructed to indicate the location of this dot as quickly as possible, either via keyboard or response box.\n\nLatency is measured automatically by the computer. The fixation cross appears again for several seconds and then the cycle is repeated. Quicker reaction time to the dot when it occurs in the previous location of a threatening stimulus is often interpreted as vigilance to threat.\n\nResearchers have recently begun using a modified version of the dot-probe task to retrain the attentional bias. In this version, the probe replaces the neutral stimuli 100% of the time or the salient stimuli 100% of the time. Over the course of a number of trials the attentional bias for salient stimuli can be reduced (in the case of the 'replace-neutral' condition) or enhanced (in the case of the 'replace-salient' condition). This method of retraining the attentional bias is called attentional retraining.\n\n"}
{"id": "197383", "url": "https://en.wikipedia.org/wiki?curid=197383", "title": "Dzogchen", "text": "Dzogchen\n\nDzogchen () or \"Great Perfection\", Sanskrit: अतियोग, is a tradition of teachings in Tibetan Buddhism aimed at discovering and continuing in the natural primordial state of being. It is a central teaching of the Nyingma school of Tibetan Buddhism and of Bon. In these traditions, Dzogchen is the highest and most definitive path of the nine vehicles to liberation.\n\n\"Dzogchen\" is composed of two terms:\n\nThe term initially referred to the \"highest perfection\" of deity visualisation, after the visualisation has been dissolved and one rests in the natural state of the innately luminous and pure mind. In the 10th and 11th century, \"Dzogchen\" emerged as a separate tantric vehicle in the Nyingma tradition, used synonymously with the Sanskrit term \"ati yoga\" (primordial yoga).\n\nAccording to van Schaik, in the 8th-century tantra \"Sarvabuddhasamāyoga\" \nAccording to the 14th Dalai Lama, the term \"dzogchen\" may be a rendering of the Sanskrit term \"mahāsandhi\".\n\nAccording to Anyen Rinpoche, the true meaning is that the student must take the entire path as an interconnected entity of equal importance. Dzogchen is perfect because it is an all-inclusive totality that leads to middle way realization, in avoiding the two extremes of nihilism and eternalism. It classifies outer, inner and secret teachings, which are only separated by the cognitive construct of words and completely encompasses Tibetan Buddhist wisdom. It can be as easy as taking Bodhicitta as the method, and failing this is missing an essential element to accomplishment.\n\nAccording to the Nyingma tradition, the primordial Buddha Samantabhadra taught Dzogchen to the Buddha Vajrasattva, who transmitted it to the first human lineage holder, the Indian Garab Dorje (fl. 55 CE). According to tradition, the Dzogchen teachings were brought to Tibet by Padmasambhava in the late 8th and early 9th centuries. He was aided by two Indian masters, Vimalamitra and Vairocana. According to the Nyingma tradition, they transmitted the Dzogchen teachings in three distinct series, namely the Mind Series (\"sem-de\"), Space series (\"long-de\"), and Secret Instruction Series (\"men-ngak-de\"). According to tradition, these teachings were concealed shortly afterward, during the 9th century, when the Tibetan empire disintegrated. From the 10th century forward, innovations in the Nyingma tradition were largely introduced historically as revelations of these concealed scriptures, known as terma.\n\nIn the fourteenth century, Loden Nyingpo revealed a terma containing the story of Tonpa Shenrab Miwoche. According to this terma, Dzogchen originated with the founder of the Bon tradition, Tonpa Shenrab, who lived 18,000 years ago, ruling the kingdom of Tazik, which supposedly lay west of Tibet. He transmitted these teachings to the region of Zhang-zhung, the far western part of the Tibetan cultural world. The earliest Bon literature only exists in Tibetan manuscripts, the earliest of which can be dated to the 11th century. The Bon tradition also has a threefold classification, namely \"Dzogchen\", \"A-tri\", and the \"Zhang-zhung Aural Lineage (\"zhang-zhung nyen-gyu\").\n\nThe written history of Tibet begins in the early 7th century, when the Tibetan kingdoms were united, and Tibet expanded throughout large parts of Central Asia. Songtsen Gampo (reign ca.617-649/50) conquered the kingdom of Zhangzhung in western Tibet, dominated Nepal, and threatened the Chinese dominance in strategically important areas of the Silk Road. He is also credited with the adoption of a writing system, the establishment of a legal code, and the introduction of Buddhism, though it probably only played a minor role. Tri Songdetsen (742-ca.797) adopted Buddhism, but also maintained the martial traditions of the Tibetan empire. The Tibetans controlled Dunhuang, a major Buddhist center, from the 780s until the mid-ninth century. Halfway through the 9th century the Tibetan empire collapsed. Royal patronage of Buddhism was lost, leading to a decline of Buddhism in Tibet, only to recover with the renaissance of Tibetan culture occurring from the late 10th century to the early 12th century, known as the later dissemination of Buddhism.\n\nTraditionally, the early Dzogchen literature is categorized into three categories, which more or less reflect the historical development of Dzogchen:\n\nAccording to Sam van Schaik, who studies early Dzogchen manuscripts from the Dunhuang caves, the Dzogchen texts are influenced by earlier Mahayana sources such as the Laṅkāvatāra Sūtra and Indian Buddhist Tantras with their teaching of emptiness and luminosity, which in Dzogchen texts are presented as 'ever-purity' (\"ka-dag\") and 'spontaneous presence' (\"lhun-grub\").\n\nSam van Schaik also notes that there is a discrepancy between the histories as presented by the traditions, and the picture that emerges from those manuscripts.\n\nThere is no record of Dzogchen as a separate tradition or vehicle prior to the 10th century, although the terms \"atiyoga\" (as a higher practice than Tantra) and \"dzogchen\" do appear in 8th and 9th century Indian tantric texts. There is also no independent attestation of the existence of any separate traditions or lineages under the name of \"Dzogchen\" outside of Tibet, and it may be a unique Tibetan teaching, drawing on multiple influences, including both native Tibetan non-Buddhist beliefs and Chinese and Indian Buddhist teachings.\n\nAccording to van Schaik, the term \"atiyoga\" first appeared in the 8th century, in an Indian tantra called \"Sarvabuddhasamāyoga\". In this text, \"Anuyoga\" is the stage of yogic bliss, while \"Atiyoga\" is the stage of the realization of the \"nature of reality.\" According to van Schaik, this fits with the three stages of deity yoga as described in a work attributed to Padmasambhava: development (kye), perfection (dzog) and great perfection (dzogchen). \"Atiyoga\" here is not a vehicle, but a stage or aspect of yogic practice. In Tibetan sources, until the 10th century \"Atiyoga\" is characterized as a \"mode\" (\"tshul\") or a \"view\" (\"lta ba\"), which is to be applied within deity yoga.\n\nAccording to van Schaik, the concept of dzogchen, \"great perfection,\" first appeared as the culmination of the meditative practice of deity yoga around the 8th century. The term \"dzogchen\" was likely taken from the \"Guhyagarbhatantra\". This tantra describes, as other tantras, how in the creation stage one generates a visualisation of a deity and its mandala. This is followed by the completion stage, in which one dissolves the deity and the mandala into oneself, merging oneself with the deity. In the \"Guhyagarbhatantra\" and some other tantras, there follows a stage called \"rdzogs chen\", in which one rests in the natural state of the innately luminous and pure mind.\n\nIn the 9th and 10th centuries deity yoga was contextualized in Dzogchen in terms of nonconceptuality, nonduality and the spontaneous presence of the enlightened state. Some Dunhuang texts dated at the 10th century show the first signs of a developing nine vehicles system. Nevertheless, \"Anuyoga\" and \"Atiyoga\" are still regarded then as modes of \"Mahāyoga\" practice. Only in the 11th century came \"Atiyoga\" to be threatened as a separate vehicle, at least in the newly emerging Nyingma tradition. Nevertheless, even in the 13th century (and later) the idea of Atiyoga as a vehicle was controversial in other Buddhist schools. Van Schaik quotes Sakya Pandita as writing, in his \"Distinguishing the Three Vows\":\nMost of the early Dzogchen literature, which are claimed to be \"translations\", are original compositions from a much later date than the 8th century. According to Germano, the Dzogchen-tradition first appeared in the first half of the 9th century, with a series of short texts attributed to Indian saints. They were codified into a canon of eighteen texts which were referred to as \"mind oriented\" (\"sems phyogs\"), and later became known as \"mind series\" (sems de\"). \n\nThe mind series reflect the teachings of early Dzogchen, which rejected all forms of practice, and asserted that striving for liberation would simply create more delusion. One has simply to recognize the nature of one's own mind, which is naturally empty (\"stong pa\"), luminous (\"'od gsal ba\"), and pure. According to Germano, its characteristic language, which is marked by naturalism and negation, is already pronounced in some Indian tantras. Nevertheless, these texts are still inextricably bound up with tantric Mahayoga, with its visualisations of deities and mandals, and complex initiations.\n\nDuring the 9th and 10th centuries these texts, which represent the dominant form of the tradition in the 9th and 10th centuries, were gradually transformed into full-fledged tantras, culminating in the Kulayarāja Tantra (\"kun byed rgyal po\", \"The All-Creating King\"), in the last half of the 10th or the first half of the 11th century. According to Germano, this tantra was historically perhaps the most important and widely quoted of all Dzogchen scriptures.\n\nEarly Dzogchen was completely transformed in the 11th century, with the renaissance of Tibetan culture occurring from the late 10th century to the early 12th century,known as the later dissemination of Buddhism. New techniques and doctrines were introduced from India, resulting in new schools of Tibetan Buddhism, and radical new developments in Dzogchen doctrine and practice, with a growing emphasis on meditative practice. The older Bon and Nyingma traditions incorporated these new influences through the process of Treasure revelation. Especially the \"yogini\" tantras were influential, involving horrific imagery and violent rituals, erotic imagery, and sexual and somatic practices. These influences are reflected in the rise of subtle body representations and practices, new pantheons of wrathful and erotic Buddhas, increasingly antinomium rhetorics, and a focus on death-motifs.\n\nThese influences were incorporated in several movements such as the \"Secret Cycle\" (\"gsang skor\"), \"Ultra Pith\" (\"yang tig\"), \"Brahmin's tradition\" (\"bram ze'i lugs\"), the \"Space Class Series,\" and especially the \"Instruction Class series\", which culminated in the \"Seminal Heart\" (\"snying thig\"), which emerged in the late 11th and early 12th century.\n\nThe \"Seminal Heart\" belongs to the \"Instruction series.\" The main texts of the instruction series are the so-called seventeen tantras and the two \"seminal heart\" collections, namely the \"bi ma snying thig\" (\"Vima Nyingthig\", \"Seminal Heart of Vimalamitra\") and the \"mkha' 'gro snying thig\" (\"Khandro nyingthig\", \"Seminal Heart of the Dakini\"). The \"Seminal Heart of Vimalamitra\" is attributed to Vimalamitra, but was largely composed by their discoverers, in the 11th and 12th century. The \"Seminal Heart of the Dakini\" was produced by Tsultrim Dorje (\"Tshul khrims rdo rje\")(1291-1315/17).\n\nThe Seminal Heart teachings became the dominant Dzogchen-teachings, but was also criticized by conservative strands within the Nyingma-school. The most important Nyingma of the 12th century, Nyangrel Nyingma Özer (\"Nyang ral nyi ma 'od zer\", 1136-1204\n) developed his \"Crown Pith\" (\"spyi ti\") to reassert the older traditions in a new form. His writings, which were also presented as revelations, are marked by a relative absence of \"yogini\" tantra influence, and transcend the prescriptions of specific practices, as well as the rhetoric of violence, sexuality and transgression.\n\nA pivotal figure in the history of Dzogchen was Longchenpa Rabjampa (1308-1364, possibly 1369). He systematized the Seminal Heart teachings and other collections of texts that were circulating at the time in Tibet, in the Seven Treasuries (\"mdzod bdun\"), the \"Trilogy of Natural Freedom\" (\"rang grol skor gsum\"), and the Trilogy of Natural Ease (\"ngal gso skor gsum\"). Longchenpa refined the terminology and interpretations, and integrated the Seminal Heart teachings with broader Mahayana literature.\n\nMalcolm Smith notes that Longchenpa's \"Tshig don mdzod,\" the \"Treasury of Subjects,\" was preceded by several other texts by other authors dealing with the same topics. Smith mentions the 12th century text \"The Eleven Subjects of The Great Perfection\" by Nyi 'bum. This itself was derived from the eighth and final chapter of the commentary to The String of Pearls Tantra.\n\nNyi 'bum's \"Eleven Subjects\" is the basis for Longchenpa's \"Treasury of Subjects\" as well as Rigzin Godem's \"The Aural Lineage of Vimalamitra\" from the Gongpa Zangthal.\n\nAccording to Smith, Nyi 'bum's \"Eleven Subjects\" provided the outline upon which Longchenpa's \"Treasury of Subjects\" was based, using the general sequence of citations, and even copying or reworking entire passages. According to Smith, Nyi 'bum's \"Eleven Subjects\" was transmitted in a close circle of disciples, with very little outside contact, whereas Longchenpa's \"Treasury of Subjects\" contains responses to 14th century scholastic objections to Dzogchen.\n\nIn subsequent centuries more additions followed, including the \"Profound Dharma of Self-Liberation through the Intention of the Peaceful and Wrathful Ones\" (\"kar-gling zhi-khro\") by Karma Lingpa, (1326–1386), popularly known as \"Karma Lingpa's Peaceful and Wrathful Ones\", which includes the two texts of the \"bar-do thos-grol\", the \"Tibetan Book of the Dead\".\n\nOther important termas are \"The Penetrating Wisdom\" (\"dgongs pa zang thal\"), revealed by Rinzin Gödem (\"rig 'dzin rgod ldem\", 1337-1409); and \"The Nucleus of Ati's Profound Meaning\" (\"rDzogs pa chen po a ti zab don snying po\") by Terdak Lingpa (\"gter bdag gling pa\", 1646-1714).\n\nParticularly influential of these later revelations are the works of Jigme Lingpa (1730–1798). His Longchen Nyingthig (\"klong chen snying thig\"), \"The Heart-essence of the Vast Expanse\" or \"The Seminal Heart of the Great Matrix\", is a hidden teaching from Padmasambhava which was revealed by Jigme Lingpa. The Longchen Nyingthig is said to be the essence of the \"Vima Nyingthig\" and \"Khandro Nyingthig\", the \"Early Nyingthig,\", and has become known as the \"later Nyingthig\". It is one of the most widely practiced teachings in the Nyingmapa school. Patrul Rinpoche (1808–1887) wrote down Jigme Lingpa's pre-liminary practices into a book called \"The Words of My Perfect Teacher\".\n\nIn the early 20th century the first publications on Tibetan Buddhism appeared in the west. An early publication on Dzogchen was the so-called \"Tibetan Book of the Dead,\" edited by W.Y. Evans-Wentz, which became highly popular, but contains many mistakes in translation and interpretation. Dzogchen has been popularized in the western world by the Tibetan diaspora, starting with the exile of 1959. Well-known teachers include Sogyal Rinpoche and Namkhai Norbu. The 14th Dalai Lama is also a qualified Dzogchen teacher.\n\nChögyam Trungpa coined the term \"Maha Ati\" for Dzogchen, a master of the Kagyu and Nyingma lineages of Tibetan Vajrayana Buddhism. He generally preferred to introduce Sanskrit rather than Tibetan terms to his students and felt \"Maha Ati\" was the closest equivalent for Dzogchen, although he acknowledged it was an unorthodox choice. The coinage does not follow the sandhi rules which would be rendered as mahāti. This serves as an indication of its pedigree as a calque.\n\nDzogchen has also been taught and practiced in the Kagyu lineage, beginning with the Third Karmapa, Rangjung Dorje (1284–1339). \n\nThe Drikung Kagyu also have a tradition of Dzogchen teachings, the \"yangzab dzogchen\". \n\nLozang Gyatso, 5th Dalai Lama (1617–1682), Thubten Gyatso, 13th Dalai Lama ( 1876–1933), and Tenzin Gyatso, 14th Dalai Lama (present), all Gelugpas, are also noted Dzogchen masters, although their adoption of the practice of Dzogchen has been a source of controversy among more conservative members of the Gelug tradition.\n\nTibetan Buddhism developed five main schools. The Madhyamika philosophy obtained a central position in the Nyingma, Kagyu, Sakya and Gelugpa schools. The Jonang school, which until recently was thought to be extinct, developed a different interpretation of ultimate truth.\nDzogchen texts use unique terminology to describe the Dzogchen view. Some of these terms deal with the different elements and features of the mind. The generic term for consciousness is \"shes pa\", and includes the six sense consciousnesses. Different forms of shes pa include \"ye shes\" (Jñāna, 'pristine consciousness') and \"shes rab\" (prajñā, wisdom). According to Sam van Schaik, two significant terms used in Dzogchen literature is the Ground (\"gzhi\") and Gnosis (\"rig pa\"), which represent the \"ontological and gnoseological aspects of the nirvanic state\" respectively. Dzogchen literature also describes nirvana as the \"expanse\" (\"klong\" or \"dbyings\") or the \"true expanse\" (\"chos dbyings\", Sanskrit: Dharmadhatu). The term Dharmakaya is also often associated with these terms in Dzogchen, as explained by Tulku Urgyen:\nDharmakaya is like space. You cannot say there is any limit to space in any direction. No matter how far you go, you never reach a point where space stops and that is the end of space. Space is infinite in all directions; so is dharmakaya. Dharmakaya is all-pervasive and totally infinite, beyond any confines or limitations. This is so for the dharmakaya of all buddhas. There is no individual dharmakaya for each buddha, as there is no individual space for each country.\n\nAccording to Malcolm Smith, the Dzogchen view is also based on the Indian Buddhist Buddha-nature doctrine of the Tathāgatagarbha sūtras. According to the 14th Dalai Lama the Ground is the Buddha-nature, the nature of mind which is emptiness. According to Rinpoche Thrangu, Rangjung Dorje (1284–1339), the third Karmapa Lama (head of the Karma Kagyu) and Nyingma lineage holder, also stated that the Ground is Buddha-nature. According to Rinpoche Thrangu, \"whether one does Mahamudra or Dzogchen practice, buddha nature is the foundation from which both of these meditations develop.\"\n\nA key concept in Dzogchen is the 'basis', 'ground' or 'primordial state' (Tibetan: \"gzhi\", Sanskrit: \"sthana\"), also called the general ground (\"spyi gzhi\") or the original ground (\"gdod ma'i gzhi\"). The basis is the original state \"before realization produced buddhas and nonrealization produced sentient beings\". It is atemporal and unchanging and yet it is \"noetically potent\", giving rise to mind, delusion and wisdom. The basis is also associated with the term Dharmata.\n\nThe basis has three qualities:\n\nThe text, \"An Aspirational prayer for the Ground, Path and Result\" defines the three aspects of the basis thus:\n\nMoreover, the basis is associated with the primordial or original Buddhahood, also called Samantabhadra, which is said to be beyond time itself and hence Buddhahood is not something to be gained, but an act of recognizing what is already immanent in all sentient beings. Likewise, this view of the basis stems from the Indian Buddha-nature theory. Other terms used to describe the basis include unobstructed (\"ma 'gags pa\"), universal (\"kun khyab\") and omnipresent.\n\n\"Rigpa\" (Sk: \"Vidya\", \"knowledge\") is a central concept in Dzogchen which means \"unconfused knowledge of the basis that is its own state\". It is \"reflexively self-aware primordial wisdom,\" which is self-reflexively aware of itself as unbounded wholeness. The analogy given by Dzogchen masters is that one's true nature is like a mirror which reflects with complete openness, but is not affected by the reflections; or like a crystal ball that takes on the colour of the material on which it is placed without itself being changed. The knowledge that ensues from recognizing this mirror-like clarity (which cannot be found by searching nor identified) is called rigpa.\n\nAccording to Alexander Berzin, there are three aspects of rigpa:\n\nAs Berzin notes, all of the good qualities (\"yon-tan\") of a Buddha are already \"are innate (\"lhan-skyes\") to rigpa, which means that they arise simultaneously with each moment of rigpa, and primordial (\"gnyugs-ma\"), in the sense of having no beginning.\n\nSam van Schaik translates rigpa as \"gnosis\" which he glosses as \"a form of awareness aligned to the nirvanic state\". He notes that other definitions of rigpa include \"free from elaborations\" (\"srpos bral\"), \"non conceptual\" (\"rtog med\") and \"transcendent of the intellect\" (\"blo 'das\"). It is also often paired with emptiness, as in the contraction \"rig stong\" (gnosis-emptiness).\n\nJohn W. Pettit notes that \"rigpa\" is seen as beyond affirmation and negation, acceptance and rejection, and therefore it is known as \"natural\" (\"ma bcos pa\") and \"effortless\" (\"rtsol med\") once recognized. Because of this, Dzogchen is also known as the pinnacle and final destination of all paths.\n\n\"Ma Rigpa\" (avidyā) is the opposite of rigpa or knowledge. \"Ma rigpa\" is ignorance or unawareness, the failure to recognize the nature of the basis. An important theme in Dzogchen texts is explaining how ignorance arises from the basis or Dharmata, which is associated with \"ye shes\" or 'pristine consciousness'. Automatically arising unawareness (\"lhan-skyes ma-rigpa\") exists because the basis is seen having a natural cognitive potentiality and luminosity (\"gdangs\"), which is the ground for samsara and nirvana. When consciousness fails to recognize that all phenomena arise as the creativity (\"rtsal\") of the nature of mind and misses its own luminescence or does not \"recognize its own face\", sentient beings arise instead of Buddhas. As explained by Tulku Urgyen:\n\nIn the case of an ignorant sentient being the mind is called empty cognizance suffused with ignorance (marigpa). The mind of all the Buddhas is called empty cognizance suffused with awareness (rigpa).\n\nAccording to Vimalamitra's \"Illuminating Lamp\", delusion arises because sentient beings \"lapse towards external mentally apprehended objects\". This external grasping is then said to produce sentient beings out of dependent origination. This dualistic conceptualizing process which leads to samsara is termed manas as well as \"awareness moving away from the ground\".\n\nAccording to Sam van Schaik, there is a certain tension in Dzogchen thought (as in other forms of Buddhism) between the idea that samsara and nirvana are immanent within each other and yet are still different. In texts such as the Longchen Nyingtig for example, the basis and rigpa are presented as being \"intrinsically innate to the individual mind\". The \"Great Perfection Tantra of the Expanse of Samantabhadra’s Wisdom\" states:\n\nIf you think that he who is called “the heart essence of all buddhas, the Primordial Lord, the noble Victorious One, Samantabhadra” is contained in a mindstream separate from the ocean-like realm of sentient beings, then this is a nihilistic view in which samsara and nirvana remain unconnected.\n\nLikewise, Longchenpa (12th century), writes in his \"Illuminating Sunlight\":\n\nEvery type of experiential content belonging to samsara and nirvana has, as its very basis, a natural state that is a spontaneously present buddha—a dimension of purity and perfection, that is perfect by nature. This natural state is not created by a profound buddha nor by a clever sentient being. Independent of causality, causes did not produce it and conditions can not make it perish. This state is one of self-existing wakefulness, defying all that words can describe, in a way that also transcends the reach of the intellect and thoughts. It is within the nonarising vastness of such a basic natural state that all phenomena belonging to samsara and nirvana are, essentially and without any exception, a state of buddha—purity and perfection.\n\nThis lack of difference between these two states, their non-dual (\"advaya\") nature, corresponds with the idea that change from one to another doesn't happen due to an ordinary process of causation but is an instantaneous and perfect 'self-recognition' (\"rang ngo sprod\") of what is already innately (\"lhan-skyes\") there. According to John W. Pettit, this idea has its roots in Indian texts such as Nagarjuna's Mulamadhyamakakarika, which states that samsara and nirvana are not separate and that there is no difference between the \"doer\", the \"going\" and the \"going to\" (i.e. the ground, path and fruit).\n\nIn spite of this emphasis on immanence, Dzogchen texts do indicate a subtle difference between terms associated with delusion (\"kun gzhi\" or \"alaya\", \"sems\" or mind) and terms associated with full enlightenment (\"dharmakaya\" and \"rigpa\"). The Alaya and Ālayavijñāna are associated with karmic imprints (\"vasana\") of the mind and with mental afflictions (\"klesa\"). The \"alaya for habits\" is the basis (\"gzhi\") along with ignorance (\"marigpa\") which includes all sorts of obscuring habits and grasping tendencies.\n\nThese terms stem from Indian Yogacara texts, such as the Ratnagotravibhāga.\n\nKoppl notes that although later Nyingma authors such as Mipham attempted to harmonize the view of Dzogchen with Madhyamaka, the earlier Nyingma author Rongzom Chokyi Zangpo did not. Rongzom held that the views of sutra such as Madhyamaka were inferior to that of tantra. In contrast, the 14th Dalai Lama, in his book Dzogchen, concludes that Madhyamaka and Dzogchen come down to the same point. The view of reality obtained through Madhyamaka philosophy and the Dzogchen view of Rigpa can be regarded as identical. With regard to the practice in these traditions, however, at the initial stages there do seem certain differences in practice and emphasis.\n\nDzogchen is a secret teaching emphasizing the rigpa view. It is a secret from those who are incapable of receiving it. The student can properly receive it with direct in-person realization under a guru's instruction. It is accessible to all; however, it is generally considered an advanced practice because safety from generating an incorrect view necessitates preliminary practices with a teacher's empowerment. \n\nDzogchen teachings emphasize naturalness, spontaneity and simplicity. Although Dzogchen is portrayed as being distinct from tantra, it has incorporated many concepts and practices from tantric Buddhism. It embraces a widely varied array of traditions, that range from a systematic rejection of all tantric practices, to a full incorporation of tantric practices.\n\nThe \"Seminal Heart of Vimalamitra\" epitomized the Dzogchen teaching in three principles, known as the Three Statements of Garab Dorje (Tsik Sum Né Dek). They give in short the development a student has to undergo:\n\nIn subsequent centuries these teachings were expanded, most notably in the Longchen Nyingthig by Jigme Lingpa (1730-1798). His systematisation is the most widely used Dzogchen-teaching nowadays.\n\nThe dzogchen teachings consist of vast anthologies of practices presented as preliminary and auxiliary contemplative techniques, including standard Buddhist meditation techniques and tantra practices which have been integrated into Dzogchen.\n\nLongchenpa, in \"Finding Comfort and Ease in Meditation\" (\"bsam gtan ngal gso\"), the second text of the Trilogy of Natural Ease (\"ngal gso skor gsum\"), and its auto-commentary the \"Shing rta rnam dag\", uses the standard triad of meditative experiences (\"nyams\") to structure the text and the practices: bliss (\"bde ba\"), radiance/clarity (\"gsal ba\"), and non-conceptuality (\"mi rtog pa\"). This triad is also presented as preliminaries, main practice, and concluding phase. The preliminaries are further divided into:\n\nThis systematisation contextualized the system in terms of Tibetan Buddhism, while simultaneously relegating these preliminaries to a lower status, while emphasizing their necessity. Longchenpa couples meditation with Guru yoga in these preliminaries.\n\nThe teachings based on the \"Longchen Nyingthig\" are divided into preliminary practices and main practices. Alexander Berzin explicitly mentions meditative practices as a preliminary of the main practice.\n\nA general overview gives the following:\n\nThe \"Ngondro\", preliminary practices, consist of outer preliminaries and inner preliminaries.\n\nAccording to Tsoknyi Rinpoche, before one starts with the Dzogchen-practices empowerment is necessary. This plants the \"seeds of realization\" within the present body, speech and mind. Empowerment \"invests us with the ability to be liberated into the already present ground.\" The practices bring the seeds to maturation, resulting in the qualities of enlightened body, speech and mind.\n\nThe outer preliminaries are as follows:\n\nThe inner preliminaries are as follows: \n\nAccording to Berzin, receiving empowerment (\"dbang\", initiation) and keeping the vows conferred at that time is a necessary step to move on to the main practice. This activates our Buddha-mind, by consciously generating a state of mind that is accompanied by understanding. Alexander Berzin further notes:\n\nWith the influence of tantra, and the systematisations of Longchenpa, the main Dzogchen practices came to be preceded by preliminary (meditative) practices.\n\nIn the text \"Finding Comfort and Ease in the Nature of Mind\" (\"sems nyid ngal gso\"), which is part of the Trilogy of Natural Ease (\"ngal gso skor gsum\"), Longchenpa arranges 141 contemplative practices, split into three sections: exoteric Buddhism (92), tantra (92), and the Great Perfection (27). Most of these practices are \"technique-free.\" The typical Buddhist meditations are relegated to the preliminary phase, while the main meditative practices are typical \"direct\" approaches.\n\nLongchenpa includes the perfection phase techniques of channels, winds and nuclei into the main and concluding phases. The \"concluding phase\" includes discussions of new contemplative techniques, which aid the practice of the main phase.\n\nThe Great Perfection practices as described by Jigme Lingpa consist of preliminary practices, specific for the Great Perfection practice, and the main practice.\n\nJigme Lingpa mentions two kinds of preliminary practices, \"'khor 'das ru shan dbye ba\", \"making a gap between samsara and nirvana,\" and \"sbyong ba\".\n\n\"Ru shan\" is a series of visualisation and recitation exercises, derived from the Seminal Heart tradition. The name reflects the dualism of the distinctions between mind and insight, ālaya and dharmakāya. Longchenpa places this practice in the \"enhancement\" (\"bogs dbyung\") section of his concluding phase. It describes a practice \"involving going to a solitary spot and acting out whatever comes to your mind.\"\n\n\"Sbyong ba\" is a variety of teachings for training (\"sbyong ba\") the body, speech and mind. The training of the body entails instructions for physical posture. The training of speech mainly entails recitation, especially of the syllable \"hūm\". The training of the mind is a Madhyamaka-like analysis of the concept of the mind, to make clear that mind cannot arise from anywhere, reside anywhere,or go anywhere. They are in effect an establishment of emptiness by means of the intellect.\n\nAccording to Alexander Berzin, after the preliminary practices follow meditative practices, in which the practitioners works with the three aspects of rigpa.\n\nThe three samadhis (ting-nge-’dzin gsum) are practiced, in which the practitioners works, in the imagination, with the three aspects of rigpa:\n\nThe Dzogchen meditation practices also include a series of exercises known as \"Semdzin\" (\"sems dzin\"), which literally means \"to hold the mind\" or \"to fix mind.\" They include a whole range of methods, including fixation, breathing, and different body postures, all aiming to bring one into the state of contemplation.\n\nThe practice of Trekchö (\"khregs chod\"), \"cutting through solidity\", reflects the earliest developments of Dzogchen, with its admonition against practice. In this practice one first identifies, and then sustains recognition of, one's own innately pure, empty awareness. Students receive pointing-out instruction (\"sems khrid\", \"ngos sprod\") in which a teacher introduces the student to the nature of his or her mind. According to Tsoknyi Rinpoche, these instructions are received after the preliminary practices, though there's also a tradition to give them before the preliminary practices.\n\nJigme Lingpa divides the trekchö practice into ordinary and extraordinary instructions. The ordinary section comprises the rejection of the \"all is mind – mind is empty\" approach, which is a conceptual establishment of emptiness. Jigme Lingpa's extraordinary instructions give the instructions on the breakthrough proper, which consist of the setting out of the view (\"lta ba\"), the doubts and errors that may occur in practice, and some general instructions thematized as \"the four ways of being at leisure\" (\"cog bzhag\"). The \"setting out of the view\" tries to point the reader toward a direct recognition of rigpa, insisting upon the immanence of rigpa, and dismissive of meditation and effort.). Insight leads to \"nyamshag\", \"being present in the state of clarity and emptiness\".\n\n\"Tögal\" (\"thod rgal\") means \"spontaneous presence\", \"direct crossing\", \"direct crossing of spontaneous presence\", or \"direct transcendence. The literal meaning is \"to proceed directly to the goal without having to go through intermediate steps.\"\n\n\"Tögal\" is also called \"the practice of vision\", or \"the practice of the Clear Light (\"od-gsal\")\". It entails progressing through the Four Visions. The practices engage the subtle body of psychic channels, winds and drops (\"rtsa rlung thig le\"). The practices aim at generating a spontaneous flow of luminous, rainbow-colored images that gradually expand in extent and complexity.\n\n\"Tögal\" is an innovative practice, and reflects the innovations of the Manngede cycles in Dzogchen, and the incorporation of complex tantric techniques and doctrines. They are an adaptation of Tantric \"perfection phase\" techniques (\"rdzogs rim\"), as outlined in the early-eleventh-century Indian Tantric \"Kalachakra\" cycle, \"The Wheel of Time\", which was probably a direct inspiration for the Seminal Heart.\n\nLhun grub practice may lead to full enlightenment and the self-liberation of the human body into a rainbow body at the moment of death, when all the fixation and grasping has been exhausted. It is a nonmaterial body of light with the ability to exist and abide wherever and whenever as pointed by one's compassion. It is a manifestation of the Sambhogakāya.\n\nSome exceptional practitioners such as Padmasambhava and Vimalamitra are held to have realized a higher type of rainbow body without dying. Having completed the four visions before death, the individual focuses on the lights that surround the fingers. His or her physical body self-liberates into a nonmaterial body of light (a Sambhogakāya) with the ability to exist and abide wherever and whenever as pointed by one's compassion.\n\n\n\n\n\n\n\n\n"}
{"id": "33439201", "url": "https://en.wikipedia.org/wiki?curid=33439201", "title": "Electoral Calculus", "text": "Electoral Calculus\n\nElectoral Calculus is a political forecasting web site which attempts to predict future United Kingdom general election results. It considers national factors but excludes local issues.\n\nThe site was developed by Martin Baxter, who is a financial analyst specialising in mathematical modelling.\n\nThe site includes maps, predictions and analysis articles. It has a separate section for elections in Scotland.\n\nThe site is based around the employment of scientific techniques on data about Britain's electoral geography, which can be used to calculate the uniform national swing. It takes account of national polls and trends but excludes local issues.\n\nThe calculations were initially based on what is termed the \"Transition Model\", which is derived from the additive uniform national swing model. This uses national swings in a proportional manner to predict local effects. The \"Strong Transition Model\" was introduced in October 2007, and considers the effects of strong and weak supporters. The models are explained in detail on the web site.\n\nIt was listed by \"The Guardian\" in 2004 as one of the \"100 most useful websites\", being \"the best\" for predictions. In 2012 it was described by PhD student Chris Prosser at the University of Oxford as \"probably the leading vote/seat predictor on the internet\". Its detailed predictions for individual seats have been noted by Paul Evans on the localdemocracy.org.uk blog. Academic Nick Anstead noted in his observations from a 2010 \"Personal Democracy Forum\" event, that Mick Fealty of Slugger O'Toole considered Electoral Calculus to be \"massively improved\" in comparison with the swingometer.\n\nWith reference to the 2010 United Kingdom general election, it was cited by journalists Andrew Rawnsley and Michael White in \"The Guardian\". John Rentoul in \"The Independent\" referred to the site after the election.\n\n"}
{"id": "37081517", "url": "https://en.wikipedia.org/wiki?curid=37081517", "title": "Energiewende in Germany", "text": "Energiewende in Germany\n\nThe Energiewende (German for energy transition) is the planned transition by Germany to a low carbon, environmentally sound, reliable, and affordable energy supply. \nThe term \"Energiewende\" is regularly used in English language publications without being translated (a loanword).\nThe new system will rely heavily on renewable energy (particularly wind, photovoltaics, and hydroelectricity), energy efficiency, and energy demand management. \nMost if not all existing coal-fired generation will need to be retired. \nThe phase-out of Germany's fleet of nuclear reactors, to be complete by 2022, is a key part of the program.\n\nLegislative support for the \"Energiewende\" was passed in late 2010 and includes greenhouse gas (GHG) reductions of 80–95% by 2050 (relative to 1990) and a renewable energy target of 60% by 2050. \nThese targets are ambitious.\nThe Berlin-based policy institute Agora Energiewende noted that \"while the German approach is not unique worldwide, the speed and scope of the \"Energiewende\" are exceptional\".\nThe \"Energiewende\" also seeks a greater transparency in relation to national energy policy formation.\n\nGermany has made significant progress on its GHG emissions reduction target, achieving a 27% decrease between 1990 and 2014. \nHowever the country will need to maintain an average GHG emissions abatement rate of 3.5% per year to reach its \"Energiewende\" goal, equal to the maximum historical value thus far.\n\nAs of 2013, Germany spends €1.5billion per year on energy research in an effort to solve the technical and social issues raised by the transition.\nThis includes a number of computer studies that have confirmed the feasibility and a similar cost (relative to business-as-usual and given that carbon is adequately priced) of the \"Energiewende\".\n\nThe term \"Energiewende\" was first contained in the title of a 1980 publication by the German Öko-Institut, calling for the complete abandonment of nuclear and petroleum energy.\nThe most groundbreaking claim was that economic growth was possible without increased energy consumption. On 16February 1980, the German Federal Ministry of the Environment also hosted a symposium in Berlin, called \"Energiewende – Atomausstieg und Klimaschutz\" (Energy Transition: Nuclear Phase-Out and Climate Protection). The Institute for Applied Ecology was funded by both environmental and religious organizations, and the importance of religious and conservative figures like Wolf von Fabeck and Peter Ahmels was crucial. In the following decades, the term \"Energiewende\" expanded in scope – in its present form it dates back to at least 2002.\n\n\"Energiewende\" designates a significant change in energy policy. The term encompasses a reorientation of policy from demand to supply and a shift from centralized to distributed generation (for example, producing heat and power in small cogeneration units), which should replace overproduction and avoidable energy consumption with energy-saving measures and increased efficiency.\n\nIn a broader sense, this transition also entails a democratization of energy.\nIn the traditional energy industry, a few large companies with large centralized power stations dominate the market as an oligopoly and consequently amass a worrisome level of both economic and political power. Renewable energies, in contrast, can, as a rule, be established in a decentralized manner. Public wind farms and solar parks can involve many citizens directly in energy production.\nPhotovoltaic systems can even be set up by individuals. Municipal utilities can also benefit citizens financially, while the conventional energy industry profits a relatively small number of shareholders. Also significant, the decentralized structure of renewable energies enables creation of value locally and minimizes capital outflows from a region. Renewable energy sources therefore play an increasingly important role in municipal energy policy, and local governments often promote them.\n\nThe key policy document outlining the \"Energiewende\" was published by the German government in September 2010, some six months before the Fukushima nuclear accident. Legislative support was passed in September 2010. On 6 June 2011, following Fukushima, the government removed the use of nuclear power as a bridging technology as part of their policy.\nAfter the 2013 federal elections, the new CDU/CSU and SPD coalition government continued the \"Energiewende\", with only minor modification of its goals in the coalition agreement. An intermediate target was introduced of a 55–60% share of renewable energy in gross electricity consumption in 2035. Germany imports more than half of its energy. Important aspects include ():\n\nIn addition, there will be an associated research and development drive. A chart showing German energy legislation in 2016 is available.\n\nThese targets go well beyond European Union legislation and the national policies of other European states. The policy objectives have been embraced by the German federal government and has resulted in a huge expansion of renewables, particularly wind power. Germany's share of renewables has increased from around 5% in 1999 to 22.9% in 2012, surpassing the OECD average of 18% usage of renewables.\nProducers have been guaranteed a fixed feed-in tariff for 20 years, guaranteeing a fixed income. Energy co-operatives have been created, and efforts were made to decentralize control and profits. The large energy companies have a disproportionately small share of the renewables market. However, in some cases poor investment designs have caused bankruptcies and low returns, and unrealistic promises have been shown to be far from reality.\nNuclear power plants were closed, and the existing nine plants will close earlier than planned, in 2022.\n\nOne factor that has inhibited efficient employment of new renewable energy has been the lack of an accompanying investment in power infrastructure to bring the power to market. It is believed 8,300 km of power lines must be built or upgraded. The different German States have varying attitudes to the construction of new power lines. Industry has had their rates frozen and so the increased costs of the \"Energiewende\" have been passed on to consumers, who have had rising electricity bills. Germans in 2013 had some of the highest electricity prices (including taxes) in Europe. In comparison, its neighbors (Poland, Sweden, Denmark and nuclear-reliant France) have some of the lowest costs (excluding taxes) in the EU.\n\nAccording to a 2014 survey conducted by TNS Emnid for the German Renewable Energies Agency among 1015 respondents, 94 per cent of the Germans support the enforced expansion of Renewable Energies. More than two-thirds of the interviewees agree to renewable power plants close to their homes.\nThe share of total final energy from renewables was 11% in 2014.\n\nOn 1 August 2014, a revised Renewable Energy Sources Act entered into force. Specific deployment corridors now stipulate the extent to which renewable energy is to be expanded in the future and the funding rates (feed-in tariffs) will no longer be fixed by the government, but will be determined by auction.\n\nMarket redesign is a key part of the \"Energiewende\". The German electricity market needs to be reworked to suit.\nAmong other things, wind and PV cannot be principally refinanced under the current marginal cost based market. Carbon pricing is also central to the \"Energiewende\" and the European Union Emissions Trading Scheme (EU ETS) needs to be reformed to create a genuine scarcity of certificates.\nThe German federal government is calling for such reform.\nMost of the computer scenarios used to analyse the \"Energiewende\" rely on a substantial carbon price to drive the transition to low-carbon technologies.\n\nCoal-fired generation needs to be retired as part of the \"Energiewende\". Some argue for an explicit negotiated phase-out of coal plants, along the lines of the well-publicized nuclear phase-out.\nCoal comprised 42% of electricity generation in 2015. If Germany is to limit its contribution to a global temperature increase to 1.5°C above pre-industrial levels, as declared in the 2015 Paris Agreement, a complete phase-out of fossil fuels together with a shift to 100% renewable energy is required by about 2040.\n\nThe \"Energiewende\" is made up of various technical building blocks. Electricity storage, while too expensive at present, may become a useful technology in the future.\nEnergy efficiency has a key but currently under-recognised role to play.\nImproved energy efficiency is one of Germany's official targets. Greater integration with adjoining national electricity networks can offer mutual benefits — indeed, systems with high shares of renewables can utilize geographical diversity to offset intermittency.\n\nGermany invested €1.5billion in energy research in 2013.\nOf that the German federal government spent €820million supporting projects ranging from basic research to applications. The federal government also foresees an export role for German expertise in the area.\n\nThe social and political dimensions of the \"Energiewende\" have been subject to study. Strunz argues that the underlying technological, political and economic structures will need to change radically — a process he calls regime shift.\nSchmid, Knopf, and Pechan analyse the actors and institutions that will be decisive in the \"Energiewende\" and how latency in the national electricity infrastructure may restrict progress.\n\nOn 3 December 2014, the German federal government released its National Action Plan on Energy Efficiency (NAPE) in order to improve the uptake of energy efficiency.\nThe areas covered are the energy efficiency of buildings, energy conservation for companies, consumer energy efficiency, and transport energy efficiency. German industry is expected to make a sizeable contribution.\n\nAn official federal government report on progress under the \"Energiewende\", updated for 2014, notes that:\n\n\nA commentary on the progress report expands on many of the issues raised.\n\nSlow progress on transmission network reinforcement has led to a deferment of new windfarms in northern Germany. The German cabinet earlier approved costly underground cabling in October 2015 in a bid to dispel local resistance against above-ground pylons and to speed up the expansion process.\n\nAnalysis by Agora Energiewende in late-2016 suggests that Germany will probably miss several of its key \"Energiewende\" targets, despite recent reforms to the Renewable Energy Sources Act and the wholesale electricity market. The goal to cut emissions by 40% by 2020 \"will most likely be missed... if no further measures are taken\" and the 55–60% share of renewable energy in gross electricity consumption by 2035 is \"unachievable\" with the current plans for renewables expansion. In November 2016, Agora Energiewende reported on the impact of the new and several other related new laws. It concludes that this new legislation will bring \"fundamental changes\" for large sections of the energy industry, but have limited effect on the economy and on consumers.\n\nThe 2016 Climate Action Plan for Germany, adopted on 14November 2016, introduced sector targets for GHG emissions. The goal for the energy sector is shown in the table. The plan states that the energy supply must be \"almost completely decarbonised\" by 2050, with renewables as its main source. For the electricity sector, \"in the long-term, electricity generation must be based almost entirely on renewable energies\" and \"the share of wind and solar power in total electricity production will rise significantly\". Notwithstanding, during the transition, \"less carbon-intensive natural gas power plants and the existing most modern coal power plants play an important role as interim technologies\".\n\nThe fifth monitoring report on the \"Energiewende\" for 2015 was published in December 2016. The expert commission which wrote the report warns that Germany will probably miss its 2020 climate targets and believes that this could threaten the credibility of the entire endeavor. The commission puts forward a number of measures to address the slowdown, including a flat national price imposed across all sectors, a greater focus on transport, and full market exposure for renewable generation. Regarding the carbon price, the commission thinks that a reformed EUETS would be better, but that achieving agreement across Europe is unlikely.\n\n, citizen support for the \"Energiewende\" remains high, with recent surveys indicating that about 80–90% of the public are in favor. \nOne reason for the high acceptance is the substantial participation of German citizens in the \"Energiewende\", as private households, land owners, or members of energy cooperatives (\"Genossenschaft\"). \nA 2016 survey showed that roughly one in two Germans would consider investing in community renewable energy projects.\nManfred Fischedick, Director of the Wuppertal Institute for Climate, Environment and Energy has commented that \"if people participate with their own money, for example in a wind or solar power plant in their area, they will also support [the \"Energiewende\"].\" \nA 2010 study shows the benefits to municipalities of community ownership of renewable generation in their locality.\nEstimates for 2012 suggested that almost half the renewable energy capacity in Germany was owned by citizens through energy cooperatives and private initiatives.\nMore specifically, citizens accounted for nearly half of all installed biogas and solar capacity and half of the installed onshore wind capacity. However, changes in energy policy, starting with the Renewable Energy Sources Act in 2014, have jeopardized the efforts of citizens to participate. The share of citizen-owned renewable energy has since dropped to 42.5% as of 2016.\n\nMuch of the policy development for the \"Energiewende\" is underpinned by computer models, run mostly by universities and research institutes. The models are usually based on scenario analysis and are used to investigate different assumptions regarding the stability, sustainability, cost, efficiency, and public acceptability of various sets of technologies. Some models cover the entire energy sector, while others are confined to electricity generation and consumption. A 2016 book investigates the usefulness and limitations of energy scenarios and energy models within the context of the \"Energiewende\".\n\nA number of computer studies confirm the feasibility of the German electricity system being 100% renewable in 2050. Some investigate the prospect of the entire energy system (all energy carriers) being fully renewable too.\n\nIn 2009 WWF Germany published a quantitative study prepared by the Öko-Institut, Prognos, and Hans-Joachim Ziesing. The study presumes a 95% reduction in greenhouse gases by the year 2050 and covers all sectors. The study shows that the transformation from a high-carbon to a low-carbon economy is possible and affordable. It notes that by committing to this transformation path, Germany could become a model for other countries.\n\nA 2011 report from the (SRU) concludes that Germany can attain 100% renewable electricity generation by 2050. The German Aerospace Center (DLR) REMix high-resolution energy model was used for the analysis. A range of scenarios were investigated and a cost-competitive transition with good security of supply is possible.\n\nThe authors presume that the transmission network will continue to be reinforced and that cooperation with Norway and Sweden would allow their hydro generation to be utilized for storage. The transition does not require Germany's nuclear phase-out (\"\") to be extended nor the construction of coal-fired plants with carbon capture and storage (CCS). Conventional generation assets need not be stranded and an orderly transition should prevail. Stringent energy efficiency and energy saving programs can bring down the future costs of electricity.\n\nThe Deep Decarbonization Pathways Project (DDPP) aims to demonstrate how countries can transform their energy systems by 2050 in order to achieve a low-carbon economy.\nThe 2015 German country report, produced in association with the Wuppertal Institute, examines the official target of reducing domestic GHG emissions by 80% to 95% by 2050 (compared with 1990). Decarbonization pathways for Germany are illustrated by means of three ambitious scenarios with energy-related emission reductions between 1990 and 2050 varying between 80% and more than 90%. Three strategies strongly contribute to GHG emission reduction: \nIn addition, some scenarios use controversially:\nPotential co-benefits for Germany include increased energy security, higher competitiveness of and global business opportunities for companies, job creation, stronger GDP growth, smaller energy bills for households, and less air pollution.\n\nUsing the model REMod-D (Renewable Energy Model – Germany), this 2015 Fraunhofer ISE study investigates several system transformation scenarios and their related costs. The guiding question of the study is: how can a cost-optimised transformation of the German energy system — with consideration of all energy carriers and consumer sectors — be achieved while meeting the declared climate protection targets and ensuring a secure energy supply at all times. Carbon capture and storage (CCS) is explicitly excluded from the scenarios. A future energy scenario emitting 85% less CO emissions than 1990 levels is compared with a reference scenario, which assumes that the German energy system operates in 2050 the same way as it does today. Under this comparison, primary energy supply drops 42%. The total cumulative costs depend on the future prices for carbon and oil. If the penalty for CO emissions increases to €100/tonne by 2030 and thereafter remains constant and fossil fuel prices increase annually by 2%, then the total cumulative costs of today's energy system are 8% higher than the costs required for the minus 85% scenario up to 2050. The report also notes:\n\nA 2015 study uses DIETER or Dispatch and Investment Evaluation Tool with Endogenous Renewables, developed by the German Institute for Economic Research (DIW), Berlin, Germany. The study examines the power storage requirements for renewables uptake ranging from 60% to 100%. Under the baseline scenario of 80% (the German government target for 2050), grid storage requirements remain moderate and other options on both the supply side and demand side offer flexibility at low cost. Nonetheless storage plays an important role in the provision of reserves. Storage becomes more pronounced under higher shares of renewables, but strongly depends on the costs and availability of other flexibility options, particularly on biomass availability. The model is fully described in the study report.\n\nA 2016 acatech-lead study focused on so-called flexibility technologies used to balance the fluctuations inherent in power generation from wind and photovoltaics. Set in 2050, several scenarios use gas power plants to stabilise the backbone of energy system, ensuring supply security during several weeks of low wind and solar radiation. Other scenarios investigate a 100% renewable system and show these to be possible but more costly. Flexible consumption and storage control (demand-side management) in households and the industrial sector is the most cost-efficient means of balancing short-term power fluctuations. Long-term storage systems, based on power-to-X, are only viable if carbon emissions are to be reduced by more than 80%. On the question of costs, the study notes:\n\nThe Atmosphere/Energy Program at Stanford University has developed roadmaps for 139 countries to achieve energy systems powered only by wind, water, and sunlight (WWS) by 2050. In the case of Germany, total end-use energy drops from 375.8 GW for business-as-usual to 260.9 GW under a fully renewable transition. Load shares in 2050 would be: on-shore wind 35%, off-shore wind 17%, wave 0.08%, geothermal 0.01%, hydro-electric 0.87%, tidal 0%, residential PV 6.75%, commercial PV 6.48%, utility PV 33.8%, and concentrating solar power 0%. The study also assess avoided air pollution, eliminated global climate change costs, and net job creation. These co-benefits are substantial.\n\nUsing biomass as a fuel produces air pollution in the form of carbon monoxide, carbon dioxide, NOx (nitrogen oxides), VOCs (volatile organic compounds), particulates and other pollutants. However, since the beginning of the Energiewende the balance of biomass vs other forms of renewable energy has shifted; biomass made up 7.0% of Germany's power generation mix in 2017. Biomass has the potential to be a carbon-neutral fuel because growing biomass absorbs carbon dioxide from the atmosphere and a portion of the carbon absorbed remains in the ground after harvest. Biomass contains less sulfur than coal and so produces much less sulfur dioxide than coal. \n\nAfter introduction of the original Renewable Energy Sources Act in 2000 there was a focus on long term costs, while in later years this has shifted to a focus on short term costs and the \"financial burden\" of the \"Energiewende\" while ignoring environmental externalities of fossil fuels.\nNonetheless, for the first time in more than ten years, electricity prices for household customers fell at the beginning of 2015.\n\nThe renewable energy levy to finance green power investment is added to Germans' electricity unit price. The surcharge (22.1% in 2016 ) pays the state-guaranteed price for renewable energy to producers and is 6.35 cents per kWh in 2016.\n\nA comprehensive study, published in \"Energy Policy\" in 2013, said that the phase-out of Germany's fleet of nuclear reactors, to be complete by 2022, is contradictory to the goal of the climate portion of the program. The Intergovernmental Panel on Climate Change (IPCC) recognizes nuclear as one of the lowest lifecycle emissions energy sources available, lower than even solar, and only bested (slightly) by wind. The US National Renewable Energy Lab (NREL) also cites nuclear as a very low lifecycle emissions source \n\nGerman Economy and Energy Minister Sigmar Gabriel admitted \"For a country like Germany with a strong industrial base, exiting nuclear and coal-fired power generation at the same time would not be possible.\"\nGermany's emissions were escalating in 2012 and 2013 and it is planned to reopen some of the dirtiest brown coal mines that had previously been closed. Coal generated electricity increased to 45% in 2013, the highest level since 2007.\nNonetheless, in 2014 carbon emissions had declined again. More renewable energy had been generated and a greater energy efficiency had been achieved. From 1999 to 2014 renewable energy production rose from 29 TWh to 161 TWh, while nuclear power fell from 180 to 97 TWh and coal power production fell from 291 to 265 TWh.\n\n\n"}
{"id": "751494", "url": "https://en.wikipedia.org/wiki?curid=751494", "title": "Ethnic violence", "text": "Ethnic violence\n\nEthnic violence refers to violence expressly motivated by ethnic hatred and ethnic conflict. It is commonly related to political violence, and often the terms are interchangeable, or one is used as a pretext for the other when politically expedient.\nForms of ethnic violence which can be argued to have the character of terrorism may be known as ethnic terrorism or ethnically-motivated terrorism. \n\"Racist terrorism\" is a form of ethnic violence dominated by overt racism and xenophobic reactionism.\n\nEthnic violence in an organized, sustained form is known as ethnic conflict or warfare (race war), in contrast to class conflict, where the dividing line is social class rather than ethnic background.\n\nCare must be taken to distinguish ethnic violence, which is violence \"motivated\" by an ethnic division, from violence that just happens to break out between groups of different ethnicity motivated by other factors (political or ideological).\n\nViolent ethnic rivalry is the subject matter of Jewish sociologist Ludwig Gumplowicz's \"Der Rassenkampf\" (\"Struggle of the Races\", 1909); and more recently of Amy Chua's notable study, \": How Exporting Free Market Democracy Breeds Ethnic Hatred and Global Instability\".\nSome academics would place all \"nationalist-based violence\" under ethnic violence, which would include the World Wars and all major conflicts between industrialised nations during the 19th century.\n\nThere are a variety of potential causes for ethnic violence. Research by the New England Complex Systems Institute (NESCI) has shown that violence results when ethnic groups are partially mixed: neither clearly separated enough to reduce contact nor thoroughly mixed enough to build common bonds. According to Dr. May Lim, a researcher affiliated with NECSI, \"Violence normally occurs when a group is large enough to impose cultural norms on public spaces, but not large enough to prevent those norms from being broken. Usually this occurs in places where boundaries between ethnic or cultural groups are unclear.\" \n\nThis theory also states that the minimum requirement for ethnic tensions to result in ethnic violence on a systemic level is a heterogeneous society and the lack of a power to prevent them from fighting. In the ethnic conflicts that erupted after the end of the Cold War, this lack of outer controls is seen as the cause; Since there was no longer a strong centralized power (in the form of the USSR) to control the various ethnic groups, they then had to provide defense for themselves. This implies that once ethnicity is established, there needs to be strong distinctions, otherwise violence is inevitable.\n\nAnother theory supports that a general feeling of lack of security can cause ethnic violence, when paired with proximity to other ethnic groups. This can eventually lead to distrust of the other ethnic groups, which leads to an unwillingness to peacefully coexist with the other ethnic groups.\n\nThe emotions that tend to cause ethnic tensions, which can lead to ethnic violence, are fear, hate, resentment, and rage. Individual identities might change throughout the years, but strong emotional issues can lead to a desire to fulfill those needs above all other concerns. This strong desire to satisfy individual needs, without harming your own group, can have violent results.\n\nAssuming that ethnic groups can be defined as a group of people who band together to protect material goods, while satisfying the need to feel a part of a group, violence resulting from ethnicity can be a result of a violation of either of these. However, this also requires that there was no peaceful solution.\n\nAnother theory states that ethnic violence is the result of past tensions. Referring to the other ethnic group based solely on their previous offences tends to increase the probability of future violence. This is referenced in the literature on ethnic violence that tends to focus on areas that have already had a history of ethnic violence, instead of comparing them with areas that have had peaceful ethnic relations.\n\nEthnic violence obviously does not exist in exactly the same conditions in every example. Where one case of ethnic violence might result in a drawn out genocide, another might result in a race riot. Different issues lead to different levels of intensity of violence. The problem mainly comes down to issues of group security. In situations where offensive and defensive actions are indistinguishable to outsiders, and the offensive actions are more effective in insuring group survival, then violence is sure to be present and harsh. This view of ethnic violence placed risk in areas where members of ethnic groups feel insecure about their future, not as a result of emotional tensions.\n\nEthnic violence often occurs as a result of individual domestic disputes spiralling out of control to large-scale conflicts. When individual disputes occur between two members of different ethnic groups, they can either result in peace or result in further violence. Peace is more likely when the offended person feels their offender will be punished sufficiently in their own ethnic group. Or peace is achieved simply through the fear of greater ethnic violence. If either the belief of retribution or the fear of violence is not present, then ethnic violence may occur.\n\nEthnic violence being particularly violent, there are numerous theories for preventing it, or once it starts, for ending it. Yaneer Bar-Yam of the New England Complex Systems Institute suggests that either \"clear boundaries\" or \"thorough mixing\" can reduce violence, citing Switzerland as an example.\n\nUnfortunately, poorly planned separations do not lead to peace among ethnic groups. Religious separation between India and Pakistan left large heterogenous areas in India where violence has since occurred.\n\nThe United States is often presented as the classic \"melting pot\" of ethnicities. \"Ethnic\" tensions in the United States are more typically viewed in terms of race.\n\nUsing the media to change perceptions of ethnicity might lead to a change in probability of ethnic violence. The use of media that results in ethnic violence is usually a cyclical relationship; one group increases messages of group cohesion in response to a perceived threat, and a neighboring group responds with messages of their own group cohesion. Of course, this only happens when outside groups are already perceived as being potential threats. Using this logic, ethnic violence might be prevented by decreasing messages of group cohesion, while increasing messages of safety and solidarity with members of other ethnic groups.\n\nOutside forces may also be effective in decreasing the likelihood of ethnic violence. However, not all interferences from outside forces may be helpful. If not handled delicately, the possibility might increase. Outside groups can help stabilize danger zones by imposing gentle economic sanctions, develop more representative political institutions that would allow for minority voices to be heard, and encourage the respect of ethnically diverse communities and minorities. However, if done incorrectly, outside interference can cause a nationalistic lash-back.\n\nExamples of ethnic violence include:\n\nA genocide qualifies as \"ethnic violence\" (of the most extreme sort), because the victims of a genocide are by definition killed only based on their membership in a given ethnic group.\n\nSome of the world's ongoing conflicts are, however, fought along religious rather than ethnic lines; an example of this is the Somali Civil War.\nThe Guatemalan Civil War was fought along ideological lines (leftist rebel groups) but acquired ethnic characteristics because the rebels were primarily supported by the indigenous Mayan groups.\n\nTerrorism against Copts in Egypt qualifies as both ethnic and religious and isn't fought in an ongoing conflict but reflects a history of sporadic and continuous attacks, over the years.\n\n"}
{"id": "46939774", "url": "https://en.wikipedia.org/wiki?curid=46939774", "title": "Euro-orphan", "text": "Euro-orphan\n\nEuro-orphan or EU orphan is a neologism used metaphorically to describe a \"social orphan\" in the European Union whose parents have migrated to another member state, typically for economic reasons. The child is left behind, often in the care of older relatives. The expression itself is a misnomer, since it is meant to describe temporary child abandonment, rather than the death of both parents. A similar name is White Orphans.\n\nSuch abandoned children may require therapeutic or psychiatric care to cope. The EU supports family reunification. Migrating families are sometimes divided by local child services (Jugendamt). The number of Euro-orphans in the EU is estimated to be between 0.5-1 million, more Euro-orphans live outside the EU, e.g. in Ukraine.\n\nŁukasz Krzyżanowski has coined a similar term, \"old euro-orphans\", describing elderly parents left behind by migrants.\n\n\n\n\n"}
{"id": "23404175", "url": "https://en.wikipedia.org/wiki?curid=23404175", "title": "Fractional Schrödinger equation", "text": "Fractional Schrödinger equation\n\nThe fractional Schrödinger equation is a fundamental equation of fractional quantum mechanics. It was discovered by Nick Laskin (1999) as a result of extending the Feynman path integral, from the Brownian-like to Lévy-like quantum mechanical paths. The term \"fractional Schrödinger equation\" was coined by Nick Laskin.\n\nThe fractional Schrödinger equation in the form originally obtained by Nick Laskin is:\n\n\nFurther,\n\nHere, the wave functions in the position and momentum spaces; formula_2 and formula_3 are related each other by the 3-dimensional Fourier transforms:\n\nThe index \"α\" in the fractional Schrödinger equation is the Lévy index, 1 < \"α\" ≤ 2. Thus, the fractional Schrödinger equation includes a space derivative of fractional order \"α\" instead of the second order (\"α\" = 2) space derivative in the standard Schrödinger equation. Thus, the fractional Schrödinger equation is a fractional differential equation in accordance with modern terminology. This is the main point of the term \"fractional Schrödinger equation\" or a more general term fractional quantum mechanics. At \"α\" = 2 fractional Schrödinger equation becomes the well-known Schrödinger equation.\n\nThe fractional Schrödinger equation has the following operator form\n\nwhere the fractional Hamilton operator formula_5 is given by\n\nThe Hamilton operator, formula_5 corresponds to the classical mechanics Hamiltonian function introduced by Nick Laskin\n\nwhere p and r are the momentum and the position vectors respectively.\n\nThe special case when the Hamiltonian formula_9 is independent of time\nis of great importance for physical applications. \nIt is easy to see that in this case there exist the special solution of the fractional Schrödinger equation\nwhere formula_12 satisfies\n\nor\n\nThis is the time-independent fractional Schrödinger equation (see, Ref.[2]).\n\nThus, we see that the wave function formula_15 oscillates with a definite frequency. In classical physics the frequency corresponds to the energy. Therefore, the quantum mechanical state has a definite energy \"E\". \nThe probability to find a particle at formula_16 is the absolute square of the wave function formula_17\nBecause of time-independent fractional Schrödinger equation this is equal to formula_18 and does not depend upon the time. \nThat is, the probability of finding the particle at formula_16 is independent of the time. One can say that the system is in a stationary\nstate. In other words, there is no variation in the probabilities as a function of time.\n\nThe conservation law of fractional quantum mechanical probability has been discovered for the first time by D.A.Tayurskii and Yu.V. Lysogorski \n\nwhere formula_21\nis the quantum mechanical probability density and the vector formula_22\ncan be called by the fractional probability current density vector\n\nand\n\nhere we use the notation (see also matrix calculus): formula_25.\n\nIt has been found in Ref.[5] that there are quantum physical conditions when the new term formula_26 is negligible and we come to the continuity equation for quantum probability current and quantum density (see, Ref.[2]):\n\nIntroducing the momentum operator formula_28 we can write the vector formula_29\nin the form (see, Ref.[2])\n\nThis is fractional generalization of the well-known equation for probability current density\nvector of standard quantum mechanics (see, Ref.[7]).\n\nThe quantum mechanical velocity operator formula_31 is defined as follows:\n\nStraightforward calculation results in (see, Ref.[2])\n\nHence,\n\nTo get the probability current density equal to 1 (the current when one\nparticle passes through unit area per unit time) the wave function of a free\nparticle has to be normalized as\n\nwhere formula_36 is the particle velocity, formula_37.\n\nThen we have\n\nthat is, the vector formula_29 is indeed the unit vector.\n\nWhen formula_40 is the potential energy of hydrogenlike atom,\n\nwhere \"e\" is the electron charge and \"Z\" is the atomic number of the hydrogenlike atom, (so \"Ze\" is the nuclear charge of the atom), we come to following fractional eigenvalue problem,\n\nThis eigenvalue problem has first been introduced and solved by Nick Laskin in.\n\nUsing the first Niels Bohr postulate yields\n\nand it gives us the equation for the Bohr radius of the fractional hydrogenlike atom\n\nHere \"a\" is the fractional Bohr radius (the radius of the lowest, \"n\" = 1, Bohr orbit) defined as,\n\nThe energy levels of the fractional hydrogenlike atom are given by\n\nwhere \"E\" is the binding energy of the electron in the lowest Bohr orbit\nthat is, the energy required to put it in a state with \"E\" = 0 corresponding to \"n\" = ∞,\n\nThe energy (\"α\" − 1)\"E\" divided by \"ħc\", (\"α\" − 1)\"E\"/\"ħc\", can be considered as fractional generalization of the\nRydberg constant of standard quantum mechanics. For \"α\" = 2 and \"Z\" = 1 the formula\nformula_48 is transformed into\n\nwhich is the well-known expression for the Rydberg formula.\n\nAccording to the second Niels Bohr postulate, the frequency of radiation formula_50 associated with the transition, say, for example from the orbit \"m\" to the orbit \"n\", is,\n\nThe above equations are fractional\ngeneralization of the Bohr model. In the special Gaussian case, when (\"α\" = 2) those equations give us the well-known results of the Bohr model.\n\nA particle in a one-dimensional well moves in a potential field formula_52, which is zero for \nformula_53 and which is infinite elsewhere,\n\nIt is evident \"a priori\" that the energy spectrum will be discrete. The solution of the fractional Schrödinger equation for the stationary state with well-defined energy \"E\" is described by a wave function formula_57, which can be written as\n\nwhere formula_59, is now time independent. \nIn regions (i) and (iii),\nthe fractional Schrödinger\nequation can be satisfied only if we take formula_60. In the middle region\n(ii), the time-independent fractional Schrödinger equation is (see, Ref.[6]).\n\nThis equation defines the wave functions and the energy spectrum within region (ii), while outside\nof the region (ii), x<-a and x>a, the wave functions are zero. The wave function formula_59 has to be continuous everywhere, thus we impose the boundary conditions formula_63 for the solutions of the \"time-independent fractional Schrödinger equation\" (see, Ref.[6]). Then the solution in region (ii) can be written as\n\nTo satisfy the boundary conditions we have to choose\n\nand\n\nIt follows from the last equation that\n\nThen the even (formula_68 under reflection formula_69) solution of the time-independent fractional Schrödinger equation formula_70 in the infinite potential well is\n\nThe odd (formula_72 under reflection formula_69) solution of the time-independent fractional Schrödinger equation formula_70 in the infinite potential well is\n\nThe solutions formula_70 and formula_77 have the\nproperty that\n\nwhere formula_79 is the Kronecker symbol and\n\nThe eigenvalues of the particle in an infinite potential well are (see, Ref.[6])\n\nIt is obvious that in the Gaussian case (\"α\" = 2) above equations are ö\ntransformed into the standard quantum mechanical equations for a particle in a box (for example, see\nEq.(20.7) in )\n\nThe state of the lowest energy, the ground state, in the infinite potential\nwell is represented by the formula_82 at \"n\"=1,\n\nand its energy is\n\n\"Fractional quantum oscillator\" introduced by Nick Laskin (see, Ref.[2]) is the fractional quantum mechanical model with the Hamiltonian operator formula_85 defined as\n\nwhere \"q\" is interaction constant.\n\nThe fractional Schrödinger equation for the wave\nfunction formula_15 of the fractional quantum oscillator is,\n\nAiming to search for solution in form\n\nwe come to the time-independent fractional Schrödinger equation,\n\nThe Hamiltonian formula_91 is the fractional\ngeneralization of the 3D quantum harmonic oscillator Hamiltonian of standard quantum\nmechanics.\n\nThe energy levels of 1D fractional quantum oscillator with the Hamiltonian function formula_92 were found in semiclassical approximation (see, Ref.[2]).\n\nWe set the total energy equal to \"E\", so that\n\nwhence\n\nAt the turning points formula_95. Hence, the classical motion is\npossible in the range formula_96.\n\nA routine use of the Bohr-Sommerfeld quantization rule yields\n\nwhere the notation formula_98 means the integral over one complete period of\nthe classical motion and formula_99 is the turning point\nof classical motion.\n\nTo evaluate the integral in the right hand we introduce a new variable formula_100. Then we have\n\nThe integral over \"dy\" can be expressed in terms of the Beta-function,\n\nTherefore,\n\nThe above equation gives the energy levels of stationary states for the 1D fractional quantum oscillator (see, Ref.[2]),\n\nThis equation is generalization of the well-known energy levels equation of the\nstandard quantum harmonic oscillator (see, Ref.[7]) and is transformed into it at \"α\" = 2 and \"β\" = 2.\nIt follows from this equation that at formula_105 the energy levels are equidistant. When formula_106 and formula_107 the equidistant energy levels can be for \"α\" = 2 and \"β\" = 2 only. It means that the only standard quantum harmonic oscillator has an equidistant energy spectrum.\n\nThe effective mass of states in solid state systems can depend on the wave vector k, i.e. formally one considers m=m(k). Polariton Bose-Einstein condensate modes are examples of states in solid state systems with mass sensitive to variations and locally in k fractional quantum mechanics is experimentally feasible .\n\n\n\n"}
{"id": "5641913", "url": "https://en.wikipedia.org/wiki?curid=5641913", "title": "Free warren", "text": "Free warren\n\nFree warren—often simply \"warren\"—refers to a type of franchise or privilege conveyed by a sovereign in medieval England to an English subject, promising to hold them harmless for killing game of certain species within a stipulated area, usually a wood or small forest. The sovereign involved might be either the monarch or a marcher lord.\n\nThe grant of free warren could be as a gift, or in exchange for consideration, and might be later alienated by the grantee. The stipulated area might be coextensive with the frank-tenement of the grantee, or it might be discontinuous or even at a considerable remove from the grantee's holdings. The right of free warren did not extend automatically to the freeholder of the soil.\n\nAlthough the rights of free warren are usually discussed in the context of forest law, the only law which applied within the warren was common law. Thus, even though the warrant ultimately derived from the sovereign, the only statutes applied to poachers in a warren were the common-law crimes of theft and trespass.\n\nThe privilege of free warren was a reciprocal relationship. The grantee of the warren was granted an exemption from the law (under which all game in the realm was property of the sovereign), but the grantee owed the sovereign the stewardship and protection of the game from all others who might wish to hunt it.\n\nModern English \"warren\" ← ME \"warrene\", \"warreine\" ← ONF \"warrenne\" ← Germanic present participle of *\"warian\" \"to take care; to cause to care (for)\" ← causative of *\"waran\" \"to care\" ← \"*war\" \"care\". Doublet of \"guarantor\". Related to OHG \"werien\" (i.e. \"*wärian\") \"to defend, protect\", and also to English \"a-ware, wary\".\nThe original use of \"free warren\" was as a legal term. However, as the franchise defined both a set of species and a geographic extent, the natural semantic extensions arose, namely for the individual animals as a group, or for the land they inhabited. As it became pragmatically necessary for freeholders not holding a free warren to enclose their breeding establishments, these \"closed warrens\" or domestic warrens began also to be designated simply as \"warrens\" (use recorded in 1378; OED). In 1649 the metaphoric use as \"cluster of densely populated living spaces\" is recorded.\n\nThe Mediaeval Latin form of the word \"warenna\" was used in legal documents such as Magna Carta. In addition, the office of warden is used for the overseer of a warren:\n\n(5) But the warden, as long as he hath the custody of the lands, shall keep up and maintain the houses, parks, warrens, ponds, mills, and other things belonging to them, our of their issues;\n\nThe warden of a Royal forest was often the castellan or constable of the nearest royal castle; over time the less exalted title of \"warrener\" evolved for the custodian of the lowest of the hunting franchises, the warren.\n\nThe adjective \"free\" in \"free warren\" does not refer to the lack of enclosure surrounding the precincts of the warren, but rather to the fact the \"liberty\" of hunting derives from a warrant of the sovereign. That is,\nThe term \"warrant\" occurs very early in constitutional documents: it is found in the Assize of Clarendon and the Assize of the Forest, both in the reign of Henry II., but in neither case in its modern meaning. The original meaning seems to have been more akin to guarantee (q.v.), warranty or security; and to some extent the term implies something in the nature of a guarantee or representation by the person issuing the warrant that the person who acts on it can do so without incurring any legal penalty.\nAll of the terms \"warrant\", \"warrantor\", and \"warranty\" are used in Henry II of England's Assize of the Forest (a.k.a. Assize of Woodstock) in 1184:\n\nThe permission to take game was limited to certain types of animals. Generally, the killing of vermin (defined as predators and other beasts not fit for the table) was not regulated. This definition was flexible, however, depending on whether the animal was thought to provide good sport, as wolves, foxes, badgers, or bears. In practice, vermin could only be killed on the commons or waste, since none but the grantee was permitted to have instruments of the hunt within the warren.\n\nThe most cited authority on Forest Law, John Manwood, cites these beasts of warren:\n\n\"The beasts and fouls of Warren are these, The Hare, the Cony, the Pheasant, and the Partridge, and none other are accompted beasts or fouls of Warren.\"\nHowever, Manwood is mistaken in his assignments, since the roe deer was transferred to \"beast of warren\" from \"beast of the forest\" in the fourteenth century. Roe deer are still found within woodlands named \"Warren\" in contemporary England. The 1911 Encyclopedia adds roe, woodcock, quail, and rail to Manwood's list. On the other hand, grouse are not birds of warren. Fox, wolf, cat, badger, and squirrel are sometimes also added.\n\nSometimes domestic swine are mistakenly thought to be beasts of warren, due the right of pannage.\n\n"}
{"id": "195947", "url": "https://en.wikipedia.org/wiki?curid=195947", "title": "Function composition", "text": "Function composition\n\nIn mathematics, function composition is the pointwise application of one function to the result of another to produce a third function. For instance, the functions and can be \"composed\" to yield a function which maps in to in . Intuitively, if is a function of , and is a function of , then is a function of . The resulting \"composite\" function is denoted , defined by for all in .\nThe notation is read as \" circle \", \" round \", \" about \", \" composed with \", \" after \", \" following \", \" of \", or \" on \". Intuitively, composing two functions is a chaining process in which the output of the inner function becomes the input of the outer function.\n\nThe composition of functions is a special case of the composition of relations, so all properties of the latter are true of composition of functions. The composition of functions has some additional properties.\n\n\nThe composition of functions is always associative—a property inherited from the composition of relations. That is, if , , and are three functions with suitably chosen domains and codomains, then , where the parentheses serve to indicate that composition is to be performed first for the parenthesized functions. Since there is no distinction between the choices of placement of parentheses, they may be left off without causing any ambiguity.\n\nIn a strict sense, the composition can be built only if 's codomain equals 's domain; in a wider sense it is sufficient that the former is a subset of the latter.\nMoreover, it is often convenient to tacitly restrict 's domain such that produces only values in 's domain; for example, the composition of the functions defined by and defined by can be defined on the interval .\nThe functions and are said to commute with each other if . Commutativity is a special property, attained only by particular functions, and often in special circumstances. For example, only when . The picture shows another example.\n\nThe composition of one-to-one functions is always one-to-one. Similarly, the composition of two onto functions is always onto. It follows that composition of two bijections is also a bijection. The inverse function of a composition (assumed invertible) has the property that .\n\nDerivatives of compositions involving differentiable functions can be found using the chain rule. Higher derivatives of such functions are given by Faà di Bruno's formula.\n\nSuppose one has two (or more) functions having the same domain and codomain; these are often called \"transformation\"s. Then one can form chains of transformations composed together, such as . Such chains have the algebraic structure of a monoid, called a transformation monoid or (much more seldom) \"composition monoid\". In general, transformation monoids can have remarkably complicated structure. One particular notable example is the de Rham curve. The set of \"all\" functions is called the full transformation semigroup or \"symmetric semigroup\" on . (One can actually define two semigroups depending how one defines the semigroup operation as the left or right composition of functions.)\nIf the transformation are bijective (and thus invertible), then the set of all possible combinations of these functions forms a transformation group; and one says that the group is generated by these functions. A fundamental result in group theory, Cayley's theorem, essentially says that any group is in fact just a subgroup of a permutation group (up to isomorphism).\n\nThe set of all bijective functions (called permutations) forms a group with respect to the composition operator. This is the symmetric group, also sometimes called the \"composition group\".\n\nIn the symmetric semigroup (of all transformations) one also finds a weaker, non-unique notion of inverse (called a pseudoinverse) because the symmetric semigroup is a regular semigroup.\n\nIf , then may compose with itself; this is sometimes denoted as . That is:\n\nMore generally, for any natural number , the th functional power can be defined inductively by . Repeated composition of such a function with itself is called iterated function.\n\nNote: If takes its values in a ring (in particular for real or complex-valued ), there is a risk of confusion, as could also stand for the -fold product of , e.g. . For trigonometric functions, usually the latter is meant, at least for positive exponents. For example, in trigonometry, this superscript notation represents standard exponentiation when used with trigonometric functions:\nHowever, for negative exponents (especially −1), it nevertheless usually refers to the inverse function, e.g., .\n\nIn some cases, when, for a given function , the equation has a unique solution , that function can be defined as the functional square root of , then written as .\n\nMore generally, when has a unique solution for some natural number , then can be defined as .\n\nUnder additional restrictions, this idea can be generalized so that the iteration count becomes a continuous parameter; in this case, such a system is called a flow, specified through solutions of Schröder's equation. Iterated functions and flows occur naturally in the study of fractals and dynamical systems.\n\nTo avoid ambiguity, some mathematicians choose to write for the \"n\"-th iterate of the function .\n\nMany mathematicians, particularly in group theory, omit the composition symbol, writing for .\n\nIn the mid-20th century, some mathematicians decided that writing \"\" to mean \"first apply , then apply \" was too confusing and decided to change notations. They write \" for \" and \" for \". This can be more natural and seem simpler than writing functions on the left in some areas – in linear algebra, for instance, when is a row vector and and denote matrices and the composition is by matrix multiplication. This alternative notation is called postfix notation. The order is important because function composition is not necessarily commutative (e.g matrix multiplication). Successive transformations applying and composing to the right agrees with the left-to-right reading sequence.\n\nMathematicians who use postfix notation may write \", meaning first apply and then apply , in keeping with the order the symbols occur in postfix notation, thus making the notation \" ambiguous. Computer scientists may write \"\" for this, thereby disambiguating the order of composition. To distinguish the left composition operator from a text semicolon, in the Z notation the ⨾ character is used for left relation composition. Since all functions are binary relations, it is correct to use the [fat] semicolon for function composition as well (see the article on composition of relations for further details on this notation).\n\nGiven a function , the composition operator is defined as that operator which maps functions to functions as\n\nComposition operators are studied in the field of operator theory.\n\nFunction composition appears in one form or another in numerous programming languages.\n\nPartial composition is possible for multivariate functions. The function resulting when some argument of the function is replaced by the function is called a composition of and in some computer engineering contexts, and is denoted \n\nWhen is a simple constant , composition degenerates into a (partial) valuation, whose result is also known as restriction or \"co-factor\".\n\nIn general, the composition of multivariate functions may involve several other functions as arguments, as in the definition of primitive recursive function. Given , a -ary function, and -ary functions , the composition of with , is the -ary function\n\nThis is sometimes called the generalized composite of \"f\" with . The partial composition in only one argument mentioned previously can be instantiated from this more general scheme by setting all argument functions except one to be suitably chosen projection functions. Note also that can be seen as a single vector/tuple-valued function in this generalized scheme, in which case this is precisely the standard definition of function composition.\n\nA set of finitary operations on some base set \"X\" is called a clone if it contains all projections and is closed under generalized composition. Note that a clone generally contains operations of various arities. The notion of commutation also finds an interesting generalization in the multivariate case; a function \"f\" of arity \"n\" is said to commute with a function \"g\" of arity \"m\" if \"f\" is a homomorphism preserving \"g\", and vice versa i.e.:\n\nA unary operation always commutes with itself, but this is not necessarily the case for a binary (or higher arity) operation. A binary (or higher arity) operation that commutes with itself is called medial or entropic.\n\nComposition can be generalized to arbitrary binary relations.\nIf and are two binary relations, then their composition is the relation defined as .\nConsidering a function as a special case of a binary relation (namely functional relations), function composition satisfies the definition for relation composition.\n\nThe composition is defined in the same way for partial functions and Cayley's theorem has its analogue called Wagner-Preston theorem.\n\nThe category of sets with functions as morphisms is the prototypical category. The axioms of a category are in fact inspired from the properties (and also the definition) of function composition. The structures given by composition are axiomatized and generalized in category theory with the concept of morphism as the category-theoretical replacement of functions. The reversed order of composition in the formula applies for composition of relations using converse relations, and thus in group theory. These structures form dagger categories.\n\nThe composition symbol is encoded as ; see the Degree symbol article for similar-appearing Unicode characters. In TeX, it is written codice_1.\n\n\n"}
{"id": "3180013", "url": "https://en.wikipedia.org/wiki?curid=3180013", "title": "IDEF1X", "text": "IDEF1X\n\nIntegration DEFinition for information modeling (IDEF1X) is a data modeling language for the development of semantic data models. IDEF1X is used to produce a graphical information model which represents the structure and semantics of information within an environment or system. \nIDEF1X permits the construction of semantic data models which may serve to support the management of data as a resource, the integration of information systems, and the building of computer databases. This standard is part of the IDEF family of modeling languages in the field of software engineering.\n\nA data modeling technique is used to model data in a standard, consistent and predictable manner in order to manage it as a resource. It can be used in projects requiring a standard means of defining and analyzing the data resources within an organization. Such projects include the incorporation of a data modeling technique into a methodology, managing data as a resource, integrating information systems, or designing computer databases. The primary objectives of the IDEF1X standard are to provide: \n\nA principal objective of IDEF1X is to support integration. The approach to integration focuses on the capture, management, and use of a single semantic definition of the data resource referred to as a “conceptual schema.” The “conceptual schema” provides a single integrated definition of the data within an enterprise which is not biased toward any single application of data and is independent of how the data is physically stored or accessed. The primary objective of this conceptual schema is to provide a consistent definition of the meanings of and interrelationships between data that can be used to integrate, share, and manage the integrity of data. A conceptual schema must have three important characteristics: \n\nThe need for semantic data models was first recognized by the U.S. Air Force in the mid-1970s as a result of the Integrated Computer Aided Manufacturing (ICAM) Program. The objective of this program was to increase manufacturing productivity through the systematic application of computer technology. The ICAM Program identified a need for better analysis and communication techniques for people involved in improving manufacturing productivity. As a result, the ICAM Program developed a series of techniques known as the IDEF (ICAM Definition) Methods which included the following: \n\nThe initial approach to IDEF information modeling (IDEF1) was published by the ICAM program in 1981, based on current research and industry needs. The theoretical roots for this approach stemmed from the early work of Edgar F. Codd on Relational model theory and Peter Chen on the entity-relationship model. The initial IDEF1 technique was based on the work of Dr R. R. Brown and Mr T. L. Ramey of Hughes Aircraft and Mr D. S. Coleman of D. Appleton Company (DACOM), with critical review and influence by Charles Bachman, Peter Chen, Dr M. A. Melkanoff, and Dr G.M. Nijssen.\n\nIn 1983, the U.S. Air Force initiated the Integrated Information Support System (I2S2) project under the ICAM program. The objective of this project was to provide the enabling technology to logically and physically integrate a network of heterogeneous computer hardware and software. As a result of this project, and industry experience, the need for an enhanced technique for information modeling was recognized.\n\nFrom the point of view of the contract administrators of the Air Force IDEF program, IDEF1X was a result of the ICAM IISS-6201 project and was further extended by the IISS-6202 project. To satisfy the data modeling enhancement requirements that were identified in the IISS-6202 project, a sub-contractor, DACOM, obtained a license to the Logical Database Design Technique (LDDT) and its supporting software (ADAM). From the point of view of the technical content of the modeling technique, IDEF1X is a renaming of LDDT.\n\nOn September 2, 2008, the associated NIST standard, FIPS 184, has been withdrawn (decision on Federal Register vol. 73 / page 51276 ).\n\nThe logical database design technique (LDDT) had been developed in 1982 by Robert G. Brown of The Database Design Group entirely outside the IDEF program and with no knowledge of IDEF1. Nevertheless, the central goal of IDEF1 and LDDT was the same: to produce a database-neutral model of the persistent information needed by an enterprise by modeling the real-world entities involved. LDDT combined elements of the relational data model, the E-R model, and data generalization in a way specifically intended to support data modeling and the transformation of the data models into database designs.\n\nLDDT included an environmental (namespace) hierarchy, multiple levels of model, the modeling of generalization/specialization, and the explicit representation of relationships by primary and foreign keys, supported by a well defined role naming facility. The primary keys and unambiguously role-named foreign keys expressed sometimes subtle uniqueness and referential integrity constraints that needed to be known and honored by whatever type of database was ultimately designed. Whether the database design used the integrity constraint based keys of the LDDT model as database access keys or indexes was an entirely separate decision. The precision and completeness of the LDDT models was an important factor in enabling the relatively smooth transformation of the models into database designs. Early LDDT models were transformed into database designs for IBM's hierarchical database, IMS. Later models were transformed into database designs for Cullinet's network database, IDMS, and many varieties of relational database.\n\nThe LDDT software, ADAM, supported view (model) entry, view merging, selective (subset) viewing, namespace inheritance, normalization, a quality assurance analysis of views, entity relationship graph and report generation, transformation to a relational database expressed as SQL data declaration statements, and referential integrity checking SQL. Logical models were serialized with a structural modeling language.\n\nThe graphic syntax of LDDT differed from that of IDEF1 and, more importantly, LDDT contained many interrelated modeling concepts not present in IDEF1. Therefore, instead of extending IDEF1, Mary E. Loomis of DACOM wrote a concise summary of the syntax and semantics of a substantial subset of LDDT, using terminology compatible with IDEF1 wherever possible. DACOM labeled the result IDEF1X and supplied it to the ICAM program, which published it in 1985. (IEEE 1998, p. iii) (Bruce 1992, p. xii) DACOM also converted the ADAM software to C and sold it under the name Leverage.\n\n\nThe three-schema approach in software engineering is an approach to building information systems and systems information management, that promotes the conceptual model as the key to achieving data integration.\n\nA schema is a model, usually depicted by a diagram and sometimes accompanied by a language description. The three schemas used in this approach are:\n\nAt the center, the conceptual schema defines the ontology of the concepts as the users think of them and talk about them. The physical schema describes the internal formats of the data stored in the database, and the external schema defines the view of the data presented to the application programs. The framework attempted to permit multiple data models to be used for external schemata.\n\nThe modeling process can be divided into five stages of model developing.\n\n\n\n\n\n\nA meta model is a model of the constructs of a modeling system. Like any model, it is used to represent and reason about the subject of the model - in this case IDEF1X. The meta model is used to reason about IDEF1X, i.e., what the constructs of IDEF1X are and how they relate to one another. The model shown is an IDEF1X model of IDEF1X. Such meta models can be used for various purposes, such as repository design, tool design, or in order to specify the set of valid IDEF1X models. Depending on the purpose, somewhat different models result. There is no “one right model.” For example, a model for a tool that supports building models incrementally must allow incomplete or even inconsistent models. The meta model for formalization, however, emphasizes alignment with the concepts of the formalization and hence incomplete or inconsistent models are not allowed.\n\nMeta models have two important limitations. First, they specify syntax but not semantics. Second, a meta model must be supplemented with constraints in natural or formal language. The formal theory of IDEF1X provides both the semantics and a means to precisely express the necessary constraints.\n\nA meta model for IDEF1X is given in the adjacent figure. The name of the view is \"mm\". The domain hierarchy and constraints are also given. The constraints are expressed as sentences in the formal theory of the meta model. The meta model informally defines the set of valid IDEF1X models in the usual way, as the sample instance tables that correspond to a valid IDEF1X model. The meta model also formally defines the set of valid IDEF1X models in the following way. The meta model, as an IDEF1X model, has a corresponding formal theory. The semantics of the theory are defined in the standard way. That is, an interpretation of a theory consists of a domain of individuals and a set of assignments:\nIn the intended interpretation, the domain of individuals consists of views, such as production; entities, such as part and vendor; domains, such as qty_on_hand; connection relationships; category clusters; and so on. If every axiom in the theory is true in the interpretation, then the interpretation is called a model for the theory. Every model for the IDEF1X theory corresponding to the IDEF1X meta model and its constraints is a valid IDEF1X model.\n\n\n\n"}
{"id": "500995", "url": "https://en.wikipedia.org/wiki?curid=500995", "title": "Invagination", "text": "Invagination\n\nIn developmental biology, invagination is a mechanism that takes place during gastrulation. This mechanism or cell movement happens mostly in the vegetal pole. Invagination consists of the folding of an area of the exterior sheet of cells towards the inside of the blastula. In each organism, the complexity will be different depending on the number of cells. Invagination can be referenced as one of the steps of the establishment of the body plan. The term, originally used in embryology, has been adopted in other disciplines as well. \nThere is more than one type of movement for invagination. Two common types are axial and orthogonal. The difference between the production of the tube formed in the cytoskeleton and extracellular matrix. Axial can be formed at a single point along the axis of a surface. Orthogonal is linear and trough.\n\n\nThe invagination in \"Amphioxus\" is the first cell movement of gastrulation. This process was first described by Conklin. During gastrulation, the blastula will be transformed by the invagination. The endoderm will fold towards the inner part and that way the blastocoel will be gone transforming into like a cup-shaped structure with a double wall. The inner wall will be called the archenteron; the primitive gut. The archenteron will open to the exterior through the blastopore. The outer wall will become the ectoderm. Later forming the epidermis and neural crest.\n\nIn tunicates, invagination is the first mechanism that takes place during gastrulation. The four largest endoderm cells induce the invagination process in the tunicates. Invagination consists of the internal movements of a sheet of cells (the endoderm) based on changes in their shape. The blastula of the tunicates is a little flattened in the vegetal pole making a change of shape from a columnar to a wedge shape. Once the endoderm cells were invaginated, the cells will keep moving beneath the ectoderm. Later, the blastopore will be formed and with this, the invagination process is complete. The blastopore will be surrounded by the mesoderm by all sides.\n\nIn Continental philosophy, the term invagination is used to explain a special kind of metanarrative. It was first used by Maurice Merleau-Ponty () to describe the dynamic self-differentiation of the 'flesh'. It was later used by Rosalind E. Krauss and Jacques Derrida (\"The Law of Genre\", \"Glyph 7\", 1980); for Derrida, an invaginated text is a narrative that folds upon itself, \"endlessly swapping outside for inside and thereby producing a structure \"en abyme\"\". He applies the term to such texts as Immanuel Kant's \"Critique of Judgment\" and Maurice Blanchot's \"La Folie du Jour\". Invagination is an aspect of différance, since according to Derrida it opens the \"inside\" to the \"other\" and denies both inside and outside a stable identity.\n"}
{"id": "5541836", "url": "https://en.wikipedia.org/wiki?curid=5541836", "title": "Johnny Bright", "text": "Johnny Bright\n\nJohnny D. Bright (June 11, 1930 – December 14, 1983) was a professional Canadian football player in the Canadian Football League. He played college football at Drake University. He is a member of the Canadian Football Hall of Fame, the National Football Foundation's College Football Hall of Fame, the Missouri Valley Conference Hall of Fame, the Edmonton Eskimos Wall of Honour, the Alberta Sports Hall of Fame, and the \"Des Moines Register's\" Iowa Sports Hall of Fame.\n\nIn 1951, Bright was named a First Team College Football All-American, and was awarded the Nils V. \"Swede\" Nelson Sportsmanship Award. In 1969, Bright was named Drake University's greatest football player of all time. Bright is the only Drake football player to have his jersey number (No. 43) retired by the school, and in June 2006, received honorable mention from \"ESPN.com\" senior writer Ivan Maisel as one of the best college football players to ever wear No. 43. In February 2006, the football field at Drake Stadium, in Des Moines, Iowa, was named in his honor. In November 2006, Bright was voted one of the CFL's Top 50 players (No. 19) of the league's modern era by Canadian sports network TSN.\n\nIn addition to his outstanding professional and college football careers, Bright is perhaps best known for his role as the victim of an intentional, most likely racially motivated, on-field assault by an opposing college football player from Oklahoma A&M (now Oklahoma State University) on October 20, 1951, that was captured in a widely disseminated and Pulitzer Prize winning photo sequence, and eventually came to be known as the \"Johnny Bright incident.\"\n\nBorn in Fort Wayne, Indiana on June 11, 1930, Bright was the second oldest of five brothers. Bright lived with his mother and step father Daniel Bates, brothers, Homer Bright, the eldest, Alfred, Milton, and Nate Bates, in a working class, predominantly African-American neighborhood in Fort Wayne.\n\nBright was a three-sport (football, basketball, track and field) star at Fort Wayne's Central High School. Bright, who also was an accomplished softball pitcher and boxer, led Central High's football team to a City title in 1945, and helped the basketball team to two state tournament Final Four appearances.\n\nFollowing his graduation from Central High in 1947, Bright initially accepted a football scholarship at Michigan State University, but, apparently unhappy with the direction of the Spartans football program, transferred to Drake University in Des Moines, Iowa, where he accepted a track and field scholarship that allowed him to try out for the football and basketball squads. Bright eventually lettered in football, track, and basketball, during his collegiate career at Drake.\n\nFollowing a mandatory freshman redshirt year, Bright began his collegiate football career in 1949, rushing for 975 yards and throwing for another 975 to lead the nation in total offense during his sophomore year, as the Drake Bulldogs finished their season at 6–2–1. In Bright's junior year, the halfback/quarterback rushed for 1,232 yards and passed for 1,168 yards, setting an NCAA record for total offense (2,400 yards) in 1950, and again led the Bulldogs to a 6–2–1 record.\n\nBright's senior year began with great promise. Bright was considered a pre-season Heisman Trophy candidate, and was leading the nation in both rushing and total offense with 821 and 1,349 yards respectively, when the Drake Bulldogs, winners of their previous five games, faced Missouri Valley Conference foe Oklahoma A&M at Lewis Field (now Boone Pickens Stadium) in Stillwater, Oklahoma, on October 20, 1951.\n\nBright's participation as a halfback/quarterback in Drake's game against Oklahoma A&M on October 20, 1951, was controversial, as it marked the first time that such a prominent African-American athlete, with national notoriety (Bright was a pre-season Heisman Trophy candidate and led the nation in total offense going into the game) and of critical importance to the success of his team (Drake was undefeated and carried a five-game winning streak into the contest, due in large part to his rushing and passing), had played against Oklahoma A&M in a home game at Lewis Field, in Stillwater.\n\nDuring the first seven minutes of the game, Bright had been knocked unconscious three times by blows from Oklahoma A&M defensive tackle Wilbanks Smith. While the final, elbow blow from Smith broke Bright's jaw, Bright was able to complete a 61-yard touchdown pass to halfback Jim Pilkington a few plays later before the injury finally forced Bright to leave the game. Bright finished the game with 75 yards (14 yards rushing and 61 yards passing), the first time he had finished a game with less than 100 yards in his three-year collegiate career at Drake. Oklahoma A&M eventually won the game 27-14.\n\nA photographic sequence by \"Des Moines Register\" cameramen Don Ultang and John Robinson clearly showed that Smith's jaw breaking blow to Bright had occurred well after Bright had handed off the ball to fullback Gene Macomber, and that the blow was delivered well behind the play. Years later, Ultang said that he and Robinson were lucky to capture the incident when they did; they'd only planned to stay through the first quarter so they could get the film developed in time for the next day's edition.\n\nIt had been an open secret before the game that A&M was planning to target Bright. Even though A&M had integrated two years earlier, the Jim Crow spirit was still very much alive in Stillwater. Both Oklahoma A&M's student newspaper, \"The Daily O'Collegian\", and the local newspaper, \"The News Press\", reported that Bright was a marked man, and several A&M students were openly claiming that Bright \"would not be around at the end of the game.\" Ultang and Robinson had actually set up their camera after rumors of Bright being targeted became too loud to ignore.\n\nWhen it became apparent that neither Oklahoma A&M nor the MVC would take any disciplinary action against Smith, Drake withdrew from the MVC in protest and stayed out until 1956 (though it didn't return for football until 1971). Fellow member Bradley University pulled out of the league as well in solidarity with Drake; while it returned for non-football sports in 1955, Bradley never played another down of football in the MVC (it dropped football in 1970).\n\nThe \"Johnny Bright Incident\", as it became widely known, eventually provoked changes in NCAA football rules regarding illegal blocking, and mandated the use of more protective helmets with face guards.\n\nRecalling the incident without apparent bitterness in a 1980 \"Des Moines Register\" interview three years before his death, Bright commented: \"There's no way it couldn't have been racially motivated... . ..What I like about the whole deal now, and what I'm smug enough to say, is that getting a broken jaw has somehow made college athletics better. It made the NCAA take a hard look and clean up some things that were bad.\"\n\nBright's jaw injury limited his effectiveness for the remainder of his senior season at Drake, but he finished his college career with 5,983 yards in total offense, averaging better than 236 yards per game in total offense, and scored 384 points in 25 games. As a senior, Bright earned 70 percent of the yards Drake gained and scored 70 percent of the Bulldogs' points, despite missing the better part of the final three games of the season.\n\nFollowing his final football season at Drake (1951), Bright was named a First Team College Football All-American and finished fifth in the balloting for the 1951 Heisman Trophy. Bright was also awarded the Nils V. \"Swede\" Nelson Sportsmanship Award, and played in both the post-season East–West Shrine Game and the Hula Bowl.\n\nIn 1969, Bright was named Drake University's greatest football player of all time. He is also the only Drake football player to have his jersey number (No. 43) retired by the school. In June 2006, Bright received honorable mention from \"ESPN.com\" senior writer Ivan Maisel as one of the best college football players to ever wear No. 43.\n\nBright was the first pick of the Philadelphia Eagles in the first round of the 1952 National Football League draft. Bright spurned the NFL, electing to play for the Calgary Stampeders of the Western Interprovincial Football Union, the precursor to the West Division of the Canadian Football League. Bright later commented: \n\nBright joined the Calgary Stampeders as a fullback/linebacker in 1952, leading the Stampeders and the WIFU in rushing with 815 yards his rookie season. Bright played fullback/linebacker with the Stampeders for the 1952, 1953, and part of the 1954 seasons. In 1954, the Calgary Stampeders traded him to the Edmonton Eskimos in mid-season. He would enjoy the most success of his professional football career as a member of the Eskimos.\n\nThough Bright played strictly defense as a linebacker in his first year with the Eskimos, he played both offense (as a fullback) and defense for two seasons (1955–1956), and played offense permanently after that (1957–1964). He, along with teammates Rollie Miles, Normie Kwong, and Jackie Parker, helped lead the Eskimos to successive Grey Cup titles in 1954, 1955, and 1956 (where Bright rushed for a then Grey Cup record of 171 yards in a 50–27 win over the Montreal Alouettes). In 1957, he rushed for eight consecutive 100-yard games, finishing the season with 1,679 yards. In 1958, he rushed for 1,722 yards. In 1959, following his third straight season as the Canadian pro rushing leader with 1,340 yards, Bright won the CFL's Most Outstanding Player Award, the first African-American or African-Canadian athlete to be so honored.\n\nBright was approached several times during his Canadian career by NFL teams about playing in the United States, but in the days before the blockbuster salaries of today's NFL players, it was common for CFL players such as him to hold regular jobs in addition to football, and he had already started a teaching career in 1957, the year he moved his family to Edmonton. \n\nBright retired in 1964 as the CFL's all-time leading rusher (Mike Pringle and George Reed have since surpassed him). Bright rushed for 10,909 yards in 13 seasons, had five consecutive 1,000 yard seasons, and led the CFL in rushing four times. While Bright is currently 15th on the All-Pro Rushing list, his career average of 5.5 yards per carry is the highest among 10,000+ yard rushers (Pro Football Hall of Famer Jim Brown is second at 5.2 yards per carry). At the time of his retirement, Bright had a then-CFL record thirty-six 100-plus-yard games, carrying the ball 200 or more times for five straight seasons. Bright led the CFL Western Conference in rushing four times, winning the Eddie James Memorial Trophy in the process, and was a CFL Western Conference All-Star five straight seasons from 1957 to 1961. Bright played in 197 consecutive CFL games as a fullback/linebacker. Bright's No. 24 jersey was added to the Edmonton Eskimos' Wall of Honour at the Eskimos' Commonwealth Stadium in 1983. Bright was inducted into the Canadian Football Hall of Fame on November 26, 1970. In November 2006, Bright was voted one of the CFL's Top 50 players (No. 19) of the league's modern era by Canadian sports network TSN.\n\nBright earned a Bachelor of Science degree in education at Drake University in 1952, becoming a teacher, coach, and school administrator, both during and after his professional football career, eventually rising to the seat of principal of D.S. Mackenzie Junior High School and Hillcrest Junior High School in Edmonton, Alberta. He was head coach at Edmonton's Bonnie Doon High School in the 1960s when the Lancers were a champion football team.\n\nHe became a Canadian citizen in 1962.\n\nBright died of a massive heart attack on December 14, 1983, at the University of Alberta Hospital in Edmonton, while undergoing elective surgery to correct a knee injury suffered during his football career. He was survived by his wife and four children.\n\nBright is buried at Holy Cross Cemetery, in Edmonton.\n\nDespite irrefutable evidence of the incident, Oklahoma A&M officials denied anything had happened. Indeed, Oklahoma A&M/State refused to make any further official comment on the incident for over half a century. This was the case even when Drake's former dean of men, Robert B. Kamm, became president of OSU in 1966; years later, he said that the determination to gloss over the affair was so strong that he knew he could not even discuss it. Finally, on September 28, 2005, Oklahoma State President David J. Schmidly wrote a letter to Drake President David Maxwell formally apologizing for the incident, calling it \"an ugly mark on Oklahoma State University and college football.\" The apology came twenty-two years after Bright's death.\n\nIn February 2006, the football field at Drake Stadium, in Des Moines, Iowa, was named in Bright's honor.\n\nIn September 2010, Johnny Bright School, a kindergarten through grade 9 school, was named in Bright's honour, and opened in the Rutherford neighbourhood of Edmonton. The school was officially opened on September 15 by representatives of the school district and Alberta Education Minister Dave Hancock, and included tributes from Bright's family, several dignitaries, and former colleagues of Bright from his both his athletic and educational careers.\n\n\n"}
{"id": "37514656", "url": "https://en.wikipedia.org/wiki?curid=37514656", "title": "List of Oslo Freedom Forum participants", "text": "List of Oslo Freedom Forum participants\n\nThe following is a list of persons who have attended the Oslo Freedom Forum. The list is categorized first by country of origin, then by profession and year.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMemory Banda (Malawian girls’ rights activist) (2017)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "50359060", "url": "https://en.wikipedia.org/wiki?curid=50359060", "title": "Marine Resources Committees", "text": "Marine Resources Committees\n\nThe Northwest Straits Marine Conservation Initiative of Washington State involves Marine Resources Committees (MRC) , located in seven counties; Whatcom, Skagit, Snohomish, Clallam, Island, Jefferson and San Juan. The first MRC was San Juan County in 1996, setting an example for others. A typical MRC has representation from local government, tribal government, the local port district, local business, and the scientific, recreational and conservation communities. Funding for the MRCs comes from federal, state, and local governments and non-profit groups. General operational funding is provided by the Northwest Straits Commission. Each MRC has a representative who sits on the Northwest Straits Commission board, making up the majority of the group.\nThese committees perform baseline studies of Northern Puget Sound and provide solutions for protection of the waters and their habitats. They also serve to carry out the overall mission of the Northwest Straits Marine Conservation Initiative.\n"}
{"id": "8983183", "url": "https://en.wikipedia.org/wiki?curid=8983183", "title": "Money", "text": "Money\n\nMoney is any item or verifiable record that is generally accepted as payment for goods and services and repayment of debts, such as taxes, in a particular country or socio-economic context. The main functions of money are distinguished as: a medium of exchange, a unit of account, a store of value and sometimes, a standard of deferred payment. Any item or verifiable record that fulfils these functions can be considered as money.\n\nMoney is historically an emergent market phenomenon establishing a commodity money, but nearly all contemporary money systems are based on fiat money. Fiat money, like any check or note of debt, is without use value as a physical commodity. It derives its value by being declared by a government to be legal tender; that is, it must be accepted as a form of payment within the boundaries of the country, for \"all debts, public and private\". Counterfeit money can cause good money to lose its value.\n\nThe money supply of a country consists of currency (banknotes and coins) and, depending on the particular definition used, one or more types of bank money (the balances held in checking accounts, savings accounts, and other types of bank accounts). Bank money, which consists only of records (mostly computerized in modern banking), forms by far the largest part of broad money in developed countries.\n\nThe word \"money\" is believed to originate from a temple of Juno, on Capitoline, one of Rome's seven hills. In the ancient world Juno was often associated with money. The temple of Juno Moneta at Rome was the place where the mint of Ancient Rome was located. The name \"Juno\" may derive from the Etruscan goddess Uni (which means \"the one\", \"unique\", \"unit\", \"union\", \"united\") and \"Moneta\" either from the Latin word \"monere\" (remind, warn, or instruct) or the Greek word \"moneres\" (alone, unique).\n\nIn the Western world, a prevalent term for coin-money has been \"specie\", stemming from Latin \"in specie\", meaning 'in kind'.\n\nThe use of barter-like methods may date back to at least 100,000 years ago, though there is no evidence of a society or economy that relied primarily on barter. Instead, non-monetary societies operated largely along the principles of gift economy and debt. When barter did in fact occur, it was usually between either complete strangers or potential enemies.\n\nMany cultures around the world eventually developed the use of commodity money. The Mesopotamian shekel was a unit of weight, and relied on the mass of something like 160 grains of barley. The first usage of the term came from Mesopotamia circa 3000 BC. Societies in the Americas, Asia, Africa and Australia used shell money – often, the shells of the cowry (\"Cypraea moneta L.\" or \"C. annulus L.\"). According to Herodotus, the Lydians were the first people to introduce the use of gold and silver coins. It is thought by modern scholars that these first stamped coins were minted around 650–600 BC.\n\nThe system of commodity money eventually evolved into a system of representative money. This occurred because gold and silver merchants or banks would issue receipts to their depositors – redeemable for the commodity money deposited. Eventually, these receipts became generally accepted as a means of payment and were used as money. Paper money or banknotes were first used in China during the Song dynasty. These banknotes, known as \"jiaozi\", evolved from promissory notes that had been used since the 7th century. However, they did not displace commodity money, and were used alongside coins. In the 13th century, paper money became known in Europe through the accounts of travelers, such as Marco Polo and William of Rubruck. Marco Polo's account of paper money during the Yuan dynasty is the subject of a chapter of his book, \"The Travels of Marco Polo\", titled \".\" Banknotes were first issued in Europe by Stockholms Banco in 1661, and were again also used alongside coins. The gold standard, a monetary system where the medium of exchange are paper notes that are convertible into pre-set, fixed quantities of gold, replaced the use of gold coins as currency in the 17th–19th centuries in Europe. These gold standard notes were made legal tender, and redemption into gold coins was discouraged. By the beginning of the 20th century almost all countries had adopted the gold standard, backing their legal tender notes with fixed amounts of gold.\n\nAfter World War II and the Bretton Woods Conference, most countries adopted fiat currencies that were fixed to the U.S. dollar. The U.S. dollar was in turn fixed to gold. In 1971 the U.S. government suspended the convertibility of the U.S. dollar to gold. After this many countries de-pegged their currencies from the U.S. dollar, and most of the world's currencies became unbacked by anything except the governments' fiat of legal tender and the ability to convert the money into goods via payment. According to proponents of modern money theory, fiat money is also backed by taxes. By imposing taxes, states create demand for the currency they issue.\n\nIn \"Money and the Mechanism of Exchange (1875)\", William Stanley Jevons famously analyzed money in terms of four functions: a \"medium of exchange\", a \"common measure of value\" (or unit of account), a \"standard of value\" (or standard of deferred payment), and a \"store of value\". By 1919, Jevons's four functions of money were summarized in the couplet:\n\nThis couplet would later become widely popular in macroeconomics textbooks. Most modern textbooks now list only three functions, that of medium of exchange, unit of account, and store of value, not considering a standard of deferred payment as a distinguished function, but rather subsuming it in the others.\n\nThere have been many historical disputes regarding the combination of money's functions, some arguing that they need more separation and that a single unit is insufficient to deal with them all. One of these arguments is that the role of money as a medium of exchange is in conflict with its role as a store of value: its role as a store of value requires holding it without spending, whereas its role as a medium of exchange requires it to circulate. Others argue that storing of value is just deferral of the exchange, but does not diminish the fact that money is a medium of exchange that can be transported both across space and time. The term \"financial capital\" is a more general and inclusive term for all liquid instruments, whether or not they are a uniformly recognized tender.\n\nWhen money is used to intermediate the exchange of goods and services, it is performing a function as a \"medium of exchange\". It thereby avoids the inefficiencies of a barter system, such as the \"coincidence of wants\" problem. Money's most important usage is as a method for comparing the values of dissimilar objects.\n\nA \"unit of account\" (in economics) is a standard numerical monetary unit of measurement of the market value of goods, services, and other transactions. Also known as a \"measure\" or \"standard\" of relative worth and deferred payment, a unit of account is a necessary prerequisite for the formulation of commercial agreements that involve debt.\n\nMoney acts as a standard measure and common denomination of trade. It is thus a basis for quoting and bargaining of prices. It is necessary for developing efficient accounting systems.\n\nWhile \"standard of deferred payment\" is distinguished by some texts, particularly older ones, other texts subsume this under other functions. A \"standard of deferred payment\" is an accepted way to settle a debt – a unit in which debts are denominated, and the status of money as legal tender, in those jurisdictions which have this concept, states that it may function for the discharge of debts. When debts are denominated in money, the real value of debts may change due to inflation and deflation, and for sovereign and international debts via debasement and devaluation.\n\nTo act as a \"store of value\", a money must be able to be reliably saved, stored, and retrieved – and be predictably usable as a medium of exchange when it is retrieved. The value of the money must also remain stable over time. Some have argued that inflation, by reducing the value of money, diminishes the ability of the money to function as a store of value.\n\nTo fulfill its various functions, money must have certain properties:\n\nIn economics, money is a broad term that refers to any financial instrument that can fulfil the functions of money (detailed above). These financial instruments together are collectively referred to as the money supply of an economy. In other words, the money supply is the number of financial instruments within a specific economy available for purchasing goods or services. Since the money supply consists of various financial instruments (usually currency, demand deposits and various other types of deposits), the amount of money in an economy is measured by adding together these financial instruments creating a \"monetary aggregate\".\n\nModern monetary theory distinguishes among different ways to measure the stock of money or money supply, reflected in different types of monetary aggregates, using a categorization system that focuses on the liquidity of the financial instrument used as money. The most commonly used monetary aggregates (or types of money) are conventionally designated M1, M2 and M3. These are successively larger aggregate categories: M1 is currency (coins and bills) plus demand deposits (such as checking accounts); M2 is M1 plus savings accounts and time deposits under $100,000; and M3 is M2 plus larger time deposits and similar institutional accounts. M1 includes only the most liquid financial instruments, and M3 relatively illiquid instruments. The precise definition of M1, M2 etc. may be different in different countries.\n\nAnother measure of money, M0, is also used; unlike the other measures, it does not represent actual purchasing power by firms and households in the economy. M0 is base money, or the amount of money actually issued by the central bank of a country. It is measured as currency plus deposits of banks and other institutions at the central bank. M0 is also the only money that can satisfy the reserve requirements of commercial banks.\n\nIn current economic systems, money is created by two procedures:\n\nLegal tender, or narrow money (M0) is the cash money created by a Central Bank by minting coins and printing banknotes.\n\nBank money, or broad money (M1/M2) is the money created by private banks through the recording of loans as deposits of borrowing clients, with partial support indicated by the cash ratio. Currently, bank money is created as electronic money.\n\nIn most countries, the majority of money is mostly created as M1/M2 by commercial banks making loans. Contrary to some popular misconceptions, banks do not act simply as intermediaries, lending out deposits that savers place with them, and do not depend on central bank money (M0) to create new loans and deposits.\n\n\"Market liquidity\" describes how easily an item can be traded for another item, or into the common currency within an economy. Money is the most liquid asset because it is universally recognised and accepted as the common currency. In this way, money gives consumers the freedom to trade goods and services easily without having to barter.\n\nLiquid financial instruments are easily tradable and have low transaction costs. There should be no (or minimal) spread between the prices to buy and sell the instrument being used as money.\n\nCurrently, most modern monetary systems are based on fiat money. However, for most of history, almost all money was commodity money, such as gold and silver coins. As economies developed, commodity money was eventually replaced by representative money, such as the gold standard, as traders found the physical transportation of gold and silver burdensome. Fiat currencies gradually took over in the last hundred years, especially since the breakup of the Bretton Woods system in the early 1970s.\n\nMany items have been used as commodity money such as naturally scarce precious metals, conch shells, barley, beads etc., as well as many other things that are thought of as having value. Commodity money value comes from the commodity out of which it is made. The commodity itself constitutes the money, and the money is the commodity. Examples of commodities that have been used as mediums of exchange include gold, silver, copper, rice, Wampum, salt, peppercorns, large stones, decorated belts, shells, alcohol, cigarettes, cannabis, candy, etc. These items were sometimes used in a metric of perceived value in conjunction to one another, in various commodity valuation or price system economies. Use of commodity money is similar to barter, but a commodity money provides a simple and automatic unit of account for the commodity which is being used as money. Although some gold coins such as the Krugerrand are considered legal tender, there is no record of their face value on either side of the coin. The rationale for this is that emphasis is laid on their direct link to the prevailing value of their fine gold content.\nAmerican Eagles are imprinted with their gold content and legal tender face value.\n\nIn 1875, the British economist William Stanley Jevons described the money used at the time as \"representative money\". Representative money is money that consists of token coins, paper money or other physical tokens such as certificates, that can be reliably exchanged for a fixed quantity of a commodity such as gold or silver. The value of representative money stands in direct and fixed relation to the commodity that backs it, while not itself being composed of that commodity.\n\nFiat money or fiat currency is money whose value is not derived from any intrinsic value or guarantee that it can be converted into a valuable commodity (such as gold). Instead, it has value only by government order (fiat). Usually, the government declares the fiat currency (typically notes and coins from a central bank, such as the Federal Reserve System in the U.S.) to be legal tender, making it unlawful not to accept the fiat currency as a means of repayment for all debts, public and private.\n\nSome bullion coins such as the Australian Gold Nugget and American Eagle are legal tender, however, they trade based on the market price of the metal content as a commodity, rather than their legal tender face value (which is usually only a small fraction of their bullion value).\n\nFiat money, if physically represented in the form of currency (paper or coins) can be accidentally damaged or destroyed. However, fiat money has an advantage over representative or commodity money, in that the same laws that created the money can also define rules for its replacement in case of damage or destruction. For example, the U.S. government will replace mutilated Federal Reserve Notes (U.S. fiat money) if at least half of the physical note can be reconstructed, or if it can be otherwise proven to have been destroyed. By contrast, commodity money which has been lost or destroyed cannot be recovered.\n\nThese factors led to the shift of the store of value being the metal itself: at first silver, then both silver and gold, and at one point there was bronze as well. Now we have copper coins and other non-precious metals as coins. Metals were mined, weighed, and stamped into coins. This was to assure the individual taking the coin that he was getting a certain known weight of precious metal. Coins could be counterfeited, but they also created a new unit of account, which helped lead to banking. Archimedes' principle provided the next link: coins could now be easily tested for their fine weight of metal, and thus the value of a coin could be determined, even if it had been shaved, debased or otherwise tampered with (see Numismatics).\n\nIn most major economies using coinage, copper, silver and gold formed three tiers of coins. Gold coins were used for large purchases, payment of the military and backing of state activities. Silver coins were used for midsized transactions, and as a unit of account for taxes, dues, contracts and fealty, while copper coins represented the coinage of common transaction. This system had been used in ancient India since the time of the Mahajanapadas. In Europe, this system worked through the medieval period because there was virtually no new gold, silver or copper introduced through mining or conquest. Thus the overall ratios of the three coinages remained roughly equivalent.\n\nIn premodern China, the need for credit and for circulating a medium that was less of a burden than exchanging thousands of copper coins led to the introduction of paper money, commonly known today as banknotes. This economic phenomenon was a slow and gradual process that took place from the late Tang dynasty (618–907) into the Song dynasty (960–1279). It began as a means for merchants to exchange heavy coinage for receipts of deposit issued as promissory notes from shops of wholesalers, notes that were valid for temporary use in a small regional territory. In the 10th century, the Song dynasty government began circulating these notes amongst the traders in their monopolized salt industry. The Song government granted several shops the sole right to issue banknotes, and in the early 12th century the government finally took over these shops to produce state-issued currency. Yet the banknotes issued were still regionally valid and temporary; it was not until the mid 13th century that a standard and uniform government issue of paper money was made into an acceptable nationwide currency. The already widespread methods of woodblock printing and then Pi Sheng's movable type printing by the 11th century was the impetus for the massive production of paper money in premodern China.\n\nAt around the same time in the medieval Islamic world, a vigorous monetary economy was created during the 7th–12th centuries on the basis of the expanding levels of circulation of a stable high-value currency (the dinar). Innovations introduced by Muslim economists, traders and merchants include the earliest uses of credit, cheques, promissory notes, savings accounts, transactional accounts, loaning, trusts, exchange rates, the transfer of credit and debt, and banking institutions for loans and deposits.\n\nIn Europe, paper money was first introduced in Sweden in 1661. Sweden was rich in copper, thus, because of copper's low value, extraordinarily big coins (often weighing several kilograms) had to be made. The advantages of paper currency were numerous: it reduced transport of gold and silver, and thus lowered the risks; it made loaning gold or silver at interest easier, since the specie (gold or silver) never left the possession of the lender until someone else redeemed the note; and it allowed for a division of currency into credit and specie backed forms. It enabled the sale of stock in joint stock companies, and the redemption of those shares in paper.\n\nHowever, these advantages held within them disadvantages. First, since a note has no intrinsic value, there was nothing to stop issuing authorities from printing more of it than they had specie to back it with. Second, because it increased the money supply, it increased inflationary pressures, a fact observed by David Hume in the 18th century. The result is that paper money would often lead to an inflationary bubble, which could collapse if people began demanding hard money, causing the demand for paper notes to fall to zero. The printing of paper money was also associated with wars, and financing of wars, and therefore regarded as part of maintaining a standing army. For these reasons, paper currency was held in suspicion and hostility in Europe and America. It was also addictive, since the speculative profits of trade and capital creation were quite large. Major nations established mints to print money and mint coins, and branches of their treasury to collect taxes and hold gold and silver stock.\n\nAt this time both silver and gold were considered legal tender, and accepted by governments for taxes. However, the instability in the ratio between the two grew over the course of the 19th century, with the increase both in supply of these metals, particularly silver, and of trade. This is called bimetallism and the attempt to create a bimetallic standard where both gold and silver backed currency remained in circulation occupied the efforts of inflationists. Governments at this point could use currency as an instrument of policy, printing paper currency such as the United States Greenback, to pay for military expenditures. They could also set the terms at which they would redeem notes for specie, by limiting the amount of purchase, or the minimum amount that could be redeemed.\nBy 1900, most of the industrializing nations were on some form of gold standard, with paper notes and silver coins constituting the circulating medium. Private banks and governments across the world followed Gresham's Law: keeping gold and silver paid, but paying out in notes. This did not happen all around the world at the same time, but occurred sporadically, generally in times of war or financial crisis, beginning in the early part of the 20th century and continuing across the world until the late 20th century, when the regime of floating fiat currencies came into force. One of the last countries to break away from the gold standard was the United States in 1971.\n\nNo country anywhere in the world today has an enforceable gold standard or silver standard currency system.\n\nCommercial bank money or demand deposits are claims against financial institutions that can be used for the purchase of goods and services. A demand deposit account is an account from which funds can be withdrawn at any time by check or cash withdrawal without giving the bank or financial institution any prior notice. Banks have the legal obligation to return funds held in demand deposits immediately upon demand (or 'at call'). Demand deposit withdrawals can be performed in person, via checks or bank drafts, using automatic teller machines (ATMs), or through online banking.\n\nCommercial bank money is created through fractional-reserve banking, the banking practice where banks keep only a \"fraction\" of their deposits in reserve (as cash and other highly liquid assets) and lend out the remainder, while maintaining the simultaneous obligation to redeem all these deposits upon demand. Commercial bank money differs from commodity and fiat money in two ways: firstly it is non-physical, as its existence is only reflected in the account ledgers of banks and other financial institutions, and secondly, there is some element of risk that the claim will not be fulfilled if the financial institution becomes insolvent. The process of fractional-reserve banking has a cumulative effect of money creation by commercial banks, as it expands money supply (cash and demand deposits) beyond what it would otherwise be. Because of the prevalence of fractional reserve banking, the broad money supply of most countries is a multiple larger than the amount of base money created by the country's central bank. That multiple (called the money multiplier) is determined by the reserve requirement or other financial ratio requirements imposed by financial regulators.\n\nThe money supply of a country is usually held to be the total amount of currency in circulation plus the total value of checking and savings deposits in the commercial banks in the country. In modern economies, relatively little of the money supply is in physical currency. For example, in December 2010 in the U.S., of the $8853.4 billion in broad money supply (M2), only $915.7 billion (about 10%) consisted of physical coins and paper money.\n\nThe development of computer technology in the second part of the twentieth century allowed money to be represented digitally. By 1990, in the United States all money transferred between its central bank and commercial banks was in electronic form. By the 2000s most money existed as digital currency in banks databases. In 2012, by number of transaction, 20 to 58 percent of transactions were electronic (dependant on country).\n\nNon-national digital currencies where developed in the early 2000s. In particular Flooz and Beenz, had gained momentum before the Dot-com bubble. Not much innovation occurred until the conception of Bitcoin in 2009, which introduced the concept of a cryptocurrency – a decentralised trustless currency.\n\nWhen gold and silver are used as money, the money supply can grow only if the supply of these metals is increased by mining. This rate of increase will accelerate during periods of gold rushes and discoveries, such as when Columbus discovered the New World and brought back gold and silver to Spain, or when gold was discovered in California in 1848. This causes inflation, as the value of gold goes down. However, if the rate of gold mining cannot keep up with the growth of the economy, gold becomes relatively more valuable, and prices (denominated in gold) will drop, causing deflation. Deflation was the more typical situation for over a century when gold and paper money backed by gold were used as money in the 18th and 19th centuries.\n\nModern day monetary systems are based on fiat money and are no longer tied to the value of gold. The control of the amount of money in the economy is known as monetary policy. Monetary policy is the process by which a government, central bank, or monetary authority manages the money supply to achieve specific goals. Usually the goal of monetary policy is to accommodate economic growth in an environment of stable prices. For example, it is clearly stated in the Federal Reserve Act that the Board of Governors and the Federal Open Market Committee should seek \"to promote effectively the goals of maximum employment, stable prices, and moderate long-term interest rates.\"\n\nA failed monetary policy can have significant detrimental effects on an economy and the society that depends on it. These include hyperinflation, stagflation, recession, high unemployment, shortages of imported goods, inability to export goods, and even total monetary collapse and the adoption of a much less efficient barter economy. This happened in Russia, for instance, after the fall of the Soviet Union.\n\nGovernments and central banks have taken both regulatory and free market approaches to monetary policy. Some of the tools used to control the money supply include:\n\nIn the US, the Federal Reserve is responsible for controlling the money supply, while in the Euro area the respective institution is the European Central Bank. Other central banks with significant impact on global finances are the Bank of Japan, People's Bank of China and the Bank of England.\n\nFor many years much of monetary policy was influenced by an economic theory known as monetarism. Monetarism is an economic theory which argues that management of the money supply should be the primary means of regulating economic activity. The stability of the demand for money prior to the 1980s was a key finding of Milton Friedman and Anna Schwartz supported by the work of David Laidler, and many others. The nature of the demand for money changed during the 1980s owing to technical, institutional, and legal factors and the influence of monetarism has since decreased.\n\nCounterfeit money is imitation currency produced without the legal sanction of the state or government. Producing or using counterfeit money is a form of fraud or forgery. Counterfeiting is almost as old as money itself. Plated copies (known as Fourrées) have been found of Lydian coins which are thought to be among the first western coins. Before the introduction of paper money, the most prevalent method of counterfeiting involved mixing base metals with pure gold or silver. A form of counterfeiting is the production of documents by legitimate printers in response to fraudulent instructions. During World War II, the Nazis forged British pounds and American dollars. Today some of the finest counterfeit banknotes are called \"Superdollars\" because of their high quality and likeness to the real U.S. dollar. There has been significant counterfeiting of Euro banknotes and coins since the launch of the currency in 2002, but considerably less than for the U.S. dollar.\n\nMoney laundering is the process in which the proceeds of crime are transformed into ostensibly legitimate money or other assets. However, in a number of legal and regulatory systems the term money laundering has become conflated with other forms of financial crime, and sometimes used more generally to include misuse of the financial system (involving things such as securities, digital currencies, credit cards, and traditional currency), including terrorism financing, tax evasion, and evading of international sanctions.\n\n\n"}
{"id": "2838331", "url": "https://en.wikipedia.org/wiki?curid=2838331", "title": "Muda (Japanese term)", "text": "Muda (Japanese term)\n\nFrom an end-customer's point of view, value-added work is any activity that produces goods or provides a service for which a customer is willing to pay; \"muda\" is any constraint or impediment that causes waste to occur.\n\nThere are two types of muda:\n\n\nOne of the key steps in lean process and TPS is to identify which activities add value and which do not, then to progressively work to improve or eliminate them. \n\nTaiichi Ohno, \"father\" of the Toyota Production System, originally identified seven forms of \"muda\" or waste:\nLater, an eighth waste, unused skills, was added.\n\nA mnemonic may be useful for remembering the categories of waste, such as TIM WOOD or TIM WOODS: \n\nEvery time a product is touched or moved unnecessarily there is a risk that it could be damaged, lost, delayed, etc. as well as being a cost for no added value. Transportation does not add value to the product, i.e. is not a transformation for which the consumer is willing to pay.\n\nWhether in the form of raw materials, work-in-progress (WIP), or finished goods, represents a capital outlay that cannot yet produce an income. The longer a product sits in one of these states, the more it contributes to waste. The smooth, continuous flow of work through each process ensures excess amounts of inventory are minimized. \n\nIn contrast to transportation, which refers to damage and transaction costs associated with moving the product, motion refers to the damage and costs inflicted on what creates the product. This can include wear and tear for equipment, repetitive strain injuries for workers, or unnecessary downtime.\n\nWhenever the product is not in transportation or being processed, it is waiting (typically in a queue). In traditional processes, a large part of an individual product's life is spent waiting to be worked on. \n\nMaking more of a product than is required results in several forms of waste, typically caused by production in large batches. The customer's needs often change over the time it takes to produce a larger batch. Over-production has been described as the worst kind of waste.\n\nDoing more to a product than is required by the end-customer results in it taking longer and costing more to produce. This also includes using components that are more precise, complex, expensive or higher quality than absolutely required.\n\nHaving to discard or rework a product due to earlier defective work or components results in additional cost and delays.\n\nOrganizations often under-utilize the skills their workers have or permit workers to operate in silos so that knowledge is not shared. This was added to the original seven forms of waste, as resolving this waste is a key enabler to resolving the others.\n\nThe eight forms of waste were developed for Toyota specific processes. Companies have further identified such other forms of waste as:\n\nGeneral uncertainty about the right thing to do, or absence of documented procedures and operating statements.\n\nWriter Jim Womack described \"thinking you can't\" as the worst form of waste, quoting Henry Ford's aphorism:\n\nShigeo Shingo divides process related activity into Process and Operation. He distinguishes \"Process\", the course of material that is transformed into product, from \"Operation\" which are the actions performed on the material by workers and machines. This distinction is not generally recognized because most people would view the \"Operations\" performed on the raw materials of a product by workers and machines as the \"Process\" by which those raw materials are transformed into the final product. He makes this distinction because value is added to the product by the process but not by most of the operations. He states that whereas many see Process and Operations in parallel he sees them at right angles (orthogonal) (see Value Stream Mapping). This starkly throws most of the operations into the waste category.\n\nMany of the TPS/Lean techniques work in a similar way. By planning to reduce manpower, or reduce change-over times, or reduce campaign lengths, or reduce lot sizes the question of waste comes immediately into focus upon those elements that prevent the plan being implemented. Often it is in the operations' area rather than the process area that muda can be eliminated and remove the blockage to the plan. Tools of many types and methodologies can then be employed on these wastes to reduce or eliminate them.\n\nThe plan is therefore to build a fast, flexible process where the immediate impact is to reduce waste and therefore costs. By ratcheting the process towards this aim with focused muda reduction to achieve each step, the improvements are 'locked in' and become required for the process to function. Without this intent to build a fast, flexible process there is a significant danger that any improvements achieved will not be sustained because they are \"just\" desirable and can slip back towards old behaviours without the process stopping.\n\nThe word was brought across cultures as the \"catchword\" of Dio Brando, a central antagonist from the popular, long running manga series JoJo's Bizarre Adventure. The word would go on to be repeated rapidly as the stand cry of Dio and later his son, Giorno Giovanna.\n\n\n"}
{"id": "45626878", "url": "https://en.wikipedia.org/wiki?curid=45626878", "title": "Mutual combat", "text": "Mutual combat\n\nMutual combat, a term commonly used in US courts, occurs when two individuals intentionally and consensually engage in a fair fight, while not hurting bystanders or damaging property. There is not an official law that forbids mutual combat in the United States. There have been numerous cases where this concept was successfully used in defense of the accused. In some cases, mutual combat may nevertheless result in killings.\n\nIn 2012, MMA fighter Phoenix Jones hit the headlines for engaging in mutual combat. A video of the fight went viral. The Seattle Police Department later defended their officers for not intervening. The same year, Gabriel Aubry and Olivier Martinez engaged in mutual combat and were not charged. In 2014, after Zac Efron had engaged in a fight in Skid Row, law enforcement officials did not make any arrests because they viewed it as mutual combat. Mutual combat has been used to deny damage claims, as a legal defense, and to drop charges against fighting students.\n\n\n"}
{"id": "53531675", "url": "https://en.wikipedia.org/wiki?curid=53531675", "title": "Official Information Act 1997", "text": "Official Information Act 1997\n\nThe Official Information Act 1997 () is a Thai act which guarantees the people's right to have full access to government information. It was approved by the Thai National Assembly in July 1997 and entered into application on 8 December 1997.\n\nThe constitutions of 1991 and 1997 had already recognised the right to information. The Official Information Act was passed at the same time as the 1997 Constitution, and was designed to formally define the conditions of access to information. The Act was the culmination of a period of political reform which started in 1992 with the popular protest in Bangkok against the government of General Suchinda Kraprayoon, and was precipitated by Thailand’s economic crisis in 1997.\n\nAt the beginning of the latter, the government perpetuated the tradition of non-disclosure towards the public. When the gravity of the crisis became evident, its lack of credibility, especially after the seriousness of the crisis became apparent, an IMF intervention was welcomed since it was considered as a way to guarantee to the Thai public relevant information on the scope of the crisis and on official actions to limit its consequences.\n\nThis chain of events led to the adoption of an official Act in 1997. According to Alexandru Grigorescu, the act was less the result of IMF demands for transparency than a signal by the Thai government that future information it offered would be verifiable.\n\nThailand was the first country in South East Asia to implement a FOI law.\n\nThe Official Information Act guarantees people’s rights regarding official information (defined under section 4 as “information in possession or control of a State agency, whether it is the information relating to the operation of the State or the information relating to a private individual”), including the right to get advice (section 7), to inspect, to request a copy (section 9), and to ask the state to correct or change personal information (section 25).\n\nThe law functions essentially through three mechanisms :\nThe act established the Official Information Board (OIB), the leader of which is to be appointed by the Prime Minister, and which is responsible for the implementation of the act.\n\nA global principle states that state agencies are not to release information unless they are convinced that the request is necessary for the protection of the rights and liberties of an individual person or is beneficial to the public (section 11).\n\nOne absolute exemption exists: official information which could harm the interests of the Thai monarchy can never be disclosed (section 14).\n\nOther exemptions exist, although they are not absolute, and thus may or may not be a motive for the prohibition of the disclosure of official information. They are listed under section 15 and include information which may jeopardize the national security or international relations, information which may endanger the life or safety of a person, and medical reports. A Royal Decree can at any time add an element to the list of exemptions.\n\nIf the disclosure of official information is refused, the requester can, invoking section 18, either file a complaint at the OIB, or appeal through the OIB to the Information Disclosure Tribunal, which is composed of non-civil servants, and they have a total of 15 days to do so after they have received an order prohibiting the disclosure. An appeal to this tribunal is final (except when a further appeal is submitted to the administrative court).\n\nA case involving Kasetsart University Demonstration School is often cited as a reference, and one of the first successful cases putting the Official Information Act in practice. In 1998, Sumalee Limpa-ovart requested to see the examination sheets and marks of the entrance test, after her daughter failed the test. Her request was rejected, and she filed a complaint at the OIB, which failed. She then appealed to the Information Disclosure Tribunal, which concluded that all the exam papers and marks were to be considered as official information. After having accessed the sheets, she realized that candidates with lower marks than her daughter had been admitted because of family connections. Consequently, Thailand's Ministry of University Affairs ordered a massive reform of admission procedures in the country. According to Andrew Harding and Peter Leyland, the case contributed greatly to the education system of the country.\n\nLess successful applications were made to gain access to an official report on the popular protests of 1992 by newspapers and families whose relatives were killed during the events. After months of no response, the Ministry of Defence eventually released a report from 1992. However, it was, according to many of the victims' families, a “whitewash of events”, and the requesters were threatened of prosecution should the information be disseminated more widely.\n\nTwo years after its implementation, in the concluding remarks of his report on the act, Nakorn Serirak argued that it was “a crucial component of democratic development as it encourages people to enjoy more political participation by directly expressing their opinions and proposing their needs or suggestions to the state”. More than half a million Thais utilized the Official Information Act in its first three years.\n\nIn 2000, following the creation of the OIB, the Bank of Thailand established an office to provide the public with access to financial and economic information.\n\nThe Thai government proclaimed 2002 the Year of Access to Official Information.\n\nDespite a relatively high amount of requests, a lack of public awareness of the act is often deplored (and the fact that a majority of these requests are made by citizens, not by journalists).\n\nA forum entitled “14 years of Thailand’s Official Information Act: Time for a Change” was organized in 2011 by the Southeast Asian Press Alliance (SEAPA). Thienchai Na Nakorn, Thailand’s Information Commissioner, acknowledged the existence of a culture of fear in the civil service that discouraged officials from providing information to the public, and that a reform of the act was necessary.\n\nAnother issue often raised regards time periods. When it has received a request, a State agency must reply “within a reasonable period of time” (section 11). This period of time is not defined precisely in the act, however Article 19’s report on the state of Thailand’s FOI law indicates that the term has been “interpreted rather liberally by State officials”. Their research has found that delays of months followed by a refusal to release without an explanation are not uncommon.\n\nThai journalist Kavi Chongkittavorn has published an article in The Nation (although it is not available on the newspaper's website anymore) also asking for a reform of the act, describing it as a “tale of disappointment, betrayal and procrastination”. He criticizes the fact that the OIB is, under section 27, under the control of the Prime Minister’s Office – and thus, part of the bureaucracy – rather than being an independent body like the Tribunal. In fact, the Board or its members do not have any specific duty to act independently.\n\nChongkittavorn also deplores the coordinated adaptation of state agencies to the implementation of the act : “After some of the big fish were caught red-handed in the first three years, all government agencies were up in arms against the OIA in order to shield them from further disclosure and possibly prosecution. Understanding was reached among them that information disclosure would be at a snail’s pace to dissuade the public from using the OIA. Apparently, this strategy has been working unchallenged.”\n"}
{"id": "44002741", "url": "https://en.wikipedia.org/wiki?curid=44002741", "title": "Perspective-taking", "text": "Perspective-taking\n\nPerspective-taking is the act of perceiving a situation or understanding a concept from an alternative point of view, such as that of another individual. There is a vast amount of scientific literature that has looked at perspective-taking and suggests that it is crucial to human development, and that it may lead to a variety of beneficial outcomes. Perspective-taking is related to other theories and concepts including theory of mind and empathy. Both theory and research have suggested ages when children are able to begin to perspective-take and how that ability develops over time. Research has also suggested that there may be deficits in people with attention deficit hyperactivity disorder and autism on the ability of individuals to engage in perspective-taking. Additionally, studies have been conducted to assess the brain regions involved in perspective-taking. These studies suggest that several regions may be involved, including the prefrontal cortex and the precuneus. Additionally, studies suggest that perspective-taking may be possible in some non-human animals.\n\nPerspective-taking is the process by which an individual views a situation from another's point-of-view. Within the scientific literature, perspective-taking has been defined along two dimensions: perceptual and conceptual. Perceptual perspective-taking is defined as the ability to understand how another person experiences things through their senses (i.e. visually or auditorily). Most of this literature has focused on visual perspective-taking: the ability to understand the way another person sees things in physical space. Conceptual perspective-taking is defined as the ability to comprehend and take on the viewpoint of another person's psychological experience (i.e. thoughts, feelings and attitudes). For instance, one can visualize the viewpoint of a taller individual (perceptual/visual) or reflect upon another's point of view on a particular concept (conceptual).\n\nTheory of mind is the awareness that people have individual psychological states that differ from one another. Within perspective-taking literature, the term perspective-taking and theory of mind are sometimes used interchangeably and some studies use theory of mind tasks in order to test if someone is engaging in perspective-taking. Some research, however, has highlighted that the two concepts are related but different, with theory of mind being the recognition that another person has different thoughts and feelings and perspective-taking being the ability to take on that other person’s point of view.\n\nEmpathy has been defined as the ability for someone to share the same emotions another person is having. Empathy and perspective-taking have been studied together in a variety of ways. Within the scientific literature, there are not always clear lines of distinction between empathy and perspective-taking, and the two concepts are often studied in conjunction with one another and viewed as related and similar concepts. Some research has distinguished the two concepts and pointed out their differences, while other literature has theorized that perspective-taking is one component of empathy.\n\nPrevious studies have assessed the age at which humans are capable of visual perspective-taking. Various studies within the literature have drawn different conclusions.\n\nIn 1956, Jean Piaget and Bärbel Inhelder conducted a study to assess the visual perspective-taking abilities of young children which has come to be known as the three mountain problem. This study found that by the ages of 9-10, children were able to successfully complete the three mountain problem and seemed able to understand that when someone is standing in a different location (i.e. on a different mountain top) they would have a different view. However, children ages 8 and under struggled with this task.\n\nSince this classic study, a number of studies have suggested that visual perspective-taking may be possible earlier than the age of 9. For example, a study that used a different method to assess visual perspective-taking suggested that children may be able to successfully visually perspective-take by the age of 4.5 years old. In this study, 4.5 year old children were able to understand that someone sitting closer to a picture would have a better view of that picture. However, these researchers found that children who were 3 and 3.5 years old struggled with this task which led them to conclude that the age range of 3 to 4.5 years old could be crucial in perspective-taking development.\n\nAdditionally, developmental psychologist John H. Flavell suggested that there are two levels of visual perspective-taking that emerge as children develop. Level 1 perspective-taking is defined as the ability to understand that someone else may see things differently and what another person can see in physical space. For example, one could understand that, while an object may be obstructing their own view, from where another person is standing they can see a cat in the room. Level 2 perspective-taking, however, is defined as the understanding that another person can see things differently in physical space and how those objects are organized from that other person’s point of view. For example, a person can understand that from another person’s point of view they can see a dog to the right but from their own point of view the dog is to the left.\n\nStudies have since been done to examine when children are able to demonstrate level 1 and level 2 perspective-taking. These studies have shown that children at 24 months old and 14 months old may be able to engage in level 1 perspective-taking. Research also suggests that children can engage in level 2 perspective-taking as early as 2.5 years old.\n\nStudies have also suggested that visual perspective-taking ability improves from childhood to adulthood. For example, in comparing 6-year-olds, 8-year-olds, 10-year-olds and adults (averaging at 19 years of age) researchers found that as people’s age increased, visual perspective-taking tasks could be done with more accuracy and speed.\n\nIn Piaget's theory of cognitive development, he suggests that perspective-taking begins in the concrete operational stage (third stage) which ranges from ages 7–12. It is within this stage that the idea of decentration is introduced as a cognitive ability. Decentration was defined as the ability to take into account the way others perceive various aspects of a given situation.\n\nAnother developmental perspective-taking theory was created by Robert L. Selman and entitled social perspective-taking theory (also known as Role-taking theory). This theory suggests that there are five developmental stages involved in perspective-taking ranging from ages 3–6 (characterized by egocentrism or an inability to think of things from another’s point of view) to teenagers and adults (where people can understand another person’s point of view and this understanding is informed by recognizing another person’s environment and culture). The theory suggests that as humans age from childhood to adulthood their ability to perspective-take improves. Studies by Selman and colleagues suggest that children are able to perspective-take in different ways at different ages.\n\nOther studies assess that children can begin to take on the viewpoint of another person considering their feelings, thoughts and attitudes as 4-years-olds.\n\nVisual perspective-taking studies that focus on brain regions are generally performed by collecting functional magnetic resonance imaging (fMRI) data while participants perform perspective-taking tasks. For example, a participant may be shown a picture of another person with objects around them and asked to take on the viewpoint of that person and indicate the number of objects they see (Level 1 visual perspective-taking) and if the objects are located to the right or left of the other person (level 2 visual perspective-taking). While the participant is completing this task they are also having an fMRI scan.\n\nA meta-analysis that looked at existing fMRI research on visual perspective-taking as of 2013 suggested that several areas of the brain have clustered activation during these perspective-taking tasks. These areas included the left prefrontal cortex, the precuneus, and the left cerebellum. Studies suggest these areas of the brain are involved in decision making, visual imagery, and attention respectively.\n\nResearch assessing the brain regions involved in conceptual perspective-taking also suggests that multiple brain areas are potentially involved. Studies have been conducted by administering a positron emission tomography (PET) scan and asking participants to engage in perspective-taking tasks. For example, in one study, participants who were all medical students were asked to consider the knowledge base someone who was not in the medical field would have on a list of medical questions.\n\nStudies have suggested that regions that are activated during cognitive perspective-taking include the right parietal lobe and the posterior cingulate cortex posterior cingulate cortex among others. The literature also points out that some areas seem to be involved both when people imagine themselves and when they imagine the perspective of others. For example, when participants were asked to imagine themselves engaging in an activity versus imagining another person engaging in that activity the precuneus and the supplementary motor area (SMA) were activated, suggesting visual imagery and motor movement thoughts were involved in both tasks.\n\nResearch has highlighted that it may be more difficult for children with attention deficit hyperactivity disorder (ADHD) to perspective-take. ADHD research has shown that children with this diagnosis have shown impairments in attention and communication. Perspective-taking research has found that children with ADHD have a harder time taking on the viewpoint of others than children who do not have ADHD.\n\nThere is evidence to suggest that children with autism may be able to engage in visual perspective-taking but may have difficulty engaging in conceptual perspective-taking. For example, a study that compared perspective-taking scores in children who had been diagnosed with autism as compared to children who did not have this diagnosis found no significant difference in scores on level 1 and level 2 visual perspective-taking. However, the study found it was much harder for autistic children to engage in conceptual perspective-taking tasks.\n\nSome studies have been done to explore potential interventions that could help improve perspective-taking abilities in children with autism. These studies suggested that the use of video may be helpful in teaching perspective-taking skills in children with autism. For example, an intervention study with autistic children, found that showing the children a video of someone engaging in perspective-taking tasks and explaining their actions led to improved perspective-taking ability.\n\nAn abundance of literature has linked perspective-taking abilities with other behaviors. Much of this literature specifically focuses on perceptual perspective-taking (or taking on the viewpoint of another person's thoughts, feelings and attitudes).\n\nMany studies have associated perspective-taking with empathy. Psychologist Mark Davis suggested that empathy consists of multiple dimensions. To assess this, Davis developed the Interpersonal Reactivity Index (IRI). The IRI consists of four subscales: fantasy, empathic concern, personal distress, and perspective-taking. The perspective-taking subscale asks participants to report how likely they are to engage in trying to see things from another person’s point of view. Studies using this widely cited measure have found that perspective-taking is associated with many prosocial behaviors. One study, which assessed cross-cultural data in 63 countries using the IRI, concluded that perspective-taking and empathic concern was associated with volunteerism and agreeableness as well as self-esteem and life satisfaction.\n\nAdditionally, research has suggested that perspective-taking leads to empathic concern. This research further suggests that in looking at perspective-taking and empathy it is important to distinguish between two different types of perspective-taking. The research posits that there is a difference between thinking of how one would act, feel and behave if placed in someone else’s situation and thinking of the way that another person thinks, feels, and behaves in their own situation. The results of this research reveals that thinking of how another person behaves and feels in their own situation leads to feelings of empathy. However, thinking of how one would behave in another person’s situation leads to feelings of empathy as well as distress.\n\nResearch has also found that in interactions involving negotiations, taking on the perspective of another person and empathizing with them may have differential outcomes. One study found that people who engaged in perspective-taking were more effective in making a deal with another person and finding innovative agreements that satisfied both parties as compared to those who empathized with someone else.\n\nResearch has revealed that perspective-taking was associated with sympathy toward others and prosocial behavior in children as young as 18 months old. Another study looking at sibling interactions found that toddlers who were older siblings were more likely to help take care of their younger siblings when they demonstrated higher perspective-taking abilities.\n\nPerspective-taking has also been associated with creativity. For example, perspective-taking has been found to increase the amount of creative ideas generated in team activities. Another study suggested that perspective-taking could lead to more creative and innovative ideas particularly in participants who were internally driven to complete a task.\n\nMany studies within perspective-taking literature has focused on the potential effects of perspective-taking on the perceptions of outgroup members and has found that there are many potential benefits to perspective-taking. Literature on perspective-taking and bias and stereotyping is generally done by asking participants to take the perspective of another person who is different from them in certain domains (i.e. asking young adult participants to take on the perspective of an elderly person or asking White participants to take on the perspective of a Black person as seen in a photograph or video). These studies have shown that perspective-taking can lead to reduced stereotyping of outgroup members, improved attitudes towards others, and increased helping behavior of outgroup members. Research also suggests that perspective-taking can lead to a reduction of in-group favoritism. Additionally, research that focused on implicit (or unconscious) biases found that perspective-taking can lead to reduced implicit bias scores (as measured by the Implicit-association test) as well as more recognition of subtle discrimination.\n\nResearch has looked at the potential differences that could arise when one is having a conversation with another person whom they agree with versus having a conversation with someone with whom they disagree. This research found that participants who interacted with people with whom they disagreed had enhanced perspective-taking ability and could better remember the conversation.\n\nSome researchers have suggested that there may be some drawbacks to perspective-taking. For example, studies have found that asking people to engage in perspective-taking tasks can lead to increased stereotyping of the target if the target is deemed as having more stereotypic qualities and adopting stereotypic behaviors of outgroup members.\n\nAlthough studies have been done to assess if nonhuman animals are able to successfully engage in perspective-taking the literature has not drawn consistent conclusions. Many of these studies assess perspective-taking by training animals on specific tasks or by measuring the consistency of animals to follow the eye gaze of humans. Researchers highlight that being able to successful follow another's eye gaze could indicate that the animal is aware that the human is seeing and paying attention to something that is different from what they see.\n\nOne study that assessed the perspective-taking abilities in spider monkeys and capuchin monkeys found that these primates successfully performed eye gazing tasks which led researchers to conclude that the monkeys demonstrated some ability to consider another person’s viewpoint. However, another study that utilized an eye gazing method in assessing perspective-taking found that Rhesus monkeys were unsuccessful at eye gazing tasks.\n\nOther studies suggest that dogs have complex social understanding. One study assessed the potential for perspective-taking in dogs by telling a dog that they were not allowed to eat a treat and then placing the food in a location that the dog could reach. These researchers found that dogs were more likely to eat the treat after being instructed not to if there was a barrier that hid the dog from the instructor. Additionally, dogs were less likely to eat the treat if the barrier was of smaller size or had a window in it. However, this study also showed that dogs struggled in other tasks that focused on the dog's own visual attention. These researchers suggest that this study provides evidence that dogs may be aware of other's visual perspectives.\n"}
{"id": "2948657", "url": "https://en.wikipedia.org/wiki?curid=2948657", "title": "Physical computing", "text": "Physical computing\n\nPhysical computing means building interactive physical systems by the use of software and hardware that can sense and respond to the analog world. While this definition is broad enough to encompass systems such as smart automotive traffic control systems or factory automation processes, it is not commonly used to describe them. In a broader sense, physical computing is a creative framework for understanding human beings' relationship to the digital world. In practical use, the term most often describes handmade art, design or DIY hobby projects that use sensors and microcontrollers to translate analog input to a software system, and/or control electro-mechanical devices such as motors, servos, lighting or other hardware.\n\nPhysical Computing intersects the range of activities often referred to in academia and industry as electrical engineering, mechatronics, robotics, computer science, and especially embedded development.\n\nPhysical computing is used in a wide variety of domains and applications.\n\nThe advantage of physicality in education and playfulness has been reflected in diverse informal learning environments. The Exploratorium, a pioneer in inquiry based learning, developed some of the earliest interactive exhibitry involving computers, and continues to include more and more examples of physical computing and tangible interfaces as associated technologies progress.\n\nIn the art world, projects that implement physical computing include the work of Scott Snibbe, Daniel Rozin, Rafael Lozano-Hemmer, Jonah Brucker-Cohen, Camille Utterback, Virtual Reality VR/shyam, Augmented Reality AR/hiren, and Electroland LED art.\n\nPhysical computing practices also exist in the product and interaction design sphere, where hand-built embedded systems are sometimes used to rapidly prototype new digital product concepts in a cost-efficient way. Firms such as IDEO and Teague are known to approach product design in this way.\n\nCommercial implementations range from consumer devices such as the Sony Eyetoy or games such as Dance Dance Revolution to more esoteric and pragmatic uses including machine vision utilized in the automation of quality inspection along a factory assembly line. Exergaming can be considered a form of physical computing. Other implementations of physical computing include voice recognition, which senses and interprets sound waves via microphones or other soundwave sensing devices, and computer vision, which applies algorithms to a rich stream of video data typically sensed by some form of camera. Haptic interfaces are also an example of physical computing, though in this case the computer is \"generating\" the physical stimulus as opposed to \"sensing\" it. Both motion capture and gesture recognition are fields that rely on computer vision to work their magic.\n\nPhysical computing can also describe the fabrication and use of custom sensors or collectors for scientific experiments, though the term is rarely used to describe them as such. An example of physical computing modeling is the \"Illustris project\", which attempts to precisely simulate the evolution of the universe from the Big Bang to the present day, 13.8 billion years later.\n\nPrototyping plays an important role in Physical Computing. Tools like the Wiring, Arduino and Fritzing as well as I-CubeX help designers and artists to quickly prototype their interactive concepts.\n\n"}
{"id": "14474114", "url": "https://en.wikipedia.org/wiki?curid=14474114", "title": "Probabilistic causation", "text": "Probabilistic causation\n\nProbabilistic causation is a concept in a group of philosophical theories that aim to characterize the relationship between cause and effect using the tools of probability theory. The central idea behind these theories is that causes raise the probabilities of their effects, all else being equal.\n\nInterpreting causation as a deterministic relation means that if \"A\" causes \"B\", then \"A\" must \"always\" be followed by \"B\". In this sense, war does not cause deaths, nor does smoking cause cancer. As a result, many turn to a notion of probabilistic causation. Informally, \"A\" probabilistically causes \"B\" if \"A\"'s occurrence increases the probability of \"B\". This is sometimes interpreted to reflect imperfect knowledge of a deterministic system but other times interpreted to mean that the causal system under study has an inherently indeterministic nature. (Propensity probability is an analogous idea, according to which probabilities have an objective existence and are not just limitations in a subject's knowledge).\n\nPhilosophers such as Hugh Mellor and Patrick Suppes have defined causation in terms of a cause preceding and increasing the probability of the effect. (Additionally, Mellor claims that cause and effect are both facts - not events - since even a non-event, such as the failure of a train to arrive, can cause effects such as my taking the bus. Suppes, by contrast, relies on events defined set-theoretically, and much of his discussion is informed by this terminology.)\n\nPearl argues that the entire enterprise of probabilistic causation has been misguided from the very beginning, because the central notion that causes \"raise the probabilities\" of their effects cannot be expressed in the language of probability theory. In particular, the inequality \"Pr(effect | cause) > Pr(effect | ~cause)\" which philosophers invoked to define causation, as well as its many variations and nuances, fails to capture the intuition behind \"probability raising\", which is inherently a manipulative or counterfactual notion. \nThe correct formulation, according to Pearl, should read:<br>\nwhere \"do(C)\" stands for an external intervention that compels the truth of \"C\". The conditional probability \"Pr(E | C)\", in contrast, represents a probability resulting from a passive observation of \"C\", and rarely coincides with \"Pr(E | do(C))\". Indeed, observing the barometer falling increases the probability of a storm coming, but does not\n\"cause\" the storm; were the act of manipulating the barometer to change the probability of storms, the falling barometer would qualify as a cause of storms. In general, formulating the notion of \"probability raising\" within the calculus of \"do\"-operators resolves the difficulties that probabilistic causation has encountered in the past half-century, among them the infamous Simpson's paradox, and clarifies precisely what relationships exist between probabilities and causation.\n\nThe establishing of cause and effect, even with this relaxed reading, is notoriously difficult, expressed by the widely accepted statement \"Correlation does not imply causation\". For instance, the observation that smokers have a dramatically increased lung cancer rate does not establish that smoking must be a \"cause\" of that increased cancer rate: maybe there exists a certain genetic defect which both causes cancer and a yearning for nicotine; or even perhaps nicotine craving is a symptom of very early-stage lung cancer which is not otherwise detectable. Scientists are always seeking the exact mechanisms by which Event \"A\" produces Event \"B\". But scientists also are comfortable making a statement like, \"Smoking probably causes cancer,\" when the statistical correlation between the two, according to probability theory, is far greater than chance. In this dual approach, scientists accept both deterministic and probabilistic causation in their terminology.\n\nIn statistics, it is generally accepted that observational studies (like counting cancer cases among smokers and among non-smokers and then comparing the two) can give hints, but can never \"establish\" cause and effect. Often, however, qualitative causal assumptions (e.g., absence of causation between some variables) may permit the derivation of consistent \ncausal effect estimates from observational studies.\n\nThe gold standard for causation here is the \"randomized experiment\": take a large number of people, randomly divide them into two groups, force one group to smoke and prohibit the other group from smoking, then determine whether one group develops a significantly higher lung cancer rate. Random assignment plays a crucial role in the inference to causation because, in the long run, it renders the two groups equivalent in terms of all other possible effects on the outcome (cancer) so that any changes in the outcome will reflect only the manipulation (smoking). Obviously, for ethical reasons this experiment cannot be performed, but the method is widely applicable for less damaging experiments. One limitation of experiments, however, is that whereas they do a good job of testing for the presence of some causal effect they do less well at estimating the size of that effect in a population of interest. (This is a common criticism of studies of safety of food additives that use doses much higher than people consuming the product would actually ingest.)\n\nIn a closed system the data may suggest that cause A * B precedes effect C in a defined interval of time τ. This relationship can determine causality with confidence bounded by τ. However, this same relationship may not be deterministic with confidence in an open system where uncontrolled factors may affect the result.\n\nAn example would be a system of A, B and C, where A, B and C are known. Characteristics are below and limited to a given time (such as 50 ms, or 50 hours):\n\n^A * ^ B => ^ C (99.9999998027%)\n\nA * ^B => ^C (99.9999998027%)\n\n^A * B => ^C (99.9999998027%)\n\nA * B => C (99.9999998027%)\n\nOne can reasonably claim, within 6 Standard Deviations, that A * B cause C given the time boundary (such as 50 ms, or 50 hours) IF And Only IF A, B and C are the only parts of the system in question. Any result outside of this may be considered a deviation.\n"}
{"id": "33456439", "url": "https://en.wikipedia.org/wiki?curid=33456439", "title": "Prospection", "text": "Prospection\n\nIn psychology, prospection is the generation and evaluation of mental representations of possible futures. The term therefore captures a wide array of future-oriented psychological phenomena, including the prediction of future emotion (affective forecasting), the imagination of future scenarios (episodic foresight), and planning. Prospection is central to various aspects of human cognition and motivation. Daniel Gilbert (psychologist) and Timothy Wilson coined the term in 2007. It has since become a central area of enquiry in the cognitive sciences \n\nEven fundamental learning processes are, in some sense, forms of prospection . Associative learning enables individual animals to track local regularities in their environments and adapt their behaviour accordingly, in order to maximise their chances of positive outcomes and minimise risks. Animals that are capable of positive and negative states (for example pleasure and pain) can eventually learn about the consequences of their actions and thereby predict imminent rewards and punishments before they occur. This enables animals to change their current actions accordingly in line with prospective consequences. \n\nMental time travel refers to the ability to mentally reconstruct personal events from the past (known as episodic memory), as well as to imagine personal future events (known as episodic foresight). Mental time travel into the future (episodic foresight or episodic future thinking) is therefore one of several types of ‘prospection’ that refers to the capacity to simulate or imagine personal future events . \n\nEpisodic foresight is the capacity to imagine personal future scenarios and shape current action accordingly . \n\nThe feelings evoked during episodic foresight enable people to infer how they would really feel if the event were to happen in reality. This thereby enables people to anticipate whether future events are desirable or undesirable, and ability called ‘affective forecasting’. \n\nSimulating the future enables people to create intentions for future actions. Prospective memory is the form of memory that involves remembering to perform these planned intentions, or to recall them at some future point in time . Prospective memory tasks are common in everyday life, ranging from remembering to post a letter to remembering to take one’s medication. \n\nPeople anticipate that it is possible to shape their future self. To acquire new knowledge or additional skills, people therefore engage in repeated actions driven by the goal to improve these future capacities. This deliberate practice is essential not only for elite performance but also in the acquirement of numerous everyday feats. \n\nIntertemporal choices are choices with outcomes that play out over time . Such decisions are ubiquitous in everyday life, ranging from routine decisions about what to eat for lunch (i.e. whether to adhere to a diet) to more profound decisions about climate change (i.e. whether to reduce current energy expenditure to avoid delayed costs). The ability to imagine future scenarios and adjust decisions accordingly may be important for making intertemporal choices in a flexible manner that accords with delayed consequences. Accumulating evidence suggests that cuing people to imagine the future in vivid detail can encourage preferences for delayed outcomes over immediate ones . This has been extended into real-world decisions such as in reducing the consumption of high-calorie food and increasing pro-environmental behaviours. \n\nIn recent years there have been a range of investigations into variation in prospection and its functions in clinical populations. Deficits to the mechanisms and functions of prospection have been observed in Alzheimer’s disease and other age-related dementias, Schizophrenia, and after brain damage (especially to the medial temporal lobes). \n\nShifts in the content and modes of prospection have been observed in affective disorders. For example, in both clinical depression and anxiety there is an overrepresentation of possible negative future events. In depression, there is additionally a reduction in the generation of possible positive future events. There are also a range of changes to the representational format (i.e. whether people tend to represent the future in episodic or semantic detail) in affective disorders . \n\n"}
{"id": "47420976", "url": "https://en.wikipedia.org/wiki?curid=47420976", "title": "Qatar Financial Information Unit", "text": "Qatar Financial Information Unit\n\nThe Qatar Financial Information Unit (QFIU) is a Qatari government regulatory agency responsible for financial intelligence efforts to combat money laundering and financing of terrorism. Like other national Financial Intelligence Units (FIU) around the world, it requires banks, investment companies, insurers and other financial institutions to report suspicious financial transactions. QFIU then analyzes the information and disseminates the relevant data to law enforcement authorities for further investigation and action.\n\nFounded in 2004, QFIU is led by Ahmad bin Eid al-Thani. Its mission is to “[protect] the integrity and economy of the State of Qatar through effective exchange of information, transparency enhancement and capacity building to help detect and deter money laundering and terrorism financing activities.” The QFIU is a “semiautonomous component” of Qatar’s National Anti-Money Laundering and Terrorism Financing Committee (NAMLC), and is housed in the Qatar Central Bank. Other national partners include the Qatar Financial Centre, State Security Bureau, Ministry of Interior, Criminal Investigation Department Section of Fighting Economic Crimes, Ministry of Justice, Ministry of Business and Trade, Public Prosecution, General Directorate of Customs, Financial Institutions, and Ministry of Social Affairs.\n\nQFIU also partners with FIUs in the region and worldwide. It is an active member of the Egmont Group, an “informal network” of international FIUs organized “to foster international cooperation and information exchange” among its 139 members. QFIU serves as co-chair of the Asia Group within Egmont, along with FIU-India. In order to promote cooperation and information sharing with international and regional FIU’s and provide professional development to its staff, QFIU is a regular participant in Egmont and other FIU and Anti-Money Laundering conferences and forums.\n\nIn 2009, QFIU signed a Statement of Cooperation with the Japan Financial Intelligence Center to expand financial intelligence sharing by “facilitating the exchange of information in assisting investigations concerning money laundering, terrorist financing and related crimes.”\n\nA 2008 report produced by the Middle East and North African Financial Action Task Force and conducted by IMF officials raised several shortcomings in QFIU’s creation and operations. According to the report, “The main shortcoming is that the administrative order establishing the FIU and empowering it with a number of functions appears to be inconsistent with the provisions of the AML Law that gave such powers to the coordinator of NAMLC.” It also noted that “the quality of STR [Suspicious Transaction Reports] needs improving,” that information was not adequately protected, and that there was periodic review of its effectiveness in combatting terrorism finance and money laundering.\n\nQFIU addressed these issues directly in its 2009 Annual Report, declaring, “The FIU attached high importance to amending Law No. (28) of 2008 on Anti-Money Laundering and its amendments, following the mutual evaluation report of the International Monetary Fund in 2007, which revealed many deficiencies and weaknesses in the legislative regulations related to anti-money laundering and terrorism financing.” To improve its operations, QFIU developed an eight-point, five-year strategic plan, which commenced in 2013.\n\nAlthough Qatar has enacted several pieces of legislation to address gaps and inconsistencies in its anti-money laundering and terrorism finance laws in recent years, the country has been criticized for not fully implementing or enforcing these measures. Former US Treasury Department official Matthew Levitt has noted, “To date, implementation and enforcement have not been a component of Qatar’s approach to these issues. Instead, Qatar routinely stresses to investors and critics alike the passage of laws that, on paper, appear robust but are almost never implemented or enforced.”\n\nIn its 2013 Annual Report, QFIU reported record numbers of Suspicious Transaction Reports (313) and suspects (352), though these numbers may reflect better adoption of anti-money laundering and terrorism finance measures as opposed to an increase in suspicious activities. Only 112 cases were disseminated to judicial or law enforcement authorities. According to a 2014 U.S. State Department report, Qatar prosecuted only one money-laundering case in 2013, and had no convictions. The same report lists Qatar as a “Major Money Laundering Country” and a “country of concern.”\n\nQFIU reported even larger numbers of Suspicious Transaction Reports (516) and suspects (787) in 2014, though the number of cases disseminated to national or foreign law enforcement agencies declined markedly.\n\n"}
{"id": "14093939", "url": "https://en.wikipedia.org/wiki?curid=14093939", "title": "Rational mysticism", "text": "Rational mysticism\n\nRational mysticism, which encompasses both rationalism and mysticism, is a term used by scholars, researchers, and other intellectuals, some of whom engage in studies of how altered states of consciousness or transcendence such as trance, visions, and prayer occur. Lines of investigation include historical and philosophical inquiry as well as scientific inquiry within such fields as neurophysiology and psychology.\n\nThe term \"rational mysticism\" was in use at least as early as 1911 when it was the subject of an article by Henry W. Clark in the \"Harvard Theological Review.\" In a 1924 book, \"Rational Mysticism,\" theosophist William Kingsland correlated rational mysticism with scientific idealism. South African philosopher J. N. Findlay frequently used the term, developing the theme in \"Ascent to the Absolute\" and other works in the 1960s and 1970s.\n\nColumbia University pragmatist John Herman Randall, Jr. characterized both Plotinus and Baruch Spinoza as “rationalists with overtones of rational mysticism” in his 1970 book \"Hellenistic Ways of Deliverance and the Making of Christian Synthesis.\" \nRice University professor of religious studies Jeffrey J. Kripal, in his 2001 book \"Roads of Excess, Palaces of Wisdom,\" defined rational mysticism as “not a contradiction in terms” but “a mysticism whose limits are set by reason.”\n\nIn response to criticism of his book \"The End of Faith\", author Sam Harris used the term rational mysticism for the title of his rebuttal. University of Pennsylvania neurotheologist Andrew Newberg has been using nuclear medicine brain imaging in similar research since the early 1990s.\n\nExecutive editor of \"Discover\" magazine Corey S. Powell, in his 2002 book, \"God in the Equation,\" attributed the term to Albert Einstein: “In creating his radical cosmology, Einstein stitched together a rational mysticism, drawing on—but distinct from—the views that came before.”\n\nScience writer John Horgan interviewed and profiled James Austin, Terence McKenna, Michael Persinger, Christian Rätsch, Huston Smith, Ken Wilber and others for \"Rational Mysticism: Dispatches from the Border Between Science and Spirituality,\" his 2003 study of “the scientific quest to explain the transcendent.”\n\n"}
{"id": "2252810", "url": "https://en.wikipedia.org/wiki?curid=2252810", "title": "Richard Carlile", "text": "Richard Carlile\n\nRichard Carlile (8 December 1790 – 10 February 1843) was an important agitator for the establishment of universal suffrage and freedom of the press in the United Kingdom.\n\nBorn in Ashburton, Devon, he was the son of a shoemaker who died in 1794; leaving Richard's mother struggling to support her three children on the income from running a small shop. At the age of six he went for free education to the local Church of England school, then at the age of twelve he left school for a seven-year apprenticeship to a tinsmith in Plymouth.\n\nIn 1813 he married, and shortly afterwards the couple moved to Holborn Hill in London where he found work as a tinsmith. Jane Carlile gave birth to five children, three of whom survived.\n\nSome time after 1829, Carlile met Eliza Sharples and she became his common law wife. Together they had at least four children.\n\n \nHis interest in politics was kindled first by economic conditions in the winter of 1816 when Carlile was put on short-time work by his employer creating serious problems for the family: \"I shared the general distress of 1816 and it was this that opened my eyes.\" He began attending political meetings where speakers like Henry Hunt complained that only three men in a hundred had the vote, and was also influenced by the publications of William Cobbett.\n\nAs a way of making a living he sold the writings of parliamentary reformers such as Tom Paine on the streets of London, often walking \"thirty miles for a profit of eighteen pence\". In April 1817 he formed a publishing business with the printer William Sherwin and rented a shop in Fleet Street. To make political texts such as Paine's books \"The Rights of Man\" and the \"Principles of Government\" available to the poor he split them into sections which he sold as small pamphlets, similarly publishing \"The Age of Reason\" and \"Principles of Nature\". He issued unauthorized copies of Southey's \"Wat Tyler\" and after the radical William Hone's arrest in May, he reissued the parody of parts of the Book of Common Prayer for which Hone was to be tried, then was himself arrested in August and held without charge until Hone was acquitted in December.\n\nHe took on distributing the banned Radical weekly \"The Black Dwarf\" at a time when the government was prosecuting publishers: \"The Habeas Corpus Act being suspended ... all was terror and alarm, but I take credit to myself in defeating the effect of these two Acts upon the Press... Of imprisonment I made sure, but I felt inclined to court it than to shrink from it\".\n\nCarlile then brought out a radical journal, \"Sherwin's Political Register\", which reported political meetings and included extracts from books and poems by supporters of the reform movement such as Percy Bysshe Shelley and Lord Byron. The popularity of this helped to soon bring his profit from his publishing venture to £50 a week.\n\nCarlile was one of the scheduled main speakers at the reform meeting on 16 August 1819 at St. Peter's Fields in Manchester. Just as Henry Hunt was about to speak, the crowd was attacked by the yeomanry in what became known as the Peterloo massacre. Carlile escaped and was hidden by radical friends before he caught the mail coach to London and published his eyewitness account, giving the first full report of what had happened, in \"Sherwin's Weekly Political Register\" of 18 August 1819. His placards proclaimed \"Horrid Massacres at Manchester\".\n\nThe government responded by closing \"Sherwin's Political Register\", confiscating the stock of newspapers and pamphlets. Carlile changed the name to \"The Republican\" and in its issue of 27 August 1819 demanded that \"The massacre... should be the daily theme of the Press until the murderers are brought to justice... Every man in Manchester who avows his opinions on the necessity of reform, should never go unarmed – retaliation has become a duty, and revenge an act of justice.\"\n\nCarlile was prosecuted for blasphemy, blasphemous libel and sedition for publishing material that might encourage people to hate the government in his newspaper, and for publishing Tom Paine's \"Common Sense\", \"The Rights of Man\" and the \"Age of Reason\" (which criticised the Church of England). In October 1819 he was found guilty of blasphemy and seditious libel and sentenced to three years in Dorchester Gaol with a fine of £1,500. When he refused to pay the fine, his premises in Fleet Street were raided and his stock was confiscated. While he was in jail he continued to write articles for \"The Republican\" which was now published by Carlile's wife Jane, and thanks to the publicity it now outsold pro-government newspapers such as \"The Times\".\n\nTo curb newspapers the government had raised the ½d tax on newspapers first imposed in 1712 to 3½d in 1797 then 4d in 1815. From December 1819 it set a minimum price of 7d and further restrictions. At a time when workers earned less than 10 shillings (120d.) a week this made it hard for them to afford radical newspapers, and publishers tried various strategies to evade the tax. Groups would pool their resources in reading societies and subscription societies to purchase a book or journal in common, and frequently read it aloud to one another as was the case with James Wilson.\n\nBy 1821, Carlile was a declared atheist (having previously been a Deist) and published his \"Address to Men of Science\", in favour of materialism and education. In the same year Jane Carlile was in turn sentenced to two years imprisonment for seditious libel, and her place as publisher was taken by Richard Carlile's sister, Mary. Within six months she was imprisoned for the same offence. The process was repeated with eight of his shop workers, and over 150 men and women were sent to prison for selling \"The Republican\". Carlile's sentence ended in 1823 but he was immediately arrested and returned to prison for not paying his £1,500 fine, so the process continued until he was eventually released on 25 November 1825. In the next edition of \"The Republican\" he expressed the hope that his long confinement would result in the freedom to publish radical political ideas. An example of the support he received from around the country is the £1.5.1 sent to him in Dorchester jail by forty working men in the West Yorkshire village of Hunslet, accompanied by a noble letter on behalf of those \"few Friends to Truth and Justice\".\n\nHe then published further journals, \"The Lion\" which campaigned against child labour and \"The Promptor\". He argued that \"equality between the sexes\" should be the objective of all reformers, and in 1826 published \"Every Woman's Book\" advocating birth control and the sexual emancipation of women. Cobbett denounced this book as \"so filthy, so disgusting, so beastly, as to shock even the lewdest men and women\".\n\nCarlile was an advocate of the Christ myth theory. He did not believe that Jesus existed. He debated Unitarian minister John Relly Beard in \"The Republican\", 1826.\n\nHe joined up with the radical and sceptical clergyman Robert Taylor and set out on an \"infidel home missionary tour\" which reached Cambridge on Thursday 21 May 1829 and caused a considerable upset to the University of Cambridge where a young Charles Darwin was a second-year student.\n\nAt their meeting in Bolton, Lancashire, Carlile met Eliza Sharples, who was to become his long term mistress.\n\nCarlile then opened a ramshackle building on the south bank of the River Thames, the Blackfriars Rotunda, and in widespread public unrest in July 1830 this became a gathering place for republicans and atheists. Taylor staged infidel melodramas, preaching outrageous sermons which got him dubbed \"The Devil's Chaplain\". Thousands of copies of these sermons were circulated in a seditious publication, \"The Devil's Pulpit\".\n\nIn 1831 he was jailed, under the charge of seditious libel, given two and a half years for writing an article in support of agricultural labourers campaigning against wage cuts and advising the strikers to regard themselves as being at war with the government. He left prison deeply in debt, and government fines had taken from him the finances needed to publish newspapers.\n\nHis political and social opinions never altered, but his philosophy underwent a change in the 1830s. In 1837 H. Robinson published the results of his later thinking in the book \"Extraordinary Conversion and Public Declaration of Richard Carlile of London to Christianity\".\n\nAfter living for some years in extreme poverty in Enfield, Carlile returned to Fleet Street in 1842, dying there the following year. He donated his body for medical research. Large numbers of people attended his funeral in Kensal Green Cemetery on Sunday 26 February 1843, where his sons protested at the Christian burial rite being administered in the common grave he was being buried in - citing that he \"passed his life in opposition to all priestcraft.\"\n\n\n\n"}
{"id": "39223327", "url": "https://en.wikipedia.org/wiki?curid=39223327", "title": "Safety-valve institution", "text": "Safety-valve institution\n\nSafety-valve organization or safety-valve institution is a term used in sociology to describe organizations which serve to allow discontented individuals to act out their opposition to other elements, as it were \"to let off steam\". Safety-valve organizations reduce tensions in society and in the structural-functionalist perspective can be said to have a tension-reducing latent function. Safety-valve organizations are outlets for behavior that is considered deviant, but cannot be eradicated from society, and such organizations prevent tensions from accumulating; thus tolerance of some deviant behavior in various safety-valve organizations prevents more serious problems. Therefore, one of the primary functions of the deviance itself is to act as a safety-valve. Without safety-valve organizations, interactions between certain groups would become much more limited, and conflict much more severe.\n\nSafety-valve institutions range from mostly legal and reputable (strikes, arts, and sports), to less so (pranks, casinos and gambling institutions in general, pornography) to mostly illegal (prostitution). Societies of different kinds vary widely in the legal status of these activities. In their most extreme, aggression in general and war in particular have also been described as safety-valve institutions.\n\nWith regard to specific organizations, Better Business Bureau has been described as a safety valve institution, as it \"mitigates conflicts between business and consumer\". Safety-valve organizations can exist in politics, where they provide an outlet for those dissatisfied with the political and social situation to legally organize and discuss it. For example, Saugat K. Biswas notes that the Indian National Congress was such an organization in late 19th-century India. Moren-Alegret similarly discusses the Portuguese NGO Secretariado Coordenador das Acções de Legalização (Coordinating Secretariat for Legalization Actions, an immigrant association) in a similar context. As an example of mostly criminal safety-valve organizations, Farchild discusses the Japanese yakuza.\n\n"}
{"id": "48234016", "url": "https://en.wikipedia.org/wiki?curid=48234016", "title": "Salynn McCollum", "text": "Salynn McCollum\n\nMary Salynn (Selyn) McCollum was the only white female Freedom Rider during the leg from Nashville, Tennessee to Birmingham, Alabama on May 17, 1961.\n\nBorn to Hilda and Walter McCollum in Tulsa, Oklahoma on April 6, 1940, her family moved around during her childhood. She eventually ended up in Amherst, New York where she attended junior high and high school. She has one sister named Rhonda. Though admitted to Syracuse University, McCollum's family wanted her to return to the South as both her parents were born and raised in Tennessee.\n\nAfter graduation, she matriculated in 1958 at George Peabody College for Teachers in Nashville, Tennessee. Her course of study focused on instruction for intellectually and developmentally disabled students. Although she was an undergraduate, she took some graduate level courses. During one such course, she met a Fisk University professor, Lester Carr, who shared her interest in developmentally challenged children. Professor Carr invited McCollum to visit Fisk University to see a classroom of autistic children. He also invited her to come hear Kelly Miller Smith speak at First Baptist, Capitol Hill.\n\nParticipation in Civil Rights Movement\n\nMcCollum attended training sessions led by Reverend James Lawson on non-violent protesting and also nightly meetings about desegregating downtown Nashville. The workshops were not tactical but instead focused on how to handle hostile citizens who opposed the student demonstrations. After the workshops and meetings, the college students would socialize and eat dinner at a restaurant on Jefferson Street. In February 1961, she attended a Nashville Student Movement meeting as a guest of Central Committee Member, Prof. Carr.\n\nAs a result of her participation in the Freedom Rides, and the strife it caused within the college administration her professor, Leonard J. Lucito, arranged for her to attend Southern Illinois University, where she was able to complete her studies. While at SIU, she helped organize lunch counter demonstrations with a few local students and even led a non-violent workshop using what she had learned from Lawson and her Nashville sit in experience. At one of these sit-ins with ten other protesters, she was assaulted with a knife and received a four-five inch cut on the back of her thigh. She was refused hospital care and ended up being driven to East St. Louis to receive care. This event garnered much attention in local and national newspapers. Soon John Lewis joined her in Cairo, Illinois. Upon completion of her course work she returned to Nashville and used scholarship money she received from Dr. Martin Luther King, Jr to continue participating in the Movement while student teaching kindergarten on the Peabody's campus.\n\nMcCollum visited Highlander Folk School during her time in Student Non-Violent Coordinating Committee. In the beginning of 1962, McCollum began working full-time for SNCC in Atlanta. Her duties included handling voter registration in Georgia, fundraising, and public speaking at churches throughout the South. She did not attend the March on Washington because by that time she had developed a dislike of being in large unorganized crowds probably due to her experiences in Cairo, Illinois.\n\nThere were ten Freedom Riders—two whites (Jim Zwerg and Salynn McCollum) and eight African Americans (John Lewis, William Barbee, Paul Brooks, Charles Butler, Allen Carson, Bill Harbour, Catherine Burks, and Lucretia Collins). McCollum was sent as an observer to report back to Diane Nash and was under orders not to be arrested with the other Riders. Leo Lillard drove her to Pulaski, Tennessee where she joined the other Freedom Riders. While some sources say McCollum missed the Nashville bus, she remembers this as an intentional decision to board someplace else to keep herself separate from the group and to try to provide a little protection.\n\nThe Freedom Riders did not converse during the rides as to maintain as much anonymity as possible. It was evident though they had been discovered by the time they reached the Alabama state line as armed gunmen were posted along the highway to Birmingham. Upon arrival at the Birmingham bus depot, she attempted to disembark to report back to Nash what was happening but was prevented from leaving the bus. All the passengers were detained for a period of time. Then the regular passengers were let off the bus, but not the Freedom Riders. When the Freedom Riders were finally let off the bus, McCollum joined Catherine Burks and Lucretia Collins thereby identifying herself as a Freedom Rider. By then, she had made the phone call to Diane Nash to inform her of what was happening in Birmingham. Eventually, they arrested all ten of them and took them to jail.\n\nMcCollum was separated from the other female Freedom Riders due to her race and held with the white women prisoners. When the white prisoners discovered she was a Freedom Rider, they beat her up and stole her candy and cigarettes. She stayed in jail about three or four days and was eventually released to her father's custody. Her parents were very disappointed and disapproved of her participation in the movement. Then, she was driven to Memphis in a police car by a patrolman and Birmingham Chief of Police Bull Conner. Once in Memphis, McCollum and her father flew back to Nashville.\n\nAfter her tenure with SNCC, McCollum took a job as a Day Care Center Director in Harlem for about twenty years. Then, she moved to Santa Fe, New Mexico where she trained dogs, rode horses, and traveled. In 2000, she moved back to Tennessee where she lived with her sister, Rhonda McCollum and family, in Nunnelly, Tennessee. She died on May 1, 2014.\n\nResources\n\nThe papers of Salynn McCollum are located at the Special Collections Library at Vanderbilt University.\n\n\n"}
{"id": "489334", "url": "https://en.wikipedia.org/wiki?curid=489334", "title": "Self-concept", "text": "Self-concept\n\nOne's self-concept (also called self-construction, self-identity, self-perspective or self-structure) is a collection of beliefs about oneself. Generally, self-concept embodies the answer to \"Who am I?\".\nSelf-concept is distinguishable from self-awareness, which refers to the extent to which self-knowledge is defined, consistent, and currently applicable to one's attitudes and dispositions. Self-concept also differs from self-esteem: self-concept is a cognitive or descriptive component of one's self (e.g. \"I am a fast runner\"), while self-esteem is evaluative and opinionated (e.g. \"I feel good about being a fast runner\").\n\nSelf-concept is made up of one's self-schemas, and interacts with self-esteem, self-knowledge, and the social self to form the self as whole. It includes the past, present, and future selves, where future selves (or possible selves) represent individuals' ideas of what they might become, what they would like to become, or what they are afraid of becoming. Possible selves may function as incentives for certain behavior.\n\nThe perception people have about their past or future selves relates to their perception of their current selves. The temporal self-appraisal theory argues that people have a tendency to maintain a positive self-evaluation by distancing themselves from their negative self and paying more attention to their positive one. In addition, people have a tendency to perceive the past self less favorably (e.g. \"I'm better than I used to be\") and the future self more positively (e.g. \"I will be better than I am now\").\n\nPsychologists Carl Rogers and Abraham Maslow had major influence in popularizing the idea of self-concept in the west. According to Rogers, everyone strives to reach an \"ideal self\". Rogers also hypothesized that psychologically healthy people actively move away from roles created by others' expectations, and instead look within themselves for validation. On the other hand, neurotic people have \"self-concepts that do not match their experiences. They are afraid to accept their own experiences as valid, so they distort them, either to protect themselves or to win approval from others.\"\n\nThe self-categorization theory developed by John Turner states that the self-concept consists of at least two \"levels\": a personal identity and a social one. In other words, one's self-evaluation relies on self-perceptions and how others perceive them. Self-concept can alternate rapidly between the personal and social identity. Children and adolescents begin integrating social identity into their own self-concept in elementary school by assessing their position among peers. By age 5, acceptance from peers significantly affects children's self-concept, affecting their behavior and academic success.\n\nThe self-concept is an internal model that uses self-assessments in order to define one's self-schemas. Features such as personality, skills and abilities, occupation and hobbies, physical characteristics, etc. are assessed and applied to self-schemas, which are ideas of oneself in a particular dimension (e.g., someone that considers themselves a geek will associate \"geek-like\" qualities to themselves). A collection of self-schemas make up one's overall self-concept. For example, the statement \"I am lazy\" is a self-assessment that contributes to self-concept. Statements such as \"I am tired\", however, would not be part of someone's self-concept, since being tired is a temporary state and therefore cannot become a part of a self-schema. A person's self-concept may change with time as reassessment occurs, which in extreme cases can lead to identity crises.\n\nAccording to Carl Rogers, the self-concept has three different components:\n\nResearchers debate over when self-concept development begins. Some assert that gender stereotypes and expectations set by parents for their children affect children's understanding of themselves by approximately age 3. However, at this developmental stage, children have a very broad sense of self, typically, they use words such as big or nice to describe themselves to others. While this represents the beginnings of self-concept,others suggest that self-concept develops later, around age 7 or 8. At this point, children are developmentally prepared to interpret their own feelings and abilities, as well as receive and consider feedback from peers, teachers, and family. In adolescence, the self-concept undergoes a significant time of change. Generally, self-concept changes more gradually, and instead, existing concepts are refined and solidified. However, the development of self-concept during adolescence shows a “U”-shaped curve, in which general self-concept decreases in early adolescence, followed by an increase in later adolescence.\n\nAdditionally, teens begin to evaluate their abilities on a continuum, as opposed to the \"yes/no\" evaluation of children. For example, while children might evaluate themselves \"smart\", teens might evaluate themselves as \"not the smartest, but smarter than average.\" Despite differing opinions about the onset of self-concept development, researchers agree on the importance of one’s self-concept, which influences people’s behaviors and cognitive and emotional outcomes including (but not limited to) academic achievement, levels of happiness, anxiety, social integration, self-esteem, and life-satisfaction.\n\nAcademic self-concept refers to the personal beliefs about their academic abilities or skills. Some research suggests that it begins developing from ages 3 to 5 due to influence from parents and early educators. By age 10 or 11, children assess their academic abilities by comparing themselves to their peers. These social comparisons are also referred to as \"self-estimates\". Self-estimates of cognitive ability are most accurate when evaluating subjects that deal with numbers, such as math. Self-estimates were more likely to be poor in other areas, such as reasoning speed.\n\nSome researchers suggest that, to raise academic self-concept, parents and teachers need to provide children with specific feedback that focuses on their particular skills or abilities. Others also state that learning opportunities should be conducted in groups (both mixed-ability and like-ability) that downplay social comparison, as too much of either type of grouping can have adverse effects on children's academic self-concept and the way they view themselves in relation to their peers.\n\nPhysical self-concept is the individual’s perception of themselves in areas of physical ability and appearance. Physical ability includes concepts such as physical strength and endurance, while appearance refers to attractiveness. Adolescents experience significant changes in general physical self-concept at the on-set of puberty, about 11 years old for girls and about 15 years old for boys. The bodily changes during puberty, in conjunction with the various psychological of this period, makes adolescence especially significant for the development of physical self-concept. An important factor of physical self-concept development is participation in physical activities. It has even been suggested that adolescent involvement in competitive sports increases physical self-concept.\n\nWorldviews about one's self in relation to others differ across and within cultures. Western cultures place particular importance on personal independence and on the expression of one's own attributes (i.e. the self is more important than the group). This is not to say those in an independent culture do not identify and support their society or culture, there is simply a different type of relationship. Non-Western cultures favor an interdependent view of the self: Interpersonal relationships are more important than one's individual accomplishments, and individuals experience a sense of oneness with the group. Such \"identity fusion\" can have positive and negative consequences. Identity fusion can give people the sense that their existence is meaningful provided the person feels included within the society (for example, in Japan, the definition of the word for self (\"jibun\") roughly translates to \"one's share of the shared life space\"). Identity fusion can also harm one's self-concept because one's behaviors and thoughts must be able to change to continue to align with those of the overall group. Non-interdependent self-concepts can also differ between cultural traditions.\n\nAdditionally, one's social norms and cultural identities have a large effect on self-concept and mental well-being. When a person can clearly define their culture's norms and how those play a part in their life, that person is more likely to have a positive self-identity, leading to better self-concept and psychological welfare. One example of this is in regards to consistency. One of the social norms within a Western, independent culture is consistency, which allows each person to maintain their self-concept over time. The social norm in a non-Western, interdependent culture has a larger focus on one's ability to be flexible and to change as the group and environment change. If this social norm is not followed in either culture, this can lead to a disconnection with one's social identity, which affects personality, behavior, and overall self-concept. Buddhists emphasize the impermanence of any self-concept.\n\nA small study carried out in Israel showed that the divide between independent and interdependent self-concepts exists \"within\" cultures as well. Researchers compared mid-level merchants in an urban community with those in a kibbutz (collective community). The managers from the urban community followed the independent culture. When asked to describe themselves, they primarily used descriptions of their own personal traits without comparison to others within their group. When the independent, urban managers gave interdependent-type responses, most were focused on work or school, due to these being the two biggest groups identified within an independent culture. The kibbutz managers followed the interdependent culture. They used hobbies and preferences to describe their traits, which is more frequently seen in interdependent cultures as these serve as a means of comparison with others in their society. There was also a large focus on residence, lending to the fact they share resources and living space with the others from the kibbutz. These types of differences were also seen in a study done with Swedish and Japanese adolescents. Typically, these would both be considered non-Western cultures, but the Swedish showed more independent traits, while the Japanese followed the expected interdependent traits.\n\nAlong with viewing one's identity as part of a group, another factor that coincides with self-concept is stereotype threat. Many working names have been used for this term: \"stigmatization\", \"stigma pressure\", \"stigma vulnerability\" and \"stereotype vulnerability\". The terminology that was settled upon to describe this \"situational predicament was 'stereotype threat.' This term captures the idea of a situational predicament as a contingency of their [marginalized] group identity, a real threat of judgment or treatment in the person's environment that went beyond any limitations within.\" Steele and Aronson described the idea of stereotype threat in their study of how this socio‐psychological notion affected the intellectual performance of African Americans. Steele and Aronson tested a hypothesis by administering a diagnostic exam between two different groups: African American and White students. For one group a stereotype threat was introduced while the other served as a control. The findings were that academic performance of the African American students was significantly lower than their White counterparts when a stereotype threat was perceived after controlling for intellectual ability. Since the inception of stereotype threat, other research has demonstrated the applicability of this idea to other groups.\n\nWhen one's actions could negatively influence general assumptions of a stereotype, those actions are consciously emphasized. Instead of one's individual characteristics, one's categorization into a social group is what society views objectively - which could be perceived as a negative stereotype, thus creating a threat. \"The notion that stereotypes held about a particular group may create psychologically threatening situations associated with fears of confirming judgment about one's group, and in turn, inhibit learning and performance.\"\n\nThe same prejudice that exists in stereotype threat also exists in the education system as it serves its communities, families, and individuals. These discriminatory practices in schools are the center of various educational and psychological researches. The research aims to increase equity in the classroom as well as academic achievement among students in minority groups.\n\nThe presence of stereotype threat perpetuates a \"hidden curriculum\" that further marginalized minority groups. Hidden curriculum refers to a covert expression of prejudice where one standard is accepted as the \"set and right way to do things\". More specifically, the hidden curriculum is an unintended transmission of social constructs that operate in the social environment of an educational setting or classroom. In the United States' educational system, this caters to dominant culture groups in American society. \"A primary source of stereotyping is often the teachers education program itself. It is in these programs that teachers learn that poor students and students of color should be expected to achieve less than their 'mainstream' counterparts.\" These child-deficit assumptions that are built into the program that instructs teachers and lead to inadvertently testing all students on a \"mainstream\" standard that is not necessarily academic and that does not account for the social values and norms of non-\"mainstream\" students.\n\nFor example, the model of \"teacher as the formal authority\" is the orthodox teaching role that has been perpetuated for many years until the 21st-century teaching model landed on the scene. As part of the 5 main teaching style proposed by Anthony Grasha, a cognitive and social psychologist until his death in 2003, the authoritarian style is described as believing that there are \"correct, acceptable, and standard ways to do things\". This system has dominated for as long as the educational system in America has, however, believing that there is a \"set and acceptable way to do things\" has in the past and can now perpetuate a \"hidden curriculum\" that is a form of institutionalized racism against marginalized groups such as Mexican Americans, Asian Americans, African Americans, and students with learning disabilities. This opens up a pathway for deficit thinking to rule and where a growth mindset is diminished.\n\nResearch from 1997, inspired by the differences in self-concept across cultures, suggested that men tend to be more independent, while women tend to be more interdependent. A study from 1999 showed that, while men and women do not differ in terms of independence or interdependence, they differ in their types of interdependence. Women utilize relational interdependence (identifying more with one-to-one relationships or small cliques), while men utilize collective interdependence (defining themselves within the contexts of large groups). In addition to their view of interdependence, men and women also view themselves differently in regards to several other traits that have to do with self-concept. For instance, in a study conducted in 1987, men were found to consider themselves more achievement and financially oriented as well as more competitive than their female counterparts. In contrast to this, the women were more likely to view themselves as sociable, moral, dependent and less assertive than the men. These differences potentially affect the individual's subjective well-being.\n\nGender differences in interdependent environments appear in early childhood: by age 3, boys and girls choose same-sex play partners, maintaining their preferences until late elementary school. Boys and girls become involved in different social interactions and relationships. Girls tend to prefer one-on-one (dyadic) interaction, forming tight, intimate bonds, while boys prefer group activities. One study in particular found that boys performed almost twice as well in groups than in pairs, whereas girls did not show such a difference. In early adolescence, males are more likely to have a positive physical self-concept. During this developmental stage, boys who develop early tend to have a more positive view of themselves as opposed to early developing females who view themselves more negatively. The largest difference during this developmental stage between males and females is the way they view their appearance. It is assumed at this age that a more attractive person has more social power. By the time they reach college-age, females continue to have lower physical self-concepts than males.\n\nGirls are more likely to wait their turn to speak, agree with others, and acknowledge the contributions of others. Boys, on the other hand, build larger group relationships based on shared interests and activities. Boys are more likely to threaten, boast, and call names, suggesting the importance of dominance and hierarchy in groups of male friends. In mixed-sex pairs of children aged 33 months, girls were more likely to passively watch a male partner play, and boys were more likely to be unresponsive to what their female partners were saying. The social characteristics of boys and girls as they develop throughout childhood tend to carry over later in life as they become men and women, although characteristics displayed as younger children are not necessarily entirely reflective of later behavior.\n\nSeveral studies have shown a difference between men and women based upon their academic self-concept. In general, men are more likely to view their overall academic self-concept higher, especially in the areas of math, science and technology. Women tend to have higher perceived abilities in their language related skills. This differing view of academic abilities has resulted in an academic achievement gap in countries such as Norway. These perceived self-concepts tend to reflect the typical gender stereotypes that are featured prominently in most cultures. In recent years, more women have been entering into the STEM field, working in predominantly math and science related careers. Many factors play a role in females adjusting their self-concept to accommodate more positive views of math and science such as; gender stereotypes, family influence and personal enjoyment of the subject. Females also tend to be more critical of their STEM abilities, leading them to require a higher level of achievement in order to have an equivalent level of self-value as their male counterparts. This leads females to, in general, be less successful in the STEM area as there aren't as many of the gender compared to males.\n\nWhy do people choose one form of media over another? According to the Galileo Model, there are different forms of media spread throughout three-dimensional space. The closer one form of media is to another the more similar the source of media is to each other. The farther away from each form of media is in space, the least similar the source of media is. For example, mobile and cell phone are located closest in space where as newspaper and texting are farthest apart in space. The study further explained the relationship between self-concept and the use of different forms of media. The more hours per day an individual uses a form of media, the closer that form of media is to their self-concept.\n\nSelf-concept is related to the form of media most used. If you consider yourself tech savvy, then you will use mobile phones more often than you would use a newspaper. If you consider yourself old fashioned, then you will use a magazine more often than you would instant message.\n\nIn this day and age, social media is where people experience most of their communication. With developing a sense of self on a psychological level, feeling as part of a greater body such as social, emotional, political bodies can effect how one feels about themselves. If a person is included or excluded from a group, that can affect how they form their identities. Growing social media is a place for not only expressing an already formed identity, but to explore and experiment with developing identities. In the United Kingdom, a study about changing identities revealed that some people believe that partaking in online social media is the first time they have felt like themselves, and they have achieved their true identities. They also revealed that these online identities transferred to their offline identities.\n\nA 2007 study was done on adolescents aged 12 to 18 to view the ways in which social media affects the formation of an identity. The study found that it affected the formation in three different ways: risk taking, communication of personal views, and perceptions of influences. In this particular study, risk taking behavior was engaging with strangers. When it came to communication about personal views, half of the participants reported that it was easier to express these opinions online, because they felt an enhanced ability to be creative and meaningful. When it came to other's opinions, one subject reported finding out more about themselves, like openness to experience, because of receiving differing opinions on things such as relationships.\n\n\n"}
{"id": "167181", "url": "https://en.wikipedia.org/wiki?curid=167181", "title": "Shopping hours", "text": "Shopping hours\n\nCustoms and regulations for shopping hours for sunday (times that shops are open) vary from countries to cities.\n\nSome countries, particularly those with predominantly Christian populations or histories do not allow Sunday shopping. In Islamic countries some shops are closed on Fridays for noontime prayers. In Israel many shops are sometimes closed on Friday evening and Saturday during the daytime for Shabbat (the Jewish Sabbath).\n\nEach state in Australia sets its own standard trading hours, but in most of the country the shops are open seven days a week for at least part of the day.\n\nFor some shops and other businesses in culturally Christian countries, Christmas Day is the only day in the year that they are closed.\n\nIn the United States and Canada, nearly all retail stores are open every day of the year except for Thanksgiving, Christmas Day, and Easter Sunday. Some suburban and smaller communities often close on Sundays. For example, Bergen County, New Jersey, next to New York City, completely bans Sunday shopping. However, nearly all stores in the United States have restricted hours on Sundays (most often 11 am or noon to 5 - 7 pm), and stores close early on important holidays, such as Christmas Eve, New Year's Eve, New Year's Day, and Independence Day. Banks, post offices and other government offices either are closed on weekends, or close early on Saturdays. Many other non-retail establishments remain closed on weekends.\n\nIn Islamic countries shops may have special opening hours during Ramadan.\n\nIn Israel, many shops may be closed on religious holidays other than Shabbat, especially on Yom Kippur when nearly all businesses are closed.\n\nShop trading hours in Australia are regulated by individual states and territories.\n\nThe Australian Capital Territory, the Northern Territory and the states of New South Wales, Victoria and Tasmania, totally or almost totally deregulate shopping hours. All retail businesses in the two territories, regardless of size or product offer are able to stipulate their trading hours to suit their individual customer demand. Non-essential shops in the three states are required to remain closed on Christmas Day and Good Friday, ANZAC Day (until 1pm), and in Tasmania and NSW on Easter Sunday, and in NSW on Boxing Day (outside the Sydney special trading precinct). Shops in the Northern Territory and the Australian Capital Territory can remain open on any public holiday. The two main supermarket operators, Woolworths and Coles, generally trade between 6 am and midnight every day, although some inner-city shops in Sydney and Melbourne operate twenty-four hours. In Canberra, Woolworths, Kmart and a few more shops open 24/7.\n\nMelbourne generally has the most relaxed rules. Almost all shopping centres in Melbourne now trade until 9pm on Thursdays and Fridays as well as being open longer hours on Sundays. Interstate late night trading only occurs on either Thursday or Friday rather than both.\n\nMelbourne is also famous for beginning the trend of 36-hour overnight trades in the lead-up to Christmas. Some of the larger shopping centres will open from 8 am December 23 until 6 pm on Christmas Eve. Centres often open to 10pm or midnight on most other nights in the fortnight before Christmas, and the first few days of the annual Boxing Day Sales.\n\nTrading hours in the Australian Capital Territory have been deregulated since the repeal of the Trading Hours Act 1996 [ACT] on 29 May 1997.\n\nShopping hours in South Australia are still regulated but the state government has passed numerous changes to relax the laws. Despite these changes retailers still face complicated and confusing trading laws, which stipulate trading hours based on size and product offer. Supermarkets that trade with fewer than 7 workers and with a trading floor less than 500 m are exempt from the laws. Larger supermarkets are required by law to close at Monday to Friday at 9pm, Saturdays at 5pm and are permitted to trade on Sundays and public holidays only from 11am to 5pm except ANZAC day which is 12 noon to 5pm; Good Friday, Easter Sunday and Christmas Day are the days that they must not open on.\n\nIn all areas of Queensland, trading hours with major supermarkets are Monday to Saturday from 7am to 9pm and Sundays and public holidays from 9am to 6pm. Most major shopping centres close at 5pm every day, with the exception of one night a week with what is so named 'late night shopping.' If a supermarket is in a major shopping centre, it must still cease trading at 9pm, with special access for just the supermarket.\n\nIn rural areas of Western Australia below the 27th parallel, local governments nominate shop closing hours to the State government, which, if accepted, are implemented by ministerial order. Shopping hours in the state's capital, Perth, are regulated by laws similar to South Australia's. Trading hours are stipulated in law, and are based on size and product offer. As in South Australia, smaller, independently operated supermarket retailers are exempted. Chain supermarkets are required to close Monday to Friday at 9pm, Saturdays at 5pm, and are permitted to trade on Sundays and public holidays only from 11am to 5pm.\n\nThe situation in Austria is very similar to that in Germany, with most public holidays being based on Catholic holidays as the country is predominantly. Until the 1990s, all shops closed around noon on Saturday and did not reopen until Monday morning. Entrepreneurs such as Richard Lugner lobbied for an expansion of shopping hours, and laws are gradually being changed, with more and more exceptions granted. Meanwhile, as in Germany, gas stations and train stations in big cities have taken on the role of \"Nahversorger\" (supplying the local population with groceries) outside regular shopping hours.\n\nUntil very recently, shopping hours remained very restrictive. In 2008 Austria modified its 2003 \"Öffnungszeitengesetz\" (\"opening times law\"), with the new regulations allowing stores to be open from 6:00 a.m. until 9:00 p.m. on weekdays, and on Saturday until 6:00 p.m. but are restricted to a total of 72 open hours per week. Bakeries can open 30 minutes earlier at 5:30 a.m. Shops are closed on Sunday, but there are exceptions for tourist locations, train stations, airports, and the Christmas season.\n\nStore hours in Canada are regulated by each province or territory and, in some provinces, individual municipalities as well.\n\nAs a general rule, there is little regulation of shopping hours across the country. In the provinces of British Columbia, Alberta, and Saskatchewan, as well as all three territories (Yukon, Northwest Territories and Nunavut), there are no restrictions at all and stores can open 24/7 every day. As well, Nova Scotia permits any store to open every day of the year except Remembrance Day (November 11).\n\nThe remaining provinces (Manitoba, Ontario, Quebec, New Brunswick, Prince Edward Island and Newfoundland and Labrador) require stores to close on most major holidays. Furthermore, three provinces have further restrictions on Sunday openings. In Manitoba, stores may open on Sundays only with municipal approval and only between 9am - 6 pm (although exceptions for essential services apply). New Brunswick allows Sunday opening all year only with both municipal and provincial approval; otherwise it is permitted only from August until the New Year. Some communities in New Brunswick (such as Woodstock, Miramichi, Sussex) restrict Sunday hours of operation to 12pm - 5pm.\n\nThe province of Quebec is the only province in Canada that regulates shopping hours outside of Sundays and holidays. As a general rule, stores are permitted to open only between 8am and 9pm weekdays and 8am - 5pm weekends, excluding holidays. However, there are several exceptions, notably with several supermarkets in Montreal, which are open later hours or twenty-four hours.\n\nIn practice, few stores in Canada (outside of a small number of grocery stores) remain open twenty-four hours. Most shopping centres open from 10am-9pm Monday to Friday, 9:30am - 6pm (or in some cases 9pm) on Saturday and 12pm - 5pm or 6pm on Sunday. Many larger stores, such as Walmart Canada and most major grocery stores remain open 8am - 10pm Monday to Saturday and 10am - 6pm (in some provinces 8 am-10 pm) on Sunday, except in provinces where further restrictions apply. The Sobeys chain stays open from 7am - 11pm on weekdays and Saturdays, though some locations are open twenty-four hours. Many Loblaws brand stores such as Zehrs Markets and Real Canadian Superstore are open from 7am-11pm, 7 days of the week.\n\nTrading hours in China, including Hong Kong and Macau special administrative regions, are commercial decisions and are not regulated. Most shops are open on public holidays. Some convenience stores are open twenty-four hours and every day of the year, but only a few large supermarkets are open twenty-four hours a day.\n\nDuring the Chinese New Year, many shops in China close for a few days, from Chinese New Year's Eve to the first day of the Chinese New Year. Or more often, to the third day of the Chinese New Year. Some shops in Hong Kong and Macau operate on Chinese New Year holidays, especially supermarket chains.\n\nShopping hours in Croatia are currently unregulated after the Constitutional Court struck down a ban on Sunday shopping, which had been in effect from mid-2008 until mid-2009.\n\nMost large out-of-town supermarkets are open between 07:30/08:00-21:00/22:00, Monday to Sunday. Shopping malls usually open at 09:00 and also close at 22:00, every day. Smaller supermarkets close earlier on Sundays, typically at 13:00. Other shops in urban areas are generally closed on Sundays.\n\nBakeries and newspaper kiosks often open very early in the morning, at 05:30 or 06:00, and open every day but not twenty-four hours. Gas stations and convenience stores along major roads as well as some pharmacies (at least one in each major city, five in Zagreb) operate twenty-four hours.\n\nStandard operating hours for most businesses are generally 8:00/8:30 - 17:30. Since 1 October 2012, Danish shops have been allowed to be open every day around the clock, except on public holidays and after 3 pm on Christmas Eve's Day and New Year Eve's Day. However, shops with a turnover of less than DKK 32.2 million (2012 figure, indexed) are allowed to be open every day of the year. Still in many small towns shops are usually closed on Saturday after 2 pm and on Sunday. Some small shops are closed on Monday.\n\nSunday shopping was first introduced in 1994.\n\nOn 15 December 2015, the Finnish parliament voted for removing all opening hour restrictions for all retailers. The law came into effect on January 1, 2016.\n\nIn Germany, shopping days and opening hours were previously regulated by a federal law called the \"Shop Closing Law\" (\"Ladenschlussgesetz\"), first enacted in 1956 and last revised on 13 March 2003. However, on 7 July 2006, the federal government handed over the authority to regulate shopping hours to the sixteen states. Since then, states have been allowed to pass their own laws regulating opening hours. The federal Ladenschlussgesetz continues to be valid in states that have not passed their own laws.\n\nUnder the old Ladenschlussgesetz, which currently applies only in the states of Bavaria and Saarland, shops may not open prior to 6 a.m. and may not stay open later than 8 p.m. from Monday to Saturday. Shops also have to stay closed on Sundays and public holidays (both federal and state), and special rules apply concerning Christmas Eve (December 24) should that day fall on a weekday.\n\nThere are several exceptions, including petrol stations and shops located in railway stations and airports, which may stay open past the normal hours. Most petrol stations in larger cities, and all situated on Autobahns, are open twenty-four hours. Shops in so-called \"tourist zones\" may also open outside the normal hours, but they are restricted to selling souvenirs, handcrafted articles and similar tourist items. In connection with fairs and public market days, communities are allowed four days per year (normally Sundays) on which shops may open outside the normal restrictions; however, such shop openings may not take place during primary church services and they must close by 6 pm. Bakeries may open for business at 5.30 a.m. and may also open for a limited time on Sundays. Restaurants, bars, theatres, and cultural establishments are generally unaffected by the shop opening time restrictions. As most public holidays in Germany are religiously based, and since the religious holidays (Protestant and Catholic) are not uniform across Germany, shops may be closed due to a public holiday in one state, and open in a neighbouring state. Bavaria even differentiates between cities with Protestant or Catholic majorities.\n\nThe Ladenschlussgesetz has been the subject of controversy, as larger stores (and many of their customers) would prefer to have fewer restrictions on shopping hours, while trade unions, small shop owners and the church are opposed to a further loosening of the rules. On June 9, 2004, the German supreme court (\"Bundesverfassungsgericht\") rejected a claim by the German department store chain \"Kaufhof AG\" that the shop-closing law was unconstitutional. Among other things, the court cited Article 140 of the German constitution \"(Grundgesetz)\" (which in turn invokes Article 139 of the 1919 Weimar Constitution) protecting Sundays and public holidays as days of rest and recuperation. However, the court in effect invited the federal parliament \"(Bundestag)\" to reconsider whether the states (\"Länder\") should regulate hours instead of the federal government.\n\nSo far, no state has passed regulation that allows for general store opening on Sundays.\n\nStates with no restrictions from Monday to Saturday and varying regulations for Sunday:\n\nStates with no restrictions from Monday to Friday and varying regulations for Saturday and Sunday:\n\nStates where shops can open between 6 a.m. and 10 p.m. from Monday to Saturday and regulations for Sunday vary:\n\nStates with no liberalisation of opening hours exceeding the federal Ladenschlussgesetz:\n\nShops in Ireland may, with few exceptions (such as those involved in the sale of alcohol), open whenever they want, including Sundays and public holidays.\n\nHere are typical hours:\n\n\"Monday - Wednesday, Friday, Saturday\":\n\n\"Thursday\":\n\n\"Sunday\":\n\nMany supermarkets are open twenty-four hours or have longer opening hours (like 8:00 - 22:00) everyday.\n\nLarge shopping centres and out-of-town (suburban) centres are typically open longer hours everyday (e.g. 09:00 - 21:00/22:00 weekdays, 09:00 - 19:00 Saturdays, 10:00 - 19:00 Sundays).\n\nIn the two weeks running up to Christmas, it is common for many shops to have extended opening hours; some may operate twenty-four hours until midnight on Christmas Eve.\n\nMost shops (other than petrol stations or convenience stores) in smaller towns and villages don't open at all on Sundays. Almost all shops (again, petrol stations, convenience stores, etc. excepted) are closed on Christmas Day, though most are open on all other holidays.\n\nConvenience stores, petrol stations and some chemists (drugstores) are normally open from early morning (05:00/06:00/07:00) to the late night (22:00/23:00/00:00), or often twenty-four hours, and New Year's Day is also Sunday hours.\n\nIn rural areas or in traditional trades, Wednesdays may be a half-day for businesses, closing at 12:30, but this practice has long passed in urban areas.\n\nAlcohol is allowed to be sold only between 10:30 and 22:00 from Monday to Saturday and 12:30 to 22:00 on Sundays, but this does not affect opening hours (supermarkets will often block access to alcoholic products outside of these times). Alcohol cannot be sold at all on Good Friday.\n\nIn Japan, most shops open at 10:00, and close at 20:00 (8 PM). Banks are open from 09:00-15:00 on weekdays, and closed on weekends; post offices are open from 09:00-17:00 on weekdays, and closed on weekends.\n\nRegular opening hours: Monday 11:00 - 18:00; Tuesday-Friday: 09:30 - 18:00; Saturday: 09:30 - 17:00; Sunday (Amsterdam, Rotterdam, The Hague, Utrecht, Almere, Leiden and smaller tourist towns): 12 noon - 18:00. In many other towns shops are open every first Sunday of the month (koopzondag).\n\nShops are allowed to stay open until 22:00 from Monday to Saturday, however most close at 18:00 on weekdays, and 17:00 on Saturdays. Many supermarkets (including outlets from the market leader Albert Heijn, several DIY-stores and IKEA) stay open until 20:00, 21:00 or 22:00. Most towns have their weekly shopping evening (koopavond), when shops stay open until 21:00, on Thursday or Friday. In touristic towns (like Amsterdam's city centre) supermarkets are allowed to be open on Sundays between 07:00 and 22:00. Many towns have one or more supermarkets (avondwinkels) that are open until later in the evening, occasionally all night. Convenience stores also have longer shopping hours; they are at many larger railway stations (\"Albert Heijn to go\") and some busy streets.\n\nA regular size supermarket that is open until midnight seven days a week is the regular Albert Heijn at Schiphol Airport near Amsterdam (in the area of the airport before ticket checks, not only for air travelers).\n\nShops that close on Sundays are usually also closed on public holidays, and other shops tend to have opening hours then like on Sundays. However, on Christmas Day and New Year's Eve almost all shops are closed.\n\nFor specific opening hours (\"openingstijden\") in the Netherlands there are several websites.\n\nShopping hours in Serbia are unregulated. Large supermarkets are usually open from 07:00/07:30/08:00 to 22:00 from Monday to Sunday. Shopping malls open at 09:00 or 10:00 and also stay open until 22:00. Smaller supermarkets close earlier on Sundays, at 15:00 or 16:00.\n\nUnlike neighbouring Croatia, many fast food outlets, bakeries, kiosks and convenience stores in urban areas operate twenty-four hours. Even some hypermarkets, like Tempo and Metro, are open twenty-four hours.\n\nShopping hours for shopping malls are usually from 10:00 to 22:00 from Monday to Sunday. Automotive shops like tire outlets are usually from 09.30 to 19:00. Some supermarkets are open twenty-four hours. Most stores do not open on the first day of Chinese New Year because of low demand patronage.\n\nIn Sweden there is no longer any law regarding shopping hours except for the nationally owned Systembolaget alcohol shops, which close at 20:00 at the latest on weekdays and 15:00 on Saturdays. On Sundays no alcohol is sold at all, although it is served in restaurants. Shopping centres and food shops are generally open every day; grocery stores often until 22:00 all days of the week and shopping centers usually until 20:00 on weekdays and 18:00 on weekends. Usually shopping centers are closed on New Year's Day, Midsummer's Day and Christmas Day, but grocery stores are open even those days albeit fewer hours than usual. Although there aren't any law that regulate business hours in general, labour laws do not allow work between midnight and 5 am in many professions including grocery stores and most shops.\n\nShopping hours are governed by cantonal law and vary accordingly, the only confederally mandated store holiday being August 1 (the national holiday), as per article 110 III of the Swiss Constitution. Most often, stores will be open from 8 or 9 am to 7 or 8 pm, 9 pm one day a week (usually a Thursday or a Friday) depending on the region. On Saturday and the day before public holidays, most stores close at around 4 or 5 pm. Stores are also generally closed on Sundays; see Sunday shopping in Switzerland.\n\nIn Great Britain, many retail stores are open every day. Some large supermarkets are open for twenty-four hours, (except on Sundays in England and Wales). Most stores do not open on Easter Sunday, New Year's Day or Christmas Day and have reduced hours on other public and bank holidays.\n\nMondays - Saturdays: 9:00 am to 5:30pm, or 10:00 am to 8:00 pm/10:00 pm.\n\nSundays: - 10:00 am to 4:00 pm, or 11:00 am to 5:00 pm, or 12 noon to 6:00 pm.\n\nSunday shopping has become more popular, and most but not all shops in towns and cities are open for business. Shops 280 m and larger in England and Wales are allowed to trade for only six hours on Sundays; shops in Northern Ireland may open from 1:00 pm to 6:00 pm. In Scotland, in theory, Sunday is considered the same as any other day, and there are no restrictions. However, in practice, some shops do not open on Sunday or open for only four hours in smaller towns. In some Free Church dominated areas, for example Stornoway on the Isle of Lewis, Sunday is considered a day of rest and consequently very few if any shops open at all.\n\nIn the U.S., the various levels of government generally do not regulate the hours of the vast majority of retailers (though there are exceptions, such as the blue law), and with the main exception being shops licensed to sell spirits and other alcoholic beverages (for shopping hours, see alcohol sale hours by state) and car dealerships. Shopping hours vary widely based on management considerations and customer needs. Key variables are the size of the metropolitan area, the type of store, and the size of the store.\n\nLas Vegas, Nevada is the notable exception to all the traditions just described. Las Vegas is world-famous for its 24-hour local culture since it is an area with large gaming and tourism industries that operate 24/7. Since many of the employees in the city's primary industries work overnight shifts — and because Nevada has few laws in regard to operating hours for any type of commercial activity — many businesses cater to such workers. Thus, Las Vegas is home to many 24-hour car dealerships, dental clinics, auto mechanics, computer shops, and even some smaller clothing stores.\n\nTypical store shopping hours:\n\n\nSupermarkets usually open at earlier hours, between 6 or 7 a.m. to 10 p.m. (7:00 - 22:00) every day. Boutiques and smaller shops often close early at 5 or 6 p.m. (17:00 or 18:00), and usually close once or twice a week, most often on Sunday.\n\nNearly all stores are closed on Easter, Thanksgiving Day and Christmas Day. However, in recent years, several department stores and discount stores have started opening during the evening on Thanksgiving Day; see Black Friday for more details. Early closing (half days) occur on Christmas Eve and New Year's Eve. Some stores might have reduced hours on other major holidays.\n\nAll malls and department stores, as well as most other stores remain open longer hours between Thanksgiving weekend and Christmas Eve for Christmas and holiday shopping. Many are open until 11 p.m. (23:00), and a few even longer.\n\nFew stores remain open twenty-four hours; the main exceptions to this rule are most Walmarts throughout the country (especially Supercenters, which combine a discount store and full supermarket); many convenience stores, especially those that also sell motor fuel; and some drug stores like CVS, especially in larger cities like New York City and Las Vegas.\n\nSome stores, especially in suburban and rural areas, might remain closed on Sundays for any reason (such as most retail in Bergen County, New Jersey due to the blue law, which is next to New York City, and home to four major malls and has the largest retail in the nation).\n\n"}
{"id": "3331474", "url": "https://en.wikipedia.org/wiki?curid=3331474", "title": "Snoezelen", "text": "Snoezelen\n\nSnoezelen or controlled multisensory environment (MSE) is a therapy for people with autism and other developmental disabilities, dementia or brain injury. It consists of placing the person in a soothing and stimulating environment, called the \"Snoezelen room\". These rooms are specially designed to deliver stimuli to various senses, using lighting effects, color, sounds, music, scents, etc. The combination of different materials on a wall may be explored using tactile senses, and the floor may be adjusted to stimulate the sense of balance. The person is usually accompanied by an aide or therapist.\n\nDeveloped in the Netherlands in the 1970s, Snoezelen rooms have been established in institutions all over the world and are especially common in Germany, where more than 1,200 exist.\n\nThere is no evidence that Snoezelen is effective for the treatment of dementia.\nThe term \"Snoezelen\" (pronounced ) is a neologism formed from a blend of the Dutch \"snuffelen\" (to snuggle, also: to sniff) and \"doezelen\" (to doze, to snooze). It was coined by Jan Hulsegge and Ad Verheul who developed the concept while working at De Hartenberg Institute in the Netherlands.\n\nIdeally, Snoezelen is a non-directive therapy, controlled by the client and not by the therapist. It can be staged to provide a multi-sensory experience or single sensory focus, simply by adapting the lighting, atmosphere, sounds, and textures to the specific needs of the client at the time of use. There is no formal focus on therapeutic outcome—the focus is to assist users to gain the maximum pleasure from the activity in which they and the enabler are involved. An advantage of Snoezelen therapy is that it does not rely on verbal communication and may be beneficial for people with profound autism, as it may provide stimulation for those who would otherwise be almost impossible to reach.\n\nSnoezelen therapy relates to the interdependence of both the space (the physical environment) and the \"client-centered\" approach of the practitioner (the human environment). The specially designed sensory physical environment together with the input of the \"enabling practitioner\" initiates changes in arousal by affecting the relaxation process, reducing anxiety/pain (both physical and emotional). It aims to maximize a person's potential to focus on his own free will and to engage on a motivational stimulus (object, activity or person), and thereby to improve communication and functioning.\n\nThe defining principles of the MSE help the practitioner to focus attention on the basic elements of this approach. The following are the principles.\n\n\nResearch on the benefits of Snoezelen treatment is scarce, with variable study designs.\n\nA small research study carried out in Brussels compared the behavior of nine adult clients with profound autism in both classroom and Snoezelen settings. Though individual results varied, the study claimed a 50% reduction in distress and stereotypical behavior, and seventy-five percent less aggression and self-injury in the Snoezelen environment.\n\n"}
{"id": "12846728", "url": "https://en.wikipedia.org/wiki?curid=12846728", "title": "Society for Quantitative Analysis of Behavior", "text": "Society for Quantitative Analysis of Behavior\n\nThe Society for the Quantitative Analyses of Behavior was founded in 1978 by Michael Lamport Commons and John Anthony Nevin. The first president was Richard J. Herrnstein. In the beginning it was called the Harvard Symposium on Quantitative Analysis of Behavior (HSQAB). This society meets once a year to discuss various topic in quantitative analysis of behavior including: behavioral economics, behavioral momentum, Connectionist systems or neural networks, hyperbolic discounting, foraging, errorless learning, learning and the Rescorla-Wagner model, matching law, Melioration, scalar expectancy, signal detection and stimulus control, connectionism or Neural Networks. Mathematical models and data are presented and discussed. The field is a branch of mathematical psychology. Some papers resulting from the symposium are published as a special issue of the journal \"Behavioural Processes\".\n\n\n"}
{"id": "36236784", "url": "https://en.wikipedia.org/wiki?curid=36236784", "title": "Statary", "text": "Statary\n\nStatary is a term currently applied in fields such as ecology, ethology, psychology. In modern use it contrasts on the one hand with such concepts as \"migratory\", \"nomadic\", or \"shifting\", and on the other with \"static\" or \"immobile\". The word also is of historical interest in its change of meaning as its usage changed.\n\nIn current usage in fields such as biology, commonly means \"in a particular location or state, but not rigidly so\". Army ant colonies for example are said to be in a \"statary phase\" when they occupy one bivouac for an extended period instead of just overnight. This is as opposed to a \"nomadic phase\", in which they travel and forage practically daily. This does not mean that ant colonies in a statary phase do not move nor even that they do not forage while statary; they often do both, sometimes daily. Correspondingly a colony in a nomadic phase does not travel without rest; it bivouacs for the night. The significance of the terms is that the colonies' behaviour patterns differ radically according to their activity phase; one pattern favours maintaining a persistent presence where brood is being raised, whereas the other favours continual nomadic wandering into new foraging grounds. Such phases have raised interest in studies in aspects of comparative psychology and evolution.\n\nThe term \"statary\" also applies in contexts other than ants or colonial organisms. Swarm-forming species of locusts go beyond having statary and nomadic phases of behaviour; their growing nymphs actually develop into different adult morphologies, depending on whether the conditions during their growth favour swarming or not. Locusts that adopt the swarming morphology are said to be the migratory morphs, while the rest are called statary morphs. Effectively similar morphs occur in some other insect species, such as army worm.\n\nIn some technical fields \"statary\" need not refer literally to location or motion, but refer figuratively to their having particular characteristic but non-rigid attributes, such as atmospheric pressure. The following section instances examples of such senses occurring in the history of the term.\n\nStatary, from the Latin root \"statarius\", meaning \"standing fast\", first came into prominent use in the English language in \"Positions\", the work of Richard Mulcaster in the sixteenth century. He spoke of \"statarie substance\" much as, in contemporary English, one might speak of fixed assets or fixed property: \"...either rich or poore : landed or unlanded, which is either the having or wanting of the most statarie substance.\" It is unclear whether he coined the English version of the word.\n\nSamuel Collins also used the word \"statary\" in the slightly different sense of \"ordinary\" or \"normal\" in his 1617 defence of the Bishop of Elie, Lancelot Andrewes: \"What is this [in comparison], ... not ... to their stately, but even statarie and ordinarie supremacie in the Church?\".\n\nIn his \"Pseudodoxia Epidemica\" Sir Thomas Browne used the word in at least two senses; firstly he used it as meaning \"fixed\" or \"regular\", as in \"...perturbed the observation of festivities and statary solemnities...\" secondly he used it in contrast to \"anniversary\" (by which he meant \"annual\" or \"seasonal\") as in: \"...we might expect a regularity in the winds ; whereof though some be statary, some anniversary, and the rest do tend to determine points of heaven, yet do the blasts and undulary breaths thereof maintain no certainty in their course...\" In this passage he explicitly does not assert the self-contradiction that winds might be static, but rather that they are unceasing, though variable. In this sense Browne's usage is consistent with the modern technical application.\n\nThe word \"statary\" appeared in sundry works after Browne's time, for example in the Literary Gazette in the \"Meteorological Journal\", the word was used in reference to winds and barometric readings in much the same sense as that in which Browne had referred to statary winds.\n\nLancelot Addison referred to \"statary prayers\" in his account of his seven years in West Barbary, published in 1671. However, the word never seems to have come into common use, though it did appear in various dictionaries, such as Samuel Johnson's 1755 Dictionary of the English Language. Such entries did not generally refer to any distinction between the various senses, and in fact some used words such as \"stationary\" in their definitions, apparently feeling no need for a separate term for the concept of something that has a non-rigid general location around specific coordinates. Accordingly the word was marked as obsolete by the compilers of the Oxford English Dictionary published in the late 19th to early 20th century, and Webster's Dictionary did likewise in the 1913 unabridged edition. The Shorter Oxford English Dictionary in turn omitted the entry altogether. If it had not been resurrected in the role of a technical term, the word \"statary\" probably would have disappeared from the language by now. In the sense of \"rigidly stationary\" it patently is redundant, but in the sense of loosely remaining at particular coordinates, it fills a need in certain fields such as in biology and climatology.\n\nA related sense appeared in the 1623 translation of Xenophon by John Bingham. As applied to soldiers, statary means: equipped for stationary combat as opposed to skirmishing.\n\nEventually, about the early 1930s, the word began to reappear in articles and textbooks, particularly on biological topics. For example, it was used in a prominent textbook of the day, Norman Maier and associates were applying the concept to ant behaviour; it seems that T. C. Schneirla had elected to use the term in 1932. It had proved useful, and by the end of the 20th century the word was in fairly common use, as can be seen from Google Ngram Viewer.\n"}
{"id": "213508", "url": "https://en.wikipedia.org/wiki?curid=213508", "title": "Subtyping", "text": "Subtyping\n\nIn programming language theory, subtyping (also subtype polymorphism or inclusion polymorphism) is a form of type polymorphism in which a subtype is a datatype that is related to another datatype (the supertype) by some notion of substitutability, meaning that program elements, typically subroutines or functions, written to operate on elements of the supertype can also operate on elements of the subtype. If S is a subtype of T, the subtyping relation is often written S <: T, to mean that any term of type S can be \"safely used in a context where\" a term of type T is expected. The precise semantics of subtyping crucially depends on the particulars of what \"safely used in a context where\" means in a given programming language. The type system of a programming language essentially defines its own subtyping relation, which may well be trivial should the language support no (or very little) conversion mechanisms.\n\nDue to the subtyping relation, a term may belong to more than one type. Subtyping is therefore a form of type polymorphism. In object-oriented programming the term 'polymorphism' is commonly used to refer solely to this \"subtype polymorphism\", while the techniques of parametric polymorphism would be considered \"generic programming\".\n\nFunctional programming languages often allow the subtyping of records. Consequently, simply typed lambda calculus extended with record types is perhaps the simplest theoretical setting in which a useful notion of subtyping may be defined and studied . Because the resulting calculus allows terms to have more than one type, it is no longer a \"simple\" type theory. Since functional programming languages, by definition, support function literals, which can also be stored in records, records types with subtyping provide some of the features of object-oriented programming. Typically, functional programming languages also provide some, usually restricted, form of parametric polymorphism. In a theoretical setting, it is desirable to study the interaction of the two features; a common theoretical setting is system F. Various calculi that attempt to capture the theoretical properties of object-oriented programming may be derived from system F.\n\nThe concept of subtyping is related to the linguistic notions of hyponymy and holonymy. It is also related to the concept of bounded quantification in mathematical logic. Subtyping should not be confused with the notion of (class or object) inheritance from object-oriented languages; subtyping is a relation between types (interfaces in object-oriented parlance) whereas inheritance is a relation between implementations stemming from a language feature that allows new objects to be created from existing ones. In a number of object-oriented languages, subtyping is called interface inheritance, with inheritance referred to as \"implementation inheritance\".\n\nThe notion of subtyping in programming languages dates back to the 1960s; it was introduced in Simula derivatives. The first formal treatments of subtyping were given by John C. Reynolds in 1980 who used category theory to formalize implicit conversions, and Luca Cardelli (1985).\n\nThe concept of subtyping has gained visibility (and synonymy with polymorphism in some circles) with the mainstream adoption of object-oriented programming. In this context, the principle of safe substitution is often called the Liskov substitution principle, after Barbara Liskov who popularized it in a keynote address at a conference on object-oriented programming in 1987. Because it must consider mutable objects, the ideal notion of subtyping defined by Liskov and Jeannette Wing, called behavioral subtyping is considerably stronger than what can be implemented in a type checker. (See Function types below for details.)\n\nA simple practical example of subtypes is shown in the diagram, right. The type \"bird\" has three subtypes \"duck\", \"cuckoo\" and \"ostrich\". Conceptually, each of these is a variety of the basic \"bird\" that inherits many \"bird\" characteristics but has some specific differences. The UML notation is used in this diagram, with open-headed arrows showing the direction and type of the relationship between the supertype and its subtypes.\n\nAs a more practical example, a language might allow integer values to be used wherever floating point values are expected (codice_1 <: codice_2), or it might define a generic type <samp>Number</samp> as a common supertype of integers and the reals. In this second case, we only have codice_1 <: codice_4 and codice_2 <: codice_4, but codice_1 and codice_2 are not subtypes of each other.\n\nProgrammers may take advantage of subtyping to write code in a more abstract manner than would be possible without it. Consider the following example:\n\nIf integer and real are both subtypes of codice_4, and an operator of comparison with an arbitrary Number is defined for both types, then values of either type can be passed to this function. However, the very possibility of implementing such an operator highly constrains the Number type (for example, one can't compare an integer with a complex number), and actually only comparing integers with integers and reals with reals makes sense. Rewriting this function so that it would only accept 'x' and 'y' of the same type requires bounded polymorphism.\n\nSubtyping in type theory is characterized by the fact that any expression of type \"A\" may also be given type \"B\" if \"A\"<samp><:</samp>\"B\"; the formal typing rule that codifies this is known as the \"subsumption\" rule.\n\nType theorists make a distinction between nominal subtyping, in which only types declared in a certain way may be subtypes of each other, and structural subtyping, in which the structure of two types determines whether or not one is a subtype of the other. The class-based object-oriented subtyping described above is nominal; a structural subtyping rule for an object-oriented language might say that if objects of type \"A\" can handle all of the messages that objects of type \"B\" can handle (that is, if they define all the same methods), then \"A\" is a subtype of \"B\" regardless of whether either inherits from the other. This so-called \"duck typing\" is common in dynamically typed object-oriented languages. Sound structural subtyping rules for types other than object types are also well known.\n\nImplementations of programming languages with subtyping fall into two general classes: \"inclusive\" implementations, in which the representation of any value of type \"A\" also represents the same value at type \"B\" if \"A\"<samp><:</samp>\"B\", and \"coercive\" implementations, in which a value of type \"A\" can be \"automatically converted\" into one of type \"B\". The subtyping induced by subclassing in an object-oriented language is usually inclusive; subtyping relations that relate integers and floating-point numbers, which are represented differently, are usually coercive.\n\nIn almost all type systems that define a subtyping relation, it is reflexive (meaning \"A\"<samp><:</samp>\"A\" for any type \"A\") and transitive (meaning that if \"A\"<samp><:</samp>\"B\" and \"B\"<samp><:</samp>\"C\" then \"A\"<samp><:</samp>\"C\"). This makes it a preorder on types.\n\nTypes of records give rise to the concepts of \"width\" and \"depth\" subtyping. These express two different ways of obtaining a new type of record that allows the same operations as the original record type.\n\nRecall that a record is a collection of (named) fields. Since a subtype is a type which allows all operations allowed on the original type, a record subtype should support the same operations on the fields as the original type supported.\n\nOne kind of way to achieve such support, called \"width subtyping\", adds more fields to the record. More formally, every (named) field appearing in the width supertype will appear in the width subtype. Thus, any operation feasible on the supertype will be supported by the subtype.\n\nThe second method, called \"depth subtyping\", replaces the various fields with their subtypes. That is, the fields of the subtype are subtypes of the fields of the supertype. Since any operation supported for a field in the supertype is supported for its subtype, any operation feasible on the record supertype is supported by the record subtype. Depth subtyping only makes sense for immutable records: for example, you can assign 1.5 to the 'x' field of a real point (a record with two real fields), but you can't do the same to the 'x' field of an integer point (which, however, is a deep subtype of the real point type) because 1.5 is not an integer (see Variance).\n\nSubtyping of records can be defined in System F, which combines parametric polymorphism with subtyping of record types and is a theoretical basis for many functional programming languages that support both features.\n\nSome systems also support subtyping of labeled disjoint union types (such as algebraic data types). The rule for width subtyping is reversed: every tag appearing in the width subtype must appear in the width supertype.\n\nIf \"T\" → \"T\" is a function type, then a subtype of it is any function type \"S\" → \"S\" with the property that \"T\" <: \"S\" and \"S\" <: \"T\". This can be summarised using the following typing rule: \nformula_1\nThe argument type of \"S\" → \"S\" is said to be contravariant because the subtyping relation is reversed for it, whereas the return type is covariant. Informally, this reversal occurs because the refined type is \"more liberal\" in the types it accepts and \"more conservative\" in the type it returns. This is what exactly works in Scala: a \"n\"-ary function is internally a class that inherits the FunctionN(-A1, -A2, …, -An, +B) trait (which can be seen as a general interface in Java-like languages), where \"A1\", \"A2\", … \"An\" are the parameter types, and \"B\" is its return type; \"-\" before the type means the type is contravariant while \"+\" means covariant.\n\nIn languages that allow side effects, like most object-oriented languages, subtyping is generally not sufficient to guarantee that a function can be safely used in the context of another. Liskov's work in this area focused on behavioral subtyping, which besides the type system safety discussed in this article also requires that subtypes preserve all invariants guaranteed by the supertypes in some contract. This definition of subtyping is generally undecidable, so it cannot be verified by a type checker.\n\nThe subtyping of mutable references is similar to the treatment of function arguments and return values. Write-only references (or \"sinks\") are contravariant, like function arguments; read-only references (or \"sources\") are covariant, like return values. Mutable references which act as both sources and sinks are invariant.\n\nSubtyping and inheritance are independent (orthogonal) relationships. They may coincide, but none is a special case of the other. In other words, between two types \"S\" and \"T\", all combinations of subtyping and inheritance are possible:\nThe first case is illustrated by independent types, such as codice_10 and codice_2.\n\nThe second case can be illustrated by codice_12 and codice_13; in most object oriented programming languages, codice_13 is not derived by inheritance from codice_12, however codice_16. Since an codice_12 value can always be replaced by an codice_13 value, the Liskov substitution principle is satisfied; therefore codice_13 can be considered a subtype of codice_12.\n\nThe third case is a consequence of function subtyping input contravariance. Assume a super class of type \"T\" having a method \"m\" returning an object of the same type (\"i.e.\" the type of \"m\" is \"T → T\", also note that the first argument of \"m\" is this/self) and a derived class type \"S\" from \"T\". By inheritance, the type of \"m\" in \"S\" is \"S → S\". In order for \"S\" to be a subtype of \"T\" the type of \"m\" in \"S\" must be a subtype of the type of \"m\" in \"T\", in other words: \"S → S ≤: T → T\". By bottom-up application of the function subtyping rule, this means: \"S ≤: T\" and \"T ≤: S\", which is only possible if \"S\" and \"T\" are the same. Since inheritance is an irreflexive relation, \"S\" can't be a subtype of \"T\".\n\nSubtyping and inheritance are compatible when all inherited fields and methods of the derived type have types which are subtypes of the corresponding fields and methods from the inherited type .\n\nIn coercive subtyping systems, subtypes are defined by implicit type conversion functions from subtype to supertype. For each subtyping relationship (\"S\" <: \"T\"), a coercion function \"coerce\": \"S\" → \"T\" is provided, and any object \"s\" of type \"S\" is regarded as the object \"coerce\"(\"s\") of type \"T\". A coercion function may be defined by composition: if \"S\" <: \"T\" and \"T\" <: \"U\" then \"s\" may be regarded as an object of type \"u\" under the compound coercion (\"coerce\" ∘ \"coerce\"). The type coercion from a type to itself \"coerce\" is the identity function \"id\"\n\nCoercion functions for records and disjoint union subtypes may be defined componentwise; in the case of width-extended records, type coercion simply discards any components which are not defined in the supertype. The type coercion for function types may be given by \"f\"'(\"s\") = \"coerce\"(\"f\"(\"coerce\"(\"t\"))), reflecting the contravariance of function arguments and covariance of return values.\n\nThe coercion function is uniquely determined given the subtype and supertype. Thus, when multiple subtyping relationships are defined, one must be careful to guarantee that all type coercions are coherent. For instance, if an integer such as 2 : \"int\" can be coerced to a floating point number (say, 2.0 : \"float\"), then it is not admissible to coerce 2.1 : \"float\" to 2 : \"int\", because the compound coercion \"coerce\" given by \"coerce\" ∘ \"coerce\" would then be distinct from the identity coercion \"id\".\n\n\nTextbooks\n\n\nPapers\n\n"}
{"id": "31110202", "url": "https://en.wikipedia.org/wiki?curid=31110202", "title": "TUN (product standard)", "text": "TUN (product standard)\n\nTUN is a Danish product standard numbering system identifying building materials, managed by Danish Timber & Building Merchants' Trade Organization (Trælasthandlerunionen). Currently more than 30,000 products are identified. TUN numbers are assigned to suppliers identifying products and therefore a product with several suppliers can have more than one TUN number. TUN numbers are currently being mapped against UNSPSC.\n\n"}
{"id": "239414", "url": "https://en.wikipedia.org/wiki?curid=239414", "title": "Teasing", "text": "Teasing\n\nTeasing has multiple meanings and uses. In human interactions, teasing exists in three major forms: \"playful\", \"hurtful\", and \"educative\". Teasing can have a variety of effects, depending on how it is utilized and its intended effect. When teasing is unwelcome, it may be regarded as harassment or mobbing, especially in the work place and school, or as a form of bullying or emotional abuse. If done in public, it may be regarded as humiliation. Teasing can also be regarded as educative when it is used as a way of Informal learning. Adults in some of the Indigenous American Communities often tease children to playfully illustrate and teach them how their behaviour negatively affects the community. Children in many Indigenous American Communities also learn by observing what others do in addition to collaborating with them. Along with teasing, this form of informal learning is different from the ways that Western American children learn. Informal ways of child learning include mutual responsibility, as well as active collaboration with adults and peers. This differentiates from the more formal way of learning because it is not adult-oriented.\n\nPeople may be teased on matters such as their appearance, weight, behavior, abilities, clothing, and intelligence. From the victim's point of view, this kind of teasing is often hurtful, irrespective of the intention of the teaser.\n\nOne may also tease an animal. Some animals, such as dogs and cats, may recognize this both as play or harassment.\n\nA common form of teasing is verbal bullying or taunting. This behavior is intended to distract, disturb, offend, sadden, anger, bother, irritate, or annoy the recipient. Because it is hurtful, it is different from joking and is generally accompanied by some degree of social rejection. Teasing can also be taken to mean \"To make fun of; mock playfully\" or be sarcastic about and use sarcasm.\n\nDacher Keltner utilizes Penelope Brown's classic study on the difference between \"on-record\" and \"off-record\" communication to illustrate how people must learn to read others' tone of voice and facial expressions in order to learn appropriate responses to teasing.\n\nA form of teasing that is usually overlooked is educational teasing. This form is commonly used by parents and caregivers in two Indigenous American Communities and Mexican Heritage communities to guide their children into responding with more Prosocial behavior. For example, when a parent teases a child who is throwing a tantrum for a piece of candy, the parent will pretend to give the child candy but then take it away and ask the child to correct their behavior before giving the child that piece of candy. In this way, the parent teaches the child the importance of maintaining self-control. When adults educate children through teasing, they are informally teaching the children. This type of learning is often overlooked because it is different from the way Western American Communities teach their children.\n\nAnother form of teasing is to pretend to give something which the other desires, or give it very slowly. This is usually done by arousing curiosity or desire, and may not actually involve the intent to satisfy or disclose. This form of teasing could be called \"tantalizing\", after the story of Tantalus. Tantalizing is generally playful among adults, although among children it can be hurtful, such as when one child acquires a possession of another's property and will not return it. It is also common in flirting and dating. For example, a man or woman who is interested in someone might reject an advance the first time in order to arouse interest and curiosity, and give in the second or third time.\n\nWhether teasing is playful or hurtful or educative is largely subject to the interpretation of the person being teased. If the person being teased feels harmed, then the teasing is hurtful. A difference in power between people may also make the behavior hurtful rather than playful. Ultimately though, if someone perceives him or herself as the victim of teasing, and experiences the teasing as unpleasant, then it is considered hurtful. If parents' intentions are positive, as in many Indigenous American Communities, then teasing to the community can be seen as an educational tool. The child may or may not understand that in the moment. If the other person continues to do it after being asked to stop then it is a form of bullying or abuse.\n\nAnother way to look at teasing is as an honest reflection on differences, expressed in a joking fashion with the goal of \"clearing the air\". It can express a comfort with the other which can be comforting. As opposed to being nice to someone's face while making disparaging remarks behind their back, teasing can be a way to express differences in a direct fashion rather than internalizing them.\n\nSome indigenous American communities use teasing to teach their children about the expectations and values of the community and to change negative behaviors. Teasing gives children a better understanding of how their behavior affects the people around them. Teasing in Indigenous American communities is used to learn community acceptance, humbleness, correcting a behavior and social control.\n\nIn some Mexican indigenous American communities, teasing is used in an effective educative way. Teasing is found more useful because it allows the child to feel and understand the relevant effect of their behavior instead of receiving out of context feedback. Some parents in Indigenous American communities believe it mildly embarrasses the children in a shared reference to give them a good sense of the consequences of their behavior. This type of teasing is thought to teach children to be less egocentric, teaches autonomy and responsibility to monitor their own behavior. Parental teasing also is practiced to encourage the child to think of their behavior in a social context. Some Indigenous American mothers have reported that this urges the children to understand how their behavior affects others around them. From examples in Eisenberg's article, parents use teasing as way of reinforcing relationships and participation in group/community activities (Prosocial behavior). Parents tease their children to be able to “control the behavior of the child and to have fun with them”.\n\nAn Inuit principal of learning that follows a similar teasing pattern is known as issumaksaiyuk, meaning to cause thought. Oftentimes, adults pose questions or hypothetical situations to the children (sometimes dangerous) but in a teasing, playful manner, often dramatizing their responses. These questions raise the child's awareness to issues surrounding their community, as well as give them a sense of agency within the community as a member capable of having an effect and creating change. Once the child begins to answer the questions reasonably, like an adult, the questions would stop.\n\nIn some Cherokee communities, teasing is a way of diffusing aggressive or hostile situations and teaching the individual about the consequences of their behavior. It allows the individual to feel how their behaviors are affecting others and control their behavior.\n\nTo tease, or to \"be a tease\" in a sexual sense can refer to the use of posture, language or other means of flirting to cause another person to become sexually aroused. Such teasing may or may not be a prelude to intercourse, an ambiguity which can lead to uncomfortable situations. In a more physical sense, it can also refer to sexual stimulation.\n\nTeasing is also used to describe playing part of a song at a concert. Jam bands will often quote the main riff of another song during jams.\n\n\"Tease it\" is also used as a slang term to smoke marijuana. The word \"tease\" can also be used as a noun to stand for marijuana.\n\nIn a very different context, hair can be teased, \"ratted\", or more correctly, \"backcombed\". As the name suggests, backcombing involves combing the hair backwards from end to root to intentionally tangle the strands to create volume. It can also be done excessively in sections to create dreadlocks.\n\n\n"}
{"id": "53241", "url": "https://en.wikipedia.org/wiki?curid=53241", "title": "The Elements of Style", "text": "The Elements of Style\n\nThe Elements of Style is a prescriptive American English writing style guide in numerous editions. The original was composed by William Strunk Jr., in 1918, and published by Harcourt, in 1920, comprising eight \"elementary rules of usage\", ten \"elementary principles of composition\", \"a few matters of form\", a list of 49 \"words and expressions commonly misused\", and a list of 57 \"words often misspelled\". E. B. White greatly enlarged and revised the book for publication by Macmillan in 1959. That was the first edition of the so-called \"Strunk & White\", which \"Time\" named in 2011 as one of the 100 best and most influential books written in English since 1923.\n\nCornell University English professor William Strunk Jr. wrote \"The Elements of Style\" in 1918 and privately published it in 1919, for use at the university. (Harcourt republished it in 52-page format in 1920.) Later, for publication, he and editor Edward A. Tenney revised it as \"The Elements and Practice of Composition\" (1935). In 1957, at \"The New Yorker\", the style guide reached the attention of E.B. White, who had studied writing under Strunk in 1919 but had since forgotten \"the little book\" that he described as a \"forty-three-page summation of the case for cleanliness, accuracy, and brevity in the use of English\". Weeks later, White wrote a feature story about Strunk's devotion to lucid English prose.\n\nMacmillan and Company subsequently commissioned White to revise \"The Elements\" for a 1959 edition (Strunk had died in 1946). White's expansion and modernization of Strunk and Tenney's 1935 revised edition yielded the writing style manual informally known as \"Strunk & White\", the first edition of which sold about two million copies in 1959. More than ten million copies of three editions were later sold. Mark Garvey relates the history of the book in \"Stylized: A Slightly Obsessive History of Strunk & White's The Elements of Style\" (2009).\n\nMaira Kalman, who provided the illustrations for \"The Elements of Style Illustrated\" (2005, see below), asked Nico Muhly to compose a cantata based on the book. It was performed at the New York Public Library in October 2005.\n\nIn \"The Elements of Style\" (1918), William Strunk concentrated on specific questions of usage—and the cultivation of good writing—with the recommendation \"Make every word tell\"; hence the 17th principle of composition is the simple instruction: \"Omit needless words.\" The book frames this within a triplet credited to an influential lecturer:\n\n\nThe 1959 edition features White's expansions of preliminary sections, the \"Introduction\" essay (derived from his magazine feature story about Prof. Strunk), and the concluding chapter, \"An Approach to Style\", a broader, prescriptive guide to writing in English. He also produced the second (1972) and third (1979) editions of \"The Elements of Style\", by which time the book's length had extended to 85 pages.\n\nThe third edition of \"The Elements of Style\" (1979) features 54 points: a list of common word-usage errors; 11 rules of punctuation and grammar; 11 principles of writing; 11 matters of form; and, in Chapter V, 21 reminders for better style. The final reminder, the 21st, \"Prefer the standard to the offbeat\", is thematically integral to the subject of \"The Elements of Style\", yet does stand as a discrete essay about writing lucid prose. To write well, White advises writers to have the proper mind-set, that they write to please themselves, and that they aim for \"one moment of felicity\", a phrase by Robert Louis Stevenson (1850–94). Thus Strunk's 1918 recommendation:\n\nStrunk Jr. no longer has a comma in his name in the 1979 and later editions, due to the modernized style recommendation about punctuating such names.\n\nThe fourth edition of \"The Elements of Style\" (2000), published 54 years after Strunk's death, omits his stylistic advice about masculine pronouns: \"unless the antecedent is or must be feminine\". In its place, the following sentence has been added: \"Currently, however, many writers find the use of the generic \"he\" or \"his\" to rename indefinite antecedents limiting or offensive.\" Further, the re-titled entry \"They. He or She\", in Chapter IV: Misused Words and Expressions, advises the writer to avoid an \"unintentional emphasis on the masculine\".\n\nComponents new to the fourth edition include a foreword by Roger Angell, stepson of E. B. White, an afterword by the American cultural commentator Charles Osgood, a glossary, and an index. Five years later, the fourth edition text was re-published as \"The Elements of Style Illustrated\" (2005), with illustrations by the designer Maira Kalman. This edition excludes the afterword by Charles Osgood and restores the first edition chapter on spelling.\n\n\"The Elements of Style\" was listed as one of the 100 best and most influential books written in English since 1923 by \"Time\" in its 2011 list. Upon its release, Charles Poor, writing for \"The New York Times\", called it \"a splendid trophy for all who are interested in reading and writing.\" American poet Dorothy Parker once proclaimedIf you have any young friends who aspire to become writers, the second-greatest favor you can do them is to present them with copies of \"The Elements of Style\". The first-greatest, of course, is to shoot them now, while they’re happy.\n\nCriticism of \"Strunk & White\" has largely focused on claims that it has a prescriptivist nature, or that it has become a general anachronism in the face of modern English usage.\n\nIn criticizing \"The Elements of Style\", Geoffrey Pullum, professor of linguistics at the University of Edinburgh, and co-author of \"The Cambridge Grammar of the English Language\" (2002), said that: \n\nPullum has argued, for example, that the authors misunderstood what constitutes the passive voice, and he criticized their proscription of established and unproblematic English usages, such as the split infinitive and the use of \"which\" in a restrictive relative clause. On \"Language Log\", a blog about language written by linguists, he further criticized \"The Elements of Style\" for promoting linguistic prescriptivism and hypercorrection among Anglophones, and called it \"the book that ate America's brain\".\n\n\"The Boston Globe\" review described \"The Elements of Style Illustrated\" (2005), with illustrations by Maira Kalman, as an \"aging zombie of a book ... a hodgepodge, its now-antiquated pet peeves jostling for space with 1970s taboos and 1990s computer advice\".\n\nOn the other hand, in \"\" (2000, p. 11), Stephen King writes: \"There is little or no detectable bullshit in that book. (Of course, it's short; at eighty-five pages it's much shorter than this one.) I'll tell you right now that every aspiring writer should read The Elements of Style. Rule 17 in the chapter titled Principles of Composition is 'Omit needless words.' I will try to do that here.\"\n\nIn 2011, Tim Skern wrote that \"The Elements of Style\" \"remains the best book available on writing good English\".\n\nIn 2013, Nevile Gwynne reproduced \"The Elements of Style\" in his work, \"Gwynne's Grammar\". Britt Peterson of the \"Boston Globe\" wrote that it was a \"curious addition\".\n\nIn 2016, the Open Syllabus Project lists \"The Elements of Style\" as the most frequently assigned text in US academic syllabi, based on an analysis of 933,635 texts appearing in over 1 million syllabi.\n\n\n\n\n\nSeveral books were titled paying homage to Strunk's, for example:\n\n\n"}
{"id": "8171240", "url": "https://en.wikipedia.org/wiki?curid=8171240", "title": "The Machine That Changed the World (book)", "text": "The Machine That Changed the World (book)\n\nThe Machine That Changed the World is a 1991 book based on the Massachusetts Institute of Technology's $5 million, five-year study on the future of the automobile, written by James P. Womack, Daniel T. Jones, and Daniel Roos. \nThis book made the term lean production known worldwide. It has been translated into eleven languages and has been sold more than 600,000 times. A revised edition was published in 2007.\n\nJames P. Womack was a highly-regarded professor and authority on systems engineering at MIT. He went on to become the founder of the Lean Enterprise Institute, Inc. Co-authors of the book, Daniel Jones and Daniel Roos have also authored many other well-respected books on lean manufacturing and engineering techniques.\n\nIn his book review, MIT then-student wrote that the history of lean manufacturing began with artisans prior to the industrial revolution when standardized supplies were not yet available to enable large production runs. It wasn't until Henry Ford revolutionized mass production for his automobiles that made it possible for minimally-trained workers to assemble cars quickly and efficiently. Japanese auto manufacturer Toyota modified the process into the first true \"lean\" method of production. They were able to eliminate much of the waste inherent in Ford's system, making smaller batches of parts to be used as needed as opposed to stocking larger quantities. Toyota also empowered its workers to improve the process and stop the line when issues and errors occurred. This new, lean method required communications to flow in both directions and increased quality while reducing time and costs. The authors, Womack, Jones, and Roos, suggest that lean production can be used outside of automobile manufacturing by adapting its principles to traditional mass production of many kinds.\n\n\n"}
{"id": "34307401", "url": "https://en.wikipedia.org/wiki?curid=34307401", "title": "Thinking, Fast and Slow", "text": "Thinking, Fast and Slow\n\nThinking, Fast and Slow is a best-selling book published in 2011 by Nobel Memorial Prize in Economic Sciences laureate Daniel Kahneman. It was the 2012 winner of the National Academies Communication Award for best creative work that helps the public understanding of topics in behavioral science, engineering and medicine.\n\nThe book summarizes research that Kahneman conducted over decades, often in collaboration with Amos Tversky. It covers all three phases of his career: his early days working on cognitive biases, his work on prospect theory, and his later work on happiness.\n\nThe central thesis is a dichotomy between two modes of thought: \"System 1\" is fast, instinctive and emotional; \"System 2\" is slower, more deliberative, and more logical. The book delineates cognitive biases associated with each type of thinking, starting with Kahneman's own research on loss aversion. From framing choices to people's tendency to replace a difficult question with one which is easy to answer, the book highlights several decades of academic research to suggest that people place too much confidence in human judgment.\n\nIn the book's first section, Kahneman describes two different ways the brain forms thoughts:\n\n\nKahneman covers a number of experiments which purport to highlight the differences between these two thought systems and how they arrive at different results even given the same inputs. Terms and concepts include coherence, attention, laziness, association, jumping to conclusions, WYSIATI (What you see is all there is), and how one forms judgments. The System 1 vs. System 2 debate dives into the reasoning or lack thereof for human decision making, with big implications for many areas including law and market research.\n\nThe second section offers explanations for why humans struggle to think statistically. It begins by documenting a variety of situations in which we either arrive at binary decisions or fail to precisely associate reasonable probabilities with outcomes. Kahneman explains this phenomenon using the theory of heuristics. Kahneman and Tversky originally covered this topic in their landmark 1974 article titled Judgment under Uncertainty: Heuristics and Biases.\n\nKahneman uses heuristics to assert that System 1 thinking involves associating new information with existing patterns, or thoughts, rather than creating new patterns for each new experience. For example, a child who has only seen shapes with straight edges would experience an octagon rather than a triangle when first viewing a circle. In a legal metaphor, a judge limited to heuristic thinking would only be able to think of similar historical cases when presented with a new dispute, rather than seeing the unique aspects of that case. In addition to offering an explanation for the statistical problem, the theory also offers an explanation for human biases.\nThe \"anchoring effect\" names our tendency to be influenced by irrelevant numbers. Shown higher/lower numbers, experimental subjects gave higher/lower responses.\n\nThis is an important concept to have in mind when navigating a negotiation or considering a price. As an example, most people, when asked whether Gandhi was more than 114 years old when he died, will provide a much larger estimate of his age at death than others who were asked whether Gandhi was more or less than 35 years old. Experiments show that our behavior is influenced, much more than we know or want, by the environment of the moment.\nThe availability heuristic is a mental shortcut that occurs when people make judgments about the probability of events on the basis of how easy it is to think of examples. The availability heuristic operates on the notion that, \"if you can think of it, it must be important.\" The availability of consequences associated with an action is positively related to perceptions of the magnitude of the consequences of that action. In other words, the easier it is to recall the consequences of something, the greater we perceive these consequences to be. Sometimes, this heuristic is beneficial, but the frequencies at which events come to mind are usually not accurate reflections of the probabilities of such events in real life.\nSystem 1 is prone to substituting a difficult question with a simpler one. In what Kahneman calls their \"best-known and most controversial\" experiment, \"the Linda problem,\" subjects were told about an imaginary Linda, young, single, outspoken, and very bright, who, as a student, was deeply concerned with discrimination and social justice. They asked whether it was more probable that Linda is a bank teller or that she is a bank teller and an active feminist. The overwhelming response was that \"feminist bank teller\" was more likely than \"bank teller,\" violating the laws of probability. (Every feminist bank teller is a bank teller.) In this case System 1 substituted the easier question, \"Is Linda a feminist?\", dropping the occupation qualifier. An alternative view is that the subjects added an unstated cultural implicature to the effect that the other answer implied an exclusive or (xor), that Linda was not a feminist.\nKahneman writes of a \"pervasive optimistic bias\", which \"may well be the most significant of the cognitive biases.\" This bias generates the illusion of control, that we have substantial control of our lives.\n\nA natural experiment reveals the prevalence of one kind of unwarranted optimism. The planning fallacy is the tendency to overestimate benefits and underestimate costs, impelling people to take on risky projects. In 2002, American kitchen remodeling was expected on average to cost $18,658, but actually cost $38,769.\n\nTo explain overconfidence, Kahneman introduces the concept he labels \"What You See Is All There Is\" (WYSIATI). This theory states that when the mind makes decisions, it deals primarily with \"Known Knowns\", phenomena it has already observed. It rarely considers \"Known Unknowns\", phenomena that it knows to be relevant but about which it has no information. Finally it appears oblivious to the possibility of \"Unknown Unknowns\", unknown phenomena of unknown relevance.\n\nHe explains that humans fail to take into account complexity and that their understanding of the world consists of a small and necessarily un-representative set of observations. Furthermore, the mind generally does not account for the role of chance and therefore falsely assumes that a future event will mirror a past event.\nFraming is the context in which choices are presented. Experiment: subjects were asked whether they would opt for surgery if the \"survival\" rate is 90 percent, while others were told that the mortality rate is 10 percent. The first framing increased acceptance, even though the situation was no different.\nRather than consider the odds that an incremental investment would produce a positive return, people tend to \"throw good money after bad\" and continue investing in projects with poor prospects that have already consumed significant resources. In part this is to avoid feelings of regret.\n\nThis section of the book is dedicated to the undue confidence in what the mind believes it knows. It suggests that people often overestimate how much they understand about the world and underestimate the role of chance in particular. This is related to the excessive certainty of hindsight, when an event appears to be understood after it has occurred or developed. Kahneman's views on overconfidence are influenced by Nassim Nicholas Taleb.\n\nIn this section Kahneman returns to economics and expands his seminal work on Prospect Theory. He discusses the tendency for problems to be addressed in isolation and how, when other reference points are considered, the choice of that reference point (called a frame) has a disproportionate impact on the outcome. This section also offers advice on how some of the shortcomings of System 1 thinking can be avoided.\nKahneman developed prospect theory, the basis for his Nobel prize, to account for experimental errors he noticed in Daniel Bernoulli's traditional utility theory. According to Kahneman, Utility Theory makes logical assumptions of economic rationality that do not reflect people's actual choices, and does not take into account cognitive biases.\n\nOne example is that people are loss-averse: they are more likely to act to avert a loss than to achieve a gain. Another example is that the value people place on a change in probability (e.g., of winning something) depends on the reference point: people appear to place greater value on a change from 0% to 10% (going from impossibility to possibility) than from, say, 45% to 55%, and they place the greatest value of all on a change from 90% to 100% (going from possibility to certainty). This occurs despite the fact that under traditional utility theory all three changes give the same increase in utility. Consistent with loss-aversion, the order of the first and third of those is reversed when the event is presented as losing rather than winning something: there, the greatest value is placed on eliminating the probability of a loss to 0.\n\nAfter the book's publication, the \"Journal of Economic Literature\" published a thorough discussion of its take on prospect theory, as well as an analysis of the four fundamental factors that it rests on.\n\nThe fifth part of the book describes recent evidence which introduces a distinction between two selves, the 'experiencing self' and 'remembering self'.\nKahneman proposed an alternative measure that assessed pleasure or pain sampled from moment to moment, and then summed over time. Kahneman called this \"experienced\" well-being and attached it to a separate \"self.\" He distinguished this from the \"remembered\" well-being that the polls had attempted to measure. He found that these two measures of happiness diverged. \nThe author's significant discovery was that the remembering self does not care about the duration of a pleasant or unpleasant experience. Instead, it retrospectively rates an experience by the peak (or valley) of the experience, and by the way it ends. The remembering self dominated the patient's ultimate conclusion.\nKahneman first took up the study of well-being in the 1990s. At the time most happiness research relied on polls about life satisfaction. Having arrived at the subject from previously studying unreliable memories, the author was doubtful of the question of life satisfaction as a good indicator of happiness. He designed a question that focused instead on the well-being on the experiencing self. The author proposed that \"Helen was happy in the month of March\" if she spent most of her time engaged in activities that she would rather continue than stop, little time in situations that she wished to escape, and not too much time in a neutral state that wouldn't prefer continuing or stopping the activity either way.\n\n\nKahneman suggests that focusing on a life event such as a marriage or a new car can provide a distorted illusion of its true value. This \"focusing illusion\" revisits earlier ideas of substituting difficult questions and WYSIATI.\n\n\nSince the book's publication it has sold over 1.5 million copies worldwide. On the year of its publication, it was on the New York Times Bestseller List. The book was reviewed in media including the \"Huffington Post\", \"The Guardian\", \"The New York Times\", \"The Financial Times\", \"The Independent\", \"Bloomberg\" and \"The New York Review of Books\".\n\nThe book was widely reviewed in specialist journals, including the \"Journal of Economic Literature\", \"American Journal of Education\", \"The American Journal of Psychology\", \"Planning Theory\", \"The American Economist\", \"The Journal of Risk and Insurance\", \"The Michigan Law Review\", \"American Scientist\", \"Contemporary Sociology\", \"Science\", \"Contexts\", \"The Wilson Quarterly\", \"Technical Communication\", \"The University of Toronto Law Journal\", \"A Review of General Semantics\" and \"Scientific American Mind\".\n\nThe book was also reviewed in an annual magazine by \"The Association of Psychological Science\".\n\n\n"}
{"id": "52687491", "url": "https://en.wikipedia.org/wiki?curid=52687491", "title": "Zero-sum thinking", "text": "Zero-sum thinking\n\nZero-sum thinking, also known as zero-sum bias, is a cognitive bias that describes when an individual thinks that one situation is like a zero-sum game, where one person's gain would be another's loss. The term is derived from game theory. However, unlike the game theory concept, zero-sum thinking refers to a psychological construct—a person's subjective interpretation of a situation. Zero-sum thinking is captured by the saying \"your gain is my loss\" (or conversely, \"your loss is my gain\"). Rozycka-Tran et al. (2015) defined zero-sum thinking as:\"A general belief system about the antagonistic nature of social relations, shared by people in a society or culture and based on the implicit assumption that a finite amount of goods exists in the world, in which one person’s winning makes others the losers, and vice versa [...] a relatively permanent and general conviction that social relations are like a zero-sum game. People who share this conviction believe that success, especially economic success, is possible only at the expense of other people’s failures.\" (pps. 526–528).\n\nThere are many examples of zero-sum thinking.\n\nZero-sum thinking is the result of both proximate and ultimate causes.\n\nIn terms of ultimate causation, zero-sum thinking might be a legacy of human evolution. Specifically, it might be understood to be a psychological adaptation that facilitated successful resource competition in the environment of ancestral humans where resources like mates, status, and food were perpetually scarce. For example, Rubin suggests that the pace of technological growth was so slow during the period in which modern humans evolved that no individual would have observed any growth during their lifetime: \"Each person would live and die in a world of constant technology and income. Thus, there was no incentive to evolve a mechanism for understanding or planning for growth\" (p. 162). Rubin also points to instances where the understanding of laypeople and economists about economic situations diverge, such as the lump-of-labor fallacy. From this perspective, zero-sum thinking might be understood as the default way that humans think about resource allocations, which must be unlearned by, for example, an education in basic economics.\n\nZero-sum thinking can also be understood in terms of proximate causation, which refers to the developmental history of individuals within their own lifetime. The proximate causes of zero-sum thinking include the experiences that individuals have with resource allocations, as well as their beliefs about specific situations, or their beliefs about the world in general.\n\nOne of the proximate causes of zero-sum thinking is the experiences that individuals have with scarce resources or zero-sum interactions in their developmental environment. In 1965, George M. Foster argued that members of \"peasant\" societies have an \"Image of Limited Good,\" which he argued was learned through by experiences in a society that was essentially zero-sum.\"The model of cognitive orientation that seems to me best to account for peasant behavior is the \"Image of Limited Good.\" By \"Image of Limited Good\" I mean that broad areas of peasant behavior are patterned in such fashion as to suggest that peasants view their social, economic, and natural universes—their total environment—as one in which all of the desired things in life such as land, wealth, health, friendship and love, manliness and honor, respect and status, power and influence, security and safety, exist in finite quantity and are always in short supply, as far as the peasant is concerned. Not only do these and all other \"good things\" exist in finite and limited quantities, but in addition there is no way directly within peasant power to increase the available quantities [...] When the peasant views his economic world as one in which Limited Good prevails, and he can progress only at the expense of another, he is usually very near the truth.\" (pps. 67-68) More recently, Rozycka-Tran et al. (2015) conducted a cross-cultural study that compared the responses of individuals in 37 nations to a scale of zero-sum beliefs. This scale asked individuals to report their agreement with statements that measured zero-sum thinking. For example, one item on the scale stated that \"Successes of some people are usually failures of others\". Rozycka-Tran et al. found that individuals in countries with lower Gross Domestic Product showed stronger zero-sum beliefs on average, suggesting that \"the belief in zero-sum game seems to arise in countries with lower income, where resources are scarce\" (p. 539). Similarly, Rozycka-Tran et al. found that individuals with lower socioeconomic status displayed stronger zero-sum beliefs.\n\nRelated to experiences with resource-scarce environments is the belief that a resource is scarce or finite. For example, the lump of labour fallacy refers to the belief that in the economy there is a fixed amount of work to be done, and thus the allocation of jobs is zero-sum. Similarly, there is the belief that the amount of wealth in the economy is fixed, and so when some individuals become more wealthy, it is at the expense of others (because there is assumed to be no growth in the \"pie\"). Although the belief that a resource is scarce might develop through experiences with resource scarcity, this is not necessarily the case. For example, individuals might come to believe that wealth is finite because it is a claim that has been repeated by politicians or journalists.\n\nAnother proximate cause of zero-sum thinking is the belief that one (or one's group) is entitled to a certain share of a resource. An extreme case is the belief that one is entitled to all of a resource that exists, implying that any gains by another is one's own loss. Less extreme is the belief that one (or one's group) is superior and therefore entitled to more than others. For example, perceptions of zero-sum group competition have been associated with the Dominance sub-scale of the Social Dominance Orientation personality trait, which itself has been characterized as a zero-sum worldview (\"a view of human existence as zero-sum,\" p. 999). Individuals who practice monogamy have also been found to think about love in consensually nonmonogamous relationships as zero-sum, and it was suggested that this might be because they believe that individuals in romantic relationships have an entitlement to their partner's love.\n\nWhen individuals think that a situation is zero-sum, they will be more likely to act competitively (or less cooperatively) towards others, because they will see others as a competitive threat. For example, when students think that they are being graded on a curve—a grading scheme that makes the allocation of grades zero-sum—they will be less likely to provide assistance to a peer who is proximate in status to themselves, because that peer's gain could be their own loss. When individuals perceive that there is a zero-sum competition in society for resources like jobs, they will be less likely to hold pro-immigration attitudes (because immigrants would deplete the resource). Zero-sum thinking may also lead to certain social prejudices. When individuals hold zero-sum beliefs about love in romantic relationships, they are more prejudiced against consensual nonmonogamists (presumably because the perception of zero-sumness makes consensual nonmonogamy seem inadequate or unfair).\n\n"}
