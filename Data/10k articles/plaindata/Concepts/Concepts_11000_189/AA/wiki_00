{"id": "38106", "url": "https://en.wikipedia.org/wiki?curid=38106", "title": "Actual infinity", "text": "Actual infinity\n\nIn the philosophy of mathematics, the abstraction of actual infinity involves the acceptance (if the axiom of infinity is included) of infinite entities, such as the set of all natural numbers or an infinite sequence of rational numbers, as given, actual, completed objects. This is contrasted with potential infinity, in which a non-terminating process (such as \"add 1 to the previous number\") produces a sequence with no last element, and each individual result is finite and is achieved in a finite number of steps.\n\nThe ancient Greek term for the potential or improper infinite was \"apeiron\" (unlimited or indefinite), in contrast to the actual or proper infinite \"aphorismenon\". \"Apeiron\" stands opposed to that which has a \"peras\" (limit). These notions are today denoted by \"potentially infinite\" and \"actually infinite\", respectively.\n\nAnaximander (610–546 BC) held that the \" apeiron\" was the principle or main element composing all things. Clearly, the 'apeiron' was some sort of basic substance. Plato's notion of the \"apeiron\" is more abstract, having to do with indefinite variability. The main dialogues where Plato discusses the 'apeiron' are the late dialogues \"Parmenides\" and the \"Philebus\".\n\nAristotle sums up the views of his predecessors on infinity thus:\n\"Only the Pythagoreans place the infinite among the objects of sense (they do not regard number as separable from these), and assert that what is outside the heaven is infinite. Plato, on the other hand, holds that there is no body outside (the Forms are not outside because they are nowhere), yet that the infinite is present not only in the objects of sense but in the Forms also. (Aristotle)\"\n\nThe theme was brought forward by Aristotle's consideration of the apeiron in the context of mathematics and physics (the study of nature).\n\nInfinity turns out to be the opposite of what people say it is. It is not 'that which has nothing beyond itself' that is infinite, but 'that which always has something beyond itself'. (Aristotle)Belief in the existence of the infinite comes mainly from five considerations:\nAristotle postulated that an actual infinity was impossible, because if it were possible, then something would have attained infinite magnitude, and would be \"bigger than the heavens.\" However, he said, mathematics relating to infinity was not deprived of its applicability by this impossibility, because mathematicians did not need the infinite for their theorems, just an arbitrarily large finite magnitude.\n\nAristotle handled the topic of infinity in \"Physics\" and in \"Metaphysics\". He distinguished between \"actual\" and \"potential\" infinity. \"Actual infinity\" is completed and definite, and consists of infinitely many elements. \"Potential infinity\" is never complete: elements can be always added, but never infinitely many. \n\nAristotle distinguished between infinity with respect to addition and division.\n\n\"As an example of a potentially infinite series in respect to increase, one number can always be added after another in the series that starts 1,2,3... but the process of adding more and more numbers cannot be exhausted or completed.\"\n\nWith respect to division, a potentially infinite sequence of divisions starts e.g. as 1, 0.5, 0.25, 0.125, 0.0625, but the process division cannot be exhausted or completed.\n\nIn mathematics, the infinite series 1 + 1/2 + 1/4 + 1/8 + 1/16 + · · · is an elementary example of a geometric series that converges.\n\nThe overwhelming majority of scholastic philosophers adhered to the motto \"Infinitum actu non datur\". This means there is only a (developing, improper, \"syncategorematic\") \"potential infinity\" but not a (fixed, proper, \"categorematic\") \"actual infinity\". There were exceptions, however, for example in England. \n\nIt is well known that in the Middle Ages all scholastic philosophers advocate Aristotle's \"infinitum actu non datur\" as an irrefutable principle. (G. Cantor [3, p. 174])\n\nThe number of points in a segment one ell long is its true measure. (R. Grosseteste [9, p. 96])\n\nActual infinity exists in number, time and quantity. (J. Baconthorpe [9, p. 96])\n\nDuring the Renaissance and by early modern times the voices in favor of actual infinity were rather rare.\n\nThe continuum actually consists of infinitely many indivisibles (G. Galilei [9, p. 97])\n\nI am so in favour of actual infinity. (G.W. Leibniz [9, p. 97])\n\nThe majority agreed with the well-known quote of Gauss:\n\nI protest against the use of infinite magnitude as something completed, which is never permissible in mathematics. Infinity is merely a way of speaking, the true meaning being a limit which certain ratios approach indefinitely close, while others are permitted to increase without restriction. (C.F. Gauss [in a letter to Schumacher, 12 July 1831])\n\nThe drastic change was initialized by Bolzano and Cantor in the 19th century.\n\nBernard Bolzano who introduced the notion of \"set\" (in German: \"Menge\") and Georg Cantor who introduced set theory opposed the general attitude. Cantor distinguished three realms of infinity: (1) the infinity of God (which he called the \"absolutum\"), (2) the infinity of reality (which he called \"nature\") and (3) the transfinite numbers and sets of mathematics.\n\nA multitude which is larger than any finite multitude, i.e., a multitude with the property that every finite set [of members of the kind in question] is only a part of it, I will call an infinite multitude. (B. Bolzano [2, p. 6])\n\nThere are twice as many focuses as centres of ellipses. (B. Bolzano [2a, § 93])\n\nAccordingly I distinguish an eternal uncreated infinity or absolutum which is due to God and his attributes, and a created infinity or transfinitum, which has to be used wherever in the created nature an actual infinity has to be noticed, for example, with respect to, according to my firm conviction, the actually infinite number of created individuals, in the universe as well as on our earth and, most probably, even in every arbitrarily small extended piece of space. (G. Cantor [3, p. 399; 8, p. 252])\n\nOne proof is based on the notion of God. First, from the highest perfection of God, we infer the possibility of the creation of the transfinite, then, from his all-grace and splendor, we infer the necessity that the creation of the transfinite in fact has happened. (G. Cantor [3, p. 400])\n\nThe numbers are a free creation of human mind. (R. Dedekind [3a, p. III])\n\nThe mathematical meaning of the term \"actual\" in actual infinity is synonymous with definite, completed, extended or existential,<ref name=\"Kleene 1952/1971:48.\">Kleene 1952/1971:48.</ref> but not to be mistaken for \"physically existing\". The question of whether natural or real numbers form definite sets is therefore independent of the question of whether infinite things exist physically in nature.\n\nProponents of intuitionism, from Kronecker onwards, reject the claim that there are actually infinite mathematical objects or sets. Consequently, they reconstruct the foundations of mathematics in a way that does not assume the existence of actual infinities. On the other hand, constructive analysis does accept the existence of the completed infinity of the integers.\n\nFor intuitionists, infinity is described as \"potential\"; terms synonymous with this notion are \"becoming\" or \"constructive\". For example, Stephen Kleene describes the notion of a Turing machine tape as \"a linear 'tape', (potentially) infinite in both directions.\" To access memory on the tape, a Turing machine moves a \"read head\" along it in finitely many steps: the tape is therefore only \"potentially\" infinite, since while there is always the ability to take another step, infinity itself is never actually reached.\n\nMathematicians generally accept actual infinities. Georg Cantor is the most significant mathematician who defended actual infinities, equating the Absolute Infinite with God. He decided that it is possible for natural and real numbers to be definite sets, and that if one rejects the axiom of Euclidean finiteness (that states that actualities, singly and in aggregates, are necessarily finite), then one is not involved in any contradiction.\n\nThe philosophical problem of actual infinity concerns whether the notion is coherent and epistemically sound.\n\nClassical set theory accepts the notion of actual, completed infinities. However, some finitist philosophers of mathematics and constructivists object to the notion.If the positive number \"n\" becomes infinitely great, the expression 1/\"n\" goes to naught (or gets infinitely small). In this sense one speaks of the improper or potential infinite. In sharp and clear contrast the set just considered is a readily finished, locked infinite set, fixed in itself, containing infinitely many exactly defined elements (the natural numbers) none more and none less. (A. Fraenkel [4, p. 6])Thus the conquest of actual infinity may be considered an expansion of our scientific horizon no less revolutionary than the Copernican system or than the theory of relativity, or even of quantum and nuclear physics. (A. Fraenkel [4, p. 245])\n\nTo look at the universe of all sets not as a fixed entity but as an entity capable of \"growing\", i.e., we are able to \"produce\" bigger and bigger sets. (A. Fraenkel et al. [5, p. 118])\n\nIntuitionists reject the very notion of an arbitrary sequence of integers, as denoting something finished and definite as illegitimate. Such a sequence is considered to be a growing object only and not a finished one. (A. Fraenkel et al. [5, p. 236])\n\nUntil then, no one envisioned the possibility that infinities come in different sizes, and moreover, mathematicians had no use for “actual infinity.” The arguments using infinity, including the Differential Calculus of Newton and Leibniz, do not require the use of infinite sets. (T. Jech )\n\nOwing to the gigantic simultaneous efforts of Frege, Dedekind and Cantor, the infinite was set on a throne and revelled in its total triumph. In its daring flight the infinite reached dizzying heights of success. (D. Hilbert [6, p. 169])\n\nOne of the most vigorous and fruitful branches of mathematics [...] a paradise created by Cantor from which nobody shall ever expel us [...] the most admirable blossom of the mathematical mind and altogether one of the outstanding achievements of man's purely intellectual activity. (D. Hilbert on set theory [6])\n\nFinally, let us return to our original topic, and let us draw the conclusion from all our reflections on the infinite. The overall result is then: The infinite is nowhere realized. Neither is it present in nature nor is it admissible as a foundation of our rational thinking – a remarkable harmony between being and thinking. (D. Hilbert [6, 190])\n\nInfinite totalities do not exist in any sense of the word (i.e., either really or ideally). More precisely, any mention, or purported mention, of infinite totalities is, literally, meaningless. (A. Robinson [10, p. 507])\n\nIndeed, I think that there is a real need, in formalism and elsewhere, to link our understanding of mathematics with our understanding of the physical world. (A. Robinson)\n\nGeorg Cantor's grand meta-narrative, Set Theory, created by him almost singlehandedly in the span of about fifteen years, resembles a piece of high art more than a scientific theory. (Y. Manin )\n\nThus, exquisite minimalism of expressive means is used by Cantor to achieve a sublime goal: understanding infinity, or rather infinity of infinities. (Y. Manin )\n\nThere is no actual infinity, that the Cantorians have forgotten and have been trapped by contradictions. (H. Poincaré [Les mathématiques et la logique III, Rev. métaphys. morale 14 (1906) p. 316])\n\nWhen the objects of discussion are linguistic entities [...] then that collection of entities may vary as a result of discussion about them. A consequence of this is that the \"natural numbers\" of today are not the same as the \"natural numbers\" of yesterday. (D. Isles )\n\nThere are at least two different ways of looking at the numbers: as a completed infinity and as an incomplete infinity... regarding the numbers as an incomplete infinity offers a viable and interesting alternative to regarding the numbers as a completed infinity, one that leads to great simplifications in some areas of mathematics and that has strong connections with problems of computational complexity. (E. Nelson )\n\nDuring the renaissance, particularly with Bruno, actual infinity transfers from God to the world. The finite world models of contemporary science clearly show how this power of the idea of actual infinity has ceased with classical (modern) physics. Under this aspect, the inclusion of actual infinity into mathematics, which explicitly started with G. Cantor only towards the end of the last century, seems displeasing. Within the intellectual overall picture of our century ... actual infinity brings about an impression of anachronism. (P. Lorenzen)\n\n\n"}
{"id": "25902548", "url": "https://en.wikipedia.org/wiki?curid=25902548", "title": "Article 9 of the Constitution of Singapore", "text": "Article 9 of the Constitution of Singapore\n\nArticle 9 of the Constitution of the Republic of Singapore, specifically Article 9(1), guarantees the right to life and the right to personal liberty. The Court of Appeal has called the right to life the most basic of human rights, but has yet to fully define the term in the Constitution. Contrary to the broad position taken in jurisdictions such as Malaysia and the United States, the High Court of Singapore has said that personal liberty only refers to freedom from unlawful incarceration or detention.\n\nArticle 9(1) states that persons may be deprived of life or personal liberty \"in accordance with law\". In \"Ong Ah Chuan v. Public Prosecutor\" (1980), an appeal to the Judicial Committee of the Privy Council from Singapore, it was held that the term \"law\" means more than just legislation validly enacted by Parliament, and includes fundamental rules of natural justice. Subsequently, in \"Yong Vui Kong v. Attorney-General\" (2011), the Court of Appeal held that such fundamental rules of natural justice embodied in the Constitution are the same in nature and function as common law rules of natural justice in administrative law, except that they operate at different levels of the legal order. A related decision, \"Yong Vui Kong v. Public Prosecutor\" (2010), apparently rejected the contention that Article 9(1) entitles courts to examine the substantive fairness of legislation, though it asserted a judicial discretion to reject bills of attainder and absurd or arbitrary legislation. In the same case, the Court of Appeal held that \"law\" in Article 9(1) does not include rules of customary international law.\n\nOther subsections of Article 9 enshrine rights accorded to persons who have been arrested, namely, the right to apply to the High Court to challenge the legality of their detention, the right to be informed of the grounds of arrest, the right to counsel, and the right to be produced before a magistrate within 48 hours of arrest. These rights do not apply to enemy aliens or to persons arrested for contempt of Parliament. The Constitution also specifically exempts the Criminal Law (Temporary Provisions) Act (), the Internal Security Act (), and Part IV of the Misuse of Drugs Act () from having to comply with Article 9.\n\nArticle 9 of the Constitution of the Republic of Singapore guarantees to all persons the right to life and right to personal liberty. It states:\n\nArticle 9(1) embodies the concept of the rule of law, an early expression of which was the 39th article of the Magna Carta of 1215: \"No freeman shall be taken captive or imprisoned, or deprived of his lands, or outlawed, or exiled, or in any way destroyed, nor will we go with force against him nor send forces against him, except by the lawful judgment of his peers or by the law of the land.\" Article 9(1) is similar, but by no means identical, to the Due Process Clause of the Fourteenth Amendment to the United States Constitution which prohibits any state from denying \"any person of life, liberty, or property, without due process of law\", and to Article 21 of the Constitution of India which states: \"No person shall be deprived of his life or personal liberty except according to procedure established by law.\" Article 5(1) of the Constitution of Malaysia and Singapore's Article 9(1) are worded the same way as the latter was adopted in 1965 from the former following Singapore's independence from Malaysia.\n\nIn \"Yong Vui Kong v. Public Prosecutor\" (2010), the Court of Appeal of Singapore called the right to life \"the most basic of human rights\". However, the courts have not yet had the opportunity to define the term \"life\" in Article 9(1).\n\nJurisdictions such as India, Malaysia and the United States interpret the same term in their respective constitutions broadly. In the United States Supreme Court case \"Munn v. Illinois\" (1877), Justice Stephen Johnson Field stated that the term \"life\" means more than mere animal existence. Rather, the definition extends to all those limbs and faculties by which life is enjoyed. His rationale was that the term should not be \"construed in any narrow or restricted sense\". Indian courts have likewise adopted a broad interpretation of \"life\" in Article 21 of the Indian Constitution to mean more than mere existence – instead, it includes the right to livelihood and the right to a healthy environment. Subsequently, in \"Samatha v. State of Andhra Pradesh\" (1997), the meaning of \"life\" was expanded to include the right to live with human dignity; and to the provision of minimum sustenance, shelter, and those other rights and aspects of life that make life meaningful and worth living. Similarly, Justice Prafullachandra Natwarlal Bhagwati held in \"Bandhua Mukti Morcha v. Union of India\" (1984) that the expression \"life\" included the right to be free from exploitation, and to the basic essentials of life included in the Directive Principles of State Policy that appear in the Indian Constitution.\n\nIn the Malaysian case \"Tan Tek Seng v. Suruhanjaya Perkhidmatan Pendidikan\" (1996), the appellant had appealed against his wrongful dismissal from employment on the grounds of procedural unfairness. One of the issues brought up was whether an unfair procedure meant that he had been deprived of his constitutional right to life or liberty protected by Article 5(1) of the Malaysian Constitution, which is identical to Singapore's Article 9(1). Judge of the Court of Appeal Gopal Sri Ram held that the courts should take into consideration the unique characteristics and situation of the country, and must not be blind to the realities of life. He went on to suggest that a liberal approach be adopted to grasp the intention of the framers of the Constitution by giving \"life\" a broad and liberal meaning. He opined that such an interpretation would include elements that form the quality of life, namely the right to seek and be engaged in lawful and gainful employment, and the right to live in a reasonably healthy and pollution-free environment. He also noted that life cannot be extinguished or taken away except according to procedure established by law.\n\nThe \"Yong Vui Kong\" case suggests that Singapore courts may interpret the word \"life\" more narrowly than the Indian and Malaysian courts when called upon to do so. The Court of Appeal stated that the scope of Article 21 of the Indian Constitution had been expanded by the Indian courts to include \"numerous rights relating to life, such as the right to education, the right to health and medical care and the right to freedom from noise pollution\", attributing this to the \"pro-active approach of the Indian Supreme Court in matters relating to the social and economic conditions of the people of India\". The Court declined to apply \"Mithu v. State of Punjab\", in which the mandatory death penalty had been found unconstitutional, stating it was \"not possible\" to interpret Singapore's Article 9(1) in the way that the Indian Supreme Court had interpreted Article 21 of the Indian Constitution.\n\n\"Lo Pui Sang v. Mamata Kapildev Dave\" (2008) took a narrow approach to the reading of \"personal liberty\" in Article 9(1). The High Court of Singapore held that \"personal liberty\" only refers to freedom from unlawful incarceration or detention, and does not include a liberty to contract. Although it was suggested this had always been the understanding of the term, no authority was cited.\n\nThe approach taken in \"Lo Pui Sang\" can be compared to the more liberal interpretation of \"liberty\" in the United States and Malaysia. In the US Supreme Court case of \"Allgeyer v. Louisiana\" (1897), where a Louisiana statute was struck down on the ground that it violated an individual's right to contract, it was held that \"liberty\" in the Fourteenth Amendment of the Constitution meant not only the right of the citizen to be free from any physical restraint of his person, but also the right to freely enjoy all his faculties – that is, to be free to use them in all lawful ways; to live and work where he will; to earn his livelihood by any lawful calling; to pursue any livelihood or avocation; and for that purpose to enter into all contracts that may be proper, necessary, and essential to his carrying out those purposes. \"Liberty\" was accorded the same broad reading in the subsequent case \"Meyer v. Nebraska\" (1923), in which the Supreme Court held that a state statute mandating that English be the only language used in schools was unconstitutional as it infringed on the liberty guaranteed by the Fourteenth Amendment. The Court stated that \"liberty\"\n\nIt was held in the Malaysian Court of Appeal case of \"Sugumar Balakrishnan v. Pengarah Imigresen Negeri Sabah\" (1998) that the term \"life\" in Article 5(1) of the Constitution is not limited to mere existence, but is a wide concept that must receive a broad and liberal interpretation. Likewise, \"personal liberty\" should be similarly interpreted, as any other approach to construction will necessarily produce an incongruous and absurd result. On the facts, personal liberty extended to the liberty of an aggrieved person to go to court and seek judicial review, and thus a statutory provision that sought to oust the power of judicial review was apparently inconsistent with this fundamental liberty. However, the apparent inconsistency could be resolved by permitting an ouster clause to immunize from judicial review only those administrative acts and decisions that are not infected by an error of law. Although the Federal Court reversed the Court of Appeal on this point, in the subsequent case \"Lee Kwan Woh v. Public Prosecutor\" (2009) the Federal Court held that the provisions of the Constitution should be interpreted \"generously and liberally\", and that \"on no account should a literal construction be placed on its language, particularly upon those provisions that guarantee to individuals the protection of fundamental rights\". In its view:\n\nThe Federal Court went on to state that \"personal liberty\" \"includes other rights\" such as the right to \"cross the frontiers in order to enter or leave the country when one so desires\".\n\nIt has been suggested that since Article 9(1) of the Singapore Constitution is pitched at a high level of generality, there is no limitation in the ordinary natural meaning of the phrase. Thus, there is no requirement in the Constitution for \"personal liberty\" to be construed narrowly to mean only freedom from physical restraint.\n\nThe meaning of the word \"law\" in Article 9(1) has a direct bearing on the scope of the Article. If \"law\" is read broadly (for example, as incorporating customary international law principles), the scope of the fundamental liberties would be wider. It would be narrower if, on the other hand, \"law\" is construed narrowly, as the Legislature would be able to curtail such rights through legislation more easily. This could lead to a watering-down of the emphasis on fundamental liberties, as any infringement might be considered legitimate so long as the statute in question was validly enacted.\n\nIn the Malaysian case \"Arumugam Pillai v. Government of Malaysia\" (1976), the Federal Court construed the phrase \"save in accordance with law\" in Article 13(1) of the Constitution of Malaysia restrictively. This provision states: \"No person shall be deprived of property save in accordance with law.\" The Court held that all that was required for the legislation in question to be constitutional was for it to have been validly passed by Parliament. Hence, the validity of any duly enacted piece of legislation could not be questioned on grounds of reasonableness, no matter how arbitrary the law appeared to be.\n\nHowever, in 1980 the Privy Council rejected this interpretation in the case of \"Ong Ah Chuan v. Public Prosecutor\", a decision on appeal from Singapore. This appeal questioned the constitutional validity of section 15 of the Misuse of Drugs Act, and one of the issues that had to be decided was the interpretation of the word \"law\" in Article 9(1). The Public Prosecutor contended that \"law\" should be given a narrow meaning. He argued that\n\nHowever, the Public Prosecutor qualified the statement by providing a limitation, namely, that \"the arbitrariness, the disregard of fundamental rules of natural justice for which the Act provides, must be of general application to all citizens of Singapore so as to avoid falling foul of the anti-discriminatory provisions of Art 12(1)\".\n\nIn a judgment delivered by Lord Diplock, the Privy Council rejected this interpretation, finding the Public Prosecutor's argument fallacious. Reading the definition of \"written law\" as stated in Article 2(1) together with Article 4, which provides that \"any law enacted by the Legislature after the commencement of this Constitution which is inconsistent with this Constitution shall, to the extent of the inconsistency, be void\", their Lordships held that \"the use of the expression 'law' in Art 9(1) ... does not, in the event of challenge, relieve the court of its duty to determine whether the provisions of an Act of Parliament passed after 16 September 1963 and relied upon to justify depriving a person of his life or liberty are inconsistent with the Constitution and consequently void\".\n\nIn line with their view that Part IV of the Constitution should be given \"a generous interpretation ... suitable to give to individuals the full measure of the\n[fundamental liberties] referred to\", their Lordships held that \"references to 'law' in such contexts as 'in accordance with law', 'equality before the law', 'protection of the law' and the like ... refer to a system of law which incorporates those fundamental rules of natural justice that had formed part and parcel of the common law of England that was in operation in Singapore at the commencement of the Constitution\". This conception of the meaning of \"law\" in Article 9(1) has been affirmed by the Court of Appeal in \"Nguyen Tuong Van v. Public Prosecutor\" (2005) and \"Yong Vui Kong v. Public Prosecutor\" (2010).\n\nIt has been highlighted that this elevation of principles of natural justice to constitutional status, with the implication that they may override local statutes due to the supremacy of the Constitution over them, creates some tension with Article 38 which vests the law-making power of Singapore in the legislature.\n\nIn \"Ong Ah Chuan\" and the subsequent decision \"Haw Tua Tau v. Public Prosecutor\" (1981), the Privy Council declined to set out a comprehensive list of fundamental rules of natural justice and merely stated some principles to deal with the issues at hand. At a 2000 conference, the Attorney-General Chan Sek Keong, who became Chief Justice in 2006, remarked that this gives the Court of Appeal a free hand to determine the scope of the fundamental rules of natural justice unencumbered by precedent.\n\nGuidance as to the scope of fundamental rules of natural justice was provided in \"Haw Tua Tau\". First, the Privy Council said that rules of natural justice are not stagnant and may change with the times. Secondly, they should be considered in the local context, in light of the entire system as a whole and from the perspective of the people operating the system. Further, in order to satisfy the rules of natural justice, the law in question should not be \"obviously unfair\". In its view, under a system of justice in which the court is invested with partly inquisitorial functions, compelling an accused to answer questions put to him by a judge cannot be regarded as contrary to natural justice. The Court of Appeal later ruled in \"Public Prosecutor v. Mazlan bin Maidun\" (1992) that the privilege against self-incrimination was not a fundamental rule of natural justice, and thus not a constitutional right.\n\nIn \"Yong Vui Kong v. Attorney-General\" (2011), the Court of Appeal stated that fundamental rules of natural justice embodied in the concept of \"law\" in constitutional provisions such as Articles 9(1) and 12(1) are the same in nature and function as common law rules of natural justice in administrative law, except that they operate at different levels of the legal order. The former invalidate legislation on the ground of unconstitutionality and can only be altered by amending the Constitution, while the latter invalidate administrative decisions on the ground of administrative law principles and can be abrogated or disapplied by ordinary legislation.\n\nTraditionally, at common law, natural justice is taken to be a procedural concept that embodies the twin pillars of \"audi alteram partem\" (hear the other party) and \"nemo iudex in causa sua\" (no one should be a judge in his or her own cause). In the United States, due process has both procedural and substantive components. Substantive due process involves the courts assessing the reasonableness of executive actions and legislation using rational basis review if a fundamental right is not implicated and strict scrutiny if it is. The question thus arises whether substantive fundamental rules of natural justice may be developed by local courts. However, a line of Malaysian cases has expressed the view that the concept of substantive due process is not applicable to Article 5(1) of the Malaysian Constitution, which is identical to Singapore's Article 9(1). There is also academic commentary that rejects the notion of \"substantive natural justice\", arguing that it is too vague and leads to problems in application. Another argument against substantive natural justice is the fear that it may become an avenue for judges to invalidate laws on the basis of their own subjective opinions, leading to unbounded judicial activism.\n\nOn the other hand, it has also been suggested that substantive natural justice would merely be a full exercise of the judiciary's proper role as conferred by the Constitution. In addition, one scholar has asserted that there is no doubt that a judicial inquiry covers both substantive and procedural aspects. It is said that Article 9(1) connotes a judicial inquiry into the \"fairness\" of the law tested against certain principles regarded as fundamental to the legal system. Distinguishing between substantive and procedural fairness is a meaningless exercise, as it merely clouds the process of judicial inquiry. Judicial review is judicial review under whatever name, and as far as Article 9(1) is concerned, there is no room for making this distinction.\n\nHowever, in \"Yong Vui Kong v. Public Prosecutor\" (2010) the Court of Appeal appeared to reject such an approach by declining to require that procedural laws must be \"fair, just and reasonable\" before they can be regarded as \"law\" for the purpose of Article 9(1). It noted that the provision neither contains such a qualification, nor can such a qualification be implied from its context or wording. The Court considered it \"too vague a test of constitutionality\" and said: \"Such a test hinges on the court’s view of the reasonableness of the law in question, and requires the court to intrude into the legislative sphere of Parliament as well as engage in policy making.\" On the other hand, the Court acknowledged that Article 9(1) does not justify all legislation whatever its nature. It held, \"obiter\", that \"law\" might not encompass colourable legislation (that is, bills of attainder – legislation purporting to be of general application but in fact directed at securing the conviction of particular individuals), or legislation \"of so absurd or arbitrary a nature that it could not possibly have been contemplated by our constitutional framers as being 'law' when they crafted the constitutional provisions protecting fundamental liberties\".\n\nIn \"Nguyen Tuong Van v. Public Prosecutor\" (2004), the Court of Appeal considered whether \"law\" in Article 9(1) includes principles of customary international law. In that case, the appellant argued that effecting a death sentence for drug trafficking by hanging is unconstitutional as a form of cruel and inhuman punishment not \"in accordance with law\". The Court agreed that there was a prohibition against torture and cruel and inhumane treatment in Article 5 of the Universal Declaration of Human Rights and that this is considered customary international law. However, a customary international law rule had to be \"clearly and firmly established\" before it was adopted by the courts, and there was insufficient practice among states to hold that death by hanging was within the ambit of this prohibition. Also, even if there was a customary international law rule against death by hanging, domestic statutes would prevail in the event of conflict.\n\nThe Court of Appeal clarified in \"Yong Vui Kong v. Public Prosecutor\" (2010), that customary international law cannot be read into the Constitution for two reasons. First, in order for a customary international law rule to have legal effect in Singapore, it has to be incorporated into domestic law. The incorporation can occur either by enactment in a statute or by a court declaration that the rule forms part of the common law. The Court felt it would be incorrect to incorporate customary international law rules into the meaning of \"law\" in Article 9(1) as this would cloak the common law with the constitutional status to nullify a statute, thus reversing the usual hierarchy of legal rules. Secondly, the term \"law\" is defined in Article 2(1) to include the common law only \"in so far as it is in operation in Singapore\". However, a court cannot treat rules of customary international law as having been incorporated into Singapore common law if they are inconsistent with existing statutes. Furthermore, if there is a conflict between such a rule and a domestic statute, the latter prevails.\n\nThe Constitution is silent as to the reception of international law in domestic law. In \"Yong Vui Kong\" the Court of Appeal accepted that domestic law, including the Constitution, should \"as far as possible\" be consistently interpreted with Singapore's international obligations. Nonetheless, while international human rights law can increase the normative pool judges may resort to in interpreting the Constitution, there are \"inherent limits\" such as the express wording of the constitutional text and constitutional history that \"[militates] against the incorporation of those international norms\".\n\nIt has also been argued that although where possible local statutes should be interpreted in light of international treaties, it is not the role of the judiciary to import international law standards into the Constitution that are inconsistent with legislation instead of deferring to the views of the executive. According to this view, which hinges on a strict adherence to the separation of powers doctrine, the judiciary should guard against unwarranted incursions into the executive sphere, as it is for the executive to determine Singapore's attitude and position in relation to foreign affairs. The judiciary must not undertake its task of interpreting the Constitution arbitrarily, but should accord with legal reasoning and sound principles. This necessarily raises the question of what the applicable legal principle during the interpretation process should be. It has been suggested that the executive and the judiciary should show solidarity by speaking with \"one voice\", and that the courts should exercise deference in favour of what the executive deems to be the nation's attitude towards the particular international law norm that is sought to be applied.\n\nIt may be submitted that such judicial deference to the executive results in a clear neglect of the enshrined fundamental liberties in the Constitution. The flip side to this criticism is that fundamental liberties may still be given due accord though other avenues, for example, the application of rules of natural justice. As the meaning accorded to a particular fundamental liberty may be a potential ground for overturning Parliamentary legislation, it is crucial that the court should not merely rely on international law to determine the meaning of the liberty, unless there is evidence that the executive considers there is indeed an adoption of the particular international law norm.\n\nOne of the most difficult questions involving the right to life is when exactly life begins and ends. If an unborn child is treated as a living person, then it should be accorded the right to life under the Constitution. Laws permitting abortion would thus be unconstitutional. This issue has yet to come before the Singapore courts.\n\nIn Singapore, the Penal Code lays out sanctions for non-compliance with the Termination of Pregnancy Act, which limits abortion to women who have not been pregnant for more than 24 weeks. By not conferring the right to life upon fetuses younger than the stipulated period, the legislation has accorded greater weight to the safety and security of expectant mothers who are threatened by their unborn children. This is in contrast with the approach taken in the Philippines, where the Constitution provides that the state shall equally protect the life of the mother and the life of the unborn from conception. Similarly, the Charter of Fundamental Rights and Basic Freedoms of the Czech Republic states that human life deserves to be protected before birth. Chances of a universal consensus on this issue are slim due to the difficulty in defining the beginning of life.\n\nIn Singapore, attempted suicide, abetment of suicide, and abetment of attempted suicide are criminal acts. This applies to physicians who aid patients in ending their lives. Such physicians are unable to claim a defence under section 88 of the Penal Code since they intended to cause the patients' deaths. However, physicians are absolved of liability if patients refuse treatment for terminal illnesses by issuing advance medical directives.\n\nWhether the right to life guaranteed by Article 9(1) encompasses a right to die – that is, a right to commit suicide or a right to assisted suicide, usually in the face of a terminal illness – has not been the subject of any Singapore court case. In other jurisdictions, the right to life has generally not been interpreted in this way. In \"Gian Kaur v. State of Punjab\" (1996) the Indian Supreme Court held that the right to life is a natural right embodied in Article 21 of the Indian Constitution, and since suicide is an unnatural termination or extinction of life it is incompatible and inconsistent with the concept of the right to life. The US Supreme Court has also declined to recognize that choosing death is a right protected by the Constitution. In \"Washington v. Glucksberg\" (1997), a group of Washington residents asserted that a state law banning assisted suicide was unconstitutional on its face. The majority held that as assisted suicide is not a fundamental liberty interest, it was not protected under the due process clause of the Fourteenth Amendment. Several of the justices seemed persuaded that the availability of palliative care to \"alleviate suffering, even to the point of causing unconsciousness and hastening death\" outweighed recognizing a new unenumerated \"right to commit suicide which itself includes a right to assistance in doing so\".\n\nArticle 9(2) of the Constitution enshrines the right of persons who have been detained to apply to the High Court challenging the legality of their detention. The application is for an order for review of detention, which was formerly called a writ of \"habeas corpus\". The Court is required to inquire into the complaint, and order the detainee to be produced before the Court and released unless it is satisfied that the detention is lawful.\n\nArticle 9(3) requires that an arrested person be informed \"as soon as may be\" of the grounds of his arrest. Article 9(4) goes on to provide that if the arrested person is not released he must, without unreasonable delay, and in any case within 48 hours (excluding the time of any necessary journey) be produced before a magistrate and cannot be further detained in custody without the authority of the magistrate. The person's attendance before the magistrate may be in person or by way of video-conferencing or other similar technology in accordance with law.\n\nArticle 9(3) also states that an arrested person must be allowed to consult and be defended by a legal practitioner of his choice.\n\nAs mentioned above, Parliament is entitled to restrict the rights to life and personal liberty as long as it acts \"in accordance with law\". More specific restrictions on Article 9 include Article 9(5), which provides that Articles 9(3) and (4) of the Constitution do not apply to enemy aliens or to persons arrested for contempt of Parliament pursuant to a warrant issued by the Speaker.\n\nArticle 9(6) saves any law\nfrom being invalid because of inconsistency with Articles 9(3) and (4). This provision took effect on 10 March 1978 but was expressed to apply to laws in force prior to that date. Introduced by the Constitution (Amendment) Act 1978, the provision immunizes the Criminal Law (Temporary Provisions) Act and Part IV of the Misuse of Drugs Act from unconstitutionality.\n\nPreventive detention is the use of executive power to detain individuals on the basis that they are predicted to commit future crimes that will threaten national interest. Among other things, the Criminal Law (Temporary Provisions) Act empowers the Minister for Home Affairs, if satisfied that a person has been associated with activities of a criminal nature, to order that he or she be detained for a period not exceeding 12 months if the Minister is of the view that the detention is necessary in the interests of public safety, peace and good order.\n\nUnder the Misuse of Drugs Act, the Director of the Central Narcotics Bureau may order drug addicts to undergo drug treatment or rehabilitation at an approved institution for renewable six-month periods up to a maximum of three years.\n\nSection 8(1) of Singapore's Internal Security Act (\"ISA\") gives the Minister for Home Affairs the power to detain a person without trial for any period not exceeding two years on the precondition that the President is: \"satisfied ... that ... it is necessary to do so ... with a view to preventing that person from acting in any manner prejudicial to the security of Singapore ... or to the maintenance of public order or essential services therein\". The period of detention may be renewed by the President indefinitely for periods not exceeding two years at a time as long as the grounds for detention continue to exist.\n\nThe ISA has its constitutional basis in Article 149 of the Constitution, which sanctions preventive detention and allows for laws passed by the legislature against subversion to override the Articles protecting the personal liberties of the individual. Specifically, Article 149(1) declares such legislation to be valid notwithstanding any inconsistency with five of the fundamental liberty provisions in the Constitution, including Article 9. Thus, detentions under the ISA cannot be challenged on the basis of deprivation of these rights.\n\n\n\n\n\n"}
{"id": "49610176", "url": "https://en.wikipedia.org/wiki?curid=49610176", "title": "Assemblage theory", "text": "Assemblage theory\n\nAssemblage theory is an ontological framework developed by Gilles Deleuze and Félix Guattari, originally presented in their book \"A Thousand Plateaus\" (1980). Assemblage theory provides a bottom-up framework for analyzing social complexity by emphasizing fluidity, exchangeability, and multiple functionalities. Assemblage theory asserts that, within a body, the relationships of component parts are not stable and fixed; rather, they can be displaced and replaced within and among other bodies, thus approaching systems through relations of exteriority. \n\nIn A Thousand Plateaus, Deleuze and Guattari draw from dynamical systems theory, which explores the way material systems self-organize, and extends the system to include that of the social, linguistic, and philosophical in order to create assemblage theory. In assemblage theory, assemblages are formed through the processes of coding, stratification, and territorialization.\n\nDeleuze and Guatarri use the term “constellation” when they talk about assemblage. A constellation, like any assemblage, is made up of imaginative contingent articulations among myriad heterogeneous elements. This process of ordering matter around a body is called coding. According to Deleuze and Guatarri, assemblages are coded by taking a particular form; they select, compose, and complete a territory. \n\nIn composing a territory, there exists the creation of hierarchical bodies, in the process of stratification. Drawing from the constellation metaphor, Deleuze and Guattari argue that the constellation includes some heavenly bodies but leaves out others; the included bodies being those in close proximity given the particular gathering and angle of view. The example constellation thus defines the relationships with the bodies around it and the way the universe is viewed.\n\nTerritorialization is the final process of assemblage theory, and is viewed as the ordering of the coded and stratified bodies that create the “assemblage”. Assemblages territorialize both forms of content and forms of expression. Forms of content, also known as material forms, include the assemblage of human and nonhuman bodies, actions, and reactions. Forms of expression include incorporeal enunciations, acts, and statements. Within this form of territorialization, assemblages do not remain static; they are further characterized by processes of deterritorialization and reterritorialization. Deterritorialization occurs when articulations are disarticulated and disconnected. Reterritorialization describes the process by which new articulations are forged, thus constituting a new assemblage. In this way, these axes of content/expressive and the processes of territorialization exist to demonstrate the complex nature of assemblages.\n\nPhilosopher Manuel DeLanda has adopted the concept of assemblage in his book \"A New Philosophy of Society\" (2006). In his book, DeLanda draws from Deleuze and Guattari to further argue that social bodies on all scales are best analyzed through their individual components. Like Deleuze and Guattari, DeLanda’s approach examines relations of exteriority, in which assemblage components are self-subsistent and retain autonomy outside of the assemblage in which they exist DeLanda further expands upon Deleuze’s assemblage theory and relations of exteriority by suggesting that assemblage components are organized through the two axes of material/expressive and territorializing/deterritorializing. Further, a third axis exists of genetic/linguistic resources exists to define the interventions involved in the coding, decoding, and recoding of the assemblage. However, DeLanda further asserts that the social does not lose its reality, nor its materiality, through its complexity. In this way, assemblages are effective in their practicality; assemblages, though fluid, are nevertheless part of historically significant processes. In that way, DeLanda offers a reconfiguration of the Deleuzian concepts that provides a more robust theoretical framework for analyzing assemblages.\n"}
{"id": "1626016", "url": "https://en.wikipedia.org/wiki?curid=1626016", "title": "Barbara Smoker", "text": "Barbara Smoker\n\nBarbara Smoker (born 2 June 1923) is a British Humanist activist and freethought advocate. She is also a former President of the National Secular Society (1972–1996), former Chair of the British Voluntary Euthanasia Society (now known as Dignity in Dying) (1981–1985) and was an Honorary Vice President of the Gay and Lesbian Humanist Association in the United Kingdom.\n\nBarbara Smoker was born in London in 1923 into a Roman Catholic family. She served in the Women's Royal Naval Service from 1942 to 1945 in southeast Asia. In 1949 she became an atheist, inspired by the writing of Hector Hawton, Managing Director of the Rationalist Press Association and editor of The Humanist.\n\nIn 1950 Smoker joined the secular humanist movement when she became a member of the South Place Ethical Society, where she was critical of seeking redress on religious grounds. Eventually she became President of the National Secular Society for nearly 25 years (1972 –1996). In that capacity, she represented the atheist viewpoint in print, on lecture platforms, speaking tours, on radio and television. \n\nShe was in demand to give addresses at secular funerals and eventually officiated at non-religious funerals, wedding ceremonies, gay and lesbian commitments and baby namings. She was active in various social campaigns, such as the abolition of the death penalty, nuclear disarmament, legalization of abortion and for the Voluntary Euthanasia Society. She claimed to have financed the manufacture of the first Make Love, Not War badges that were popular in Britain during the 1960s.\n\nBarbara Smoker became the South Place Ethical Society's last and only female Appointed Lecturer in 1986. As of May 2014, with the death of Dr Harry Stopes-Roe, she became the only living Appointed Lecturer. In 2005 Barbara Smoker received the Distinguished Humanist Service Award from the International Humanist and Ethical Union. She is also an Honorary Member of Humanists UK.\n\nSmoker lives in southeast London and in 2012 was elected the Honorary life president of the South East London Humanist Group in recognition that she is its last surviving founder member.\n\nEditor:\nContributor:\n\n\n"}
{"id": "2119179", "url": "https://en.wikipedia.org/wiki?curid=2119179", "title": "Climate change mitigation", "text": "Climate change mitigation\n\nClimate change mitigation consists of actions to limit the magnitude or rate of long-term climate change. Climate change mitigation generally involves reductions in human (anthropogenic) emissions of greenhouse gases (GHGs). Mitigation may also be achieved by increasing the capacity of carbon sinks, e.g., through reforestation. Mitigation policies can substantially reduce the risks associated with human-induced global warming.\n\nAccording to the IPCC's 2014 assessment report, \"Mitigation is a public good; climate change is a case of the 'tragedy of the commons'. Effective climate change mitigation will not be achieved if each agent (individual, institution or country) acts independently in its own selfish interest (see International cooperation and Emissions trading), suggesting the need for collective action. Some adaptation actions, on the other hand, have characteristics of a private good as benefits of actions may accrue more directly to the individuals, regions, or countries that undertake them, at least in the short term. Nevertheless, financing such adaptive activities remains an issue, particularly for poor individuals and countries.\"\n\nExamples of mitigation include reducing energy demand by increasing energy efficiency, phasing out fossil fuels by switching to low-carbon energy sources, and removing carbon dioxide from Earth's atmosphere. for example, through improved building insulation. Another approach to climate change mitigation is climate engineering.\n\nMost countries are parties to the United Nations Framework Convention on Climate Change (UNFCCC). The ultimate objective of the UNFCCC is to stabilize atmospheric concentrations of GHGs at a level that would prevent dangerous human interference of the climate system. Scientific analysis can provide information on the impacts of climate change, but deciding which impacts are dangerous requires value judgments.\n\nIn 2010, Parties to the UNFCCC agreed that future global warming should be limited to below 2.0 °C (3.6 °F) relative to the pre-industrial level. With the Paris Agreement of 2015 this was confirmed, but was revised with a new target laying down \"parties will do the best\" to achieve warming below 1.5 °C. The current trajectory of global greenhouse gas emissions does not appear to be consistent with limiting global warming to below 1.5 or 2 °C. Other mitigation policies have been proposed, some of which are more stringent or modest than the 2 °C limit.\n\nOne of the issues often discussed in relation to climate change mitigation is the stabilization of greenhouse gas concentrations in the atmosphere. The United Nations Framework Convention on Climate Change (UNFCCC) has the ultimate objective of preventing \"dangerous\" anthropogenic (i.e., human) interference of the climate system. As is stated in Article 2 of the Convention, this requires that greenhouse gas (GHG) concentrations are stabilized in the atmosphere at a level where ecosystems can adapt naturally to climate change, food production is not threatened, and economic development can proceed in a sustainable fashion.\n\nThere are a number of anthropogenic greenhouse gases. These include carbon dioxide (chemical formula: ), methane (), nitrous oxide (), and a group of gases referred to as halocarbons. Another greenhouse gas, water vapor, has also risen as an indirect result of human activities. The emissions reductions necessary to stabilize the atmospheric concentrations of these gases varies. is the most important of the anthropogenic greenhouse gases (see radiative forcing).\n\nThere is a difference between stabilizing emissions and stabilizing atmospheric concentrations of . Stabilizing emissions of CO at current levels would not lead to a stabilization in the atmospheric concentration of CO. In fact, stabilizing emissions at current levels would result in the atmospheric concentration of CO continuing to rise over the 21st century and beyond (see the graphs opposite).\n\nThe reason for this is that human activities are adding CO to the atmosphere faster than natural processes can remove it (see carbon dioxide in Earth's atmosphere for a complete explanation). This is analogous to a flow of water into a bathtub. So long as the tap runs water (analogous to the emission of carbon dioxide) into the tub faster than water escapes through the plughole (the natural removal of carbon dioxide from the atmosphere), then the level of water in the tub (analogous to the concentration of carbon dioxide in the atmosphere) will continue to rise.\n\nAccording to some studies, stabilizing atmospheric CO concentrations would require anthropogenic CO emissions to be reduced by 80% relative to the peak emissions level. An 80% reduction in emissions would stabilize concentrations for around a century, but even greater reductions would be required beyond this. Other research has found that, after leaving room for emissions for food production for 9 billion people and to keep the global temperature rise below 2 °C, emissions from energy production and transport will have to peak almost immediately in the developed world and decline at ca. 10% per annum until zero emissions are reached around 2030. In developing countries energy and transport emissions would have to peak by 2025 and then decline similarly.\n\nStabilizing the atmospheric concentration of the other greenhouse gasses humans emit also depends on how fast their emissions are added to the atmosphere, and how fast the GHGs are removed. Stabilization for these gases is described in the later section on non-CO GHGs.\n\nIn 2018 an international team of scientist published research saying that the current mitigation policy in Paris Agreement is insufficient to limit the temperature rise to 2 degrees. They say that even if all the current pledges will be accomplished there is a chance for a 4.5 degree temperature rise in decades. To preventing that, restoration of natural Carbon sinks, Carbon dioxide removal, changes in society and values will be necessary.\n\nProjections of future greenhouse gas emissions are highly uncertain. In the absence of policies to mitigate climate change, GHG emissions could rise significantly over the 21st century.\n\nNumerous assessments have considered how atmospheric GHG concentrations could be stabilized. The lower the desired stabilization level, the sooner global GHG emissions must peak and decline. GHG concentrations are unlikely to stabilize this century without major policy changes.\n\nTo create lasting climate change mitigation, the replacement of high carbon emission intensity power sources, such as conventional fossil fuels—oil, coal, and natural gas—with low-carbon power sources is required. Fossil fuels supply humanity with the vast majority of our energy demands, and at a growing rate. In 2012 the IEA noted that coal accounted for half the increased energy use of the prior decade, growing faster than all renewable energy sources. Both hydroelectricity and nuclear power together provide the majority of the generated low-carbon power fraction of global total power consumption.\n\nAssessments often suggest that GHG emissions can be reduced using a portfolio of low-carbon technologies. At the core of most proposals is the reduction of greenhouse gas (GHG) emissions through reducing energy waste and switching to low-carbon power sources of energy. As the cost of reducing GHG emissions in the electricity sector appears to be lower than in other sectors, such as in the transportation sector, the electricity sector may deliver the largest proportional carbon reductions under an economically efficient climate policy.\n\n\"Economic tools can be useful in designing climate change mitigation policies.\" \"While the limitations of economics and social welfare analysis, including cost–benefit analysis, are widely documented, economics nevertheless provides useful tools for assessing the pros and cons of taking, or not taking, action on climate change mitigation, as well as of adaptation measures, in achieving competing societal goals. Understanding these pros and cons can help in making policy decisions on climate change mitigation and can influence the actions taken by countries, institutions and individuals.\"\n\nOther frequently discussed means include efficiency, public transport, increasing fuel economy in automobiles (which includes the use of electric hybrids), charging plug-in hybrids and electric cars by low-carbon electricity, making individual changes, and changing business practices. Many fossil fuel driven vehicles can be converted to use electricity, the US has the potential to supply electricity for 73% of light duty vehicles (LDV), using overnight charging. The US average emissions for a battery-electric car is 180 grams per mile vs 430 grams per mile for a gasoline car. The emissions would be displaced away from street level, where they have \"high human-health implications. Increased use of electricity \"generation for meeting the future transportation load is primarily fossil-fuel based\", mostly natural gas, followed by coal, but could also be met through nuclear, tidal, hydroelectric and other sources.\n\nA range of energy technologies may contribute to climate change mitigation. These include nuclear power and renewable energy sources such as biomass, hydroelectricity, wind power, solar power, geothermal power, ocean energy, and; the use of carbon sinks, and carbon capture and storage. For example, Pacala and Socolow of Princeton have proposed a 15 part program to reduce CO emissions by 1 billion metric tons per year − or 25 billion tons over the 50-year period using today's technologies as a type of global warming game.\n\nAnother consideration is how future socioeconomic development proceeds. Development choices (or \"pathways\") can lead differences in GHG emissions. Political and social attitudes may affect how easy or difficult it is to implement effective policies to reduce emissions.\n\nThe IPCC Fifth Assessment Report emphasises that behaviour, lifestyle, and cultural change have a high mitigation potential in some sectors, particularly when complementing technological and structural change.\nIn general, higher consumption lifestyles have a greater environmental impact. Several scientific studies have shown that when people, especially those living in developed countries but more generally including all countries, wish to reduce their carbon footprint, there are four key \"high-impact\" actions they can take:\n\nThese appear to differ significantly from the popular advice for “greening” one's lifestyle, which seem to fall mostly into the “low-impact” category: Replacing a typical car with a hybrid (0.52 tonnes); Washing clothes in cold water (0.25 tonnes); Recycling (0.21 tonnes); Upgrading light bulbs (0.10 tonnes); etc. The researchers found that public discourse on reducing one's carbon footprint overwhelmingly focuses on low-impact behaviors, and that mention of the high-impact behaviors is almost non-existent in the mainstream media, government publications, K-12 school textbooks, etc.\n\nThe researchers added that “Our recommended high-impact actions are more effective than many more commonly discussed options (e.g. eating a plant-based diet saves eight times more emissions than upgrading light bulbs). More significantly, a US family who chooses to have one fewer child would provide the same level of emissions reductions as 684 teenagers who choose to adopt comprehensive recycling for the rest of their lives.”\n\nOverall, food accounts for the largest share of consumption-based GHG emissions with nearly 20% of the global carbon footprint, followed by housing, mobility, services, manufactured products, and construction. Food and services are more significant in poor countries, while mobility and manufactured goods are more significant in rich countries. A 2014 study into the real-life diets of British people estimates their greenhouse gas contributions (eq) to be: 7.19kg/day for high meat-eaters through to 3.81kg/day for vegetarians and 2.89kg/day for vegans.\nThe widespread adoption of a vegetarian diet could cut food-related greenhouse gas emissions by 63% by 2050.\nChina introduced new dietary guidelines in 2016 which aim to cut meat consumption by 50% and thereby reduce greenhouse gas emissions by 1billion tonnes by 2030. A 2016 study concluded that taxes on meat and milk could simultaneously result in reduced greenhouse gas emissions and healthier diets. The study analyzed surcharges of 40% on beef and 20% on milk and suggests that an optimum plan would reduce emissions by 1billion tonnes per year.\n\nEfficient energy use, sometimes simply called \"energy efficiency\", is the goal of efforts to reduce the amount of energy required to provide products and services. For example, insulating a home allows a building to use less heating and cooling energy to achieve and maintain a comfortable temperature. Installing LED lighting, fluorescent lighting, or natural skylight windows reduces the amount of energy required to attain the same level of illumination compared to using traditional incandescent light bulbs. Compact fluorescent lamps use only 33% of the energy and may last 6 to 10 times longer than incandescent lights. LED lamps use only about 10% of the energy an incandescent lamp requires.\n\nEnergy efficiency has proved to be a cost-effective strategy for building economies without necessarily growing energy consumption. For example, the state of California began implementing energy-efficiency measures in the mid-1970s, including building code and appliance standards with strict efficiency requirements. During the following years, California's energy consumption has remained approximately flat on a per capita basis while national US consumption doubled. As part of its strategy, California implemented a \"loading order\" for new energy resources that puts energy efficiency first, renewable electricity supplies second, and new fossil-fired power plants last.\n\nEnergy conservation is broader than energy efficiency in that it encompasses using less energy to achieve a lesser energy demanding service, for example through behavioral change, as well as encompassing energy efficiency. Examples of conservation without efficiency improvements would be heating a room less in winter, driving less, or working in a less brightly lit room. As with other definitions, the boundary between efficient energy use and energy conservation can be fuzzy, but both are important in environmental and economic terms. This is especially the case when actions are directed at the saving of fossil fuels.\n\nReducing energy use is seen as a key solution to the problem of reducing greenhouse gas emissions. According to the International Energy Agency, improved energy efficiency in buildings, industrial processes and transportation could reduce the world's energy needs in 2050 by one third, and help control global emissions of greenhouse gases.\n\nFuel switching on the demand side refers to changing the type of fuel used to satisfy a need for an energy service. To meet deep decarbonization goals, like the 80% reduction by 2050 goal being discussed in California and the European Union, many primary energy changes are needed. Energy efficiency alone may not be sufficient to meet these goals, switching fuels used on the demand side will help lower carbon emissions. Progressively coal, oil and eventually natural gas for space and water heating in buildings will need to be reduced. For an equivalent amount of heat, burning natural gas produces about 45 per cent less carbon dioxide than burning coal. There are various ways in which this could happen, and different strategies will likely make sense in different locations. While the system efficiency of a gas furnace may be higher than the combination of natural gas power plant and electric heat, the combination of the same natural gas power plant and an electric heat pump has lower emissions per unit of heat delivered in all but the coldest climates. This is possible because of the very efficient coefficient of performance of heat pumps.\n\nAt the beginning of this century 70% of all electricity was generated by fossil fuels, and as carbon free sources eventually make up half of the generation mix, replacing gas or oil furnaces and water heaters with electric ones will have a climate benefit. In areas like Norway, Brazil, and Quebec that have abundant hydroelectricity, electric heat and hot water are common.\n\nThe economics of switching the demand side from fossil fuels to electricity for heating, will depend on the price of fuels vs electricity and the relative prices of the equipment. The EIA Annual Energy Outlook 2014 suggests that domestic gas prices will rise faster than electricity prices which will encourage electrification in the coming decades.\nElectrifying heating loads may also provide a flexible resource that can participate in demand response. Since thermostatically controlled loads have inherent energy storage, electrification of heating could provide a valuable resource to integrate variable renewable resources into the grid.\n\nAlternatives to electrification, include decarbonizing pipeline gas through power to gas, biogas, or other carbon-neutral fuels. A 2015 study by Energy+Environmental Economics shows that a hybrid approach of decarbonizing pipeline gas, electrification, and energy efficiency can meet carbon reduction goals at a similar cost as only electrification and energy efficiency in Southern California.\n\nExpanding intermittent electrical sources such as wind power, creates a growing problem balancing grid fluctuations. Some of the plans include building pumped storage or continental super grids costing billions of dollars. However instead of building for more power, there are a variety of ways to affect the size and timing of electricity demand on the consumer side. Designing for reduced demands on a smaller power grid is more efficient and economic than having extra generation and transmission for intermittentcy, power failures and peak demands. Having these abilities is one of the chief aims of a smart grid.\n\nTime of use metering is a common way to motivate electricity users to reduce their peak load consumption. For instance, running dishwashers and laundry at night after the peak has passed, reduces electricity costs.\n\nDynamic demand plans have devices passively shut off when stress is sensed on the electrical grid. This method may work very well with thermostats, when power on the grid sags a small amount, a low power temperature setting is automatically selected reducing the load on the grid. For instance millions of refrigerators reduce their consumption when clouds pass over solar installations. Consumers would need to have a smart meter in order for the utility to calculate credits.\n\nDemand response devices could receive all sorts of messages from the grid. The message could be a request to use a low power mode similar to dynamic demand, to shut off entirely during a sudden failure on the grid, or notifications about the current and expected prices for power. This would allow electric cars to recharge at the least expensive rates independent of the time of day. The vehicle-to-grid suggestion would use a car's battery or fuel cell to supply the grid temporarily.\n\n \n\nRenewable energy flows involve natural phenomena such as sunlight, wind, rain, tides, plant growth, and geothermal heat, as the International Energy Agency explains:\nClimate change concerns and the need to reduce carbon emissions are driving increasing growth in the renewable energy industries. Low-carbon renewable energy replaces conventional fossil fuels in three main areas: power generation, hot water/ space heating, and transport fuels. In 2011, the share of renewables in electricity generation worldwide grew for the fourth year in a row to 20.2%. Based on REN21's 2014 report, renewables contributed 19% to supply global energy consumption. This energy consumption is divided as 9% coming from burning biomass, 4.2% as heat energy (non-biomass), 3.8% hydro electricity and 2% as electricity from wind, solar, geothermal, and biomass thermal power plants.\n\nRenewable energy use has grown much faster than anyone anticipated. The Intergovernmental Panel on Climate Change (IPCC) has said that there are few fundamental technological limits to integrating a portfolio of renewable energy technologies to meet most of total global energy demand. At the national level, at least 30 nations around the world already have renewable energy contributing more than 20% of energy supply.\n\nAs of 2012, renewable energy accounts for almost half of new electricity capacity installed and costs are continuing to fall. Public policy and political leadership helps to \"level the playing field\" and drive the wider acceptance of renewable energy technologies. , 118 countries have targets for their own renewable energy futures, and have enacted wide-ranging public policies to promote renewables. Leading renewable energy companies include BrightSource Energy, First Solar, Gamesa, GE Energy, Goldwind, Sinovel, Suntech, Trina Solar, Vestas, and Yingli.\n\nThe incentive to use 100% renewable energy has been created by global warming and other ecological as well as economic concerns. Mark Z. Jacobson says producing all new energy with wind power, solar power, and hydropower by 2030 is feasible and existing energy supply arrangements could be replaced by 2050. Barriers to implementing the renewable energy plan are seen to be \"primarily social and political, not technological or economic\". Jacobson says that energy costs with a wind, solar, water system should be similar to today's energy costs. According to a 2011 projection by the (IEA)International Energy Agency, solar power generators may produce most of the world's electricity within 50 years, dramatically reducing harmful greenhouse gas emissions. Critics of the \"100% renewable energy\" approach include Vaclav Smil and James E. Hansen. Smil and Hansen are concerned about the variable output of solar and wind power, NIMBYism, and a lack of infrastructure.\n\nEconomic analysts expect market gains for renewable energy (and efficient energy use) following the 2011 Japanese nuclear accidents. In his 2012 State of the Union address, President Barack Obama restated his commitment to renewable energy and mentioned the long-standing Interior Department commitment to permit 10,000 MW of renewable energy projects on public land in 2012. Globally, there are an estimated 3 million direct jobs in renewable energy industries, with about half of them in the biofuels industry.\n\nSome countries, with favorable geography, geology, and weather well suited to an economical exploitation of renewable energy sources, already get most of their electricity from renewables, including from geothermal energy in Iceland (100 percent), and hydroelectric power in Brazil (85 percent), Austria (62 percent), New Zealand (65 percent), and Sweden (54 percent). Renewable power generators are spread across many countries, with wind power providing a significant share of electricity in some regional areas: for example, 14 percent in the US state of Iowa, 40 percent in the northern German state of Schleswig-Holstein, and 20 percent in Denmark. Solar water heating makes an important and growing contribution in many countries, most notably in China, which now has 70 percent of the global total (180 GWth). Worldwide, total installed solar water heating systems meet a portion of the water heating needs of over 70 million households. The use of biomass for heating continues to grow as well. In Sweden, national use of biomass energy has surpassed that of oil. Direct geothermal heating is also growing rapidly. Renewable biofuels for transportation, such as ethanol fuel and biodiesel, have contributed to a significant decline in oil consumption in the United States since 2006. The 93 billion liters of biofuels produced worldwide in 2009 displaced the equivalent of an estimated 68 billion liters of gasoline, equal to about 5 percent of world gasoline production.\n\nSince about 2001 the term \"nuclear renaissance\" has been used to refer to a possible nuclear power industry revival, driven by rising fossil fuel prices and new concerns about meeting greenhouse gas emission limits. However, in March 2011 the Fukushima nuclear disaster in Japan and associated shutdowns at other nuclear facilities raised questions among some commentators over the future of nuclear power. Platts has reported that \"the crisis at Japan's Fukushima nuclear plants has prompted leading energy-consuming countries to review the safety of their existing reactors and cast doubt on the speed and scale of planned expansions around the world\".\n\nThe World Nuclear Association has reported that nuclear electricity generation in 2012 was at its lowest level since 1999. Several previous international studies and assessments, suggested that as part of the portfolio of other low-carbon energy technologies, nuclear power will continue to play a role in reducing greenhouse gas emissions. Historically, nuclear power usage is estimated to have prevented the atmospheric emission of 64 gigatonnes of CO2-equivalent as of 2013. Public concerns about nuclear power include the fate of spent nuclear fuel, nuclear accidents, security risks, nuclear proliferation, and a concern that nuclear power plants are very expensive. Of these concerns, nuclear accidents and disposal of long-lived radioactive fuel/\"waste\" have probably had the greatest public impact worldwide. Although generally unaware of it, both of these glaring public concerns are greatly diminished by present passive safety designs, the experimentally proven, \"melt-down proof\" EBR-II, future molten salt reactors, and the use of conventional and more advanced fuel/\"waste\" pyroprocessing, with the latter recycling or reprocessing not presently being commonplace as it is often considered to be cheaper to use a once-through nuclear fuel cycle in many countries, depending on the varying levels of intrinsic value given by a society in reducing the long-lived waste in their country, with France doing a considerable amount of reprocessing when compared to the US.\n\nNuclear power, with a 10.6% share of world electricity production as of 2013, is second only to hydroelectricity as the largest source of low-carbon power. Over 400 reactors generate electricity in 31 countries.\n\nA Yale University review published in the Journal of Industrial Ecology analyzing life cycle assessment(LCA) emissions from nuclear power(light water reactors) determined that: \"The collective LCA literature indicates that life cycle GHG emissions from nuclear power are only a fraction of traditional fossil sources and comparable to renewable technologies.\"\nWhile some have raised uncertainty surrounding the future GHG emissions of nuclear power as a result of an extreme potential decline in uranium ore grade without a corresponding increase in the efficiency of enrichment methods. In a scenario analysis of future global nuclear development, as it could be effected by a decreasing global uranium market of average ore grade, the analysis determined that depending on conditions, median life cycle nuclear power GHG emissions could be between 9 and 110 g -eq/kWh by 2050, with the latter high figure being derived from a \"worst-case scenario\" that is not \"considered very robust\" by the authors of the paper, as the \"ore grade\" in the scenario is lower than the uranium concentration in many lignite coal ashes.\n\nAlthough this future analyses primarily deals with extrapolations for present Generation II reactor technology, the same paper also summarizes the literature on \"FBRs\"/Fast Breeder Reactors, of which two are in operation as of 2014 with the newest being the BN-800, for these reactors it states that the \"median life cycle GHG emissions ... [are] similar to or lower than [present light water reactors] LWRs and purports to consume little or no uranium ore.\"\n\nIn their 2014 report, the IPCC comparison of energy sources global warming potential per unit of electricity generated, which notably included albedo effects, mirror the median emission value derived from the Warner and Heath Yale meta-analysis for the more common non-breeding light water reactors, a -equivalent value of 12 g -eq/kWh, which is the lowest global warming forcing of all baseload power sources, with comparable low carbon power baseload sources, such as hydropower and biomass, producing substantially more global warming forcing 24 and 230 g -eq/kWh respectively.\n\nIn 2014, Brookings Institution published \"The Net Benefits of Low and No-Carbon Electricity Technologies\" which states, after performing an energy and emissions cost analysis, that \"The net benefits of new nuclear, hydro, and natural gas combined cycle plants far outweigh the net benefits of new wind or solar plants\", with the most cost effective low carbon power technology being determined to be nuclear power.\n\nDuring his presidential campaign, Barack Obama stated, \"Nuclear power represents more than 70% of our noncarbon generated electricity. It is unlikely that we can meet our aggressive climate goals if we eliminate nuclear power as an option.\"\n\nAnalysis in 2015 by Professor and Chair of Environmental Sustainability Barry W. Brook and his colleagues on the topic of replacing fossil fuels entirely, from the electric grid of the world, has determined that at the historically modest and proven-rate at which nuclear energy was added to and replaced fossil fuels in France and Sweden during each nation's building programs in the 1980s, within 10 years nuclear energy could displace or remove fossil fuels from the electric grid completely, \"allow[ing] the world to meet the most stringent greenhouse-gas mitigation targets.\". In a similar analysis, Brook had earlier determined that 50% of all global energy, that is not solely electricity, but transportation synfuels etc. could be generated within approximately 30 years, if the global nuclear fission build rate was identical to each of these nation's already proven decadal rates(in units of installed nameplate capacity, GW per year, per unit of global GDP(GW/year/$).\n\nThis is in contrast to the completely conceptual paper-studies for a \"100% renewable energy\" world, which would require an orders of magnitude more costly global investment per year, an investment rate that has no historical precedent, having never been attempted due to its prohibitive cost, and with far greater land area that would be required to be devoted to the wind, wave and solar projects, along with the inherent assumption that humanity will use less, and not more, energy in the future. As Brook notes the \"principal limitations on nuclear fission are not technical, economic or fuel-related, but are instead linked to complex issues of societal acceptance, fiscal and political inertia, and inadequate critical evaluation of the real-world constraints facing [the other] low-carbon alternatives.\"\n\nNuclear power may be uncompetitive compared with fossil fuel energy sources in countries without a carbon tax program, and in comparison to a fossil fuel plant of the same power output, nuclear power plants take a longer amount of time to construct.\n\nTwo new, first of their kind, EPR reactors under construction in Finland and France have been delayed and are running over-budget. However learning from experience, two further EPR reactors under construction in China are on, and ahead, of schedule respectively. As of 2013, according to the IAEA and the European Nuclear Society, worldwide there were 68 civil nuclear power reactors under construction in 15 countries. China has 29 of these nuclear power reactors under construction, as of 2013, with plans to build many more, while in the US the licenses of almost half its reactors have been extended to 60 years, and plans to build another dozen are under serious consideration. There are also a considerable number of new reactors being built in South Korea, India, and Russia. At least 100 older and smaller reactors will \"most probably be closed over the next 10–15 years\". This is probable only if one does not factor in the ongoing Light Water Reactor Sustainability Program, created to permit the extension of the life span of the USA's 104 nuclear reactors to 60 years. The licenses of almost half of the USA's reactors have been extended to 60 years as of 2008. Two new \"passive safety\" AP1000 reactors are, as of 2013, being constructed at Vogtle Electric Generating Plant.\n\nPublic opinion about nuclear power varies widely between countries. A poll by Gallup International (2011) assessed public opinion in 47 countries. The poll was conducted following a tsunami and earthquake which caused an accident at the Fukushima nuclear power plant in Japan. 49% stated that they held favourable views about nuclear energy, while 43% held an unfavourable view. Another global survey by Ipsos (2011) assessed public opinion in 24 countries. Respondents to this survey showed a clear preference for renewable energy sources over coal and nuclear energy (refer to graph opposite). Ipsos (2012) found that solar and wind were viewed by the public as being more environmentally friendly and more viable long-term energy sources relative to nuclear power and natural gas. However, solar and wind were viewed as being less reliable relative to nuclear power and natural gas. In 2012 a poll done in the UK found that 63% of those surveyed support nuclear power, and with opposition to nuclear power at 11%. In Germany, strong anti-nuclear sentiment led to eight of the seventeen operating reactors being permanently shut down following the March 2011 Fukushima nuclear disaster.\n\nNuclear fusion research, in the form of the International Thermonuclear Experimental Reactor is underway. Fusion powered electricity generation was initially believed to be readily achievable, as fission power had been. However, the extreme requirements for continuous reactions and plasma containment led to projections being extended by several decades. In 2010, more than 60 years after the first attempts, commercial power production was still believed to be unlikely before 2050. Although rather than an either, or, issue economical fusion-fission hybrid reactors could be built before any attempt at this more demanding commercial \"pure-fusion reactor\"/DEMO reactor takes place.\n\nMost mitigation proposals imply—rather than directly state—an eventual reduction in global fossil fuel production. Also proposed are direct quotas on global fossil fuel production.\n\nNatural gas emits far fewer greenhouse gases (i.e. and methane—CH) than coal when burned at power plants, but evidence has been emerging that this benefit could be completely negated by methane leakage at gas drilling fields and other points in the supply chain.\n\nA study performed by the Environmental Protection Agency (EPA) and the Gas Research Institute (GRI) in 1997 sought to discover whether the reduction in carbon dioxide emissions from increased natural gas (predominantly methane) use would be offset by a possible increased level of methane emissions from sources such as leaks and emissions. The study concluded that the reduction in emissions from increased natural gas use outweighs the detrimental effects of increased methane emissions. More recent peer-reviewed studies have challenged the findings of this study, with researchers from the National Oceanic and Atmospheric Administration (NOAA) reconfirming findings of high rates of methane (CH4) leakage from natural gas fields.\n\nA 2011 study by noted climate research scientist, Tom Wigley, found that while carbon dioxide () emissions from fossil fuel combustion may be reduced by using natural gas rather than coal to produce energy, it also found that additional methane (CH4) from leakage adds to the radiative forcing of the climate system, offsetting the reduction in forcing that accompanies the transition from coal to gas. The study looked at methane leakage from coal mining; changes in radiative forcing due to changes in the emissions of sulfur dioxide and carbonaceous aerosols; and differences in the efficiency of electricity production between coal- and gas-fired power generation. On balance, these factors more than offset the reduction in warming due to reduced emissions. When gas replaces coal there is additional warming out to 2,050 with an assumed leakage rate of 0%, and out to 2,140 if the leakage rate is as high as 10%. The overall effects on global-mean temperature over the 21st century, however, are small. Petron et al. (2013) and Alvarez et al. (2012) note that estimated that leakage from gas infrastructure is likely to be underestimated. These studies indicate that the exploitation of natural gas as a \"cleaner\" fuel is questionable. A 2014 meta-study of 20 years of natural gas technical literature shows that methane emissions are consistently underestimated but on a 100-year scale, the climate benefits of coal to gas fuel switching are likely larger than the negative effects of natural gas leakage.\n\nA heat pump is a device that provides heat energy from a source of heat to a destination called a \"heat sink\". Heat pumps are designed to move thermal energy opposite to the direction of spontaneous heat flow by absorbing heat from a cold space and releasing it to a warmer one. A heat pump uses some amount of external power to accomplish the work of transferring energy from the heat source to the heat sink.\n\nWhile air conditioners and freezers are familiar examples of heat pumps, the term \"heat pump\" is more general and applies to many HVAC (heating, ventilating, and air conditioning) devices used for space heating or space cooling. When a heat pump is used for heating, it employs the same basic refrigeration-type cycle used by an air conditioner or a refrigerator, but in the opposite direction—releasing heat into the conditioned space rather than the surrounding environment. In this use, heat pumps generally draw heat from the cooler external air or from the ground. In heating mode, heat pumps are three to four times more efficient in their use of electric power than simple electrical resistance heaters.\n\nIt has been concluded that heat pumps are the single technology that could reduce the greenhouse gas emissions of households better than every other technology that is available on the market. With a market share of 30% and (potentially) clean electricity, heat pumps could reduce global emissions by 8% annually. Using ground source heat pumps could reduce around 60% of the primary energy demand and 90% of emissions in Europe in 2050 and make handling high shares of renewable energy easier. Using surplus renewable energy in heat pumps is regarded as the most effective household means to reduce global warming and fossil fuel depletion.\n\nWith significant amounts of fossil fuel used in electricity production, demands on the electrical grid also generate greenhouse gases. Without a high share of low-carbon electricity, a domestic heat pump will produce more carbon emissions than using natural gas.\n\nFossil fuel may be phased-out with carbon-neutral and carbon-negative pipeline and transportation fuels created with power to gas and gas to liquids technologies. Carbon dioxide from fossil fuel flue gas can be used to produce plastic lumber allowing carbon negative reforestation.\n\nA carbon sink is a natural or artificial reservoir that accumulates and stores some carbon-containing chemical compound for an indefinite period, such as a growing forest. A negative carbon dioxide emission on the other hand is a permanent removal of carbon dioxide out of the atmosphere. Examples are direct air capture, enhanced weathering technologies such as storing it in geologic formations underground and biochar. These processes are sometimes considered as variations of sinks or mitigation, and sometimes as geoengineering. In combination with other mitigation measures, sinks in combination with negative carbon emissions are considered crucial for meeting the 350 ppm target.\n\nThe Antarctic Climate and Ecosystems Cooperative Research Centre (ACE-CRC) notes that one third of humankind's annual emissions of are absorbed by the oceans. However, this also leads to ocean acidification, with potentially significant impacts on marine life. Acidification lowers the level of carbonate ions available for calcifying organisms to form their shells. These organisms include plankton species that contribute to the foundation of the Southern Ocean food web. However acidification may impact on a broad range of other physiological and ecological processes, such as fish respiration, larval development and changes in the solubility of both nutrients and toxins.\n\nAlmost 20 percent (8GtCO/year) of total greenhouse-gas emissions were from deforestation in 2007. It is estimated that avoided deforestation reduces CO emissions at a rate of 1tonne of CO per $1–5 in opportunity costs from lost agriculture. Reforestation could save at least another 1GtCO/year, at an estimated cost of $5–15/tCO. Afforestation is where there was previously no forest - such plantations are estimated to have to be prohibitively massive to be reduce emissions by itself.\n\nTransferring rights over land from public domain to its indigenous inhabitants, who have had a stake for millennia in preserving the forests that they depend on, is argued to be a cost effective strategy to conserve forests. This includes the protection of such rights entitled in existing laws, such as India's Forest Rights Act. The transferring of such rights in China, perhaps the largest land reform in modern times, has been argued to have increased forest cover. Granting title of the land has shown to have two or three times less clearing than even state run parks, notably in the Brazilian Amazon. Excluding humans and even evicting inhabitants from protected areas (called \"fortress conservation\"), sometimes as a result of lobbying by environmental groups, often lead to more exploitation of the land as the native inhabitants then turn to work for extractive companies to survive.\n\nWith increased intensive agriculture and urbanization, there is an increase in the amount of abandoned farmland. By some estimates, for every half a hectare of original old-growth forest cut down, more than 20 hectares of new secondary forests are growing, even though they do not have the same biodiversity as the original forests and original forests store 60% more carbon than these new secondary forests. According to a study in \"Science\", promoting regrowth on abandoned farmland could offset years of carbon emissions.\n\nRestoring grasslands store CO from the air into plant material. Grazing livestock, usually not left to wander, would eat the grass and would minimize any grass growth. However, grass left alone would eventually grow to cover its own growing buds, preventing them from photosynthesizing and the dying plant would stay in place. A method proposed to restore grasslands uses fences with many small paddocks and moving herds from one paddock to another after a day a two in order to mimick natural grazers and allowing the grass to grow optimally. Additionally, when part of leaf matter is consumed by a herding animal, a corresponding amount of root matter is sloughed off too as it would not be able to sustain the previous amount of root matter and while most of the lost root matter would rot and enter the atmosphere, part of the carbon is sequestered into the soil. It is estimated that increasing the carbon content of the soils in the world's 3.5 billion hectares of agricultural grassland by 1% would offset nearly 12 years of CO emissions. Allan Savory, as part of holistic management, claims that while large herds are often blamed for desertification, prehistoric lands supported large or larger herds and areas where herds were removed in the United States are still desertifying.\n\nAdditionally, the global warming induced thawing of the permafrost, which stores about two times the amount of the carbon currently released in the atmosphere, releases the potent greenhouse gas, methane, in a positive feedback cycle that is feared to lead to a tipping point called runaway climate change. A method proposed to prevent such a scenario is to bring back large herbivores such as seen in Pleistocene Park, where their trampling naturally keep the ground cooler by eliminating shrubs and keeping the ground exposed to the cold air.\n\nCarbon capture and storage (CCS) is a method to mitigate climate change by capturing carbon dioxide (CO) from large point sources such as power plants and subsequently storing it away safely instead of releasing it into the atmosphere. The IPCC estimates that the costs of halting global warming would double without CCS. The International Energy Agency says CCS is \"the most important single new technology for CO savings\" in power generation and industry. Though it requires up to 40% more energy to run a CCS coal power plant than a regular coal plant, CCS could potentially capture about 90% of all the carbon emitted by the plant. Norway's Sleipner gas field, beginning in 1996, stores almost a million tons of CO a year to avoid penalties in producing natural gas with unusually high levels of CO. As of late 2011, the total planned storage capacity of all 14 projects in operation or under construction is over 33 million tonnes a year. This is broadly equivalent to preventing the emissions from more than six million cars from entering the atmosphere each year. According to a Sierra Club analysis, the US coal fired Kemper Project due to be online in 2017, is the most expensive power plant ever built for the watts of electricity it will generate.\n\nEnhanced weathering is the removal of carbon from the air into the earth, enhancing the natural carbon cycle where carbon is mineralized into rock. The CarbFix project couples with carbon capture and storage in power plants to turn carbon dioxide into stone in a relatively short period of two years, addressing the common concern of leakage in CCS projects. While this project used basalt rocks, olivine has also shown promise.\n\nGeoengineering is seen by Olivier Sterck as an alternative to mitigation and adaptation, but by Gernot Wagner as an entirely separate response to climate change. In a literature assessment, Barker \"et al.\" (2007) described geoengineering as a type of mitigation policy. IPCC (2007) concluded that geoengineering options, such as ocean fertilization to remove CO from the atmosphere, remained largely unproven. It was judged that reliable cost estimates for geoengineering had not yet been published.\n\nChapter 28 of the National Academy of Sciences report \"Policy Implications of Greenhouse Warming: Mitigation, Adaptation, and the Science Base\" (1992) defined geoengineering as \"options that would involve large-scale engineering of our environment in order to combat or counteract the effects of changes in atmospheric chemistry.\" They evaluated a range of options to try to give preliminary answers to two questions: can these options work and could they be carried out with a reasonable cost. They also sought to encourage discussion of a third question — what adverse side effects might there be. The following types of option were examined: reforestation, increasing ocean absorption of carbon dioxide (carbon sequestration) and screening out some sunlight. NAS also argued \"Engineered countermeasures need to be evaluated but should not be implemented without broad understanding of the direct effects and the potential side effects, the ethical issues, and the risks.\". In July 2011 a report by the United States Government Accountability Office on geoengineering found that \"[c]limate engineering technologies do not now offer a viable response to global climate change.\"\n\nCarbon dioxide removal has been proposed as a method of reducing the amount of radiative forcing. A variety of means of artificially capturing and storing carbon, as well as of enhancing natural sequestration processes, are being explored. The main natural process is photosynthesis by plants and single-celled organisms (see biosequestration). Artificial processes vary, and concerns have been expressed about the long-term effects of some of these processes.\n\nIt is notable that the availability of cheap energy and appropriate sites for geological storage of carbon may make carbon dioxide air capture viable commercially. It is, however, generally expected that carbon dioxide air capture may be uneconomic when compared to carbon capture and storage from major sources — in particular, fossil fuel powered power stations, refineries, etc. As in the case of the US Kemper Project with carbon capture, costs of energy produced will grow significantly. However, captured CO can be used to force more crude oil out of oil fields, as Statoil and Shell have made plans to do. CO can also be used in commercial greenhouses, giving an opportunity to kick-start the technology. Some attempts have been made to use algae to capture smokestack emissions, notably the GreenFuel Technologies Corporation, who have now shut down operations.\n\nThe main purpose of solar radiation management seek to reflect sunlight and thus reduce global warming. The ability of stratospheric sulfate aerosols to create a global dimming effect has made them a possible candidate for use in climate engineering projects.\n\nCO is not the only GHG relevant to mitigation, and governments have acted to regulate the emissions of other GHGs emitted by human activities (anthropogenic GHGs). The emissions caps agreed to by most developed countries under the Kyoto Protocol regulate the emissions of almost all the anthropogenic GHGs.\nThese gases are CO, methane (CH), nitrous oxide (NO), the hydrofluorocarbons (HFC), perfluorocarbons (PFC), and sulfur hexafluoride (SF).\n\nStabilizing the atmospheric concentrations of the different anthropogenic GHGs requires an understanding of their different physical properties. Stabilization depends both on how quickly GHGs are added to the atmosphere and how fast they are removed. The rate of removal is measured by the atmospheric lifetime of the GHG in question (see the main GHG article for a list). Here, the lifetime is defined as the time required for a given perturbation of the GHG in the atmosphere to be reduced to 37% of its initial amount.\nMethane has a relatively short atmospheric lifetime of about 12 years, while NO's lifetime is about 110 years. For methane, a reduction of about 30% below current emission levels would lead to a stabilization in its atmospheric concentration, while for NO, an emissions reduction of more than 50% would be required.\n\nMethane is a significantly more potent greenhouse gas than carbon dioxide in the amount of heat it can trap, especially in the short term. Burning one molecule of methane generates one molecule of carbon dioxide, indicating there may be no net benefit in using gas as a fuel source. Reducing the amount of waste methane produced in the first place and moving away from use of gas as a fuel source will have a greater beneficial impact, as might other approaches to productive use of otherwise-wasted methane. In terms of prevention, vaccines are being developed in Australia to reduce the significant global warming contributions from methane released by livestock via flatulence and eructation.\n\nAnother physical property of the anthropogenic GHGs relevant to mitigation is the different abilities of the gases to trap heat (in the form of infrared radiation). Some gases are more effective at trapping heat than others, e.g., SF is 22,200 times more effective a GHG than CO on a per-kilogram basis.\nA measure for this physical property is the global warming potential (GWP), and is used in the Kyoto Protocol.\n\nAlthough not designed for this purpose, the Montreal Protocol has probably benefited climate change mitigation efforts.\nThe Montreal Protocol is an international treaty that has successfully reduced emissions of ozone-depleting substances (for example, CFCs), which are also greenhouse gases.\n\nTransportation emissions account for roughly 1/4 of emissions worldwide, and are even more important in terms of impact in developed nations especially in North America and Australia. Many citizens of countries like the United States and Canada who drive personal cars often, see well over half of their climate change impact stemming from the emissions produced from their cars. Modes of mass transportation such as bus, light rail (metro, subway, etc.), and long-distance rail are far and away the most energy-efficient means of motorized transportation for passengers, able to use in many cases over twenty times less energy per person-distance than a personal automobile. Modern energy-efficient technologies, such as plug-in hybrid electric vehicles and carbon-neutral synthetic gasoline & Jet fuel may also help to reduce the consumption of petroleum, land use changes and emissions of carbon dioxide. Utilizing rail transport, especially electric rail, over the far less efficient air transport and truck transport\nsignificantly reduces emissions. With the use of electric trains and cars in transportation there is the opportunity to run them with low-carbon power, producing far fewer emissions.\n\nEffective urban planning to reduce sprawl aims to decrease Vehicle Miles Travelled (VMT), lowering emissions from transportation. Personal cars are extremely inefficient at moving passengers, while public transport and bicycles are many times more efficient (as is the simplest form of human transportation, walking). All of these are encouraged by urban/community planning and are an effective way to reduce greenhouse gas emissions. Between 1982 and 1997, the amount of land consumed for urban development in the United States increased by 47 percent while the nation's population grew by only 17 percent.\nInefficient land use development practices have increased infrastructure costs as well as the amount of energy needed for transportation, community services, and buildings.\n\nAt the same time, a growing number of citizens and government officials have begun advocating a smarter approach to land use planning. These smart growth practices include compact community development, multiple transportation choices, mixed land uses, and practices to conserve green space. These programs offer environmental, economic, and quality-of-life benefits; and they also serve to reduce energy usage and greenhouse gas emissions.\n\nApproaches such as New Urbanism and transit-oriented development seek to reduce distances travelled, especially by private vehicles, encourage public transit and make walking and cycling more attractive options. This is achieved through \"medium-density\", mixed-use planning and the concentration of housing within walking distance of town centers and transport nodes.\n\nSmarter growth land use policies have both a direct and indirect effect on energy consuming behavior. For example, transportation energy usage, the number one user of petroleum fuels, could be significantly reduced through more compact and mixed use land development patterns, which in turn could be served by a greater variety of non-automotive based transportation choices.\n\nEmissions from housing are substantial, and government-supported energy efficiency programmes can make a difference.\n\nFor institutions of higher learning in the United States, greenhouse gas emissions depend primarily on total area of buildings and secondarily on climate. If climate is not taken into account, annual greenhouse gas emissions due to energy consumed on campuses plus purchased electricity can be estimated with the formula, \"E=aS\", where \"a\" =0.001621 metric tonnes of CO equivalent/square foot or 0.0241 metric tonnes of CO equivalent/square meter and \"b\"= 1.1354.\n\nNew buildings can be constructed using passive solar building design, low-energy building, or zero-energy building techniques, using renewable heat sources. Existing buildings can be made more efficient through the use of insulation, high-efficiency appliances (particularly hot water heaters and furnaces), double- or triple-glazed gas-filled windows, external window shades, and building orientation and siting. Renewable heat sources such as shallow geothermal and passive solar energy reduce the amount of greenhouse gasses emitted. In addition to designing buildings which are more energy-efficient to heat, it is possible to design buildings that are more energy-efficient to cool by using lighter-coloured, more reflective materials in the development of urban areas (e.g. by painting roofs white) and planting trees. This saves energy because it cools buildings and reduces the urban heat island effect thus reducing the use of air conditioning.\n\nAccording to the EPA, agricultural soil management practices can lead to production and emission of nitrous oxide (N2O), a major greenhouse gas and air pollutant. Activities that can contribute to emissions include fertilizer usage, irrigation, and tillage. The management of soils accounts for over half of the emissions from the Agriculture sector. Cattle livestocks account for one third of emissions, through methane emissions. Manure management and rice cultivation also produce gaseous emissions.\n\nMethods that significantly enhance carbon sequestration in soil include no-till farming, residue mulching, cover cropping, and crop rotation, all of which are more widely used in organic farming than in conventional farming. Because only 5% of US farmland currently uses no-till and residue mulching, there is a large potential for carbon sequestration.\n\nA 2015 study found that farming can deplete soil carbon and render soil incapable of supporting life; however, the study also showed that conservation farming can protect carbon in soils, and repair damage over time.\n\nThe farming practise of cover crops has been recognized as climate-smart agriculture by the White House.\n\nIn Europe the estimation of the current 0–30 cm SOC stock of agricultural soils was 17.63 Gt. In a subsequent study, authors estimated the best management practices to mitigate soil organic carbon: conversion of arable land to grassland (and vice versa), straw incorporation, reduced tillage, straw incorporation combined with reduced tillage, ley cropping system and cover crops.\n\nAnother method being examined is to make carbon a new currency by introducing tradeable \"personal carbon credits\". The idea being it will encourage and motivate individuals to reduce their 'carbon footprint' by the way they live. Each citizen will receive a free annual quota of carbon that they can use to travel, buy food, and go about their business. It has been suggested that by using this concept it could actually solve two problems; pollution and poverty, old age pensioners will actually be better off because they fly less often, so they can cash in their quota at the end of the year to pay heating bills and so forth.\n\nVarious organizations promote population control as a means for mitigating global warming. Proposed measures include improving access to family planning and reproductive health care and information, reducing natalistic politics, public education about the consequences of continued population growth, and improving access of women to education and economic opportunities.\n\nPopulation control efforts are impeded by there being somewhat of a taboo in some countries against considering any such efforts. Also, various religions discourage or prohibit some or all forms of birth control.\n\nPopulation size has a different per capita effect on global warming in different countries, since the per capita production of anthropogenic greenhouse gases varies greatly by country.\n\nThe Stern Review proposes stabilising the concentration of greenhouse-gas emissions in the atmosphere at a maximum of 550ppm COe by 2050. The Review estimates that this would mean cutting total greenhouse-gas emissions to three quarters of 2007 levels. The Review further estimates that the cost of these cuts would be in the range −1.0 to +3.5% of World GDP, (i.e. GWP), with an average estimate of approximately 1%. Stern has since revised his estimate to 2% of GWP. For comparison, the Gross World Product (GWP) at PPP was estimated at $74.5 trillion in 2010, thus 2% is approximately $1.5 trillion. The Review emphasises that these costs are contingent on steady reductions in the cost of low-carbon technologies. Mitigation costs will also vary according to how and when emissions are cut: early, well-planned action will minimise the costs.\n\nOne way of estimating the cost of reducing emissions is by considering the likely costs of potential technological and output changes. Policy makers can compare the marginal abatement costs of different methods to assess the cost and amount of possible abatement over time. The marginal abatement costs of the various measures will differ by country, by sector, and over time.\n\nYohe \"et al.\" (2007) assessed the literature on sustainability and climate change. With high confidence, they suggested that up to the year 2050, an effort to cap greenhouse gas (GHG) emissions at 550 ppm would benefit developing countries significantly. This was judged to be especially the case when combined with enhanced adaptation. By 2100, however, it was still judged likely that there would be significant effects of global warming. This was judged to be the case even with aggressive mitigation and significantly enhanced adaptive capacity.\n\nOne of the aspects of mitigation is how to share the costs and benefits of mitigation policies. There is no scientific consensus over how to share these costs and benefits (Toth \"et al.\", 2001). In terms of the politics of mitigation, the UNFCCC's ultimate objective is to stabilize concentrations of GHG in the atmosphere at a level that would prevent \"dangerous\" climate change (Rogner \"et al.\", 2007).\n\nGHG emissions are an important correlate of wealth, at least at present (Banuri \"et al.\", 1996, pp. 91–92). Wealth, as measured by per capita income (i.e., income per head of population), varies widely between different countries. Activities of the poor that involve emissions of GHGs are often associated with basic needs, such as heating to stay tolerably warm. In richer countries, emissions tend to be associated with things like cars, central heating, etc. The impacts of cutting emissions could therefore have different impacts on human welfare according to wealth.\n\nThere have been different proposals on how to allocate responsibility for cutting emissions (Banuri \"et al.\", 1996, pp. 103–105):\n\n\nMany countries, both developing and developed, are aiming to use cleaner technologies (World Bank, 2010, p. 192). Use of these technologies aids mitigation and could result in substantial reductions in CO emissions. Policies include targets for emissions reductions, increased use of renewable energy, and increased energy efficiency. It is often argued that the results of climate change are more damaging in poor nations, where infrastructures are weak and few social services exist. The Commitment to Development Index is one attempt to analyze rich country policies taken to reduce their disproportionate use of the global commons. Countries do well if their greenhouse gas emissions are falling, if their gas taxes are high, if they do not subsidize the fishing industry, if they have a low fossil fuel rate per capita, and if they control imports of illegally cut tropical timber.\n\nThe main current international agreement on combating climate change is the Kyoto Protocol. On the 11th of December 1997 it was implemented by the 3rd conference of parties, which was coming together in kyoto, which came into force on 16 February 2005. The Kyoto Protocol is an amendment to the United Nations Framework Convention on Climate Change (UNFCCC). Countries that have ratified this protocol have committed to reduce their emissions of carbon dioxide and five other greenhouse gases, or engage in emissions trading if they maintain or increase emissions of these gases. For Kyoto reporting, governments are obliged to be told on the present state of the countries forests and the related ongoing processes.\n\nActions to mitigate climate change are sometimes based on the goal of achieving a particular temperature target. One of the targets that has been suggested is to limit the future increase in global mean temperature (global warming) to below 2 °C, relative to the pre-industrial level. The 2 °C target was adopted in 2010 by Parties to the United Nations Framework Convention on Climate Change. Most countries of the world are Parties to the UNFCCC. The target had been adopted in 1996 by the European Union Council.\n\nTemperatures have increased by 0.8 °C compared to the pre-industrial level, and another 0.5–0.7 °C is already committed. The 2 °C rise is typically associated in climate models with a carbon dioxide equivalent concentration of 400–500 ppm by volume; the current (January 2015) level of carbon dioxide alone is 400 ppm by volume, and rising at 1–3 ppm annually. Hence, to avoid a very likely breach of the 2 °C target, CO levels would have to be stabilised very soon; this is generally regarded as unlikely, based on current programs in place to date. The importance of change is illustrated by the fact that world economic energy efficiency is improving at only half the rate of world economic growth.\n\nThere is disagreement among experts over whether or not the 2 °C target can be met. For example, according to Anderson and Bows (2011), \"there is little to no chance\" of meeting the target. On the other hand, according to Alcamo \"et al.\" (2013):\n\nScientific analysis can provide information on the impacts of climate change and associated policies, such as reducing GHG emissions. However, deciding what policies are best requires value judgements. For example, limiting global warming to 1 °C relative to pre-industrial levels may help to reduce climate change damages more than a 2 °C limit. However, a 1 °C limit may be more costly to achieve than a 2 °C limit.\n\nAccording to some analysts, the 2 °C \"guardrail\" is inadequate for the needed degree and timeliness of mitigation. On the other hand, some economic studies suggest more modest mitigation policies. For example, the emissions reductions proposed by Nordhaus (2010) might lead to global warming (in the year 2100) of around 3 °C, relative to pre-industrial levels.\n\nIn 2015, two official UNFCCC scientific expert bodies came to the conclusion that, \"in some regions and vulnerable ecosystems, high risks are projected even for warming above 1.5°C\". This expert position was, together with the strong diplomatic voice of the poorest countries and the island nations in the Pacific, the driving force leading to the decision of the Paris Conference 2015, to lay down this 1.5 °C long-term target on top of the existing 2 °C goal.\n\nAn emissions tax on greenhouse gas emissions requires individual emitters to pay a fee, charge or tax for every tonne of greenhouse gas released into the atmosphere. Most environmentally related taxes with implications for greenhouse gas emissions in OECD countries are levied on energy products and motor vehicles, rather than on CO emissions directly.\n\nEmission taxes can be both cost-effective and environmentally effective. Difficulties with emission taxes include their potential unpopularity, and the fact that they cannot guarantee a particular level of emissions reduction. Emissions or energy taxes also often fall disproportionately on lower income classes. In developing countries, institutions may be insufficiently developed for the collection of emissions fees from a wide variety of sources.\n\nAccording to Mark Z. Jacobson, a program of subsidization balanced against expected flood costs could pay for conversion to 100% renewable power by 2030. Jacobson, and his colleague Mark Delucchi, suggest that the cost to generate and transmit power in 2020 will be less than 4 cents per kilowatt hour (in 2007 dollars) for wind, about 4 cents for wave and hydroelectric, from 4 to 7 cents for geothermal, and 8 cents per kWh for solar, fossil, and nuclear power.\n\nAnother indirect method of encouraging uses of renewable energy, and pursue sustainability and environmental protection, is that of prompting investment in this area through legal means, something that is already being done at national level as well as in the field of international investment.\n\nWith the creation of a market for trading carbon dioxide emissions within the Kyoto Protocol, it is likely that London financial markets will be the centre for this potentially highly lucrative business; the New York and Chicago stock markets may have a lower trade volume than expected as long as the US maintains its rejection of the Kyoto.\n\nHowever, emissions trading may delay the phase-out of fossil fuels.\n\nIn the north-east United States, a successful cap and trade program has shown potential for this solution.\n\nThe European Union Emission Trading Scheme (EU ETS) is the largest multi-national, greenhouse gas emissions trading scheme in the world. It commenced operation on 1 January 2005, and all 28 member states of the European Union participate in the scheme which has created a new market in carbon dioxide allowances estimated at 35 billion Euros (US$43 billion) per year. The Chicago Climate Exchange was the first (voluntary) emissions market, and is soon to be followed by Asia's first market (Asia Carbon Exchange). A total of 107 million metric tonnes of carbon dioxide equivalent have been exchanged through projects in 2004, a 38% increase relative to 2003 (78 Mt COe).\n\nTwenty three multinational corporations have come together in the G8 Climate Change Roundtable, a business group formed at the January 2005 World Economic Forum. The group includes Ford, Toyota, British Airways, and BP. On 9 June 2005 the Group published a statement stating that there was a need to act on climate change and claiming that market-based solutions can help. It called on governments to establish \"clear, transparent, and consistent price signals\" through \"creation of a long-term policy framework\" that would include all major producers of greenhouse gases.\n\nThe Regional Greenhouse Gas Initiative is a proposed carbon trading scheme being created by nine North-eastern and Mid-Atlantic American states; Connecticut, Delaware, Maine, Massachusetts, New Hampshire, New Jersey, New York, Rhode Island, and Vermont. The scheme was due to be developed by April 2005 but has not yet been completed.\n\nImplementation puts into effect climate change mitigation strategies and targets. These can be targets set by international bodies or voluntary action by individuals or institutions. This is the most important, expensive and least appealing aspect of environmental governance.\n\nImplementation requires funding sources but is often beset by disputes over who should provide funds and under what conditions. A lack of funding can be a barrier to successful strategies as there are no formal arrangements to finance climate change development and implementation. Funding is often provided by nations, groups of nations and increasingly NGO and private sources. These funds are often channelled through the Global Environmental Facility (GEF). This is an environmental funding mechanism in the World Bank which is designed to deal with global environmental issues. The GEF was originally designed to tackle four main areas: biological diversity, climate change, international waters and ozone layer depletion, to which land degradation and persistent organic pollutant were added. The GEF funds projects that are agreed to achieve global environmental benefits that are endorsed by governments and screened by one of the GEF's implementing agencies.\n\nThere are numerous issues which result in a current perceived lack of implementation. It has been suggested that the main barriers to implementation are Uncertainty, Fragmentation, Institutional void, Short time horizon of policies and politicians and Missing motives and willingness to start adapting. The relationships between many climatic processes can cause large levels of uncertainty as they are not fully understood and can be a barrier to implementation. When information on climate change is held between the large numbers of actors involved it can be highly dispersed, context specific or difficult to access causing fragmentation to be a barrier. Institutional void is the lack of commonly accepted rules and norms for policy processes to take place, calling into question the legitimacy and efficacy of policy processes. The Short time horizon of policies and politicians often means that climate change policies are not implemented in favour of socially favoured societal issues. Statements are often posed to keep the illusion of political action to prevent or postpone decisions being made. Missing motives and willingness to start adapting is a large barrier as it prevents any implementation. The issues that arise with a system which involves international government cooperation, such as cap and trade, could potentially be improved with a polycentric approach where the rules are enforced by many small sections of authority as opposed to one overall enforcement agency. Concerns about metal requirement and/or availability for essential decarbonization technoloqies such as photovoltaics, nuclear power, and (plug-in hybrid) electric vehicles have also been expressed as obstacles.\n\nDespite a perceived lack of occurrence, evidence of implementation is emerging internationally. Some examples of this are the initiation of NAPA's and of joint implementation. Many developing nations have made National Adaptation Programs of Action (NAPAs) which are frameworks to prioritize adaption needs. The implementation of many of these is supported by GEF agencies. Many developed countries are implementing 'first generation' institutional adaption plans particularly at the state and local government scale. There has also been a push towards joint implementation between countries by the UNFCCC as this has been suggested as a cost-effective way for objectives to be achieved.\n\nEfforts to reduce greenhouse gas emissions by the United States include energy policies which encourage efficiency through programs like Energy Star, Commercial Building Integration, and the Industrial Technologies Program. On 12 November 1998, Vice President Al Gore symbolically signed the Kyoto Protocol, but he indicated participation by the developing nations was necessary prior its being submitted for ratification by the United States Senate.\n\nIn 2007, Transportation Secretary Mary Peters, with White House approval, urged governors and dozens of members of the House of Representatives to block California's first-in-the-nation limits on greenhouse gases from cars and trucks, according to e-mails obtained by Congress. The US Climate Change Science Program is a group of about twenty federal agencies and US Cabinet Departments, all working together to address global warming.\n\nThe Bush administration pressured American scientists to suppress discussion of global warming, according to the testimony of the Union of Concerned Scientists to the Oversight and Government Reform Committee of the US House of Representatives. \"High-quality science\" was \"struggling to get out,\" as the Bush administration pressured scientists to tailor their writings on global warming to fit the Bush administration's skepticism, in some cases at the behest of an ex-oil industry lobbyist. \"Nearly half of all respondents perceived or personally experienced pressure to eliminate the words 'climate change,' 'global warming' or other similar terms from a variety of communications.\" Similarly, according to the testimony of senior officers of the Government Accountability Project, the White House attempted to bury the report \"National Assessment of the Potential Consequences of Climate Variability and Change,\" produced by US scientists pursuant to US law. Some US scientists resigned their jobs rather than give in to White House pressure to underreport global warming.\n\nIn the absence of substantial federal action, state governments have adopted emissions-control laws such as the Regional Greenhouse Gas Initiative in the Northeast and the Global Warming Solutions Act of 2006 in California.\n\nIn order to reconcile economic development with mitigating carbon emissions, developing countries need particular support, both financial and technical. One of the means of achieving this is the Kyoto Protocol's Clean Development Mechanism (CDM). The World Bank's Prototype Carbon Fund is a public private partnership that operates within the CDM.\n\nAn important point of contention, however, is how overseas development assistance not directly related to climate change mitigation is affected by funds provided to climate change mitigation. One of the outcomes of the UNFCC Copenhagen Climate Conference was the Copenhagen Accord, in which developed countries promised to provide US$30 million between 2010 and 2012 of new and additional resources. Yet it remains unclear what exactly the definition of additional is and the European Commission has requested its member states to define what they understand to be additional, and researchers at the Overseas Development Institute have found four main understandings:\nThe main point being that there is a conflict between the OECD states budget deficit cuts, the need to help developing countries adapt to develop sustainably and the need to ensure that funding does not come from cutting aid to other important Millennium Development Goals.\n\nHowever, none of these initiatives suggest a quantitative cap on the emissions from developing countries. This is considered as a particularly difficult policy proposal as the economic growth of developing countries are proportionally reflected in the growth of greenhouse emissions. Critics of mitigation often argue that, the developing countries' drive to attain a comparable living standard to the developed countries would doom the attempt at mitigation of global warming. Critics also argue that holding down emissions would shift the human cost of global warming from a general one to one that was borne most heavily by the poorest populations on the planet.\n\nIn an attempt to provide more opportunities for developing countries to adapt clean technologies, UNEP and WTO urged the international community to reduce trade barriers and to conclude the Doha trade round \"which includes opening trade in environmental goods and services\".\n\nWhile many of the proposed methods of mitigating global warming require governmental funding, legislation and regulatory action, individuals and businesses can also play a part in the mitigation effort.\n\nEnvironmental groups encourage individual action against global warming, often aimed at the consumer. Common recommendations include lowering home heating and cooling usage, burning less gasoline, supporting renewable energy sources, buying local products to reduce transportation, turning off unused devices, and various others.\n\nA geophysicist at Utrecht University has urged similar institutions to hold the vanguard in voluntary mitigation, suggesting the use of communications technologies such as videoconferencing to reduce their dependence on long-haul flights.\n\nIn 2008, climate scientist Kevin Anderson raised concern about the growing effect of rapidly increasing global air transport on the climate in a paper, and a presentation, suggesting that reversing this trend is necessary to reduce emissions.\n\nPart of the difficulty is that when aviation emissions are made at high altitude, the climate impacts are much greater than otherwise. Others have been raising the related concerns of the increasing hypermobility of individuals, whether traveling for business or pleasure, involving frequent and often long distance air travel, as well as air shipment of goods.\n\nOn 9 May 2005 Jeff Immelt, the chief executive of General Electric (GE), announced plans to reduce GE's global warming related emissions by one percent by 2012. \"GE said that given its projected growth, those emissions would have risen by 40 percent without such action.\"\n\nOn 21 June 2005 a group of leading airlines, airports, and aerospace manufacturers pledged to work together to reduce the negative environmental impact of aviation, including limiting the impact of air travel on climate change by improving fuel efficiency and reducing carbon dioxide emissions of new aircraft by fifty percent per seat kilometre by 2020 from 2000 levels. The group aims to develop a common reporting system for carbon dioxide emissions per aircraft by the end of 2005, and pressed for the early inclusion of aviation in the European Union's carbon emission trading scheme.\n\nClimate change is also a concern for large institutional investors who have a long term time horizon and potentially large exposure to the negative impacts of global warming because of the large geographic footprint of their multi-national holdings. SRI (Socially responsible investing) Funds allow investors to invest in funds that meet high ESG (environmental, social, governance) standards as such funds invest in companies that are aligned with these goals. Proxy firms can be used to draft guidelines for investment managers that take these concerns into account.\n\nIn some countries, those affected by climate change may be able to sue major producers. Attempts at litigation have been initiated by entire peoples such as Palau and the Inuit, as well as non-governmental organizations such as the Sierra Club. Although proving that particular weather events are due specifically to global warming may never be possible, methodologies have been developed to show the increased risk of such events caused by global warming.\n\nFor a legal action for negligence (or similar) to succeed, \"Plaintiffs ... must show that, more probably than not, their individual injuries were caused by the risk factor in question, as opposed to any other cause. This has sometimes been translated to a requirement of a relative risk of at least two.\" Another route (though with little legal bite) is the World Heritage Convention, if it can be shown that climate change is affecting World Heritage Sites like Mount Everest.\n\nBesides countries suing one another, there are also cases where people in a country have taken legal steps against their own government. Legal action for instance has been taken to try to force the US Environmental Protection Agency to regulate greenhouse gas emissions under the Clean Air Act, and against the Export-Import Bank and OPIC for failing to assess environmental impacts (including global warming impacts) under NEPA.\n\nIn the Netherlands and Belgium, organisations such as Urgenda and the vzw Klimaatzaak in Belgium have also sued their governments as they believe their governments aren't meeting the emission reductions they agreed to. Urgenda have already won their case against the Dutch government.\n\nAccording to a 2004 study commissioned by Friends of the Earth, ExxonMobil, and its predecessors caused 4.7 to 5.3 percent of the world's man-made carbon dioxide emissions between 1882 and 2002. The group suggested that such studies could form the basis for eventual legal action.\n\nIn 2015, Exxon received a subpoena. According to the \"Washington Post\" and confirmed by the company, the attorney general of New York, Eric Schneiderman, opened an investigation into the possibility that the company had misled the public and investors about the risks of climate change.\n\nLow carbon investment and ethical banking has been suggested as a tactic to allow consumers to drive a low-carbon transition. Roughly 5% of OECD disposable income is saved, and could alternately be saved in low-carbon investment funds to substantially increase overall low-carbon investment. \n\nVoluntarily funded, low-carbon investment funds have been suggested as a way to provide revenue for adaptation costs in the post-fossil fuel era. It is suggested that voluntary donations can be invested, rather than spent, and long-term returns used to pay for adaptation costs. \n\n\n\n\n\n\n"}
{"id": "1476440", "url": "https://en.wikipedia.org/wiki?curid=1476440", "title": "Cobweb model", "text": "Cobweb model\n\nThe cobweb model or cobweb theory is an economic model that explains why prices might be subject to periodic fluctuations in certain types of markets. It describes cyclical supply and demand in a market where the amount produced must be chosen before prices are observed. Producers' expectations about prices are assumed to be based on observations of previous prices. Nicholas Kaldor analyzed the model in 1934, coining the term \"cobweb theorem\" (see Kaldor, 1938 and Pashigian, 2008), citing previous analyses in German by Henry Schultz and .\n\nThe cobweb model is based on a time lag between supply and demand decisions. Agricultural markets are a context where the cobweb model might apply, since there is a lag between planting and harvesting (Kaldor, 1934, p. 133-134 gives two agricultural examples: rubber and corn). Suppose for example that as a result of unexpectedly bad weather, farmers go to market with an unusually small crop of strawberries. This shortage, equivalent to a leftward shift in the market's supply curve, results in high prices. If farmers expect these high price conditions to continue, then in the following year, they will raise their production of strawberries relative to other crops. Therefore, when they go to market the supply will be high, resulting in low prices. If they then expect low prices to continue, they will decrease their production of strawberries for the next year, resulting in high prices again.\n\nThis process is illustrated by the adjacent diagrams. The equilibrium price is at the intersection of the supply and demand curves. A poor harvest in period 1 means supply falls to Q, so that prices rise to P. If producers plan their period 2 production under the expectation that this high price will continue, then the period 2 supply will be higher, at Q. Prices therefore fall to P when they try to sell all their output. As this process repeats itself, oscillating between periods of low supply with high prices and then high supply with low prices, the price and quantity trace out a spiral. They may spiral inwards, as in the top figure, in which case the economy converges to the equilibrium where supply and demand cross; or they may spiral outwards, with the fluctuations increasing in magnitude.\n\nThe cobweb model can have two types of outcomes:\n\nTwo other possibilities are:\n\nIn either of the first two scenarios, the combination of the spiral and the supply and demand curves often looks like a cobweb, hence the name of the theory.\n\nThe outcomes of the cobweb model are stated above in terms of slopes, but they are more commonly described in terms of elasticities. In terms of slopes, the \"convergent case\" requires that the slope of the supply curve be greater than the absolute value of the slope of the demand curve:\n\nIn standard microeconomics terminology, define the \"elasticity of supply\" as formula_2, and the \"elasticity of demand\" as formula_3. If we evaluate these two elasticities at the equilibrium point, that is formula_4 and formula_5, then we see that the \"convergent case\" requires\nwhereas the \"divergent case\" requires\n\nIn words, the \"convergent case\" occurs when the demand curve is more elastic than the supply curve, at the equilibrium point. The \"divergent case\" occurs when the supply curve is more elastic than the demand curve, at the equilibrium point (see Kaldor, 1934, page 135, propositions (i) and (ii).)\n\nOne reason to be skeptical of this model's predictions is that it assumes producers are extremely shortsighted. Assuming that farmers look back at the most recent prices in order to forecast future prices might seem very reasonable, but this backward-looking forecasting (which is called adaptive expectations) turns out to be crucial for the model's fluctuations. When farmers expect high prices to continue, they produce too much and therefore end up with low prices, and vice versa.\n\nIn the stable case, this may not be an unbelievable outcome, since the farmers' prediction errors (the difference between the price they expect and the price that actually occurs) become smaller every period. In this case, after several periods prices and quantities will come close to the point where supply and demand cross, and predicted prices will be very close to actual prices. But in the unstable case, the farmers' errors get \"larger\" every period. This seems to indicate that adaptive expectations is a misleading assumption—how could farmers fail to notice that last period's price is \"not\" a good predictor of this period's price?\n\nThe fact that agents with adaptive expectations may make ever-increasing errors over time has led many economists to conclude that it is better to assume rational expectations, that is, expectations consistent with the actual structure of the economy. However, the rational expectations assumption is controversial since it may exaggerate agents' understanding of the economy. The cobweb model serves as one of the best examples to illustrate why understanding expectation formation is so important for understanding economic dynamics, and also why expectations are so controversial in recent economic theory.\n\nThe German concepts which translate literally \"adjustment to lower\" and \"screw to lower\" are known from the works of Hans-Peter Martin and Harald Schumann, the authors of \"The Global Trap\". Martin and Schumann see the process to worsened living standards as screw-shaped. Mordecai Ezekiel's \"The Cobweb Theorem\" (1938) illustrate a screw-shaped expectations-driven process. Eino Haikala has analyzed Ezekiel's work among others, and clarified that time constitutes the axis of the screw-shape. Thus Martin and Schumann point out that the cobweb theorem works to worsen standards of living as well. The idea of expectations-variation and thus modeled and induced expectations is shown clearly in Oskar Morgenstern's \"Vollkommene Voraussicht und Wirtschaftliches Gleichgewicht\". This article shows also that the concept of perfect foresight (vollkommene Voraussicht) is not a Robert E. Lucas or rational expectations invention but rests in game theory, Morgenstern and John von Neumann being the authors of \"Theory of Games and Economic Behavior\". This does not mean that the rational expectations hypothesis (REH) is not game theory or separate from the cobweb theorem, but vice versa. The \"there must be\" a random component claim by Alan A. Walters alone shows that rational (consistent) expectations is game theory, since the component is there to create an illusion of random walk.\n\nAlan A. Walters (1971) also claims that \"extrapolators\" are \"unsophisticated\", thus differentiating between prediction and forecasting. Using induced modeled expectations is prediction, not forecasting, unless these expectations are based on extrapolation. A prediction does not have to even try to be true. To avoid a prediction to be falsified it has to be, according to Franco Modigliani and Emile Grunberg's article \"The Predictability of Social Events\", kept private. Thus public prediction serves private one in REH. Haikala (1956) claims that cobweb theorem is a theorem of deceiving farmers, thus seeing cobweb theorem as a kind of rational or rather, consistent, expectations model with a game-theoretic feature. This makes sense when considering the argument of Hans-Peter Martin and Harald Schumann. The truth-value of a prediction is one measure in differentiating between non-deceiving and deceiving models. In Martin and Schumann's context, a claim that anti-Keynesian policies lead to a greater welfare of the majority of mankind should be analyzed in terms of truth. One way to do this is to investigate past historical data. This is contrary to the principles of REH, where the measure of policies is an economic model, not reality, and credibility, not truth. The importance of intellectual climate emphasized in Friedmans' work means that the credibility of a prediction can be increased by manipulating public opinion, despite its lack of truth. Morgenstern (1935) states that when varying expectations, the expectation of future has always to be positive (and prediction has to be credible).\n\nExpectation is a dynamic component in both REH and cobweb theorem, and the question of expectation formation is the key to Hans-Peter Martin's and Harald Schumann's argument, which deals with trading current welfare for expected future welfare with actually worsening policies in the middle. This 'in order to achieve that then we have to do this now' is the key in Bertrand de Jouvenel's work. Cobweb theorem and the rational (consistent) expectations hypothesis are part of welfare economics which according to Martin and Schumann's argument act now to worsen the welfare of the majority of mankind. Nicholas Kaldor's work \"The Scourge of Monetarism\" is an analysis of how the policies described by Martin and Schumann came to the United Kingdom.\n\nThe cobweb model has been interpreted as an explanation of fluctuations in various livestock markets, like those documented by Arthur Hanau in German hog markets; see Pork cycle. However, Rosen et al. (1994) proposed an alternative model which showed that because of the three-year life cycle of beef cattle, cattle populations would fluctuate over time even if ranchers had perfectly rational expectations.\n\nIn 1989, Wellford conducted twelve experimental sessions each conducted with five participants over thirty periods simulating the stable and unstable cases. Her results show that the unstable case did not result in the divergent behavior we see with cobweb expectations but rather the participants converged toward the rational expectations equilibrium. However, the price path variance in the unstable case was greater than that in the stable case (and the difference was shown to be statistically significant).\n\nOne way of interpreting these results is to say that in the long run, the participants behaved as if they had rational expectations, but that in the short run they made mistakes. These mistakes caused larger fluctuations in the unstable case than in the stable case.\n\nThe residential construction sector of Israel was, primarily as a result of waves of immigration, and still is, a principal factor in the structure of the business cycles in Israel. The increasing population, financing methods, higher income, and investment needs converged and came to be reflected through the skyrocketing demand for housing. On the other hand, technology, private and public entrepreneurship, the housing inventory and the availability of workforce have converged on the supply side. The position and direction of the housing sector in the business cycle can be identified by using a cobweb model (see Tamari, 1981).\n\n\n"}
{"id": "3586156", "url": "https://en.wikipedia.org/wiki?curid=3586156", "title": "Commensurability (ethics)", "text": "Commensurability (ethics)\n\nIn ethics, two values (or norms, reasons, or goods) are incommensurable (or incommensurate, or incomparable) when they do not share a common standard of measurement or cannot be compared to each other in a certain way.\n\nThere is a cluster of related ideas, and many philosophers use the terms differently. On one common usage: \nThis page is concerned almost entirely with the second phenomenon. For clarity, the term 'incomparable' is used.\n\nIn terminology due to Ruth Chang, the three \"trichotomous comparisons\" are betterness, worseness, and equal goodness. For example, one artist, drawing, or cup of coffee might be better or worse than another, or precisely equally as good as it.\n\nWhen two items are incomparable, none of the trichotomous comparisons holds between them (or at least it seems that way).\n\nThe clearest way of arguing that two options are incomparable is a small improvement argument.\n\nThe purpose of such examples is to show that none of the trichotomous comparisons apply. Here is an example. Suppose that (for you, taking everything into account), a certain job as a professor and a certain job as a banker are such that neither seems better than the other. The professor job offers more freedom and security, and the banking job offers more money and excitement. But we might say that though they are good in different ways, they are just too different to be compared with one of the trichotomous comparisons.\n\nLet's suppose that this means that the banking job is not better, and the professor job is not better. This seems to rule out two of the three trichotomous comparisons.\n\nBut what about the third? Might the jobs be exactly equally good? The small improvement argument is supposed to show that they could not. Suppose for the sake of argument that they \"are\" precisely equally good.\n\nSuppose also that in order to tempt you, the bankers offer you a tiny pay rise, perhaps 5 cents a year. This new banking job (often called 'banking+') is clearly better than banking, albeit only by a tiny amount. You could (under normal circumstances) never rationally choose banking over banking+: they are the same in every way, except that the latter pays more.\n\nHere is the crux of the small improvement argument: if banking is exactly equally as good as professor, and banking+ is better than banking, then banking+ must be better than professor. But this seems very implausible: if banking and professor were so different that we could not say that professor is better, and we could not say that banking is better, then how could adding 5 cents a year to a huge salary make the difference?\n\nThis seems to show that one of our assumptions was incorrect. Defenders of incomparability will say it is most plausible that the assumption that banking and philosophy are equally good that is incorrect. So they conclude that this assumption as false, and thus that none of the trichotomous comparisons apply.\n\nThere are four main philosophical accounts of incommensurability/incomparability. Their task is to explain (or explain away) the phenomenon, and the small improvement argument. Some philosophers are pluralists about the phenomenon: they think that (for example) genuine incomparability might be the correct account in some cases, and parity in others.\n\nOne way to understand the difference between the theories is to see how they respond to the small-improvement argument.\n\nOne possibility is that this is all a mistake: that there is no genuine incomparability, and when it seems like none of the three trichotomous comparisons apply, in fact one of them does but we do not know which. This is where the small-improvement argument goes wrong: one of the trichotomous comparisons \"does\" apply between banking and philosophy.\n\nAccording to this view, apparent incomparability is merely ignorance. An advantage of this account is that the various puzzles surrounding incomparability dissolve rather quickly. Choice between incomparable options is no more than choice between options when we do not know which is better.\n\nThe main objection to this kind of view is that it seems very implausible, for similar reasons to epistemicism about vagueness. In particular, it is hard to see how we could be ignorant of the kinds of facts involved in incomparability.\n\nJoseph Raz has argued that in cases of incomparability, no comparison applies. Neither option is better, and they are not equally good.\n\nOn this view, the small-improvement argument is sound.\n\nRuth Chang has argued that (at least some of the time), options may be comparable even if they are not trichotomously comparable. She does this by denying that the three trichotomous comparisons are the only ones on offer. She defends the existence of a fourth comparison, which she calls 'parity'. Luke Elson has criticised this argument, claiming that the apparent possibility of parity is really an artefact of the vagueness of the (trichotomous) comparisons involved.\n\nFinally, a set of philosophers led by John Broome has argued that incomparability is vagueness. This theory says that it is vague or indeterminate which trichotomous comparison applies.\n\nThe argument for this position is complex, and how 'incomparability as vagueness' is to be understood depends on one's theory of vagueness. But the main idea behind the theory is fairly simple. What is the \"precise\" minimum number of grains of sand needed to count as a heap, or hairs required to count as non-bald? If there is no precise number, only a rough range, then these are instances of vagueness. On one set of theories of vagueness, it is \"indeterminate\" how many heaps or hairs are required. Perhaps our language simply does not specify a sharp boundary.\n\nIn the small-improvement argument, the incomparability as vagueness view might say that it is indeterminate whether banking is better or worse than philosophy, or precisely equally good.\n\nOne taxonomic complication is distinguishing the view that incomparability is vagueness, combined with epistemicism about vagueness, from epistemicism about incomparability.\n\nIncomparability has figured prominently in several philosophical debates concerning moral and rational action. In general, incomparability can add complications to any view according to which one ought to do the best thing that one can, or the better of two options. If the options are incomparable, it may be that neither is better. (Depending on which view of incomparability is true.)\n\nConsequentialists think that the morally right thing to do is what promotes the most overall good. But if two actions produce incomparable outcomes, it may be that neither is better.\n\nThe topic of incommensurability has also frequently arisen in discussions of the version of natural law theory associated with John Finnis and others.\n\nJoseph Raz has argued that incommensurability undermines the 'rationalistic' view of human action according to which distinctively rational action is doing what one has most reason to do.\n\nPhilosophical reflection about practical reason typically aims for a description of the principles relevant in answering the question, \"What is to be done in this or that circumstance?\" On one popular view, answers to this question can be found by comparing the relative strengths of the various values or norms in play in some given situation. For example, if one is trying to decide on some nice afternoon whether they should stay in to do work or go for a walk, on this view of practical reason they will compare the merits of these two options. If going for a walk is the better or more reasonable course of action, they should put aside their books and go for a stroll. The topic of incommensurability—and the topic of incomparability in particular—is especially important to those who advocate this view of practical reason. For if one's options in certain circumstances are of incomparable value, he or she cannot settle the question of what to do by choosing the better option. When the competing options are incomparable, then by definition neither is better than the other.\n\n\n"}
{"id": "34865455", "url": "https://en.wikipedia.org/wiki?curid=34865455", "title": "Coxeter complex", "text": "Coxeter complex\n\nIn mathematics, the Coxeter complex, named after H. S. M. Coxeter, is a geometrical structure (a simplicial complex) associated to a Coxeter group. Coxeter complexes are the basic objects that allow the construction of buildings; they form the apartments of a building.\n\nThe first ingredient in the construction of the Coxeter complex associated to a Coxeter group \"W\" is a certain representation of \"W\", called the canonical representation of \"W\".\n\nLet formula_1 be a Coxeter system associated to \"W\", with Coxeter matrix formula_2. The canonical representation is given by a vector space \"V\" with basis of formal symbols formula_3, which is equipped with the symmetric bilinear form formula_4. The action of \"W\" on this vector space \"V\" is then given by formula_5, as motivated by the expression for reflections in root systems.\n\nThis representation has several foundational properties in the theory of Coxeter groups; for instance, the bilinear form \"B\" is positive definite if and only if \"W\" is finite. It is (always) a faithful representation of \"W\".\n\nOne can think of this representation as expressing \"W\" as some sort of reflection group, with the caveat that \"B\" might not be positive definite. It becomes important then to distinguish the representation \"V\" from its dual \"V\". The vectors formula_6 lie in \"V\", and have corresponding dual vectors formula_7 in \"V\", given by:\n\nwhere the angled brackets indicate the natural pairing of a dual vector in \"V\" with a vector of \"V\", and \"B\" is the bilinear form as above.\n\nNow \"W\" acts on \"V\", and the action satisfies the formula\n\nfor formula_10 and any \"f\" in \"V\". This expresses \"s\" as a reflection in the hyperplane formula_11. One has the fundamental chamber formula_12, this has faces the so-called walls, formula_13. The other chambers can be obtained from formula_14 by translation: they are the formula_15 for formula_16.\n\nGiven a fundamental chamber formula_17, the Tits cone is defined to be formula_18. This need not be the whole of \"V\". Of major importance is the fact that the Tits cone \"X\" is convex. The action of \"W\" on the Tits cone \"X\" has fundamental domain the fundamental chamber formula_17.\n\nOnce one has defined the Tits cone \"X\", the Coxeter complex formula_20 of \"W\" with respect to \"S\" can be defined as the quotient of \"X\", with the origin removed, by the positive reals (ℝ, ×):\n\nThe dihedral groups formula_22 (of order 2\"n\") are Coxeter groups, of corresponding type formula_23. These have the presentation formula_24.\n\nThe canonical linear representation of formula_23 is the usual reflection representation of the dihedral group, as acting on a \"n\"-gon in the plane (so formula_26 in this case). For instance, in the case \"n\" = 3, we get the Coxeter group of type formula_27, acting on an equilateral triangle in the plane. Each reflection \"s\" has an associated hyperplane \"H\" in the dual vector space (which can be canonically identified with the vector space itself using the bilinear form \"B\", which is an inner product in this case as remarked above), these are the walls. They cut out chambers, as seen below:\n\nThe Coxeter complex is then the corresponding 2\"n\"-gon, as in the image above. This is a simplicial complex of dimension 1, and it can be colored by cotype.\n\nAnother motivating example is the infinite dihedral group formula_28. This can be seen as the group of symmetries of the real line that preserves the set of points with integer coordinates; it is generated by the reflections in formula_29 and formula_30. This group has the Coxeter presentation formula_31.\n\nIn this case, it is no longer possible to identify \"V\" with the dual space \"V\", as \"B\" is not positive definite. It is then better to work solely with \"V\", which is where the hyperplanes are defined. This then gives the following picture:\n\nIn this case, the Tits cone is not the whole plane, but only the upper half plane. Quotienting out by the positive reals then yields another copy of the real line, with marked points at the integers. This is the Coxeter complex of the infinite dihedral group.\n\nAnother description of the Coxeter complex uses standard cosets of the Coxeter group \"W\". A standard coset is a coset of the form formula_32, where formula_33 for some subset \"J\" of \"S\". For instance, formula_34 and formula_35. \n\nThe Coxeter complex formula_36 is then the poset of standard cosets, ordered by reverse inclusion. This has a canonical structure of a simplicial complex, as do all posets that satisfy:\n\nThe Coxeter complex associated to formula_1 has dimension formula_39. It is homeomorphic to a formula_40-sphere if \"W\" is finite and is contractible if \"W\" is infinite.\n\n\n"}
{"id": "5785", "url": "https://en.wikipedia.org/wiki?curid=5785", "title": "Crime", "text": "Crime\n\nIn ordinary language, a crime is an unlawful act punishable by a state or other authority. The term \"crime\" does not, in modern criminal law, have any simple and universally accepted definition, though statutory definitions have been provided for certain purposes. The most popular view is that crime is a category created by law; in other words, something is a crime if declared as such by the relevant and applicable law. One proposed definition is that a crime or offence (or criminal offence) is an act harmful not only to some individual but also to a community, society or the state (\"a public wrong\"). Such acts are forbidden and punishable by law.\n\nThe notion that acts such as murder, rape and theft are to be prohibited exists worldwide. What precisely is a criminal offence is defined by criminal law of each country. While many have a catalogue of crimes called the criminal code, in some common law countries no such comprehensive statute exists.\n\nThe state (government) has the power to severely restrict one's liberty for committing a crime. In modern societies, there are procedures to which investigations and trials must adhere. If found guilty, an offender may be sentenced to a form of reparation such as a community sentence, or, depending on the nature of their offence, to undergo imprisonment, life imprisonment or, in some jurisdictions, execution.\n\nUsually, to be classified as a crime, the \"act of doing something criminal\" (\"actus reus\") mustwith certain exceptionsbe accompanied by the \"intention to do something criminal\" (\"mens rea\").\n\nWhile every crime violates the law, not every violation of the law counts as a crime. Breaches of private law (torts and breaches of contract) are not automatically punished by the state, but can be enforced through civil procedure.\n\nWhen informal relationships and sanctions prove insufficient to establish and maintain a desired social order, a government or a state may impose more formalized or stricter systems of social control. With institutional and legal machinery at their disposal, agents of the State can compel populations to conform to codes and can opt to punish or attempt to reform those who do not conform.\n\nAuthorities employ various mechanisms to regulate (encouraging or discouraging) certain behaviors in general. Governing or administering agencies may for example codify rules into laws, police citizens and visitors to ensure that they comply with those laws, and implement other policies and practices that legislators or administrators have prescribed with the aim of discouraging or preventing crime. In addition, authorities provide remedies and sanctions, and collectively these constitute a criminal justice system. Legal sanctions vary widely in their severity; they may include (for example) incarceration of temporary character aimed at reforming the convict. Some jurisdictions have penal codes written to inflict permanent harsh punishments: legal mutilation, capital punishment or life without parole.\n\nUsually, a natural person perpetrates a crime, but legal persons may also commit crimes. Conversely, at least under U.S. law, nonpersons such as animals cannot commit crimes.\n\nThe sociologist Richard Quinney has written about the relationship between society and crime. When Quinney states \"crime is a social phenomenon\" he envisages both how individuals conceive crime and how populations perceive it, based on societal norms.\n\nThe word \"crime\" is derived from the Latin root \"cernō\", meaning \"I decide, I give judgment\". Originally the Latin word \"crīmen\" meant \"charge\" or \"cry of distress.\" The Ancient Greek word \"krima\" (κρίμα), from which the Latin cognate derives, typically referred to an intellectual mistake or an offense against the community, rather than a private or moral wrong.\n\nIn 13th century English \"crime\" meant \"sinfulness\", according to etymonline.com. It was probably brought to England as Old French \"crimne\" (12th century form of Modern French \"crime\"), from Latin \"crimen\" (in the genitive case: \"criminis\"). In Latin, \"crimen\" could have signified any one of the following: \"charge, indictment, accusation; crime, fault, offense\".\n\nThe word may derive from the Latin \"cernere\" – \"to decide, to sift\" (see crisis, mapped on Kairos and Chronos). But Ernest Klein (citing Karl Brugmann) rejects this and suggests *cri-men, which originally would have meant \"cry of distress\". Thomas G. Tucker suggests a root in \"cry\" words and refers to English plaint, plaintiff, and so on. The meaning \"offense punishable by law\" dates from the late 14th century. The Latin word is glossed in Old English by \"facen\", also \"deceit, fraud, treachery\", [cf. fake]. \"Crime wave\" is first attested in 1893 in American English.\n\nWhether a given act or omission constitutes a crime does not depend on the nature of that act or omission. It depends on the nature of the legal consequences that may follow it. An act or omission is a crime if it is capable of being followed by what are called criminal proceedings.\n\nHistory\n\nThe following definition of \"crime\" was provided by the Prevention of Crimes Act 1871, and applied for the purposes of section 10 of the Prevention of Crime Act 1908:\n\nFor the purpose of section 243 of the Trade Union and Labour Relations (Consolidation) Act 1992, a crime means an offence punishable on indictment, or an offence punishable on summary conviction, and for the commission of which the offender is liable under the statute making the offence punishable to be imprisoned either absolutely or at the discretion of the court as an alternative for some other punishment.\n\nA normative definition views crime as deviant behavior that violates prevailing normscultural standards prescribing how humans ought to behave normally. This approach considers the complex realities surrounding the concept of crime and seeks to understand how changing social, political, psychological, and economic conditions may affect changing definitions of crime and the form of the legal, law-enforcement, and penal responses made by society.\n\nThese structural realities remain fluid and often contentious. For example: as cultures change and the political environment shifts, societies may criminalise or decriminalise certain behaviours, which directly affects the statistical crime rates, influence the allocation of resources for the enforcement of laws, and (re-)influence the general public opinion.\n\nSimilarly, changes in the collection and/or calculation of data on crime may affect the public perceptions of the extent of any given \"crime problem\". All such adjustments to crime statistics, allied with the experience of people in their everyday lives, shape attitudes on the extent to which the State should use law or social engineering to enforce or encourage any particular social norm. Behaviour can be controlled and influenced by a society in many ways without having to resort to the criminal justice system.\n\nIndeed, in those cases where no clear consensus exists on a given norm, the drafting of criminal law by the group in power to prohibit the behaviour of another group may seem to some observers an improper limitation of the second group's freedom, and the ordinary members of society have less respect for the law or laws in generalwhether the authorities actually enforce the disputed law or not.\n\nLegislatures can pass laws (called \"mala prohibita\") that define crimes against social norms. These laws vary from time to time and from place to place: note variations in gambling laws, for example, and the prohibition or encouragement of duelling in history. Other crimes, called \"mala in se\", count as outlawed in almost all societies, (murder, theft and rape, for example).\n\nEnglish criminal law and the related criminal law of Commonwealth countries can define offences that the courts alone have developed over the years, without any actual legislation: common law offences. The courts used the concept of \"malum in se\" to develop various common law offences.\n\nOne can view criminalization as a procedure deployed by society as a preemptive harm-reduction device, using the threat of punishment as a deterrent to anyone proposing to engage in the behavior causing harm. The State becomes involved because governing entities can become convinced that the costs of not criminalizing (through allowing the harms to continue unabated) outweigh the costs of criminalizing it (restricting individual liberty, for example, to minimize harm to others).\n\nStates control the process of criminalization because:\n\nThe label of \"crime\" and the accompanying social stigma normally confine their scope to those activities seen as injurious to the general population or to the State, including some that cause serious loss or damage to individuals. Those who apply the labels of \"crime\" or \"criminal\" intend to assert the hegemony of a dominant population, or to reflect a consensus of condemnation for the identified behavior and to justify any punishments prescribed by the State (in the event that standard processing tries and convicts an accused person of a crime).\n\nJustifying the State's use of force to coerce compliance with its laws has proven a consistent theoretical problem. One of the earliest justifications involved the theory of natural law. This posits that the nature of the world or of human beings underlies the standards of morality or constructs them. Thomas Aquinas wrote in the 13th century: \"the rule and measure of human acts is the reason, which is the first principle of human acts\" (Aquinas, ST I-II, Q.90, A.I). He regarded people as by nature rational beings, concluding that it becomes morally appropriate that they should behave in a way that conforms to their rational nature. Thus, to be valid, any law must conform to natural law and coercing people to conform to that law is morally acceptable. In the 1760s William Blackstone (1979: 41) described the thesis:\n\nBut John Austin (1790–1859), an early positivist, applied utilitarianism in accepting the calculating nature of human beings and the existence of an objective morality. He denied that the legal validity of a norm depends on whether its content conforms to morality. Thus in Austinian terms a moral code can objectively determine what people ought to do, the law can embody whatever norms the legislature decrees to achieve social utility, but every individual remains free to choose what to do. Similarly, Hart (1961) saw the law as an aspect of sovereignty, with lawmakers able to adopt any law as a means to a moral end.\n\nThus the necessary and sufficient conditions for the truth of a proposition of law simply involved internal logic and consistency, and that the state's agents used state power with responsibility. Ronald Dworkin (2005) rejects Hart's theory and proposes that all individuals should expect the equal respect and concern of those who govern them as a fundamental political right. He offers a theory of compliance overlaid by a theory of deference (the citizen's duty to obey the law) and a theory of enforcement, which identifies the legitimate goals of enforcement and punishment. Legislation must conform to a theory of legitimacy, which describes the circumstances under which a particular person or group is entitled to make law, and a theory of legislative justice, which describes the law they are entitled or obliged to make.\n\nIndeed, despite everything, the majority of natural-law theorists have accepted the idea of enforcing the prevailing morality as a primary function of the law. This view entails the problem that it makes any moral criticism of the law impossible: if conformity with natural law forms a necessary condition for legal validity, all valid law must, by definition, count as morally just. Thus, on this line of reasoning, the legal validity of a norm necessarily entails its moral justice.\n\nOne can solve this problem by granting some degree of moral relativism and accepting that norms may evolve over time and, therefore, one can criticize the continued enforcement of old laws in the light of the current norms. People may find such law acceptable, but the use of State power to coerce citizens to comply with that law lacks moral justification. More recent conceptions of the theory characterise crime as the violation of individual rights.\n\nSince society considers so many rights as natural (hence the term \"right\") rather than man-made, what constitutes a crime also counts as natural, in contrast to laws (seen as man-made). Adam Smith illustrates this view, saying that a smuggler would be an excellent citizen, \"...had not the laws of his country made that a crime which nature never meant to be so.\"\n\nNatural-law theory therefore distinguishes between \"criminality\" (which derives from human nature) and \"illegality\" (which originates with the interests of those in power). Lawyers sometimes express the two concepts with the phrases \"malum in se\" and \"malum prohibitum\" respectively. They regard a \"crime \"malum in se\"\" as inherently criminal; whereas a \"crime \"malum prohibitum\"\" (the argument goes) counts as criminal only because the law has decreed it so.\n\nIt follows from this view that one can perform an illegal act without committing a crime, while a criminal act could be perfectly legal. Many Enlightenment thinkers (such as Adam Smith and the American Founding Fathers) subscribed to this view to some extent, and it remains influential among so-called classical liberals and libertarians.\n\nSome religious communities regard sin as a crime; some may even highlight the crime of sin very early in legendary or mythological accounts of originsnote the tale of Adam and Eve and the theory of original sin. What one group considers a crime may cause or ignite war or conflict. However, the earliest known civilizations had codes of law, containing both civil and penal rules mixed together, though not always in recorded form.\n\nThe Sumerians produced the earliest surviving written codes. Urukagina (reigned , short chronology) had an early code that has not survived; a later king, Ur-Nammu, left the earliest extant written law system, the Code of Ur-Nammu (), which prescribed a formal system of penalties for specific cases in 57 articles. The Sumerians later issued other codes, including the \"code of Lipit-Ishtar\". This code, from the 20th century BCE, contains some fifty articles, and scholars have reconstructed it by comparing several sources. \n\nSuccessive legal codes in Babylon, including the code of Hammurabi (c. 1790 BC), reflected Mesopotamian society's belief that law derived from the will of the gods (see Babylonian law).\nMany states at this time functioned as theocracies, with codes of conduct largely religious in origin or reference. In the Sanskrit texts of Dharmaśāstra (), issues such as legal and religious duties, code of conduct, penalties and remedies, etc. have been discussed and forms one of the elaborate and earliest source of legal code.\n\nSir Henry Maine (1861) studied the ancient codes available in his day, and failed to find any criminal law in the \"modern\" sense of the word. While modern systems distinguish between offences against the \"State\" or \"community\", and offences against the \"individual\", the so-called penal law of ancient communities did not deal with \"crimes\" (Latin: \"crimina\"), but with \"wrongs\" (Latin: \"delicta\"). Thus the Hellenic laws treated all forms of theft, assault, rape, and murder as private wrongs, and left action for enforcement up to the victims or their survivors. The earliest systems seem to have lacked formal courts.\n\nThe Romans systematized law and applied their system across the Roman Empire. Again, the initial rules of Roman law regarded assaults as a matter of private compensation. The most significant Roman law concept involved \"dominion\". The \"pater familias\" owned all the family and its property (including slaves); the \"pater\" enforced matters involving interference with any property. The \"Commentaries\" of Gaius (written between 130 and 180 AD) on the Twelve Tables treated \"furtum\" (in modern parlance: \"theft\") as a tort.\n\nSimilarly, assault and violent robbery involved trespass as to the \"pater's\" property (so, for example, the rape of a slave could become the subject of compensation to the \"pater\" as having trespassed on his \"property\"), and breach of such laws created a \"vinculum juris\" (an obligation of law) that only the payment of monetary compensation (modern \"damages\") could discharge. Similarly, the consolidated Teutonic laws of the Germanic tribes, included a complex system of monetary compensations for what courts would consider the complete range of criminal offences against the person, from murder down.\n\nEven though Rome abandoned its Britannic provinces around 400 AD, the Germanic mercenarieswho had largely become instrumental in enforcing Roman rule in Britanniaacquired ownership of land there and continued to use a mixture of Roman and Teutonic Law, with much written down under the early Anglo-Saxon kings. But only when a more centralized English monarchy emerged following the Norman invasion, and when the kings of England attempted to assert power over the land and its peoples, did the modern concept emerge, namely of a crime not only as an offence against the \"individual\", but also as a wrong against the \"State\".\n\nThis idea came from common law, and the earliest conception of a criminal act involved events of such major significance that the \"State\" had to usurp the usual functions of the civil tribunals, and direct a special law or \"privilegium\" against the perpetrator. All the earliest English criminal trials involved wholly extraordinary and arbitrary courts without any settled law to apply, whereas the civil (delictual) law operated in a highly developed and consistent manner (except where a king wanted to raise money by selling a new form of writ). The development of the idea that the \"State\" dispenses justice in a court only emerges in parallel with or after the emergence of the concept of sovereignty.\n\nIn continental Europe, Roman law persisted, but with a stronger influence from the Christian Church.\nCoupled with the more diffuse political structure based on smaller feudal units, various legal traditions emerged, remaining more strongly rooted in Roman jurisprudence, but modified to meet the prevailing political climate.\n\nIn Scandinavia the effect of Roman law did not become apparent until the 17th century, and the courts grew out of the \"things\"the assemblies of the people. The people decided the cases (usually with largest freeholders dominating). This system later gradually developed into a system with a royal judge nominating a number of the most esteemed men of the parish as his board, fulfilling the function of \"the people\" of yore.\n\nFrom the Hellenic system onwards, the policy rationale for requiring the payment of monetary compensation for wrongs committed has involved the avoidance of feuding between clans and families.\nIf compensation could mollify families' feelings, this would help to keep the peace. On the other hand, the institution of oaths also played down the threat of feudal warfare. Both in archaic Greece and in medieval Scandinavia, an accused person walked free if he could get a sufficient number of male relatives to swear him not guilty. (Compare the United Nations Security Council, in which the veto power of the permanent members ensures that the organization does not become involved in crises where it could not enforce its decisions.)\n\nThese means of restraining private feuds did not always work, and sometimes prevented the fulfillment of justice. But in the earliest times the \"state\" did not always provide an independent policing force. Thus criminal law grew out of what 21st-century lawyers would call torts; and, in real terms, many acts and omissions classified as crimes actually overlap with civil-law concepts.\n\nThe development of sociological thought from the 19th century onwards prompted some fresh views on crime and criminality, and fostered the beginnings of criminology as a study of crime in society. Nietzsche noted a link between crime and creativityin \"The Birth of Tragedy\" he asserted: \"The best and brightest that man can acquire he must obtain by crime\". In the 20th century Michel Foucault in \"Discipline and Punish\" made a study of criminalization as a coercive method of state control.\n\nThe following classes of offences are used, or have been used, as legal terms of art:\n\nResearchers and commentators have classified crimes into the following categories, in addition to those above:\n\nOne can categorise crimes depending on the related punishment, with sentencing tariffs prescribed in line with the perceived seriousness of the offence. Thus fines and noncustodial sentences may address the crimes seen as least serious, with lengthy imprisonment or (in some jurisdictions) capital punishment reserved for the most serious.\n\nUnder the common law of England, crimes were classified as either treason, felony or misdemeanour, with treason sometimes being included with the felonies. This system was based on the perceived seriousness of the offence. It is still used in the United States but the distinction between felony and misdemeanour is abolished in England and Wales and Northern Ireland.\n\nThe following classes of offence are based on mode of trial:\n\nIn common law countries, crimes may be categorised into common law offences and statutory offences. In the US, Australia and Canada (in particular), they are divided into federal crimes and under state crimes.\n\n\nIn the United States since 1930, the FBI has tabulated Uniform Crime Reports (UCR) annually from crime data submitted by law enforcement agencies across the United States.\nOfficials compile this data at the city, county, and state levels into the UCR. They classify violations of laws based on common law as Part I (index) crimes in UCR data. These are further categorized as violent or property crimes. Part I violent crimes include murder and criminal homicide (voluntary manslaughter), forcible rape, aggravated assault, and robbery; while Part I property crimes include burglary, arson, larceny/theft, and motor-vehicle theft. All other crimes count come under Part II.\n\nFor convenience, such lists usually include infractions although, in the U.S., they may come into the sphere not of the criminal law, but rather of the civil law. Compare tortfeasance.\n\nBooking arrests require detention for a time-frame ranging 1 to 24 hours.\n\nThere are several national and International organizations offering studies and statistics about global and local crime activity, such as United Nations Office on Drugs and Crime, the United States of America Overseas Security Advisory Council (OSAC) safety report or national reports generated by the law-enforcement authorities of EU state member reported to the Europol.\n\nIn England and Wales, as well as in Hong Kong, the term \"offence\" means the same thing as, and is interchangeable with, the term \"crime\", They are further split into:\n\nMany different causes and correlates of crime have been proposed with varying degree of empirical support. They include socioeconomic, psychological, biological, and behavioral factors. Controversial topics include media violence research and effects of gun politics.\n\nEmotional state (both chronic and current) have a tremendous impact on individual thought processes and, as a result, can be linked to criminal activities. The positive psychology concept of Broaden and Build posits that cognitive functioning expands when an individual is in a good-feeling emotional state and contracts as emotional state declines. In positive emotional states an individual is able to consider more possible solutions to problems, but in lower emotional states fewer solutions can be ascertained. The narrowed thought-action repertoires can result in the only paths perceptible to an individual being ones they would never use if they saw an alternative, but if they can't conceive of the alternatives that carry less risk they will choose one that they can see. Criminals who commit even the most horrendous of crimes, such as mass murders, did not see another solution.\n\nCrimes defined by treaty as crimes against international law include:\n\nFrom the point of view of state-centric law, extraordinary procedures (usually international courts) may prosecute such crimes. Note the role of the International Criminal Court at The Hague in the Netherlands.\n\nDifferent religious traditions may promote distinct norms of behaviour, and these in turn may clash or harmonise with the perceived interests of a state. Socially accepted or imposed religious morality has influenced secular jurisdictions on issues that may otherwise concern only an individual's conscience. Activities sometimes criminalized on religious grounds include (for example) alcohol consumption (prohibition), abortion and stem-cell research. In various historical and present-day societies, institutionalized religions have established systems of earthly justice that punish crimes against the divine will and against specific devotional, organizational and other rules under specific codes, such as Roman Catholic canon law.\n\nIn the military sphere, authorities can prosecute both regular crimes and specific acts (such as mutiny or desertion) under martial-law codes that either supplant or extend civil codes in times of (for example) war.\n\nMany constitutions contain provisions to curtail freedoms and criminalize otherwise tolerated behaviors under a state of emergency in the event of war, natural disaster or civil unrest. Undesired activities at such times may include assembly in the streets, violation of curfew, or possession of firearms.\n\nTwo common types of employee crime exist: embezzlement and wage theft.\n\nThe complexity and anonymity of computer systems may help criminal employees camouflage their operations. The victims of the most costly scams include banks, brokerage houses, insurance companies, and other large financial institutions.\n\nIn the United States, it is estimated that workers are not paid at least $19 billion every year in overtime and that in total $40 billion to $60 billion are lost annually due to all forms of wage theft. This compares to national annual losses of $340 million due to robbery, $4.1 billion due to burglary, $5.3 billion due to larceny, and $3.8 billion due to auto theft in 2012. In Singapore, as in the United States, wage theft was found to be widespread and severe. In a 2014 survey it was found that as many as one-third of low wage male foreign workers in Singapore, or about 130,000, were affected by wage theft from partial to full denial of pay.\n\n\n"}
{"id": "26998547", "url": "https://en.wikipedia.org/wiki?curid=26998547", "title": "Degrees of freedom (physics and chemistry)", "text": "Degrees of freedom (physics and chemistry)\n\nIn physics, a degree of freedom is an independent physical parameter in the formal description of the state of a physical system. The set of all states of a system is known as the system's phase space, and degrees of freedom of the system, are the dimensions of the phase space.\n\nA degree of freedom of a physical system is an independent parameter that is necessary to characterize the state of a physical system. In general, a degree of freedom may be any useful property that is not dependent on other variables.\n\nThe location of a particle in three-dimensional space requires three position coordinates. Similarly, the direction and speed at which a particle moves can be described in terms of three velocity components, each in reference to the three dimensions of space. If the time evolution of the system is deterministic, where the state at one instant uniquely determines its past and future position and velocity as a function of time, such a system has six degrees of freedom. If the motion of the particle is constrained to a lower number of dimensions, for example, the particle must move along a wire or on a fixed surface, then the system has fewer than six degrees of freedom. On the other hand, a system with an extended object that can rotate or vibrate can have more than six degrees of freedom.\n\nIn classical mechanics, the state of a point particle at any given time is often described with position and velocity coordinates in the Lagrangian formalism, or with position and momentum coordinates in the Hamiltonian formalism.\n\nIn statistical mechanics, a degree of freedom is a single scalar number describing the microstate of a system. The specification of all microstates of a system is a point in the system's phase space.\n\nIn the 3D ideal chain model in chemistry, two angles are necessary to describe the orientation of each monomer.\n\nIt is often useful to specify quadratic degrees of freedom. These are degrees of freedom that contribute in a quadratic function to the energy of the system.\n\nIn three-dimensional space, three degrees of freedom are associated with the movement of a particle. A diatomic gas molecule has 6 degrees of freedom. This set may be decomposed in terms of translations, rotations, and vibrations of the molecule. The center of mass motion of the entire molecule accounts for 3 degrees of freedom. In addition, the molecule has two rotational degrees of motion and one vibrational mode. The rotations occur around the two axes perpendicular to the line between the two atoms. The rotation around the atom–atom bond is not a physical rotation. This yields, for a diatomic molecule, a decomposition of:\n\nFor a general, non-linear molecule, all 3 rotational degrees of freedom are considered, resulting in the decomposition:\n\nwhich means that an -atom molecule has vibrational degrees of freedom for . In special cases, such as adsorbed large molecules, the rotational degrees of freedom can be limited to only one.\n\nAs defined above one can also count degrees of freedom using the minimum number of coordinates required to specify a position. This is done as follows:\nLet's say one particle in this body has coordinate and the other has coordinate with unknown. Application of the formula for distance between two coordinates\nresults in one equation with one unknown, in which we can solve for .\nOne of , , , , , or can be unknown.\n\nContrary to the classical equipartition theorem, at room temperature, the vibrational motion of molecules typically makes negligible contributions to the heat capacity. This is because these degrees of freedom are \"frozen\" because the spacing between the energy eigenvalues exceeds the energy corresponding to ambient temperatures (). In the following table such degrees of freedom are disregarded because of their low effect on total energy. Then only the translational and rotational degrees of freedom contribute, in equal amounts, to the heat capacity ratio. This is why = for monatomic gases and = for diatomic gases at room temperature.\n\nHowever, at very high temperatures, on the order of the vibrational temperature (Θ), vibrational motion cannot be neglected.\n\nVibrational temperatures are between 10 K and 10 K.\n\nThe set of degrees of freedom of a system is independent if the energy associated with the set can be written in the following form:\n\nwhere is a function of the sole variable .\n\nexample: if and are two degrees of freedom, and is the associated energy:\n\nFor from 1 to , the value of the th degree of freedom is distributed according to the Boltzmann distribution. Its probability density function is the following:\n\nIn this section, and throughout the article the brackets formula_8 denote the mean of the quantity they enclose.\n\nThe internal energy of the system is the sum of the average energies associated with each of the degrees of freedom:\n\nA degree of freedom is quadratic if the energy terms associated with this degree of freedom can be written as\n\nwhere is a linear combination of other quadratic degrees of freedom.\n\nexample: if and are two degrees of freedom, and is the associated energy:\n\nFor example, in Newtonian mechanics, the dynamics of a system of quadratic degrees of freedom are controlled by a set of homogeneous linear differential equations with constant coefficients.\n\n are quadratic and independent degrees of freedom if the energy associated with a microstate of the system they represent can be written as:\n\nIn the classical limit of statistical mechanics, at thermodynamic equilibrium, the internal energy of a system of quadratic and independent degrees of freedom is:\n\nHere, the mean energy associated with a degree of freedom is:\n\nSince the degrees of freedom are independent, the internal energy of the system is equal to the sum of the mean energy associated with each degree of freedom, which demonstrates the result.\n\nThe description of a system's state as a point in its phase space, although mathematically convenient, is thought to be fundamentally inaccurate. In quantum mechanics, the motion degrees of freedom are superseded with the concept of wave function, and operators which correspond to other degrees of freedom have discrete spectra. For example, intrinsic angular momentum operator (which corresponds to the rotational freedom) for an electron or photon has only two eigenvalues. This discreteness becomes apparent when action has an order of magnitude of the Planck constant, and individual degrees of freedom can be distinguished.\n"}
{"id": "45194398", "url": "https://en.wikipedia.org/wiki?curid=45194398", "title": "Domain reduction algorithm", "text": "Domain reduction algorithm\n\nDomain reduction algorithms are algorithms used to reduce constraints and degrees of freedom in order to provide solutions for partial differential equations.\n"}
{"id": "619545", "url": "https://en.wikipedia.org/wiki?curid=619545", "title": "Eduardo Kac", "text": "Eduardo Kac\n\nEduardo Kac [ɛdwardoʊ kæts; ĕd·wâr′·dō kăts] (1962) is an American contemporary artist and professor whose artworks that span a wide range of practices, including performance art, poetry, holography, interactive art, telematic art and transgenic art. He is particularly well known for his works that integrate biotechnology, politics and aesthetics. \n\nKac began his art career in the early 1980s as performance artist in Rio De Janeiro, Brazil. Within a few years he was involved in exploring holography as an interactive art form. At the same time, he began creating animated poetic works on the French Minitel platform. \n\nBefore moving from Brazil to the United States in the late 1980s, Kac began creating his first telematic artworks that used electronic technologies to bridge two or more physical locations. During the 1990s he continued to produce these works, at the same time as he coined the phrase \"bioart\". In addition to \"bioart\" Kac coined numerous neologisms to describe his transdisciplinary art practice, including \"holoart\", \"transgenic art\" (the integration of human genes in an artwork) and \"plantimal\", referring to a plant infused with human genes. \n\nKac is perhaps best known for his transgenic artworks that use biotechnology to intervene on the natural genetic structure of plants and animals. His works \"Time Capsule\", \"GFP Bunny\" and \"Natural History of the Enigma\" in particular are recognized as notable works for having joined biotechnology, art and bioethics together into politically charged artworks. \n\nIn 2017 Kac collaborated with the French Space Observatory office to have a sculpture made aboard the international space station. \n\nA multidisciplinary artist, Kac has also employed poetry, fax, photocopying, photography, video, fractals, poetry, RFID implants, virtual reality, networks, robotics, satellites, telerobotics, Morse code and DNA extraction in his practice. \nKac was born July 3, 1962 in Rio de Janeiro, Brazil. He studied at the School of Communications of the Pontifical Catholic University of Rio de Janeiro, receiving a BA degree in 1985, and then at the School of the Art Institute of Chicago where he received an MFA degree in 1990. In 2003 he received a doctorate from the University of Wales, Great Britain. Kac is a professor of art at the School of the Art Institute of Chicago.\n\nBetween 1980 and 1982, Kac belonged to a group that did performance art in the Cinelandia area of Rio de Janeiro. The performances, a response to the extreme conservative political climate of Brazil under military dictatorship, became known as \" Movimento de Arte Pornô\" (\"Porn Art Movement\"). \n\nBeginning in 1983, Kac created holographic poetry works, the first of which was \"HOLO/OLHO\", named after the Portuguese word for \"eye\". 24 holographic poems followed this first work, including \"Quando (When?)\" (1987), a moebius-like work that could be read in two directions. \n\nAround the same time, and drawing on his interest in experimental poetry forms, Kac began making animated poetry works with the French Minitel videtext terminals that were then in use in Rio. In 1985 he contributed one such work, \"Reabracadabra\", to the \"Arte On Line\" exhibition, organized by the Livraria Nobel bookstore in Sao Paolo. Other videotext animated poems by Kac include \"Recaos\" (1986), \"Tesão\" (1985/86) and \"D/eu/s\" (1986). In 1986, with Flavio Ferraz, Kac organized the \"Brasil High-Tech\" exhibition at the Galeria de Arte Centro Empresarial Rio in Rio de Janeiro.\n\nFrom 1985 to as late as 1994, Kac did a number of telecommunications performances that used Slow-scan television (SSTV) and FAX technologies to create performances between separate locations. \n\nIn the late 80s Kac began work on his \"Ornitorrinco\" project, a telepresence artwork made in collaboration with Ed Bennett in Chicago. The work brought together robotics, telecommunications technologies and interactivity to create a robot that was controlled remotely. The piece allowed viewers in one location to control the robot's camera and motion, creating a telepresent work and effecting the experience of viewers in the other location.\n\nIn 1989 Kac moved from Brazil to Chicago, where he would complete his MFA at the Art Institute of Chicago the following year.\n\nIn the 1990s, Kac continued creating telematic works, with \"Dialogical Drawing\" (1994) and \"Essay Concerning Human Understanding\" (1994) both using networks to explore the viewer experience of an artwork mediated between two site in real time. In the latter case, the artwork joined a plant in New York city and a live canary in Kentucky in conversation. The inclusion of a bird as part of its system, making it an early example of what Kac called \"transgenic art\".\n\nIn \"Teleporting An Unknown State\" (1994-96), Kac built a system that allowed a plant to survive in a gallery, illuminated not by sunlight but by the internet viewers of the work. In practice, online viewers of the work triggered a video projection onto the plant of an image representing the sky in the viewer's home location. \n\nNotably, Kac coined the term \"bioart\" with his 1997 performance work \"Time Capsule\". In \"Time Capsule\", Kac implanted himself with an RFID chip originally designed for use in pets. A participant in Chicago then triggered the RFID scanned in the Brazilian Gallery where Kac was performing, casing the scanner to display a unique code for the implant. Kac then registered himself on the pet database associated with the implant, becoming the first human to do so. \n\nBy the late 90s Kac defined himself either a \"transgenic artist\" or \"bio artist,\" and was using biotechnology and genetics to create provocative works that concomitantly explore scientific techniques and critique them.\n\nKac's next transgenic artwork, created in 1998/99 and titled \"Genesis,\" involved him taking a quote from the Bible (Genesis 1:26 - \"And God said, Let us make man in our image, after our likeness: and let them have dominion over the fish of the sea, and over the fowl of the air, and over the cattle, and over all the earth, and over every creeping thing that creepeth upon the earth\"), transferring it into Morse code, and finally, translating that Morse code (by a conversion principle specially developed by the artist for this work) into the base pairs of genetics.\n\nIn one of his best known works, \"Alba\", presented in 2000 in Avignon, France, Kac claimed to have commissioned a French laboratory to create a green-fluorescent rabbit; a rabbit implanted with a Green Fluorescent Protein (GFP) gene from a type of jellyfish. Under a specific blue light, the rabbit fluoresces green. The work proved to be hugely controversial, which was later mitigated by the revelation that the GFP process was not new but was, rather, already in use on rabbits at the lab in question. Kac's original aim was for Alba to live with his family, but prior to the scheduled release of Alba to Kac, the lab retracted their agreement and decided that Alba should remain in the lab.\n\nHis work \"Natural History of the Enigma\" (2003-8) continued in the theme of bio art by merging his DNA with that of a petunia, creating a hybrid organism that Kac called a \"plantimal\". The plant, also given the name \"Eudinia\" (from Eduardo and Petunia), mimicked the flow of blood through human veins by mixing Kac's DNA only with the plant's genetic components that made the veins in its leaves red.\n\nIn 2017 Kac collaborated with the French astronaut Thomas Pesquet to create an artwork in space called \"Inner Telescope\". Following Kac's instructions, Pesquet cut and folded a piece of paper into a sculptural form that could be read as the three letters forming the French word for \"me\", M-O-I.\n\nKac's work is included int he permanent collections of the Institute Valencia in Valencia, Spain and the Victoria and Albert Museum, London. Several of Kac's artist books are included in the library of the Museum of Modern Art, New York.\n\nIn 1998 he received the Leonardo Award for Excellence from ISAST. In 1999, he received the Inter Communication Center (Tokyo) Biennial Award in 1999.\n\nIn 2002 he received the Creative Capital Award in the discipline of Emerging Fields.\n\nIn 2008 he received the Golden Nica award at Ars Electronica for his project \"Natural History of the Enigma\".\n\nBooks by Eduardo Kac\n\n\nCatalogues and Monographs of Eduardo Kac's exhibitions\n\n\nBooks about the Art of Eduardo Kac\n\n\n"}
{"id": "25271012", "url": "https://en.wikipedia.org/wiki?curid=25271012", "title": "Environmental governance", "text": "Environmental governance\n\nEnvironmental governance is a concept in political ecology and environmental policy that advocates sustainability (sustainable development) as the supreme consideration for managing all human activities—political, social and economic. Governance includes government, business and civil society, and emphasizes whole system management. To capture this diverse range of elements, environmental governance often employs alternative systems of governance, for example watershed-based management.\n\nIt views natural resources and the environment as global public goods, belonging to the category of goods that are not diminished when they are shared. This means that everyone benefits from for example, a breathable atmosphere, stable climate and stable biodiversity.\n\nPublic goods are non-rivalrous—a natural resource enjoyed by one person can still be enjoyed by others—and non-excludable—it is impossible to prevent someone consuming the good (breathing). Nevertheless, public goods are recognized as beneficial and therefore have value. The notion of a global public good thus emerges, with a slight distinction: it covers necessities that must not be destroyed by one person or state.\n\nThe non-rivalrous character of such goods calls for a management approach that restricts public and private actors from damaging them. One approach is to attribute an economic value to the resource. Water is possibly the best example of this type of good.\n\nAs of 2013 environmental governance is far from meeting these imperatives. “Despite a great awareness of environmental questions from developed and developing countries, there is environmental degradation and the appearance of new environmental problems. This situation is caused by the parlous state of global environmental governance, wherein current global environmental governance is unable to address environmental issues due to many factors. These include fragmented governance within the United Nations, lack of involvement from financial institutions, proliferation of environmental agreements often in conflict with trade measures; all these various problems disturb the proper functioning of global environmental governance. Moreover, divisions among northern countries and the persistent gap between developed and developing countries also have to be taken into account to comprehend the institutional failures of the current global environmental governance.\"\nWhat is Environmental Governance?\n\nEnvironmental governance refers to the processes of decision-making involved in the control and management of the environment and natural resources. International Union for Conservation of Nature (IUCN), define environmental governance as the 'multi-level interactions (i.e., local, national, international/global) among, but not limited to, three main actors, i.e., state, market, and civil society, which interact with one another, whether in formal and informal ways; in formulating and implementing policies in response to environment-related demands and inputs from the society; bound by rules, procedures, processes, and widely accepted behavior; possessing characteristics of “good governance”; for the purpose of attaining environmentally-sustainable development' (IUCN 2014)\n\nKey principles of environmental governance include:\n\n\nNeoliberal Environmental Governance – is an approach to the theory of environmental governance framed by a perspective on neoliberalism as an ideology, policy and practice in relation to the biophysical world. There are many definitions and applications of neoliberalism, e.g. in economic, international relations, etc. However, the traditional understanding of neoliberalism is often simplified to the notion of the primacy of market-led economics through the rolling back of the state, deregulation and privatisation. Neoliberalism has evolved particularly over the last 40 years with many scholars leaving their ideological footprint on the neoliberal map. Hayek and Friedman believed in the superiority of the free market over state intervention. As long as the market was allowed to act freely, the supply/demand law would ensure the ‘optimal’ price and reward. In Karl Polanyi’s opposing view this would also create a state of tension in which self-regulating free markets disrupt and alter social interactions and “displace other valued means of living and working”. However, in contrast to the notion of an unregulated market economy there has also been a “paradoxical increase in [state] intervention” in the choice of economic, legislative and social policy reforms, which are pursued by the state to preserve the neoliberal order. This contradictory process is described by Peck and Tickell as roll back/roll out neoliberalism in which on one hand the state willingly gives up the control over resources and responsibility for social provision while on the other, it engages in “purposeful construction and consolidation of neoliberalised state forms, modes of governance, and regulatory relations\".\n\nThere has been a growing interest in the effects of neoliberalism on the politics of the non-human world of environmental governance. Neoliberalism is seen to be more than a homogenous and monolithic ‘thing’ with a clear end point. It is a series of path-dependent, spatially and temporally “connected neoliberalisation” processes which affect and are affected by nature and environment that “cover a remarkable array of places, regions and countries”. Co-opting neoliberal ideas of the importance of private property and the protection of individual (investor) rights, into environmental governance can be seen in the example of recent multilateral trade agreements (see in particular the North American Free Trade Agreement). Such neoliberal structures further reinforce a process of nature enclosure and primitive accumulation or “accumulation by dispossession” that serves to privatise increasing areas of nature. The ownership-transfer of resources traditionally not privately owned to free market mechanisms is believed to deliver greater efficiency and optimal return on investment. Other similar examples of neo-liberal inspired projects include the enclosure of minerals, the fisheries quota system in the North Pacific and the privatisation of water supply and sewage treatment in England and Wales. All three examples share neoliberal characteristics to “deploy markets as the solution to environmental problems” in which scarce natural resources are commercialized and turned into commodities. The approach to frame the ecosystem in the context of a price-able commodity is also present in the work of neoliberal geographers who subject nature to price and supply/demand mechanisms where the earth is considered to be a quantifiable resource (Costanza, for example, estimates the earth ecosystem's service value to be between 16 and 54 trillion dollars per year).\n\n\"Economic growth\" – The development-centric vision that prevails in most countries and international institutions advocates a headlong rush towards more economic growth. Environmental economists on the other hand, point to a close correlation between economic growth and environmental degradation, arguing for qualitative development as an alternative to growth. As a result, the past couple of decades has seen a big shift towards sustainable development as an alternative to neo-liberal economics. There are those, particularly within the alternative globalization movement, who maintain that it is feasible to change to a degrowth phase without losing social efficiency or lowering the quality of life.\n\n\"Consumption\" – The growth of consumption and the cult of consumption, or consumerist ideology, is the major cause of economic growth. Overdevelopment, seen as the only alternative to poverty, has become an end in itself. The means for curbing this growth are not equal to the task, since the phenomenon is not confined to a growing middle class in developing countries, but also concerns the development of irresponsible lifestyles, particularly in northern countries, such as the increase in the size and number of homes and cars per person.\n\n\"Destruction of biodiversity\" – The complexity of the planet’s ecosystems means that the loss of any species has unexpected consequences. The stronger the impact on biodiversity, the stronger the likelihood of a chain reaction with unpredictable negative effects. Another important factor of environmental degradation that falls under this destruction of biodiversity, and must not be ignored is deforestation. Despite all the damage inflicted, a number of ecosystems have proved to be resilient. Environmentalists are endorsing a precautionary principle whereby all potentially damaging activities would have to be analyzed for their environmental impact.\n\n\"Population growth\" – Forecasts predict 8.9 billion people on the planet in 2050. This is a subject which primarily affects developing countries, but also concerns northern countries; although their demographic growth is lower, the environmental impact per person is far higher in these countries. Demographic growth needs to be countered by developing education and family planning programs and generally improving women’s status.\n\n\"Pollution\" - Pollution caused by the use of fossil fuels is another driver of environmental destruction. The burning of carbon-based fossil fuels such as coal and oil, releases carbon dioxide into the atmosphere. One of the major impacts of this is the climate change that is currently taking place on the planet, where the earth's temperature is gradually rising. Given that fuels such as coal and oil are the most heavily used fuels, this a great concern to many environmentalists.\n\n\"Agricultural practices\" - Destructive agricultural practices such as overuse of fertilizers and overgrazing lead to land degradation. The soil gets eroded, and leads to silting in rivers and reservoirs. Soil erosion is a continuous cycle and ultimately results in desertification of the land. Apart from land degradation, water pollution is also a possibility; chemicals used in farming can run-off into rivers and contaminate the water.\n\nThe crisis by the impact of human activities on nature calls for governance. Which includes responses by international institutions, governments and citizens, who should meet this crisis by pooling the experience and knowledge of each of the agents and institutions concerned.\n\nThe environmental protection measures taken remain insufficient. The necessary reforms require time, energy, money and diplomatic negotiations. The situation has not generated a unanimous response. Persistent divisions slow progress towards global environmental governance.\n\nThe global nature of the crisis limits the effects of national or sectoral measures. Cooperation is necessary between actors and institutions in international trade, sustainable development and peace.\n\nGlobal, continental, national and local governments have employed a variety of approaches to environmental governance. Substantial positive and negative spillovers limit the ability of any single jurisdiction to resolve issues.\n\nChallenges facing environmental governance include:\n\n\nAll of these challenges have implications on governance, however international environmental governance is necessary. The IDDRI claims that rejection of multilateralism in the name of efficiency and protection of national interests conflicts with the promotion of international law and the concept of global public goods. Others cite the complex nature of environmental problems.\n\nOn the other hand, The Agenda 21 program has been implemented in over 7,000 communities. Environmental problems, including global-scale problems, may not always require global solutions. For example, marine pollution can be tackled regionally, and ecosystem deterioration can be addressed locally. Other global problems such as climate change benefit from local and regional action.\n\nBäckstrand and Saward wrote, “sustainability and environmental protection is an arena in which innovative experiments with new hybrid, plurilateral forms of governance, along with the incorporation of a transnational civil society spanning the public-private divide, are taking place.”\n\nA 1997 report observed a global consensus that sustainable development implementation should be based on local level solutions and initiatives designed with and by the local communities. Community participation and partnership along with the decentralisation of government power to local communities are important aspects of environmental governance at the local level. Initiatives such as these are integral divergence from earlier environmental governance approaches which was “driven by state agendas and resource control” and followed a top-down or trickle down approach rather than the bottom up approach that local level governance encompasses. The adoption of practices or interventions at a local scale can, in part, be explained by diffusion of innovation theory. In Tanzania and in the Pacific, researchers have illustrated that aspects of the intervention, of the adopter, and of the social-ecological context all shape why community-centered conservation interventions spread through space and time. Local level governance shifts decision making power away from the state and/or governments to the grassroots. Local level governance is extremely important even on a global scale. Environmental governance at the global level is defined as international and as such has resulted in the marginalisation of local voices. Local level governance is important to bring back power to local communities in the global fight against environmental degridation. Pulgar Vidal observed a “new institutional framework, [wherein] decision-making regarding access to and use of natural resources has become increasingly decentralized.” He noted four techniques that can be used to develop these processes:\n\n\nHe found that the key conditions for developing decentralized environmental governance are:\n\n\nThe legitimacy of decisions depends on the local population's participation rate and on how well participants represent that population.\nWith regard to public authorities, questions linked to biodiversity can be faced by adopting appropriate policies and strategies, through exchange of knowledge and experience, the forming of partnerships, correct management of land use, monitoring of biodiversity and optimal use of resources, or reducing consumption, and promoting environmental certifications, such as EMAS and/or ISO 14001. Local authorities undoubtedly have a central role to play in the protection of biodiversity and this strategy is successful above all when the authorities show strength by involving stakeholders in a credible environmental improvement project and activating a transparent and effective communication policy (Ioppolo et al., 2013).\n\nStates play a crucial role in environmental governance, because \"however far and fast international economic integration proceeds, political authority remains vested in national governments\". It is for this reason that governments should respect and support the commitment to implementation of international agreements.\n\nAt the state level, environmental management has been found to be conducive to the creation of roundtables and committees. In France, the \"Grenelle de l’environnement\" process:\n\n\nIf environmental issues are excluded from e.g., the economic agenda, this may delegitimize those institutions.\n\n“In southern countries, the main obstacle to the integration of intermediate levels in the process of territorial environmental governance development is often the dominance of developmentalist inertia in states’ political mindset. The question of the environment has not been effectively integrated in national development planning and programs. Instead, the most common idea is that environmental protection curbs economic and social development, an idea encouraged by the frenzy for exporting raw materials extracted using destructive methods that consume resources and fail to generate any added value.” Of course they are justified in this thinking, as their main concerns are social injustices such as poverty alleviation. Citizens in some of these states have responded by developing empowerment strategies to ease poverty through sustainable development. In addition to this, policymakers must be more aware of these concerns of the global south, and must make sure to integrate a strong focus on social justice in their policies.\n\nAt the global level there are numerous important actors involved in environmental governance and \"a range of institutions contribute to and help define the practice of global environmental governance. The idea of global environmental governance is to govern the environment at a global level through a range of nation states and non state actors such as national governments, NGOs and other international organisations such as UNEP (United Nations Environment Programme). Global environmental governance is the answer to calls for new forms of governance because of the increasing complexity of the international agenda. It is perceived to be an effective form of multilateral management and essential to the international community in meeting goals of mitigation and the possible reversal of the impacts on the global environment. However, a precise definition of global environmental governance is still vague and there are many issues surrounding global governance.\nElliot argues that “the congested institutional terrain still provides more of an appearance than a reality of comprehensive global governance.” This meant that there are too many institutions within the global governance of the environment for it to be completely inclusive and coherent leaving it merely portraying the image of this to the global public. Global environmental governance is about more than simply expanding networks of institutions and decision makers. “It is a political practice which simultaneously reflects, constitutes and masks global relations of power and powerlessness.” State agendas exploit the use of global environmental governance to enhance their oven agendas or wishes even if this is at the detriment of the vital element behind global environmental governance which is the environment. Elliot states that global environmental governance “is neither normatively neutral nor materially benign.”\nAs explored by Newell, report notes by The Global Environmental Outlook noted that the systems of global environmental governance are becoming increasingly irrelevant or impotent due to patterns of globalisation such as; imbalances in productivity and the distribution of goods and services, unsustainable progression of extremes of wealth and poverty and population and economic growth overtaking environmental gains. Newell states that, despite such acknowledgements, the “managing of global environmental change within International Relations continues to look to international regimes for the answers.”\n\nThe literature on governance scale shows how changes in the understanding of environmental issues have led to the movement from a local view to recognising their larger and more complicated scale. This move brought an increase in the diversity, specificity and complexity of initiatives. Meadowcroft pointed out innovations that were layered on top of existing structures and processes, instead of replacing them.\n\nLafferty and Meadowcroft give three examples of multi-tiered governance: internationalisation, increasingly comprehensive approaches, and involvement of multiple governmental entities. Lafferty and Meadowcroft described the resulting multi-tiered system as addressing issues on both smaller and wider scales.\n\nHans Bruyninckx claimed that a mismatch between the scale of the environmental problem and the level of the policy intervention was problematic. Young claimed that such mismatches reduced the effectiveness of interventions. Most of the literature addresses the level of governance rather than ecological scale.\n\nElinor Ostrom, amongst others, claimed that the mismatch is often the cause of unsustainable management practices and that simple solutions to the mismatch have not been identified.\n\nConsiderable debate has addressed the question of which level(s) should take responsibility for fresh water management. Development workers tend to address the problem at the local level. National governments focus on policy issues. This can create conflicts among states because rivers cross borders, leading to efforts to evolve governance of river basins.\n\nSoil and land deterioration reduces its capacity for capturing, storing and recycling water, energy and food. Alliance 21 proposed solutions in the following domains:\n\n\nThe scientific consensus on climate change is expressed in the reports of Intergovernmental Panel on Climate Change (IPCC) and also in the statements by all major scientific bodies in the United States such as National Academy of Sciences.\n\nThe drivers of climate change can include\n- Changes in solar irradiance\n- Changes in atmospheric trace gas and aerosol concentrations\nEvidence of climate change can be identified by examining\n- Atmospheric concentrations of Green House Gases (GHGs) such as carbon dioxide (CO2)\n- Land and sea surface temperatures\n- Atmospheric water vapor\n- Precipitation\n- The occurrence or strength of extreme weather and climate events\n- Glaciers\n- Rapid sea ice loss\n- Sea level\n\nIt is suggested by climate models that the changes in temperature and sea level can be the causal effects of human activities such as consumption of fossil fuels, deforestation, increased agricultural production and production of xenobiotic gases.\n\nThere has been increasing actions in order to mitigate climate change and reduce its impact at national, regional and international levels. Kyoto protocol and United Nations Framework Convention on Climate Change (UNFCCC) plays the most important role in addressing climate change at an international level.\n\nThe goal of combating climate change led to the adoption of the Kyoto Protocol by 191 states, an agreement encouraging the reduction of greenhouse gases, mainly . Since developed economies produce more emissions per capita, limiting emissions in all countries inhibits opportunities for emerging economies, the only major success in efforts to produce a global response to the phenomenon.\n\nTwo decades following the Brundtland Report, however, there has been no improvement in the key indicators highlighted.\n\nEnvironmental governance for protecting the biodiversity has to act in many levels. Biodiversity is fragile because it is threatened by almost all human actions. To promote conservation of biodiversity, agreements and laws have to be created to regulate agricultural activities, urban growth, industrialization of countries, use of natural resources, control of invasive species, the correct use of water and protection of air quality. Before making any decision for a region or country decision makers, politicians and community have to take into account what are the potential impacts for biodiversity, that any project can have.\n\nPopulation growth and urbanization have been a great contributor for deforestation. Also, population growth requires more intense agricultural areas use, which also results in necessity of new areas to be deforested. This causes habitat loss, which is one of the major threats for biodiversity. Habitat loss and habitat fragmentation affects all species, because they all rely on limited resources, to feed on and to breed.\n\n‘\"Species are genetically unique and irreplaceable their loss is irreversible. Ecosystems vary across a vast range of parameters, and similar ecosystems (whether wetlands, forests, coastal reserves etc) cannot be presumed to be interchangeable, such that the loss of one can be compensated by protection or restoration of another\"’.\n\nTo avoid habitat loss, and consequently biodiversity loss, politicians and lawmakers should be aware of the precautionary principle, which means that before approving a project or law all the pros and cons should be carefully analysed. Sometimes the impacts are not explicit, or not even proved to exist. However, if there is any chance of an irreversible impact happen, it should be taken into consideration.\n\nTo promote environmental governance for biodiversity protection there has to be a clear articulation between values and interests while negotiating environmental management plans. International agreements are good way to have it done right.\n\nThe Convention on Biological Diversity (CBD) was signed in Rio de Janeiro in 1992 human activities. The CBD’s objectives are: “to conserve biological diversity, to use biological diversity in a sustainable fashion, to share the benefits of biological diversity fairly and equitably.” The Convention is the first global agreement to address all aspects of biodiversity: genetic resources, species and ecosystems. It recognizes, for the first time, that the conservation of biological diversity is “a common concern for all humanity”. The Convention encourages joint efforts on measures for scientific and technological cooperation, access to genetic resources and the transfer of clean environmental technologies.\n\nThe Convention on Biological Diversity most important edition happened in 2010 when the Strategic Plan for Biodiversity 2011-2020 and the Aichi Targets, were launched. These two projects together make the United Nations decade on Biodiversity. It was held in Japan and has the targets of ‘\"halting and eventually reversing the loss of biodiversity of the planet\"’. The Strategic Plan for Biodiversity has the goal to ‘\"promote its overall vision of living in harmony with nature\"’ As result (...) ‘\"mainstream biodiversity at different levels. Throughout the United Nations Decade on Biodiversity, governments are encouraged to develop, implement and communicate the results of national strategies for implementation of the Strategic Plan for Biodiversity\"’. According to the CBD the five Aichi targets are:\n\nThe 2003 UN World Water Development Report claimed that the amount of water available over the next twenty years would drop by 30%.\nIn the same report, it is indicated that in 1998, 2.2 million people died from diarrhoeal diseases. In 2004, the UK’s WaterAid charity reported that one child died every 15 seconds from water-linked diseases.\n\nAccording to Alliance 21 “All levels of water supply management are necessary and independent. The integrated approach to the catchment areas must take into account the needs of irrigation and those of towns, jointly and not separately as is often seen to be the case...The governance of a water supply must be guided by the principles of sustainable development.”\n\nAustralian water resources have always been variable but they are becoming increasingly so with changing climate conditions. Because of how limited water resources are in Australia, there needs to be an effective implementation of environmental governance conducted within the country. Water restrictions are an important policy device used in Australian environmental governance to limit the amount of water used in urban and agricultural environments (Beeton et al. 2006). There is increased pressure on surface water resources in Australia because of the uncontrolled growth in groundwater use and the constant threat of drought. These increased pressures not only affect the quantity and quality of the waterways but they also negatively affect biodiversity. The government needs to create policies that preserve, protect and monitor Australia’s inland water. The most significant environmental governance policy imposed by the Australian government is environmental flow allocations that allocate water to the natural environment. The proper implementation of water trading systems could help to conserve water resources in Australia. Over the years there has been an increase in demand for water, making Australia the third largest per capita user of water in the world (Beeton et al. 2006). If this trend continues, the gap between supply and demand will need to be addressed. The government needs to implement more efficient water allocations and raise water rates (UNEP, 2014). By changing public perception to promote the action of reusing and recycling water some of the stress of water shortages can be alleviated. More extensive solutions like desalination plants, building more dams and using aquifer storage are all options that could be taken to conserve water levels but all these methods are controversial. With caps on surface water use, both urban and rural consumers are turning to groundwater use; this has caused groundwater levels to decline significantly. Groundwater use is very hard to monitor and regulate. There is not enough research currently being conducted to accurately determine sustainable yields. Some regions are seeing improvement in groundwater levels by applying caps on bores and the amount of water that consumers are allowed to extract. There have been projects in environmental governance aimed at restoring vegetation in the riparian zone. Restoring riparian vegetation helps increase biodiversity, reduce salinity, prevent soil erosion and prevent riverbank collapse. Many rivers and waterways are controlled by weirs and locks that control the flow of rivers and also prevent the movement of fish. The government has funded fish-ways on some weirs and locks to allow for native fish to move upstream. Wetlands have significantly suffered under restricted water resources with water bird numbers dropping and a decrease in species diversity. The allocation of water for bird breeding through environmental flows in Macquarie Marshes has led to an increase in breeding (Beeton et al. 2006). Because of dry land salinity throughout Australia there has been an increase in the levels of salt in Australian waterways. There has been funding in salt interception schemes which help to improve in-stream salinity levels but whether river salinity has improved or not is still unclear because there is not enough data available yet. High salinity levels are dangerous because they can negatively affect larval and juvenile stages of certain fish. The introduction of invasive species into waterways has negatively affected native aquatic species because invasive species compete with native species and alter natural habitats. There has been research in producing daughterless carp to help eradicate carp. Government funding has also gone into building in-stream barriers that trap the carp and prevent them from moving into floodplains and wetlands. Investment in national and regional programmes like the Living Murray (MDBC), Healthy Waterways Partnership and the Clean Up the Swan Programme are leading to important environmental governance. The Healthy Rivers programme promotes restoration and recovery of environmental flows, riparian re-vegetation and aquatic pest control. The Living Murray programme has been crucial for the allocation of water to the environment by creating an agreement to recover 500 billion litres of water to the Murray River environment. Environmental governance and water resource management in Australia must be constantly monitored and adapted to suit the changing environmental conditions within the country (Beeton et al. 2006). If environmental programmes are governed with transparency there can be a reduction in policy fragmentation and an increase in policy efficiency (Mclntyre, 2010). In Arab countries, the extensive use of water for agriculture also needs critical attention since agriculture in this region has less contribution for its national income. \n\nOn 16 September 1987 the United Nations General Assembly signed the Montreal Protocol to address the declining ozone layer. Since that time, the use of chlorofluorocarbons (industrial refrigerants and aerosols) and farming fungicides such as methyl bromide has mostly been eliminated, although other damaging gases are still in use.\n\nThe Nuclear non-proliferation treaty is the primary multilateral agreement governing nuclear activity.\n\nGenetically modified organisms are not the subject of any major multilateral agreements. They are the subject of various restrictions at other levels of governance. GMOs are in widespread use in the US, but are heavily restricted in many other jurisdictions.\n\nControversies have ensued over golden rice, genetically modified salmon, genetically modified seeds, disclosure and other topics.\n\nThe precautionary principle or precautionary approach states that if an action or policy has a suspected risk of causing harm to the public or to the environment, in the absence of scientific consensus that the action or policy is harmful, the burden of proof that it is not harmful falls on those taking an action. As of 2013 it was not the basis of major multilateral agreements.\nThe Precautionary Principle is put into effect if there is a chance that proposed action may cause harm to the society or the environment. Therefore, those involved in the proposed action must provide evidence that it will not be harmful, even if scientists do not believe that it will cause harm. It falls upon the policymakers to make the optimal decision, if there is any risk, even without any credible scientific evidence. However, taking precautionary action also means that there is an element of cost involved, either social or economic. So if the cost was seen as insignificant the action would be taken without the implementation of the precautionary principle. But often the cost is ignored, which can lead to harmful repercussions. This is often the case with industry and scientists who are primarily concerned with protecting their own interests.\n\nLeading experts have emphasized on the importance of taking into account the security aspects the environment and natural resources will cause. The twenty-first century is looking into a future with an increase in mass migrations of refugees, wars and praetorian regimes caused by the effect of environmental degradation such as water scarcity, deforestation and soil erosion, air pollution and, climate change effects such as rising sea levels. For a long time, foreign-policy challenges have focused on social causes as being the only reason for social and political changes. However, it is a crucial moment to understand and take into consideration the security implications that environmental stress will bring to the current political and social structure around the globe.\n\nThe main multilateral conventions, also known as Rio Conventions, are as follows:\n\nConvention on Biological Diversity (CBD) (1992–1993): aims to conserve biodiversity. Related agreements include the Cartagena Protocol on biosafety.\n\nUnited Nations Framework Convention on Climate Change (UNFCC) (1992–1994): aims to stabilize concentrations of greenhouse gases at a level that would stabilize the climate system without threatening food production, and enabling the pursuit of sustainable economic development; it incorporates the Kyoto Protocol.\n\nUnited Nations Convention to Combat Desertification (UNCCD) (1994–1996): aims to combat desertification and mitigate the effects of drought and desertification, in developing countries (Though initially the convention was primarily meant for Africa).\n\nFurther conventions:\n\nThe Rio Conventions are characterized by:\n\n\nEnvironmental conventions are regularly criticized for their:\n\n\nUntil now, the formulation of environmental policies at the international level has been divided by theme, sector or territory, resulting in treaties that overlap or clash. International attempts to coordinate environment institutions, include the Inter-Agency Coordination Committee and the Commission for Sustainable Development, but these institutions are not powerful enough to effectively incorporate the three aspects of sustainable development.\n\nMEAs are agreements between several countries that apply internationally or regionally and concern a variety of environmental questions. As of 2013 over 500 Multilateral Environmental Agreements (MEAs), including 45 of global scope involve at least 72 signatory countries. Further agreements cover regional environmental problems, such as deforestation in Borneo or pollution in the Mediterranean. Each agreement has a specific mission and objectives ratified by multiple states.\n\nMany Multilateral Environmental Agreements have been negotiated with the support from the United Nations Environmental Programme and work towards the achievement of the United Nations Millennium Development Goals as a means to instil sustainable practices for the environment and its people. Multilateral Environmental Agreements are considered to present enormous opportunities for greener societies and economies which can deliver numerous benefits in addressing food, energy and water security and in achieving sustainable development. These agreements can be implemented on a global or regional scale, for example the issues surrounding the disposal of hazardous waste can be implemented on a regional level as per the Bamako Convention on the Ban of the Import into Africa and the Control of Transboundary Movement and Management of Hazardous Waste within Africa which applies specifically to Africa, or the global approach to hazardous waste such as the Basel Convention on the Control of Transboundary Movements of Hazardous Wastes and their Disposal which is monitored throughout the world.\n\n“The environmental governance structure defined by the Rio and Johannesburg Summits is sustained by UNEP, MEAs and developmental organizations and consists of assessment and policy development, as well as project implementation at the country level.\n\n\"The governance structure consists of a chain of phases:\n\n\n\"Traditionally, UNEP has focused on the normative role of engagement in the first three\nphases. Phases (d) to (f) are covered by MEAs and the sustainable development phase involves developmental organizations such as UNDP and the World Bank.”\n\nLack of coordination affects the development of coherent governance. The report shows that donor states support development organizations, according to their individual interests. They do not follow a joint plan, resulting in overlaps and duplication. MEAs tend not to become a joint frame of reference and therefore receive little financial support. States and organizations emphasize existing regulations rather than improving and adapting them.\n\nThe risks associated with nuclear fission raised global awareness of environmental threats. The 1963 Partial Nuclear Test Ban Treaty prohibiting atmospheric nuclear testing was the beginning of the globalization of environmental issues. Environmental law began to be modernized and coordinated with the Stockholm Conference (1972), backed up in 1980 by the Vienna Convention on the Law of Treaties. The Vienna Convention for the Protection of the Ozone Layer was signed and ratified in 1985. In 1987, 24 countries signed the Montreal Protocol which imposed the gradual withdrawal of CFCs.\n\nThe Brundtland Report, published in 1987 by the UN Commission on Environment and Development, stipulated the need for economic development that “meets the needs of the present without compromising the capacity of future generations to meet their needs.\n\nThe United Nations Conference on Environment and Development (UNCED), better known as the 1992 Earth Summit, was the first major international meeting since the end of the Cold War and was attended by delegations from 175 countries. Since then the biggest international conferences that take place every 10 years guided the global governance process with a series of MEAs. Environmental treaties are applied with the help of secretariats.\n\nGovernments created international treaties in the 1990s to check global threats to the environment. These treaties are far more restrictive than global protocols and set out to change non-sustainable production and consumption models.\n\nAgenda 21 is a detailed plan of actions to be implemented at the global, national and local levels by UN organizations, member states and key individual groups in all regions. Agenda 21 advocates making sustainable development a legal principle law. At the local level, local Agenda 21 advocates an inclusive, territory-based strategic plan, incorporating sustainable environmental and social policies.\n\nThe Agenda has been accused of using neoliberal principles, including free trade to achieve environmental goals. For example, chapter two, entitled “International Cooperation to Accelerate Sustainable Development in Developing Countries and Related Domestic Policies” states, “The international economy should provide a supportive international climate for achieving environment and development goals by: promoting sustainable development through trade liberalization.”\n\nThe UNEP has had its biggest impact as a monitoring and advisory body, and in developing environmental agreements. It has also contributed to strengthening the institutional capacity of environment ministries.\n\nIn 2002 UNEP held a conference to focus on product lifecycle impacts, emphasizing the fashion, advertising, financial and retail industries, seen as key agents in promoting sustainable consumption.\n\nAccording to Ivanova, UNEP adds value in environmental monitoring, scientific assessment and information sharing, but cannot lead all environmental management processes. She proposed the following tasks for UNEP:\n\n\nOther proposals offer a new mandate to “produce greater unity amongst social and environmental agencies, so that the concept of ‘environment for development’ becomes a reality. It needs to act as a platform for establishing standards and for other types of interaction with national and international organizations and the United Nations. The principles of cooperation and common but differentiated responsibilities should be reflected in the application of this revised mandate.”\n\nSherman proposed principles to strengthen UNEP:\n\n\nAnother group stated, “Consider the specific needs of developing countries and respect of the fundamental principle of 'common but differentiated responsibilities'. Developed countries should promote technology transfer, new and additional financial resources, and capacity building for meaningful participation of developing countries in international environmental governance. Strengthening of international environmental governance should occur in the context of sustainable development and should involve civil society as an important stakeholder and agent of transformation.”\n\nCreated in 1991, the Global Environment Facility is an independent financial organization initiated by donor governments including Germany and France. It was the first financial organization dedicated to the environment at the global level. As of 2013 it had 179 members. Donations are used for projects covering biodiversity, climate change, international waters, destruction of the ozone layer, soil degradation and persistent organic pollutants.\n\nGEF's institutional structure includes UNEP, UNDP and the World Bank. It is the funding mechanism for the four environmental conventions: climate change, biodiversity, persistent organic pollutants and desertification. GEF transfers resources from developed countries to developing countries to fund UNDP, UNEP and World Bank projects. The World Bank manages the annual budget of US$561.10 million.\n\nThe GEF has been criticized for its historic links with the World Bank, at least during its first phase during the 1990s, and for having favoured certain regions to the detriment of others. Another view sees it as contributing to the emergence of a global \"green market\". It represents “an adaptation (of the World Bank) to this emerging world order, as a response to the emergence of environmental movements that are becoming a geopolitical force.” Developing countries demanded financial transfers to help them protect their environment.\n\nGEF is subject to economic profitability criteria, as is the case for all the conventions. It received more funds in its first three years than the UNEP has since its creation in 1972. GEF funding represents less than 1% of development aid between 1992 and 2002.\n\nThis intergovernmental institution meets twice a year to assess follow-up on Rio Summit goals. The CSD is made up of 53 member states, elected every three years and was reformed in 2004 to help improve implementation of Agenda 21. It meets twice a year, focusing on a specific theme during each two-year period: 2004-2005 was dedicated to water and 2006-2007 to climate change. The CSD has been criticized for its low impact, general lack of presence and the absence of Agenda 21 at the state level specifically, according to a report by the World Resources Institute. Its mission focuses on sequencing actions and establishing agreements puts it in conflict with institutions such as UNEP and OECD.\n\nA proposed World Environment Organization, analogous to the World Health Organization could be capable of adapting treaties and enforcing international standards.\n\nThe European Union, particularly France and Germany, and a number of NGOs favour creating a WEO. The United Kingdom, the US and most developing countries prefer to focus on voluntary initiatives. WEO partisans maintain that it could offer better political leadership, improved legitimacy and more efficient coordination. Its detractors argue that existing institutions and missions already provide appropriate environmental governance; however the lack of coherence and coordination between them and the absence of clear division of responsibilities prevents them from greater effectiveness.\n\nThe World Bank influences environmental governance through other actors, particularly the GEF. The World Bank’s mandate is not sufficiently defined in terms of environmental governance despite the fact that it is included in its mission. However, it allocates 5 to 10% of its annual funds to environmental projects. The institution's capitalist vocation means that its investment is concentrated solely in areas which are profitable in terms of cost benefits, such as climate change action and ozone layer protection, whilst neglecting other such as adapting to climate change and desertification. Its financial autonomy means that it can make its influence felt indirectly on the creation of standards, and on international and regional negotiations.\n\nFollowing intense criticism in the 1980s for its support for destructive projects which, amongst other consequences, caused deforestation of tropical forests, the World Bank drew up its own environment-related standards in the 1990s so it could correct its actions. These standards differ from UNEP’s standards, meant to be the benchmark, thus discrediting the institution and sowing disorder and conflict in the world of environmental governance. Other financial institutions, regional development banks and the private sector also drew up their own standards. Criticism is not directed at the World Bank’s standards in themselves, which Najam considered as “robust”, but at their legitimacy and efficacy.\n\nThe GEF's account of itself as of 2012 is as \"the largest public funder of projects to improve the global environment\", period, which \"provides grants for projects related to biodiversity, climate change, international waters, land degradation, the ozone layer, and persistent organic pollutants.\" It claims to have provided \"$10.5 billion in grants and leveraging $51 billion in co-financing for over 2,700 projects in over 165 countries [and] made more than 14,000 small grants directly to civil society and community-based organizations, totaling $634 million.\" It serves as mechanism for the:\nThis mandate reflects the restructured GEF as of October 2011 .\n\nThe WTO’s mandate does not include a specific principle on the environment. All the problems linked to the environment are treated in such a way as to give priority to trade requirements and the principles of the WTO’s own trade system. This produces conflictual situations. Even if the WTO recognizes the existence of MEAs, it denounces the fact that around 20 MEAs are in conflict with the WTO’s trade regulations. Furthermore, certain MEAs can allow a country to ban or limit trade in certain products if they do not satisfy established environmental protection requirements. In these circumstances, if one country’s ban relating to another country concerns two signatories of the same MEA, the principles of the treaty can be used to resolve the disagreement, whereas if the country affected by the trade ban with another country has not signed the agreement, the WTO demands that the dispute be resolved using the WTO’s trade principles, in other words, without taking into account the environmental consequences.\n\nSome criticisms of the WTO mechanisms may be too broad. In a recently dispute over labelling of dolphin safe labels for tuna between the US and Mexico, the ruling was relatively narrow and did not, as some critics claimed,\n\nThe IMF’s mission is \"to ensure the stability of the international monetary system\".\n\nThe IMF Green Fund proposal of Dominique Strauss-Kahn specifically to address \"climate-related shocks in Africa\", despite receiving serious attention was rejected. Strauss-Kahn's proposal, backed by France and Britain, was that \"developed countries would make an initial capital injection into the fund using some of the $176 billion worth of SDR allocations from last year in exchange for a stake in the green fund.\" However, \"most of the 24 directors ... told Strauss-Kahn that climate was not part of the IMF's mandate and that SDR allocations are a reserve asset never intended for development issues.\"\n\nThe UN's main body for coordinating municipal and urban decision-making is named the International Council for Local Environmental Initiatives. Its slogan is \"Local Governments for Sustainability\".\nThis body sponsored the concept of full cost accounting that makes environmental governance the foundation of other governance.\n\nICLEIs projects and achievements include:\n\nICLEI promotes best practice exchange among municipal governments globally, especially green infrastructure, sustainable procurement.\n\nOther international institutions incorporate environmental governance in their action plans, including:\n\n\nOver 30 UN agencies and programmes support environmental management, according to Najam. This produces a lack of coordination, insufficient exchange of information and dispersion of responsibilities. It also results in proliferation of initiatives and rivalry between them.\n\nAccording to Bauer, Busch and Siebenhüner, the different conventions and multilateral agreements of global environmental regulation is increasing their secretariats' influence. Influence varies according to bureaucratic and leadership efficiency, choice of technical or client-centered.\n\nThe United Nations is often the target of criticism, including from within over the multiplication of secretariats due to the chaos it produces. Using a separate secretariat for each MEA creates enormous overhead given the 45 international-scale and over 500 other agreements.\n\nEnvironmental protection has created opportunities for mutual and collective monitoring among neighbouring states. The European Union provides an example of the institutionalization of joint regional and state environmental governance. Key areas include information, led by the European Environment Agency (EEA), and the production and monitoring of norms by states or local institutions.\nSee also the Environmental policy of the European Union.\n\nUS refusal to ratify major environment agreements produced tensions with ratifiers in Europe and Japan.\n\nThe World Bank, IMF and other institutions are dominated by the developed countries and do not always properly consider the requirements of developing countries.\n\nEnvironmental governance applies to business as well as government. Considerations are typical of those in other domains:\n\n\nWhite and Klernan among others discuss the correlation between environmental governance and financial performance. This correlation is higher in sectors where environmental impacts are greater.\n\nBusiness environmental issues include emissions, biodiversity, historical liabilities, product and material waste/recycling, energy use/supply and many others.\n\nEnvironmental governance has become linked to traditional corporate governance as an increasing number of shareholders are corporate environmental impacts. Corporate governance is the set of processes, customs, policies, laws, and institutions affecting the way a corporation (or company) is managed. Corporate governance is affected by the relationships among stakeholders. These stakeholders research and quantify performance to compare and contrast the environmental performance of thousands of companies.\n\nLarge corporations with global supply chains evaluate the environmental performance of business partners and suppliers for marketing and ethical reasons. Some consumers seek environmentally friendly and sustainable products and companies.\n\nAccording to Bäckstrand and Saward, “broader participation by non-state actors in multilateral environmental decisions (in varied roles such as agenda setting, campaigning, lobbying, consultation, monitoring, and implementation) enhances the democratic legitimacy of environmental governance.”\n\nLocal activism is capable of gaining the support of the people and authorities to combat environmental degradatation. In Cotacachi, Ecuador, a social movement used a combination of education, direct action, the influence of local public authorities and denunciation of the mining company’s plans in its own country, Canada, and the support of international environmental groups to influence mining activity.\n\nFisher cites cases in which multiple strategies were used to effect change. She describes civil society groups that pressure international institutions and also organize local events. Local groups can take responsibility for environmental governance in place of governments.\n\nAccording to Bengoa, “social movements have contributed decisively to the creation of an institutional platform wherein the fight against poverty and exclusion has become an inescapable benchmark.” But despite successes in this area, “these institutional changes have not produced the processes for transformation that could have made substantial changes to the opportunities available to rural inhabitants, particularly the poorest and those excluded from society.” He cites several reasons:\n\n\nA successful initiative in Ecuador involved the establishment of stakeholder federations and management committees (NGOs, communities, municipalities and the ministry) for the management of a protected forest.\n\nThe International Institute for Sustainable Development proposed an agenda for global governance. These objectives are:\n\n\nDespite the increase in efforts, actors, agreements and treaties, the global environment continue to degrade at a rapid rate. From the big hole in Earth’s ozone layer to over-fishing to the uncertainties of climate change, the world is confronted by several intrinsically global challenges. However, as the environmental agenda becomes more complicated and extensive, the current system has proven ineffective in addressing and tackling problems related to trans-boundary externalities and the environment is still experiencing degradation at unprecedented levels.\n\nInforesources identifies four major obstacles to global environmental governance, and describes measures in response. The four obstacles are:\n\n\nRecommended measures:\n\n\nContemporary debates surrounding global environmental governance have converged on the idea of developing a stronger and more effective institutional framework. The views on how to achieve this, however, still hotly debated. Currently, rather than teaming up with the United Nations Environment Programme (UNEP), international environmental responsibilities have been spread across many different agencies including: a) specialised agencies within the UN system such as the World Meteorological Organisation, the International Maritime Organisation and others; b) the programs in the UN system such as the UN Development Program; c) the UN regional economic and social commission; d) the Bretton Woods institutions; e) the World Trade Organisation and; f) the environmentally focused mechanisms such as the Global Environment Facility and close to 500 international environmental agreements.\n\nSome analysts also argue that multiple institutions and some degree of overlap and duplication in policies is necessary to ensure maximum output from the system. Others, however, claim that institutions have become too dispersed and lacking in coordination which can be damaging to their effectiveness in global environmental governance. Whilst there are various arguments for and against a WEO, the key challenge, however, remains the same: how to develop a rational and effective framework that will protect the global environment efficiently.\n\nStarting in 2002, Saward and others began to view the Earth Summit process as capable opening up the possibility of stakeholder democracy. The summits were deliberative rather than simply participative, with NGOs, women, men, indigenous peoples and businesses joining the decision-making process alongside states and international organizations, characterized by:\n\n\nAs of 2013, the absence of joint rules for composing such fora leads to the development of non-transparent relations that favour the more powerful stakeholders. Criticisms assert that they act more as a lobbying platform, wherein specific interest groups attempt to influence governments.\n\nActors inside and outside the United Nations are discussing possibilities for global environmental governance that provides a solution to current problems of fragility, coordination and coherence. Deliberation is focusing on the goal of making UNEP more efficient. A 2005 resolution recognizes “the need for more efficient environmental activities in the United Nations system, with enhanced coordination, improved policy advice and guidance, strengthened scientific knowledge, assessment and cooperation, better treaty compliance, while respecting the legal autonomy of the treaties, and better integration of environmental activities in the broader sustainable development framework.”\n\nProposals include:\n\n\nOne of the main studies addressing this issue proposes:\n\n\nA 2001 Alliance 21 report proposes six fields of action:\n\n\nIndividuals can modify consumption, based on voluntary simplicity: changes in purchasing habits, simplified lifestyles (less work, less consumption, more socialization and constructive leisure time). But individual actions must not replace vigilance and pressure on policies. Notions of responsible consumption developed over decades, revealing the political nature of individual purchases, according to the principle that consumption should satisfy the population’s basic needs. These needs comprise the physical wellbeing of individuals and society, a healthy diet, access to drinking water and plumbing, education, healthcare and physical safety. The general attitude centres on the need to reduce consumption and reuse and recycle materials. In the case of food consumption, local, organic and fair trade products which avoid ill treatment of animals has become a major trend.\n\nAlternatives to the personal automobile are increasing, including public transport, car sharing and bicycles and alternative propulsion systems.\n\nAlternative energy sources are becoming less costly.\n\nEcological industrial processes turn the waste from one industry into raw materials for another.\n\nGovernments can reduce subsidies/increase taxes/tighten regulation on unsustainable activities.\n\nThe Community Environmental governance Global Alliance encourages holistic approaches to environmental and economic challenges, incorporating indigenous knowledge. Okotoks, Alberta capped population growth based on the carrying capacity of the Sheep River. The Fraser Basin Council Watershed Governance in British Columbia, Canada, manages issues that span municipal jurisdictions. Smart Growth is an international movement that employs key tenets of Environmental governance in urban planning.\n\nEstablish policies and regulations that promote “infrastructures for well being” whilst addressing the political, physical and cultural levels.\n\nEliminate subsidies that have a negative environmental impact and tax pollution\n\nPromoting workers’ personal and family development.\n\nA programme of national workshops on synergies between the three Rio Conventions launched in late 2000, in collaboration with the relevant secretariats. The goal was to strengthen coordination at the local level by:\n\n\nAccording to Campbell, “In the context of globalization, the question of linking up environmental themes with other subjects, such as trade, investment and conflict resolution mechanisms, as well as the economic incentives to participate in and apply agreements would seem to provide an important lesson for the effective development of environmental governance structures.” Environmental concerns would become part of the global economic system. “These problems also contain the seeds of a new generation of international conflicts that could affect both the stability of international relations and collective security. Which is why the concept of ‘collective security’ has arisen.”\n\nMoving local decisions to the global level is as important as the way in which local initiatives and best practices are part of a global system. Kanie points out that NGOs, scientists, international institutions and stakeholder partnerships can reduce the distance that separates the local and international levels.\n\n"}
{"id": "9683515", "url": "https://en.wikipedia.org/wiki?curid=9683515", "title": "Epistemic commitment", "text": "Epistemic commitment\n\nEpistemic commitment is an obligation, which may be withdrawn only under appropriate circumstances, to uphold the factual truth of a given proposition, and to provide reasons for one's belief in that proposition.\nEpistemic means 'of, or relating to knowledge'. An epistemic commitment of some kind, on the part of the participants, underlies most arguments. For instance, each participant in an argument would have a position that they are expressing, and an underlying epistemic commitment fundamental to their reasoning.\n\nJoe states that whales are gentle creatures, and as such, should never be killed.\n\nSusan replies that killer whales are not 'gentle', in fact, they eat seal pups.\n\nJoe, instead of revising his earlier stated opinion of whales based on new information from Susan, asserts that killer whales must not actually be whales, because a 'true whale eats plankton.' Joe has now redefined 'whale' to suit his argument.\n\nIf Susan shows Joe a biology textbook which asserts that killer whales are, in fact whales, then Joe might (or might not) decide to withdraw his epistemic commitment to the original statement about whales.\n\n"}
{"id": "41443085", "url": "https://en.wikipedia.org/wiki?curid=41443085", "title": "External image", "text": "External image\n\nIn psychology, the external image (also alien image, foreign image, public image, third-party image; ) is the image other people have of a person, i.e., a person's external image is the way they are viewed by other people. It contrasts with a person's self image (); how the external image is communicated to a person may affect their self esteem positively or negatively.\n\nAn external image is the totality of all perceptions, feelings, and judgments that third parties make about an individual. These interpersonal perceptions are automatically linked to earlier experiences with the person being observed, and with the feelings arising from these interactions and evaluations. The image that others have of a person shapes their expectations of this person, and significantly affects their mutual social interaction.\n\nA person's external image, or more precisely, how this image is communicated to the individual, and how others react to the individual as a result of his or her external image, significantly affects the person's self image. Positive, appreciative external images strengthen an individual's self confidence and self esteem. In extreme cases, negative or conflicting external images can cause mental illness.\n\nThe external image is always different from an individual's self-image. From the two perspectives and the differences between them - or more accurately, the inferences that the two parties draw for themselves, social interactions evolve, influenced by the parties' own selves.\n\nConscious handling of images about each other plays an important part in group dynamics. In feedback exercises, subjects are trained in giving and receiving external images. The Johari window describes the relationship between external and self images, and that between conscious and unconscious parts of these images. With mindful \"awareness exercises\", a person is trained to detect previously unconscious expectations of third parties, and with communication exercises, they are trained to reconcile their own and others' images and expectations of each other.\n\nPsychotherapy also deals with external images when treating depression or in dealing with the effects of trauma or bullying, or more generally in counseling members of marginalized groups.\n\n"}
{"id": "528180", "url": "https://en.wikipedia.org/wiki?curid=528180", "title": "Freeter", "text": "Freeter\n\nFreeters may also be described as \"underemployed\". These people do not start a career after high school or university, but instead earn money from low-skilled and low-paid jobs.\n\nThe word \"freeter\" or \"freeta\" was first used around 1987 or 1988 and is thought to be a portmanteau of the English word \"free\" (or perhaps \"freelance\") and the German word \"Arbeiter\" (\"labourer\"). \"Arubaito\" is a Japanese loanword from \"Arbeiter\", and perhaps from \"Arbeit\" (\"work\"). As German (along with English) was used in Japanese universities before World War II, especially for science and medicine, \"arubaito\" became common among students to describe part-time work for university students.\n\nAbout 10% of high school and university graduates could not find steady employment in the spring of 2000, and a full 50% of those who could find a job left within three years after employment. The employment situation is worse for the youngest freeters.\n\nFrom 2000–2009, the number of freeters increased rapidly. In 1982 there were an estimated 0.5 million freeters in Japan, 0.8 million in 1987, 1.01 million in 1992 and 1.5 million in 1997. The number for 2001 is 4.17 million freeters according to one estimate, and 2 million in 2002 according to another estimate. According to some estimates, there will be 10 million freeters in Japan by 2014. \n\nMany Japanese people worry about the future impact of freeters on society. If they work at all, freeters often work at convenience stores, supermarkets, fast food outlets, restaurants, and other low paying, low skill jobs. According to a survey by the Japan Institute of Labor in 2000, the average freeter works 4.9 days per week and earns ¥139,000 per month (ca. $1,300 U.S.). Two thirds of freeters have never had a regular, full-time job.\n\nThe rise of internet business has allowed some freeters to work from home and be self-employed. Some experts predict that Japan's aging population will create a labor shortage that will increase career options for freeters.\n\nThe Japan Institute of Labor classifies freeters into three groups: the \"\"moratorium\" type\" that wants to wait before starting a career, the \"\"dream pursuing\" type\", and the \"\"no alternative\" type\". \n\nMany freeters live for free with their parents as parasite singles. Parents in Japan usually do not force their offspring out of the house. Once the parents die, the children will have to pay for their housing themselves. Even if they inherit the house or apartment, they still have to bear the costs of ownership.\n\nJapanese housing is compact, and is too small for two families. If freeters want to marry they have to find their own housing, usually at their own expense. \n\nStarting a career becomes more difficult the longer somebody is a freeter, as Japanese companies prefer to hire new workers fresh out of high school or university. While the employment situation is changing, large traditional companies still see a new employee as a lifetime investment. They much prefer to hire a young person who offers a longer period of service, and who will be easier to mold.\n\nOften the only option left for freeters is to continue working at low income part-time jobs, making it difficult to establish their own household. Some join the many homeless in Japan.\n\nPart-time jobs usually do not include any health or retirement benefits. Freeters' low income makes payment of medical expenses onerous.\n\nThe biggest problem for freeters is that the Japanese pension system is based on the number of years a person has paid into the system. The freeter usually has little or no pension insurance or savings, which may force him or her to work beyond the usual retirement age. \n\nJapan faces the problem of an aging population. The pension system will be under increasing strain as the ratio of pensioners to workers increases.\n\nThe advantage of being a freeter is that one has more freedom of choice, and more time for hobbies, volunteering, and community service. If they are living with their parents, they can spend their entire income on themselves.\n\nFreeters lack the benefits of union membership, which would give them some legal protection against firing.\n\nWhile they are young, freeters commonly live with their parents and have disposable income that would otherwise go towards rent. Their spending helps the manufacturing sector of the Japanese economy.\n\nBy living in the same house as their parents and often not owning a car, freeters have a much lower impact on the natural environment than \"high consumption\" members of society owning cars.\n\nLarge numbers of workers trying to start careers in their thirties may have a significant impact on the current corporate culture of Japan. It may change hiring and employment practices, particularly since demographers predict a future labor shortage due to the nation's aging population.\n\nMany male freeters have difficulties marrying because of their low income. They may thus have children later in life, or not at all. This will further inflame the low birth rate in Japan and compound social and economic problems related to the aging population, such as underfunding of the Japanese pension system. As of today, freeters pay little or no money into the pension system. The situation could become even worse in the future as more people become freeters, the ratio of workers to pensioners decreases, and the current pension policies around the world stay the same.\n\nThe Japanese government has established a number of offices called \"Young Support Plaza\" to help young people find jobs. These offices offer basic training for job hunting: teaching young people how to write a résumé, and how to conduct themselves during interviews. The demand for their services has been fairly low so far.\n\n"}
{"id": "13737492", "url": "https://en.wikipedia.org/wiki?curid=13737492", "title": "Frequent deliveries", "text": "Frequent deliveries\n\nFrequent deliveries are a largely ignored but powerful way of leveling apparent demand within a supply chain and thereby reducing Mura. What is perhaps not so obvious is that this will reduce inventory levels and thereby assist progress along the Lean journey at the same time. The historical focus upon carrying full loads, sometimes of only one product, reduces the unit cost of transport but has sometimes hidden the true costs associated with achieving those transport discounts. It is also possible to gain some of these benefits by 'faking' frequent deliveries at the supply site.\n\nIf we model this idea using a factory that produces three products (Triangles, Circles and Squares) and is making a regular daily delivery to its customer at the end of each day then we can represent this as below.\n\nStock builds up during the day until the factory has completed the production campaign of three products each of which is produced in a lot of four. After each lot of four a line-changeover happens, there are in fact three as we must include the one before the first production lot. Stock builds here to a maximum of twelve units. If the number of changeovers were doubled to six then the stock levels would remain the same same all the stock is still on site until the shipment.\n\nThe customer is receiving a mix of products that it is likely that they consume (or their in-stock equivalent) during, say, the next day. If the customer consumes the provided products during the day then their stock level will decrease by twelve over the day.\n\nSo if the customer will agree to receive half the daily shipment of each product halfway through the day and the second half at the original time of end of day and we reduce lot sizes by a factor of two the factory schedule would look like this.\n\nWe now see that the benefits of the lot size reduction in the stock levels, both at the customer and at the supplying factory. Stock at both locations has been reduced by six. A possible downside for the factory is that it now has twice the changeovers (See Single-Minute Exchange of Die (SMED)).\n\nIf we take this to the extreme where the factory now has a changeover after every unit of production (single piece flow) and where shipments occur after every campaign (every product has been made) then the factory has this situation.\n\nNow stock levels, again at the customer and supplier, are down to three from twelve. To achieve this cost effectively the changeovers must be very quick.\n\nSo starting from the same original situation.\n\nThe factory chooses to just make the deliveries more frequent but not change production lot size, and to keep some extra stock so that deliveries can be exactly as in the model where lot sizes were changed as well.\n\nHere, although an extra stock of two is constantly being held it can be seen that the deliveries still reduce the holding by six giving a net benefit of four at the factory and six at the customer. This is without changing the production schedule.\n\nIf, again, this is taken to the extreme where deliveries are going to be made of one unit of each product and stock keeping adjusted to make this possible then this situation is seen.\n\nHere, although an extra stock of three is constantly being held it can be seen that the deliveries still reduce the holding by nine giving a net benefit of six at the factory and nine at the customer. This is still without changing the production schedule.\n\nSo from this example it can be seen that just increasing delivery frequency reduces the stock held in the system. This is no surprise to those in the context of station to station within a factory. It does seem to surprise many when used in the context of supplier to customer. The summary of this argument is in the table below.\n\nSo now the question is how to achieve this more frequent delivery. Well in fact many of the benefits within the factory can be achieved by 'faking' frequent deliveries while discussions with the customer about actual delivery frequencies takes place.\n\nThe removal of items in the factory from the 'manufacturing system' will trigger the resupply that we wish to smooth via kanban or other signals. The frequent deliveries will provide a smoother sequence of smaller resupply signals. So by 'faking it' what is meant is that the actual delivery schedule will be de-coupled from the resupply triggers in the factory.\n\nThis can be done by marking a position on the floor, say a rectangular outline of perhaps the same size as the truck, in the loading bay and designating it to be a specific planned delivery or part of one. Let's call that outline a 'virtual truck'. Clearly if all the items for the delivery were now loaded into the virtual truck then the impact on demand signals into the factory would be the same as a real truck load. The secret here is to schedule a steady flow of items from the factory into the virtual truck so that demand appears as flat as possible. Obviously this may seem like 'smoke and mirrors' since the goods are still actually in the loading bay. The importance is that demand and supply are now decoupled. So whilst a real truck can still be loaded at the required speed, from the virtual truck, the signals for resupply passing via kanbans etc. back into the factory have created a smooth demand. This method can also be used to give early warning to the factory that it is falling behind the required schedule if it is to have all goods ready for shipment when the real truck arrives.\n\nThe downside of this trick is that there are now two movements of the goods, one to the virtual truck and one from it to the real truck.\n\nSince these need to meet agreement with the customer these will be less flexible than the 'virtual truck's' unless they are part of an internal process. Between 1982 and 1990 Toyota reorganised its service and crash parts business and as part of that it established Local Distribution Centres (LDCs) in each metropolitan centre. It also encouraged dealers to work intensively with customers so that maintenance was scheduled sufficiently in advance that parts requirements could be precisely predicted.\n\nBecause the LDCs are so close to the dealers it was possible to establish a 'milk run' which visited every dealer every two hours. So when the service is booked a prelimiinary order is prepared for the required parts. The day before the scheduled service the customer is called to confirm the service and then a firm order to the LDC is placed for delivery on the next 'milk run'. Finally, when the car arrives for its service it is inspected and any other required parts ordered for delivery with 2–4 hours (the next run). This has resulted in very significant stock reductions throughout the system as the table below illustrates.\n"}
{"id": "39478592", "url": "https://en.wikipedia.org/wiki?curid=39478592", "title": "Function block diagram", "text": "Function block diagram\n\nThe Function Block Diagram (FBD) is a graphical language for programmable logic controller design, that can describe the function between input variables and output variables. A function is described as a set of elementary blocks. Input and output variables are connected to blocks by connection lines.\n\nInputs and outputs of the blocks are wired together with connection lines, or links. Single lines may be used to connect two logical points of the diagram:\n\nThe connection is oriented, meaning that the line carries associated data from the left end to the right end. The left and right ends of the connection line must be of the same type.\n\nMultiple right connection, also called divergence can be used to broadcast information from its left end to each of its right ends. All ends of the connection must be of the same type.\n\nFunction Block Diagram is one of five languages for logic or control configuration supported by standard IEC 61131-3 for a control system such as a Programmable Logic Controller (PLC) or a Distributed Control System (DCS). The other supported languages are ladder logic, sequential function chart, structured text, and instruction list.\n\n\n"}
{"id": "54714897", "url": "https://en.wikipedia.org/wiki?curid=54714897", "title": "Gao Minglu", "text": "Gao Minglu\n\nGao Minglu (born 29 October 1949) is a scholar in Chinese contemporary art. He is the Chair of the Department of Art History, Professor for Distinguished Service, and Chair of Art and is an instructor at the University of Pittsburgh University of Pittsburgh. He is also distinguished professor at Tianjin Academy of Fine Arts.\n\nGao was the Chair of the Department of Art History and Professor for Distinguished Service at Sichuan Fine Art Institute, China. His works focus on the influence and nature of Chinese art.\n\nMinglu was born in Tianjin, China in October 1949. One of his exhibitions in China was shut down by the Chinese government in 1989 after only a few hours. In 1985, \n\nMinglu graduated from the China National Academy of Art. In the same year he worked as an editor of one of China’s art magazines. Minglu serves as professor, artist, curator and art critic. During the cultural revolution \"Up to the Mountains and Down to the Countryside Movement\" he traveled to Inner Mongolia and worked as a herdsman for five years. After the Cultural Revolution, he attended the Tianjin Academy of Fine Arts to study art history. In 1982 he attended the Graduate School of Chinese Art Research Institute where he received his master's degree. From 1984–1989 he was the editor of \"Art\" magazine. While there, he regularly wrote about Chinese Art news. He was forced to study Marxism–Leninism in China from 1989–1991.\n\nHe planned a Chinese avant-garde art exhibition entitled \"Inside Out: New Chinese from 1995 to 1998. During the same time He was showing his work in the \"Global Concept Art Exhibition from 1950s to 1980s\". He also participated in the exhibition of the \"Global Concept Art Exhibition from 1950s to 1980s\", In 1999 he showed his work in the \"Five Continents and a City\" exhibition in Mexico in and was one of the curators from China. During this same time, Minglu wrote an English monograph describing the exhibitions. He wrote other papers on Chinese art.\n\nGao was awarded the American Academy of Sciences \"American Academic Exchange Committee\" postdoctoral project scholarship in 1991. This allowed him to come to the US to research and study at Ohio University and Harvard University. He received his doctorate in the history of art research. He graduated from Harvard University.\n\nDuring this same time his criticism research and criticism perspectives focused on the present and contemporary art of China, Taiwan and Hong Kong. These studies included comparative research on modernism and avant-garde in China and the West. His work has been shown in a number of shows and exhibitions that include:\n\nHe has authored papers and books. Some titles include:\n\nDuring his time as editor of \"Art\" Magazine, Gao was heavily involved in the Chinese 85 New Wave art movement. He acted as the planner, curator, and critic. He directed the 1989 \"China/Avant-Garde Exhibition\", the first and only large gathering of avant-garde artists at the National Art Museum of China, as well as other avant-garde art exhibits.\n\nThe University of Pittsburgh University Library System holds Minglu's extensive archive of Chinese contemporary art, manuscripts, paintings, slides, posters, recording materials.\n\n"}
{"id": "18272728", "url": "https://en.wikipedia.org/wiki?curid=18272728", "title": "Gravity tractor", "text": "Gravity tractor\n\nA gravity tractor is a theoretical spacecraft that would deflect another object in space, typically a potentially hazardous asteroid that might impact Earth, without physically contacting it, using only its gravitational field to transmit the required impulse. The gravitational force of a nearby space vehicle, though minuscule, is able to alter the trajectory of a much larger asteroid if the vehicle spends enough time close to it; all that is required is that the vehicle thrust in a consistent direction relative to the asteroid's trajectory, and that neither the vehicle nor its expelled reaction mass come in direct contact with the asteroid. The tractor spacecraft could either hover near the object being deflected, or orbit it, directing its exhaust perpendicular to the plane of the orbit. The concept has two key advantages: namely that essentially nothing needs to be known about the mechanical composition and structure of the asteroid in advance; and that the relatively small amounts of force used enable extremely precise manipulation and determination of the asteroid's orbit around the sun. Whereas other methods of deflection would require the determination of the asteroid's exact center of mass, and considerable effort might be necessary to halt its spin or rotation, by using the tractor method these considerations are irrelevant.\n\nA number of considerations arise concerning means for avoiding a devastating collision with an asteroidal object, should one be discovered on a trajectory that were determined to lead to Earth impact at some future date. One of the main challenges is how to transmit the impulse required (possibly quite large), to an asteroid of unknown mass, composition, and mechanical strength, without shattering it into fragments, some of which might be themselves dangerous to Earth if left in a collision orbit.\nThe gravity tractor solves this problem by gently accelerating the object as a whole over an extended period of time, using the spacecraft's own mass and associated gravitational field to effect the necessary deflecting force.\nBecause of the universality of gravitation, affecting as it does all mass alike, the asteroid would be accelerated almost uniformly as a whole, with only tidal forces (which should be extremely small) causing any stresses to its internal structure.\n\nA further advantage is that a transponder on the spacecraft, by continuously monitoring the position and velocity of the tractor/asteroid system, could enable the post-deflection trajectory of the asteroid to be accurately known, ensuring its final placement into a safe orbit.\n\nLimitations of the tractor concept include the exhaust configuration. With the most efficient hovering design (that is, pointing the exhaust directly at the target object for maximum force per unit of fuel), the expelled reaction mass hits the target head-on, imparting a force in exactly the opposite direction to the gravitational pull of the tractor. It would therefore be necessary to use the orbiting-tractor scheme described below, or else design the hovering tractor so that its exhaust is directed at a slight angle away from the object, while still pointing \"down\" enough to keep a steady hover. This requires greater thrust and correspondingly increased fuel consumption for each metre per second change in the target's velocity.\n\nIssues of the effect of ion propulsion thrust on the dust of asteroids have been raised, suggesting that alternative means to control the station keeping position of the gravity tractor may need to be considered. In this respect, solar sails have been suggested.\n\nAccording to Rusty Schweickart, the gravitational tractor method is also controversial because during the process of changing an asteroid's trajectory the point on Earth where it could most likely hit would be slowly shifted across different countries. It means that the threat for the entire planet would be minimized at the cost of some specific states' security. In Schweickart's opinion, choosing the way the asteroid should be \"dragged\" would be a tough diplomatic decision.\n\nTo get a feel for the magnitude of these issues, let us suppose that a NEO of size around 100 m, and mass of one million metric tons, threatened to impact Earth. Suppose also that\n\n\nWith these parameters, the required impulse would be: \"V\" × \"M\"  = 0.01 [m/s]×10 [kg] = 10 [N-s], so that the average tractor force on the asteroid for 10 years (which is 3.156×10 seconds), would need to be about 0.032 newtons.\nAn ion-electric spacecraft with a specific impulse of 10,000 N-s per kg, corresponding to an ion beam velocity of 10 kilometres per second (about 20 times that obtained with the best chemical rockets), would require 1,000 kg of reaction mass (xenon is currently favored) to provide the impulse.\nThe kinetic power of the ion beam would then be approximately 158 watts; the input electric power to the power converter and ion drive would of course be substantially higher.\nThe spacecraft would need to have enough mass and remain sufficiently close to the asteroid that the component of the average gravitational force on the asteroid in the desired direction would equal or exceed the required 0.032 newtons.\nAssuming the spacecraft is hovering over the asteroid at a distance of 200 m to its centre of mass, that would\nrequire it to have a mass of about 20 metric tonnes, because due to the gravitational force we\nhave\n\nformula_1\n\nConsidering possible hovering positions or orbits of the tractor around the asteroid, note that if two objects are gravitationally bound in a mutual orbit, then if one receives an arbitrary impulse which is less than that needed to free it from orbit around the other, because of the gravitational forces between them, the impulse will alter the momentum of both, together regarded as a composite system.\nThat is, so long as the tractor remains in a bound orbit, any propulsive force applied to it will be effectively transferred to the asteroid it orbits.\nThis permits a wide variety of orbits or hovering strategies for the tractor.\nOne obvious possibility is for the spacecraft to orbit the NEO with the normal to the orbit in the direction of the desired force.\nThe ion beam would then be directed in the opposite direction, also perpendicular to the orbit plane. This would result in the plane of the orbit being shifted somewhat away from the center of the asteroid, \"towing\" it, while the orbital velocity, normal to the thrust, remains constant. The orbital period would be a few hours, essentially independent of size, but weakly dependent on the density of the target body.\n\n"}
{"id": "7985928", "url": "https://en.wikipedia.org/wiki?curid=7985928", "title": "Jean Broke-Smith", "text": "Jean Broke-Smith\n\nJean Broke-Smith is an English etiquette, deportment and grooming teacher. She was the principal of the Lucie Clayton School of grooming and modelling for thirty years, during which time she supervised a curriculum that included etiquette and deportment. Graduates of the school include actress Joanna Lumley. Prior to that she trained as fashion designer, milliner and make-up specialist, and herself was a model. She is a regular expert contributor for TV, radio, newspapers and often appears on live news programmes and is interviewed on live radio 'link-ups'.\n\nIn recent years Broke-Smith has appeared in a series of television programmes on the subject of manners. She was 'head-teacher' in \"Ladette to Lady\", a reality television programme that emulated the curriculum of a finishing school; she helped struggling bed and breakfast owners fix up their businesses on BBC1's \"B&B the Best\" (which is currently syndicated in the United States on Vibrant TV Network); she was the etiquette expert on \"The Family\", another reality-television show in which a large extended family were sent to live in a large mansion house with a full domestic staff; she was one of the judges of the \"Australia Princess\" series, and the original NBC series \"American Princess\", which set out to teach young women how to conduct themselves like princesses and make them fit to marry a prince. She appeared on \"Faking It\", teaching a girl from the north of England to present herself as a society 'lady'; in \"Snobs\"; and as an etiquette expert on \"Britain's Next Top Model\".\n\n"}
{"id": "16862", "url": "https://en.wikipedia.org/wiki?curid=16862", "title": "Kōan", "text": "Kōan\n\nA (; , ; \"gong-an\"; ) is a story, dialogue, question, or statement which is used in Zen practice to provoke the \"great doubt\" and test a student's progress in Zen practice.\n\nThe Japanese term \"kōan\" is the Sino-Japanese reading of the Chinese word \"gong'an\" (). The term is a compound word, consisting of the characters \"public; official; governmental; common; collective; fair; equitable\" and \"table; desk; (law) case; record; file; plan; proposal.\"\n\nAccording to the Yuan Dynasty Zen master Zhongfeng Mingben ( 1263–1323), \"gōng'àn\" originated as an abbreviation of \"gōngfǔ zhī àndú\" (, Japanese \"kōfu no antoku\"—literally the \"andu\" \"official correspondence; documents; files\" of a \"gongfu\" \"government post\"), which referred to a \"public record\" or the \"case records of a public law court\" in Tang-dynasty China. \"Kōan/gong'an\" thus serves as a metaphor for principles of reality beyond the private opinion of one person, and a teacher may test the student's ability to recognize and understand that principle.\n\nCommentaries in kōan collections bear some similarity to judicial decisions that cite and sometimes modify precedents. An article by T. Griffith Foulk claims\n\"Gong'an\" was itself originally a metonym—an article of furniture involved in setting legal precedents came to stand for such precedents. For example, \"Di Gong'an\" () is the original title of \"Celebrated Cases of Judge Dee\", the famous Chinese detective novel based on a historical Tang dynasty judge. Similarly, Zen kōan collections are public records of the notable sayings and actions of Zen masters and disciples attempting to pass on their teachings.\n\nGong'ans developed during the Tang dynasty (618–907) from the recorded sayings collections of Chán-masters, which quoted many stories of \"a famous past Chán figure's encounter with disciples or other interlocutors and then offering his own comment on it\". Those stories and the accompanying comments were used to educate students, and broaden their insight into the Buddhist teachings.\n\nThose stories came to be known as gongan, \"public cases\". Such a story was only considered a gongan when it was commented upon by another Chán-master. This practice of commenting on the words and deeds of past masters confirmed the master's position as an awakened master in a lineage of awakened masters of the past.\n\nKoan practice developed from a literary practice, styling snippets of encounter-dialogue into well-edited stories. It arose in interaction with \"educated literati\". There were dangers involved in such a literary approach, such as ascribing specific meanings to the cases. Dahui Zonggao is even said to have burned the woodblocks of the \"Blue Cliff Record\", for the hindrance it had become to study of Chán by his students. Kōan literature was also influenced by the pre-Zen Chinese tradition of the \"literary game\"—a competition involving improvised poetry.\n\nThe style of writing of Zen texts has been influenced by \"a variety of east Asian literary games\":\nDuring the Song dynasty (960–1297) the use of gongans took a decisive turn. Dahui Zonggao (1089–1163) introduced the use of \"kanhua\", \"observing the phrase\". In this practice students were to observe (\"kan\") or concentrate on a single word or phrase (\"huatou\"), such as the famous \"mu\" of the mu-koan.\n\nIn the eleventh century this practice had become common. A new literary genre developed from this tradition as well. Collections of such commented cases were compiled which consisted of the case itself, accompanied by verse or prose commentary.\n\nDahui's invention was aimed at balancing the insight developed by reflection on the teachings with developing samatha, calmness of mind. Ironically, this development became in effect \"silent illumination\", a \"[re-absorbing] of koan-study into the \"silence\" of meditation (\"ch'an\")\". It led to a rejection of Buddhist learning:\nThis development left Chinese Chan vulnerable to criticisms by neo-Confucianism, which developed after the Sung Dynasty. Its anti-intellectual rhetoric was no match for the intellectual discourse of the neo-Confucianists.\n\nThe recorded encounter dialogues, and the koan collections which derived from this genre, mark a shift from solitary practice to interaction between master and student:\nThis mutual enquiry of the meaning of the encounters of masters and students of the past gave students a role model:\nKōan training requires a qualified teacher who has the ability to judge a disciple's depth of attainment. In the Rinzai Zen school, which uses kōans extensively, the teacher certification process includes an appraisal of proficiency in using that school's extensive kōan curriculum.\n\nIn China and Korea, \"observing the phrase\" is still the sole form of koan-practice, though Seung Sahn used the Rinzai-style of koan-practice in his Kwan Um School of Zen.\n\nJapanese Zen, both Rinzai and Sōtō, took over the use of koan-study and commenting. In Sōtō-Zen, koan commentary was not linked to seated meditation.\n\nWhen the Chán-tradition was introduced in Japan, Japanese monks had to master the Chinese language and specific expressions used in the koan-training. The desired \"spontaneity\" expressed by enlightened masters required a thorough study of Chinese language and poetry. Japanese Zen imitated the Chinese \"syntax and stereotyped norms\".\n\nIn the officially recognized monasteries belonging to the Gozan (Five Mountain System) the Chinese system was fully continued. Senior monks were supposed to compose Chinese verse in a complex style of matched counterpoints known as \"bienli wen\". It took a lot of literary and intellectual skills for a monk to succeed in this system.\n\nThe Rinka-monasteries, the provincial temples with less control of the state, laid less stress on the correct command of the Chinese cultural idiom. These monasteries developed \"more accessible methods of koan instruction\". It had three features:\n\nBy standardizing the koan-curriculum every generation of students proceeded to the same series of koans. Students had to memorize a set number of stereotyped sayings, \"agyō\", \"appended words\". The proper series of responses for each koan were taught by the master in private instruction-sessions to selected individual students who would inherit the dharma lineage.\n\n\"Missanroku\" and \"missanchō\", \"Records of secret instruction\" have been preserved for various Rinzai-lineages. They contain both the koan-curricula and the standardized answers. In Sōtō-Zen they are called \"monsan\", an abbreviation of \"monto hissan\", \"secret instructions of the lineage\". The \"monsan\" follow a standard question-and-answer format. A series of questions is given, to be asked by the master. The answers are also given by the master, to be memorized by the student.\n\nIn the eighteenth century the Rinzai school became dominated by the legacy of Hakuin, who laid a strong emphasis on koan study as a means to gain kensho and develop insight. There are two curricula used in Rinzai, both derived from the principal heirs of Rinzai: the Takuju curriculum, and the Inzan curriculum. According to AMA Samy, \"the koans and their standard answers are fixed.\"\n\nDuring the late eighteenth and nineteenth century the tradition of koan-commentary became suppressed in the Sōtō-school, due to a reform movement that sought to standardise the procedures for dharma transmission. One reason for suppressing the koan-tradition in the Sōtō-school may have been to highlight the differences with the Rinzai-school, and create a clear identity. This movement also started to venerate Dogen as the founding teacher of the Sōtō-school. His teachings became the standard for the Sōtō-teachings, neglecting the fact that Dogen himself made extensive use of koan-commentary.\n\nThe popular western understanding sees \"kōan\" as referring to an unanswerable question or a meaningless statement. However, in Zen practice, a kōan is not meaningless, and not a riddle or a puzzle. Teachers do expect students to present an appropriate response when asked about a kōan. \n\nKoans are also understood as pointers to an unmediated \"Pure Consciousness\", devoid of cognitive activity. Victor Hori criticizes this understanding:\nAccording to Hori, a central theme of many koans is the 'identity of opposites':\nComparable statements are: \"Look at the flower and the flower also looks\"; \"Guest and host interchange\".\n\nStudy of kōan literature is common to all schools of Zen, though with varying emphases and curricula. The Rinzai-school uses extensive koan-curricula, checking questions, and \"jakogo\" (\"capping phrases\", quotations from Chinese poetry) in its use of koans. The Sanbo Kyodan, and its western derivates of Taizan Maezumi and the White Plum Asanga, also use koan-curricula, but have omitted the use of capping phrases. In Chinese Chán and Korean Seon, the emphasis is on Hua Tou, the study of one koan throughout one's lifetime. In Japanese Sōtō-Zen, the use of koans has been abandoned since the late eighteenth and nineteenth century.\n\nIn the Rinzai-school, the Sanbo Kyodan, and the White Plum Asanga, koan practice starts with the assignment of a \"hosshi\" or \"break-through koan\", usually the mu-koan or \"the sound of one hand clapping\". In Chinese Chán and Korean Seon, various koan can be used for the hua-tou practice.\n\nStudents are instructed to concentrate on the \"word-head\", like the phrase \"mu\". In the Wumenguan (Mumonkan), public case #1 (\"Zhaozhou's Dog\"), Wumen (Mumon) wrote:\nArousing this great inquiry or \"Great Doubt\" is an essential element of kōan practice. It builds up \"strong internal pressure (\"gidan\"), never stopping knocking from within at the door of [the] mind, demanding to be resolved\". To illustrate the enormous concentration required in kōan meditation, Zen Master Wumen commented,\nAnalysing the koan for its literal meaning won't lead to insight, though understanding the context from which koans emerged can make them more intelligible. For example, when a monk asked Zhaozhou (Joshu) \"does a dog have Buddha-nature or not?\", the monk was referring to the understanding of the teachings on Buddha-nature, which were understood in the Chinese context of absolute and relative reality.\n\nThe continuous pondering of the break-through koan (\"shokan\") or Hua Tou, \"word head\", leads to kensho, an initial insight into \"seeing the (Buddha-)nature.\n\nThe aim of the break-through koan is to see the \"nonduality of subject and object\":\nVarious accounts can be found which describe this \"becoming one\" and the resulting breakthrough:\nBut the use of the mu-koan has also been criticised. According to AMA Samy, the main aim is merely to \"'become one' with the koan\". Showing to have 'become one' with the first koan is enough to pass the first koan. According to Samy, this is not equal to prajna:\n\nTeachers may probe students about their kōan practice using \"sassho\", \"checking questions\" to validate their satori (understanding) or kensho (seeing the nature). For the \"mu-koan\" and the \"clapping hand-koan\" there are twenty to a hundred checking questions, depending on the teaching lineage. The checking questions serve to deepen the insight of the student, but also to test his or her understanding.\n\nThose checking questions, and their answers, are part of a standardised set of questions and answers. Students are learning a \"ritual performance\", learning how to behave and response in specific ways, learning \"clever repartees, ritualized language and gestures and be submissive to the master’s diktat and arbitration.\"\n\nIn the Rinzai-school, passing a koan and the checking questions has to be supplemented by \"jakugo\", \"capping phrases\", citations of Chinese poetry to demonstrate the insight. Students can use collections of those citations, instead of composing poetry themselves.\n\nAfter the initial insight further practice is necessary, to deepen the insight and learn to express it in daily life. In Chinese Chán and Korean Seon, this further practice consists of further pondering of the same Hua Tou. In Rinzai-Zen, this further practice is undertaken by further koan-study, for which elaborate curriculae exist. In Sōtō-Zen, Shikantaza is the main practice for deepening insight.\n\nIn Chinese Chán and Korean Seon, the primary form of Koan-study is \"kanhua\", \"reflection on the koan\", also called Hua Tou, \"word head\". In this practice, a fragment of the koan, such as \"mu\", or a \"what is\"-question is used by focusing on this fragment and repeating it over and over again:\nThe student is assigned only one hua-tou for a lifetime. In contrast to the similar-sounding \"who am I?\" question of Ramana Maharshi, hua-tou involves raising \"great doubt\":\nKōan practice is particularly important among Japanese practitioners of the Rinzai sect.\n\nThis importance is reflected in writings in the Rinzai-school on the koan-genre. Zhongfeng Mingben (1263–1323), a Chinese Chán-master who lived at the beginning of the Yuan Dynasty, revitalized the Rinzai-tradition, and put a strong emphasis on the use of koans. He saw the kung-ans as \"work of literature [that] should be used as objective, universal standards to test the insight of monks who aspired to be recognized as Ch'an masters\":\nMusō Soseki (1275–1351), a Japanese contemporary of Zhongfeng Mingben, relativized the use of koans. The study of koans had become popular in Japan, due to the influence of Chinese masters such as Zhongfeng Mingben. Despite belonging to the Rinzai-school, Musō Soseki also made extensive use of \"richi\" (teaching), explaining the sutras, instead of \"kikan\" (koan). According to Musō Soseki, both are \"upaya\", \"skillful means\" meant to educate students. Musō Soseki called both \"shōkogyu\", \"little jewels\", tools to help the student to attain satori.\n\nIn Rinzai a gradual succession of koans is studied. There are two general branches of curricula used within Rinzai, derived from the principal heirs of Rinzai: the Takuju curriculum, and the Inzan curriculum. However, there are a number of sub-branches of these, and additional variations of curriculum often exist between individual teaching lines which can reflect the recorded experiences of a particular lineage's members. Koan curricula are, in fact, subject to continued accretion and evolution over time, and thus are best considered living traditions of practice rather than set programs of study.\n\nKoan practice starts with the \"shokan\", or \"first barrier\", usually the mu-koan or the koan \"What is the sound of one hand clapping?\" After having attained \"kensho\", students continue their practice investigating subsequent koans. In the Takuju-school, after breakthrough students work through the \"Gateless Gate\" (Mumonkan), the \"Blue Cliff Record\" (Hekigan-roku), the \"Entangling Vines\" (Shumon Kattoshu), and the \"Collection of Wings of the Blackbird\" (, \"Chin'u shū\"). The Inzan-school uses its own internally generated list of koans.\n\nHakuin's descendants developed a fivefold classification system:\n\nAccording to Akizuki there was an older classification-system, in which the fifth category was \"Kojo\", \"Directed upwards\". This category too was meant to rid the monk of any \"stink of Zen\". The very advanced practitioner may also receive the \"Matsugo no rokan\", \"The last barrier, and \"Saigo no ikketsu\", \"The final confirmation\". \"The last barrier\" when one left the training hall, for example \"Sum up all of the records of Rinzai in one word!\" It is not meant to be solved immediately, but to be carried around in order to keep practising. \"the final confirmation\" may be another word for the same kind of koan.\n\nCompleting the koan-curriculum in the Rinzai-schools traditionally also led to a mastery of Chinese poetry and literary skills:\nAfter completing the koan-training, \"Gogo no shugyo\", post-satori training is necessary:\nHakuin Ekaku, the 17th century revitalizer of the Rinzai school, taught several practices which serve to correct physical and mental imbalances arising from, among other things, incorrect or excessive koan practice. The \"soft-butter\" method (\"nanso no ho\") and \"introspection method\" (\"naikan no ho\") involve cultivation of ki centered on the tanden (Chinese:dantian). These practices are described in Hakuin's works Orategama and Yasen Kanna, and are still taught in some Rinzai lineages today.\n\nThough few Sōtō practitioners concentrate on kōans during meditation, the Sōtō sect has a strong historical connection with kōans, since many kōan collections were compiled by Sōtō priests.\n\nDuring the 13th century, Dōgen, founder of the Sōtō sect in Japan, quoted 580 kōans in his teachings. He compiled some 300 kōans in the volumes known as the Greater Shōbōgenzō. Dōgen wrote of Genjokōan, which points out that everyday life experience is the fundamental kōan.\n\nHowever, according to Michel Mohr,\nThe Sanbo Kyodan school and the White Plum Asanga, which originated with the Sōtō-priest Hakuun Yasutani, incorporates koan-study. The Sanbo kyodan places great emphasis on kensho, initial insight into one's true nature, as a start of real practice. It follows the so-called Harada-Yasutani koan-curriculum, which is derived from Hakuin's student Takuju. It is a shortened koan-curriculum, in which the socalled \"capping phrases\" are removed. The curriculum takes considerably less time to study than the Takuju-curriculum of Rinzai.\n\nTo attain kensho, most students are assigned the mu-koan. After breaking through, the student first studies twenty-two \"in-house\" koans, which are \"unpublished and not for the general public\", but are nevertheless published and commented upon. There-after, the students goes through the \"Gateless Gate\" (Mumonkan), the \"Blue Cliff Record\", the \"Book of Equanimity\", and the \"Record of Transmitting the Light\". The koan-curriculum is completed by the Five ranks of Tozan and the precepts.\n\nKōans collectively form a substantial body of literature studied by Zen practitioners and scholars worldwide. Kōan collections commonly referenced in English include:\nIn these and subsequent collections, a terse \"main case\" of a kōan often accompanies prefatory remarks, poems, proverbs and other phrases, and further commentary about prior emendations.\n\nThe \"Blue Cliff Record\" (Chinese: Bìyán Lù; Japanese: Hekiganroku) is a collection of 100 kōans compiled in 1125 by Yuanwu Keqin ( 1063–1135).\n\nThe \"Book of Equanimity\" or \"Book of Serenity\" (Chinese: Cóngróng lù; Japanese: Shōyōroku) is a collection of 100 Kōans by Hongzhi Zhengjue (Chinese: ; Japanese: Wanshi Shōgaku) (1091–1157), compiled with commentaries by Wansong Xingxiu (1166–1246). The full title is \"The Record of the Temple of Equanimity With the Classic Odes of Venerable Tiantong Jue and the Responsive Commentary of Old Man Wansong\" (Wansong Laoren Pingchang Tiantong Jue Heshang Songgu Congrong An Lu) (Taisho Tripitaka Vol. 48, No. 2004)\n\n\"The Gateless Gate\" (Chinese: Wumenguan; Japanese: Mumonkan) is a collection of 48 kōans and commentaries published in 1228 by Chinese monk Wumen () (1183–1260). The title may be more accurately rendered as \"Gateless Barrier\" or \"Gateless Checkpoint\").\n\nFive kōans in the collection derive from the sayings and doings of Zhaozhou Congshen, (transliterated as Chao-chou in Wade-Giles and pronounced Jōshū in Japanese).\n\n\"The True Dharma Eye 300\" (Shōbōgenzō Sanbyakusoku) is a collection of 300 kōans compiled by Eihei Dōgen.\nUpon arriving in China, Dogen Kigen first studied under Wuji Lepai, a disciple of Dahui, which is where he probably came into contact with Dahui Zonggaos (大慧宗杲) (1089–1163) Shōbōgenzō, the \"Zhengfayan zang\" (正法眼藏), \"Treasury of the true dharma eye\" (W-G.: Cheng-fa yen-tsang, J.: Shōbōgenzō) a collection of koans and dialogues compiled between 1147 and 1150 by Dahui Zonggao . Dahui's Shōbōgenzō is composed of three scrolls prefaced by three short introductory pieces. The Zongmen liandeng huiyao 宗門聯燈會要 was compiled in 1183 by Huiweng Wuming 晦翁悟明 (n.d.), three generations after Dahui in the same line; the sermon is found in zh 20 (x 79: 173a).\n\nOther kōan collections compiled and annotated by Sōtō priests include:\n\nVictor Hori comments:\nHuìnéng asked Hui Ming, \"Without thinking of good or evil, show me your original face before your mother and father were born.\" (This is a fragment of case #23 of the \"Wumenguan\".)\n\nThinking about the Buddha as an entity or deity is delusion, not awakening. One must destroy the preconception of the Buddha as separate and external before one can become internally as their own Buddha. Zen master Shunryu Suzuki wrote in \"Zen Mind, Beginner's Mind\" during an introduction to Zazen,\nOne is only able to see a Buddha as he exists in separation from Buddha; the mind of the practitioner is thus still holding onto apparent duality.\n\n\n\n\n\n"}
{"id": "53505467", "url": "https://en.wikipedia.org/wiki?curid=53505467", "title": "List of rampage killers (religious, political or racial crimes)", "text": "List of rampage killers (religious, political or racial crimes)\n\nThis section of the list of rampage killers contains mass murders, committed by lone wolf perpetrators, that have a foremost religious, racial or political background.\n\nA rampage killer has been defined as follows:\n\nThis list should contain every case with at least one of the following features:\n\nAll abbreviations used in the table are explained below. \nThe W-column gives a basic description of the weapons used in the murders\n"}
{"id": "1189560", "url": "https://en.wikipedia.org/wiki?curid=1189560", "title": "Loop (topology)", "text": "Loop (topology)\n\nA loop in mathematics, in a topological space \"X\" is a continuous function \"f\" from the unit interval \"I\" = [0,1] to \"X\" such that \"f\"(0) = \"f\"(1). In other words, it is a path whose initial point is equal to the terminal point.\n\nA loop may also be seen as a continuous map \"f\" from the pointed unit circle \"S\" into \"X\", because \"S\" may be regarded as a quotient of \"I\" under the identification of 0 with 1.\n\nThe set of all loops in \"X\" forms a space called the loop space of \"X\".\n\n"}
{"id": "3595285", "url": "https://en.wikipedia.org/wiki?curid=3595285", "title": "Maximum power principle", "text": "Maximum power principle\n\nThe maximum power principle or Lotka's principle has been proposed as the fourth principle of energetics in open system thermodynamics, where an example of an open system is a biological cell. According to Howard T. Odum, \"The maximum power principle can be stated: During self-organization, system designs develop and prevail that maximize power intake, energy transformation, and those uses that reinforce production and efficiency.\"\n\nChen (2006) has located the origin of the statement of maximum power as a formal principle in a tentative proposal by Alfred J. Lotka (1922a, b). Lotka's statement sought to explain the Darwinian notion of evolution with reference to a physical principle. Lotka's work was subsequently developed by the systems ecologist Howard T. Odum in collaboration with the Chemical Engineer Richard C. Pinkerton, and later advanced by the Engineer Myron Tribus.\n\nWhile Lotka's work may have been a first attempt to formalise evolutionary thought in mathematical terms, it followed similar observations made by Leibniz and Volterra and Ludwig Boltzmann, for example, throughout the sometimes controversial history of natural philosophy. In contemporary literature it is most commonly associated with the work of Howard T. Odum.\n\nThe significance of Odum's approach was given greater support during the 1970s, amid times of oil crisis, where, as Gilliland (1978, pp. 100) observed, there was an emerging need for a new method of analysing the importance and value of energy resources to economic and environmental production. A field known as energy analysis, itself associated with net energy and EROEI, arose to fulfill this analytic need. However, in energy analysis intractable theoretical and practical difficulties arose when using the energy unit to understand, a) the conversion among concentrated fuel types (or energy types), b) the contribution of labour, and c) the contribution of the environment.\n\nLotka said (1922b: 151): \nGilliland noted that these difficulties in analysis in turn required some new theory to adequately explain the interactions and transactions of these different energies (different concentrations of fuels, labour and environmental forces). Gilliland (Gilliland 1978, p. 101) suggested that Odum's statement of the maximum power principle (H.T.Odum 1978, pp. 54–87) was, perhaps, an adequate expression of the requisite theory:\nThis theory Odum called maximum power theory. In order to formulate maximum power theory Gilliland observed that Odum had added another law (the maximum power principle) to the already well established laws of thermodynamics. In 1978 Gilliland wrote that Odum's new law had not yet been validated (Gilliland 1978, p. 101). Gilliland stated that in maximum power theory the second law efficiency of thermodynamics required an additional physical concept: \"the concept of second law efficiency under maximum power\" (Gilliland 1978, p. 101):\nIn this way the concept of maximum power was being used as a principle to quantitatively describe the selective law of biological evolution. Perhaps H.T.Odum's most concise statement of this view was (1970, p. 62):\n\nThe Odum–Pinkerton approach to Lotka's proposal was to apply Ohm's law – and the associated maximum power theorem (a result in electrical power systems) – to ecological systems. Odum and Pinkerton defined \"power\" in electronic terms as the rate of work, where Work is understood as a \"useful energy transformation\". The concept of maximum power can therefore be defined as the \"maximum rate of useful energy transformation\". Hence the underlying philosophy aims to unify the theories and associated laws of electronic and thermodynamic systems with biological systems. This approach presupposed an analogical view which sees the world as an ecological-electronic-economic engine.\n\nOdum et al. viewed the maximum power theorem as a principle of power-efficiency reciprocity selection with wider application than just electronics. For example, Odum saw it in open systems operating on solar energy, like both photovoltaics and photosynthesis (1963, p. 438). Like the maximum power theorem, Odum's statement of the maximum power principle relies on the notion of 'matching', such that high-quality energy maximizes power by matching and amplifying energy (1994, pp. 262, 541): \"in surviving designs a matching of high-quality energy with larger amounts of low-quality energy is likely to occur\" (1994, p. 260). As with electronic circuits, the resultant rate of energy transformation will be at a maximum at an intermediate power efficiency. In 2006, T.T. Cai, C.L. Montague and J.S. Davis said that, \"The maximum power principle is a potential guide to understanding the patterns and processes of ecosystem development and sustainability. The principle predicts the selective persistence of ecosystem designs that capture a previously untapped energy source.\" (2006, p. 317). In several texts H.T. Odum gave the Atwood machine as a practical example of the 'principle' of maximum power.\n\nThe mathematical definition given by H.T. Odum is formally analogous to the definition provided on the maximum power theorem article. (For a brief explanation of Odum's approach to the relationship between ecology and electronics see Ecological Analog of Ohm's Law)\n\nWhether or not the principle of maximum power efficiency can be considered the fourth law of thermodynamics and the fourth principle of energetics is moot. Nevertheless, H.T. Odum also proposed a corollary of maximum power as the organisational principle of evolution, describing the evolution of microbiological systems, economic systems, planetary systems, and astrophysical systems. He called this corollary the maximum empower principle. This was suggested because, as S.E. Jorgensen, M.T. Brown, H.T. Odum (2004) note,\n\nC. Giannantoni may have confused matters when he wrote \"The \"Maximum Em-Power Principle\" (Lotka–Odum) is generally considered the \"Fourth Thermodynamic Principle\" (mainly) because of its practical validity for a very wide class of physical and biological systems\" (C. Giannantoni 2002, § 13, p. 155). Nevertheless, Giannantoni has proposed the Maximum Em-Power Principle as the fourth principle of thermodynamics (Giannantoni 2006).\n\nThe preceding discussion is incomplete. The \"maximum power\" was discovered several times independently, in physics and engineering, see: Novikov (1957), El-Wakil (1962), and Curzon and Ahlborn (1975). The incorrectness of this analysis and design evolution conclusions was demonstrated by Gyftopoulos (2002).\n\n\n"}
{"id": "8264965", "url": "https://en.wikipedia.org/wiki?curid=8264965", "title": "Monte Carlo simulation modelling of industrial systems", "text": "Monte Carlo simulation modelling of industrial systems\n\nThe simulation technique used is a next event Monte Carlo Simulation, with full array of building blocks, animation, graphical interface, unlimited hierarchical decomposition, full connectivity and interactivity with other programs, library based, build in compiled C-like programming language, to mention some of the most important features. The flexible method enables modelling of multi purpose tasks like system availability, on demand failure probability, multiple product flow, loading/storage capacities and spare part optimization. In offshore development projects all these features are used during the concept selection process. There is an interface between the simulation tool and Excel for input data, which makes it easy to structure data, extract data, running sensitivities and customize results presentation.\n\nThe simulation tool is built of hierarchical blocks where blocks can be constructed by the user, both regarding interface (pictures etc.) and programming the logic of the block. This feature is used to build up the model as a flow network with similar interface as a PFD (process flow diagram). The flow net-work is intended to simplify the comprehension of the availability model for design engineers and management. Although some customers used to reliability block diagrams (RBD), prefer this interface for transparency, showing component redundancy instead of product flow.\n"}
{"id": "50213", "url": "https://en.wikipedia.org/wiki?curid=50213", "title": "Morganatic marriage", "text": "Morganatic marriage\n\nMorganatic marriage, sometimes called a left-handed marriage, is a marriage between people of unequal social rank, which in the context of royalty prevents the passage of the husband's titles and privileges to the wife and any children born of the marriage.\n\nGenerally, this is a marriage between a man of high birth (such as from a reigning, deposed or mediatised dynasty) and a woman of lesser status (such as a daughter of a low-ranked noble family or a commoner). Usually, neither the bride nor any children of the marriage have a claim on the bridegroom's succession rights, titles, precedence, or entailed property. The children are considered legitimate for all other purposes and the prohibition against bigamy applies. In some countries, a woman could also marry a man of lower rank morganatically.\n\nAfter World War I the heads of both ruling and formerly reigning dynasties initially continued the practice of rejecting dynastic titles and/or rights for descendants of \"morganatic\" unions, but gradually allowed them, sometimes retroactively, effectively de-morganatizing the wives and children. This was accommodated by Perthes' \"Almanach de Gotha\" (which categorised princely families by rank until it ceased publication after 1944) by inserting the offspring of such marriages in a third section of the almanac under entries denoted by a symbol (a dot within a circle) that \"signifies some princely houses which, possessing no specific princely patent, have passed from the first part, A, or from the second part into the third part in virtue of special agreements.\" The \"Fürstliche Häuser\" (\"Princely Houses\") series of the \"Genealogisches Handbuch des Adels\" (\"Genealogical Manual of the Nobility\") has followed this lead, likewise enrolling some issue of unapproved marriages in its third section, \"III B\", with a similar explanation: \"Families in this section, although verified, received no specific decree, but have been included by special agreement in the 1st and 2nd sections\".\n\nVariations of morganatic marriage were also practised by non-European dynasties, such as the Royal Family of Thailand, the polygamous Mongols as to their non-principal wives, and other families of Africa and Asia.\n\n\"Morganatic\", already in use in English by 1727 (according to the \"Oxford English Dictionary\"), is derived from the medieval Latin \"morganaticus\" from the Late Latin phrase \"matrimonium ad morganaticam\" and refers to the gift given by the groom to the bride on the morning after the wedding, the morning gift, i.e., dower. The Latin term, applied to a Germanic custom, was adopted from the Old High German term \"*morgangeba\" (modern German \"Morgengabe\"), corresponding to Early English \"morgengifu\". The literal meaning is explained in a 16th-century passage quoted by Du Cange as, \"a marriage by which the wife and the children that may be born are entitled to no share in the husband's possessions beyond the 'morning-gift'\".\n\nThe \"morning gift\" has been a customary property arrangement for marriage found first in early medieval German cultures (such as the Lombards) and also among ancient Germanic tribes, and the church drove its adoption into other countries in order to improve the wife's security by this \"additional\" benefit. The bride received property from the bridegroom's clan. It was intended to ensure her livelihood in widowhood, and it was to be kept separate as the wife's discrete possession. However, when a marriage contract is made wherein the bride and the children of the marriage will not receive anything else (other than the dower) from the bridegroom or from his inheritance or clan, that sort of marriage was dubbed as \"marriage with only the dower and no other inheritance\", i.e., \"matrimonium morganaticum\".\n\nRoyal men who married morganatically:\n\nRoyal women who married morganatically:\n\nSuccession to the Danish throne followed the specifications of the \"Lex Regia\" until the Danish Act of Succession was passed in 1953. Prominent morganatic marriages include the 1615 marriage of King Christian IV of Denmark to noblewoman Kirsten Munk. Kirsten was titled \"Countess of Schleswig-Holstein\" and bore the King 12 children, all styled \"Count/Countess of Schleswig-Holstein\". King Frederick VII married the ballerina Louise Rasmussen, who was raised to the rank of \"Countess Danner\" in 1850. There were no children of this marriage. When Christian IX of Denmark's brother, Prince Julius of Schleswig-Holstein-Sonderburg-Glucksburg married Elisabeth von Ziegesar in 1883, the king granted her the title \"Countess af Røst\".\n\nUntil 1971, Danish princes who married women who did not belong to a royal or noble family were refused the sovereign's authorization, renouncing their right of succession to the throne and royal title (Prince Aage of Denmark morganatically eloped with Matilda Calvi, daughter of Count Carlo Giorgio di Bergolo, in January 1914 but renounced his dynastic rights and titles subsequently). They were granted the non-royal prefix of \"Prince\" and their descendants bear the title Count af Rosenborg in the Danish nobility.\n\nNeither of the children of Queen Margrethe II has married a person of either royal birth or of the titled aristocracy. Members of the Royal Family may still lose their place in the line of succession for themselves and their descendants if they marry without the monarch's permission.\n\nMorganatic marriage was not recognized in French law. Since the law did not distinguish, for marital purposes, between ruler and subjects, marriages between royalty and the noble heiresses to great fiefs became the norm through the 16th century, helping to aggrandize the House of Capet while gradually diminishing the number of large domains held in theoretical vassalage by nobles who were, in practice, virtually independent of the French crown: by the marriage of Catherine de' Medici to the future King Henry II in 1533, the last of these provinces, the county of Auvergne, came to the crown of France.\n\nAntiquity of nobility in the legitimate male line, not noble quarterings, was the main criterion of rank in the \"ancien régime\". Unlike the status of a British peer's wife and descendants (yet typical of the nobility of every continental European country), the legitimate children and male-line descendants of any French nobleman (whether titled or not, whether possessing a French peerage or not) were also legally noble \"ad infinitum\". Rank was not based on hereditary titles, which were often assumed or acquired by purchase of a noble estate rather than granted by the Crown. Rather, the main determinant of relative rank among the French nobility was how far back the nobility of a family's male line could be verifiably traced. Other factors influencing rank included the family's history of military command, high-ranking offices held at court and marriages into other high-ranking families. A specific exception was made for bearers of the title of duke who, regardless of their origin, outranked all other nobles. But the ducal title in post-medieval France (even when embellished with the still higher status of \"peer\") ranked its holder and his family among France's nobility and not, as in Germany and Scandinavia (and, occasionally, Italy, viz. Savoy, Medici, Este, della Rovere, Farnese and Cybo-Malaspina) among Europe's reigning dynasties which habitually intermarried with one another.\n\nOnce the Bourbons inherited the throne of France from the House of Valois in 1589, their dynasts married daughters of even the oldest ducal families of France — let alone noblewomen of lower rank — quite rarely (viz., Anne de Montafié in 1601, Charlotte Marguerite de Montmorency in 1609 and, in exile from revolutionary France, Maria Caterina Brignole in 1798). Exceptions were made for equal royal intermarriage with the \"princes étrangers\" and, by royal command, with the so-called \"princes légitimés\" (i.e., out-of-wedlock but legitimised descendants of Henry IV and Louis XIV), as well as with the nieces of Cardinal-prime ministers (i.e., Richelieu, Mazarin). Just as the French king could authorize a royal marriage that would otherwise have been deemed unsuitable, by 1635 it had been established by Louis XIII that the king could also legally void the canonically valid, equal marriage of a French dynast to which he had not given consent (e.g., Marguerite of Lorraine, Duchess of Orléans).\n\nMoreover, there was a French practice, legally distinct from morganatic marriage but used in similar situations of inequality in status between a member of the royal family and a spouse of lower rank: an \"openly secret\" marriage. French kings authorized such marriages only when the bride was past child-bearing or the marrying prince already had dynastic heirs by a previous spouse of royal descent. The marriage ceremony took place without banns, in private (with only a priest, the bride and groom, and a few legal witnesses present), and the marriage was never officially acknowledged (although sometimes widely known). Thus, the wife never publicly shared in her husband's titles, rank, or coat of arms. The lower-ranked spouse, male or female, could only receive from the royal spouse what property the king allowed.\n\nIn secret marriage, Louis XIV wed his second wife, Madame de Maintenon, in 1683 (she was nearly 50, so no children were likely); Louis the Grand Dauphin wed Marie Émilie de Joly de Choin in 1695; Anne Marie d'Orléans (\"La Grande Mademoiselle\") wed Antoine, Duke de Lauzun in 1682; and Louis Philippe I, Duke of Orléans wed the Marquise de Montesson in 1773. The mechanism of the \"secret marriage\" rendered it unnecessary for France to legislate the morganatic marriage \"per se\". Within post-monarchical dynasties, until the end of the 20th century the heads of the Spanish and Italian Bourbon branches, the Orléans of both France and Brazil, and the Imperial Bonapartes have, in exile, exercised claimed authority to exclude from their dynasty descendants born of unapproved marriages — albeit without calling these marriages \"morganatic\".\n\nThe practice of morganatic marriage was most common in the German-speaking parts of Europe, where equality of birth (\"Ebenbürtigkeit\") between the spouses was considered an important principle among the reigning houses and high nobility. The German name was \"Ehe zur linken Hand\" (\"marriage by the left hand\") and the husband gave his left hand during the wedding ceremony instead of the right.\n\nPerhaps the most famous example in modern times was the 1900 marriage of the heir to the throne of Austria-Hungary, Archduke Franz Ferdinand, and Bohemian aristocrat Countess Sophie Chotek von Chotkowa. The marriage was initially resisted by Emperor Franz Joseph I, but after pressure from family members and other European rulers, he relented in 1899 (but did not attend the wedding himself). The bride was made Princess (later Duchess) of Hohenberg, their children took their mother's new name and rank, but were excluded from the imperial succession. The Sarajevo assassination in 1914, killing both the Archduke Franz Ferdinand and his wife Sophie, triggered the First World War.\n\nAlthough the issue of morganatic marriages were ineligible to succeed to their families' respective thrones, children of morganatic marriages have gone on to achieve dynastic success elsewhere in Europe. Descendants of the 1851 marriage of Prince Alexander of Hesse and by Rhine to the German-Polish noblewoman Countess Julia von Hauke (created Princess of Battenberg) include Alexander, sovereign prince of Bulgaria, queen-consorts of Spain (Victoria Eugenie of Battenberg) and of Sweden (Louise Mountbatten), and, in the female line, Charles, Prince of Wales (through his paternal grandmother, Alice of Battenberg).\n\nLikewise, from the morganatic marriage of Duke Alexander of Württemberg and Countess Claudine Rhédey von Kis-Rhéde (created Countess von Hohenstein) descends Mary of Teck, who became Britain's queen in 1911 as the consort of King George V.\n\nOccasionally, children of morganatic marriages have overcome their non-dynastic origins and succeeded to their family's realms. Margrave Leopold inherited the throne of Baden, despite being born of a morganatic marriage, after all dynastic males of the House of Zähringen died out. The son of Charles Frederick, Grand Duke of Baden, by his second wife Louise Caroline Geyer von Geyersberg, who belonged to the minor nobility, Leopold became a prince in 1817, at the age of 27, as the result of a new law of succession. Baden's grand-ducal family faced extinction, so Leopold was enfranchised by international treaty and married to a princess, ascending the throne in 1830. His descendants ruled the grand duchy until the abolition of the monarchy in 1918.\n\nOther reigning German families adopted similar approaches when facing a lack of male heirs. In 1896 the Princely House of Schwarzburg, with the Sondershausen branch numbering two elderly childless princes and Rudolstadt just one childless prince, recognised Prince Sizzo von Leutenberg, morganatic son of Friedrich Günther, Prince of Schwarzburg-Rudolstadt, as a Prince of Schwarzburg and heir to the two principalities.\n\nThe senior line of the dynasty ruling the Principality of Lippe was bordering on extinction as the 20th century approached, prompting a succession dispute between the Lippe-Biesterfeld and Schaumburg-Lippe branches of the dynasty which evoked international intervention and troop movements. It centered on whether some ancestresses of the Biesterfeld branch had been legally dynastic; if so, that line stood next to inherit the princely crown according to primogeniture. If not, the Biesterfelds would be deemed morganatic and the Schaumburg-Lippes would inherit the throne. Lippe's Parliament was blocked from voting on the matter by the German Empire's \"Reichstag\", which instead created a panel of jurists selected by the King of Saxony to evaluate the evidence concerning the historical marital rules of the House of Lippe and render a decision in the matter, all parties agreeing to abide by their judgment. In 1897 and 1905 panels ruled in favour of the dynasticity of the challenged ancestresses and their descendants, largely because, although neither had been of dynastic rank, the Lippes had historically accepted such marriages when approved by the Head of House.\n\nIn the late 19th and early 20th centuries, a few families considered in Germany to be morganatic were considered for crowns elsewhere, constituting unexpected rehabilitation of their status. The first of these was Prince Alexander of Battenberg, who in 1877 was agreed upon by the Great Powers as the best candidate for the new throne of Bulgaria. He was, however, unable to hold onto his crown, and was also unable to obtain the hand in marriage of Princess Viktoria of Prussia despite the efforts of her imperial mother and grandmother.\n\nWilhelm, Duke of Urach (1864–1928), whose father was the morganatic son of a Württemberg prince, had the distinction of being under consideration for the crowns of five realms at different times: that of the Kingdom of Württemberg in the 1890s, as the senior agnate by primogeniture when it became likely that King William II would die without male descendants, leaving as heir Duke Albrecht of Württemberg, a more distantly related, albeit dynastic, royal kinsman; the Principality of Albania in 1913; the Principality of Monaco at turn of the 20th century, as the next heir by proximity of blood following the Hereditary Prince Louis, until the Monaco succession crisis of 1918 was resolved as the First World War ended; the prospective Grand Duchy of Alsace-Lorraine in 1917; and his abortive election by the \"Taryba\" as King Mindaugas II of Lithuania in July 1918. In the event, Duke Wilhelm obtained none of these thrones.\n\nRelying upon the \"Almanach de Gotha\" to gazette dynastic events, Germany's deposed heads of state continued to notify its editors of changes in family members' status and traditional titles. In 1919 the morganatic wife and children of Prince Oskar of Prussia, the counts and countesses von Ruppin, were upgraded to princes and princesses of Prussia by the exiled Kaiser Wilhelm II. In 1928 Georg, Count von Carlow, morganatic son of Duke George Alexander of Mecklenburg and commoner Natalia Vanljarskaya, became a duke of Mecklenburg and heir to his uncle Duke Charles Michael. In 1949, and again in 1999, various morganatic members of the Bavarian Royal House were recognised as princes and princesses of Bavaria, with the current head of the house, Franz, Duke of Bavaria, being among the beneficiaries of his father's ruling, having been born of a marriage initially deemed morganatic.\n\nIn the former Royal Family of Saxony Maria Emanuel, Margrave of Meissen adopted and designated as his heir his nephew Alexander de Afif, thus bypassing his agnatic cousin’s morganatic son, Prince Rüdiger of Saxony, and his three sons.\n\nWhen the Grand Duchy of Luxembourg found itself without a male heir at the beginning of the 20th century, the morganatic counts von Merenberg proposed themselves as heirs, being the last legitimate descendants in the male line of the House of Nassau. Grand Duke William IV, however, chose to confirm the law of succession stipulated in 1815 by the Congress of Vienna to allow a female descendant in the Nassau male line to become successor to the throne (his own daughter Marie-Adélaïde) instead.\n\nPaul I of Russia promulgated a strict new house law for Russia in 1797, eliminating the sovereign's right to designate the heir to the throne, but requiring that dynasts be born of authorized marriages. In 1820 a new law also stipulated that only children of Romanovs born of marriages with persons of equal status, i.e., members of a \"royal or sovereign family\", could transmit succession rights and titles to descendants. Alexander III forbade Romanov morganatic marriages altogether by issuance of \"ukase\" #5868 on 24 March 1889 amending article #63 of the Statute on the Imperial Family in the Pauline laws. By \"ukase\" #35731, dated 11 August 1911, Nicholas II amended the amendment, reducing application of this restriction from all members of the Imperial Family to grand dukes and grand duchesses only. This decree allowed marriages of the princes and princesses of the Blood Imperial with non-royal spouses, on the conditions that the emperor's consent be obtained, that the dynast renounce his or her personal succession rights, and that the Pauline laws restricting succession rights to those born of equal marriages continue in force.\n\nAn early victim of the Pauline laws was Grand Duke Constantine Pavlovich, grandson of Catherine the Great, and viceroy of Poland. On 20 March 1820 his marriage to Princess Juliane of Saxe-Coburg-Saalfeld was annulled to allow him to morganatically wed his longtime mistress, Countess Joanna Grudna-Grudzińska, in Warsaw on 24 May 1820, who was elevated to the title \"Princess Łowicza\" upon marriage, which produced no children.\n\nOne emperor, Alexander II, married morganatically in 1880. Princess Ekaterina Mihailovna Dolgorukova, Alexander's second bride, had previously been his long-term mistress and the mother of his three legitimised children, the princes and princesses Yurievsky.\n\nBeginning a novel tradition, one of that couple's daughters, Princess Olga Aleksandrovna Yurievskaya (1873–1925), in 1895 married the child of an 1868 morganatic marriage in the House of Nassau, George, Count von Merenberg (1871–1965). His mother was a daughter of renowned author Alexander Pushkin but, despite being of noble birth, she could not in 1868 dynastically marry the younger brother of a then-exiled Duke of Nassau. The count filed a futile suit to establish that his morganatic status in Germany should not exclude him from succession to the throne of Luxembourg after the last male of the House of Orange, King William III of the Netherlands, died in 1890 and it became apparent that the House of Nassau faced the imminent extinction of its male members, as well, upon the eventual death of Grand Duke William IV. Olga's brother, Prince George Aleksandrovich Yurievsky (1872–1913), in 1900 wed Countess Alexandra von Zarnekau (1883–1957), daughter of the morganatic marriage of the Russo-German Duke Constantine Petrovich of Oldenburg with Agrafena Djaparidize. Merenberg's sister, Sophia (1868–1927), likewise contracted a morganatic marriage in 1891, with Grand Duke Michael Mikhailovich of Russia, whose cousin, Emperor Nicholas II banished them to England, unwittingly saving the couple from the maelstrom of the Russian Revolution which proved fatal to so many Romanovs. She and her children were made \"counts de Torby\", her younger daughter, Countess Nada (1896–1963) marrying, in 1916 Prince George of Battenberg, future Marquess of Milford Haven and scion of the House of Battenberg, a morganatic branch of the grandducal House of Hesse which had settled in England and inter-married with descendants of Queen Victoria.\n\nLess fortunate among the Romanovs was Grand Duke Paul Aleksandrovich, who went into exile in Paris to marry a commoner, Olga Valerianovna Karnovich in 1902. Paul returned to serve in the Russian army during the First World War, and Nicholas II rewarded his uncle's loyalty by elevating Olga and her children as Princess and Princes Paley in 1915. Paul's patriotism, however, sealed his fate, and he died at the hands of Russia's revolutionaries in 1919. One of his daughters, Princess Irene Pavlovna Paley (1903–1990), was married while in exile in 1923, to her cousin, Prince Theodor Aleksandrovich of Russia, (1898-1968).\n\nNicholas II forbade his brother, Grand Duke Michael Alexandrovich of Russia, from marrying twice-divorced noblewoman Natalya Sergeyevna Wulfert (\"née\" Sheremetevskaya), but the couple eloped abroad in 1911. The Tsar refused his brother's request to grant the bride or their son, George Mikhailovich (1910–1931) a title, but legitimated George and incorporated him into the Russian nobility under the surname \"Brassov\" in 1915: nonetheless he and his mother used the comital title from 1915, only being granted a princely prefix in exile by Cyril Vladimirovich, Grand Duke of Russia in 1928. In the throes of the First World War, Nicholas II allowed his sister Grand Duchess Olga Alexandrovna of Russia to end her loveless marriage to her social equal, Duke Peter Alexandrovich of Oldenburg, and quietly marry commoner Colonel Nikolai Alexandrovich Kulikovsky. Both Michael's and Olga's descendants from these marriages were excluded from the succession.\n\nAfter the murder of Nicholas II and his children, the Imperial Family's morganatic marriages restricted the number of possible claimants. Grand Duke Cyril Vladimirovich, Nicholas's cousin, proclaimed himself as Emperor in exile. Controversy accompanied the marriage of his son Grand Duke Vladimir Cyrillovich to Princess Leonida Georgievna Bagration-Mukhransky, a descendant of the deposed Royal House of Georgia. After the annexation of Georgia in 1801, Leonida's family were deemed ordinary nobility in Imperial Russia rather than royalty, leading to claims that her 1948 marriage to Vladimir (who, however, also belonged to a deposed dynasty by then) was unequal and should be considered morganatic. As a result, some factions within Russia's monarchist movement did not support the couple's daughter, Grand Duchess Maria Vladimirovna, as the rightful heir to the Romanov dynasty (see Line of succession to the Russian throne for further details of the controversy).\n\nStandards of social classification and marital rules resembling the traditions of dynastic Europe can also be found in places as far afield as Africa. Here, a number of its constituent sovereign nations have legalized traditional authority as manifested in the recognized hereditary transmission of chieftaincy in historically relevant regions of the continent (e.g., the Asantehene of Ghana). For an example of the form that morganatic unions tend to take amongst African royalty, we have only to look at the biography of the continent's favourite son: President Nelson Mandela, the late leader of South Africa.\n\nMandela, a nobleman by birth of the Xhosa Thembus that reside in the Transkei region of the Cape coast, was nevertheless unable to ascend the throne of the \"Kumkani\" (or king) of the entire Thembu tribe, even though he descended in the legitimate, male line from the holders of this title. Nearly two centuries ago, Ngubengcuka (d. 1832), who ruled as the \"Kumkani\" of the Thembu people, married and subsequently left a son named Mandela, who became Nelson's grandfather and the source of his surname. However, because Mandela was only the Inkosi's child by a wife of the \"Ixhiba\" lineage, a so-called \"Left-Hand House\", the descendants of his cadet branch of the Thembu royal family remain ineligible to succeed to the Thembu throne, which is itself one of the several traditional seats that are still officially recognized by South Africa's government. Instead, the Mandelas were given the chiefdom of Mvezo and made hereditary counsellors to the \"Kumkani\" (i.e., privy counsellors) in deference to their royal ancestry. Following the loss of this chiefdom (which has since been restored to the family) in the Apartheid era, the Mandelas retained their positions as nobles of the Transkei. This status entailed, however, a degree of subjugation to the head of the dynasty, in particular in the matter of marital selection, which proved so onerous an issue to Nelson Mandela that it prompted the departure to Johannesburg that eventually led to his political career. Like the House of Battenberg in Europe, Mandela's family has since rehabilitated its dynastic status to some extent: Mandela was still in prison when his daughter Zenani was married to Prince Thumbumuzi Dlamini in 1973, elder brother of both King Mswati III of Swaziland and Queen Mantfombi, Great Wife of Goodwill Zwelithini, King of the Zulus.\n\nIn the erstwhile princely state of Travancore, in India, the male members of the Travancore Royal Family were, under the existing matrilineal Marumakkathayam system of inheritance and family, permitted to contract marriages with women of the Nair caste only. These were morganatic marriages called Sambandhams wherein the children gained their mother's caste and family name, due to Marumakkathayam. Although they could not inherit the throne, they did receive a title of nobility, Thampi (son of the Maharajah) and Kochamma (daughter of the Maharajah). These were the members of the Ammaveedus and their titles ensured a comfortable lifestyle and all other luxuries. The descendants of these Ammaveedu members were simply called Thampi and Thankachi and they didn't get any other distinguishing privileges.\n\nThe Cochin Royal family also followed the system of Marumakkatayam. Traditionally the female members of the family married Namboodiri Brahmins while male members marry ladies of the Nair caste. These wives of the male members are not royalty or didn't receive any royal titles or power, per the matrilineal system but instead get the title of Nethyar Amma. Their position ceases when the Maharaja dies. The children born to Neytharammas will be known by their mother's caste and hold no key royal titles. Currently the family marries mostly within the Kerala Kshatriya class.\n\nThe concept of morganatic marriage has never clearly existed in any part of the United Kingdom, and historically the English crown descended through marriages with commoners as late as the 17th century. Only two of the six marriages Henry VIII made to secure an heir were with royal brides, and Elizabeth Woodville, queen of Edward IV of England, was also a commoner.\n\nAnother link in the English succession involving marriage with a commoner was between John of Gaunt and Katherine Swynford. When they married after co-habiting for several years all children born previously were subsequently legitimated by Act of Parliament. King Henry IV later declared that they could not inherit the crown, but it is not clear that he had the right to do this. This marriage was important, as King Henry VII was descended from it, but Parliament still declared that he was king, so some issues remained unresolved.\n\nAs in nearly all European monarchies extant in the 21st century, most approved marriages in the British Royal Family are with untitled commoners and have been for several generations. In 1923 the future George VI (then second in line to the throne as Duke of York), was the first future English monarch to marry a non-princess or prince since 1659 when the future James II eloped with Anne Hyde. Wives of British peers are entitled to use the feminine form of their husbands' peerages under English common law, while wives of royal princes share their husbands' styles by custom unless the Sovereign formally objects.\n\nFor example, Catherine Middleton, a commoner, became the Duchess of Cambridge upon her marriage on 29 April 2011 to Prince William, who had been created Duke of Cambridge that morning. Camilla Parker Bowles, second wife of The Prince of Wales, legally holds the title \"Princess of Wales\" but at the time that the engagement was announced it was declared that she would be known by the title \"Duchess of Cornwall\" and, in Scotland, Duchess of Rothesay (derived from other titles her husband holds as heir apparent) in deference, it has been reported, to public feelings about the title's previous holder, the Prince's first wife Lady Diana Spencer. It was simultaneously stated that at such time, if any, that her husband accedes to the throne, she will be known as \"Princess Consort\" rather than \"Queen\", although as the king's wife she would legally be queen.\n\nOn 16 November 1936 Edward VIII informed Prime Minister Stanley Baldwin that he intended to marry the American divorcée Wallis Simpson, proposing that he be allowed to do so morganatically and remain king. Baldwin expressed his belief that Mrs. Simpson would be unacceptable to the British people as queen, but agreed to take further soundings. The prospect of the marriage was rejected by the British Cabinet. The other Dominion governments were consulted pursuant to the Statute of Westminster 1931, which provided in part that \"any alteration in the law touching the Succession to the Throne or the Royal Style and Titles shall hereafter require the assent as well of the Parliaments of all the Dominions as of the Parliament of the United Kingdom.\" Baldwin suggested three options to the prime ministers of the five Dominions of which Edward was also king: Canada, Australia, New Zealand, South Africa and the Irish Free State. The options were:\n\n\nThe second option had European precedents, including Edward's own maternal great-grandfather, Duke Alexander of Württemberg, but no unambiguous parallel in British constitutional history. William Lyon Mackenzie King (Prime Minister of Canada), Joseph Lyons (Prime Minister of Australia) and J. B. M. Hertzog (Prime Minister of South Africa) opposed options 1 and 2. Michael Joseph Savage (Prime Minister of New Zealand) rejected option 1 but thought that option 2 \"might be possible ... if some solution along these lines were found to be practicable\" but \"would be guided by the decision of the Home government\". Thus the majority of the Commonwealth's prime ministers agreed that there was \"no alternative to course (3)\". On 24 November, Baldwin consulted the three leading opposition politicians in Britain: Leader of the Opposition Clement Attlee, Liberal leader Sir Archibald Sinclair and Winston Churchill. Sinclair and Attlee agreed that options 1 and 2 were unacceptable and Churchill pledged to support the government.\n\nThe letters and diaries of working-class people and ex-servicemen generally demonstrate support for the King, while those from the middle and upper classes tend to express indignation and distaste. \"The Times\", \"The Morning Post\", the \"Daily Herald\", and newspapers owned by Lord Kemsley, such as \"The Daily Telegraph\", opposed the marriage. On the other hand, the \"Express\" and \"Mail\" newspapers, owned by Lord Beaverbrook and Lord Rothermere, respectively, appeared to support a morganatic marriage. The King estimated that the newspapers in favour had a circulation of 12.5 million, and those against had 8.5 million.\n\nBacked by Churchill and Beaverbrook, Edward proposed to broadcast a speech indicating his desire to remain on the throne or to be recalled to it if forced to abdicate, while marrying Mrs Simpson morganatically. In one section, Edward proposed to say:\nBaldwin and the British Cabinet blocked the speech, saying that it would shock many people and would be a grave breach of constitutional principles.\n\nUltimately, Edward decided to give up the throne for \"the woman I love,\" whereupon he and his descendants were deprived of all right to the Crown by Parliament's passage of His Majesty's Declaration of Abdication Act 1936. He was created Duke of Windsor on 8 March 1937 by his brother, the new George VI. He would marry Wallis Simpson in France on 3 June 1937, after her second divorce became final. In the meantime, however, letters patent dated 27 May 1937, which re-conferred upon the Duke of Windsor the \"title, style, or attribute of Royal Highness\", specifically stated that \"his wife and descendants, if any, shall not hold said title or attribute\". This decree was issued by the new king and unanimously supported by the Dominion governments, The king's authority to withhold from the lawful wife of a prince the attribute hitherto accorded to the wives of other modern British princes was addressed by the Crown's legal authorities: On 14 April 1937, Attorney General Sir Donald Somervell submitted to Home Secretary Sir John Simon a memorandum summarising the views of Lord Advocate T. M. Cooper, Parliamentary Counsel Sir Granville Ram, and himself:\n\nThe new King's firm view, that the Duchess should not be given a royal title, was shared by Queen Mary and George's wife, Queen Elizabeth. The Duchess bitterly resented the denial of the royal title and the refusal of the Duke's relatives to accept her as part of the family. In the early days of George VI's reign the Duke telephoned daily, importuning for money and urging that the Duchess be granted the style of Royal Highness, until the harassed King ordered that the calls not be put through. However, within the household of the Duke and Duchess, the style \"Her Royal Highness\" was used by those who were close to the couple.\n\nIt has been suggested that William, Prince of Orange, expected to have a strong claim to the throne of England after the Duke of York during the reign of Charles II. In fact, the Duke's two daughters from his first marriage, Princess Mary and Princess Anne, were considered to have the stronger claim by the English establishment. William's expectation was based on the continental practice of morganatic marriage, since the mother of both princesses, Anne Hyde, was a commoner and a lady-in-waiting to William's mother, Princess Mary. It was through his mother, a sister of Charles II and the Duke of York, that William claimed the throne, because, to his mind, the son of a princess had a stronger claim than the daughter of a commoner. It was to shore up his own claim to the throne that he agreed to marry his first cousin, Princess Mary. When James II fled at the Glorious Revolution, William refused to accept the title of king consort (which Philip II of Spain had been granted under Queen Mary I in the 1550s) and insisted on being named King in his own right. The compromise solution involved naming both to the crown as had rarely happened in the past (see for example King Henry II and his son Young King Henry, who ruled England simultaneously).\n\nThe Royal Marriages Act of 1772 made it illegal for all persons born into the British royal family to marry without the permission of the sovereign, and any marriage contracted without the sovereign’s consent was considered invalid. This led to several prominent cases of British princes who had gone through marriage ceremonies, and who cohabited with their partners as if married, but whose relationships were not legally recognised. As a result, their partners and children (the latter considered illegitimate) held no titles, and had no succession rights. This differs from morganatic marriages, which are considered legally valid.\n\n\nArticles related to unequal marriage:\n"}
{"id": "2100873", "url": "https://en.wikipedia.org/wiki?curid=2100873", "title": "Non-aggression principle", "text": "Non-aggression principle\n\nThe non-aggression principle (or NAP; also called the non-aggression axiom, the anti-coercion, zero aggression principle or non-initiation of force) is an ethical stance asserting that aggression is inherently wrong. In this context, \"aggression\" is defined as initiating or threatening any \"forcible\" interference with an individual or individual's property. In contrast to pacifism, it does not forbid forceful defense.\n\nThe NAP is considered by some to be a defining principle of natural-rights libertarianism. It is also a prominent idea in anarcho-capitalism, (classical) liberalism, libertarianism, and minarchism.\nThe non-aggression principle has existed in various forms. Although the principle has been traced back as far as antiquity, it was first formally described by this name by the Objectivist philosopher Ayn Rand, and then further popularized by libertarian thinkers.\n\nA number of authors have created their own formulation of the non-aggression principle, as shown in the table below.\nThe principle has been derived by various philosophical approaches, including:\n\nMany supporters and opponents of abortion rights justify their position on NAP grounds. One important question to determine whether or not abortion is consistent with NAP is at what stage of development a fertilized human egg cell can be considered a human being with the status and rights attributed to personhood. Some supporters of NAP argue this occurs at the moment of conception. Others argue that since they believe the fetus lacks sentience until a certain stage of development, it does not qualify as a human being, and as such may be considered property of the mother. Opponents of abortion, on the other hand, state sentience is not a qualifying factor. They refer to the animal rights discussion and point out the \"Argument from marginal cases\" that concludes NAP also applies to non-sentient (i.e. mentally handicapped) humans.\n\nAnother important question is whether an unwelcome fetus should be considered to be an unauthorized trespasser in its mother's body. The non-aggression principle does not protect trespassers from the owners of the property on which they are trespassing. It can also be argued that unwelcome fetuses are themselves committing aggression against their mothers, by taking materials (oxygen, water, nutrients) from her bloodstream, by injecting toxic metabolic end-products (carbon dioxide and creatinine) into her bloodstream, and by preparing to subject her to major medical/surgical trauma in the form of full-term labor and delivery.\n\nObjectivist philosopher Leonard Peikoff has argued that a fetus has no right to life inside the womb because it is not an \"independently existing, biologically formed organism, let alone a person.\" Pro-choice libertarian Murray Rothbard held the same stance, maintaining that abortion is justified at any time during pregnancy if the fetus is no longer welcome inside its mother. Likewise, other pro-choice supporters base their argument on criminal trespass. In that case, they claim, NAP is not violated when the fetus is forcibly removed, with deadly force if need be, from the mother’s body, just as NAP is not violated when an owner removes from the owner’s property an unwanted visitor who is not willing to leave voluntarily. Libertarian theorist Walter Block follows this line of argument but makes a distinction between evicting the fetus prematurely so that it dies and actively killing it (see \"Libertarian perspectives on abortion\").\n\nPro-life libertarians, however, argue that because the parents were actively involved in creating a new human life and that life has not consented to his or her own existence, that life is in the womb by necessity and no parasitism or trespassing is involved (see also legal necessity). They state that as the parents are responsible for that life's position, NAP would be violated when that life is killed with abortive techniques.\n\nNAP is defined as applicable to any unauthorized actions towards a person’s \"physical\" property. Supporters of NAP disagree on whether it should apply to intellectual property (IP) rights as well as physical property rights. Some argue that because intellectual concepts are non-rivalrous, intellectual property rights are unnecessary. while others argue that intellectual property rights are as valid and important as physical ones.\n\nThough NAP is meant to guarantee an individual’s sovereignty, libertarians greatly differ on the conditions under which NAP applies. Especially unsolicited intervention by others, either to prevent society from being harmed by the individual’s actions or to prevent an incompetent individual from being harmed by his own (in)actions, is an important issue. The debate centers on topics like the age of consent for children, intervention counseling (i.e. for addicted persons, or in case of domestic violence), involuntary commitment and involuntary treatment with regards to mental illness, medical assistance (i.e. prolonged life support vs euthanasia in general and for the senile or comatose in particular), human organ trade, state paternalism (including economic intervention) and foreign intervention by states. Other discussion topics on whether intervention is in line with NAP include nuclear weapons proliferation, and human trafficking and (illegal) immigration.\n\nSome libertarians justify the existence of a minimal state on the grounds that anarchism implies that the non-aggression principle is optional because the enforcement of laws is open to competition. They claim competing law enforcement would always result in war and the rule of the most powerful.\n\nAnarcho-capitalists usually respond to this argument that this presumed outcome of coercive competition (e.g. PMCs or PDAs that enforce local law) is not likely because of the very high cost, in lives and economically, of war. They claim that war drains those involved and leaves non-combatant parties as the most powerful, economically and militarily, ready to take over. Therefore, anarcho-capitalists claim that in practice, and in more advanced societies with large institutions that have a responsibility to protect their vested interests, disputes are most likely to be settled peacefully. Anarcho-capitalists also point out that a state monopoly of law enforcement does not necessarily make NAP present throughout society as corruption and corporatism, as well as lobby group clientelism in democracies, favor only certain people or organizations. Anarcho-capitalists aligned with the Rothbardian philosophy generally contend that the state violates the non-aggression principle by its very nature because, it is argued, governments necessarily use force against those who have not stolen private property, vandalized private property, assaulted anyone, or committed fraud.\n\nProponents of the NAP see taxes as a violation of NAP, while critics of the NAP argue that because of the free-rider problem in case security is a public good, enough funds would not be obtainable by voluntary means to protect individuals from aggression of a greater severity. The latter therefore accept taxation, and consequently a breach of NAP with regard to any free-riders, as long as no more is levied than is necessary to optimise protection of individuals against aggression. Geolibertarians, who following the classical economists and Georgists adhere to the Lockean labor theory of property, argue that land value taxation is fully compatible with the NAP.\n\nAnarcho-capitalists argue that the protection of individuals against aggression is self-sustaining like any other valuable service, and that it can be supplied without coercion by the free market much more effectively and efficiently than by a government monopoly. Their approach, based on proportionality in justice and damage compensation, argues that full restitution is compatible with both retributivism and a utilitarian degree of deterrence while consistently maintaining NAP in a society. They extend their argument to all public goods and services traditionally funded through taxation, like security offered by dikes.\n\nSupporters of the NAP often appeal to it in order to argue for the immorality of theft, vandalism, assault, and fraud. Compared to nonviolence, the non-aggression principle does not preclude violence used in self-defense or defense of others. Many supporters argue that NAP opposes such policies as victimless crime laws, taxation, and military drafts. NAP is the foundation of libertarian philosophy.\n\nNAP faces two kinds of criticism: the first holds that the principle is immoral, and the second argues that it is impossible to apply consistently in practice; respectively, consequentialist or deontological criticisms, and inconsistency criticisms. Libertarian academic philosophers have noted the implausible results consistently applying the principle yields: for example, Professor Matt Zwolinski notes that, because pollution necessarily violates the NAP by encroaching (even if slightly) on other people's property, consistently applying the NAP would prohibit driving, starting a fire, and other activities necessary to the maintenance of industrial society.\n\nCritics argue that the non-aggression principle is not ethical because it opposes the initiation of force even when they would consider the results of such initiation to be morally superior to the alternatives that they have identified. In arguing against the NAP, philosopher Matt Zwolinski has proposed the following scenario: \"Suppose that by imposing a very, very small tax on billionaires, I could provide life-saving vaccination for tens of thousands of desperately poor children. Even if we grant that taxation is aggression, and that aggression is generally wrong, is it really so obvious that the relatively minor aggression involved in these examples is wrong, given the tremendous benefit it produces?\"\n\nZwolinski also notes that the NAP is incompatible with any practice that produces any pollution, because pollution encroaches on the property rights of others. Therefore, the NAP prohibits both driving and starting fires. Citing Professor David Friedman, Professor Zwolinski notes that the NAP is unable to place a sensible limitation on risk-creating behavior. Writes Zwolinski,\n\nSome critics use the example of the trolley problem to invalidate NAP. In case of the runaway trolley, headed for five victims tied to the track, NAP does not allow a trolley passenger to flip the switch that diverts the trolley to a different track if there is a person tied to that track. That person would have been unharmed if nothing was done, therefore by flipping the switch NAP is violated. Another example often cited by critics is human shields.\n\nSome supporters argue that no one initiates force if their only option for self-defense is to use force against a greater number of people as long as they were not responsible for being in the position they are in. Murray Rothbard's and Walter Block's formulations of NAP avoid these objections by either specifying that the NAP applies only to a civilized context (and not 'lifeboat situations') or that it applies only to legal rights (as opposed to general morality). Thus a starving man may, in consonance with general morality, break into a hunting cabin and steal food, but nevertheless he is aggressing, i.e. violating the NAP, and (by most rectification theories) should pay compensation. Critics argue that the legal rights approach might allow people who can afford to pay a sufficiently large amount of compensation to get away with murder. They point out that local law, though based on NAP, may vary from proportional compensation to capital punishment to no compensation at all.\n\nOther critics state that the NAP is unethical because it does not provide for the violent prohibition of, and thereby supposedly legitimizes, several forms of aggression that do not involve intrusion on property rights, such as verbal sexual harassment, defamation, boycotting, noninvasive striking, and noninvasive discrimination.  If a victim thus provoked would turn to physical violence, according to the NAP, he would be labeled an aggressor.  Supporters of the NAP, however, state that boycotting and defamation both constitute freedoms of speech and that boycotting, noninvasive striking, and noninvasive discrimination all constitute freedoms of association, and that both freedoms of association and of speech are nonaggressive.  Supporters also point out that prohibiting physical retaliation against an action is not itself condonement \"of\" said action, and that generally there are other, nonphysical means by which one can combat social ills (\"e.g.\", discrimination) that do \"not\" violate the NAP.  Some supporters also state that while, most of the time, individuals choose voluntarily to engage in situations that may cause some degree of mental battering, this mental battering begins, when it cannot be avoided, to constitute unauthorized physical overload of the senses (\"i.e.\", eardrum and retina) and that the NAP, at that point, does apply.\n\nMany supporters consider verbal and written threats of imminent physical violence sufficient justification for a defensive response in a physical manner.<ref name=\"verbal/written threats of physical force\">Murray N. Rothbard, \"Self-Defense,\" ch. 12, in \"A Theory of Liberty,\" pt. 2 of \"The Ethics of Liberty\" (New York, N. Y.: New York University Press, 1998; orig. 1982), pp. 77–78, 80.  \"Cf.\", \"For a New Liberty\", p. 27.</ref><ref name=\"use of/threat of/substitution for physical force\">Linda & Morris Tannehill, \"The Market for Liberty\" (San Francisco: Fox & Wilkes, October 1993; orig. March 1970), pp. 4, 10.  \"Cf.\", pp. 77, 80</ref>  Such threats would then constitute a legitimate limit to permissible speech.  Because freedom of association entails the right of owners to choose who is permitted to enter or remain on their premises, legitimate property owners may \"also\" impose limitation on speech.  For example, the owner of a theatre, wishing to avoid a stampede, may prohibit those on her property from calling ‘fire!’ without just cause.  The owner of a bank, however, may not prohibit anyone from urging the general public to a bank run, except insofar as this occurs on the property of said owner.\n\nH.L. Mencken, a writer who influenced many libertarians, puts an ethical limit on the freedom of speech:\nSupporters also consider physical threats of imminent physical violence (\"e.g.\", pointing a firearm at innocent people, stocking up nuclear weapons that cannot be used discriminately against specific individual aggressors) sufficient justification for a defensive response in a physical manner.  Such threats would then constitute a legitimate limit to permissible action.\n\nCritics argue it is not possible to uphold NAP when protecting the environment as most pollution can never be traced back to the party that caused it. They therefore claim that only general broad government regulations will be able to protect the environment. Supporters cite the problem of the tragedy of the commons and argue that free-market environmentalism will be much more effective in conserving nature.\nPolitical theorist Hillel Steiner emphasizes that all things made come from natural resources and that the validity of any rights to those made things depends on the validity of the rights to the natural resources. If land was stolen then anyone buying produce from that land would not be the legitimate owner of the goods. Also, if natural resources cannot be privately owned but are, and always will be, the property of all of mankind then NAP would be violated if such a resource would be used without everybody’s consent (see the Lockean proviso and free-market anarchism). Libertarian philosopher Roderick Long points out that, as natural resources are required not only for the production of goods but for the production of the human body as well, the very concept of self-ownership can only exist if the land itself is privately owned.\n\nConsequentialist libertarian David Friedman, who believes that the NAP should be understood as a relative rather than absolute principle, defends his view by using a Sorites argument. Friedman begins by stating what he considers obvious: A neighbor aiming his flashlight at someone's property is not aggression, or if it is, it is only aggression in a trivial technical sense. However, aiming at the same property with a gigawatt laser is certainly aggression by any reasonable definition. Yet both flashlight and laser shine photons onto the property, so there must be some cutoff point of how many photons one is permitted to shine upon a property before it is considered aggression. But the cutoff point cannot be found by deduction alone, because of the Sorites paradox, so the non-aggression principle is necessarily ambiguous. Friedman points out the difficulty of undertaking any activity that poses a certain amount of risk to third parties (e.g. flying) if the permission of thousands of people that might be affected by the activity is required.\n\n"}
{"id": "2681589", "url": "https://en.wikipedia.org/wiki?curid=2681589", "title": "Noogenesis", "text": "Noogenesis\n\nNoogenesis is the emergence and evolution of intelligence.\n\nNoo, nous (, ) – from the ancient Greek , has synonyms in other languages (Chinese), is a term that currently encompasses the semantics: mind, intelligence, intellect, reason; wisdom; insight, intuition, thought, - in a single phenomenon .\n\nNoogenesis was first mentioned in the posthumously published in 1955 book \"The Phenomenon of Man\" by Pierre Teilhard de Chardin, an anthropologist and philosopher, in a few places:\n\nThe lack of any kind of definition of the term has led to a variety of interpretations reflected in the book, including \"the contemporary period of evolution on Earth, signified by transformation of biosphere onto the sphere of intelligence—noosphere\", \"evolution run by human mind\" etc.\nThe most widespread interpretation is thought to be \"the emergence of mind, which follows geogenesis, biogenesis and anthropogenesis, forming a new sphere on Earth—noosphere\".\n\nIn 2005 Alexey Eryomin in the monograph Noogenesis and Theory of Intellect proposed a new concept of noogenesis in understanding the evolution of intellectual systems, concepts of intellectual systems, information logistics, information speed, intellectual energy, intellectual potential, consolidated into a theory of the intellect which combines the biophysical parameters of intellectual energy—the amount of information, its acceleration (frequency, speed) and the distance it's being sent—into a formula.\nAccording to the new concept—proposed hypothesis continue prognostic progressive evolution of the species \"Homo sapiens\", the analogy between the human brain with the enormous amount of neural cells firing at the same time and a similarly functioning human society.\nA new understanding of the term \"noogenesis\" as an evolution of the intellect was proposed by A. Eryomin. A hypothesis based on recapitulation theory links the evolution of the human brain to the development of human civilization. The parallel between the amount of people living on Earth and the amount of neurons becomes more and more obvious leading us to viewing global intelligence as an analogy for human brain.\nAll of the people living on this planet have undoubtedly inherited the amazing cultural treasures of the past, be it production, social and intellectual ones. We are genetically hardwired to be a sort of \"live RAM\" of the global intellectual system. Alexey Eryomin suggests that humanity is moving towards a unified self-contained informational and intellectual system. His research has shown the probability of Super Intellect realizing itself as Global Intelligence on Earth. We could get closer to understanding the most profound patterns and laws of the Universe if these kinds of research were given enough attention. Also, the resemblance between the individual human development and such of the whole human race has to be explored further if we are to face some of the threats of the future.\n\nTherefore, generalizing and summarizing: \nThe term \"noogenesis\" can be used in a variety of fields i.e. medicine, biophysics, semiotics, mathematics, information technology, psychology, theory of global evolution etc. thus making it a truly cross-disciplinary one. In astrobiology noogenesis concerns the origin of intelligent life and more specifically technological civilizations capable of communicating with humans and or traveling to Earth. The lack of evidence for the existence of such extraterrestrial life creates the Fermi paradox.\n\nThe emergence of the human mind is considered to be one of the five fundamental phenomenons of emergent evolution. \nTo understand the mind, it is necessary to determine how human thinking differs from other thinking beings. Such differences include the ability to generate calculations, to combine dissimilar concepts, to use mental symbols, and to think abstractly.\nThe knowledge of the phenomenon of intelligent systems—the emergence of reason (noogenesis) boils down to:\n\n\nSeveral published works which do not employ the term \"noogenesis\", however, address some patterns in the emergence and functioning of the human intelligence: working memory capacity ≥ 7, ability to predict, prognosis, hierarchical (6 layers neurons) system of information analysis, consciousness, memory, generated and consumed information properties etc. They also set the limits of several physiological aspects of human intelligence. Сonception of emergence of insight.\n\nHistorical evolutionary development and emergence of \"H. sapiens\" as species, include emergence of such concepts as anthropogenesis, phylogenesis, morphogenesis, cephalization, systemogenesis, cognition systems autonomy.\n\nOn the other hand, development of an individual's intellect deals with concepts of embryogenesis, ontogenesis, morphogenesis, neurogenesis, higher nervous function of I.P.Pavlov and his philosophy of mind.\nDespite the fact that the morphofunctional maturity is usually reached by the age of 13, the definitive functioning of the brain structures is not complete until about 16–17 years of age.\n\nThe fields of Bioinformatics, genetic engineering, noopharmacology, cognitive load, brain stimulations, the efficient use of altered states of consciousness, use of non-human cognition, information technology (IT), artificial intelligence (AI) are all believed to be effective methods of intelligence advancement and may be the future of intelligence on earth and the galaxy.\n\nThe development of the human brain, perception, cognition, memory and neuroplasticity are unsolved problems in neuroscience. Several megaprojects are being carried out in: American BRAIN Initiative, European Human Brain Project, China Brain Project, Blue Brain Project, Allen Brain Atlas, Human Connectome Project, Google Brain, - in attempt to better our understanding of the brain's functionality along with the intention to develop human cognitive performance in the future with artificial intelligence, informational, communication and cognitive technology.\n"}
{"id": "32037653", "url": "https://en.wikipedia.org/wiki?curid=32037653", "title": "Omni art", "text": "Omni art\n\nOmni art is an art movement that emerged in 1988 in New York City (United States). Omni art followed Pop art and Postmodernism by shifting attention from social and political issues focusing on the emerging convergence of science, consciousness and art in the late 20th and early 21st century.\n\nIt draws from ancient Sanskrit mysticism, sacred geometry, and theoretical quantum physics, and its premise maintains that sentient experience is immersed in a multidimensional quantum field that can be distilled into 5 distinct yet connected planes or dimensions that together weave a Universal fabric or “cosmic soup” of existence.\n\nWithin the Omni art paradigm, there is an ascending series of vibrational fields ranging from the most coarse in vibratory rate (the physical dimension) to the most subtle and sublime (the etheric dimension). Each of the 5 dimensions (described below) maintain environmental vibrational fields that correlate with universal particle and wave energy signatures with qualities that can be ascertained through both empirical and subjective methodologies.\n\nOmni art is concerned with exploring and expressing sentient life experiences of and within these 5-dimensional fields that exhibit powerful influences within and upon sentient and other life forms.\n\nThe quantum physicist postulates that a wave/particle model of energy and matter substructs universal cosmology and in this way quantum physicists and Omni artists are concerned with the same observable and intuited phenomenon.\n\nArtist Jeffrey Milburn postulates that these 5-dimensional vibrational particle and wave fields are experienced in the human consciousness through light and sound waves forming a 5-dimensional matrix that sustains the fabric of existence for all life in the visible and invisible universe.\n\nThe Physical Dimension - The primary vibrational field has the most dense oscillation pattern and energy signature manifested as matter within the physical universe.\n\nThe Astral Dimension - The secondary vibrational field has a faster and finer oscillation pattern and energy signature than the physical dimension. This is where dreams, psychic cognition, human emotions, and paranormal phenomenon take place.\n\nThe Causal Dimension - The tertiary vibrational field has a faster and finer oscillation pattern and energy signature than the astral dimension, and is described by many researchers as a resonant “brain” or record of all the cause-effect relationships in the universe, known in ancient Vedic texts as the akasha.\n\nThe Mental Dimension - The quaternary vibrational field has a faster and finer oscillation pattern and energy signature than the causal dimension. The human mind experiences this vibrational field as thoughts, ideas, concepts and connectivity.\n\nThe Etheric Dimension - The quinary dimensional field has a faster and finer oscillation pattern and energy signature than the mental dimension. This is the subconscious portal to the non-dual dimensions.\nThis dimension acts as the gateway into non-duality or singularity and can be observed in sudden, unexplainable awareness that spontaneously appears as an \"aha!\", or sudden, moment of realization.\n\nArtist Jeffrey Milburn mounted a museum installation of the foundational collection of 12 artworks called “The Universalist Group.” in 1991 at The Islip Art Museum in Islip, New York. This group of 12 assemblage artworks were created over a period of 10 years beginning in 1981, and provide the basic vocabulary for the language of the Omni art style. Milburn coined this style of art “Omni art” in 1988 when he founded this nascent art movement. The Universalist Group collection has appeared in whole or in part in all of Milburn's performance installations over the last 30 years and is described in detail in his book, OMNI ART - Language of Consciousness, Milburn’s treatise regarding Omni art.\n\nMilburn's artworks are assemblages of found objects, canvas, and paint. Milburn describes the juxtaposition of objects in his artworks as creating a gestalt “language” allowing the observer to communicate with an inner experience he describes as a “language of consciousness” that is attuned to the observer's current state of developmental awareness. In this way, each artwork serves as a catalyst for a personal inspiration or realization that mirrors what quantum physicists describe as the observer and observed duality that collapses into a singularity field. This is the same singularity that scientists and ancient spiritual teachers describe as “Source” or “Oneness” that is the origination point for all particles and waves that form the phenomenon of existence.\n\nIn this way, Omni art is literally defined as “God art” or “Source art” both by intentional application and pragmatic utilization. The term Omni was discovered by Milburn when researching terminology that would apply to this emerging art form. Milburn describes his role as one of an artist/mystic and subsequently he has come to be known as “The Oracle of Omni.”\n\nOne of the two primary artworks from The Universalist Group collection titled “SOUL” (1981) contains many elements that are fundamental in the science of sacred geometry. Surrounding the mirror in the center circle on this art work are 5 hand cut quartz crystals in the shape of the 5 platonic solids originally discovered and articulated upon by Plato.\n\nMilburn identified these platonic solids as corresponding to the 5-dimensional vibrational fields which he deduced constitute the basic elements forming a universal cosmological structure.\n\nThis center circle area is fronted with a 3-dimensional metaform mandala projecting out from the front of the artwork also containing many sacred geometry's. Milburn discovered 16 years after creating this artwork that its basic shape reflects the geometric shape of a black hole and posited that this peculiarity pertains to the ancient Kabbalistic theory that all in the invisible universe is reflected in the visible universe - or more commonly stated as “as above, so below.” In quantum physics jargon this same cosmological structure was articulated by physicist David Bohm as an “implicate and explicate order.”\n\nMilburn further describes this artwork as a symbolic map or representation of universal cosmological structures that integrate with emerging scientific theories of conscious human evolution.\n\nAccording to Milburn, conscious human evolution is intrinsic and incontrovertible and can be seen in all aspects of life. It naturally follows that this conscious evolution can then be observed in all the arts, sciences, and cultural idioms and he coined this process “meta-evolution” as it transcends the boundaries of scientific materialism. Reaching into the invisible regions of discovery not accessible by current technology, this evolutionary phenomenon is seen by Milburn to be the definition of the Omni art movement in the widest sense.\n\nMilburn states that in popular culture one can observe the contemporary appearance of this meta-evolutionary process in contemporary films, books, and videos exploring the many aspects of emerging awareness as it intersects with a popular fascination with metaphysical and paranormal phenomenon. Some of this phenomenon such as UFO sightings, and most recently the light orbs that are appearing by the hundreds of thousands in low resolution (below 2 mega-pixels) digital photographs espoused upon in Miceal Ledwith and physicist Klaus Heinemann's 2008 book The Orb Project, have captured the imaginations of millions of people around the world and show no sign of abating.\n\n\nWriter has compiled the history on Omni art from multiple audio interview and conversation sources on The Omni Art Salon cyber art salon 2005-2008 and archived written materials from artist Jeffrey Milburn's personal archives. For more information see, The Omni Art Salon archive at www.omniartsalon.com with additional descriptive sources at www.freshperspectiveswithjeffreymilburn.com\n\n\n\n"}
{"id": "36047835", "url": "https://en.wikipedia.org/wiki?curid=36047835", "title": "Orthology (language)", "text": "Orthology (language)\n\nOrthology is the study of the right use of words in language. The word comes from Greek \"ortho\"- (\"correct\") and -\"logy\" (\"science of\"). This science is a place where psychology, philosophy, linguistics, and many other fields of learning come together. The most noted use of \"Orthology\" is for the selection of words \nfor the language of Basic English by the Orthological Institute.\n\nThe book, \"The Meaning of Meaning\", by C.K. Ogden and I.A. Richards, is an important book dealing with orthology. The term \"Orthology\" comes from the book \"The Grammar of Science\" by Karl Pearson.\n"}
{"id": "2404840", "url": "https://en.wikipedia.org/wiki?curid=2404840", "title": "Parampara", "text": "Parampara\n\nParampara (Sanskrit: परम्परा, \"paramparā\") denotes a succession of teachers and disciples in traditional Vedic culture and Indian religions such as Hinduism, Sikhism, Jainism and Buddhism. It is also known as \"guru–shishya tradition\" (\"succession from guru to disciple\"). Each parampara belongs to a specific sampradaya, and may have own akharas and gurukulas.\n\nThe Sanskrit word literally means \"an uninterrupted row or series, order, succession, continuation, mediation, tradition\". In the traditional residential form of education, the shishya remains with his or her guru as a family member and gets the education as a true learner.\n\nIn some traditions there is never more than one active master at the same time in the same \"guruparamaparya\" (lineage).\n\nIn the paramparā system, knowledge (in any field) is passed down (undiluted) through successive generations. For example, division of Veda and its transfer through paramparas describes Bhagavata Purana. Other fields of knowledge taught may include spiritual, artistic (music or dance), or educational.\n\nThe current Acaryas, the heads of the maṭhas, trace their authority back to the four main disciples of Shankara, and each of the heads of these four maṭhas takes the title of Shankaracharya (\"the learned Shankara\") after Adi Shankara.\n\nThe Advaita guru-paramparā (\"Lineage of Gurus in Non-dualism\") begins with the mythological time of the Daiva-paramparā, followed by the vedic seers of the Ṛṣi-paramparā, and the Mānava-paramparā of historical times and personalities:\n\nIn paramapara, not only is the immediate guru revered, the three preceding gurus are also worshipped or revered. These are known variously as the \"kala-guru\" or as the \"four gurus\" and are designated as follows:\n\n\n\n"}
{"id": "23539", "url": "https://en.wikipedia.org/wiki?curid=23539", "title": "Probability axioms", "text": "Probability axioms\n\nThe Kolmogorov axioms are a fundamental part of Andrey Kolmogorov's probability theory.\nIn it, the probability \"P\" of some event \"E\", denoted formula_1, is usually defined as to satisfy these axioms.\nThe axioms are described below.\n\nThese assumptions can be summarised as follows: Let (Ω, \"F\", \"P\") be a measure space with \"P\"(Ω) = 1. Then (Ω, \"F\", \"P\") is a probability space, with sample space Ω, event space \"F\" and probability measure \"P\".\n\nAn alternative approach to formalising probability, favoured by some Bayesians, is given by Cox's theorem.\n\nThe probability of an event is a non-negative real number:\n\nwhere formula_3 is the event space. In particular, formula_1 is always finite, in contrast with more general measure theory. Theories which assign negative probability relax the first axiom.\n\nThis is the assumption of unit measure: that the probability that at least one of the elementary events in the entire sample space will occur is 1.\n\nThis is the assumption of σ-additivity:\nSome authors consider merely finitely additive probability spaces, in which case one just needs an algebra of sets, rather than a σ-algebra. Quasiprobability distributions in general relax the third axiom.\n\nFrom the Kolmogorov axioms, one can deduce other useful rules for calculating probabilities.\n\nIn some cases, formula_9 is not the only event with probability 0.\n\nIf A is a subset of, or equal to B, then the probability of A is less than, or equal to the probability of B.\n\nIt immediately follows from the monotonicity property that\n\nThe proofs of these properties are both interesting and insightful. They illustrate the power of the third axiom, and its interaction with the remaining two axioms. When studying axiomatic probability theory, many deep consequences follow from merely these three axioms.\nIn order to verify the monotonicity property, we set formula_12 and formula_13, where formula_14 and formula_15 for formula_16. It is easy to see that the sets formula_17 are pairwise disjoint and formula_18. Hence, we obtain from the third axiom that\nSince the left-hand side of this equation is a series of non-negative numbers, and since it converges to formula_20 which is finite, we obtain both formula_21 and formula_22.\nThe second part of the statement is seen by contradiction: if formula_23 then the left hand side is not less than infinity\nIf formula_25 then we obtain a contradiction, because the sum does not exceed formula_20 which is finite. Thus, formula_27. We have shown as a byproduct of the proof of monotonicity that formula_22.\n\nAnother important property is:\n\nThis is called the addition law of probability, or the sum rule.\nThat is, the probability that \"A\" \"or\" \"B\" will happen is the sum of the probabilities that \"A\" will happen and that \"B\" will happen, minus the probability that both \"A\" \"and\" \"B\" will happen. The proof of this is as follows:\n\nFirstly,\n\nSo,\n\nAlso, \n\nand eliminating formula_34 from both equations gives us the desired result.\n\nAn extension of the addition law to any number of sets is the inclusion–exclusion principle.\n\nSetting \"B\" to the complement \"A\" of \"A\" in the addition law gives\n\nThat is, the probability that any event will \"not\" happen (or the event's complement) is 1 minus the probability that it will.\n\nConsider a single coin-toss, and assume that the coin will either land heads (H) or tails (T) (but not both). No assumption is made as to whether the coin is fair.\n\nWe may define:\n\nKolmogorov's axioms imply that:\n\nThe probability of \"neither\" heads \"nor\" tails, is 0.\n\nThe probability of \"either\" heads \"or\" tails, is 1.\n\nThe sum of the probability of heads and the probability of tails, is 1.\n\n\n\n"}
{"id": "13932295", "url": "https://en.wikipedia.org/wiki?curid=13932295", "title": "Reflected appraisal", "text": "Reflected appraisal\n\nReflected appraisal is a term used in psychology to describe a person's perception of how others see and evaluate him or her. The reflected appraisal process concludes that people come to think of themselves in the way they believe others think of them (Mead, 1934; Cooley, 1902; Sullivan, 1947). This process has been deemed important to the development of a person's self-esteem, especially because it includes interaction with people outside oneself, and is considered one of the main influences on the development of self-concept.\n\nHarry Stack Sullivan first coined the term reflected appraisal in 1953 when he published \"The Interpersonal Theory of Psychiatry\", though Charles H. Cooley was the first to describe the process of reflected appraisal when he discussed his concept of the looking-glass self (1902). Although some of our self-views are gained by direct experience with our environment, most of what we know about ourselves is derived from others.\n\nIn 1979, Shrauger and Shoeneman found that rather than our self-concepts resembling the way others actually see us, our self-concepts are filtered through our perceptions and resemble how we think others see us. Felson (1993) explained that individuals are not very accurate in judging what others think of them. Among the causes of the discrepancy is the apprehension of others about revealing their views. At best, they may reveal primarily favorable views rather than both favorable and unfavorable views. Consistent with other research (DePaulo, Kenny, Hoover, Webb & Oliver, 1987, Keny & Albright, 1987), Felson found that individuals have a better idea of how groups see them than of how specific individuals see them. Presumably, individuals learn the group standards and then apply those standards. In turn, when group members judge individuals, they use the same standards that individuals originally applied to themselves. Thus we find a correspondence in self-appraisals and other’s appraisals of the self.\n\nThe extent to which reflected appraisals affect the person being appraised depends upon characteristics of the appraiser and his or her appraisal. Greater impact on the development of a person’s self-concept is said to occur when:\n(1) the appraiser is perceived as a highly credible source\n(2) the appraiser takes a very personal interest in the person being appraised \n(3) the appraisal is very discrepant with the person’s self-concept at the moment\n(4) the number of confirmations of a given appraisal is high \n(5) the appraisals coming from a variety of sources are consistent and \n(6) appraisals are supportive of the person’s own beliefs about himself or herself.\n\nSeveral studies have been conducted on the way reflected appraisal affects various relationships in a person's life. The idea that a person's self-concept is related to what that person perceives as another's opinion usually holds more weight with significant others. Appraisals from significant others such as parents, close friends, trusted colleagues, and other people the individual strongly admires, influences self-concept development and often has more influence than a stranger on a child's developing self-esteem. Study of this topic has led to the realization that people sometimes tend to anticipate what will happen in the future based on a previous perception.\n\nReflected appraisals are present among family members. All family members have opinions about one another and are typically less reticent to express them to each other than is the case outside of family relations. Siblings, especially, may be only too eager to give critical feedback regarding each other's behavior, appearance, social skills, and intelligence. Not all of these appraisals, of course, are equally significant for one's self-esteem. Both what is being appraised (with regard to its importance for one's self-concept) and who does the appraising, are important qualifiers. For children, on most things, the reflected appraisals of their parents may matter much more than those of their siblings.\n\nReflected appraisal has been the main process examined in studies of self-esteem within families. The bulk of this research has focused on the effects of parental behavior on children's self-esteem. In general, these studies find that parental support and encouragement, responsiveness, and use of inductive control are related positively to children's self-esteem (Gecas and Seff 1990). Most of these parental variables could be considered indicators of positive reflected appraisals of the child. They are also the parental behaviors found to be associated with the development of other positive socialization outcomes in children and adolescents (such as moral development, pro-social behavior, and academic achievement).\nIn the investigation of the reflected appraisal process with newly married couples, social status derived from one’s position in the social structure also influences the appraisal process. The spouse with the higher status (education, occupation, and income) in the marriage is more likely to not only influence their partner’s self-views, but also their partner’s views of them (Cast, Stets, & Burke, 1999). Spouses with a lower status in the marriage have less influence on the self-view of their higher status counterparts or on how their higher-status counterparts view them.\n\nThrough reflected appraisals, we are also given lines to speak in everyday situations (Murial & Joneward, 1971, pp. 68–100) that are sometimes so specific that some people refer to them as scripts. Through these scripts, we are given our lines, our gestures, and our characterizations. The scripts tell us how to act in future scenarios, and what is expected of us. Others tell us what they expect from us, how we should look, how we should behave, and how we should say our lines.\n\nThe messages we receive about ourselves during the process of reflected appraisal can become self-fulfilling prophecies. The Pygmalion effect, Rosenthal effect, and observer-expectancy effect show that biased expectancies could affect reality and create self-fulfilling prophecies through reflected appraisal. If you were given positive reflected appraisals when you were young, you probably have a good self-concept; if the appraisals were largely negative, your self-concept may suffer.\n"}
{"id": "16544416", "url": "https://en.wikipedia.org/wiki?curid=16544416", "title": "Source reduction", "text": "Source reduction\n\nSource reduction is activities designed to reduce the volume, mass, or toxicity of products throughout the life cycle. It includes the design and manufacture, use, and disposal of products with minimum toxic content, minimum volume of material, and/or a longer useful life. \n\nAn example of source reduction is use of a Reusable shopping bag at the grocery store; although it uses more material than a single-use disposable bag, the material per use is less. \n\nPollution Prevention (or P2) and Toxics use reduction are also called source reduction because they address the use of hazardous substances at the source.\n\nSource Reduction is achieved through improvements in design, production, use, reuse, recycling, and through Environmentally Preferable Purchasing (EPP). A Life-cycle assessment is useful to help choose among several alternatives and options.\n\nIn the United States, the Federal Trade Commission offers guidance for labelling claims: \"Source reduction\" refers to reducing or lowering the weight, volume or toxicity of a product or package. To avoid being misleading, source reduction claims must qualify the amount of the source reduction and give the basis for any comparison that is made. These principles apply regardless of whether a term like \"source reduced\" is used.\n\nThe Massachusetts Toxics Use Reduction Program (TURA) offers 6 strategies to achieve source reduction:\n\n\n"}
{"id": "173196", "url": "https://en.wikipedia.org/wiki?curid=173196", "title": "Spin network", "text": "Spin network\n\nIn physics, a spin network is a type of diagram which can be used to represent states and interactions between particles and fields in quantum mechanics. From a mathematical perspective, the diagrams are a concise way to represent multilinear functions and functions between representations of matrix groups. The diagrammatic notation often simplifies calculation because simple diagrams may be used to represent complicated functions. \n\nRoger Penrose is credited with the invention of spin networks in 1971, although similar diagrammatic techniques existed before his time. Spin networks have been applied to the theory of quantum gravity by Carlo Rovelli, Lee Smolin, Jorge Pullin, Rodolfo Gambini and others. \n\nSpin networks can also be used to construct a particular functional on the space of connections which is invariant under local gauge transformations.\n\nA spin network, as described in Penrose (1971), is a kind of diagram in which each line segment represents the world line of a \"unit\" (either an elementary particle or a compound system of particles). Three line segments join at each vertex. A vertex may be interpreted as an event in which either a single unit splits into two or two units collide and join into a single unit. Diagrams whose line segments are all joined at vertices are called \"closed spin networks\". Time may be viewed as going in one direction, such as from the bottom to the top of the diagram, but for closed spin networks the direction of time is irrelevant to calculations.\n\nEach line segment is labelled with an integer called a spin number. A unit with spin number \"n\" is called an \"n\"-unit and has angular momentum \"nħ/2\", where \"ħ\" is the reduced Planck constant. For bosons, such as photons and gluons, \"n\" is an even number. For fermions, such as electrons and quarks, \"n\" is odd.\n\nGiven any closed spin network, a non-negative integer can be calculated which is called the \"norm\" of the spin network. Norms can be used to calculate the probabilities of various spin values. A network whose norm is zero has zero probability of occurrence. The rules for calculating norms and probabilities are beyond the scope of this article. However, they imply that for a spin network to have nonzero norm, two requirements must be met at each vertex. Suppose a vertex joins three units with spin numbers \"a\", \"b\", and \"c\". Then, these requirements are stated as:\nFor example, \"a\" = 3, \"b\" = 4, \"c\" = 6 is impossible since 3 + 4 + 6 = 13 is odd, and \"a\" = 3, \"b\" = 4, \"c\" = 9 is impossible since 9 > 3 + 4. However, \"a\" = 3, \"b\" = 4, \"c\" = 5 is possible since 3 + 4 + 5 = 12 is even, and the triangle inequality is satisfied. \nSome conventions use labellings by half-integers, with the condition that the sum \"a\" + \"b\" + \"c\" must be a whole number.\n\nMore formally, a spin network is a (directed) graph whose edges are associated with irreducible representations of a compact Lie group and whose vertices are associated with intertwiners of the edge representations adjacent to it.\n\nA spin network, immersed into a manifold, can be used to define a functional on the space of connections on this manifold. One computes holonomies of the connection along every link (closed path) of the graph, determines representation matrices corresponding to every link, multiplies all matrices and intertwiners together, and contracts indices in a prescribed way. A remarkable feature of the resulting functional is that it is invariant under local gauge transformations.\n\nIn loop quantum gravity (LQG), a spin network represents a \"quantum state\" of the gravitational field on a 3-dimensional hypersurface. The set of all possible spin networks (or, more accurately, \"s-knots\" - that is, equivalence classes of spin networks under diffeomorphisms) is countable; it constitutes a basis of LQG Hilbert space.\n\nOne of the key results of loop quantum gravity is quantization of areas: the operator of the area \"A\" of a two-dimensional surface Σ should have a discrete spectrum. Every spin network is an eigenstate of each such operator, and the area eigenvalue equals\n\nwhere the sum goes over all intersections \"i\" of Σ with the spin network. In this formula,\n\nAccording to this formula, the lowest possible non-zero eigenvalue of the area operator corresponds to a link that carries spin 1/2 representation. Assuming an Immirzi parameter on the order of 1, this gives the smallest possible measurable area of ~10 cm.\n\nThe formula for area eigenvalues becomes somewhat more complicated if the surface is allowed to pass through the vertices, as with anomalous diffusion models. Also, the eigenvalues of the area operator \"A\" are constrained by ladder symmetry.\n\nSimilar quantization applies to the volume operator. The volume of 3D submanifold that contains part of spin network is given by a sum of contributions from each node inside it. One can think that every node in a spin network is an elementary \"quantum of volume\" and every link is a \"quantum of area\" surrounding this volume.\n\nSimilar constructions can be made for general gauge theories with a compact Lie group G and a connection form. This is actually an exact duality over a lattice. Over a manifold however, assumptions like diffeomorphism invariance are needed to make the duality exact (smearing Wilson loops is tricky). Later, it was generalized by Robert Oeckl to representations of quantum groups in 2 and 3 dimensions using the Tannaka–Krein duality.\n\nMichael A. Levin and Xiao-Gang Wen have also defined string-nets using tensor categories that are objects very similar to spin networks. However the exact connection with spin networks is not clear yet. String-net condensation produces topologically ordered states in condensed matter.\n\nIn mathematics, spin networks have been used to study skein modules and character varieties, which correspond to spaces of connections.\n\n\n\n\n"}
{"id": "30715017", "url": "https://en.wikipedia.org/wiki?curid=30715017", "title": "Sustainability standards and certification", "text": "Sustainability standards and certification\n\nSustainability standards and certifications are voluntary, usually third party-assessed, norms and standards relating to environmental, social, ethical and food safety issues, adopted by companies to demonstrate the performance of their organizations or products in specific areas. There are perhaps up to 500 such standards and the pace of introduction has increased in the last decade. The trend started in the late 1980s and 90s with the introduction of Ecolabels and standards for Organic food and other products. In recent years, numerous standards have been established and adopted in the food industry in particular. Most of them refer to the triple bottom line of environmental quality, social equity, and economic prosperity. A standard is normally developed by a broad range of stakeholders and experts in a particular sector and includes a set of practices or criteria for how a crop should be sustainable grown or a resource should be ethically harvested. This might cover, for instance, responsible fishing practices that don't endanger marine biodiversity, or respect for human rights and the payment of fair wages on a coffee or tea plantation. Normally sustainability standards are accompanied by a verification process - often referred to as \"certification\" - to evaluate that an enterprise complies with a standard, as well as a traceability process for certified products to be sold along the supply chain, often resulting in a consumer-facing label. Certification programmes also focus on capacity building and working with partners and other organisations to support smallholders or disadvantaged producers to make the social and environmental improvements needed to meet the standard.\n\nThe basic premise of sustainability standards is twofold. Firstly, they emerged in areas where national and global legislation was weak but where the consumer and NGO movements around the globe demanded action. For example, campaigns by Global Exchange and other NGOs against the purchase of goods from “sweatshop” factories by the likes of Nike, Inc., Levi Strauss & Co. and other leading brands led to the emergence of social welfare standards like the SA8000 and others. Secondly, leading brands selling to both consumers and to the B2B supply chain may wish to demonstrate the environmental or organic merits of their products, which has led to the emergence of hundreds of ecolabels, organic and other standards. A leading example of a consumer standard is the Fairtrade movement, administered by FLO International and exhibiting huge sales growth around the world for ethically sourced produce. An example of a B2B standard which has grown tremendously in the last few years is the Forest Stewardship Council’s standard (FSC) for forest products made from sustainable harvested trees.\n\nHowever, the line between consumer and B2B sustainability standards is becoming blurred, with leading trade buyers increasingly demanding Fairtrade certification, for example, and consumers increasingly recognizing the FSC mark. In recent years, the business-to-business focus of sustainability standards has risen as it has become clear that consumer demand alone cannot drive the transformation of major sectors and industries. In commodities such as palm oil, soy, farmed seafood, and sugar, certification initiatives are targeting the mainstream adoption of better practices and precompetitive industry collaboration. Major brands and retailers are also starting to make commitments to certification in their whole supply chain or product offering, rather than a single product line or ingredient .\n\nWith the growth of standards and certification as the major tool for global production and trade to become more sustainable and for the private sector to demonstrate sustainability leadership, it is essential that there are ways to assess the legitimacy and performance of different initiatives. Company and government buyers, as well as NGOs and civil society groups committed to sustainable production, need clarity on which standards and ecolabels are delivering \"real\" social, environmental and economic results. The ISEAL Alliance has emerged as the authority on good practice for sustainability standards and its Codes of Good Practice represent the most widely recognised guidance on how standards should be set up and implemented in order to be effective. By complying with these Codes and working with other certification initiatives, ISEAL members demonstrate their credibility and work towards improving their positive impacts.\n\nAttempts to address the problems caused by a multiplicity of certification initiatives led to the launch of The State of Sustainability Initiatives (SSI) project, facilitated by the United Nations Conference on Trade and Development (UNCTAD) and the International Institute for Sustainable Development (IISD) under the auspices of the Sustainable Commodity Initiative (SCI).\n\n\"Most\" sustainability standards that are being adopted today were initiated by social movements in particular countries, such as Rainforest Alliance in the United States and Fairtrade in the Netherlands. Other standards were initiated by individual companies, such as Utz Certified (Ahold), Starbucks C.A.F.E. (Starbucks), and Nespresso AAA (Nespresso). Some standards were launched by coalitions of private firms, development agencies, NGOs, and other stakeholders. For example, the Common Code for the Coffee Community (4C) was initiated by an alliance of large American coffee roasters, including Kraft Foods, Sara Lee and Nestle, assisted by the German Agency for Technical Cooperation and Development (GIZ). One important facilitator for the development of most global standards were series of local development projects involving NGOs, coffee roasters and producers in different developing countries. For example, the Fairtrade standard was developed based on pilot projects with Mexican farmers. 4C builds on development projects in Peru, Colombia and Vietnam, involving GIZ, major coffee roasters, and local producers.\n\nThe most widely established and adopted standards are in agriculture, with 40% of global coffee production certified to one of the main schemes, and approximately 15-20% of cocoa and tea production being compliant with major international standards. Forestry and wild seafood are also sectors in which standards have been influential, with certified production pushing past 10% of the global share. Cotton, palm oil, soy, biofuels and farmed seafood are some of the commodities in which certification is growing the fastest, due in part to major roundtables that have been set up to bring the whole industry together. More recently, standards have started to emerge for mining and the extraction of metals - including gold, silver, aluminium, and oil and gas - as well as for cattle, electronics, plastics and tourism.\n\nEvidence suggests that Corporate Social Responsibility (CSR) adopted willingly by firms will be much more effective than government regulated CSR so global standards by private companies show promise for effective social impact.\n\nThe creation of the ISEAL Alliance in 2002 was the first collaborative effort amongst a group of sustainability standards organisations to agree to follow common good practices in how their standards are implemented and also to work together to drive up the use of standards and certification globally.\n\nNumerous sustainability standards have been developed in recent years to address issues of environmental quality, social equity, and economic prosperity of global production and trade practices. Despite similarities in major goals and certification procedures, there are some significant differences in terms of their historical development, target groups of adopters, geographical diffusion, and emphasis on environmental, social or economic issues.\n\nOne of the major differences to be aware of is based on the level of strictness of the standard. Some standards set the bar high for a sector, promoting the strongest social and environmental practices and working with the top performers to constantly push up sustainability expectations. Other standards are more focused on the elimination of the worst practices and operate at more of an entry-level to get a large proportion of an industry working incrementally towards better practices. Often times there are strategies between standards to move producers along this performance ladder of sustainability. Another important distinction is that some standards can be applied internationally (usually with mechanisms to ensure local relevance and appropriateness) whereas other standards are developed entirely with a regional or national focus.\n\nAdditional differences between standards might relate to the certification process and whether it is conducted by a first, second or third party; the traceability system in place and whether it allows for the segregation or mixing of certified and non-certified materials; and the types of sustainability claims that are made on products.\n\nThe Fairtrade label was developed in the late 1980s by a Dutch development agency in collaboration with Mexican farmers. The initiative performs development work and promotes its political vision of an alternative economy, seeing its main objective in empowering small producers and providing these with access to and improving their position on global markets. The \"most distinguishing\" feature of the Fairtrade label is the guarantee of a minimum price and a social premium that goes to the cooperative and not to the producers directly. Recently, Fairtrade also adopted environmental objectives as part of their certification system.\n\nThe Rainforest Alliance was created in the late 1980s from a social movement and is committed to conserving rainforests and their biodiversity. One key element of the standard is the compulsory elaboration and implementation of a detailed plan for the development of a sustainable farm management system to assist wildlife conservation. Another objective is to improve workers’ welfare by establishing and securing \"sustainable\" livelihoods. Producer prices may carry a premium. Yet instead of guaranteeing a fixed floor price, the standard seeks to improve the economic situation of producers through higher yields and enhanced cost efficiency.\n\nUTZ Certified (formerly Utz Kapeh) was co-founded by the Dutch coffee roaster Ahold Coffee Company in 1997. It aims to create an open and transparent marketplace for socially and environmentally responsible agricultural products. \"Instruments\" include the UTZ Traceability System and the UTZ Code of Conduct. The traceability system makes certified products traceable from producer to final buyer and has \"stringent\" chains of custody requirements. The UTZ Code of Conduct emphasizes both environmental practices (e.g. biodiversity conservation, waste handling and water use) and social benefits (e.g. access to medical care, access to sanitary facilities at work).\n\nThe Organic standard was developed in the 1970s and is based on IFOAM Basic Standards. IFOAM stands for International Federation of Organic Agriculture Movements and is the leading global umbrella organization for the organic farming movement. The IFOAM Basic Standards provide a framework of minimum requirements, including the omission of agrochemicals such as pesticides and chemical-synthetic fertilizers. The use of animal feeds is also strictly regulated. Genetic engineering and the use of genetically modified organisms (GMOs) are forbidden.\n\nThe trustea code is designed to evaluate the social, economic, agronomic and environmental performance of Indian tea estates, smallholders and Bought Leaf Factories (BLFs).\n\nIt is expected that the compliance with the code not only improves competitiveness of the tea farms but also facilitates the tea farms in achieving compliance with national regulations and international sustainability standards in a step-wise approach. The applicable control points under eleven chapters are required to be adhered to within a four-year period. The India tea code allows producers to show that they operate responsibly– producing quality tea according to strict social and environmental standards. The verification under the code provides manufacturers with the assurance of responsible production and provides opportunities to credibly demonstrate this to their customers.\n\nSuRe® is a global voluntary standard which integrates key criteria of resilience and sustainability into infrastructure development, through various criteria across governance, social and environmental factors. It is currently developed under ISEAL guidelines by the Swiss Global Infrastructure Basel Foundation (GIB) and the French bank Natixis. GIB and Natixis launched the SuRe® standard at a COP21 event on 9 December 2015.\n\nOther types of standards include sector-specific schemes such as the Roundtable on Sustainable Palm Oil (RSPO); standards for climate and development interventions like the Gold Standard, retailer-led sustainability certification initiatives such as GlobalGAP;\nCorporate own-brand sustainability initiatives such as Starbucks' CAFE Practices; and national programmes such as the Irish Food Board's 'Origin Green' scheme.\n\nThe United Nations Forum on Sustainability Standards (UNFSS) is a joint initiative of FAO, UNEP, ITC, UNCTAD, and UNIDO on Sustainability Standards.\n\n\n\n"}
{"id": "44159140", "url": "https://en.wikipedia.org/wiki?curid=44159140", "title": "Symmetry breaking of escaping ants", "text": "Symmetry breaking of escaping ants\n\nSymmetry breaking of escaping ants is a phenomenon that happens when ants are constrained into a cell with two equivalent exits, and perturbed with an insect repellent. Contrary to intuition, ants tend to use one door more than the other on average (i.e., there is a symmetry breaking in the escape behavior), so they crowd on one of the doors, which decreases the evacuation efficiency.\n\nThe symmetry breaking phenomenon arises in experiments described as follows. Worker ants freshly collected from the field are enclosed into a circular cell with a glass cover in such a way that they can only move in two dimensions (i.e., ants cannot pass over each other). The cell has two exits located symmetrically relative to its center. We will describe first a \"reference\" experiment, and then the one where the \"escape symmetry\" is broken.\n\nIn the reference experiment, both doors are opened at the same time and let the ants escape. If the experiment is realized many times, in average we see that approximately the same number of ants use the left and the right doors. In this experiment the escape symmetry is not broken.\n\nThe second experiment involves a further step before opening the doors: an insect repellent fluid is poured into the cell at its center through a small hole in the glass cover. As a result, ants get very excited. If the experiment is realized many times, we see that the number of ants escaping by one of the doors (which can be randomly either the left one or the right ones) is significantly higher than the number of ants escaping by the other one, i.e., the escape symmetry is broken. The crowding of ants at one of the doors while the other one may be eventually free results in an inefficient evacuation in terms of time.\n\nGeng Li and coworkers from Beijing Normal University tested whether and how the density of a group influenced symmetry breaking in escaping ants. They used the red imported fire ant (\"Solenopsis invicta\") to repeat the experiment mentioned above with different total number of ants. The result shows that the symmetry breaking increases at low density but decreases after a peak. That is to say, when density is low, the ant group produces a collective escaping behavior; while at high density, the ant group behaves more like random particles.\n\nInspired by earlier computer simulations that predicted a symmetry breaking phenomenon when panicked humans escape from a room with two equivalent exits, E. Altshuler and coworkers from the University of Havana designed the experiment described in the section above, which revealed the symmetry breaking effect in the leaf-cutting ant \"Atta insularis\".\n\nThe ant \"Atta insularis\" (commonly called \"bibijagua\" in Cuba) displays a high degree of swarm intelligence, like most social insects, which has made them great survivors through millions of years of evolution. In Cuba, a person may be described as \"smarter than \"bibijaguas\"\" (\"sabe mas que las bibijaguas\"). The poor evacuation efficiency illustrated by the phenomenon of symmetry breaking in escaping ants is one of the few examples where bibijaguas do not seem to behave collectively in a smart way.\n\nThe basic idea is that the action of the repellent induces herd behavior in the ants. When ants are in \"panic\", they experience a strong tendency to follow each other. As a result, if a random fluctuation in the system produces a locally large amount of ants trying to reach one of the two doors, the fluctuation can be amplified because ants tend to follow the direction of the majority of individuals. Then, that specific door becomes crowded.\n\nE. Altshuler and coworkers were able to reproduce their symmetry breaking experiments in ants using a simplified version of the theoretical model proposed earlier by Helbing et al. for humans, based on the fact that walkers tend to follow the general direction of motion of their neighbors (\"Vicsek's rule\"), and such herd behavior increases as the so-called \"panic parameter\" increases. In the case of ants, the panic parameter is supposed to be low when no repellent is used, and high when the repellent is used.\n\nA more \"biologically-sensible\" model based on the deposition of an alarm pheromone by ants under stress also reproduces the symmetry breaking phenomenon, with the advantage that it also predicts the experimental output for different concentrations of ants in the cell.\n\nThe pheromone mechanism shares the key element of the previous models: stressed ants tend to \"follow the crowd\".\n\nFrom the statistical point of view, human beings and ants seem to behave in a similar way in certain scenarios, such as evacuation from a room or cell under stress, resulting in symmetry breaking phenomena: while ants increase their tendency to follow each other under stress, human beings seem to forget about Emergency evacuation strategies, and just run towards the door where most people run to. However, the enormous physical and behavioural differences between humans and ants imply also big differences in their collective behavior.\n\nIn the symmetry breaking experiment on ants, for example, the individuals are relatively \"polite\": rarely ants are crushed by other ants in the process or evacuation. When humans are in panic, however, deaths by overpressure or suffocation are common, like in the case of the Love Parade disaster occurred in Germany in 2010.\n\nVideos on symmetry breaking in escaping ants are available on YouTube.\n"}
{"id": "7737939", "url": "https://en.wikipedia.org/wiki?curid=7737939", "title": "Talmudic law", "text": "Talmudic law\n\nTalmudic law is the law that is derived from the Talmud based on the teachings of the Talmudic Sages. \n\n"}
{"id": "255059", "url": "https://en.wikipedia.org/wiki?curid=255059", "title": "Tensor (intrinsic definition)", "text": "Tensor (intrinsic definition)\n\nIn mathematics, the modern component-free approach to the theory of a tensor views a tensor as an abstract object, expressing some definite type of multi-linear concept. Their well-known properties can be derived from their definitions, as linear maps or more generally; and the rules for manipulations of tensors arise as an extension of linear algebra to multilinear algebra.\n\nIn differential geometry an intrinsic geometric statement may be described by a tensor field on a manifold, and then doesn't need to make reference to coordinates at all. The same is true in general relativity, of tensor fields describing a physical property. The component-free approach is also used extensively in abstract algebra and homological algebra, where tensors arise naturally.\n\nGiven a finite set of vector spaces over a common field \"F\", one may form their tensor product , an element of which is termed a tensor.\n\nA tensor on the vector space \"V\" is then defined to be an element of (i.e., a vector in) a vector space of the form:\n\nwhere \"V\" is the dual space of \"V\".\n\nIf there are \"m\" copies of \"V\" and \"n\" copies of \"V\" in our product, the tensor is said to be of and contravariant of order \"m\" and covariant order \"n\" and total order . The tensors of order zero are just the scalars (elements of the field \"F\"), those of contravariant order 1 are the vectors in \"V\", and those of covariant order 1 are the one-forms in \"V\" (for this reason the last two spaces are often called the contravariant and covariant vectors). The space of all tensors of type is denoted\n\nThe type tensors\n\nare isomorphic in a natural way to the space of linear transformations from \"V\" to \"V\". A bilinear form on a real vector space \"V\", , corresponds in a natural way to a type tensor in\n\nAn example of such a bilinear form may be defined, termed the associated \"metric tensor\" (or sometimes misleadingly the \"metric\" or \"inner product\"), and is usually denoted \"g\".\n\nA simple tensor (also called a tensor of rank one, elementary tensor or decomposable tensor ) is a tensor that can be written as a product of tensors of the form\nwhere \"a\", \"b\", ..., \"d\" are nonzero and in \"V\" or \"V\" – that is, if the tensor is nonzero and completely factorizable. Every tensor can be expressed as a sum of simple tensors. The rank of a tensor \"T\" is the minimum number of simple tensors that sum to \"T\" .\n\nThe zero tensor has rank zero. A nonzero order 0 or 1 tensor always has rank 1. The rank of a non-zero order 2 or higher tensor is less than or equal to the product of the dimensions of all but the highest-dimensioned vectors in (a sum of products of) which the tensor can be expressed, which is \"d\" when each product is of \"n\" vectors from a vector space of finite-dimensional vector space of dimension \"d\".\n\nThe term \"rank of a tensor\" extends the notion of the rank of a matrix in linear algebra, although the term is also often used to mean the order (or degree) of a tensor. The rank of a matrix is the minimum number of column vectors needed to span the range of the matrix. A matrix thus has rank one if it can be written as an outer product of two nonzero vectors:\nThe rank of a matrix \"A\" is the smallest number of such outer products that can be summed to produce it:\nIn indices, a tensor of rank 1 is a tensor of the form\n\nThe rank of a tensor of order 2 agrees with the rank when the tensor is regarded as a matrix , and can be determined from Gaussian elimination for instance. The rank of an order 3 or higher tensor is however often \"very hard\" to determine, and low rank decompositions of tensors are sometimes of great practical interest . Computational tasks such as the efficient multiplication of matrices and the efficient evaluation of polynomials can be recast as the problem of simultaneously evaluating a set of bilinear forms\nfor given inputs \"x\" and \"y\". If a low-rank decomposition of the tensor \"T\" is known, then an efficient evaluation strategy is known .\n\nThe space formula_10 can be characterized by a universal property in terms of multilinear mappings. Amongst the advantages of this approach are that it gives a way to show that many linear mappings are \"natural\" or \"geometric\" (in other words are independent of any choice of basis). Explicit computational information can then be written down using bases, and this order of priorities can be more convenient than proving a formula gives rise to a natural mapping. Another aspect is that tensor products are not used only for free modules, and the \"universal\" approach carries over more easily to more general situations.\n\nA scalar-valued function on a Cartesian product (or direct sum) of vector spaces\nis multilinear if it is linear in each argument. The space of all multlinear mappings from the product into \"W\" is denoted\n\"L\"(\"V\",\"V\"...,\"V\"; \"W\"). When \"N\" = 1, a multilinear mapping is just an ordinary linear mapping, and the space of all linear mappings from \"V\" to \"W\" is denoted .\n\nThe universal characterization of the tensor product implies that, for each multilinear function\nthere exists a unique linear function\nsuch that\nfor all \"v\" ∈ \"V\" and \"α\" ∈ \"V\".\n\nUsing the universal property, it follows that the space of (\"m\",\"n\")-tensors admits a natural isomorphism\n\nIn the formula above, the roles of \"V\" and \"V\" are reversed. In particular, one has\n\nand\nand\n\nDifferential geometry, physics and engineering must often deal with tensor fields on smooth manifolds. The term \"tensor\" is sometimes used as a shorthand for \"tensor field\". A tensor field expresses the concept of a tensor that varies from point to point on the manifold.\n\n"}
{"id": "259112", "url": "https://en.wikipedia.org/wiki?curid=259112", "title": "Terrorism insurance", "text": "Terrorism insurance\n\nTerrorism insurance is insurance purchased by property owners to cover their potential losses and liabilities that might occur due to terrorist activities.\n\nIt is considered to be a difficult product for insurance companies, as the odds of terrorist attacks are very difficult to predict and the potential liability enormous. For example, the September 11, 2001 attacks resulted in an estimated $31.7 billion loss. This combination of uncertainty and potentially huge losses makes the setting of premiums a difficult matter. Most insurance companies therefore exclude terrorism from coverage in casualty and property insurance, or else require endorsements to provide coverage.\n\nConcentration of risk is another factor in determining availability for terrorism insurance. Due to the concentrated losses of the World Trade Center, carriers were hit with large losses in one centralized location. Insurers seek to spread the coverage over a wider geographic area than as with other aggregate perils, such as flood.\n\nInsurance companies are using an approach that is similar to that used with natural catastrophe risks. A Swiss report suggested that in this case where demand is greater than the supply for terrorism coverage that a short-term solution is possible: a mix of government and private resource to make easy the transition. In this situation, the government would serve two functions: to establish rules to overcome the capacity shortage and to be the insurer of last resort.\n\nInsurance payments related to terrorism are restricted to a billion euro per year for all insurance companies together. This regards property insurance, but also life insurance, medical insurance, etc.\n\nIn 2002, the US Congress enacted the Terrorism Risk Insurance Act, in which the government shared the cost of large insurance losses.\n\nOn December 26, 2007, the President of the United States signed into law the Terrorism Risk Insurance Program Reauthorization Act of 2007 which extends the Terrorism Risk Insurance Act (TRIA) through December 31, 2014. The law extends the temporary federal Program that provides for a transparent system of shared public and private compensation for insured losses resulting from acts of terrorism.\n\nSome economists have supported U.S. government subsidies of terrorism insurance. Soon after the 9/11 terrorist attacks, economist Edwin Mills expressed concern over whether private developers could build real estate without subsidies for insurance. Economist David R. Barker argued that properly structured subsidies could increase overall economic efficiency. Other economists have argued against these subsidies.\n\nThe United States insurance market offers coverage to the majority of large companies which ask for it in their policies. The price of the policy depends on where the clients are residing and how much limit they buy.\nAccording to \"The Economist\", one of the best studies to understand TRIA has been the one undertaken in 2005 by the Center for Risk Management at the Wharton Business School (\"TRIA and Beyond\"; available on their website below).\n\nIn mid-2007 the idea of another extension to TRIA was tabled and is officially known as TRIREA, (Terrorism Risk Insurance Revision and Extension Act). Initially TRIREA contained several new provisions including a mandatory 'make available' clause for NCBR coverage (Nuclear, Chemical, Biological and Radiological) and the ending of the distinction between domestic and foreign events.\n\nThe act expired on December 31, 2014, but was renewed at the start of the next congress, with Obama signing the extension of the TIRA through 2020 on January 12, 2015. Up until the 2014 expiration, many experts warned that \"construction projects could be stalled and commercial loans on shopping malls, utilities and skyscrapers could be in jeopardy.\" In addition, according to the Baltimore Sun, the National Football League denied rumors that it would cancel the Super Bowl over the issue.\n\nThe New York Times reports that in Baghdad personal terrorism insurance is available. One company offers such insurance for $90, and if the customer is a victim of terrorism in the next year, it pays the heirs $3,500.\n\nIn the UK, following the Baltic Exchange bomb in 1992, all UK insurers stopped including terrorism coverage on their commercial insurance policies with effect from 1 January 1993 (home insurance policies were unaffected). As a consequence, the government and insurance industry established the Pool Reinsurance Company Ltd (Pool Re). Primarily funded by premiums paid by policyholders, the government guarantees the fund although any such support must be repaid from future premiums. To date, despite paying over £600 million in relation to thirteen separate claims, no government support has been necessary.\n\nIn France, a pool of insurers and reinsurers was set up on 1 January 2002 under the name Gestion de l'Assurance et de la RÉassurance des risques attentats et Actes de Terrorisme (GAREAT). GAREAT is constituted upon the principle of mutuality between its Members, all of whom are jointly liable, and relies on the support given to GAREAT by international reinsurers as well as by the French State which provides unlimited coverage to the GAREAT programme via unlimited treaties reinsured 100% by . As a non-profit-making Economic Interest Grouping mandated by its Members, GAREAT returns to the latter that part of the premiums which are not used to finance the reinsurance coverage at the close of each year.\n\nAccording to the policy agenda of The Real Estate Roundtable, long-term terrorism insurance is available in the following countries:\n\n\n\n"}
{"id": "46843499", "url": "https://en.wikipedia.org/wiki?curid=46843499", "title": "Timeline of the civil rights movement", "text": "Timeline of the civil rights movement\n\nThis is a timeline of the civil rights movement, a nonviolent freedom movement to gain legal equality and the enforcement of constitutional rights for African Americans. The goals of the movement included securing equal protection under the law, ending legally established racial discrimination, and gaining equal access to public facilities, education reform, fair housing, and the ability to vote.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "1588509", "url": "https://en.wikipedia.org/wiki?curid=1588509", "title": "Unimodality", "text": "Unimodality\n\nIn mathematics, unimodality means possessing a unique mode. More generally, unimodality means there is only a single highest value, somehow defined, of some mathematical object.\n\nIn statistics, a unimodal probability distribution or unimodal distribution is a probability distribution which has a single peak. The term \"mode\" in this context refers to any peak of the distribution, not just to the strict definition of mode which is usual in statistics. \n\nIf there is a single mode, the distribution function is called \"unimodal\". If it has more modes it is \"bimodal\" (2), \"trimodal\" (3), etc., or in general, \"multimodal\". Figure 1 illustrates normal distributions, which are unimodal. Other examples of unimodal distributions include Cauchy distribution, Student's t-distribution, chi-squared distribution and exponential distribution. Among discrete distributions, the binomial distribution and Poisson distribution can be seen as unimodal, though for some parameters they can have two adjacent values with the same probability.\n\nFigure 2 and Figure 3 illustrate bimodal distributions.\n\nOther definitions of unimodality in distribution functions also exist.\n\nIn continuous distributions, unimodality can be defined through the behavior of the cumulative distribution function (cdf). If the cdf is convex for \"x\" < \"m\" and concave for \"x\" > \"m\", then the distribution is unimodal, \"m\" being the mode. Note that under this definition the uniform distribution is unimodal, as well as any other distribution in which the maximum distribution is achieved for a range of values, e.g. trapezoidal distribution. Note also that usually this definition allows for a discontinuity at the mode; usually in a continuous distribution the probability of any single value is zero, while this definition allows for a non-zero probability, or an \"atom of probability\", at the mode.\n\nCriteria for unimodality can also be defined through the characteristic function of the distribution or through its Laplace–Stieltjes transform.\n\nAnother way to define a unimodal discrete distribution is by the occurrence of sign changes in the sequence of differences of the probabilities. A discrete distribution with a probability mass function, formula_1, is called unimodal if the sequence formula_2 has exactly one sign change (when zeroes don't count).\n\nOne reason for the importance of distribution unimodality is that it allows for several important results. Several inequalities are given below which are only valid for unimodal distributions. Thus, it is important to assess whether or not a given data set comes from a unimodal distribution. Several tests for unimodality are given in the article on multimodal distribution.\n\nA first important result is Gauss's inequality. Gauss's inequality gives an upper bound on the probability that a value lies more than any given distance from its mode. This inequality depends on unimodality.\n\nA second is the Vysochanskiï–Petunin inequality, a refinement of the Chebyshev inequality. The Chebyshev inequality guarantees that in any probability distribution, \"nearly all\" the values are \"close to\" the mean value. The Vysochanskiï–Petunin inequality refines this to even nearer values, provided that the distribution function is continuous and unimodal. Further results were shown by Sellke & Sellke.\n\nGauss also showed in 1823 that for a unimodal distribution\n\nand\n\nwhere the median is \"ν\", the mean is \"μ\" and \"ω\" is the root mean square deviation from the mode.\n\nIt can be shown for a unimodal distribution that the median \"ν\" and the mean \"μ\" lie within (3/5) ≈ 0.7746 standard deviations of each other. In symbols,\n\nwhere |.| is the absolute value.\n\nA similar relation holds between the median and the mode \"θ\": they lie within 3 ≈ 1.732 standard deviations of each other:\n\nIt can also be shown that the mean and the mode lie within 3 of each other.\n\nRohatgi and Szekely have shown that the skewness and kurtosis of a unimodal distribution are related by the inequality:\n\nwhere \"κ\" is the kurtosis and \"γ\" is the skewness.\n\nKlaassen, Mokveld, and van Es derived a slightly different inequality (shown below) from the one derived by Rohatgi and Szekely (shown above), which tends to be more inclusive (i.e., yield more positives) in tests of unimodality:\n\nAs the term \"modal\" applies to data sets and probability distribution, and not in general to functions, the definitions above do not apply. The definition of \"unimodal\" was extended to functions of real numbers as well.\n\nA common definition is as follows: a function \"f\"(\"x\") is a unimodal function if for some value \"m\", it is monotonically increasing for \"x\" ≤ \"m\" and monotonically decreasing for \"x\" ≥ \"m\". In that case, the maximum value of \"f\"(\"x\") is \"f\"(\"m\") and there are no other local maxima.\n\nProving unimodality is often hard. One way consists in using the definition of that property, but it turns out to be suitable for simple functions only. A general method based on derivatives exists, but it does not succeed for every function despite its simplicity.\n\nExamples of unimodal functions include quadratic polynomial functions with a negative quadratic coefficient, tent map functions, and more.\n\nThe above is sometimes related to as \"strong unimodality\", from the fact that the monotonicity implied is \"strong monotonicity\". A function \"f\"(\"x\") is a weakly unimodal function if there exists a value \"m\" for which it is weakly monotonically increasing for \"x\" ≤ \"m\" and weakly monotonically decreasing for \"x\" ≥ \"m\". In that case, the maximum value \"f\"(\"m\") can be reached for a continuous range of values of \"x\". An example of a weakly unimodal function which is not strongly unimodal is every other row in a Pascal triangle.\n\nDepending on context, unimodal function may also refer to a function that has only one local minimum, rather than maximum. For example, local unimodal sampling, a method for doing numerical optimization, is often demonstrated with such a function. It can be said that a unimodal function under this extension is a function with a single local extremum.\n\nOne important property of unimodal functions is that the extremum can be found using search algorithms such as golden section search, ternary search or successive parabolic interpolation.\n\nA function \"f\"(\"x\") is \"S-unimodal\" (often referred to as \"S-unimodal map\") if its Schwarzian derivative is negative for all formula_10, where formula_11 is the critical point.\n\nIn computational geometry if a function is unimodal it permits the design of efficient algorithms for finding the extrema of the function.\n\nA more general definition, applicable to a function f(X) of a vector variable X is that f is unimodal if there is a one-to-one differentiable mapping\n\"X\" = \"G\"(\"Z\") such that \"f\"(\"G\"(\"Z\")) is convex. Usually one would want \"G\"(\"Z\") to be continuously differentiable with nonsingular Jacobian matrix.\n\nQuasiconvex functions and quasiconcave functions extend the concept of unimodality to functions whose arguments belong to higher-dimensional Euclidean spaces.\n\n"}
{"id": "5816107", "url": "https://en.wikipedia.org/wiki?curid=5816107", "title": "Universal law", "text": "Universal law\n\nIn law and ethics, universal law or universal principle refers as concepts of legal legitimacy actions, whereby those principles and rules for governing human beings' conduct which are most universal in their acceptability, their applicability, translation, and philosophical basis, are therefore considered to be most legitimate. One type of Universal Law is the Law of Logic which prohibits logical contradictions known as sophistry. The Law of Logic is based upon the universal idea that logic is defined as that which is not illogical and that which is illogical is that which involves a logical contradiction, such as attempting to assert that an apple and no apple can exist at and in the same time and in the same place, and attempting to assert that A and not A can exist at and in the same time and in the same place. What goes around , comes around.\n\n"}
{"id": "434818", "url": "https://en.wikipedia.org/wiki?curid=434818", "title": "Veil of ignorance", "text": "Veil of ignorance\n\nThe \"veil of ignorance\" is a method of determining the morality of political issues proposed in 1971 by American philosopher John Rawls in his \"original position\" political philosophy. It is based upon the following thought experiment: people making political decisions imagine that they know nothing about the particular talents, abilities, tastes, social class, and positions they will have within a social order. When such parties are selecting the principles for distribution of rights, positions, and resources in the society in which they will live, this \"veil of ignorance\" prevents them from knowing who will receive a given distribution of rights, positions, and resources in that society. For example, for a proposed society in which 50% of the population is kept in slavery, it follows that on entering the new society there is a 50% likelihood that the participant would be a slave. The idea is that parties subject to the veil of ignorance will make choices based upon moral considerations, since they will not be able to make choices based on their own self- or class-interest.\n\nAs Rawls put it, \"no one knows his place in society, his class position or social status; nor does he know his fortune in the distribution of natural assets and abilities, his intelligence and strength, and the like\". The idea of the thought experiment is to render obsolete those personal considerations that are morally irrelevant to the justice or injustice of principles meant to allocate the benefits of social cooperation. The veil of ignorance is part of a long tradition of thinking in terms of a social contract that includes the writings of Immanuel Kant, Thomas Hobbes, John Locke, Jean Jacques Rousseau, and Thomas Jefferson.\n\nSpencer J. Maxcy outlines the concept as follows:Imagine that you have set for yourself the task of developing a totally new social contract for today's society. How could you do so fairly? Although you could never actually eliminate all of your personal biases and prejudices, you would need to take steps at least to minimize them. Rawls suggests that you imagine yourself in an original position behind a veil of ignorance. Behind this veil, you know nothing of yourself and your natural abilities, or your position in society. You know nothing of your sex, race, nationality, or individual tastes. Behind such a veil of ignorance all individuals are simply specified as rational, free, and morally equal beings. You do know that in the \"real world\", however, there will be a wide variety in the natural distribution of natural assets and abilities, and that there will be differences of sex, race, and culture that will distinguish groups of people from each other.\n\nIt has been argued that such a concept can have grand effects if it were to be practiced both in the present and in the past. Referring again to the example of slavery, if the slave-owners were forced through the veil of ignorance to imagine that they themselves may be slaves, then suddenly slavery may no longer seem justifiable. A grander example would be if each individual in society were to base their practices off the fact that they could be the least advantaged member of society. In this scenario, freedom and equality could possibly coexist in a way that has been the ideal of many philosophers. For example, in the imaginary society, one might or might not be intelligent, rich, or born into a preferred class. Since one may occupy any position in the society once the veil is lifted, the device forces the parties to consider society from the perspective of all members, including the worst-off and best-off members.\n\nThe concept of the veil of ignorance has been in use by other names for centuries by philosophers such as John Stuart Mill and Immanuel Kant whose work discussed the concept of the social contract. John Harsanyi helped to formalize the concept in economics, and argued that it provides an argument in favor of utilitarianism rather than an argument for a social contract. The modern usage was developed by John Rawls in his 1971 book \"A Theory of Justice\"\n\n\n"}
{"id": "45391102", "url": "https://en.wikipedia.org/wiki?curid=45391102", "title": "Viral dynamics", "text": "Viral dynamics\n\nViral dynamics is a field of applied mathematics concerned with describing the progression of viral infections within a host organism. It employs a family of mathematical models that describe changes over time in the populations of cells targeted by the virus and the viral load. These equations may also track competition between different viral strains and the influence of immune responses. The original viral dynamics models were inspired by compartmental epidemic models (e.g. the SI model), with which they continue to share many common mathematical features, such as the concept of the basic reproductive ratio (\"R\"). The major distinction between these fields is in the scale at which the models operate: while epidemiological models track the spread of infection between individuals within a population (i.e. \"between host\"), viral dynamics models track the spread of infection between cells within an individual (i.e. \"within host\"). Analyses employing viral dynamic models have been used extensively to study HIV, hepatitis B virus, and hepatitis C virus, among other infections\n\n"}
{"id": "57899729", "url": "https://en.wikipedia.org/wiki?curid=57899729", "title": "Völkerabfälle", "text": "Völkerabfälle\n\nVölkerabfälle is a term used by Frederick Engels to describe small nations which he considered residual fragments of former peoples who had succumbed to more powerful neighbours in the historic process of social development and which Engels considered prone to become \"fanatical standard-bearers of counter-revolution\".\n\nHe offers as examples:\n\nEngels was referring also specifically to the Serb uprising of 1848–49, in which Serbs from Vojvodina fought against the previously victorious Hungarian revolution. Engels finished the article with the folwing prediction:\nBut at the first victorious uprising of the French proletariat, which Louis Napoleon is striving with all his might to conjure up, the Austrian Germans and Magyars will be set free and wreak a bloody revenge on the Slav barbarians. The general war which will then break out will smash this Slav Sonderbund and wipe out all these petty hidebound nations, down to their very names.The next world war will result in the disappearance from the face of the earth not only of reactionary classes and dynasties, but also of entire reactionary peoples. And that, too, is a step forward.\n\nEngels views were criticised by leading socialist Karl Kautsky who in 1915 argued that subsequent to 1848/9 the Czech and Austrian South Slavs had transformed themselves in a manner which in practice refuted the views of Marx and Engels. Referring to Heinrich Cunow he wrote:\nToday when those people have achieved such great power and significance to refer to them in the old Marxian terms of 1848/9 does appear most unfortunate. Should someone today still hold onto that standpoint he has a lot to unlearn\n\nRoman Rozdolsky analyses Engels' position in his book \"Engels and the 'Nonhistoric' Peoples: the National Question in the Revolution of 1848\".\nThere are two ways to look at Marx and Engels: as the creators of a brilliant, but in its deepest essence, thoroughly critical, scientific method; or as church fathers of some sort, the bronzed figures of a monument. Those who have the latter vision will not have found this study to their taste. We, however, prefer to see them as they were in reality.\n\nRosdolsky examines Engels' viewpoint at length. Both Marx and Engels had supported the 1848 revolution which had spread throughout Europe. They saw this as a necessary first step towards the Socialist revolution, which was hoped to be imminent. However, the forces of reaction rallied in Klemens von Metternich's Austria in October 1848 with the brutal suppression of the Vienna Uprising. Engels was prompted to write his article thanks to the Austrian Slavs' rejection of the opportunity to liberate themselves from the oppressive rule of the Habsburgs, as well their enthusiastic embrace of Metternich’s counter-revolution.\n\nThis last passage, the final paragraph of Engels' article in 1849, has fuelled accusations that this constitutes a call for genocide. The British Liberal historian, George Watson, for example, cited this text by Engels as evidence for his view that Hitler was a Marxist. He also participated in the conservative documentary \"The Soviet Story\", where he uses the translation of Völkerabfälle as \"racial trash\". The film attracted criticism from a range of people including Latvian MEP Tatjana Ždanoka, Finnish political activist Johan Bäckman and Latvian MP Boriss Cilevičs. Alexander Dyukov has provided evidence that the film relies on falsified images to give an historically inaccurate account of the Soviet Union.\n"}
{"id": "34154764", "url": "https://en.wikipedia.org/wiki?curid=34154764", "title": "WORM (Rotterdam)", "text": "WORM (Rotterdam)\n\nWORM is a Rotterdam based non-profit foundation and a multi-media alternative cultural centre focused on experimental, new media art, avant-garde and underground art, primarily music and movies. WORM is funded by the Triodos Bank and part of the culture nota 2009-2012 from the Dutch government. The foundation has received the Pendrecht Culture Prize and its venue is part of the Rotterdam culture plan.\n\nWORM organises festivals and concerts, movie nights, runs an independent record label, a radio station. Part of the organisation is also a media lab, a hackspace, and a music studio. The media lab and music studio both run free artist in residence programmes for artists and experimental musicians. Experimental musician Lukas Simonis is the programmer of the artist in residence programme for the music studio. The studio has a collection of experimental musical instruments built by Yuri Landman. Artists that have been in residence at WORM were Tujiko Noriko, Toktek, Machinefabriek, Jørgen Teller, One Man Nation, Blevin Blectum, Kevin Blechdom, Joe Howe, Eugene Chadbourne, Lucas Crane, Jim Xentos, Knull, Ben Butler and Mousepad, Colin Black, Harry Taylor (Action Beat), Stignoise, Hovatron, Martijn Comes, Danielle Lemaire and others.\n\nBesides the venue and studio the foundation also has a shop with left field music, art house movies and books about subjects related to the cultural focus of the organisation.\n\nWORM developed the Web 2.0 Suicide Machine. The organisation co-operates closely with International Film Festival Rotterdam, Poetry International, Incubate, State-X New Forms, Museumnacht.\n\nWORM was based at the Achterhaven till 2010. In 2011 it moved to the centre of Rotterdam and is currently located in the building formerly used as the Nederlands Fotomuseum (Dutch Photo Museum) at the Witte de Withstraat.\n\n\n"}
{"id": "5836134", "url": "https://en.wikipedia.org/wiki?curid=5836134", "title": "Waterfall chart", "text": "Waterfall chart\n\nA waterfall chart is a form of data visualization that helps in understanding the cumulative effect of sequentially introduced positive or negative values. These intermediate values can either be time based or category based. The waterfall chart is also known as a flying bricks chart or Mario chart due to the apparent suspension of columns (bricks) in mid-air. Often in finance, it will be referred to as a bridge.\n\nWaterfall charts were popularized by the strategic consulting firm McKinsey & Company in its presentations to clients.\n\nComplexity can be added to waterfall charts with multiple total columns and values that cross the axis. Increments and decrements that are sufficiently extreme can cause the cumulative total to fall above and below the axis at various points. Intermediate subtotals, depicted with whole columns, can be added to the graph between floating columns. \n\nThe waterfall, also known as a bridge or cascade, chart is used to portray how an initial value is affected by a series of intermediate positive or negative values. Usually the initial and the final values (end points) are represented by whole columns, while the intermediate values are shown as floating columns that begin based on the value of the previous column. The columns can be color-coded for distinguishing between positive and negative values.\n\nA waterfall chart can be used for analytical purposes, especially for understanding or explaining the gradual transition in the quantitative value of an entity which is subjected to increment or decrement. Often, a waterfall or cascade chart is used to show changes in revenue or profit between two time periods.\n\nWaterfall charts can be used for various types of quantitative analysis, ranging from inventory analysis to performance analysis.\n\nWaterfall charts are also commonly used in financial analysis to display how a net value is arrived at through gains and losses over time or between actual and budgeted amounts. Changes in cash flows or income statement line items can also be shown via a waterfall chart. Other non-business applications include tracking demographic and legal activity changes over time.\n\nThere are several sources for automatic creations of Waterfall Charts ( PlusX , Origin, etc.)\n"}
{"id": "389989", "url": "https://en.wikipedia.org/wiki?curid=389989", "title": "Wenedyk", "text": "Wenedyk\n\nWenedyk () is a naturalistic constructed language, created by the Dutch translator Jan van Steenbergen (who also co-created the international auxiliary language Interslavic). It is used in the fictional \"Republic of the Two Crowns\" (based on the \"Republic of Two Nations\"), in the alternate timeline of Ill Bethisad. Officially, Wenedyk is a descendant of Vulgar Latin with a strong Slavic admixture, based on the premise that the Roman Empire incorporated the ancestors of the Poles in their territory. Less officially, it tries to show what Polish would have looked like if it had been a Romance instead of a Slavic language. On the Internet, it is well-recognized as an example of the altlang genre, much like Brithenig and Breathanach.\n\nThe idea for the language was inspired by such languages as Brithenig and Breathanach, languages that bear a similar relationship to the Celtic languages as Wenedyk does to Polish. The language itself is based entirely on (Vulgar) Latin and Polish: all phonological, morphological, and syntactic changes that made Polish develop from Common Slavic are applied to Vulgar Latin. As a result, vocabulary and morphology are predominantly Romance in nature, whereas phonology, orthography and syntax are essentially the same as in Polish. Wenedyk uses the modern standard Polish orthography, including (for instance) for and for .\n\nWenedyk plays a role in the alternate history of Ill Bethisad, where it is one of the official languages of the Republic of the Two Crowns. In 2005 Wenedyk underwent a major revision due to a better understanding of Latin and Slavic sound and grammar changes. In the process, the author was assisted by the Polish linguist Grzegorz Jagodziński.\n\nThe dictionary on the WWW page linked below contains over 4000 entries.\n\nThe language has acquired some media attention in Poland, including a few online news articles and an article in the monthly \"Wiedza i Życie\" (\"Knowledge and Life\").\n\nWenedyk uses the Polish alphabet, which consists of the following 32 letters :\n\nAlso, there are seven digraphs, representing five phonemes (ch being identical with h, and rz with ż):\n\nPronunciation is exactly as in Polish. Stress almost always falls on the penultimate syllable. A preposition and a pronoun are generally treated as one word, and therefore, when the pronoun has only one syllable, the preposition is stressed.\n\nWenedyk does not have articles. This is a feature that distinguishes Wenedyk from all natural Romance languages. The reason for this is that Vulgar Latin showed only a rudimentary tendency toward the formation of articles, whereas they are absent in Polish and most other Slavic languages.\n\nNouns, pronouns and adjectives can have three genders (masculine, feminine, neuter), two numbers (singular, plural), and three cases:\nWenedyk also has a \"vocative case\". In most cases it has the same form as the direct case, but there are exceptions: O potrze! \"Oh father!\"\n\nNoun can be subdivided into four declensions. They are similar to the declension system in Latin:\n\nAdjectives always agree in gender, number and case with the noun they modify. They can be placed both before and after it.\n\nUnlike nouns, adjectives and other pronouns, personal pronouns do not use the direct case, but preserve the distinction between the nominative and accusative instead. They are displayed in the following chart:\n\nVerbs are inflected for person, number, mood and tense. The forms in the present tense are:\nBecause Latin and Proto-Slavonic had virtually identical person/number inflections, Wenedyk and Polish do also.\n\nWenedyk verbs have the following moods and tenses:\n\nWenedyk vocabulary as published on the internet consists of over 4000 words. Because of how it was developed from Vulgar Latin, Wenedyk words are closest to Italian, but with phonologic differences from Italian which may be compared to those distinguishing Portuguese from Spanish. The following charts of 30 shows what Wenedyk looks like in comparison to a number of other Romance languages; note that unlike Brithenig, where one-quarter of the words resembled Welsh words, only four Wenedyk words (not counting \"szkoła\", borrowed into Polish from Latin) resemble Polish words, due to the Slavic languages' greater distance from the Romance languages compared to the Celtic languages:\n\nThe Lord's Prayer:\n\n\"Potrze nostry, kwały jesz en czałór, sąciewkaty si twej numię.\" \n\"Owień twej rzeń.\" \n\"Foca si twa włątać, komód en czału szyk i sur cierze.\" \n\"Da nów odzej nostry pań kocidzany.\" \n\"I dziemieć nów nostrze dziewta, komód i nu dziemiećmy świew dziewtorzew.\" \n\"I nie endycz nosz en ciętaceń, uta liwra nosz dzie mału.\" \n\"Nąk twie są rzeń i pociestać i głurza, o siąprz. Amen.\"\nUniversal Declaration of Human Rights, Article I\n\n\"Tuci ludzie noską sie liwrzy i jekwali z rześpiece świej dzińtacie i swór drzecór. Li są dotaci ku rocenie i koszczęce i dziewię ocar piara wyniałtru en jenie frotrzeńtacie.\"\nIn the Ill Bethisad universe, there are two other languages which are related to Wenedyk: Slevan, which is spoken in that universe's counterpart of Slovakia; and Šležan, or Silesian, spoken in Silesia. Šležan mirrors Czech in much the same way Wenedyk does Polish, whereas Slevan, despite being located in Slovakia, is more similar to Hungarian and Croatian in its orthography. (The Romance \"mirror\" of Slovak is a dialect of Slevan spoken in Moravia called Moravľaňec.) (As if in compensation, Croatian in Ill Bethisad is forced to be noticeably different from Serbian by being made to resemble the now-virtually-missing Czech and Slovak. )\n\nAdditionally, in the famous \"The Adventures of Tintin\" series, the fictional language Syldavian may be thought of as the Germanic counterpart of Wenedyk, showing what Polish might have looked like if it were a Germanic and not a Slavic language. The nearly extinct Wymysorys language provides another real-life example of this. Ill Bethisad also has such a \"Slavo-Germanic\" language: Bohemian, spoken in that universe's Czech Republic, developed by amateur Czech linguist Jan Havliš.\n\n\n"}
