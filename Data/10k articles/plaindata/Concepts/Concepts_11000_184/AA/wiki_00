{"id": "2944526", "url": "https://en.wikipedia.org/wiki?curid=2944526", "title": "Abhyasa", "text": "Abhyasa\n\nAbhyāsa, in Hinduism, is a spiritual practice which is regularly and constantly practised over a long period of time. It has been prescribed by the great sage Patanjali Maharishi in his Yoga Sutras, and by Lord Krishna in the Bhagavad Gita as an essential means to control the mind, together with Vairāgya.\n\nSutra 1:12 \"Both practice (abhyāsa) and non-reaction (vairāgya) are required to still the patterning of consciousness.\" \n\nSutra 1:13 \"Practice is the sustained effort to rest in that stillness.\"—as translated by Chip Hartranft in his work \"The Yoga Sutra of Patanjali\".\n"}
{"id": "47548176", "url": "https://en.wikipedia.org/wiki?curid=47548176", "title": "Abortion Rights Campaign", "text": "Abortion Rights Campaign\n\nThe Abortion Rights Campaign (ARC) is an Irish pro-choice group. Its main aim is the introduction of free, safe and legal abortion care in Ireland and Northern Ireland. A significant aim prior to May 2018 was the repeal of the Eighth Amendment of the Irish Constitution, which was achieved by the successful passing of the Thirty-Sixth Amendment 2018. ARC also campaigns for the Northern Ireland Assembly to introduce extensive abortion legislation in common with the rest of the United Kingdom and \"to ensure the health of women in pregnancy is protected in line with international human rights standards\". ARC supports the full decriminalisation of abortion in Ireland.\n\nThe Abortion Rights Campaign was formed on 10 July 2012, when 40 people came together. Initially formed as the Irish Choice Network, after another meeting in the Gresham Hotel in Dublin on 8 December 2012 and another meeting in 19 January 2013 the Abortion Rights Campaign was formally launched. It has organised the annual March for Choice in Dublin since 2013.\n\nMany Irish politicians have supported ARC's aims, such as Richard Boyd Barrett (Anti-Austerity Alliance–People Before Profit), Catherine Murphy (Social Democrats), Mick Wallace, Joan Collins and Clare Daly (Independents 4 Change).. However ARC itself is not affiliated with any political party.\n\nARC was one of the main partner organisations in Together for Yes, the civil society group advocating a Yes vote in the 2018 referendum. One of ARC's founding members was a co-director of Together for Yes. ARC's network of regional groups across the island of Ireland formed the basis for many Together for Yes groups in several counties in Ireland. \n\nIn January 2016, they received a grant of €23,000 from Open Society Foundation for \"educational and stigma-busting projects.\" SIPO reasoned that the money had been raised for a political purpose. The ARC returned the money in November 2016, while disagreeing with the finding. The story was first published in \"The Irish Catholic\" in late March 2017.\n\nARC is an all-volunteer, non-hierarchical organisation. Membership elect a Convening Group annually, which convenes a Steering Group made up of representatives of several working groups and regional groups. Ongoing decision-making is carried out by the Steering Group during regular meetings. There is no one leader or spokesperson of ARC as the Convening and Working Group roles change on a regular basis. This structure is designed to encourage engagement and activity by all members, and to avoid power hierarchies.\n\nThe Abortion Rights Campaign's most visible event is the annual March for Choice, normally held on the Global Day of Action for Safe and Legal Access to Abortion. The March is organised by ARC and attended by the public, as well as various national pro-choice groups, including trade unions such as Unite, Mandate, Teachers Union of Ireland, National Union of Journalists and Union of Students in Ireland, political parties including Labour, Solidarity–People Before Profit, and the Social Democrats, and international groups such as Catholics for Choice.\n\nThe most recent March for Choice was held on 29 September 2018, the first since the vote to repeal the Eighth Amendment. The theme was 'Free Safe Legal', the slogan of ARC, and was chosen to \"show [their] support for stigma-free abortion access for anyone who wants or needs one, regardless of their financial or legal status\".\n\nSpeakers included ARC co-conveners Sarah Monaghan and Denise O'Toole, Zanele Sibindi of Movement of Asylum Seekers in Ireland, Evie Nevin of Disabled Women for Yes and Emma Campbell and Kellie O'Dowd from the Belfast-based Alliance for Choice.\n\nThe 2017 March for Choice was held on 30 September and over 40,000 people took part. The rally at the march's conclusion was addressed by Bernadette Devlin McAliskey. Additional \"sympathy marches\" were held in 20 cities around the world, including London, Sydney and Nicosia.\n"}
{"id": "57751953", "url": "https://en.wikipedia.org/wiki?curid=57751953", "title": "Active Student Response Techniques", "text": "Active Student Response Techniques\n\nActive student response (ASR) techniques are strategies to elicit observable responses from students in a classroom. They are grounded in the field of behavioralism and operate by increasing opportunities reinforcement during class time, typically in the form of instructor praise. Active student response techniques are designed so that student behavior, such as responding aloud to a question, is quickly followed by reinforcement if correct. Common form of active student response techniques are choral responding, response cards, guided notes, and clickers. While they are commonly used for disabled populations, these strategies can be applied at many different levels of education. Implementing active student response techniques has been shown to increase learning, but may require extra supplies or preparation by the instructor.\n\nActive student response techniques are grounded in the field of behaviorism, a movement in psychology that believes behaviors are responses to stimuli and motivated by past reinforcement. The field has its origins in experiments of Edward Thorndike, who pioneered the Law of effect, which is now known as reinforcement and punishment. Thorndike explained that behaviors that produce a positive effect become more likely to reoccur, given the same scenario. Conversely, behaviors that produce a negative effect become less likely to reoccur.\n\nPsychologist B.F. Skinner applied the principles of behaviorism to influence education. Skinner believed that students must be active in the classroom and that effective instruction is based on positive reinforcement. According to Skinner, teachers should avoid punishment, as it only teaches students to avoid punishment. Instead, lessons should be broken into small tasks with clear instruction and positive reinforcement. His beliefs led him to invent the teaching machine. Active student response techniques use Skinner's model to provide rapid reinforcement for desired responses. This increases the likelihood the responses will occur again. Also, these techniques may give instructors an opportunity to embrace technology in the classroom.\n\nThe foundation of active student response techniques is behaviorism's stimulus-response-reinforcement paradigm. A stimulus is any environmental change that may produce a response. In an academic setting, a stimulus is often a verbal cue. The response may be any change by the subject, such as an emotion or a behavior. Reinforcers are either positive or negative. In an academic setting, confirmation of a correct answer may be a positive reinforcer. So, active student response techniques aim to arrange the paradigm so the response is most correct. This includes separating instruction into small, achievable steps, providing clear and quick feedback, and including many repetitions. As a result, the correct response will have a strong relationship to the stimulus, creating more learning.\n\nChoral responding is a low-tech, high-ASR strategy that paces instruction throughout a class. In this strategy, students are prompted to respond orally in unison to questions posed by an instructor. For choral responding to work, questions must be presented clearly, be able to answered briefly, and have one correct answer. Choral responding is useful for reviewing subject matter, solving problems, or spelling words. It may be used to review previously covered content or provide feedback throughout a class period. Choral responding is effective in both small- and whole-group instruction, for students from preschool through secondary grades, in both general education and special education, as measured by learning outcomes.\n\nThe instructional format for choral responding is the following:\n\n\nResponse cards are cards, signs, or other objects held up by students in unison in response to a question posed by an instructor. Write-on response cards, such as whiteboards, allow students to write their answers on the spot and erase between learning trials. Pre-printed response cards are pre-made and are often true/false or colors to indicate answers. Both types of response cards promote high-ASR by allowing all students to respond to all questions, instead of one student responding to each. Instructors can easily gauge learning using response cards.\n\nThe instructional format of response cards is similar to choral responding, as follows:\n\n\nImplementing response cards in a classroom may increase questions posed by the instructor, increase academic performance, and be favored by students. The technique is effective in both general education and special education. Response cards may also increase on-task behavior in the classroom and decrease disruptive behavior. Response cards are most effective when paired with brisk instructional pacing. Instructors have been easily able to implement response cards and achieve a response rate of approximately one response per minute. The brisk instructional pace maximized responses in a class period without sacrificing accuracy and while minimizing problem behavior.\n\nNotetaking serves both a process and product function. But, traditional notetaking is dependent on attendance, criterion knowledge, and attention. Often, students may be inefficient note takers by recording incomplete, verbatim notes. Distributing instructor's notes increases academic performance. Guided notes aims to improve notetaking behavior by ensuring complete and accurate notes.\n\nGuided notes are prepared handouts containing standard cues to guide a student through a lecture. The handouts are often based on the instructor's notes with blank spaces throughout for the student to fill in. In the blank spaces, the student is to complete the information or concept, creating a high-ASR strategy with many response opportunities throughout the lecture. The blank spaces should be varied and may allow for many different types of responses, such as drawing pictures or graphs. While mostly low-tech, guided notes has the option to use many high-tech applications, such as software that makes guided notes from completed notes. Guided notes are used in both K-12 level and college levels. The active student response technique improves assessment scores, accuracy of notes, and student responses during lecture. Most students prefer guided notes over traditional notetaking.\n\nThe instructional format of guided notes is as follows:\n\n\nExample of guided notes:\n\nResponse strategies can be implemented using commercially available technologies, such as clickers or mobile phone apps. Similar to response cards, the instructor would pose a question and ask for a response from the class. Some of the process may be automated with a software that uses a clear signal for response and collects student responses. Though, high-tech strategies are not as well researched.\n\nActive student response techniques can be applied to nearly all levels of education. Also, the techniques can be used with multiple populations. The Individuals with Disabilities Education Act (IDEA) prioritizes accessible general education that includes individuals with disabilities, including those with autism spectrum disorder, intellectual disabilities, learning disabilities, and behavior disorders. Active student response benefits the inclusion of students with disabilities in classrooms by facilitating all students' participation in the classroom.\n\nHigher education may also benefit from active student response techniques.. Undergraduate classrooms would often benefit from implementing the techniques. While traditional lectures serve a function in higher education, such as setting context and synthesizing information, the reliance on passive learning is not as effective as active student learning.\n\nActive student response strategies can be either low- or high-tech. High-tech strategies, which use electrical devices, may utilize mobile phones, clickers, or other devices. Low-tech strategies do not require any electrical devices and may not require anything more than pencil and paper. Examples include guided notes and response cards. The range of technology that can be used broadens the applications of active student response.\n\nTraditional lecturing, in which an instructor speaks uninterrupted for the majority of the class, is often less effective than active student response techniques. Without active participation from students and contingent positive reinforcement for correct responses, traditional lecturing does not reinforce desired behaviors. Traditional lecturing may allow disorganized delivery of information. Implementing active student response techniques into lectures ensures the lectures function to assess, instruct, plan, and evaluate.\n\nActive student response techniques have been shown to increase learning, compared to traditional lecture. During lectures with active student response, students exhibited more on-task behavior, and instructors received more feedback. Frequency of active student responses is correlated with performance on evaluations. Instructors also remarked that active student participation provides instructors with clear feedback and promotes a more inclusive nature of the classroom.\n\nHowever, active student response also demands a higher cost. Instructors must supply or prepare any necessary materials. For example, to implement guided notes, instructors must write and print the notes to distribute to students. Similarly, the techniques demand more time from instructors. It may take added preparation to plan all of the questions throughout class. If an instructor chooses to use response cards, the instructor must prepare questions to which the class can respond.\n"}
{"id": "657858", "url": "https://en.wikipedia.org/wiki?curid=657858", "title": "Affordance", "text": "Affordance\n\nAffordance is what the environment offers the individual. James J. Gibson, coined the term in his 1966 book, \"The Senses Considered as Perceptual Systems\", and it occurs in many of his earlier essays (e.g.). However, his best-known definition is taken from his seminal 1979 book, \"The Ecological Approach to Visual Perception\":\n\nThe original definition in psychology includes all transactions that are possible between an individual and their environment. When the concept was applied to design, it started also referring to only those physical action possibilities of which one is aware.\n\nThe word is used in a variety of fields: perceptual psychology, cognitive psychology, environmental psychology, industrial design, human–computer interaction (HCI), interaction design, user-centered design, communication studies, instructional design, science, technology and society (STS), sports science and artificial intelligence.\n\nPsychologist James J. Gibson developed the concept of affordance over many years, culminating in his final book \"The Ecological Approach to Visual Perception\" in 1979. He defined an affordance as what the environment provides or furnishes the animal. Notably, Gibson compares an affordance with an ecological niche emphasizing the way niches characterize how an animal lives in its environment. An affordance is independent of an individual's ability to recognize it or even take advantage of it, and so, should not be confused with a privately perceived world or a phenomenology of perception. The key to understanding affordance is that it is relational and characterizes the suitability of the environment to the observer, and so, depends on their current intentions and their capabilities. For instance, a set of steps which rises four feet high does not afford climbing to the crawling infant, yet might provide rest to a tired adult or the opportunity to move to another floor for an adult who wished to reach an alternative destination. This notion of intention/needs is critical to an understanding of affordance, as it explains how the same aspect of the environment can provide different affordances to different people, and even to the same individual at another point in time. As Gibson puts it, “Needs control the perception of affordances (selective attention) and also initiate acts.” Gibson's is the prevalent definition in cognitive psychology.\n\nAccording to Gibson, humans tend to alter and modify their environment so as to change its affordances to better suit them. On his view, humans change the environment to make it easier to live in (even if making it harder for other animals to live in it): to keep warm, to see at night, to rear children, and to move around. This tendency to change the environment is natural to humans, and Gibson argues that it is a mistake to treat the social world apart from the material world or the tools apart from the natural environment. He points out that manufacturing was originally done by hand as a kind of manipulation.\n\nGibson argues that learning to perceive an affordance is an essential part of socialization. The theory of affordances introduces a \"value-rich ecological object\". Affordances cannot be described within the value-neutral language of physics, but rather introduces notions of benefits and injuries to someone. An affordance captures this beneficial/injurious aspect of objects and relates them to the animal for whom they are well/ill-suited. During childhood development, a child learns to perceive not only the affordances for the self, but also how those same objects furnish similar affordances to another. A child can be introduced to the conventional meaning of an object by manipulating which objects command attention and demonstrating how to use the object through performing its central function. By learning how to use an artifact, a child “enters into the shared practices of society” as when they learn to use a toilet or brush their teeth. And so, by learning the affordances, or conventional meaning of an artifact, children learn the artifact's social world and further, become a member of that world.\n\nAffordances were further studied by Eleanor J. Gibson, wife of James J. Gibson, who created her theory of perceptual learning around this concept. Her book, \"An Ecological Approach to Perceptual Learning and Development\", explores affordances further.\n\nJakob von Uexküll had already discussed the concept in the early twentieth century, calling it the \"functional tinting\" (') of organisms with respect to stimuli.\n\nAnderson, Yamagishi and Karavia (2002) found that merely looking at an object primes the human brain to perform the action the object affords.\n\nIn 1988, Donald Norman appropriated the term \"affordances\" in the context of human–machine interaction to refer to just those action possibilities that are readily perceivable by an actor. This new definition of \"action possibilities\" has now become synonymous with Gibson's work, although Gibson himself never made any reference to action possibilities in any of his writing. Through Norman's book \"The Design of Everyday Things\", this interpretation was popularized within the fields of HCI, interaction design, and user-centered design. It makes the concept dependent not only on the physical capabilities of an actor, but also on their goals, beliefs, and past experiences. If an actor steps into a room containing an armchair and a softball, Gibson's original definition of affordances allows that the actor may throw the chair and sit on the ball, because this is objectively possible. Norman's definition of (perceived) affordances captures the likelihood that the actor will sit on the armchair and throw the softball. Effectively, Norman's affordances \"suggest\" how an object may be interacted with. For example, the size, shape and weight of a softball make it perfect for throwing by humans, and it matches their past experience with similar objects. The focus on perceived affordances is much more pertinent to practical design problems , which may explain its widespread adoption.\n\nNorman later explained that this restriction of the term's meaning had been unintended, and that he would replace it by \"perceived affordance\" in any future revision of the book. However, the definition from his book has been widely adopted in HCI and interaction design, and both meanings are now commonly used in these fields.\n\nFollowing Norman's adaptation of the concept, \"affordance\" has seen a further shift in meaning where it is used as an uncountable noun, referring to the easy discoverability of an object or system's action possibilities, as in \"this button has good affordance\". This in turn has given rise to a use of the verb \"afford\" – from which Gibson's original term was derived – that is not consistent with its dictionary definition (to provide or make available): designers and those in the field of HCI often use \"afford\" as meaning \"to suggest\" or \"to invite\".\n\nThe different interpretations of affordances, although closely related, can be a source of confusion in writing and conversation if the intended meaning is not made explicit and if the word is not used consistently. Even authoritative textbooks can be inconsistent in their use of the term.\n\nWhen affordances are used to describe information and communications technology (ICT) an analogy is created with everyday objects with their attendant features and functions. Yet, ICT's features and functions derive from the product classifications of its developers and designers. This approach emphasizes an artifact’s convention to be wholly located in how it was designed to be used. In contrast, affordance theory draws attention to the fit of the technology to the activity of the user and so lends itself to studying how ICTs may be appropriated by users or even misused. One meta-analysis reviewed the evidence from a number of surveys about the extent to which the Internet is transforming or enhancing community. The studies showed that the internet is used for connectivity locally as well as globally, although the nature of its use varies in different countries. It found that internet use is adding on to other forms of communication, rather than replacing them.\n\nWilliam Gaver divided affordances into three categories: perceptible, hidden, and false.\n\n\nThis means that, when affordances are perceptible, they offer a direct link between perception and action, and, when affordances are hidden or false, they can lead to mistakes and misunderstandings.\n\n\nAnd by \n\n"}
{"id": "47411653", "url": "https://en.wikipedia.org/wiki?curid=47411653", "title": "Brain Emotional Learning Inspired Models", "text": "Brain Emotional Learning Inspired Models\n\nBrain Emotional Learning-inspired Models or BELiMs can be considered as a primary step to take inspiration from emotional systems to form new computational Intelligence (CI) models. BELIMs have been inspired by fear conditioning that is one of the emotional systems in mammalians and is a mechanism by which a biological system learns how to predict aversive events.\n\nThe hypothesis behind the development of BELiMs is that \"the neural structure of fear conditioning can be considered as a natural system that consists of some components (e.g., the amygdala, thalamus, and the sensory cortex) and whose functionality is fulfilled through interaction among its components. Fear conditioning as a system shows an intelligent behavior by learning how to respond to fear-induced stimuli and thus can be utilized to design new CI models for prediction and decision-making applications\" \n"}
{"id": "2454072", "url": "https://en.wikipedia.org/wiki?curid=2454072", "title": "Device paradigm", "text": "Device paradigm\n\nIn the philosophy of technology, the device paradigm is the way \"technological devices\" are perceived and consumed in modern society, according to Albert Borgmann. It explains the intimate relationship between people, things and technological devices, defining most economic relations and also shapes social and moral relations in general.\n\nThe concept of the device paradigm is a critical response to the Heidegger's notion of Gestell. It has been widely endorsed by philosophers of technology, including Hubert Dreyfus, Andrew Feenberg, and Eric Higgs, as well as environmental philosopher David Strong.\n\nFor Borgmann, a device is a thing that is used as a means to an end. Therefore, a device is seen as \"the compound of commodity and machinery\" while \"the distinctive pattern of division and connection of its components is the device paradigm.\" This term is meant to signify or distinguish between technological devices and \"focal things and practices,\" which matter to people in their everyday affairs. \n\nA focal thing is something of ultimate concern and significance, which may be masked by the device paradigm, and must be preserved by its intimate connection with practice. Borgmann used the case of wine to explain this. He cited that the focal thing in winemaking involves the implements used to produce wine. Wine becomes a device when it employs technology and machinery not merely to produce wine but obtain specific qualities such as grapey, smooth, light, and fruity flavors or clean and clear appearance. \n\nAs technological devices increase the availability of a commodity or service, they also push these devices into the background where people do not pay attention to their destructive tendencies. For example, the technology of central heating means that warmth is readily available and family members can retreat into the solitude of their rooms instead of working to chop wood or stoke the fires. Social interaction is reduced and the family struggles to find activities that enable such nurturing and care for each other. The ubiquitous nature of information technology also makes it an important example of device paradigm.\n\n\n"}
{"id": "7306242", "url": "https://en.wikipedia.org/wiki?curid=7306242", "title": "Epstein–Zin preferences", "text": "Epstein–Zin preferences\n\nIn economics, Epstein–Zin preferences refers to a specification of recursive utility.\n\nA recursive utility function can be constructed from two components: a time aggregator that characterizes preferences in the absence of uncertainty and a risk aggregator that defines the certainty equivalent function that characterizes preferences over static gambles and is used to aggregate the risk associated with future utility. With Epstein–Zin preferences, the time aggregator is a linearly homogenous CES aggregate of current consumption and the certainty equivalent of future utility. Specifically, the date-t utility index, formula_1, for a sequence of positive scalar consumptions formula_2, that are potentially stochastic for time periods beyond date t, is defined recursively as the solution to the nonlinear stochastic difference equation\nwhere formula_4 is a real-valued certainty equivalent operator. The parameter formula_5 determines the marginal rate of time preference, formula_6, and the parameter formula_7 determines the elasticity of intertemporal substitution, formula_8. Epstein and Zin considered a variety of certainty equivalent operators, but a popular choice for both theoretical and empirical research has been formula_9, where formula_10 denotes the expected value of probability distribution of formula_11, conditional on information available to the planner in date t. The parameter formula_12 encodes risk aversion, with smaller values of formula_13, other things equal, implying a stronger aversion to risk. The parameter restriction formula_14 results in a time-additive von Neumann–Morgenstern expected utility index.\n\nImportantly, unlike VNM utility functions (e.g. isoelastic utility), Epstein-Zin preferences allow the elasticity of intertemporal substitution (determined above by formula_15) to be unrelated to risk aversion (determined above by formula_13).\n\n"}
{"id": "45197219", "url": "https://en.wikipedia.org/wiki?curid=45197219", "title": "Escalation archetype", "text": "Escalation archetype\n\nThe escalation archetype is one of possible types of system behaviour that are known as system archetypes.\n\nThe escalation archetype is common for situations of non-cooperative games where each player can make own decisions and these decisions lead to the outcome for the player. However, when both player try to maximize their output (at the expense of the other one) they can get into a loop where each player will try harder and harder to surpass the opponent. While it can have favourable consequences it can also lead to self-destructive behaviour.\n\nEscalation archetype system can be described using causal loop diagrams which may consist of balancing and reinforcing loops.\n\nBalancing loop is a structure representing negative feedback process. In such a structure, a change in system leads to actions that usually eliminate the effect of that change which means that the system tends to remain stable in time.\n\nBalancing loop is a structure representing positive feedback process. This reinforcing feedback causes that even a small change in the system can lead to huge disturbances, e.g. variable A is increased which leads to an increase of variable B which leads to another increase of A and so there might be an exponential growth over time.\n\nThe image below shows escalation archetype as two balancing loops. \nWhen X makes an action, it leads to a change in results of X relative to results of Y. Y then makes action to equalize the situation and the result again changes the balance and induces another action by X. As this repeats actions done by X and Y are bigger and bigger to keep up with other's actions and results.\nThe causal loop diagram below shows escalation archetype as a single reinforcing loop. It can be read simply as that more action done by X creates bigger results of action done by X. The bigger results of X, the bigger difference between X and Y results. The bigger difference means more action by Y and more action by Y leads to bigger result of Y. The bigger result of Y leads to a smaller difference between X and Y, but the smaller is this difference, the bigger will be the action of X and it starts all over again.\nThe image below simplifies reinforcing loop and shows general idea of what happens. Increased activity of X leads to an increase of threat for Y which leads to an increased activity by Y. Increased activity by Y leads to increased threat for X which creates another potential for activity of X to grow.\n\nA well known example of escalation archetype is the arms race. The idea is that in the arms race two (or more) parties are competing to have the strongest army and weapons. An example is the race in producing nuclear weapons by the United States and the Soviet Union that was an important factor in the Cold War. Over the time, each party can get temporarily a slight advantage, but then the other one produces or obtains in other way more weapons and gets the advantage on its side, temporarily. In the end, both parties have great military power that is very threatening not only to them but also to non-aligned parties.\n\nThe escalation archetype can turn up in situations like picking apples in an orchard. Imagine a big apple orchard with a wealthy crop. An owner of such plantation cannot pick the whole harvest himself because there are simply too many trees. Therefore, he employs gatherers to do the work for him. He tries to figure a way how to measure the work they do so that he can reward them fairly. As he is suspicious that workers would work slowly he is hesitant to link the wage only to hours worked and he comes up with an idea. He divides workers into two groups and declares that the group which will harvest more apples will get a bonus to their regular wage.\n\nBoth groups start harvesting apples with passion and energy. First group X collects a pallet a little bit sooner than the second group Y. Therefore, the Y group motivates those who were a little bit slacking to increase their pace. Now Y is a little bit better so they not only catch up but even finish the second load before X group. The X comes with an idea that they should assign roles to their members – some will pick apples from the upper part of trees using ladders while some will collect those that are in the lower part of trees, other will load packs and one person will organise the work and help where necessary. This advantage enables X group to outrun the other again. While Y adapts the model of X, they make some changes and tweaks and soon they are the leading party. This improving of processes continues in several iterations until both parties are exceptionally effective in harvesting apples on the orchard. The owner can be really happy with the situation as containers are quickly being loaded with apples. Should everything continue this way, the orchard could be harvested in few hours with group X beating the group Y by a tenth of a percent. He could reward only the winning team or both teams because they were almost equally hardworking.\n\nHowever, due to the fact that one group was always a little bit behind, the situation in the middle of day is bad for one of the groups which is slightly behind, let's say it is group Y. Now, they can continue working like up to that moment and they would finish second with a loss of a tenth of a percent. Or they can come up with another idea that would enable them to increase their production output. They have an idea that harvesting topmost apples is slower that harvesting the rest of them. Because of that they decide to skip these apples and only collect the others. This way, the situation escalated into a trouble. While Y could win the competition now, there will be considerable amount of apples left on trees. Or if both groups are instructed not to leave a single apple in the orchard they will have to stay much longer to finish these apples thus the owner will have higher costs for their wage.\n\nOf course the owner could set a condition that none team could leave a tree that until it is fully harvested. That would help in some way to break the escalation archetype unless workers realize they are not punished for some other undesirable behaviour, for example being careless of tree condition after the harvest.\n\nAs can be seen in this example, the escalation archetype might bring positive results (faster harvesting) but it is necessary to monitor behaviour of the affected system to ensure long-term prosperity.\n\nTo avoid naming in this example of escalation archetype occurrence, behaviour of imaginary persons Amy, Betty, Cecile and Danielle is described.\n\nAmy, Betty, Cecile and Danielle want to become famous celebrities with a lot of money, loving husband and amazing children. They already have many friends and everybody in the town knows each one of them. They all work hard to be best in their field of expertise and soon they are known to the whole country. They know of each other and try to work harder to become more famous than each other. This is when escalation archetype comes into play. They become the most famous celebrities in the country and each one of them realizes that to draw more attention than others they need to be unique and very special. As Amy starts to work even harder than before Betty notices that Amy's fame is growing more than hers and starts to work harder than Amy. This is noted by Cecile and she does what must be done - starts working more than anyone else. But there is also Danielle whose ambitions are not even slightly smaller, she wants to be the most famous celebrity and so she starts working even harder than anyone else. As Amy notices her effort is not sufficient, she does what is obvious - starts to work more than she was.\n\nNow this cycle could repeat continuously until Amy, Betty, Cecile or Danielle dies of overwork. In the meanwhile some of them could start taking drugs with the presumption that it could boost their productivity and ability to concentrate or with the aim to get rid of depressions from working all the time. Another solution presumed by them as feasible would be some way of elimination the opponents - by false accusation or by pointing to their faults.\n\nOr if they found it impossible to be better by simply working more, they could try to figure out some way how to attract attention by qualitative change instead of quantitative. This way Amy could say something shocking in TV. The issue is that due to escalation archetype being in place unfortunately. Betty could simply follow by saying something even more shocking or controversial. Then, Cecile would feel threatened and so she will come up with an idea that she could make controversial photographs. Then Danielle will try to surpass everyone and will do some action that will attract attention of media and public. And they would escalate this to extreme situation (as the behaviour can develop exponentially).\n\nWhile at the beginning the competitiveness was beneficial for both Amy, Betty, Cecile, Danielle and public, in the long term many issues raised.\n\nWhat could be a meaningful solution for these ladies? They could have set some limits themselves in their minds, for example how much time they are willing to work to achieve their desire to be a famous celebrity and what is alright to do and what is not. If they are not able to do so, there has to be some mechanism from outside to stop them - e.g. family or friend giving them an advice, as it would be difficult to set legal boundaries in this area.\n\nTendency of parents to compare their children creates nourishing environment for escalation archetype. Parents tend to compare their kids with other children and among own kids. This creates pressure on children as they are expected to perform well.\n\nImagine a family with two kids named, for example, Lisa and Bartolomeo. Their parents are very much children-focused and hope their kids will get proper educations and live what they consider successful lives. They invest significant portions of both their family budget and time into both children and hope that this investment will pay off in the form of Lisa and Bartolomeo being successful in school and later in life.\n\nLisa and Bartolomeo are usual children with some hobbies who study diligently but without extreme passion. They simply do what they got to do. Their results are good but not perfect. So their parents come and start the escalation archetype by telling the kids that the one with better marks will go on special holiday of her or his choice. As both Lisa and Bartolomeo like travelling a lot, they start to study hard. To satisfaction of their parents children's marks get soon better which is very positive outcome. Yet a problem arises. As they both study really hard to keep pace with the other one, something might go wrong.\n\nFor example, when Bartolomeo is very creative and skillful painter but not so talented in studying usual subjects like math or languages. Sooner or later he will reach his limits. Then to keep up the good marks he will have to abandon his painting hobby and his talent will be wasted. Or he will try to sustain great marks and then Bartolomeo will start cheating during exams.\n\nHowever even when no negative effect happens there will be a difficulty how to reward children after Bartolome finishes second very closely. Should their parents appreciate only Lisa's effort or acknowledge also Bartolomeo's achievements. When they reward only Lisa, Bartolomeo could easily become demotivated and then it would be impossible to make use of positive effects of escalation archetype. On the other hand, rewarding both could mean the same and for both children as they realise that their extra effort does not have necessarily mean better outcome.\n\nThere is also an alternative version of how competition amongst children can be affected by escalation archetype. When all parents motivate children to improve in comparison to their peers, they will all study harder and harder while the differences amongst participating kids will remain relatively stable (and if teachers increase requirements they will even retain their marks). Under such simple circumstances most children might benefit from the competition nevertheless children with weaker intellectual skill may become isolated when they are no longer able to pursue others. Reversely in another alternative scenario where all children are demotivated to study for some reason, their results are worse and worse (and if teachers decrease requirements they will retain their marks while being less educated) and downward spiral is working in a way that situation gets worse and worse.\n\nThe dangers of systems with escalation archetypes are various. First, it might be difficult to identify the existence of archetype at the first sight. Then the behaviour of the system might look desirable at first and therefore requiring no immediate action. Another risk is a possibility of exponential growth within such structure. Finally the system might have different outcome in short term and long term.\n\nEscalation archetype comes with a possibility to make a big change in the system with a little input or a small action done at the beginning (due to the fact that it behaves like reinforcing loop).\n\nTo remove downward or upward spiral effect of escalation archetype there is a change in the system necessary which breaks the ongoing pattern. That change is typically switching the actors from non-cooperative game mode to cooperative game behaviour so that they stop escalating their actions to keep with others and rather find mutual solution and movement.\n\n"}
{"id": "1376899", "url": "https://en.wikipedia.org/wiki?curid=1376899", "title": "Euthymia (philosophy)", "text": "Euthymia (philosophy)\n\nEuthymia (, \"gladness, good mood, serenity\", literally \"good thumos\") is a term used by Democritus to refer to one of the root aspects of human life's goal.\n\nDiogenes Laërtius records Democritus' position as \"The chief good he asserts to be cheerfulness (euthymia); which, however, he does not consider the same as pleasure; as some people, who have misunderstood him, have fancied that he meant; but he understands by cheerfulness, a condition according to which the soul lives calmly and steadily, being disturbed by no fear, or superstition, or other passion.\"\n\nIn Seneca’s essay on tranquility, he uses the Greek word \"euthymia\", which he defines as “believing in yourself and trusting that you are on the right path, and not being in doubt by following the myriad footpaths of those wandering in every direction.”\n\n\n"}
{"id": "9302", "url": "https://en.wikipedia.org/wiki?curid=9302", "title": "Existence", "text": "Existence\n\nExistence is the ability to, directly or indirectly, interact with reality or, in more specific cases, the universe. The exact definition of existence is one of the most important and fundamental topics of ontology, the philosophical study of the nature of being, existence, or reality in general, as well as of the basic categories of being and their relations. Traditionally listed as a part of the major branch of philosophy known as metaphysics, ontology deals with questions concerning what things or entities exist or can be said to exist, and how such things or entities can be grouped, related within a hierarchy, and subdivided according to similarities and differences.\n\nMaterialism holds that the only things that exist are matter and energy, that all things are composed of material, that all actions require energy, and that all phenomena (including consciousness) are the result of material interactions.\n\nIdealism holds that the only things that exist are thoughts and ideas, that all things are composed of strings of reasoning, that all thing(s) require an associated idea of the thing(s), and that all the phenomena (including consciousness) are the result of an understanding of the imprint from the noumenal world in which lies beyond the thing-in-itself.\n\nThe word \"existence\" comes from the Latin word \"exsistere\" meaning \"to appear\", \"to arise\", \"to become\", or \"to be\", but literally, it means \"to stand out\" (\"ex-\" being the Latin prefix for \"out\" added to the causative of the verb \"stare\", meaning \"to stand\"). In a technical sense, this refers to \"standing out\" of both being and becoming, thus having the qualities of both.\n\nIn the Western tradition of philosophy, the earliest known comprehensive treatments of the subject are from Plato's \"Phaedo\", \"Republic\", and \"Statesman\" and Aristotle's \"Metaphysics\", though earlier fragmentary writing exists. Aristotle developed a comprehensive theory of being, according to which only individual things, called substances, fully have to be, but other things such as relations, quantity, time, and place (called the categories) have a derivative kind of being, dependent on individual things. In Aristotle's \"Metaphysics\", there are four causes of existence or change in nature: the material cause, the formal cause, the efficient cause and the final cause.\n\nThe Neo-Platonists and some early Christian philosophers argued about whether existence had any reality except in the mind of God. Some taught that existence was a snare and a delusion, that the world, the flesh, and the devil existed only to tempt weak humankind away from God.\n\nThe medieval philosopher Thomas Aquinas argued that God is pure being, and that in God essence and existence are the same. At about the same time, the nominalist philosopher William of Ockham argued, in Book I of his \"Summa Totius Logicae\" (\"Treatise on all Logic\", written some time before 1327), that Categories are not a form of Being in their own right, but derivative on the existence of individuals.\n\nThe early modern treatment of the subject derives from Antoine Arnauld and Pierre Nicole's \"Logic\", or \"The Art of Thinking\", better known as the \"Port-Royal Logic\", first published in 1662. Arnauld thought that a proposition or judgment consists of taking two different ideas and either putting them together or rejecting them:\n\nThe two terms are joined by the verb \"is\" (or \"is not\", if the predicate is denied of the subject). Thus every proposition has three components: the two terms, and the \"copula\" that connects or separates them. Even when the proposition has only two words, the three terms are still there. For example, \"God loves humanity\", really means \"God is a lover of humanity\", \"God exists\" means \"God is a thing\".\n\nThis theory of judgment dominated logic for centuries, but it has some obvious difficulties: it only considers proposition of the form \"All A are B.\", a form logicians call universal. It does not allow propositions of the form \"Some A are B\", a form logicians call existential. If neither A nor B includes the idea of existence, then \"some A are B\" simply adjoins A to B. Conversely, if A or B do include the idea of existence in the way that \"triangle\" contains the idea \"three angles equal to two right angles\", then \"A exists\" is automatically true, and we have an ontological proof of A's existence. (Indeed, Arnauld's contemporary Descartes famously argued so, regarding the concept \"God\" (discourse 4, Meditation 5)). Arnauld's theory was current until the middle of the nineteenth century.\n\nDavid Hume argued that the claim that a thing exists, when added to our notion of a thing, does not add anything to the concept. For example, if we form a complete notion of Moses, and superadd to that notion the claim that Moses existed, we are not adding anything to the notion of Moses.\n\nKant also argued that existence is not a \"real\" predicate, but gave no explanation of how this is possible. Indeed, his famous discussion of the subject is merely a restatement of Arnauld's doctrine that in the proposition \"God is omnipotent\", the verb \"is\" signifies the joining or separating of two concepts such as \"God\" and \"omnipotence\".\n\nSchopenhauer claimed that “everything that exists for knowledge, and hence the whole of this world, only object in relation to the subject, the perception of the perceiver, in a word, representation.” According to him there can be \"No object without subject\" because \"everything objective is already conditioned as such in manifold ways by the knowing subject with the forms of its knowing, and presupposes these forms…\"\n\nJohn Stuart Mill (and also Kant's pupil Herbart) argued that the predicative nature of existence was proved by sentences like \"A centaur is a poetic fiction\" or \"A greatest number is impossible\" (Herbart). Franz Brentano challenged this; so also (as is better known) did Frege. Brentano argued that we can join the concept represented by a noun phrase \"an A\" to the concept represented by an adjective \"B\" to give the concept represented by the noun phrase \"a B-A\". For example, we can join \"a man\" to \"wise\" to give \"a wise man\". But the noun phrase \"a wise man\" is not a sentence, whereas \"some man is wise\" is a sentence. Hence the copula must do more than merely join or separate concepts. Furthermore, adding \"exists\" to \"a wise man\", to give the complete sentence \"a wise man exists\" has the same effect as joining \"some man\" to \"wise\" using the copula. So the copula has the same effect as \"exists\". Brentano argued that every categorical proposition can be translated into an existential one without change in meaning and that the \"exists\" and \"does not exist\" of the existential proposition take the place of the copula. He showed this by the following examples:\n\nFrege developed a similar view (though later) in his great work \"The Foundations of Arithmetic\", as did Charles Sanders Peirce (but Peirce held that ). The Frege-Brentano view is the basis of the dominant position in modern Anglo-American philosophy: that existence is asserted by the existential quantifier (as expressed by Quine's slogan \"To be is to be the value of a variable.\" — \"On What There Is\", 1948).\n\nIn mathematical logic, there are two quantifiers, \"some\" and \"all\", though as Brentano (1838–1917) pointed out, we can make do with just one quantifier and negation. The first of these quantifiers, \"some\", is also expressed as \"there exists\". Thus, in the sentence \"There exists a man\", the term \"man\" is asserted to be part of existence. But we can also assert, \"There exists a triangle.\" Is a \"triangle\" — an abstract idea — part of existence in the same way that a \"man\" — a physical body — is part of existence? Do abstractions such as goodness, blindness, and virtue exist in the same sense that chairs, tables, and houses exist? What categories, or kinds of thing, can be the subject or the predicate of a proposition?\n\nWorse, does \"existence\" exist?\n\nIn some statements, existence is implied without being mentioned. The statement \"A bridge crosses the Thames at Hammersmith\" cannot just be about a bridge, the Thames, and Hammersmith. It must be about \"existence\" as well. On the other hand, the statement \"A bridge crosses the Styx at Limbo\" has the same form, but while in the first case we understand a real bridge in the real world made of stone or brick, what \"existence\" would mean in the second case is less clear.\n\nThe nominalist approach is to argue that certain noun phrases can be \"eliminated\" by rewriting a sentence in a form that has the same meaning but does not contain the noun phrase. Thus Ockham argued that \"Socrates has wisdom\", which apparently asserts the existence of a reference for \"wisdom\", can be rewritten as \"Socrates is wise\", which contains only the referring phrase \"Socrates\". This method became widely accepted in the twentieth century by the analytic school of philosophy.\n\nHowever, this argument may be inverted by realists in arguing that since the sentence \"Socrates is wise\" can be rewritten as \"Socrates has wisdom\", this proves the existence of a hidden referent for \"wise\".\n\nA further problem is that human beings seem to process information about fictional characters in much the same way that they process information about real people. For example, in the 2008 United States presidential election, a politician and actor named Fred Thompson ran for the Republican Party nomination. In polls, potential voters identified Fred Thompson as a \"law and order\" candidate. Thompson plays a fictional character on the television series \"Law and Order\". The people who make the comment are aware that \"Law and Order\" is fiction, but at some level, they may process fiction as if it were fact, a process included in what is called the Paradox of Fiction Another example of this is the common experience of actresses who play the villain in a soap opera being accosted in public as if they are to blame for the actions of the characters they play.\n\nA scientist might make a clear distinction between objects that exist, and assert that all objects that exist are made up of either matter or energy. But in the layperson's worldview, existence includes real, fictional, and even contradictory objects. Thus if we reason from the statement \"Pegasus flies\" to the statement \"Pegasus exists\", we are not asserting that Pegasus is made up of atoms, but rather that Pegasus exists in the worldview of classical myth. When a mathematician reasons from the statement \"ABC is a triangle\" to the statement \"triangles exist\", she is not asserting that triangles are made up of atoms but rather that triangles exist within a particular mathematical model.\n\nAccording to Bertrand Russell's Theory of Descriptions, the negation operator in a singular sentence can take either wide or narrow scope: we distinguish between \"some S is not P\" (where negation takes \"narrow scope\") and \"it is not the case that 'some S is P'\" (where negation takes \"wide scope\"). The problem with this view is that there appears to be no such scope distinction in the case of proper names. The sentences \"Socrates is not bald\" and \"it is not the case that Socrates is bald\" both appear to have the same meaning, and they both appear to assert or presuppose the existence of someone (Socrates) who is not bald, so that negation takes a narrow scope. However, Russell's theory analyses proper names into a logical structure which makes sense of this problem. According to Russell, Socrates can be analyzed into the form 'The Philosopher of Greece.' In the wide scope, this would then read: It is not the case that there existed a philosopher of Greece who was bald. In the narrow scope, it would read the Philosopher of Greece was not bald.\n\nAccording to the direct-reference view, an early version of which was originally proposed by Bertrand Russell, and perhaps earlier by Gottlob Frege, a proper name strictly has no meaning when there is no object to which it refers. This view relies on the argument that the semantic function of a proper name is to tell us \"which\" object bears the name, and thus to identify some object. But no object can be identified if none exists. Thus, a proper name must have a bearer if it is to be meaningful.\n\nAccording to the \"two sense\" view of existence, which derives from Alexius Meinong, existential statements fall into two classes.\n\n\nThe problem is then evaded as follows. \"Pegasus flies\" implies existence in the wide sense, for it implies that \"something\" flies. But it does not imply existence in the narrow sense, for we deny existence in this sense by saying that Pegasus does not exist. In effect, the world of all things divides, on this view, into those (like Socrates, the planet Venus, and New York City) that have existed in the narrow sense, and those (like Sherlock Holmes, the goddess Venus, and Minas Tirith) that do not.\n\nHowever, common sense suggests the non-existence of such things as fictional characters or places.\n\nInfluenced by the views of Brentano's pupil Alexius Meinong, and by Edmund Husserl, Germanophone and Francophone philosophy took a different direction regarding the question of existence.\n\nAnti-realism is the view of idealists who are skeptics about the physical world, maintaining either: (1) that nothing exists outside the mind, or (2) that we would have no access to a mind-independent reality even if it may exist. Realists, in contrast, hold that perceptions or sense data are caused by mind-independent objects. An \"anti-realist\" who denies that other minds exist (i. e., a solipsist) is different from an \"anti-realist\" who claims that there is no fact of the matter as to whether or not there are unobservable other minds (i. e., a logical behaviorist).\n\nThe Indian philosopher Nagarjuna (c. 150–250 CE) largely advanced existence concepts and founded the Madhyamaka school of Mahāyāna Buddhism.\n\nIn Eastern philosophy, Anicca (Sanskrit \"anitya\") or \"impermanence\" describes existence. It refers to the fact that all conditioned things (sankhara) are in a constant state of flux. In reality there is no thing that ultimately ceases to exist; only the appearance of a thing ceases as it changes from one form to another. Imagine a leaf that falls to the ground and decomposes. While the appearance and relative existence of the leaf ceases, the components that formed the leaf become particulate material that goes on to form new plants. Buddhism teaches a middle way, avoiding the extreme views of eternalism and nihilism. The middle way recognizes there are vast differences between the way things are perceived to exist and the way things really exist. The differences are reconciled in the concept of Shunyata by addressing the existing object's served purpose for the subject's identity in being. What exists is in non-existence, because the subject changes.\n\nTrailokya elaborates on three kinds of existence, those of desire, form, and formlessness in which there are karmic rebirths. Taken further to the Trikaya doctrine, it describes how the Buddha exists. In this philosophy, it is accepted that Buddha exists in more than one absolute way.\n\n\n\n"}
{"id": "47222814", "url": "https://en.wikipedia.org/wiki?curid=47222814", "title": "Fetal protection policies in the United States", "text": "Fetal protection policies in the United States\n\nFetal protection policies in the United States are various private sector rules intended to protect women's reproductive health and the health of developing fetuses in the workplace. These policies have evolved in response to the nature of many modern businesses, which use toxic chemicals or ionizing radiation during ordinary business and production activities. These policies have also evolved based on the liability a given business entity might incur, for example, for causing sterility or damage to an otherwise healthy fetus during pregnancy.\n\nThese policies were highlighted in the national media in the early 1990s when the U.S. Supreme Court reviewed a lower federal appellate court's decision in \"UAW v. Johnson Controls, Inc.\". Also, scholarly journals have discussed these policies before and after the \"Johnson Controls\" case.\nThe U.S. Supreme Court held that these policies violate Title VII of the 1964 Civil Rights Act as amended by the Pregnancy Discrimination Act of 1978 by promoting gender discrimination.\n"}
{"id": "303739", "url": "https://en.wikipedia.org/wiki?curid=303739", "title": "Generic role-playing game system", "text": "Generic role-playing game system\n\nA generic or universal role-playing game system is a role-playing game system designed to be independent of setting and genre. Its rules should, in theory, work the same way for any setting, world, environment or genre in which one would want to play.\n\nThe term \"generic\" has been used since the earliest days of gaming to describe a system that can be used for any type or style of game. There is some dispute among role-playing enthusiasts on when the concept of a generic system originated and which was the first one published.\n\nAccording to Shannon Appelcline, Chaosium's \"Basic Role-Playing\" (\"BRP\", 1980), was the first generic role-playing system. \"BRP\" was a \"cut-down\" version of Chaosium's \"RuneQuest\" role-playing game and formed the foundation for the \"Stormbringer\" RPG, and was also adopted for \"Call of Cthulhu\", the first horror role-playing game. The publication of \"GURPS\" (\"Generic Universal Role-Playing System\", 1986) as a completely setting-independent game and its commercial and creative success added credence to the movement. The development of the \"Hero System\" (1989) from the superhero role-playing game \"Champions\" also had a profound influence in popularizing the concept. \n\nIt truly became a dominant subject in RPG design with the release of the Third Edition of \"Dungeons & Dragons\" (2000) and the \"d20 System\" along with the creation of the Open Gaming License (OGL).\n\nThe \"Fuzion\" 5.02 rules uses the term \"generic\" to describe its basic ruleset as separate from its \"Champions\" and \"Interlock\" forerunners. \nIn the second paragraph of the introduction to \"GURPS\" 3rd Edition the authors define \"generic\" as a means to satisfy players and game masters of many styles of play and feel for rules. \nThis is repeated in the updated 4th edition rules along with acknowledgments to \"Champions\" as the first truly flexible character creation system.\n\nSome \"d20\" derivative, such as Green Ronin Publishing's Mutants & Masterminds and \"True20 Adventure Roleplaying\", are presented as fully generic systems.\n\n\n"}
{"id": "29213071", "url": "https://en.wikipedia.org/wiki?curid=29213071", "title": "Georges Besançon", "text": "Georges Besançon\n\nGeorges Besançon (1866–1934) founded and edited the aeronautical journal \"L'Aérophile\".\n\nBesançon was a ballonist (\"aeronaut\") and journalist.\nBesançon helped train the later-celebrated balloonist Salomon Andrée, probably in the late 1880s.\n\nIn 1892, Besançon and scientist Gustave Hermite sent instruments on fabric or paper balloons into the upper atmosphere for meteorological research.\nIn 1901, Hermite and Besançon sent up small instrumented rubber balloons that were designed to expand until at a high altitude they would burst. Then their instruments would descend by parachute.\n\nBesançon founded the aeronautical periodical \"L'Aérophile\" in 1893, and remained its director until at least 1910. There he covered and reported on the era in which the airplane was invented and an international airplane industry arose.\n"}
{"id": "544685", "url": "https://en.wikipedia.org/wiki?curid=544685", "title": "Gottfried Helnwein", "text": "Gottfried Helnwein\n\nGottfried Helnwein (born 8 October 1948) is an Austrian-Irish visual artist. He has worked as a painter, draftsman, photographer, muralist, sculptor, installation and performance artist, using a wide variety of techniques and media.\n\nHis work is concerned primarily with psychological and sociological anxiety, historical issues and political topics. His subject matter is the human condition. The metaphor for his art is dominated by the image of the child, particularly the wounded child, scarred physically and emotionally from within. His works often reference taboo and controversial issues from recent history, especially the Nazi rule and the horror of the Holocaust. As a result, his work is often considered provocative and controversial.\n\nHelnwein studied at the University of Visual Art in Vienna (Akademie der Bildenden Künste, Wien). He lives and works in Ireland and Los Angeles.\n\nHelnwein was born in Vienna shortly after World War II. His father Joseph Helnwein worked for the Austrian Post and Telegraphy administration (Österreichische Post- und Telegraphenverwaltung), and his mother Margarethe was a housewife.\n\nHelnwein spent his childhood in a strict Roman Catholic upbringing. As a student he organized plays and art exhibitions at the Catholic Marian Society (Marianische Kongregation) of the Jesuit University Church in Vienna.\n1965 he enrolled at the \"Higher Federal Institution for Graphic Education and Experimentation\" in Vienna (Höhere Bundes-Graphische Lehr- und Versuchsanstalt, Wien).\nIn the following years he started his first performances for small audiences where he cut his face and hands with razor blades and bandaged himself.\n\nFrom 1969 to 1973 he studied at the University of Visual Art in Vienna (Akademie der Bildenden Künste, Wien). \nHe was awarded the \"Master-class prize\" () of the University of Visual Art, Vienna, the Kardinal-König prize and the Theodor-Körner prize.\n\nIn 1983 Helnwein met Andy Warhol in his Factory in New York City, who posed for a series of photo-sessions.\n\nHelnwein was offered a chair by the University of Applied Sciences in Hamburg in 1982. When his demand to admit also children to study at the university was rejected, he declined.\n\nIn 1985 Rudolf Hausner, recommended Helnwein as his successor as professor of the master-class for painting at the University of Visual Art in Vienna, but Helnwein left Vienna and moved to Germany.\n\nHe bought a medieval castle close to Cologne and the Rhine-river. Four years later in 1989 he established a studio in Tribeca New York and thenceforth spent his time between the United States and Germany.\n\nHelnwein moved to Dublin, Ireland in 1997 and one year later, he bought Castle Gurteen de la Poer in County Waterford.\nIn 2002 he established a studio in downtown Los Angeles and he lives and works since then in Ireland and Los Angeles. \nHelnwein has four children with his wife Renate: Cyril, Mercedes, Ali Elvis and Wolfgang Amadeus, who are all artists. In 2004 Helnwein received Irish citizenship.\n\nOn 3 December 2005, his friend Marilyn Manson and Dita Von Teese were married in a private, non-denominational ceremony at Helnwein's castle. The wedding was officiated by surrealist film director Alejandro Jodorowsky Gottfried Helnwein was best man \nIn 2013 the Albertina Museum in Vienna organized a retrospective of Helnwein's work. The show was seen by 250,000 visitors and was the most successful exhibition of a contemporary artist in the history of the Albertina.\n\nHelnwein is part of a tradition going back to the 18th century, to which Messerschmidt's grimacing sculptures belong. One sees, too, the common ground of his works with those of Arnulf Rainer and Hermann Nitsch, two other Viennese, who display their own bodies in the frame of reference of injury, pain, and death. And one sees how this fascination with body language goes back to the expressive gesture in the work of Egon Schiele.\n\nHelnwein's early work consists mainly of hyper-realistic watercolors, depicting wounded children, as well as photographs and performances – often with children – in public spaces. The bandaged child became the most important figure next to the artist himself allied with him in his actions: the embodiment of the innocent, defenceless individual at the mercy of brute force.\n\nArt historian Peter Gorsen specified the relation between Helnwein's work and Viennese Actionism:\n\nHelnwein must be set apart from Viennese Actionism as he does not reduce the child's body to mere aesthetic material (as in the \"material actions\" of Günter Brus, Hermann Nitsch, and Otto Muehl), but instead endows it with a symbolic function in representing defenceless, sacrificed man. The sexualistic concept of the child in (Freud-influenced) Viennese Actionism is countered by the moralist and utopian Helnwein with the child as a sexless salvation figure.\n\nIn 2004, The Fine Arts Museums of San Francisco organized the first one-person exhibition of Gottfried Helnwein at an American Museum: \"The Child, works by Gottfried Helnwein\" at the California Palace of the Legion of Honor.\n\nHarry S. Parker III, director of the Fine Arts Museums of San Francisco stated: \nFor Helnwein, the child is the symbol of innocence, but also of innocence betrayed. In today's world, the malevolent forces of war, poverty, and sexual exploitation and the numbing, predatory influence of modern media assault the virtue of children. Helnwein's work concerning the child includes paintings, drawings, and photographs, and it ranges from subtle inscrutability to scenes of stark brutality.\nOf course, brutal scenes – witness The Massacre of the Innocents – have been important and regularly visited motifs in the history of art. What makes Helnwein's art significant is its ability to make us reflect emotionally and intellectually on the very expressive subjects he chooses. Many people feel that museums should be a refuge in which to experience quiet beauty divorced from the coarseness of the world. This notion sells short the purposes of art, the function of museums, and the intellectual curiosity of the public.\nThe Child: Works by Gottfried Helnwein will inspire and enlighten many; it is also sure to upset some. It is not only the right but the responsibility of the museum to present art that deals with important and sometimes controversial topics in our society\".\n\nThe show was seen by almost 130,000 visitors and the \"San Francisco Chronicle\" quoted it the most important exhibition of a contemporary artist in 2004.\nSteven Winn, \"Chronicle\" Arts and Culture critic, wrote: \"Helnwein's large format, photo-realist images of children of various demeanors boldly probed the subconscious. Innocence, sexuality, victimization and haunting self-possession surge and flicker in Helnwein's unnerving work\".\n\nAt the same time when Helnwein painted watercolors of injured and abused children, from 1969, around 1970/71 he also began a series of self-portrayals in photographs and performances (actions) in his studio and in the streets of Vienna. \nActionistic self-portrayals in the manner of a happening featuring his injured and bandaged body and surgical instruments deforming his face go back to Helnwein's student days. Since then, bandages have become part of the aesthetic \"uniform\" of his self-portraits.\n\nThe artist exposed himself as victim and martyr: bandages around his head and forks and surgical instruments piercing his mouth or cheek. Frequently the distortions of these tormented images make it difficult to recognize Helnwein's face. He appears as a screaming man, mirroring the frightening aspects of life: a twentieth-century Man of Sorrows. His frozen cry, showing the artist in a state of implacable trauma, recalls Edvard Munch's \"Scream\" and Francis Bacon's screaming popes. \nSome of Helnwein's grimacing faces also recall the grotesque physiognomic distortions by the eighteenth-century Viennese sculptor Franz Xavier Messerschmidt. They could also be seen as part of the Austrian pictorial tradition that resurfaced in the perturbed and distorted expressionist faces painted by Kokoschka and Egon Schiele before World War I, reappearing in the exaggerated mimicry in Arnulf Rainer's \"Face Farces.\"\nWilliam S. Burroughs commented on Helnwein's self-portraits in an essay in 1992:\nThere is a basic misconception that any given face, at any given time, looks more or less the same, like a statue's face. Actually, the human face is as variable from moment to moment as a screen on which images are reflected, from within and from without.\nGottfried Helnwein's paintings and photographs attack this misconception, showing the variety of faces of which any face is capable. And in order to attack the basic misconception, he must underline and exaggerate by distortion, by bandages and metal instruments that force the face into impossible molds. Images of torture and madness abound, as happens from moment to moment in the face seen as a sensitive reflection of extreme perceptions and experience. How can a self-portrait depict statuesque calm in the face of the horrors that surround us all?\n\nThe central importance of the \"self-portrait\" in Helnwein's work, the mutable art of a doppelgänger, is no accident. It becomes the projection surface of world events. \"The artist doesn't make history, history makes him\" (Auguste Comte). The artist's doppelgänger role as victim and perpetrator, martyr and satyr, penitent and accuser, proxy and self-portrayer, moralist and autist, and in many other metamorphoses embodies and stages the antagonistic social forces on a stage of his inner-world consciousness.\n\nIn a conversation with Robert A. Sobieszek, curator of the Los Angeles County Museum of Art, Helnwein declared:\n\"The reason why I took up the subject of self-portraits and why I have put myself on stage was to function as a kind of representative for the suffering, abused and oppressed human being. I needed a living body to demonstrate and exemplify the effect of violence inflicted upon a defenseless victim.\nThere is nothing autobiographical or therapeutical about it, and I don’t think it says anything about me personally. \nAlso I was the best possible model for my experiments: endlessly patient and always available.\"\n\nAnother strong element in his works are comics. Helnwein has sensed the superiority of cartoon life over real life ever since he was a child. Growing up in a dreary, destroyed post-war Vienna, the young boy was surrounded by unsmiling people, haunted by a recent past they could never speak about. What changed his life was the first German-language Donald Duck comic book that his father brought home one day. Opening the book felt like finally arriving in a world where he belonged:\n\"...a decent world where one could get flattened by steam-rollers and perforated by bullets without serious harm. A world in which the people still looked proper, with yellow beaks or black knobs instead of noses.\" (Helnwein)\n\nIn 2000, the San Francisco Museum of Modern Art presented Helnwein's painting \"Mouse I\" (1995, oil and acrylic on canvas, 210 cm x 310 cm) at the exhibition \"The Darker Side of Playland: Childhood Imagery from the Logan Collection\".\nAlicia Miller commented on Helnwein's work in \"Artweek\":\n\"In 'The Darker Side of Playland', the endearing cuteness of beloved toys and cartoon characters turns menacing and monstrous. Much of the work has the quality of childhood nightmares. In those dreams, long before any adult understanding of the specific pains and evils that live holds, the familiar and comforting objects and images of a child's world are rent with something untoward. For children, not understanding what really to be afraid of, these dreams portend some pain and disturbance lurking into the landscape.\nPerhaps nothing in the exhibition exemplifies this better than Gottfried Helnwein's 'Mickey'. His portrait of Disney's favorite mouse occupies an entire wall of the gallery; rendered from an oblique angle, his jaunty, ingenuous visage looks somehow sneaky and suspicious. His broad smile, encasing a row of gleaming teeth, seems more a snarl or leer. This is Mickey as Mr. Hyde, his hidden other self now disturbingly revealed.\nHelnwein's Mickey is painted in shades of gray, as if pictured on an old black-and-white TV set. We are meant to be transported to the flickering edges of our own childhood memories in a time imaginably more blameless, crime-less and guiltless. But Mickey's terrifying demeanor hints of things to come...\".\n\nAlthough Helnwein's work is rooted in the legacy of German expressionism, he has absorbed elements of American pop culture. In the 1970s, he began to include cartoon characters in his paintings. In several interviews he claimed: \"I learned more from Donald Duck than from all the schools that I have ever attended.\" Commenting on that aspect in Helnwein's work, Julia Pascal wrote in the \"New Statesman\": \"His early watercolor \"Peinlich\" (\"Embarrassing\") shows a typical little 1950s girl in a pink dress and carrying a comic book. Her innocent appeal is destroyed by the gash deforming her cheek and lips. It is as if Donald Duck had met Mengele\".\n\nLiving between Los Angeles and Ireland, Helnwein met and photographed the Rolling Stones in London, and his portrait of John F. Kennedy made the front cover of Time magazine on the 20th anniversary of the president's assassination. His Self-portrait as screaming bandaged man, blinded by forks (1982) became the cover of the Scorpions album \"Blackout\". Andy Warhol, Muhammad Ali, William Burroughs and the German industrial metal band Rammstein posed for him; some of his art-works appeared in the cover-booklet of Michael Jackson's \"History\" album. Referring to the fall of the Berlin Wall Helnwein created the book \"Some Facts about Myself\", together with Marlene Dietrich. In 2003 he became friends with Marilyn Manson and started a collaboration with him on the multi-media art-project \"The Golden Age of Grotesque\" and on several experimental video-projects. Among his widely published works is a spoof of the famous Edward Hopper painting \"Nighthawks\", entitled \"Boulevard of Broken Dreams\", depicting Elvis Presley, Marylin Monroe, James Dean and Humphrey Bogart. This painting also inspired the Green Day song of the same name.\n\nExamining his imagery from the 1970s to the present, one sees influences as diverse as Bosch, Goya, John Heartfield, Beuys and Mickey Mouse, all filtered through a postwar Viennese childhood. 'Helnwein's oeuvre embraces total antipodes: The trivial alternates with visions of spiritual doom, the divine in the child contrasts with horror-images of child-abuse. But violence remains to be his basic theme – the physical and the emotional suffering, inflicted by one human being unto another.'\n\n1988 in remembrance of Kristallnacht (Cristal Night) 50 years earlier, Helnwein erected a large installation in the city center of Cologne, between Ludwig Museum and the Cologne Cathedral: \"Selektion – Neunter November Nacht (\"Selektion – Ninth November Night)\"\nA four-meter-high, hundred-meter-long picture lane in which the artist recalls the events of Reichskristallnacht, the actual beginning of the Holocaust, on 9 November 1938. He confronts the passersby with larger-than-life children's faces lined up in a seemingly endless row, as if for concentration camp selection. Just days into the exhibit, these portraits were vandalized by unknown persons, symbolically cutting the throats of the depicted children's faces. Helnwein consciously left the panels with the gashes and included them into the presentation, because he decided it made the work stronger and more relevant.\n\nMitchell Waxman wrote 2004, in \"The Jewish Journal\", Los Angeles: \"The most powerful images that deal with Nazism and Holocaust themes are by Anselm Kiefer and Helnwein, although, Kiefer's work differs considerably from Helnwein's in his concern with the effect of German aggression on the national psyche and the complexities of German cultural heritage. Kiefer is known for evocative and soulful images of barren German landscapes. But Kiefer and Helnwein's work are both informed by the personal experience of growing up in a post-war German speaking country...\nWilliam Burroughs said that the American revolution begins in books and music, and political operatives implement the changes after the fact. To this maybe we can add art. And Helnwein's art might have the capacity to instigate change by piercing the veil of political correctness to recapture the primitive gesture inherent in art.\".\n\nOne of the best known paintings of Helnwein's oeuvre is \"Epiphany I\" – \"Adoration of the Magi\", (1996, oil and acrylic on canvas, 210 cm x 333 cm, collection of the Denver Art Museum). It is part of a series of three paintings: \"Epiphany I\", \"Epiphany II (Adoration of the Shepherds)\", \"Epiphany III (Presentation at the Temple)\", created between 1996 and 1998. In Epiphany I, SS officers surround a mother and child group. To judge by their looks and gestures, they appear to be interested in details such as head, face, back and genitals. The arrangement of the figures clearly relates to motive and iconography of the adoration of the three Magi, such as were common especially in the German, Italian and Dutch 15th century artworks. Julia Pascal wrote about this work in the New Statesman: \"This Austrian Catholic Nativity scene has no Magi bearing gifts. Madonna and child are encircled by five respectful Waffen SS officers palpably in awe of the idealised, blonde Virgin. The Christ toddler, who stands on Mary's lap, stares defiantly out of the canvas.\" Helnwein's baby Jesus is often considered to represent Adolf Hitler.\n\nHelnwein is also known for his stage and costume designs for theater, ballet and opera productions. Amongst them: \"Macbeth\" by William Shakespeare, (director, choreographer: Johann Kresnik), Theater Heidelberg, 1988, Volksbühne Berlin, 1995; \"The Persecution and Murder of Jean-Paul Marat as Performed by the Inmates of the Asylum at Charenton under the Direction of the Marquis de Sade\" by Peter Weiss, (director: Johann Kresnik), Stuttgart National Theatre, 1989; \"Pasolini, Testament des Körpers\", (director: Johann Kresnik), Deutsches Schauspielhaus Hamburg, 1996; \"Hamletmaschine\" by Heiner Müller, (director: Gert Hof), 47. Berliner Festwochen, Berlin 1997, Muffathalle, München, 1997; \"The Rake's Progress\" by Igor Stravinsky, (director: Jürgen Flimm), at Hamburg State Opera, 2001; \"Paradise and the Peri\", oratorio by Robert Schumann, (director, choreographer: Gregor Seyffert & Compagnie Berlin), Robert-Schumann-Festival 2004, Tonhalle Düsseldorf; Der Rosenkavalier\" by Richard Strauss, (director: Maximilian Schell) at Los Angeles Opera, 2005, and Israeli Opera Tel Aviv, 2006;\"Der Ring des Nibelungen, part I, Rheingold und Walküre\", choreographic theatre after Richard Wagner, (director, choreographer: Johann Kresnik), Oper Bonn, 2006; \"Der Ring des Nibelungen\", part II, \"Siegfried\" and \"Götterdämmerung\", director, choreographer: Johann Kresnik), Oper Bonn, 2008, \"The Child Dreams\", by Hanoch Levin, composer: Gil Shohat, directed by Omri Nitzan, Israeli Opera, Tel Aviv, 2009/2010, \"Die 120 Tage von Sodom\" (\"Salò, or the 120 Days of Sodom\"), nach de Sade und Pasolini, director: Johann Kresnik, Volksbühne Berlin, 2015.\n2016 Helnwein became Honorary Member of iSTAN, the International Stage Art Network, a joint venture between the International Theatre Institute ITI and the Central Academy of Drama CAD Beijing.\n\n\n\nWilliam Burroughs said of Helnwein:\n\"Helnwein is one of the few exciting painters we have today.\"\nNorman Mailer\n\n\"Well, the world is a haunted house, and Helnwein at times is our tour guide through it. In his work he is willing to take on the sadness, the irony, the ugliness and the beauty. But not all of Gottfried's work is on a canvas. A lot of it is the way he's approached life. And it doesn't take someone knowing him to know that. You take one look at the paintings and you say \"this guy has been around.\" You can't sit in a closet – and create this. This level of work is earned.\"Sean Penn\n\n\"Gottfried Helnwein is my mentor. His fight for expression and stance against oppression are reasons why I chose him as an artistic partner. An artist that doesn't provoke will be invisible. Art that doesn't cause strong emotions has no meaning. Helnwein has that internalized.\"\nMarilyn Manson\n\n\"Helnwein's subject matter is the human condition. The metaphor for his art is dominated by the image of the child, but not the carefree innocent child of popular imagination. Helnwein instead creates the profoundly disturbing yet compellingly provocative image of the wounded child. The child scarred physically and the child scarred emotionally from within.\"\nRobert Flynn Johnson, Fine Arts Museums of San Francisco\n\n\"Warhol is the pre-Helnwein ...\"\nDieter Ronte, Museum of Modern Art, Vienna\n\n\n\n\n"}
{"id": "31951830", "url": "https://en.wikipedia.org/wiki?curid=31951830", "title": "Guglielmo Janni", "text": "Guglielmo Janni\n\nGuglielmo Janni (born 1892 - died 1958), was an Italian painter belonging to the modern movement of the \"Scuola romana (Roman School)\".\n\nSon of a renowned Roman family - his father Giuseppe was a lawyer and his mother Teresa Belli was the niece of famous Italian poet Giuseppe Gioachino Belli - Guglielmo Janni will be much influenced by his inherited literary background. He graduated at law in 1914 and, just after World War I, he attended an Art Nouveau decorative course at the Academy of Fine Arts in Rome, while studying philosophy and literature on his own.\n\nIn 1921, he exhibited at the 1st Biennale of Rome and around 1924 he was called to decorate the Headquarters of the Banca d'Italia, where he painted a mural on the history of Italian coinage. Between 1926 and 1927 he collaborated successively with the Ministry of the Interior and Ministry of Justice, with the Istituto Nazionale delle Assicurazioni and with Montecatini Terme. During this period (1923–1928), his main themes involved religious motifs and he exhibited his work at the II Biennale of Rome; at the Franciscan Art Contest of Milan; at the Art Show of the National Artists Association of Florence. Janni also became a lifelong friend of painter Alberto Ziveri.\n\nIn 1928, Janni painted a fresco in the votive chapel of St. Bartholomew (\"Monument to the Fallen\") at Busseto (Parma), with influences from Piero della Francesca. He was congratulated by art critic Roberto Longhi, who praised Janni's \"literary\" style of painting - an appropriate evaluation, as the artist's figurative work had acquired a \"vocation to myth\" that will be reflected in all his subsequent work: he will convey it through a tormented and sensual contemplation of the virile form, often concealed by an ambiguous and mundane veil: his \"Endymion\" of 1931 confirms this fully.\n\nGiuseppe Ungaretti presented in a relevant catalogue, Janni's first personal exhibition at the Galleria della Cometa, in Rome in 1936, where many important paintings were displayed, among which: \"Figura d'aprile, Giovani atleti, Lo specchio, Figura di Balletto\". Highly acclaimed by public and art critics, he organised again at the same Gallery a second personal exhibition and, in 1936, he attended the Biennale di Venezia and displayed three paintings at the Modern Italian Art Expo of Budapest.\n\nAfter a visit to the Universal Exposition of Paris with Alberto Ziveri in 1937, Janni suffered an existential crisis and abandoned painting completely. He then decided to dedicate the rest of his life to studying, editing and cataloguing his grandfather Belli's unpublished writings, stored in the family library. Janni died while doing this labouring work, which included a monumental biographical opus on Belli's life and poetry, in ten volumes.\n\n\n\n"}
{"id": "11650517", "url": "https://en.wikipedia.org/wiki?curid=11650517", "title": "Guilty pleasure", "text": "Guilty pleasure\n\nA guilty pleasure is something, such as a film, a television program or a piece of music, that one enjoys despite understanding that it is not generally held in high regard, or is seen as unusual or weird. For example, a person may secretly like a movie but will admit that particular movie is poorly made and/or generally seen as \"not good.\"\n\nFashion, video games, music, theatre, television series, films, food and fetishes can be examples of guilty pleasures.\n\n"}
{"id": "1773852", "url": "https://en.wikipedia.org/wiki?curid=1773852", "title": "Gutmann method", "text": "Gutmann method\n\nThe Gutmann method is an algorithm for securely erasing the contents of computer hard disk drives, such as files. Devised by Peter Gutmann and Colin Plumb and presented in the paper \"Secure Deletion of Data from Magnetic and Solid-State Memory\" in July 1996, it involved writing a series of 35 patterns over the region to be erased.\n\nThe selection of patterns assumes that the user does not know the encoding mechanism used by the drive, so it includes patterns designed specifically for three types of drives. A user who knows which type of encoding the drive uses can choose only those patterns intended for their drive. A drive with a different encoding mechanism would need different patterns.\n\nMost of the patterns in the Gutmann method were designed for older MFM/RLL encoded disks. Gutmann himself has noted that more modern drives no longer use these older encoding techniques, making parts of the method irrelevant. He said \"In the time since this paper was published, some people have treated the 35-pass overwrite technique described in it more as a kind of voodoo incantation to banish evil spirits than the result of a technical analysis of drive encoding techniques\".\n\nSince about 2001, some ATA IDE and SATA hard drive manufacturer designs include support for the ATA Secure Erase standard, obviating the need to apply the Gutmann method when erasing an entire drive. However, a 2011 research found that 4 out of 8 manufacturers did not implement ATA Secure Erase correctly.\n\nOne standard way to recover data that has been overwritten on a hard drive is to capture and process the analog signal obtained from the drive's read/write head prior to this analog signal being digitized. This analog signal will be close to an ideal digital signal, but the differences will reveal important information. By calculating the ideal digital signal and then subtracting it from the actual analog signal, it is possible to amplify the obtained difference signal and use it to determine what had previously been written on the disk.\n\nFor example:\n\nThis can then be done again to see the previous data written:\n\nHowever, even when overwriting the disk repeatedly with random data it is theoretically possible to recover the previous signal. The permittivity of a medium changes with the frequency of the magnetic field. This means that a lower frequency field will penetrate deeper into the magnetic material on the drive than a high frequency one. So a low frequency signal will, in theory, still be detectable even after it has been overwritten hundreds of times by a high frequency signal.\n\nThe patterns used are designed to apply alternating magnetic fields of various frequencies and various phases to the drive surface and thereby approximate degaussing the material below the surface of the drive.\n\nAn overwrite session consists of a lead-in of four random write patterns, followed by patterns 5 to 31 (see rows of table below), executed in a random order, and a lead-out of four more random patterns.\n\nEach of patterns 5 to 31 was designed with a specific magnetic media encoding scheme in mind, which each pattern targets. The drive is written to for all the passes even though the table below only shows the bit patterns for the passes that are specifically targeted at each encoding scheme. The end result should obscure any data on the drive so that only the most advanced physical scanning (e.g., using a magnetic force microscope) of the drive is likely to be able to recover any data. \n\nThe series of patterns is as follows:\n\nEncoded bits shown in bold are what should be present in the ideal pattern, although due to the encoding the complementary bit is actually present at the start of the track.\n\nThe delete function in most operating systems simply marks the space occupied by the file as reusable (removes the pointer to the file) without immediately removing any of its contents. At this point the file can be fairly easily recovered by numerous recovery applications. However, once the space is overwritten with other data, there is no known way to use software to recover it. It cannot be done with software alone since the storage device only returns its current contents via its normal interface. Gutmann claims that intelligence agencies have sophisticated tools, including magnetic force microscopes, which together with image analysis, can detect the previous values of bits on the affected area of the media (for example hard disk).\n\nDaniel Feenberg of the National Bureau of Economic Research, an American private nonprofit research organization, criticized Gutmann's claim that intelligence agencies are likely to be able to read overwritten data, citing a lack of evidence for such claims. Nevertheless, some published government security procedures consider a disk overwritten once to still be sensitive.\n\nGutmann himself has responded to some of these criticisms and also criticized how his algorithm has been abused in an epilogue to his original paper, in which he states:\n\n\n"}
{"id": "4558820", "url": "https://en.wikipedia.org/wiki?curid=4558820", "title": "Hovmöller diagram", "text": "Hovmöller diagram\n\nA Hovmöller diagram is a commonly used way of plotting meteorological data to highlight the role of waves. The axes of a Hovmöller diagram are typically longitude or latitude (abscissa or x-axis) and time (ordinate or y-axis) with the value of some field represented through color or shading. Hovmöller diagrams are also used to plot the time evolution of vertical profiles of scalar quantities such as temperature, density, or concentrations of constituents in the atmosphere or ocean. In that case time is plotted along the abscissa and vertical position (depth, height, pressure) along the ordinate.\n\nThe diagram first introduced by Ernest Aabo Hovmöller (1912-2008), a Danish meteorologist, in a paper of 1949.\n\n\n"}
{"id": "14282", "url": "https://en.wikipedia.org/wiki?curid=14282", "title": "Hubris", "text": "Hubris\n\nHubris ( from ancient Greek ) describes a personality quality of extreme or foolish pride or dangerous overconfidence, often in combination with (or synonymous with) arrogance. In its ancient Greek context, it typically describes behavior that defies the norms of behavior or challenges the gods, and which in turn brings about the downfall, or nemesis, of the perpetrator of hubris.\n\nThe adjectival form of the noun \"hubris\" is \"hubristic\". Hubris is usually perceived as a characteristic of an individual rather than a group, although the group the offender belongs to may suffer collateral consequences from the wrongful act. Hubris often indicates a loss of contact with reality and an overestimation of one's own competence, accomplishments or capabilities. \n\nIn ancient Greek, \"hubris\" referred to actions that shamed and humiliated the victim for the pleasure or gratification of the abuser. The term had a strong sexual connotation, and the shame reflected upon the perpetrator as well.\n\nViolations of the law against hubris included what might today be termed assault and battery; sexual crimes; or the theft of public or sacred property. Two well-known cases are found in the speeches of Demosthenes, a prominent statesman and orator in ancient Greece. These two examples occurred when first Midias punched Demosthenes in the face in the theatre (\"Against Midias\"), and second when (in \"Against Conon\") a defendant allegedly assaulted a man and crowed over the victim. Yet another example of hubris appears in Aeschines' \"Against Timarchus\", where the defendant, Timarchus, is accused of breaking the law of hubris by submitting himself to prostitution and anal intercourse. Aeschines brought this suit against Timarchus to bar him from the rights of political office and his case succeeded.\n\nIn ancient Athens, hubris was defined as the use of violence to shame the victim (this sense of hubris could also characterize rape). Aristotle defined hubris as shaming the victim, not because of anything that happened to the committer or might happen to the committer, but merely for that committer's own gratification:\nto cause shame to the victim, not in order that anything may happen to you, nor because anything has happened to you, but merely for your own gratification. Hubris is not the requital of past injuries; this is revenge. As for the pleasure in hubris, its cause is this: naive men think that by ill-treating others they make their own superiority the greater.\n\nCrucial to this definition are the ancient Greek concepts of honour (τιμή, \"timē\") and shame (αἰδώς, \"aidōs\"). The concept of honour included not only the exaltation of the one receiving honour, but also the shaming of the one overcome by the act of hubris. This concept of honour is akin to a zero-sum game. Rush Rehm simplifies this definition of hubris to the contemporary concept of \"insolence, contempt, and excessive violence\".\n\nThe Greek word for sin, hamartia (ἁμαρτία), originally meant \"error\" in the ancient dialect, and so poets like Hesiod and Aeschylus used the word \"hubris\" to describe transgressions against the gods. A common way that hubris was committed was when a mortal claimed to be better than a god in a particular skill or attribute. Claims like these were rarely left unpunished, and so Arachne, a talented young weaver, was transformed into a spider when she said that her skills exceeded those of the goddess Athena. Additional examples include Icarus, Phaethon, Salmoneus, Niobe, Cassiopeia, and Tereus.\n\nThese events were not limited to myth, and certain figures in history were considered to be have been punished for committing hubris through their arrogance. One such person was king Xerxes as portrayed in Aeschylus's play The Persians, and who allegedly threw chains to bind the Hellespont sea as punishment for daring to destroy his fleet.\n\nWhat is common to all these examples is the breaching of limits, as the Greeks believed that the Fates (Μοῖραι) had assigned each being with a particular area of freedom, an area that even the gods could not breach. \n\nThe goddess Hybris has been described as having \"insolent encroachment upon the rights of others\".\n\nThe word hubris as used in the New Testament parallels the Hebrew word \"pasha\", meaning transgression. It represents a sense of false pride that makes a man defy God, sometimes to the degree that he considers himself an equal. In contrast to this, the common word for sin was hamartia, which refers to an error and reflects the complexity of the human condition. Its result is guilt rather than direct punishment as in the case of hubris .\n\nIn its modern usage, hubris denotes overconfident pride combined with arrogance. Hubris is often associated with a lack of humility. Sometimes a person's hubris is also associated with ignorance. The accusation of hubris often implies that suffering or punishment will follow, similar to the occasional pairing of hubris and nemesis in Greek mythology. The proverb \"pride goeth (goes) before destruction, a haughty spirit before a fall\" (from the biblical Book of Proverbs, 16:18) is thought to sum up the modern use of hubris. Hubris is also referred to as \"pride that blinds\" because it often causes a committer of hubris to act in foolish ways that belie common sense. In other words, the modern definition may be thought of as, \"that pride that goes just before the fall.\"\n\nExamples of hubris are often found in literature, most famously in John Milton's \"Paradise Lost\", in which Lucifer attempts to compel the other angels to worship him, is cast into hell by God and the innocent angels, and proclaims: \"Better to reign in hell than serve in heaven.\" Victor in Mary Shelley's \"Frankenstein\" manifests hubris in his attempt to become a great scientist; he creates life through technological means, but comes to regret his project. Marlowe's play \"Doctor Faustus\" portrays the eponymous character as a scholar whose arrogance and pride compel him to sign a deal with the Devil, and retain his haughtiness until his death and damnation, despite the fact that he could easily have repented had he chosen to do so.\n\nAn example in pop culture is the comic book hero Doctor Strange, wherein highly talented and arrogant neurosurgeon Dr. Stephen Strange is involved in a vehicular accident. Unlike the Greek figures Salmoneus, Icarus and Phaethon, he survives, though his hands are severely damaged, and thus his career as a neurosurgeon is shattered. After western medicine fails to help him, he seeks healing in the mystic arts, and though he never fully recovers, he becomes a powerful sorcerer.\n\nA historical example of hubris was furnished by General George Armstrong Custer in the decisions that culminated in the Battle of Little Big Horn; Custer is apocryphally quoted as having exclaimed: \"Where did all those damned Indians come from?\"\n\nC. S. Lewis wrote in \"Mere Christianity\" that pride is the \"anti-God\" state, the position in which the ego and the self are directly opposed to God: \"Unchastity, anger, greed, drunkenness, and all that, are mere fleabites in comparison: it was through Pride that the devil became the devil: Pride leads to every other vice: it is the complete anti-God state of mind.\"\n\n\n\n"}
{"id": "35480692", "url": "https://en.wikipedia.org/wiki?curid=35480692", "title": "Ignorance management", "text": "Ignorance management\n\nIgnorance management is a knowledge management practice that addresses the concept of ignorance in organizations.\n\nIgnorance Management has been described by John Israilidis, Russell Lock, and Louise Cooke of Loughborough University as:\n\nThe key principle of this theory is that knowledge management (KM) could better be seen as Ignorance Management due to the fact that it is impossible for someone to comprehend and understand everything in a complete way. The only real wisdom is in recognising the limits and extent of one's knowledge and therefore, KM is essentially a matter of sharing the extent of one's ignorance with other people, and thus learning together. This process of knowing what is needed to know and also acknowledging the power of understanding the unknown, could develop a tacit understanding and could improve both short-term opportunistic value capture and longer term business sustainability (Israilidis et al. 2012).\n\nSeveral attempts have been made to explore the value of managing organisational ignorance in order to prevent failures within knowledge transfer contexts. The need to recognise the role and significance of power in the management of ignorance has been introduced to further enhance such efforts. Also, a growing body of psychology research shows that humans find it intrinsically difficult to get a sense of what we don’t know and argues that incompetence deprives people of the ability to recognise their own incompetence (the Dunning–Kruger effect). Finally, the viewpoint of developing our understanding of organisational ignorance can yield impressive benefits, if successfully incorporated within a company’s KM strategy.\n\n\n"}
{"id": "1589554", "url": "https://en.wikipedia.org/wiki?curid=1589554", "title": "Inversion of control", "text": "Inversion of control\n\nIn software engineering, inversion of control (IoC) is a design principle in which custom-written portions of a computer program receive the flow of control from a generic framework. A software architecture with this design inverts control as compared to traditional procedural programming: in traditional programming, the custom code that expresses the purpose of the program calls into reusable libraries to take care of generic tasks, but with inversion of control, it is the framework that calls into the custom, or task-specific, code.\n\nInversion of control is used to increase modularity of the program and make it extensible, and has applications in object-oriented programming and other programming paradigms. The term was used by Michael Mattsson in a thesis, taken from there by Stefano Mazzocchi and popularized by him in 1999 in a defunct Apache Software Foundation project, Avalon, then further popularized in 2004 by Robert C. Martin and Martin Fowler.\n\nThe term is related to, but different from, the dependency inversion principle, which concerns itself with decoupling dependencies between high-level and low-level layers through shared abstractions. The general concept is also related to event-driven programming in that it is often implemented using IoC, so that the custom code is commonly only concerned with the handling of events, whereas the event loop and dispatch of events/messages is handled by the framework or the runtime environment.\n\nAs an example, with traditional programming, the main function of an application might make function calls into a menu library to display a list of available commands and query the user to select one. The library thus would return the chosen option as the value of the function call, and the main function uses this value to execute the associated command. This style was common in text based interfaces. For example, an email client may show a screen with commands to load new mails, answer the current mail, start a new mail, etc., and the program execution would block until the user presses a key to select a command.\n\nWith inversion of control, on the other hand, the program would be written using a software framework that knows common behavioral and graphical elements, such as windowing systems, menus, controlling the mouse, and so on. The custom code \"fills in the blanks\" for the framework, such as supplying a table of menu items and registering a code subroutine for each item, but it is the framework that monitors the user's actions and invokes the subroutine when a menu item is selected. In the mail client example, the framework could follow both the keyboard and mouse inputs and call the command invoked by the user by either means, and at the same time monitor the network interface to find out if new messages arrive and refresh the screen when some network activity is detected. The same framework could be used as the skeleton for a spreadsheet program or a text editor. Conversely, the framework knows nothing about Web browsers, spreadsheets or text editors; implementing their functionality takes custom code.\n\nInversion of control carries the strong connotation that the reusable code and the problem-specific code are developed independently even though they operate together in an application. Software frameworks, callbacks, schedulers, event loops, dependency injection, and the template method are examples of design patterns that follow the inversion of control principle, although the term is most commonly used in the context of object-oriented programming.\n\nInversion of control serves the following design purposes:\n\nInversion of control is sometimes facetiously referred to as the \"Hollywood Principle: Don't call us, we'll call you\".\n\nInversion of control is not a new term in computer science. Martin Fowler traces the etymology of the phrase back to 1988, but it is closely \nrelated to the concept of program inversion described by Michael Jackson in his Jackson Structured Programming methodology in the 1970s. A bottom-up parser can be seen as an inversion of a top-down parser: in the one case, the \ncontrol lies with the parser, in the other case, it lies with the receiving application.\n\nDependency injection is a specific type of IoC. A service locator such as the Java Naming and Directory Interface (JNDI) is similar. In an article by Loek Bergman, it is presented as an architectural principle.\n\nIn an article by Robert C. Martin, the dependency inversion principle and abstraction by layering come together. His reason to use the term \"inversion\" is in comparison with traditional software development methods. He describes the uncoupling of services by the abstraction of layers when he is talking about dependency inversion. The principle is used to find out where system borders are in the design of the abstraction layers.\n\nIn traditional programming, the flow of the business logic is determined by objects that are statically bound to one another. With inversion of control, the flow depends on the object graph that is built up during program execution. Such a dynamic flow is made possible by object interactions that are defined through abstractions. This run-time binding is achieved by mechanisms such as dependency injection or a service locator. In IoC, the code could also be linked statically during compilation, but finding the code to execute by reading its description from external configuration instead of with a direct reference in the code itself.\n\nIn dependency injection, a dependent object or module is coupled to the object it needs at run time. Which particular object will satisfy the dependency during program execution typically cannot be known at compile time using static analysis. While described in terms of object interaction here, the principle can apply to other programming methodologies besides object-oriented programming.\n\nIn order for the running program to bind objects to one another, the objects must possess compatible interfaces. For example, class codice_1 may delegate behavior to interface codice_2 which is implemented by class codice_3; the program instantiates codice_1 and codice_3, and then injects codice_3 into codice_1.\n\nIn object-oriented programming, there are several basic techniques to implement inversion of control. These are:\n\nIn an original article by Martin Fowler, the first three different techniques are discussed. In a description about inversion of control types, the last one is mentioned. Often the contextualized lookup will be accomplished using a service locator\n\nMost frameworks such as .NET or Enterprise Java display this pattern:\n\nThis basic outline in Java gives an example of code following the IoC methodology. It is important, however, that in the a lot of assumptions are made about the data returned by the data access object (DAO).\n\nAlthough all these assumptions might be valid at some time, they couple the implementation of the to the DAO implementation. Designing the application in the manner of inversion of control would hand over the control completely to the DAO object. The code would then become\n\nThe example shows that the way the method is constructed determines if IoC is used. It is the way that parameters are used that define IoC. This resembles the message-passing style that some object-oriented programming languages use.\n\n"}
{"id": "369040", "url": "https://en.wikipedia.org/wiki?curid=369040", "title": "Is–ought problem", "text": "Is–ought problem\n\nThe is–ought problem, as articulated by the Scottish philosopher and historian David Hume (1711–76), states that many writers make claims about what \"ought\" to be, based on statements about what \"is\". Hume found that there seems to be a significant difference between positive statements (about what is) and prescriptive or normative statements (about what ought to be), and that it is not obvious how one can coherently move from descriptive statements to prescriptive ones. The is–ought problem is also known as Hume's law, Hume's guillotine or fact–value gap.\n\nA similar view is defended by G. E. Moore's open-question argument, intended to refute any identification of moral properties with natural properties. This so-called naturalistic fallacy stands in contrast to the views of ethical naturalists.\n\nHume discusses the problem in book III, part I, section I of his book, \"A Treatise of Human Nature\" (1739):\n\nHume calls for caution against such inferences in the absence of any explanation of how the ought-statements follow from the is-statements. But how exactly \"can\" an \"ought\" be derived from an \"is\"? The question, prompted by Hume's small paragraph, has become one of the central questions of ethical theory, and Hume is usually assigned the position that such a derivation is impossible. This complete severing of \"is\" from \"ought\" has been given the graphic designation of \"Hume's Guillotine\".\n\nThe apparent gap between \"is\" statements and \"ought\" statements, when combined with Hume's fork, renders \"ought\" statements of dubious validity. Hume's fork is the idea that all items of knowledge are based either on logic and definitions, or else on observation. If the is–ought problem holds, then \"ought\" statements do not seem to be known in either of these two ways, and it would seem that there can be no moral knowledge. Moral skepticism and non-cognitivism work with such conclusions.\n\nEthical naturalists contend that moral truths exist, and that their truth value relates to facts about physical reality. Many modern naturalistic philosophers see no impenetrable barrier in deriving \"ought\" from \"is\", believing it can be done whenever we analyze goal-directed behavior. They suggest that a statement of the form \"In order for agent \"A\" to achieve goal \"B\", \"A\" reasonably ought to do \"C\"\" exhibits no category error and may be factually verified or refuted. \"Oughts\" exist, then, in light of the existence of goals. A counterargument to this response is that it merely pushes back the 'ought' to the subjectively valued 'goal' and thus provides no fundamentally objective basis to one's goals which, consequentially, provides no basis of distinguishing moral value of fundamentally different goals. A possible basis for an objective, moral realist, morality might be an appeal to teleonomy.\n\nThis is similar to work done by moral philosopher Alasdair MacIntyre, who attempts to show that because ethical language developed in the West in the context of a belief in a human telos—an end or goal—our inherited moral language, including terms such as good and bad, have functioned, and function, to evaluate the way in which certain behaviors facilitate the achievement of that telos. In an evaluative capacity, therefore, good and bad carry moral weight without committing a category error. For instance, a pair of scissors that cannot easily cut through paper can legitimately be called bad since it cannot fulfill its purpose effectively. Likewise, if a person is understood as having a particular purpose, then behaviour can be evaluated as good or bad in reference to that purpose. In plainer words, a person is acting good when that person fulfills that person's purpose.\n\nEven if the concept of an \"ought\" is meaningful, this need not involve morality. This is because some goals may be morally neutral, or (if they exist) against what is moral. A poisoner might realize his victim has not died and say, for example, \"I ought to have used more poison,\" since his goal is to murder. The next challenge of a moral realist is thus to explain what is meant by a \"\"moral\" ought\".\n\nProponents of discourse ethics argue that the very act of discourse implies certain \"oughts\", that is, certain presuppositions that are necessarily accepted by the participants in discourse, and can be used to further derive prescriptive statements. They therefore argue that it is incoherent to argumentatively advance an ethical position on the basis of the is–ought problem, which contradicts these implied assumptions.\n\nAs MacIntyre explained, someone may be called a good person if people have an inherent purpose. Many ethical systems appeal to such a purpose. This is true of some forms of moral realism, which states that something can be wrong, even if every thinking person \"believes\" otherwise (the idea of brute fact about morality). The ethical realist might suggest that humans were created for a purpose (e.g. to serve God), especially if they are an ethical non-naturalist. If the ethical realist is instead an ethical naturalist, they may start with the fact that humans have evolved and pursue some sort of evolutionary ethics (which risks “committing” the moralistic fallacy).\nNot all moral systems appeal to a human telos or purpose. This is because it is not obvious that people even \"have\" any sort of natural purpose, or what that purpose would be. Although many scientists do recognize teleonomy (a tendency in nature), few philosophers appeal to it (this time, to avoid the naturalistic fallacy).\nGoal-dependent oughts run into problems even without an appeal to an innate human purpose. Consider cases where one has no desire to be good—whatever it is. If, for instance, a person wants to be good, and good means washing one's hands, then it seems one morally ought to wash their hands. The bigger problem in moral philosophy is what happens if someone does \"not\" want to be good, whatever its origins? Put simply, in what sense \"ought\" we to hold the goal of being good? It seems one can ask \"how am I rationally required to hold 'good' as a value, or to pursue it?\"\n\nThe issue above mentioned is a result of an important ethical relativist critique. Even if \"oughts\" depend on goals, the ought seems to vary with the person's goal. This is the conclusion of the ethical subjectivist, who says a person can only be called good according to whether they fulfill their own, \"self-assigned\" goal. Alasdair MacIntyre himself suggests that a person's purpose comes from their culture, making him a sort of ethical relativist. Ethical relativists acknowledge local, institutional facts about what is right, but these are facts that can still vary by society. Thus, without an objective \"moral goal\", a moral ought is difficult to establish. G. E. M. Anscombe was particularly critical of the word \"ought\" for this reason; understood as \"We need such and such, and the only way to get it is this way\"—a person may need something immoral, or else find that their noble need requires immoral action.\n\nIf moral goals depend on private assumptions or public agreement, so may morality as a whole. For example, Canada might call it good to maximize global welfare, where a citizen, Alice, calls it good to focus on herself, and then her family, and finally her friends (with little empathy for strangers). It does not seem that Alice can be objectively or rationally bound—without regard to her personal values \"nor\" those of groups of other people—to act a certain way. In other words, we may not be able to say \"You just \"should\" do this\". Moreover, persuading her to help strangers would necessarily mean appealing to values she already possesses (or else we would never even have a hope of persuading her). This is another interest of normative ethics—questions of binding forces.\n\nThere may be responses to the above relativistic critiques. As mentioned above, ethical realists that are non-natural can appeal to God's purpose for humankind. On the other hand, naturalistic thinkers may posit that valuing people's well-being is somehow 'obviously' the purpose of ethics, or else the only relevant purpose worth talking about. This is the move made by natural law, scientific moralists and some utilitarians.\n\nJohn Searle also attempts to derive \"ought\" from \"is\". He tries to show that the act of making a promise places one under an obligation by definition, and that such an obligation amounts to an \"ought\". This view is still widely debated, and to answer criticisms, Searle has further developed the concept of institutional facts, for example, that a certain building \"is\" in fact a bank and that certain paper \"is\" in fact money, which would seem to depend upon general recognition of those institutions and their value.\n\nIndefinables are concepts so global that they cannot be defined; rather, in a sense, they themselves, and the objects to which they refer, define our reality and our ideas. Their meanings cannot be stated in a true definition, but their meanings can be referred to instead by being placed with their incomplete definitions in self-evident statements, the truth of which can be tested by whether or not it is impossible to think the opposite without a contradiction. Thus, the truth of indefinable concepts and propositions using them is entirely a matter of logic.\n\nAn example of the above is that of the concepts \"finite parts\" and \"wholes\"; they cannot be defined without reference to each other and thus with some amount of circularity, but we can make the self-evident statement that \"the whole is greater than any of its parts\", and thus establish a meaning particular to the two concepts.\n\nThese two notions being granted, it can be said that statements of \"ought\" are measured by their \"prescriptive\" truth, just as statements of \"is\" are measured by their \"descriptive\" truth; and the descriptive truth of an \"is\" judgment is defined by its correspondence to reality (actual or in the mind), while the prescriptive truth of an \"ought\" judgment is defined according to a more limited scope—its correspondence to right desire (conceivable in the mind and able to be found in the rational appetite, but not in the more \"actual\" reality of things independent of the mind or rational appetite).\n\nTo some, this may immediately suggest the question: \"How can we know what is a right desire if it is already admitted that it is not based on the more actual reality of things independent of the mind?\" The beginning of the answer is found when we consider that the concepts \"good\", \"bad\", \"right\" and \"wrong\" are indefinables. Thus, right desire cannot be defined properly, but a way to refer to its meaning \"may\" be found through a self-evident prescriptive truth.\n\nThat self-evident truth which the moral cognitivist claims to exist upon which all other prescriptive truths are ultimately based is: \"One ought to desire what is really good for one and nothing else.\" The terms \"real good\" and \"right desire\" cannot be defined apart from each other, and thus their definitions would contain some degree of circularity, but the stated self-evident truth indicates a meaning particular to the ideas sought to be understood, and it is (the moral cognitivist might claim) impossible to think the opposite without a contradiction. Thus combined with other descriptive truths of what is good (goods in particular considered in terms of whether they suit a particular end and the limits to the possession of such particular goods being compatible with the general end of the possession of the total of all real goods throughout a whole life), a valid body of knowledge of right desire is generated.\n\nSeveral counterexamples have been offered by philosophers claiming to show that there are cases when an \"ought\" logically follows from an \"is.\" First of all, Hilary Putnam, by tracing back the quarrel to Hume's dictum, claims fact/value entanglement as an objection, since the distinction between them entails a value. A. N. Prior points out, from the statement \"He is a sea captain,\" it logically follows, \"He ought to do what a sea captain ought to do.\" Alasdair MacIntyre points out, from the statement \"This watch is grossly inaccurate and irregular in time-keeping and too heavy to carry about comfortably,\" the evaluative conclusion validly follows, \"This is a bad watch.\" John Searle points out, from the statement \"Jones promised to pay Smith five dollars,\" it logically follows that \"Jones ought to pay Smith five dollars.\" The act of promising by definition places the promiser under obligation.\n\nPhilippa Foot adopts a moral realist position, criticizing the idea that when evaluation is superposed on fact there has been a \"committal in a new dimension.\" She introduces, by analogy, the practical implications of using the word \"injury.\" Not just anything counts as an injury. There must be some impairment. When we suppose a man wants the things the injury prevents him from obtaining, haven’t we fallen into the old naturalist fallacy?\n\nFoot argues that the virtues, like hands and eyes in the analogy, play so large a part in so many operations that it is implausible to suppose that a committal in a non-naturalist dimension is necessary to demonstrate their goodness.\nHilary Putnam argues philosophers that accept Hume's \"is–ought\" distinction reject his reasons in making this, and thus undermine the entire claim.\n\nVarious scholars have also indicated that, in the very work where Hume argues for the is–ought problem, Hume himself derives an \"ought\" from an \"is\". Such seeming inconsistencies in Hume have led to an ongoing debate over whether Hume actually held to the is–ought problem in the first place, or whether he meant that ought inferences can be made but only with good argumentation.\n\n\n"}
{"id": "1572078", "url": "https://en.wikipedia.org/wiki?curid=1572078", "title": "Kripke–Platek set theory with urelements", "text": "Kripke–Platek set theory with urelements\n\nThe Kripke–Platek set theory with urelements (KPU) is an axiom system for set theory with urelements, based on the traditional (urelement-free) Kripke–Platek set theory. It is considerably weaker than the (relatively) familiar system ZFU. The purpose of allowing urelements is to allow large or high-complexity objects (such as the set of all reals) to be included in the theory's transitive models without disrupting the usual well-ordering and recursion-theoretic properties of the constructible universe; KP is so weak that this is hard to do by traditional means.\n\nThe usual way of stating the axioms presumes a two sorted first order language formula_1 with a single binary relation symbol formula_2.\nLetters of the sort formula_3 designate urelements, of which there may be none, whereas letters of the sort formula_4 designate sets. The letters formula_5 may denote both sets and urelements.\n\nThe letters for sets may appear on both sides of formula_2, while those for urelements may only appear on the left, i.e. the following are examples of valid expressions: formula_7, formula_8.\n\nThe statement of the axioms also requires reference to a certain collection of formulae called formula_9-formulae. The collection formula_9 consists of those formulae that can be built using the constants, formula_2, formula_12, formula_13, formula_14, and bounded quantification. That is quantification of the form formula_15 or formula_16 where formula_17 is given set.\n\nThe axioms of KPU are the universal closures of the following formulae:\n\n\nTechnically these are axioms that describe the partition of objects into sets and urelements.\n\n\nKPU can be applied to the model theory of infinitary languages. Models of KPU considered as sets inside a maximal universe that are transitive as such are called admissible sets.\n\n\n\n"}
{"id": "8932262", "url": "https://en.wikipedia.org/wiki?curid=8932262", "title": "Law in Europe", "text": "Law in Europe\n\nThe law of Europe is diverse and changing fast today. Europe saw the birth of both the Roman Empire and the British Empire, which form the basis of the two dominant forms of legal system of private law, civil and common law.\n\nThe law of Europe has a diverse history. Roman law underwent major codification in the Corpus Juris Civilis of Emperor Justinian, as later developed through the Middle Ages by medieval legal scholars. In Medieval England, judges retained greater power than their continental counterparts and began to develop a body of precedent. Originally civil law was one common legal system in much of Europe, but with the rise of nationalism in the 17th century Nordic countries and around the time of the French Revolution, it became fractured into separate national systems. This change was brought about by the development of separate national codes, of which the French Napoleonic Code and the German and Swiss codes were the most influential. Around this time civil law incorporated many ideas associated with the Enlightenment. The European Union's Law is based on a codified set of laws, laid down in the Treaties. Law in the EU is however mixed with precedent in case law of the European Court of Justice. In accordance with its history, the interpretation of European law relies less on policy considerations than U.S. law.\n\n\n"}
{"id": "728513", "url": "https://en.wikipedia.org/wiki?curid=728513", "title": "Laziness", "text": "Laziness\n\nLaziness (also known as indolence) is disinclination to activity or exertion despite having the ability to act or \nexert oneself. It is often used as a pejorative; terms for a person seen to be lazy\ninclude couch potato, slacker, and bludger.\n\nDespite Sigmund Freud's discussion of the pleasure principle, Leonard Carmichael notes that \"laziness is not a word that appears in the table of contents of most technical books on psychology... It is a guilty secret of modern psychology that more is understood about the motivation of thirsty rats and hungry pecking pigeons as they press levers than about the way in which poets make themselves write poems or scientists force themselves into the laboratory when the good golfing days of spring arrive.\" A 1931 survey found that high school students were more likely to attribute their failing performance to laziness, while teachers ranked \"lack of ability\" as the major cause, with laziness coming in second. Laziness is not to be confused with avolition, a negative symptom of certain mental health issues such as depression, ADHD, sleep disorders, and schizophrenia.\n\nLaziness is a habit rather than a mental health issue. It may reflect a lack of self-esteem, a lack of positive recognition by others, a lack of discipline stemming from low self-confidence, or a lack of interest in the activity or belief in its efficacy. Laziness may manifest as procrastination or vacillation. Studies of motivation suggest that laziness may be caused by a decreased level of motivation, which in turn can be caused by over-stimulation or excessive impulses or distractions. These increase the release of dopamine, a neurotransmitter responsible for reward and pleasure. The more dopamine that is released, the greater intolerance one has for valuing and accepting productive and rewarding action. This desensitization leads to dulling of the neural patterns and affects negatively the anterior insula of the brain responsible for risk perception.\n\nADHD specialists say engaging in multiple activities can cause behavioral problems such as attention/focus failure or perfectionism and subsequently pessimism. In these circumstances laziness can manifest as a negative coping mechanism (aversion), the desire to avoid certain situations in the hopes of countering certain experiences and preconceived ill results. Lacanian thought says laziness is the \"acting out\" of archetypes from societal programming and negative child rearing practices. Boredom is sometimes conflated with laziness; one study shows that the average Briton is bored 6 hours a week. Thomas Goetz, University of Konstanz, Germany, and John Eastwood, York University, Canada, concur that aversive states such as laziness can be equally adaptive for making change and toxic if allowed to fester. An outlook found to be helpful in their studies is \"being mindful and not looking for ways out of it, simultaneously to be also open to creative and active options if they should arise.\" They point out that a relentless engaging in activities without breaks can cause oscillations of failure, which may result in mental health issues. \n\nIt has also been shown that laziness can render one apathetic to reactant mental health issues such as anger, anxiety, indifference, substance abuse, and depression.\n\nEconomists have differing views of laziness. Frédéric Bastiat argues that idleness is the result of people focusing on the pleasant immediate effects of their actions rather than potentially negative long-term consequences. Others note that humans seem to have a tendency to seek after leisure. Hal Cranmer writes, \"For all these arguments against laziness, it is amazing we work so hard to achieve it. Even those hard-working Puritans were willing to break their backs every day in exchange for an eternity of lying around on a cloud and playing the harp. Every industry is trying to do its part to give its customers more leisure time.\" Ludwig von Mises writes, \"The expenditure of labor is deemed painful. Not to work is considered a state of affairs more satisfactory than working. Leisure is, other things being equal, preferred to travail (work). People work only when they value the return of labor higher than the decrease in satisfaction brought about by the curtailment of leisure. To work involves disutility.\"\n\nLaziness in American literature is figured as a fundamental problem with social and spiritual consequences. In 1612 John Smith in his \"Map to Virginia\" is seen using a jeremiad to address idleness. In the 1750s this sort of advocating reached at its apex in literature's. David Bertelson in \"The Lazy South\" (1767) expressed this as a substitution of \"spiritual industry\" over \"patriotic industry\". Writers like William Byrd went to a great extent and censured North Carolina as land of lubbers. Thomas Jefferson in his \"Notes on the State of Virginia\" (1785) acknowledges a small portion of the people have only seen labor and identifies the cause of this indolence to the rise of \"slave-holding\" society. Jefferson raised his concerns what this deleterious system will bring to the economic system. Later by the 1800s the rise of Romanticism changed attitudes of the society, values of work were re-written; stigmatization of idleness was overthrown with glamorous notions. John Pendleton Kennedy was a prominent writer in romanticizing sloth and slavery, In Swallow Barn (1832) he equated idleness and its flow as living in oneness with nature. Mark Twain in \"Adventures of Huckleberry Finn\" (1885) contrasts realist and romantic perspective of \"laziness\" and calls attention to the essential convention of aimlessness and transcendence that connects the character. In 20th century the poor whites were portrayed in the grotesque caricatures of early southern laziness. In Flannery O'Connor's Wise Blood (1952) and Good Country People (1955) depicts spiritual backwardness as the cause for disinclination to work. Lacking in any social function which was termed equally with luxurious lifestyle was closely portrayed through lives of displaced aristocrats and their indolence. Jason Compson, Robert Penn Warren, William Styron were some of the writers who explored this perspective. The lack of meaningful work was defined as a void which aristocrats needed to fill with pompous culture, Walker Percy is a writer who have thoroughly mined on the subject. Percy's characters often exposes to the emptiness (spiritual sloth) of contemporary life and come to rectify it with renewed resources of spiritual resources.\n\nOne of the Catholic seven deadly sins is sloth, which is often defined as spiritual and/or physical apathy or laziness. Sloth is discouraged in (), 2 Thessalonians, and associated with wickedness in one of the parables of Jesus in the \"Gospel of Matthew\" (). In the Wisdom books of \"Proverbs\" and \"Ecclesiastes\", it is stated that laziness can lead to poverty (, ). According to Peter Binsfeld's \"Binsfeld's Classification of Demons\", Belphegor is thought to be its chief demon.\n\nThe Arabic term used in the Quran for laziness, inactivity and sluggishness is كَسَل (\"kasal\"). The opposite of laziness is Jihad al-Nafs, i.e. the struggle against the self, against one’s own ego. Among the five pillars of Islam, praying five times a day and fasting during Ramaḍān are part of actions against laziness.\n\nIn Buddhism, the term \"kausīdya\" is commonly translated as \"laziness\" or \"spiritual sloth\". \"Kausīdya\" is defined as clinging to unwholesome activities such as lying down and stretching out, procrastinating, and not being enthusiastic about or engaging in virtuous activity.\n\nFrom 1909 to 1915, the Rockefeller Sanitary Commission for the Eradication of Hookworm Disease sought to eradicate hookworm infestation from 11 southern U.S. states. Hookworms were popularly known as \"the germ of laziness\" because they produced listlessness and weakness in the people they infested. Hookworms infested 40 percent of southerners and were identified in the North as the cause of the South's alleged backwardness.\n\nIt was alleged that indolence was the reason for backward conditions in Indonesia, such as the failure to implement Green Revolution agricultural methods. But a counter-argument is that the Indonesians, living very precariously, sought to play it safe by not risking a failed crop, given that not all experiments introduced by outsiders had been successful.\n\nIt is common for animals (even those like hummingbirds that have high energy needs) to forage for food until satiated, and then spend most of their time doing nothing, or at least nothing in particular. They seek to \"satisfice\" their needs rather than obtaining an optimal diet or habitat. Even diurnal animals, which have a limited amount of daylight in which to accomplish their tasks, follow this pattern. Social activity comes in a distant third to eating and resting for foraging animals. When more time must be spent foraging, animals are more likely to sacrifice time spent on aggressive behavior than time spent resting. Extremely efficient predators have more free time and thus often appear more lazy than relatively inept predators that have little free time. Beetles likewise seem to forage lazily due to a lack of foraging competitors. On the other hand, some animals, such as pigeons and rats, seem to prefer to respond for food rather than eat equally available \"free food\" in some conditions.\n\n"}
{"id": "177725", "url": "https://en.wikipedia.org/wiki?curid=177725", "title": "Magic and religion", "text": "Magic and religion\n\nMagical thinking in various forms is a cultural universal and an important aspect of religion.\nMagic is prevalent in all societies, regardless of whether they have organized religion or more general systems of animism or shamanism.\nReligion and magic became conceptually separated with the development of western monotheism, where the distinction arose between supernatural events sanctioned by mainstream religious doctrine (miracles) and magic rooted in folk belief or occult speculation.\nIn pre-monotheistic religious traditions, there is no fundamental distinction between religious practice and magic; tutelary deities concerned with magic are sometimes called hermetic deities or spirit guides.\n\nIt is a postulate of modern anthropology, at least since early 1930s, that there is complete continuity between magic and religion.\n\nEarly sociological interpretations of magic by Marcel Mauss and Henri Hubert emphasized the social conditions in which the phenomenon of magic develops. According to them, religion is the expression of a social structure and serves to maintain the cohesion of a community (religion is therefore public) and magic is an individualistic action (and therefore private).\n\nRalph Merrifield, the British archaeologist credited as producing the first full-length volume dedicated to a material approach to magic, defined the differences between religion and magic: \n\"'Religion' is used to indicate the belief in supernatural or spiritual beings; 'magic', the use of practices intended to bring occult forces under control and so to influence events; 'ritual', prescribed or customary behaviour that may be religious, if it is intended to placate or win favour of supernatural beings, magical if it is intended to operate through impersonal forces of sympathy or by controlling supernatural beings, or social if its purpose is to reinforce a social organisation or facilitate social intercourse\".\n\nIn 1991 Henk Versnel argued that magic and religion function in different ways and that these can be broadly defined in four areas: Intention - magic is employed to achieve clear and immediate goals for an individual, whereas religion is less purpose-motivated and has its sights set on longer-term goals; Attitude – magic is manipulative as the process is in the hands of the user, “instrumental coercive manipulation”, opposed to the religious attitude of “personal and supplicative negotiation”; Action – magic is a technical exercise that often requires professional skills to fulfil an action, whereas religion is not dependent upon these factors but the will and sentiment of the gods; Social – the goals of magic run counter to the interests of a society (in that personal gain for an individual gives them an unfair advantage over peers), whereas religion has more benevolent and positive social functions.\n\nThis separation of the terms 'religion' and 'magic' in a functional sense is disputed. It has been argued that abandoning the term magic in favour of discussing \"belief in spiritual beings\" will help to create a more meaningful understanding of all associated ritual practices. However using the word 'magic' alongside 'religion' is one method of trying to understand the supernatural world, even if some other term can eventually take its place.\n\nBoth magic and religion contain rituals. \nMost cultures have or have had in their past some form of magical tradition that recognizes a shamanistic interconnectedness of spirit. This may have been long ago, as a folk tradition that died out with the establishment of a major world religion, such as Judaism, Christianity, Islam or Buddhism, or it may still co-exist with that world religion.\n\nThere is a long-standing belief in the power of true names, this often descends from the magical belief that knowing a being's true name grants power over it.\n\nIf names have power, then knowing the name of a god regarded as supreme in a religion should grant the greatest power of all. This belief is reflected in traditional Wicca, where the names of the Goddess and the Horned God - the two supreme deities in Wicca - are usually held as a secret to be revealed only to initiates. This belief is also reflected in ancient Judaism, which used the Tetragrammaton (YHWH, usually translated as \"Lord\" in small caps) to refer to God in the Tanakh. The same belief is seen in Hinduism, but with different conclusions; rather, attaining transcendence and the power of God is seen as a \"good\" thing. Thus, some Hindus chant the name of their favorite deities as often as possible, the most common being Krishna.\n\nMagic and Abrahamic religions have had a somewhat checkered past. The King James Version of the Bible included the famous translation \"Thou shalt not suffer a witch to live\" (Exodus 22:18), and Saul is rebuked by God for seeking advice from a diviner who could contact spirits. On the other hand, seemingly magical signs are documented in the Bible: For example, both the staff of Pharaoh's sorcerers as well as the staff of Moses and Aaron could be turned into snakes (Exodus 7:8-13). However, as Scott Noegel points out, the critical difference between the magic of Pharaoh's magicians and the non-magic of Moses is in the means by which the staff becomes a snake. For the Pharaoh's magicians, they employed \"their secret arts\" whereas Moses merely throws down his staff to turn it into a snake. To an ancient Egyptian, the startling difference would have been that Moses neither employed secret arts nor magical words. In the Torah, Noegel points out that YHWH does not need magical rituals to act.\n\nThe words 'witch' and 'witchcraft' appear in some English versions of the Bible. One verse that is probably responsible for more deaths of suspected witches than any other passage from the Hebrew Scriptures (Old Testament) is Exodus 22:18. In the King James Version, this reads: \"Thou shalt not suffer a witch to live.\" The precise meaning of the Hebrew word \"kashaph\", here translated as 'witch' and in some other modern versions, 'sorceress', is uncertain. In the Septuagint it was translated as \"pharmakeia\", meaning 'pharmacy', and on this basis, Reginald Scot claimed in the 16th century that 'witch' was an incorrect translation and poisoners were intended.\n"}
{"id": "4172982", "url": "https://en.wikipedia.org/wiki?curid=4172982", "title": "Mahoroba", "text": "Mahoroba\n\nMahoroba is an ancient Japanese word describing a far-off land full of bliss and peace. It is roughly comparable to the western concepts of arcadia, a place surrounded by mountains full of harmony and quiet.\n\n\"Mahoroba\" is now written only in hiragana as まほろば. The origins of the word are not clear; it is described in a poem in the ancient \"Kojiki\" (古事記) as being the perfect place in Yamato:\nNote that the Kojiki itself did not use hiragana; the above is a modernized version.\n\n"}
{"id": "15893970", "url": "https://en.wikipedia.org/wiki?curid=15893970", "title": "Marc Rembold", "text": "Marc Rembold\n\nMarc Rembold (born 1963 in Zurich) is a Swiss artist. He developed the \"Light in Colour\" concept during the 1980s. The concept was defined as living colours which change their value with the temperature of the light. In 2000 he worked on the realisation of his series \"Liquids\", in which colours were inspired from the electromagnetic waves of the light spectrum. The work, \"a manifest of the immaterial to material\", brings visible the realm of light's invisible colour spectrum.\n\nA series of postcards from his sculpture series \"Filimini Maximalisme\" were shown in 2003 at the Museum für Schrift Druck und Papier in Basel. The film festival \"volts&visions\" 2007 in Zurich had a live Art Screen Animation during the projection of Michelangelo Antonioni's \"Blow-Up\".\n\nMarc Rembold has exhibited in Europe, North America and Asia. He lives and works in Basel, Switzerland.\n"}
{"id": "4603217", "url": "https://en.wikipedia.org/wiki?curid=4603217", "title": "Maxwell's equations in curved spacetime", "text": "Maxwell's equations in curved spacetime\n\nIn physics, Maxwell's equations in curved spacetime govern the dynamics of the electromagnetic field in curved spacetime (where the metric may not be the Minkowski metric) or where one uses an arbitrary (not necessarily Cartesian) coordinate system. These equations can be viewed as a generalization of the vacuum Maxwell's equations which are normally formulated in the local coordinates of flat spacetime. But because general relativity dictates that the presence of electromagnetic fields (or energy/matter in general) induce curvature in spacetime, Maxwell's equations in flat spacetime should be viewed as a convenient approximation.\n\nWhen working in the presence of bulk matter, it is preferable to distinguish between free and bound electric charges. Without that distinction, the vacuum Maxwell's equations are called the \"microscopic\" Maxwell's equations. When the distinction is made, they are called the macroscopic Maxwell's equations.\n\nThe electromagnetic field also admits a coordinate-independent geometric description, and Maxwell's equations expressed in terms of these geometric objects are the same in any spacetime, curved or not. Also, the same modifications are made to the equations of flat Minkowski space when using local coordinates that are not Cartesian. For example, the equations in this article can be used to write Maxwell's equations in spherical coordinates. For these reasons, it may be useful to think of Maxwell's equations in Minkowski space as a special case, rather than Maxwell's equations in curved spacetimes as a generalization.\n\nIn general relativity, the metric, \"g\", is no longer a constant (like η as in Examples of metric tensor) but can vary in space and time, and the equations of electromagnetism in a vacuum become:\n\nwhere \"f\" is the density of the Lorentz force, \"g\" is the reciprocal of the metric tensor \"g\", and \"g\" is the determinant of the metric tensor. Notice that \"A\" and \"F\" are (ordinary) tensors while formula_5, \"J\", and \"f\" are tensor \"densities\" of weight +1. Despite the use of partial derivatives, these equations are invariant under arbitrary curvilinear coordinate transformations. Thus if one replaced the partial derivatives with covariant derivatives, the extra terms thereby introduced would cancel out. (Cf. manifest covariance#Example.)\n\nThe electromagnetic potential is a covariant vector, \"A\" which is the undefined primitive of electromagnetism. As a covariant vector, its rule for transforming from one coordinate system to another is\n\nThe electromagnetic field is a covariant antisymmetric tensor of degree 2 which can be defined in terms of the electromagnetic potential by\n\nTo see that this equation is invariant, we transform the coordinates (as described in the classical treatment of tensors)\n\nThis definition implies that the electromagnetic field satisfies \n\nwhich incorporates Faraday's law of induction and Gauss's law for magnetism. This is seen by\n\nAlthough there appear to be 64 equations in Faraday–Gauss, it actually reduces to just four independent equations. Using the antisymmetry of the electromagnetic field one can either reduce to an identity (0 = 0) or render redundant all the equations except for those with \"λ\", \"μ\", \"ν\" being either 1, 2, 3 or 2, 3, 0 or 3, 0, 1 or 0, 1, 2.\n\nThe Faraday–Gauss equation is sometimes written\n\nwhere a semicolon indicates a covariant derivative, a comma indicates a partial derivative, and square brackets indicate anti-symmetrization (see Ricci calculus for the notation). The covariant derivative of the electromagnetic field is\n\nwhere Γ\"\" is the Christoffel symbol, which is symmetric in its lower indices.\n\nThe electric displacement field, D and the auxiliary magnetic field, H form an antisymmetric contravariant rank 2 tensor density of weight +1. In a vacuum, this is given by\n\nThis equation is the only place where the metric (and thus gravity) enters into the theory of electromagnetism. Furthermore, the equation is invariant under a change of scale, that is, multiplying the metric by a constant has no effect on this equation. Consequently, gravity can only affect electromagnetism by changing the speed of light relative to the global coordinate system being used. Light is only deflected by gravity because it is slower when near to massive bodies. So it is as if gravity increased the index of refraction of space near massive bodies.\n\nMore generally, in materials where the magnetization–polarization tensor is non-zero, we have\n\nThe transformation law for electromagnetic displacement is\n\nwhere the Jacobian determinant is used. If the magnetization-polarization tensor is used, it has the same transformation law as the electromagnetic displacement.\n\nThe electric current is the divergence of the electromagnetic displacement. In a vacuum,\n\nIf magnetization-polarization is used, then this just gives the free portion of the current\n\nThis incorporates Ampere's Law and Gauss's Law.\n\nIn either case, the fact that the electromagnetic displacement is antisymmetric implies that the electric current is automatically conserved\n\nbecause the partial derivatives commute.\n\nThe Ampere-Gauss definition of the electric current is not sufficient to determine its value because the electromagnetic potential (from which it was ultimately derived) has not been given a value. Instead, the usual procedure is to equate the electric current to some expression in terms of other fields, mainly the electron and proton, and then solve for the electromagnetic displacement, electromagnetic field, and electromagnetic potential.\n\nThe electric current is a contravariant vector density, and as such it transforms as follows\n\nVerification of this transformation law\n\nSo all that remains is to show that\n\nwhich is a version of a known theorem (see Inverse functions and differentiation#Higher derivatives).\n\nThe density of the Lorentz force is a covariant vector density given by\n\nThe force on a test particle subject only to gravity and electromagnetism is\n\nwhere \"p\" is the linear 4-momentum of the particle, \"t\" is any time coordinate parameterizing the world line of the particle, Γ is the Christoffel symbol (gravitational force field), and \"q\" is the electric charge of the particle.\n\nThis equation is invariant under a change in the time coordinate; just multiply by formula_25 and use the chain rule. It is also invariant under a change in the \"x\" coordinate system.\n\nUsing the transformation law for the Christoffel symbol\n\nwe get\n\nIn a vacuum, the Lagrangian density for classical electrodynamics (in joules/meter) is a scalar density\n\nwhere\n\nThe four-current should be understood as an abbreviation of many terms expressing the electric currents of other charged fields in terms of their variables.\n\nIf we separate free currents from bound currents, the Lagrangian becomes\n\nAs part of the source term in the Einstein field equations, the electromagnetic stress–energy tensor is a covariant symmetric tensor\n\nusing a metric of signature (−,+,+,+). If using the metric with signature (+,−,−,−), the expression for formula_32 will have opposite sign. The stress–energy tensor is trace-free\n\nbecause electromagnetism propagates at the invariant speed.\n\nIn the expression for the conservation of energy and linear momentum, the electromagnetic stress–energy tensor is best represented as a mixed tensor density\n\nFrom the equations above, one can show that\n\nwhere the semicolon indicates a covariant derivative.\n\nThis can be rewritten as\n\nwhich says that the decrease in the electromagnetic energy is the same as the work done by the electromagnetic field on the gravitational field plus the work done on matter (via the Lorentz force), and similarly the rate of decrease in the electromagnetic linear momentum is the electromagnetic force exerted on the gravitational field plus the Lorentz force exerted on matter.\n\nDerivation of conservation law\n\nwhich is zero because it is the negative of itself (see four lines above).\n\nThe nonhomogeneous electromagnetic wave equation in terms of the field tensor is modified from the special relativity form to\n\nwhere \"R\" is the covariant form of the Riemann tensor and formula_39 is a generalization of the d'Alembertian operator for covariant derivatives. Using\n\nMaxwell's source equations can be written in terms of the 4-potential [ref 2, p. 569] as,\n\nor, assuming the generalization of the Lorenz gauge in curved spacetime\n\nwhere formula_44 is the Ricci curvature tensor.\n\nThis the same form of the wave equation as in flat spacetime, except that the derivatives are replaced by covariant derivatives and there is an additional term proportional to the curvature. The wave equation in this form also bears some resemblance to the Lorentz force in curved spacetime where \"A\" plays the role of the 4-position.\n\nWhen Maxwell's equations are treated in a background independent manner, that is, when the spacetime metric is taken to be a dynamical variable dependent on the electromagnetic field, then the electromagnetic wave equation and Maxwell's equations are nonlinear. This can be seen by noting that the curvature tensor depends on the stress–energy tensor through the Einstein field equation\n\nwhere\n\nis the Einstein tensor, \"G\" is the gravitational constant, \"g\" is the metric tensor, and \"R\" (scalar curvature) is the trace of the Ricci curvature tensor. The stress–energy tensor is composed of the stress-energy from particles, but also stress-energy from the electromagnetic field. This generates the nonlinearity.\n\nIn the differential geometric formulation of the electromagnetic field, the antisymmetric Faraday tensor can be considered as the Faraday 2-form F. In this view, one of Maxwell's two equations is dF= 0, where \"d\" is the exterior derivative operator. This equation is completely coordinate and metric independent and says that the electro-magnetic flux through a closed two dimensional surface in space time is topological, more precisely, depends only on its homology class (a generalization of the integral form of Gauss law and Maxwell-Faraday equation as the homology class in Minkowski space is automatically 0). By the Poincaré lemma, this equation implies, (at least locally) that there exists a 1-form A satisfying F = d A. The other Maxwell equation is d * F = J.\nIn this context, J is the current 3-form (or even more precise, twisted three form), the asterisk * denotes the Hodge star operator, and d is the exterior derivative operator. The dependence of Maxwell's equation on the metric of spacetime lies in the Hodge star operator * on two forms, which is conformally invariant. Written this way, Maxwell's equation is the same in any space time, manifestly coordinate invariant, and convenient to use (even in Minkowski space or Euclidean space and time especially with curvilinear coordinates).\n\nAn even more geometric interpretation is that the Faraday two form F is (up to a factor i) the curvature 2-form formula_47 of a \"U\"(1)-connection formula_48 on a principal \"U\"(1)-bundle whose sections represent charged fields. The connection is much like the vector potential since every connection can be written as formula_49 for a \"base\" connection formula_50 and F = F + d A. In this view, the Maxwell \"equation\", d F= 0, is a mathematical identity known as the Bianchi identity. The equation d * F = J is the only equation with any physical content in this formulation. This point of view is particularly natural when considering charged fields or quantum mechanics. It can be interpreted as saying that, much like gravity can be understood as being the result of the necessity of a connection to parallel transport vectors at different points, electromagnetic phenomena, or more subtle quantum effects like the Aharanov-Bohm effect, can be understood as a result from the necessity of a connection to parallel transport charged fields or wave sections at different points. In fact, just as the Riemann tensor is the holonomy of the Levi Civita connection along an infinitesimal closed curve, the curvature of the connection is the holonomy of the U(1)-connection.\n\n\n\n"}
{"id": "313055", "url": "https://en.wikipedia.org/wiki?curid=313055", "title": "Mere addition paradox", "text": "Mere addition paradox\n\nThe mere addition paradox, also known as the repugnant conclusion, is a problem in ethics, identified by Derek Parfit and discussed in his book \"Reasons and Persons\" (1984). The paradox identifies the mutual incompatibility of four intuitively compelling assertions about the relative value of populations.\n\nConsider the four populations depicted in the following diagram: A, A+, B− and B. Each bar represents a distinct group of people, with the group's size represented by the bar's width and the happiness of each of the group's members represented by the bar's height. Unlike A and B, A+ and B− are complex populations, each comprising two distinct groups of people.\n\nHow do these populations compare in value? \n\nParfit observes that i) A+ seems no worse than A. This is because the people in A are no worse-off in A+, while the additional people who exist in A+ are better off in A+ compared to A (if it is stipulated that their lives are good enough that living them is better than not existing).\n\nNext, Parfit suggests that ii) B− seems better than A+. This is because B− has greater total and average happiness than A+.\n\nThen, he notes that iii) B seems equally as good as B−, as the only difference between B− and B is that the two groups in B− are merged to form one group in B.\n\nTogether, these three comparisons entail that B is better than A. However, Parfit observes that when we directly compare A (a population with high average happiness) and B (a population with lower average happiness, but more total happiness because of its larger population), it may seem that B can be worse than A.\n\nThus, there is a paradox. The following intuitively plausible claims are jointly incompatible: (a) that A+ is no worse than A, (b) that B− is better than A+, (c) that B− is equally as good as B, and (d) that B can be worse than A.\n\nSome scholars, such as Larry Temkin and Stuart Rachels, argue that the apparent inconsistency between the four claims just outlined relies on the assumption that the \"better than\" relation is transitive. We may resolve the inconsistency, thus, by rejecting the assumption. On this view, from the fact that A+ is no worse than A and that B− is better than A+ it simply does not follow that B− is better than A.\n\nTorbjörn Tännsjö argues that the intuition that B is worse than A is wrong. While the lives of those in B are worse than those in A, there are more of them and thus the collective value of B is greater than A. Michael Huemer also argues that the repugnant conclusion is not repugnant and that normal intuition is wrong.\n\nHowever, the above discussion fails to appreciate the true source of repugnance. On the face of it, it may not be absurd to think that B is better than A. Suppose, then, that B is in fact better than A, as Huemer argues. It follows that this revised intuition must hold in subsequent iterations of the original steps. For example, the next iteration would add even more people to B+, and then take the average of the total happiness, resulting in C-. If these steps are repeated over and over, the eventual result will be Z, a massive population with the minimum level of average happiness; this would be a population in which every member is leading a life barely worth living. Z is the repugnant conclusion.\n\nAn alternative use of the term \"mere addition paradox\" was presented in a paper by Hassoun in 2010. It identifies paradoxical reasoning that occurs when certain statistical measures are used to calculate results over a population. For example, if a group of 100 people together control $100 worth of resources, the average wealth per capita is $1. If a single rich person then arrives with 1 million dollars, then the total group of 101 people controls $1,000,100, making average wealth per capita $9,901, implying a drastic shift away from poverty even though nothing has changed for the original 100 people. Hassoun defines a \"no mere addition axiom\" to be used for judging such statistical measures: \"merely adding a rich person to a population should not decrease poverty\" (although acknowledging that in actual practice adding rich people to a population may provide some benefit to the whole population).\n\nThis same argument can be generalized to many cases where proportional statistics are used: for example, a game sold on a download service may be considered a failure if less than 20% of those who download the demo then purchase the game. Thus, if 10,000 people download the demo of a game and 2,000 buy it, the game is a borderline success; however, it would be rendered a failure by an extra 500 people downloading the demo and not buying, even though this \"mere addition\" changes nothing with regard to income or consumer satisfaction from the previous situation.\n\n\n\n"}
{"id": "13870049", "url": "https://en.wikipedia.org/wiki?curid=13870049", "title": "Microwave International New Media Arts Festival", "text": "Microwave International New Media Arts Festival\n\nMicrowave International New Media Arts Festival (微波國際新媒體藝術節）is a new media art festival based in Hong Kong. It began in 1996 as the annual video art festival for local video art collective Videotage.\n\nIn 2006, it became independent from Videotage and in 2007 it officially became fully independent as the government's previous \"presenter\" role was handed over to Microwave. The government remains the main sponsor of the Festival, although many more sponsors are sought for each year's Festival. \n\nThe annual festival generally includes a main exhibition at the Hong Kong City Hall, a smaller scale and usually more alternative exhibition in a separate venue, a keynote conference, performances, screening programmes and other special events organised each year. \n\nIn 2007, the performance by US art collective Graffiti Research Lab that \"hacked the city\" with their L.A.S.E.R.T.A.G along with the participation of local graffiti artist MC Yan, they achieved a record-breaking tag 1,200 metres \"across\" Victoria Harbour (James Powderly and MC Yan tagging from Central ferry, with Evan Roth and the local crew controlling the set up in front of the Cultural Centre in Tsim Sha Tsui).\n\nIn its first extended event outside of the annual November Festival, Microwave also held the \"A-Glow-Glow\" Macro Interactive Media Art Exhibition in April 2008, which was funded by the Hong Kong Arts Development Council and aimed for more mass appeal. Two large-scale interactive LED artworks were placed by the Tsim Sha Tsui waterfront, right across the harbour from the Hong Kong City Hall, where the annual festival main exhibition is held.\n\nMicrowave's festival design, by local design partner Milkxhake, also consistently wins design awards almost every year since their partnership commenced. \n\nNature Transformer (自然反）\n\nTransient Creatures (異生界）\n\n \"A-Glow-Glow\" Macro Interactive Media Art Exhibition\n\n\"A-Glow-Glow\" was held by the waterfront in Tsim Sha Tsui (Kowloon Peninsula), with two interactive LED installations by two artist groups:\n\n\nLuminous Echo (形光譜)\n"}
{"id": "11737650", "url": "https://en.wikipedia.org/wiki?curid=11737650", "title": "Molecular drive", "text": "Molecular drive\n\nMolecular drive is a term coined by Gabriel Dover in 1982 to describe evolutionary processes that change the genetic composition of a population through DNA turnover mechanisms. Molecular drive operates independently of natural selection and genetic drift.\n\nThe best-known such process is the concerted evolution of genes present in many tandem copies, such as those for ribosomal RNAs or silk moth egg shell chorion proteins, in sexually reproducing species. The concept has been proposed to extend to the diversification of multigene families. The mechanisms involved include gene conversion, unequal crossing-over, transposition, slippage replication and RNA-mediated exchanges. Because mutations changing the sequence of one copy are less common than deletions, duplications and replacement of one copy by another, the copies gradually come to resemble each other much more than they would if they had been evolving independently.\n\nConcerted evolution can be unbiased, in which case every version has an equal probability of being the one that replaces the others. However, if the molecular events have any bias favouring one version of the sequence over others, that version will dominate the process and eventually replace the others. The name 'molecular drive' reflects the similarity of the process with what was originally the better-known process of meiotic drive.\n\nMolecular drive can also act in bacteria, where parasexual processes such as natural transformation cause DNA turnover.\n\nAccording to Dover, TRAM is a genetic system that has features of Non-mendelian inheritance Turnover, copy number and functional Redundancy And Modulatory. To date all regulatory regions (promoters) and genes, that have been examined in detail at the molecular level, have TRAM characteristics. As such, part of their evolutionary history will have been influenced by the molecular drive process.\n\nAccording to Dover, Adoptation is an evolved feature of an organism that contributes to its viability and reproduction (established by molecular drive) and that adopts some previously inaccessible component of the environment.\n"}
{"id": "20580", "url": "https://en.wikipedia.org/wiki?curid=20580", "title": "Motion (physics)", "text": "Motion (physics)\n\nIn physics, motion is a change in position of an object over time. Motion is mathematically described in terms of displacement, distance, velocity, acceleration, time, and speed. Motion of a body is observed by attaching a frame of reference to an observer and measuring the change in position of the body relative to that frame.\n\nIf the position of a body is not changing with respect to a given frame of reference (reference point), the body is said to be \"at rest\", \"motionless\", \"immobile\", \"stationary\", or to have constant (time-invariant) position. An object's motion cannot change unless it is acted upon by a force, as described. Momentum is a quantity which is used for measuring the motion of an object. An object's momentum is directly related to the object's mass and velocity, and the total momentum of all objects in an isolated system (one not affected by external forces) does not change with time, as described by the law of conservation of momentum.\n\nAs there is no absolute frame of reference, \"absolute motion\" cannot be determined. Thus, everything in the universe can be considered to be moving.\n\nMotion applies to objects, bodies, and matter particles, to radiation, radiation fields and radiation particles, and to space, its curvature and space-time. One can also speak of motion of shapes and boundaries. So, the term motion, in general, signifies a continuous change in the configuration of a physical system. For example, one can talk about motion of a wave or about motion of a quantum particle, where the configuration consists of probabilities of occupying specific positions.\n\nIn physics, motion is described through two sets of apparently contradictory laws of mechanics. Motions of all large-scale and familiar objects in the universe (such as projectiles, planets, cells, and humans) are described by classical mechanics. Whereas the motion of very small atomic and sub-atomic objects is described by quantum mechanics.\n\nClassical mechanics is used for describing the motion of macroscopic objects, from projectiles to parts of machinery, as well as astronomical objects, such as spacecraft, planets, stars, and galaxies. It produces very accurate results within these domains, and is one of the oldest and largest in science, engineering, and technology.\n\nClassical mechanics is fundamentally based on Newton's laws of motion. These laws describe the relationship between the forces acting on a body and the motion of that body. They were first compiled by Sir Isaac Newton in his work \"Philosophiæ Naturalis Principia Mathematica\", first published on July 5, 1687. Newton's three laws are:\n\nNewton's three laws of motion were the first to accurately provide a mathematical model for understanding orbiting bodies in outer space. This explanation unified the motion of celestial bodies and motion of objects on earth.\n\nClassical mechanics was further enhanced by Albert Einstein's special relativity and general relativity. Special relativity is concerned with the motion of objects with a high velocity, approaching the speed of light; general relativity is employed to handle gravitational motion at a deeper level.\n\nUniform Motion:\nWhen an object moves with a constant speed at a particular direction at regular intervals of time it's known as the \"uniform motion.\" For example: a bike moving in a straight line with a constant speed.\n\nEQUATIONS OF UNIFORM MOTION:\n\nIf v = final velocity, u = initial velocity, a = acceleration, t = time, s = displacement, then :\n\nQuantum mechanics is a set of principles describing physical reality at the atomic level of matter (molecules and atoms) and the subatomic particles (electrons, protons, neutrons, and even smaller elementary particles such as quarks). These descriptions include the simultaneous wave-like and particle-like behavior of both matter and radiation energy as described in the wave–particle duality.\n\nIn classical mechanics, accurate measurements and predictions of the state of objects can be calculated, such as location and velocity. In the quantum mechanics, due to the Heisenberg uncertainty principle, the complete state of a subatomic particle, such as its location and velocity, cannot be simultaneously determined. \n\nIn addition to describing the motion of atomic level phenomena, quantum mechanics is useful in understanding some large-scale phenomenon such as superfluidity, superconductivity, and biological systems, including the function of smell receptors and the structures of proteins.\n\nHumans, like all known things in the universe, are in constant motion; however, aside from obvious movements of the various external body parts and locomotion, humans are in motion in a variety of ways which are more difficult to perceive. Many of these \"imperceptible motions\" are only perceivable with the help of special tools and careful observation. The larger scales of imperceptible motions are difficult for humans to perceive for two reasons: Newton's laws of motion (particularly the third) which prevents the feeling of motion on a mass to which the observer is connected, and the lack of an obvious frame of reference which would allow individuals to easily see that they are moving. The smaller scales of these motions are too small to be detected conventionally with human senses.\n\nSpacetime (the fabric of the universe) is expanding meaning everything in the universe is stretching like a rubber band. This motion is the most obscure as it is not physical motion as such, but rather a change in the very nature of the universe. The primary source of verification of this expansion was provided by Edwin Hubble who demonstrated that all galaxies and distant astronomical objects were moving away from Earth, known as Hubble's law, predicted by a universal expansion.\n\nThe Milky Way Galaxy is moving through space and many astronomers believe the velocity of this motion to be approximately relative to the observed locations of other nearby galaxies. Another reference frame is provided by the Cosmic microwave background. This frame of reference indicates that the Milky Way is moving at around .\n\nThe Milky Way is rotating around its dense galactic center, thus the sun is moving in a circle within the galaxy's gravity. Away from the central bulge, or outer rim, the typical stellar velocity is between . All planets and their moons move with the sun. Thus, the solar system is moving.\n\nThe Earth is rotating or spinning around its axis. This is evidenced by day and night, at the equator the earth has an eastward velocity of . The Earth is also orbiting around the Sun in an orbital revolution. A complete orbit around the sun takes one year, or about 365 days; it averages a speed of about .\n\nThe Theory of Plate tectonics tells us that the continents are drifting on convection currents within the mantle causing them to move across the surface of the planet at the slow speed of approximately per year. However, the velocities of plates range widely. The fastest-moving plates are the oceanic plates, with the Cocos Plate advancing at a rate of per year and the Pacific Plate moving per year. At the other extreme, the slowest-moving plate is the Eurasian Plate, progressing at a typical rate of about per year.\n\nThe human heart is constantly contracting to move blood throughout the body. Through larger veins and arteries in the body, blood has been found to travel at approximately 0.33 m/s. Though considerable variation exists, and peak flows in the venae cavae have been found between . additionally, the smooth muscles of hollow internal organs are moving. The most familiar would be the occurrence of peristalsis which is where digested food is forced throughout the digestive tract. Though different foods travel through the body at different rates, an average speed through the human small intestine is . The human lymphatic system is also constantly causing movements of excess fluids, lipids, and immune system related products around the body. The lymph fluid has been found to move through a lymph capillary of the skin at approximately 0.0000097 m/s.\n\nThe cells of the human body have many structures which move throughout them. Cytoplasmic streaming is a way which cells move molecular substances throughout the cytoplasm, various motor proteins work as molecular motors within a cell and move along the surface of various cellular substrates such as microtubules, and motor proteins are typically powered by the hydrolysis of adenosine triphosphate (ATP), and convert chemical energy into mechanical work. Vesicles propelled by motor proteins have been found to have a velocity of approximately 0.00000152 m/s.\n\nAccording to the laws of thermodynamics, all particles of matter are in constant random motion as long as the temperature is above absolute zero. Thus the molecules and atoms which make up the human body are vibrating, colliding, and moving. This motion can be detected as temperature; higher temperatures, which represent greater kinetic energy in the particles, feel warm to humans who sense the thermal energy transferring from the object being touched to their nerves. Similarly, when lower temperature objects are touched, the senses perceive the transfer of heat away from the body as feeling cold.\n\nWithin each atom, electrons exist in an area around the nucleus. This area is called the electron cloud. According to Bohr's model of the atom, electrons have a high velocity, and the larger the nucleus they are orbiting the faster they would need to move. If electrons 'move' about the electron cloud in strict paths the same way planets orbit the sun, then electrons would be required to do so at speeds which far exceed the speed of light. However, there is no reason that one must confine one's self to this strict conceptualization, that electrons move in paths the same way macroscopic objects do. Rather one can conceptualize electrons to be 'particles' that capriciously exist within the bounds of the electron cloud. Inside the atomic nucleus, the protons and neutrons are also probably moving around due to the electrical repulsion of the protons and the presence of angular momentum of both particles.\n\nLight propagates at 299,792,458 m/s, often approximated as in a vacuum. The speed of light (or \"c\") is also the speed of all massless particles and associated fields in a vacuum, and it is the upper limit on the speed at which energy, matter, information or causation can travel; the speed of light is the limit of speed for all physical systems.\n\nIn addition, the speed of light is an invariant quantity: it has the same value, irrespective of the position or speed of the observer. This property makes the speed of light \"c\" a natural measurement unit for speed.\n\n\n\n"}
{"id": "1228073", "url": "https://en.wikipedia.org/wiki?curid=1228073", "title": "Napoleon and the Jews", "text": "Napoleon and the Jews\n\nRevolutionary France enacted laws that first emancipated Jews in France, establishing them as equal citizens to other Frenchmen. In countries that Napoleon Bonaparte's ensuing First French Empire conquered during the Napoleonic Wars, he emancipated the Jews and introduced other ideas of freedom from the French Revolution. For instance, he overrode old laws restricting Jews to reside in ghettos, as well as lifting laws that limited Jews' rights to property, worship, and certain occupations.\n\nIn an effort to promote Jewish integration into French society, however, Napoleon also implemented several policies that eroded Jewish distinction. For example, he restricted the Jewish practice of money-lending, restricted the regions to which Jews were allowed to migrate, and required Jews to adopt formal names. He also implemented a series of consistories -- which served as an effective channel utilized by the French government to regulate Jewish religious life. \n\nHistorians have disagreed about Napoleon's intentions in these actions, as well as his personal and political feelings about the Jewish community. Some have said he had political reasons but did not have sympathy for the Jews. His actions were generally opposed by the leaders of monarchies in other countries. After his defeat by Great Britain, a counter-revolution swept many of these countries and they restored discriminatory measures against the Jews.\n\nThe French Revolution abolished the different treatment of people according to religion or origin that had existed under the monarchy. Roman Catholicism had been the established state religion, closely tied historically to the monarchy, which represented both religious and political authority. The 1789 Declaration of the Rights of Man and of the Citizen guaranteed freedom of religion and free exercise of worship, provided that it did not contradict public order. At that time, most other European countries implemented measures that restricted the rights of people in their nations who practiced minority religions.\n\nIn the early 19th century, through his conquests in Europe, Napoleon Bonaparte spread the modernist ideas of revolutionary France: equality of citizens and the rule of law. Napoleon's personal attitude towards the Jews has been interpreted in various ways by different historians, as at various times he made statements both in support and opposition to the Jewish people. Orthodox Rabbi Berel Wein in \"Triumph of Survival: The Story of the Jews in the Modern Era 1650-1990\" (1990) claims that Napoleon was interested primarily in seeing the Jews assimilate, rather than prosper as a distinct community: \"Napoleon's outward tolerance and fairness toward Jews was actually based upon his grand plan to have them disappear entirely by means of total assimilation, intermarriage, and conversion.\"\n\nNapoleon was concerned about the role of Jews as money lenders, wanting to end that. The treatment of the Alsace Jews and their debtors was raised in the Imperial Council on 30 April 1806. His liberation of the Jewish communities in Italy (notably in Ancona in the Papal States) and his insistence on the integration of Jews as equals in French and Italian societies demonstrate that he distinguished between usurers (whether Jewish or not), whom he compared to locusts, and Jews who accepted non-Jews as their equals.\n\nHis letter to Champagny, Minister of the Interior of 29 November 1806, expresses his thoughts:\n\n(While insisting on the primacy of civil law over the military, Napoleon retained a deep respect and affection for the military as a profession. He often hired former soldiers in civilian occupations).\n\nThrough his policies overall, Napoleon greatly improved the condition of the Jews in France and Europe, and they widely admired him. Starting in 1806, Napoleon passed a number of measures enhancing the position of the Jews in the French Empire. He recognized a representative group elected by the Jewish community, the \"Sanhedrin,\" as their representatives to the French government.\n\nIn conquered countries, he abolished laws restricting Jews to living in ghettos. In 1807, he designated Judaism as one of the official religions of France, along with Roman Catholicism (long the established state religion), and Lutheran and Calvinist Protestantism. (Followers of the latter had been severely persecuted by the monarchy in the late 17th and early 18th centuries.)\n\nIn 1808 Napoleon rolled back a number of reforms (under the so-called \"décret infâme\", or Infamous Decree, of 17 March 1808), declaring all debts with Jews to be annulled, reduced or postponed. The Infamous Decree imposed a ten-year ban on any kind of Jewish money-lending activity. Similarly, Jewish individuals who were in subservient positions -- such as a Jewish servant, military officer, or wife -- were unable to engage in any kind of money-lending activity without the explicit consent of their superiors. Napoleon's goal in implementing the Infamous Decree of 1808 was to integrate Jewish culture and customs into those of France. By restricting money-lending activity, Jews would be forced to engage in other practices for a living. Likewise, in order to even engage in money-lending activity, the decree required Jews to apply for an annual license, granted only with the recommendation of the Jews' local consistory and with the surety of the Jews' honesty. This caused so much financial loss that the Jewish community nearly collapsed. On a different note, the Infamous Decree also placed heavy restrictions on the Jews' ability to migrate. The decree prevented the Jews from relocating to the regions of Alsace and required that Jews wishing to move to other regions of France own or purchase land to farm. It also required them to refrain from engaging in any kind of money-lending activity. (The law eventually only placed these restrictions on the Jews of the northeast.) Likewise, the French decree required Jewish individuals to serve in the French military, without any opportunity to provide a replacement individual. The last component of the Infamous Decree – established in July 1808 – required Jews to adopt formal names with which they would be addressed. (Before, Jews were often referred to as “Joseph son of Benjamin.”) They were also prevented from selecting names of cities or names in the Hebrew Bible. Napoleon sought to integrate the Jewish people more fully into French society by establishing the guidelines they were required to follow in adopting names. Napoleon ended these restrictions by 1811.\n\nHistorian Ben Weider argued that Napoleon had to be extremely careful in defending oppressed minorities such as Jews, because of keeping balance with other political interests, but says that the leader clearly saw political benefit to his Empire in the long term in supporting them. Napoleon hoped to use equality as a way of gaining advantage from discriminated groups, like Jews or Protestants.\n\nBoth aspects of his thinking can be seen in an 1822 response to physician Barry O'Meara, who had written to Napoleon after he had been exiled, asking why he pressed for the emancipation of the Jews:\n\nIn a private letter to his brother Jérome Napoleon, dated 6 March 1808, Napoleon expressed a conflicting view:\n\nDuring Napoleon's siege of Acre in 1799, \"Le Moniteur Universel,\" the main French newspaper during the French Revolution, published on 3 Prairial, Year vii (French Republican Calendar, equivalent to 22 May 1799) a short statement that: \n\"Buonaparte a fait publier une proclamation, dans laquelle il invite les juifs de l'Asie et de l'Afrique à venir se ranger sous ses drapeaux, pour rétablir l'ancienne Jérusalem; il en a déjà armé un grand nombre, et leurs bataillons menacent Alep.\"\n\nThis has been translated in English as:\n\"Bonaparte has published a proclamation in which he invites all the Jews of Asia and Africa to gather under his flag in order to re-establish the ancient Jerusalem. He has already given arms to a great number, and their battalions threaten Aleppo.\"\n\nNapoleon's forces lost to Great Britain and he never carried out his alleged plan. Historians such as Nathan Schur in \"Napoleon and the Holy Land\" (2006) believe that Napoleon intended the proclamation for propaganda and to build support for his campaign among the Jews in those regions. Ronald Schechter believes that the newspaper was reporting a rumor, as there is no documentation that Napoleon contemplated such a policy. Other historians suggest that the proclamation was intended to gain support from Haim Farhi, the Jewish advisor to Ahmed al Jazzar, the Muslim ruler of Acre, and to bring him over to Napoleon's side. Farhi commanded the defence of Acre on the field.\n\nIn 1940, historian Franz Kobler claimed to have found a detailed version of the proclamation from a German translation. Kobler's claim was published in \"The New Judaea,\" the official periodical of the Zionist Organisation. The Kobler version suggests that Napoleon was inviting Jews across the Mideast and North Africa to create a Jewish state. It includes phrases such as \"Rightful heirs of Palestine!\" and \"your political existence as a nation among the nations.\" These concepts have been more commonly associated with the Zionist movement, which developed in the late 19th century.\n\nHistorians such as Henry Laurens, Ronald Schechter, and Jeremy Popkin believe that the German document (which has never been found) was a forgery, as asserted by Simon Schwarzfuchs in his 1979 book.\n\nRabbi Aharon Ben-Levi of Jerusalem also added his voice to the proclamation, calling on the Jews to enlist in Napoleon's army \"to return to Zion as in the days of Ezra and Nehemiah\" and rebuild the Temple. According to Prof. Mordechai Gichon, a military historian and archaeologist from Tel Aviv University, who summarized 40 years of research on the subject, Napoleon had an idea to establish a national home for the Jews in the Land of Israel, \"Napoleon believed the Jews would repay his favors by serving French interests in the region.\"Gichon claimed. \"After returning to France, all he was interested in when it came to the Jews was how to use them to reinforce the French nation,\" Gichon says. \"Therefore, he tried to conceal the Zionist chapter of his past.\" On the other hand, Prof. Ze'ev Sternhell of Jerusalem's Hebrew University, however, believes the entire story is nothing more than an oddity. \"Napoleon's big contribution came, in fact, in form of promoting the incorporation of the Jews into French society,\" \n\nNapoleon had more influence on the Jews in Europe than detailed in his decrees. By breaking up the feudal castes of mid-Europe and introducing the equality of the French Revolution, he achieved more for Jewish emancipation than had been accomplished during the three preceding centuries. As part of recognizing the Jewish community, he established a national Israelite Consistory in France. It was intended to serve as a centralizing authority for Jewish religious and community life. Napoleon implemented several other regional consistories throughout the French Empire, as well, with the Israelite Consistory serving as the lead consistory; it had the responsibility of overseeing the various regional consistories. The regional consistories, in turn, oversaw the economic and religious aspects of Jewish life. The regional consistories consisted of a five-individual board, usually occupied by one (or, in some cases two) rabbis, and the rest lay individuals who lived in the districts over which the consistory maintained jurisdiction. The Israelite Consistory in France consisted of three rabbis and two lay individuals. In charge of the Israelite Consistory (and the consistory system in general) was a 25-member board. The members of the board, all Jews, were appointed by the prefect, thus allowing the French government to regulate the system of consistories and, in turn, various aspects of Jewish life. Similarly he established the Westphalia (). This served as a model for other German states until after the fall of Napoleon. Napoleon permanently improved the condition of the Jews in the Prussian Rhine provinces by his rule of this area.\n\nHeine and Börne both recorded their sense of obligation to Napoleon's principles of action. The German Jews in particular have historically regarded Napoleon as the major forerunner of Jewish emancipation in Germany. When the government required Jews to select surnames according to the mainstream model, some are said to have taken the name of \"Schöntheil,\" a translation of \"Bonaparte.\" In the Jewish ghettos, legends grew up about Napoleon's actions. Twentieth-century Italian author Primo Levi wrote that Italian Jews often chose \"Napoleone\" and \"Bonaparte\" as their given name to recognize their historic liberator.\n\nThe Russian Czar Alexander I objected to Napoleon's emancipation of the Jews and establishment of the Great Sanhedrin. He vehemently denounced the liberties given Jews and demanded that the Russian Orthodox Church protest against Napoleon's tolerant religious policy. He referred to the Emperor in a proclamation as \"the Anti-Christ\" and the \"Enemy of God\".\n\nThe Holy Synod of Moscow proclaimed: \"In order to destroy the foundations of the Churches of Christendom, the Emperor of the French has invited into his capital all the Judaic synagogues and he furthermore intends to found a new Hebrew Sanhedrin ― the same council that the Christian bible states, condemned to death (by crucifixion) the revered figure, Jesus of Nazareth.\"\n\nThe Czar persuaded Napoleon to sign a 17 March 1808 decree restricting the freedoms accorded to the Jews. Napoleon expected in exchange that the Czar would help persuade Great Britain to end the war in Europe. Absent that, three months later, Napoleon effectively cancelled the decree by allowing local authorities to implement his earlier reforms. More than half of the French départements restored citizens' guaranteed freedoms to the Jews.\n\nIn Austria, Chancellor Metternich wrote, \"I fear that the Jews will believe (Napoleon) to be their promised Messiah\".\n\nIn Prussia, leaders of the Lutheran Church were extremely hostile to Napoleon's actions. Italian kingdoms were suspicious of his actions, although expressing less violent opposition. \n\nGreat Britain, which was at war with Napoleon, rejected the principle and doctrine of the Sanhedrin.\n\nAll the states under French authority applied Napoleon's reforms. In Italy, the Netherlands, and the German states, the Jews were emancipated and able to act as free men for the first time in those nations. After the British defeated Napoleon at Waterloo, a counter-revolution in many of these countries resulted in the restoration of discriminatory measures against Jews.\n\n\n"}
{"id": "612291", "url": "https://en.wikipedia.org/wiki?curid=612291", "title": "Naïve realism", "text": "Naïve realism\n\nIn philosophy of mind, naïve realism, also known as direct realism, common sense realism or perceptual realism, is the idea that the senses provide us with direct awareness of objects as they \"really\" are. Objects obey the laws of physics and retain all their properties whether or not there is anyone to observe them. They are composed of matter, occupy space and have properties, such as size, shape, texture, smell, taste and colour, that are usually perceived correctly.\n\nIn contrast, some forms of idealism claim that no world exists apart from mind-dependent ideas, and some forms of skepticism say we cannot trust our senses. Naïve realism is known as \"direct\" as against \"indirect\" or \"representational\" realism when its arguments are developed to counter the latter position, also known as epistemological dualism; that our conscious experience is not of the real world but of an internal representation of the world.\n\nFor a history of direct realist theories, see \"Direct and indirect realism § History\".\n\nThe naïve realist theory may be characterized as the acceptance of the following five beliefs:\n\n\nIn the area of visual perception in psychology, the leading direct realist theorist was J. J. Gibson. Other psychologists were heavily influenced by this approach, including William Mace, Claire Michaels, Edward Reed, Robert Shaw, and Michael Turvey. More recently, Carol Fowler has promoted a direct realist approach to speech perception.\n\nAmong contemporary analytic philosophers who defended direct realism one might refer to, for example, Hilary Putnam, John McDowell, Galen Strawson, and John R. Searle.\n\nSearle, for instance, addresses the popular but perhaps mistaken assumption that \"we can only directly perceive our own subjective experiences, but never objects and states of affairs in the world themselves\". According to Searle, it has influenced many thinkers to reject direct realism. But Searle contends that the rejection of direct realism is based on a bad argument: the Argument from illusion, which in turn relies on vague assumptions on the nature or existence of \"sense data\". Various sense data theories were deconstructed in 1962 by the British philosopher J. L. Austin in a book titled \"Sense and Sensibilia\".\n\nTalk of sense data has been replaced by talk of representational perception in a broader sense, and scientific realists typically assume representational perception. But the assumption is philosophical, and arguably little prevents scientific realists from assuming direct perception, as in direct or \"naïve\" realism. In a blog-post on \"Naive realism and color realism\" Putnam sums up with the following words: \n\"\"... Being an apple is not a natural kind in physics, but it is in biology, recall. Being complex and of no interest to fundamental physics isn't a failure to be \"real\". I think green is as real as applehood.\".\"\n\nSimon Blackburn has argued that whatever positions they may take in books, articles or lectures, naive realism is the view of \"philosophers when they are off-duty.\"\n\nIt is not uncommon to think of naïve realism as distinct from scientific realism, which states that the universe contains just those properties that feature in a scientific description of it, not properties like colour \"per se\" but merely objects that reflect certain wavelengths owing to their microscopic surface texture. This lack of supervenience of experience on the physical world has influenced many thinkers to reject naïve realism as a physical theory.\n\nOne should add, however, that naïve realism does not claim that reality is only what we see, hear, etc. Likewise, scientific realism does not claim that reality is only what can be described by fundamental physics. It follows that the relevant distinction to make is not between naïve and scientific realism but between \"direct and indirect realism\".\n\nThe direct realist claims that the experience of a sunset, for instance, is the real sunset that we directly experience. The indirect realist claims that our relation to reality is indirect, so the experience of a sunset is a subjective copy of what really is radiation as described by physics. But the direct realist does not deny that the sunset is radiation; the experience has a hierarchical structure, and the radiation is part of what amounts to the direct experience.\n\nAn example of a scientific realist is John Locke, who held the world only contains the primary qualities that feature in a corpuscularian scientific account of the world (see corpuscular theory), and that other properties were entirely subjective, depending for their existence upon some perceiver who can observe the objects.\"\n\nThe modern philosopher of science Howard Sankey argues for a form of scientific realism which has commonsense realism as one of its foundations.\n\nRealism in physics refers to the fact that physical systems must have definite properties when measured or observed. Physics until the 19th century was implicitly and sometimes explicitly based on philosophical realism.\n\nScientific realism in classical physics has remained compatible with the naïve realism of everyday thinking on the whole, but there is no consistent way to visualize the world underlying quantum theory in terms of ideas of the everyday world. \"The general conclusion is that in quantum theory naïve realism, although necessary at the level of observations, fails at the microscopic level.\" Experiments such as the Stern–Gerlach experiment and quantum phenomena such as complementarity lead quantum physicists to conclude that \"[w]e have no satisfactory reason for ascribing objective existence to physical quantities as distinguished from the numbers obtained when we make the measurements which we correlate with them. There is no real reason for supposing that a particle has at every moment a definite, but unknown, position which may be revealed by a measurement of the right kind ... On the contrary, we get into a maze of contradiction as soon as we inject into quantum mechanics such concepts as carried over from the language and philosophy of our ancestors ... It would be more exact if we spoke of 'making measurements' of this, that, or the other type instead of saying that we measure this, that, or the other 'physical quantity'.\" It is no longer possible to adhere to both the principle of locality (that distant objects cannot affect local objects), and counterfactual definiteness, a form of ontological realism implicit in classical physics. Some interpretations of quantum mechanics hold that a system lacks an actualized property until it is measured, which implies that quantum systems exhibit a non-local behavior. Bell's theorem proved that every quantum theory must either violate locality or counterfactual definiteness. This claim has given rise to a contentious debate of the interpretation of quantum mechanics. Although locality and 'realism,' in the sense of counterfactual definiteness, are jointly false, it is possible to retain one of them. The majority of working physicists discard counterfactual definiteness in favor of locality, since non-locality is held to be contrary to relativity. The implications of this stance are rarely discussed outside of the microscopic domain but the thought experiment of Schrödinger's cat illustrates the difficulties presented. As quantum mechanics is applied to larger and larger objects, even a one-ton bar, proposed to detect gravity waves, must be analysed quantum mechanically, while in cosmology a wave function for the whole universe is written to study the Big Bang. It is difficult to accept the quantum world as somehow not physically real, so \"Quantum mechanics forces us to abandon naïve realism\", though it can also be argued that the counterfactual definiteness 'realism' of physics is a much more specific notion than general philosophical realism.\n\n\"'[W]e have to give up the idea of realism to a far greater extent than most physicists believe today.' (Anton Zeilinger) ... By realism, he means the idea that objects have specific features and properties — that a ball is red, that a book contains the works of Shakespeare, or that an electron has a particular spin ... for objects governed by the laws of quantum mechanics, like photons and electrons, it may make no sense to think of them as having well-defined characteristics. Instead, what we see may depend on how we look.\"\n\n\"Virtual realism\" is closely related to the above theories.\n\nIn the research paper \"The reality of virtual reality\" it is proposed that, \"virtuality is itself a bona fide mode of reality, and that 'virtual reality' must be understood as 'things, agents and events that exist in cyberspace'. These proposals resolve the incoherences found in the ordinary uses of these terms... 'virtual reality', though based on recent information technology, does not refer to mere technological equipment or purely mental entities, or to some fake environment as opposed to the real world, but that it is an ontological mode of existence which leads to an expansion of our ordinary world.\"\n\n\"The emergence of teleoperation and virtual environments has greatly increased interest in \"synthetic experience\", a mode of experience made possible by both these newer technologies and earlier ones, such as telecommunication and sensory prosthetics... understanding synthetic experience must begin by recognizing the fallacy of naïve realism and with the recognition that the phenomenology of synthetic experience is continuous with that of ordinary experience.\"\n\nThe alleged necessity to recognize a \"fallacy of naïve realism\" seems, however, unwarranted. One should not be misled by the word \"naïve\", for a naïve realist normally understands what a picture is: that the depicted face on a photograph, for instance, is not the real face, and that the things seen on a computer-screen are symbols or electronic depictions, and so on. A majority of the population arguably subscribes to naïve common sense notions of reality, without a recognizable loss in capacity to interact in cyberspace.\n\n"}
{"id": "47880066", "url": "https://en.wikipedia.org/wiki?curid=47880066", "title": "Negative utilitarianism", "text": "Negative utilitarianism\n\nNegative utilitarianism is a version of the ethical theory utilitarianism that gives greater priority to reducing suffering (negative utility or 'disutility') than to increasing happiness (positive utility). This differs from classical utilitarianism, which does not claim that reducing suffering is intrinsically more important than increasing happiness. Both versions of utilitarianism hold that morally right and morally wrong actions depend solely on the consequences for overall well-being. 'Well-being' refers to the state of the individual. The term 'negative utilitarianism' is used by some authors to denote the theory that reducing negative well-being is the \"only\" thing that ultimately matters morally. Others distinguish between 'strong' and 'weak' versions of negative utilitarianism, where strong versions are \"only\" concerned with reducing negative well-being, and weak versions say that \"both\" positive and negative well-being matter but that negative well-being matters more.\n\nOther versions of negative utilitarianism differ in how much weight they give to negative well-being ('disutility') compared to positive well-being (positive utility), as well as the different conceptions of what well-being (utility) is. For example, negative preference utilitarianism says that the well-being in an outcome depends on frustrated preferences. Negative hedonistic utilitarianism thinks of well-being in terms of pleasant and unpleasant experiences. There are many other variations on how negative utilitarianism can be specified.\n\nThe term \"negative utilitarianism\" was introduced by R. Ninian Smart in 1958 in his reply to Karl Popper's \"The Open Society and Its Enemies\". Smart also presented the most famous argument against negative utilitarianism: that negative utilitarianism would entail that a ruler who is able to instantly and painlessly destroy the human race would have a duty to do so. Furthermore, every human being would have a moral responsibility to commit suicide, thereby preventing future suffering. Many authors have endorsed versions of this argument, and some have presented counterarguments against it.\n\nThe term ‘negative utilitarianism’ was introduced by R. N. Smart in his 1958 reply to Karl Popper's book \"The Open Society and Its Enemies\", published in 1945. In the book, Popper emphasizes the importance of preventing suffering in public policy. The ideas in negative utilitarianism have similarities with ancient traditions such as Jainism and Buddhism. Ancient Greek philosopher Hegesias of Cyrene has been said to be “one of the earliest exponents of NU [Negative Utilitarianism].” In more recent times, ideas similar to negative utilitarianism can be found in the works of 19th century psychologist Edmund Gurney who wrote:\n\nLike other kinds of utilitarianism, negative utilitarianism can take many forms depending on what specific claims are taken to constitute the theory. For example, negative preference utilitarianism says that the utility of an outcome depends on frustrated and satisfied preferences. Negative hedonistic utilitarianism thinks of utility in terms of hedonic mental states such as suffering and unpleasantness. Versions of (negative) utilitarianism can also differ based on whether the \"actual\" or \"expected\" consequences matter, and whether the aim is stated in terms of the \"average\" outcome among individuals or the \"total\" net utility (or lack of disutility) among them. Negative utilitarianism can aim either to \"optimize\" the value of the outcome or it can be a \"satisficing\" negative utilitarianism, according to which an action ought to be taken if and only if the outcome would be \"sufficiently\" valuable (or have sufficiently low disvalue). A key way in which negative utilitarianisms can differ from one another is with respect to how much weight they give to negative well-being (disutility) compared to positive well-being (positive utility). This is a key area of variation because the key difference between negative utilitarianism and non-negative kinds of utilitarianism is that negative utilitarianism gives more weight to negative well-being.\n\nPhilosophers Gustaf Arrhenius and Krister Bykvist develop a taxonomy of negative utilitarian views based on how the views weigh disutility against positive utility. In total, they distinguish among 16 kinds of negative utilitarianism. They first distinguish between \"strong negativism\" and \"weak negativism\". Strong negativism \"give all weight to disutility\" and weak negativism \"give some weight to positive utility, but more weight to disutility.\" The most commonly discussed subtypes are probably two versions of weak negative utilitarianism called 'lexical' and 'lexical threshold' negative utilitarianism. According to 'lexical' negative utilitarianism, positive utility gets weight only when outcomes are equal with respect to disutility. That is, positive utility functions as a tiebreaker in that it determines which outcome is better (or less bad) when the outcomes considered have equal disutility. 'Lexical threshold' negative utilitarianism says that there is some disutility, for instance some extreme suffering, such that no positive utility can counterbalance it. 'Consent-based' negative utilitarianism is a specification of lexical threshold negative utilitarianism, which specifies where the threshold should be located. It says that if an individual is suffering and would at that moment not \"agree to continue the suffering in order to obtain something else in the future\" then the suffering cannot be outweighed by any happiness.\n\nThomas Metzinger proposes the 'principle of negative utilitarianism,' which is the broad idea that suffering should be minimized when possible. Mario Bunge writes about negative utilitarianism in his \"Treatise on Basic Philosophy\" but in a different sense than most others. In Bunge's sense, negative utilitarianism is about not harming. In contrast, most other discussion of negative utilitarianism takes it to imply a duty both not to harm and to help (at least in the sense of reducing negative well-being).\n\nIn the 1958 article where R. N. Smart introduced the term ‘negative utilitarianism’ he argued against it, stating that negative utilitarianism would entail that a ruler who is able to instantly and painlessly destroy the human race, \"a benevolent world-exploder,\" would have a duty to do so. This is the most famous argument against negative utilitarianism, and it is directed against sufficiently strong versions of negative utilitarianism. Many authors have endorsed this argument, and some have presented counterarguments against it. Below are replies to this argument that have been presented and discussed.\n\nOne possible reply to this argument is that only a naive interpretation of negative utilitarianism would endorse world destruction. The conclusion can be mitigated by pointing out the importance of cooperation between different value systems. There are good consequentialist reasons why one should be cooperative towards other value systems and it is particularly important to avoid doing something harmful to other value systems. The destruction of the world would strongly violate many other value systems and endorsing it would therefore be uncooperative. Since there are many ways to reduce suffering which do not infringe on other value systems, it makes sense for negative utilitarians to focus on these options. In an extended interpretation of negative utilitarianism, cooperation with other value systems is considered and the conclusion is that it is better to reduce suffering without violating other value systems.\n\nAnother reply to the benevolent world-exploder argument is that it does not distinguish between eliminating and reducing negative well-being, and that negative utilitarianism should plausibly be formulated in terms of reducing and not eliminating. A counterargument to that reply is that elimination is a form of reduction, similar to how zero is a number.\n\nSeveral philosophers have argued that to try to destroy the world (or to kill many people) would be counterproductive from a negative utilitarian perspective. One such argument is provided by David Pearce, who says that \"planning and implementing the extinction of all sentient life couldn't be undertaken painlessly. Even contemplating such an enterprise would provoke distress. Thus a negative utilitarian is not compelled to argue for the apocalyptic solution.\" Instead, Pearce advocates the use of biotechnology to phase out the biology of suffering throughout the living world, and he says that \"life-long happiness can be genetically pre-programmed.\" A similar reply to the similar claim that negative utilitarianism would imply that we should kill off the miserable and needy is that we rarely face policy choices and that \"anyway there are excellent utilitarian reasons for avoiding such a policy, since people would find out about it and become even more miserable and fearful.\" The Negative Utilitarianism FAQ's answer to question \"3.2 Should NUs try to increase extinction risk?\" begins with \"No, that would very bad even by NU standards.\"\n\nSome replies to the benevolent world exploder-argument take the form that even if the world were destroyed, that would or might be bad from a negative utilitarian perspective. One such reply provided by John W. N. Watkins is that even if life were destroyed, life could evolve again, perhaps in a worse way. So the world-exploder would need to destroy the possibility of life, but that is in principle beyond human power. To this, J. J. C. Smart replies,\n\nAnother related reply to the world-exploder argument is that getting killed would be a great evil. Erich Kadlec defends negative utilitarianism and replies to the benevolent world-exploder argument (in part) as follows: \"He [R. N. Smart] also dispenses with the generally known fact that all people (with a few exceptions in extreme situations) like to live and would consider being killed not a benefit but as the greatest evil done to them.\"\n\nNegative preference utilitarianism has a preferentialist conception of well-being. That is, it is bad for an individual to get his aversions fulfilled (or preferences frustrated), and depending on version of negative utilitarianism, it may also be good for him to get his preferences satisfied. A negative utilitarian with such a conception of well-being, or whose conception of well-being includes such a preferentialist component, could reply to the benevolent world-exploder argument by saying that the explosion would be bad because it would fulfill many individuals' aversions. Arrhenius and Bykvist provide two criticisms of this reply. First, it could be claimed that frustrated preferences require that someone exists who has the frustrated preference. But if everyone is dead there are no preferences and hence no badness. Second, even if a world-explosion would involve frustrated preferences that would be bad from a negative preference utilitarian perspective, such a negative utilitarian should still favor it as the lesser of two evils compared to all the frustrated preferences that would likely exist if the world continued to exist.\n\nThe Negative Utilitarianism FAQ suggests two replies to Arrhenius and Bykvist's first type of criticism (the criticism that if no one exists anymore then there are no frustrated preferences anymore): The first reply is that past preferences count, even if the individual who held them no longer exists. The second is that \"instead of counting past preferences, one could look at the matter in terms of life-goals. The earlier the death of a person who wants to go on living, the more unfulfilled her life-goal.\" The Negative Utilitarianism FAQ also replies to Arrhenius and Bykvist's second type of criticism. The reply is (in part) that the criticism relies on the empirical premise that there would be more frustrated preferences in the future if the world continued to exist than if the world was destroyed. But that negative preference utilitarianism would say that extinction would be better (in theory), assuming that premise, should not count substantially against the theory, because for any view on population ethics that assigns disvalue to something, one can imagine future scenarios such that extinction would be better according to the given view.\n\nA part of Clark Wolf's response to the benevolent world-exploder objection is that negative utilitarianism can be combined with a theory of rights. He says,\n\nNegative utilitarianism can be combined, in particular, with Rawls' theory of justice. Rawls knew Popper’s normative claims and may have been influenced by his concern for the worst-off.\n\nToby Ord provides a critique of negative utilitarianism in his essay \"Why I'm Not a Negative Utilitarian,\" to which David Pearce and Bruno Contestabile have replied. Other critical views of negative utilitarianism are provided by Thaddeus Metz, Christopher Belshaw, and Ingmar Persson. On the other hand, Joseph Mendola develops a modification of utilitarianism, and he says that his principle\n\n\n"}
{"id": "387403", "url": "https://en.wikipedia.org/wiki?curid=387403", "title": "Nonsense", "text": "Nonsense\n\nNonsense is a communication, via speech, writing, or any other symbolic system, that lacks any coherent meaning. Sometimes in ordinary usage, nonsense is synonymous with absurdity or the ridiculous. Many poets, novelists and songwriters have used nonsense in their works, often creating entire works using it for reasons ranging from pure comic amusement or satire, to illustrating a point about language or reasoning. In the philosophy of language and philosophy of science, nonsense is distinguished from sense or meaningfulness, and attempts have been made to come up with a coherent and consistent method of distinguishing sense from nonsense. It is also an important field of study in cryptography regarding separating a signal from noise.\n\nThe phrase \"Colorless green ideas sleep furiously\" was coined by Noam Chomsky as an example of nonsense. However, this can easily be confused with poetic symbolism. The individual \"words\" make sense and are arranged according to proper grammatical rules, yet the result is nonsense. The inspiration for this attempt at creating verbal nonsense came from the idea of contradiction and seemingly irrelevant and/or incompatible characteristics, which conspire to make the phrase meaningless, but are open to interpretation. The phrase \"the square root of Tuesday\" (not a similar example; the lemondrop sunshine is more comparable) operates on the latter principle. This principle is behind the inscrutability of the \"kōan\" \"What is the sound of one hand clapping?\", where one hand would presumably be insufficient for clapping without the intervention of another.\n\nJames Joyce’s final novel \"Finnegans Wake\" also uses nonsense: full of portmanteau and strong words, it \"appears\" to be pregnant with multiple layers of meaning, but in many passages it is difficult to say whether any one human’s interpretation of a text could be the intended or unintended one.\n\n\"Jabberwocky\", a poem (of nonsense verse) found in \"Through the Looking-Glass, and What Alice Found There\" by Lewis Carroll (1871), is a nonsense poem written in the English language. The word \"jabberwocky\" is also occasionally used as a synonym of nonsense.\n\nNonsense verse is the verse form of literary nonsense, a genre that can manifest in many other ways. Its best-known exponent is Edward Lear, author of \"The Owl and the Pussycat\" and hundreds of limericks.\n\nNonsense verse is part of a long line of tradition predating Lear: the nursery rhyme \"Hey Diddle Diddle\" could also be termed a nonsense verse. There are also some works which \"appear\" to be nonsense verse, but actually are not, such as the popular 1940s song Mairzy Doats.\n\nLewis Carroll, seeking a nonsense riddle, once posed the question \"How is a raven like a writing desk?\". Someone answered him, \"Because Poe wrote on both\". However, there are other possible answers (e.g. \"both have inky quills\").\n\nLines of nonsense frequently figure in the refrains of folksongs, where nonsense riddles and knock-knock jokes are often encountered.\n\nThe first verse of \"Jabberwocky\" by Lewis Carroll;\nThe first four lines of \"On the Ning Nang Nong\" by Spike Milligan;\nThe first verse of \"Spirk Troll-Derisive\" by James Whitcomb Riley;\nThe first four lines of \"The Mayor of Scuttleton\" by Mary Mapes Dodge;\n\"Oh Freddled Gruntbuggly\" by Prostetnic Vogon Jeltz; a creation of Douglas Adams\n\nIn the philosophy of language and the philosophy of science, nonsense refers to a lack of sense or meaning. Different technical definitions of meaning delineate sense from nonsense.\n\nIn Ludwig Wittgenstein's writings, the word \"nonsense\" carries a special technical meaning which differs significantly from the normal use of the word. In this sense, \"nonsense\" does not refer to meaningless gibberish, but rather to the lack of sense in the context of sense and reference. In this context, logical tautologies, and purely mathematical propositions may be regarded as \"nonsense\". For example, \"1+1=2\" is a nonsensical proposition. Wittgenstein wrote in Tractatus Logico Philosophicus that some of the propositions contained in his own book should be regarded as nonsense. Used in this way, \"nonsense\" does not necessarily carry negative connotations.\n\nStarting from Wittgenstein, but through an original perspective, the Italian philosopher Leonardo Vittorio Arena, in his book \"Nonsense as the meaning\", highlights this positive meaning of nonsense to undermine every philosophical conception which does not take note of the absolute lack of meaning of the world and life. Nonsense implies the destruction of all views or opinions, on the wake of the Indian Buddhist philosopher Nagarjuna. In the name of nonsense, it is finally refused the conception of duality and the Aristotelian formal logic.\n\nThe problem of distinguishing sense from nonsense is important in cryptography and other intelligence fields. For example, they need to distinguish signal from noise. Cryptanalysts have devised algorithms to determine whether a given text is in fact nonsense or not. These algorithms typically analyze the presence of repetitions and redundancy in a text; in meaningful texts, certain frequently used words recur, for example, \"the\", \"is\" and \"and\" in a text in the English language. A random scattering of letters, punctuation marks and spaces do not exhibit these regularities. Zipf's law attempts to state this analysis mathematically. By contrast, cryptographers typically seek to make their cipher texts resemble random distributions, to avoid telltale repetitions and patterns which may give an opening for cryptanalysis.\n\nIt is harder for cryptographers to deal with the presence or absence of meaning in a text in which the level of redundancy and repetition is \"higher\" than found in natural languages (for example, in the mysterious text of the Voynich manuscript).\n\nScientists have attempted to teach machines to produce nonsense. The Markov chain technique is one method which has been used to generate texts by algorithm and randomizing techniques that seem meaningful. Another method is sometimes called the \"Mad Libs\" method: it involves creating templates for various sentence structures and filling in the blanks with noun phrases or verb phrases; these phrase-generation procedures can be looped to add recursion, giving the output the appearance of greater complexity and sophistication. Racter was a computer program which generated nonsense texts by this method; however, Racter’s book, \"The Policeman’s Beard is Half Constructed\", proved to have been the product of heavy human editing of the program's output.\n\n\n\n"}
{"id": "6195361", "url": "https://en.wikipedia.org/wiki?curid=6195361", "title": "Overqualification", "text": "Overqualification\n\nOverqualification is the state of being skilled or educated beyond what is necessary for a job. There can often be high costs for companies associated with training employees. This could be a problem for professionals applying for a job where they significantly exceed the job requirements because potential employers may feel they are using the position as a stepping stone.\n\nIn some societies, overqualification has become increasingly common as the proportion of college graduates in a population grows faster than the proportion of jobs in an economy which actually require college-level skills.\n\nThe concept of overqualification is often a euphemism used by employers when they do not want to reveal their true reasons for not hiring an applicant. The term \"overqualified\" can mask age discrimination, but it can also mask legitimate concerns of an employer, such as uncertainty of an applicant's ability to do the job, or concerns that they only want a job on a temporary basis, while they seek another more desirable position. Being overqualified also often means that a person was asking for too high a salary. \"Overqualified\" can also be used to describe a resistance to new technologies, or a pompous approach.\n\nIn the definition above, which states that an overqualified person may take a job to gain knowledge and leave the company, this could also apply to all other employees of the same company. The term overqualified, in any definition, should be considered as a subjective term developed by the person doing the evaluation of the applicant based upon their point of view which may in itself be biased. There comes a time in a person's life, when a choice is made to reduce the level of responsibility and one could consider the perceived overqualification as \"added value\" to the company when the applicant is willing to take a lower-level position, accompanied by a lower salary. When the decision is not based upon factual or unbiased factors, discrimination has occurred.\n\nIn the United States, the term \"overqualified\" has been found by the courts to sometimes be used as a \"code word for too old\" (i.e., age discrimination) in the hiring process.\n\nThe governmental employing institution may have written or unwritten upper qualification limits for a particular position. These limits protect less qualified people like newly graduated students, allowing them to find a job as well. For instance, in countries like Germany or Switzerland, a paid position of a PhD student may normally not be given for an applicant who already has a PhD degree.\n\nAlso, a short but successful career may be preferred over longer (so more various different experience) but overall less successful career.\n\nNoluthando Crockett-Ntonga recommends that job applicants address potential concerns such as salary requirements in a cover letter and interview before the employer makes any comments about overqualification. Barbara Moses advises applicants who are described as being overqualified to emphasize their willingness to mentor younger co-workers, and to focus on what attracts them about the position they are applying to rather than emphasizing their ambition or desire to be challenged. Being overqualified can be an asset for employers, especially when the breadth of one's experience enables them to take on additional responsibilities in ways that benefit the employer.\n\nThe PhD can reflect overspecialization that manifests itself as a lack of perspective; for example, a PhD might not adequately prepare one for careers in development, manufacturing, or technical management.\n\nIn the corporate world, some PhD graduates have been criticized as being unable to turn theories into useful strategies and being unable to work on a team, although PhDs are seen as desirable and even essential in many positions, such as supervisory roles in research, especially PhDs in biomedical sciences.\n\nEven in some college jobs, people can associate negative factors with the PhD, including a lack of focus on teaching, overspecialization, and an undesirable set of professional priorities, often focusing on self-promotion. These forces have led both to an increase in some educational institutions hiring candidates without PhDs as well as a focus on the development of other doctoral degrees, such as the D.A. or Doctor of Arts.\n\nSome employers have reservations about hiring people with PhDs in full-time, entry-level positions but are eager to hire them in temporary positions.\n\nSome argue that this reservation is rather a reaction associated with job insecurity, especially in situations where most of the company leaders hold lower qualifications than the PhD; as part of the wide phenomenon of credential creep.\n\n"}
{"id": "34118956", "url": "https://en.wikipedia.org/wiki?curid=34118956", "title": "Perception of infrasound", "text": "Perception of infrasound\n\nInfrasound is sound at frequencies lower than the low frequency end of human hearing threshold at 20 Hz. It is known, however, that humans can perceive sounds below this frequency at very high pressure levels. Infrasound can come from many natural as well as man-made sources, including weather patterns, topographic features, ocean wave activity, thunderstorms, geomagnetic storms, earthquakes, jet streams, mountain ranges, and rocket launchings. Infrasounds are also present in the vocalizations of some animals. Low frequency sounds can travel for long distances with very little attenuation and can be detected hundreds of miles away from their sources.\n\nThe production and perception of infrasound has been observed in multiple mammals, including whale, elephant, giraffe, hippopotamus, and rhinoceros. For most of these animals, observations are preliminary and their sensitivity to infrasound has not been quantified. If an animal produces a low frequency sound, and uses it in communication, it suggests the animal might also be sensitive to infrasound.\n\nElephants are the terrestrial animal in which the production of infrasonic calls was first noted by M. Krishnan, later discovered by Katy Payne. The use of low frequency sounds to communicate over long distances may explain certain elephant behaviors that have previously puzzled observers. Elephant groups that are separated by several kilometers have been observed to travel in parallel or to change the direction simultaneously and move directly towards each other in order to meet. The time of estrus for females is asynchronous, lasts only for a few days, and occurs only every several years. Nevertheless, males, which usually wander apart from female groups, rapidly gather from many directions to compete for a receptive female. Since infrasound can travel for very long distances, it has been suggested that calls in the infrasonic range might be important for long distance communication for such coordinated behaviors among separated elephants.\n\nRecordings and playback experiments support that elephants use the infrasonic components of their calls for communication. Infrasonic vocalizations have been recorded from captive elephants in many different situations. The structure of the calls varies greatly but most of them range in frequency from 14 to 24 Hz, with durations of 10–15 seconds. When the nearest elephant is 5 m from the microphone, the recorded sound pressure levels can be 85 to 90 dB SPL. Some of these calls are completely inaudible to humans, while others have audible components that are probably due to higher frequency harmonics of below 20 Hz fundamentals. Sometimes, vocalizations cause perceptible rumbles that are accompanied by a fluttering of the skin on the calling elephant’s forehead where the nasal passage enters the skull. This fluttering can also occur without causing any perceptible sound, suggesting the production of a purely infrasonic call. The mechanism of infrasonic call production in elephants has not been determined.\n\nPlayback experiments using prerecorded elephant vocalizations show that elephants can perceive infrasound and how they respond to these stimuli. In playback experiments, certain behaviors that occur commonly after vocalizations are scored before and after a call is played. These behaviors include lifting and stiffening of ears, vocalization, walking or running towards the concealed speaker, clustering in a tight group, and remaining motionless (\"freezing\"), with occasional scanning movements of the head. The occurrence of such behaviors consistently increases after the playing of a call, whether it is a full-bandwidth playback or a playback in which most of the energy above 25 Hz was filtered out. This filtering shows that the behaviorally significant information of the call is contained in the infrasonic range, and it also simulates the effect of frequency-dependent attenuation over distance as it might occur in the wild. Behavioral responses do not increase for pure tone stimuli that are similar to recorded infrasonic calls in frequency and intensity. This shows that the responses are specifically to signals that were meaningful to the elephants.\n\nThe use of prerecorded playbacks and behavioral scoring also shows that the infrasonic elephant calls are behaviorally significant over long distances. The degree of response behaviors performed by an elephant group, such as lifting of ears, walking towards the speakers, “freezing”, or scanning movements, was compared visually before and after the presentation of a stimulus, scoring a trial as a positive response if the amount of behaviors is greater after the stimulus. In one particular experiment performed on elephants living in the wild, the presentation of playbacks for 20–40 seconds from loudspeakers at distances of 1.2 km and 2 km caused a significant increase in response behaviors. Since the playbacks were done at half the amplitude at which they were recorded, it is estimated that these calls would be perceptible by elephants at distances of at least 4 km Even this may be an underestimate because animals do not respond every time they perceive a conspecific call, and they are probably less likely to respond to calls from further distances even if they do perceive them.\n\nThere are some confounding factors that might influence the results of this kind of experiment. Firstly, the animals might actually be more sensitive than the experiments would indicate owing to habituation of the animals to the playback stimuli after several trial repetitions. To avoid this, researchers present several different types of playbacks in random order. Another problem that might arise in interpreting field experiments done on groups of animals is that animals may be responding to signals from other elephants in the group rather than the playback stimulus. However, an assumption is made that at least one animal in the group did perceive and respond directly to the stimulus.\n\nThe auditory sensitivity thresholds have been measured behaviorally for one individual young female Indian elephant. The conditioning test for sensitivity requires the elephant to respond to a stimulus by pressing a button with its trunk, which results in a sugar water reward if the elephant correctly identified the appropriate stimulus occurrence. To determine auditory sensitivity thresholds, a certain frequency of sound is presented at various intensities to see at which intensity the stimulus ceases to evoke a response. The auditory sensitivity curve of this particular elephant began at 16 Hz with a threshold of 65 dB. A shallow slope decreased to the best response at 1 kHz with a threshold of 8 dB, followed by a steep threshold increase above 4 kHz. According to the 60 dB cut-off, the upper limit was 10.5 kHz with absolutely no detectable response at 14 kHz. The upper limit for humans is considered to be 18 kHz. The upper and lower limits of elephant hearing are the lowest measured for any animals aside from the pigeon. By contrast, the average best frequency for animal hearing is 9.8 kHz, the average upper limit is 55 kHz.\n\nThe ability to differentiate frequencies of two successive tones was also tested for this elephant using a similar conditioning paradigm. The elephant’s responses were somewhat erratic, which is typical for mammals in this test. Nevertheless, the ability to discriminate sounds was best at frequencies below 1 kHz particularly at measurements of 500 Hz and 250 Hz.\n\nTests of the ability to localize sounds also showed the significance of low frequency sound perception in elephants. Localization was tested by observing the successful orienting towards the left or the right source loudspeakers when they were positioned at different angles from the elephant’s head. The elephant could localize sounds best at a frequency below 1 kHz, with perfect identification of the left or right speaker at angles of 20 degrees or more, and chance level discriminations below 2 degrees. Sound localization ability was measured to be best at 125 Hz and 250 Hz, intermediate at 500 Hz, 1 kHz, and 2 kHz, and very poor at frequencies at 4 kHz and above. A possible reason for this is that elephants are very good at using interaural phase differences which are effective for localizing low frequency sounds, but not as good at using interaural intensity differences which are better for higher frequency sounds. Because of the elephant head size and the large distance between their ears, interaural difference cues become confused when wavelengths are shorter, explaining why sound localization was very poor at frequencies above 4 kHz. It was observed that the elephant spread the pinna of its ears only during the sound localization tasks, however the precise effect of this behavior is unknown.\n\nAlthough birds do not produce vocalizations in the infrasonic range, reactions to infrasonic stimuli have been observed in several species, such as the homing pigeon, the guinea fowl, and the Asian grouse. It is postulated that birds might use the detection of naturally occurring infrasound for long-range directional cues from distant landmarks, or for weather detection.\n\nInfrasound perception has been observed and quantified in the homing pigeon which has particularly good long distance navigation skills. The precise relevance of such signals for the pigeon is still unknown, but several uses for infrasound have been hypothesized, such as navigation and detection of air turbulences when flying and landing.\n\nIn experiments using heart-rate conditioning, Pigeons have been found to be able to detect sounds in the infrasonic range at frequencies as low as 0.5 Hz. For frequencies below 10 Hz, the pigeon threshold is at about 55 dB which is at least 50 dB more sensitive than humans. Pigeons are able to discriminate small frequency differences in sounds at between 1 Hz and 20 Hz, with sensitivity ranging from a 1% shift at 20 Hz to a 7% shift at 1 Hz. Sensitivities are measured through a heart-rate conditioning test. In this test, an anesthetized bird is presented with a single sound or a sequence of sounds, followed by an electric shock. The bird’s heart-rate will increase in anticipation of a shock. Therefore, a measure of the heart-rate can determine whether the bird is able to distinguish between stimuli that would be followed by a shock from stimuli that would not. Similar methods have also been used to determine the pigeon’s sensitivity to barometric pressure changes, polarized light, and UV light. These experiments were conducted in sound isolation chambers to avoid the influence of ambient noise. Infrasonic stimuli are hard to produce and are often transmitted through a filter that attenuates higher frequency components. Also, the tone burst stimuli used in these experiments were presented with stimulus onset and offsets ramped on and off gradually in order to prevent initial turn-on and turn-off transients.\n\nIn order to use infrasound for navigation, it is necessary to be able to localize the source of the sounds. The known mechanisms for sound localizations make use of the time difference cues at the two ears. However, infrasound has such long wavelengths that these mechanisms would not be effective for an animal the size of a pigeon. An alternative method that has been hypothesized is through the use of the Doppler shift. A Doppler shift occurs when there is relative motion between a sound source and a perceiver and slightly shifts the perceived frequency of the sound. When a flying bird is changing direction, the amplitude of the Doppler shift between it and an infrasonic source would change, enabling the bird to locate the source. This kind of mechanism would require the ability to detect very small changes in frequency. A pigeon typically flies at 20 km/hr, so a turn could cause up to a 12% modulation of an infrasonic stimulus. According to response measurements, pigeons are able to distinguish frequency changes of 1-7% in the infrasonic range, showing that the use of Doppler shifts for infrasound localization may be within the pigeon’s perceptive capabilities.\n\nIn early experiments with infrasound sensitivity in pigeons, surgical removal of the calumella, a bone that links the tympanic membrane to the inner ear, in each ear severely reduced the ability to respond to infrasound, increasing the sensitivity threshold by about 50 dB. Complete surgical removal of the entire cochlea, lagena, and calumellae completely abolishes any response to infrasound. This shows that the receptors for infrasonic stimuli may be located in the inner ear.\n\nNeural fibers that are sensitive to infrasonic stimuli have been identified in the pigeon and their characteristics have been studied. It turns out that, although these fibers also originate in the inner ear, they are quite different from normal acoustic fibers. \nInfrasound sensitive fibers have very high rates of spontaneous discharge, with a mean of 115imp/s, which is much higher than the spontaneous discharge of other auditory fibers. Recordings show that discharge rates do not increase in response to infrasound stimuli but are modulated at levels comparable to the behavioral thresholds. Modulation depth is dependent on stimulus frequency and intensity. The modulation is phase locked so that the discharge rate increases during one phase of the stimulus and decreases during the other, leaving the mean discharge rate constant. Such pulse-frequency modulation allows the stimulus analysis to be independent of the peripheral tuning of the basilar membrane or the hair cells, which is already poor at low auditory frequencies. Unlike other acoustic fibers, infrasonic fibers do not show any indication of being tuned to a particular characteristic frequency.\n\nBy injecting fibers that were identified to be sensitive to infrasound with HRP (Horseradish Peroxidase), the location and morphology of the stained fibers can be observed in sections under a microscope. Infrasound sensitive fibers are found to be simple bipolar cells in the auditory ganglion with a diameter of 1.6-2.2 µm at the axon and 0.9-1.2 µm at the dendrites. They originate in the apical end of the cochlea and they are located near fibers that transmit low frequency sounds in the acoustic range. Unlike the ordinary acoustic fibers which terminate on the neural limbus, the infrasonic ones terminate on cells on the free basilar membrane. Furthermore, infrasonic fibers terminate on 2-9 hair cells while normal acoustic fibers connect to only one. Such characteristics would make these fibers analogous to fibers connecting to the outer hair cells in mammals, except that mammalian outer hair cells are known to have efferent fibers only and no afferents. These observations suggest that the infrasound sensitive fibers are in a class separate from ordinary acoustic fibers.\n\n"}
{"id": "1242444", "url": "https://en.wikipedia.org/wiki?curid=1242444", "title": "Pointed set", "text": "Pointed set\n\nIn mathematics, a pointed set (also based set or rooted set) is an ordered pair formula_1 where formula_2 is a set and formula_3 is an element of formula_2 called the base point, also spelled basepoint.\n\nMaps between pointed sets formula_1 and formula_6 (called based maps, pointed maps, or point-preserving maps) are functions from formula_2 to formula_8 that map one basepoint to another, i.e. a map formula_9 such that formula_10. This is usually denoted\n\nPointed sets may be regarded as a rather simple algebraic structure. In the sense of universal algebra, they are structures with a single nullary operation which picks out the basepoint. i.e., An formula_1 may be regarded as a formula_13 where formula_14 is the one-point set. Pointed maps are the homomorphisms of these algebras.\n\nThe class of all pointed sets together with the class of all based maps form a category. In this category the pointed singleton set formula_15 is an initial object and a terminal object, i.e. a zero object. There is a faithful functor from pointed sets to usual sets, but it is not full and these categories are not equivalent. In particular, the empty set is not a pointed set, for it has no element that can be chosen as base point.\n\nThe category of pointed sets and based maps is equivalent to but not isomorphic with the category of sets and partial functions. One textbook notes that \"This formal completion of sets and partial maps by adding 'improper', 'infinite' elements was reinvented many times, in particular, in topology (one-point compactification) and in theoretical computer science.\"\n\nThe category of pointed sets and pointed maps is isomorphic to the co-slice category formula_16, where formula_17 is a singleton set. This coincides with the algebraic characterization, since the unique map formula_18 extends the commutative triangles defining arrows of the coslice category to form the commutative squares defining homomorphisms of the algebras.\n\nThe category of pointed sets and pointed maps has both products and co-products, but it is not a distributive category. It is also an example of a category where formula_19 is not isomorphic to formula_20.\n\nMany algebraic structures are pointed sets in a rather trivial way. For example, groups are pointed sets by choosing the identity element as the basepoint, so that group homomorphisms are point-preserving maps. This observation can be restated in category theoretic terms as the existence of a forgetful functor from groups to pointed sets.\n\nA pointed set may be seen as a pointed space under the discrete topology or as a vector space over the field with one element.\n\nAs \"rooted set\" the notion naturally appears in the study of antimatroids and transportation polytopes.\n\n\n"}
{"id": "5386990", "url": "https://en.wikipedia.org/wiki?curid=5386990", "title": "Political criticism", "text": "Political criticism\n\nPolitical criticism (also referred to as political commentary or political discussion) is criticism that is specific of or relevant to politics, including policies, politicians, political parties, and types of government.\n\nThere has been controversy over the relevance and importance of political criticism in civilizations, particularly democratic societies.\n\nAdvocates argue that political discussion creates and promotes the variety of opinions necessary for a true democracy. The American constitution is often pointed to as support for the belief, ensuring for all peoples under its administration such maxims as free speech.\n\nCritics of this philosophy argue that the general public (and, on a more individual basis, the \"Average Joe\") lacks the resources and capability to conceive opinions that are educated enough to be taken seriously. Thus, the abundance and fervent promotion of such opinions merely confuses and complicates political matters that, given an appropriate amount of factual education, are either easily understood or should be discussed only by those with sufficient intelligence to do the matter justice.\n\nThere are many methods used throughout history of promoting political opinions and with the development of new technologies new ways have materialized both in recent and ancient history. Some of the most common include the following:\n\nThroughout history one of the most influential methods (arguably the most influential method) of promoting political opinions has been literary. This peculiar pattern of books influencing the thinking of the masses, reinstated with such books as the Bible, Uncle Tom's Cabin, the Qur'an, \"The Diary of a Young Girl\", \"Nineteen Eighty-Four\" and many others, has been attributed to many of the characteristics of writing. While it has been proven that a well-written book can indeed appeal to one's intellect with reasoning and ideas sprung from common sense, the drive of literature and writing is most commonly considered to be derivative of the emotional impact of the text, guiding people to think a certain way by making them feel a certain way. This often has great political consequences, most particularly when the subject of the emotional reaction is a plea for moral justice, as can be seen (to use America as an example once more) in the aftermath of the publication of Uncle Tom's Cabin concerning American views about slavery. It is from this correlation between books and politics that the phrase, \"the pen is mightier than the sword\", derives.\n\nSince their development in this past century, both television and films have often had far-reaching political effects throughout the world. Influential films and television events include Countdown with Keith Olbermann, Citizen Kane, the works of Walt Disney, and such political satire as that found in The Simpsons. Like literature (see above), a movie or television event has the capacity to have emotional impact on its viewers, making it an invaluable tool for politics. This attribute is noticed and used frequently by politicians and ordinary citizens alike; political propaganda on the screen now has an effect on everything from a nation's outlook on its economy to the smallest of elections and political decisions.\n\nPolitical cartoons (also known as \"editorial cartoons\") are infamous for their ability to promote political views through satirical means. With cartoons the fight for the minds of the public is not a verbal but a visual one and leaving strong impressions with the use of powerful symbols or metaphors in order to communicate the artist's messages.\n\nSince many people know only a very little about their civilization and its present state except for what is told to them by the media, the potential for influence is extremely high with the publication of newspapers, news broadcasts, etc. Any bias in these mediums alters people's impressions of current events into different understandings and opinions than those that may have been chosen with more accurate information.\n\nWith the recent invention of the Internet, political criticism has been greatly extended to anyone with a connection to the World Wide Web. In a few decades already this has revolutionized politics to the extent that the public now has a virtually unlimited education quite literally at its fingertips, allowing people to choose with less work what aspects of current politics they wish to research.\n\nThe many means of exchanging ideas, including blogs and internet forums, has extended the political debate to anyone that cares to contribute. This ability and speed that which ideas can flow has literally changed the way that political parties stay connected to constituents.\n\n"}
{"id": "172851", "url": "https://en.wikipedia.org/wiki?curid=172851", "title": "Radiosonde", "text": "Radiosonde\n\nA radiosonde is a battery-powered telemetry instrument carried into the atmosphere usually by a weather balloon that measures various atmospheric parameters and transmits them by radio to a ground receiver. Modern radiosondes measure or calculate the following variables: altitude, pressure, temperature, relative humidity, wind (both wind speed and wind direction), cosmic ray readings at high altitude and geographical position (latitude/longitude). Radiosondes measuring ozone concentration are known as ozonesondes.\n\nRadiosondes may operate at a radio frequency of 403 MHz or 1680 MHz. A radiosonde whose position is tracked as it ascends to give wind speed and direction information is called a rawinsonde (\"radar wind -sonde\"). Most radiosondes have radar reflectors and are technically rawinsondes. A radiosonde that is dropped from an airplane and falls, rather than being carried by a balloon is called a \"dropsonde\". Radiosondes are an essential source of meteorological data, and hundreds are launched all over the world daily.\n\nThe first flights of aerological instruments were done in the second half of the 19th century with kites and meteographs, a recording device measuring pressure and temperature that was recuperated after the experiment. This proved to be difficult because the kites were linked to the ground and were very difficult to manoeuvre in gusty conditions. Furthermore, the sounding was limited to low altitudes because of the link to the ground.\n\nGustave Hermite and Georges Besançon, from France, were the first in 1892 to use a balloon to fly the meteograph. In 1898, Léon Teisserenc de Bort organized at the \"Observatoire de Météorologie Dynamique de Trappes\" the first regular daily use of these balloons. Data from these launches showed that the temperature lowered with height up to a certain altitude, which varied with the season, and then stabilized above this altitude. De Bort's discovery of the tropopause and stratosphere was announced in 1902 at the French Academy of Sciences. Other researchers, like Richard Aßmann and William Henry Dines, were working at the same times with similar instruments.\n\nIn 1924, Colonel William Blaire in the U.S. Signal Corps did the first primitive experiments with weather measurements from balloon, making use of the temperature dependence of radio circuits. The first true radiosonde that sent precise encoded telemetry from weather sensors was invented in France by . Bureau coined the name \"radiosonde\" and flew the first instrument on January 7, 1929. Developed independently a year later, Pavel Molchanov flew a radiosonde on January 30, 1930. Molchanov's design became a popular standard because of its simplicity and because it converted sensor readings to Morse code, making it easy to use without special equipment or training.\n\nWorking with a modified Molchanov sonde, Sergey Vernov was the first to use radiosondes to perform cosmic ray readings at high altitude. On April 1, 1935, he took measurements up to using a pair of Geiger counters in an anti-coincidence circuit to avoid counting secondary ray showers. This became an important technique in the field, and Vernov flew his radiosondes on land and sea over the next few years, measuring the radiation's latitude dependence caused by the Earth's magnetic field.\n\nIn 1936, the U.S. Navy assigned the U.S. Bureau of Standards (NBS) to develop an official radiosonde for the Navy to use. The NBS gave the project to Harry Diamond, who had previously worked on radio navigation and invented a blind landing system for airplanes. The organization led by Diamond eventually (in 1992) became a part of the U.S. Army Research Laboratory. In 1937, Diamond, along with his associates Francis Dunmore and Wilbur Hinmann, Jr., created a radiosonde that employed audio-frequency subcarrier modulation with the help of a resistance-capacity relaxation oscillator. In addition, this NBS radiosonde was capable of measuring temperature and humidity at higher altitudes than conventional radiosondes at the time due to the use of electric sensors.\n\nIn 1938, Diamond developed the first ground receiver for the radiosonde, which prompted the first service use of the NBS radiosondes in the Navy. Then in 1939, Diamond and his colleagues developed a ground-based radiosonde called the “remote weather station,” which allowed them to automatically collect weather data in remote and inhospitable locations. By 1940, the NBS radiosonde system included a pressure drive, which measured temperature and humidity as functions of pressure. It also gathered data on cloud thickness and light intensity in the atmosphere. Due to this and other improvements in cost (about $25), weight (> 1 kilogram), and accuracy, hundreds of thousands of NBS-style radiosondes were produced nationwide for research purposes, and the apparatus was officially adopted by the U.S. Weather Bureau.\n\nDiamond was given the Washington Academy of Sciences Engineering Award in 1940 and the IRE Fellow Award (which was later renamed the Harry Diamond Memorial Award) in 1943 for his contributions to radio-meteorology.\n\nThe expansion of economically important government weather forecasting services during the 1930s and their increasing need for data motivated many nations to begin regular radiosonde observation programs\n\nIn 1985, as part of the Soviet Union's Vega program, the two Venus probes, Vega 1 and Vega 2, each dropped a radiosonde into the atmosphere of Venus. The sondes were tracked for two days.\n\nAlthough modern remote sensing by satellites, aircraft and ground sensors is an increasing source of atmospheric data, none of these systems can match the vertical resolution ( or less) and altitude coverage () of radiosonde observations, so they remain essential to modern meteorology.\n\nAlthough hundreds of radiosondes are launched worldwide each day year-round, fatalities attributed to radiosondes are rare. The first know example was the electrocution of a lineman in the United States who was attempting to free a radiosonde from high-tension power lines in 1943. In 1970 an Antonov 24 operating Aeroflot Flight 1661 suffered a loss of control after striking a radiosonde in flight resulting in the death of all 45 people on board.\n\nA rubber or latex balloon filled with either helium or hydrogen lifts the device up through the atmosphere. The maximum altitude to which the balloon ascends is determined by the diameter and thickness of the balloon. Balloon sizes can range from . As the balloon ascends through the atmosphere, the pressure decreases, causing the balloon to expand. Eventually, the balloon will expand to the extent that its skin will break, terminating the ascent. An balloon will burst at about . After bursting, a small parachute on the radiosonde's support line carries it to Earth. A typical radiosonde flight lasts 60 to 90 minutes. One radiosonde from Clark Air Base, Philippines, reached an altitude of .\n\nThe modern radiosonde communicates via radio with a computer that stores all the variables in real time. The first radiosondes were observed from the ground with a theodolite, and gave only a wind estimation by the position. With the advent of radar by the Signal Corps it was possible to track a radar target carried by the balloons with the SCR-658 radar. Modern radiosondes can use a variety of mechanisms for determining wind speed and direction, such as a radio direction finder or GPS. The weight of a radiosonde is typically .\n\nSometimes radiosondes are deployed by being dropped from an aircraft instead of being carried aloft by a balloon. Radiosondes deployed in this way are called dropsondes.\n\nWorldwide there are about 1,300 radiosonde launch sites. Most countries share data with the rest of the world through international agreements. Nearly all routine radiosonde launches occur 45 minutes before the official observation time of 0000 UTC and 1200 UTC, so as to provide an instantaneous snapshot of the atmosphere. This is especially important for numerical modeling. In the United States the National Weather Service is tasked with providing timely upper-air observations for use in weather forecasting, severe weather watches and warnings, and atmospheric research. The National Weather Service launches radiosondes from 92 stations in North America and the Pacific Islands twice daily. It also supports the operation of 10 radiosonde sites in the Caribbean.\n\nA list of U.S. operated land based launch sites can be found in Appendix C, U.S. Land-based Rawinsonde Stations of the Federal Meteorological Handbook #3, titled Rawinsonde and Pibal Observations, dated May 1997.\n\nRaw upper air data is routinely ingested by numerical models. Forecasters often view the data in a graphical format, plotted on thermodynamic diagrams such as Skew-T log-P diagrams, Tephigrams, and or Stüve diagrams, all useful for the interpretation of the atmosphere's vertical thermodynamics profile of temperature and moisture as well as kinematics of vertical wind profile.\n\nRadiosonde data is a crucially important component of numerical weather prediction. Because a sonde may drift several hundred kilometers during the 90- to 120-minute flight, there may be concern that this could introduce problems into the model initialization. However, this appears not to be so except perhaps locally in jet stream regions in the stratosphere.\n\nAccording to \"article 1.109\" of the International Telecommunication Union´s (ITU) ITU Radio Regulations (RR): \n\n"}
{"id": "36970467", "url": "https://en.wikipedia.org/wiki?curid=36970467", "title": "Simple Energy", "text": "Simple Energy\n\nSimple Energy is a privately held software-as-a-service (\"SaaS\") company.\n\nSimple Energy is headquartered in Boulder, Colorado.\n\nSimple Energy was founded in 2010 by longtime friends Yoav Lurie and Justin Segall, classmates at Duke University. The two had met on a backpacking trip in Pisgah National Forest in 2000.\n\nIn September 2011, then-US CTO Aneesh Chopra challenged the energy industry to model a Green Button, off the successful Blue Button, where energy providers would give energy users their consumption data in an easy to read and use format at the click of the button. In January 2012, Simple Energy became the first third-party application developer to implement the Green Button standard to deploy publicly available applications.\n\n\n"}
{"id": "41375756", "url": "https://en.wikipedia.org/wiki?curid=41375756", "title": "Social construction of disability", "text": "Social construction of disability\n\nThe social construction of disability is the idea that society and its institutions have the power to construct disability around social expectations of health. This idea argues that disability is construction based on several localized social expectations. For example, in the medieval period disability was constructed around a person's moral behavior. Disability was seen as divine punishment or a side effect of a moral failing being physically or biologically different was not enough to be considered disabled. Only until the European Enlightenment did society change its definition of disability to be more related to biology. However these biological definitions of health centered around what is considered to be healthy for most Western Europeans.\n\nAround the year 1970, in North America various groups including sociologists, disabled people, and disabled focused political groups began to pull away from the accepted medical lens of viewing disability. These groups began to discuss things like oppression, civil rights, and accessibility. This change in discourse resulted in conceptualizations of disability that was rooted in social constructions.\n\nCanada and the United States have operated under the premise that social assistance benefits should not exceed the amount of money that can be earned through labor in order to give citizens an incentive to search for and maintain employment. This has led to widespread poverty amongst disabled citizens. In the 1950s, disability pensions were established and included various forms of direct economic assistance; however, these amounts were set at exceedingly low monetary levels. Since the 1970s, both governments have viewed unemployed disabled citizens as excess labor due to continuous high rates of unemployment and have made minimal attempts to increase employment, which keeps disabled people at poverty- level incomes due to the ‘incentive’ principle. Poverty is the most debilitating circumstance handicapped people face, resulting in the inability to afford proper medical, technological and other assistance necessary to participate in society.\n\nLaws have helped to recognize disability as a social construct rather than simply physical impairment. In 1776, the Continental Congress passed the first national law regarding wounded soldiers. Rather than stating attitudes towards disability, the act proposed that how the US viewed disability was closely linked to its views about the worth of the soldiers. This segment of the Act contains a few inferences which relate to disability:\n\n\"Whereas, in the course of the present war, some commissioned and non-commissioned officers of the army and navy, as also private soldiers, marines, and seamen, may lose a limb, or be otherwise so disabled as to prevent their serving in the army or navy, or getting their livelihood, and may stand in need of relief[.]\"\n\nAlthough this resolution related disability to “getting a livelihood,” it also implies through the recognition of an additional connection to serving in the military that disability is not just a failure to function properly but rather built systemically through the ability to function in certain settings. Also, this resolution states that disability is not only based upon physical capabilities and societal roles but social aspects as well.\n\nThe Americans With Disabilities Act (ADA) is a prime example of how laws have changed the societal roles of disabled people. The Act was signed by President Bush in July 1990. It prohibited the discrimination of disabled people by employers and also required that mass transportation, commercial buildings and public accommodations be accessible to disabled people. After the law was passed, Congress realized that many people with disabilities were secluded from society and could not live independently. Before the ADA was made into a law, disabled people could not land a job, enter a restaurant or store, or access the bus. A 1986 poll revealed that 66 percent of disabled citizens were jobless, in spite of the fact that many proclaimed they desired and were capable of employment. The same poll showed that 40 percent of handicapped people could not find a job due to the inaccessibility of transportation and public places. As a result of implementation of the ADA, 6 million private businesses and 80,000 state and local governments made their facilities available to the disabled. In July 1992, the provisions of the Act incorporating 43 million disabled individuals in to the workforce were set into motion. Many business owners realized that the changes made to their establishment were of little financial consequence, typically under $500. The conditions of disabled people before and after this law was passed are examples of how society constructs disability through discrimination and physical barriers. By eliminating these obstacles, the disabled are more integrated into society.\n\nIn the United States, according to the Social Security Administration, to be considered \"disabled,\" a claimant must provide medical evidence displaying proof of and severity of the impairment(s). Medical evidence comes from sources that have treated or evaluated the claimant for his/her impairment(s). Acceptable medical sources include medical professionals including licensed physicians, licensed or certified psychologists, licensed optometrists (only for purposes of establishing visual disorders), licensed podiatrists (only for purposes of establishing impairments of the foot and/or ankle), and qualified speech-language pathologists (only for purposes of establishing speech or language impairments). Qualified speech-language pathologists must be licensed by the State education in the State in which he/she practices, or hold a Certificate of clinical Competence from the American Speech-Language-Hearing Association.\n\nApplying for disability (and the assessment of) can take place in person at the Social Security field office, by telephone, by mail, or by filing online. The application process involves supplying a description of the impairment, treatment sources, and other information. A field office then verifies non-medical eligibility requirement (including age, employment, marital status, or Social Security coverage information) and sends the case to the state agency (Disability Determination Services, or DDS) for evaluation (DDS are state agencies that develop medical evidence and deciding whether or not a claimant - the person requesting disability benefits - is disabled or blind under the law).\n\nSince the invention of the television in the early 1900s, this medium has held a pervasive influence on our outlook on many aspects of society, disability being one of them. One example is how the 2000 Paralympics versus the Olympics were televised. The 2000 Sydney Paralympic Games, one of the biggest in history, was barely acknowledged by mainstream media prior to the event. The Sydney Paralympic organizers worked extensively to try to solicit coverage of the Games. For more than two years, they negotiated with Channel 7 to broadcast the competitions. Channel 7 proposed that if the Paralympics paid them $3 million in case of lack of advertising revenue they would agree to broadcast the event. Eventually, the Australian Broadcasting Company (ABC) and Channel 7 announced they would be broadcasting the Games and Channel 7 would “complement” the coverage with a highlights package that ran daily on its pay-TV Channel. ABC also promised to broadcast at least 60 minutes of daily highlights. Later on, ABC finally agreed to air a live broadcast of the opening and closing ceremonies. The opening and closing ceremonies were actually quite popular amongst viewers, watched by 2.5 million; however the rest of the games proved to be quite the opposite. While the Olympics were covered live throughout the entire event, the Paralympics apparently was not important enough to deserve the same live coverage before the initial showing. By deeming the disabled to be less important than those without disabilities and by separating the Olympics and Paralympics, disability is socially constructed.\n\nOver the last several decades, technology has transformed networks, services, and communication, and promoted convergence of telecommunications, computer use, etc. Technology changes, or, the Digital Revolution, has changed how people work, learn, and interact. The digital revolution has directed people to use computers, cell phones, and other technologies more and more. However, many people who use such technology experience a form of disability. Even if it is not physically visible, those with, for example, cognitive impairments, hand tremors, vision impairments, have some form of disability that don't allow them to be able to fully access technology the way that those without a \"technological disability\" do.\n\nSpecial education \"...is a social construction whereby a society can label a group in order to isolate responsibility for that group's welfare and thereby protect normal sources of power. In education, society has created many social categories, including learning disabilities. Such social categories represent the structure of society. The success of special educations students who have learning disabilities seems to be measured in material terms; but if they don't have the opportunity to achieve this success, it is hard to turn to the American community for support, as so many people in this community condemn them for their differences.\n\nThus, education tries to restore the idea of a moral community, one in which the members question what constitutes a good life, to form a reconceptualization of education, where physical and mental conditions would be seen as part of a range of abilities, where different talents were distributed in different ways, and all talents would be recognized. All students would be included in the educational network instead of being set apart as special cases, and it would be acknowledged that all humans have special needs and no one is normal.\n\n\n"}
{"id": "22653593", "url": "https://en.wikipedia.org/wiki?curid=22653593", "title": "Special classes of semigroups", "text": "Special classes of semigroups\n\nIn mathematics, a semigroup is a nonempty set together with an associative binary operation. A special class of semigroups is a class of semigroups satisfying additional properties or conditions. Thus the class of commutative semigroups consists of all those semigroups in which the binary operation satisfies the commutativity property that \"ab\" = \"ba\" for all elements \"a\" and \"b\" in the semigroup.\nThe class of finite semigroups consists of those semigroups for which the underlying set has finite cardinality. Members of the class of Brandt semigroups are required to satisfy not just one condition but a set of additional properties. A large collection of special classes of semigroups have been defined though not all of them have been studied equally intensively.\n\nIn the algebraic theory of semigroups, in constructing special classes, attention is focused only on those properties, restrictions and conditions which can be expressed in terms of the binary operations in the semigroups and occasionally on the cardinality and similar properties of subsets of the underlying set. The underlying sets are not assumed to carry any other mathematical structures like order or topology.\n\nAs in any algebraic theory, one of the main problems of the theory of semigroups is the classification of all semigroups and a complete description of their structure. In the case of semigroups, since the binary operation is required to satisfy only the associativity property the problem of classification is considered extremely difficult. Descriptions of structures have been obtained for certain special classes of semigroups. For example, the structure of the sets of idempotents of regular semigroups is completely known. Structure descriptions are presented in terms of better known types of semigroups. The best known type of semigroup is the group.\n\nA (necessarily incomplete) list of various special classes of semigroups is presented below. To the extent possible the defining properties are formulated in terms of the binary operations in the semigroups. The references point to the locations from where the defining properties are sourced.\n\nIn describing the defining properties of the various special classes of semigroups, the following notational conventions are adopted.\n\nFor example, the definition \"xab\" = \"xba\" should be read as:\n\nThe third column states whether this set of semigroups forms a variety. And whether the set of finite semigroups of this special class forms a variety of finite semigroups. Note that if this set is a variety, its set of finite elements is automatically a variety of finite semigroups.\n"}
{"id": "28927", "url": "https://en.wikipedia.org/wiki?curid=28927", "title": "Stellar classification", "text": "Stellar classification\n\nIn astronomy, stellar classification is the classification of stars based on their spectral characteristics. Electromagnetic radiation from the star is analyzed by splitting it with a prism or diffraction grating into a spectrum exhibiting the rainbow of colors interspersed with spectral lines. Each line indicates a particular chemical element or molecule, with the line strength indicating the abundance of that element. The strengths of the different spectral lines vary mainly due to the temperature of the photosphere, although in some cases there are true abundance differences. The \"spectral class\" of a star is a short code primarily summarizing the ionization state, giving an objective measure of the photosphere's temperature.\n\nMost stars are currently classified under the Morgan-Keenan (MK) system using the letters \"O\", \"B\", \"A\", \"F\", \"G\", \"K\", and \"M\", a sequence from the hottest (\"O\" type) to the coolest (\"M\" type). Each letter class is then subdivided using a numeric digit with \"0\" being hottest and \"9\" being coolest (e.g. A8, A9, F0, and F1 form a sequence from hotter to cooler). The sequence has been expanded with classes for other stars and star-like objects that do not fit in the classical system, such as class \"D\" for white dwarfs and classes \"S\" and \"C\" for carbon stars.\n\nIn the MK system, a luminosity class is added to the spectral class using Roman numerals. This is based on the width of certain absorption lines in the star's spectrum, which vary with the density of the atmosphere and so distinguish giant stars from dwarfs. Luminosity class \"0\" or \"Ia+\" is used for \"hypergiants\", class \"I\" for \"supergiants\", class \"II\" for bright \"giants\", class \"III\" for regular \"giants\", class \"IV\" for \"sub-giants\", class \"V\" for \"main-sequence stars\", class \"sd\" (or \"VI\") for \"sub-dwarfs\", and class \"D\" (or \"VII\") for \"white dwarfs\". The full spectral class for the Sun is then G2V, indicating a main-sequence star with a temperature around 5,800 K.\n\nThe conventional color description takes into account only the peak of the stellar spectrum. In actuality, however, stars radiate in all parts of the spectrum. Because all spectral colors combined appear white, the actual apparent colors the human eye would observe are far lighter than the conventional color descriptions would suggest. This characteristic of 'lightness' indicates that the simplified assignment of colors within the spectrum can be misleading. Excluding color-contrast illusions in dim light, there are no green, indigo, or violet stars. Red dwarfs are a deep shade of orange, and brown dwarfs do not literally appear brown, but hypothetically would appear dim grey to a nearby observer.\n\nThe modern classification system is known as the \"Morgan–Keenan\" (MK) classification. Each star is assigned a spectral class from the older Harvard spectral classification and a luminosity class using Roman numerals as explained below, forming the star's spectral type.\n\nOther modern stellar classification systems, such as the UBV system, are based on color indexes—the measured differences in three or more color magnitudes. Those numbers are given labels such as \"U-V\" or \"B-V\", which represent the colors passed by two standard filters (e.g. \"U\"ltraviolet, \"B\"lue and \"V\"isual).\n\nThe \"Harvard system\" is a one-dimensional classification scheme by astronomer Annie Jump Cannon, who re-ordered and simplified a prior alphabetical system. Stars are grouped according to their spectral characteristics by single letters of the alphabet, optionally with numeric subdivisions. Main-sequence stars vary in surface temperature from approximately 2,000 to 50,000 K, whereas more-evolved stars can have temperatures above 100,000 K. Physically, the classes indicate the temperature of the star's atmosphere and are normally listed from hottest to coldest.\n\nThe spectral classes O through M, as well as other more specialized classes discussed later, are subdivided by Arabic numerals (0–9), where 0 denotes the hottest stars of a given class. For example, A0 denotes the hottest stars in class A and A9 denotes the coolest ones. Fractional numbers are allowed; for example, the star Mu Normae is classified as O9.7. The Sun is classified as G2.\n\nConventional color descriptions are traditional in astronomy, and represent colors relative to the mean color of an A class star, which is considered to be white. The apparent color descriptions are what the observer would see if trying to describe the stars under a dark sky without aid to the eye, or with binoculars. However, most stars in the sky, except the brightest ones, appear white or bluish white to the unaided eye because they are too dim for color vision to work. Red supergiants are cooler and redder than dwarfs of the same spectral type, and stars with particular spectral features such as carbon stars may be far redder than any black body.\n\nThe fact that the Harvard classification of a star indicated its surface or photospheric temperature (or more precisely, its effective temperature) was not fully understood until after its development, though by the time the first Hertzsprung–Russell diagram was formulated (by 1914), this was generally suspected to be true. In the 1920s, the Indian physicist Meghnad Saha derived a theory of ionization by extending well-known ideas in physical chemistry pertaining to the dissociation of molecules to the ionization of atoms. First he applied it to the solar chromosphere, then to stellar spectra.\n\nHarvard astronomer Cecilia Payne then demonstrated that the \"O-B-A-F-G-K-M\" spectral sequence is actually a sequence in temperature. Because the classification sequence predates our understanding that it is a temperature sequence, the placement of a spectrum into a given subtype, such as B3 or A7, depends upon (largely subjective) estimates of the strengths of absorption features in stellar spectra. As a result, these subtypes are not evenly divided into any sort of mathematically representable intervals.\n\nThe \"Yerkes spectral classification\", also called the \"MKK\" system from the authors' initials, is a system of stellar spectral classification introduced in 1943 by William Wilson Morgan, Philip C. Keenan, and Edith Kellman from Yerkes Observatory. This two-dimensional (temperature and luminosity) classification scheme is based on spectral lines sensitive to stellar temperature and surface gravity, which is related to luminosity (whilst the \"Harvard classification\" is based on just surface temperature). Later, in 1953, after some revisions of list of standard stars and classification criteria, the scheme was named the \"Morgan–Keenan classification\", or \"MK\", and this system remains in use.\n\nDenser stars with higher surface gravity exhibit greater pressure broadening of spectral lines. The gravity, and hence the pressure, on the surface of a giant star is much lower than for a dwarf star because the radius of the giant is much greater than a dwarf of similar mass. Therefore, differences in the spectrum can be interpreted as \"luminosity effects\" and a luminosity class can be assigned purely from examination of the spectrum.\n\nA number of different \"luminosity classes\" are distinguished, as listed in the table below.\n\nMarginal cases are allowed; for example, a star may be either a supergiant or a bright giant, or may be in between the subgiant and main-sequence classifications. \nIn these cases, two special symbols are used:\n\nFor example, a star classified as A3-4III/IV would be in between spectral types A3 and A4, while being either a giant star or a subgiant.\n\nSub-dwarf classes have also been used: VI for sub-dwarfs (stars slightly less luminous than the main sequence).\n\nNominal luminosity class VII (and sometimes higher numerals) is now rarely used for white dwarf or \"hot sub-dwarf\" classes, since the temperature-letters of the main sequence and giant stars no longer apply to white dwarfs.\n\nOccasionally, letters \"a\" and \"b\" are applied to luminosity classes other than supergiants; for example, a giant star slightly more luminous than typical may be given a luminosity class of IIIb.\n\nAn sample of extreme V stars with strong absorption in He II λ4686 spectral lines have been given the \"Vz\" designation. An example star is HD 93129 B.\n\nAdditional nomenclature, in the form of lower-case letters, can follow the spectral type to indicate peculiar features of the spectrum.\n\nFor example, 59 Cygni is listed as spectral type B1.5Vnne, indicating a spectrum with the general classification B1.5V, as well as very broad absorption lines and certain emission lines.\n\nThe reason for the odd arrangement of letters in the Harvard classification is historical, having evolved from the earlier Secchi classes and been progressively modified as understanding improved.\n\nDuring the 1860s and 1870s, pioneering stellar spectroscopist Angelo Secchi created the \"Secchi classes\" in order to classify observed spectra. By 1866, he had developed three classes of stellar spectra, shown in the table below.\n\nIn the late 1890s, this classification began to be superseded by the Harvard classification, which is discussed in the remainder of this article.\n\nThe Roman numerals used for Secchi classes should not be confused with the completely unrelated Roman numerals used for Yerkes luminosity classes.\n\nIn the 1880s, the astronomer Edward C. Pickering began to make a survey of stellar spectra at the Harvard College Observatory, using the objective-prism method. A first result of this work was the \"Draper Catalogue of Stellar Spectra\", published in 1890. Williamina Fleming classified most of the spectra in this catalogue.\n\nThe catalogue used a scheme in which the previously used Secchi classes (I to IV) were subdivided into more specific classes, given letters from A to N. Also, the letters O, P, and Q were used – O for stars whose spectra consisted mainly of bright lines, P for planetary nebulae, and Q for stars not fitting into any other class.\n\nIn 1897, another worker at Harvard, Antonia Maury, placed the Orion subtype of Secchi class I ahead of the remainder of Secchi class I, thus placing the modern type B ahead of the modern type A. She was the first to do so, although she did not use lettered spectral types, but rather a series of twenty-two types numbered from I to XXII.\n\nIn 1901, Annie Jump Cannon returned to the lettered types, but dropped all letters except O, B, A, F, G, K, and M, used in that order, as well as P for planetary nebulae and Q for some peculiar spectra. She also used types such as B5A for stars halfway between types B and A, F2G for stars one-fifth of the way from F to G, and so on. Finally, by 1912, Cannon had changed the types B, A, B5A, F2G, etc. to B0, A0, B5, F2, etc. This is essentially the modern form of the Harvard classification system.\n\nA common mnemonic for remembering the order of the spectral type letters, from hottest to coolest, is \"Oh, Be A Fine Guy/Girl, Kiss Me\".\n\nA luminosity classification known as the Mount Wilson system was used to distinguish between stars of different luminosities. This notation system is still sometimes seen on modern spectra.\n\nThe stellar classification system is taxonomic, based on type specimens, similar to classification of species in biology: The categories are defined by one or more standard stars for each category and sub-category, with an associated description of the distinguishing features.\n\nStars are often referred to as \"early\" or \"late\" types. \"Early\" is a synonym for \"hotter\", while \"late\" is a synonym for \"cooler\".\n\nDepending on the context, \"early\" and \"late\" may be absolute or relative terms. \"Early\" as an absolute term would therefore refer to O or B, and possibly A stars. As a relative reference it relates to stars hotter than others, such as \"early K\" being perhaps K0, K1, and K3.\n\n\"Late\" is used in the same way, with an unqualified use of the term indicating stars with spectral types such as K and M, but it can also be used for stars that are cool relative to other stars, as in using \"late G\" to refer to G7, G8, and G9.\n\nIn the relative sense, \"early\" means a lower Arabic numeral following the class letter, and \"late\" means a higher number.\n\nThis obscure terminology is a hold-over from an early 20th century model of stellar evolution, which supposed that stars were powered by gravitational contraction via the Kelvin–Helmholtz mechanism, which is now known to not apply to main sequence stars. If that were true, then stars would start their lives as very hot \"early-type\" stars and then gradually cool down into \"late-type\" stars. This mechanism provided ages of the Sun that were much smaller than what is observed in the geologic record, and was rendered obsolete by the discovery that stars are powered by nuclear fusion. The terms \"early\" and \"late\" were carried over, beyond the demise of the model they were based on.\n\nO-type stars are very hot and extremely luminous, with most of their radiated output in the ultraviolet range. These are the rarest of all main-sequence stars. About 1 in 3,000,000 (0.00003%) of the main-sequence stars in the solar neighborhood are O-type stars. Some of the most massive stars lie within this spectral class. O-type stars frequently have complicated surroundings that make measurement of their spectra difficult.\n\nO-type spectra formerly were defined by the ratio of the strength of the He II λ4541 relative to that of He I λ4471, where λ is the wavelength, measured in ångströms. Spectral type O7 was defined to be the point at which the two intensities are equal, with the He I line weakening towards earlier types. Type O3 was, by definition, the point at which said line disappears altogether, although it can be seen very faintly with modern technology. Due to this, the modern definition uses the ratio of the nitrogen line N IV λ4058 to N III λλ4634-40-42.\n\nO-type stars have dominant lines of absorption and sometimes emission for He II lines, prominent ionized (Si IV, O III, N III, and C III) and neutral helium lines, strengthening from O5 to O9, and prominent hydrogen Balmer lines, although not as strong as in later types. Because they are so massive, O-type stars have very hot cores and burn through their hydrogen fuel very quickly, so they are the first stars to leave the main sequence.\n\nWhen the MKK classification scheme was first described in 1943, the only subtypes of class O used were O5 to O9.5. The MKK scheme was extended to O9.7 in 1971 and O4 in 1978, and new classification schemes that add types O2, O3 and O3.5 have subsequently been introduced.\n\n\nB-type stars are very luminous and blue. Their spectra have neutral helium lines, which are most prominent at the B2 subclass, and moderate hydrogen lines. As O- and B-type stars are so energetic, they only live for a relatively short time. Thus, due to the low probability of kinematic interaction during their lifetime, they are unable to stray far from the area in which they formed, apart from runaway stars.\n\nThe transition from class O to class B was originally defined to be the point at which the He II λ4541 disappears. However, with modern equipment, the line is still apparent in the early B-type stars. Today for main-sequence stars, the B-class is instead defined by the intensity of the He I violet spectrum, with the maximum intensity corresponding to class B2. For supergiants, lines of silicon are used instead; the Si IV λ4089 and Si III λ4552 lines are indicative of early B. At mid B, the intensity of the latter relative to that of Si II λλ4128-30 is the defining characteristic, while for late B, it is the intensity of Mg II λ4481 relative to that of He I λ4471.\n\nThese stars tend to be found in their originating OB associations, which are associated with giant molecular clouds. The Orion OB1 association occupies a large portion of a spiral arm of the Milky Way and contains many of the brighter stars of the constellation Orion. About 1 in 800 (0.125%) of the main-sequence stars in the solar neighborhood are B-type main-sequence stars.\n\nMassive yet non-supergiant entities known as \"Be stars\" are main-sequence stars that notably have, or had at some time, one or more Balmer lines in emission, with the hydrogen-related electromagnetic radiation series projected out by the stars being of particular interest. Be stars are generally thought to feature unusually strong stellar winds, high surface temperatures, and significant attrition of stellar mass as the objects rotate at a curiously rapid rate. Objects known as \"B(e)\" or \"B[e]\" stars possess distinctive neutral or low ionisation emission lines that are considered to have 'forbidden mechanisms', undergoing processes not normally allowed under current understandings of quantum mechanics.\n\n\nA-type stars are among the more common naked eye stars, and are white or bluish-white. They have strong hydrogen lines, at a maximum by A0, and also lines of ionized metals (Fe II, Mg II, Si II) at a maximum at A5. The presence of Ca II lines is notably strengthening by this point. About 1 in 160 (0.625%) of the main-sequence stars in the solar neighborhood are A-type stars.\n\n\nF-type stars have strengthening spectral lines \"H\" and \"K\" of Ca II. Neutral metals (Fe I, Cr I) beginning to gain on ionized metal lines by late F. Their spectra are characterized by the weaker hydrogen lines and ionized metals. Their color is white. About 1 in 33 (3.03%) of the main-sequence stars in the solar neighborhood are F-type stars.\n\n\nG-type stars, including the Sun have prominent spectral lines \"H\" and \"K\" of Ca II, which are most pronounced at G2. They have even weaker hydrogen lines than F, but along with the ionized metals, they have neutral metals. There is a prominent spike in the G band of CH molecules. Class G main-sequence stars make up about 7.5%, nearly one in thirteen, of the main-sequence stars in the solar neighborhood.\n\nG is host to the \"Yellow Evolutionary Void\". Supergiant stars often swing between O or B (blue) and K or M (red). While they do this, they do not stay for long in the yellow supergiant G class, as this is an extremely unstable place for a supergiant to be.\n\n\nK-type stars are orangish stars that are slightly cooler than the Sun. They make up about 12% of the main-sequence stars in the solar neighborhood. There are also giant K-type stars, which range from hypergiants like RW Cephei, to giants and supergiants, such as Arcturus, whereas orange dwarfs, like Alpha Centauri B, are main-sequence stars.\n\nThey have extremely weak hydrogen lines, if they are present at all, and mostly neutral metals (Mn I, Fe I, Si I). By late K, molecular bands of titanium oxide become present. There is a suggestion that K-spectrum stars may potentially increase the chances of life developing on orbiting planets that are within the habitable zone.\n\n\nClass M stars are by far the most common. About 76% of the main-sequence stars in the solar neighborhood are class M stars. However, class M main-sequence stars (red dwarfs) have such low luminosities that none are bright enough to be seen with the unaided eye, unless under exceptional conditions. The brightest known M-class main-sequence star is M0V Lacaille 8760, with magnitude 6.6 (the limiting magnitude for typical naked-eye visibility under good conditions is typically quoted as 6.5), and it is extremely unlikely that any brighter examples will be found.\n\nAlthough most class M stars are red dwarfs, most of the largest ever supergiant stars in the Milky Way are M stars. such as VY Canis Majoris, Antares and Betelgeuse are also class M. Furthermore, the larger, hotter brown dwarfs are late class M, usually in the range of M6.5 to M9.5.\n\nThe spectrum of a class M star contains lines from oxide molecules (in the visible spectrum, especially TiO) and all neutral metals, but absorption lines of hydrogen are usually absent. TiO bands can be strong in class M stars, usually dominating their visible spectrum by about M5. Vanadium(II) oxide bands become present by late M.\n\n\nA number of new spectral types have been taken into use from newly discovered types of stars.\n\nSpectra of some very hot and bluish stars exhibit marked emission lines from carbon or nitrogen, or sometimes oxygen.\n\nClass W or WR represents the Wolf–Rayet stars, notable for spectra lacking hydrogen lines. Instead their spectra are dominated by broad emission lines of highly ionized helium, nitrogen, carbon and sometimes oxygen. They are thought to mostly be dying supergiants with their hydrogen layers blown away by stellar winds, thereby directly exposing their hot helium shells. Class W is further divided into subclasses according to the relative strength of nitrogen and carbon emission lines in their spectra (and outer layers).\n\nWR spectra range is listed below:\n\nAlthough the central stars of most planetary nebulae (CSPNe) show O type spectra, around 10% are hydrogen-deficient and show WR spectra. These are low-mass stars and to distinguish them from the massive Wolf-Rayet stars, their spectra are enclosed in square brackets: e.g. [WC]. Most of these show [WC] spectra, some [WO], and very rarely [WN].\n\nThe \"slash\" stars are O-type stars with WN-like lines in their spectra. The name \"slash\" comes from their printed spectral type having a slash in it (e.g. \"Of/WNL\").\n\nThere is a secondary group found with this spectra, a cooler, \"intermediate\" group designated \"Ofpe/WN9\". These stars have also been referred to as WN10 or WN11, but that has become less popular with the realisation of the evolutionary difference from other Wolf–Rayet stars. Recent discoveries of even rarer stars have extended the range of slash stars as far as O2-3.5If/WN5-7, which are even hotter than the original \"slash\" stars.\n\nThe new spectral types L, T, and Y were created to classify infrared spectra of cool stars. This includes both red dwarfs and brown dwarfs that are very faint in the visible spectrum.\n\nBrown dwarfs, whose energy comes from gravitational attraction alone, cool as they age and so progress to later spectral types. Brown dwarfs start their lives with M-type spectra and will cool through the L, T, and Y spectral classes, faster the less massive they are; the highest-mass brown dwarfs cannot have cooled to Y or even T dwarfs within the age of the universe. Because this leads to an unresolvable overlap between spectral types effective temperature and luminosity for some masses and ages of different L-T-Y types, no distinct temperature or luminosity values can be given.\n\nClass L dwarfs get their designation because they are cooler than M stars and L is the remaining letter alphabetically closest to M. Some of these objects have masses large enough to support hydrogen fusion and are therefore stars, but most are of substellar mass and are therefore brown dwarfs. They are a very dark red in color and brightest in infrared. Their atmosphere is cool enough to allow metal hydrides and alkali metals to be prominent in their spectra.\n\nDue to low surface gravity in giant stars, TiO- and VO-bearing condensates never form. Thus, L-type stars larger than dwarfs can never form in an isolated environment. However, it may be possible for these L-type supergiants to form through stellar collisions, an example of which is V838 Monocerotis while in the height of its luminous red nova eruption.\n\nClass T dwarfs are cool brown dwarfs with surface temperatures between approximately . Their emission peaks in the infrared. Methane is prominent in their spectra.\n\nClasses T and L could be more common than all the other classes combined if recent research is accurate. Because brown dwarfs persist for so long—a few times the age of the universe—in the absence of catastrophic collisions these smaller bodies can only increase in number.\n\nStudy of the number of proplyds (protoplanetary disks, clumps of gas in nebulae from which stars and planetary systems are formed) indicates that the number of stars in the galaxy should be several orders of magnitude higher than what was previously conjectured. It is theorized that these proplyds are in a race with each other. The first one to form will become a protostar, which are very violent objects and will disrupt other proplyds in the vicinity, stripping them of their gas. The victim proplyds will then probably go on to become main-sequence stars or brown dwarfs of the L and T classes, which are quite invisible to us.\n\nBrown dwarfs of spectral class Y are cooler than those of spectral class T and have qualitatively different spectra from them. A total of 17 objects have been placed in class Y as of August 2013. Although such dwarfs have been modelled and detected within forty light-years by the Wide-field Infrared Survey Explorer (WISE) there is no well-defined spectral sequence yet and no prototypes. Nevertheless, several objects have been proposed as spectral classes Y0, Y1, and Y2.\n\nThe spectra of these prospective Y objects display absorption around 1.55 micrometers. Delorme et al. have suggested that this feature is due to absorption from ammonia, and that this should be taken as the indicative feature for the T-Y transition. In fact, this ammonia-absorption feature is the main criterion that has been adopted to define this class. However, this feature is difficult to distinguish from absorption by water and methane, and other authors have stated that the assignment of class Y0 is premature.\n\nThe latest brown dwarf proposed for the Y spectral type, WISE 1828+2650, is a > Y2 dwarf with an effective temperature originally estimated around 300 K, the temperature of the human body. Parallax measurements have, however, since shown that its luminosity is inconsistent with it being colder than ~400 K. The coolest Y dwarf currently known is WISE 0855−0714 with an approximate temperature of 250 K.\n\nThe mass range for Y dwarfs is 9–25 Jupiter masses, but young objects might reach below one Jupiter mass, which means that Y class objects straddle the 13 Jupiter mass deuterium-fusion limit that marks the current IAU division between brown dwarfs and planets.\n\nCarbon-stars are stars whose spectra indicate production of carbon—a byproduct of triple-alpha helium fusion. With increased carbon abundance, and some parallel s-process heavy element production, the spectra of these stars become increasingly deviant from the usual late spectral classes G, K, and M. Equivalent classes for carbon-rich stars are S and C.\n\nThe giants among those stars are presumed to produce this carbon themselves, but some stars in this class are double stars, whose odd atmosphere is suspected of having been transferred from a companion that is now a white dwarf, when the companion was a carbon-star.\n\nOriginally classified as R and N stars, these are also known as \"carbon stars\". These are red giants, near the end of their lives, in which there is an excess of carbon in the atmosphere. The old R and N classes ran parallel to the normal classification system from roughly mid G to late M. These have more recently been remapped into a unified carbon classifier C with N0 starting at roughly C6. Another subset of cool carbon stars are the C-J type stars, which are characterized by the strong presence of molecules of CN in addition to those of CN. A few main-sequence carbon stars are known, but the overwhelming majority of known carbon stars are giants or supergiants. There are several subclasses:\n\nClass S stars form a continuum between class M stars and carbon stars. Those most similar to class M stars have strong ZrO absorption bands analogous to the TiO bands of class M stars, whereas those most similar to carbon stars have strong sodium D lines and weak C bands. Class S stars have excess amounts of zirconium and other elements produced by the s-process, and have more similar carbon and oxygen abundances than class M or carbon stars. Like carbon stars, nearly all known class S stars are asymptotic-giant-branch stars.\n\nThe spectral type is formed by the letter S and a number between zero and ten. This number corresponds to the temperature of the star and approximately follows the temperature scale used for class M giants. The most common types are S3 to S5. The non-standard designation S10 has only been used for the star Chi Cygni when at an extreme minimum.\n\nThe basic classification is usually followed by an abundance indication, following one of several schemes: S2,5; S2/5; S2 Zr4 Ti2; or S2*5. A number following a comma is a scale between 1 and 9 based on the ratio of ZrO and TiO. A number following a slash is a more recent but less common scheme designed to represent the ratio of carbon to oxygen on a scale of 1 to 10, where a 0 would be an MS star. Intensities of zirconium and titanium may be indicated explicitly. Also occasionally seen is a number following an asterisk, which represents the strength of the ZrO bands on a scale from 1 to 5.\n\nIn between the M and S classes, border cases are named MS stars. In a similar way, border cases between the S and C-N classes are named SC or CS. The sequence M → MS → S → SC → C-N is hypothesized to be a sequence of increased carbon abundance with age for carbon stars in the asymptotic giant branch.\n\nThe class D (for Degenerate) is the modern classification used for white dwarfs – low-mass stars that are no longer undergoing nuclear fusion and have shrunk to planetary size, slowly cooling down. Class D is further divided into spectral types DA, DB, DC, DO, DQ, DX, and DZ. The letters are not related to the letters used in the classification of other stars, but instead indicate the composition of the white dwarf's visible outer layer or atmosphere.\n\nThe white dwarf types are as follows:\n\nThe type is followed by a number giving the white dwarf's surface temperature. This number is a rounded form of 50400/\"T\", where \"T\" is the effective surface temperature, measured in kelvins. Originally, this number was rounded to one of the digits 1 through 9, but more recently fractional values have started to be used, as well as values below 1 and above 9.\n\nTwo or more of the type letters may be used to indicate a white dwarf that displays more than one of the spectral features above.\n\nExtended white dwarf spectral types:\n\nA different set of spectral peculiarity symbols are used for white dwarfs than for other types of stars:\n\nStellar remnants are objects associated with the death of stars. Included in the category are white dwarfs, and as can be seen from the radically different classification scheme for class D, non-stellar objects are difficult to fit into the MK system.\n\nThe Hertzsprung-Russell diagram, which the MK system is based on, is observational in nature so these remnants cannot easily be plotted on the diagram, or cannot be placed at all. Old neutron stars are relatively small and cold, and would fall on the far right side of the diagram. Planetary nebulae are dynamic and tend to quickly fade in brightness as the progenitor star transitions to the white dwarf branch. If shown, a planetary nebula would be plotted to the right of the diagram's upper right quadrant. A black hole emits no visible light of its own, and therefore would not appear on the diagram.\n\nSeveral spectral types, all previously used for non-standard stars in the mid-20th century, have been replaced during revisions of the stellar classification system. They may still be found in old editions of star catalogs: R and N have been subsumed into the new C class as C-R and C-N.\n\n\n"}
{"id": "29374497", "url": "https://en.wikipedia.org/wiki?curid=29374497", "title": "The Flying Grass Carpet", "text": "The Flying Grass Carpet\n\nThe Flying Grass Carpet is a huge rug entirely made of artificial grass. It travels around the world, as a temporary landscape. It's intended as space to play on and enjoy, but can also be used for picnics, open-air festivals and sports .\n\nThe Flying Grass Carpet has been to the following cities: \n\nThe designers of The Flying Grass Carpet live in Rotterdam, a city known for its designers and architects. The designers of the Flying Grass Carpet are fond of cities and city-life but are concerned about the loss of quality of the public space in a lot of cities. To make a positive gesture they created The Flying Grass Carpet.\n\nThe Flying Grass Carpet travels to different places around the world, and stays in the different locations for a short period of time. It's intended to function as an actual park, allowing people to enjoy all the activities they can normally only enjoy in the park, in the middle of the city. The Flying Grass Carpet is also intended to connect different cities and their citizens with each other, and create what its designers call a \"worldwide shared public domain\". \n\nThe carpet contains several types and colors of grass. It weighs 6375 kilograms (6.5 kg per square meter). The size of the carpet is adjustable, and can be up to 25 by 36 meters.\n\nThe Flying Grass Carpet won a 'Dutch Design Award' in 2009\n\n"}
{"id": "21537836", "url": "https://en.wikipedia.org/wiki?curid=21537836", "title": "Violent non-state actor", "text": "Violent non-state actor\n\nIn international relations violent non-state actors (VNSA) (also known as non-state armed actors or non-state armed groups) are individuals and groups that are wholly or partly independent of state governments and which threaten or use violence to achieve their goals.\n\nVNSAs vary widely in their goals, size, and methods. They may include narcotics cartels, popular liberation movements, religious and ideological organizations, corporations (e.g. private military contractors), self-defence militia, and paramilitary groups established by state governments to further their interests.\n\nWhile some VNSAs oppose governments, others are allied to them. Some VNSAs are organized as paramilitary groups, adopting methods and structure similar to those of state armed forces. Others may be informally structured and use violence in other ways, such as kidnapping, using improvised explosive devices, or hacking into computer systems.\n\nThomas, Kiser, and Casebeer asserted in 2005 that \"VNSA play a prominent, often destabilizing role in nearly every humanitarian and political crisis faced by the international community\". As a new type of actor in international relations, VNSAs represent a departure from the traditional Westphalian sovereignty system of states in two ways: by providing an alternative to state governance; and by challenging the state's monopoly of violence.\n\nPhil Williams stated in 2008 that in the 21st century, they \"have become a pervasive challenge to nation-states\". Williams argues that VNSAs develop out of poor state governance but also contribute to the further undermining of governance by the state. He explains that when weak states are \"unable to create or maintain the loyalty and allegiance of their populations\", \"individuals and groups typically revert to or develop alternative patterns of affiliation\". This causes the family, tribe, clan or other group to become \"the main reference points for political action, often in opposition to the state\". According to Williams, globalization has \"not only... challenged individual state capacity to manage economic affairs, it has also provided facilitators and force multipliers for VNSAs\". Transnational flows of arms, for example, are no longer under the exclusive surveillance of states. Globalization helps VNSAs develop transnational social capital and alliances as well as funding opportunities.\n\nThe term has been used in several papers published by the US military.\n\nSome common and influential types of VNSAs include:\nPhil Williams, in an overview article, identifies five types of VNSAs:\n\nThere is no commonly accepted definition of \"terrorism\", and the term is frequently used as a political tactic to denounce opponents whose status as terrorists is disputed. An attempt at a global definition appears in the working draft of Comprehensive Convention Against International Terrorism, which defines terrorism as a type of act, rather than as a type of group. Specifically, \"terrorism\" in the draft refers to the threatened or actual intentional injury to others, and serious damage to property resulting in major economic loss:\n\nSince the definition encompasses the actions of some violent non-state actors (and of some state actors) and not others, disagreements remain and the treaty has yet to be agreed, . For example, the Organisation of Islamic Cooperation has called for acts of terrorism to be distinguished from:\n\nViolent non-state actors have drawn international condemnation for relying heavily on children under the age of 18 as combatants, scouts, porters, spies, informants, and in other roles (although many state armed forces also recruit children). In 2017, for example, the United Nations identified 14 countries where children were widely used by armed groups: Afghanistan, Colombia, Central African Republic, Democratic Republic of the Congo, Iraq, Mali, Myanmar, Nigeria, The Philippines, Somalia, South Sudan, Sudan, Syria, and Yemen.\n\nNot all armed groups use children, and approximately 60 that used to do so have entered agreements to reduce or end the practice since 1999. For example, by 2017 the Moro Islamic Liberation Front (MILF) in the Philippines had released nearly 2,000 children from its ranks, and the FARC-EP guerilla movement in Colombia agreed in 2016 to stop recruiting children. In other situations, the use of children was increasing in 2017, particularly in Afghanistan, Iraq, Nigeria and Syria, where Islamist militants and groups opposing them intensified efforts to recruit children.\n\nResearchers at the Overseas Development Institute propose that engagement with VNSAs, which they call armed non-state actors, is essential to humanitarian efforts in conflicts. They claim that it is often necessary to do so to facilitate access to those affected and to provide humanitarian assistance. However, humanitarian agencies often fail to engage strategically with VNSAs. This tendency has strengthened since the end of the Cold War, partly because of the strong discouragement of humanitarian engagement with VNSAs in counterterrorism legislation and donor funding restrictions. In their opinion, further study is necessary to identify ways in which humanitarian agencies can develop productive dialogue with VNSAs.\n\nThe International Security Department and the International Law Programme at Chatham House are seeking to understand the dynamics that will determine support for a principle-based approach to engagement by humanitarian actors with VNSAs.\n\n\n\n\n"}
{"id": "4021184", "url": "https://en.wikipedia.org/wiki?curid=4021184", "title": "Vitality", "text": "Vitality\n\nVitality is life, life force, health, youth, or ability to live or exist. The word vitality is derived from the Latin word \"vita\", which means \"life.\"\n\nAccording to Jain philosophy, there are ten vitalities or life-principles:-\n\nThe table below summaries the vitalities, living beings possess in accordance to their senses.\nAccording to major Jain text, Tattvarthsutra: \"The severance of vitalities out of passion is injury\".\n\nIn the context of urban planning, that vitality of a place is its capacity to grow or develop its liveliness and level of economic activity.\n"}
{"id": "213446", "url": "https://en.wikipedia.org/wiki?curid=213446", "title": "World view", "text": "World view\n\nA world view or worldview is the fundamental cognitive orientation of an individual or society encompassing the whole of the individual's or society's knowledge and point of view. A world view can include natural philosophy; fundamental, existential, and normative postulates; or themes, values, emotions, and ethics. The term is a calque of the German word Weltanschauung , composed of \"Welt\" ('world') and \"Anschauung\" ('view' or 'outlook'). The German word is also used in English.\n\nIt is a concept fundamental to German philosophy and epistemology and refers to a \"wide world perception\". Additionally, it refers to the framework of ideas and beliefs forming a global description through which an individual, group or culture watches and interprets the world and interacts with it.\n\nWorldview remains a confused and confusing concept in English, used very differently by linguists and sociologists. It is for this reason that James W. Underhill suggests five subcategories: world-perceiving, world-conceiving, cultural mindset, personal world, and perspective.\n\nWorldviews are often taken to operate at a conscious level, directly accessible to articulation and discussion, as opposed to existing at a deeper, pre-conscious level, such as the idea of \"ground\" in Gestalt psychology and media analysis. However, core worldview beliefs are often deeply rooted, and so are only rarely reflected on by individuals, and are brought to the surface only in moments of crises of faith.\n\nThe Prussian philologist Wilhelm von Humboldt (1767–1835) originated the idea that language and worldview are inextricable. Humboldt saw language as part of the creative adventure of mankind. Culture, language and linguistic communities developed simultaneously and could not do so without one another. In stark contrast to linguistic determinism, which invites us to consider language as a constraint, a framework or a prison house, Humboldt maintained that speech is inherently and implicitly creative. Human beings take their place in speech and continue to modify language and thought by their creative exchanges. \n\nEdward Sapir (1884–1939) also gives an account of the relationship between thinking and speaking in English.\n\nThe linguistic relativity hypothesis of Benjamin Lee Whorf (1897–1941) describes how the syntactic-semantic structure of a language becomes an underlying structure for the world view or \"Weltanschauung\" of a people through the organization of the causal perception of the world and the linguistic categorization of entities. As linguistic categorization emerges as a representation of worldview and causality, it further modifies social perception and thereby leads to a continual interaction between language and perception.\n\nWhorf's hypothesis became influential in the late 1940s, but declined in prominence after a decade. In the 1990s, new research gave further support for the linguistic relativity theory in the works of Stephen Levinson (1947–) and his team at the Max Planck institute for psycholinguistics at Nijmegen, Netherlands.\nThe theory has also gained attention through the work of Lera Boroditsky at Stanford University.\n\nOne of the most important concepts in cognitive philosophy and cognitive sciences is the German concept of \"Weltanschauung\". This expression has often been used to refer to the \"wide worldview\" or \"wide world perception\" of a people, family, or person. The \"Weltanschauung\" of a people originates from the unique world experience of a people, which they experience over several millennia.The language of a people reflects the \"Weltanschauung\" of that people in the form of its syntactic structures and untranslatable connotations and its denotations.\n\nThe term \"Weltanschauung\" is often wrongly attributed to Wilhelm von Humboldt, the founder of German ethnolinguistics. However, as Jürgen Trabant points out, and as James W. Underhill reminds us, Humboldt's key concept was \"Weltansicht\". \"Weltansicht\" was used by Humboldt to refer to the overarching conceptual and sensorial apprehension of reality shared by a linguistic community (Nation). On the other hand, \"Weltanschauung\", first used by Kant and later popularized by Hegel, was always used in German and later in English to refer more to philosophies, ideologies and cultural or religious perspectives, than to linguistic communities and their mode of apprehending reality.\n\nA worldview can be expressed as the \"fundamental cognitive, affective, and evaluative presuppositions a group of people make about the nature of things, and which they use to order their lives.\"\n\nIf it were possible to draw a map of the world on the basis of \"Weltanschauung\", it would probably be seen to cross political borders—\"Weltanschauung\" is the product of political borders and common experiences of a people from a geographical region, environmental-climatic conditions, the economic resources available, socio-cultural systems, and the language family. (The work of the population geneticist Luigi Luca Cavalli-Sforza aims to show the gene-linguistic co-evolution of people).\n\nIf the Sapir–Whorf hypothesis is correct, the worldview map of the world would be similar to the linguistic map of the world. However, it would also almost coincide with a map of the world drawn on the basis of music across people.\n\nAs natural language becomes manifestations of world perception, the literature of a people with common \"Weltanschauung\" emerges as holistic representations of the wide world perception of the people. Thus the extent and commonality between world folk-epics becomes a manifestation of the commonality and extent of a worldview.\n\nEpic poems are shared often by people across political borders and across generations. Examples of such epics include the Nibelungenlied of the Germanic people, the Iliad for the Ancient Greeks and Hellenized societies, the Silappadhikaram of the Tamil people, the Ramayana and Mahabharata of the Hindus, the Gilgamesh of the Mesopotamian-Sumerian civilization and the people of the Fertile Crescent at large, The Book of One Thousand and One Nights (Arabian nights) of the Arab world and the Sundiata epic of the Mandé people.\n\nA worldview, according to terror management theory (TMT), serves as a buffer against death anxiety. It is theorised that living up to the ideals of one's worldview provides a sense of self-esteem which provides a sense of transcending the limits of human life (e.g. literally, as in religious belief in immortality, symbolically, as in art works or children to live on after one's death, or in contributions to one's culture). Evidence in support of terror management theory includes a series of experiments by Jeff Schimel and colleagues in which a group of Canadians found to score highly on a measure of patriotism were asked to read an essay attacking the dominant Canadian worldview.\n\nUsing a test of death-thought accessibility (DTA), involving an ambiguous word completion test (e.g. \"COFF__\" could either be completed as either \"COFFEE\" or \"COFFIN\" or \"COFFER\"), participants who had read the essay attacking their worldview were found to have a significantly higher level of DTA than the control group, who read a similar essay attacking Australian cultural values. Mood was also measured following the worldview threat, to test whether the increase in death thoughts following worldview threat were due to other causes, for example, anger at the attack on one's cultural worldview. No significant changes on mood scales were found immediately following the worldview threat.\n\nTo test the generalisability of these findings to groups and worldviews other than those of nationalistic Canadians, Schimel \"et al\" conducted a similar experiment on a group of religious individuals whose worldview included that of creationism. Participants were asked to read an essay which argued in support of the theory of evolution, following which the same measure of DTA was taken as for the Canadian group. Religious participants with a creationist worldview were found to have a significantly higher level of death-thought accessibility than those of the control group.\n\nGoldenberg \"et al\" found that highlighting the similarities between humans and other animals increases death-thought accessibility, as does attention to the physical rather than meaningful qualities of sex.\n\nThe term World View denotes a comprehensive set of opinions, seen as an organic unity, about the world as the medium and exercise of human existence. World View serves as a framework for generating various dimensions of human perception and experience like knowledge, politics, economics, religion, culture, science and ethics. For example, worldview of causality as \"uni-directional\", \"cyclic\", or \"spiral\" generates a framework of the world that reflects these systems of causality.\n\nAn unidirectional view of causality is present in some monotheistic views of the world with a beginning and an end and a single great force with a single end (e.g., Christianity and Islam), while a cyclic worldview of causality is present in religious traditions which are cyclic and seasonal and wherein events and experiences recur in systematic patterns (e.g., Zoroastrianism, Mithraism and Hinduism). These worldviews of causality not only underlie religious traditions but also other aspects of thought like the purpose of history, political and economic theories, and systems like democracy, authoritarianism, anarchism, capitalism, socialism and communism.\n\nThe worldview of a linear and non-linear causality generates various related/conflicting disciplines and approaches in scientific thinking. The \"Weltanschauung\" of the temporal contiguity of act and event leads to underlying diversifications like \"determinism\" vs. \"free will\". A worldview of free will leads to disciplines that are governed by simple laws that remain constant and are static and empirical in scientific method, while a worldview of determinism generates disciplines that are governed with generative systems and rationalistic in scientific method.\n\nSome forms of philosophical naturalism and materialism reject the validity of entities inaccessible to natural science. They view the scientific method as the most reliable model for building an understanding of the world.\n\nNishida Kitaro wrote extensively on \"the Religious Worldview\" in exploring the philosophical significance of Eastern religions.\n\nAccording to Neo-Calvinist David Naugle's \"World view: The History of a Concept\", \"Conceiving of Christianity as a worldview has been one of the most significant developments in the recent history of the church.\"\n\nThe Christian thinker James W. Sire defines a worldview as \"a commitment, a fundamental orientation of the heart, that can be expressed as a story or in a set of presuppositions (assumptions which may be true, partially true, or entirely false) which we hold (consciously or subconsciously, consistently or inconsistently) about the basic construction of reality, and that provides the foundation on which we live and move and have our being.\" He suggests that \"we should all think in terms of worldviews, that is, with a consciousness not only of our own way of thought but also that of other people, so that we can first understand and then genuinely communicate with others in our pluralistic society.\"\n\nThe commitment mentioned by James W. Sire can be extended further. The worldview increases the commitment to serve the world. With the change of a person's view towards the world, he/she can be motivated to serve the world. This serving attitude has been illustrated by Tareq M Zayed as the 'Emancipatory Worldview' in his writing \"History of emancipatory worldview of Muslim learners\".\n\nThe question mentioned above - on whether the super-smart machines, that is, any \"superintelligences\", as expected by some, could have worldviews - is interesting in this context and this would influence human worldviews. \n\nThe philosophical importance of worldviews became increasingly clear during the 20th century for a number of reasons, such as increasing contact between cultures, and the failure of some aspects of the Enlightenment project, such as the rationalist project of attaining all truth by reason alone. Mathematical logic showed that fundamental choices of axioms were essential in deductive reasoning and that, even having chosen axioms not everything that was true in a given logical system could be proven. Some philosophers believe the problems extend to \"the inconsistencies and failures which plagued the Enlightenment attempt to identify universal moral and rational principles\"; although Enlightenment principles such as universal suffrage and the universal declaration of human rights are accepted, if not taken for granted, by many.\n\nThe theory of relativity offers a Weltanschauung that is revolting to absolute space and time, yet provides a context for modern theories of electromagnetism and gravity. In a book review for a new undergraduate textbook on relativity by Wolfgang Rindler, Kenneth Jacobs noted that \"during the post-Sputnik era, special relativity began to take its rightful place in the undergraduate curriculum\". On the adoption of the Weltanschauung, he notes, \"The historical impact of any world picture is ... partly attributable to the zeal of the promulgators and to the efficacy of their teachings.\"\n\nPhilosophers also distinguish the manifest image from the scientific image. These phrases are due to the American 20th century philosopher Wilfrid Sellars. This is one angle on the ancient philosophical distinction between appearance and reality which is particularly pertinent to everyday contemporary living. Indeed, many believe that the scientific image, with its reductionist methodology, will undermine our sense of individual freedom and responsibility. So, many worry that as science advances, particularly cognitive neuroscience, we will be dehumanized. This certainly has powerful Nietzschean undertones. When our immediately given, manifest (sc. obvious) self-conception is shaken, what is lost for the individual and society? And does it have to be that way? Some questions well worth working on, then, are those concerning the refinement of the manifest view of such centrally important concepts such as free will, the self and individuality, and the possibility of real or lived meaning.\n\nWhile Leo Apostel and his followers clearly hold that individuals can construct worldviews, other writers regard worldviews as operating at a community level, or in an unconscious way. For instance, if one's worldview is fixed by one's language, as according to a strong version of the Sapir–Whorf hypothesis, one would have to learn or invent a new language in order to construct a new worldview.\n\nAccording to Apostel, a worldview is an ontology, or a descriptive model of the world. It should comprise these six elements:\n\nFrom across the world across all of the cultures, Roland Muller has suggested that cultural world views can be broken down into three separate world views. It is not simple enough to say that each person is one of these three cultures. Instead, each individual is a mix of the three. For example, a person may be raised in a Power–Fear society, in an Honor–Shame family, and go to school under a Guilt–Innocence system.\n\nIn a Guilt–Innocence focused culture, schools focus on deductive reasoning, cause and effect, good questions, and process. Issues are often seen as black and white. Written contracts are paramount. Communication is direct, and can be blunt.\n\nSocieties with a predominantly Honor–Shame worldview teach children to make honorable choices according to the situations they find themselves in. Communication, interpersonal interaction, and business dealings are very relationship-driven, with every interaction having an effect on the Honor–Shame status of the participants. In an Honor–Shame society the crucial objective is to avoid shame and to be viewed honorably by other people. The Honor–Shame paradigm is especially strong in most regions of Asia.\n\nSome cultures can be seen very clearly in operating under a Power–Fear worldview. In these cultures it is very important to assess the people around you and know where they fall in line according to their level of power. This can be used for good or for bad. A benevolent king rules with power and his citizens fully support him wielding that power. On the converse, a ruthless dictator can use his power to create a culture of fear where his citizens are oppressed.\n\nAccording to Michael Lind, \"a worldview is a more or less coherent understanding of the nature of reality, which permits its holders to interpret new information in light of their preconceptions. Clashes among worldviews cannot be ended by a simple appeal to facts. Even if rival sides agree on the facts, people may disagree on conclusions because of their different premises.\" This is why politicians often seem to talk past one another, or ascribe different meanings to the same events. Tribal or national wars are often the result of incompatible worldviews. Lind has organized American political worldviews into five categories:\nLind argues that even though not all people will fit neatly into only one category or the other, their core worldview shape how they frame their arguments.\n\nOne can think of a worldview as comprising a number of basic beliefs which are philosophically equivalent to the axioms of the worldview considered as a logical theory. These basic beliefs cannot, by definition, be proven (in the logical sense) within the worldview precisely because they are axioms, and are typically argued \"from\" rather than argued \"for\". However their coherence can be explored philosophically and logically.\n\nIf two different worldviews have sufficient common beliefs it may be possible to have a constructive dialogue between them.\n\nOn the other hand, if different worldviews are held to be basically incommensurate and irreconcilable, then the situation is one of cultural relativism and would therefore incur the standard criticisms from philosophical realists.\nAdditionally, religious believers might not wish to see their beliefs relativized into something that is only \"true for them\".\nSubjective logic is a belief-reasoning formalism where beliefs explicitly are subjectively held by individuals but where a consensus between different worldviews can be achieved.\n\nA third alternative sees the worldview approach as only a methodological relativism, as a suspension judgment about the truth of various belief systems but not a declaration that there is no global truth. For instance, the religious philosopher Ninian Smart begins his \"Worldviews: Cross-cultural Explorations of Human Beliefs\" with \"Exploring Religions and Analysing Worldviews\" and argues for \"the neutral, dispassionate study of different religious and secular systems—a process I call worldview analysis.\"\n\nThe comparison of religious, philosophical or scientific worldviews is a delicate endeavor, because such worldviews start from different presuppositions and cognitive values. Clément Vidal has proposed metaphilosophical criteria for the comparison of worldviews, classifying them in three broad categories:\n\n\nDavid Bell has raised interesting questions on worldviews for the designers of superintelligences – machines much smarter than humans.  'Would they need worldviews, where would they get their worldviews and what would they be like?'. The answers would have to relate to, for example, Christian worldviews. Some of the people who consider features of superintelligences say they will have characteristics that are often associated with divinity, raising big open questions for Christian believers. For example, very advanced machines could, perhaps,   ultimately engender in people a terrified reverence and mystical awe in the light of, say, an artificial agent's impressive understanding of the human condition. And perhaps some humans might even be induced to 'worship and serve the creature rather than the Creator'? On the other hand what would the agent's relationship to God be? Anyone attempting to accommodate concepts such as an omnipotent, personal creator's sacrificial, emotional, spiritual and attitudinal demands being made of any man-made entity, superintelligent or not, could be said to have strayed into \"terra prohibita\" theologically, of course. And how would the worldviews of any superintelligences handle the relationships with what it might regard as its \"human\" 'creator'? \n\n"}
